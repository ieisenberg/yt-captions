[
  {
    "start": "0",
    "end": "79000"
  },
  {
    "text": "all right good morning everyone or good afternoon or wherever you are uh thank you for joining me today my",
    "start": "7279",
    "end": "13679"
  },
  {
    "text": "name is yusri muhammad i work for telstra purple and i'm here today to talk you",
    "start": "13679",
    "end": "19279"
  },
  {
    "text": "through a journey of tips and trips for developing efficient big data applications and",
    "start": "19279",
    "end": "24960"
  },
  {
    "text": "operating them as well so a switch to",
    "start": "24960",
    "end": "30240"
  },
  {
    "text": "my presentation so initially two seconds about myself originally from egypt hence",
    "start": "30240",
    "end": "37280"
  },
  {
    "text": "the funny accent and then now an australian citizen i started with redefine a few years ago",
    "start": "37280",
    "end": "45039"
  },
  {
    "text": "and then rd5 has been acquired by telstra and then last year a few entities inside tetra combined",
    "start": "45039",
    "end": "52399"
  },
  {
    "text": "into a new brand called called telstra purple and you can imagine that at telstra we",
    "start": "52399",
    "end": "58000"
  },
  {
    "text": "have more than a few data sets to handle so for the last two years i was working",
    "start": "58000",
    "end": "63039"
  },
  {
    "text": "with um a bunch of big data applications and pipelines and i've got uh",
    "start": "63039",
    "end": "68560"
  },
  {
    "text": "some experience and knowledge and i'd like to share that with you but before sharing the details i'd like to give you a brief uh",
    "start": "68560",
    "end": "75680"
  },
  {
    "text": "background on beget applications i'm assuming that most of you might be aware of that but just",
    "start": "75680",
    "end": "80799"
  },
  {
    "start": "79000",
    "end": "79000"
  },
  {
    "text": "a quick reminder so initially it was google around the",
    "start": "80799",
    "end": "86159"
  },
  {
    "text": "1998 uh being founded and then google needed to index the",
    "start": "86159",
    "end": "91759"
  },
  {
    "text": "internet and process uh massive amounts of data so invented map reviews plus a",
    "start": "91759",
    "end": "97200"
  },
  {
    "text": "bunch of other algorithms as well and this triggered the introduction of hadoop platform",
    "start": "97200",
    "end": "104320"
  },
  {
    "text": "and around 2012 a new resource manager was introduced in hadoop to make things",
    "start": "104320",
    "end": "109520"
  },
  {
    "text": "uh much more reliable but hadoop still was a bit slow so spot",
    "start": "109520",
    "end": "114640"
  },
  {
    "text": "has been invented to do stuff more in memory and make things fast and uh",
    "start": "114640",
    "end": "120880"
  },
  {
    "text": "2.0 version of spark has been introduced such that",
    "start": "120880",
    "end": "126560"
  },
  {
    "text": "people with sql knowledge can interact with the application and",
    "start": "126560",
    "end": "133120"
  },
  {
    "text": "use it without the need to learn stuff like java or scala and then",
    "start": "133120",
    "end": "139040"
  },
  {
    "text": "a few months ago we have versions 3.0 which is which is more about operating in environments like kubernetes and so",
    "start": "139040",
    "end": "145440"
  },
  {
    "text": "on so most of my demos today and examples and so on and the whole inter sorry",
    "start": "145440",
    "end": "152160"
  },
  {
    "text": "whole conversation will be around hadoop and spock and this kind of tools or ecosystem",
    "start": "152160",
    "end": "157280"
  },
  {
    "text": "i'm assuming that you might have um some sort of experience or knowledge about it but yeah even even if even even",
    "start": "157280",
    "end": "165200"
  },
  {
    "text": "if not most of the stuff you will see today would be very very general and can be",
    "start": "165200",
    "end": "170400"
  },
  {
    "text": "applied in many many other tools uh stuff like azure synapse analytics all right",
    "start": "170400",
    "end": "176160"
  },
  {
    "text": "so the agenda for today is very uh kind of high level here uh",
    "start": "176160",
    "end": "181360"
  },
  {
    "start": "177000",
    "end": "177000"
  },
  {
    "text": "six themes the first one will be around storage and then we'll have a look on file systems used to uh",
    "start": "181360",
    "end": "186800"
  },
  {
    "text": "manage and upgrade data for big data applications and then we'll have a look about on skewness",
    "start": "186800",
    "end": "192400"
  },
  {
    "text": "and data quality and we'll wrap up with a quick overview about governance and",
    "start": "192400",
    "end": "197840"
  },
  {
    "text": "some tooling ideas all right let's dig in into the first high level tip",
    "start": "197840",
    "end": "204560"
  },
  {
    "text": "cool so initially big data the first view of big data application is normally",
    "start": "204560",
    "end": "209599"
  },
  {
    "text": "volume storage so um we need first to understand your search format most of the big data",
    "start": "209599",
    "end": "216080"
  },
  {
    "text": "applications you will find that we use two columnar format two common ones uh orc",
    "start": "216080",
    "end": "221840"
  },
  {
    "text": "and part k and you need to understand the schema of your data set and how it works with that storage",
    "start": "221840",
    "end": "228400"
  },
  {
    "text": "format you need to understand i mean or read the schema or the specs of the storage format and understand if have an",
    "start": "228400",
    "end": "234239"
  },
  {
    "text": "integer or a string string or whatever is stored in such a storage format and then",
    "start": "234239",
    "end": "240640"
  },
  {
    "text": "uh when you see the outcome of your big data pipelines you might need to think okay it is just generating too much data",
    "start": "240640",
    "end": "247040"
  },
  {
    "text": "can i uh optimize it a little bit so you might need to experiment until you find an equilibrium between",
    "start": "247040",
    "end": "254159"
  },
  {
    "text": "optimizing the storage costs and also making uh compute fast because the less data",
    "start": "254159",
    "end": "261600"
  },
  {
    "text": "you store the general speaking the faster your compute will be all right so let's go to the demo i'm",
    "start": "261600",
    "end": "269280"
  },
  {
    "text": "quite here for the slide but we'll have too many demos all right so our first demo here will be",
    "start": "269280",
    "end": "275520"
  },
  {
    "text": "around storage and it will be mainly about creating",
    "start": "275520",
    "end": "280560"
  },
  {
    "text": "a frame think about it a data frame or a table or something like that and uh per listing get to desk",
    "start": "280560",
    "end": "286720"
  },
  {
    "text": "and every now and ah every step will add a column or modify a column type or something like that and then",
    "start": "286720",
    "end": "292960"
  },
  {
    "text": "measure the amount of storage consumed and see what will work better all right",
    "start": "292960",
    "end": "299040"
  },
  {
    "text": "so i'm running now uh vs code connected to",
    "start": "299040",
    "end": "304320"
  },
  {
    "text": "wsl2 on windows so i'll start spark shell feed it but actually i need to go into",
    "start": "304320",
    "end": "312080"
  },
  {
    "text": "my demo folder first crop shell input and",
    "start": "312080",
    "end": "318639"
  },
  {
    "text": "demo wait for the shell",
    "start": "318639",
    "end": "324560"
  },
  {
    "text": "oops come on okay so initially",
    "start": "324560",
    "end": "330560"
  },
  {
    "text": "we'll create a data frame of 10 million rows and it will contain by default this kind of api will create a data frame",
    "start": "330560",
    "end": "336400"
  },
  {
    "text": "with a single integer or long column all right and then we'll persist it to",
    "start": "336400",
    "end": "342240"
  },
  {
    "text": "disk and see uh how much storage do we need here so this one is still kind of",
    "start": "342240",
    "end": "349039"
  },
  {
    "text": "yep kicking off so to create a 10 million uh record data frame it will take we need 200k okay",
    "start": "349039",
    "end": "357039"
  },
  {
    "text": "fine cool uh by the way most of this kind of column or data format for maps",
    "start": "357039",
    "end": "363120"
  },
  {
    "text": "they have a built-in completion so yeah you can have 10 million records in 200k let's assume that we'll need to",
    "start": "363120",
    "end": "370000"
  },
  {
    "text": "add another string column and the string column will contain a prefix and suffix",
    "start": "370000",
    "end": "375039"
  },
  {
    "text": "and some random number in the range from 0 to 1 2 million whatever while hit",
    "start": "375039",
    "end": "380800"
  },
  {
    "text": "enter to create to run the next snippet here",
    "start": "380800",
    "end": "386479"
  },
  {
    "text": "so you can see here the sample data for the next example which is an id or",
    "start": "386560",
    "end": "392160"
  },
  {
    "text": "sequence number and the string column in this kind of format you can see it's a random number in the range of",
    "start": "392160",
    "end": "397520"
  },
  {
    "text": "zero two million and boom we increase increase into like",
    "start": "397520",
    "end": "402560"
  },
  {
    "text": "what 71 megabytes of data i needed yeah that's too much i mean going from 200k to 71 makes us a",
    "start": "402560",
    "end": "409440"
  },
  {
    "text": "bit too much anyway let's go to the next step and for this step",
    "start": "409440",
    "end": "414960"
  },
  {
    "text": "i change the cardinality of that string column to be just 10 unique values okay",
    "start": "414960",
    "end": "420479"
  },
  {
    "text": "so if you can see here we have set the id we have a string column but the number of unique",
    "start": "420479",
    "end": "427120"
  },
  {
    "text": "combinations of the string column is much much more smaller now and you can see here that when we have",
    "start": "427120",
    "end": "432960"
  },
  {
    "text": "less cardinality the amount of that i needed to store the array is uh",
    "start": "432960",
    "end": "438479"
  },
  {
    "text": "i got 120 times smaller or something like that different",
    "start": "438479",
    "end": "444000"
  },
  {
    "text": "okay so to understand why what is happening and if you read the specs of worthy or parquet data",
    "start": "444000",
    "end": "450080"
  },
  {
    "text": "data formats you'll understand that strings are stored as dictionaries and the individual value of each string is just",
    "start": "450080",
    "end": "456880"
  },
  {
    "text": "an index into that dictionary so the less the cardinality less the data",
    "start": "456880",
    "end": "462560"
  },
  {
    "text": "or the less storage you will need okay let's go into the next one which will be",
    "start": "462560",
    "end": "468400"
  },
  {
    "text": "about what happened if we just that string size is double like i'll double the",
    "start": "468400",
    "end": "474639"
  },
  {
    "text": "prefix and suffix but it will still have the same specs",
    "start": "474639",
    "end": "480080"
  },
  {
    "text": "so if we are talking about dictionaries then probably even if we double the size of the string",
    "start": "480080",
    "end": "485840"
  },
  {
    "text": "itself should it matter much so we still",
    "start": "485840",
    "end": "491280"
  },
  {
    "text": "need 5.2 megabytes to sort which is good all right so actually for strings as",
    "start": "491280",
    "end": "496400"
  },
  {
    "text": "long as that coordinate is a bit low then you don't need to worry about that",
    "start": "496400",
    "end": "503199"
  },
  {
    "text": "what about constant data or columns assuming that you would like to put a version or some piece of metadata",
    "start": "503199",
    "end": "508639"
  },
  {
    "text": "metadata or whatever so hit this one and let's see what will happen so still",
    "start": "508639",
    "end": "516159"
  },
  {
    "text": "the same but we are adding a constant string and the constant decimal here yeah increased a little bit from five to",
    "start": "516159",
    "end": "522719"
  },
  {
    "text": "roughly nine but a small about um metadata overhead inside the file itself",
    "start": "522719",
    "end": "529680"
  },
  {
    "text": "so the increase is really uh marginal here all right so next i'll hit enter first",
    "start": "529680",
    "end": "536080"
  },
  {
    "text": "before i explain this one because it will take a minute or something so next step assume that we have stuff",
    "start": "536080",
    "end": "542399"
  },
  {
    "text": "like complex data types and array or struct or something like that so here i'm adding an",
    "start": "542399",
    "end": "549519"
  },
  {
    "text": "array array column the column will contain an array of numbers 100 elements and every one of them will",
    "start": "549519",
    "end": "556560"
  },
  {
    "text": "be a random number but as you know as you are using this kind of pseudo-random",
    "start": "556560",
    "end": "562000"
  },
  {
    "text": "number generators it depends on the seed so if you have a seed",
    "start": "562000",
    "end": "567440"
  },
  {
    "text": "with 10 000 unique combinations then the elements of the array will have 10 sorry",
    "start": "567440",
    "end": "572880"
  },
  {
    "text": "1000 unique combinations of the array elements so you can imagine now that it's taking",
    "start": "572880",
    "end": "578640"
  },
  {
    "text": "uh more time now to persist the new data frame containing all the",
    "start": "578640",
    "end": "584480"
  },
  {
    "text": "previous stuff along with yay done",
    "start": "584480",
    "end": "591560"
  },
  {
    "text": "along with the new array column and you can see here that we went from nine megabytes to",
    "start": "591680",
    "end": "597560"
  },
  {
    "text": "974. it's like two orders of magnitude okay so the",
    "start": "597560",
    "end": "603120"
  },
  {
    "text": "most important thing to remember here is that normally for um but",
    "start": "603120",
    "end": "609279"
  },
  {
    "text": "complex types like arrays or structs or whatever you will not be optimized well",
    "start": "609279",
    "end": "614720"
  },
  {
    "text": "even if you have something like um 10 1000 unique combinations of the array",
    "start": "614720",
    "end": "620320"
  },
  {
    "text": "elements i mean eventually they will consume too much uh storage so",
    "start": "620320",
    "end": "625440"
  },
  {
    "text": "we went from 9 to 974 but can we do anything about it i mean",
    "start": "625440",
    "end": "631440"
  },
  {
    "text": "it happened to us practic practically in one of the data sets it was like but consuming too much storage and making",
    "start": "631440",
    "end": "637920"
  },
  {
    "text": "other pipelines very slow and then we started to investigate and see if we can yeah i mean limit that",
    "start": "637920",
    "end": "644560"
  },
  {
    "text": "storage explosion so uh the trick here if you understand or you have noticed the idea around the",
    "start": "644560",
    "end": "650640"
  },
  {
    "text": "string dictionaries that yeah why to store it as an array can we store it as a string yes we can",
    "start": "650640",
    "end": "656959"
  },
  {
    "text": "so if you can come here and especially if we have low cardinality not too many unique",
    "start": "656959",
    "end": "662160"
  },
  {
    "text": "combinations so let's store it as a comma separated",
    "start": "662160",
    "end": "667279"
  },
  {
    "text": "string so instead of an array you can see here it is an array it will be a comma separated value or you can store",
    "start": "667279",
    "end": "672800"
  },
  {
    "text": "if this is a more complex side you can sort as a json string or base64 or whatever",
    "start": "672800",
    "end": "678399"
  },
  {
    "text": "so i'll hit enter to run the next snippet and this snippet will do the same we still have the array",
    "start": "678399",
    "end": "685120"
  },
  {
    "text": "column but this time we'll store it as a string and if you remember strings are stored",
    "start": "685120",
    "end": "690800"
  },
  {
    "text": "as dictionaries and as long as we don't have too many combinations yeah",
    "start": "690800",
    "end": "696480"
  },
  {
    "text": "storage will be very very minimal all right let's have a look on the impact now",
    "start": "696480",
    "end": "702399"
  },
  {
    "text": "takes also some time to yeah to resist the data but",
    "start": "702399",
    "end": "708079"
  },
  {
    "text": "i'm not sure if we have some sort of drum roll effect here",
    "start": "708079",
    "end": "713120"
  },
  {
    "text": "nearly there excitement okay boom done all right i went back to almost",
    "start": "713120",
    "end": "720079"
  },
  {
    "text": "the previous uh kind of size range so we don't have to store 974 megabytes with",
    "start": "720079",
    "end": "725839"
  },
  {
    "text": "we need only to store 30 megabytes even if we still have uh that kind of what",
    "start": "725839",
    "end": "732720"
  },
  {
    "text": "integer array every element of them is like what or every record has uh 10 element 100 elements",
    "start": "732720",
    "end": "739279"
  },
  {
    "text": "all right so this is uh one of the core ideas around how to optimize storage and",
    "start": "739279",
    "end": "745200"
  },
  {
    "text": "practically speaking this kind of saved us around 90 percent of a storage for a certain data set and",
    "start": "745200",
    "end": "752160"
  },
  {
    "text": "make made also subsequent pipelines much more faster so once you understand just to",
    "start": "752160",
    "end": "759440"
  },
  {
    "text": "recap here i'll go to back to the presentation the",
    "start": "759440",
    "end": "764480"
  },
  {
    "start": "764000",
    "end": "764000"
  },
  {
    "text": "take away here if you understand the schema of your underlying storage format understand the data type",
    "start": "764480",
    "end": "770240"
  },
  {
    "text": "cardinality and don't store data that you don't need and so on probably you will achieve a",
    "start": "770240",
    "end": "776720"
  },
  {
    "text": "very good storage size which will make also your",
    "start": "776720",
    "end": "781920"
  },
  {
    "text": "subsequent jobs or pipelines run faster because it's not about storing as much",
    "start": "781920",
    "end": "786959"
  },
  {
    "text": "data as you need it's about storing them efficiently to make things uh running faster",
    "start": "786959",
    "end": "793680"
  },
  {
    "text": "all right so that was good first demo done and dusted so next one it's more",
    "start": "793680",
    "end": "799440"
  },
  {
    "text": "about file system it's somehow related to storage but yeah still near the file system uh area",
    "start": "799440",
    "end": "805600"
  },
  {
    "start": "805000",
    "end": "805000"
  },
  {
    "text": "so most of the big data applications normally in hadoop environment or similar we use a distributed file system",
    "start": "805600",
    "end": "812399"
  },
  {
    "text": "and then you need to understand if there's an object store like azure blob storage or a pro proper distributed file system",
    "start": "812399",
    "end": "819279"
  },
  {
    "text": "like uh something like azure data lake okay because that will make difference",
    "start": "819279",
    "end": "824959"
  },
  {
    "text": "and also you need to like what pick as big from different access methods",
    "start": "824959",
    "end": "830800"
  },
  {
    "text": "sometimes you can access a certain folder uh using a local uh url or something or an",
    "start": "830800",
    "end": "837040"
  },
  {
    "text": "network network url and that will make a difference in the performance in your of your jobs and we'll have a look on that",
    "start": "837040",
    "end": "844720"
  },
  {
    "text": "but eventually uh most of big data applications uh we store data in",
    "start": "844720",
    "end": "849839"
  },
  {
    "text": "partitions most of the time it would be date-based partitions so if you have a",
    "start": "849839",
    "end": "855600"
  },
  {
    "text": "very deep positioning scheme you some of your queries will be somehow slow due to the overhead of enumerating",
    "start": "855600",
    "end": "862720"
  },
  {
    "text": "file system folders but you can add a bit of metadata to have",
    "start": "862720",
    "end": "868079"
  },
  {
    "text": "environment specifically to a tool called hive to make such queries fast and let's have a look",
    "start": "868079",
    "end": "875680"
  },
  {
    "text": "so i'll close this one exit the last demo and switch to the",
    "start": "875680",
    "end": "882880"
  },
  {
    "text": "next one so what i have here",
    "start": "882880",
    "end": "889600"
  },
  {
    "text": "i have here two data sets that actually identical some um",
    "start": "889600",
    "end": "895440"
  },
  {
    "text": "dummy data actually and it is stored into oops",
    "start": "895440",
    "end": "901680"
  },
  {
    "text": "uh a year month day kind of partition scheme can",
    "start": "902000",
    "end": "907680"
  },
  {
    "text": "show you a quick where is that",
    "start": "907680",
    "end": "913839"
  },
  {
    "text": "okay you can think about it i think most of you might have seen that before like this kind of years and months and",
    "start": "914399",
    "end": "920560"
  },
  {
    "text": "day and and so on okay so this dummy data is stored twice once in inside my",
    "start": "920560",
    "end": "928800"
  },
  {
    "text": "my what my linux vm and once in the windows uh machine or file system so just to give you an idea about",
    "start": "928880",
    "end": "936959"
  },
  {
    "text": "open another shell here about the difference in performance so i'll just try to print the side of each individual",
    "start": "937360",
    "end": "943759"
  },
  {
    "text": "months folder this is my linux vm or wsl",
    "start": "943759",
    "end": "949279"
  },
  {
    "text": "very quick okay every folder contains like 17 megs and what about the rendered one",
    "start": "949279",
    "end": "956880"
  },
  {
    "text": "i'm accessing the windows file system from inside my wsl now okay and it contains the same identical data",
    "start": "957920",
    "end": "965519"
  },
  {
    "text": "okay i think you can feel it now it takes a little bit more time so the same concept applies especially",
    "start": "965519",
    "end": "971440"
  },
  {
    "text": "if you are accessing uh slow file systems or if your name node on your hadoop environment is",
    "start": "971440",
    "end": "978000"
  },
  {
    "text": "under heavy load all right so let's go back to",
    "start": "978000",
    "end": "983199"
  },
  {
    "text": "our screen here and i'll run i read from my linux file system",
    "start": "983199",
    "end": "989120"
  },
  {
    "text": "and filter for a certain year a month and day or day range actually and count",
    "start": "989120",
    "end": "994160"
  },
  {
    "text": "how many records i have nothing very complicated here this one",
    "start": "994160",
    "end": "999839"
  },
  {
    "text": "just like what spark will need to bootstrap but eventually should finish",
    "start": "999839",
    "end": "1005600"
  },
  {
    "text": "yeah now okay so it takes like a second or two it was just bootstrapping spark",
    "start": "1005600",
    "end": "1011680"
  },
  {
    "text": "and the access using the linux file system was fast let's have a look if we need to access",
    "start": "1011680",
    "end": "1017199"
  },
  {
    "text": "the same data from the that kind of mount file system format think from the windows file system",
    "start": "1017199",
    "end": "1025438"
  },
  {
    "text": "okay this one should be slow should be really much slower than the other one",
    "start": "1025439",
    "end": "1031839"
  },
  {
    "text": "because yeah animating folders and files or whatever from the slower file system",
    "start": "1031839",
    "end": "1036959"
  },
  {
    "text": "will make your jobs run slower all right so it should take time but eventually maybe",
    "start": "1036959",
    "end": "1043520"
  },
  {
    "text": "we can go to the browser meanwhile and i'll show you another thing",
    "start": "1043520",
    "end": "1048880"
  },
  {
    "text": "with the spark history thing i think both have finished the first one was like what one second",
    "start": "1049360",
    "end": "1054400"
  },
  {
    "text": "the second one actually the duration duration wise this one was like 30 seconds",
    "start": "1054400",
    "end": "1059600"
  },
  {
    "text": "but spark does not choose that and sometimes you will be misled because in sport history your",
    "start": "1059600",
    "end": "1066799"
  },
  {
    "text": "jobs seems to be running fast but actually duration wise this this took like 30 seconds or one minute okay",
    "start": "1066799",
    "end": "1074160"
  },
  {
    "text": "so how to handle such thing as a thing assuming that you would like to work against against this file system",
    "start": "1074160",
    "end": "1081520"
  },
  {
    "text": "cool there are some tweaks in spark and different tools such as that you can",
    "start": "1081520",
    "end": "1086720"
  },
  {
    "text": "eliminate the idea of what of an emulating file system folder so you are just",
    "start": "1086720",
    "end": "1091840"
  },
  {
    "text": "helping spark and giving it a hint about which folders continue that and yeah this is",
    "start": "1091840",
    "end": "1099440"
  },
  {
    "text": "much faster okay but this is clunky no one would like to write a code a good piece like that",
    "start": "1099440",
    "end": "1105919"
  },
  {
    "text": "and yeah too many too many lines anyway so most of big data applications you will",
    "start": "1105919",
    "end": "1111440"
  },
  {
    "text": "have a tool or a product called hive something like what sql over big data",
    "start": "1111440",
    "end": "1117520"
  },
  {
    "text": "files so you can create an external table",
    "start": "1117520",
    "end": "1122720"
  },
  {
    "text": "over the same uh folder structure or path you can partition it by the same",
    "start": "1122720",
    "end": "1128799"
  },
  {
    "text": "partition stuff near a month and day this kind of snippet will create",
    "start": "1128799",
    "end": "1134000"
  },
  {
    "text": "something like what metadata behind the scenes inside hive if we run it because it takes",
    "start": "1134000",
    "end": "1139679"
  },
  {
    "text": "a second or so so the idea is similar to the idea of",
    "start": "1139679",
    "end": "1144880"
  },
  {
    "text": "optimizing uh relational queries if you have a slow query you will go and create an index and then the query will be",
    "start": "1144880",
    "end": "1150480"
  },
  {
    "text": "faster okay so similarly here we are creating a little bit of metadata about the folder",
    "start": "1150480",
    "end": "1156880"
  },
  {
    "text": "structures and so on so eventually later when you create like what run in a query",
    "start": "1156880",
    "end": "1162160"
  },
  {
    "text": "involving the partition uh folders or partition columns here i think it will",
    "start": "1162160",
    "end": "1167679"
  },
  {
    "text": "be fast spark will know where to look for the data quickly and he doesn't it doesn't need to go",
    "start": "1167679",
    "end": "1173280"
  },
  {
    "text": "and enumerate the whole file system so let's go and have a look on this one",
    "start": "1173280",
    "end": "1179840"
  },
  {
    "text": "come in less than a second second so this one the main difference here i'm using that kind of external table thing",
    "start": "1182559",
    "end": "1188400"
  },
  {
    "text": "instead of referencing the file system directly okay",
    "start": "1188400",
    "end": "1193679"
  },
  {
    "text": "so this is the second tip here depending on try to understand the underlying file system in your application and whether",
    "start": "1193679",
    "end": "1199440"
  },
  {
    "text": "you need this kind of tweaks or additions to your applications",
    "start": "1199440",
    "end": "1205520"
  },
  {
    "text": "because if not if you have a software system or a deportation server you might have",
    "start": "1205520",
    "end": "1211840"
  },
  {
    "text": "stuff like this in your spark history things that look fast but actually",
    "start": "1211840",
    "end": "1218000"
  },
  {
    "text": "duration wise it will be very slow okay very good so we have a little bit",
    "start": "1218000",
    "end": "1224159"
  },
  {
    "text": "more understanding now around file system and what what what the impact you can have on your",
    "start": "1224159",
    "end": "1229600"
  },
  {
    "text": "application let's go to the next one it's more about maybe sport and gems here so normally if",
    "start": "1229600",
    "end": "1236080"
  },
  {
    "start": "1235000",
    "end": "1235000"
  },
  {
    "text": "you go to the gym and pick up a couple of dumbbells dumbbells to play with",
    "start": "1236080",
    "end": "1241280"
  },
  {
    "text": "you will pick dumbbells with identical weight i'm not expecting that you will pick 20",
    "start": "1241280",
    "end": "1246320"
  },
  {
    "text": "kilograms and the 15. it will be weird so the same concept applies for big data applications try to make things balanced",
    "start": "1246320",
    "end": "1253280"
  },
  {
    "text": "and even so if you have most of the time you'll have a cluster with multiple nodes so try to distribute your compute",
    "start": "1253280",
    "end": "1260400"
  },
  {
    "text": "across all the nodes don't have like what uh two nodes running and 10 nodes just",
    "start": "1260400",
    "end": "1266320"
  },
  {
    "text": "ideal waiting for something all right the same applies for storage it's better to have your files with even size so",
    "start": "1266320",
    "end": "1273840"
  },
  {
    "text": "don't have for the same data set because for bigger applications files are generated in",
    "start": "1273840",
    "end": "1279360"
  },
  {
    "text": "partitions and every node will generate one part or one file so it's",
    "start": "1279360",
    "end": "1284640"
  },
  {
    "text": "mainly preferred to have files with very similar size or even size",
    "start": "1284640",
    "end": "1291200"
  },
  {
    "text": "because big data applications are normally uh executed in on distributed systems so if things are fair compute",
    "start": "1291200",
    "end": "1297679"
  },
  {
    "text": "and storage uh the application will run smoothly otherwise there will be",
    "start": "1297679",
    "end": "1302960"
  },
  {
    "text": "there will be problems and timeouts and slowness all right let's go and have a look",
    "start": "1302960",
    "end": "1308960"
  },
  {
    "text": "where are we now so i'll quit this demo and",
    "start": "1308960",
    "end": "1314640"
  },
  {
    "text": "switch to the next one it's about skewness so",
    "start": "1314640",
    "end": "1321200"
  },
  {
    "text": "kind of spark shell so i have some tables stored somewhere",
    "start": "1324559",
    "end": "1329840"
  },
  {
    "text": "okay contains some some data some special information",
    "start": "1329840",
    "end": "1336000"
  },
  {
    "text": "and okay",
    "start": "1336799",
    "end": "1340080"
  },
  {
    "text": "so this is the table just scroll up continue some information state column",
    "start": "1343440",
    "end": "1350400"
  },
  {
    "text": "some other request information from abs supply name brew bureau of statistics",
    "start": "1350400",
    "end": "1355919"
  },
  {
    "text": "all right and i'd like to filter that table",
    "start": "1355919",
    "end": "1361120"
  },
  {
    "text": "uh pick a certain sa4 thing column and generate four files the outcome of",
    "start": "1361120",
    "end": "1367840"
  },
  {
    "text": "the final outcome should contain four files and write it to some location okay so we can",
    "start": "1367840",
    "end": "1374960"
  },
  {
    "text": "do it in spark using two um apis one of them is called coalesce one of them is called repartition",
    "start": "1374960",
    "end": "1380960"
  },
  {
    "text": "there is a little bit more detail about those kpis but i'll give you an idea about the effect or the difference in",
    "start": "1380960",
    "end": "1387679"
  },
  {
    "text": "selecting which api to satisfy your requirement so let's run the qls",
    "start": "1387679",
    "end": "1393360"
  },
  {
    "text": "you can see here that spark is running four tasks so generally speaking if you use an api",
    "start": "1393360",
    "end": "1399039"
  },
  {
    "text": "like this one you will use four tests or four threads among your um cluster nodes i have um",
    "start": "1399039",
    "end": "1406320"
  },
  {
    "text": "got a 12 core laptop now so it is not even consuming all the compute i'm i have on my laptop okay",
    "start": "1406320",
    "end": "1413600"
  },
  {
    "text": "eventually it will finish but yeah because it is not consuming all the computer i have it will be somehow slow",
    "start": "1413600",
    "end": "1421120"
  },
  {
    "text": "the next one meanwhile okay can imagine is slow now yeah",
    "start": "1421120",
    "end": "1429279"
  },
  {
    "text": "okay done the next one using a different api you can see here just spark is creating",
    "start": "1429279",
    "end": "1436799"
  },
  {
    "text": "like 40 tasks so if i open my task manager now a lot like what it will",
    "start": "1436799",
    "end": "1442000"
  },
  {
    "text": "deviate from the explanation thread here you will find that most of the codes",
    "start": "1442000",
    "end": "1447440"
  },
  {
    "text": "like that 12 calls or something are already running and these are number of completed jobs and this number of",
    "start": "1447440",
    "end": "1453279"
  },
  {
    "text": "completed three tasks and active tasks okay so this was supposed to be faster i'll go now and do",
    "start": "1453279",
    "end": "1460400"
  },
  {
    "text": "the same here refresh so the first one which was using a coalesce api took 37 seconds the second",
    "start": "1460400",
    "end": "1467200"
  },
  {
    "text": "one and it was using like what yeah four threads only or spawning four tasks",
    "start": "1467200",
    "end": "1473600"
  },
  {
    "text": "the second one which was using a reportation api yeah created much more trustless and",
    "start": "1473600",
    "end": "1479440"
  },
  {
    "text": "finished quickly all right so the idea is that depending on the api you select in your",
    "start": "1479440",
    "end": "1485039"
  },
  {
    "text": "application you might limit the processing on your cluster cluster to a few nodes",
    "start": "1485039",
    "end": "1491600"
  },
  {
    "text": "or machines all right which is not good actually think about this kind of example if you are processing",
    "start": "1491600",
    "end": "1497120"
  },
  {
    "text": "terabytes of data so instead of this kind of slight difference here between uh 37",
    "start": "1497120",
    "end": "1503919"
  },
  {
    "text": "seconds and 20 seconds there will be um quite a difference between five minutes and five days all right",
    "start": "1503919",
    "end": "1511039"
  },
  {
    "text": "so one other thing other than the distributing compute evenly",
    "start": "1511039",
    "end": "1516080"
  },
  {
    "text": "among the nodes of the cluster let's have a look on the file system now",
    "start": "1516080",
    "end": "1521279"
  },
  {
    "text": "so the two apis created the outcome in this kind of folders one data colls and one",
    "start": "1523360",
    "end": "1528799"
  },
  {
    "text": "data reportation so have a look at the outcome four files rest of the files were",
    "start": "1528799",
    "end": "1535440"
  },
  {
    "text": "markers and crcs so the coalesce one created two files with 3.8 megs and 1.9 megs not too bad",
    "start": "1535440",
    "end": "1544080"
  },
  {
    "text": "but yeah two files are roughly half the size of the other files which is not balancing okay",
    "start": "1544080",
    "end": "1551440"
  },
  {
    "text": "let's go to the reportation thing i hope you can see my file system small front here",
    "start": "1551440",
    "end": "1557440"
  },
  {
    "text": "i think this is obvious here okay this one so this one is fairly identical size or roughly",
    "start": "1557440",
    "end": "1564240"
  },
  {
    "text": "identical size so the good thing about the other api or i'll go back here this one this api will distribute load",
    "start": "1564240",
    "end": "1571520"
  },
  {
    "text": "uh more evenly among cluster nodes and it will also produce files with",
    "start": "1571520",
    "end": "1577600"
  },
  {
    "text": "somehow uh identical size which is also good it will incur a little bit more um",
    "start": "1577600",
    "end": "1583840"
  },
  {
    "text": "across cluster nodes but eventually you will pay a little bit more compute for much more faster and",
    "start": "1583840",
    "end": "1590480"
  },
  {
    "text": "better outcomes all right so this idea was around or this step was around making things",
    "start": "1590480",
    "end": "1597200"
  },
  {
    "text": "balanced compute wise and storage wise so that your your applications and jobs will run smoothly",
    "start": "1597200",
    "end": "1603520"
  },
  {
    "text": "and consistently back to the presentation",
    "start": "1603520",
    "end": "1609278"
  },
  {
    "text": "okay so next one it's more about data quality so",
    "start": "1609840",
    "end": "1616640"
  },
  {
    "text": "in class equip applications or desktop applications or whatever we have unit tests or integration tests as well and so on",
    "start": "1616960",
    "end": "1624000"
  },
  {
    "start": "1618000",
    "end": "1618000"
  },
  {
    "text": "and the same applies for big data yeah you can pick whatever preferred library in spark or whatever",
    "start": "1624000",
    "end": "1629679"
  },
  {
    "text": "tool you use and do your tests before you push your code to whatever environment",
    "start": "1629679",
    "end": "1635760"
  },
  {
    "text": "but in big that application there is another aspect to consider which is data quality i mean",
    "start": "1635760",
    "end": "1642320"
  },
  {
    "text": "on a web application for example the outcome is a screen or some sort of interaction between the user and the",
    "start": "1642320",
    "end": "1647679"
  },
  {
    "text": "application but the outcome of a big data application or a pipeline is a piece of data stored on a file system",
    "start": "1647679",
    "end": "1654799"
  },
  {
    "text": "that can be like what consumed one year from now or something like that and then you need to make sure that the",
    "start": "1654799",
    "end": "1660960"
  },
  {
    "text": "outcome of that job is kind of um acceptable and okay think about it the",
    "start": "1660960",
    "end": "1667039"
  },
  {
    "text": "basic idea is like if you have a unique column it should it should be unique if you have a column that is supposed to",
    "start": "1667039",
    "end": "1673120"
  },
  {
    "text": "contain positive numbers only it shouldn't have negative numbers and so on so it's also recommended here to",
    "start": "1673120",
    "end": "1679760"
  },
  {
    "text": "apply this kind of data quality checks probably at the end of the paper of your pipeline and also you can apply it at",
    "start": "1679760",
    "end": "1685520"
  },
  {
    "text": "the beginning to capture any like problems with your input data if you need and for sure",
    "start": "1685520",
    "end": "1691679"
  },
  {
    "text": "this kind of pipelines are composed of multiple steps and it's also recommended to add as much",
    "start": "1691679",
    "end": "1698000"
  },
  {
    "text": "diagnostics and logging as needed so that you can track the problems later and for sure there",
    "start": "1698000",
    "end": "1704399"
  },
  {
    "text": "might be some notifications needed in case a job fails or whatever then you need to identify someone to have a look",
    "start": "1704399",
    "end": "1712799"
  },
  {
    "text": "cool let's go and close this one",
    "start": "1712799",
    "end": "1717919"
  },
  {
    "text": "i'll do some data quality stuff now no magic but the idea of how to apply",
    "start": "1717919",
    "end": "1724640"
  },
  {
    "text": "that such things to a big data application",
    "start": "1724640",
    "end": "1729278"
  },
  {
    "text": "all right we'll go to data quality so what do we have here",
    "start": "1729840",
    "end": "1738278"
  },
  {
    "text": "all right uh i'm using or i'll start spark shell",
    "start": "1740559",
    "end": "1747360"
  },
  {
    "text": "uh using what with some sort of a dependency some library that will make it easy to for me",
    "start": "1747360",
    "end": "1753600"
  },
  {
    "text": "to check the quality of a certain data frame and what i'm going to do",
    "start": "1753600",
    "end": "1760320"
  },
  {
    "text": "let me just go to that folder first and then start sculpture",
    "start": "1760320",
    "end": "1768399"
  },
  {
    "text": "and run that snippet",
    "start": "1773919",
    "end": "1777720"
  },
  {
    "text": "so the example we have here let's assume we have an orders table",
    "start": "1780320",
    "end": "1787600"
  },
  {
    "text": "we'll display now or actually let's have a look here so we have an order table or a csv file",
    "start": "1787600",
    "end": "1793360"
  },
  {
    "text": "containing order information and sell amount and country code and i have a country lookup",
    "start": "1793360",
    "end": "1799120"
  },
  {
    "text": "country code name continent and i'd like to generate some sort of a report that will contain",
    "start": "1799120",
    "end": "1805279"
  },
  {
    "text": "other information and country name and continental to regional stuff like that so i'd like to join the country table",
    "start": "1805279",
    "end": "1811919"
  },
  {
    "text": "with the orders table here is the join where is that so we are joining the country",
    "start": "1811919",
    "end": "1818720"
  },
  {
    "text": "orders table the country table and showing the first for example five",
    "start": "1818720",
    "end": "1825520"
  },
  {
    "text": "records order id stuff and plus",
    "start": "1825520",
    "end": "1830399"
  },
  {
    "text": "the joint country i think no this is not ah last one",
    "start": "1831360",
    "end": "1837120"
  },
  {
    "text": "click enter okay so yeah this is the",
    "start": "1837120",
    "end": "1842159"
  },
  {
    "text": "order information and then this is the joined country information so what can we do now we can create some",
    "start": "1842159",
    "end": "1848000"
  },
  {
    "text": "sort of declarative code here that can say okay",
    "start": "1848000",
    "end": "1853039"
  },
  {
    "text": "for the final data set generated i should have an order column which should not have nulls",
    "start": "1853039",
    "end": "1859279"
  },
  {
    "text": "and it should be unique the country name there should be a country name column and it should be",
    "start": "1859279",
    "end": "1865279"
  },
  {
    "text": "also uh does not contain nulls and the cell amount should be non-negative all right",
    "start": "1865279",
    "end": "1872559"
  },
  {
    "text": "so i'm just creating a small helper method here",
    "start": "1872559",
    "end": "1878080"
  },
  {
    "text": "we do what we take a data frame and apply a certain uh kind of data quality check",
    "start": "1878080",
    "end": "1884640"
  },
  {
    "text": "and return the value all right so i run this kind of method against my join data frame",
    "start": "1884640",
    "end": "1891360"
  },
  {
    "text": "and see the result of data quality thing",
    "start": "1891360",
    "end": "1896480"
  },
  {
    "text": "then okay success all the data quality like what constraints are already satisfied and everything is good",
    "start": "1899120",
    "end": "1905120"
  },
  {
    "text": "green signal ship it all right cool so let's assume that uh that country",
    "start": "1905120",
    "end": "1910480"
  },
  {
    "text": "table is not managed or uh owned by your team and comes from a third party or another team in your",
    "start": "1910480",
    "end": "1916559"
  },
  {
    "text": "company or whatever and for any reason at a certain point of time it contained",
    "start": "1916559",
    "end": "1922000"
  },
  {
    "text": "duplicates stuff like that so i'm just simulating information here all right and joining with that duplication data",
    "start": "1922000",
    "end": "1928399"
  },
  {
    "text": "frame all right so if we see here the",
    "start": "1928399",
    "end": "1934480"
  },
  {
    "text": "generated data frame that is supposed to be the output for the duplicate lookup",
    "start": "1934480",
    "end": "1940320"
  },
  {
    "text": "country what do we have here yeah we have the order id i mean every order is",
    "start": "1940320",
    "end": "1946000"
  },
  {
    "text": "represented here twice it's kind of quite expensive nothing magic here all right so let's run the same stuff",
    "start": "1946000",
    "end": "1952480"
  },
  {
    "text": "the same verification stuff oops",
    "start": "1952480",
    "end": "1956240"
  },
  {
    "text": "again it's a new data frame and boom will get an error somehow expect it for sure but the idea here is that yes",
    "start": "1959519",
    "end": "1965200"
  },
  {
    "text": "you'll just encode your expected data quality checks or constraints or",
    "start": "1965200",
    "end": "1971600"
  },
  {
    "text": "whatever and then make sure that before you ride the data frame to desk run this kind of stuff it will produce a",
    "start": "1971600",
    "end": "1977679"
  },
  {
    "text": "result and then based on the result if it is an error yeah you can there is the library contains uh some other apis",
    "start": "1977679",
    "end": "1984880"
  },
  {
    "text": "to like what trend that it is what was the failure is it the order id column or the country",
    "start": "1984880",
    "end": "1991120"
  },
  {
    "text": "column or whatever and then you can resist this kind of uh results send an email or fail the job up to you",
    "start": "1991120",
    "end": "1997919"
  },
  {
    "text": "okay and by the way i forgot to mention that the library is called dq it's from aws okay so yeah the idea is that uh",
    "start": "1997919",
    "end": "2006080"
  },
  {
    "text": "i mean there is no magic or actually a basic idea garbage in garbage out so make sure that you",
    "start": "2006080",
    "end": "2011760"
  },
  {
    "text": "don't produce garbage out all right cool",
    "start": "2011760",
    "end": "2016480"
  },
  {
    "text": "next one and we have a look on governance all right it's a boring thing but i'll try",
    "start": "2017039",
    "end": "2024080"
  },
  {
    "text": "to make it somehow exciting all right governance",
    "start": "2024080",
    "end": "2029440"
  },
  {
    "text": "so uh very related to working with big data most of the time you'll be working with a",
    "start": "2029440",
    "end": "2034799"
  },
  {
    "start": "2030000",
    "end": "2030000"
  },
  {
    "text": "big inter enterprise and there might be some sort of compliance or requirement or sorry governance",
    "start": "2034799",
    "end": "2040240"
  },
  {
    "text": "requirements and so on security is kind of quite expected so you need to have an asset catalog you",
    "start": "2040240",
    "end": "2047039"
  },
  {
    "text": "need to have a tool it's not enough to like create a small uh accessory to whatever to",
    "start": "2047039",
    "end": "2052320"
  },
  {
    "text": "manage your uh artifacts and that assets it will go out of control very quickly",
    "start": "2052320",
    "end": "2059679"
  },
  {
    "text": "and then you need to have some sort of lineage for each dataset generated in your system you need to understand from",
    "start": "2059679",
    "end": "2065280"
  },
  {
    "text": "where it originated what was the sport application your version produced it what are the input data sets such that",
    "start": "2065280",
    "end": "2071440"
  },
  {
    "text": "for example if you have a certain input that i said that was identified to be broken or whatever you will find using",
    "start": "2071440",
    "end": "2078320"
  },
  {
    "text": "that kind of lineage what are the impacted uh data pieces and then you can go and rebuild them or whatever",
    "start": "2078320",
    "end": "2085040"
  },
  {
    "text": "you need also to have some sort of classification sometimes related to this kind of uh",
    "start": "2085040",
    "end": "2091118"
  },
  {
    "text": "pii or financial classification so if data set is to be identified as a confidential or private or whatever you",
    "start": "2091119",
    "end": "2098160"
  },
  {
    "text": "can find them quickly using a classification or tag and then based on such questions and tagging you can apply",
    "start": "2098160",
    "end": "2104800"
  },
  {
    "text": "some sort of automatic security so if a data set is classified as",
    "start": "2104800",
    "end": "2110160"
  },
  {
    "text": "private only or something you can or confidential you can apply a certain rule",
    "start": "2110160",
    "end": "2115359"
  },
  {
    "text": "rule-based security thing against that data set using the common tool in that space is called",
    "start": "2115359",
    "end": "2121680"
  },
  {
    "text": "the apache ranger and the other tool for doing the governance here and lineage is apache atlas there is",
    "start": "2121680",
    "end": "2128480"
  },
  {
    "text": "maybe other tools that you can uh write your own but yeah i mean just don't reinvent any wheels",
    "start": "2128480",
    "end": "2135200"
  },
  {
    "text": "you can do the general recommendation so let's go to the governance thing",
    "start": "2135200",
    "end": "2140880"
  },
  {
    "text": "have a look here with this shell and what do we have here",
    "start": "2140880",
    "end": "2148319"
  },
  {
    "text": "so i have a similar kind of query the same stuff like orders and countries and then joining",
    "start": "2151200",
    "end": "2156640"
  },
  {
    "text": "them and then writing the results to the file system okay",
    "start": "2156640",
    "end": "2163760"
  },
  {
    "text": "so i have now switching from my wsl vm",
    "start": "2164000",
    "end": "2169839"
  },
  {
    "text": "i've switched to a docker container containing spark along with that apache atlas project or",
    "start": "2170000",
    "end": "2177119"
  },
  {
    "text": "a product sorry so i think it is already open",
    "start": "2177119",
    "end": "2182800"
  },
  {
    "text": "and by the way so we have i mean just to let you know that is the same country table same order table and i have that",
    "start": "2185359",
    "end": "2191760"
  },
  {
    "text": "demo script i'm using some connector from data breaks it's called",
    "start": "2191760",
    "end": "2198079"
  },
  {
    "text": "the spark atlas connector that will make things easy so that you write your spark job normally and then you'll get the",
    "start": "2198079",
    "end": "2204560"
  },
  {
    "text": "lineage created for you without no extra effort from your side",
    "start": "2204560",
    "end": "2210880"
  },
  {
    "text": "so i'll grab this one i think i am",
    "start": "2210880",
    "end": "2215838"
  },
  {
    "text": "i'm inside that thing now uh that docker container so i have a folder",
    "start": "2216480",
    "end": "2222640"
  },
  {
    "text": "called ndc 2020 containing the country csv the order csv and my demo script so",
    "start": "2222640",
    "end": "2229520"
  },
  {
    "text": "i hope from i run spark and feed it with that demo",
    "start": "2229520",
    "end": "2235680"
  },
  {
    "text": "uh script and the rest is like what the config files to",
    "start": "2235680",
    "end": "2241280"
  },
  {
    "text": "make or wire up that spark atlas connector so i run that drop",
    "start": "2241280",
    "end": "2248720"
  },
  {
    "text": "okay give it a second",
    "start": "2250560",
    "end": "2255640"
  },
  {
    "text": "nothing like what very amazing we will see in this shell when do now but we'll switch to atlas uh",
    "start": "2262960",
    "end": "2269280"
  },
  {
    "text": "screen in a second okay so this is i've got the final outcome to be written to the first",
    "start": "2269280",
    "end": "2275280"
  },
  {
    "text": "system and the job completed successfully then i'll go here to browser laser browser",
    "start": "2275280",
    "end": "2281839"
  },
  {
    "text": "here is my browser okay so this is a ui of atlas",
    "start": "2281839",
    "end": "2288000"
  },
  {
    "text": "once you log in here you can go and search for your data set so",
    "start": "2288000",
    "end": "2293040"
  },
  {
    "text": "let's go you can search by name but i had a problem with the name search today so",
    "start": "2293040",
    "end": "2298640"
  },
  {
    "text": "i'll do another approach i'll set for any data type or any data artifact with this type we have stuff",
    "start": "2298640",
    "end": "2305280"
  },
  {
    "text": "like an hdfs path or file a piece of data you have hive tables you",
    "start": "2305280",
    "end": "2311280"
  },
  {
    "text": "have other types of artifacts as well so it should be a file system",
    "start": "2311280",
    "end": "2318078"
  },
  {
    "text": "thing and i'll go",
    "start": "2318160",
    "end": "2325800"
  },
  {
    "text": "show more data here so it should be named i'll search just for ndc 2020 okay here we have it",
    "start": "2327119",
    "end": "2334560"
  },
  {
    "text": "okay so we have this hope i can open it in a new tab okay cool",
    "start": "2334560",
    "end": "2340838"
  },
  {
    "text": "good so we have a data frame here created in the that folder actually if i go back to the",
    "start": "2340960",
    "end": "2346320"
  },
  {
    "text": "shell here i'll find ls so we have the original quad files and now we have a new folder coordinated",
    "start": "2346320",
    "end": "2352240"
  },
  {
    "text": "containing the outcome data frame and if we go and have a look on the lineage here",
    "start": "2352240",
    "end": "2358640"
  },
  {
    "text": "oops",
    "start": "2358640",
    "end": "2360960"
  },
  {
    "text": "there might be a problem in my machine now or something but yeah just to imagine i think this can happen here",
    "start": "2365920",
    "end": "2372720"
  },
  {
    "text": "so maybe i can grab the lineage from another data center whatever but there was something like what uh countries as",
    "start": "2372720",
    "end": "2378640"
  },
  {
    "text": "a node and um what else um orders as a node and then connect it to",
    "start": "2378640",
    "end": "2384480"
  },
  {
    "text": "a sport application and the outcome will be uh this kind of data frame",
    "start": "2384480",
    "end": "2390079"
  },
  {
    "text": "okay and then related to the classification let's let me go and",
    "start": "2390079",
    "end": "2396800"
  },
  {
    "text": "go to ndc for example orders you remember that the orders we have here stuff like that so this",
    "start": "2396800",
    "end": "2404280"
  },
  {
    "text": "is the lineage i'm not sure it was not showing for the other data set for some reason but think about it like what the",
    "start": "2404280",
    "end": "2410480"
  },
  {
    "text": "final outcome was kind of composed by two inputs running for a certain i mean running",
    "start": "2410480",
    "end": "2416079"
  },
  {
    "text": "through a certain spark application but if you remember that order data set it contains an um a cell amount",
    "start": "2416079",
    "end": "2423680"
  },
  {
    "text": "okay so let me go and create a new classification maybe",
    "start": "2423680",
    "end": "2428800"
  },
  {
    "text": "oops all right i can create it here sorry i think i have a classification called financial already created so i'll",
    "start": "2429520",
    "end": "2436000"
  },
  {
    "text": "just go and yes i have a classification called financial so i'd like to classify",
    "start": "2436000",
    "end": "2441520"
  },
  {
    "text": "this uh piece of data which is my input data set as a financial debt",
    "start": "2441520",
    "end": "2447280"
  },
  {
    "text": "okay so i'll apply a financial classification i can also take propagate such that any data set",
    "start": "2447280",
    "end": "2453599"
  },
  {
    "text": "that was uh kind of linked or produced via the orders data set will also be market",
    "start": "2453599",
    "end": "2459680"
  },
  {
    "text": "financial all right so i'm applying that classification now",
    "start": "2459680",
    "end": "2465040"
  },
  {
    "text": "and propagating it as well",
    "start": "2465040",
    "end": "2468640"
  },
  {
    "text": "so yeah just my apologies because yeah single laptops not really a real cluster",
    "start": "2473920",
    "end": "2481440"
  },
  {
    "text": "so wait a few seconds but while waiting should finish now",
    "start": "2481440",
    "end": "2488480"
  },
  {
    "text": "the same idea applies i mean if we run the same job once again because the input data is marked as",
    "start": "2488480",
    "end": "2494800"
  },
  {
    "text": "financial uh if we run the job once again and produce a new data set in another folder",
    "start": "2494800",
    "end": "2500720"
  },
  {
    "text": "the new folder will be marked also as financial all right so maybe you can go to the",
    "start": "2500720",
    "end": "2506079"
  },
  {
    "text": "other window and refresh it or something",
    "start": "2506079",
    "end": "2510319"
  },
  {
    "text": "okay so this this was the output data set and you can see here that i i think the other one should have finished but",
    "start": "2512480",
    "end": "2518800"
  },
  {
    "text": "maybe the ui is a stack or something so this data set is market now uh",
    "start": "2518800",
    "end": "2524480"
  },
  {
    "text": "financial because it's a propagated classification and then you can use such classifications and tags to apply",
    "start": "2524480",
    "end": "2530000"
  },
  {
    "text": "security uh automatically so anything that is a classified financial is allowed only by the finance department",
    "start": "2530000",
    "end": "2536560"
  },
  {
    "text": "and stuff like that okay and yeah seems to be uh some sort of a lag",
    "start": "2536560",
    "end": "2542079"
  },
  {
    "text": "some uh stuff happening in the background but this is that lineage idea we have the country's data set we have",
    "start": "2542079",
    "end": "2547839"
  },
  {
    "text": "the orders that assets running through a certain spot application named xyz and then producing my",
    "start": "2547839",
    "end": "2554880"
  },
  {
    "text": "outcome data set okay so yeah this kind of tool will make your life easy especially if you have",
    "start": "2554880",
    "end": "2561119"
  },
  {
    "text": "too many datasets and you need to track them and apply certain kind of custom security uh policies",
    "start": "2561119",
    "end": "2568560"
  },
  {
    "text": "all right so that was about the governance and how to make things a little bit more exciting",
    "start": "2568560",
    "end": "2574640"
  },
  {
    "text": "back to the presentation all right yep okay good for the time",
    "start": "2574640",
    "end": "2580319"
  },
  {
    "text": "final piece here tooling so i mean tuning will help you",
    "start": "2580319",
    "end": "2585920"
  },
  {
    "start": "2584000",
    "end": "2584000"
  },
  {
    "text": "make your daily development experience enjoyable and yeah um",
    "start": "2585920",
    "end": "2594160"
  },
  {
    "text": "make a little bit of other things run faster as well so let me give you an idea here most of the",
    "start": "2594160",
    "end": "2599359"
  },
  {
    "text": "big data uh kind of workloads will start first with interactive queries so think",
    "start": "2599359",
    "end": "2604960"
  },
  {
    "text": "about it like 70 or 80 percent of your time will be running inside something like zeppelin or jupiter and writing a bunch of",
    "start": "2604960",
    "end": "2612160"
  },
  {
    "text": "interactive queries and sharing your findings with your team so you'd like to have something very",
    "start": "2612160",
    "end": "2618800"
  },
  {
    "text": "very good related to interactive communities and how to handle them uh related to stuff like search control",
    "start": "2618800",
    "end": "2623920"
  },
  {
    "text": "integration and differing notebooks and how to where to sort notebooks",
    "start": "2623920",
    "end": "2629520"
  },
  {
    "text": "and also sometimes if you have a big problem it's better to divide and conquer the problem so that you will",
    "start": "2629520",
    "end": "2635280"
  },
  {
    "text": "divide it into small pieces and keep one notebook each small problem and then you",
    "start": "2635280",
    "end": "2640800"
  },
  {
    "text": "pipeline them and run them i mean once you are done with each individual problem run them in a pipeline and",
    "start": "2640800",
    "end": "2647280"
  },
  {
    "text": "having the tools that will allow you to do a notebook pipelining will be also very appreciative",
    "start": "2647280",
    "end": "2653040"
  },
  {
    "text": "and finally most of the time you'd like to have a local experience plus cloud integration",
    "start": "2653040",
    "end": "2658400"
  },
  {
    "text": "to elaborate that and look at experiences that you can try stuff ideas or concepts locally or even small",
    "start": "2658400",
    "end": "2666079"
  },
  {
    "text": "small big data workloads on your machine but still once you are done with your",
    "start": "2666079",
    "end": "2671680"
  },
  {
    "text": "script or interactive notebook or whatever again subset of the data or whatever you can take it uh the same",
    "start": "2671680",
    "end": "2677119"
  },
  {
    "text": "notebook the same script and push it to the cloud run it on a spark on a kubernetes cluster stuff like",
    "start": "2677119",
    "end": "2683839"
  },
  {
    "text": "that okay so we need to have the same tools that can do stuff like that and literally you have tools like cube flow",
    "start": "2683839",
    "end": "2689760"
  },
  {
    "text": "we have spark magic and i'll do a demo using a tool called lyra",
    "start": "2689760",
    "end": "2696800"
  },
  {
    "text": "okay so where is that tool i'm just switching to my container list",
    "start": "2696800",
    "end": "2703440"
  },
  {
    "text": "here i have a container running containing this kind of tool it's from github you can just go and um",
    "start": "2703440",
    "end": "2710480"
  },
  {
    "text": "it's an extension on top of jupiter and jupiter lab so you can just install it locally or",
    "start": "2710480",
    "end": "2716000"
  },
  {
    "text": "install it inside a container and then once you install it",
    "start": "2716000",
    "end": "2722000"
  },
  {
    "text": "you'll have interface like this okay very similar to job eater but the main thing is i'd like to",
    "start": "2722640",
    "end": "2729359"
  },
  {
    "text": "show here our source control integration so i have",
    "start": "2729359",
    "end": "2734960"
  },
  {
    "text": "the ripple for the other stuff from this talk and some github",
    "start": "2734960",
    "end": "2741599"
  },
  {
    "text": "repo and the link is shared already in slack but i mean i've",
    "start": "2741599",
    "end": "2747440"
  },
  {
    "text": "went to this um docker container and get cloned the repository and then boom",
    "start": "2747440",
    "end": "2752720"
  },
  {
    "text": "i have the repository here and then you can even like put latest changes and do the same i mean i think",
    "start": "2752720",
    "end": "2759200"
  },
  {
    "text": "we have a very basic kind of get client here so that you can do comments uh push merge",
    "start": "2759200",
    "end": "2765760"
  },
  {
    "text": "everything roughly and def as well and i'll show you the diff from the browser you don't need any",
    "start": "2765760",
    "end": "2771680"
  },
  {
    "text": "other tools so let me show switch to the tooling folder",
    "start": "2771680",
    "end": "2776720"
  },
  {
    "text": "i have a couple of notebooks here they are like for a basic machine learning uh",
    "start": "2776720",
    "end": "2783040"
  },
  {
    "text": "scenario kind of hello world using a data set called iris flowers all right",
    "start": "2783040",
    "end": "2788640"
  },
  {
    "text": "so yeah i'm showing here the example of divide and conquer and maybe separating stuff across multiple notebooks so i",
    "start": "2788640",
    "end": "2795200"
  },
  {
    "text": "have one notebook for downloading data and maybe printing some sort of",
    "start": "2795200",
    "end": "2801280"
  },
  {
    "text": "or doing some sort of exploratory analysis all right and another notebook for",
    "start": "2801280",
    "end": "2807280"
  },
  {
    "text": "pulling this data from the local file system and doing some sort of machine learning decision tree classifier i mean this",
    "start": "2807280",
    "end": "2813839"
  },
  {
    "text": "kind of example you will find it grabs a code from anywhere like kaggle or",
    "start": "2813839",
    "end": "2819359"
  },
  {
    "text": "else yeah socket layer stuff like that so this this type of code is very very",
    "start": "2819359",
    "end": "2824880"
  },
  {
    "text": "common everywhere putting data doing exploratory analysis and then",
    "start": "2824880",
    "end": "2830400"
  },
  {
    "text": "doing the machine learning or whatever later and actually i'm deviating from from spark and hadoop here",
    "start": "2830400",
    "end": "2835760"
  },
  {
    "text": "although we still have the same concepts there especially if you are doing machine learning using spark all right",
    "start": "2835760",
    "end": "2840960"
  },
  {
    "text": "so i have the first notebook here it can go and",
    "start": "2840960",
    "end": "2847040"
  },
  {
    "text": "run all cells all right so i'm just like assuming i'm having",
    "start": "2847920",
    "end": "2853839"
  },
  {
    "text": "this kind of branch or code base from a teammate and i'd like to run it",
    "start": "2853839",
    "end": "2859040"
  },
  {
    "text": "and see what's happening okay it's running a new file has been downloaded here that method of csv",
    "start": "2859040",
    "end": "2865119"
  },
  {
    "text": "and yeah we have some sort of what they call it here pair plot",
    "start": "2865119",
    "end": "2870319"
  },
  {
    "text": "to identify the correlations between different variables or features inside your data set which is cool okay i'll",
    "start": "2870319",
    "end": "2876960"
  },
  {
    "text": "save this one i'll go to the next notebook i run the machine learning thing and it",
    "start": "2876960",
    "end": "2882960"
  },
  {
    "text": "will produce some sort of an accuracy metric i think okay cool so this is like",
    "start": "2882960",
    "end": "2888160"
  },
  {
    "text": "what classic scikit-learn stuff i'll do the same",
    "start": "2888160",
    "end": "2892799"
  },
  {
    "text": "it's pulling data from the local file system because it has been maybe downloaded and maybe prepared or tweaked",
    "start": "2894559",
    "end": "2901200"
  },
  {
    "text": "using a previous notebook okay it's done now but we have an accuracy of",
    "start": "2901200",
    "end": "2907760"
  },
  {
    "text": "0.9 i mean 91 percent assume that okay i got this quote from a teammate",
    "start": "2907760",
    "end": "2913440"
  },
  {
    "text": "and i know a little bit more than him about decision trees and machine learning so i know that yeah if you just",
    "start": "2913440",
    "end": "2920960"
  },
  {
    "text": "maybe for this case that he didn't notice but if you increase the depth to",
    "start": "2920960",
    "end": "2926559"
  },
  {
    "text": "three or something i mean for sure not to introduce overfitting but that's not what dudes are training",
    "start": "2926559",
    "end": "2932319"
  },
  {
    "text": "again but with a depth of three instead of two okay boom going to 98 now which is amazing",
    "start": "2932319",
    "end": "2939839"
  },
  {
    "text": "cool so i can save this notebook maybe create a branch or whatever and then",
    "start": "2939839",
    "end": "2945359"
  },
  {
    "text": "push my changes so that the other team members can review and maybe merge to master whatever and let's have",
    "start": "2945359",
    "end": "2952000"
  },
  {
    "text": "a look on the get section here so it has been kind of very",
    "start": "2952000",
    "end": "2957839"
  },
  {
    "text": "common that yeah defining uh jupiter notebooks or zeppelin or whatever is a very",
    "start": "2957839",
    "end": "2963839"
  },
  {
    "text": "non-pleasant experience due to the json storage format of such notebooks but we",
    "start": "2963839",
    "end": "2968960"
  },
  {
    "text": "have tools like uh elira and some other extensions uh behind it",
    "start": "2968960",
    "end": "2974880"
  },
  {
    "text": "doing quite good def so let me go to this one and double click but right click def",
    "start": "2974880",
    "end": "2981760"
  },
  {
    "text": "okay and you can see there are a bunch of differences here but if you can just ignore this one because this is about",
    "start": "2981760",
    "end": "2987839"
  },
  {
    "text": "the low time stamps and stuff like that but other than that i mean this kind of notebook nothing changes",
    "start": "2987839",
    "end": "2994559"
  },
  {
    "text": "the output changing this is kind of metadata like what when the cell has been executed and",
    "start": "2994559",
    "end": "3000079"
  },
  {
    "text": "the timing but if i go to the second notebook because the second notebook i have done i could change really read code change",
    "start": "3000079",
    "end": "3006720"
  },
  {
    "text": "so i'll go to def here if you have a lock here you'll find that okay",
    "start": "3006720",
    "end": "3012960"
  },
  {
    "text": "okay i can zoom in so that you can see it easily grab this one",
    "start": "3012960",
    "end": "3018800"
  },
  {
    "text": "so we have max depth here the original value was two and the new value of three okay this is a good change and even the",
    "start": "3018800",
    "end": "3025200"
  },
  {
    "text": "output if the output was kind of changed and other than the execution timeouts and this kind of",
    "start": "3025200",
    "end": "3030240"
  },
  {
    "text": "stuff i think you can understand okay it was 91 accuracy and now oops",
    "start": "3030240",
    "end": "3036480"
  },
  {
    "text": "it's 98. all right so i can like create a comment or something or something and",
    "start": "3036480",
    "end": "3042160"
  },
  {
    "text": "comment it in a branch or stuff and then push it for other people to review and this kind of things so yeah that",
    "start": "3042160",
    "end": "3047920"
  },
  {
    "text": "will make uh make this kind of workflows very easy and pleasant",
    "start": "3047920",
    "end": "3053280"
  },
  {
    "text": "and finally i don't have to do it but i think there is also option to run stuff",
    "start": "3053280",
    "end": "3058640"
  },
  {
    "text": "locally in a pipeline so i have prepared some pipeline before",
    "start": "3058640",
    "end": "3064000"
  },
  {
    "text": "so running uh actually you can just drag a notebook like this one and then",
    "start": "3064000",
    "end": "3069440"
  },
  {
    "text": "run it in a pipeline connect stuff together so i'll delete this one so i'm running the part one of",
    "start": "3069440",
    "end": "3075119"
  },
  {
    "text": "the workflow and connected to part two i can run this stuff locally or in the cloud i",
    "start": "3075119",
    "end": "3081200"
  },
  {
    "text": "mean you can run locally in some sort of a container using uh pandas and stuff",
    "start": "3081200",
    "end": "3087200"
  },
  {
    "text": "or whatever local image you would like to run it against or you can push it to kubeflow and run it in a spark",
    "start": "3087200",
    "end": "3092640"
  },
  {
    "text": "environment and kubernetes all right so yeah um whatever the tool you are going",
    "start": "3092640",
    "end": "3097680"
  },
  {
    "text": "to use make sure that it will satisfy this kind of requirements and it will make your development experience",
    "start": "3097680",
    "end": "3103200"
  },
  {
    "text": "enjoyable and fast as well and yeah right also collaboration should be key",
    "start": "3103200",
    "end": "3108880"
  },
  {
    "text": "here all right we don't have to run this one it's just clicking buttons and the monitoring thing is running but you can",
    "start": "3108880",
    "end": "3115280"
  },
  {
    "text": "imagine all these kind of features and their value so tooling i think we have good tools",
    "start": "3115280",
    "end": "3122720"
  },
  {
    "text": "here and yeah if you are doing sport there is a spark magic so that you can connect remotely",
    "start": "3122720",
    "end": "3128880"
  },
  {
    "text": "via a product called levy and for stuff in the cloud we have q flow which is available in azure and aws",
    "start": "3128880",
    "end": "3136319"
  },
  {
    "text": "and maybe google as well and yes that's it i think we still have like",
    "start": "3136319",
    "end": "3142319"
  },
  {
    "text": "what 10 minutes for questions or feedback or whatever and they have i hope you have learned",
    "start": "3142319",
    "end": "3148240"
  },
  {
    "text": "one of two things uh on how to efficiently develop and manage and operate your big data pipelines and",
    "start": "3148240",
    "end": "3154720"
  },
  {
    "text": "applications or maybe even if you are not using spark or hadoop maybe some of the concepts",
    "start": "3154720",
    "end": "3159920"
  },
  {
    "text": "here are applicable to other tools as well and thank you so much for having me",
    "start": "3159920",
    "end": "3165680"
  },
  {
    "text": "today i'm very glad to share this kind of information with you uh the code uh",
    "start": "3165680",
    "end": "3171119"
  },
  {
    "text": "the repo for the demos and presentation is available on github and i've left the link in",
    "start": "3171119",
    "end": "3176880"
  },
  {
    "text": "this channel room five yeah and i'm happy to take some questions here or in slack we still have",
    "start": "3176880",
    "end": "3182640"
  },
  {
    "text": "seven minutes",
    "start": "3182640",
    "end": "3185960"
  }
]