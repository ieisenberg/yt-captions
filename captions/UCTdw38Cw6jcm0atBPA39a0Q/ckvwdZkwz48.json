[
  {
    "text": "alright let's kick this off hi everybody my name is Philip laureano I work at",
    "start": "6509",
    "end": "13240"
  },
  {
    "text": "domaine and today I'm going to be talking mostly about a cadet net but not so much on the how to do certain things",
    "start": "13240",
    "end": "21190"
  },
  {
    "text": "within a cadet because within domain we actually did this into production and",
    "start": "21190",
    "end": "26919"
  },
  {
    "text": "what we implemented was a real-time clickstream event processing system and",
    "start": "26919",
    "end": "34480"
  },
  {
    "text": "this is more of a retrospective of what happened how we approached it and there's a few different things that we",
    "start": "34480",
    "end": "41200"
  },
  {
    "text": "did during the course of our journey into reactive programming so as with",
    "start": "41200",
    "end": "49900"
  },
  {
    "text": "everything else I'll start with the beginning we ran into a very interesting problem which was",
    "start": "49900",
    "end": "57900"
  },
  {
    "text": "how many people here have batch process processes that take more than a few",
    "start": "57900",
    "end": "63310"
  },
  {
    "text": "hours to run every day and how many of you have reporting at",
    "start": "63310",
    "end": "70170"
  },
  {
    "text": "some capacity within the business now with a domain one of the problems that",
    "start": "70170",
    "end": "76359"
  },
  {
    "text": "we had is as you know if you're familiar with domain we have a lot of properties in both residential and commercial real",
    "start": "76359",
    "end": "83890"
  },
  {
    "text": "estate and one of the problems that we have there is that we might have",
    "start": "83890",
    "end": "89319"
  },
  {
    "text": "hundreds of thousands of properties but on the other hand we can afford to wait",
    "start": "89319",
    "end": "94929"
  },
  {
    "text": "till the end of the day to see whether or not a particular property has the the",
    "start": "94929",
    "end": "101079"
  },
  {
    "text": "clickstream or the views and the inquiries that it needs so in the first version of what we called",
    "start": "101079",
    "end": "109869"
  },
  {
    "text": "our stat system what happened was it was pretty much what you would expect from any traditional system we had we",
    "start": "109869",
    "end": "118450"
  },
  {
    "text": "collected all our vents waited till the end of the day and we stored it inside of a sequel server instance and for a",
    "start": "118450",
    "end": "126729"
  },
  {
    "text": "time that was okay and at the end of the day what we would do is we would run a store proc collect all the stats for the",
    "start": "126729",
    "end": "133420"
  },
  {
    "text": "day and then by the time the agents would look at it the next day everything was great",
    "start": "133420",
    "end": "139060"
  },
  {
    "text": "the problem was that was five years ago the problem was that the scale of",
    "start": "139060",
    "end": "145170"
  },
  {
    "text": "our growth was such that it was no longer practical to be doing this on a daily on",
    "start": "145170",
    "end": "152110"
  },
  {
    "text": "just a daily basis because as the more and more properties we had",
    "start": "152110",
    "end": "158560"
  },
  {
    "text": "there was a greater need to make things closer to real-time as possible so",
    "start": "158560",
    "end": "165120"
  },
  {
    "text": "there's this one question we have to ask in this case what was a better way to do",
    "start": "165120",
    "end": "170350"
  },
  {
    "text": "this is it better to do one long batch",
    "start": "170350",
    "end": "175480"
  },
  {
    "text": "process that would take six hours or an entire day or",
    "start": "175480",
    "end": "181769"
  },
  {
    "text": "could we somehow split it into say",
    "start": "181769",
    "end": "187080"
  },
  {
    "text": "86,400 batch jobs in one day that take one second so we did a bit of research",
    "start": "187080",
    "end": "193630"
  },
  {
    "text": "and we came up with a lot of things but the first thing that we we found oh I found was the actor model now if you've",
    "start": "193630",
    "end": "202480"
  },
  {
    "text": "been paying attention to Erin's talks or Sergey stocks with Orleans there's a couple of flavors of that",
    "start": "202480",
    "end": "209070"
  },
  {
    "text": "but along the way the actor model poses quite a few it actually solves quite a",
    "start": "209070",
    "end": "215320"
  },
  {
    "text": "few problems that we weren't able to solve with a traditional store Prok model",
    "start": "215320",
    "end": "222120"
  },
  {
    "text": "first thing that we tried was doing async we quickly found out that it became",
    "start": "222120",
    "end": "228269"
  },
  {
    "text": "unmaintainable simply because well I'm a fallible human I can't think past 10",
    "start": "228269",
    "end": "235239"
  },
  {
    "text": "threads so we were looking for better ways to actually do this and",
    "start": "235239",
    "end": "241049"
  },
  {
    "text": "if you're not familiar with async it's fairly straightforward but the short explanation of this is it's basically a",
    "start": "241049",
    "end": "247810"
  },
  {
    "text": "syntactic compiled syntactic sugar over what is essentially the task",
    "start": "247810",
    "end": "254079"
  },
  {
    "text": "parallel library now it's fine if you we were dealing",
    "start": "254079",
    "end": "261280"
  },
  {
    "text": "with just a few properties but in domain since we had about a hundred thousand properties and we had all these",
    "start": "261280",
    "end": "266919"
  },
  {
    "text": "different click stream events coming in at once and when I say click stream I'm",
    "start": "266919",
    "end": "272300"
  },
  {
    "text": "talking about when somebody does something with a property listing on domain whether it's click on email a",
    "start": "272300",
    "end": "279199"
  },
  {
    "text": "friend or they want to look at the pictures we want to record every single thing that they do and this kind of",
    "start": "279199",
    "end": "286759"
  },
  {
    "text": "clickstream data is really really valuable because it gives us an insight to what the market wants and then in",
    "start": "286759",
    "end": "292430"
  },
  {
    "text": "turn we could take this data and use it to come up with better solutions for",
    "start": "292430",
    "end": "297740"
  },
  {
    "text": "some of the problems that we're currently having at the time so with async the problem that we had was mostly",
    "start": "297740",
    "end": "305300"
  },
  {
    "text": "around shared state you could spawn up several thousand async threads but you",
    "start": "305300",
    "end": "312979"
  },
  {
    "text": "start worrying about whether or not you have deadlocks whether there's something",
    "start": "312979",
    "end": "318289"
  },
  {
    "text": "some resource that you haven't released and at the same time we didn't really",
    "start": "318289",
    "end": "323569"
  },
  {
    "text": "scale beyond a single machine it's very hard to debug these sorts of things simply because if you catch an aggregate",
    "start": "323569",
    "end": "331430"
  },
  {
    "text": "exception now you've got to drill down into every single thread that fails even though the c-sharp compiler does do a",
    "start": "331430",
    "end": "338570"
  },
  {
    "text": "good job over abstracting away a lot of the state machine logic that is in place",
    "start": "338570",
    "end": "345909"
  },
  {
    "text": "now with the actor model we don't have to worry about those things yeah with the actor model it's as Aaron once said",
    "start": "345909",
    "end": "355310"
  },
  {
    "text": "in the previous session it's it's just as old as the concept of relational",
    "start": "355310",
    "end": "360740"
  },
  {
    "text": "databases with and the idea here is that there's no locks",
    "start": "360740",
    "end": "366699"
  },
  {
    "text": "second we let actors fail and fail hard because we could easily restart them if",
    "start": "366699",
    "end": "374360"
  },
  {
    "text": "there's any issues and the other thing is that it scales very very well to thousands of threads per",
    "start": "374360",
    "end": "381050"
  },
  {
    "text": "machine and in our case we scaled it up to",
    "start": "381050",
    "end": "386380"
  },
  {
    "text": "thousands of machines in AWS so we had it had thousands of threads not only go",
    "start": "386380",
    "end": "392419"
  },
  {
    "text": "across one machine to machine three machines but we got we put together an",
    "start": "392419",
    "end": "397520"
  },
  {
    "text": "infrastructure that could tell whether there was a high CPU spike inside of a cluster and we took it to the next step",
    "start": "397520",
    "end": "405110"
  },
  {
    "text": "in made it closer to Erlang because in Erlang what they do is if if there's a",
    "start": "405110",
    "end": "410750"
  },
  {
    "text": "you what you can do is remotely deploy code so essentially within domain what",
    "start": "410750",
    "end": "417389"
  },
  {
    "text": "we did was combined akka octopus deploy and if there's any kind",
    "start": "417389",
    "end": "424260"
  },
  {
    "text": "of situation where we sense that the CPU is spiking we can provision two or three",
    "start": "424260",
    "end": "430290"
  },
  {
    "text": "more machines within ten minutes and deploy another set of actor systems and just keep it running",
    "start": "430290",
    "end": "436400"
  },
  {
    "text": "there was a quite a few challenges here and I'll go over them but the challenge",
    "start": "436400",
    "end": "442580"
  },
  {
    "text": "some of the challenges that we ran into was mostly around state and sinking",
    "start": "442580",
    "end": "448320"
  },
  {
    "text": "state and how do you sink that state across all those nodes as they spin up",
    "start": "448320",
    "end": "454110"
  },
  {
    "text": "and spin down for the actor model itself it's fairly straightforward",
    "start": "454110",
    "end": "461330"
  },
  {
    "text": "and it and it works on a few simple rules itself",
    "start": "461330",
    "end": "468050"
  },
  {
    "text": "every actor has one mailbox and the only way actors can actually talk to",
    "start": "468050",
    "end": "474570"
  },
  {
    "text": "each other is if you send one message between the actors the other important thing about actors",
    "start": "474570",
    "end": "482280"
  },
  {
    "text": "themselves is that they could spawn child actors so how this works is similar to what you would see in I OC",
    "start": "482280",
    "end": "488940"
  },
  {
    "text": "container where you push all your riskiest dependencies out to the leaves",
    "start": "488940",
    "end": "494520"
  },
  {
    "text": "rather than leaving it at the core so in this sense you what you would do is",
    "start": "494520",
    "end": "499889"
  },
  {
    "text": "you'd start spawning child actors and push it all the way down until you have specific implementations that are",
    "start": "499889",
    "end": "506639"
  },
  {
    "text": "localized and it will prevent the entire system from going down",
    "start": "506639",
    "end": "512419"
  },
  {
    "text": "that being said actors have at least within a canet have",
    "start": "512419",
    "end": "518430"
  },
  {
    "text": "this ability to supervise other actors you get these nice little trees that",
    "start": "518430",
    "end": "523770"
  },
  {
    "text": "what you when you send a message to the top level actor it might delegate all",
    "start": "523770",
    "end": "528930"
  },
  {
    "text": "the messages down to all of its child actors in practice what we've done is we've had these coordinator actors and",
    "start": "528930",
    "end": "535740"
  },
  {
    "text": "what they would do is they'd we'd have one that would read off of sqs we have",
    "start": "535740",
    "end": "541019"
  },
  {
    "text": "one that do it would do calculations and it works very not very very nicely because we didn't have to worry about",
    "start": "541019",
    "end": "547669"
  },
  {
    "text": "failures because we didn't have this monolithic task parallel library that we",
    "start": "547669",
    "end": "553319"
  },
  {
    "text": "had to worry about it and if there were and parsing aggregate exceptions if it",
    "start": "553319",
    "end": "558359"
  },
  {
    "text": "failed the other thing as I mentioned before is the actors don't necessarily take down the",
    "start": "558359",
    "end": "564449"
  },
  {
    "text": "entire system since we have child actors there's a few things that you can do with a cadet that allow you to either",
    "start": "564449",
    "end": "572279"
  },
  {
    "text": "restart failed actors kill them off or",
    "start": "572279",
    "end": "577669"
  },
  {
    "text": "continue with the air and just keep going the other thing to note with a cadet and",
    "start": "577669",
    "end": "584759"
  },
  {
    "text": "as well as the act at JVM is that it's fast it handles roughly around 50",
    "start": "584759",
    "end": "591089"
  },
  {
    "text": "million messages on a single machine and one of the things that we learned the hard way is that with this cut with this",
    "start": "591089",
    "end": "598259"
  },
  {
    "text": "kind of power we had to scale back our actors because it was too powerful on",
    "start": "598259",
    "end": "604259"
  },
  {
    "text": "our machines there was one instance where we actually get a nice email from Amazon s3 because we turned it into a",
    "start": "604259",
    "end": "611609"
  },
  {
    "text": "logging system and we were doing about a gigabit a second of just log files and",
    "start": "611609",
    "end": "617759"
  },
  {
    "text": "then when we had we had to scale it back so this is a very powerful system that",
    "start": "617759",
    "end": "624299"
  },
  {
    "text": "we have in place now with akka itself",
    "start": "624299",
    "end": "629809"
  },
  {
    "text": "it's pretty straightforward so with actors being able to send actors",
    "start": "629809",
    "end": "635269"
  },
  {
    "text": "messages to other actors this is probably the simplest example that I could come up with now a message in this",
    "start": "635269",
    "end": "644009"
  },
  {
    "text": "case is pretty much any immutable reference object that you pass from one",
    "start": "644009",
    "end": "651660"
  },
  {
    "text": "actor to the other it's also worth mentioning that actors themselves are location transparent so as long as you",
    "start": "651660",
    "end": "660149"
  },
  {
    "text": "have the actor reference and you'd call a send to another actor you never have",
    "start": "660149",
    "end": "665459"
  },
  {
    "text": "to worry about where it is the other thing is that with",
    "start": "665459",
    "end": "670999"
  },
  {
    "text": "this as simple as it is this is more of syntactic sugar this is the strongly",
    "start": "670999",
    "end": "677079"
  },
  {
    "text": "typed version so if I go through the constructor here you could see that I'm sending a string in practice you",
    "start": "677079",
    "end": "682660"
  },
  {
    "text": "wouldn't want to send a string you'd probably want to send something like an immutable class with no public setters",
    "start": "682660",
    "end": "689290"
  },
  {
    "text": "just to make sure that you could serialize it and have guarantees about concurrency never have to worry about",
    "start": "689290",
    "end": "695889"
  },
  {
    "text": "locking or any kind of threading issues and as you can see here this is pretty",
    "start": "695889",
    "end": "703480"
  },
  {
    "text": "much an example of what you would do for an immutable message you've got all public getters and private setters and",
    "start": "703480",
    "end": "711100"
  },
  {
    "text": "in a Quebec ax or a cadet net what it does is it handles all the serialization for you in some form of fashion it would",
    "start": "711100",
    "end": "720000"
  },
  {
    "text": "serialize this down to either Jason I think the latest versions do binary serialization you can always check with",
    "start": "720000",
    "end": "726399"
  },
  {
    "text": "Aaron and sending messages is really really simple",
    "start": "726399",
    "end": "731620"
  },
  {
    "text": "so the TEL method is you've got one message and then the",
    "start": "731620",
    "end": "738519"
  },
  {
    "text": "second one the second parameter there is the actor ref or address in this case I",
    "start": "738519",
    "end": "744730"
  },
  {
    "text": "just said itself just because I want to reflect the message back and process it it's really really that easy",
    "start": "744730",
    "end": "751319"
  },
  {
    "text": "the other thing you should note is that with actors there's this concept of an",
    "start": "751319",
    "end": "756940"
  },
  {
    "text": "actor ref the address itself there's a few things here at the protocol it's",
    "start": "756940",
    "end": "763480"
  },
  {
    "text": "fairly straightforward so it's it's TCP because you want to send it over an HTTP",
    "start": "763480",
    "end": "768550"
  },
  {
    "text": "connection you have the name of the actor system so that's just in case you might have more than one actor system",
    "start": "768550",
    "end": "774220"
  },
  {
    "text": "inside of a machine and at the same time you also have the host name and port as",
    "start": "774220",
    "end": "782199"
  },
  {
    "text": "well as the path now if we look back at some of the other diagrams where you had",
    "start": "782199",
    "end": "787569"
  },
  {
    "text": "supervision where you have a Youth slash user slash actor name that's basically the path in the tree for whatever actor",
    "start": "787569",
    "end": "796000"
  },
  {
    "text": "you want to talk to so for example if I want to say user actor name one and",
    "start": "796000",
    "end": "801010"
  },
  {
    "text": "there's an actor under it called foo it would just be slash foo and I'd be sending messages that way so",
    "start": "801010",
    "end": "806830"
  },
  {
    "text": "it's fairly simple to be able to address and send messages across the wire using",
    "start": "806830",
    "end": "813040"
  },
  {
    "text": "this kind of addressing scheme now the other interesting thing that we",
    "start": "813040",
    "end": "820360"
  },
  {
    "text": "found out pretty quickly is if we want to do a kind of broadcast is you could do a wild card based broadcast just by",
    "start": "820360",
    "end": "827769"
  },
  {
    "text": "using something called the actor selection now with actor selection what you can do is if you're in an actor at a",
    "start": "827769",
    "end": "834940"
  },
  {
    "text": "certain point of a tree the context object what that does is say from this",
    "start": "834940",
    "end": "840279"
  },
  {
    "text": "point in the tree onwards I could send these sets of messages so in the same",
    "start": "840279",
    "end": "845560"
  },
  {
    "text": "sense you could send message to an individual system but you could also use actor selections to do",
    "start": "845560",
    "end": "851940"
  },
  {
    "text": "wildcard broadcasts to child actors if necessary",
    "start": "851940",
    "end": "858600"
  },
  {
    "text": "now in the simplest sense if you really wanted to create an actor system it's",
    "start": "859470",
    "end": "864519"
  },
  {
    "text": "one line of code plus maybe the name of the actor system if I inlined it",
    "start": "864519",
    "end": "869670"
  },
  {
    "text": "in a production environment of course you there's a few more configuration",
    "start": "869670",
    "end": "876040"
  },
  {
    "text": "settings that you have to set you can you can die you could do that through either ho con or you could do it",
    "start": "876040",
    "end": "882730"
  },
  {
    "text": "programmatically which is what we prefer inside of an AWS environment and the reason why we went with programmatic is",
    "start": "882730",
    "end": "889470"
  },
  {
    "text": "that more often than not when we deployed to these boxes in AWS it became",
    "start": "889470",
    "end": "896440"
  },
  {
    "text": "impractical to have to RDP into those boxes and make sure that they have the",
    "start": "896440",
    "end": "902380"
  },
  {
    "text": "right configs we want to make sure they're all homogeneous but at the same time we want to make it as zero-touch as",
    "start": "902380",
    "end": "909040"
  },
  {
    "text": "possible and still be able to function now the syntax for creating an actor",
    "start": "909040",
    "end": "916270"
  },
  {
    "text": "system is something that's been inherited from the JVM and",
    "start": "916270",
    "end": "920910"
  },
  {
    "text": "what this essentially means is this is analogous to what com did back in the",
    "start": "921660",
    "end": "926709"
  },
  {
    "text": "90s when you said I want to create an instance of an actor now inside of the props doc create is basically a lambda",
    "start": "926709",
    "end": "936570"
  },
  {
    "text": "tells a canet how to create your actor",
    "start": "936570",
    "end": "941840"
  },
  {
    "text": "regardless of whether or not it is on your local machine or on a remote machine if you're going to be using",
    "start": "941840",
    "end": "947010"
  },
  {
    "text": "clustering so in the same way once we create the actor you could just do a",
    "start": "947010",
    "end": "953040"
  },
  {
    "text": "tell on the actor without having to worry about where it is",
    "start": "953040",
    "end": "958100"
  },
  {
    "text": "at the same time if I want to create a child actor all I have to do is use",
    "start": "958100",
    "end": "963870"
  },
  {
    "text": "context which basically means that what wherever you are in the hierarchy you",
    "start": "963870",
    "end": "969570"
  },
  {
    "text": "want to create a child actor of my child actor in this case again it's the syntax",
    "start": "969570",
    "end": "975210"
  },
  {
    "text": "is very very regulus very straight forward simply because there's not really a lot of code that you have to do",
    "start": "975210",
    "end": "982590"
  },
  {
    "text": "in order to get it done the one thing to remember in this case is that while you're doing all these",
    "start": "982590",
    "end": "989070"
  },
  {
    "text": "things with actors you never have to worry about locking it's very very safe to have private",
    "start": "989070",
    "end": "996260"
  },
  {
    "text": "collections that are not concurrent collections for example you don't need a sync lock object you can do whatever you",
    "start": "996260",
    "end": "1003080"
  },
  {
    "text": "want with it provided that all your state is immutable and not publicly",
    "start": "1003080",
    "end": "1009680"
  },
  {
    "text": "exposed the other thing that's really",
    "start": "1009680",
    "end": "1015910"
  },
  {
    "text": "interesting at least from an architectural standpoint is does anybody remember Windows 3.1 or",
    "start": "1015910",
    "end": "1023950"
  },
  {
    "text": "ya or does anybody not want to remember Windows 3.1 so",
    "start": "1023950",
    "end": "1031150"
  },
  {
    "text": "what we realized is that this is actually a",
    "start": "1031150",
    "end": "1036189"
  },
  {
    "text": "degenerate concurrent cooperative multi-threading distributed",
    "start": "1036190",
    "end": "1043250"
  },
  {
    "text": "system so basically it's like distributed Windows 3.1 without the disadvantages",
    "start": "1043250",
    "end": "1049390"
  },
  {
    "text": "because when you spawn actors you're basically saying you're basically",
    "start": "1049390",
    "end": "1054680"
  },
  {
    "text": "sending it out into the world with the assumption that nothing can break it because it doesn't expose any state the",
    "start": "1054680",
    "end": "1062030"
  },
  {
    "text": "other thing to remember is that with these actors you can just keep doing all",
    "start": "1062030",
    "end": "1067040"
  },
  {
    "text": "these things in parallel and you never have to worry about how are they going to interfere with each other and later",
    "start": "1067040",
    "end": "1073140"
  },
  {
    "text": "on I'll show you how we could spawn it up to several thousand threads but still",
    "start": "1073140",
    "end": "1078360"
  },
  {
    "text": "be able to treat it as if it were a single thread and not have to worry about it of course not everything goes",
    "start": "1078360",
    "end": "1084630"
  },
  {
    "text": "according to plan with akka dotnet actors when things start to hit the fan",
    "start": "1084630",
    "end": "1090360"
  },
  {
    "text": "the problem is that how do you handle errors when things happen now there's",
    "start": "1090360",
    "end": "1095549"
  },
  {
    "text": "something in from the original paper from Carl Hewitt which which is this",
    "start": "1095549",
    "end": "1100890"
  },
  {
    "text": "idea of the air per kernel and how this works is that the air is bubble up from",
    "start": "1100890",
    "end": "1106860"
  },
  {
    "text": "the child nodes all the way to the top and in these child nodes whenever things go",
    "start": "1106860",
    "end": "1113220"
  },
  {
    "text": "wrong the parent actor has has a couple of choices about what they want to do in",
    "start": "1113220",
    "end": "1118679"
  },
  {
    "text": "case things fail in general there's really two strategies",
    "start": "1118679",
    "end": "1125789"
  },
  {
    "text": "you could come up with your own custom strategy the first strategy is going to be that you could kill off the actor",
    "start": "1125789",
    "end": "1133500"
  },
  {
    "text": "that caused the failure as a parent actor or what you could do is you could",
    "start": "1133500",
    "end": "1139289"
  },
  {
    "text": "kill off all the child actors and restart there's also the option to resume but it's like vb6 on a resume",
    "start": "1139289",
    "end": "1147900"
  },
  {
    "text": "next you don't want to do that because you want to make sure that when it fails you restart it and keep it in the right",
    "start": "1147900",
    "end": "1154830"
  },
  {
    "text": "state and that's what we've learned the other thing is",
    "start": "1154830",
    "end": "1160309"
  },
  {
    "text": "so at this point I've been talking about how to handle a single machine",
    "start": "1160309",
    "end": "1165950"
  },
  {
    "text": "multi-threaded scenario but how do we handle cases where we want to go distributed because it's great we want",
    "start": "1165950",
    "end": "1173250"
  },
  {
    "text": "to do it use a canet we want to be able to spawn any number of threads we want",
    "start": "1173250",
    "end": "1178350"
  },
  {
    "text": "but how do you make it so that this works on a very large scale so for",
    "start": "1178350",
    "end": "1184590"
  },
  {
    "text": "us we had this huge problem the one megalithic database server that just",
    "start": "1184590",
    "end": "1189780"
  },
  {
    "text": "couldn't handle the load we would get thousands millions upon millions of",
    "start": "1189780",
    "end": "1195000"
  },
  {
    "text": "events per day but we couldn't process it at all and now we have this challenge where we wanted to do it in real time",
    "start": "1195000",
    "end": "1202700"
  },
  {
    "text": "so the first thing we can do when we start",
    "start": "1202700",
    "end": "1207929"
  },
  {
    "text": "talking about how to scale up with the single machine is that there's these there's a special kind of actors called",
    "start": "1207929",
    "end": "1215029"
  },
  {
    "text": "routers and router pools now with a",
    "start": "1215029",
    "end": "1220409"
  },
  {
    "text": "round-robin pool what that means is that it is a specific type of actor that",
    "start": "1220409",
    "end": "1228149"
  },
  {
    "text": "spawns thousands and thousands of other child actors of this type",
    "start": "1228149",
    "end": "1235399"
  },
  {
    "text": "depending on the load so what this means is if I send a message",
    "start": "1235399",
    "end": "1242029"
  },
  {
    "text": "the message will come in through the router there might be up to 1,000",
    "start": "1242389",
    "end": "1247889"
  },
  {
    "text": "instances inside of that router actor and it'll",
    "start": "1247889",
    "end": "1253019"
  },
  {
    "text": "just go through every single one of the actors in a round robin fashion until every single one of the messages are",
    "start": "1253019",
    "end": "1259710"
  },
  {
    "text": "handled in parallel so the nice thing about this is that I don't have to worry about",
    "start": "1259710",
    "end": "1266029"
  },
  {
    "text": "managing state I don't have to worry about what to do with every single actor because this is kind of like a classic",
    "start": "1266029",
    "end": "1272399"
  },
  {
    "text": "object-oriented programming where every single class is responsible for its own state every single actor is responsible",
    "start": "1272399",
    "end": "1280379"
  },
  {
    "text": "for making sure that as child actors are behaving properly and if it doesn't then",
    "start": "1280379",
    "end": "1285779"
  },
  {
    "text": "the router will kill off the the child actor depending on the strategy of course",
    "start": "1285779",
    "end": "1291049"
  },
  {
    "text": "now as I mentioned before you don't have to worry about locks and you don't even",
    "start": "1291049",
    "end": "1297870"
  },
  {
    "text": "have to worry about actor allocations because the child the round-robin routers have a parameter in there that",
    "start": "1297870",
    "end": "1303960"
  },
  {
    "text": "say what's the minimum number of actors you want and what's the number what's the maximum number of actors that you",
    "start": "1303960",
    "end": "1310440"
  },
  {
    "text": "want to spin up so in the main what we did it was for a single machine configuration we started getting all",
    "start": "1310440",
    "end": "1317789"
  },
  {
    "text": "these messages that it would flow in and obviously if we're you're thinking of an actor as a single threaded actor you",
    "start": "1317789",
    "end": "1323519"
  },
  {
    "text": "don't want to handle this on one thread you want to spin this up about a thousand actors so that we could",
    "start": "1323519",
    "end": "1329120"
  },
  {
    "text": "potentially process them all in parallel",
    "start": "1329120",
    "end": "1334100"
  },
  {
    "text": "so there's a few other pool router types but this is more of a exercise for",
    "start": "1334840",
    "end": "1340659"
  },
  {
    "text": "Google there's random which doesn't make any sense but because we want to evenly",
    "start": "1340659",
    "end": "1345999"
  },
  {
    "text": "distribute the load there's consistent hash routing which is more useful if",
    "start": "1345999",
    "end": "1351700"
  },
  {
    "text": "you're going to go down the path of clustering and then there's a small mailboxes which is just trying to route",
    "start": "1351700",
    "end": "1358419"
  },
  {
    "text": "the messages to the actor with the least amount of activity now I know this the code is fairly small",
    "start": "1358419",
    "end": "1367960"
  },
  {
    "text": "in this case but this is pretty much all we needed to do to scale to thousands of",
    "start": "1367960",
    "end": "1373570"
  },
  {
    "text": "threads per machine this is just a simple PowerShell script that starts up",
    "start": "1373570",
    "end": "1379119"
  },
  {
    "text": "an actor system five times on the same machine there's nothing magical about this and that's the point we didn't have",
    "start": "1379119",
    "end": "1385419"
  },
  {
    "text": "to do anything special to do that the interesting part is that we were",
    "start": "1385419",
    "end": "1390669"
  },
  {
    "text": "able to scale up our capacity in two ways we could either deploy two more",
    "start": "1390669",
    "end": "1396490"
  },
  {
    "text": "machines or run more instances of the actor system on a single machine so that",
    "start": "1396490",
    "end": "1401860"
  },
  {
    "text": "we see the CPU spike up to 100% now the interesting part is how do we",
    "start": "1401860",
    "end": "1409809"
  },
  {
    "text": "get these actor systems to work together there's a few it depending on what your requirements are there's a few ways you",
    "start": "1409809",
    "end": "1415929"
  },
  {
    "text": "could do this now the first way which I don't recommend is",
    "start": "1415929",
    "end": "1421210"
  },
  {
    "text": "really going with a cadet remote that's really just peer-to-peer connecting to actor systems together",
    "start": "1421210",
    "end": "1427090"
  },
  {
    "text": "with nope no redundancy at all and that's what akka cluster is based on an",
    "start": "1427090",
    "end": "1432610"
  },
  {
    "text": "aqua cluster is a good tool in the sense that it's completely decentralized it's",
    "start": "1432610",
    "end": "1438669"
  },
  {
    "text": "a peer-to-peer node cluster that Alexis leaders and there's quite a bit of",
    "start": "1438669",
    "end": "1444190"
  },
  {
    "text": "gossip between the nodes but there are some drawbacks and of course we'll go over that as well the",
    "start": "1444190",
    "end": "1450940"
  },
  {
    "text": "one we chose that was most favorable was the cloud-based queues so we're on AWS we use SQS to do batch jobs and what we",
    "start": "1450940",
    "end": "1461350"
  },
  {
    "text": "found is that it's in most cases AWS is reliable unless you you know it was a",
    "start": "1461350",
    "end": "1467270"
  },
  {
    "text": "couple months ago where it did went down go down for a couple days but for most of the time it was perfectly fine",
    "start": "1467270",
    "end": "1474280"
  },
  {
    "text": "now a crow remote so as you can see here this is just a simple",
    "start": "1474280",
    "end": "1480580"
  },
  {
    "text": "diagram of all the states every single node is in inside of a cluster so what",
    "start": "1480580",
    "end": "1487940"
  },
  {
    "text": "they do is they start talking to each other and the algorithm is basically determined",
    "start": "1487940",
    "end": "1493730"
  },
  {
    "text": "which nodes are connected within the system and if they're not able to talk to each other they actually talk through",
    "start": "1493730",
    "end": "1499220"
  },
  {
    "text": "an intermediary node so they could be all of the sync now clusters themselves",
    "start": "1499220",
    "end": "1504280"
  },
  {
    "text": "make it so that every single node inside of the cluster behaves like a",
    "start": "1504280",
    "end": "1509840"
  },
  {
    "text": "large monolithic system in most cases it does work well there are systems like",
    "start": "1509840",
    "end": "1515180"
  },
  {
    "text": "the saboteur react and whatnot that do this but what we found inside a domain that this doesn't really scale well at",
    "start": "1515180",
    "end": "1521750"
  },
  {
    "text": "least for our requirements and given that fact that we would have lots of machines spin up and sometimes disappear",
    "start": "1521750",
    "end": "1529220"
  },
  {
    "text": "depending on what the CPU load was so again there's a few disadvantages here",
    "start": "1529220",
    "end": "1536210"
  },
  {
    "text": "the number one disadvantage with in a cluster is the fact that if",
    "start": "1536210",
    "end": "1542380"
  },
  {
    "text": "for some reason let's say you had five nodes and in those five nodes they were",
    "start": "1542380",
    "end": "1548390"
  },
  {
    "text": "behaving as part of a cluster and in our environment we did a WS detected that",
    "start": "1548390",
    "end": "1556280"
  },
  {
    "text": "the CPU dropped below a certain threshold and we could kill off two boxes or let's say three if you kill off",
    "start": "1556280",
    "end": "1563750"
  },
  {
    "text": "another enough boxes within a cluster somebody actually has to manually",
    "start": "1563750",
    "end": "1568970"
  },
  {
    "text": "intervene and tell the rest of the remaining nodes that the cluster and all",
    "start": "1568970",
    "end": "1574760"
  },
  {
    "text": "the missing nodes have been removed which is quite messy because it's not",
    "start": "1574760",
    "end": "1580100"
  },
  {
    "text": "very cloudy because we want to do this in such a way that doesn't require any kind of human intervention this should",
    "start": "1580100",
    "end": "1586370"
  },
  {
    "text": "just work so I took the time to just draw a very",
    "start": "1586370",
    "end": "1594980"
  },
  {
    "text": "crude diagram of how we what the flow the data is with in domain",
    "start": "1594980",
    "end": "1601309"
  },
  {
    "text": "so we have a quick stream where people do things like send inquiries we say for",
    "start": "1601309",
    "end": "1606860"
  },
  {
    "text": "example you might have somebody click on a nice house and say I'm really interested in buying this property or",
    "start": "1606860",
    "end": "1613130"
  },
  {
    "text": "renting it and I want to contact the agent and figure out whether it's still available we need to record every single",
    "start": "1613130",
    "end": "1620510"
  },
  {
    "text": "thing that everybody does on the domain website and that's what's happening inside the click stream now the tricky",
    "start": "1620510",
    "end": "1627320"
  },
  {
    "text": "part about this is that we have to save this at this roughly around the same",
    "start": "1627320",
    "end": "1632600"
  },
  {
    "text": "time we have to do aggregates for every single property on the domain website so",
    "start": "1632600",
    "end": "1638750"
  },
  {
    "text": "you can imagine if there's a million properties that are active per day we",
    "start": "1638750",
    "end": "1643820"
  },
  {
    "text": "want to come up with the total number of inquiries how many times somebody clicked on something and have those",
    "start": "1643820",
    "end": "1649120"
  },
  {
    "text": "metrics available at any time and they should be up to date within 10 or 20 seconds or at least something much",
    "start": "1649120",
    "end": "1656149"
  },
  {
    "text": "faster than the six hour or 12 hour latency that we were experiencing in the beginning so how this works is that we",
    "start": "1656149",
    "end": "1663740"
  },
  {
    "text": "found that we had to save it in a couple places we had to save it in s the s3 so",
    "start": "1663740",
    "end": "1670850"
  },
  {
    "text": "that we can export it into redshift and at the same time I didn't put it on the diagram but we had to save it into",
    "start": "1670850",
    "end": "1676610"
  },
  {
    "text": "elasticsearch because what we found is that even though in theory if we had two",
    "start": "1676610",
    "end": "1681919"
  },
  {
    "text": "copies of the same data one going to elasticsearch one going to s3 they",
    "start": "1681919",
    "end": "1687200"
  },
  {
    "text": "should be identical at the end of every day we always found that there was some messages that are dropped",
    "start": "1687200",
    "end": "1692950"
  },
  {
    "text": "so to prevent any kind of loss what we would do is we would do",
    "start": "1692950",
    "end": "1699340"
  },
  {
    "text": "how we compile a hash list of all the events that have occurred within a day",
    "start": "1699340",
    "end": "1706580"
  },
  {
    "text": "and compare it to what was exported in s3 and compared to what we had in",
    "start": "1706580",
    "end": "1713360"
  },
  {
    "text": "elasticsearch we would do a diff and then once we figure it out which",
    "start": "1713360",
    "end": "1719059"
  },
  {
    "text": "properties were affected as a result of the loss of that diff we do an",
    "start": "1719059",
    "end": "1724429"
  },
  {
    "text": "end-of-day recompute to make sure that we didn't we have the right numbers and we still have the eventual consistency",
    "start": "1724429",
    "end": "1730880"
  },
  {
    "text": "that we wanted without sacrificing accuracy or losing anything which was",
    "start": "1730880",
    "end": "1736400"
  },
  {
    "text": "very very important for us now the other thing that we found is",
    "start": "1736400",
    "end": "1742090"
  },
  {
    "text": "that Amazon sqs is really really good at distributing workloads",
    "start": "1742090",
    "end": "1749230"
  },
  {
    "text": "at first we were going to go with Kinesis but the API depending on what",
    "start": "1749230",
    "end": "1755290"
  },
  {
    "text": "SDKs you use it was a bit raw and we needed something really simple we wanted",
    "start": "1755290",
    "end": "1761120"
  },
  {
    "text": "to reduce the consumption and and posting messages down to a single HTTP",
    "start": "1761120",
    "end": "1767540"
  },
  {
    "text": "GET or post in this case so that's why we went with Amazon sqs",
    "start": "1767540",
    "end": "1775120"
  },
  {
    "text": "now if your is everybody here familiar with AWS SQS or",
    "start": "1776050",
    "end": "1782770"
  },
  {
    "text": "how many people here use Azure how many people here use AWS",
    "start": "1782770",
    "end": "1790540"
  },
  {
    "text": "so that's about half and half so when we were looking into SQS it had a",
    "start": "1790540",
    "end": "1797870"
  },
  {
    "text": "really interesting property where there was something called a timeout an",
    "start": "1797870",
    "end": "1803150"
  },
  {
    "text": "invisibility timeout and the idea was that if you pulled a message out of sqs",
    "start": "1803150",
    "end": "1809840"
  },
  {
    "text": "you could set a time limit where we it would become invisible and if you didn't delete it it would pop back into the",
    "start": "1809840",
    "end": "1817970"
  },
  {
    "text": "queue as if it was never processed now normally that doesn't seem like a lot",
    "start": "1817970",
    "end": "1824360"
  },
  {
    "text": "but for us it had an interesting side effect because we could process jobs in",
    "start": "1824360",
    "end": "1830570"
  },
  {
    "text": "parallel and if the jobs failed we didn't have to know whether they failed or not because we would know that they",
    "start": "1830570",
    "end": "1837530"
  },
  {
    "text": "failed if they were still in the queue so if it failed the first time and an",
    "start": "1837530",
    "end": "1843050"
  },
  {
    "text": "actor didn't delete the message off of the queue it goes straight back into the queue and we have a different actor",
    "start": "1843050",
    "end": "1849470"
  },
  {
    "text": "system process it all over again and it would do it completely in parallel now the best part about this is that when we",
    "start": "1849470",
    "end": "1856100"
  },
  {
    "text": "had five or six different actor systems they didn't need to talk to each other at all there was no sync time when we",
    "start": "1856100",
    "end": "1864530"
  },
  {
    "text": "had cases where we were getting we're about a million messages behind it would",
    "start": "1864530",
    "end": "1869960"
  },
  {
    "text": "became a really easy scenario to handle because now all we had to do is create",
    "start": "1869960",
    "end": "1875540"
  },
  {
    "text": "another set of actor systems or spin up about five or six more machines and then",
    "start": "1875540",
    "end": "1880760"
  },
  {
    "text": "we were back in business and they were able to process the load there were a",
    "start": "1880760",
    "end": "1885830"
  },
  {
    "text": "couple trade offs of course because with this kind of amount of power we could easily take down any database server so",
    "start": "1885830",
    "end": "1892790"
  },
  {
    "text": "as you can see here the diagram pretty much shows what I was talking about you",
    "start": "1892790",
    "end": "1899480"
  },
  {
    "text": "have all these data sources that post a clickstream event which triggers an",
    "start": "1899480",
    "end": "1905540"
  },
  {
    "text": "aggregation request that we stick inside of the queue on the blue side we would",
    "start": "1905540",
    "end": "1911540"
  },
  {
    "text": "have an aggregate of an aggregator that would just spin up pull the message off",
    "start": "1911540",
    "end": "1917360"
  },
  {
    "text": "the queue pull all this data into a snapshot in elasticsearch do the",
    "start": "1917360",
    "end": "1923210"
  },
  {
    "text": "computations and then throw it away and save the result so it was a really",
    "start": "1923210",
    "end": "1929060"
  },
  {
    "text": "interesting scenario because we did have it we didn't have strong consistency but",
    "start": "1929060",
    "end": "1934520"
  },
  {
    "text": "we did have eventual consistency that fit within the latency period that most people were expecting at the time",
    "start": "1934520",
    "end": "1940460"
  },
  {
    "text": "because at the time agents were expecting things to happen within 24 hours if we get away with doing this",
    "start": "1940460",
    "end": "1947180"
  },
  {
    "text": "within a couple of seconds that and they notice a few glitches here and there it's not a big deal the fact that we",
    "start": "1947180",
    "end": "1954170"
  },
  {
    "text": "were also using event sourcing where we might save events straight into",
    "start": "1954170",
    "end": "1959390"
  },
  {
    "text": "elasticsearch and there might be inconsistencies between two sets of",
    "start": "1959390",
    "end": "1964670"
  },
  {
    "text": "actors trying to do the same aggregation was not a big deal because we never really deleted any data at all they",
    "start": "1964670",
    "end": "1971480"
  },
  {
    "text": "would get a different snapshot and maybe one would overlap the other but we compensated for that by doing end of a",
    "start": "1971480",
    "end": "1977900"
  },
  {
    "text": "snapshots where we would figure out which properties were active and do one last",
    "start": "1977900",
    "end": "1985160"
  },
  {
    "text": "calculation so we have the final sense of consistency that we are looking for",
    "start": "1985160",
    "end": "1990370"
  },
  {
    "text": "now from an academic standpoint you might be asking so what's the difference between",
    "start": "1990370",
    "end": "1995810"
  },
  {
    "text": "clusters and grids and it's actually quite simple I mean I don't have to go through this list but what it comes down",
    "start": "1995810",
    "end": "2001750"
  },
  {
    "text": "- is that clusters require consensus when things go bad inside of a cluster",
    "start": "2001750",
    "end": "2007810"
  },
  {
    "text": "you all the notes basically say are you okay are you okay what happened to this node a what happened to note B is that",
    "start": "2007810",
    "end": "2015370"
  },
  {
    "text": "node okay but within a grid we could linearly",
    "start": "2015370",
    "end": "2020530"
  },
  {
    "text": "linearly scale by adding five or six more nodes and since every single actor",
    "start": "2020530",
    "end": "2026350"
  },
  {
    "text": "system is behaving as if all the other actor systems don't exist it's very easy",
    "start": "2026350",
    "end": "2032440"
  },
  {
    "text": "for us to scale up and be able to push new nodes out without even have to worry",
    "start": "2032440",
    "end": "2038380"
  },
  {
    "text": "about this so our sample grid configuration looks like this so in a in a classic",
    "start": "2038380",
    "end": "2046660"
  },
  {
    "text": "grid configuration you would have a master node that would start handing out jobs but in this case we use sqs because",
    "start": "2046660",
    "end": "2053620"
  },
  {
    "text": "what happens is that when we push something into the queue one actor",
    "start": "2053620",
    "end": "2059429"
  },
  {
    "text": "pulls one message off the queue at a time just to be fair and we might have",
    "start": "2059430",
    "end": "2064899"
  },
  {
    "text": "twenty actor systems that are pulling one message at a time and processing it so every second they pull one message",
    "start": "2064900",
    "end": "2070600"
  },
  {
    "text": "and process it and I wouldn't know where it's actually occurring on but it",
    "start": "2070600",
    "end": "2075940"
  },
  {
    "text": "doesn't matter I don't even need to know whether successful or not because if it fails it drops back into the queue and",
    "start": "2075940",
    "end": "2081760"
  },
  {
    "text": "it's processed all over again now for easy two instances this is the interesting part depending on the load",
    "start": "2081760",
    "end": "2088960"
  },
  {
    "text": "we we fixed it to a minimum of two instances just so we had redundancy but",
    "start": "2088960",
    "end": "2095050"
  },
  {
    "text": "if there was a high amount of traffic we would scale up to 20 nodes and that came",
    "start": "2095050",
    "end": "2101080"
  },
  {
    "text": "that would come out to be a hundred actor systems in parallel now with this kind of scalability it's not something",
    "start": "2101080",
    "end": "2107020"
  },
  {
    "text": "you could easily pull off in cluster with cluster you wouldn't be able to handle the fact that you're taking out",
    "start": "2107020",
    "end": "2113860"
  },
  {
    "text": "and putting in so many nodes at once depending on the demand in which this is what we needed",
    "start": "2113860",
    "end": "2119730"
  },
  {
    "text": "the other thing that we have is that we actually in domain we have this custom tool that we built into octopus which is",
    "start": "2119730",
    "end": "2127720"
  },
  {
    "text": "what we call the robot army where we could spin up entire clusters in a line of code and deploy certain versions of octopus",
    "start": "2127720",
    "end": "2136799"
  },
  {
    "text": "deploy packages straight into these machines without even having to worry",
    "start": "2136799",
    "end": "2142450"
  },
  {
    "text": "about any issues I mean in most cases what we were able to do is hit the button go up get coffee come back and",
    "start": "2142450",
    "end": "2149619"
  },
  {
    "text": "everything was all good now in this scenario when we are handling clickstream events what was",
    "start": "2149619",
    "end": "2156609"
  },
  {
    "text": "happening was the AWS would detect a high CPU demand the robot army would",
    "start": "2156609",
    "end": "2164890"
  },
  {
    "text": "respond by creating two or three more instances and then push the actor",
    "start": "2164890",
    "end": "2171220"
  },
  {
    "text": "systems out to these boxes and then all of a sudden you see this massive CPU drop that would be able to handle the",
    "start": "2171220",
    "end": "2178809"
  },
  {
    "text": "click streams as they came in now compared to say your traditional relational database this was way better",
    "start": "2178809",
    "end": "2185170"
  },
  {
    "text": "because we found that we could we had a latency of about 10 seconds and that's",
    "start": "2185170",
    "end": "2190660"
  },
  {
    "text": "across any number of properties at once and this was doing this hundreds and",
    "start": "2190660",
    "end": "2195760"
  },
  {
    "text": "thousands of times per second the other interesting but notable thing that we",
    "start": "2195760",
    "end": "2201519"
  },
  {
    "text": "did was we log to slack so in cases of failure one of the things that we ran",
    "start": "2201519",
    "end": "2207369"
  },
  {
    "text": "into is that since all these systems are completely disconnected we need to be able to log this information to a",
    "start": "2207369",
    "end": "2213279"
  },
  {
    "text": "centralized location so slack was the best place so we just created a custom channel with just a bunch of alerts in",
    "start": "2213279",
    "end": "2219490"
  },
  {
    "text": "it and then we were able to push out all this information that now for grid actor roles the idea behind",
    "start": "2219490",
    "end": "2226119"
  },
  {
    "text": "grit grids was something I pulled out of",
    "start": "2226119",
    "end": "2230700"
  },
  {
    "text": "swarm intelligence AI so if you ever look at how bugs or ants behave you'll",
    "start": "2231509",
    "end": "2239589"
  },
  {
    "text": "see that within a colony they always follow a very simple set of rules by",
    "start": "2239589",
    "end": "2245109"
  },
  {
    "text": "themselves they don't seem very smart but once you start to multiply them to larger numbers they start doing pretty",
    "start": "2245109",
    "end": "2252549"
  },
  {
    "text": "amazing things and in that sense that's what we did with the grid so",
    "start": "2252549",
    "end": "2258150"
  },
  {
    "text": "there's a few things we did so when for pulling aggregation requests off the queue we had one actor that did it and",
    "start": "2258150",
    "end": "2265390"
  },
  {
    "text": "then it would forward it all the other actors that would process it we also had another actor that would just fetch the",
    "start": "2265390",
    "end": "2272589"
  },
  {
    "text": "snapshot for a particular property and all the clickstream events that occurred during that day and then do the",
    "start": "2272589",
    "end": "2280640"
  },
  {
    "text": "calculation and then we just dispose of the actor naturally and the other thing to keep in mind is that when we moved",
    "start": "2280640",
    "end": "2288940"
  },
  {
    "text": "events from sqs to s3 buckets we also had actors for that as well we also had",
    "start": "2288940",
    "end": "2295190"
  },
  {
    "text": "different actors that would do this kind of reconciliation where we would look at",
    "start": "2295190",
    "end": "2302690"
  },
  {
    "text": "what we had in elasticsearch and at the same time we would also look at what we",
    "start": "2302690",
    "end": "2307700"
  },
  {
    "text": "saved into s3 because ultimately what we were trying to do was do an export into",
    "start": "2307700",
    "end": "2313039"
  },
  {
    "text": "s3 and have it imported into redshift so we can do analytics on it so on one hand",
    "start": "2313039",
    "end": "2320029"
  },
  {
    "text": "we had a data warehousing solution that was the ultimate goal but at the same",
    "start": "2320029",
    "end": "2326180"
  },
  {
    "text": "time we want also want to main consistency in what we're exporting and calculating in real time at the same",
    "start": "2326180",
    "end": "2332059"
  },
  {
    "text": "time so there's a few things that we learned when we were",
    "start": "2332059",
    "end": "2338200"
  },
  {
    "text": "going through this process now for top one of the interesting parts that we we",
    "start": "2338200",
    "end": "2344119"
  },
  {
    "text": "found is that when we are doing these kinds of aggregations we were actually",
    "start": "2344119",
    "end": "2349489"
  },
  {
    "text": "sacrificing availability we had weak consistency and we did have strong",
    "start": "2349489",
    "end": "2355670"
  },
  {
    "text": "partition tolerance it was a really interesting case because the only form",
    "start": "2355670",
    "end": "2360829"
  },
  {
    "text": "consistency that we actually had was what we were storing in elastic certian",
    "start": "2360829",
    "end": "2366019"
  },
  {
    "text": "if if you do a bit of research in elasticsearch you'll know that it's not entirely strongly consistent but it is",
    "start": "2366019",
    "end": "2372229"
  },
  {
    "text": "eventually consistent and that's brigly to the first point now",
    "start": "2372229",
    "end": "2377749"
  },
  {
    "text": "given that you could just with this kind of capability with a code on net and working with multiple nodes you could",
    "start": "2377749",
    "end": "2384019"
  },
  {
    "text": "easily take down any server that is elasticsearch of that",
    "start": "2384019",
    "end": "2390200"
  },
  {
    "text": "is your relational database you name it with enough machines and enough actor",
    "start": "2390200",
    "end": "2396259"
  },
  {
    "text": "systems you could pretty much do anything you want with it so for us what we learn is",
    "start": "2396259",
    "end": "2401720"
  },
  {
    "text": "that it was more of an exercise in restraint rather than trying to throw all this capacity at it because even",
    "start": "2401720",
    "end": "2408170"
  },
  {
    "text": "though we had hundreds of thousands of threads that could go at once we weren't we were ultimately bottlenecked by i/o",
    "start": "2408170",
    "end": "2415900"
  },
  {
    "text": "at the end of the day now the other interesting thing but it",
    "start": "2415900",
    "end": "2422900"
  },
  {
    "text": "does sound a bit contradictory is that grid actor systems are great if they store absolutely no intrinsic state that",
    "start": "2422900",
    "end": "2431000"
  },
  {
    "text": "is long term state when we are doing these kinds of calculations what we would do is we pull in the snapshot and",
    "start": "2431000",
    "end": "2437119"
  },
  {
    "text": "it was consistent for that moment calculate it and then throw it away",
    "start": "2437119",
    "end": "2442210"
  },
  {
    "text": "now what that guarantees is that we could pretty much kill off any actor and the next actor that came along to",
    "start": "2442210",
    "end": "2449030"
  },
  {
    "text": "process the property itself wouldn't have to worry about whether it was wrong or not because we knew number one we",
    "start": "2449030",
    "end": "2455270"
  },
  {
    "text": "never deleted any events we're using events or see number two we never have any kind of inconsistency",
    "start": "2455270",
    "end": "2462410"
  },
  {
    "text": "problems because towards the end of the day we do one last calculation when we have everything and that would guarantee",
    "start": "2462410",
    "end": "2468680"
  },
  {
    "text": "that numbers were correct the other thing to keep in mind is that",
    "start": "2468680",
    "end": "2473810"
  },
  {
    "text": "we had to operate on the assumption that any one of our machines could just",
    "start": "2473810",
    "end": "2478970"
  },
  {
    "text": "vanish at any minute and that's because of auto scaling both up and down and",
    "start": "2478970",
    "end": "2485950"
  },
  {
    "text": "this is where the partition tolerance part comes into play if we don't have any state we keep the state in an",
    "start": "2485950",
    "end": "2492560"
  },
  {
    "text": "external node which is effectively elasticsearch we don't have to worry about any kind of consistency problems",
    "start": "2492560",
    "end": "2499930"
  },
  {
    "text": "so there were some pretty interesting trade-offs because essentially every time we would get a new event we start",
    "start": "2499930",
    "end": "2506480"
  },
  {
    "text": "querying events back into every single actor which there is a bit of",
    "start": "2506480",
    "end": "2512050"
  },
  {
    "text": "overhead but it was a good balance for us because we were finding that",
    "start": "2512050",
    "end": "2518470"
  },
  {
    "text": "we still maintain consistency we still main accuracy but it it did take a while",
    "start": "2518470",
    "end": "2524030"
  },
  {
    "text": "for everything to line up and that was more towards the end of the day but the side effect there is that when agents",
    "start": "2524030",
    "end": "2530450"
  },
  {
    "text": "started looking at their reports they could just refresh the page and do something and they could see it happen",
    "start": "2530450",
    "end": "2536420"
  },
  {
    "text": "in real time or close to real time and that's what we were looking for because we came from an environment where we",
    "start": "2536420",
    "end": "2542869"
  },
  {
    "text": "were waiting an entire day just for somebody to click on something in the number to go up now we are in a state",
    "start": "2542869",
    "end": "2549529"
  },
  {
    "text": "where if somebody just clicked like four or five times and did anything we'd be able to see it within a minute or so",
    "start": "2549529",
    "end": "2556130"
  },
  {
    "text": "which was like infinitely better than where we were now",
    "start": "2556130",
    "end": "2562569"
  },
  {
    "text": "it goes without saying that if you have an elastic",
    "start": "2562569",
    "end": "2569619"
  },
  {
    "text": "cluster of actor systems you need to have a matching data source that could",
    "start": "2569950",
    "end": "2576229"
  },
  {
    "text": "scale up with it in our case we found that s3 because of how its built could",
    "start": "2576229",
    "end": "2583729"
  },
  {
    "text": "scale up well with the demand at the same time elasticsearch depending on your instance",
    "start": "2583729",
    "end": "2590900"
  },
  {
    "text": "type can scale up along with the number of actors that you have so depending on",
    "start": "2590900",
    "end": "2597680"
  },
  {
    "text": "how many actor systems you have as well as the elasticsearch instances and whether or",
    "start": "2597680",
    "end": "2604910"
  },
  {
    "text": "not the auto scale if we start hammering the server what we have seen is that we",
    "start": "2604910",
    "end": "2610759"
  },
  {
    "text": "would spawn of another elasticsearch instance and it would be able to take the load and once the demand started to",
    "start": "2610759",
    "end": "2618319"
  },
  {
    "text": "go down what would happen is they pull the elasticsearch instance out but it would still keep running and that was",
    "start": "2618319",
    "end": "2625009"
  },
  {
    "text": "really important for us the fifth thing that we learned during the storm from",
    "start": "2625009",
    "end": "2630259"
  },
  {
    "text": "hell about a month and a half ago was that",
    "start": "2630259",
    "end": "2635949"
  },
  {
    "text": "it's better to make sure that even though you think that everything's going to be okay there you always have to",
    "start": "2635949",
    "end": "2643699"
  },
  {
    "text": "assume that you're going to have some sort of message loss that occurs so if",
    "start": "2643699",
    "end": "2649130"
  },
  {
    "text": "we have all these events coming in even if it's one in a million if we have 20",
    "start": "2649130",
    "end": "2654739"
  },
  {
    "text": "million events coming you're still going to lose 20 messages and the problem with that is that within",
    "start": "2654739",
    "end": "2661640"
  },
  {
    "text": "domain we can't afford to do that simply because one of those messages could have been a message to an agent saying hey",
    "start": "2661640",
    "end": "2667310"
  },
  {
    "text": "I'm really interested in your property you know that could have been a multi million dollar sale depending on where",
    "start": "2667310",
    "end": "2673010"
  },
  {
    "text": "in Sydney or any other capital city that's selling property so we wanted to",
    "start": "2673010",
    "end": "2679369"
  },
  {
    "text": "make sure that we don't lose anything so the interesting story here is that when the site went down because AWS went down",
    "start": "2679369",
    "end": "2686869"
  },
  {
    "text": "in the Sydney region we noticed that there's a lot of data",
    "start": "2686869",
    "end": "2692390"
  },
  {
    "text": "that we had within elasticsearch that was not in s3 we lost three days worth",
    "start": "2692390",
    "end": "2700130"
  },
  {
    "text": "of data now three days worth of clickstream data is nothing to laugh at it is a horrible",
    "start": "2700130",
    "end": "2707810"
  },
  {
    "text": "horrible situation but since we what we did is we came up",
    "start": "2707810",
    "end": "2713660"
  },
  {
    "text": "with making sure that we created two copies of the data every time it came in",
    "start": "2713660",
    "end": "2719710"
  },
  {
    "text": "we were able to use that diff to fill in the blanks it took us forever to do it",
    "start": "2719710",
    "end": "2725359"
  },
  {
    "text": "took us about a day or two to do it but we were able to fill in those blanks so the important thing to remember in this",
    "start": "2725359",
    "end": "2730970"
  },
  {
    "text": "case is that as long as you have a backup and you could you could have the",
    "start": "2730970",
    "end": "2736849"
  },
  {
    "text": "same kind of eventual consistency where you can merge the two sources and fill",
    "start": "2736849",
    "end": "2742430"
  },
  {
    "text": "in the blanks then you should be alright but the lesson to take away from this is the fact that you it's never keep all",
    "start": "2742430",
    "end": "2750890"
  },
  {
    "text": "your eggs in one basket because you will lose it at some point in time",
    "start": "2750890",
    "end": "2756460"
  },
  {
    "text": "the other thing that I probably didn't mention in this case is that for those",
    "start": "2756460",
    "end": "2763579"
  },
  {
    "text": "of you who were interested in the differences between how akka does things and",
    "start": "2763579",
    "end": "2770050"
  },
  {
    "text": "Orleans behaves it's really more of a difference",
    "start": "2770050",
    "end": "2775369"
  },
  {
    "text": "in philosophy now with akka it's its base the difference between the two is",
    "start": "2775369",
    "end": "2780770"
  },
  {
    "text": "really manual versus auto in Orleans there's this tendency of let's take care",
    "start": "2780770",
    "end": "2788660"
  },
  {
    "text": "of everything for you and you basically treat it as distributed c-sharp and that's fine for some people but in this",
    "start": "2788660",
    "end": "2796010"
  },
  {
    "text": "case if we were to do the kind of scaling it just wouldn't work for us because if we take machines out and all",
    "start": "2796010",
    "end": "2801820"
  },
  {
    "text": "of a sudden I don't think the Orleans cluster would do exactly what we want to do and it wouldn't allow us to tweak it",
    "start": "2801820",
    "end": "2808120"
  },
  {
    "text": "in the same way that we would be able to do with in AWS especially considering",
    "start": "2808120",
    "end": "2813580"
  },
  {
    "text": "they were in Azure so but aside from that I'm pretty much open",
    "start": "2813580",
    "end": "2819280"
  },
  {
    "text": "to any questions because this I know this is how are we doing with time",
    "start": "2819280",
    "end": "2825660"
  },
  {
    "text": "yep cool so what I'm going to do is this is more of a QA thing because I",
    "start": "2828300",
    "end": "2834910"
  },
  {
    "text": "know that I didn't really go over the how-to side though but there's a lot of interesting stuff that we came up with insider domain so the other thing I",
    "start": "2834910",
    "end": "2843130"
  },
  {
    "text": "would also plug is that in the same room in the next session are going to be to",
    "start": "2843130",
    "end": "2848860"
  },
  {
    "text": "our DevOps engineers who pretty much pulled it off and they're gonna be talking about how they do auto scaling",
    "start": "2848860",
    "end": "2855250"
  },
  {
    "text": "in excruciating detail in AWS including all the PowerShell scripts including how",
    "start": "2855250",
    "end": "2862120"
  },
  {
    "text": "they hook into octopus so please please please if you can watch the session",
    "start": "2862120",
    "end": "2867520"
  },
  {
    "text": "because that's really worth it and I did hear a rumor that they will open-source it very very soon so all the tools that",
    "start": "2867520",
    "end": "2875410"
  },
  {
    "text": "I've mentioned here are going to be open sourced so with that thanks for coming I",
    "start": "2875410",
    "end": "2881910"
  },
  {
    "text": "pretty much open to any questions",
    "start": "2881910",
    "end": "2885930"
  },
  {
    "text": "yeah so there's for the actors themselves",
    "start": "2896890",
    "end": "2902319"
  },
  {
    "text": "there was a heavy amount of pub/sub so how it works is when we were pulling",
    "start": "2902319",
    "end": "2908960"
  },
  {
    "text": "things from sqs we'd have one actor that would keep on pulling SQS for a certain",
    "start": "2908960",
    "end": "2915380"
  },
  {
    "text": "amount of messages on a timer and every time it would send something out",
    "start": "2915380",
    "end": "2920869"
  },
  {
    "text": "the way it works is that we had a base class that was",
    "start": "2920869",
    "end": "2926500"
  },
  {
    "text": "basically did all the pub/sub where it would just do a tell and we would keep a list of actor refs and it would go",
    "start": "2926500",
    "end": "2933500"
  },
  {
    "text": "through that list and just basically do your standard pub/sub and and forward it over the big one of course that I did mention",
    "start": "2933500",
    "end": "2940519"
  },
  {
    "text": "before is the competing consumers bit there were a couple instances where we",
    "start": "2940519",
    "end": "2947269"
  },
  {
    "text": "try to farm out a lot of the functionality just to make sure that if we had failures we didn't",
    "start": "2947269",
    "end": "2955279"
  },
  {
    "text": "really take down the whole system but I do have to mention that one of the problems that we did run into is that if",
    "start": "2955279",
    "end": "2962180"
  },
  {
    "text": "you scale the actor too much the actors too much you start running out of memory",
    "start": "2962180",
    "end": "2967420"
  },
  {
    "text": "one of the problems is that if you pull it in too much information at once",
    "start": "2967420",
    "end": "2972970"
  },
  {
    "text": "you will basically run into an out of memory exception and kill off all the actors are in every single actor system",
    "start": "2972970",
    "end": "2980329"
  },
  {
    "text": "so it's a balancing act and it's really about tweaking and making sure that you",
    "start": "2980329",
    "end": "2986599"
  },
  {
    "text": "have the right settings and that's just part of a cadet and",
    "start": "2986599",
    "end": "2992500"
  },
  {
    "text": "what else yeah",
    "start": "2992500",
    "end": "2997089"
  },
  {
    "text": "yep it's definitely not using our leg yeah",
    "start": "3000830",
    "end": "3009400"
  },
  {
    "text": "yep well in this case like I said akka is",
    "start": "3016150",
    "end": "3023990"
  },
  {
    "text": "really manual versus auto if you the functionality for you to do it is there so it's more of an infrastructure tool",
    "start": "3023990",
    "end": "3030940"
  },
  {
    "text": "rather than giving you something that does everything because that there's",
    "start": "3030940",
    "end": "3036130"
  },
  {
    "text": "depent I would have to say that akka is more of a true to what",
    "start": "3036130",
    "end": "3044920"
  },
  {
    "text": "it's a really true implementation of the actor model but they don't really try to hide anything and in that sense if",
    "start": "3044920",
    "end": "3051320"
  },
  {
    "text": "you're familiar with Kathy you're very confident about how you're going to use",
    "start": "3051320",
    "end": "3057770"
  },
  {
    "text": "it then you just use what you need and you don't just don't touch the ones you don't need",
    "start": "3057770",
    "end": "3063190"
  },
  {
    "text": "and so to answer your question there is no car kind of duplication in our case what we had to do was we did have to",
    "start": "3063190",
    "end": "3070580"
  },
  {
    "text": "create an actor that would just push to two different places one would save to elasticsearch one would say to s3 but",
    "start": "3070580",
    "end": "3077570"
  },
  {
    "text": "that was pretty simple to do and we controlled all the instancing we also determined how many actors we'd spin up",
    "start": "3077570",
    "end": "3083960"
  },
  {
    "text": "so overall it's it's just that fine grain of control that we needed and in this case you can't really do a grid and",
    "start": "3083960",
    "end": "3090620"
  },
  {
    "text": "say Orleans it just doesn't make any sense because that's not really what they want to do",
    "start": "3090620",
    "end": "3097210"
  },
  {
    "text": "well it's I guess it really depends on which philosophy would fit your",
    "start": "3122470",
    "end": "3128800"
  },
  {
    "text": "requirements they're very different so with Orleans",
    "start": "3128800",
    "end": "3134410"
  },
  {
    "text": "the Udo controlling instancing you don't control the lifetime of an actor you",
    "start": "3134410",
    "end": "3140890"
  },
  {
    "text": "just assume that somewhere in that black box is an actor instance that you could maintain and you know send messages to",
    "start": "3140890",
    "end": "3148750"
  },
  {
    "text": "and they'll guarantee that it's there but if it goes idle there's a good chance it might get killed off and you",
    "start": "3148750",
    "end": "3154569"
  },
  {
    "text": "have no control of it it's kind of like garbage collection you",
    "start": "3154569",
    "end": "3159690"
  },
  {
    "text": "well they have this they have this terminology where they say they hydrate or dehydrate actors where you put in",
    "start": "3160380",
    "end": "3167650"
  },
  {
    "text": "plugins and what they would do is deserialize itself and then once it's activated then it would try to rehydrate",
    "start": "3167650",
    "end": "3173859"
  },
  {
    "text": "itself and handle the message as if it was always always there they call that virtual actors in akka if you go with clustering the",
    "start": "3173859",
    "end": "3183550"
  },
  {
    "text": "trade-off is that they don't have any cluster metrics so you wouldn't be able to directly",
    "start": "3183550",
    "end": "3188819"
  },
  {
    "text": "distribute the load across five like say five nodes whereas in Orleans they are",
    "start": "3188819",
    "end": "3195670"
  },
  {
    "text": "very smart about making sure that if you get you've got five nodes it'll equally distribute the work so it depends if you",
    "start": "3195670",
    "end": "3202359"
  },
  {
    "text": "want to do like transparent like distributed c-sharp without having to worry about the low-level details then",
    "start": "3202359",
    "end": "3208210"
  },
  {
    "text": "that would work in Orleans but if you wanted to make sure that you could scale up without any hiccups then you really",
    "start": "3208210",
    "end": "3215560"
  },
  {
    "text": "have to go manual and that's what we did anything else",
    "start": "3215560",
    "end": "3222900"
  },
  {
    "text": "well that's an easy crowd thanks for coming [Applause]",
    "start": "3222900",
    "end": "3231340"
  },
  {
    "text": "this in for Manning you can check this one out because it is an intro to a",
    "start": "3233339",
    "end": "3239829"
  },
  {
    "text": "Canet and it's coming up pretty soon if not out already so just have a look at",
    "start": "3239829",
    "end": "3246970"
  },
  {
    "text": "this one you could also check the references here and as I mentioned before the domain DevOps guys who do all",
    "start": "3246970",
    "end": "3253119"
  },
  {
    "text": "the magical stuff they're going to be doing quick in a very in-depth session",
    "start": "3253119",
    "end": "3258429"
  },
  {
    "text": "on how they actually do it and how they move domain into like fully full auto",
    "start": "3258429",
    "end": "3264189"
  },
  {
    "text": "devops and octopus so please please check that out thanks again",
    "start": "3264189",
    "end": "3271049"
  }
]