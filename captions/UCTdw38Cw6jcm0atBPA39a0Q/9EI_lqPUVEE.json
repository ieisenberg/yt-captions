[
  {
    "start": "0",
    "end": "49000"
  },
  {
    "text": "so hi everyone can you hear me so hi thanks so much for coming to my",
    "start": "5420",
    "end": "10800"
  },
  {
    "text": "talk um so as you can see from the screen I'm the developer advocate in data science",
    "start": "10800",
    "end": "15960"
  },
  {
    "text": "at jetbrains and I worked as a data scientist for quite some time but prior",
    "start": "15960",
    "end": "21539"
  },
  {
    "text": "to that I was actually an academic researcher like many data scientists and",
    "start": "21539",
    "end": "27000"
  },
  {
    "text": "my background is actually psychology and biostatistics so one of the first lessons that you",
    "start": "27000",
    "end": "32460"
  },
  {
    "text": "learn as a researcher is the classic garbage in garbage out and unfortunately you usually learn this the hard way by",
    "start": "32460",
    "end": "40260"
  },
  {
    "text": "getting really excited about some result that you found or some model that you've built that ends up being complete",
    "start": "40260",
    "end": "45480"
  },
  {
    "text": "garbage it's an artifact of something that's gone wrong with your data so in contrast the introduction that",
    "start": "45480",
    "end": "52140"
  },
  {
    "start": "49000",
    "end": "49000"
  },
  {
    "text": "most people get to data in the context of machine learning are these beautiful little toy data sets and most people who",
    "start": "52140",
    "end": "59160"
  },
  {
    "text": "have done any machine learning whatsoever would be familiar with this data set the iris data set where you're",
    "start": "59160",
    "end": "65100"
  },
  {
    "text": "asked to classify flowers into one of three Iris species so if we have a look at the iris data",
    "start": "65100",
    "end": "72240"
  },
  {
    "text": "set you can see how beautifully curated it is and how well each of those three flower species separate out in fact it's",
    "start": "72240",
    "end": "80640"
  },
  {
    "text": "so neat that you don't actually need to build a model to classify which flower belongs to which group you can actually",
    "start": "80640",
    "end": "86939"
  },
  {
    "text": "just create some rules based on eyeballing that graph let's contrast this with some real world",
    "start": "86939",
    "end": "92880"
  },
  {
    "text": "data so I'm sure from looking at it you can see a lot of issues and any model",
    "start": "92880",
    "end": "98040"
  },
  {
    "text": "that's going to try and use this data is going to struggle to basically work out which points belong to which group and",
    "start": "98040",
    "end": "104820"
  },
  {
    "text": "this is the core of this talk we need to understand how to spot and remedy issues in real world data in order to give our",
    "start": "104820",
    "end": "111899"
  },
  {
    "text": "models a Fighting Chance at actually working so before continuing I'm going to be",
    "start": "111899",
    "end": "118259"
  },
  {
    "text": "using a few Concepts pretty heavily in this presentation I just want to make sure everyone's on the same page so",
    "start": "118259",
    "end": "123479"
  },
  {
    "text": "we're just going to start with some basic definitions so the first thing that I want to explain is how we refer to different",
    "start": "123479",
    "end": "129899"
  },
  {
    "text": "data fields in the context of machine learning so the fields in our data can be broken up into two groups the first",
    "start": "129899",
    "end": "136260"
  },
  {
    "text": "is the target which is the thing that we're just trying to predict in the case of the iris data set what type of Iris",
    "start": "136260",
    "end": "141959"
  },
  {
    "text": "it is and the second are the features this is the information about our Target that we",
    "start": "141959",
    "end": "147840"
  },
  {
    "text": "use to make those predictions in our model the next thing I want to explain are the",
    "start": "147840",
    "end": "153959"
  },
  {
    "start": "151000",
    "end": "151000"
  },
  {
    "text": "basic model types that we'll be talking about in this presentation so the first are classification models this is where",
    "start": "153959",
    "end": "160800"
  },
  {
    "text": "we're trying to predict some sort of categorical variable or one of a number of classes or groups",
    "start": "160800",
    "end": "167580"
  },
  {
    "text": "the second are regression models where the goal is to predict some sort of continuous variable say price",
    "start": "167580",
    "end": "175019"
  },
  {
    "text": "and then finally I'm going to talk about the toolkit that we're going to be using to diagnose issues with our data so the",
    "start": "175019",
    "end": "180959"
  },
  {
    "text": "first is summary statistics nothing fancy you'd be familiar with these things like Min marks mean deviations",
    "start": "180959",
    "end": "188400"
  },
  {
    "text": "the second is a very powerful type of table called across tab this is where",
    "start": "188400",
    "end": "193440"
  },
  {
    "text": "you take two categorical variables and you count the number of observations where their levels interact so you can",
    "start": "193440",
    "end": "199200"
  },
  {
    "text": "see in this cross tab where feature one and feature two both equal zero they're around eight million observations",
    "start": "199200",
    "end": "205019"
  },
  {
    "text": "nothing too fancy here either and then finally we have graphs graphs",
    "start": "205019",
    "end": "211140"
  },
  {
    "text": "are of course one of the most important tools for making sense of your data working out witchcraft to use in which",
    "start": "211140",
    "end": "218280"
  },
  {
    "text": "context is a bit of an art in and of itself it would make it a whole nother talk so we're not going to go into it in too much detail but what I'll be doing",
    "start": "218280",
    "end": "225540"
  },
  {
    "text": "is showing a few specific ways you can use graphs in order to find issues in your data and I hope it sets you up with",
    "start": "225540",
    "end": "231540"
  },
  {
    "text": "the mindset that when something's acting a bit weird with your data if you visualize it there's a good chance that",
    "start": "231540",
    "end": "237000"
  },
  {
    "text": "whatever's going wrong will just pop out and to make this presentation a little",
    "start": "237000",
    "end": "242220"
  },
  {
    "text": "bit more Dynamic rather than just turning it into a research methodology textbook we're going to play with a toy",
    "start": "242220",
    "end": "247980"
  },
  {
    "text": "problem and see how it can be affected by different issues with the data and this is actually based on a problem that",
    "start": "247980",
    "end": "253379"
  },
  {
    "text": "I worked on in my last job it's working as a lead data scientist at an ad tech company and actually some of the",
    "start": "253379",
    "end": "258900"
  },
  {
    "text": "examples that I've used are things that happen to us so it goes to show you know it doesn't matter how much experience",
    "start": "258900",
    "end": "264000"
  },
  {
    "text": "you have there are pitfalls that can always trip you up so our toy problem",
    "start": "264000",
    "end": "270479"
  },
  {
    "text": "let's say a mobile app wants to sell their ad space so yes I used to work in",
    "start": "270479",
    "end": "275580"
  },
  {
    "text": "selling that ad space too I am sorry I've moved on so they're working with your company this",
    "start": "275580",
    "end": "281880"
  },
  {
    "text": "ad tech company and your company runs real-time auctions in order to sell",
    "start": "281880",
    "end": "287580"
  },
  {
    "text": "those ads to advertisers now this sounds pretty simple but we're not just talking about one ad space of",
    "start": "287580",
    "end": "294300"
  },
  {
    "text": "course we're talking about more than two million per second so because of infrastructure costs it's",
    "start": "294300",
    "end": "301080"
  },
  {
    "text": "not profitable to send out every single ad space to every single Advertiser so what else sales team have been doing",
    "start": "301080",
    "end": "307500"
  },
  {
    "text": "is manually blocking the ones that don't really seem profitable the issue is that there's so many of them and it's always",
    "start": "307500",
    "end": "313560"
  },
  {
    "text": "changing so they can't keep up this is where we come in we've been",
    "start": "313560",
    "end": "319080"
  },
  {
    "text": "asked to try and build a model to automate this process so we decide to try and build two models a model to",
    "start": "319080",
    "end": "325320"
  },
  {
    "text": "categorize whether the ad will sell or not so a classification model and also to try and predict the price if it does",
    "start": "325320",
    "end": "331919"
  },
  {
    "text": "sell so a regression model so to get started with our ad space",
    "start": "331919",
    "end": "337320"
  },
  {
    "text": "models we obviously need to grab some data and this is where all of the problems begin",
    "start": "337320",
    "end": "343979"
  },
  {
    "text": "so when building a machine learning model it's really rare that you would design and collect a data set specifically for",
    "start": "343979",
    "end": "350759"
  },
  {
    "text": "your project rather you'll need to use data that was gathered for some other purpose and this",
    "start": "350759",
    "end": "356460"
  },
  {
    "text": "brings a number of problems such as you don't really understand where the measurement came from you don't actually",
    "start": "356460",
    "end": "362880"
  },
  {
    "text": "understand what the field is measuring or you'll feel is a proxy for something that you're actually interested in",
    "start": "362880",
    "end": "369900"
  },
  {
    "text": "so one of the first mistakes you can make when working with data is making unfounded assumptions about what your",
    "start": "369900",
    "end": "375840"
  },
  {
    "start": "370000",
    "end": "370000"
  },
  {
    "text": "data is measuring and to illustrate this I want to tell a story that I read recently in a paper",
    "start": "375840",
    "end": "381720"
  },
  {
    "text": "so a group of researchers decided that they would create a project called criminality from faces and they claim",
    "start": "381720",
    "end": "388919"
  },
  {
    "text": "that they can identify based on a person's face whether they're a criminal or not",
    "start": "388919",
    "end": "394199"
  },
  {
    "text": "now let's have a look at the data they used to train their model here on the left we have two faces in",
    "start": "394199",
    "end": "401039"
  },
  {
    "text": "the non-criminal group and here on the right we have a face from the criminal group",
    "start": "401039",
    "end": "406440"
  },
  {
    "text": "do you notice anything different about these faces well you might have guessed they managed to train a model which",
    "start": "406440",
    "end": "412680"
  },
  {
    "text": "could classify whether a picture was a mug shot not whether a person was a criminal so this is of course a",
    "start": "412680",
    "end": "418500"
  },
  {
    "text": "fundamental mistake one of the first lessons you learn in research methodology is check for confounders",
    "start": "418500",
    "end": "424620"
  },
  {
    "text": "like this because they can really mess up the relationship between your features and your targets",
    "start": "424620",
    "end": "430979"
  },
  {
    "text": "so in our data one of the first things that we need to check is whether our Fields mean what we think they do",
    "start": "430979",
    "end": "436860"
  },
  {
    "start": "431000",
    "end": "431000"
  },
  {
    "text": "in the case that we don't have a data dictionary or a nice person at the company to explain this we need to do a",
    "start": "436860",
    "end": "442740"
  },
  {
    "text": "bit of Investigation for ourselves so let's say we have this case we've dug up two features that we think could be",
    "start": "442740",
    "end": "448740"
  },
  {
    "text": "useful for our model country and region so we're looking at the names and we",
    "start": "448740",
    "end": "453780"
  },
  {
    "text": "don't really know what region means based on the field it's just a bunch of numbers but we think maybe it could be",
    "start": "453780",
    "end": "459660"
  },
  {
    "text": "broader Geographic classifications like North America Asia Europe Etc",
    "start": "459660",
    "end": "465000"
  },
  {
    "text": "so if that were true what we'd expect is our crosstab to look something like what I'm showing on the screen so we'd expect",
    "start": "465000",
    "end": "471660"
  },
  {
    "text": "you know USA and Mexico to be in one region we would expect the Asian",
    "start": "471660",
    "end": "476940"
  },
  {
    "text": "countries to be in another region the European countries in another region and so forth",
    "start": "476940",
    "end": "482699"
  },
  {
    "text": "however our actual cross tab looks rather different in fact there's no clear relationship between the two",
    "start": "482699",
    "end": "488460"
  },
  {
    "text": "Fields I don't know if you can see one actually I set up this example there is no relationship so",
    "start": "488460",
    "end": "494099"
  },
  {
    "text": "um this means we don't actually know what region means and in that case it's actually safer not to use it because we",
    "start": "494099",
    "end": "500340"
  },
  {
    "text": "don't really know how to interpret what those different levels mean so this emphasizes the importance of",
    "start": "500340",
    "end": "506759"
  },
  {
    "text": "checking your assumptions about what your Fields mean no matter how straightforward those fields seem from their name",
    "start": "506759",
    "end": "512820"
  },
  {
    "text": "and what it means is you know you should check things like do similar Fields",
    "start": "512820",
    "end": "518399"
  },
  {
    "text": "relate to each other do dissimilar Fields not relate to each other and these two ideas are called convergent",
    "start": "518399",
    "end": "525060"
  },
  {
    "text": "and Divergent validity respectively we also need to check whether any of our",
    "start": "525060",
    "end": "530820"
  },
  {
    "text": "features might be measuring the same thing as the target so if you look here at the relationship",
    "start": "530820",
    "end": "535860"
  },
  {
    "text": "between one of our potential features valid ad category and one of our targets",
    "start": "535860",
    "end": "541019"
  },
  {
    "text": "whether the ads sold you can see that these two Fields have the same value 99",
    "start": "541019",
    "end": "546660"
  },
  {
    "text": "of the time so if you look at when they both equal zero 81 of the observations",
    "start": "546660",
    "end": "553019"
  },
  {
    "text": "are there when you look at when they both equal one another 18 of the observations are there so it's only in",
    "start": "553019",
    "end": "560040"
  },
  {
    "text": "one of the boxes one percent of the time where those two Fields do not have the",
    "start": "560040",
    "end": "565140"
  },
  {
    "text": "same value and this should really set off alarm Bells because it indicates they're basically measuring the same",
    "start": "565140",
    "end": "570899"
  },
  {
    "text": "thing so when we do a bit of digging we find out that this field is only populated after the ad is sold and this means it",
    "start": "570899",
    "end": "578160"
  },
  {
    "text": "contains information the model will not have at the time of prediction so this is a particularly nasty way in",
    "start": "578160",
    "end": "584760"
  },
  {
    "text": "which a field may not represent what we assume it does and this is called Data leakage we're going to come back to this",
    "start": "584760",
    "end": "590760"
  },
  {
    "text": "topic later because it's very very important in machine learning data preparation",
    "start": "590760",
    "end": "596760"
  },
  {
    "text": "relationships between Fields can sometimes be complex so if you have a look at this scatter plot between a",
    "start": "596760",
    "end": "603240"
  },
  {
    "text": "feature and a Target it looks like a random mass of points it doesn't seem to be a meaningful pattern",
    "start": "603240",
    "end": "609959"
  },
  {
    "text": "and when we fit a line the statistics agree with us there's no relationship here",
    "start": "609959",
    "end": "615480"
  },
  {
    "text": "however what happens when we group this data by a second feature what we can see is that in class 0",
    "start": "615480",
    "end": "621720"
  },
  {
    "text": "there's a strong negative relationship between feature one and the target",
    "start": "621720",
    "end": "626760"
  },
  {
    "text": "and in class one there's a strong positive relationship between them this is called an interaction with a",
    "start": "626760",
    "end": "633480"
  },
  {
    "text": "relationship between a feature and a Target only emerges in the presence of a second feature",
    "start": "633480",
    "end": "640320"
  },
  {
    "text": "due to these interactions between features you can't necessarily tell which features are going to be useful",
    "start": "640320",
    "end": "645779"
  },
  {
    "text": "for your model just by looking at their one-to-one relationship with the target so what are you going to do are you",
    "start": "645779",
    "end": "651420"
  },
  {
    "text": "going to look at every single combination of every feature see how it relates to the Target and then work it",
    "start": "651420",
    "end": "656459"
  },
  {
    "text": "out that way like that sounds really tedious and data scientists are all about automation so luckily you don't",
    "start": "656459",
    "end": "662579"
  },
  {
    "text": "need to do that one really straightforward way of assessing whether a feature is going to be useful in your model is called an",
    "start": "662579",
    "end": "668880"
  },
  {
    "text": "orbit X approach where you add every feature but one to your model run the model get whatever your model metrics",
    "start": "668880",
    "end": "676079"
  },
  {
    "text": "going to be see how it changes and then repeat that for every possible feature so you can see here if we exclude the",
    "start": "676079",
    "end": "683700"
  },
  {
    "text": "operating system the OS then add request from the model that has a much bigger impact on the model accuracy than",
    "start": "683700",
    "end": "690600"
  },
  {
    "text": "repeating uh sorry removing something like whether the ad request is coming from Mexico so that means that OS is one",
    "start": "690600",
    "end": "697440"
  },
  {
    "text": "of our most important features this is called multivariate feature selection it's a really big and complex",
    "start": "697440",
    "end": "703800"
  },
  {
    "text": "topic all but orbit X is just one approach but what I would suggest is or what I'd",
    "start": "703800",
    "end": "709800"
  },
  {
    "text": "recommend is take the time to go through and see what methods offered by your framework or language of choice because",
    "start": "709800",
    "end": "716100"
  },
  {
    "text": "it's really important in terms of setting up your models with the right features",
    "start": "716100",
    "end": "721260"
  },
  {
    "text": "so now that we've discussed problems with what your data is measuring let's have a look at some ways your data can",
    "start": "721260",
    "end": "727079"
  },
  {
    "text": "trick you into thinking your model is performing better than it actually is so let's start with a fundamental",
    "start": "727079",
    "end": "733560"
  },
  {
    "start": "732000",
    "end": "732000"
  },
  {
    "text": "question how do we assess how a model performs well a naive approach would be take your",
    "start": "733560",
    "end": "740279"
  },
  {
    "text": "whole data set train a bunch of models and then pick the very best one the problem with this approach is that",
    "start": "740279",
    "end": "746700"
  },
  {
    "text": "every data set has its own idiosyncrasies and models are really good at learning patterns they don't",
    "start": "746700",
    "end": "752579"
  },
  {
    "text": "just learn the broad patterns between features and targets they also pick up",
    "start": "752579",
    "end": "757800"
  },
  {
    "text": "on all these little patterns in data sets if we let them this is a phenomenon known as",
    "start": "757800",
    "end": "763620"
  },
  {
    "text": "overfitting and a model can learn or this is where a model learns patterns in the training data so well can no longer",
    "start": "763620",
    "end": "771120"
  },
  {
    "text": "generalize to new data so let's have a look at a really extreme example so let's say we've fitted a model so",
    "start": "771120",
    "end": "778500"
  },
  {
    "text": "well that it describes the training data with zero error however when we try to fit this model to",
    "start": "778500",
    "end": "784800"
  },
  {
    "text": "a new set of data Maybe we apply it in the real world we can see that it actually misses most",
    "start": "784800",
    "end": "790800"
  },
  {
    "text": "of the points in the fresh data set meaning that the error is going to be much higher in the real world than in",
    "start": "790800",
    "end": "796620"
  },
  {
    "text": "the training data this is where the famous train test",
    "start": "796620",
    "end": "801959"
  },
  {
    "start": "800000",
    "end": "800000"
  },
  {
    "text": "split comes in so you've probably heard of this if you've done any sort of machine learning at all",
    "start": "801959",
    "end": "807420"
  },
  {
    "text": "so what we want to do is set aside a portion of the data that the model hasn't seen while training and we then",
    "start": "807420",
    "end": "813300"
  },
  {
    "text": "use this data set as a proxy to assess how well the model is going to perform when we put it into production or put it",
    "start": "813300",
    "end": "819839"
  },
  {
    "text": "out into into the real world so the expectation is that model metrics should be roughly the same in the",
    "start": "819839",
    "end": "825660"
  },
  {
    "text": "training data and on our test set so let's look at an example let's say",
    "start": "825660",
    "end": "831300"
  },
  {
    "text": "that we train uh train test split and we create a model using the train split",
    "start": "831300",
    "end": "836760"
  },
  {
    "text": "so we achieve a very good accuracy on the train set however when we apply this",
    "start": "836760",
    "end": "841860"
  },
  {
    "text": "model to the test set the accuracy drops to almost random so we can see how the test set has allowed us to spot that",
    "start": "841860",
    "end": "848519"
  },
  {
    "text": "overfitting to the train set so let's look at another scenario",
    "start": "848519",
    "end": "854100"
  },
  {
    "text": "this time we created our train and our test sets and everything seems fine our model is performing super well we're",
    "start": "854100",
    "end": "861000"
  },
  {
    "text": "able to get similar results on the train and the test sets and when we um so and then let's go ahead and deploy that",
    "start": "861000",
    "end": "867779"
  },
  {
    "text": "model into production this time our accuracy is really bad",
    "start": "867779",
    "end": "873120"
  },
  {
    "text": "what happened here the issue is is that information from the test set has leaked into the train",
    "start": "873120",
    "end": "879959"
  },
  {
    "text": "set meaning that the model's performance is being artificially inflated this is",
    "start": "879959",
    "end": "885240"
  },
  {
    "text": "another form of that data leakage that I mentioned in the previous section and creating a trained test split that",
    "start": "885240",
    "end": "891060"
  },
  {
    "text": "doesn't involve data leakage or some other issue is actually quite tricky so",
    "start": "891060",
    "end": "896100"
  },
  {
    "text": "let's have a look at a few different ways that a poorly designed trains test split can impact your model performance",
    "start": "896100",
    "end": "903420"
  },
  {
    "text": "so let's go back to our ad space data let's say that we created our train test split by just leaving the last 30 of",
    "start": "903420",
    "end": "911100"
  },
  {
    "text": "auctions aside for the test set and let's compare the summary statistics between the two",
    "start": "911100",
    "end": "917040"
  },
  {
    "text": "the first thing that we notice is that option ID which should be unique to every single option somehow has less",
    "start": "917040",
    "end": "924060"
  },
  {
    "text": "unique values than total values in both the train and the test set when we dig into this we realize they're",
    "start": "924060",
    "end": "931019"
  },
  {
    "text": "actually duplicates in the auction data so what this means is that the test data set likely contains some of the exact",
    "start": "931019",
    "end": "937260"
  },
  {
    "text": "same observations that we trained on and this allows the model to overfit and then still achieve good performance on",
    "start": "937260",
    "end": "944040"
  },
  {
    "text": "the test set by just fitting to the observations it's already seen",
    "start": "944040",
    "end": "949800"
  },
  {
    "text": "the next problem is we have a different number of unique fields in the country field or unique values",
    "start": "949800",
    "end": "956040"
  },
  {
    "text": "so we can see that the test set contains all eight levels of this feature but the",
    "start": "956040",
    "end": "961079"
  },
  {
    "text": "train set only contains six so this means that the model is only going to be able to learn patterns for",
    "start": "961079",
    "end": "967139"
  },
  {
    "text": "the six countries that it has access to meaning that it can't make accurate predictions for the missing two",
    "start": "967139",
    "end": "972660"
  },
  {
    "text": "countries when it comes to predicting on fresh data and finally you can see that the mean",
    "start": "972660",
    "end": "979139"
  },
  {
    "text": "and the mean of ads sold and the ad Price is very different in both the train and the test set",
    "start": "979139",
    "end": "985560"
  },
  {
    "text": "so from the looks of it the auctions in the train set seem to be less successful and perhaps have a lower value than the",
    "start": "985560",
    "end": "991920"
  },
  {
    "text": "auctions in the test serve so this means that the model may not be able to predict the performance of higher value",
    "start": "991920",
    "end": "997800"
  },
  {
    "text": "successful auctions when it sees them in Productions so this illustrates of course the need",
    "start": "997800",
    "end": "1003199"
  },
  {
    "text": "to make sure that your trained test split is done very carefully it needs to",
    "start": "1003199",
    "end": "1008300"
  },
  {
    "text": "yield equally representative samples from your original data set and it's",
    "start": "1008300",
    "end": "1014000"
  },
  {
    "text": "safest to use methods that are specifically designed to do this make sure that you're representing both the",
    "start": "1014000",
    "end": "1019040"
  },
  {
    "text": "features and the target equally in each of those data sets so an example if you're using python is the train test",
    "start": "1019040",
    "end": "1024980"
  },
  {
    "text": "split method from scikitlearn so let's say that we've solved our issue",
    "start": "1024980",
    "end": "1030860"
  },
  {
    "text": "with having a clean train test split with no data leakage I'm actually just wondering is it possible to get the",
    "start": "1030860",
    "end": "1036860"
  },
  {
    "text": "doors closed there's a little bit of noise coming in and it's quite distracting",
    "start": "1036860",
    "end": "1042159"
  },
  {
    "text": "oh there's no doors okay I'll power on okay so let's say we've",
    "start": "1043760",
    "end": "1050360"
  },
  {
    "text": "solved our problem with having a clean Tran test split we don't have any data leakage so we go ahead and we train a",
    "start": "1050360",
    "end": "1057620"
  },
  {
    "text": "variety of models and we assess each on the test set as we go so we come up with a model that seems to",
    "start": "1057620",
    "end": "1063620"
  },
  {
    "text": "perform really well on both the train and the test set so we go ahead and productionize it this time we're feeling",
    "start": "1063620",
    "end": "1069320"
  },
  {
    "text": "good however we yet again have an issue with a performance on the fresh data is",
    "start": "1069320",
    "end": "1075559"
  },
  {
    "text": "lower than either the train or the test set what happened this time so the issue is that by using the test",
    "start": "1075559",
    "end": "1082340"
  },
  {
    "text": "set multiple times in order to test out our models we're actually now",
    "start": "1082340",
    "end": "1087380"
  },
  {
    "text": "overfitting our model to both the train and the test sets models are impossible",
    "start": "1087380",
    "end": "1092900"
  },
  {
    "text": "so we actually should have only used the test set once using it multiple times is",
    "start": "1092900",
    "end": "1097940"
  },
  {
    "text": "an improper use of it so if that's the case how do we make sure that we're peaking the right model",
    "start": "1097940",
    "end": "1103580"
  },
  {
    "text": "and we haven't just got something that overfits to our train set",
    "start": "1103580",
    "end": "1108759"
  },
  {
    "text": "the first approach is quite simple what we do is we take our whole data set and",
    "start": "1108980",
    "end": "1114080"
  },
  {
    "text": "then we split it into train and test we then create a third split from the train set and we call this the",
    "start": "1114080",
    "end": "1120500"
  },
  {
    "text": "validation set what we can then do is we train multiple models and our train set and we use that",
    "start": "1120500",
    "end": "1127220"
  },
  {
    "text": "validation data in order to assess its true performance in the real world once we've come up with that model that",
    "start": "1127220",
    "end": "1133700"
  },
  {
    "text": "we think performs the best on the validation set we then apply it to our pristine clean test set",
    "start": "1133700",
    "end": "1141400"
  },
  {
    "text": "however you might have kind of thought to yourself it actually is going to require quite a lot of data in order to",
    "start": "1141500",
    "end": "1147020"
  },
  {
    "text": "create three full data sets that are going to be representative what do we do if we don't have that in",
    "start": "1147020",
    "end": "1154340"
  },
  {
    "text": "this case we can use a method called k-fold cross-validation how do we do this so in this case we",
    "start": "1154340",
    "end": "1161720"
  },
  {
    "text": "take the whole data set again and again we split it into train and test sets",
    "start": "1161720",
    "end": "1167539"
  },
  {
    "text": "however this time instead of partitioning off one validation set we",
    "start": "1167539",
    "end": "1172760"
  },
  {
    "text": "now split our train set into K different fault hence the name of the technique",
    "start": "1172760",
    "end": "1178340"
  },
  {
    "text": "we then set aside one of these folds as our validation set and we train on the",
    "start": "1178340",
    "end": "1183799"
  },
  {
    "text": "other two so we train the model on Folds One and Two and we test it on fold three",
    "start": "1183799",
    "end": "1189799"
  },
  {
    "text": "we then repeat that exercise training on one fold that's left out each time",
    "start": "1189799",
    "end": "1196640"
  },
  {
    "text": "so what we end up with for every single model is K different performance metrics",
    "start": "1196640",
    "end": "1202520"
  },
  {
    "text": "because we've trained that method that model K times so what do we do with that we simply",
    "start": "1202520",
    "end": "1208160"
  },
  {
    "text": "average it and that gives us our validation estimate for how that model is going to perform in the real world",
    "start": "1208160",
    "end": "1215299"
  },
  {
    "text": "once we've gone through all of the different models that we want to train we then can apply that to our pristine",
    "start": "1215299",
    "end": "1221179"
  },
  {
    "text": "test set again again hopefully giving us a good indication of how it's going to perform in the real world",
    "start": "1221179",
    "end": "1228700"
  },
  {
    "text": "so now we've successfully dodged all of our issues with assessing model performance we now have an additional",
    "start": "1231140",
    "end": "1236780"
  },
  {
    "text": "set of problems this is a general collection of issues that model that data can throw at us in terms of being",
    "start": "1236780",
    "end": "1243620"
  },
  {
    "text": "dirty or having issues with the data quality",
    "start": "1243620",
    "end": "1248660"
  },
  {
    "text": "so unsurprisingly the first thing to look for when checking for dirty data is",
    "start": "1248660",
    "end": "1253700"
  },
  {
    "text": "just checking whether all of the values of your variables seem plausible so let's say we look at the summary",
    "start": "1253700",
    "end": "1259760"
  },
  {
    "text": "statistics for our app downloads feature and we see that the minimum number is a negative number",
    "start": "1259760",
    "end": "1265640"
  },
  {
    "text": "how is that possible we're dealing with count values here well maybe it is plausible sometimes",
    "start": "1265640",
    "end": "1271580"
  },
  {
    "text": "negative numbers are a stand-in for missing values sometimes there's just something wrong",
    "start": "1271580",
    "end": "1277640"
  },
  {
    "text": "with the Upstream data so we're going to have to dig into that and try and work out what's happening",
    "start": "1277640",
    "end": "1283280"
  },
  {
    "text": "similarly let's look at the proportions for our country variable and what we can",
    "start": "1283280",
    "end": "1288740"
  },
  {
    "text": "see is we have a really high number of auctions from Iceland a really small country and we're really",
    "start": "1288740",
    "end": "1294440"
  },
  {
    "text": "not doing much business there so this also seems implausible again perhaps it's an issue with how the data's been",
    "start": "1294440",
    "end": "1301280"
  },
  {
    "text": "written maybe it's an issue with our sample which we'll get into in the next section as you can kind of see checking for",
    "start": "1301280",
    "end": "1307880"
  },
  {
    "text": "implausible values and solving for them there's no real silver bullet you have to kind of dig around and actually check",
    "start": "1307880",
    "end": "1313940"
  },
  {
    "text": "what the cause of these implausible or weird values can be",
    "start": "1313940",
    "end": "1319700"
  },
  {
    "start": "1319000",
    "end": "1319000"
  },
  {
    "text": "speaking of missing values this is of course another major issue that can pollute your data so certain algorithms",
    "start": "1319700",
    "end": "1325940"
  },
  {
    "text": "won't actually work when you have missing data and missing data has the potential to cause massive issues for",
    "start": "1325940",
    "end": "1332179"
  },
  {
    "text": "your model's performance if it's missing in a certain way so of course we need to",
    "start": "1332179",
    "end": "1338299"
  },
  {
    "text": "check the ways that the data can be missing so there are three different patterns that we should be checking for",
    "start": "1338299",
    "end": "1344840"
  },
  {
    "text": "missing data can be missing completely at random this means that it's truly random there's no relationship between",
    "start": "1344840",
    "end": "1351080"
  },
  {
    "text": "the missingness and your features and your targets it can be missing at random this means",
    "start": "1351080",
    "end": "1357559"
  },
  {
    "text": "it has a relationship with the features but not the target and that it can be missing not at random",
    "start": "1357559",
    "end": "1363559"
  },
  {
    "text": "I don't know why it's called this it's a very awkward name but that means that the missingness has a relationship with",
    "start": "1363559",
    "end": "1368900"
  },
  {
    "text": "the target this is the most serious form of missingness because it can bias the predictions that your model makes",
    "start": "1368900",
    "end": "1376600"
  },
  {
    "text": "let's have a look at our ad space data when we have a look at this pattern of missingness we can see that the same",
    "start": "1376640",
    "end": "1383900"
  },
  {
    "text": "auctions have missing data for three different variables for operating system for app category and app downloads",
    "start": "1383900",
    "end": "1391400"
  },
  {
    "text": "we do some additional checks and we find out that that missingness fortunately doesn't have a relationship with our",
    "start": "1391400",
    "end": "1396919"
  },
  {
    "text": "Target that means that it's missing at random this gives us a starting point to",
    "start": "1396919",
    "end": "1403280"
  },
  {
    "text": "investigate why this data is missing and it's like a sense of how it's going to impact our model",
    "start": "1403280",
    "end": "1409340"
  },
  {
    "text": "when we dig in a little bit further we find out that these are options from apps that simply don't provide us",
    "start": "1409340",
    "end": "1415820"
  },
  {
    "text": "information about that field that means that when we build our model if we decide to use these features we're",
    "start": "1415820",
    "end": "1422120"
  },
  {
    "text": "simply not going to be able to use it to predict for those particular apps depending on the importance of the apps",
    "start": "1422120",
    "end": "1427940"
  },
  {
    "text": "and depending on how many auctions we have coming from those apps that's going to impact the coverage and basically the",
    "start": "1427940",
    "end": "1436059"
  },
  {
    "text": "effectiveness of our model so again like with implausible values",
    "start": "1436059",
    "end": "1441380"
  },
  {
    "text": "there's no one-size-fits-all solution here you might have to delete your missing observations you might be able",
    "start": "1441380",
    "end": "1446960"
  },
  {
    "text": "to replace them you might have to delete the fields or you might have to throw out your data altogether and not be able",
    "start": "1446960",
    "end": "1453140"
  },
  {
    "text": "to use it and this is again another one where I could do a whole nother talk on it and it would be another hour",
    "start": "1453140",
    "end": "1460640"
  },
  {
    "text": "so the next issue that can lead to dirty data is the presence of outliers so what are outliers they're simply extreme",
    "start": "1460640",
    "end": "1467480"
  },
  {
    "text": "values in your data and they can occur for a variety of reasons this can range from actual issues with the data",
    "start": "1467480",
    "end": "1473900"
  },
  {
    "text": "mistakes to real but rare extreme values that need to be accounted for when",
    "start": "1473900",
    "end": "1479299"
  },
  {
    "text": "building your model so let's have a look at the distribution of our ad Price Target we can see a",
    "start": "1479299",
    "end": "1485299"
  },
  {
    "text": "really weird pattern here on the left hand side we can see that most of our prices fall within a certain",
    "start": "1485299",
    "end": "1491059"
  },
  {
    "text": "range but we have this quite large group of auctions that seems to have a higher price than all of the others why is this",
    "start": "1491059",
    "end": "1499159"
  },
  {
    "text": "when we dig into it we find out this is a currency conversion issue so in our case we can simply correct for this and",
    "start": "1499159",
    "end": "1505460"
  },
  {
    "text": "get rid of those outliers but what if we couldn't why is leaving outliers in your data such a big issue",
    "start": "1505460",
    "end": "1512900"
  },
  {
    "text": "well let's consider what would happen if we tried to make a regression model which predicts ad Price using only the",
    "start": "1512900",
    "end": "1519500"
  },
  {
    "text": "app downloads as a feature so you can see that without the outliers the regression line fits quite neatly",
    "start": "1519500",
    "end": "1524840"
  },
  {
    "text": "through the observations and it gives us a pretty good model that describes the data pretty well",
    "start": "1524840",
    "end": "1530600"
  },
  {
    "text": "however what happens when we fail to fix those outliers well our aggression line does what regression lines do and it",
    "start": "1530600",
    "end": "1537559"
  },
  {
    "text": "skews quite aggressively in the direction of those outliers so this gives us a line that doesn't really describe our data properly anymore and",
    "start": "1537559",
    "end": "1545000"
  },
  {
    "text": "pretty much is going to give us a model that doesn't fit and generalize very well",
    "start": "1545000",
    "end": "1550700"
  },
  {
    "start": "1550000",
    "end": "1550000"
  },
  {
    "text": "so how do we detect these outliers the easiest way is just to visualize them so you can use box slots and as",
    "start": "1550700",
    "end": "1557539"
  },
  {
    "text": "I've already mentioned violin plots for continuous variables and there are statistical methods such",
    "start": "1557539",
    "end": "1563840"
  },
  {
    "text": "as the Cox or mahalobus distances so these are basically statistics to detect",
    "start": "1563840",
    "end": "1569240"
  },
  {
    "text": "whether your outliers exceed some sort of deviation from the bulk of your observations and then one final and very important",
    "start": "1569240",
    "end": "1576200"
  },
  {
    "text": "check is just build your model with and without the outliers so some techniques as we've already covered like regression",
    "start": "1576200",
    "end": "1582679"
  },
  {
    "text": "models are super super sensitive to outliers others like tree based methods are much more robust you can also apply",
    "start": "1582679",
    "end": "1590240"
  },
  {
    "text": "certain Transformations which reduce the impact of the outliers as well",
    "start": "1590240",
    "end": "1595640"
  },
  {
    "text": "so the next and most important issue next and very important issue to discuss",
    "start": "1595640",
    "end": "1600740"
  },
  {
    "text": "is data imbalance so data imbalance is when you have an uneven distribution in your categorical variables and it can",
    "start": "1600740",
    "end": "1607100"
  },
  {
    "text": "affect both targets and features so let's start with targets",
    "start": "1607100",
    "end": "1612500"
  },
  {
    "text": "in our case we have an uneven distribution in our add sold Target where 82 of the time an option is not",
    "start": "1612500",
    "end": "1620360"
  },
  {
    "text": "successful we cannot sell the ad so let's say we decide to fit a really",
    "start": "1620360",
    "end": "1625460"
  },
  {
    "text": "dumb model where we always predict that the ad doesn't sell but you're probably going to think this",
    "start": "1625460",
    "end": "1631700"
  },
  {
    "text": "is this is stupid this model's not going to perform very well because we're just always predicting the same outcome well",
    "start": "1631700",
    "end": "1637700"
  },
  {
    "text": "that stupid model gets us to 82 accuracy because 82 percent of the time it agrees",
    "start": "1637700",
    "end": "1644480"
  },
  {
    "text": "with the outcome that actually happened so you can probably see how this is an issue with machine learning models the",
    "start": "1644480",
    "end": "1650480"
  },
  {
    "text": "way that machine learning models work generally is that they try to reduce the error between what they observe and what",
    "start": "1650480",
    "end": "1656900"
  },
  {
    "text": "they predict when you have one really big class which dominates your targets it can just be",
    "start": "1656900",
    "end": "1664220"
  },
  {
    "text": "lazy and just guess the same outcome a lot of the time and actually reduce that",
    "start": "1664220",
    "end": "1669559"
  },
  {
    "text": "error a lot just by doing this lazy sort of guessing without actually kind of learning the relationship between your",
    "start": "1669559",
    "end": "1676159"
  },
  {
    "text": "features and your target so what can we do about it there are two broad approaches",
    "start": "1676159",
    "end": "1682520"
  },
  {
    "start": "1678000",
    "end": "1678000"
  },
  {
    "text": "the first is called under sampling this is basically a oops went too far",
    "start": "1682520",
    "end": "1689679"
  },
  {
    "text": "this is where you attempt to basically reduce the size of your larger target class by randomly removing rows so",
    "start": "1689679",
    "end": "1697580"
  },
  {
    "text": "pretty naive approach it doesn't work very well if the imbalance is too aggressive or your data are too small",
    "start": "1697580",
    "end": "1706100"
  },
  {
    "text": "the other approach as I already spoiled is called over sampling so this is where you attempt to create more data in your",
    "start": "1706100",
    "end": "1712640"
  },
  {
    "text": "smaller Target classes by some sort of method so the most naive way of doing it",
    "start": "1712640",
    "end": "1718039"
  },
  {
    "text": "is just to Simply copy rows this doesn't give the data any sale the model anything new to learn from but at least",
    "start": "1718039",
    "end": "1724159"
  },
  {
    "text": "takes away that cop out where it can just guess the majority class there are more sophisticated methods where you can",
    "start": "1724159",
    "end": "1730279"
  },
  {
    "text": "statistically generate more data using what you have already as a seed",
    "start": "1730279",
    "end": "1737020"
  },
  {
    "text": "imbalance classes can also be an issue with features so if we take a look at the distribution",
    "start": "1737360",
    "end": "1742880"
  },
  {
    "text": "of levels within the app category feature we can see that it's obviously really imbalanced so most of our",
    "start": "1742880",
    "end": "1749059"
  },
  {
    "text": "auctions are coming from gaming apps and we have these really small groups at the bottom which to do with Ceramics or",
    "start": "1749059",
    "end": "1754940"
  },
  {
    "text": "netting so this makes it really hard to work with",
    "start": "1754940",
    "end": "1761179"
  },
  {
    "text": "these sort of features because it doesn't really give the model much to learn from when it comes to these small",
    "start": "1761179",
    "end": "1767480"
  },
  {
    "text": "classes these kind of imbalances also compound if you have multiple features that are",
    "start": "1767480",
    "end": "1774200"
  },
  {
    "text": "categorical and really imbalanced so let's take a really extreme case and let's say that we have a combination of",
    "start": "1774200",
    "end": "1780679"
  },
  {
    "text": "all of the rarest levels of all of our categorical variables so let's say we",
    "start": "1780679",
    "end": "1786440"
  },
  {
    "text": "have options from Malta in Ceramics apps on Android devices",
    "start": "1786440",
    "end": "1792980"
  },
  {
    "text": "we have one auction and this is in a data set with hundreds of millions of observations",
    "start": "1792980",
    "end": "1800299"
  },
  {
    "start": "1800000",
    "end": "1800000"
  },
  {
    "text": "how does this affect our model this is related to a related issue called The",
    "start": "1800299",
    "end": "1805760"
  },
  {
    "text": "Curse of dimensionality it's a little bit of a dramatic name but you know there you go so this is when you have too many",
    "start": "1805760",
    "end": "1812000"
  },
  {
    "text": "features compared to observations in the worst cases you can actually have more features than observations",
    "start": "1812000",
    "end": "1818960"
  },
  {
    "text": "the reason why this is a problem is it because is because models tend to learn patterns when you only present one",
    "start": "1818960",
    "end": "1827299"
  },
  {
    "text": "example the model just memorizes what it sees so let's see what would happen with the",
    "start": "1827299",
    "end": "1833059"
  },
  {
    "text": "example that we just talked about the model will basically see that in our",
    "start": "1833059",
    "end": "1839360"
  },
  {
    "text": "very one training example the auction sold for 1.68 so what the",
    "start": "1839360",
    "end": "1846440"
  },
  {
    "text": "model learns is that next time it sees this combination of features it's going to make this exact prediction",
    "start": "1846440",
    "end": "1852440"
  },
  {
    "text": "it's just going to regurgitate what it saw in the training data however that's not the case and like",
    "start": "1852440",
    "end": "1858020"
  },
  {
    "text": "statistically it's not likely that that's going to happen again in our case the model the auction did",
    "start": "1858020",
    "end": "1863720"
  },
  {
    "text": "not sell and so the price is zero and so what that means is when you have too",
    "start": "1863720",
    "end": "1869659"
  },
  {
    "text": "many of these uniquely identified observations the model will start to",
    "start": "1869659",
    "end": "1875120"
  },
  {
    "text": "perform much worse because it's starting to overfit the training data just by memorizing training examples",
    "start": "1875120",
    "end": "1882799"
  },
  {
    "start": "1882000",
    "end": "1882000"
  },
  {
    "text": "so a very popular remedy for high dimensional data is feature engineering it's really just a fancy way of saying",
    "start": "1882799",
    "end": "1889279"
  },
  {
    "text": "that you need to clean up your features and turn them into something appropriate for your model so if we revisit this app category",
    "start": "1889279",
    "end": "1896120"
  },
  {
    "text": "variable what you can see is hey wait we've got a pattern here all of these",
    "start": "1896120",
    "end": "1901340"
  },
  {
    "text": "really tiny categories belong to a Hobbies group and if we look through all the other levels it actually turns out",
    "start": "1901340",
    "end": "1907640"
  },
  {
    "text": "we can see a similar pattern they seem to belong to subcategories so we just",
    "start": "1907640",
    "end": "1913399"
  },
  {
    "text": "use a bit of simple string manipulation and voila we go from something like 40 messy imbalance levels to something like",
    "start": "1913399",
    "end": "1921500"
  },
  {
    "text": "10 nice neat ones that are much more balanced so feature engineering another",
    "start": "1921500",
    "end": "1927200"
  },
  {
    "text": "very big area in machine learning this is just a really simple example and it can be really simple depending on your",
    "start": "1927200",
    "end": "1933200"
  },
  {
    "text": "input data but it can also be very complex and the methods are generally",
    "start": "1933200",
    "end": "1938419"
  },
  {
    "text": "quite idiosyncratic for the type of data that you're using so we've cleaned up our data and now we",
    "start": "1938419",
    "end": "1946460"
  },
  {
    "text": "need to check whether we've been trapped by the next pitfall our data doesn't represent our Target group",
    "start": "1946460",
    "end": "1953360"
  },
  {
    "start": "1953000",
    "end": "1953000"
  },
  {
    "text": "so to start to understand this we need to take a step back and explicitly Define what our Target group is",
    "start": "1953360",
    "end": "1960799"
  },
  {
    "text": "as much as we'd like them to models will never be able to represent everything and we therefore need to strictly Define",
    "start": "1960799",
    "end": "1967520"
  },
  {
    "text": "what we want our models to be able to do to do this we Define our model's",
    "start": "1967520",
    "end": "1973460"
  },
  {
    "text": "population of Interest this is the group that we want our model to apply to in the ad space example our population",
    "start": "1973460",
    "end": "1981020"
  },
  {
    "text": "is all real-time auctions for mobile ad space conducted by our company you can",
    "start": "1981020",
    "end": "1987860"
  },
  {
    "text": "see how careful and explicit that definition is it explicitly excludes things like web ad space things not sold",
    "start": "1987860",
    "end": "1995179"
  },
  {
    "text": "by our company non-auction ad space sales",
    "start": "1995179",
    "end": "2001140"
  },
  {
    "text": "it's usually not feasible or practical to actually get the whole population of Interest however in fact it's not even",
    "start": "2001419",
    "end": "2007539"
  },
  {
    "text": "really necessary most of the time and this is where samples come in samples are designed to be a microcosm of our",
    "start": "2007539",
    "end": "2014320"
  },
  {
    "text": "population of Interest they need to be representative however and the representativeness is the tricky",
    "start": "2014320",
    "end": "2021340"
  },
  {
    "text": "bit if you're not careful it's really easy to end up with a sample that somehow",
    "start": "2021340",
    "end": "2026679"
  },
  {
    "text": "misses important parts of the population or the distribution of those features deviate from your population of Interest",
    "start": "2026679",
    "end": "2035620"
  },
  {
    "text": "so these are called Data biases and they can have really problematic implications for how your model performs in the real",
    "start": "2035620",
    "end": "2041380"
  },
  {
    "text": "world so one example of this is facial recognition apps in developed by U.S",
    "start": "2041380",
    "end": "2046840"
  },
  {
    "text": "tech companies so you can see here in this diagram these algorithms generally",
    "start": "2046840",
    "end": "2052358"
  },
  {
    "text": "fail to perform worse on darker skinned females and this is kind of a massively",
    "start": "2052359",
    "end": "2059200"
  },
  {
    "text": "problematic thing because essentially these are algorithms that",
    "start": "2059200",
    "end": "2064599"
  },
  {
    "text": "are not just being used for things like recognition for opening a phone or opening a computer in some countries",
    "start": "2064599",
    "end": "2071260"
  },
  {
    "text": "like the US and the UK they're actually being used for law enforcement purposes so the companies have not really said",
    "start": "2071260",
    "end": "2078280"
  },
  {
    "text": "why this is the case but it seems likely that it's just easier to get hold of",
    "start": "2078280",
    "end": "2083679"
  },
  {
    "text": "images of white male faces when building the training sets and no one really",
    "start": "2083679",
    "end": "2088898"
  },
  {
    "text": "thought to check whether they were actually performing across all facial types",
    "start": "2088899",
    "end": "2095760"
  },
  {
    "start": "2095000",
    "end": "2095000"
  },
  {
    "text": "this leads into our first type of data a bias selection bias so this is where a non-random sampling",
    "start": "2095859",
    "end": "2103359"
  },
  {
    "text": "from is done from the population of interest and this means that the sample doesn't fully represent the population",
    "start": "2103359",
    "end": "2110320"
  },
  {
    "text": "it might mean that the distribution of the features or Target is different to the population of Interest or in the",
    "start": "2110320",
    "end": "2116800"
  },
  {
    "text": "worst case we might be leaving out big chunks of our population as potentially happened with those facial recognition",
    "start": "2116800",
    "end": "2122619"
  },
  {
    "text": "apps and this is much more likely to happen in smaller samples because it's just harder to get hold of data that",
    "start": "2122619",
    "end": "2129880"
  },
  {
    "text": "represents everyone when you don't have a big enough data set in the ad space sample let's say that",
    "start": "2129880",
    "end": "2136000"
  },
  {
    "text": "with the data that we have it looks like the price of ads increases linearly with",
    "start": "2136000",
    "end": "2141280"
  },
  {
    "text": "the number of downloads that an app has so if we're trying to extrapolate from this what we're going to guess is we can",
    "start": "2141280",
    "end": "2147700"
  },
  {
    "text": "get more and more money the more and more downloads that are in our parts so great right",
    "start": "2147700",
    "end": "2153880"
  },
  {
    "text": "whoops turns out we have a sample a sampling bias so we somehow left out a",
    "start": "2153880",
    "end": "2160060"
  },
  {
    "text": "bunch of data on really highly downloaded apps and because we",
    "start": "2160060",
    "end": "2165339"
  },
  {
    "text": "accidentally left these out of our training data we've missed the true relationship between the number of app",
    "start": "2165339",
    "end": "2170859"
  },
  {
    "text": "downloads and the actual price that we get it's much more logarithmic than linear so without knowing this we would",
    "start": "2170859",
    "end": "2177640"
  },
  {
    "text": "have made an incorrect extrapolation about the relationship between this feature and the target",
    "start": "2177640",
    "end": "2184540"
  },
  {
    "start": "2184000",
    "end": "2184000"
  },
  {
    "text": "the final bias that I want to talk about is survivorship buyers this is a form of bias where our sample only includes",
    "start": "2184540",
    "end": "2191800"
  },
  {
    "text": "those observations that made it some past some sort of selection process",
    "start": "2191800",
    "end": "2197520"
  },
  {
    "text": "so to set the scene let's go back to World War II and the U.S we're conducting surveys on planes that had",
    "start": "2200260",
    "end": "2206859"
  },
  {
    "text": "made it back to base and trying to see where the damage occurred in order to be able to reinforce them so what they did",
    "start": "2206859",
    "end": "2212740"
  },
  {
    "text": "is they created this lovely diagram where they plotted out where all of the bullets had hit and they're like okay",
    "start": "2212740",
    "end": "2218260"
  },
  {
    "text": "great we're the greatest concentration of red is let's reinforce there",
    "start": "2218260",
    "end": "2224020"
  },
  {
    "text": "a very clever mathematician called Abraham Wald said wait a minute these are the planes that made it back what",
    "start": "2224020",
    "end": "2231099"
  },
  {
    "text": "about the ones that didn't so he'd of course correctly identified survivorship bias it would have been",
    "start": "2231099",
    "end": "2237400"
  },
  {
    "text": "pointless to base the the reinforcements on these planes because these were the",
    "start": "2237400",
    "end": "2242920"
  },
  {
    "text": "ones that could actually fly after sustaining damage so let's have a look at an example of",
    "start": "2242920",
    "end": "2249400"
  },
  {
    "text": "survivorship bias in our ad space data so remember back to the beginning where I said our sales team were trying to",
    "start": "2249400",
    "end": "2254740"
  },
  {
    "text": "help us by blocking ad space that wasn't very profitable well this introduced a",
    "start": "2254740",
    "end": "2260560"
  },
  {
    "text": "survivorship bias into our data so if we compare the true distribution of price",
    "start": "2260560",
    "end": "2266260"
  },
  {
    "text": "to the distribution from our sample after they had blocked that unprofitable ad space we can see there's a big",
    "start": "2266260",
    "end": "2272440"
  },
  {
    "text": "difference the orange line is the distribution of price in our sample whereas the blue is the true",
    "start": "2272440",
    "end": "2279280"
  },
  {
    "text": "distribution of price so what we need to do is ask our sales team to turn off all",
    "start": "2279280",
    "end": "2284680"
  },
  {
    "text": "of that filtering at least for a little while get that unbiased sample and then basically be able to create our models",
    "start": "2284680",
    "end": "2291160"
  },
  {
    "text": "to predict properly so we're on the home stretch we finally",
    "start": "2291160",
    "end": "2296800"
  },
  {
    "text": "have an unbiased representative sample of data that measures what we want it to measure and is correctly split into",
    "start": "2296800",
    "end": "2304180"
  },
  {
    "text": "training validation and test sets we've gotten rid of all of those nasty dirty data pitfalls",
    "start": "2304180",
    "end": "2310119"
  },
  {
    "text": "now we need to make sure we don't screw it up by choosing the right model for our data type",
    "start": "2310119",
    "end": "2316780"
  },
  {
    "start": "2316000",
    "end": "2316000"
  },
  {
    "text": "so remember back to the beginning when we were discussing the criminality from faces study",
    "start": "2316780",
    "end": "2321960"
  },
  {
    "text": "I pointed out that the reason that the models seem to predict criminality so",
    "start": "2321960",
    "end": "2327040"
  },
  {
    "text": "accurately is because they were trying to compare mug shots to non-mug shots so",
    "start": "2327040",
    "end": "2333099"
  },
  {
    "text": "apart from the fact that I think trying to predict criminality from phases in the first place is not the best idea",
    "start": "2333099",
    "end": "2339400"
  },
  {
    "text": "if those authors had actually understood a little bit about how image recognition algorithms work they probably wouldn't",
    "start": "2339400",
    "end": "2346060"
  },
  {
    "text": "have made that mistake in the first place and this is the case with any machine learning algorithm I don't want to",
    "start": "2346060",
    "end": "2352540"
  },
  {
    "text": "suggest that you need to have a completely deep understanding of how every algorithm works from the ground up",
    "start": "2352540",
    "end": "2357940"
  },
  {
    "text": "in order to be able to use it but it is wise to at least have an idea of how the",
    "start": "2357940",
    "end": "2364180"
  },
  {
    "text": "relationship between features and targets are being represented by the model in order to be able to diagnose",
    "start": "2364180",
    "end": "2369820"
  },
  {
    "text": "any unexpected Behavior so for example linear regression models",
    "start": "2369820",
    "end": "2375880"
  },
  {
    "text": "assume that the relationship between features and targets are well they're linear they're a straight line",
    "start": "2375880",
    "end": "2383079"
  },
  {
    "text": "however if we go back to that relationship between app downloads and the ad Price we can see that it's not",
    "start": "2383079",
    "end": "2391119"
  },
  {
    "text": "and if we try to fit a linear regression model we can see that this rmsc value",
    "start": "2391119",
    "end": "2396820"
  },
  {
    "text": "the model error is quite High what this is telling us if we try to fit a straight line between those two values",
    "start": "2396820",
    "end": "2403300"
  },
  {
    "text": "our prediction is going to be off by an average of 87 cents so does this mean that we can't use",
    "start": "2403300",
    "end": "2409900"
  },
  {
    "text": "linear regression here well we actually can but we need to adjust the shape of",
    "start": "2409900",
    "end": "2414940"
  },
  {
    "text": "the line to fit the data in this case as we've already discussed that relationship is logarithmic so what",
    "start": "2414940",
    "end": "2421839"
  },
  {
    "text": "we can do is just tell the model please adjust the shape of this line to be logarithmic it does and we can see that",
    "start": "2421839",
    "end": "2429040"
  },
  {
    "text": "that model error shoots down to around seven cents so just by having a very simple understanding of how that model",
    "start": "2429040",
    "end": "2434980"
  },
  {
    "text": "is trying to fit that relationship and adjusting for it we can drastically improve our model fear",
    "start": "2434980",
    "end": "2442660"
  },
  {
    "start": "2442000",
    "end": "2442000"
  },
  {
    "text": "another thing that can trip us up at this stage is not using the right model metrics for our data",
    "start": "2442660",
    "end": "2449680"
  },
  {
    "text": "so let's revisit that example of having an imbalanced Target and let's assume that we didn't correct for it so let's",
    "start": "2449680",
    "end": "2456640"
  },
  {
    "text": "uh think about how our model metric might be affected by this so we fit a model and we work out the",
    "start": "2456640",
    "end": "2463420"
  },
  {
    "text": "number of times that we correctly and incorrectly predicted that the ad space wouldn't wouldn't sell",
    "start": "2463420",
    "end": "2469060"
  },
  {
    "text": "so you can see here that we have divided up the auctions that actually sold and did not sell and those that we predicted",
    "start": "2469060",
    "end": "2475839"
  },
  {
    "text": "would sell so so far we've been happily chugging along using accuracy to measure all of",
    "start": "2475839",
    "end": "2482859"
  },
  {
    "text": "our models so let's see how we actually calculate that we can calculate accuracy as all of the",
    "start": "2482859",
    "end": "2489520"
  },
  {
    "text": "correct predictions divided by the total number of observations so how does it go with this model",
    "start": "2489520",
    "end": "2495940"
  },
  {
    "text": "well we get 82 not bad right well maybe not",
    "start": "2495940",
    "end": "2502060"
  },
  {
    "text": "the problem with the accuracy is it tends to hide how well a model is performing for both outcomes in our case",
    "start": "2502060",
    "end": "2509320"
  },
  {
    "text": "how well it's predicting that an ad sold and how well it's predicting that an ad did not sell",
    "start": "2509320",
    "end": "2515619"
  },
  {
    "text": "so this is where alternative measures come in and I'm going to talk about two called precision and recall they're very",
    "start": "2515619",
    "end": "2521560"
  },
  {
    "text": "popular so what's precision Precision is the proportion of all of",
    "start": "2521560",
    "end": "2527260"
  },
  {
    "text": "the ad spaces we predicted would sell that actually solve",
    "start": "2527260",
    "end": "2532480"
  },
  {
    "text": "hey but in this case it's a hundred percent this is looking even better right well of course I'm going to hit you with",
    "start": "2532480",
    "end": "2538780"
  },
  {
    "text": "a no um let's introduce the third measure recall this is the proportioned group",
    "start": "2538780",
    "end": "2545920"
  },
  {
    "text": "this is the proportion of all ad spaces that actually sold meaning that we",
    "start": "2545920",
    "end": "2551380"
  },
  {
    "text": "accurately predicted would sell so in this case it's close to random and this",
    "start": "2551380",
    "end": "2557560"
  },
  {
    "text": "sort of discrepancy between accuracy and measures like precision and recall is pretty typical in imbalanced data and it",
    "start": "2557560",
    "end": "2564880"
  },
  {
    "text": "makes accuracy generally a poor choice when you have these sort of Target distributions",
    "start": "2564880",
    "end": "2570099"
  },
  {
    "text": "so what the Precision and recall values are telling us is our model has a tendency to underestimate How likely it",
    "start": "2570099",
    "end": "2577119"
  },
  {
    "text": "is that an ad space will sell so if we're asking it to select automatically add spaces to sell out or send out it's",
    "start": "2577119",
    "end": "2584560"
  },
  {
    "text": "going to mess around half of them in our case we actually want to reduce the number of AD spaces that we send out",
    "start": "2584560",
    "end": "2590619"
  },
  {
    "text": "maybe this is a bit too extreme but in our case maybe that might be okay but let's say we were trying to do the",
    "start": "2590619",
    "end": "2595900"
  },
  {
    "text": "opposite we're trying to maximize the number of auctions that we had in this case this model would be performing very",
    "start": "2595900",
    "end": "2602380"
  },
  {
    "text": "very poorly so it goes to kind of emphasize that your choice of model metric needs to be",
    "start": "2602380",
    "end": "2608380"
  },
  {
    "text": "made with both your data in mind but also the goals of your model",
    "start": "2608380",
    "end": "2613960"
  },
  {
    "text": "okay so when I first started getting into machine learning I had this impression that newer and more complex",
    "start": "2613960",
    "end": "2620980"
  },
  {
    "start": "2614000",
    "end": "2614000"
  },
  {
    "text": "and shinier models were better and this was quickly dispelled and this is such a",
    "start": "2620980",
    "end": "2626800"
  },
  {
    "text": "common assumption that it has its own name the no free lunch theorem different",
    "start": "2626800",
    "end": "2632859"
  },
  {
    "text": "models have been optimized for different data types and different problems so you can't just throw say I don't know",
    "start": "2632859",
    "end": "2639119"
  },
  {
    "text": "175 billion parameter Transformer model at any problem and expect the Optimal",
    "start": "2639119",
    "end": "2644440"
  },
  {
    "text": "Performance so one very famous example that's been discussed a lot lately is the poor",
    "start": "2644440",
    "end": "2650619"
  },
  {
    "text": "performance that neural Nets have with tabular data tabular data is sort of what we've been working with like where",
    "start": "2650619",
    "end": "2656740"
  },
  {
    "text": "you have rows and columns as opposed to text or images which is the more kind of",
    "start": "2656740",
    "end": "2662980"
  },
  {
    "text": "classic import for neural Nets so you can see that here in a paper from a couple of months ago so the authors",
    "start": "2662980",
    "end": "2670119"
  },
  {
    "text": "compared how well a bunch of tree based models compared to neural Nets in terms",
    "start": "2670119",
    "end": "2676420"
  },
  {
    "text": "of predicting tabular data so what you can see is that on classification problems the neural Nets",
    "start": "2676420",
    "end": "2683260"
  },
  {
    "text": "down the bottom here tended to only get an accuracy between 70 and 80 percent",
    "start": "2683260",
    "end": "2689200"
  },
  {
    "text": "the tree base models on the other hand were able to get between 80 to 95 accuracy",
    "start": "2689200",
    "end": "2694900"
  },
  {
    "text": "and you can see to a lesser extent not quite as dramatic a similar pattern for",
    "start": "2694900",
    "end": "2700420"
  },
  {
    "text": "regression-based problems so I was curious about this I was wondering why this this case and I",
    "start": "2700420",
    "end": "2706839"
  },
  {
    "text": "started digging through the papers and it actually seems that researchers are not 100 sure why this is happening and",
    "start": "2706839",
    "end": "2713800"
  },
  {
    "text": "what it shows is how complex the fit is between a model and a data set and the",
    "start": "2713800",
    "end": "2719560"
  },
  {
    "text": "best approach is really just to experiment with a variety of models that seem appropriate and you just start with",
    "start": "2719560",
    "end": "2725140"
  },
  {
    "text": "the simplest Baseline possible and work your way up from there so that's it thank you so much for",
    "start": "2725140",
    "end": "2731680"
  },
  {
    "text": "listening um I hope this gives a taster of what to do when your model is not performing as expected and how to kind of deal with",
    "start": "2731680",
    "end": "2738339"
  },
  {
    "text": "pitfalls in messy real world data thanks [Applause]",
    "start": "2738339",
    "end": "2749989"
  }
]