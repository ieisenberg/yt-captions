[
  {
    "start": "0",
    "end": "122000"
  },
  {
    "text": "welcome um to the last day of ndc and to my session auditing data and answering that lifelong question is at the end of",
    "start": "3520",
    "end": "10559"
  },
  {
    "text": "the day yet now if you're not familiar with that question don't worry about it we're gonna go deep into that one in the",
    "start": "10559",
    "end": "17119"
  },
  {
    "text": "next 45 minutes or so so what are we going to talk about heads up it's not going to be dotnet",
    "start": "17119",
    "end": "23760"
  },
  {
    "text": "related wait clicker yes um so what are we going to talk about",
    "start": "23760",
    "end": "30400"
  },
  {
    "text": "we'll start off with looking at nielsen's architecture and we'll eventually see how it really",
    "start": "30400",
    "end": "35600"
  },
  {
    "text": "had little fires going on everywhere especially in regards to data arrival",
    "start": "35600",
    "end": "40640"
  },
  {
    "text": "rates and data visibility we'll start dwelving into that end of the day question and we'll really understand",
    "start": "40640",
    "end": "47200"
  },
  {
    "text": "that need for an auditing subsystem we'll then start designing our auditing",
    "start": "47200",
    "end": "52239"
  },
  {
    "text": "data understand how it looks like we'll store it we'll query it will maybe",
    "start": "52239",
    "end": "57680"
  },
  {
    "text": "finally get an answer to that end of the day question and then some alerts and add-ons to top",
    "start": "57680",
    "end": "62879"
  },
  {
    "text": "a lot to top it all up but first who am i",
    "start": "62879",
    "end": "68560"
  },
  {
    "text": "so who am i i'm simona i'm a senior big data engineer at adoc but i was working",
    "start": "68560",
    "end": "74560"
  },
  {
    "text": "for nielsen previously so this is totally based on a true story i love data i've been dealing with data",
    "start": "74560",
    "end": "80479"
  },
  {
    "text": "for the past decade now which feels like a lot i love music the weirder the better and",
    "start": "80479",
    "end": "87920"
  },
  {
    "text": "i love traveling so basically this is me in a slide",
    "start": "87920",
    "end": "93200"
  },
  {
    "text": "now i'd like to hear a little bit about you guys um how many of you are actually data",
    "start": "93200",
    "end": "99040"
  },
  {
    "text": "engineers you can raise your hands awesome how many of you",
    "start": "99040",
    "end": "105280"
  },
  {
    "text": "are software developers amazing how many of you work on aws platform",
    "start": "105280",
    "end": "114880"
  },
  {
    "text": "cool yes great um so let's start talking about",
    "start": "114880",
    "end": "120240"
  },
  {
    "text": "nielsen's data infrastructure so what you see before you is how nielsen collects data",
    "start": "120240",
    "end": "126799"
  },
  {
    "start": "122000",
    "end": "122000"
  },
  {
    "text": "more than 50 more than 60 terabytes of data billions of events flow through",
    "start": "126799",
    "end": "132239"
  },
  {
    "text": "this architecture daily and we'd collect them from various devices so computers",
    "start": "132239",
    "end": "137440"
  },
  {
    "text": "tvs cell phones and they first and foremost would flow into the front-end",
    "start": "137440",
    "end": "142720"
  },
  {
    "text": "layer and more specifically the data serving infrastructure now the data serving infrastructure is",
    "start": "142720",
    "end": "148640"
  },
  {
    "text": "written in java slas are very very important to them being that front-end layer for all of",
    "start": "148640",
    "end": "154640"
  },
  {
    "text": "these events they collect these events they enrich them and they produce them",
    "start": "154640",
    "end": "159760"
  },
  {
    "text": "to their very own kafka so it would be very important to mention",
    "start": "159760",
    "end": "165200"
  },
  {
    "text": "that at the time that this architecture slide was designed we were running the front-end layer regionally what does",
    "start": "165200",
    "end": "171760"
  },
  {
    "text": "that mean we had several data centers all across the world and each in one of these data",
    "start": "171760",
    "end": "178080"
  },
  {
    "text": "centers was running its own front-end layer so the data serving infrastructure",
    "start": "178080",
    "end": "183599"
  },
  {
    "text": "collects these events enriches them and then produces them to its own regional",
    "start": "183599",
    "end": "189040"
  },
  {
    "text": "kafka from there the data gets replicated using the eu replicator into the one",
    "start": "189040",
    "end": "196000"
  },
  {
    "text": "single single kafka on aws so we're already in the data layer",
    "start": "196000",
    "end": "201680"
  },
  {
    "text": "then we have the data loaders these are big robust apache spark streaming applications they're responsible for",
    "start": "201680",
    "end": "209040"
  },
  {
    "text": "consuming that data deserializing it and then eventually storing it in rdr",
    "start": "209040",
    "end": "216000"
  },
  {
    "text": "rdr that just stands for raw data repository and that's our data lake",
    "start": "216000",
    "end": "221200"
  },
  {
    "text": "sitting there on s3 we then have our data processors these are",
    "start": "221200",
    "end": "226720"
  },
  {
    "text": "big robust spark jobs they consume that data enrich it aggregate it and then",
    "start": "226720",
    "end": "232560"
  },
  {
    "text": "eventually store it in all of these different data stores and even back into rdr and if we talk",
    "start": "232560",
    "end": "239360"
  },
  {
    "text": "about the applications consuming this data some of them consume it all together through hive and some through",
    "start": "239360",
    "end": "246640"
  },
  {
    "text": "each and one of these dedicated data stores and i want to take a little moment into",
    "start": "246640",
    "end": "252000"
  },
  {
    "text": "explaining some of the things i talked about so in regards to kafka we were using",
    "start": "252000",
    "end": "257280"
  },
  {
    "text": "avro format across all of our topics avro format for those of you who do not",
    "start": "257280",
    "end": "263520"
  },
  {
    "text": "know is a serialization format and you can actually use json just to define all of your data types and we'll",
    "start": "263520",
    "end": "270880"
  },
  {
    "text": "later on see how this actually made our job really really easy",
    "start": "270880",
    "end": "275919"
  },
  {
    "text": "we use schema registry to store all of our schemas and the chart that you see before is a",
    "start": "275919",
    "end": "282320"
  },
  {
    "text": "very classical chart so you could see that in a lot of organizations",
    "start": "282320",
    "end": "288160"
  },
  {
    "text": "and if we take a moment to actually just look at the data lake we'd have several buckets",
    "start": "288160",
    "end": "294080"
  },
  {
    "start": "289000",
    "end": "289000"
  },
  {
    "text": "on s3 aws each bucket was just dedicated to a different topic",
    "start": "294080",
    "end": "299840"
  },
  {
    "text": "and we were saving our data in daily folders so you have a bucket for a topic",
    "start": "299840",
    "end": "305680"
  },
  {
    "text": "a daily folder and just parquet files if you're not familiar with 4k files",
    "start": "305680",
    "end": "312240"
  },
  {
    "text": "because you're not a data engineer then this is just an open source columnar format that just",
    "start": "312240",
    "end": "318960"
  },
  {
    "text": "makes consuming and accessing that data later on easy and this architecture really had little",
    "start": "318960",
    "end": "326080"
  },
  {
    "text": "fires going on everywhere and what we'll do now is we'll just start discussing all of these little fires so you'll",
    "start": "326080",
    "end": "332720"
  },
  {
    "text": "really understand that need for an auditing subsystem",
    "start": "332720",
    "end": "337960"
  },
  {
    "start": "337000",
    "end": "337000"
  },
  {
    "text": "so first thing i really want to zoom into this connecting point so what's happening here is that the data is being",
    "start": "338560",
    "end": "345840"
  },
  {
    "text": "produced to the original kafka and then it's being replicated using the eu replicator into that one single kafka",
    "start": "345840",
    "end": "353440"
  },
  {
    "text": "on aws what could go wrong well",
    "start": "353440",
    "end": "359520"
  },
  {
    "text": "several things if you're not familiar with the euro applicator it's a project made by uber",
    "start": "359520",
    "end": "365600"
  },
  {
    "text": "based on kafka's own mirror maker in version one so aside from the most obvious problems",
    "start": "365600",
    "end": "373280"
  },
  {
    "text": "like the replicator crashing data is not being replicated some figs",
    "start": "373280",
    "end": "379360"
  },
  {
    "text": "could be even worse so at nielsen israel we had some pretty big topics some of them were more than 1",
    "start": "379360",
    "end": "387600"
  },
  {
    "text": "000 partitions big and sometimes just sometimes",
    "start": "387600",
    "end": "393280"
  },
  {
    "text": "the your applicator would get stuck on some 5 or 10 partitions",
    "start": "393280",
    "end": "399039"
  },
  {
    "text": "so most of your data is being replicated but just a little bit is not",
    "start": "399039",
    "end": "405840"
  },
  {
    "text": "next let's talk about this part so this is what happens after the data arrives into kafka and aws",
    "start": "407199",
    "end": "414479"
  },
  {
    "text": "so as i've already said we have the data loaders they consume that data deserialize it from april format stored",
    "start": "414479",
    "end": "422160"
  },
  {
    "text": "in per k on s3 what could go wrong well several things",
    "start": "422160",
    "end": "428400"
  },
  {
    "text": "kafka broke is crashing someone said the unclean leader election",
    "start": "428400",
    "end": "433520"
  },
  {
    "text": "variable to true suddenly you get an entire partition wiped out",
    "start": "433520",
    "end": "439199"
  },
  {
    "text": "spark streaming applications are crashing alerts start popping up",
    "start": "439199",
    "end": "444319"
  },
  {
    "text": "bottom line data is being laid to arrive to rdr or it's not arriving at all",
    "start": "444319",
    "end": "451680"
  },
  {
    "start": "451000",
    "end": "451000"
  },
  {
    "text": "and while talking about all of these pain points what didn't i mention",
    "start": "451680",
    "end": "457039"
  },
  {
    "text": "well i actually didn't even mention how we recovered from all of these failures",
    "start": "457039",
    "end": "462400"
  },
  {
    "text": "and moreover what was our criteria for recovery",
    "start": "462400",
    "end": "468400"
  },
  {
    "text": "well if we talk about this point again your replicator crashing",
    "start": "468479",
    "end": "473520"
  },
  {
    "text": "data is not being replicated we'll just start the replicator back up again we can see the data is being replicated",
    "start": "473520",
    "end": "480800"
  },
  {
    "text": "we can see that it's flowing again and at this point we'd say okay great",
    "start": "480800",
    "end": "486879"
  },
  {
    "text": "problem solved and very similar uh scenarios for this",
    "start": "486879",
    "end": "493360"
  },
  {
    "text": "part so data loader's crashing kafka broke is crashing we just start them back up again",
    "start": "493360",
    "end": "500000"
  },
  {
    "text": "so cluster status is back to healthy we can see the data is being consumed again we can see",
    "start": "500000",
    "end": "505919"
  },
  {
    "text": "that the data is flowing again and at this point we'd say okay great",
    "start": "505919",
    "end": "511280"
  },
  {
    "text": "everything's fine but we didn't really know any of the implications that these crashes and",
    "start": "511280",
    "end": "517440"
  },
  {
    "text": "failures were having on our data we didn't really know if we had any data duplication or data loss",
    "start": "517440",
    "end": "525040"
  },
  {
    "text": "and nielsen is a data company so data integrity is very very important to us",
    "start": "525040",
    "end": "531279"
  },
  {
    "text": "and at this point i really want to start discussing that end of the day question and why it is so important to in nielsen",
    "start": "531279",
    "end": "539040"
  },
  {
    "start": "534000",
    "end": "534000"
  },
  {
    "text": "and to get you to understand that i really just need to take you to nielsen's world to the media world",
    "start": "539040",
    "end": "546240"
  },
  {
    "text": "so what we'd want to do at nielsen is count unique users",
    "start": "546240",
    "end": "552160"
  },
  {
    "text": "and we want to do that on a daily basis so how many unique users do we see per day",
    "start": "552160",
    "end": "559760"
  },
  {
    "text": "so essentially you'd want to run your processor your aggregator only on a full day's worth of data",
    "start": "559760",
    "end": "567680"
  },
  {
    "text": "so that would mean only when yesterday is over right because if you run your processor",
    "start": "567680",
    "end": "573920"
  },
  {
    "text": "on partial amounts of data you would have to run it again and dealing with more than 60 terabytes",
    "start": "573920",
    "end": "580320"
  },
  {
    "text": "of data per day that would cost you quite a lot of money so",
    "start": "580320",
    "end": "585839"
  },
  {
    "text": "the question of the end of the day is just a question of can i run my",
    "start": "585839",
    "end": "590959"
  },
  {
    "text": "processor now has all of yesterday data actually arrived into the data lake",
    "start": "590959",
    "end": "598320"
  },
  {
    "text": "and at first we tried solving this problem with a pretty straightforward solution",
    "start": "598480",
    "end": "604079"
  },
  {
    "start": "599000",
    "end": "599000"
  },
  {
    "text": "we said okay what time of day would yesterday be definitely over okay",
    "start": "604079",
    "end": "611040"
  },
  {
    "text": "so he said okay probably like seven a.m seven a.m yesterday is definitely over",
    "start": "611040",
    "end": "617200"
  },
  {
    "text": "and we set our processor to run at 7am but then this one time we had this",
    "start": "617200",
    "end": "622959"
  },
  {
    "text": "pretty big kafka outage no data was flowing whatsoever we couldn't solve it by 7am",
    "start": "622959",
    "end": "629760"
  },
  {
    "text": "data processor started running and we were back at square one then we said okay",
    "start": "629760",
    "end": "637040"
  },
  {
    "text": "we store our data in daily in daily folders right how about we use aws s3ls and we use",
    "start": "637040",
    "end": "644959"
  },
  {
    "text": "that timestamp as an indicator the timestamp of the last file that was written into the",
    "start": "644959",
    "end": "650160"
  },
  {
    "text": "folder but then this one time the eu replicator wasn't working for one of our regions",
    "start": "650160",
    "end": "658560"
  },
  {
    "text": "so our processors took that as a signal that the day was actually over and they started running again on",
    "start": "658560",
    "end": "665120"
  },
  {
    "text": "partial amounts of data and again we were back to square one and i think that if we look at nielsen's",
    "start": "665120",
    "end": "671760"
  },
  {
    "text": "data architecture again you'll really see what i'm talking about so",
    "start": "671760",
    "end": "677279"
  },
  {
    "text": "there's a problem there's a problem with kafka there's a problem with the data loaders bottom line data is being laid",
    "start": "677279",
    "end": "683279"
  },
  {
    "text": "to arrive to the rdr data processors not aware they start processing all of these partial amounts",
    "start": "683279",
    "end": "689680"
  },
  {
    "text": "of data they store all of these partial aggregations in all of these different data stores",
    "start": "689680",
    "end": "695279"
  },
  {
    "text": "but the problem doesn't actually end there because it's not only the data processors that will have to reprocess",
    "start": "695279",
    "end": "701760"
  },
  {
    "text": "their data all of these applications reports bi billing data science they",
    "start": "701760",
    "end": "707440"
  },
  {
    "text": "actually build their own data sets on top of all of these data stores so they will have to rebuild their data",
    "start": "707440",
    "end": "714399"
  },
  {
    "text": "sets as well and so i think if you look at this architecture slide again",
    "start": "714399",
    "end": "720880"
  },
  {
    "start": "717000",
    "end": "717000"
  },
  {
    "text": "you'll really agree that it had little fires going on everywhere",
    "start": "720880",
    "end": "726240"
  },
  {
    "text": "and so for these two reasons first one just being able to understand",
    "start": "726240",
    "end": "731519"
  },
  {
    "text": "the implications of all of these failures and crashes on our data",
    "start": "731519",
    "end": "736720"
  },
  {
    "text": "and b being able to answer that end of the day question we've decided to design",
    "start": "736720",
    "end": "742560"
  },
  {
    "text": "our auditing subsystem and what we'll do now is i'll walk you through the key points that we need to",
    "start": "742560",
    "end": "749279"
  },
  {
    "start": "745000",
    "end": "745000"
  },
  {
    "text": "keep in mind to design our data so the first point is just the fact that",
    "start": "749279",
    "end": "756160"
  },
  {
    "text": "we had several topics we didn't have one and that number was not set either",
    "start": "756160",
    "end": "761760"
  },
  {
    "text": "so if tomorrow we have a new need a new data type we might define a new kafka topic",
    "start": "761760",
    "end": "769519"
  },
  {
    "text": "the next several set of points has to do with the data serving infrastructure and i've mentioned some of them before",
    "start": "769519",
    "end": "776480"
  },
  {
    "text": "so i told you that the data serving infrastructure is written in java",
    "start": "776480",
    "end": "782240"
  },
  {
    "text": "it's not running one process it's running two processes and each and every one of those is actually running against",
    "start": "782240",
    "end": "789519"
  },
  {
    "text": "a different set of topics according to its own configuration another thing is the fact that they",
    "start": "789519",
    "end": "796880"
  },
  {
    "text": "don't use a simple kafka producer they use a wraptop version a nielsen kafka",
    "start": "796880",
    "end": "803200"
  },
  {
    "text": "producer and of course as i've already said slas",
    "start": "803200",
    "end": "808399"
  },
  {
    "text": "very very important to them last but not least of course as i've",
    "start": "808399",
    "end": "813440"
  },
  {
    "text": "already mentioned we use the average serialization format across all of our",
    "start": "813440",
    "end": "818480"
  },
  {
    "text": "topics and as i've already said this is going to make her job really really easy",
    "start": "818480",
    "end": "824639"
  },
  {
    "text": "so last but not least why is the word window written on my screen what does that mean",
    "start": "824639",
    "end": "831680"
  },
  {
    "text": "well the auditing window is going to be our metadata object can you guys see the avro definition all",
    "start": "831680",
    "end": "838720"
  },
  {
    "text": "okay cool um so what do you see you can see",
    "start": "838720",
    "end": "844560"
  },
  {
    "text": "everything that i've mentioned we have topic server process so for every topic for every server for",
    "start": "844560",
    "end": "851680"
  },
  {
    "text": "every process we're going to just count the number of messages being produced",
    "start": "851680",
    "end": "857680"
  },
  {
    "text": "but we can't really audit forever we have to audit for a specific time frame",
    "start": "857680",
    "end": "862720"
  },
  {
    "text": "and that's exactly our auditing window it's that defined time frame that we're going to audit for",
    "start": "862720",
    "end": "869120"
  },
  {
    "text": "so for every topic for every server for every process for every auditing time",
    "start": "869120",
    "end": "876480"
  },
  {
    "text": "we're going to keep account of the number of messages being produced essentially we're identifying every",
    "start": "876480",
    "end": "883760"
  },
  {
    "text": "producer in this massive scale system and we're enabling auditing for it",
    "start": "883760",
    "end": "890240"
  },
  {
    "text": "now if you're wondering what is the optimal auditing window and how big it",
    "start": "890240",
    "end": "896160"
  },
  {
    "text": "should be it really depends on your scale and throughput we've set it to be five minutes we wrote",
    "start": "896160",
    "end": "903040"
  },
  {
    "text": "our code in such a way that it was completely configurable we never changed it ever since",
    "start": "903040",
    "end": "909920"
  },
  {
    "text": "five minutes was just fine and what you see before you is just the",
    "start": "909920",
    "end": "915360"
  },
  {
    "text": "avro definition that has has everything that we've discussed so now that we have this audit window",
    "start": "915360",
    "end": "921920"
  },
  {
    "text": "objects our metadata we still need to connect it to our actual data",
    "start": "921920",
    "end": "928720"
  },
  {
    "text": "we're going to do that using a new object a new avro object",
    "start": "928720",
    "end": "933759"
  },
  {
    "text": "that is going to be called the audit header and the audit header is just going to contain all of that complementary",
    "start": "933759",
    "end": "940000"
  },
  {
    "text": "information that our audit window does not so what does",
    "start": "940000",
    "end": "945360"
  },
  {
    "text": "what does our what does our data does not contain well we definitely know the topic right",
    "start": "945360",
    "end": "951839"
  },
  {
    "text": "because we're consuming from that topic we don't have the server the process",
    "start": "951839",
    "end": "957040"
  },
  {
    "text": "and we don't have the window timestamp if you're wondering why process id is not up here",
    "start": "957040",
    "end": "963040"
  },
  {
    "text": "it's just because it's incorporated in the server name and if you've noticed the sequence id",
    "start": "963040",
    "end": "969040"
  },
  {
    "text": "and you're wondering why i didn't mention it it's a bit of a spoiler and we're really going to discuss it later on",
    "start": "969040",
    "end": "976639"
  },
  {
    "text": "so now that we have our audit header we still still need to make it part of our",
    "start": "976639",
    "end": "981680"
  },
  {
    "text": "actual data the first thing we're going to do is we're going to edit as a field",
    "start": "981680",
    "end": "988560"
  },
  {
    "text": "in our topic so if we look at a very generic definition for for an avro model right for example for",
    "start": "988560",
    "end": "996399"
  },
  {
    "text": "for one of our topics um so we have uh this is a very generic",
    "start": "996399",
    "end": "1001519"
  },
  {
    "text": "record we call it relevant data and we have a list of fields and what we're doing is we're just",
    "start": "1001519",
    "end": "1008160"
  },
  {
    "text": "actually adding audit header as the second field in this list of fields",
    "start": "1008160",
    "end": "1013920"
  },
  {
    "text": "nice but we still need to actually add that data in it's part of our model we still",
    "start": "1013920",
    "end": "1020720"
  },
  {
    "text": "need to to edit in and we're going to do it using this piece of code here it's java code",
    "start": "1020720",
    "end": "1028480"
  },
  {
    "start": "1024000",
    "end": "1024000"
  },
  {
    "text": "not net i'm sorry but let me explain what's happening",
    "start": "1028480",
    "end": "1034880"
  },
  {
    "text": "so what we're doing is we created we're creating the audit header it has the window time and the server",
    "start": "1034880",
    "end": "1040880"
  },
  {
    "text": "name then what we're doing is because we're processing all of these generic events",
    "start": "1040880",
    "end": "1046400"
  },
  {
    "text": "we're just taking this avro record and we're checking its schema so if that",
    "start": "1046400",
    "end": "1051760"
  },
  {
    "text": "schema has the audit header in it meaning auditing is enabled for this topic we're just running it over",
    "start": "1051760",
    "end": "1059120"
  },
  {
    "text": "using the object we've created and it's really that simple just because we're using avro",
    "start": "1059120",
    "end": "1066880"
  },
  {
    "text": "so now we have the audit window metadata object that's keeping account for all of our producers",
    "start": "1066880",
    "end": "1072720"
  },
  {
    "text": "and we also have the audit header as part of our data",
    "start": "1072720",
    "end": "1077600"
  },
  {
    "text": "where should all of this be i really recommend that if you design an",
    "start": "1078400",
    "end": "1083919"
  },
  {
    "text": "auditing subsystem for your kafka then you should really put it on your",
    "start": "1083919",
    "end": "1089120"
  },
  {
    "text": "producer so what we're doing is we're getting all of these generic messages right before",
    "start": "1089120",
    "end": "1095280"
  },
  {
    "text": "they're being produced if auditing is enabled for them we're running over the audit header we're incrementing the",
    "start": "1095280",
    "end": "1102480"
  },
  {
    "text": "we're incrementing the counter and we're just producing these messages",
    "start": "1102480",
    "end": "1108400"
  },
  {
    "text": "and in context of everything we've just discussed everything is happening right here",
    "start": "1108400",
    "end": "1113600"
  },
  {
    "text": "so everything's happening in the data serving infrastructure so and our data is just still being",
    "start": "1113600",
    "end": "1120080"
  },
  {
    "text": "produced into that one regional kafka cool",
    "start": "1120080",
    "end": "1126240"
  },
  {
    "start": "1124000",
    "end": "1124000"
  },
  {
    "text": "now the question is of course how we're producing that audit window metadata object",
    "start": "1126240",
    "end": "1131600"
  },
  {
    "text": "because at first being just like in avro format we've decided",
    "start": "1131600",
    "end": "1136720"
  },
  {
    "text": "straightforwardly to just produce it to a kafka topic but then we noticed it's a bit silly",
    "start": "1136720",
    "end": "1142799"
  },
  {
    "text": "right because we're actually auditing kafka using kafka",
    "start": "1142799",
    "end": "1149120"
  },
  {
    "text": "so instead we've just designed a really really simple rest api on aws you have the",
    "start": "1149120",
    "end": "1155679"
  },
  {
    "text": "amazon api gateway that allows you to do that and so we just started shipping our",
    "start": "1155679",
    "end": "1161120"
  },
  {
    "text": "audit window metadata objects through that api",
    "start": "1161120",
    "end": "1166159"
  },
  {
    "text": "and again in the same context so now on the aws layer we have that very simple",
    "start": "1166480",
    "end": "1172320"
  },
  {
    "text": "api responsible for receiving all of these messages",
    "start": "1172320",
    "end": "1178000"
  },
  {
    "start": "1177000",
    "end": "1177000"
  },
  {
    "text": "cool so we have our data we know what it looks like we're producing it how are we going to",
    "start": "1178000",
    "end": "1184799"
  },
  {
    "text": "consume it for the audit window metadata object i've already explained i've already",
    "start": "1184799",
    "end": "1190720"
  },
  {
    "text": "mentioned that so we're shipping it straight through the api if we're still producing it to the kafka",
    "start": "1190720",
    "end": "1196160"
  },
  {
    "text": "topic we of course can just consume it on the other side for the audit header",
    "start": "1196160",
    "end": "1202400"
  },
  {
    "text": "it's just part of our data it's going through exactly the same pipeline so now",
    "start": "1202400",
    "end": "1208480"
  },
  {
    "text": "our data loaders they're consuming that data they're still consuming it from kafka",
    "start": "1208480",
    "end": "1213840"
  },
  {
    "text": "deserializing it and just storing it in 4k format with all of our data in the data lake",
    "start": "1213840",
    "end": "1219760"
  },
  {
    "text": "so we can just consume it from s3 using a very simple s-free consumer",
    "start": "1219760",
    "end": "1226320"
  },
  {
    "start": "1225000",
    "end": "1225000"
  },
  {
    "text": "again in context of everything we've discussed so our data is still being produced to",
    "start": "1226880",
    "end": "1232000"
  },
  {
    "text": "that regional kafka replicated to kafka aws data loaders",
    "start": "1232000",
    "end": "1238240"
  },
  {
    "text": "spilling it into the data lake called the rdr and then we have the s3 consumer",
    "start": "1238240",
    "end": "1243360"
  },
  {
    "text": "putting it in the data auditing table that we're going to discuss in just in just a moment",
    "start": "1243360",
    "end": "1248960"
  },
  {
    "text": "and then the audit window metadata objects they're shipped through the api and also",
    "start": "1248960",
    "end": "1254799"
  },
  {
    "text": "arrive in the same table cool let's discuss that table right now",
    "start": "1254799",
    "end": "1263200"
  },
  {
    "start": "1260000",
    "end": "1260000"
  },
  {
    "text": "so we've decided to store all of our data in postgres sql on aws",
    "start": "1263200",
    "end": "1268320"
  },
  {
    "text": "you do have that service it's called rds so we've chose rds we have a postgresql",
    "start": "1268320",
    "end": "1274480"
  },
  {
    "text": "there why postgres well essentially the data is structured okay it's it's definitely structured we",
    "start": "1274480",
    "end": "1281280"
  },
  {
    "text": "have a key that you should know all by by heart right now so it's a window timestamp topic server and process id",
    "start": "1281280",
    "end": "1288159"
  },
  {
    "text": "and we have our counter right also if you think about auditing data",
    "start": "1288159",
    "end": "1294080"
  },
  {
    "text": "and the volume of auditing data it's pretty similar to log data in terms that users don't actually want to look",
    "start": "1294080",
    "end": "1301679"
  },
  {
    "text": "at data that's older than a month or two back so just taking both of these factors",
    "start": "1301679",
    "end": "1307840"
  },
  {
    "text": "into consideration we've decided to store it in a phosphorous sql",
    "start": "1307840",
    "end": "1313519"
  },
  {
    "start": "1312000",
    "end": "1312000"
  },
  {
    "text": "in order to design our table we need to understand what questions we actually",
    "start": "1313600",
    "end": "1318880"
  },
  {
    "text": "want answered the first one would be on what levels of granularity would we want our questions",
    "start": "1318880",
    "end": "1325440"
  },
  {
    "text": "answered so there are the obvious ones like time granularity for which we would use",
    "start": "1325440",
    "end": "1332960"
  },
  {
    "text": "our audit window and then topic server and process but maybe something else",
    "start": "1332960",
    "end": "1340320"
  },
  {
    "text": "we were running several data centers all across the world so why not get an answer on a regional",
    "start": "1340320",
    "end": "1347360"
  },
  {
    "text": "granularity then we have arrival rates that's pretty straightforward right so",
    "start": "1347360",
    "end": "1353360"
  },
  {
    "text": "that's just how many of the messages that were produced actually arrived on the other side",
    "start": "1353360",
    "end": "1360080"
  },
  {
    "text": "but then we also maybe probably want to know arrival latency so that's just how many of the messages",
    "start": "1360080",
    "end": "1366799"
  },
  {
    "text": "that were produced arrived within two hours or four",
    "start": "1366799",
    "end": "1372840"
  },
  {
    "text": "hours cool so let's look how let's understand how it looks like um you can see everything we've",
    "start": "1372840",
    "end": "1380000"
  },
  {
    "text": "discussed so we have the audit timestamp the topic the server and the process",
    "start": "1380000",
    "end": "1385280"
  },
  {
    "text": "we have a location column just because we're storing all of that data in the same table so in order to",
    "start": "1385280",
    "end": "1392159"
  },
  {
    "text": "differentiate between the headers and the metadata we're going to use the location column",
    "start": "1392159",
    "end": "1398320"
  },
  {
    "text": "the event count and we also have our add-ons so we have region and we have insert time",
    "start": "1398320",
    "end": "1404480"
  },
  {
    "text": "and just deducting insert time from audit timestamp we'll be able to calculate arrival latency",
    "start": "1404480",
    "end": "1412240"
  },
  {
    "text": "nice so i have an anonymized version of this table here for you today some cool things to notice",
    "start": "1412880",
    "end": "1419760"
  },
  {
    "text": "insert time and window timestamp they are both in utc we had developers working in israel we",
    "start": "1419760",
    "end": "1426159"
  },
  {
    "text": "had developers working in the united states so keeping all of our timestamps in utc was very very important to us",
    "start": "1426159",
    "end": "1434159"
  },
  {
    "text": "region we just extracted it from the server name i don't really recommend that strategy",
    "start": "1434159",
    "end": "1440400"
  },
  {
    "text": "but it was the quickest way to go and it worked you can see that location we have kafka",
    "start": "1440400",
    "end": "1447039"
  },
  {
    "text": "windows so these are our metadata objects we have rdr headers that's our",
    "start": "1447039",
    "end": "1452480"
  },
  {
    "text": "actual data another cool thing to notice in my opinion would be the event count so you",
    "start": "1452480",
    "end": "1458400"
  },
  {
    "text": "can see that some of the event counts are pretty big right they're over 90k 30k",
    "start": "1458400",
    "end": "1463760"
  },
  {
    "text": "but then you can notice that we have an event count of zero how is that possible that's pretty weird",
    "start": "1463760",
    "end": "1471039"
  },
  {
    "text": "well if you look closely you can notice that the event count of zero goes only for the location of kafka window",
    "start": "1471039",
    "end": "1478880"
  },
  {
    "text": "so what's actually happening is that the auditing subsystem is working all the time so even if that",
    "start": "1478880",
    "end": "1485679"
  },
  {
    "text": "server is not receiving any traffic whatsoever we're still going to create all of these",
    "start": "1485679",
    "end": "1492080"
  },
  {
    "text": "metadata objects and ship them to the other side so they're going to arrive with an event count of zero",
    "start": "1492080",
    "end": "1498720"
  },
  {
    "text": "so that's just a really nice indication that the auditing subsystem is still working and um i think that's really",
    "start": "1498720",
    "end": "1505360"
  },
  {
    "text": "cool to notice cool so you see before you now is the logo",
    "start": "1505360",
    "end": "1511200"
  },
  {
    "text": "for apache superset if you're not familiar with it it's a bi tool um under apache",
    "start": "1511200",
    "end": "1517919"
  },
  {
    "text": "and that's the bi tool that we've used at nielsen israel for all of our data visualization needs and also for data",
    "start": "1517919",
    "end": "1525039"
  },
  {
    "text": "auditing so some nice charts we have the data arrival rate within",
    "start": "1525039",
    "end": "1530720"
  },
  {
    "text": "three hours and you can notice that the number is a bit higher than 100",
    "start": "1530720",
    "end": "1536080"
  },
  {
    "text": "and yes we did have data duplication and you can say that yeah but with over 60",
    "start": "1536080",
    "end": "1541840"
  },
  {
    "text": "terabytes of data having that 100.22 is nothing who cares",
    "start": "1541840",
    "end": "1547679"
  },
  {
    "text": "but when you see that number go over 160 or 200",
    "start": "1547679",
    "end": "1553279"
  },
  {
    "text": "you know that you're having a problem some more nice charts so it's just a",
    "start": "1553279",
    "end": "1560159"
  },
  {
    "text": "data arrival rate for one hour and 24 hours and i think that's really",
    "start": "1560159",
    "end": "1565279"
  },
  {
    "text": "important to be able to just have these metrics and also being able to visualize them",
    "start": "1565279",
    "end": "1572320"
  },
  {
    "text": "and then also just the query that we've used to create these charts so it looks complicated but essentially",
    "start": "1572880",
    "end": "1580080"
  },
  {
    "text": "it's very very simple and we're just calculating arrival latency here and deducting the insert",
    "start": "1580080",
    "end": "1587200"
  },
  {
    "text": "time the window timestamp from the insert time in order to get that arrival",
    "start": "1587200",
    "end": "1592960"
  },
  {
    "text": "latency metric",
    "start": "1592960",
    "end": "1596440"
  },
  {
    "text": "what you see before you now is again the logo for apache superset and we were seeing that quite a lot um while our",
    "start": "1598240",
    "end": "1605360"
  },
  {
    "text": "queries were loading for a really long time and what i want to do now is just give a",
    "start": "1605360",
    "end": "1612159"
  },
  {
    "start": "1612000",
    "end": "1612000"
  },
  {
    "text": "little shout out to my dad my dad who was a dba who is a dba and programmer um",
    "start": "1612159",
    "end": "1618559"
  },
  {
    "text": "for 20 years more than me and when i came to my dad with a query that was running for one minute",
    "start": "1618559",
    "end": "1625440"
  },
  {
    "text": "he didn't give up and he made it run for less than 10 seconds so thanks dad who's probably",
    "start": "1625440",
    "end": "1631520"
  },
  {
    "text": "watching so how did we do it how did me and my dad optimize a data auditing table to be",
    "start": "1631520",
    "end": "1639039"
  },
  {
    "start": "1633000",
    "end": "1633000"
  },
  {
    "text": "able to give us answers in a reasonable time so the first couple of things we did",
    "start": "1639039",
    "end": "1644240"
  },
  {
    "text": "were pretty basic we've defined weekly partitioning which is not as easy as it sounds with the",
    "start": "1644240",
    "end": "1650559"
  },
  {
    "text": "password sequel and of course we created um indexes so unique indexes and",
    "start": "1650559",
    "end": "1657279"
  },
  {
    "text": "complementary indexes according to the queries and their predicates",
    "start": "1657279",
    "end": "1662320"
  },
  {
    "text": "there's another set of variables that if you use postgresql especially on rds in",
    "start": "1662320",
    "end": "1668000"
  },
  {
    "text": "amazon i really recommend you take a look at them so these are max",
    "start": "1668000",
    "end": "1673200"
  },
  {
    "text": "worker processes max parallel workers and the max parallel workers per gather",
    "start": "1673200",
    "end": "1678399"
  },
  {
    "text": "and so these are the ones that really improve your performance especially when you run your queries in a distributed",
    "start": "1678399",
    "end": "1685120"
  },
  {
    "text": "way and so you would be amazed at what this can do to your query when you use",
    "start": "1685120",
    "end": "1690320"
  },
  {
    "text": "10 workers instead of two last but not least i know that we all",
    "start": "1690320",
    "end": "1696559"
  },
  {
    "text": "think that the optimizer knows best but you'll be amazed as how you can trick the optimizer if you just use a",
    "start": "1696559",
    "end": "1704000"
  },
  {
    "text": "union instead of an in predicate when you have several values that you want to compare to i really encourage you to",
    "start": "1704000",
    "end": "1710640"
  },
  {
    "text": "check that so if you're wondering if you can",
    "start": "1710640",
    "end": "1715840"
  },
  {
    "start": "1713000",
    "end": "1713000"
  },
  {
    "text": "actually create weekly partitioning in postgres 11 i guess that's something",
    "start": "1715840",
    "end": "1721360"
  },
  {
    "text": "that's something something pretty basic that we have in most databases today",
    "start": "1721360",
    "end": "1726559"
  },
  {
    "text": "the answer is definitely no you can't do that with an automatic fashion and you will have to",
    "start": "1726559",
    "end": "1733039"
  },
  {
    "text": "create a scheduled process to do that i used apache airflow so apache airflow",
    "start": "1733039",
    "end": "1740159"
  },
  {
    "text": "is the set is the scheduler that we'd use at nielsen israel",
    "start": "1740159",
    "end": "1745200"
  },
  {
    "text": "and the way i did that was just by using very strict standards for my partition names",
    "start": "1745200",
    "end": "1750880"
  },
  {
    "text": "so just using pretty basic queries in your dag in your in your workflow that you schedule you",
    "start": "1750880",
    "end": "1757679"
  },
  {
    "text": "can create these partitions on a weekly basis",
    "start": "1757679",
    "end": "1763960"
  },
  {
    "start": "1765000",
    "end": "1765000"
  },
  {
    "text": "the last thing we did was just offloading data to history being a data company being a data",
    "start": "1766000",
    "end": "1772960"
  },
  {
    "text": "engineer i don't really enjoy throwing data away so i said hey instead of deleting all of",
    "start": "1772960",
    "end": "1778159"
  },
  {
    "text": "the old data i can just offload it and then you can just actually create a job",
    "start": "1778159",
    "end": "1783360"
  },
  {
    "text": "to your liking with the metrics that you like and aggregate on on a different level of granularity you",
    "start": "1783360",
    "end": "1790000"
  },
  {
    "text": "don't have to actually keep it with the auditing timestamp so just aggregating on an",
    "start": "1790000",
    "end": "1795600"
  },
  {
    "text": "hourly granularity or on a daily granularity and just keeping all of that historical data",
    "start": "1795600",
    "end": "1803278"
  },
  {
    "text": "nice so let's get to the point do you think that we can actually answer that end of the",
    "start": "1805200",
    "end": "1811679"
  },
  {
    "text": "day question now spoiler alert we can",
    "start": "1811679",
    "end": "1817039"
  },
  {
    "text": "and i have that pseudo code here for you so the first thing that we're going to",
    "start": "1817039",
    "end": "1822399"
  },
  {
    "text": "do is check the validity of the input parameters but i'm just kidding we're not we're not going to do that",
    "start": "1822399",
    "end": "1828240"
  },
  {
    "text": "what we're going to do is just see what are the main questions that we need answered in order to see if the day is",
    "start": "1828240",
    "end": "1835440"
  },
  {
    "text": "over so the first one is going to be what's the data arrival rate for the entire",
    "start": "1835440",
    "end": "1840960"
  },
  {
    "text": "scope so if we're checking for yesterday and the data arrival rate is below our",
    "start": "1840960",
    "end": "1846240"
  },
  {
    "text": "threshold then for sure the day is not over and we don't need to check anything else",
    "start": "1846240",
    "end": "1852240"
  },
  {
    "text": "but let's say that the data arrival rate is just fine the second thing we're going to check is",
    "start": "1852240",
    "end": "1858159"
  },
  {
    "text": "the number of audit windows because if the number of audit windows is below what we'd expect",
    "start": "1858159",
    "end": "1864559"
  },
  {
    "text": "then there might be a problem with the data auditing subsystem and we can't really trust it",
    "start": "1864559",
    "end": "1870880"
  },
  {
    "text": "but let's say that the data arrival rate is just fine and also the number of auditing window",
    "start": "1870880",
    "end": "1876640"
  },
  {
    "text": "is also okay the last thing that we're going to check is the data arrival rate for the very",
    "start": "1876640",
    "end": "1883519"
  },
  {
    "text": "last window so in our case the window for five minutes before midnight",
    "start": "1883519",
    "end": "1889120"
  },
  {
    "text": "across all of our auditing windows why are we doing that that's pretty",
    "start": "1889120",
    "end": "1894880"
  },
  {
    "text": "weird well if you remember when we looked at the super superset charts",
    "start": "1894880",
    "end": "1900720"
  },
  {
    "text": "i told you that sometimes we'd get a data arrival rate that was higher than 150 or 200.",
    "start": "1900720",
    "end": "1907760"
  },
  {
    "text": "sometimes we'd get a data arrival rate that was really really low so at this point we realized so that we",
    "start": "1907760",
    "end": "1915120"
  },
  {
    "text": "might get a calculation as q in our calculation of the arrival rate",
    "start": "1915120",
    "end": "1920640"
  },
  {
    "text": "so we said okay let's use the arrival rate for the very",
    "start": "1920640",
    "end": "1926000"
  },
  {
    "text": "last window just as an indicator to whether the date was over or not",
    "start": "1926000",
    "end": "1932720"
  },
  {
    "text": "so only if the data arrival rate for the entire scope is just fine as the and the number of audit windows",
    "start": "1932720",
    "end": "1939840"
  },
  {
    "text": "for the entire scope is also okay but also the arrival rate for the very",
    "start": "1939840",
    "end": "1945760"
  },
  {
    "text": "last window is also great only then",
    "start": "1945760",
    "end": "1951039"
  },
  {
    "text": "yes the day is actually over",
    "start": "1951039",
    "end": "1955679"
  },
  {
    "start": "1956000",
    "end": "1956000"
  },
  {
    "text": "so now that we can answer that question and we know that it is the end of the day",
    "start": "1957120",
    "end": "1962880"
  },
  {
    "text": "what we wanted to do was give our developers our big data developers the ability",
    "start": "1962880",
    "end": "1967919"
  },
  {
    "text": "to schedule the data processors schedule the jobs according to the answer of the end of the day",
    "start": "1967919",
    "end": "1974720"
  },
  {
    "text": "that we've created a very simple end of the day api um what you give that api is a simple",
    "start": "1974720",
    "end": "1981919"
  },
  {
    "text": "question so for a topic for scope right for a date and for a specific threshold",
    "start": "1981919",
    "end": "1988799"
  },
  {
    "text": "is my day over or not and the answer would look something like",
    "start": "1988799",
    "end": "1993840"
  },
  {
    "start": "1993000",
    "end": "1993000"
  },
  {
    "text": "this so for example this is not an end of the day sad smiley face um",
    "start": "1993840",
    "end": "2001200"
  },
  {
    "text": "why because basically the current arrival rate is below our threshold",
    "start": "2001200",
    "end": "2007360"
  },
  {
    "text": "um but here the ris the reason is different so we can see that the reason is just that",
    "start": "2008320",
    "end": "2015200"
  },
  {
    "text": "the last window did not arrive entirely yet",
    "start": "2015200",
    "end": "2019840"
  },
  {
    "text": "and then here yes our day is definitely over the threshold is fine the number of windows is fine",
    "start": "2020799",
    "end": "2027679"
  },
  {
    "text": "and also that very last window arrived entirely",
    "start": "2027679",
    "end": "2033799"
  },
  {
    "start": "2034000",
    "end": "2034000"
  },
  {
    "text": "cool so at this point i just want to start discussing alerts and add-ons because having all of our data we can schedule",
    "start": "2034799",
    "end": "2042559"
  },
  {
    "text": "our processors we can see if it's the end of the day but we're not done yet because we were",
    "start": "2042559",
    "end": "2048079"
  },
  {
    "text": "having all of these failures we were having trouble investigating this huge architecture",
    "start": "2048079",
    "end": "2055520"
  },
  {
    "start": "2055000",
    "end": "2055000"
  },
  {
    "text": "so what we did we've defined a couple of sql queries as the one that i've showed you before",
    "start": "2055520",
    "end": "2061919"
  },
  {
    "text": "in order to actually define alerts so how would these alerts look like",
    "start": "2061919",
    "end": "2068158"
  },
  {
    "text": "exactly like this and this is a real start for me because these alerts were popping up",
    "start": "2068159",
    "end": "2074878"
  },
  {
    "text": "faster than any of the heartbeat alerts of spark or kafka than any of the failure alerts of our jobs these were",
    "start": "2074879",
    "end": "2082320"
  },
  {
    "text": "getting there first and that problem with our eu replicator when it's being stuck on some 10 or 50",
    "start": "2082320",
    "end": "2089440"
  },
  {
    "text": "partitions no one could discover that no no no alerts only the data auditing",
    "start": "2089440",
    "end": "2094560"
  },
  {
    "text": "alert and moreover when you see that you have an alert on a specific region you can",
    "start": "2094560",
    "end": "2101359"
  },
  {
    "text": "know for sure that the problem is with the eu replicator not with your kafka not with a data loader if you see that",
    "start": "2101359",
    "end": "2108640"
  },
  {
    "text": "you're getting an alert on a specific topic then you know that the problem is probably with the data loader and not",
    "start": "2108640",
    "end": "2115760"
  },
  {
    "text": "with your view replicator so this really made our life",
    "start": "2115760",
    "end": "2121599"
  },
  {
    "text": "a lot better and now we can discuss detecting",
    "start": "2121599",
    "end": "2127200"
  },
  {
    "start": "2123000",
    "end": "2123000"
  },
  {
    "text": "duplications so if you remember when we looked at the auditing header i told you hey here's a",
    "start": "2127200",
    "end": "2133839"
  },
  {
    "text": "sequence id but that's a bit of a spoiler well we're going to discuss it now",
    "start": "2133839",
    "end": "2139359"
  },
  {
    "text": "when we started seeing these arrival rates that were higher than 100 we said",
    "start": "2139359",
    "end": "2144400"
  },
  {
    "text": "okay we probably don't have data duplication the problem is probably with a data",
    "start": "2144400",
    "end": "2150079"
  },
  {
    "text": "auditing subsystem so we thought about a way of how to check that and what we did we simply",
    "start": "2150079",
    "end": "2156960"
  },
  {
    "text": "added a sequence id so that would be a running sequence id for every audit header so your audit",
    "start": "2156960",
    "end": "2163599"
  },
  {
    "text": "header is unique right for every window for every producer and when you edit the sequence id it really makes that audit",
    "start": "2163599",
    "end": "2170079"
  },
  {
    "text": "header for a specific message unique so in order to detect duplications",
    "start": "2170079",
    "end": "2175760"
  },
  {
    "text": "because all of our data arrives to s3 all you need to do is just read your data using spark",
    "start": "2175760",
    "end": "2183119"
  },
  {
    "text": "into a data frame right it's an audit header it's in parquet format and then if you aggregate on the audit",
    "start": "2183119",
    "end": "2190000"
  },
  {
    "text": "header and count the distinct number of sequence ids you get the amount of duplications",
    "start": "2190000",
    "end": "2197280"
  },
  {
    "text": "we were having duplications it was not a problem with the data auditing subsystem",
    "start": "2197359",
    "end": "2203599"
  },
  {
    "text": "and with this i'm done so if anyone has any questions i'd be really happy to",
    "start": "2204079",
    "end": "2209359"
  },
  {
    "text": "answer okay then not",
    "start": "2209359",
    "end": "2215119"
  },
  {
    "text": "so thank you for coming and enjoy the rest of your day",
    "start": "2215119",
    "end": "2221320"
  },
  {
    "text": "you",
    "start": "2230800",
    "end": "2232880"
  }
]