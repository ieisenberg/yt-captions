[
  {
    "text": "okay so hi everyone I'm I'm David I'm the architect for proof points",
    "start": "4750",
    "end": "10760"
  },
  {
    "text": "information protection product we build",
    "start": "10760",
    "end": "15799"
  },
  {
    "text": "the service that provides security for data on very skilled providers like Google Drive s3 stuff like that we",
    "start": "15799",
    "end": "23480"
  },
  {
    "text": "detect information leaks and in our authorized access and internally pretty",
    "start": "23480",
    "end": "28670"
  },
  {
    "text": "much is just we bring in a bunch of data from different API is from cloud providers and we do a bunch of data processing which means that we have",
    "start": "28670",
    "end": "35899"
  },
  {
    "text": "something like 50 different services doing all kinds of application logic on",
    "start": "35899",
    "end": "41060"
  },
  {
    "text": "the data we bring in and they're spread across hundreds of different machines and to scale a different at different",
    "start": "41060",
    "end": "49160"
  },
  {
    "text": "speeds and they have different bottlenecks Simon CPU some IO bound so we do just lots of production were",
    "start": "49160",
    "end": "56540"
  },
  {
    "text": "clouds and so when I came to proof point about six months ago and the",
    "start": "56540",
    "end": "62780"
  },
  {
    "text": "architectures project pretty much every engineer came up to me at some point and asks me this question",
    "start": "62780",
    "end": "68960"
  },
  {
    "text": "so whenever going to do containers like I said all right well optimistically probably six months from now",
    "start": "68960",
    "end": "75729"
  },
  {
    "text": "pessimistically maybe a year and it all be shocked like what we're talking about I can do docker run my machine right I",
    "start": "75729",
    "end": "82220"
  },
  {
    "text": "have I have my service I wrapped it in a docker file I can just run it in collection right now like give me ten",
    "start": "82220",
    "end": "87530"
  },
  {
    "text": "seconds and the keys to the to the Amazon website web servers now I'll do Dockers right now I say no I'll hang on",
    "start": "87530",
    "end": "94070"
  },
  {
    "text": "have you considered how you're going to monitor this thing say well no and how",
    "start": "94070",
    "end": "99530"
  },
  {
    "text": "are you going to collect logs from New York container I know we'll figure something out and how are you gonna scale it right and what are you gonna",
    "start": "99530",
    "end": "105799"
  },
  {
    "text": "build and how you can update images and there's like 50 different things and they'd all look dejected like I kicked",
    "start": "105799",
    "end": "111650"
  },
  {
    "text": "their puppy or something and and every movie and up be disappointed right because we can go to containers anytime",
    "start": "111650",
    "end": "118610"
  },
  {
    "text": "soon and so this is how this talk came about really which is like this conversation which is I'm guessing very",
    "start": "118610",
    "end": "124790"
  },
  {
    "text": "common in the industry which is there's a there exists a gap between using docker or using containers on your",
    "start": "124790",
    "end": "131599"
  },
  {
    "text": "machine and actually running stuff in the real world in production at",
    "start": "131599",
    "end": "137020"
  },
  {
    "text": "our scale right and there's a bunch of things you to consider you don't",
    "start": "137020",
    "end": "142900"
  },
  {
    "text": "necessarily have to do them and a lot of them are DevOps oriented so I'm kind of going to kind of ignore the devil stuff",
    "start": "142900",
    "end": "149140"
  },
  {
    "text": "right because we're in the developer conference I'm guessing most of your developers raise hands who's a developer",
    "start": "149140",
    "end": "154710"
  },
  {
    "text": "pretty much everybody or or no one willing to admit that there are DevOps anyway so yeah so I'm gonna skip the",
    "start": "154710",
    "end": "162400"
  },
  {
    "text": "devil see stuff because there's a bunch of dev stuff that needs to be done before it can go from writing a docker",
    "start": "162400",
    "end": "167860"
  },
  {
    "text": "file on your laptop to actually running a containerized service in production at",
    "start": "167860",
    "end": "173110"
  },
  {
    "text": "high scale stable you know with updates and all the other stuff this is what",
    "start": "173110",
    "end": "178540"
  },
  {
    "text": "we're gonna be talking about today and all the lessons we learned in the six months it actually took to roll out the",
    "start": "178540",
    "end": "184120"
  },
  {
    "text": "first container into production so the first thing you do when you approach by",
    "start": "184120",
    "end": "190090"
  },
  {
    "text": "the way let me ask you this who has used docker pretty much everybody excellent",
    "start": "190090",
    "end": "195580"
  },
  {
    "text": "you can all go no I'm kidding please stay who works at a company that has",
    "start": "195580",
    "end": "201760"
  },
  {
    "text": "containers in production not testing production okay a lot fewer hands there there's like 20 hands out of 80 right",
    "start": "201760",
    "end": "208780"
  },
  {
    "text": "who has a proper managed kubernetes cluster in production it's like one guy",
    "start": "208780",
    "end": "214600"
  },
  {
    "text": "- okay good you're the right audience for this so you're all sitting on this",
    "start": "214600",
    "end": "220180"
  },
  {
    "text": "gap of okay well I know what doctor is I can use docker how do I run this in production and so the first thing we're",
    "start": "220180",
    "end": "225730"
  },
  {
    "text": "gonna talk about is how you actually build containers right because obviously you can do doctor build I'm trusting",
    "start": "225730",
    "end": "231459"
  },
  {
    "text": "everybody here is very intelligent and you can Google how to build a docker container and discover how to do that",
    "start": "231459",
    "end": "238080"
  },
  {
    "text": "but there's a difference between doing that and actually having containers that are appropriate to run in production so",
    "start": "238080",
    "end": "245050"
  },
  {
    "text": "I'm gonna show you just one thing as a first demo because I like demos I think the first thing you need to do is show",
    "start": "245050",
    "end": "251050"
  },
  {
    "text": "something useful all right so I'm guessing if so many people showed hands they know how to use docker yeah you can",
    "start": "251050",
    "end": "256840"
  },
  {
    "text": "recognize a docker file all right this is docker file for a very simple dotnet core application all it does is run",
    "start": "256840",
    "end": "263500"
  },
  {
    "text": "HelloWorld and as you can see we use the Dutton SDK base image right I'm sure",
    "start": "263500",
    "end": "268720"
  },
  {
    "text": "everyone's familiar with base images you the dockerfile you can specify what darker image is going to be built from",
    "start": "268720",
    "end": "274570"
  },
  {
    "text": "all right in this case is the official microsoft.net 2.1 sdk which lets me compile and build a dotnet core program",
    "start": "274570",
    "end": "283570"
  },
  {
    "text": "and all it does it's copy my code code over run build on it and then run the application that's all it does so it's",
    "start": "283570",
    "end": "288850"
  },
  {
    "text": "very very straightforward obviously we can build the image right so let's make it a bit bigger yeah Kirby build just",
    "start": "288850",
    "end": "300580"
  },
  {
    "text": "build the whole thing it builds very quickly because I read that I've done this before so let's do docker images",
    "start": "300580",
    "end": "307120"
  },
  {
    "text": "grab four all right so we can see our image right and the first thing we",
    "start": "307120",
    "end": "312580"
  },
  {
    "text": "notice is this image is almost two gigabytes large because it's based on the.net core 2.1 sdk which is itself is",
    "start": "312580",
    "end": "320290"
  },
  {
    "text": "almost two gigabytes large so if we start pushing these images in production with updates every now and then first",
    "start": "320290",
    "end": "325720"
  },
  {
    "text": "we're gonna have to store them in our repo waiting some registry or whatever a pre is set up and that's a large image",
    "start": "325720",
    "end": "331240"
  },
  {
    "text": "to push the production and it takes a while to build and run and it contains a whole bunch of stuff we don't actually",
    "start": "331240",
    "end": "337000"
  },
  {
    "text": "need in order to run this application in production most of these two gigabytes is just the environment to build the",
    "start": "337000",
    "end": "344110"
  },
  {
    "text": "application so this is why doctor introduced phased builds a while ago or multistage builds and so the first thing",
    "start": "344110",
    "end": "350230"
  },
  {
    "text": "we do is we just lets colorize it",
    "start": "350230",
    "end": "355500"
  },
  {
    "text": "so does anyone is anyone familiar with multistage bills are you using it look a",
    "start": "356670",
    "end": "364120"
  },
  {
    "text": "few people alright so this is a new thing it's out about six months ago docker 11 I think 17 I think and so what",
    "start": "364120",
    "end": "370780"
  },
  {
    "text": "you can do is you can specify different building images for the build stage and for the actual execution right and so",
    "start": "370780",
    "end": "376630"
  },
  {
    "text": "basically it will compile your application in one image and then copy the artifact that gets created and it",
    "start": "376630",
    "end": "382720"
  },
  {
    "text": "will run it in another image which is hopefully much smaller so there's only two lines that we added right so we say",
    "start": "382720",
    "end": "388540"
  },
  {
    "text": "we still build it in this data 2.1 SDK but then we say ok we'll take this other",
    "start": "388540",
    "end": "394270"
  },
  {
    "text": "building based image which is nothing 2.0 2.1 runtime which doesn't include any of the build environment will copy",
    "start": "394270",
    "end": "401530"
  },
  {
    "text": "the output from the previous image into this and and save that and this is the one we're gonna use and when we do this all right",
    "start": "401530",
    "end": "409629"
  },
  {
    "text": "so all right we run this and we build",
    "start": "409629",
    "end": "416259"
  },
  {
    "text": "this image gets built just as quickly and so now we do docker images grab",
    "start": "416259",
    "end": "423279"
  },
  {
    "text": "multi and get an image that's 180 megabytes exactly one-tenth the size of",
    "start": "423279",
    "end": "428769"
  },
  {
    "text": "the previous image which is much more manageable and obviously I picked it dot net because it's a very extreme example like the base image is huge but it works",
    "start": "428769",
    "end": "436869"
  },
  {
    "text": "pretty much the same across most applications right well there is golang or Java or Scala whatever you have a",
    "start": "436869",
    "end": "442119"
  },
  {
    "text": "giant build tools image you have a much smaller runtime environment so it's the thing that's very trivial to do but you",
    "start": "442119",
    "end": "447789"
  },
  {
    "text": "really need to do this obviously this is something you do once and you forget about it but it's important to actually pay",
    "start": "447789",
    "end": "453099"
  },
  {
    "text": "attention to small things like this and obviously docker files are code just like any other code you need to maintain",
    "start": "453099",
    "end": "458589"
  },
  {
    "text": "usually you put them in the same source control as the actual application and you deploy and you store them at the",
    "start": "458589",
    "end": "464800"
  },
  {
    "text": "same time so this is just the first thing and I know it's a small thing but I'm starting something light now if you",
    "start": "464800",
    "end": "472990"
  },
  {
    "text": "have 50 different services written in three different languages as we do it still takes a lot of work to actually go",
    "start": "472990",
    "end": "478509"
  },
  {
    "text": "and wrap each one and container right and you don't want to go and handcraft a docker file for each one of them you",
    "start": "478509",
    "end": "483579"
  },
  {
    "text": "need to build some kind of tool chain which generates the docker files automatically hopefully and there's some",
    "start": "483579",
    "end": "488709"
  },
  {
    "text": "stuff to be built around this all right this is just the first thing now that we",
    "start": "488709",
    "end": "494289"
  },
  {
    "text": "have our build image image and we have something around in production the next question comes okay well how do I run",
    "start": "494289",
    "end": "500589"
  },
  {
    "text": "this right obviously doing it on my laptop is easier I do docker run give it the name of the image and it runs if you",
    "start": "500589",
    "end": "508179"
  },
  {
    "text": "want to do it in production you have a few things to consider first of all is it going to be on virtual machines or is it going to be on bare metal right we at",
    "start": "508179",
    "end": "514990"
  },
  {
    "text": "proof point we use Amazon for everything so it's pretty much a no-brainer when I'd be using Amazon for this as well and",
    "start": "514990",
    "end": "521409"
  },
  {
    "text": "then the question comes okay well do we just run containers directly so we can because we can do docker run on the",
    "start": "521409",
    "end": "527769"
  },
  {
    "text": "machine do we use some kind of Orchestrator specifically Dockers worm or kubernetes",
    "start": "527769",
    "end": "533890"
  },
  {
    "text": "or do we spin up some kind of knowledge service pretty much every major platform today has some kind of managed container",
    "start": "533890",
    "end": "540190"
  },
  {
    "text": "service there's Amazon Elastic containers there's Google Cloud manage kubernetes",
    "start": "540190",
    "end": "545290"
  },
  {
    "text": "all kinds all right and so we sat down with our DevOps we brainstormed and the",
    "start": "545290",
    "end": "552070"
  },
  {
    "text": "first line write the standalone containers that was a legitimate option we considered because in our case our",
    "start": "552070",
    "end": "557560"
  },
  {
    "text": "main bottleneck is DevOps resources we have a lot more developers than we do DevOps and so we look for things we can",
    "start": "557560",
    "end": "564610"
  },
  {
    "text": "actually do quickly to get benefits from running containers without spending six",
    "start": "564610",
    "end": "569769"
  },
  {
    "text": "months just doing DevOps works to spin up a managed instance of something",
    "start": "569769",
    "end": "574920"
  },
  {
    "text": "ultimately we discarded this idea mostly because we were afraid to get locked in into this suboptimal deployment and we",
    "start": "574920",
    "end": "582640"
  },
  {
    "text": "just enshrined something we were initially not happy about and so we threw that idea away and then it was a",
    "start": "582640",
    "end": "588130"
  },
  {
    "text": "question of okay do we do darker and darker swarm or do we do kubernetes in this case Google is one pretty much",
    "start": "588130",
    "end": "593529"
  },
  {
    "text": "pretty easily it's much more complicated and complex and it also for that reason",
    "start": "593529",
    "end": "600160"
  },
  {
    "text": "it has a lot more options which we actually like it has things that address automatic scaling and better health",
    "start": "600160",
    "end": "606339"
  },
  {
    "text": "monitoring than docker swarm and it has a stateful sets which lets you run",
    "start": "606339",
    "end": "612130"
  },
  {
    "text": "stateful applications much easier in in containers which I'll talk about in just a bit so overall it's much more",
    "start": "612130",
    "end": "618850"
  },
  {
    "text": "complicated there's a much higher learning curve to actually using kubernetes both for the developers and",
    "start": "618850",
    "end": "624130"
  },
  {
    "text": "for actually deploying that has DevOps and ultimately this thing that actually",
    "start": "624130",
    "end": "629440"
  },
  {
    "text": "changed it for us in favor of kubernetes is the only cloud provider which gives us managed docker swarm is docker club",
    "start": "629440",
    "end": "636399"
  },
  {
    "text": "itself and so there's a single provider and we didn't want to get locked into",
    "start": "636399",
    "end": "641709"
  },
  {
    "text": "that whereas pretty much every major platform today has kubernetes as a service in fact amazon AWS came out with",
    "start": "641709",
    "end": "648880"
  },
  {
    "text": "kubernetes as a service just last week on June 6 I think right which is pretty new so as your had managed kubernetes",
    "start": "648880",
    "end": "656550"
  },
  {
    "text": "since last year I think October last year or so Google have had manage kubernetes forever pretty much and so",
    "start": "656550",
    "end": "663100"
  },
  {
    "text": "even though right now we're looking at actually deploying our own kubernetes cluster the goal is eventually to migrate to a",
    "start": "663100",
    "end": "669140"
  },
  {
    "text": "managed kubernetes cluster and save all the management overhead from running error and monitoring our own cluster and",
    "start": "669140",
    "end": "674300"
  },
  {
    "text": "give it to someone else and just pay for the service that's one of the reasons for example we wouldn't go with Amazon",
    "start": "674300",
    "end": "680660"
  },
  {
    "text": "container service if anyone played with the Amazon ECS it's built on top of cloud formation deployment scripts which",
    "start": "680660",
    "end": "687860"
  },
  {
    "text": "is like its own different language which has nothing to do with other either darker or kubernetes and if we build and",
    "start": "687860",
    "end": "693560"
  },
  {
    "text": "automate all this infrastructure around Amazon container services and we want to go to kubernetes eventually we're gonna",
    "start": "693560",
    "end": "699560"
  },
  {
    "text": "have to rewrite the whole thing from scratch whereas if you just do flow Cobras over our own right now build all the",
    "start": "699560",
    "end": "704990"
  },
  {
    "text": "automations around that if we move to a managed kubernetes instance it's gonna save some management overhead when we",
    "start": "704990",
    "end": "710960"
  },
  {
    "text": "actually have to rewrite all over it def deployment to change at all so that's that's pretty much the part that",
    "start": "710960",
    "end": "716630"
  },
  {
    "text": "actually sold it for us even though it has the highest initial cost of setting it up right so actually deploying",
    "start": "716630",
    "end": "723350"
  },
  {
    "text": "kubernetes is a large job mostly DevOps and DevOps related once you have that",
    "start": "723350",
    "end": "729080"
  },
  {
    "text": "then you can start talking about all the other things on the dev side of how you actually run stuff on kubernetes so",
    "start": "729080",
    "end": "734480"
  },
  {
    "text": "let's take a look in general there's three ways to run things on kubernetes one you can just use commands directly",
    "start": "734480",
    "end": "740990"
  },
  {
    "text": "right you can go and you can do kubernetes run same as we would do with docker and give it a name of the",
    "start": "740990",
    "end": "746300"
  },
  {
    "text": "container and it will actually run so for those who aren't familiar when",
    "start": "746300",
    "end": "751430"
  },
  {
    "text": "you're on kubernetes you have a kubernetes cluster by the way on my laptop i'm using mini cube if you want to play around with communities I",
    "start": "751430",
    "end": "756800"
  },
  {
    "text": "strongly recommend you download and install mini cube it's it's very very interesting to just play around with and",
    "start": "756800",
    "end": "763640"
  },
  {
    "text": "then you use the command line tool called cube CTL which is like the command line interface to talk to your kubernetes instance and you can do all",
    "start": "763640",
    "end": "770090"
  },
  {
    "text": "the normal stuff like you still get pods we don't have any yet but we'll build some in just a moment and so you can",
    "start": "770090",
    "end": "777860"
  },
  {
    "text": "submit commands directly to the cluster that's actually not very sustainable if you want to look at you know automation and",
    "start": "777860",
    "end": "783590"
  },
  {
    "text": "DevOps tooling so the second thing everyone looks at is actually writing deployment",
    "start": "783590",
    "end": "789200"
  },
  {
    "text": "files usually as the anvil file or Jason files and the deployment file describes what you want to deploy that's very",
    "start": "789200",
    "end": "795650"
  },
  {
    "text": "similar to docker in docker there's a file format called docker compose files describe all of your services in that so",
    "start": "795650",
    "end": "802370"
  },
  {
    "text": "let's take a look at one example of how you would do that I have a very silly nodejs application which is does hello",
    "start": "802370",
    "end": "808460"
  },
  {
    "text": "world we don't care what the application we just want to look at how you deploy that right so we create a llamo file",
    "start": "808460",
    "end": "814040"
  },
  {
    "text": "called deployment llamo there's a format there's several different file different",
    "start": "814040",
    "end": "819920"
  },
  {
    "text": "files for kubernetes one for pods which are collections of containers deployment is a higher level abstraction it can",
    "start": "819920",
    "end": "825560"
  },
  {
    "text": "have multiple pods so multiple services talking to each other in the single deployment and above that we have a",
    "start": "825560",
    "end": "831740"
  },
  {
    "text": "service which Explorer exposes parts of your deployment to an external endpoint so it gives it an external API and and a",
    "start": "831740",
    "end": "840020"
  },
  {
    "text": "load balancer on top of it and you can actually access the deployment from outside the cluster so here we're going",
    "start": "840020",
    "end": "845390"
  },
  {
    "text": "to deploy whatever it's called node app right and here we can specify how exactly we're going to deploy this so we",
    "start": "845390",
    "end": "851630"
  },
  {
    "text": "can specify replicas we're gonna run run one replicas for now and we'll change it later what are we gonna run we're gonna",
    "start": "851630",
    "end": "857570"
  },
  {
    "text": "run an image called node app 11.0 this is actually in our doctor repository",
    "start": "857570",
    "end": "864050"
  },
  {
    "text": "right now exposing port sixty sixty and then we can specify a bunch of stuff which you can do both in kubernetes and",
    "start": "864050",
    "end": "871010"
  },
  {
    "text": "in docker it's just more more advanced in docker and sorry in kubernetes can",
    "start": "871010",
    "end": "876530"
  },
  {
    "text": "specify a few limits this is Amelia MIDI chords so 1000 million cores is one core",
    "start": "876530",
    "end": "882500"
  },
  {
    "text": "so this will actually be using one-tenth of one CPU core and 64 megabytes of RAM",
    "start": "882500",
    "end": "888080"
  },
  {
    "text": "this is the maximum amount of resources they can use this is what requests initially so I can actually tell and",
    "start": "888080",
    "end": "893660"
  },
  {
    "text": "give it less you can use I don't know 32 megabytes of RAM initially and then is requests more it can go up to 64 we can",
    "start": "893660",
    "end": "903290"
  },
  {
    "text": "specify things like health checks this is one this is one of the things I actually like in kubernetes you can specify health checks as a rest endpoint",
    "start": "903290",
    "end": "909380"
  },
  {
    "text": "you can't do that in darker darker actually runs a command in the in the container itself to do health checks",
    "start": "909380",
    "end": "915380"
  },
  {
    "text": "which is kind of annoying and not sure what to do about that because if you have a rest endpoint on all of your",
    "start": "915380",
    "end": "921140"
  },
  {
    "text": "services for example we used to run all our services on VMs and all of them has a health check rest endpoint which we",
    "start": "921140",
    "end": "927650"
  },
  {
    "text": "would we just probe normally so migrating to kubernetes is very natural because it just does the same thing it goes to the health checks",
    "start": "927650",
    "end": "933870"
  },
  {
    "text": "and point on project for docker we had to do something weird like running curl at the end point and looking at the exit",
    "start": "933870",
    "end": "939990"
  },
  {
    "text": "code for curl and that also means you have to package curl in your base image which for example in the ductile example",
    "start": "939990",
    "end": "946560"
  },
  {
    "text": "there's no curl in the base image so you have to bake the tools to do the health checks into every production image which",
    "start": "946560",
    "end": "952980"
  },
  {
    "text": "is cumbersome and just not very pleasant to work with this is a thing we like in",
    "start": "952980",
    "end": "958080"
  },
  {
    "text": "kubernetes a lot so we specify this this is the health and point it'll get called every sir every 30 seconds and the",
    "start": "958080",
    "end": "966300"
  },
  {
    "text": "readiness check which is different this just specifies when the container is actually ready and this is when kubernetes considers",
    "start": "966300",
    "end": "973290"
  },
  {
    "text": "the container to be running until then it's in a sort of unready state all right okay specify a bunch of other",
    "start": "973290",
    "end": "978480"
  },
  {
    "text": "stuff things like mounting disks and more with networking and services and",
    "start": "978480",
    "end": "984090"
  },
  {
    "text": "stuff and some when you run this it'll spin up one container and we'll run our",
    "start": "984090",
    "end": "989310"
  },
  {
    "text": "node app right of course that's not enough because it will spin up the container and only expose it internally within the cluster if you want to",
    "start": "989310",
    "end": "995340"
  },
  {
    "text": "actually have the end point out accessible outside the cluster we have to also specify a service build a",
    "start": "995340",
    "end": "1000560"
  },
  {
    "text": "service on top of it service is much easier it's a another llamó file kind service it says okay well you take node",
    "start": "1000560",
    "end": "1007010"
  },
  {
    "text": "app you look at port 60 60 and you expose it through a load balancer so",
    "start": "1007010",
    "end": "1012440"
  },
  {
    "text": "however many containers we end up with this label note app they'll all be exposed with one load balancer the load",
    "start": "1012440",
    "end": "1018800"
  },
  {
    "text": "balancer it will run grobin through all the containers right and so from here on itself is quite easy supply deployment",
    "start": "1018800",
    "end": "1032630"
  },
  {
    "text": "camel this goes and creates a new container and then we can create a service right and so we can all look and",
    "start": "1032630",
    "end": "1040370"
  },
  {
    "text": "see if we have any container containers running so get pods we see that we do have one pod running as you can see",
    "start": "1040370",
    "end": "1045890"
  },
  {
    "text": "that's not in ready state yet it takes about 15 seconds for the initial ready check to actually be completed so",
    "start": "1045890",
    "end": "1052280"
  },
  {
    "text": "eventually this will there we go it is now ready if we had multiple different",
    "start": "1052280",
    "end": "1057560"
  },
  {
    "text": "types of containers in our deployment and we could specify that one of them needs the other to be ready before it",
    "start": "1057560",
    "end": "1063740"
  },
  {
    "text": "spins up for example if I have a set of micro services which pretty much everyone has days one service needs another certain",
    "start": "1063740",
    "end": "1071059"
  },
  {
    "text": "service to actually be available before it can come up three well example we have a config service the rest service",
    "start": "1071059",
    "end": "1076340"
  },
  {
    "text": "that exposes application configurations to all the other applications and an authentication service where which other",
    "start": "1076340",
    "end": "1081980"
  },
  {
    "text": "services used to generate auth tokens so both of those need to actually be up and available before any other service can",
    "start": "1081980",
    "end": "1087530"
  },
  {
    "text": "actually come up otherwise it'll come up try to get the identity fail and crash again right and so this is very",
    "start": "1087530",
    "end": "1093289"
  },
  {
    "text": "convenient we can specify how and in which order each one comes in this is",
    "start": "1093289",
    "end": "1098480"
  },
  {
    "text": "also very useful when you do rolling upgrades so you can't let's say you have ten instances of the same service you",
    "start": "1098480",
    "end": "1104870"
  },
  {
    "text": "won't be able you want the Commission to many of those before new ones become ready so you won't have an outage at any",
    "start": "1104870",
    "end": "1110210"
  },
  {
    "text": "point during the rolling upgrade okay so we have a service we can actually access it service no app this just tells me to",
    "start": "1110210",
    "end": "1121940"
  },
  {
    "text": "open it in a browser as you can see yes we have it is exposed on this particular port and it just says hello world it's",
    "start": "1121940",
    "end": "1128900"
  },
  {
    "text": "not a very interesting service I'll admit but it works okay so we can do all",
    "start": "1128900",
    "end": "1134090"
  },
  {
    "text": "kinds of exploring we can do describe on",
    "start": "1134090",
    "end": "1139100"
  },
  {
    "text": "the service no the app you can get all kinds of information from it the internal port the IP all that so this is",
    "start": "1139100",
    "end": "1145490"
  },
  {
    "text": "very very useful and at this point we have enough to work with we can start defining automation around this right so",
    "start": "1145490",
    "end": "1152570"
  },
  {
    "text": "we can have the deployment camel and the service camel as part of our code same as the docker file which builds the",
    "start": "1152570",
    "end": "1158929"
  },
  {
    "text": "actual container if the deployment refers to multiple services it makes",
    "start": "1158929",
    "end": "1164240"
  },
  {
    "text": "sense to have it sit outside the actual service and then a different rep OH which just talks about deployments it's",
    "start": "1164240",
    "end": "1170720"
  },
  {
    "text": "little cumbersome to manage right when you go and you change one service maybe add the port or so or change something",
    "start": "1170720",
    "end": "1176179"
  },
  {
    "text": "about it you have to go and remember to go and update the deployment which is in a different rep oh in in git it's just",
    "start": "1176179",
    "end": "1182809"
  },
  {
    "text": "the process thing you have to get used to it's pretty much because it makes more sense to separate deployment files it's more of a DevOps job this is by the",
    "start": "1182809",
    "end": "1189830"
  },
  {
    "text": "way a question that I get a lot where does the line cross between what's the",
    "start": "1189830",
    "end": "1195679"
  },
  {
    "text": "devil a deaf job and what's a DevOps job right you can actually pretty much we can specify a few things which are",
    "start": "1195679",
    "end": "1202440"
  },
  {
    "text": "clearly dev or devil sorry so writing the code writing the dockerfile clearly a developer job because you're you the",
    "start": "1202440",
    "end": "1208830"
  },
  {
    "text": "one who you're the one who knows what the service exposes how it works all that stuff right writing automation around actually",
    "start": "1208830",
    "end": "1215430"
  },
  {
    "text": "running containers right so running the deployment llamo file calling could bring cube CTL clearly a DevOps job",
    "start": "1215430",
    "end": "1221370"
  },
  {
    "text": "right because that's what they do managing the container all that stuff somewhere in the middle there stuff like buildings deployment right and",
    "start": "1221370",
    "end": "1227760"
  },
  {
    "text": "specifying the hardware requirements for the service so this is something that probably you will need to have some",
    "start": "1227760",
    "end": "1234450"
  },
  {
    "text": "process where dev and DevOps have to work together and specify okay so we know how the service we wrote behaves it",
    "start": "1234450",
    "end": "1240300"
  },
  {
    "text": "needs this much memory in this Mar CPU and the devil's will come in and say ok well are our machines have this many",
    "start": "1240300",
    "end": "1246750"
  },
  {
    "text": "cores in each one of them let's change the granularity of how we specify resources this is something that will have to be collaborative it really helps",
    "start": "1246750",
    "end": "1253860"
  },
  {
    "text": "to have your dev and DevOps in close communications at least it's not physically in the same location in our",
    "start": "1253860",
    "end": "1260010"
  },
  {
    "text": "case we actually have the dev end up stealing together mixed so they can throw stuff at each other you know if",
    "start": "1260010",
    "end": "1266070"
  },
  {
    "text": "things break which they do all right so that's the pretty much the short how do",
    "start": "1266070",
    "end": "1272520"
  },
  {
    "text": "you do stuff with kubernetes there's one extra thing there's a tool called helm for kubernetes it's how many people use",
    "start": "1272520",
    "end": "1279660"
  },
  {
    "text": "a any kind of package manager so NPM as they brew for Mac OS right all right",
    "start": "1279660",
    "end": "1287040"
  },
  {
    "text": "chocolate a for Windows or it's a package manager we go new chocolaty install wherever 7-zip and it goes in",
    "start": "1287040",
    "end": "1292890"
  },
  {
    "text": "download a package install 7-zip for you right our same thing with NPM so there's a tool similar to that for cuban",
    "start": "1292890",
    "end": "1299430"
  },
  {
    "text": "kubernetes packages it's called helm it has a server component called tiller which runs in your kubernetes cluster",
    "start": "1299430",
    "end": "1304620"
  },
  {
    "text": "and so you can use helm to submit deployment packages to tiller and",
    "start": "1304620",
    "end": "1309960"
  },
  {
    "text": "internally tiller will go and run all the cubes ETL commands for you it will actually tsukuba manages for you and execute all the deployments will",
    "start": "1309960",
    "end": "1316260"
  },
  {
    "text": "download images set up stuff it just saves you a lot of work and so deploying something becomes just a single line of",
    "start": "1316260",
    "end": "1321900"
  },
  {
    "text": "how install something right in our case there's a large library of pre-built",
    "start": "1321900",
    "end": "1327210"
  },
  {
    "text": "helm packages or you can build your own home package and so instead of having a bunch of deployment camera and service",
    "start": "1327210",
    "end": "1333570"
  },
  {
    "text": "Yama files around you'll have your own single repository of just charts for your home",
    "start": "1333570",
    "end": "1339600"
  },
  {
    "text": "and the deployment becomes ok I submit changes to the package and then the",
    "start": "1339600",
    "end": "1344760"
  },
  {
    "text": "automation calls helm install this new packaging of service 1.0 right works something like this",
    "start": "1344760",
    "end": "1350220"
  },
  {
    "text": "and so let's say right know pretty sure",
    "start": "1350220",
    "end": "1360210"
  },
  {
    "text": "we had WordPress or something [Music]",
    "start": "1360210",
    "end": "1366909"
  },
  {
    "text": "update you know we're gonna do this in the middle of a talk you can just trust",
    "start": "1374149",
    "end": "1380179"
  },
  {
    "text": "me it will install WordPress and it will be wonderful we can all blog about this experience together and then you can",
    "start": "1380179",
    "end": "1386719"
  },
  {
    "text": "delete it with helm deletes whatever right so pretty much the whole point is this is like the highest level of",
    "start": "1386719",
    "end": "1393169"
  },
  {
    "text": "abstraction you can have for deploying stuff in kubernetes where you don't even specify the deployments all the",
    "start": "1393169",
    "end": "1399859"
  },
  {
    "text": "deployments parameters you hide them within this home package it's very convenient we fill it's mostly overkill",
    "start": "1399859",
    "end": "1405949"
  },
  {
    "text": "for for for us we don't use it right now because it takes more work to actually",
    "start": "1405949",
    "end": "1411229"
  },
  {
    "text": "write helm packages then it does to generate deployment Campbell files which we just do with the script at some point",
    "start": "1411229",
    "end": "1417739"
  },
  {
    "text": "in the future if we have more services running in kubernetes we will probably go to helm for now just it's not very",
    "start": "1417739",
    "end": "1425899"
  },
  {
    "text": "useful for us where to talk with Jonas over there yesterday about that and he",
    "start": "1425899",
    "end": "1431119"
  },
  {
    "text": "mentioned one really good use case where I didn't think of which is if you have a lot of customers and you want to spin up",
    "start": "1431119",
    "end": "1436789"
  },
  {
    "text": "a dedicated instance of something for every one of your talents your customers II think it's actually very convenient to just wrap whatever environment you",
    "start": "1436789",
    "end": "1444199"
  },
  {
    "text": "want to spin up as a home chart and then you can go and do helm install this for every customer and it automates it very",
    "start": "1444199",
    "end": "1449869"
  },
  {
    "text": "easily which is pretty cool not useful for us but just a cool idea in general alright so this the counterpart to",
    "start": "1449869",
    "end": "1458409"
  },
  {
    "text": "kubernetes deployment scripts in docker would be a docker swarm which is the actual Orchestrator for darker which",
    "start": "1458409",
    "end": "1464959"
  },
  {
    "text": "takes care of scheduling containers and running stuff and Dockers warm uses",
    "start": "1464959",
    "end": "1470559"
  },
  {
    "text": "deployment files called docker compose files so we can take a look at one docker compose file just for comparison",
    "start": "1470559",
    "end": "1477019"
  },
  {
    "text": "it's mostly similar a compose file looks like this you see it's actually",
    "start": "1477019",
    "end": "1483289"
  },
  {
    "text": "conceptually very similar to what we had with kubernetes deployment we have the image right we're gonna run we have the",
    "start": "1483289",
    "end": "1490489"
  },
  {
    "text": "deployment parameters let's say we're on three three instances of that we can",
    "start": "1490489",
    "end": "1495829"
  },
  {
    "text": "specify the restart policy for the service if you want to do something we can specify I didn't do it here but",
    "start": "1495829",
    "end": "1501469"
  },
  {
    "text": "Hardware limits and we can specify other services it will depend on and so",
    "start": "1501469",
    "end": "1507220"
  },
  {
    "text": "for example we have two services here with a service called web and a service called Redis and so web will be able to",
    "start": "1507220",
    "end": "1512860"
  },
  {
    "text": "access Redis with the DNS name Redis this also automatically creates DNS",
    "start": "1512860",
    "end": "1518500"
  },
  {
    "text": "routing for every service so they can actually see each other and talk to each other this replaces with whatever",
    "start": "1518500",
    "end": "1524280"
  },
  {
    "text": "complex service discovery mechanism you might have or maybe not maybe in some",
    "start": "1524280",
    "end": "1529780"
  },
  {
    "text": "case it makes sense to actually use an external service discovery mechanism we still run with zookeeper and vault in",
    "start": "1529780",
    "end": "1535179"
  },
  {
    "text": "our case so this works pretty much the same way right and you go and you in darker stock deploy let's let's just try",
    "start": "1535179",
    "end": "1542140"
  },
  {
    "text": "one you know to actually do an a different example to make it interesting",
    "start": "1542140",
    "end": "1551700"
  },
  {
    "text": "one last thing to talk about is managed services right right now there's a",
    "start": "1553470",
    "end": "1558490"
  },
  {
    "text": "reason we didn't go to manage kubernetes on Amazon it came out last week it's not even available in the data center where",
    "start": "1558490",
    "end": "1565059"
  },
  {
    "text": "we're running our stuff it's only available in North Virginia and I think Ireland our stuff isn't in either one of",
    "start": "1565059",
    "end": "1572140"
  },
  {
    "text": "the data centers so we can't use it also it's one week old right it's one week out of beta",
    "start": "1572140",
    "end": "1577150"
  },
  {
    "text": "so we're gonna give it some time before again we put all of our production infrastructure on that if you're an",
    "start": "1577150",
    "end": "1583030"
  },
  {
    "text": "azure so the azure - kubernetes has been around for almost a year at this point",
    "start": "1583030",
    "end": "1588309"
  },
  {
    "text": "I'd be more inclined to trust it I actually consider doing a POC on that if",
    "start": "1588309",
    "end": "1593559"
  },
  {
    "text": "we were actually writing on Azure which is I said we aren't and if you're",
    "start": "1593559",
    "end": "1600039"
  },
  {
    "text": "in Google cloud you're in luck that's actually the most trustworthy and battle-tested - kubernetes deployment you can get so",
    "start": "1600039",
    "end": "1609130"
  },
  {
    "text": "you can give it a try tell me about it if you want so let's talk about the next thing right there's",
    "start": "1609130",
    "end": "1614830"
  },
  {
    "text": "a bunch of stuff we once we have our containers running right there's a bunch of stuff we need to take care of which",
    "start": "1614830",
    "end": "1621190"
  },
  {
    "text": "doesn't work quite the same when you have a lot of containers working in a",
    "start": "1621190",
    "end": "1626890"
  },
  {
    "text": "bunch of anonymous instances or pods instead of just named VMs right and the first thing that happens is logging",
    "start": "1626890",
    "end": "1633669"
  },
  {
    "text": "changes right so normally the way people do logging in productions these days is you have some kind of log shipper if",
    "start": "1633669",
    "end": "1639490"
  },
  {
    "text": "you're using the elasticsearch stack right so you'll have log stash or file beach or something running on every instance and they will get your log",
    "start": "1639490",
    "end": "1646600"
  },
  {
    "text": "files and then ship them somewhere else if you're using Splunk yeah you have this func agents shipping logs to the",
    "start": "1646600",
    "end": "1652360"
  },
  {
    "text": "Splunk indexer and this pretty much works all the same way right if you using a service like analogs io something online they give you a log",
    "start": "1652360",
    "end": "1659350"
  },
  {
    "text": "shipper which takes all the logs pushes us to them alright this works nicely if you have a bunch of VMs running your workloads",
    "start": "1659350",
    "end": "1666420"
  },
  {
    "text": "because games are named they don't disappear very quickly even though technically you can restart VMs in",
    "start": "1666420",
    "end": "1672340"
  },
  {
    "text": "practice I checked on average we start every every one of our VMs less than one time a month all right so",
    "start": "1672340",
    "end": "1679030"
  },
  {
    "text": "we can actually trust the VM to stay up for quite a while and then the names of the VMS don't actually change right if we have a VM called web server 3 it's",
    "start": "1679030",
    "end": "1686440"
  },
  {
    "text": "going to restart and come back as web server 3 that's not the same way it works on containers right a container first of all can get rescheduled",
    "start": "1686440",
    "end": "1692590"
  },
  {
    "text": "willy-nilly so kubernetes can just decide it needs to move some resources around it'll just kill a container spin",
    "start": "1692590",
    "end": "1697960"
  },
  {
    "text": "it back up on some other node and then the container name is going to be a random good so you can't trust the name",
    "start": "1697960",
    "end": "1703390"
  },
  {
    "text": "of the container to remain the same right and so in your application the first thing it needs to be change is how you actually treat logging if you want",
    "start": "1703390",
    "end": "1710080"
  },
  {
    "text": "to go and you and look for things that you've logged for diagnose problems right let's say there's an issue which",
    "start": "1710080",
    "end": "1715930"
  },
  {
    "text": "happens once every a million operations and you want to find out if it's related",
    "start": "1715930",
    "end": "1721270"
  },
  {
    "text": "to a specific machine or specific image version so you need a way to actually go",
    "start": "1721270",
    "end": "1727780"
  },
  {
    "text": "and track the image which sorry the docker yeah that container which",
    "start": "1727780",
    "end": "1733930"
  },
  {
    "text": "actually changed names several times all right maybe it came down it came up again came down come again and the name",
    "start": "1733930",
    "end": "1739570"
  },
  {
    "text": "of the container machine itself changed a bunch of times but you need to have a way to actually connect all these logs",
    "start": "1739570",
    "end": "1744640"
  },
  {
    "text": "together right and so yeah you have to consider how you're actually going to mark your log messages to make them",
    "start": "1744640",
    "end": "1751120"
  },
  {
    "text": "available both on the application level let's say about 50 services so each service has to identify itself so you",
    "start": "1751120",
    "end": "1757270"
  },
  {
    "text": "can find the particular service logs and the container itself right chefs I have to actually mark somehow which container",
    "start": "1757270",
    "end": "1764380"
  },
  {
    "text": "is the sending the log and so you'll able to diagnose whether the message is for example the problem is systemic",
    "start": "1764380",
    "end": "1770530"
  },
  {
    "text": "writes on all containers or if it's related to one particular container right and so the next thing you do you",
    "start": "1770530",
    "end": "1776980"
  },
  {
    "text": "think of is where does the log shipper sit alright so one of the things we had",
    "start": "1776980",
    "end": "1782320"
  },
  {
    "text": "when I came to proof point is we're logging we log a lot of things right so we do a lot of data processing we process",
    "start": "1782320",
    "end": "1788529"
  },
  {
    "text": "hundreds of millions of messages and for every I have every event we process we actually log something like Neverest",
    "start": "1788529",
    "end": "1795309"
  },
  {
    "text": "well different messages into the log so our log database is actually growing faster than our actual database and so",
    "start": "1795309",
    "end": "1803640"
  },
  {
    "text": "the first thing we did was we stopped parsing logs instead of actually writing logs to disk and then having log stash",
    "start": "1803640",
    "end": "1810639"
  },
  {
    "text": "in our case we use elk the logs I screwed them from disk and parse them we spun up log stashes on every instance",
    "start": "1810639",
    "end": "1816490"
  },
  {
    "text": "and so the application sends logs to log stash directly through the through TCP this lowered the CPU usage by more than",
    "start": "1816490",
    "end": "1823059"
  },
  {
    "text": "50% we were spending actually more CPU parsing logs than doing the real business work we were actually meant to",
    "start": "1823059",
    "end": "1829059"
  },
  {
    "text": "do and so the first thing we got fear of actually reading log files when you move to containers it's actually not trivial",
    "start": "1829059",
    "end": "1835210"
  },
  {
    "text": "to get rid of reading log files because the default way all containers container orchestrators work is you write to STD",
    "start": "1835210",
    "end": "1842950"
  },
  {
    "text": "out instance collects all the logs Apple docker collects all the logs that get",
    "start": "1842950",
    "end": "1849340"
  },
  {
    "text": "outputted from the container and then it's exposed through docker logs you can go and do copies logs or doctor logs and",
    "start": "1849340",
    "end": "1855190"
  },
  {
    "text": "it brings it logs when you have enough logs that becomes completely impractical because this takes longer than the",
    "start": "1855190",
    "end": "1860590"
  },
  {
    "text": "actual job the container is doing so that that had to go and we deployed lock shippers that our applications talk to",
    "start": "1860590",
    "end": "1868600"
  },
  {
    "text": "through the network and so here's the question do you put the log shippers inside every container and the benefit",
    "start": "1868600",
    "end": "1874000"
  },
  {
    "text": "of that of course is the log shipper is local to the application all the application has to do is send logs to localhost wherever 5,000 and it will",
    "start": "1874000",
    "end": "1881350"
  },
  {
    "text": "always find something listening for logs there the problem with that is we have to add log stash to every container we",
    "start": "1881350",
    "end": "1887529"
  },
  {
    "text": "have hundreds of containers right so you multiply however resource is log stash means by the number of containers we have that",
    "start": "1887529",
    "end": "1893559"
  },
  {
    "text": "becomes a lot of resources right because log stash is a Java program it's pretty fat so that's probably not the best idea",
    "start": "1893559",
    "end": "1900130"
  },
  {
    "text": "the second thing that we considered would be putting log stash on every host right instead of putting it in every",
    "start": "1900130",
    "end": "1905260"
  },
  {
    "text": "container you put it in every host which hosts you know ten docker images ten containers yeah",
    "start": "1905260",
    "end": "1911409"
  },
  {
    "text": "and so it's still kind of local to the image there's no network traffic involved because talks to the local",
    "start": "1911409",
    "end": "1916659"
  },
  {
    "text": "loopback in first interface but that would mean that we can't go to a managed",
    "start": "1916659",
    "end": "1922059"
  },
  {
    "text": "kubernetes deployment anytime soon because when you're using a managed service you don't actually have access",
    "start": "1922059",
    "end": "1927490"
  },
  {
    "text": "to what's running on the host you only have containers and everything outside the cluster so you can't put log",
    "start": "1927490",
    "end": "1934240"
  },
  {
    "text": "shippers and this is gonna be the same thing when we talk about metrics in just a second you can put any kind of",
    "start": "1934240",
    "end": "1939340"
  },
  {
    "text": "receiver on the host itself so the last option obviously is to have your log ship a log collector or stats collector",
    "start": "1939340",
    "end": "1946000"
  },
  {
    "text": "outside your container right which means either it's a container of itself so you spin the log shipper as another",
    "start": "1946000",
    "end": "1952480"
  },
  {
    "text": "container in the cluster or you just have it completely separate it's just the VM running somewhere else that the",
    "start": "1952480",
    "end": "1957669"
  },
  {
    "text": "cluster connects you can see right the benefit of having it in the container obviously it's very nice you can do manage it the same way you do all the",
    "start": "1957669",
    "end": "1964059"
  },
  {
    "text": "other containers unfortunately it can go down any time right you need to actually now take care of it there's a network in",
    "start": "1964059",
    "end": "1970480"
  },
  {
    "text": "the traffic involved maybe you're sending logs and there's a networking disruption so now you lose logs on the",
    "start": "1970480",
    "end": "1976059"
  },
  {
    "text": "way to the log shipper alright same thing with metrics it's probably it's less of a problem with metrics because they're mostly statistics based but with",
    "start": "1976059",
    "end": "1982870"
  },
  {
    "text": "logs really we really don't want to lose any logs so in our case we're still deploying log shippers on the host",
    "start": "1982870",
    "end": "1988450"
  },
  {
    "text": "itself even when we go to manage kubernetes we will have to rethink this because it's not gonna be possible to do",
    "start": "1988450",
    "end": "1995200"
  },
  {
    "text": "that on the host and so just take a small example of how we actually can do",
    "start": "1995200",
    "end": "2000750"
  },
  {
    "text": "logging a small logger application here [Music]",
    "start": "2000750",
    "end": "2006919"
  },
  {
    "text": "so let's just look at the code for just a sec before you do that mr. Luger it's",
    "start": "2006919",
    "end": "2012090"
  },
  {
    "text": "a very small node.js application why not yes I just figured I'll mix things up a lot so just be glad I didn't frightened",
    "start": "2012090",
    "end": "2018720"
  },
  {
    "text": "f-sharp so in log stash it's very easy to talk to log stash from OGS you still at which",
    "start": "2018720",
    "end": "2025799"
  },
  {
    "text": "hosted is its surveyed DNS name log log stash port 5000 and then you just send",
    "start": "2025799",
    "end": "2031440"
  },
  {
    "text": "logs into it right so it just sends a message when the application comes up and every time I go to a rest endpoint",
    "start": "2031440",
    "end": "2037710"
  },
  {
    "text": "it says okay so we call the endpoints right so it's very trivial logging and there are some interesting things I",
    "start": "2037710",
    "end": "2043320"
  },
  {
    "text": "want to show you that we can see in the logs right so we'll spin up a bunch of these alright let's go and spin up a few",
    "start": "2043320",
    "end": "2049440"
  },
  {
    "text": "of the logo applications that's our docker file and are composed",
    "start": "2049440",
    "end": "2058220"
  },
  {
    "text": "we can actually spin up a few of those let's actually have more than one running and does matter so this is the",
    "start": "2058220",
    "end": "2069629"
  },
  {
    "text": "written docker compose just so I can show both kubernetes deployment and docker compose so the way to run it is",
    "start": "2069630",
    "end": "2075270"
  },
  {
    "text": "darker stack deploy this file and we'll call it lager sorry it's - see actually",
    "start": "2075270",
    "end": "2084320"
  },
  {
    "text": "to use the kubernetes alright so now it's created a service same thing as a cube CTL deploy does it created a",
    "start": "2085340",
    "end": "2091860"
  },
  {
    "text": "service called lager we can look and see that is there and docker stack unless",
    "start": "2091860",
    "end": "2098130"
  },
  {
    "text": "you see it now we have a service called blogger running it has one container we can scale it we can go darker service",
    "start": "2098130",
    "end": "2105090"
  },
  {
    "text": "scale logger goes three did not name it",
    "start": "2105090",
    "end": "2113580"
  },
  {
    "text": "right or something",
    "start": "2113580",
    "end": "2116210"
  },
  {
    "text": "oh it's called lager lower lager equals",
    "start": "2119250",
    "end": "2130349"
  },
  {
    "text": "three and now we'll have two more of those and we'll be able to just call it",
    "start": "2130349",
    "end": "2135780"
  },
  {
    "text": "whenever you want it's exposed on port 8080 okay so curl localhost:8080 right and",
    "start": "2135780",
    "end": "2143550"
  },
  {
    "text": "obviously we get to the world wonderful each one of these will probably look something as it happens I have I have",
    "start": "2143550",
    "end": "2155190"
  },
  {
    "text": "elk running right here and we can go can",
    "start": "2155190",
    "end": "2160650"
  },
  {
    "text": "see all of our logs as you can see we have one log saying let's make it bigger so I can actually see it the first",
    "start": "2160650",
    "end": "2166710"
  },
  {
    "text": "message was the service coming up the next two messages is two extra containers spinning up and each one of",
    "start": "2166710",
    "end": "2171869"
  },
  {
    "text": "them says okay I came up and the thing you can see here is each one of them has a different host name right this one is",
    "start": "2171869",
    "end": "2178650"
  },
  {
    "text": "this is the first one because I deploy it to a service and they get a send I signed random host names so you can't",
    "start": "2178650",
    "end": "2185190"
  },
  {
    "text": "actually trust them because each container is a different host name and so every time I called them as you can see we have a load balancer so every one",
    "start": "2185190",
    "end": "2191250"
  },
  {
    "text": "of my curl commands actually hit a different container and each one of them logged something else and so here we can",
    "start": "2191250",
    "end": "2197430"
  },
  {
    "text": "actually look at two things one each one has a label I specify in the application the label so I do have",
    "start": "2197430",
    "end": "2203640"
  },
  {
    "text": "something central connecting all these log messages to my logo application so if I parse this a bit smarter because I",
    "start": "2203640",
    "end": "2209910"
  },
  {
    "text": "was too lazy to actually go and configure configure log stash this would be a separate field and I'd be able to filter by the label itself by the",
    "start": "2209910",
    "end": "2217170"
  },
  {
    "text": "application label so I could go and find all the log messages belonging to the application logger from all the",
    "start": "2217170",
    "end": "2222420"
  },
  {
    "text": "containers but the containers go up or down it doesn't matter or I could go I can sort or filter by the actual",
    "start": "2222420",
    "end": "2228780"
  },
  {
    "text": "container name so we can go and you find just this container so I only want messages from this host and so now we",
    "start": "2228780",
    "end": "2236010"
  },
  {
    "text": "only have the two messages from this particular container so I can go any trace problems that occurred within this",
    "start": "2236010",
    "end": "2241410"
  },
  {
    "text": "specific container and not any of the other copies all right and so this is something we didn't actually have to",
    "start": "2241410",
    "end": "2246780"
  },
  {
    "text": "address at all when we were running VMs it's the thing that you actually have to consider how are you going to be looking",
    "start": "2246780",
    "end": "2252119"
  },
  {
    "text": "for logs from you can see okay this works more or less the same four metrics so if we go and we look at",
    "start": "2252119",
    "end": "2259529"
  },
  {
    "text": "monitoring right there's a few things you can monitor whether everyone clear on the difference between logging and",
    "start": "2259529",
    "end": "2265170"
  },
  {
    "text": "monitoring right logging is writing log messages with the internal state of the application somewhere and monitoring is",
    "start": "2265170",
    "end": "2270900"
  },
  {
    "text": "runtime monitoring so you send some kind of metrics for logical operations that the application does and for things like",
    "start": "2270900",
    "end": "2276599"
  },
  {
    "text": "you know CPU usage disk usage how many requests and how long is request took things that are actually statistical in",
    "start": "2276599",
    "end": "2282809"
  },
  {
    "text": "nature and you can go and build pretty graphs on top of them so for monitoring there's also a few common tools in the",
    "start": "2282809",
    "end": "2288420"
  },
  {
    "text": "industry does anyone use Stasi or collect D in their application it's like",
    "start": "2288420",
    "end": "2294869"
  },
  {
    "text": "two people all right well this fuel is Dana dog anyone's running data dog okay",
    "start": "2294869",
    "end": "2300750"
  },
  {
    "text": "whew so that's an online service which provides monitoring and metrics collection and there's a few others and",
    "start": "2300750",
    "end": "2306210"
  },
  {
    "text": "generally it works the same thing as with the logs you have some kind of log collector usually they use UDP because",
    "start": "2306210",
    "end": "2314010"
  },
  {
    "text": "in the case of logs you don't really need reliable delivery you just need to be very fast and very cheap so stats D",
    "start": "2314010",
    "end": "2320339"
  },
  {
    "text": "is an open source project from Etsy which just come it's a UDP service which",
    "start": "2320339",
    "end": "2325529"
  },
  {
    "text": "collects a bunch of UDP packets as metrics aggregates them in memory and then writes them all at once to some",
    "start": "2325529",
    "end": "2331319"
  },
  {
    "text": "kind of metric storage could be elasticsearch for example and you're on",
    "start": "2331319",
    "end": "2336690"
  },
  {
    "text": "graph Anna would graphs on top of it could be influenced DB or any kind of time series data base there's a bunch of them",
    "start": "2336690",
    "end": "2342930"
  },
  {
    "text": "there's collect D as well and here we have the same question of first of all do you render all the metrics cheaper on",
    "start": "2342930",
    "end": "2349410"
  },
  {
    "text": "the container on the host or completely externally in this case it's much much easier to decide to run it externally",
    "start": "2349410",
    "end": "2355440"
  },
  {
    "text": "because as I said we don't really care about losing single metrics we care about trends usually with metrics so in",
    "start": "2355440",
    "end": "2360990"
  },
  {
    "text": "our case we just just spin up the log collector as another container exposure to the service and everyone can run to",
    "start": "2360990",
    "end": "2366809"
  },
  {
    "text": "that and write to that very easily which is what I'm gonna do in the demo here in just a sec and so the second thing is",
    "start": "2366809",
    "end": "2373259"
  },
  {
    "text": "the most important thing is how do you collect metrics right do you collect metrics at the application level and",
    "start": "2373259",
    "end": "2379289"
  },
  {
    "text": "ignore the fact that they they could be coming from different containers do you collect metrics at the container level but then how you need to aggregate",
    "start": "2379289",
    "end": "2387030"
  },
  {
    "text": "coming from all the containers into one application or do you do both right so for different use cases and the answer",
    "start": "2387030",
    "end": "2393450"
  },
  {
    "text": "is it may different things make different sense in different contexts so for example let's look at one small",
    "start": "2393450",
    "end": "2399270"
  },
  {
    "text": "example of how you would actually collect metrics right here so let's take",
    "start": "2399270",
    "end": "2406320"
  },
  {
    "text": "a look at the metrics application a very straightforward dotnet application Web",
    "start": "2406320",
    "end": "2411540"
  },
  {
    "text": "API I hope it can everyone read that c-sharp who can read c-sharp everybody",
    "start": "2411540",
    "end": "2418320"
  },
  {
    "text": "almost everybody ok good enough ok so we have a very very silly REST API right we",
    "start": "2418320",
    "end": "2423630"
  },
  {
    "text": "have get and we have get by ID that's all it is that's pretty much 95% of all",
    "start": "2423630",
    "end": "2429870"
  },
  {
    "text": "applications by the way in anywhere I all you do is just get stuff or get stopped by ID so here it makes sense to",
    "start": "2429870",
    "end": "2437310"
  },
  {
    "text": "collect a few things right I want to of course for how many get commands were handled I might also be interested in",
    "start": "2437310",
    "end": "2445770"
  },
  {
    "text": "how many get commands are handled by every container instance the reason for",
    "start": "2445770",
    "end": "2450990"
  },
  {
    "text": "counting all of them at once would be because I want to know how many gets actually handle in my application so I",
    "start": "2450990",
    "end": "2457380"
  },
  {
    "text": "want to graph that says ok do you have this many gets per second at any given moment and if I see that graph deep I",
    "start": "2457380",
    "end": "2462660"
  },
  {
    "text": "can say ok well something wrong happened I also want to see how many requests every container serves because maybe it",
    "start": "2462660",
    "end": "2469200"
  },
  {
    "text": "makes sense to monitor and see if one particular container is underserved or over-served maybe I have problems with",
    "start": "2469200",
    "end": "2474690"
  },
  {
    "text": "routing or a little balancing or something and so it makes sense trucks also collect gets metrics by container",
    "start": "2474690",
    "end": "2480930"
  },
  {
    "text": "in this case the only one the way I'm choosing to slug containers just look at the machine name the machine name for a",
    "start": "2480930",
    "end": "2486750"
  },
  {
    "text": "conductor container is just a gooood right it's always random now there's a few problems with that will deploy a sec",
    "start": "2486750",
    "end": "2494240"
  },
  {
    "text": "I'll just talk about the second case first in the second case we're also passing the ID into the get request now",
    "start": "2494240",
    "end": "2501120"
  },
  {
    "text": "here's where it gets tricky because now I could collect metrics by ID as well so I could do something like create a new",
    "start": "2501120",
    "end": "2508230"
  },
  {
    "text": "metric called get dot the ID and so I'd have a separate metric for every unique ID in my system all right when you",
    "start": "2508230",
    "end": "2516150"
  },
  {
    "text": "compound that with containers I'd launched metrics called container name or machine name in this",
    "start": "2516150",
    "end": "2522180"
  },
  {
    "text": "case they'll get dot ID and so I'd get at separate metric for every ID for",
    "start": "2522180",
    "end": "2527369"
  },
  {
    "text": "every container instance ever in my system this can exponentially increase",
    "start": "2527369",
    "end": "2533400"
  },
  {
    "text": "to a lot of different metrics metrics cost money if you're using the dog they directly cost money you pay per custom",
    "start": "2533400",
    "end": "2539819"
  },
  {
    "text": "Ettrick if you're using any kind of local infrastructure like Stasi or Griffin or graphite or whatever each",
    "start": "2539819",
    "end": "2546569"
  },
  {
    "text": "metric is usually a file on disk for example graphite uses whisper it I'll look it pre allocates 80 megabytes",
    "start": "2546569",
    "end": "2552930"
  },
  {
    "text": "for every metric so even if you only send one data item into the metric it still uses 80 megabytes of disk space",
    "start": "2552930",
    "end": "2559170"
  },
  {
    "text": "you take let's say a thousand containers like we do multiply it by however many",
    "start": "2559170",
    "end": "2564270"
  },
  {
    "text": "containers actually come back up and down multiply it by the number of unique IDs in the system multiplied by 80",
    "start": "2564270",
    "end": "2570000"
  },
  {
    "text": "megabytes very quickly we're looking at tens of terabytes of just matrix data right and so this becomes very expensive",
    "start": "2570000",
    "end": "2576300"
  },
  {
    "text": "very quickly and even though we might be interested in how many metrics of those we want it is just sometimes not",
    "start": "2576300",
    "end": "2583170"
  },
  {
    "text": "practical to log everything the other side of it is so you might ask why are",
    "start": "2583170",
    "end": "2588960"
  },
  {
    "text": "you logging both this just yet and get by machine name can't you just log only",
    "start": "2588960",
    "end": "2594089"
  },
  {
    "text": "this the second one by machine name and just aggregate some all the metrics together yes technically I can do that",
    "start": "2594089",
    "end": "2599520"
  },
  {
    "text": "but that actually also costs money right because you aggregates its runtime when you read the metric so if you build a",
    "start": "2599520",
    "end": "2605339"
  },
  {
    "text": "graph which says build me a graph by summing up all the machine name get",
    "start": "2605339",
    "end": "2611819"
  },
  {
    "text": "metrics together it will do that in memory at runtime every time you refresh the graph for example in the case of",
    "start": "2611819",
    "end": "2617490"
  },
  {
    "text": "graph on I it will go and it will query the data source every time so every five seconds it refreshes if you're looking",
    "start": "2617490",
    "end": "2623250"
  },
  {
    "text": "at you know let's say a million different IDs you query a million files and do this the aggregation in memory we",
    "start": "2623250",
    "end": "2631470"
  },
  {
    "text": "crashed our graphite service almost every day before we solve that just a",
    "start": "2631470",
    "end": "2637020"
  },
  {
    "text": "thing that to consider all right so let's run this and see that actually runs we can go and do let's go to our",
    "start": "2637020",
    "end": "2643770"
  },
  {
    "text": "metrics stack deploy metrics so this actually",
    "start": "2643770",
    "end": "2653549"
  },
  {
    "text": "runs that application where you saw as a service and now it exposes it let's say docker serve this LS and then we can see",
    "start": "2653549",
    "end": "2663539"
  },
  {
    "text": "that there we go we have our metrics service on port 50 50 and now we can spam it a bit right so we can do curl",
    "start": "2663539",
    "end": "2670259"
  },
  {
    "text": "local host 50 50 and hopefully it will actually respond",
    "start": "2670259",
    "end": "2675660"
  },
  {
    "text": "it's a dotnet application deathcore it takes it a while to actually come up and there we go and do compiling and I I'm",
    "start": "2675660",
    "end": "2686099"
  },
  {
    "text": "not sure I think I gave it way too little CPU to actually do this so as you can see it responds every time with the",
    "start": "2686099",
    "end": "2691289"
  },
  {
    "text": "hello world and the idea of the container where it came from and every time I run this it's a different container right so now that we",
    "start": "2691289",
    "end": "2697440"
  },
  {
    "text": "have all the where three containers they all guts actually compiled so now the responses are very quick we can also go",
    "start": "2697440",
    "end": "2703680"
  },
  {
    "text": "and get them you know by I do I get by ID or something and then we can we'll go",
    "start": "2703680",
    "end": "2709739"
  },
  {
    "text": "and take a look at all the metrics that we collected so here we can look at",
    "start": "2709739",
    "end": "2715099"
  },
  {
    "text": "Griffin ax which is actually what we use and I prepared in advance a few different types of metrics so this is",
    "start": "2715099",
    "end": "2720959"
  },
  {
    "text": "the gate count right this is how many get requests we actually sent all right we can also get the gate count by ID",
    "start": "2720959",
    "end": "2726930"
  },
  {
    "text": "right I was only calling ID one but if I called a few of the other IDs will get different graphs for every ID we",
    "start": "2726930",
    "end": "2733799"
  },
  {
    "text": "requested and you can see it gets progressively worse because we got more metrics right we can go and look at get",
    "start": "2733799",
    "end": "2740459"
  },
  {
    "text": "by container you'll see that there are four containers I only had only spawned up three there's also my macbook here because i was playing with it earlier",
    "start": "2740459",
    "end": "2746190"
  },
  {
    "text": "but this metric is it remains the container is gone but the metric remains it's not gonna go away until we do clean",
    "start": "2746190",
    "end": "2752579"
  },
  {
    "text": "up so this is going to expand to however many containers there exists in our system or have ever existed within the",
    "start": "2752579",
    "end": "2759630"
  },
  {
    "text": "retention period alright so this this particular counter is going to get very complicated very quickly because",
    "start": "2759630",
    "end": "2765900"
  },
  {
    "text": "containers tend to go up and down up and down also the worst part is let's say I",
    "start": "2765900",
    "end": "2770959"
  },
  {
    "text": "rescale our metric system right so let's go here we'll do docker service sir",
    "start": "2770959",
    "end": "2779190"
  },
  {
    "text": "this scale metrics metrics equals five okay so it's about two more well we'll",
    "start": "2779190",
    "end": "2788069"
  },
  {
    "text": "have spun up in just a sec waiting for it to spin up and I'll scale it back down rightly so we can figure it",
    "start": "2788069",
    "end": "2794190"
  },
  {
    "text": "auto-scaling we want auto scaling so we're going to scale it back down to two right and three containers just disappeared and",
    "start": "2794190",
    "end": "2799859"
  },
  {
    "text": "now we're going to run some metrics and we'll see that do curl localhost",
    "start": "2799859",
    "end": "2807079"
  },
  {
    "text": "50-something and it happens it killed three of the newer containers and key",
    "start": "2807079",
    "end": "2814170"
  },
  {
    "text": "and kept the old ones well good just as easily have killed the new containers and kept sorry killed two the old",
    "start": "2814170",
    "end": "2819450"
  },
  {
    "text": "containers and kept the new ones and so what we would see here is graphs would go up with metrics and then disappear",
    "start": "2819450",
    "end": "2825329"
  },
  {
    "text": "because the container died and there will be a new section of the graph starting and going up and then disappearing again because every",
    "start": "2825329",
    "end": "2830880"
  },
  {
    "text": "container would actually log its own metrics and then disappear and the new container would start and there will be no continuity and this is a graph that's",
    "start": "2830880",
    "end": "2836730"
  },
  {
    "text": "actually very hard to use and if you go over to this one this is the one that",
    "start": "2836730",
    "end": "2842400"
  },
  {
    "text": "actually met logs yet by ID and by container ID which is the most Moore's",
    "start": "2842400",
    "end": "2851310"
  },
  {
    "text": "cardinality we have and this will become unreadable very quickly right so it will have as many containers and as many IDs",
    "start": "2851310",
    "end": "2858780"
  },
  {
    "text": "as you have and so it'll be very short small graphs as soon as the container dies all the graphs become unreadable",
    "start": "2858780",
    "end": "2864240"
  },
  {
    "text": "and so you'll probably never use this as is unless you're specifically investigating a problem with a",
    "start": "2864240",
    "end": "2870060"
  },
  {
    "text": "particular container idea which hardly ever happens so to sum it up it makes sense to to go",
    "start": "2870060",
    "end": "2879270"
  },
  {
    "text": "over every metric and in your application and and evaluate every use case whether you want to log it at the",
    "start": "2879270",
    "end": "2885210"
  },
  {
    "text": "application level because what you care is how many operations the whole application done or if you want to log",
    "start": "2885210",
    "end": "2891180"
  },
  {
    "text": "it by container and if you want and if you will want to log it by container and",
    "start": "2891180",
    "end": "2896250"
  },
  {
    "text": "then aggregate it makes sense to just create a new metric even though you're paying for the new metric it's still cheaper than aggregating all the other",
    "start": "2896250",
    "end": "2902760"
  },
  {
    "text": "metrics in memory every time you actually read them all right and so as you saw earlier there's one small",
    "start": "2902760",
    "end": "2908640"
  },
  {
    "text": "addition to this monitoring health and so in kubernetes you want more health you can either run",
    "start": "2908640",
    "end": "2914760"
  },
  {
    "text": "a command in the container itself or call a restaurant point which is much easier we strongly prefer using rest",
    "start": "2914760",
    "end": "2921600"
  },
  {
    "text": "endpoints and there's a couple guidelines which I don't necessarily",
    "start": "2921600",
    "end": "2926910"
  },
  {
    "text": "agree with from kubernetes about health checks they strongly recommend that health checks be very quick I don't know",
    "start": "2926910",
    "end": "2933300"
  },
  {
    "text": "if that's true because usually what people do for health checks is just an empty endpoint which returns okay",
    "start": "2933300",
    "end": "2940080"
  },
  {
    "text": "presumably if the service is down the right endpoints will not return okay and that's how you determine the container",
    "start": "2940080",
    "end": "2946230"
  },
  {
    "text": "is healthy which isn't really the case in real life because for example we have containers or applications that come up",
    "start": "2946230",
    "end": "2952170"
  },
  {
    "text": "they connect to two different databases and one elastic search and kafka and a bunch of other stuff and so the health",
    "start": "2952170",
    "end": "2958650"
  },
  {
    "text": "of this service isn't defined by the fact that service is still up is defined by whether I can actually do its job right and determining whether the",
    "start": "2958650",
    "end": "2965430"
  },
  {
    "text": "service can do this job means probing every database or every connection actually it relies on or in our case",
    "start": "2965430",
    "end": "2972630"
  },
  {
    "text": "where you actually do it in the background and store the last time we actually saw a healthy database or so",
    "start": "2972630",
    "end": "2978210"
  },
  {
    "text": "got a message from Kafka and then the health check is whether we had a",
    "start": "2978210",
    "end": "2983580"
  },
  {
    "text": "successful health check within the last five seconds right and if you haven't seen any messages from calc of five seconds and we haven't used the database",
    "start": "2983580",
    "end": "2990390"
  },
  {
    "text": "in five seconds and it's far as we're concerned the service isn't healthy right and so this is something that",
    "start": "2990390",
    "end": "2996060"
  },
  {
    "text": "actually bears thinking on because you need to go for to every service if you have many services you do need to",
    "start": "2996060",
    "end": "3001130"
  },
  {
    "text": "evaluate each one and consider what defines health for this particular service right is it just the the process",
    "start": "3001130",
    "end": "3007910"
  },
  {
    "text": "itself is up is that enough or is it because it actually can do all of its job right and how how did you find is it",
    "start": "3007910",
    "end": "3017000"
  },
  {
    "text": "the first time you see you fail to connect to the database or is it several times we fed scratched database this is actually pretty much similar to what you",
    "start": "3017000",
    "end": "3023030"
  },
  {
    "text": "would hopefully do regardless of containers but indicate some containers it becomes much more important because",
    "start": "3023030",
    "end": "3029140"
  },
  {
    "text": "first of all containers are much less reliable then you know in terms of if they can go up and down and you don't",
    "start": "3029140",
    "end": "3034910"
  },
  {
    "text": "really control that's very much and also containers use the health check much more rigorously Quadra news will",
    "start": "3034910",
    "end": "3041090"
  },
  {
    "text": "actually go and probe the health check every however long you define and it will use the radio",
    "start": "3041090",
    "end": "3046130"
  },
  {
    "text": "check to make sure that the container is ready before it spins up anything that depends on it and so you need to",
    "start": "3046130",
    "end": "3051589"
  },
  {
    "text": "actually consider for example how do you find ready in our cases today nur is ready when it's actually connected to",
    "start": "3051589",
    "end": "3056900"
  },
  {
    "text": "all the databases and made sure it can connect to Kafka and make sure it can see it elasticsearch if none of these",
    "start": "3056900",
    "end": "3062480"
  },
  {
    "text": "thing is one of these things doesn't happen the container isn't ready and so spinning up any dependencies makes no",
    "start": "3062480",
    "end": "3067910"
  },
  {
    "text": "sense because they would also not be ready they would just all fail because their health check defines being able to",
    "start": "3067910",
    "end": "3073430"
  },
  {
    "text": "access the first service this alright so",
    "start": "3073430",
    "end": "3082130"
  },
  {
    "text": "this is a small thing I want to touch on before I move on the whole point really of using containers is so you can manage",
    "start": "3082130",
    "end": "3088309"
  },
  {
    "text": "resources more granularly right it's create isolation there's a bunch of other benefits obviously you know you can spin up identical environments it",
    "start": "3088309",
    "end": "3095660"
  },
  {
    "text": "helps QA and testing but when we talk about production running stuff in production the thing I really most care",
    "start": "3095660",
    "end": "3101390"
  },
  {
    "text": "about as the architects to this product is I want to be able to cram as much workload into as few VMs as possible to",
    "start": "3101390",
    "end": "3109730"
  },
  {
    "text": "reduce costs to increase isolation and to make it easier to manage all this stuff centrally right and so the main",
    "start": "3109730",
    "end": "3117049"
  },
  {
    "text": "thing to do that is to be able to manage resources right and one of the things all the container orchestras allow you",
    "start": "3117049",
    "end": "3123769"
  },
  {
    "text": "is to over provision resources right you can go and you say if you don't actually specify any limits by default docker",
    "start": "3123769",
    "end": "3130400"
  },
  {
    "text": "will let the container do whatever it wants it will allocate as much memory as there is on the host and it will be able",
    "start": "3130400",
    "end": "3135650"
  },
  {
    "text": "to use as many CPUs as there are on the host all right if you don't actually give it hard limits to resources same",
    "start": "3135650",
    "end": "3141980"
  },
  {
    "text": "wood block i/o by the way there's no way to limit network i/o yet as far as I know",
    "start": "3141980",
    "end": "3147710"
  },
  {
    "text": "and so it's strongly recommended to at least define the upper limits you might not define the initial resources you can",
    "start": "3147710",
    "end": "3154759"
  },
  {
    "text": "say okay well use whatever you want but go only up until a certain amount you don't actually when we started playing",
    "start": "3154759",
    "end": "3161539"
  },
  {
    "text": "with this one of the things that we got wrong is we didn't set any limits and so it was pretty much effectively the same",
    "start": "3161539",
    "end": "3168289"
  },
  {
    "text": "as running a bunch of services all on the same VM they still interfere with each other they still created burst",
    "start": "3168289",
    "end": "3174380"
  },
  {
    "text": "usage of CPU which affected other services it wasn't actually we weren't reaping the benefits of you",
    "start": "3174380",
    "end": "3179510"
  },
  {
    "text": "in containers and so we went around we actually defined what makes sense in terms of resources for every class of",
    "start": "3179510",
    "end": "3185000"
  },
  {
    "text": "service and once we did that we had the minimum resources and the maximum wishes you can get and then it became much",
    "start": "3185000",
    "end": "3190880"
  },
  {
    "text": "easier because we could actually calculate okay this VM the size of VM on Amazon can run up to X number of certain",
    "start": "3190880",
    "end": "3198680"
  },
  {
    "text": "service and made much easier to actually provision services and rely on resources",
    "start": "3198680",
    "end": "3204620"
  },
  {
    "text": "being available when you spin up new services or scale up and down right also most docker swarm and kubernetes let you",
    "start": "3204620",
    "end": "3212210"
  },
  {
    "text": "specify auto scaling it's much more powerful in kubernetes you can create very complex rules for when to scale",
    "start": "3212210",
    "end": "3218380"
  },
  {
    "text": "unfortunately they all pretty much rely on criteria like CPU our services except",
    "start": "3218380",
    "end": "3224510"
  },
  {
    "text": "for two small services are not CPU bound there usually are either IO bound and they read from Kafka and that's what",
    "start": "3224510",
    "end": "3230930"
  },
  {
    "text": "defines how often how fast their work or their network bound because they do outside Network requests and so we can't",
    "start": "3230930",
    "end": "3236720"
  },
  {
    "text": "really use auto scaling to scale them because they might be using zero CPU and still be overloaded because they're",
    "start": "3236720",
    "end": "3242420"
  },
  {
    "text": "waiting for a millionaire quest to return all right and so we we are we're having to evaluate this and actually",
    "start": "3242420",
    "end": "3248750"
  },
  {
    "text": "create external tools to detect what can what constitutes load for a container and want to scale it externally because",
    "start": "3248750",
    "end": "3255680"
  },
  {
    "text": "we can't really rely on CPU based metrics to scale the container all right this is this is where it helps to have",
    "start": "3255680",
    "end": "3261980"
  },
  {
    "text": "external metrics like we were talking about earlier because then we can probe the metrics see how for example how many",
    "start": "3261980",
    "end": "3267020"
  },
  {
    "text": "requests each container serves and if it's above a certain threshold you spin up you scale out the service you spin up",
    "start": "3267020",
    "end": "3272720"
  },
  {
    "text": "new containers of the same type right and so the whole point was it's",
    "start": "3272720",
    "end": "3278780"
  },
  {
    "text": "dangerous to over provision and new to actually consider whether it makes sense to pin certain containers to certain",
    "start": "3278780",
    "end": "3284390"
  },
  {
    "text": "machines this actually brings us to managing state right and I'm not sure",
    "start": "3284390",
    "end": "3289490"
  },
  {
    "text": "what time it is are we good so managing state is there's",
    "start": "3289490",
    "end": "3295130"
  },
  {
    "text": "two things one there are no stateless applications right I mean as as soon as you allocate memory you have state in",
    "start": "3295130",
    "end": "3301070"
  },
  {
    "text": "your application yes they are a very stateful application and applications and less stateful applications but all",
    "start": "3301070",
    "end": "3306560"
  },
  {
    "text": "applications have States and usually it's not a good thing to lose the states because the container went down and came up in a different machine",
    "start": "3306560",
    "end": "3313170"
  },
  {
    "text": "right so even if you're doing things like for example how many people use Kafka or kabuki nieces or something",
    "start": "3313170",
    "end": "3319829"
  },
  {
    "text": "similar to read messages it's like you guys disappoint me very much is everyone just doing microservices who's doing",
    "start": "3319829",
    "end": "3325740"
  },
  {
    "text": "micro-services with rests what are the rest of you doing alright fair enough",
    "start": "3325740",
    "end": "3331980"
  },
  {
    "text": "but okay imagine you have a rest service alright and it gets a rest request now sure the services state is stateless",
    "start": "3331980",
    "end": "3338339"
  },
  {
    "text": "it'll go to a database but while its handling presumably not just a single request but a bunch of requests coming",
    "start": "3338339",
    "end": "3343710"
  },
  {
    "text": "in all at once all these requests are state in memory if the container suddenly just goes down you will have",
    "start": "3343710",
    "end": "3349980"
  },
  {
    "text": "lost however much work you were doing at that moment right sure you can build reliable services and the services call",
    "start": "3349980",
    "end": "3356640"
  },
  {
    "text": "them we'll try it again all that stuff is true but the fact remains you lost some state is gonna you",
    "start": "3356640",
    "end": "3362880"
  },
  {
    "text": "have to have to handle all that stuff and this happens with VMs as well but as",
    "start": "3362880",
    "end": "3367980"
  },
  {
    "text": "I said VMs actually restart all that often that's not a normal occurrence containers do restarts very very often",
    "start": "3367980",
    "end": "3374069"
  },
  {
    "text": "right and they give you a certain grace period to shutdown cleanly but you can't just stop services cleanly every time",
    "start": "3374069",
    "end": "3380250"
  },
  {
    "text": "and so this is something you have to consider because when you build this application when you're building those cloud native application which is a",
    "start": "3380250",
    "end": "3385859"
  },
  {
    "text": "silly buzzword but the point is you have to consider that your application is very impermanent and do as little work",
    "start": "3385859",
    "end": "3393150"
  },
  {
    "text": "in States as possible unfortunately there are some applications which are the very definition of stateful right",
    "start": "3393150",
    "end": "3398700"
  },
  {
    "text": "databases obviously there's really nowhere now you can't you can only roll statelessness so much until you can",
    "start": "3398700",
    "end": "3405869"
  },
  {
    "text": "actually have to store the states somewhere so it goes into a database or you know Kafka or whatever and then the",
    "start": "3405869",
    "end": "3413609"
  },
  {
    "text": "question becomes do you run your databases or data stores own containers right and the answer is yes up to a",
    "start": "3413609",
    "end": "3421019"
  },
  {
    "text": "certain point if it's a single node database let's say you're running them you know my sequel or any kind of sequel",
    "start": "3421019",
    "end": "3426509"
  },
  {
    "text": "flavor which is single node not clustered or you're running you know one  nodes or whatever it is it makes",
    "start": "3426509",
    "end": "3433589"
  },
  {
    "text": "perfect sense to do that is there's no issue with it every Orchestrator allows you to pin a",
    "start": "3433589",
    "end": "3438990"
  },
  {
    "text": "container to a specific machine so for example you can define that this particular host only hosts container",
    "start": "3438990",
    "end": "3446910"
  },
  {
    "text": "targets with database and you only have one container tag database in your system so this whole machine belongs to",
    "start": "3446910",
    "end": "3453180"
  },
  {
    "text": "this one container where sequel server lives okay that's perfectly fine that it doesn't actually give you any isolation",
    "start": "3453180",
    "end": "3459210"
  },
  {
    "text": "benefits but you can manage your database with the same tools you manage all the rest of your system and it makes sense as soon as you go to cluster data",
    "start": "3459210",
    "end": "3466200"
  },
  {
    "text": "stores right so your Kafka is your elasticsearch Couchbase whatever they do base you have that's actually",
    "start": "3466200",
    "end": "3471569"
  },
  {
    "text": "clustered and there is significant cost to losing a node and bringing those node",
    "start": "3471569",
    "end": "3477059"
  },
  {
    "text": "back up it pretty much stops making sense to deploy those on containers because above certain points for example",
    "start": "3477059",
    "end": "3484200"
  },
  {
    "text": "our main elasticsearch cluster has 15 terabytes of data in it right it's 32",
    "start": "3484200",
    "end": "3489390"
  },
  {
    "text": "nodes with 15 terabytes of data replicated once and so if we lose a node",
    "start": "3489390",
    "end": "3494730"
  },
  {
    "text": "then I mean that means we need to bring a node in and then replicate something like half a terabyte of data from all",
    "start": "3494730",
    "end": "3500700"
  },
  {
    "text": "the other nodes to that node if that happens more than once every week or so we're gonna have serious issues both in",
    "start": "3500700",
    "end": "3508710"
  },
  {
    "text": "the platform in the dev ops it's not a painless operation and this elasticsearch where it's read-only right we're not actually using it to store",
    "start": "3508710",
    "end": "3515010"
  },
  {
    "text": "that much data we mostly Street it from it if it's Kafka restoring a Kafka node",
    "start": "3515010",
    "end": "3520380"
  },
  {
    "text": "is a very painful operation it can it can actually break things so so far",
    "start": "3520380",
    "end": "3527220"
  },
  {
    "text": "we've strongly avoided putting any kind of stateful data stores on containers I",
    "start": "3527220",
    "end": "3533089"
  },
  {
    "text": "personally wouldn't be feel comfortable doing that any time soon until we actually get better tools to handle in state",
    "start": "3533089",
    "end": "3538700"
  },
  {
    "text": "kubernetes are moving in that direction there's there's a thing called stateful sets which lets you create contain",
    "start": "3538700",
    "end": "3545190"
  },
  {
    "text": "containers with identity which is kind of an oxymoron the whole point of containers is that they have no identity they're you know whatever but stable",
    "start": "3545190",
    "end": "3551700"
  },
  {
    "text": "sets lets you create a container that has an identity it keeps the same IP even if it goes down and comes back up somewhere else keeps the same IP it's the same",
    "start": "3551700",
    "end": "3558299"
  },
  {
    "text": "container ID which is important a lot of clustered systems use the IP and then",
    "start": "3558299",
    "end": "3563579"
  },
  {
    "text": "and the hostname as the identifier for the node so for example Couchbase I",
    "start": "3563579",
    "end": "3569130"
  },
  {
    "text": "worked at cash base a while ago if you know goes down and comes back up with a different node theme it will not be able",
    "start": "3569130",
    "end": "3574980"
  },
  {
    "text": "to rejoin the cluster the cluster will reject it because it will think it's a new node right so in a stateful set it",
    "start": "3574980",
    "end": "3580049"
  },
  {
    "text": "will come it will go down there's gonna be some downtime obviously for some of the data but it will come back up and be able to",
    "start": "3580049",
    "end": "3586260"
  },
  {
    "text": "rejoin the cluster because it's the same but not really node right so kubernetes is moving in that direction we still",
    "start": "3586260",
    "end": "3593190"
  },
  {
    "text": "don't really feel comfortable using this for our you know 15 terabytes of data because it's just too expensive to move",
    "start": "3593190",
    "end": "3598560"
  },
  {
    "text": "terabytes later around at some point it will probably be okay I'm not sure when it's gonna be I guesstimate probably a year from now",
    "start": "3598560",
    "end": "3605670"
  },
  {
    "text": "at least right so last thing before we be finished because we're coming up on",
    "start": "3605670",
    "end": "3612000"
  },
  {
    "text": "time there's a few things you need to consider there's mostly more devops related things but you need to define",
    "start": "3612000",
    "end": "3617820"
  },
  {
    "text": "the behavior of restarts and updates all right what does it mean for you to",
    "start": "3617820",
    "end": "3622950"
  },
  {
    "text": "update an application sure in your case right you build a new docker image you update you commit that again to github",
    "start": "3622950",
    "end": "3628650"
  },
  {
    "text": "you do dr. build or whatever and you have a new in new version of a container but then deploying that container might",
    "start": "3628650",
    "end": "3635460"
  },
  {
    "text": "not be as easy as okay well just update every in every container in turn all right because what if in a lot of cases",
    "start": "3635460",
    "end": "3642660"
  },
  {
    "text": "having two different versions of the same container running side-by-side is a bad idea right because one has one",
    "start": "3642660",
    "end": "3648510"
  },
  {
    "text": "scheme and the other has another schema and now we have to do a bunch of work on the dev side to handle two schemas at",
    "start": "3648510",
    "end": "3653520"
  },
  {
    "text": "the same time all right do different versions of objects you're going to be storing and reading right and so it's",
    "start": "3653520",
    "end": "3658770"
  },
  {
    "text": "not as easy as just saying okay well when you're doing rolling updates in the deployment file update container is two",
    "start": "3658770",
    "end": "3664230"
  },
  {
    "text": "by two or and one by one with ten seconds difference between them the thing is that you know you have ten containers",
    "start": "3664230",
    "end": "3669480"
  },
  {
    "text": "with ten seconds in between there's a hundred seconds where you have a mix of versions in your application and you",
    "start": "3669480",
    "end": "3675180"
  },
  {
    "text": "can't be sure which version is working at any given time and so this is something that you don't really have a",
    "start": "3675180",
    "end": "3680880"
  },
  {
    "text": "good answer in built into in user experience right now you actually have",
    "start": "3680880",
    "end": "3686280"
  },
  {
    "text": "to manually for example create a new service and switch the load balancer to it so you have a new version working all",
    "start": "3686280",
    "end": "3692760"
  },
  {
    "text": "at once or you have to do a lot of work and make sure that your containers or",
    "start": "3692760",
    "end": "3698430"
  },
  {
    "text": "your services are fully backwards compatible so if they get an old version of an object they can actually handle it",
    "start": "3698430",
    "end": "3703920"
  },
  {
    "text": "even if or the new version of an object so they have to be both backward and forward compatible if you want to do",
    "start": "3703920",
    "end": "3709410"
  },
  {
    "text": "rolling updates all right so I have like three seconds for questions and she's like just one really quick",
    "start": "3709410",
    "end": "3718079"
  },
  {
    "text": "question all right no one volunteers that's fine we're gonna end 30 seconds early and if you have any",
    "start": "3718079",
    "end": "3724230"
  },
  {
    "text": "questions I'll be here I can have you down sir otherwise thank you very much coming it's very late let's go to the party",
    "start": "3724230",
    "end": "3729830"
  },
  {
    "text": "[Applause]",
    "start": "3729830",
    "end": "3737090"
  }
]