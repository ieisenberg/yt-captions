[
  {
    "text": "we're ready to get going yep okay well thank you very much for everyone for",
    "start": "9299",
    "end": "15130"
  },
  {
    "text": "coming along to the closing keynote as you that will be next year that's yeah",
    "start": "15130",
    "end": "21279"
  },
  {
    "text": "thank you for coming along there's still time to go and see John Sookie if you want to if you want to duck out now I",
    "start": "21279",
    "end": "26770"
  },
  {
    "text": "won't be offended that's totally fine so i'd say twitter poll a few months before",
    "start": "26770",
    "end": "34030"
  },
  {
    "text": "this and said hey you know what do people want to hear about elasticsearch and it was a unanimous decision they",
    "start": "34030",
    "end": "39940"
  },
  {
    "text": "want to hear about using elasticsearch word net so that's what this talk is going to be about so just to kind of run",
    "start": "39940",
    "end": "47440"
  },
  {
    "text": "through a few other things that we're going to we're going to look at today this is going to be a very very example",
    "start": "47440",
    "end": "54219"
  },
  {
    "text": "heavy talk so I'm going to be switching between slides and examples are like I",
    "start": "54219",
    "end": "60010"
  },
  {
    "text": "like to see code myself so there's going to be plenty of code here for you to see so we're going to start off basically",
    "start": "60010",
    "end": "65800"
  },
  {
    "text": "with what elasticsearch is how that fits into the great scheme of the things in the elastic stack and then we're going",
    "start": "65800",
    "end": "72070"
  },
  {
    "text": "to get up and running with the dotnet clients so there are two clients and we'll go into that in a little bit it",
    "start": "72070",
    "end": "78460"
  },
  {
    "text": "will take a slight detour to just kind of go over mappings and analysis we need to kind of do that because in order to",
    "start": "78460",
    "end": "85960"
  },
  {
    "text": "talk about queries and aggregations we kind of need to just kind of dive into that a little bit and then we're going",
    "start": "85960",
    "end": "91660"
  },
  {
    "text": "to finish off with completion and recommendations so those queries and aggregations are going to we're going to",
    "start": "91660",
    "end": "97480"
  },
  {
    "text": "look at a few different things their buckets pipeline aggregations so for this particular talk here I'm using a",
    "start": "97480",
    "end": "103960"
  },
  {
    "text": "few different data sets one of them is the stack overflow data set I've taken the entire history of slack overflow",
    "start": "103960",
    "end": "111130"
  },
  {
    "text": "questions answers uses their badges and I've indexed that into lastik search running on my old trusty laptop here so",
    "start": "111130",
    "end": "118330"
  },
  {
    "text": "if it holds up hopefully we'll see some some interesting insights so they say",
    "start": "118330",
    "end": "124300"
  },
  {
    "text": "that every every talk every good talk should start off with a pissy quote so I scoured the internet I find this guy",
    "start": "124300",
    "end": "131489"
  },
  {
    "text": "saying something about elasticsearch enabling use finding answers to questions you don't yet know you have",
    "start": "131489",
    "end": "137560"
  },
  {
    "text": "and think that kind of thumbs-up elasticsearch quite well you have all these dreams of data coming in you have",
    "start": "137560",
    "end": "144770"
  },
  {
    "text": "no idea what what you want to do with it right now but but for the moment you're just gathering all of that data up shoving it into elasticsearch and then",
    "start": "144770",
    "end": "152330"
  },
  {
    "text": "at some point someone asks you hey this server goes down or some attacking the server over here and you're aggregating",
    "start": "152330",
    "end": "158030"
  },
  {
    "text": "all of those logs into elasticsearch you can then start going and exploring those",
    "start": "158030",
    "end": "163220"
  },
  {
    "text": "particular pieces of information and try and piece together a puzzle of exactly",
    "start": "163220",
    "end": "168470"
  },
  {
    "text": "what is happening there so what is elasticsearch this is a very high-level",
    "start": "168470",
    "end": "174200"
  },
  {
    "text": "view so it's a distributed and scalable search and analytics engine it's",
    "start": "174200",
    "end": "179810"
  },
  {
    "text": "schema-less although we'll see that that's actually an implied schema from what you send it",
    "start": "179810",
    "end": "185450"
  },
  {
    "text": "but we'll talk a little bit about that in mappings you interact it with it with the json rest api and there are client",
    "start": "185450",
    "end": "192769"
  },
  {
    "text": "libraries for pretty much every programming language you can think of and so elasticsearch itself is open",
    "start": "192769",
    "end": "199580"
  },
  {
    "text": "source that's under the apache license bill it's built on top of leucine which itself is also under apache license so",
    "start": "199580",
    "end": "206900"
  },
  {
    "text": "it allows you to perform real-time near real-time search on large amounts of data full-text search is well search",
    "start": "206900",
    "end": "214579"
  },
  {
    "text": "relevancy scoring aggregations geospatial also handles multiple different languages and there are",
    "start": "214579",
    "end": "221870"
  },
  {
    "text": "plugins for handling non-english languages as well it's highly available so you can run elastic search on a",
    "start": "221870",
    "end": "228019"
  },
  {
    "text": "single node you can run it on a bunch of nodes and those can be separate machines they can be the same machine and",
    "start": "228019",
    "end": "234040"
  },
  {
    "text": "elasticsearch will largely handle the the scaling out of that for you so if you find that you're reaching the limits",
    "start": "234040",
    "end": "240230"
  },
  {
    "text": "of your particular cluster you can add another node elastic search will balance the shards of an urban index across",
    "start": "240230",
    "end": "246380"
  },
  {
    "text": "those other nodes so elastic search that's probably all I'm going to say",
    "start": "246380",
    "end": "251930"
  },
  {
    "text": "about elastic search because we could talk for hours and hours about that but elastic search itself is part of",
    "start": "251930",
    "end": "258079"
  },
  {
    "text": "something known as the elk beasts pack so those of you who are familiar with",
    "start": "258079",
    "end": "263110"
  },
  {
    "text": "elastic search it's very the Elks pack is something that's very popular within",
    "start": "263110",
    "end": "269030"
  },
  {
    "text": "the logging world so using elastic search log stashing Caban so using logstash to bring logs within",
    "start": "269030",
    "end": "275389"
  },
  {
    "text": "to elasticsearch than using Cabana as a visualization and back year and a half ago we also built on on top beats as",
    "start": "275389",
    "end": "282770"
  },
  {
    "text": "well so then we were kind of thinking well how does the be fit into ELQ you",
    "start": "282770",
    "end": "287779"
  },
  {
    "text": "know is it ELQ B is it black is it club so we came up with out B but we didn't",
    "start": "287779",
    "end": "294259"
  },
  {
    "text": "really we it's actually the elastic stack so elastic stack comprises of",
    "start": "294259",
    "end": "300050"
  },
  {
    "text": "Cabana which is the user interface so if you've come and visited the booth you've probably seen the nice pretty visuals",
    "start": "300050",
    "end": "305839"
  },
  {
    "text": "there are various different data sets that we've had elastic search there is the the actual search analytics tension",
    "start": "305839",
    "end": "312050"
  },
  {
    "text": "where all of the data is stored and is searched then logs fashion beats are the",
    "start": "312050",
    "end": "317209"
  },
  {
    "text": "ingest to actually get data into elastic search on the side there x-pac are a",
    "start": "317209",
    "end": "323599"
  },
  {
    "text": "bunch of additional commercial plugins elastic offers that provide additional functionality to elastic search and then",
    "start": "323599",
    "end": "330289"
  },
  {
    "text": "finally elastic cloud is our elastic search of the service offering so we take care of managing the cluster for",
    "start": "330289",
    "end": "337219"
  },
  {
    "text": "you if you want to scale it up we take care of that getting it down etc etc",
    "start": "337219",
    "end": "343300"
  },
  {
    "text": "multiple use cases people using it for log analysis so Verizon and after using",
    "start": "343330",
    "end": "350089"
  },
  {
    "text": "it for analytics of the Mars rover telemetry data github uses it for search so if you've ever searched for anything",
    "start": "350089",
    "end": "355789"
  },
  {
    "text": "on github that's going to be hitting an elastic search cluster so enough",
    "start": "355789",
    "end": "361550"
  },
  {
    "text": "preamble let's get started with dotnet so the dotnet side of things so there",
    "start": "361550",
    "end": "368990"
  },
  {
    "text": "are essentially two clients there's elastic search net which is the low-level client so it's a very",
    "start": "368990",
    "end": "374209"
  },
  {
    "text": "unappealing dependency free client there it's has the entire lastik search api",
    "start": "374209",
    "end": "381319"
  },
  {
    "text": "mapped it works basically with strings objects lists of strings lists of objects and byte arrays very unappealing",
    "start": "381319",
    "end": "389119"
  },
  {
    "text": "ated that's the low level and then on top of elastic search net you have nest",
    "start": "389119",
    "end": "395419"
  },
  {
    "text": "which is the higher level clients of all the all the request types all the response types are mapped to strongly",
    "start": "395419",
    "end": "401209"
  },
  {
    "text": "typed c-sharp pipes for you to be you to work with uses JSON net under the covers for JSON",
    "start": "401209",
    "end": "408440"
  },
  {
    "text": "serialization and it exposes and actually can uses elasticsearch net",
    "start": "408440",
    "end": "413660"
  },
  {
    "text": "under the covers so you can still use the low-level client if you're using nest simply by calling client low-level",
    "start": "413660",
    "end": "421400"
  },
  {
    "text": "you have access to the low-level client some of the nice things about Neff though that it has on top of",
    "start": "421400",
    "end": "429130"
  },
  {
    "text": "elasticsearch dotnet and why I think it's a better choice to get started with is it provides you with additional",
    "start": "429130",
    "end": "435020"
  },
  {
    "text": "functionality things like covariant results set so for example an elastic search when you index data you index a",
    "start": "435020",
    "end": "443690"
  },
  {
    "text": "JSON document that document goes into an index in elastic search and that document is going to be of a particular",
    "start": "443690",
    "end": "449480"
  },
  {
    "text": "type as well you're going to expect file types of that document and you may have",
    "start": "449480",
    "end": "454550"
  },
  {
    "text": "more than one type in one index so for example in the case of Stack Overflow you might have a question like overflow",
    "start": "454550",
    "end": "461600"
  },
  {
    "text": "question then you have a collection of answers and those are a collection of answers for a particular question so you",
    "start": "461600",
    "end": "467060"
  },
  {
    "text": "may have questions and answers both indexed into the same index and you want to return questions and answers that's",
    "start": "467060",
    "end": "474170"
  },
  {
    "text": "what covariant results sets will be able to give you a nest nest handles that and we will be serialize into the correct",
    "start": "474170",
    "end": "480470"
  },
  {
    "text": "type also has some functionality for providing auto mappings so given a a",
    "start": "480470",
    "end": "485870"
  },
  {
    "text": "poco given a c-sharp type it can infer the correct mapping to send so lastik",
    "start": "485870",
    "end": "492170"
  },
  {
    "text": "search there and provide some nice other functionality there with operator overloading for example if you need to",
    "start": "492170",
    "end": "498470"
  },
  {
    "text": "combine to query form a compound query from two different queries that there's some overloading there that allow you to",
    "start": "498470",
    "end": "505040"
  },
  {
    "text": "do that we'll look at examples of all of these so let's get started so this is",
    "start": "505040",
    "end": "516620"
  },
  {
    "text": "how we would end up connecting to elasticsearch with the low-level client simply new offer client and this client",
    "start": "516620",
    "end": "525050"
  },
  {
    "text": "here is going to be trying to talk to elastic search on our local host on port",
    "start": "525050",
    "end": "530330"
  },
  {
    "text": "9 200 so that's simple now sometimes we",
    "start": "530330",
    "end": "536330"
  },
  {
    "text": "want to provide additional settings as well to the client so specify a different URI for a start",
    "start": "536330",
    "end": "541690"
  },
  {
    "text": "maybe we want to use a different type of connection pool so for example elasticsearch may be running on a bunch",
    "start": "541690",
    "end": "547880"
  },
  {
    "text": "of nodes they all have a different address to them we may want to use all of those addresses and pass them to the",
    "start": "547880",
    "end": "554480"
  },
  {
    "text": "settings there so that when a request comes in the client is able to send it to any one of those nodes so same as",
    "start": "554480",
    "end": "562579"
  },
  {
    "text": "before we can run that and we have a client so",
    "start": "562579",
    "end": "567589"
  },
  {
    "text": "the client here and CIMMYT so similarly we've messed we can you up a new client and we can see here that we have we have",
    "start": "567589",
    "end": "576380"
  },
  {
    "text": "the low-level client here available as a property on the nest client so a little",
    "start": "576380",
    "end": "583069"
  },
  {
    "text": "bit more of a complicated example here so this is using some of the some of the",
    "start": "583069",
    "end": "588620"
  },
  {
    "text": "additional next features that I was talking about so we have a bunch of your eyes here so we have a three node",
    "start": "588620",
    "end": "595160"
  },
  {
    "text": "cluster here running on line 209 201 mine 202 we pass those to a sniffing",
    "start": "595160",
    "end": "601310"
  },
  {
    "text": "connection pool so significant we have a bunch of different connection pool types sniffing connection pool is quite good",
    "start": "601310",
    "end": "607160"
  },
  {
    "text": "for clusters that automatically scale because that sniffing connection pool is able to send a sniff request to",
    "start": "607160",
    "end": "613160"
  },
  {
    "text": "elasticsearch understand if new nodes have been added to the cluster or if new earth nodes have dropped out of the",
    "start": "613160",
    "end": "619279"
  },
  {
    "text": "cluster and then use those in order to send requests to the to the correct place there in addition to in addition",
    "start": "619279",
    "end": "625819"
  },
  {
    "text": "to the connection pool we have a set of connection settings here where we can",
    "start": "625819",
    "end": "630920"
  },
  {
    "text": "specify a default index for elasticsearch for the client to use if an index hasn't been specified on the",
    "start": "630920",
    "end": "637760"
  },
  {
    "text": "request we can also specify an index here for a particular type as well so",
    "start": "637760",
    "end": "643519"
  },
  {
    "text": "when at any time that we're going to perform a search on users posts questions answers use this particular",
    "start": "643519",
    "end": "650089"
  },
  {
    "text": "index and we can pass in a collection of indices there as well if for example we",
    "start": "650089",
    "end": "655819"
  },
  {
    "text": "have our data across multiple indices we could pass in a comma-separated string of indices to use there obviously this",
    "start": "655819",
    "end": "663139"
  },
  {
    "text": "is all over rideable on an individual request basis these are just defaults that we can run with",
    "start": "663139",
    "end": "670420"
  },
  {
    "text": "so let's let's hit our first make our first request with the low-level client",
    "start": "671220",
    "end": "677220"
  },
  {
    "text": "so we're just going to perform a search here simple search on the posts index the question document type and we're",
    "start": "677220",
    "end": "684850"
  },
  {
    "text": "just going to perform a query here so we specified our query here with an anomalous type this is going to be",
    "start": "684850",
    "end": "690399"
  },
  {
    "text": "serialized into the JSON query DSL but you probably seen on this site we're",
    "start": "690399",
    "end": "696100"
  },
  {
    "text": "simply doing a match phrase query here this should never happen and we're just",
    "start": "696100",
    "end": "701770"
  },
  {
    "text": "going to take the response of that so that the the generic string type here on this on the search cool is the the",
    "start": "701770",
    "end": "707920"
  },
  {
    "text": "response type that we expect to get back so if we run this lump we have a massive",
    "start": "707920",
    "end": "714970"
  },
  {
    "text": "blob of JSON come back which is a very",
    "start": "714970",
    "end": "720940"
  },
  {
    "text": "easy to read like this so what we're going to do is we're going to run this",
    "start": "720940",
    "end": "727089"
  },
  {
    "text": "example again but now what we're going to do instead so we we have the same",
    "start": "727089",
    "end": "732520"
  },
  {
    "text": "query as before but what we're going to do instead is we're going to take advantage of some of the connection settings so what we're going to do is",
    "start": "732520",
    "end": "739240"
  },
  {
    "text": "we're going to disable direct streaming so we're going to capture we're going to buffer the request bytes and we're going",
    "start": "739240",
    "end": "744279"
  },
  {
    "text": "to buffer the response bytes and then when that request is that she completed we're going to logout the request and",
    "start": "744279",
    "end": "749529"
  },
  {
    "text": "the response to the console just so we can see what's going on so if all of these examples up stuffs what I'm going",
    "start": "749529",
    "end": "755200"
  },
  {
    "text": "to do so we can get an idea and understanding of what's the incentive elasticsearch and what's coming back so",
    "start": "755200",
    "end": "762040"
  },
  {
    "text": "if we run that same query again we can see it we can see it's a little bit",
    "start": "762040",
    "end": "768100"
  },
  {
    "text": "easier to read now so this is the query",
    "start": "768100",
    "end": "774010"
  },
  {
    "text": "that we sent in much phrase this should never happen so we have a 200 response back it took 61 milliseconds and inside",
    "start": "774010",
    "end": "782200"
  },
  {
    "text": "appear inside of source we should have a collection of sorry insider hits here we",
    "start": "782200",
    "end": "788649"
  },
  {
    "text": "have a collection of the documents within elasticsearch that were a match for our particular query so in each one",
    "start": "788649",
    "end": "795910"
  },
  {
    "text": "of these documents this source property here contains the original JSON document",
    "start": "795910",
    "end": "801160"
  },
  {
    "text": "that I sent in so we can see there's a few there's a two different properties here I said it",
    "start": "801160",
    "end": "806680"
  },
  {
    "text": "they atop the title of the question so what's your should never happen",
    "start": "806680",
    "end": "811840"
  },
  {
    "text": "exception the creation date the body which we can see as HTML here so we were",
    "start": "811840",
    "end": "817570"
  },
  {
    "text": "searching in the body so this should never happen so we should see that",
    "start": "817570",
    "end": "822970"
  },
  {
    "text": "somewhere yep should should never happen inside of the body and we can just see that we've",
    "start": "822970",
    "end": "829090"
  },
  {
    "text": "returned back the the first 10 records there and in total that's 521 questions",
    "start": "829090",
    "end": "835090"
  },
  {
    "text": "in Stack Overflow that have this should never happen in the body",
    "start": "835090",
    "end": "841680"
  },
  {
    "text": "so I'm just going to submit that with all with the low-level client we're now going to start looking at the high level",
    "start": "847760",
    "end": "852800"
  },
  {
    "text": "client and the rest of the the examples here are going to be working with that high level client so I've done a similar thing as before",
    "start": "852800",
    "end": "859550"
  },
  {
    "text": "I've disabled their experience we capture the request and response bytes and again I'm just going to be logging",
    "start": "859550",
    "end": "866269"
  },
  {
    "text": "those out to the console so down here we can see we're going to perform a search request specifying our question type",
    "start": "866269",
    "end": "873380"
  },
  {
    "text": "performing a match phrase query same couriers before we're just using the the",
    "start": "873380",
    "end": "878389"
  },
  {
    "text": "nest query DSL here instead and this query DSL is based around lambda",
    "start": "878389",
    "end": "884089"
  },
  {
    "text": "expressions so we can see here we can specify a query match phrase query we",
    "start": "884089",
    "end": "889880"
  },
  {
    "text": "want to run it on the body field and this is our particular query text and",
    "start": "889880",
    "end": "895570"
  },
  {
    "text": "you can what you'll be able to see here when I run when I run this query is that",
    "start": "895570",
    "end": "901190"
  },
  {
    "text": "the layer of this lambda API is very very similar to the layer of the JSON",
    "start": "901190",
    "end": "906769"
  },
  {
    "text": "query DSL but actually ends up getting sent elasticsearch so if you're ever looking at examples with the the JSON",
    "start": "906769",
    "end": "913940"
  },
  {
    "text": "query DSL and you're trying to translate that into what it looks like with nest you can you can almost you can almost",
    "start": "913940",
    "end": "920540"
  },
  {
    "text": "picture what they're going to they're going to end up looking like and we can see again that we have we have results",
    "start": "920540",
    "end": "927410"
  },
  {
    "text": "back here interestingly for these connection settings here I post I've",
    "start": "927410",
    "end": "934639"
  },
  {
    "text": "performed a search on questions but the actual the actual URL that has ended up",
    "start": "934639",
    "end": "940790"
  },
  {
    "text": "being hit here is the local host just to search so what I've ended up doing here is actually searching across all indices",
    "start": "940790",
    "end": "947779"
  },
  {
    "text": "and all types in all indices and that's because I didn't specify a default index",
    "start": "947779",
    "end": "953329"
  },
  {
    "text": "to use I didn't I specified a type here which was the question type I didn't",
    "start": "953329",
    "end": "958459"
  },
  {
    "text": "specify an index so if I was to add in an index now say to use the posts index",
    "start": "958459",
    "end": "967160"
  },
  {
    "text": "and run that one again we can see now we're just we're just looking at the",
    "start": "967160",
    "end": "973220"
  },
  {
    "text": "posts index elasticsearch and we're only looking at question types",
    "start": "973220",
    "end": "978459"
  },
  {
    "text": "so those of you that are less comfortable with using the lambda API",
    "start": "984020",
    "end": "989820"
  },
  {
    "text": "there's also an object initializer API as well so you can initialize a search",
    "start": "989820",
    "end": "995220"
  },
  {
    "text": "request object here same as before same as using the lambda API except you can",
    "start": "995220",
    "end": "1001430"
  },
  {
    "text": "work with objects here and pass your search request object in the results of this would be the same as as we saw",
    "start": "1001430",
    "end": "1007190"
  },
  {
    "text": "before",
    "start": "1007190",
    "end": "1009400"
  },
  {
    "text": "okay so we've kind of looked at the first kind of searches obviously elasticsearch can do more than just",
    "start": "1021410",
    "end": "1027060"
  },
  {
    "text": "search we actually need to get the data into there first of all so running through this particular example we're going to create an index",
    "start": "1027060",
    "end": "1035069"
  },
  {
    "text": "we're going to specify one charge about index so an index can be split into n number of shards where a charred itself",
    "start": "1035069",
    "end": "1041730"
  },
  {
    "text": "is a leucine index so we're just going to specify one shard for our elasticsearch index here we don't need",
    "start": "1041730",
    "end": "1048270"
  },
  {
    "text": "any replicas we're only running on my machine I don't need to replicate the shard to other machines so we're simply",
    "start": "1048270",
    "end": "1054330"
  },
  {
    "text": "going to run with one shard here I'm going to map my constant content type here that I've specified and just going",
    "start": "1054330",
    "end": "1063270"
  },
  {
    "text": "to new up a new conference so for here and I'm just going to index that into elasticsearch so if I run that one we",
    "start": "1063270",
    "end": "1076290"
  },
  {
    "text": "see this put request here so I've created the documents index with a bunch",
    "start": "1076290",
    "end": "1081780"
  },
  {
    "text": "of settings for which to use to create that specified my mapping which we'll look at in a sec and then I've just",
    "start": "1081780",
    "end": "1088500"
  },
  {
    "text": "posted a document to there and I've received the two are 201 back there to failure document has been created so",
    "start": "1088500",
    "end": "1095910"
  },
  {
    "text": "your typical HTTP response codes you'll get back four created okay bad requests etc etc",
    "start": "1095910",
    "end": "1104179"
  },
  {
    "text": "now of course if we're indexing at many many documents we don't want to be",
    "start": "1111250",
    "end": "1116409"
  },
  {
    "text": "inserting those documents one at a time so we can also create a couple of documents and index those both at the",
    "start": "1116409",
    "end": "1122679"
  },
  {
    "text": "same time now what what index many",
    "start": "1122679",
    "end": "1129400"
  },
  {
    "text": "actually ends up doing it uses a the bulk API behind the scenes there's a bulk API for performing a bulk of",
    "start": "1129400",
    "end": "1136450"
  },
  {
    "text": "operations a bunch of creation operations deletions update updates etc",
    "start": "1136450",
    "end": "1141610"
  },
  {
    "text": "so index many it's just a really nice shorthand for using the bulk API so we can see here that this has ended up",
    "start": "1141610",
    "end": "1148390"
  },
  {
    "text": "calling the bulk endpoint there are two documents there that we have indexed and we can see that we have a 201 response",
    "start": "1148390",
    "end": "1156039"
  },
  {
    "text": "back for each one of these documents the order of these items that come back are the same ordering as you saw it the",
    "start": "1156039",
    "end": "1163809"
  },
  {
    "text": "order in which we sent the bulk operations to them so we should see this what this one would be the Oslo Spektrum",
    "start": "1163809",
    "end": "1170169"
  },
  {
    "text": "Norway location and this one here would be the Hilton Sydney",
    "start": "1170169",
    "end": "1176100"
  },
  {
    "text": "and as I mentioned index many is really just the shorthand for the bulk API so",
    "start": "1178020",
    "end": "1183760"
  },
  {
    "text": "the bulk API here what we just saw is exactly the same as doing this here so",
    "start": "1183760",
    "end": "1189460"
  },
  {
    "text": "calling the bulk API calling create many passing it our collection of conferences",
    "start": "1189460",
    "end": "1194620"
  },
  {
    "text": "and indexing those into elasticsearch so",
    "start": "1194620",
    "end": "1204820"
  },
  {
    "text": "we can we can create we can index documents we can also update them",
    "start": "1204820",
    "end": "1209919"
  },
  {
    "text": "obviously so if I create a conference object here index it pull that document",
    "start": "1209919",
    "end": "1217030"
  },
  {
    "text": "back and then go and update it so here",
    "start": "1217030",
    "end": "1223929"
  },
  {
    "text": "we go but I put my conference object in are then pulled out I then change the",
    "start": "1223929",
    "end": "1232240"
  },
  {
    "text": "end date so then change the end date of the conference because we all want it to",
    "start": "1232240",
    "end": "1237610"
  },
  {
    "text": "finish tomorrow and then I've just updated that conference document there what I've also done here",
    "start": "1237610",
    "end": "1244750"
  },
  {
    "text": "elasticsearch of a concept of optimistic concurrency control so when I pulled that document back from back from",
    "start": "1244750",
    "end": "1252250"
  },
  {
    "text": "elasticsearch here I also got a version number back as well so the version of",
    "start": "1252250",
    "end": "1258490"
  },
  {
    "text": "that document as it exists in elasticsearch so that when I've gone and done my update now so I have deserialize",
    "start": "1258490",
    "end": "1264940"
  },
  {
    "text": "that I have a conference object now in my application when I go to make the update it's possible that someone else",
    "start": "1264940",
    "end": "1270880"
  },
  {
    "text": "has modified that document in between so I can also send the version number back that I received when I originally",
    "start": "1270880",
    "end": "1277180"
  },
  {
    "text": "searched the document I can send that back and update the document so if the document has changed and the version",
    "start": "1277180",
    "end": "1283660"
  },
  {
    "text": "numbers changed I should get thought I would get a four a nine conflict back to say hey this document has actually",
    "start": "1283660",
    "end": "1289480"
  },
  {
    "text": "changed in between you fetching it and you updating it so we don't actually",
    "start": "1289480",
    "end": "1297520"
  },
  {
    "text": "have to we don't actually have to grab the document back each time we can just",
    "start": "1297520",
    "end": "1302710"
  },
  {
    "text": "perform an update instead by sending",
    "start": "1302710",
    "end": "1307750"
  },
  {
    "text": "just a partial document to elasticsearch so again create a conference but now I",
    "start": "1307750",
    "end": "1312940"
  },
  {
    "text": "can just use an anonymous type that has the same property name as the end date",
    "start": "1312940",
    "end": "1317980"
  },
  {
    "text": "here on my conference I can send that to elasticsearch specifying the idea of the document that I want to update and let",
    "start": "1317980",
    "end": "1325540"
  },
  {
    "text": "elasticsearch do the perform the update on elasticsearch itself so we just have",
    "start": "1325540",
    "end": "1331690"
  },
  {
    "text": "a quick look at what the JSON ends up looking like for that create my conference get a 201 response back and",
    "start": "1331690",
    "end": "1338760"
  },
  {
    "text": "then I just post my document just purely with the end date that I want to change",
    "start": "1338760",
    "end": "1344040"
  },
  {
    "text": "so receive a 200 back there and then I finally fetch the document here just to",
    "start": "1344040",
    "end": "1349480"
  },
  {
    "text": "prove that that that end date has actually been changed as you can see here so partial updates good for when",
    "start": "1349480",
    "end": "1358120"
  },
  {
    "text": "you just want to modify things about going faxing fetching documents and",
    "start": "1358120",
    "end": "1365250"
  },
  {
    "text": "similarly we can also delete individual documents as well we can also delete many documents in older versions of",
    "start": "1365250",
    "end": "1371350"
  },
  {
    "text": "elasticsearch there was a delete many API that allows you to specify a query for a bunch",
    "start": "1371350",
    "end": "1377290"
  },
  {
    "text": "documents that you wanted to delete and in newer versions of elasticsearch there's a task-based api for performing",
    "start": "1377290",
    "end": "1383440"
  },
  {
    "text": "that deletion as well so that was kind",
    "start": "1383440",
    "end": "1394780"
  },
  {
    "text": "of a brief rundown of all of the crud operations and a brief look at some searches so you have a JSON blob that",
    "start": "1394780",
    "end": "1403030"
  },
  {
    "text": "you send into elasticsearch you you've indexed that document now what actually happens so you have represented",
    "start": "1403030",
    "end": "1410740"
  },
  {
    "text": "elasticsearch cluster there as a gray thundercloud there to represent the",
    "start": "1410740",
    "end": "1415900"
  },
  {
    "text": "speed which it operates lightning-fast so we now need to look at mappings so",
    "start": "1415900",
    "end": "1423900"
  },
  {
    "text": "we've indexed a document what what is actually going to happen so by default",
    "start": "1424050",
    "end": "1434680"
  },
  {
    "text": "elasticsearch is as I said before it's schema-less but when you send a JSON",
    "start": "1434680",
    "end": "1441370"
  },
  {
    "text": "document to elasticsearch it's going to by default first of all if you send it",
    "start": "1441370",
    "end": "1446920"
  },
  {
    "text": "to an index that doesn't yet exist it will create the index for you and then what it will also do is it will infer",
    "start": "1446920",
    "end": "1453720"
  },
  {
    "text": "the mapping for the document that you sent to it based on the properties of",
    "start": "1453720",
    "end": "1458890"
  },
  {
    "text": "that document so for example if it sees ISO 8601 strings it will expect most to",
    "start": "1458890",
    "end": "1465310"
  },
  {
    "text": "be date times if it sees integers it will map those as integers so on and so",
    "start": "1465310",
    "end": "1470590"
  },
  {
    "text": "forth for all the primitive types you can imagine where it sees a an object",
    "start": "1470590",
    "end": "1475900"
  },
  {
    "text": "property it will map that as an object type so you can have nested objects",
    "start": "1475900",
    "end": "1480910"
  },
  {
    "text": "within your JSON document so in this particular example here this is this is",
    "start": "1480910",
    "end": "1486880"
  },
  {
    "text": "my stackoverflow user with a bunch of different properties ID reputation",
    "start": "1486880",
    "end": "1493230"
  },
  {
    "text": "profile age and they have a collection of badges so these are the badges for",
    "start": "1493230",
    "end": "1498250"
  },
  {
    "text": "questions that they've answered they've received a question received a badge for the date which they've received that",
    "start": "1498250",
    "end": "1504240"
  },
  {
    "text": "whether it's a gold silver or bronze badge and a name for the badge as well so the",
    "start": "1504240",
    "end": "1511020"
  },
  {
    "text": "name for the badge is that usually the language which they advanced that they've received this badge for us a C",
    "start": "1511020",
    "end": "1516270"
  },
  {
    "text": "sharp F sharp etc so within nest we have",
    "start": "1516270",
    "end": "1522120"
  },
  {
    "text": "when you want to map a particular strong type so those those default mappings are",
    "start": "1522120",
    "end": "1528180"
  },
  {
    "text": "usually good for when you want to get up and running but for most scenarios you actually want to control the mapping for",
    "start": "1528180",
    "end": "1534120"
  },
  {
    "text": "your documents the reason you want to do that is there are various you know there are domains in which you operate you",
    "start": "1534120",
    "end": "1540930"
  },
  {
    "text": "know more about your data than elasticsearch does so for example in the Stack Overflow questions the body of",
    "start": "1540930",
    "end": "1547890"
  },
  {
    "text": "those the body of each question is HTML there's a bunch of HTML tags in there do",
    "start": "1547890",
    "end": "1554250"
  },
  {
    "text": "we want to be when we search do we want to be searching with HTML tags in there no we don't but by default what",
    "start": "1554250",
    "end": "1561690"
  },
  {
    "text": "elasticsearch will do would just map that using a standard analyzer and we'll",
    "start": "1561690",
    "end": "1567090"
  },
  {
    "text": "end up indexing a bunch of HTML tags as well which is what we don't want to do",
    "start": "1567090",
    "end": "1573860"
  },
  {
    "text": "so we look at mapping I use a type here we're going to specify that it's going",
    "start": "1573890",
    "end": "1579360"
  },
  {
    "text": "to go into the users index and we're just going to say to the client look at",
    "start": "1579360",
    "end": "1585390"
  },
  {
    "text": "this user type here look at all of the properties on on it and tell elasticsearch which properties to use",
    "start": "1585390",
    "end": "1591840"
  },
  {
    "text": "for the mapping for this user type so if we run this one we can see here so the",
    "start": "1591840",
    "end": "1600540"
  },
  {
    "text": "properties here are yet the eyes the ID of the user is an integer reputation it",
    "start": "1600540",
    "end": "1606270"
  },
  {
    "text": "comes through as an integer state some offset a date so on so on and so forth it's mapped badges here as an object",
    "start": "1606270",
    "end": "1612900"
  },
  {
    "text": "type and it's also recursed down there and seen that each badge type also has a",
    "start": "1612900",
    "end": "1619050"
  },
  {
    "text": "string and integer and a date as well for for the name class and date so",
    "start": "1619050",
    "end": "1626550"
  },
  {
    "text": "although muffinz great that gets us going but we might want a bit more control than that and that's where",
    "start": "1626550",
    "end": "1633960"
  },
  {
    "text": "properties come in so we can also map to say to say to the client in further",
    "start": "1633960",
    "end": "1640710"
  },
  {
    "text": "types that you need to send a lot such a user type but now I actually want to override your in your the clients in",
    "start": "1640710",
    "end": "1648580"
  },
  {
    "text": "third types so I want to map age here as a bite in elasticsearch because I don't",
    "start": "1648580",
    "end": "1654190"
  },
  {
    "text": "want it taking up in integers worth of space no one no one's going to be older than 255 so I'm just going to map age as",
    "start": "1654190",
    "end": "1663550"
  },
  {
    "text": "a type I still want to work with it as an integer or as a nullable integers it is in in my application but I want to",
    "start": "1663550",
    "end": "1670720"
  },
  {
    "text": "map it as a byte in elasticsearch and I also want to map badges as a nested type as well I don't want them to be object",
    "start": "1670720",
    "end": "1678070"
  },
  {
    "text": "types I want them to be nested typed so I can also say auto map badges as well",
    "start": "1678070",
    "end": "1684430"
  },
  {
    "text": "and then I want to override the properties there and whenever you see the class for gold silver bronze badge I",
    "start": "1684430",
    "end": "1690270"
  },
  {
    "text": "want you to not only analyze the the actual text that comes in for that class",
    "start": "1690270",
    "end": "1697180"
  },
  {
    "text": "badge so silver gold bronze I also want you to just store it verbatim as well I",
    "start": "1697180",
    "end": "1702850"
  },
  {
    "text": "want you to store also what comes in and I want you to store that in a field that's going to end up being called",
    "start": "1702850",
    "end": "1708640"
  },
  {
    "text": "class rule so I'm matching mapping the class property of a badge here in two",
    "start": "1708640",
    "end": "1715150"
  },
  {
    "text": "different ways I'm having it analyzed because I may want to perform some searches on it which we'll have a look",
    "start": "1715150",
    "end": "1720370"
  },
  {
    "text": "at in a second and I also want to store it for Batum as well so that I can aggregate on it and see the original",
    "start": "1720370",
    "end": "1726490"
  },
  {
    "text": "values so if we have if we have a look at this this one here and we look at I",
    "start": "1726490",
    "end": "1736930"
  },
  {
    "text": "use a type here and we have a look at a badge type so badges is being mapped as",
    "start": "1736930",
    "end": "1744310"
  },
  {
    "text": "a nested type our class type here has the type of string so it's going to be",
    "start": "1744310",
    "end": "1750370"
  },
  {
    "text": "analyzed but then it also has this fields object here as well with a rule",
    "start": "1750370",
    "end": "1756160"
  },
  {
    "text": "with a rule field a rule multi field as they're called a multi field as they're called underneath as well which is in",
    "start": "1756160",
    "end": "1762640"
  },
  {
    "text": "not analyzed field",
    "start": "1762640",
    "end": "1765690"
  },
  {
    "text": "another way that we can map as well so we can we can auto map we can override",
    "start": "1773130",
    "end": "1778180"
  },
  {
    "text": "the mappings using properties we can also pass a visitor in as well to say for example we wanted to map all of our",
    "start": "1778180",
    "end": "1784570"
  },
  {
    "text": "string properties on our on our user and we also wanted to set up a rule mapping there we wanted a store verbatim the",
    "start": "1784570",
    "end": "1791470"
  },
  {
    "text": "original text all of the string properties of a user we can use a we can",
    "start": "1791470",
    "end": "1796780"
  },
  {
    "text": "use the visitor pattern to do that as well so we can use our not analyze visitor here we can specify a rule field",
    "start": "1796780",
    "end": "1803700"
  },
  {
    "text": "specified as being not and not analyzed and we can pass our visitor then into",
    "start": "1803700",
    "end": "1808720"
  },
  {
    "text": "the auto map function so that's going to as it's going through and inferring all the string all the strings field of our",
    "start": "1808720",
    "end": "1815080"
  },
  {
    "text": "user it's also going to map each one as a rule field as well so if we just run",
    "start": "1815080",
    "end": "1821590"
  },
  {
    "text": "that one we can see here that display name yet comes through all of all of the string fields have also have a rule not",
    "start": "1821590",
    "end": "1828850"
  },
  {
    "text": "analyzed field as well",
    "start": "1828850",
    "end": "1832080"
  },
  {
    "text": "sort of a brief detour and mapping so we've indexed we looked at how we want",
    "start": "1837230",
    "end": "1843350"
  },
  {
    "text": "to map our JSON documents now we need to have a look at exactly how individual",
    "start": "1843350",
    "end": "1848450"
  },
  {
    "text": "fields there on our documents are going to be analyzed and this is really kind",
    "start": "1848450",
    "end": "1855080"
  },
  {
    "text": "of if if there's nothing else you take away from this talk this is probably the one thing that's worth knowing when you",
    "start": "1855080",
    "end": "1863540"
  },
  {
    "text": "index a string field in silastic search you undergoes analysis by default and",
    "start": "1863540",
    "end": "1869120"
  },
  {
    "text": "the analysis that undergoes follows this kind of analysis change it uses an",
    "start": "1869120",
    "end": "1875630"
  },
  {
    "text": "analyzer and an analyzer is made up of zero or more character filters a",
    "start": "1875630",
    "end": "1880690"
  },
  {
    "text": "tokenizer and zero more token filters so what character filters do first of all",
    "start": "1880690",
    "end": "1887419"
  },
  {
    "text": "they will pre process that string maybe we want to strip HTML characters maybe",
    "start": "1887419",
    "end": "1893210"
  },
  {
    "text": "we want to replace C++ with symbols with C++ words then it's going to go to the",
    "start": "1893210",
    "end": "1901190"
  },
  {
    "text": "tokenizer the tokenizer is going to take that entire text and split it into tokens and then the token filters are",
    "start": "1901190",
    "end": "1908059"
  },
  {
    "text": "going to do additional things on top of those tokens so they may remove tokens",
    "start": "1908059",
    "end": "1913220"
  },
  {
    "text": "that we don't want there so stop words for example the R etc maybe they provide",
    "start": "1913220",
    "end": "1920059"
  },
  {
    "text": "synonyms for words there as well maybe [Music] maybe they do a bunch of different",
    "start": "1920059",
    "end": "1926510"
  },
  {
    "text": "things as well so the standard analyzers is the one as I said that normally gets applied to string fields so it has no",
    "start": "1926510",
    "end": "1934940"
  },
  {
    "text": "character filters but the tokenizer that it uses uses Unicode text segmentation to break down that strip that string of",
    "start": "1934940",
    "end": "1942260"
  },
  {
    "text": "characters into words into tokens and then it runs it through a bunch of token",
    "start": "1942260",
    "end": "1949549"
  },
  {
    "text": "filters here so it lower cases it removes stop words so if we were to look",
    "start": "1949549",
    "end": "1956179"
  },
  {
    "text": "at basically what how that happens if someone makes the claim f-sharp is",
    "start": "1956179",
    "end": "1961610"
  },
  {
    "text": "clearly the superior language then running that through the standard",
    "start": "1961610",
    "end": "1966679"
  },
  {
    "text": "tokenizer we're going to get split up into these Brookins by the standard tokenizer so we",
    "start": "1966679",
    "end": "1972630"
  },
  {
    "text": "can see that the the sharp symbol is being removed serves the stars and as",
    "start": "1972630",
    "end": "1978660"
  },
  {
    "text": "that's passed through the standard token filter which is a placeholder doesn't do anything right now so that gets passed",
    "start": "1978660",
    "end": "1984450"
  },
  {
    "text": "through exactly as it came from the standard tokenizer then gets lowercase and then stop words get removed in this",
    "start": "1984450",
    "end": "1991980"
  },
  {
    "text": "particular case in the normal fund analyzer configuration no stop words",
    "start": "1991980",
    "end": "1997050"
  },
  {
    "text": "that you get removed there and then what happens for those tokens they go into the inverted index so the original field",
    "start": "1997050",
    "end": "2005540"
  },
  {
    "text": "from which these tokens came from title and then we have LexA lexicographically",
    "start": "2005540",
    "end": "2011120"
  },
  {
    "text": "sorted the individual tokens from that analysis stored in the inverted index",
    "start": "2011120",
    "end": "2016190"
  },
  {
    "text": "and then the ID from which that from which that token originally came from the document IDs from which token as you",
    "start": "2016190",
    "end": "2023720"
  },
  {
    "text": "can for can see someone else has made the claim here for JavaScript being the superior language as well so let's have",
    "start": "2023720",
    "end": "2033500"
  },
  {
    "text": "a brief look at how analyzers work so I've got a very very contrived example",
    "start": "2033500",
    "end": "2039350"
  },
  {
    "text": "here first of all to have a look at I have my type here a semantic version type has an ID has a version string for",
    "start": "2039350",
    "end": "2046730"
  },
  {
    "text": "semantic version string so we all familiar with semantic versions yep so",
    "start": "2046730",
    "end": "2053120"
  },
  {
    "text": "what we're going to do is we're going to specify a bunch of token filters and",
    "start": "2053120",
    "end": "2058638"
  },
  {
    "text": "what these token filters are basically going to do because semantic version numbers they're effectively strings we",
    "start": "2058639",
    "end": "2066080"
  },
  {
    "text": "have maybe three or four elements their main a major minor patch maybe a build",
    "start": "2066080",
    "end": "2071960"
  },
  {
    "text": "maybe then a pre-release suffix as well we what we want to do is we want to take",
    "start": "2071960",
    "end": "2077810"
  },
  {
    "text": "that but we want to pad out each of a major minor build such that for example",
    "start": "2077810",
    "end": "2084470"
  },
  {
    "text": "semantic version 11 comes with comes after semantic version two so if we're",
    "start": "2084470",
    "end": "2090408"
  },
  {
    "text": "just looking at these lexicographically we would find that 11 would end up preparing before 2 so with this analyzer",
    "start": "2090409",
    "end": "2097520"
  },
  {
    "text": "here we're specifying a bunch of token filters they're going to pad out each each number there and then",
    "start": "2097520",
    "end": "2103820"
  },
  {
    "text": "so preserve that fruit that pre-release suffix as well so we have a bunch of",
    "start": "2103820",
    "end": "2109970"
  },
  {
    "text": "token filters to do that that are going to pad out the various sections and then we have a token filter at the end that's",
    "start": "2109970",
    "end": "2116420"
  },
  {
    "text": "going to then d-pad all of those filters and we put all of those together under",
    "start": "2116420",
    "end": "2123110"
  },
  {
    "text": "an analyzer a custom analyzer that we have here called semantic version tokenizer we're going to use is a",
    "start": "2123110",
    "end": "2128450"
  },
  {
    "text": "keyword one which is basically just going to take the entire semantic version string as it is and just pass it",
    "start": "2128450",
    "end": "2134270"
  },
  {
    "text": "to the token I - the token filters and then we've just provided some mapping for a semantic version here so if we run",
    "start": "2134270",
    "end": "2141350"
  },
  {
    "text": "this one we've created our index with our with our custom analyzer then we",
    "start": "2141350",
    "end": "2149510"
  },
  {
    "text": "want to we want to check to make sure that our analyzer is actually doing what we think it should be doing so we can",
    "start": "2149510",
    "end": "2155060"
  },
  {
    "text": "use the analyze API here to do it so we have we have a one point two zero here we have one dot 149 zero alpha three so",
    "start": "2155060",
    "end": "2164900"
  },
  {
    "text": "yeah they still haven't released their version so we're just going to run those through specifying our semantic version",
    "start": "2164900",
    "end": "2172340"
  },
  {
    "text": "analyzer we just set up and see what we get back there so we can see this is the",
    "start": "2172340",
    "end": "2177410"
  },
  {
    "text": "token that's going to get generated for the first one so it's nicely padded out 1 2 etc etc it's also pended these herbs",
    "start": "2177410",
    "end": "2186860"
  },
  {
    "text": "on the end because this isn't a pre-release and we want the the actual releases to appear after any pre",
    "start": "2186860",
    "end": "2193520"
  },
  {
    "text": "releases and we can see that similarly that we also have this token is padded",
    "start": "2193520",
    "end": "2200330"
  },
  {
    "text": "out here as well so our analyzer seems to be doing what we think it should be doing but we can we can run a little",
    "start": "2200330",
    "end": "2206960"
  },
  {
    "text": "test on that as well so we can index all of the semantic versions from 0 0 0 alpha 1 all the way up to 19 19 19 so",
    "start": "2206960",
    "end": "2214910"
  },
  {
    "text": "let's index all of those",
    "start": "2214910",
    "end": "2217780"
  },
  {
    "text": "so we've indexed those and now let's run a term range query on our semantic",
    "start": "2225279",
    "end": "2230809"
  },
  {
    "text": "version index so we want to be looking for numbers between 1 to 0 alpha or 1 to",
    "start": "2230809",
    "end": "2236809"
  },
  {
    "text": "0 a and 1 to 1 so what we see we get",
    "start": "2236809",
    "end": "2244130"
  },
  {
    "text": "back here is 1 to 0 alpha 1 alpha 2 alpha 3 etc etc and we're just sorting on the the key here that the actual",
    "start": "2244130",
    "end": "2251900"
  },
  {
    "text": "taupe which would end up being the tokenized the analyzed key from that",
    "start": "2251900",
    "end": "2258548"
  },
  {
    "text": "interesting to point out here term range query itself so some queries undergo analysis some queries don't analyze the",
    "start": "2259390",
    "end": "2266900"
  },
  {
    "text": "input to the query the term range query here doesn't perform any analysis on the input to the query so what we're doing",
    "start": "2266900",
    "end": "2274279"
  },
  {
    "text": "here is we're cheating a little bit we're actually expanding these tokens out as well in a similar similar way to",
    "start": "2274279",
    "end": "2281779"
  },
  {
    "text": "which our analyzer works so that we can see we can search on those ranges in a sensible way and then we can have a look",
    "start": "2281779",
    "end": "2291440"
  },
  {
    "text": "also then it's worth to see well I expected this document to match and I didn't expect this document to match you",
    "start": "2291440",
    "end": "2298430"
  },
  {
    "text": "know what's going on there so we can we can ask elasticsearch to say hey why did",
    "start": "2298430",
    "end": "2303650"
  },
  {
    "text": "this one match why did this one not match so given the idea of semantic version 4000 one of 8,000 and this",
    "start": "2303650",
    "end": "2310549"
  },
  {
    "text": "particular query tell me whether it matches or not so we can see for the",
    "start": "2310549",
    "end": "2316489"
  },
  {
    "text": "first one yes within the explanation we can see a description here that it did",
    "start": "2316489",
    "end": "2323029"
  },
  {
    "text": "match because it was in this version range and if we look at the second one we can see there's a failure to meet",
    "start": "2323029",
    "end": "2328969"
  },
  {
    "text": "condition so 8,000 didn't the somatic version with ID 8,000 wasn't within this",
    "start": "2328969",
    "end": "2334190"
  },
  {
    "text": "range we can have a little look and see why that might be the case so yep the",
    "start": "2334190",
    "end": "2341660"
  },
  {
    "text": "semantic version is at 2 to 0 alpha 1 so it wasn't within the range that we were",
    "start": "2341660",
    "end": "2346759"
  },
  {
    "text": "specifying in our query",
    "start": "2346759",
    "end": "2349930"
  },
  {
    "text": "so let's have a little brief look at queries okay so we're not going to be looking for line or this time but we're",
    "start": "2354980",
    "end": "2362640"
  },
  {
    "text": "going to be looking at some Stack Overflow data so touch briefly on queries and filters queries essentially",
    "start": "2362640",
    "end": "2369990"
  },
  {
    "text": "fall into two kind of buckets you have structured queries so does this particular value fall within this range",
    "start": "2369990",
    "end": "2376020"
  },
  {
    "text": "that does it match this particular value does it match this reg X does it match",
    "start": "2376020",
    "end": "2381330"
  },
  {
    "text": "this particular prefix and then you have unstructured full-text queries these are",
    "start": "2381330",
    "end": "2387030"
  },
  {
    "text": "more kind of a blended more of a more of a fuzzy kind of query so tell me stuff",
    "start": "2387030",
    "end": "2393630"
  },
  {
    "text": "give it give it a score to tell me how relevant this thing is to my particular query and that uses things such as",
    "start": "2393630",
    "end": "2401190"
  },
  {
    "text": "tf-idf BM 25 they're two different types of similarity algorithms to determine a",
    "start": "2401190",
    "end": "2406860"
  },
  {
    "text": "score for a particular document and so let's have a look at list of a look at a",
    "start": "2406860",
    "end": "2413100"
  },
  {
    "text": "few different queries so again we're just indexing our conferences the ones that we saw before one four zero one",
    "start": "2413100",
    "end": "2419910"
  },
  {
    "text": "four one four here and then we're just going to perform a query on those and we're just going to look for those that",
    "start": "2419910",
    "end": "2426660"
  },
  {
    "text": "contain spectrum or Sydney match for spectrum of Sydney so we can see we've",
    "start": "2426660",
    "end": "2434010"
  },
  {
    "text": "got a back we got a match for both of them but we can see here those at the the hilton sydney document was actually",
    "start": "2434010",
    "end": "2441180"
  },
  {
    "text": "scored higher than our oslo spektrum norway document whilst looking at the",
    "start": "2441180",
    "end": "2448410"
  },
  {
    "text": "location field does it does anyone know why that might be the case okay so yeah",
    "start": "2448410",
    "end": "2456240"
  },
  {
    "text": "so it comes down to something called norms which is based essentially on the length of that individual field so",
    "start": "2456240",
    "end": "2463590"
  },
  {
    "text": "although a spectrum Norway's having three words and being a longer being a longer field than Hilton Sydney Hilton",
    "start": "2463590",
    "end": "2469950"
  },
  {
    "text": "Sydney has ended up being scored higher than then also spectrum Norway so what we can do is we could look at just",
    "start": "2469950",
    "end": "2475740"
  },
  {
    "text": "disabling norms for that particular field when we index it so we can go and",
    "start": "2475740",
    "end": "2480960"
  },
  {
    "text": "have a look at using a non disabled so when it visits when it visits any",
    "start": "2480960",
    "end": "2486570"
  },
  {
    "text": "particular string query let's disable norms for that field when we actually construct the the mapping for that for",
    "start": "2486570",
    "end": "2493620"
  },
  {
    "text": "that type and let's perform the same query again and we can see now that we",
    "start": "2493620",
    "end": "2501690"
  },
  {
    "text": "get the same score out now if the hilton Sydney and for also spectrum Norway so",
    "start": "2501690",
    "end": "2513120"
  },
  {
    "text": "we can also perform a term query here so let's index those same two documents again have a look at Sydney so we don't",
    "start": "2513120",
    "end": "2521880"
  },
  {
    "text": "get a match what's going on forth so we",
    "start": "2521880",
    "end": "2527790"
  },
  {
    "text": "have a look back to the analysis is because this location field has been analyzed and it's been analyzed with the",
    "start": "2527790",
    "end": "2533460"
  },
  {
    "text": "standard analyzer so it's being lowercased so as it's gone through analyzer it's been tokenized and then",
    "start": "2533460",
    "end": "2539970"
  },
  {
    "text": "it's in lower case so a term query here doesn't perform any analysis on the",
    "start": "2539970",
    "end": "2545400"
  },
  {
    "text": "inputs of the query so we specified upper case Sydney here we don't find the match and we specify lower case Sydney",
    "start": "2545400",
    "end": "2553520"
  },
  {
    "text": "you should expect to find a match which we do so we get our Hilton Sydney document back we can also use the",
    "start": "2553520",
    "end": "2561350"
  },
  {
    "text": "verbatim rule field that we also set up here as well so using location suffix",
    "start": "2561350",
    "end": "2567870"
  },
  {
    "text": "rules so we can still strong type against the location field here but then specified to use the rule field that we",
    "start": "2567870",
    "end": "2574560"
  },
  {
    "text": "set up earlier and then we can use the original value that we actually provided here so we would expect to see a match",
    "start": "2574560",
    "end": "2581100"
  },
  {
    "text": "here for Hilton Sydney as well which we do",
    "start": "2581100",
    "end": "2585200"
  },
  {
    "text": "so then we can also combine queries together here so - too much queries here",
    "start": "2589670",
    "end": "2595590"
  },
  {
    "text": "one for Sydney once the spectrum and we we've put those inside of a boolean",
    "start": "2595590",
    "end": "2601140"
  },
  {
    "text": "query and assured query here so either-or and then we've also applied a",
    "start": "2601140",
    "end": "2606150"
  },
  {
    "text": "filter here as well so filter is not going to influence the scoring but it's",
    "start": "2606150",
    "end": "2611760"
  },
  {
    "text": "just going to filter on those that have a name that contains NDC and also that now going to boost anything that",
    "start": "2611760",
    "end": "2618000"
  },
  {
    "text": "contains spectrum we're going to give it a boost of two so we would expect to see those appear higher up then those from",
    "start": "2618000",
    "end": "2624000"
  },
  {
    "text": "Sydney yep so we see our also spectrum",
    "start": "2624000",
    "end": "2631710"
  },
  {
    "text": "Norway appear first and as I mentioned earlier we actually have a shorthand for",
    "start": "2631710",
    "end": "2637950"
  },
  {
    "text": "this so boolean queries compound queries of a very common case so within nest we",
    "start": "2637950",
    "end": "2642990"
  },
  {
    "text": "have a way of actually shortcutting that so with operator overloading so here we can specify the same query again we can",
    "start": "2642990",
    "end": "2650370"
  },
  {
    "text": "or these two queries together and then we can and this filter query here so",
    "start": "2650370",
    "end": "2655470"
  },
  {
    "text": "using the unary plus there we can specify a billion filter there and we",
    "start": "2655470",
    "end": "2660750"
  },
  {
    "text": "can run this one again we will get the same out but you can see that this ends up being translated to a boolean should",
    "start": "2660750",
    "end": "2667440"
  },
  {
    "text": "query with our too much queries and a boolean filter there as well so yep I",
    "start": "2667440",
    "end": "2678510"
  },
  {
    "text": "briefly touched on covariant types I have questions and answers within the same particular index in aiesec search",
    "start": "2678510",
    "end": "2685020"
  },
  {
    "text": "here so we can run a query here looking for a sink c-sharp a sink awake with",
    "start": "2685020",
    "end": "2690570"
  },
  {
    "text": "weight within the body and then all I'm doing is dumping out just that the type that that particular document has been",
    "start": "2690570",
    "end": "2696840"
  },
  {
    "text": "deserialized to so we have a bunch of answer types here and a bunch of question types so that was kind of",
    "start": "2696840",
    "end": "2705360"
  },
  {
    "text": "looking at text and looking at term queries and a couple of range queries now let's have a look at a geo query so",
    "start": "2705360",
    "end": "2712830"
  },
  {
    "text": "as well as all of the stack overflow data I've also taken all of the Australian suburbs and I've index those",
    "start": "2712830",
    "end": "2718230"
  },
  {
    "text": "into lastic search so we we have the Geo JSON Gong's for every single Australian",
    "start": "2718230",
    "end": "2724920"
  },
  {
    "text": "suburb here so let's have a look and see which suburb this this conference has",
    "start": "2724920",
    "end": "2730710"
  },
  {
    "text": "taken place in apparently we're in Willem Alou which I don't think is true",
    "start": "2730710",
    "end": "2737970"
  },
  {
    "text": "but this is based on 2006 suburb data so",
    "start": "2737970",
    "end": "2743310"
  },
  {
    "text": "I've just done a geo shape point query here specifying the geometry field which",
    "start": "2743310",
    "end": "2748350"
  },
  {
    "text": "contains the actual polygon or multi polygon for a particular suburb and I've",
    "start": "2748350",
    "end": "2754050"
  },
  {
    "text": "just passed it the coordinates for this so the Hilton Sydney so we can see Willem Alou there so taking that a",
    "start": "2754050",
    "end": "2760800"
  },
  {
    "text": "little bit further why don't we try and find the surrounding suburbs to Willem Alou so again we can use a geo query",
    "start": "2760800",
    "end": "2766980"
  },
  {
    "text": "here and we can do an intersection so let's grab the polygon for Willem Alou and tell me which other suburbs",
    "start": "2766980",
    "end": "2774000"
  },
  {
    "text": "intersect with this given with this given suburb so we got you go and grab the suburb let's get the coordinates",
    "start": "2774000",
    "end": "2781080"
  },
  {
    "text": "from it and then perform a Jewish shoot polygon and exclude this particular",
    "start": "2781080",
    "end": "2786570"
  },
  {
    "text": "suburb as well we don't want to get the original suburb that we're passing in back in the results so we can run that",
    "start": "2786570",
    "end": "2793680"
  },
  {
    "text": "one we can see there's a massive a bunch of Geo JSON here representing the polygon and this is our this is the",
    "start": "2793680",
    "end": "2800369"
  },
  {
    "text": "start of our query and then we can see here that Potts point Sydney",
    "start": "2800369",
    "end": "2806670"
  },
  {
    "text": "Darlinghurst are the intersecting suburbs with with Willem Alou now we",
    "start": "2806670",
    "end": "2815580"
  },
  {
    "text": "don't need to go and grab the polygon we can actually just say to elasticsearch hey I want to find the intersecting",
    "start": "2815580",
    "end": "2820859"
  },
  {
    "text": "polygons to a polygon that already exists in elasticsearch so use this use this ID and go and actually perform that",
    "start": "2820859",
    "end": "2828570"
  },
  {
    "text": "intersection on elasticsearch without me fetching without me fetching it so this will give you the same result here as",
    "start": "2828570",
    "end": "2834240"
  },
  {
    "text": "well just using the Geo index shape and",
    "start": "2834240",
    "end": "2839270"
  },
  {
    "text": "here we're only returning the names of the suburbs here we don't want the polygons for the suburbs coming back as",
    "start": "2839270",
    "end": "2845910"
  },
  {
    "text": "well we only want the names so that's using fields there just to specify the name",
    "start": "2845910",
    "end": "2852380"
  },
  {
    "text": "that was a kind of a brief tour of queries geo queries term queries seller",
    "start": "2853390",
    "end": "2859280"
  },
  {
    "text": "saw a brief look at aggregations they kind of generally fall into three categories metrics they can give you",
    "start": "2859280",
    "end": "2865400"
  },
  {
    "text": "single new numeric values or multi numeric values so stats for example give you the mean median average and a",
    "start": "2865400",
    "end": "2872780"
  },
  {
    "text": "deviation variance etc buckets is pick a bunch of documents and put them into",
    "start": "2872780",
    "end": "2879050"
  },
  {
    "text": "individual buckets based on some particular query or based on some aspect of each document so there are ways of",
    "start": "2879050",
    "end": "2886100"
  },
  {
    "text": "doing that to say specify put questions into particular weeks in which they were",
    "start": "2886100",
    "end": "2891950"
  },
  {
    "text": "asked based on the their creation date or we may specify to put questions into",
    "start": "2891950",
    "end": "2898280"
  },
  {
    "text": "buckets based on the the tags that they've been tagged with so this question about c-sharp question about",
    "start": "2898280",
    "end": "2903710"
  },
  {
    "text": "PHP etc the nice thing with bucket aggregations we can aggregate on those aggregations so we can specify buckets",
    "start": "2903710",
    "end": "2909860"
  },
  {
    "text": "and then specify buckets for those buckets and buckets for those buckets and n buckets deep and pipeline",
    "start": "2909860",
    "end": "2916040"
  },
  {
    "text": "aggregations there as well those work on the output of other aggregations so we will see an example of these ones here",
    "start": "2916040",
    "end": "2924490"
  },
  {
    "text": "so let's start off simple let's just do a search using Mexico aggregation so",
    "start": "2925570",
    "end": "2931850"
  },
  {
    "text": "find me the the highest-scoring question on Stack Overflow so the highest score",
    "start": "2931850",
    "end": "2941570"
  },
  {
    "text": "there is fourteen thousand seven hundred seventy-two and it's a question here",
    "start": "2941570",
    "end": "2946610"
  },
  {
    "text": "about wise process processing a sorted array faster than an unsorted array there's a question about Java so that's",
    "start": "2946610",
    "end": "2956360"
  },
  {
    "text": "great so we've seen the max score excellent let's go just go a little bit further let's take questions and let's",
    "start": "2956360",
    "end": "2964580"
  },
  {
    "text": "look at the top five tag questions on Stack Overflow and then for each of those top five tags let's calculate some",
    "start": "2964580",
    "end": "2971630"
  },
  {
    "text": "statistics for them so meme media and etc and just simply dump those out in a table now so we can easily see those",
    "start": "2971630",
    "end": "2980650"
  },
  {
    "text": "so most questions about JavaScript however the highest-scoring question as",
    "start": "2981130",
    "end": "2988120"
  },
  {
    "text": "we saw before was that you Java but if we look here a c-sharp guys with our",
    "start": "2988120",
    "end": "2995230"
  },
  {
    "text": "questions are generally scored a little bit higher than both Java and JavaScript so maybe we're asking slightly better",
    "start": "2995230",
    "end": "3001380"
  },
  {
    "text": "questions so that was an example of a",
    "start": "3001380",
    "end": "3009390"
  },
  {
    "text": "bucket aggregation with another aggregation statistics on top of each one of those tags let's look at an even",
    "start": "3009390",
    "end": "3016350"
  },
  {
    "text": "more complicated example here so we're going to have a look at questions tagged",
    "start": "3016350",
    "end": "3021840"
  },
  {
    "text": "vnx dotnet core from 29th of June 2015",
    "start": "3021840",
    "end": "3026910"
  },
  {
    "text": "and then we're going to perform an aggregation on on those questions that match our query but we only want to look",
    "start": "3026910",
    "end": "3034410"
  },
  {
    "text": "at the NX and dotnet core because questions can be packed with more than one things but we only want to look at those two tags in this particular case",
    "start": "3034410",
    "end": "3040260"
  },
  {
    "text": "and then we're going to bucket those into weekly questions and then on top of each one of those weekly buckets we're",
    "start": "3040260",
    "end": "3046740"
  },
  {
    "text": "going to perform a count of the number of questions there and then on top of that cang we're going to do a moving average so we're going to look at we're",
    "start": "3046740",
    "end": "3054630"
  },
  {
    "text": "going to look at the moving average of those questions there using halt winters and we're going to look at moving",
    "start": "3054630",
    "end": "3060510"
  },
  {
    "text": "average window of 12 weeks and we're going to have a looking to see what that gives us so we have it back in a table",
    "start": "3060510",
    "end": "3072210"
  },
  {
    "text": "here we can see the count we can see date moving average etc but that's no good for us we want to see it and we",
    "start": "3072210",
    "end": "3077250"
  },
  {
    "text": "want to see in a graph so let's have a look in a graph so what we can see here",
    "start": "3077250",
    "end": "3085460"
  },
  {
    "text": "the DNX questions this peak here in this moving average is when beta 6 came out",
    "start": "3085460",
    "end": "3091710"
  },
  {
    "text": "you can see here this is when RT 1 was released and then we can see these",
    "start": "3091710",
    "end": "3099630"
  },
  {
    "text": "gradually kind of kind of taper off a little bit but we can see boom there's suddenly these dotnet core questions are",
    "start": "3099630",
    "end": "3105300"
  },
  {
    "text": "just coming in and this was when there was a move over from vnx to dotnet core",
    "start": "3105300",
    "end": "3110640"
  },
  {
    "text": "tooling one of nice things with moving averages we can also use it too predict where these things are going to",
    "start": "3110640",
    "end": "3116520"
  },
  {
    "text": "go as well so we can make we can use that data to make a prediction so let's",
    "start": "3116520",
    "end": "3121619"
  },
  {
    "text": "see where things are going to go in the next six weeks okay so yes on that",
    "start": "3121619",
    "end": "3127740"
  },
  {
    "text": "course idea next questions are going downhill but we can see hopefully people",
    "start": "3127740",
    "end": "3133559"
  },
  {
    "text": "getting more familiar with dotnet core questions are gradually slipping off nice thing with this moving average is",
    "start": "3133559",
    "end": "3139200"
  },
  {
    "text": "where I can take it takes into account seasonality as well so as a brief look",
    "start": "3139200",
    "end": "3149579"
  },
  {
    "text": "at aggregation so let's have a look at all auto completion so more kind of",
    "start": "3149579",
    "end": "3154740"
  },
  {
    "text": "familiar with Google of course we are and if we want to do the same thing in",
    "start": "3154740",
    "end": "3159930"
  },
  {
    "text": "elasticsearch we just simply do the sequel equivalent of we're filled like this thing and as the data center burns",
    "start": "3159930",
    "end": "3167309"
  },
  {
    "text": "we you know we walk away into the sunset and we can do that as well if we want to in elastic search by using a prefix of",
    "start": "3167309",
    "end": "3174780"
  },
  {
    "text": "wild card or a regex query but that's kind of not the way to do it really you",
    "start": "3174780",
    "end": "3180780"
  },
  {
    "text": "can do these these are crews that exist but they're very the more ad hoc rather than particular queries you want to use",
    "start": "3180780",
    "end": "3187500"
  },
  {
    "text": "to solve a problem there are usually better ways so what better ways are there there's something called a",
    "start": "3187500",
    "end": "3192690"
  },
  {
    "text": "completion suggester they are super super fast they are particularly designed for solving that search as you",
    "start": "3192690",
    "end": "3199680"
  },
  {
    "text": "type scenario so they use something called an FFT which is finite state transducer essentially a graph that maps",
    "start": "3199680",
    "end": "3207180"
  },
  {
    "text": "out the relative path from each letter within a particular string of characters",
    "start": "3207180",
    "end": "3213349"
  },
  {
    "text": "you can also apply weighting here as well so use a weight from so say for",
    "start": "3213349",
    "end": "3218880"
  },
  {
    "text": "example if you're searching Stack Overflow for questions you might want to weight those questions that have a",
    "start": "3218880",
    "end": "3225960"
  },
  {
    "text": "higher score as being more you know being more important as you're searching",
    "start": "3225960",
    "end": "3231680"
  },
  {
    "text": "terms adjusters a slightly different type of suggester they allow you to provide things such as did you mean so",
    "start": "3235060",
    "end": "3240730"
  },
  {
    "text": "they're useful for spelling Corrections so given given a set of terms show me",
    "start": "3240730",
    "end": "3246580"
  },
  {
    "text": "one show me terms based on what exists within the corpus of documents show me terms that may you know may indicate",
    "start": "3246580",
    "end": "3253690"
  },
  {
    "text": "someone has slightly misspelled something and phrase adjusters are really similar two terms adjusters",
    "start": "3253690",
    "end": "3259510"
  },
  {
    "text": "except they take into account the entire phrase rather than individual individual",
    "start": "3259510",
    "end": "3264640"
  },
  {
    "text": "terms so we can have a quick look at those so this is a regex query we can",
    "start": "3264640",
    "end": "3275620"
  },
  {
    "text": "see here the top top example here is doesn't start with WH as we expected and",
    "start": "3275620",
    "end": "3281380"
  },
  {
    "text": "that's because this field here has been analyzed and therefore we are seeing a",
    "start": "3281380",
    "end": "3286540"
  },
  {
    "text": "match here on when here on this particular token here so we can see that",
    "start": "3286540",
    "end": "3294070"
  },
  {
    "text": "quickly that we're falling down with this with this prefix query here if we",
    "start": "3294070",
    "end": "3300820"
  },
  {
    "text": "have a look at using a completion suggester for that instead we can see",
    "start": "3300820",
    "end": "3307720"
  },
  {
    "text": "that we we start getting some answers back and we have a slight bit of fuzziness here as well so it can take",
    "start": "3307720",
    "end": "3313180"
  },
  {
    "text": "into account some spelling errors there so you know so yet what the difference",
    "start": "3313180",
    "end": "3322990"
  },
  {
    "text": "so it's been able to understand that I've spelt things slightly incorrectly",
    "start": "3322990",
    "end": "3330480"
  },
  {
    "text": "but that's kind of completions adjusters terms suggestive Ruby on reins active",
    "start": "3331470",
    "end": "3337810"
  },
  {
    "text": "recorder let's suggest some other terms for that so for the Ruby term for Ruby",
    "start": "3337810",
    "end": "3346420"
  },
  {
    "text": "miss spelt here we're getting back the Ruby term here this is the highest frequency so we probably miss spelt this",
    "start": "3346420",
    "end": "3352210"
  },
  {
    "text": "for range we can see here that we're",
    "start": "3352210",
    "end": "3358450"
  },
  {
    "text": "getting high frequency here for rails so probably miss spelt as well active recorder yeah okay active records maybe",
    "start": "3358450",
    "end": "3367300"
  },
  {
    "text": "we might want to start to the user instead we might also use the result of this term suggested to",
    "start": "3367300",
    "end": "3373240"
  },
  {
    "text": "feed into a query as well - maybe cast a wider net so it's kind of a brief look",
    "start": "3373240",
    "end": "3387580"
  },
  {
    "text": "at terms suggestive so let's um look at recommendations now so looking at things given this title share me questions that",
    "start": "3387580",
    "end": "3394570"
  },
  {
    "text": "may already answer the question I'm about to ask - what shards and elasticsearch and we can take more like",
    "start": "3394570",
    "end": "3400960"
  },
  {
    "text": "this so given a piece of text or given an actual document show me two other documents that are like this document",
    "start": "3400960",
    "end": "3406930"
  },
  {
    "text": "and we can take that a step further with function scoring so with more like this we might also want to factor in other",
    "start": "3406930",
    "end": "3413320"
  },
  {
    "text": "signals about the document so our friends at domain here for example they're able to when you're looking at a",
    "start": "3413320",
    "end": "3419680"
  },
  {
    "text": "given property you might want to know about other properties that have similar attributes such as beds baths parking",
    "start": "3419680",
    "end": "3425680"
  },
  {
    "text": "etc but you may also want to wait those properties that are near the property",
    "start": "3425680",
    "end": "3431140"
  },
  {
    "text": "that you're looking at higher than other ones but then gradually grade degrade the weighting that you apply to those",
    "start": "3431140",
    "end": "3436810"
  },
  {
    "text": "and maybe properties that are newer you might want to wait those hot slightly higher as well that's what function",
    "start": "3436810",
    "end": "3442960"
  },
  {
    "text": "scoring allows you to do and significant terms is another type of approach that",
    "start": "3442960",
    "end": "3449680"
  },
  {
    "text": "you can take to get recommendations for a given term so for a given field of a document so we'll of I'll have a very",
    "start": "3449680",
    "end": "3456160"
  },
  {
    "text": "brief look at those things so performing a more like this query reduce shards and",
    "start": "3456160",
    "end": "3463180"
  },
  {
    "text": "elasticsearch I'm typing out my question that's my that's my question title we",
    "start": "3463180",
    "end": "3469030"
  },
  {
    "text": "can go and have a look in Stack Overflow reducing number of shards how to improve we can see a bunch of questions here",
    "start": "3469030",
    "end": "3475540"
  },
  {
    "text": "around shards and elasticsearch which is what what we would expect so given a",
    "start": "3475540",
    "end": "3483820"
  },
  {
    "text": "particular document so the document that we were looking at last time let's pass that in use that to find docket other",
    "start": "3483820",
    "end": "3492520"
  },
  {
    "text": "documents similar to this particular one so we can see here again questions about",
    "start": "3492520",
    "end": "3498700"
  },
  {
    "text": "elasticsearch kind of generally scored a little bit low so let's see if we can improve this by",
    "start": "3498700",
    "end": "3506599"
  },
  {
    "text": "using function scoring so maybe we want to maybe we want to wait questions that",
    "start": "3506599",
    "end": "3512390"
  },
  {
    "text": "have a score a high score or higher than other questions so we still want to find",
    "start": "3512390",
    "end": "3517579"
  },
  {
    "text": "questions like the one that we're looking at but then we also want to we",
    "start": "3517579",
    "end": "3525410"
  },
  {
    "text": "also want to favor those that have a higher score and we also want to favor those that are newer creation of newer",
    "start": "3525410",
    "end": "3530660"
  },
  {
    "text": "questions because you know maybe elasticsearch is changing quite quickly so the older older questions may not be",
    "start": "3530660",
    "end": "3537500"
  },
  {
    "text": "Sorella Ventoux things now so we can use that tool to weight things and if we have a look at the answers now we can",
    "start": "3537500",
    "end": "3544130"
  },
  {
    "text": "see we've got some much higher scoring questions coming out near the top here and we've also weighted those as well as",
    "start": "3544130",
    "end": "3551089"
  },
  {
    "text": "a slightly newer so looking at",
    "start": "3551089",
    "end": "3557630"
  },
  {
    "text": "significant terms let's have a look at those users that have the c-sharp badge and let's look at other badges that",
    "start": "3557630",
    "end": "3564170"
  },
  {
    "text": "users that have the c-sharp badge also significantly tend to have so some",
    "start": "3564170",
    "end": "3572540"
  },
  {
    "text": "familiar some familiar kind of tags there those are answer c-sharp questions generally dotnet asp.net link there's no",
    "start": "3572540",
    "end": "3579650"
  },
  {
    "text": "kind of surprises there that's just running significant terms on c-sharp why",
    "start": "3579650",
    "end": "3584660"
  },
  {
    "text": "don't we just have a look and see running it on those questions that are",
    "start": "3584660",
    "end": "3589700"
  },
  {
    "text": "tagged C sharp F sharp and vb.net and now let's have a look at the significant terms for each one of those different",
    "start": "3589700",
    "end": "3595819"
  },
  {
    "text": "badges so if we run that we can you find",
    "start": "3595819",
    "end": "3601400"
  },
  {
    "text": "something kind of interesting here so those have c-sharp badges they're very",
    "start": "3601400",
    "end": "3606470"
  },
  {
    "text": "much on the asp.net MVC sequel server javascript kind of train our vb.net guys",
    "start": "3606470",
    "end": "3613490"
  },
  {
    "text": "they seem to be stuck in Visual Studio 2010 WinForms yeah maybe a little bit C",
    "start": "3613490",
    "end": "3621349"
  },
  {
    "text": "sharp for cross-pollination there and our F sharp guys well we're pattern matching functional programming",
    "start": "3621349",
    "end": "3627770"
  },
  {
    "text": "oh camel house going away so it's embarrassing very different some very",
    "start": "3627770",
    "end": "3633470"
  },
  {
    "text": "different badges there that we see",
    "start": "3633470",
    "end": "3636849"
  },
  {
    "text": "and we can use them significant terms as well to power a graph exploration I probably don't have time to go into that",
    "start": "3638500",
    "end": "3645020"
  },
  {
    "text": "but if you'd like to grab me afterwards we can we can have a look through that stuff but it's a it's another API with",
    "start": "3645020",
    "end": "3651859"
  },
  {
    "text": "an elastic search that since you can use those significant terms to understand some implied relationships between the",
    "start": "3651859",
    "end": "3661700"
  },
  {
    "text": "values within particular fields in documents so for example we can look at the F sharp tag here and we we can see",
    "start": "3661700",
    "end": "3673250"
  },
  {
    "text": "as we saw before in significant terms we have these are the badges that are most significantly related to the F sharp",
    "start": "3673250",
    "end": "3679310"
  },
  {
    "text": "badge so users having a sharp badges generally have functional programming oh",
    "start": "3679310",
    "end": "3684680"
  },
  {
    "text": "camel Haskell etc and what we also received back here are the relevant",
    "start": "3684680",
    "end": "3690260"
  },
  {
    "text": "weightings for those particular badges so how how significantly housed",
    "start": "3690260",
    "end": "3696980"
  },
  {
    "text": "statistically significant is dispatch related to this other badge and we get the source badge here in the target",
    "start": "3696980",
    "end": "3702650"
  },
  {
    "text": "badge and we can actually use those then to form to form a",
    "start": "3702650",
    "end": "3709359"
  },
  {
    "text": "to form a natural graph so with in Cabana so let's open up Cabana we talked briefly about Cabana we have a grass",
    "start": "3714380",
    "end": "3722690"
  },
  {
    "text": "plug-in so we can kind of do the same thing that we saw in the API there let's have a look at users have a look at",
    "start": "3722690",
    "end": "3730220"
  },
  {
    "text": "badges F sharp so this is using that graph API in the background but it's",
    "start": "3730220",
    "end": "3736550"
  },
  {
    "text": "just linking those badges up together there so we can see there's a very strong relationship here between X sharp",
    "start": "3736550",
    "end": "3742580"
  },
  {
    "text": "and functional programming based on the thickness of the connection there we can",
    "start": "3742580",
    "end": "3747800"
  },
  {
    "text": "see that there's a quite a large overlap there between between those those those",
    "start": "3747800",
    "end": "3753680"
  },
  {
    "text": "badges",
    "start": "3753680",
    "end": "3756309"
  },
  {
    "text": "one second I've lost my slides that was",
    "start": "3774170",
    "end": "3782900"
  },
  {
    "text": "a brief look at recommendations and that that was a very very brief overview of a",
    "start": "3782900",
    "end": "3788750"
  },
  {
    "text": "bunch of different functionality with elasticsearch hope it was helpful to everyone do you have any questions no",
    "start": "3788750",
    "end": "3800839"
  },
  {
    "text": "obviously did a good job yes",
    "start": "3800839",
    "end": "3804339"
  },
  {
    "text": "yep we can talk about it afterwards",
    "start": "3815779",
    "end": "3824880"
  },
  {
    "text": "because it's a very long answer but it can complement it yeah but it's a yeah",
    "start": "3824880",
    "end": "3831960"
  },
  {
    "text": "world war chat afterwards thank you very much and if you'd like to know a bit",
    "start": "3831960",
    "end": "3837420"
  },
  {
    "text": "more about elastic it's a slight and the github repo for the client thank you",
    "start": "3837420",
    "end": "3845808"
  }
]