[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "so I am lace I am gonna be presenting on data DevOps for the modern data",
    "start": "9599",
    "end": "14849"
  },
  {
    "text": "warehouse and Azure so just a little bit about me I'm a senior software engineer as part of the commercial software",
    "start": "14849",
    "end": "21390"
  },
  {
    "text": "engineering team at Microsoft I am based in Melbourne and my specialization is around data specifically around spark",
    "start": "21390",
    "end": "28080"
  },
  {
    "text": "and data bricks so there's my github and Twitter if you want to reach me",
    "start": "28080",
    "end": "34170"
  },
  {
    "text": "okay so agenda for today very simple so I'll start up with an overview of the modern data warehouse what this what is",
    "start": "34170",
    "end": "40830"
  },
  {
    "start": "35000",
    "end": "57000"
  },
  {
    "text": "this what kind of patterns are we proposing from the Microsoft side and also more importantly how you actually",
    "start": "40830",
    "end": "46950"
  },
  {
    "text": "operationalize a data pipeline and also be talking about learnings that we we found out along the way along the best",
    "start": "46950",
    "end": "53460"
  },
  {
    "text": "practices from working with customers okay so modern data warehouse so before",
    "start": "53460",
    "end": "58649"
  },
  {
    "text": "I talk about actually the data warehousing side of it I want to talk about the analytics so from because this",
    "start": "58649",
    "end": "64680"
  },
  {
    "text": "is essentially you know why the reason you actually do data warehousing so from left to right we have descriptive",
    "start": "64680",
    "end": "70619"
  },
  {
    "text": "analytics which is essentially asks the question what happened it answers the",
    "start": "70619",
    "end": "75689"
  },
  {
    "text": "question what happened essentially for example your sales what was my sales from the last month and then you have",
    "start": "75689",
    "end": "81750"
  },
  {
    "text": "diagnostic analytics which answers the question why did it happen so perhaps my sales for last month was quite low",
    "start": "81750",
    "end": "88290"
  },
  {
    "text": "because the specific store didn't perform in in as expected and then we go",
    "start": "88290",
    "end": "93780"
  },
  {
    "text": "into the realm of predictive analytics what what will happen in the future what will my sales be in the next month or",
    "start": "93780",
    "end": "100409"
  },
  {
    "text": "the next quarter and finally we have prescriptive analytics which meet which tries to ask the question or answer the",
    "start": "100409",
    "end": "107520"
  },
  {
    "text": "question how can we make it happen like what are the certain steps that we can take in order to actually increase",
    "start": "107520",
    "end": "113850"
  },
  {
    "text": "sales for next quarter now you can imagine as the value of these analytics",
    "start": "113850",
    "end": "118979"
  },
  {
    "text": "increase so that's the difficulty of actually implementing these the first two analytics have a very top-down",
    "start": "118979",
    "end": "124860"
  },
  {
    "text": "approach that is you typically start with your fact and dimension tables the reports that you have in mind that you",
    "start": "124860",
    "end": "131280"
  },
  {
    "text": "want to create and then you populate this with data well the the previous the",
    "start": "131280",
    "end": "137510"
  },
  {
    "text": "the second two analytics has a very bottom-up approach that is you don't necessarily",
    "start": "137510",
    "end": "143230"
  },
  {
    "text": "know which datasets are going to be useful for your machine learning models perhaps that your data scientist has",
    "start": "143230",
    "end": "149739"
  },
  {
    "text": "tactics do some exploratory analysis so you have to gather all this data so for your data scientist to use and then",
    "start": "149739",
    "end": "155920"
  },
  {
    "text": "they'll probably figure out oh these are the certain data sets we're going to be reporting on so it's a very bottom-up",
    "start": "155920",
    "end": "160959"
  },
  {
    "text": "approach of actually building your data pipelines the traditional data warehouse would get you as far as the first two",
    "start": "160959",
    "end": "167830"
  },
  {
    "text": "analytics but really to get through all the four analytics you have to have a different pattern which we call the",
    "start": "167830",
    "end": "174069"
  },
  {
    "text": "modern data warehouse okay so the folks who are coming from a data warehousing",
    "start": "174069",
    "end": "180340"
  },
  {
    "start": "177000",
    "end": "213000"
  },
  {
    "text": "background this should be very very familiar you have your typical sources from the from the left side which are",
    "start": "180340",
    "end": "187090"
  },
  {
    "text": "usually relational stores maybe a bunch of flat files you have an extract transform load process your ETL process",
    "start": "187090",
    "end": "194170"
  },
  {
    "text": "which basically transform it transforms into your fact and dimension tables in your warehouse you might have a staging",
    "start": "194170",
    "end": "200230"
  },
  {
    "text": "database in the middle that will facilitate this this could be a schema within your data warehouse and then",
    "start": "200230",
    "end": "206109"
  },
  {
    "text": "finally you have your reports which get trans which get populated typically from your data warehouse we want to transform",
    "start": "206109",
    "end": "214329"
  },
  {
    "start": "213000",
    "end": "372000"
  },
  {
    "text": "this this traditional data warehouse into the modern data warehouse architecture and the modern data",
    "start": "214329",
    "end": "220750"
  },
  {
    "text": "warehouse architecture fundamentally revolves around the concept of a data Lake which is essentially the response",
    "start": "220750",
    "end": "227109"
  },
  {
    "text": "to this this explosion of data sets right so you no longer just have relational data sets you have speech",
    "start": "227109",
    "end": "232780"
  },
  {
    "text": "video sound all of this coming into your data Lake that relational based beta",
    "start": "232780",
    "end": "240519"
  },
  {
    "text": "pipeline no longer fits the bill so typically you'd have some sort of ingestion mechanism so something that",
    "start": "240519",
    "end": "247000"
  },
  {
    "text": "actually moves your data into your data Lake you need to have some sort of exploratory tool so that your data",
    "start": "247000",
    "end": "254349"
  },
  {
    "text": "scientist or your analyst can actually query the stuff in your data Lake and of course you need to have some sort of",
    "start": "254349",
    "end": "259930"
  },
  {
    "text": "analytics engine that so that you can actually transform prep cleanse or train machine learning models on top of your",
    "start": "259930",
    "end": "265780"
  },
  {
    "text": "data finally you need some sort of serving layer so that the serving later can be used by a downstream consumer",
    "start": "265780",
    "end": "271719"
  },
  {
    "text": "such as bi reporting and so on so this is what it looks like like when we slap on some adjure",
    "start": "271719",
    "end": "278470"
  },
  {
    "text": "services on top of the different components here so typically data/factory is your go to ingestion mechanism along with your",
    "start": "278470",
    "end": "285130"
  },
  {
    "text": "orchestration mechanism and then you have data bricks which is in a managed patch is part platform that's available",
    "start": "285130",
    "end": "293230"
  },
  {
    "text": "in Azure where you can as for folks who are familiar with spark spark is essentially a distributed computing",
    "start": "293230",
    "end": "299740"
  },
  {
    "text": "engine which is a very very rich set of API so you can use to transform your data train machine learning models and",
    "start": "299740",
    "end": "306400"
  },
  {
    "text": "so on or you can use sequel data warehouse with a technology called poly base which essentially allows you to",
    "start": "306400",
    "end": "312550"
  },
  {
    "text": "query files that's sitting in your data Lake that's if it was such a sequel table and for prepping and training",
    "start": "312550",
    "end": "318730"
  },
  {
    "text": "there's also data breaks but there's also HD inside if your company is already quite invested in the Hadoop",
    "start": "318730",
    "end": "324430"
  },
  {
    "text": "ecosystem hdinsight is essentially a managed to do platform that you can",
    "start": "324430",
    "end": "329590"
  },
  {
    "text": "utilize and for the serving layer there's a myriad of different services I've only added a few of them over here",
    "start": "329590",
    "end": "335500"
  },
  {
    "text": "so obviously the the most common one is add your sequel there's also all the major open-source databases that is",
    "start": "335500",
    "end": "341980"
  },
  {
    "text": "available on a past service like Postgres and my sequel if that's what you're using in your company and you",
    "start": "341980",
    "end": "348190"
  },
  {
    "text": "have the MPP file databases such as the sequel data warehouse and all of this fundamentally the one of the core",
    "start": "348190",
    "end": "356200"
  },
  {
    "text": "components of this sit on top of data like Gen 2 so data like Gen 2 is a layer",
    "start": "356200",
    "end": "361270"
  },
  {
    "text": "on top of blob storage however it's optimized for big data workloads it gives you file level and folder level",
    "start": "361270",
    "end": "366850"
  },
  {
    "text": "security along with a hierarchical namespace ok so there's much to be said",
    "start": "366850",
    "end": "373450"
  },
  {
    "start": "372000",
    "end": "472000"
  },
  {
    "text": "of how you actually structure your data in your data Lake but I like to think about them in different tiers and I've",
    "start": "373450",
    "end": "378880"
  },
  {
    "text": "seen this vary in one form in another in different multiple customers so typically you would structure your first",
    "start": "378880",
    "end": "385240"
  },
  {
    "text": "data the first sort of portion of your data Lake dedicated to your bronze data sets these are your raw unprocessed data",
    "start": "385240",
    "end": "392290"
  },
  {
    "text": "sets that you've just landed in your data Lake and then you have your silver data sets these are semi transformed",
    "start": "392290",
    "end": "399220"
  },
  {
    "text": "cleansed validated and usually typecast already to the appropriate types and",
    "start": "399220",
    "end": "404530"
  },
  {
    "text": "they might be augmented maybe additional features or columns might be added",
    "start": "404530",
    "end": "409940"
  },
  {
    "text": "and finally you have your gold datasets these are your fact and dimensions these are highly processed highly optimized",
    "start": "409940",
    "end": "416750"
  },
  {
    "text": "datasets that's ready for consumption now to extend the analogy of a data Lake your gold data sets is like your bottled",
    "start": "416750",
    "end": "424520"
  },
  {
    "text": "water your bottled water is clean it's convenient and ready for use",
    "start": "424520",
    "end": "430510"
  },
  {
    "text": "now you imagine your business user that's the folks who's gonna be using your gold data sets but I get I get the",
    "start": "430510",
    "end": "437420"
  },
  {
    "text": "question why do we need a silver data sense well that's for your data scientist your data scientist typically",
    "start": "437420",
    "end": "443360"
  },
  {
    "text": "they don't want to be concerned with the intricacies of the source systems like",
    "start": "443360",
    "end": "448460"
  },
  {
    "text": "your raw data sets they might still be interested and they they don't really want to do you know converting your your",
    "start": "448460",
    "end": "454730"
  },
  {
    "text": "string your date date strings into actual dates they want some sort of cleanse data set but they don't also",
    "start": "454730",
    "end": "461210"
  },
  {
    "text": "want a really processed data set they still wants the normally set that exists within that data set so the silver data",
    "start": "461210",
    "end": "467540"
  },
  {
    "text": "set is the perfect data sets for your data scientists which bring me to the",
    "start": "467540",
    "end": "473060"
  },
  {
    "start": "472000",
    "end": "567000"
  },
  {
    "text": "first learning within the pipeline when you build your pipelines it's validate early in your pipeline what do I mean by",
    "start": "473060",
    "end": "479000"
  },
  {
    "text": "this so going through the analogy of the T of the data tearing typically you one valid ate around here right before you",
    "start": "479000",
    "end": "486350"
  },
  {
    "text": "go into your silver that way once it's silver and onwards you know that you can",
    "start": "486350",
    "end": "491720"
  },
  {
    "text": "write code in your in your data pipelines which have certain assumptions with the data invariants that your",
    "start": "491720",
    "end": "497210"
  },
  {
    "text": "validation logics logic actually make sure it holds so you can write logic at",
    "start": "497210",
    "end": "504560"
  },
  {
    "text": "this point so that any other code downstream can make that assumption that this for example column has this set and",
    "start": "504560",
    "end": "511370"
  },
  {
    "text": "set of codes win it these columns or it will never be null and so on and so forth and from this validation logic",
    "start": "511370",
    "end": "518300"
  },
  {
    "text": "typically you'd have some rows that won't make through make through this validation logic so you don't want to be",
    "start": "518300",
    "end": "524330"
  },
  {
    "text": "dropping your data you probably want to sink this something some sort of malformed record schema and you",
    "start": "524330",
    "end": "529940"
  },
  {
    "text": "obviously need to put on monitoring on this so in case that this kind of blows up for whatever reason okay so a",
    "start": "529940",
    "end": "539240"
  },
  {
    "text": "question for a question that I get a lot is why not be valid here before we even hit the Natalie well",
    "start": "539240",
    "end": "547699"
  },
  {
    "text": "we all know that code net always works right well not really so the problem",
    "start": "547699",
    "end": "554509"
  },
  {
    "text": "with validating there is that you might introduce a bug and then you essentially",
    "start": "554509",
    "end": "559930"
  },
  {
    "text": "tear down or potentially corrupt the data sets in your data link so that",
    "start": "559930",
    "end": "565490"
  },
  {
    "text": "brings me to my second learning is ensure your data pipeline is replayable so going back to the data tiers your",
    "start": "565490",
    "end": "573560"
  },
  {
    "start": "567000",
    "end": "619000"
  },
  {
    "text": "bronze layer typically has almost zero transformation it's just a copy over to your data lake that way you are the the",
    "start": "573560",
    "end": "580160"
  },
  {
    "text": "likelihood of introducing a bug which corrupts your data is very very minimal it's a append-only immutable store of",
    "start": "580160",
    "end": "587089"
  },
  {
    "text": "data and then your silver data sets that's when you start your transformation your validation and your",
    "start": "587089",
    "end": "592939"
  },
  {
    "text": "goal is your highly optimized transform data set so that way if you do introduce",
    "start": "592939",
    "end": "598009"
  },
  {
    "text": "a bug and I am pretty sure it will happen as we know we're all not perfect",
    "start": "598009",
    "end": "603680"
  },
  {
    "text": "and you do corrupt your silver and gold you can just replay your your pipeline",
    "start": "603680",
    "end": "608720"
  },
  {
    "text": "and then repopulate your silver and gold that way you're not dropping data and you can actually reconstruct your silver",
    "start": "608720",
    "end": "615800"
  },
  {
    "text": "and gold data sets okay for the demo I am gonna be using this publicly",
    "start": "615800",
    "end": "623180"
  },
  {
    "start": "619000",
    "end": "1054000"
  },
  {
    "text": "available API which has been gracefully provided by the Melbourne government so the Melbourne government actually has a",
    "start": "623180",
    "end": "629120"
  },
  {
    "text": "bunch of sensors parking sensors spread across the CBD and these are in ground sensors which detect if there's a",
    "start": "629120",
    "end": "635449"
  },
  {
    "text": "vehicle part or not and it's all available in API if you're interested you can go to that link right here and",
    "start": "635449",
    "end": "641630"
  },
  {
    "text": "for the demo that I'll be presenting I'll be using this API as my source data",
    "start": "641630",
    "end": "646930"
  },
  {
    "text": "now the details of the data is not necessarily important I do want everyone to be focusing more on the architecture",
    "start": "646930",
    "end": "653959"
  },
  {
    "text": "of the data pipeline so here's a sample of the modern data warehouse patterns actually in a concrete data pipeline so",
    "start": "653959",
    "end": "660649"
  },
  {
    "text": "from left to right we have the parking service we have a copy job that actually lands it into the landing area or a",
    "start": "660649",
    "end": "666500"
  },
  {
    "text": "landing schema and then we have a Clemson standardized job which takes this this raw data entrance and cleanses",
    "start": "666500",
    "end": "673490"
  },
  {
    "text": "it and transforms it into what we call the silver data sets in this case I'm calling that interim schema and there might be some malformed",
    "start": "673490",
    "end": "680759"
  },
  {
    "text": "records that come along so I put my bad data in that schema and then I have a second transform job which takes this",
    "start": "680759",
    "end": "686939"
  },
  {
    "text": "transforms it to the data where a schema and finally I have a pulley base I'm",
    "start": "686939",
    "end": "692310"
  },
  {
    "text": "using poly base to actually take the data that's sitting in data Lake and move it into sequel data warehouse all",
    "start": "692310",
    "end": "697589"
  },
  {
    "text": "of this is orchestrated using data Factory okay so let's jump into our demo okay so",
    "start": "697589",
    "end": "710100"
  },
  {
    "text": "I am here in data Factory and as you can see this data pipeline is very similar",
    "start": "710100",
    "end": "715110"
  },
  {
    "text": "to the one that I've shown in my diagram and for those who are new to data",
    "start": "715110",
    "end": "720149"
  },
  {
    "text": "factory data this is the data factory workspace or wizard if you want to call it and you have a number of activities",
    "start": "720149",
    "end": "726059"
  },
  {
    "text": "here that you can just drag and drop and author your pipelines all in this wizard so the first step in my pipeline I'm",
    "start": "726059",
    "end": "733559"
  },
  {
    "text": "just setting a variable called set in file folder this is just a timestamp so I'm stamping my folders and then I'm",
    "start": "733559",
    "end": "740970"
  },
  {
    "text": "using a copy date activity which basically reads from that REST API and",
    "start": "740970",
    "end": "746129"
  },
  {
    "text": "then sinks into the data Lake and I'm using that timestamp right here which is",
    "start": "746129",
    "end": "752939"
  },
  {
    "text": "the file folder the timestamp which will be the name of the file folder it sinks into so if we have a look at that in",
    "start": "752939",
    "end": "760019"
  },
  {
    "text": "data Lake it looks like this so we have the data Lake in the landing schema",
    "start": "760019",
    "end": "765750"
  },
  {
    "text": "which is time-stamped in this folder and we have do JSON files sitting in data Lake nice and easy next job we have the",
    "start": "765750",
    "end": "773670"
  },
  {
    "text": "standardized data job which is essentially a data bricks job it calls a data bricks notebook and the beauty of",
    "start": "773670",
    "end": "780240"
  },
  {
    "text": "this is that it spins up a spark cluster when it gets triggered and then pairs it",
    "start": "780240",
    "end": "785250"
  },
  {
    "text": "down afterwards so it's quite cost efficient in that regard that you don't have a 24-hour spark cluster running and",
    "start": "785250",
    "end": "791399"
  },
  {
    "text": "as you can see I am calling this part this notebook in this path let's",
    "start": "791399",
    "end": "797999"
  },
  {
    "text": "navigate to that notebook so this is in the data breaks workspace if you use",
    "start": "797999",
    "end": "804540"
  },
  {
    "text": "Jupiter notebooks before it's very similar it's a notebook sort of it way of coding",
    "start": "804540",
    "end": "809880"
  },
  {
    "text": "and I'm I'm using Python but data bricks can support Scala sequel and are so you",
    "start": "809880",
    "end": "817020"
  },
  {
    "text": "can write your data transformation in those three four languages okay the first bit of code I have here is just a",
    "start": "817020",
    "end": "823980"
  },
  {
    "text": "way of parameterizing notebook so that this notebook actually knows where which which files to actually pick up and read",
    "start": "823980",
    "end": "830160"
  },
  {
    "text": "and then I'm just setting that file path right here and this is the bulk of the",
    "start": "830160",
    "end": "835620"
  },
  {
    "text": "code so the first line here is I'm I'm importing a Python package that I",
    "start": "835620",
    "end": "841110"
  },
  {
    "text": "Britian and I'll get into that in later parts of the demo but essentially that Python package has a set of functions",
    "start": "841110",
    "end": "847740"
  },
  {
    "text": "that I'm calling in this set of code in this code I'm retrieving the schema that it expects I'm using a spark dot read",
    "start": "847740",
    "end": "855000"
  },
  {
    "text": "and then filtering out the bad rows that don't conform to the schema that I",
    "start": "855000",
    "end": "860520"
  },
  {
    "text": "expect into this file folder and then once I have my data frame I'm calling",
    "start": "860520",
    "end": "866820"
  },
  {
    "text": "standardized parking Bay and standardized sensor data passing in the",
    "start": "866820",
    "end": "872010"
  },
  {
    "text": "raw data frame and along with other variables and it outputs a tuple of data",
    "start": "872010",
    "end": "877020"
  },
  {
    "text": "frames the first the first data frame is your good rose and the second data frame",
    "start": "877020",
    "end": "882930"
  },
  {
    "text": "are your bad rows these are my malformed records and then the good rows go into my interim schema and then the bad rows",
    "start": "882930",
    "end": "890370"
  },
  {
    "text": "go into the malformed schema so nice and simple just read transform and load now",
    "start": "890370",
    "end": "897240"
  },
  {
    "text": "the next step is my transform data the transform data is very similar I have a",
    "start": "897240",
    "end": "903120"
  },
  {
    "text": "parameter which just identifies which load is this but instead of reading the JSON file sitting in data Lake I'm",
    "start": "903120",
    "end": "909750"
  },
  {
    "text": "reading the interim schema I got populated in the previous job now in",
    "start": "909750",
    "end": "914820"
  },
  {
    "text": "here it's also very similar I'm just calling instead of standardized I'm calling process dim parking Bay data and",
    "start": "914820",
    "end": "921480"
  },
  {
    "text": "so on and I'm writing out to the data warehouse tables right here that's for",
    "start": "921480",
    "end": "927810"
  },
  {
    "text": "my dimension tables and I'm doing exactly the same with my fact tables so very standard very simple transformation",
    "start": "927810",
    "end": "934610"
  },
  {
    "text": "logic right here and finally I have a stored procedure call within data",
    "start": "934610",
    "end": "941070"
  },
  {
    "text": "factory where I'm calling a stored procedure called load data warehouse so let's have",
    "start": "941070",
    "end": "946230"
  },
  {
    "text": "a look at that the load data warehouse sort procedure looks like this it's very",
    "start": "946230",
    "end": "952710"
  },
  {
    "text": "simple so I'm just truncating my dimension tables and reloading it but why via an insert into select star from",
    "start": "952710",
    "end": "960230"
  },
  {
    "text": "external table right so it the logic is quite simple the magic here is that",
    "start": "960230",
    "end": "966690"
  },
  {
    "text": "external table what is this external table so if you've ever used poly base this should look quite familiar an external table is",
    "start": "966690",
    "end": "974360"
  },
  {
    "text": "essentially a pointer to files in data link so you can see that I'm creating",
    "start": "974360",
    "end": "979529"
  },
  {
    "text": "and the definition of this external table is I'm saying data source data leak storage which I've defined which",
    "start": "979529",
    "end": "986310"
  },
  {
    "text": "has all the credentials that it needs to connect to that specific instance and I'm saying look for my data in this",
    "start": "986310",
    "end": "992010"
  },
  {
    "text": "location so let's have a look at that location that location in data Lake is essentially these these are my parka",
    "start": "992010",
    "end": "998700"
  },
  {
    "text": "files these are the files that underpin the sparks equal tables that I've inserted in my previous transform job so",
    "start": "998700",
    "end": "1005870"
  },
  {
    "text": "if sparks equal tables all of all is just a metadata pointer to the files that's actually sitting in data Lake",
    "start": "1005870",
    "end": "1012490"
  },
  {
    "text": "okay so once that's run we have data and just a select star and there goes",
    "start": "1012490",
    "end": "1019040"
  },
  {
    "text": "there's my fact table data cool so I'll just jump back to the presentation",
    "start": "1019040",
    "end": "1027069"
  },
  {
    "text": "okay so that wasn't great eat anything groundbreaking that's a relatively straightforward pipeline and to be fair",
    "start": "1030410",
    "end": "1037740"
  },
  {
    "text": "to the modern data warehouse pattern this is this pattern has been proposed by Microsoft for the past few years now",
    "start": "1037740",
    "end": "1044270"
  },
  {
    "text": "the common question and the common thing that folks and clients that I have",
    "start": "1044270",
    "end": "1050670"
  },
  {
    "text": "worked with that have trouble with is how do you operationalize this how do we",
    "start": "1050670",
    "end": "1056640"
  },
  {
    "start": "1054000",
    "end": "1069000"
  },
  {
    "text": "move that entire thing to another environment and the standard answer is",
    "start": "1056640",
    "end": "1066170"
  },
  {
    "text": "okay so let's let's dig in deeper than just rubbing some DevOps on it so DevOps",
    "start": "1068360",
    "end": "1074010"
  },
  {
    "start": "1069000",
    "end": "1101000"
  },
  {
    "text": "essentially is the union of people process and product now this talk is a",
    "start": "1074010",
    "end": "1079410"
  },
  {
    "text": "little bit focused on the product but the people in process parts this is definitely very important and integral",
    "start": "1079410",
    "end": "1084480"
  },
  {
    "text": "to the DevOps process and ultimately it's about doing the continuous delivery",
    "start": "1084480",
    "end": "1089550"
  },
  {
    "text": "of value to your end-users how can we accelerate this value to the end-users",
    "start": "1089550",
    "end": "1094650"
  },
  {
    "text": "through potentially tools like CI CD and so on like the way I like to think about",
    "start": "1094650",
    "end": "1102090"
  },
  {
    "start": "1101000",
    "end": "1210000"
  },
  {
    "text": "DevOps and this is actually based on this blog post right here in the Lincoln outpost apply the slides up so you can",
    "start": "1102090",
    "end": "1108180"
  },
  {
    "text": "have a read but essentially I like the way they describe it here where you",
    "start": "1108180",
    "end": "1113310"
  },
  {
    "text": "think of the data pipeline as a value pipeline right it's taking raw data raw",
    "start": "1113310",
    "end": "1118580"
  },
  {
    "text": "data sets and then converting them into valuable assets that your users can use",
    "start": "1118580",
    "end": "1124410"
  },
  {
    "text": "so in a way it's like it's the continuous delivery of value in your data your data Factory is a pipeline",
    "start": "1124410",
    "end": "1130770"
  },
  {
    "text": "that continuously delivers value to your end-users however this is not the only pipeline we have a second pipeline",
    "start": "1130770",
    "end": "1137310"
  },
  {
    "text": "called the innovation pipeline and this is the pipeline that essentially updates your value pipeline so that any ideas",
    "start": "1137310",
    "end": "1144810"
  },
  {
    "text": "that your your data analyst or a data engineers have can be deployed into",
    "start": "1144810",
    "end": "1150570"
  },
  {
    "text": "production and then be used by your value pipeline to produce the best value for your users now there's a bit of",
    "start": "1150570",
    "end": "1156420"
  },
  {
    "text": "duality here because you'll do orchestrators you have the orchestrator for your pipeline and you have the",
    "start": "1156420",
    "end": "1161790"
  },
  {
    "text": "orchestrator for your innovation pipeline so and along with your testing right so your test you have to test both your data and",
    "start": "1161790",
    "end": "1168530"
  },
  {
    "text": "your pipeline for your value pipeline your data is changing but your value pipeline is not well your innovation",
    "start": "1168530",
    "end": "1174980"
  },
  {
    "text": "pipeline your data is fixed you're giving a fixed data while your innovation pipeline is changing so",
    "start": "1174980",
    "end": "1181059"
  },
  {
    "text": "there's a there's a little bit complications of when you think about it and just have to remember which part are",
    "start": "1181059",
    "end": "1186260"
  },
  {
    "text": "you testing and which part which pipeline are you actually orchestrating for the innovation pipeline I'm using",
    "start": "1186260",
    "end": "1192500"
  },
  {
    "text": "Azure pipelines as my CI CD mechanism and Azure pipelines is part of the",
    "start": "1192500",
    "end": "1198800"
  },
  {
    "text": "bigger as your dev books suite for example a few boards and repost expense and artifacts all of this is available",
    "start": "1198800",
    "end": "1204710"
  },
  {
    "text": "natural DevOps but we'll be focusing specifically just an ad your pipelines okay so going back to our little",
    "start": "1204710",
    "end": "1213200"
  },
  {
    "start": "1210000",
    "end": "1448000"
  },
  {
    "text": "architecture there's really three components really here that we need to automate we have the data bricks jobs",
    "start": "1213200",
    "end": "1219380"
  },
  {
    "text": "right here your transformation your notebooks and that package that I was loading we have sequel data warehouse",
    "start": "1219380",
    "end": "1224990"
  },
  {
    "text": "which have all your sequel schema your your the stored procedures and all that sort of stuff and then you have your",
    "start": "1224990",
    "end": "1230660"
  },
  {
    "text": "actual pipeline the data factory pipeline itself let's go through one of them at a time so the first thing I want",
    "start": "1230660",
    "end": "1236929"
  },
  {
    "text": "to focus is your add your data breaks now if you remember in the demo I showed you that one line of code which I was",
    "start": "1236929",
    "end": "1243650"
  },
  {
    "text": "importing that package that's actually quite fundamental in the way you structure your data transformation code",
    "start": "1243650",
    "end": "1249170"
  },
  {
    "text": "so data transformation codes belong in packages not in notebooks now this might",
    "start": "1249170",
    "end": "1255200"
  },
  {
    "text": "seem obvious this is the folks coming from a software engineering background but it's very common at least from my",
    "start": "1255200",
    "end": "1262610"
  },
  {
    "text": "experience to see you know thousands of lines of code in a notebook and it's very difficult to write unit tests if",
    "start": "1262610",
    "end": "1268550"
  },
  {
    "text": "your code is in a notebook so my advice to lots of folks when I see this is you want to really think about your",
    "start": "1268550",
    "end": "1274940"
  },
  {
    "text": "transformation logic what which are really specific to transformation and which is specific to dowel or data",
    "start": "1274940",
    "end": "1280700"
  },
  {
    "text": "access code and you rip out your transformation code and put it in a package that it's now encapsulated where",
    "start": "1280700",
    "end": "1286040"
  },
  {
    "text": "you can just call those functions within your notebook that way you can actually write proper unit tests against your",
    "start": "1286040",
    "end": "1291410"
  },
  {
    "text": "transformations and data bricks also comes with the",
    "start": "1291410",
    "end": "1296850"
  },
  {
    "text": "integrated version control mechanism so you can wire up your notebook so that it wires up to get github add your DevOps",
    "start": "1296850",
    "end": "1304409"
  },
  {
    "text": "or bitbucket and it also comes with this good CLI so that you can automate different steps within your workspace or",
    "start": "1304409",
    "end": "1311429"
  },
  {
    "text": "management of your workspace so let's have a look at what that looks like so I'm gonna jump into Visual Studio code",
    "start": "1311429",
    "end": "1318809"
  },
  {
    "text": "and Visual Studio core vs code and this is basically my Python package so you",
    "start": "1318809",
    "end": "1324389"
  },
  {
    "text": "can see that I have my functions my get scheme and my standardized parking Bay data and all of that has all the SPARC",
    "start": "1324389",
    "end": "1331860"
  },
  {
    "text": "code encapsulated within it along with my validation code that way I can",
    "start": "1331860",
    "end": "1337799"
  },
  {
    "text": "actually I can actually call and run my unit tests so I have a make file here",
    "start": "1337799",
    "end": "1343619"
  },
  {
    "text": "the make file has just basically simplified the way I've run my tests I",
    "start": "1343619",
    "end": "1349470"
  },
  {
    "text": "can just simply say make tests so I'll just run that here and this goes and triggers all my unit tests so the unit",
    "start": "1349470",
    "end": "1355710"
  },
  {
    "text": "test looks like this I'm simply instantiating a spark session locally and spark is open source right spark can",
    "start": "1355710",
    "end": "1363389"
  },
  {
    "text": "be installed locally in your box it doesn't you don't need to have a data breaks cluster running and then I'm",
    "start": "1363389",
    "end": "1368999"
  },
  {
    "text": "simply calling I'm just simply reading a known data set that's saved locally",
    "start": "1368999",
    "end": "1374210"
  },
  {
    "text": "calling my function and then doing some asserts so now all of a sudden I can",
    "start": "1374210",
    "end": "1379710"
  },
  {
    "text": "actually write proper unit tests against my my data transformation code okay and",
    "start": "1379710",
    "end": "1386249"
  },
  {
    "text": "the other thing I wanted to show you is in data breaks you can actually",
    "start": "1386249",
    "end": "1391399"
  },
  {
    "text": "integrate your notebook with source control so there's two ways to actually bring down your files through your to",
    "start": "1391399",
    "end": "1397950"
  },
  {
    "text": "look to your local computer one is you can just simply export it that's the manual way or you can click on revision",
    "start": "1397950",
    "end": "1405210"
  },
  {
    "text": "history and then you can link it to get so I haven't linked this because I used a different way and you just simply add",
    "start": "1405210",
    "end": "1411629"
  },
  {
    "text": "your details over here and to link to your relevant git repo the way I like to",
    "start": "1411629",
    "end": "1418230"
  },
  {
    "text": "do it and this is more of a personal preference than anything else is I just use the data break CLI so the data break",
    "start": "1418230",
    "end": "1424350"
  },
  {
    "text": "CLI you can do a data breaks workspace export dear and it will export down your notebooks",
    "start": "1424350",
    "end": "1429899"
  },
  {
    "text": "locally and I also use a data break CLI in my CI CD pipelines to actually",
    "start": "1429899",
    "end": "1435239"
  },
  {
    "text": "automate importing this into the workspace when I go into staging ok",
    "start": "1435239",
    "end": "1443239"
  },
  {
    "text": "next sequel data warehouse so as there's this really wonderful tool called sequel",
    "start": "1447610",
    "end": "1454640"
  },
  {
    "start": "1448000",
    "end": "1623000"
  },
  {
    "text": "server data tools and what this tool does is that it allows developers or data engineers to pull down your",
    "start": "1454640",
    "end": "1461300"
  },
  {
    "text": "database objects locally and not only that you can also do a schema compare such that if anyone does some",
    "start": "1461300",
    "end": "1468350"
  },
  {
    "text": "development on sequel server management studio or someone changes the table definition you can do a schema compare",
    "start": "1468350",
    "end": "1474680"
  },
  {
    "text": "to your local instance or local code base and it will pull down the diff between what is deployed in your server",
    "start": "1474680",
    "end": "1482000"
  },
  {
    "text": "and what is what you have locally then you can actually generate something called a DAC back and the DAC pack is",
    "start": "1482000",
    "end": "1488360"
  },
  {
    "text": "this nice package you can use to deploy and point to a different server it will incrementally update that server based",
    "start": "1488360",
    "end": "1495620"
  },
  {
    "text": "on the diff between the DAC pack and what's deployed in that server it comes to the CLI I do want to point out that",
    "start": "1495620",
    "end": "1503090"
  },
  {
    "text": "the sequel data warehouse support is still in preview so just just take a note of that but sequel address equal",
    "start": "1503090",
    "end": "1509600"
  },
  {
    "text": "database has been supported in GA for for many years now so let's have a look",
    "start": "1509600",
    "end": "1515090"
  },
  {
    "text": "quickly look at sequel data tools so I'm gonna jump into Visual Studio so this is actually a visual visual Student it",
    "start": "1515090",
    "end": "1521780"
  },
  {
    "text": "comes out-of-the-box with Visual Studio 2019 and I believe in 2017 you have to download the latest for sequel data",
    "start": "1521780",
    "end": "1527630"
  },
  {
    "text": "warehouse support and you can see here that all my definite my sequel database",
    "start": "1527630",
    "end": "1532930"
  },
  {
    "text": "objects are in Visual Studio so all of my sword' procedures my table",
    "start": "1532930",
    "end": "1538250"
  },
  {
    "text": "definitions and so on and if I want to update the if let's say someone created",
    "start": "1538250",
    "end": "1544070"
  },
  {
    "text": "the table so for in this case I created a sample table in my dev database I can",
    "start": "1544070",
    "end": "1549260"
  },
  {
    "text": "just do a schema compare and click compare and it will detect that someone created this database and I can just",
    "start": "1549260",
    "end": "1555530"
  },
  {
    "text": "click update and this will update this definitions locally so it's a very really powerful way of making sure that",
    "start": "1555530",
    "end": "1562370"
  },
  {
    "text": "you have the latest version of your sequel server objects in source control and other cool thing about this is",
    "start": "1562370",
    "end": "1568790"
  },
  {
    "text": "verify if I do a build this will try to validate of all my objects so if you",
    "start": "1568790",
    "end": "1574430"
  },
  {
    "text": "ever done database development and you're in a stored procedure which reference a table and someone dropped a",
    "start": "1574430",
    "end": "1579740"
  },
  {
    "text": "column yours that's not yours es SMS is not gonna complain and you'll find out later",
    "start": "1579740",
    "end": "1585539"
  },
  {
    "text": "in production that your stored procedures referencing a column that no longer exists however here when I do a",
    "start": "1585539",
    "end": "1591539"
  },
  {
    "text": "build and that column doesn't exist it will tell me that oh hey your stored",
    "start": "1591539",
    "end": "1596580"
  },
  {
    "text": "procedures not consistent you need to update your stored procedures so just simply doing a build is actually",
    "start": "1596580",
    "end": "1601890"
  },
  {
    "text": "incredibly powerful that build produces what I mentioned earlier is your DAC",
    "start": "1601890",
    "end": "1607110"
  },
  {
    "text": "pack file and this DAC pack file is what I'm going to be used seeing in my CIC pipeline to deploy to my stage and my",
    "start": "1607110",
    "end": "1614220"
  },
  {
    "text": "production environments okay so coming",
    "start": "1614220",
    "end": "1619649"
  },
  {
    "text": "back to the slides so the next thing",
    "start": "1619649",
    "end": "1624840"
  },
  {
    "start": "1623000",
    "end": "1660000"
  },
  {
    "text": "that I want to focus on is data Factory so data factory also comes with its own and get integration so as you can see",
    "start": "1624840",
    "end": "1630929"
  },
  {
    "text": "here it integrates with github or Azure DevOps repos out-of-the-box and you can",
    "start": "1630929",
    "end": "1636720"
  },
  {
    "text": "just select your github account what repository the collaboration branch which is essentially which branch can",
    "start": "1636720",
    "end": "1643260"
  },
  {
    "text": "you publish changes to your factory that it's linked to and which folder you want",
    "start": "1643260",
    "end": "1648419"
  },
  {
    "text": "actually pulled or saved the artifacts that data/factory creates oops sorry",
    "start": "1648419",
    "end": "1655549"
  },
  {
    "text": "okay once you've actually wired up data/factory this is the the recommended",
    "start": "1659490",
    "end": "1665159"
  },
  {
    "start": "1660000",
    "end": "1855000"
  },
  {
    "text": "way of actually using the artifacts or the flow that you would undertake to do CI CD using your data the in artifacts",
    "start": "1665159",
    "end": "1673140"
  },
  {
    "text": "within data factory so typically you would create your data factory that's synced to a dead branch your which is",
    "start": "1673140",
    "end": "1679470"
  },
  {
    "text": "which we call the working branches and once you're happy you do a pull request to master and this is essentially your",
    "start": "1679470",
    "end": "1686159"
  },
  {
    "text": "collaboration branch this is the branch where you can publish changes to your Deb breath your Deb factory now your dev",
    "start": "1686159",
    "end": "1692909"
  },
  {
    "text": "factory then once you publish the changes you're dead factory sinks automatically the arm templates for this",
    "start": "1692909",
    "end": "1700830"
  },
  {
    "text": "data factory into a publish branch this publish branch by default is called a DF",
    "start": "1700830",
    "end": "1706230"
  },
  {
    "text": "publish and this a DF publish has your arm templates which we you can then use",
    "start": "1706230",
    "end": "1711419"
  },
  {
    "text": "as an artifact to your release definition when you create a release you just pick up these arm templates and",
    "start": "1711419",
    "end": "1717809"
  },
  {
    "text": "then parameterize them with the correct parameters and you can deploy these arm",
    "start": "1717809",
    "end": "1722940"
  },
  {
    "text": "templates the relevant environments such as test or prod or pre prod and so on so",
    "start": "1722940",
    "end": "1729210"
  },
  {
    "text": "let's have a look at what that looks like so I'll just jump to data factory",
    "start": "1729210",
    "end": "1734840"
  },
  {
    "text": "so I add worry I've already integrated this pipeline but if I haven't done so",
    "start": "1734840",
    "end": "1741570"
  },
  {
    "text": "this is how you do it so you just go to this is the landing portion of data",
    "start": "1741570",
    "end": "1747480"
  },
  {
    "text": "factory and you just click on setup code repo and then you just click github and then you fill up the relevant details",
    "start": "1747480",
    "end": "1753059"
  },
  {
    "text": "but once you've done that you should see something like this so you can see that",
    "start": "1753059",
    "end": "1759059"
  },
  {
    "text": "I'm on my dev branch so any changes I do here I it will automatically sync when I",
    "start": "1759059",
    "end": "1765390"
  },
  {
    "text": "get saved so let's have a quick change here and say hello from NDC Sydney and",
    "start": "1765390",
    "end": "1775640"
  },
  {
    "text": "when I said save this will automatically sync to my Deb branch so how do I want",
    "start": "1775640",
    "end": "1781860"
  },
  {
    "text": "to publish this I will then go and create a pull request and you can see",
    "start": "1781860",
    "end": "1788700"
  },
  {
    "text": "that the pull request has detected my change and I just go and create the pull request I want the politicus just yet oh I'll do that",
    "start": "1788700",
    "end": "1795020"
  },
  {
    "text": "in the later demos but I do want to show you the other thing here which is your",
    "start": "1795020",
    "end": "1800210"
  },
  {
    "text": "master branch once I go to my master branch this is the only branch where I",
    "start": "1800210",
    "end": "1805220"
  },
  {
    "text": "can actually publish changes into into your deploy data Factory now the master",
    "start": "1805220",
    "end": "1810560"
  },
  {
    "text": "branch whatever you publish from the master branch creates this branch called ADF",
    "start": "1810560",
    "end": "1815570"
  },
  {
    "text": "publish this is your publish branch this is automatically maintained by data factory what is in this branch or your",
    "start": "1815570",
    "end": "1822350"
  },
  {
    "text": "arm templates so if I go into this folder this is my arm template that I",
    "start": "1822350",
    "end": "1828560"
  },
  {
    "text": "can use the deploy data factory so those who are not familiar with arm templates this is essentially a Jers way of doing",
    "start": "1828560",
    "end": "1835550"
  },
  {
    "text": "infrastructure as code so it's a big JSON file which tells exactly how to deploy the pipeline's of your data",
    "start": "1835550",
    "end": "1841910"
  },
  {
    "text": "factory with in Azure okay so jumping",
    "start": "1841910",
    "end": "1847880"
  },
  {
    "text": "back to the slides are we done is that all we need not yet",
    "start": "1847880",
    "end": "1854990"
  },
  {
    "text": "we smelled some configuration to do what happens to all this connection string right well how do you wire up all the",
    "start": "1854990",
    "end": "1860510"
  },
  {
    "start": "1855000",
    "end": "2009000"
  },
  {
    "text": "connection strings of data factory the keys of data Lake and all that sort of stuff that's when key volt comes in so",
    "start": "1860510",
    "end": "1867740"
  },
  {
    "text": "key bulb is this service and Azure where you can use to securely store any",
    "start": "1867740",
    "end": "1872780"
  },
  {
    "text": "relevant secrets or certificates that your applications might use the great",
    "start": "1872780",
    "end": "1877880"
  },
  {
    "text": "thing about key vault it comes with integrations with various Azure services one is data factory so in data factor",
    "start": "1877880",
    "end": "1884270"
  },
  {
    "text": "you can create a key boat service linked service which can be used to pull the",
    "start": "1884270",
    "end": "1889670"
  },
  {
    "text": "secrets from keyboard and then use with other link services for example yours your link services which connects to",
    "start": "1889670",
    "end": "1897020"
  },
  {
    "text": "data warehouse same with data bricks you can also create a cable back secret",
    "start": "1897020",
    "end": "1903320"
  },
  {
    "text": "scope which then is essentially a set of secrets within a scope in in data breaks",
    "start": "1903320",
    "end": "1908750"
  },
  {
    "text": "which mirror whatever it is in keyboard and finally in Azure in add your",
    "start": "1908750",
    "end": "1915230"
  },
  {
    "text": "pipelines you can create something called the variable group which is also linked to key bolt and you can bring in",
    "start": "1915230",
    "end": "1921140"
  },
  {
    "text": "secrets from key vault within a variable group so you can use within your pipe",
    "start": "1921140",
    "end": "1926230"
  },
  {
    "text": "so let's just see how they look what that looks like in data factory so in",
    "start": "1926230",
    "end": "1931940"
  },
  {
    "text": "data factory if I go to do my connections you can see I have a key",
    "start": "1931940",
    "end": "1939320"
  },
  {
    "text": "vote link service this key vote link service is referencing my cable and I've",
    "start": "1939320",
    "end": "1946040"
  },
  {
    "text": "already authenticated the MSI the manage identity of data factory to key votes so",
    "start": "1946040",
    "end": "1952520"
  },
  {
    "text": "that it's authorized to pull the secrets from that specific key boat so my dev factory is authorized to pull from the",
    "start": "1952520",
    "end": "1959060"
  },
  {
    "text": "dev key boat and my stage from a stage and so on and so forth once that's rigged up I can simply in my",
    "start": "1959060",
    "end": "1966950"
  },
  {
    "text": "linked service for my via data factory so this is my data warehouse leaked",
    "start": "1966950",
    "end": "1972020"
  },
  {
    "text": "service it's simply using that linked service of Kibo and pulling down the",
    "start": "1972020",
    "end": "1977210"
  },
  {
    "text": "relevant secrets the beauty of this is as you move across environments all you",
    "start": "1977210",
    "end": "1982520"
  },
  {
    "text": "need to do is update your key book URL and your link service this pull automatically the secrets relevant to",
    "start": "1982520",
    "end": "1989210"
  },
  {
    "text": "that environment as long as your secret name is its uniform across all the environments so it's a really elegant",
    "start": "1989210",
    "end": "1995330"
  },
  {
    "text": "way of swapping over different environments okay so I think we are",
    "start": "1995330",
    "end": "2005740"
  },
  {
    "text": "ready to go a sample release this is how we're gonna do it so we have a build",
    "start": "2005740",
    "end": "2012970"
  },
  {
    "start": "2009000",
    "end": "2231000"
  },
  {
    "text": "pipeline which is the purpose of it is to do up doing some QA so it's running",
    "start": "2012970",
    "end": "2018160"
  },
  {
    "text": "my unit tests some linting building my backpack building so that validates the backpack and so on that will get",
    "start": "2018160",
    "end": "2026320"
  },
  {
    "text": "triggered on any pull request master and then any commit the master will run the second part of the build pipeline which",
    "start": "2026320",
    "end": "2033340"
  },
  {
    "text": "will build my backpack will bill will create the wheel file for the Python package along with publishing the",
    "start": "2033340",
    "end": "2039340"
  },
  {
    "text": "artifacts so my notebooks my date of my any other scripts that's relevant to my",
    "start": "2039340",
    "end": "2044650"
  },
  {
    "text": "pipeline now this produce all produces all the build artifacts and I'm using",
    "start": "2044650",
    "end": "2051220"
  },
  {
    "text": "also that ADF publish branch the one with my arm templates as a build artifacts artifact which will then feed",
    "start": "2051220",
    "end": "2057760"
  },
  {
    "text": "into my pipeline so the release pipeline then will deploy to stage and to prod for the",
    "start": "2057760",
    "end": "2063310"
  },
  {
    "text": "demo only deploying to stage for simplicity purposes but you can imagine you have a keyboard that's specific to",
    "start": "2063310",
    "end": "2069940"
  },
  {
    "text": "each environment and the pipelines will be pulling from this cable the the",
    "start": "2069940",
    "end": "2075909"
  },
  {
    "text": "keyboard relevant to that environment for the specific for the configurations needed to deploy okay",
    "start": "2075910",
    "end": "2082060"
  },
  {
    "text": "so let's jump into the demo okay so I'm gonna first show you the pipeline so",
    "start": "2082060",
    "end": "2090310"
  },
  {
    "text": "here's the pipeline that's sitting here it's not yet running but once I go into my pull request and actually create a",
    "start": "2090310",
    "end": "2098230"
  },
  {
    "text": "pull request and I'll say yep create a pull request there's probably gonna be a",
    "start": "2098230",
    "end": "2103960"
  },
  {
    "text": "code review process and so on and so forth but you can see that it's now triggered a bunch of pipelines to",
    "start": "2103960",
    "end": "2110200"
  },
  {
    "text": "validate my PR so let's have a look at my pipeline there should be a pipeline",
    "start": "2110200",
    "end": "2115540"
  },
  {
    "text": "coming up so you can see now it's queued and it should be running in one second so here it's running the first stage of",
    "start": "2115540",
    "end": "2122530"
  },
  {
    "text": "that pipeline so let's have a look at the first day that's the validate PR stage and it's validating my Python",
    "start": "2122530",
    "end": "2128440"
  },
  {
    "text": "packages and my sequel packages so while that's running let's have a look at the definition of the build artifact a build",
    "start": "2128440",
    "end": "2134740"
  },
  {
    "text": "pipeline right the build pipelines are all defined in llam so yeah mow it",
    "start": "2134740",
    "end": "2140380"
  },
  {
    "text": "build pipe other pipelines in the relatively latest release you can you",
    "start": "2140380",
    "end": "2145840"
  },
  {
    "text": "can define your pipelines all in llam so the first job here sir first stage is",
    "start": "2145840",
    "end": "2152470"
  },
  {
    "text": "my validate PR stage let me just open it up and you see you can see here that I",
    "start": "2152470",
    "end": "2157990"
  },
  {
    "text": "am triggering when the reason is a pull request so only trigger this part if I'm",
    "start": "2157990",
    "end": "2163870"
  },
  {
    "text": "doing a pull request within it I have two jobs the validate Python and validate sequel packages so in my first",
    "start": "2163870",
    "end": "2171100"
  },
  {
    "text": "job I am simply installing any dependencies",
    "start": "2171100",
    "end": "2176350"
  },
  {
    "text": "that my package needs to build I'm running some linting and I'm running my",
    "start": "2176350",
    "end": "2181780"
  },
  {
    "text": "unit tests very simple now my second job is simply doing a vs build so if you",
    "start": "2181780",
    "end": "2191950"
  },
  {
    "text": "think about it you're to a server database project it's just a simple visual studio project so it follows the same pattern of actually",
    "start": "2191950",
    "end": "2198609"
  },
  {
    "text": "building it so I'm just doing a new get restore just in case someone wants to put nougat packages in the solution I'm",
    "start": "2198609",
    "end": "2204910"
  },
  {
    "text": "doing a build I'm doing some tests if I have unit tests and and that's it",
    "start": "2204910",
    "end": "2211089"
  },
  {
    "text": "so that basically validates that my dad Pat can actually build and can actually pass the unit tests okay so I can see",
    "start": "2211089",
    "end": "2219309"
  },
  {
    "text": "that my pipeline has should be all complete by now if not it's still",
    "start": "2219309",
    "end": "2227890"
  },
  {
    "text": "running okay let's give it a little bit more it should be completing in the next",
    "start": "2227890",
    "end": "2234910"
  },
  {
    "start": "2231000",
    "end": "2346000"
  },
  {
    "text": "few seconds but let's jump first the next next stage which is essentially",
    "start": "2234910",
    "end": "2240099"
  },
  {
    "text": "when I commit to master what will happen so this is this is actually publishing the artifacts now one of the most",
    "start": "2240099",
    "end": "2246279"
  },
  {
    "text": "important conditions here is you do not want to publish artifacts unless it's from the master branch because this is a",
    "start": "2246279",
    "end": "2253359"
  },
  {
    "text": "common thing that I find sometimes is that you're publishing artifacts even it's just pull requests you don't want",
    "start": "2253359",
    "end": "2258700"
  },
  {
    "text": "to do that because then you can really create a release from unvalidated code so you just want to publish when it's on",
    "start": "2258700",
    "end": "2264700"
  },
  {
    "text": "master and you can see I have three other jobs here which is my Python package my static artifacts and my",
    "start": "2264700",
    "end": "2271719"
  },
  {
    "text": "sequel packages so before I just continue there let me just double check okay cool that's done let me merge that",
    "start": "2271719",
    "end": "2277960"
  },
  {
    "text": "in so I have my my my pull requests have been validated I can just merge that",
    "start": "2277960",
    "end": "2285039"
  },
  {
    "text": "pull request into master now this should trigger the second part of the build now once it's actually merged into master so",
    "start": "2285039",
    "end": "2292839"
  },
  {
    "text": "I am here in the pipeline on my master branch and if I do a refresh I should",
    "start": "2292839",
    "end": "2299140"
  },
  {
    "text": "see the text pop out in the description there you go hello from NDC Sydenham",
    "start": "2299140",
    "end": "2305829"
  },
  {
    "text": "because that's the master branch now I'm gonna publish this this art of this pipeline to my dev data Factory and",
    "start": "2305829",
    "end": "2312489"
  },
  {
    "text": "you can see that it's trying to figure out the deaf and when it's not it's figured out oh I've updated the pipeline",
    "start": "2312489",
    "end": "2318579"
  },
  {
    "text": "description I'm gonna hit okay this is going to deploy the data factory to my dev data factory cool and it's",
    "start": "2318579",
    "end": "2325920"
  },
  {
    "text": "generate the arm templates in the ADF publish branch so these are the arm templates are getting updated in that",
    "start": "2325920",
    "end": "2332370"
  },
  {
    "text": "branch the the ATF publish branch which then I can use in my my deployment pipelines great ok so once that's done",
    "start": "2332370",
    "end": "2342140"
  },
  {
    "text": "it should trigger the second release so you can see that's right a second part",
    "start": "2342140",
    "end": "2347880"
  },
  {
    "text": "of my build pipeline so that's running and within that it's running the package the the remaining part of my build",
    "start": "2347880",
    "end": "2354960"
  },
  {
    "text": "pipeline so if I go back to my build pipeline let's just look at what that looks like and that's basically",
    "start": "2354960",
    "end": "2362510"
  },
  {
    "text": "essentially doing exactly the same as the pipe the Python packages so I'm",
    "start": "2362510",
    "end": "2367920"
  },
  {
    "text": "installing installing the requirements in order to build it I'm calling make",
    "start": "2367920",
    "end": "2373440"
  },
  {
    "text": "this my make file and a make file basically just creates the wheel package in Python world it's a wheel package in",
    "start": "2373440",
    "end": "2380610"
  },
  {
    "text": "dotnet world is a DLL sort of same same analogy so that's creating the wheel package and I'm just doing some some",
    "start": "2380610",
    "end": "2387210"
  },
  {
    "text": "simple versioning here and I'm I'm setting the patch version as my build ID so that so that it increments every time",
    "start": "2387210",
    "end": "2393540"
  },
  {
    "text": "I do a build and then finally I just do a publish build artifact step which",
    "start": "2393540",
    "end": "2400860"
  },
  {
    "text": "basically publishes this as a build art event so that's my Python package the",
    "start": "2400860",
    "end": "2408750"
  },
  {
    "start": "2406000",
    "end": "2437000"
  },
  {
    "text": "static artifacts is that I have the number of others started at the artifacts that I'll be using in my release pack pipeline you can simply",
    "start": "2408750",
    "end": "2415950"
  },
  {
    "text": "just publish this as is so these are my notebooks because this this data bricks",
    "start": "2415950",
    "end": "2421050"
  },
  {
    "text": "bit is mine they make the data bricks notebooks and this one is a deployment script that I'll be using for my data",
    "start": "2421050",
    "end": "2427560"
  },
  {
    "text": "factory publish it's a deployment ADF it's a PowerShell script which I will be explaining once I get to the release",
    "start": "2427560",
    "end": "2433800"
  },
  {
    "text": "part portion and then finally you have your Seco packages so your sequel",
    "start": "2433800",
    "end": "2441690"
  },
  {
    "start": "2437000",
    "end": "2493000"
  },
  {
    "text": "packages this is your DAC pack so I'm simply doing a build I am running my tests again if just just for kicks and",
    "start": "2441690",
    "end": "2450780"
  },
  {
    "text": "then I'm finally doing a publish build and I'm publishing that specific backpack",
    "start": "2450780",
    "end": "2457549"
  },
  {
    "text": "okay so let's have a look at our run",
    "start": "2458100",
    "end": "2462740"
  },
  {
    "text": "okay so it's still going so you can see",
    "start": "2463520",
    "end": "2468900"
  },
  {
    "text": "in Asscher are in a DevOps you can inspect the jobs as it runs through and",
    "start": "2468900",
    "end": "2473970"
  },
  {
    "text": "you can get all the logs as it goes as it runs through the pipeline's okay so once that's right I'll leave it",
    "start": "2473970",
    "end": "2482490"
  },
  {
    "text": "there for now and I will just jump back to the slides while well that's still running actually before I jump to the",
    "start": "2482490",
    "end": "2492960"
  },
  {
    "text": "slides let's let's actually look at the release so I'm once this is done what I'm gonna do is I'm gonna be creating a",
    "start": "2492960",
    "end": "2498300"
  },
  {
    "start": "2493000",
    "end": "2576000"
  },
  {
    "text": "release so the release is now gonna pick the artifacts that the build has created",
    "start": "2498300",
    "end": "2504240"
  },
  {
    "text": "and then publish this as as into my stage in my prod environments so the",
    "start": "2504240",
    "end": "2512010"
  },
  {
    "text": "stage and prod environments look like this so I have already my stage and prod",
    "start": "2512010",
    "end": "2517650"
  },
  {
    "text": "resource groups and the one that I'm going to be publishing is the stage so let's have a look at that and it already",
    "start": "2517650",
    "end": "2523710"
  },
  {
    "text": "has the relevant artifacts within that's already pre deployed so you have the",
    "start": "2523710",
    "end": "2528960"
  },
  {
    "text": "data bricks workspace you'd have your data factory and key vault and so on so",
    "start": "2528960",
    "end": "2534330"
  },
  {
    "text": "there you go so these are the we've already have a bunch of artifacts sitting there and this is the staging",
    "start": "2534330",
    "end": "2540930"
  },
  {
    "text": "data factory that we are publishing to and we want to expect that once the",
    "start": "2540930",
    "end": "2547200"
  },
  {
    "text": "release completes we'll have the text sitting here okay so let's have a look",
    "start": "2547200",
    "end": "2553200"
  },
  {
    "text": "back in there in the it's still going",
    "start": "2553200",
    "end": "2560010"
  },
  {
    "text": "it's a little slow today you see that takes in two minutes I'll pause here",
    "start": "2560010",
    "end": "2565770"
  },
  {
    "text": "just to take any questions because we do need to wait for this to finish to kick off the release no questions",
    "start": "2565770",
    "end": "2574440"
  },
  {
    "text": "okay let's let's just look at the release definition while that's going okay so we have three stages and this is",
    "start": "2574440",
    "end": "2583890"
  },
  {
    "start": "2576000",
    "end": "2614000"
  },
  {
    "text": "what a release pipeline goes I'll let me just go back one step further so if I go to release pipeline and click Edit",
    "start": "2583890",
    "end": "2591150"
  },
  {
    "text": "you can see that I am pulling from two artifacts for my data pipeline CI artifact the build pipeline and also the",
    "start": "2591150",
    "end": "2599430"
  },
  {
    "text": "ADF publish branch this is my get the ADF get get branch that I showed you",
    "start": "2599430",
    "end": "2605759"
  },
  {
    "text": "earlier so ADF publish and then in my stage you can create a stage this is deploying to",
    "start": "2605759",
    "end": "2612539"
  },
  {
    "text": "do the staging environment I have three jobs or agent jobs the first agent job",
    "start": "2612539",
    "end": "2619799"
  },
  {
    "start": "2614000",
    "end": "2676000"
  },
  {
    "text": "deploys two data bricks so the first step within this job I'm simply just setting the Python version",
    "start": "2619799",
    "end": "2625859"
  },
  {
    "text": "and then I'm using I'm installing the data brick CLI in the build agent and",
    "start": "2625859",
    "end": "2632029"
  },
  {
    "text": "then I am using the dis data break CLI to actually upload any app library so my",
    "start": "2632029",
    "end": "2638910"
  },
  {
    "text": "wheel packaged into dbfs and then the notebooks to my workspace so once that's",
    "start": "2638910",
    "end": "2645269"
  },
  {
    "text": "uploaded I can that essentially you've ready deployed your notebooks and your",
    "start": "2645269",
    "end": "2650430"
  },
  {
    "text": "packages the way your data bricks workspace actually deploys in the correct environment is using environment",
    "start": "2650430",
    "end": "2656309"
  },
  {
    "text": "variables so here I am setting the data bricks host and token and this host and",
    "start": "2656309",
    "end": "2662309"
  },
  {
    "text": "token basically tells the CLI which workspace to deploy to and this",
    "start": "2662309",
    "end": "2667890"
  },
  {
    "text": "workspace and token is coming from a bunch of variables your my build variables or my release variables so",
    "start": "2667890",
    "end": "2674670"
  },
  {
    "text": "let's have a look at the variables here the variables are all set in the",
    "start": "2674670",
    "end": "2680130"
  },
  {
    "start": "2676000",
    "end": "2715000"
  },
  {
    "text": "variables tab in Azure DevOps some of them are not necessarily secret so I'm",
    "start": "2680130",
    "end": "2685680"
  },
  {
    "text": "just putting them straight up here in the variables but I'm also pulling some",
    "start": "2685680",
    "end": "2691259"
  },
  {
    "text": "variables from Keeble so these are my secrets right so two of the secrets that",
    "start": "2691259",
    "end": "2696779"
  },
  {
    "text": "I'm pulling is the domain and token this is the host and token of my data bricks workspace okay so let me just okay cool",
    "start": "2696779",
    "end": "2704309"
  },
  {
    "text": "the the package has been published that's great let us now just make sure",
    "start": "2704309",
    "end": "2711900"
  },
  {
    "text": "that that's okay let's create a release so this when you do a crate release you",
    "start": "2711900",
    "end": "2719970"
  },
  {
    "start": "2715000",
    "end": "2781000"
  },
  {
    "text": "can either trigger it manually and you can select the specific version of of the build pipeline yet you want to",
    "start": "2719970",
    "end": "2726300"
  },
  {
    "text": "actually release and you can also set up CI CD so that once there's a new",
    "start": "2726300",
    "end": "2732210"
  },
  {
    "text": "artifacts actually available it will trigger the release automatically now I",
    "start": "2732210",
    "end": "2737760"
  },
  {
    "text": "do find that lots of folks that I work with typically want some manual or manual steps within their release you",
    "start": "2737760",
    "end": "2744600"
  },
  {
    "text": "can definitely put some manual validation gates so that it blocks in certain stages for someone to actually",
    "start": "2744600",
    "end": "2749670"
  },
  {
    "text": "verify the release as it's actually good so let me create that release and that",
    "start": "2749670",
    "end": "2756120"
  },
  {
    "text": "would go and queue up the release so let's just jump back to the release pipeline now the second part of the",
    "start": "2756120",
    "end": "2765000"
  },
  {
    "text": "release pipeline so we've talked about the data breaks bit the second part is data factory so data factory is your",
    "start": "2765000",
    "end": "2771090"
  },
  {
    "text": "just standard arm template right so the arm template deployment resource arm as",
    "start": "2771090",
    "end": "2777120"
  },
  {
    "text": "your resource group deployment task is perfectly suitable for this this basically takes the arm template that we",
    "start": "2777120",
    "end": "2784020"
  },
  {
    "start": "2781000",
    "end": "2853000"
  },
  {
    "text": "that's produced by the ABF publish and deploys it to a relevant resource group",
    "start": "2784020",
    "end": "2789360"
  },
  {
    "text": "in this case the staging resource group the most important part of this is essentially this guy you want to",
    "start": "2789360",
    "end": "2796080"
  },
  {
    "text": "override the template parameters right so you want to make sure that you're",
    "start": "2796080",
    "end": "2801120"
  },
  {
    "text": "actually overriding that the parameters in your arm template that was pointing to dev and make sure that you're",
    "start": "2801120",
    "end": "2806370"
  },
  {
    "text": "actually pointing to staging so once you've override the parameters to the",
    "start": "2806370",
    "end": "2811440"
  },
  {
    "text": "correct data factor it should deploy to that environment yes",
    "start": "2811440",
    "end": "2818360"
  },
  {
    "text": "yep so this this guy incremental so incremental means it will always do an",
    "start": "2826730",
    "end": "2833040"
  },
  {
    "text": "incremental change it won't drop and recreate but you can change that to complete which will drop and recreate",
    "start": "2833040",
    "end": "2840200"
  },
  {
    "text": "obviously exactly so you do not want to do a complete because this it will drop",
    "start": "2840200",
    "end": "2847320"
  },
  {
    "text": "whatever is in that resource group and make sure it conforms to that arm template right okay and this other steps",
    "start": "2847320",
    "end": "2855060"
  },
  {
    "start": "2853000",
    "end": "2887000"
  },
  {
    "text": "that your that I've added here which I've haven't really talked about this one basically just stops in and restarts",
    "start": "2855060",
    "end": "2860550"
  },
  {
    "text": "a trigger because there's this work and data factory where you can't deploy to an existing data factory if you have",
    "start": "2860550",
    "end": "2866610"
  },
  {
    "text": "then stopped the triggers so you need to manually or code encode manually stop the triggers deploy your data factory",
    "start": "2866610",
    "end": "2871830"
  },
  {
    "text": "and then restart the trigger so that's what that PowerShell script was doing in my build by plane that PowerShell script",
    "start": "2871830",
    "end": "2878700"
  },
  {
    "text": "by the way is available in Azure dogs so I just in the tree just copy paste in it so they they they know that this scenario exists so they've provided it",
    "start": "2878700",
    "end": "2885600"
  },
  {
    "text": "to everyone and lastly we have the backpack task so the backpack task as",
    "start": "2885600",
    "end": "2891330"
  },
  {
    "start": "2887000",
    "end": "2934000"
  },
  {
    "text": "you imagine I'm taking that backpack and just using that and deploying it to the relevant server now this backpack the",
    "start": "2891330",
    "end": "2899070"
  },
  {
    "text": "cool thing here is that you can override or you can set variables that are parameterize your DAC pack deployments",
    "start": "2899070",
    "end": "2904800"
  },
  {
    "text": "so in this case I'm a mature eyes the ABL s location and credential so just if",
    "start": "2904800",
    "end": "2910560"
  },
  {
    "text": "I just look into my into what that looks like in my script",
    "start": "2910560",
    "end": "2915930"
  },
  {
    "text": "I've actually parameterised this location setting in my external data source that way I can pass the parameter",
    "start": "2915930",
    "end": "2922770"
  },
  {
    "text": "when I deploy and it will it will point in the relevant data Lake okay so I am",
    "start": "2922770",
    "end": "2931410"
  },
  {
    "text": "hoping that my release pipeline is almost done but if it's not that's okay I I have one",
    "start": "2931410",
    "end": "2938250"
  },
  {
    "start": "2934000",
    "end": "2984000"
  },
  {
    "text": "last thing to show which i think is still very important and I lots of",
    "start": "2938250",
    "end": "2943530"
  },
  {
    "text": "people kind of miss is monitoring so monitoring I know lots of people like to",
    "start": "2943530",
    "end": "2949170"
  },
  {
    "text": "focus on the CI CD bit when we talk about DevOps but monitoring is just an important in fact I think",
    "start": "2949170",
    "end": "2954630"
  },
  {
    "text": "when you go into production this will be the most important thing that you can actually have so you want to monitor",
    "start": "2954630",
    "end": "2961230"
  },
  {
    "text": "your infrastructure pipeline and data don't forget your data as your monitor",
    "start": "2961230",
    "end": "2966540"
  },
  {
    "text": "is this all Ingham very very feature-rich monitoring suite on Azure it comes with all of this services here",
    "start": "2966540",
    "end": "2974340"
  },
  {
    "text": "and I obviously don't have talked time to talk about everything but I will be focusing on how you actually monitor",
    "start": "2974340",
    "end": "2980220"
  },
  {
    "text": "your data because this is something that lots of people kind of miss so while my",
    "start": "2980220",
    "end": "2985530"
  },
  {
    "start": "2984000",
    "end": "3046000"
  },
  {
    "text": "release is still going let's jump to monitoring and I'll just go to my",
    "start": "2985530",
    "end": "2991410"
  },
  {
    "text": "notebook again and I didn't scroll all the way to the end because there's this metrics piece of code right here this",
    "start": "2991410",
    "end": "2999030"
  },
  {
    "text": "metrics piece of code is using app insight so I'm instantiating an",
    "start": "2999030",
    "end": "3004300"
  },
  {
    "text": "telemetry client using the app insights library and then I'm doing a count on",
    "start": "3004300",
    "end": "3009710"
  },
  {
    "text": "the data frames that I've loaded that way I can track how many records I've loaded and expose this in a dashboard",
    "start": "3009710",
    "end": "3015500"
  },
  {
    "text": "now you can add even more sophisticated monitoring for example are there nulls what's that what's the average of this",
    "start": "3015500",
    "end": "3022550"
  },
  {
    "text": "this column and so and so forth right and then you can send that to application insights as a metric and",
    "start": "3022550",
    "end": "3029210"
  },
  {
    "text": "then you can then post this in the dashboard so that you can know if there's bad data coming in I I would I",
    "start": "3029210",
    "end": "3036140"
  },
  {
    "text": "would also definitely track my malformed count so if that goes up you can send an alert if we all of a sudden you have a",
    "start": "3036140",
    "end": "3042890"
  },
  {
    "text": "spike in bad rows that's coming through your pipe so once you've set that up this will run every time your notebook",
    "start": "3042890",
    "end": "3049670"
  },
  {
    "start": "3046000",
    "end": "3175000"
  },
  {
    "text": "job runs then you can just navigate to application inside so this is application insights and if I go to the",
    "start": "3049670",
    "end": "3057050"
  },
  {
    "text": "metric namespace and you can see I have a custom metric namespace right here I'm just gonna go select that and all my",
    "start": "3057050",
    "end": "3064340"
  },
  {
    "text": "metrics that I'm capturing is available here for me to chart so if I go here as",
    "start": "3064340",
    "end": "3069560"
  },
  {
    "text": "I want to do a bar chart and I'll make it sum of all the records that's been",
    "start": "3069560",
    "end": "3075950"
  },
  {
    "text": "loaded in the past every run and you can see there's they're slowly adding new dimensions to my dimension tables and I",
    "start": "3075950",
    "end": "3083480"
  },
  {
    "text": "can just simply tell pin that to my current dashboard and if I view my dashboard this is what site so I have my row counts of the",
    "start": "3083480",
    "end": "3091250"
  },
  {
    "text": "different dimensions and this is the new thing that I pinned so you can definitely build sophisticated dashboards using the azure monitor suite",
    "start": "3091250",
    "end": "3099320"
  },
  {
    "text": "the other thing that I quickly want to show you is you can also use cousteau so",
    "start": "3099320",
    "end": "3104480"
  },
  {
    "text": "once you have your data factory so this is my data factory you can go to diagnostic settings go to diagnosis",
    "start": "3104480",
    "end": "3111920"
  },
  {
    "text": "settings and you can enable diagnostic settings here and send your diagnostic settings to log analytics now once you",
    "start": "3111920",
    "end": "3119480"
  },
  {
    "text": "have your logs flowing in to add your data into log analytics you can use",
    "start": "3119480",
    "end": "3124820"
  },
  {
    "text": "cousteau which is a query language available in Azure ledger log analytics",
    "start": "3124820",
    "end": "3131300"
  },
  {
    "text": "to actually write sophisticated queries to expose this into your dashboard so",
    "start": "3131300",
    "end": "3137270"
  },
  {
    "text": "this is a relatively simple query but you can you get the gist so I'm just some I'm just filtering the logs based",
    "start": "3137270",
    "end": "3144770"
  },
  {
    "text": "on my specific resource and I'm just doing a group by in cousteau it's called summarize count by level and level is",
    "start": "3144770",
    "end": "3151490"
  },
  {
    "text": "the type of log in this case I don't have any errors it's all informational logs but if there were error so it's",
    "start": "3151490",
    "end": "3156950"
  },
  {
    "text": "gonna show up here and it will I can then post this into my dashboard which I've already done so this is basically",
    "start": "3156950",
    "end": "3163190"
  },
  {
    "text": "saying that should be a pie chart but I think it's my browser resolution is kind of screwing up with the rendering but",
    "start": "3163190",
    "end": "3169160"
  },
  {
    "text": "that should be a pie chart saying 72 Gudrun's have been performed okay so I",
    "start": "3169160",
    "end": "3175580"
  },
  {
    "start": "3175000",
    "end": "3380000"
  },
  {
    "text": "am hoping my release pipeline okay cool it's done so now if I navigate to my staging data",
    "start": "3175580",
    "end": "3182060"
  },
  {
    "text": "factory so this is my staging data factory and if i refresh this refresh",
    "start": "3182060",
    "end": "3187520"
  },
  {
    "text": "this I have hello from NEC Sydney cool",
    "start": "3187520",
    "end": "3192920"
  },
  {
    "text": "deploy two-stage awesome alright so just to summarize the key learnings is you",
    "start": "3192920",
    "end": "3203720"
  },
  {
    "text": "want to validate early your pipeline push your validation right before you hit your silver silver tier data sets",
    "start": "3203720",
    "end": "3209290"
  },
  {
    "text": "ensure your data pipeline is replayable 1/8 aka you want to build idempotent",
    "start": "3209290",
    "end": "3214400"
  },
  {
    "text": "pipelines you because you know things will go wrong you will want to be able to replay your pipelines ok",
    "start": "3214400",
    "end": "3222290"
  },
  {
    "text": "the first one was leverage data tearing in the data late and automate your deployment pipelines now there's an",
    "start": "3222290",
    "end": "3228440"
  },
  {
    "text": "asterisk here because I know certain organizations want some manual steps manual validation Sept and that's",
    "start": "3228440",
    "end": "3233750"
  },
  {
    "text": "totally fine with if that's their process but it's definitely good if you have at least a run sheet or a csv",
    "start": "3233750",
    "end": "3240530"
  },
  {
    "text": "pipeline to get you partial to the all the way or at least 90% of the way",
    "start": "3240530",
    "end": "3245830"
  },
  {
    "text": "ensure your data transformation code accessible so you can write proper unit tests secure and centralize your",
    "start": "3245830",
    "end": "3251780"
  },
  {
    "text": "configuration and last but not the least is monitoring infrastructure pipelines",
    "start": "3251780",
    "end": "3257030"
  },
  {
    "text": "and data all the artifacts that I've shown here is available in my github you",
    "start": "3257030",
    "end": "3263060"
  },
  {
    "text": "can just go to github dev lace data DevOps and all of that I've shown including some deployment scripts arm",
    "start": "3263060",
    "end": "3268970"
  },
  {
    "text": "templates notebooks Python packages are all in here and that is my talk so if",
    "start": "3268970",
    "end": "3277070"
  },
  {
    "text": "you can reach me Twitter or you can navigate to github I do have other demos",
    "start": "3277070",
    "end": "3283460"
  },
  {
    "text": "in github if you're interested and with that I think I have around five minutes for questions",
    "start": "3283460",
    "end": "3290470"
  },
  {
    "text": "yes set okay that's a good question",
    "start": "3292210",
    "end": "3298070"
  },
  {
    "text": "so this talk has been in the making for the past few months but I it's not like",
    "start": "3298070",
    "end": "3303590"
  },
  {
    "text": "I've been working on it you know 24 you know every every day on this talk so I actually gave an iteration of this talk",
    "start": "3303590",
    "end": "3309680"
  },
  {
    "text": "maybe in July and in May but it was a much shorter talk so I've really built out those artifacts maybe in total I'd",
    "start": "3309680",
    "end": "3317210"
  },
  {
    "text": "say a week or two of work mostly because the the transformation logic was fairly",
    "start": "3317210",
    "end": "3324890"
  },
  {
    "text": "simple and if you're familiar with the services it should be relatively it's just an exercise I'm just sending it up",
    "start": "3324890",
    "end": "3331220"
  },
  {
    "text": "right so I think the biggest roadblock to this is there's so many moving parts",
    "start": "3331220",
    "end": "3336470"
  },
  {
    "text": "and there's so many different services that's involved that every one of them",
    "start": "3336470",
    "end": "3342140"
  },
  {
    "text": "if you haven't played with it will there will have a learning curve so that's probably what if you're familiar with",
    "start": "3342140",
    "end": "3348830"
  },
  {
    "text": "already the individual parts putting down together shouldn't be that painful",
    "start": "3348830",
    "end": "3354250"
  },
  {
    "text": "any other questions no well lastly",
    "start": "3354410",
    "end": "3360630"
  },
  {
    "text": "please make sure to put a green post-it as you go out if not let me know give me",
    "start": "3360630",
    "end": "3367049"
  },
  {
    "text": "some feedback what can be better with my talk which reach out to me and Twitter but with that thank you all for coming",
    "start": "3367049",
    "end": "3374579"
  },
  {
    "text": "and I hope you enjoyed your talk the dog [Applause]",
    "start": "3374579",
    "end": "3381869"
  }
]