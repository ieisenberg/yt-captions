[
  {
    "text": "hello and welcome to this session about",
    "start": "9830",
    "end": "15180"
  },
  {
    "text": "ethics and AI my name is Tess and I am a data scientist and software engineer at",
    "start": "15180",
    "end": "21060"
  },
  {
    "text": "Microsoft and I'm also very very interested in ethics around AI and I'm part of ethics groups to sort of",
    "start": "21060",
    "end": "29670"
  },
  {
    "text": "determine what we should and shouldn't do in in AI in Microsoft and specifically in our group and last year",
    "start": "29670",
    "end": "37830"
  },
  {
    "text": "a couple of researchers in LA and came up with this model or this software to",
    "start": "37830",
    "end": "47820"
  },
  {
    "text": "try to figure out if some crime was gang-related or not so they published a",
    "start": "47820",
    "end": "54000"
  },
  {
    "text": "paper called partially generative neural networks for gang crime classification",
    "start": "54000",
    "end": "60090"
  },
  {
    "text": "with partial information now the partial in this case is that they base their",
    "start": "60090",
    "end": "66240"
  },
  {
    "text": "decisions on text in the crime descriptions and if they don't have text",
    "start": "66240",
    "end": "73170"
  },
  {
    "text": "and they generate some text so right there it sounds a bit fishy I guess but",
    "start": "73170",
    "end": "78630"
  },
  {
    "text": "they also base it on something called a kal gang database the California gang-related activity database so this",
    "start": "78630",
    "end": "86189"
  },
  {
    "text": "is obviously something that seems like a very very useful piece of software to have if you can classify if a crime is",
    "start": "86189",
    "end": "93600"
  },
  {
    "text": "gang-related or not you can put more scrutiny on these crimes and you can put more efforts you can make sure you track",
    "start": "93600",
    "end": "101040"
  },
  {
    "text": "these people and maybe look at the areas where they live and things like that and",
    "start": "101040",
    "end": "107579"
  },
  {
    "text": "I kind of get rid of the gang-related crime you know but what happens if you",
    "start": "107579",
    "end": "114030"
  },
  {
    "text": "get it wrong like what happens you classify crime as gang-related and it",
    "start": "114030",
    "end": "119610"
  },
  {
    "text": "turns out not to be in that case not only the perpetrators but also the",
    "start": "119610",
    "end": "126420"
  },
  {
    "text": "victims will be labeled as gang related or gang members and they will probably",
    "start": "126420",
    "end": "133230"
  },
  {
    "text": "be stored in this database the in the kal gang database and get a lot more scrutiny from hereon and unfortunately",
    "start": "133230",
    "end": "141569"
  },
  {
    "text": "there a couple of problems with this database apart from someone being labeled as",
    "start": "141569",
    "end": "147230"
  },
  {
    "text": "gang-related that's not and that is the fact that about 25 percent of the people",
    "start": "147230",
    "end": "153770"
  },
  {
    "text": "in this database and that the research is based on are in fact not proven to be",
    "start": "153770",
    "end": "159200"
  },
  {
    "text": "gang-related most of them are Hispanic and black from certain neighborhoods and",
    "start": "159200",
    "end": "166850"
  },
  {
    "text": "solely in there because they happen to live in these neighborhoods but the problem is also that once you get into",
    "start": "166850",
    "end": "172970"
  },
  {
    "text": "this database you hardly ever get purged from it and this database is used by",
    "start": "172970",
    "end": "178910"
  },
  {
    "text": "companies that hire to determine if you should be hired or not so it's quite consequential if you get labeled as an",
    "start": "178910",
    "end": "186890"
  },
  {
    "text": "active gang member and you're not and when they did audits on this database",
    "start": "186890",
    "end": "192950"
  },
  {
    "text": "and found these things that for example a lot of people in there are actually not gang related they also found that 42",
    "start": "192950",
    "end": "202280"
  },
  {
    "text": "of the people in there are actually under the age of 1 out of which 28 have",
    "start": "202280",
    "end": "209060"
  },
  {
    "text": "admitted to being active gang members so when they presented this research at a",
    "start": "209060",
    "end": "216350"
  },
  {
    "text": "conference naturally they were met with a lot of questions a lot of questions",
    "start": "216350",
    "end": "222320"
  },
  {
    "text": "like what did you do to fix the database like the problems in the database before you used it for ass data for your model",
    "start": "222320",
    "end": "229610"
  },
  {
    "text": "and what did you do to make sure you didn't used assume that a crime was",
    "start": "229610",
    "end": "234709"
  },
  {
    "text": "gang-related because there were Hispanics or blacks and in this zip code and whatnot and the guy who presented it",
    "start": "234709",
    "end": "242780"
  },
  {
    "text": "he stood up and said I'm just an engineer like what do I have to do with",
    "start": "242780",
    "end": "249709"
  },
  {
    "text": "this and that one sentence is sort of",
    "start": "249709",
    "end": "257359"
  },
  {
    "text": "what I want to talk about today the sentence angered a lot of people get",
    "start": "257359",
    "end": "264260"
  },
  {
    "text": "angry by this but it specifically angered this guy from Google called Blake Lim while stood up and quoted a",
    "start": "264260",
    "end": "270890"
  },
  {
    "text": "song by Tom Lehrer about a German rocket it's called burner from brown whose",
    "start": "270890",
    "end": "279169"
  },
  {
    "text": "rockets were supposed to land on the moon but unfortunately landed in London in the second world war so he said so",
    "start": "279169",
    "end": "289399"
  },
  {
    "text": "once the rockets are up who cares where they fall down Matt is not my department",
    "start": "289399",
    "end": "295339"
  },
  {
    "text": "said further from Brown by the way an awesome documentary too but this I'm",
    "start": "295339",
    "end": "301489"
  },
  {
    "text": "just an engineer I'm here to suggest that maybe we are not just engineers",
    "start": "301489",
    "end": "306759"
  },
  {
    "text": "we're put in charge not only with AI but we know what everything we do is",
    "start": "306759",
    "end": "312589"
  },
  {
    "text": "software engineers in shorts have some very very consequential systems like for",
    "start": "312589",
    "end": "317989"
  },
  {
    "text": "example systems that determine if someone's CV should or should not pass",
    "start": "317989",
    "end": "324259"
  },
  {
    "text": "through to the next face of interviews basically should they get hired should",
    "start": "324259",
    "end": "330559"
  },
  {
    "text": "it get insurance should they get loans",
    "start": "330559",
    "end": "335769"
  },
  {
    "text": "should they live or die like real medical systems that determine who",
    "start": "335769",
    "end": "341539"
  },
  {
    "text": "should get treatment and not so we're not just engineers we have a little bit more responsibility than that but two",
    "start": "341539",
    "end": "350989"
  },
  {
    "text": "years ago probably a lot of you have seen this research that stated that so",
    "start": "350989",
    "end": "357649"
  },
  {
    "text": "Michael comes in ski and created this research and basically said what 50 likes on Facebook I can tell you who you",
    "start": "357649",
    "end": "364849"
  },
  {
    "text": "are better than your friends and sometimes better than your spouse so he",
    "start": "364849",
    "end": "371809"
  },
  {
    "text": "did this research to to sort of scare or",
    "start": "371809",
    "end": "377689"
  },
  {
    "text": "to show that with social media you can do a lot of profiling and someone picked",
    "start": "377689",
    "end": "385729"
  },
  {
    "text": "up this profiling Cambridge analytic ah and use this exact same research in",
    "start": "385729",
    "end": "393829"
  },
  {
    "text": "order to win an election by doing if you went to Gary Schwartz talk and this is",
    "start": "393829",
    "end": "399649"
  },
  {
    "text": "what they base who they should a B and C test their ads on this particular",
    "start": "399649",
    "end": "406490"
  },
  {
    "text": "research now later that year he also published another extremely",
    "start": "406490",
    "end": "411949"
  },
  {
    "text": "extremely controversial paper called the AI gaydar paper or commonly known as dai",
    "start": "411949",
    "end": "417860"
  },
  {
    "text": "gaydar favor so in this paper he claims that giving is the picture of someone's face it can",
    "start": "417860",
    "end": "426650"
  },
  {
    "text": "determine what their sexual orientation is now I can hear some thoughts in here",
    "start": "426650",
    "end": "436430"
  },
  {
    "text": "anything well maybe that's actually doable maybe you could actually do that",
    "start": "436430",
    "end": "441530"
  },
  {
    "text": "and once again he did this sort of to scare and to say that this is something",
    "start": "441530",
    "end": "448699"
  },
  {
    "text": "that people could do we should be very worried that this type of thing could be",
    "start": "448699",
    "end": "454580"
  },
  {
    "text": "out there now so he also wrote and a lot of documents after this paper saying",
    "start": "454580",
    "end": "462469"
  },
  {
    "text": "that he knew that what he liked the methods that he was using were wrong so",
    "start": "462469",
    "end": "469330"
  },
  {
    "text": "actually his his work didn't really work the way it was stated in the paper but",
    "start": "469330",
    "end": "476990"
  },
  {
    "text": "he wanted to point out that someone could come in and do this so a couple of things he did in this paper were he used",
    "start": "476990",
    "end": "485300"
  },
  {
    "text": "data from dating websites where maybe a little bit more stereotypical than you",
    "start": "485300",
    "end": "491360"
  },
  {
    "text": "would be in normal life and his testing for accuracy was also a little bit",
    "start": "491360",
    "end": "498289"
  },
  {
    "text": "dubious but he found a couple of things he found that for example if we look at",
    "start": "498289",
    "end": "505039"
  },
  {
    "text": "from top left so here's like the typical man who likes prefers women men who",
    "start": "505039",
    "end": "512240"
  },
  {
    "text": "prefer men women to prefer women and women who prefer or sorry women who",
    "start": "512240",
    "end": "517279"
  },
  {
    "text": "prefer and men and women who prefer women and he found like stereotypes like",
    "start": "517279",
    "end": "524920"
  },
  {
    "text": "women who prefer men wear more makeup men who prefer women are worse at taking",
    "start": "524920",
    "end": "531800"
  },
  {
    "text": "selfies like you can see their nostrils more in these compsat pictures but",
    "start": "531800",
    "end": "537050"
  },
  {
    "text": "there's a lot bigger issue at stake here and that's that we're classified",
    "start": "537050",
    "end": "542600"
  },
  {
    "text": "these as ones and zeros whether you're not on one end of the spectrum one a man",
    "start": "542600",
    "end": "552260"
  },
  {
    "text": "who likes men and on the other end of the spectrum you're a man who likes",
    "start": "552260",
    "end": "557570"
  },
  {
    "text": "women and that's the only two classes that exist whereas in reality there's a",
    "start": "557570",
    "end": "564890"
  },
  {
    "text": "lot more there's a lot more today's there is anything from bisexual",
    "start": "564890",
    "end": "573320"
  },
  {
    "text": "bicurious someone who hasn't discovered what their sexuality is yet or or",
    "start": "573320",
    "end": "579560"
  },
  {
    "text": "anything in between so classify any things like this even if",
    "start": "579560",
    "end": "585230"
  },
  {
    "text": "we could is very very simplistic and unfortunately also very very",
    "start": "585230",
    "end": "590840"
  },
  {
    "text": "consequential so right now for me for example I live in Sweden and if I get",
    "start": "590840",
    "end": "596630"
  },
  {
    "text": "classified the wrong way it doesn't really matter to me particularly I",
    "start": "596630",
    "end": "603560"
  },
  {
    "text": "already have like grown-up I'd have my my life quite set but if I lived in a",
    "start": "603560",
    "end": "611060"
  },
  {
    "text": "country where being a homosexual was illegal that would have a very very much",
    "start": "611060",
    "end": "616850"
  },
  {
    "text": "higher consequence for me and especially in some countries where it's not only illegal but it's punishable by death so",
    "start": "616850",
    "end": "625190"
  },
  {
    "text": "you can see how this can be used in some very very bad ways and even if you think",
    "start": "625190",
    "end": "631490"
  },
  {
    "text": "like country like Sweden not too long ago we're talking like a couple of",
    "start": "631490",
    "end": "637310"
  },
  {
    "text": "decades ago this was considered sort of a mental disorder luckily not anymore",
    "start": "637310",
    "end": "642890"
  },
  {
    "text": "but even still in libraries it's still some sometimes classified like books",
    "start": "642890",
    "end": "650890"
  },
  {
    "text": "about sexuality that's not super hetero normal are put in in in mental disorder",
    "start": "650890",
    "end": "661010"
  },
  {
    "text": "categories which you can see that having a classification like this has a very",
    "start": "661010",
    "end": "666710"
  },
  {
    "text": "very long tail and especially when there is actually not a classification should be made even",
    "start": "666710",
    "end": "674230"
  },
  {
    "text": "speaking of criminal and a couple of researchers same year in China came up",
    "start": "674240",
    "end": "682560"
  },
  {
    "text": "with this paper that basically claimed that given your face I can tell you if",
    "start": "682560",
    "end": "688829"
  },
  {
    "text": "you're a criminal or not mmm very very useful so they picked out some faces mm",
    "start": "688829",
    "end": "696570"
  },
  {
    "text": "Shani's men and it's based on one face they could say with 90% accuracy whether",
    "start": "696570",
    "end": "703290"
  },
  {
    "text": "or not this person was a criminal or not so yet a data said about third of them",
    "start": "703290",
    "end": "709800"
  },
  {
    "text": "were criminals and two-thirds were not and you might think to get 90% you would",
    "start": "709800",
    "end": "716220"
  },
  {
    "text": "have to pick out some really really hardened criminals like Hannibal Lecter style criminals but no in fact only",
    "start": "716220",
    "end": "725370"
  },
  {
    "text": "about 30 percent of them are violent the rest are fraud and it's random",
    "start": "725370",
    "end": "732089"
  },
  {
    "text": "things like that so does anyone in here I got a speeding ticket if you I can see",
    "start": "732089",
    "end": "738810"
  },
  {
    "text": "that on your face actually or Alex Ned could um again there are problems with",
    "start": "738810",
    "end": "748920"
  },
  {
    "text": "these first off like the the pictures that they used for for the criminals",
    "start": "748920",
    "end": "754529"
  },
  {
    "text": "were government issued IDs very stern whereas the non criminals were corporate",
    "start": "754529",
    "end": "760560"
  },
  {
    "text": "headshots and I kind of present so so what they found was in technical terms",
    "start": "760560",
    "end": "765779"
  },
  {
    "text": "the people who were non criminals had this triangle between the nostrils and and the mouth that had a certain",
    "start": "765779",
    "end": "772860"
  },
  {
    "text": "curvature over 30-percent yada yada very technical in reality like these people",
    "start": "772860",
    "end": "777899"
  },
  {
    "text": "were smiling and these people were not that's sort of what they found and I kind of present like exhibit a of me",
    "start": "777899",
    "end": "785699"
  },
  {
    "text": "being yeah you can see the thing in there was just like in the case of your",
    "start": "785699",
    "end": "794459"
  },
  {
    "text": "sexual orientation this is a very fluid thing and to say that you can say with",
    "start": "794459",
    "end": "800190"
  },
  {
    "text": "90% accuracy whether you're not you're a 1 or 0 not only means that",
    "start": "800190",
    "end": "807260"
  },
  {
    "text": "it should be super super good but it also means that somehow there is a line",
    "start": "807830",
    "end": "813840"
  },
  {
    "text": "between criminal and non-criminal where machines faces or where my face changes",
    "start": "813840",
    "end": "819530"
  },
  {
    "text": "and the question is does that change happen like when I was born because mmm was I born a criminal or did",
    "start": "819530",
    "end": "826920"
  },
  {
    "text": "it happen like when I stole that pen at the office like how is this even",
    "start": "826920",
    "end": "832740"
  },
  {
    "text": "supposed to happen and 90% is also a very very interesting figure because at",
    "start": "832740",
    "end": "837900"
  },
  {
    "text": "the time this was using Alec's net a certain architecture of neural networks",
    "start": "837900",
    "end": "843980"
  },
  {
    "text": "but at a time could determine gender in 86% of the cases so they were doing",
    "start": "843980",
    "end": "853500"
  },
  {
    "text": "pretty good with a 90% I think so the question is maybe not can we do it",
    "start": "853500",
    "end": "859650"
  },
  {
    "text": "because all of these suggestions that we can are quite flawed and even if we",
    "start": "859650",
    "end": "868290"
  },
  {
    "text": "could the question is not can we do it but should we do it should we really be",
    "start": "868290",
    "end": "874340"
  },
  {
    "text": "classifying personal behavior personal sort of personality based on faces is",
    "start": "874340",
    "end": "883200"
  },
  {
    "text": "that what we should be doing Kate Crawford she used to be a researcher at",
    "start": "883200",
    "end": "890610"
  },
  {
    "text": "Microsoft and now she leads the AI now Institute for fairness and accountability she said we're currently",
    "start": "890610",
    "end": "899220"
  },
  {
    "text": "in the biggest experimentation of classification in human history we've",
    "start": "899220",
    "end": "906870"
  },
  {
    "text": "been doing this a lot before we've been doing this through the ages since Aristotle we've been doing",
    "start": "906870",
    "end": "913470"
  },
  {
    "text": "classification based on fighting facial features but suddenly now we're doing it",
    "start": "913470",
    "end": "918540"
  },
  {
    "text": "at scale and without anyone knowing because we will don't have to come up and measure your face they can just take",
    "start": "918540",
    "end": "925980"
  },
  {
    "text": "a picture of you that's readily available somewhere or as you pass something and make a judgement on this",
    "start": "925980",
    "end": "933150"
  },
  {
    "text": "scale and you might think that this is sort of like a black mayor episode of",
    "start": "933150",
    "end": "939930"
  },
  {
    "text": "course people are not doing so of course people realize that this is",
    "start": "939930",
    "end": "945180"
  },
  {
    "text": "not doable that you can't you can't assess with 90% accuracy if someone is a",
    "start": "945180",
    "end": "951150"
  },
  {
    "text": "criminal or not if you can only assess sort of male/female with 86% but it",
    "start": "951150",
    "end": "958290"
  },
  {
    "text": "turns out people do this is a real website where based on your face this",
    "start": "958290",
    "end": "965460"
  },
  {
    "text": "website says you can determine your IQ whether or not your academic research or",
    "start": "965460",
    "end": "971310"
  },
  {
    "text": "a professional poker player or a terrorist using yes your face and you",
    "start": "971310",
    "end": "977250"
  },
  {
    "text": "can clearly see where this could be used right immigrations customs why not",
    "start": "977250",
    "end": "984770"
  },
  {
    "text": "computer says you can this is very very scary because as soon as you make it",
    "start": "984770",
    "end": "992640"
  },
  {
    "text": "into an algorithm and a computer application people will trust it um last",
    "start": "992640",
    "end": "999270"
  },
  {
    "text": "year a Tennessee Oslo my friend Bill Wagner was telling me a story how he and his",
    "start": "999270",
    "end": "1005660"
  },
  {
    "text": "wife were were remodeling their house so they went into a store and guide a",
    "start": "1005660",
    "end": "1011839"
  },
  {
    "text": "couple of two by fours and then they got to the cashier and the cashier sort of",
    "start": "1011839",
    "end": "1018170"
  },
  {
    "text": "scammed their two by fours and it wasn't showing up in the system so he brought",
    "start": "1018170",
    "end": "1024170"
  },
  {
    "text": "over another cashier and he scanned it two by fours and they weren't showing up in the system either and then a curious",
    "start": "1024170",
    "end": "1032780"
  },
  {
    "text": "thing happened because both of them were the two by force in their hands said hmm",
    "start": "1032780",
    "end": "1039280"
  },
  {
    "text": "computer says no we don't carry these we",
    "start": "1039280",
    "end": "1044660"
  },
  {
    "text": "don't carry these because the computer says we don't carry them so weird we're",
    "start": "1044660",
    "end": "1052670"
  },
  {
    "text": "just making it into an algorithm and claiming but you have 90% accurate of",
    "start": "1052670",
    "end": "1057860"
  },
  {
    "text": "something you're doing sort of like this sort of in this accountability",
    "start": "1057860",
    "end": "1063620"
  },
  {
    "text": "laundering because suddenly it's not you as a person is saying missus um as a",
    "start": "1063620",
    "end": "1069260"
  },
  {
    "text": "computer and the computer is kind of right because",
    "start": "1069260",
    "end": "1074740"
  },
  {
    "text": "computer system algorithm assist math it can't be biased it can't pick up its own",
    "start": "1074740",
    "end": "1081600"
  },
  {
    "text": "feelings about whether or not this is a criminal or whether or not this person",
    "start": "1081600",
    "end": "1087760"
  },
  {
    "text": "has this personality trait it's like it's math but unfortunately it's not",
    "start": "1087760",
    "end": "1095290"
  },
  {
    "text": "well it is math but it's math based on a",
    "start": "1095290",
    "end": "1100390"
  },
  {
    "text": "lot of data that contain a lot of biases that we have so I have to admit I'm not",
    "start": "1100390",
    "end": "1108190"
  },
  {
    "text": "a big star Wars fan not at all I can tell one wiki from the next so if you",
    "start": "1108190",
    "end": "1115450"
  },
  {
    "text": "had she waka and Chewbacca and his wife and a couple of other wiki's in the",
    "start": "1115450",
    "end": "1121059"
  },
  {
    "text": "lineup I would not be able to tell like who was who at all and similarly if if I",
    "start": "1121059",
    "end": "1130270"
  },
  {
    "text": "was in a situation and someone asked me which one of these ten Chinese men was",
    "start": "1130270",
    "end": "1137920"
  },
  {
    "text": "in the scene at the same time I could not tell that because I have not like I",
    "start": "1137920",
    "end": "1144250"
  },
  {
    "text": "have not been exposed to enough Chinese men like I don't have many Chinese",
    "start": "1144250",
    "end": "1150309"
  },
  {
    "text": "friends and I have a couple but not enough to give me sort of like the",
    "start": "1150309",
    "end": "1155920"
  },
  {
    "text": "knowledge of how to differentiate between them especially under stress for example because we all have this very",
    "start": "1155920",
    "end": "1164980"
  },
  {
    "text": "very strong in-group bias I'm very very good at determining who's who among",
    "start": "1164980",
    "end": "1171179"
  },
  {
    "text": "European females I know exactly it's like the eye color",
    "start": "1171179",
    "end": "1176679"
  },
  {
    "text": "the hair color things like that things that don't apply to maybe Chinese men I",
    "start": "1176679",
    "end": "1184350"
  },
  {
    "text": "don't have enough training material and in the same way machine learning has the",
    "start": "1184350",
    "end": "1190270"
  },
  {
    "text": "same problem if it's not been exposed to someone outside of its group it won't",
    "start": "1190270",
    "end": "1197110"
  },
  {
    "text": "learn the features that you need to use to differentiate between them so now the",
    "start": "1197110",
    "end": "1203050"
  },
  {
    "text": "question is what is machine learnings in-group bias so I do a lot of computer vision",
    "start": "1203050",
    "end": "1208270"
  },
  {
    "text": "Brod yes and i helped a lot of customers with computer vision and computer vision",
    "start": "1208270",
    "end": "1214340"
  },
  {
    "text": "and determining like objects one object from the other is largely based on this",
    "start": "1214340",
    "end": "1221950"
  },
  {
    "text": "huge data set called image net that's one of the big data sets that we use and",
    "start": "1221950",
    "end": "1228140"
  },
  {
    "text": "basically most other machine learning algorithms are derivatives of this like",
    "start": "1228140",
    "end": "1235490"
  },
  {
    "text": "transfer learning from this one database this database has a lot of like everyday",
    "start": "1235490",
    "end": "1241550"
  },
  {
    "text": "objects or cats and dogs and tables and",
    "start": "1241550",
    "end": "1246740"
  },
  {
    "text": "shares and things like that but unfortunately it's derived like able to",
    "start": "1246740",
    "end": "1253130"
  },
  {
    "text": "say 70 80 percent of pictures taken in Europe and the US which means that if",
    "start": "1253130",
    "end": "1260750"
  },
  {
    "text": "you look at like everyday objects from",
    "start": "1260750",
    "end": "1265760"
  },
  {
    "text": "Africa they will not show up in this data set and it also means that if you look at a picture like this it will",
    "start": "1265760",
    "end": "1274190"
  },
  {
    "text": "classify it as baseball right it is only",
    "start": "1274190",
    "end": "1279230"
  },
  {
    "text": "because this is what it knows this is sort of what what it knows understands",
    "start": "1279230",
    "end": "1284710"
  },
  {
    "text": "remembers and now recall that this is sort of what everything else is built on",
    "start": "1284710",
    "end": "1291260"
  },
  {
    "text": "in the same way when we do face recognition we've largely used a",
    "start": "1291260",
    "end": "1297200"
  },
  {
    "text": "benchmark that's called faces in the wild or labeled faces in the wild where",
    "start": "1297200",
    "end": "1302240"
  },
  {
    "text": "it's been labeled with who it is and how old they are and things like that so labeled faces in the wild is a data set",
    "start": "1302240",
    "end": "1309650"
  },
  {
    "text": "that comes from Yahoo News from 2002 to 2004",
    "start": "1309650",
    "end": "1316060"
  },
  {
    "text": "so it's 77% male because the political",
    "start": "1316120",
    "end": "1322430"
  },
  {
    "text": "hierarchy or like the society hierarchy",
    "start": "1322430",
    "end": "1328130"
  },
  {
    "text": "of who gets in the news it's like that so 70% of the of the pictures in there",
    "start": "1328130",
    "end": "1333980"
  },
  {
    "text": "are male in fact most of the females in there are actresses so lots of makeup",
    "start": "1333980",
    "end": "1340150"
  },
  {
    "text": "long hair like very um up done 80% of the people",
    "start": "1340150",
    "end": "1347600"
  },
  {
    "text": "in this dataset are white or paler shade stuff of white and can you guess who the",
    "start": "1347600",
    "end": "1355880"
  },
  {
    "text": "main person in there like a person who actually makes up a good 5 percent of",
    "start": "1355880",
    "end": "1361730"
  },
  {
    "text": "the data set and I guess is ya know",
    "start": "1361730",
    "end": "1370930"
  },
  {
    "text": "George W Bush I guess now it could be Kim Kardashian actually but back in 2002",
    "start": "1370930",
    "end": "1378230"
  },
  {
    "text": "to 2004 is George W Bush so this means said when we benchmark our face",
    "start": "1378230",
    "end": "1384320"
  },
  {
    "text": "recognition whether or not it's like Facebook Google Microsoft whoever they",
    "start": "1384320",
    "end": "1389690"
  },
  {
    "text": "benchmark against this so it's really really good at recognizing George W Bush",
    "start": "1389690",
    "end": "1395020"
  },
  {
    "text": "awesome at recognizing George W Bush unfortunately less awesome at",
    "start": "1395020",
    "end": "1401030"
  },
  {
    "text": "recognizing other people so this is a data set like many of the data sets that",
    "start": "1401030",
    "end": "1407270"
  },
  {
    "text": "we're using that's sort of label in the industries mail pale and stale and it",
    "start": "1407270",
    "end": "1415880"
  },
  {
    "text": "causes issues like this one so Nick on this is quite an old one but Nikhil was",
    "start": "1415880",
    "end": "1422240"
  },
  {
    "text": "doing this AI software that was supposed to figure out if someone was blinking and in a picture and then not take a",
    "start": "1422240",
    "end": "1429500"
  },
  {
    "text": "picture fortunately not tested for a",
    "start": "1429500",
    "end": "1434900"
  },
  {
    "text": "large group of the population and didn't work for them or when I was using and",
    "start": "1434900",
    "end": "1442070"
  },
  {
    "text": "this still happens when I use face recognition software and like you know",
    "start": "1442070",
    "end": "1448790"
  },
  {
    "text": "how old am i what am I it comes up with mail I'm very happy",
    "start": "1448790",
    "end": "1454520"
  },
  {
    "text": "about 226 I have to say I'm quite alright I'm even to the point where I'm",
    "start": "1454520",
    "end": "1459830"
  },
  {
    "text": "like I'm okay with that enough that I don't care about the other one but it's a bit unfortunate and this is because I",
    "start": "1459830",
    "end": "1467660"
  },
  {
    "text": "don't look like the typical actress that are in these benchmark datasets so",
    "start": "1467660",
    "end": "1475100"
  },
  {
    "text": "I want to show a demo let's see if we can yeah this is gonna work so if we use",
    "start": "1475100",
    "end": "1483050"
  },
  {
    "text": "this is OpenCV a very widely used",
    "start": "1483050",
    "end": "1489670"
  },
  {
    "text": "library for doing all sorts of image recognition tasks or image tasks and one",
    "start": "1490150",
    "end": "1498440"
  },
  {
    "text": "of the things it has is it allows you to bring in models to do face detection for example and if we run days on an image",
    "start": "1498440",
    "end": "1506690"
  },
  {
    "text": "it has very very nice accuracy it can show the squares around in the face of",
    "start": "1506690",
    "end": "1513320"
  },
  {
    "text": "these women but unfortunately if you run it on a picture like this one it does",
    "start": "1513320",
    "end": "1526070"
  },
  {
    "text": "not even recognize the face as a face and we can clearly see that this face is",
    "start": "1526070",
    "end": "1533240"
  },
  {
    "text": "there but unfortunately the database did not contain or the database of these",
    "start": "1533240",
    "end": "1539440"
  },
  {
    "text": "models are targeted against do not contain enough black women so not only",
    "start": "1539440",
    "end": "1546230"
  },
  {
    "text": "is she gonna maybe be a male but she's not even existing as a person so she",
    "start": "1546230",
    "end": "1552860"
  },
  {
    "text": "can't use the systems that you build on top of this type of model this is a very",
    "start": "1552860",
    "end": "1559010"
  },
  {
    "text": "very scary proposition that we're creating a software that's not inclusive",
    "start": "1559010",
    "end": "1564440"
  },
  {
    "text": "enough for some people to even use and the fact that she's wearing a mask in her hand that's white demonstrate this",
    "start": "1564440",
    "end": "1571190"
  },
  {
    "text": "because she's a researcher in AI her name is Joey blue meanie and she's doing",
    "start": "1571190",
    "end": "1576890"
  },
  {
    "text": "a research around face detection but she can't use face detection herself so",
    "start": "1576890",
    "end": "1586640"
  },
  {
    "text": "let's see",
    "start": "1586640",
    "end": "1589270"
  },
  {
    "text": "so what she's done is she's done some research on and the largest face",
    "start": "1593890",
    "end": "1600370"
  },
  {
    "text": "recognition or face detection software's out there and while they had",
    "start": "1600370",
    "end": "1607360"
  },
  {
    "text": "like super super good accuracy like these these have accuracies in like 99",
    "start": "1607360",
    "end": "1613990"
  },
  {
    "text": "something as far as like determining gender and recognizing your faces and",
    "start": "1613990",
    "end": "1620290"
  },
  {
    "text": "things like that when you split it a different way in this case in the ways of female male and then she also used",
    "start": "1620290",
    "end": "1629440"
  },
  {
    "text": "like a shade index like how dark you were she discovered that for example for",
    "start": "1629440",
    "end": "1635980"
  },
  {
    "text": "the IBM solution only in 65% of the cases did it attacked the gender right",
    "start": "1635980",
    "end": "1642010"
  },
  {
    "text": "for for black women since then a lot of",
    "start": "1642010",
    "end": "1647169"
  },
  {
    "text": "things have happened so she's I don't know if she is now or she used she at",
    "start": "1647169",
    "end": "1653350"
  },
  {
    "text": "least has word for for Microsoft but she's changed the industry to the point",
    "start": "1653350",
    "end": "1658600"
  },
  {
    "text": "where they're no longer targeting this label faces in the wild as far as",
    "start": "1658600",
    "end": "1663820"
  },
  {
    "text": "benchmarking but instead IBM generated a much more diverse data set to to train",
    "start": "1663820",
    "end": "1671530"
  },
  {
    "text": "and and to benchmark against so now we're actually pretty even-steven on on",
    "start": "1671530",
    "end": "1679179"
  },
  {
    "text": "all of the gender definitions here so the problem with something like this doe is that when you have a system like this",
    "start": "1679179",
    "end": "1685870"
  },
  {
    "text": "that suddenly can't differentiate between Chewbacca's because it hasn't seen enough Chewbacca's you get",
    "start": "1685870",
    "end": "1693250"
  },
  {
    "text": "something like this so this was in the UK I think last summer where this one",
    "start": "1693250",
    "end": "1700450"
  },
  {
    "text": "guy he was just walking down the street and police came out got him on the floor",
    "start": "1700450",
    "end": "1707320"
  },
  {
    "text": "like a Piquet I'm buzzed and got him on the floor and basically treating",
    "start": "1707320",
    "end": "1713080"
  },
  {
    "text": "treating him as a very hardened criminal but it turned out that what happened was",
    "start": "1713080",
    "end": "1718750"
  },
  {
    "text": "so they of CCTV camera so everywhere doing face detection but they happened",
    "start": "1718750",
    "end": "1725350"
  },
  {
    "text": "to catch the wrong guy because to face detection like he looks",
    "start": "1725350",
    "end": "1731650"
  },
  {
    "text": "a lot like many other people because face detection hasn't her face",
    "start": "1731650",
    "end": "1736960"
  },
  {
    "text": "identification hasn't learned to differentiate between them now this",
    "start": "1736960",
    "end": "1742720"
  },
  {
    "text": "brings up a very different topic off perpetual lineup that I'll get to a little bit later but for now saying",
    "start": "1742720",
    "end": "1752200"
  },
  {
    "text": "algorithms can't be racist because they're yes math is just sort of like",
    "start": "1752200",
    "end": "1759030"
  },
  {
    "text": "biased laundering like taking it to an algorithm used to say that you no longer",
    "start": "1759030",
    "end": "1765460"
  },
  {
    "text": "have bias and this is not only invasion but for example also in in text and",
    "start": "1765460",
    "end": "1772030"
  },
  {
    "text": "again this happens in systems that we all depend on that we're all using sort",
    "start": "1772030",
    "end": "1777550"
  },
  {
    "text": "of downstream in all our applications so if you go into Google and this will",
    "start": "1777550",
    "end": "1782950"
  },
  {
    "text": "happen today if you do this you type in she is a computer programmer and he is a",
    "start": "1782950",
    "end": "1789070"
  },
  {
    "text": "nurse and then you're translated to Turkish which is a gender-neutral",
    "start": "1789070",
    "end": "1794140"
  },
  {
    "text": "language which means that we don't have he's or she's in here instead we have",
    "start": "1794140",
    "end": "1799630"
  },
  {
    "text": "the O and then you translate it back it will translate it back like this and",
    "start": "1799630",
    "end": "1806730"
  },
  {
    "text": "then it might go okay so um this might",
    "start": "1806730",
    "end": "1813070"
  },
  {
    "text": "be quite natural because most of the computer engineers are male so it's like",
    "start": "1813070",
    "end": "1819370"
  },
  {
    "text": "if I went if I were to venture a guess I would guess P and most of the nurses are",
    "start": "1819370",
    "end": "1825550"
  },
  {
    "text": "female so I would venture a guess of XI so we can think that is correct but the",
    "start": "1825550",
    "end": "1832210"
  },
  {
    "text": "waned is this used downstream makes this not correct because suddenly computer",
    "start": "1832210",
    "end": "1838390"
  },
  {
    "text": "programmer is a very male thing like these have very these have these",
    "start": "1838390",
    "end": "1844390"
  },
  {
    "text": "definitions of gender or arrays or things like that and grading into them",
    "start": "1844390",
    "end": "1851440"
  },
  {
    "text": "this is how the systems work the problem is that it doesn't stop here because this I mean it's unfortunate for",
    "start": "1851440",
    "end": "1859299"
  },
  {
    "text": "me but oh we could think that this is actually how it's supposed to work but",
    "start": "1859299",
    "end": "1865119"
  },
  {
    "text": "what about this one translated through this so this happens",
    "start": "1865119",
    "end": "1871869"
  },
  {
    "text": "for all your words and everything so all the biases that we've picked up in text",
    "start": "1871869",
    "end": "1878109"
  },
  {
    "text": "are going to be coming through in everything you do",
    "start": "1878109",
    "end": "1884220"
  },
  {
    "text": "there are ways to diiie bias these word embeddings so there's an embedding is basically a description of a word it",
    "start": "1885039",
    "end": "1891940"
  },
  {
    "text": "contains a lot of information that of which one or two two of the numbers",
    "start": "1891940",
    "end": "1897519"
  },
  {
    "text": "together may mean female or male for example there are ways to diiie bias",
    "start": "1897519",
    "end": "1902769"
  },
  {
    "text": "them but when you start to do technical things like D biasing them like removing",
    "start": "1902769",
    "end": "1908109"
  },
  {
    "text": "the male or the female it's sort of hard to determine when you should stop or not",
    "start": "1908109",
    "end": "1914200"
  },
  {
    "text": "like for example will to say computer programmer maybe that should be Tobias maybe that should be gender-neutral but",
    "start": "1914200",
    "end": "1922720"
  },
  {
    "text": "what about something like colorblind should that be gender-neutral or",
    "start": "1922720",
    "end": "1927730"
  },
  {
    "text": "pregnant or are you know things that actually there is a real meaning and",
    "start": "1927730",
    "end": "1935019"
  },
  {
    "text": "having gender or or something in there so it's a very very difficult problem to",
    "start": "1935019",
    "end": "1942070"
  },
  {
    "text": "tackle even if you think you okay so I see the problem I can fix it by deep",
    "start": "1942070",
    "end": "1947320"
  },
  {
    "text": "biasing but there is question enough if you should even but if you don't do bias",
    "start": "1947320",
    "end": "1953350"
  },
  {
    "text": "and you use these systems or these word",
    "start": "1953350",
    "end": "1958629"
  },
  {
    "text": "embeddings and systems like this one this is an Amazon CV tool basically they",
    "start": "1958629",
    "end": "1967570"
  },
  {
    "text": "took I took your CV and then said she does even go to an interviewer or not",
    "start": "1967570",
    "end": "1972629"
  },
  {
    "text": "and they found out that if it was something that came from a female it was",
    "start": "1972629",
    "end": "1978759"
  },
  {
    "text": "never going to a recruiter no matter what so they did a lot of slicing and",
    "start": "1978759",
    "end": "1985179"
  },
  {
    "text": "dicing and said okay so let's remove the the name let's remove doesn't lead and",
    "start": "1985179",
    "end": "1990970"
  },
  {
    "text": "remove like obsessed female or male let's remove some school study one two because some of them kind of have",
    "start": "1990970",
    "end": "1997090"
  },
  {
    "text": "telltale signs but even when it came down to the actual wording things like",
    "start": "1997090",
    "end": "2003510"
  },
  {
    "text": "using the word executed it's a very very male thing I've executed a people or two",
    "start": "2003510",
    "end": "2010140"
  },
  {
    "text": "but I am not fitting none on my CV I do you know what I'm saying so when you use",
    "start": "2010140",
    "end": "2018120"
  },
  {
    "text": "something like this it gets even problematic in the downstream case when",
    "start": "2018120",
    "end": "2023910"
  },
  {
    "text": "you use like data from a google translator from these big systems that",
    "start": "2023910",
    "end": "2029880"
  },
  {
    "text": "have this biasing grade another problem now totally different from bias is",
    "start": "2029880",
    "end": "2036750"
  },
  {
    "text": "something like this I'm sure you've all been in this situation bought all the",
    "start": "2036750",
    "end": "2042480"
  },
  {
    "text": "laces and yeah no longer need laces this",
    "start": "2042480",
    "end": "2048600"
  },
  {
    "text": "is part of something called confirmation bias bubble so what is is basically when",
    "start": "2048600",
    "end": "2057120"
  },
  {
    "text": "you take the results of a machine learning system and then you decide to",
    "start": "2057120",
    "end": "2063120"
  },
  {
    "text": "use that as your only truth essentially if you take the result that all computer",
    "start": "2063120",
    "end": "2070980"
  },
  {
    "text": "engineers are are male and then you say we're now only gonna hire males and you",
    "start": "2070980",
    "end": "2076260"
  },
  {
    "text": "just basically perpetuate the problem by and make the problem even worse but in",
    "start": "2076260",
    "end": "2081780"
  },
  {
    "text": "this case for example looking at your Facebook feed or even your Google or",
    "start": "2081780",
    "end": "2088260"
  },
  {
    "text": "Bing search results and depending on what you click on that will now be your",
    "start": "2088260",
    "end": "2094710"
  },
  {
    "text": "preference and that will now be the only thing you see the more you like",
    "start": "2094710",
    "end": "2099990"
  },
  {
    "text": "something the less you see of the things that you don't like so we end up in",
    "start": "2099990",
    "end": "2105390"
  },
  {
    "text": "these bubbles of the sort of like feeling that everyone is like us and all",
    "start": "2105390",
    "end": "2110730"
  },
  {
    "text": "everything we search for will always be very tailored to us so when we get into",
    "start": "2110730",
    "end": "2116970"
  },
  {
    "text": "the real world and realize that someone actually thinks just a little bit there and you go like I had no clue that there",
    "start": "2116970",
    "end": "2126270"
  },
  {
    "text": "was even like in now people that thought is totally different way because you've",
    "start": "2126270",
    "end": "2132600"
  },
  {
    "text": "never been exposed to them or similarly for example on Spotify you end up he's",
    "start": "2132600",
    "end": "2139170"
  },
  {
    "text": "listening to the same music over and over so the question is what can you do",
    "start": "2139170",
    "end": "2144420"
  },
  {
    "text": "to break a bias bubble like that what can you do to break the fact that you",
    "start": "2144420",
    "end": "2150270"
  },
  {
    "text": "actually need some outside intervention so you don't end up being used more and",
    "start": "2150270",
    "end": "2157320"
  },
  {
    "text": "more and more in the same of yourself so in the case for example in this case and",
    "start": "2157320",
    "end": "2163230"
  },
  {
    "text": "actually Bing and Google both do this so typically if you search for CEO and this",
    "start": "2163230",
    "end": "2169050"
  },
  {
    "text": "is the the results you're gonna get and then normally like if you search for",
    "start": "2169050",
    "end": "2176040"
  },
  {
    "text": "something most people end up picking a result from one of the first two or",
    "start": "2176040",
    "end": "2181710"
  },
  {
    "text": "three maybe four items and using that s as a result which means that if you now",
    "start": "2181710",
    "end": "2188010"
  },
  {
    "text": "go in and click on any of the people on the top three rows you'll eventually end up with a system that is only ever gonna",
    "start": "2188010",
    "end": "2196290"
  },
  {
    "text": "show in this case male CEOs but in order to prevent that both Google and",
    "start": "2196290",
    "end": "2204030"
  },
  {
    "text": "Microsoft and other search engines inject female CEOs like the top female",
    "start": "2204030",
    "end": "2212310"
  },
  {
    "text": "CEOs into this to at least leave the status quo of what the world actually",
    "start": "2212310",
    "end": "2218490"
  },
  {
    "text": "looks like as compared to what it would look like if we only had a majority option all the time now is that a good",
    "start": "2218490",
    "end": "2227760"
  },
  {
    "text": "or a bad thing it's a political decision it's not necessarily a technical",
    "start": "2227760",
    "end": "2233450"
  },
  {
    "text": "yes-or-no answer it's always going to be like any solution to something like this it's always going to be a political",
    "start": "2233450",
    "end": "2239820"
  },
  {
    "text": "decision another type of confirmation bias bubble",
    "start": "2239820",
    "end": "2244950"
  },
  {
    "text": "is let's say you have a very high crime area this is a place that's known for",
    "start": "2244950",
    "end": "2253470"
  },
  {
    "text": "having a lot of crimes so you send a lot of police there because your machine learning algorithms",
    "start": "2253470",
    "end": "2259720"
  },
  {
    "text": "tell you that this is where you should be sending all your people now they go through and report that yeah there was",
    "start": "2259720",
    "end": "2268030"
  },
  {
    "text": "crime there and suddenly you have even more crime reported from this one single",
    "start": "2268030",
    "end": "2274600"
  },
  {
    "text": "location which causes you to send even more people there and more crime and",
    "start": "2274600",
    "end": "2281770"
  },
  {
    "text": "reported to send more police do more arrests and suddenly end up in this",
    "start": "2281770",
    "end": "2287590"
  },
  {
    "text": "runaway feedback loop so you need to sort of like be careful when you use a",
    "start": "2287590",
    "end": "2293890"
  },
  {
    "text": "machine learning system for anything that you don't end up in these situations where your only shoes sort of",
    "start": "2293890",
    "end": "2300310"
  },
  {
    "text": "like the talk suggestions and never interject it with something else and end",
    "start": "2300310",
    "end": "2305530"
  },
  {
    "text": "up with these bias bubbles so to make",
    "start": "2305530",
    "end": "2313240"
  },
  {
    "text": "errors that's human we all have biases the reason why these systems have biases is",
    "start": "2313240",
    "end": "2319869"
  },
  {
    "text": "not because someone was evil and entered they stand in the other that we didn't want",
    "start": "2319869",
    "end": "2325119"
  },
  {
    "text": "it's literally because we've had these biases all along only one where humans",
    "start": "2325119",
    "end": "2331890"
  },
  {
    "text": "actually I'm but it sort of biases human but to to put it into an AI system we do",
    "start": "2331890",
    "end": "2340270"
  },
  {
    "text": "it at scale and we propagate it and with cost ease I runaway feedback loops because when we buy us as humans we at",
    "start": "2340270",
    "end": "2346990"
  },
  {
    "text": "least have like this Shack this constant Shack that allows us to say maybe we",
    "start": "2346990",
    "end": "2353440"
  },
  {
    "text": "should be thinking a little bit different but algorithms don't have that common sense of what we should be doing",
    "start": "2353440",
    "end": "2359710"
  },
  {
    "text": "to sort of fix the situation so buyers",
    "start": "2359710",
    "end": "2366430"
  },
  {
    "text": "and machine learning is in an np-hard problem there are so many variables do",
    "start": "2366430",
    "end": "2372880"
  },
  {
    "text": "this a lot of what we've been looking at here is female versus male because it's",
    "start": "2372880",
    "end": "2378160"
  },
  {
    "text": "very very hard to accept if I would female versus male but and in that case",
    "start": "2378160",
    "end": "2384310"
  },
  {
    "text": "you can go through and say okay so let me do the job you owe me anything let me make sure that it works the same",
    "start": "2384310",
    "end": "2391030"
  },
  {
    "text": "way for males and females but what do you do for something like ethnicity is",
    "start": "2391030",
    "end": "2396940"
  },
  {
    "text": "it's like Caucasians against all other ethnicities is it you know in that case",
    "start": "2396940",
    "end": "2404620"
  },
  {
    "text": "you have a lot of different and types and it's a very very fluid situation and",
    "start": "2404620",
    "end": "2410980"
  },
  {
    "text": "then even if you fix that what do you do for like sexual orientation political",
    "start": "2410980",
    "end": "2416830"
  },
  {
    "text": "orientation religion like where does it stop does it stop where you think maybe",
    "start": "2416830",
    "end": "2423360"
  },
  {
    "text": "what should we do for people that are I live in like rural areas and urban areas",
    "start": "2423360",
    "end": "2429190"
  },
  {
    "text": "because maybe they get treated differently you know there is always like in any situation that you can come",
    "start": "2429190",
    "end": "2435310"
  },
  {
    "text": "up with there's gonna be a minority that there's gonna be someone who's not very",
    "start": "2435310",
    "end": "2441610"
  },
  {
    "text": "very represented by the system and in some cases like with a face recognition where it's like male versus female it's",
    "start": "2441610",
    "end": "2448480"
  },
  {
    "text": "not even a minority it just happens to be minority in your data set and let's",
    "start": "2448480",
    "end": "2453520"
  },
  {
    "text": "say we figure out that we have bias then what do we do so we take like a loan",
    "start": "2453520",
    "end": "2459340"
  },
  {
    "text": "system and so we have a system that they're supposed to tell you if you",
    "start": "2459340",
    "end": "2465130"
  },
  {
    "text": "should get a loan or not and then you say okay so let's say I have thirty",
    "start": "2465130",
    "end": "2473260"
  },
  {
    "text": "percent females and seventy percent males that apply what should the outcome",
    "start": "2473260",
    "end": "2479080"
  },
  {
    "text": "be should the outcome be that we have",
    "start": "2479080",
    "end": "2484200"
  },
  {
    "text": "will remove the general we say let's not look at whether or not they're females",
    "start": "2484200",
    "end": "2489610"
  },
  {
    "text": "or males similar cysts do our thing but then you discover that maybe females",
    "start": "2489610",
    "end": "2495880"
  },
  {
    "text": "still get less loans because they have some other issues like they have less",
    "start": "2495880",
    "end": "2502210"
  },
  {
    "text": "income they and they've generally had a job history where during some years of",
    "start": "2502210",
    "end": "2509800"
  },
  {
    "text": "this job history they didn't have a job because they were home with the kids or whatever so then you might think well",
    "start": "2509800",
    "end": "2515890"
  },
  {
    "text": "what if we do like equal outcomes so if we have 70/30 the results",
    "start": "2515890",
    "end": "2522340"
  },
  {
    "text": "it also be 70/30 unfortunately that's also not a very very good proposition",
    "start": "2522340",
    "end": "2528460"
  },
  {
    "text": "because what if all these 30 women had like no money at all and all these 70",
    "start": "2528460",
    "end": "2536470"
  },
  {
    "text": "percent men had a lot of money so they should clearly they're clearly more",
    "start": "2536470",
    "end": "2542500"
  },
  {
    "text": "eligible for loan so we don't want to sort of like take some people that",
    "start": "2542500",
    "end": "2548380"
  },
  {
    "text": "clearly shouldn't get it alone from this area and and give them a loan or whereas",
    "start": "2548380",
    "end": "2554860"
  },
  {
    "text": "some people that should be getting along from this area not give them a loan because they weren't filling up the",
    "start": "2554860",
    "end": "2560050"
  },
  {
    "text": "quota so then there is a question of like should we give them equal opportunity so they should be treated",
    "start": "2560050",
    "end": "2568120"
  },
  {
    "text": "equally based on if they have all the sort of prerequisites to get a loan or",
    "start": "2568120",
    "end": "2575080"
  },
  {
    "text": "not in deciding where on this scale you fall it's again a very very political",
    "start": "2575080",
    "end": "2582190"
  },
  {
    "text": "decision it's not a technical decision but it's a decision that somehow we as software engineers are suddenly in",
    "start": "2582190",
    "end": "2589180"
  },
  {
    "text": "charge of right so if it's this",
    "start": "2589180",
    "end": "2594610"
  },
  {
    "text": "difficult what do we do like do we just give up because um she seems too hard",
    "start": "2594610",
    "end": "2602550"
  },
  {
    "text": "well I I wouldn't stand here if I thought it would just give up so instead",
    "start": "2602550",
    "end": "2610270"
  },
  {
    "text": "I'm suggesting that we look at it from a little bit different angle and instead",
    "start": "2610270",
    "end": "2616540"
  },
  {
    "text": "of giving up a look at what we can do to sort of mitigate the problems so at Microsoft we have these ethics boards to",
    "start": "2616540",
    "end": "2624280"
  },
  {
    "text": "go through all our projects and figure out basically what could possibly go wrong what's the headline that we don't",
    "start": "2624280",
    "end": "2632200"
  },
  {
    "text": "want to have because of this like what is the negative thing is that going to happen and how can we make sure that",
    "start": "2632200",
    "end": "2638530"
  },
  {
    "text": "those things don't happen so a couple of the things that would go through are",
    "start": "2638530",
    "end": "2644290"
  },
  {
    "text": "looking and making sure that they are fair and inclusive so this means that testing for the people that's supposed",
    "start": "2644290",
    "end": "2652270"
  },
  {
    "text": "to be in our population as best we can and making sure that things act the way we want like whatever",
    "start": "2652270",
    "end": "2661040"
  },
  {
    "text": "decision we take on how we want the outcome to be that the outcome is actually like that that is transparent",
    "start": "2661040",
    "end": "2668630"
  },
  {
    "text": "so this means that everyone in the process from the software engineer to the product owner to the person who's",
    "start": "2668630",
    "end": "2675170"
  },
  {
    "text": "using this the person collecting the data the person it affects like the actual decisions know how the decisions",
    "start": "2675170",
    "end": "2682760"
  },
  {
    "text": "are made so basically know what data is used for it and can control and test things",
    "start": "2682760",
    "end": "2689510"
  },
  {
    "text": "sometimes it's not very easy to figure out exactly how machine learning got to the decision but at least if you have",
    "start": "2689510",
    "end": "2695839"
  },
  {
    "text": "the data you can go back and make sure that the data itself and the decision",
    "start": "2695839",
    "end": "2701330"
  },
  {
    "text": "seem reasonable and also and be accountable so someone needs to be",
    "start": "2701330",
    "end": "2708230"
  },
  {
    "text": "accountable it's not okay to say that the computer said so instead if",
    "start": "2708230",
    "end": "2714080"
  },
  {
    "text": "something happened because something is gonna happen like bad things are gonna happen it will always happen and then",
    "start": "2714080",
    "end": "2720470"
  },
  {
    "text": "the question is how do we deal with it so having someone accountable or and",
    "start": "2720470",
    "end": "2728300"
  },
  {
    "text": "making sure it's safe and reliable so that we don't run into a runaway",
    "start": "2728300",
    "end": "2733339"
  },
  {
    "text": "feedback loop instead we have safeguards to make sure that we stay within certain parameters but we don't have liked a the",
    "start": "2733339",
    "end": "2743089"
  },
  {
    "text": "Twitter bot that suddenly went racist like we have to have safeguards to make sure or things like that doesn't happen",
    "start": "2743089",
    "end": "2749150"
  },
  {
    "text": "and then also of course ethically look at security and privacy to make sure",
    "start": "2749150",
    "end": "2755450"
  },
  {
    "text": "that the data that you're giving this algorithm it's not spread so a typical",
    "start": "2755450",
    "end": "2762980"
  },
  {
    "text": "example of this apart from like the obvious security and privacy thing is things like if you have something that",
    "start": "2762980",
    "end": "2770530"
  },
  {
    "text": "autocorrect and then suddenly pops up with words that you may use a lot but",
    "start": "2770530",
    "end": "2777380"
  },
  {
    "text": "that you don't want other people to know that you're using a lot that might not be very nice if they show up in your",
    "start": "2777380",
    "end": "2783920"
  },
  {
    "text": "autocorrect when you're in public places for example so in general thinking about how how",
    "start": "2783920",
    "end": "2794960"
  },
  {
    "text": "your software may be used downstream or may be used in general to make sure that",
    "start": "2794960",
    "end": "2802220"
  },
  {
    "text": "you account for like the worst possible situation that could happen with this and when it comes to face recognition",
    "start": "2802220",
    "end": "2809810"
  },
  {
    "text": "for example we have at Microsoft we have all these sort of like we've determined",
    "start": "2809810",
    "end": "2818360"
  },
  {
    "text": "a set of rules for when you can do this specifically to avoid for it to be used",
    "start": "2818360",
    "end": "2823490"
  },
  {
    "text": "in in places like predictive policing like the case we saw it's a while back",
    "start": "2823490",
    "end": "2831340"
  },
  {
    "text": "because the threat of sort of like a perpetual lineup of having CCTV cameras",
    "start": "2831340",
    "end": "2838430"
  },
  {
    "text": "where you're always on a lineup and you're always watched and everyone can",
    "start": "2838430",
    "end": "2843470"
  },
  {
    "text": "always see what you do is a very very real threat it's already happening not only in the UK but in the US and in a",
    "start": "2843470",
    "end": "2850880"
  },
  {
    "text": "lot of cases and we need to sort of um if you're building big systems that can",
    "start": "2850880",
    "end": "2856430"
  },
  {
    "text": "be used in these cases you need to put safeguards around where they can use",
    "start": "2856430",
    "end": "2861460"
  },
  {
    "text": "another thing that you can do is create data sheets for data set so what this",
    "start": "2861460",
    "end": "2868640"
  },
  {
    "text": "means is for example for the label faces in a while that was never meant to be a",
    "start": "2868640",
    "end": "2873860"
  },
  {
    "text": "benchmark for these kind of systems in fact if you would have looked through it",
    "start": "2873860",
    "end": "2879440"
  },
  {
    "text": "and found out how it was gathered and a couple of things like sort of what the",
    "start": "2879440",
    "end": "2884960"
  },
  {
    "text": "parameters look like for this data set what they like how many male and how",
    "start": "2884960",
    "end": "2891020"
  },
  {
    "text": "many female and that kind of stuff it would have never been used as a benchmark for this so understanding how",
    "start": "2891020",
    "end": "2896630"
  },
  {
    "text": "the data is scattered registering that and also setting sort of similarly to saying data sheets for for electrical",
    "start": "2896630",
    "end": "2904130"
  },
  {
    "text": "circuits determining under what circumstances someone can actually use this data or at least describing it to",
    "start": "2904130",
    "end": "2910940"
  },
  {
    "text": "make sure it's not used a wrong way there's also some very nice tools to use",
    "start": "2910940",
    "end": "2916820"
  },
  {
    "text": "for like fairness and an inclusion checks so this is from the goo",
    "start": "2916820",
    "end": "2922130"
  },
  {
    "text": "what if tool where you can label your data for example that these are people",
    "start": "2922130",
    "end": "2930079"
  },
  {
    "text": "with these ethnicities these are people with these whatever it might be and then",
    "start": "2930079",
    "end": "2937099"
  },
  {
    "text": "you can test your algorithms and your models independently of how they were generated you can test them and see how",
    "start": "2937099",
    "end": "2944180"
  },
  {
    "text": "they fare in different scenarios how they fare for different groups of of your population and then for their",
    "start": "2944180",
    "end": "2952369"
  },
  {
    "text": "transparency there is also and this is something that's highly on the rise and",
    "start": "2952369",
    "end": "2958720"
  },
  {
    "text": "different systems or different libraries to understand how networks come to the",
    "start": "2958720",
    "end": "2964970"
  },
  {
    "text": "conclusions that they come to so this is from a Lyme database where the canonical",
    "start": "2964970",
    "end": "2970910"
  },
  {
    "text": "test is you have a set of Doc's and your set of wolves and it came up with a model that was like super accurate like",
    "start": "2970910",
    "end": "2979220"
  },
  {
    "text": "it could almost in every case figure out whether or not it was a dog or a wolf but then when you take it through Lyme",
    "start": "2979220",
    "end": "2985759"
  },
  {
    "text": "and you can find out that this was what it made a decision based on yeah and",
    "start": "2985759",
    "end": "2993859"
  },
  {
    "text": "then you think like okay so maybe while it was accurate for this particular test set detecting snow is not exactly what",
    "start": "2993859",
    "end": "3002650"
  },
  {
    "text": "we want our model to do so making sure to run it through these these",
    "start": "3002650",
    "end": "3009130"
  },
  {
    "text": "interpreters and things like that to determine how it actually came to the",
    "start": "3009130",
    "end": "3014170"
  },
  {
    "text": "decisions to make sure that it states would would common sense and then um",
    "start": "3014170",
    "end": "3020369"
  },
  {
    "text": "when we look at predictive policing in which we shouldn't ever be doing",
    "start": "3020369",
    "end": "3025809"
  },
  {
    "text": "predictive policing instead looking at detecting versus predicting so this is",
    "start": "3025809",
    "end": "3032549"
  },
  {
    "text": "from a case that I was working recently where we look at cameras from stores and",
    "start": "3032549",
    "end": "3040569"
  },
  {
    "text": "try to figure out if someone is shoplifting or not so when we look at the shoplifting cases we notice that",
    "start": "3040569",
    "end": "3049240"
  },
  {
    "text": "they were covering their faces they were usually alone men 20 to 40 wearing",
    "start": "3049240",
    "end": "3055480"
  },
  {
    "text": "hoodies and using like certain poses for shoplifting and if we would have used",
    "start": "3055480",
    "end": "3064239"
  },
  {
    "text": "built an end-to-end model on this trying to say is this a shoplifter or not we",
    "start": "3064239",
    "end": "3071680"
  },
  {
    "text": "would have very likely detected everyone who was a man 22:40 wearing a hoodie",
    "start": "3071680",
    "end": "3077319"
  },
  {
    "text": "covering their face is a shoplifter obviously not not really correct so when",
    "start": "3077319",
    "end": "3087779"
  },
  {
    "text": "so actually you can't use even the even",
    "start": "3087779",
    "end": "3093369"
  },
  {
    "text": "the cover two faces are alone as a predictor point being that you should never predict that someone is going to",
    "start": "3093369",
    "end": "3101019"
  },
  {
    "text": "shoplift because that's always going to give you a lot of false positives and it's not",
    "start": "3101019",
    "end": "3106869"
  },
  {
    "text": "really even indicative of the actual action of shoplifting instead you need to be careful and look at predicting",
    "start": "3106869",
    "end": "3113529"
  },
  {
    "text": "instead of so looking at for example the the poses because that's a real",
    "start": "3113529",
    "end": "3118749"
  },
  {
    "text": "indicator of the action that they took and this is a system in China that's in",
    "start": "3118749",
    "end": "3128349"
  },
  {
    "text": "use were they use facial recognition to to determine if someone was yeh walking",
    "start": "3128349",
    "end": "3133690"
  },
  {
    "text": "and then automatically text him affine which yeah but it didn't really work out",
    "start": "3133690",
    "end": "3142449"
  },
  {
    "text": "the way they wanted to because this lady got a whole lot of fine it's like I",
    "start": "3142449",
    "end": "3148420"
  },
  {
    "text": "think she can even use her phone but um so obviously whenever she the bus passed",
    "start": "3148420",
    "end": "3154180"
  },
  {
    "text": "but you got a fine so there will be things that go wrong no matter what",
    "start": "3154180",
    "end": "3160329"
  },
  {
    "text": "safeguards you put in place there will always always always be things that will",
    "start": "3160329",
    "end": "3165999"
  },
  {
    "text": "go wrong and the question is for you is how do you safeguard against that and",
    "start": "3165999",
    "end": "3171430"
  },
  {
    "text": "and deal with the errors and one very common thing to do is put a human in the",
    "start": "3171430",
    "end": "3178930"
  },
  {
    "text": "loop and this is something that's preferable in most machine learning cases until you're very very secure that",
    "start": "3178930",
    "end": "3185259"
  },
  {
    "text": "it works the way it should so if you're gonna be detecting and that",
    "start": "3185259",
    "end": "3190890"
  },
  {
    "text": "someone is shoplifting don't actually like I don't know sound a bar when they",
    "start": "3190890",
    "end": "3196410"
  },
  {
    "text": "when they leave but to have someone that can actually look at the footage and determine if that's what actually",
    "start": "3196410",
    "end": "3203220"
  },
  {
    "text": "happened and then tap them on the shoulder because if you go through and",
    "start": "3203220",
    "end": "3209280"
  },
  {
    "text": "tap them on the shoulder without without any human intervention then you'll end",
    "start": "3209280",
    "end": "3215430"
  },
  {
    "text": "up accusing someone and even the accusation is is quite a negative thing",
    "start": "3215430",
    "end": "3221010"
  },
  {
    "text": "like taking this guy in the UK and I'm putting him on the floor and treating",
    "start": "3221010",
    "end": "3226559"
  },
  {
    "text": "him as a criminal it's a very very detrimental thing for a person to have",
    "start": "3226559",
    "end": "3231930"
  },
  {
    "text": "to live through but all in all when all",
    "start": "3231930",
    "end": "3236940"
  },
  {
    "text": "is said and done the last thing I would encourage today's speak up when you see",
    "start": "3236940",
    "end": "3243150"
  },
  {
    "text": "something that doesn't feel right that doesn't feel like this should be what",
    "start": "3243150",
    "end": "3248490"
  },
  {
    "text": "we're doing do speak up because we're not just engineers we are the guardians",
    "start": "3248490",
    "end": "3255569"
  },
  {
    "text": "of our future thank you [Applause]",
    "start": "3255569",
    "end": "3264579"
  }
]