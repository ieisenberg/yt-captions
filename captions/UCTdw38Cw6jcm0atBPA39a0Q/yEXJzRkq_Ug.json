[
  {
    "text": "all right I think we can start thank you thank you very much for coming I'm David",
    "start": "1579",
    "end": "7220"
  },
  {
    "text": "I'm a Solutions Architect at Couchbase cadres develop say no sequel database kind of like MongoDB which probably",
    "start": "7220",
    "end": "14090"
  },
  {
    "text": "you've heard of but not really much better obviously the site what I'm actually going to talk about today has",
    "start": "14090",
    "end": "19579"
  },
  {
    "text": "nothing to do with databases we're going to be talking about processing streaming data a lot of it I'd like to point out",
    "start": "19579",
    "end": "25519"
  },
  {
    "text": "there's only one person brave enough to sit in the front row and I salute you for that and the rest of you obviously",
    "start": "25519",
    "end": "31369"
  },
  {
    "text": "should feel shame alright so we've before we start right I like to do a little a little quiz to see who we have",
    "start": "31369",
    "end": "38719"
  },
  {
    "text": "in the room so how many people here write code for a living everybody who doesn't who does something else like",
    "start": "38719",
    "end": "45170"
  },
  {
    "text": "managing or architecture stuff nobody is willing to admit to managing all right",
    "start": "45170",
    "end": "50269"
  },
  {
    "text": "good so who writes code in dotnet it's like again everybody all right how about",
    "start": "50269",
    "end": "55370"
  },
  {
    "text": "Java it's like five people all right oh this is going to beat our crowd because",
    "start": "55370",
    "end": "60739"
  },
  {
    "text": "I do have a lot of job of examples and I don't say I didn't warn you how about JavaScript it's like half the",
    "start": "60739",
    "end": "67250"
  },
  {
    "text": "room I don't know what you guys are doing here honestly but all right okay",
    "start": "67250",
    "end": "72880"
  },
  {
    "text": "anything we're done this is Eric like PHP Python Erlang alright cool so anyway",
    "start": "72880",
    "end": "80840"
  },
  {
    "text": "we're going to be talking about streaming data and processing it in real time and getting interesting stuff out",
    "start": "80840",
    "end": "85910"
  },
  {
    "text": "of it but obviously the first thing is why do we even need this stuff right now we can always write an application which",
    "start": "85910",
    "end": "92270"
  },
  {
    "text": "you know listens to a queue does something and puts it in a database and this is basically like 95% of",
    "start": "92270",
    "end": "97550"
  },
  {
    "text": "applications in the world that's basically what they do they listen for incoming messages do something with them",
    "start": "97550",
    "end": "102560"
  },
  {
    "text": "and put them in a database so when I say streaming data and processing data first of all what I mean is at large scale and",
    "start": "102560",
    "end": "108860"
  },
  {
    "text": "when I say large scale let's say who knows how many twists happen every",
    "start": "108860",
    "end": "115580"
  },
  {
    "text": "second approximately let's look hundreds who thinks it's a hundred nobody thinks",
    "start": "115580",
    "end": "121040"
  },
  {
    "text": "it's hundred a thousand about ten thousands per seconds mind you first",
    "start": "121040",
    "end": "126950"
  },
  {
    "text": "thing all right twenty thousand a hundred thousand tweets per second",
    "start": "126950",
    "end": "133660"
  },
  {
    "text": "what no it's more like 6000 per second an average which is like half a billion a day which is a lot of data and if you",
    "start": "133660",
    "end": "140800"
  },
  {
    "text": "consider that every tweet comes with it's not just 140 characters right it's actually a lot more that's metadata and all that it's a lot of data to process",
    "start": "140800",
    "end": "148030"
  },
  {
    "text": "in a single day and even in a second what which aren't actually a lot of data",
    "start": "148030",
    "end": "153600"
  },
  {
    "text": "how many Google searches happen every second 10,000 100,000 a million Google",
    "start": "153600",
    "end": "162520"
  },
  {
    "text": "searches every second like no one's willing to write so yeah I confuse everybody through this it's more like",
    "start": "162520",
    "end": "168310"
  },
  {
    "text": "50,000 per second Google searches a lot let's go higher YouTube video views",
    "start": "168310",
    "end": "173740"
  },
  {
    "text": "having a youtube video views 10,000 no",
    "start": "173740",
    "end": "178750"
  },
  {
    "text": "one's willing to actually play this game anymore all right I'll just tell you so it's around a hundred thousand YouTube",
    "start": "178750",
    "end": "184390"
  },
  {
    "text": "views per second and when I say views you have to think about what actually happens when a user goes and does a",
    "start": "184390",
    "end": "190690"
  },
  {
    "text": "Google search or does a YouTube video view or goes to a website that you guys built and assuming your do some kind of",
    "start": "190690",
    "end": "196300"
  },
  {
    "text": "metric collection or user tracking and a bunch of stuff every visit actually is a lot of operations in the background",
    "start": "196300",
    "end": "202120"
  },
  {
    "text": "right it's going to generate a bunch of user interactions with the website which you probably need to record somewhere and eventually probably process any of",
    "start": "202120",
    "end": "208750"
  },
  {
    "text": "show ads to the user because that's what the internet is basically about and every one of those causes a whole chain",
    "start": "208750",
    "end": "216430"
  },
  {
    "text": "reaction of different things that you have to process so a single tweet is one event on the front end but it's hundreds",
    "start": "216430",
    "end": "223450"
  },
  {
    "text": "of events on the back end let's change the process you need to find all the users who follow that person and updates their feeds and you need to do some",
    "start": "223450",
    "end": "230320"
  },
  {
    "text": "processing to discover trends and you know separate out to the hashtags and there's a bunch of stuff and by the way",
    "start": "230320",
    "end": "235989"
  },
  {
    "text": "I'm going to keep going back to twist because it's just very convenient to talk about tweets in a streaming context but you can just mentally map this to",
    "start": "235989",
    "end": "242470"
  },
  {
    "text": "pretty much any domain you work in whether it's you know doing for analysis or online shopping or whatever you can",
    "start": "242470",
    "end": "248860"
  },
  {
    "text": "pretty much think of it in terms of events coming in from some kind of edge whether it's a device or a website going",
    "start": "248860",
    "end": "254830"
  },
  {
    "text": "into your back-end server system doing some kind of processing with them and the reason I'm talking about streaming",
    "start": "254830",
    "end": "260079"
  },
  {
    "text": "data is because not all data is created equal right data loses value very",
    "start": "260080",
    "end": "265180"
  },
  {
    "text": "quickly right if you're if you're going to be monitoring again for example tweets if you have a startup which does social",
    "start": "265180",
    "end": "271000"
  },
  {
    "text": "network monitoring this like 100 starters to do this right and they create alerts and possibly you know change bids on ads or do kind of some",
    "start": "271000",
    "end": "278349"
  },
  {
    "text": "kind of stuff in response to social media events the difference between doing is a second later or a now later",
    "start": "278349",
    "end": "284020"
  },
  {
    "text": "it is huge right so data actually can i tape result and loses value very quickly which is why we actually want to process",
    "start": "284020",
    "end": "290080"
  },
  {
    "text": "data in real time for a lot of applications and again doing this at scale is very difficult and there's a",
    "start": "290080",
    "end": "295750"
  },
  {
    "text": "lot of different tools which I'm going to be talking about throughout this talk so in general stream processing can be",
    "start": "295750",
    "end": "301690"
  },
  {
    "text": "divided into a couple of things and it's not new right stream processing actually has been around for a couple of decades",
    "start": "301690",
    "end": "307120"
  },
  {
    "text": "at least it originated as complex event processing mostly in the stock market",
    "start": "307120",
    "end": "312310"
  },
  {
    "text": "where there was a lot of need for analyzing trends and aggregating different data merging different stock",
    "start": "312310",
    "end": "318009"
  },
  {
    "text": "feeds and trying to predict you know what to do with stocks and that kind of complex events processing was very",
    "start": "318009",
    "end": "324190"
  },
  {
    "text": "expensive and very processor and time consuming but modern applications don't",
    "start": "324190",
    "end": "330069"
  },
  {
    "text": "really need that kind of complexity so we have a whole separate domain which is simple stream processing where you have",
    "start": "330069",
    "end": "335830"
  },
  {
    "text": "events coming in you have to do something very simple with them it may be update a database profile or maybe",
    "start": "335830",
    "end": "341009"
  },
  {
    "text": "run some kind of process in the background as the response to that event and then you forget about it or you",
    "start": "341009",
    "end": "346240"
  },
  {
    "text": "write it to some permanent storage so these two worlds they're kind of close together but they're actually separate",
    "start": "346240",
    "end": "351430"
  },
  {
    "text": "I'm mostly going to be talking about stream simple stream processing and we'll talk a little bit about complex event processing a bit later because the",
    "start": "351430",
    "end": "358210"
  },
  {
    "text": "tools used there are different and they're kind of outdated so it's not very interesting for this talk so let's talk about the types of",
    "start": "358210",
    "end": "364479"
  },
  {
    "text": "data processing you can do right and when we talk about data processing we can easily talk about the throughput of",
    "start": "364479",
    "end": "369940"
  },
  {
    "text": "how many things you can actually process per second versus how quickly you can process them right and if you think",
    "start": "369940",
    "end": "375909"
  },
  {
    "text": "about something like a database right databases we expect them to process hundreds maybe thousands of events per",
    "start": "375909",
    "end": "382030"
  },
  {
    "text": "second square ease and transactions and we expect latencies in the second or ten second range if you want to do something",
    "start": "382030",
    "end": "388930"
  },
  {
    "text": "much more high scale you know you go to no sequel databases or in-memory databases where you can processing on",
    "start": "388930",
    "end": "395259"
  },
  {
    "text": "hundreds of thousands of events at millisecond latencies and the higher are going to memory you can process things",
    "start": "395259",
    "end": "400750"
  },
  {
    "text": "quicker and it becomes much more expensive and much more volatile because keeping things in memory is not as durable as",
    "start": "400750",
    "end": "407860"
  },
  {
    "text": "keeping them on disk if you go in the other direction you talked about things like Hadoop and spark how many people",
    "start": "407860",
    "end": "413560"
  },
  {
    "text": "have actually heard of a Dupin spark pretty much everyone how many people have use that in their company does not",
    "start": "413560",
    "end": "419320"
  },
  {
    "text": "necessarily yourselves like know you've heard of it but no one except one person",
    "start": "419320",
    "end": "424390"
  },
  {
    "text": "is using it you have no Hadoop or spark or any kind of data processing in your organization at all",
    "start": "424390",
    "end": "430410"
  },
  {
    "text": "excellent you'll learn a lot of new stuff today all right it's actually good",
    "start": "430410",
    "end": "436390"
  },
  {
    "text": "so the kind of large-scale batch processing which we expect to take minutes or hours and but then we expect",
    "start": "436390",
    "end": "444100"
  },
  {
    "text": "a very large volume of processing right if you have to process a petabyte of events or a petabyte of data it will",
    "start": "444100",
    "end": "450040"
  },
  {
    "text": "take a long time but that's what we have batch processing for and real-time processing is on the absolute scale of",
    "start": "450040",
    "end": "455320"
  },
  {
    "text": "that right we want to process events as quickly as possible and obviously at the high scale as possible which kind of",
    "start": "455320",
    "end": "461950"
  },
  {
    "text": "conflicts with each other right so higher throughput means we will take longer to process every event so let's",
    "start": "461950",
    "end": "467650"
  },
  {
    "text": "talk about what we can use for that right and there are a lot of open source frameworks which can do all of this",
    "start": "467650",
    "end": "473820"
  },
  {
    "text": "hopefully you've heard at least the names of some of them spark has anyone heard of sparked our precious Park",
    "start": "473820",
    "end": "478890"
  },
  {
    "text": "District has the room look we're so good and there's a bunch of others there's a patchy a storm all of these projects",
    "start": "478890",
    "end": "484990"
  },
  {
    "text": "have the names starting with Apache something right there are all open-source projects all of them JVM based we'll get to Microsoft stuff in",
    "start": "484990",
    "end": "490750"
  },
  {
    "text": "just a moment there's flink and Kafka Casca has I'm sure everyone's heard of Kafka right",
    "start": "490750",
    "end": "496540"
  },
  {
    "text": "it's a message queue okay don't have to raise your hands it's fine all right so",
    "start": "496540",
    "end": "502480"
  },
  {
    "text": "a storm and each one of those actually does something somewhat different and so it's interesting to think about not just",
    "start": "502480",
    "end": "507520"
  },
  {
    "text": "about frameworks because the frameworks themselves you know there's new frameworks coming out every year and all frameworks die off but the concepts each",
    "start": "507520",
    "end": "513880"
  },
  {
    "text": "one implements are very interesting even if you're going to go and roll your own which I don't recommend but if you're",
    "start": "513880",
    "end": "519669"
  },
  {
    "text": "going to roll your own framework it's important to understand what it is you're going to be processing and what the trade-offs are when you're trying to",
    "start": "519669",
    "end": "525430"
  },
  {
    "text": "do real-time data processing on streams of data right so storm is one of the",
    "start": "525430",
    "end": "531130"
  },
  {
    "text": "oldest ones it's been developed at Dinn sorry a Twitter actually summers",
    "start": "531130",
    "end": "537250"
  },
  {
    "text": "been developed at Twitter like five or six years ago and it's been the underlying framework for all of their",
    "start": "537250",
    "end": "543910"
  },
  {
    "text": "real-time processing so every time you go and you do something with a tweet and all the processing in the background happens on a storm topology which does a",
    "start": "543910",
    "end": "551620"
  },
  {
    "text": "lot of processing and they recently came out with storm 2.0 which they're which",
    "start": "551620",
    "end": "556810"
  },
  {
    "text": "is a new project called Heron which is basically almost the same thing but just better implemented now spark spark is",
    "start": "556810",
    "end": "562870"
  },
  {
    "text": "the process that evolves to replace Hadoop because a doufu is really very slow in disk bound and spark does a lot",
    "start": "562870",
    "end": "568240"
  },
  {
    "text": "of the stuff in memory and in our case we're actually going to be more focused on spark streaming which is an extension to spark to the distributed processing",
    "start": "568240",
    "end": "575170"
  },
  {
    "text": "engine which can do this in memory in a streaming fashion so it will continuously process new data it comes",
    "start": "575170",
    "end": "580510"
  },
  {
    "text": "in and spark is very popular it's a huge project lots of contributors and it's",
    "start": "580510",
    "end": "585970"
  },
  {
    "text": "one of the really mainstream very basic solid tools to use and there's the two",
    "start": "585970",
    "end": "591580"
  },
  {
    "text": "on the bottom are relatively new both flink and Casca Cafe is a message queue",
    "start": "591580",
    "end": "596590"
  },
  {
    "text": "it's very solid and everyone a lot of companies use Casca but not everyone uses it for processing messages which is",
    "start": "596590",
    "end": "602470"
  },
  {
    "text": "autumn what I'm going to be talking about today so if we go over to Microsoft there is a very cool tool from",
    "start": "602470",
    "end": "607840"
  },
  {
    "text": "axis research called Microsoft Orleans as anyone heard of Microsoft Orleans almost everyone that there was actually",
    "start": "607840",
    "end": "613960"
  },
  {
    "text": "a very good talk by Sergey Bick of last year here about Orleans if anyone caught it I was really good so",
    "start": "613960",
    "end": "620170"
  },
  {
    "text": "Orleans is actually an actor system right it's a similar to a cow or a cadet net it's not actually a stream",
    "start": "620170",
    "end": "626050"
  },
  {
    "text": "processing system to begin with it's a lets you create actors which is just",
    "start": "626050",
    "end": "631300"
  },
  {
    "text": "pieces of user code which you can deploy and they run within the system and can pass messages to each other and it's",
    "start": "631300",
    "end": "637120"
  },
  {
    "text": "very good for creating distributed systems willing to be robust and scalable but on top of Orleans there is",
    "start": "637120",
    "end": "642850"
  },
  {
    "text": "an extension called Orleans streams which does stream processing because if you look at message passing between",
    "start": "642850",
    "end": "649360"
  },
  {
    "text": "actors in a system it's very similar to how streaming and data processing works because the data processing model for",
    "start": "649360",
    "end": "656290"
  },
  {
    "text": "streaming data is basically to build a graph a directed graph of operations and",
    "start": "656290",
    "end": "661300"
  },
  {
    "text": "then you stream data into it from one source you each one of the operators does something with the data and",
    "start": "661300",
    "end": "667779"
  },
  {
    "text": "passes it on to one or more downstream operators each one of those does something else isn't eventually they all",
    "start": "667779",
    "end": "673930"
  },
  {
    "text": "go to some kind of sink where it's either write it to disk or it doesn't do anything let's talk about a practical",
    "start": "673930",
    "end": "679540"
  },
  {
    "text": "example write in this is an example actually I'm going to show a bit later we want to do some we were building a",
    "start": "679540",
    "end": "686019"
  },
  {
    "text": "startup and we're going to do something no one's ever done before we're going to do sentiment analysis on tweets because",
    "start": "686019",
    "end": "692709"
  },
  {
    "text": "it's the hot new thing right it combines machine learning with IOT and the ten different other buzz words which will",
    "start": "692709",
    "end": "698110"
  },
  {
    "text": "definitely get it funded so we're doing that right so what do we need to",
    "start": "698110",
    "end": "703870"
  },
  {
    "text": "actually do in order to get tweets in real time and remember there's you know half a billion of those happening every",
    "start": "703870",
    "end": "708999"
  },
  {
    "text": "day we need to bring it into our system we need to store them and remember",
    "start": "708999",
    "end": "714129"
  },
  {
    "text": "Twitter the Twitter feed is a unwind double screen which means if we miss a",
    "start": "714129",
    "end": "719559"
  },
  {
    "text": "tweet or if we can't process it in time it's gone forever we can't go back and restream it because meet with scheme",
    "start": "719559",
    "end": "725649"
  },
  {
    "text": "coming in and the API doesn't support rolling back so it's very important to actually per system very durably as fast",
    "start": "725649",
    "end": "732069"
  },
  {
    "text": "as possible and then do all the other processing stuff we want to do with them so want to put your tweet somewhere",
    "start": "732069",
    "end": "737699"
  },
  {
    "text": "presumably in some kind of cue a distributed cue or maybe a database that can actually absorb the six thousand",
    "start": "737699",
    "end": "744519"
  },
  {
    "text": "writes per second so it's going to be scaled out to a lot of machines and then we're going to take all this stuff take",
    "start": "744519",
    "end": "750490"
  },
  {
    "text": "it out of our store do some kind of processing on it maybe enrich it with you know geolocation data maybe in",
    "start": "750490",
    "end": "756309"
  },
  {
    "text": "Richard obviously do semantic analysis on the tweet add that to the basic data then enrich it with maybe data about the",
    "start": "756309",
    "end": "762910"
  },
  {
    "text": "user if we do some kind of user tracking or profiling put it all back into the data store and then expose all of this",
    "start": "762910",
    "end": "768939"
  },
  {
    "text": "stuff for querying but then keep in mind we have 15 billion tweets every month and that's you know maybe 15 terabytes",
    "start": "768939",
    "end": "775629"
  },
  {
    "text": "of data or so so we tend to keep all of this stuff in our database forever right",
    "start": "775629",
    "end": "780850"
  },
  {
    "text": "we need to take that offloaded somewhere else and then we're going to go back to last year's Twitter and reprocess them is",
    "start": "780850",
    "end": "787120"
  },
  {
    "text": "going to be in a different system which can actually process now petabytes of data whereas we'll keep near let's say a",
    "start": "787120",
    "end": "792790"
  },
  {
    "text": "month of tweets back and process them in real time in a more real-time fashion so",
    "start": "792790",
    "end": "799149"
  },
  {
    "text": "for this we need some other set some kind of was actually collect tweets and if you want to mentally map this to whatever",
    "start": "799149",
    "end": "805300"
  },
  {
    "text": "you want like a website which has events and interactions with ads you can do that right instead of connecting to a",
    "start": "805300",
    "end": "810370"
  },
  {
    "text": "Twitter - Twitter and streaming tweets from there imagine that your users come to your website they click something it",
    "start": "810370",
    "end": "815589"
  },
  {
    "text": "causes a server event suppose back to the server and then your web application or your micro service or whatever it is has an event it's a real-time event you",
    "start": "815589",
    "end": "823779"
  },
  {
    "text": "can't go back because the user isn't going to go back and redo the event again you have to store it somewhere if you want to record it forever",
    "start": "823779",
    "end": "830110"
  },
  {
    "text": "then you store it quickly and you know hopefully your website is very successful so there's millions of users doing it's called concurrently so",
    "start": "830110",
    "end": "836410"
  },
  {
    "text": "there's a lot of events happening right and we don't put all of them into a durable storage and then process them so",
    "start": "836410",
    "end": "843640"
  },
  {
    "text": "there are basically two types of processing we can do in real time one is actually really real time which means as",
    "start": "843640",
    "end": "850269"
  },
  {
    "text": "soon as the tweet comes into our processing framework we do something",
    "start": "850269",
    "end": "855850"
  },
  {
    "text": "with it and we can't rely on any kind of history or any kind of other data because every class or every user code",
    "start": "855850",
    "end": "863019"
  },
  {
    "text": "in our system just has this one event to work with so the most we can do is",
    "start": "863019",
    "end": "868390"
  },
  {
    "text": "either account how many of those there are and keep local track locally or maybe we can talk to an external store",
    "start": "868390",
    "end": "874930"
  },
  {
    "text": "maybe you know put it in a database or data field in a database that's pretty much all we can do with a single event",
    "start": "874930",
    "end": "880019"
  },
  {
    "text": "the other kind of processing we can do is micro batching there's obviously also batching but batching is by definition",
    "start": "880019",
    "end": "885850"
  },
  {
    "text": "not real-time right you can go back and look at all the tweets and calculate stuff but in as close to me as you get",
    "start": "885850",
    "end": "892029"
  },
  {
    "text": "to the real-time is micro batching which is taking a collection of events all together let's say 10 seconds of tweets",
    "start": "892029",
    "end": "898570"
  },
  {
    "text": "or a minute of tweets doing some kind of processing on them and then extracting a bit more knowledge for example I want to",
    "start": "898570",
    "end": "904959"
  },
  {
    "text": "know what hashtags are trending for the past minute right if I know what's trending obviously I need more than a",
    "start": "904959",
    "end": "911019"
  },
  {
    "text": "single tweet to each to work with so I'm going to collect let's say a minutes of tweets which is going to be you know",
    "start": "911019",
    "end": "916209"
  },
  {
    "text": "quite substantial amount it's going to be several hundred thousands and then I can do some kind of real-time processing it's still close enough to real-time",
    "start": "916209",
    "end": "922000"
  },
  {
    "text": "right it's a minute minute is you know it's sort of on the edge of real-time then I can extract something that's trending and then I can send this",
    "start": "922000",
    "end": "928209"
  },
  {
    "text": "downstream from my streaming topology and something will happen with the tuner maybe will adjust bids on ads or maybe",
    "start": "928209",
    "end": "934149"
  },
  {
    "text": "will what you know sends you a large to users saying that you know this thing is trending so does it you want the two",
    "start": "934149",
    "end": "940420"
  },
  {
    "text": "models we can do and this frameworks I'm going to be talking about they all implement one or the other right because",
    "start": "940420",
    "end": "946180"
  },
  {
    "text": "it's a it's a very different kind of processing engine and you can freely combine them to very efficiently so",
    "start": "946180",
    "end": "951970"
  },
  {
    "text": "storm attaches to a patchy storm does it implements a very continuous model you",
    "start": "951970",
    "end": "958690"
  },
  {
    "text": "basically define a lot of classes user classes which I can think of them as actors or just pieces of user code and",
    "start": "958690",
    "end": "965230"
  },
  {
    "text": "each one of those classes represents a single operation and it can receive",
    "start": "965230",
    "end": "970600"
  },
  {
    "text": "inputs from a different one or maybe from some kind of cue or source you can do something with that input internet",
    "start": "970600",
    "end": "976690"
  },
  {
    "text": "can program send the input on to the next in in the chain alright and this happens in real time in which one of",
    "start": "976690",
    "end": "982270"
  },
  {
    "text": "them process is exactly one event at a time all right obviously you can store local store kits user class right you",
    "start": "982270",
    "end": "988660"
  },
  {
    "text": "can define any kind of variables in it but if the application dies anything that's stored in memory is going to",
    "start": "988660",
    "end": "994150"
  },
  {
    "text": "disappear so it's not actually a robust way to store local data you do have to have some kind of external store which",
    "start": "994150",
    "end": "1000480"
  },
  {
    "text": "can store your state and the reason it scales so well is because storm will",
    "start": "1000480",
    "end": "1005580"
  },
  {
    "text": "create a lot of copies of your processing class right so if you have a class which let's say calculates sentiment on the tweet which we will",
    "start": "1005580",
    "end": "1012240"
  },
  {
    "text": "have in just a moment we can't just run one of them right because there's no way it can process to it fast enough we need",
    "start": "1012240",
    "end": "1018330"
  },
  {
    "text": "a hundreds of those maybe a thousand of those to actually do the processing in parallel right and so we have some kind",
    "start": "1018330",
    "end": "1024180"
  },
  {
    "text": "of is the framework itself takes care of distributing the twist to the different instances of our processing class and then collecting the messages from them",
    "start": "1024180",
    "end": "1030720"
  },
  {
    "text": "and pushing them downstream to the next class in the next class right and spark",
    "start": "1030720",
    "end": "1036089"
  },
  {
    "text": "streaming is built on spark itself so it's as its roots in Hadoop and my produce so it does micro batches it",
    "start": "1036089",
    "end": "1042870"
  },
  {
    "text": "actually fools us and you think in its real time by taking very small batches of data and then doing normal spark",
    "start": "1042870",
    "end": "1048540"
  },
  {
    "text": "processing on them right doing normal MapReduce stuff which takes all the batch crunches it together and then gets",
    "start": "1048540",
    "end": "1054000"
  },
  {
    "text": "a bunch of a batch of results and pushes a downstream Kefka is the message queue originally it's only did messages so if",
    "start": "1054000",
    "end": "1062280"
  },
  {
    "text": "you think of you know how many people know what as your cues are your cues you use them right",
    "start": "1062280",
    "end": "1068410"
  },
  {
    "text": "simple cues in Amazon right so it was kind of like that right where you can actually put a rain read data out you",
    "start": "1068410",
    "end": "1074230"
  },
  {
    "text": "had a bunch of different topics you can read in the end consume and eventually",
    "start": "1074230",
    "end": "1079870"
  },
  {
    "text": "the guys at Lincoln who built Casca left Lincoln and founded a new company called constant and they're starting and they",
    "start": "1079870",
    "end": "1086860"
  },
  {
    "text": "created a actual processing platform based on Kafka and they have enterprise",
    "start": "1086860",
    "end": "1092620"
  },
  {
    "text": "support and they build a bunch of other stuff around it which does more than just push messages around it actually processes messages you can create",
    "start": "1092620",
    "end": "1099040"
  },
  {
    "text": "basically the same kind of topology for processing messages as you can with other frameworks and we'll show that in just a bit and flink is actually very",
    "start": "1099040",
    "end": "1105850"
  },
  {
    "text": "similar conceptually to storm where we define a bunch of actors which will process your topologies and then we can",
    "start": "1105850",
    "end": "1112270"
  },
  {
    "text": "talk about how you do that like let's get to the actual code right let's look at some code so there's two ways to do",
    "start": "1112270",
    "end": "1117790"
  },
  {
    "text": "this right there's two ways to actually declare her how our processing framework is going to work we can have a",
    "start": "1117790",
    "end": "1123330"
  },
  {
    "text": "declarative style which looks very much like linking dotnet everybody knows how link works in dotnet pretty much yeah so",
    "start": "1123330",
    "end": "1130660"
  },
  {
    "text": "you actually link by the way is an excellent example for stream processing right you can have an infinitely large",
    "start": "1130660",
    "end": "1136780"
  },
  {
    "text": "enumerable and you can do some kind of processing and anyone know how P link works killing yes parallel link it's",
    "start": "1136780",
    "end": "1144160"
  },
  {
    "text": "like one kind saying yeah I use it every day so you can actually do the same kind",
    "start": "1144160",
    "end": "1149590"
  },
  {
    "text": "of declarative processing in parallel and if you take that and you can think of it as doing the same thing but on",
    "start": "1149590",
    "end": "1155500"
  },
  {
    "text": "multiple machines that's basically what stream processing is when you do it declaratively you take some kind of streaming source and you",
    "start": "1155500",
    "end": "1162630"
  },
  {
    "text": "transform it in different ways for example this very simple example which is like the hello world of the big data",
    "start": "1162630",
    "end": "1169030"
  },
  {
    "text": "world which is word count you can take it take text you had count how many of each word there are this is basically",
    "start": "1169030",
    "end": "1175060"
  },
  {
    "text": "the equivalent of writing hello world when you learn a new programming language so we take every string we",
    "start": "1175060",
    "end": "1181390"
  },
  {
    "text": "split it into into words and then we map every word to account which is 1 and then we reduce them we count how many of",
    "start": "1181390",
    "end": "1188920"
  },
  {
    "text": "each words we've seen right and this is actually very concise syntax and for",
    "start": "1188920",
    "end": "1195040"
  },
  {
    "text": "simple things it works very well for more complex things it doesn't work very well because for example maybe instead",
    "start": "1195040",
    "end": "1200560"
  },
  {
    "text": "of just counting words what we want to do is go and update a database right so in one of the steps we",
    "start": "1200560",
    "end": "1205750"
  },
  {
    "text": "take an event in and we want to go into beta database it's not really something you can put in a single line in a processor right and then so you can use",
    "start": "1205750",
    "end": "1212830"
  },
  {
    "text": "a compositional API style which lets you actually define user created classes which you know just all share some kind",
    "start": "1212830",
    "end": "1219700"
  },
  {
    "text": "of common ancestor in this case the example on the left is from storm and",
    "start": "1219700",
    "end": "1225429"
  },
  {
    "text": "storm uses of a metaphor of spouting bolts they have a thing with water going on so a spout is the source of your data",
    "start": "1225429",
    "end": "1232269"
  },
  {
    "text": "it's a class which takes data from somewhere and sends it downstream and every bolt in the chain does some kind",
    "start": "1232269",
    "end": "1238330"
  },
  {
    "text": "of transformation so we can have a bolt which just prints out the data it receives for example so it doesn't",
    "start": "1238330",
    "end": "1243850"
  },
  {
    "text": "produce any data it only consumes data or you can have a boat which takes in a sentence it splits it into words and it",
    "start": "1243850",
    "end": "1250659"
  },
  {
    "text": "sends each out each word out so it takes a small stream and it expands it into a longer stream of multiple words right",
    "start": "1250659",
    "end": "1257080"
  },
  {
    "text": "and then what we can do is declare a topology a graph of these components so",
    "start": "1257080",
    "end": "1262149"
  },
  {
    "text": "we can take you know three bolts you can say this is one sends data to the second one the second one sends data to the",
    "start": "1262149",
    "end": "1267639"
  },
  {
    "text": "third bolt and then the spouse will start sending messages into the bolts each one will do their thing and the",
    "start": "1267639",
    "end": "1272679"
  },
  {
    "text": "whole topology will work and obviously we will have multiple copies of which bolt which is how the whole thing scaled",
    "start": "1272679",
    "end": "1278309"
  },
  {
    "text": "right and most of these tools are unfortunately written in JVM unfortunately because even I'm in a",
    "start": "1278309",
    "end": "1284440"
  },
  {
    "text": "mostly dotnet conference in a Java conference I'd say luckily they're all written in Java in Java or Scala but",
    "start": "1284440",
    "end": "1292779"
  },
  {
    "text": "yeah this is the unfortunate truth of the big data world most of the tools are written in Java and there are very few",
    "start": "1292779",
    "end": "1299220"
  },
  {
    "text": "Microsoft issue or JVM related tools so they're all they're all so for JVM some",
    "start": "1299220",
    "end": "1304750"
  },
  {
    "text": "of them actually support Python if you want to declare your code in Python and Orleans is basically the only example we",
    "start": "1304750",
    "end": "1310600"
  },
  {
    "text": "can point you and say Mike Stetz have done something in this and we'll talk about Orleans I'll show you an example of that in just a minute let's look at",
    "start": "1310600",
    "end": "1317049"
  },
  {
    "text": "some examples right let's look at actual code because it's early in the morning everyone's still fresh we can look at code right so let's look at code and",
    "start": "1317049",
    "end": "1323799"
  },
  {
    "text": "we'll start with storm because storm is a has the simplest API and most expressiveness so we can actually",
    "start": "1323799",
    "end": "1330360"
  },
  {
    "text": "demonstrate it is the easiest way excellent all right so let's go to our",
    "start": "1330360",
    "end": "1339799"
  },
  {
    "text": "storms follows you there's going to be some Java if you can just mentally pretend it's broken c-sharp everyone's",
    "start": "1339799",
    "end": "1346880"
  },
  {
    "text": "going to be happy right it's kind of this is kind of like Russian speakers feel about Ukrainian and vice versa they",
    "start": "1346880",
    "end": "1352639"
  },
  {
    "text": "can kind of understand each other but they don't don't admit it all right so",
    "start": "1352639",
    "end": "1359120"
  },
  {
    "text": "what we're going to do is as I said the simplest thing we're going to count words it's not exciting we're going to do something more exciting in just a",
    "start": "1359120",
    "end": "1364850"
  },
  {
    "text": "couple of minutes but for starters we'll do something very basic so we're going to have a spout a storm spout which is a",
    "start": "1364850",
    "end": "1371809"
  },
  {
    "text": "class that sends out a bunch of text and we're going to count how many times we've seen every word it's a contrived",
    "start": "1371809",
    "end": "1377389"
  },
  {
    "text": "example but it demonstrates what we're talking about very easily all right so the storm topology is connected is know",
    "start": "1377389",
    "end": "1388330"
  },
  {
    "text": "yeah that everyone can see it especially in the back it's a very weirdly shaped room all right good so actually defining",
    "start": "1389799",
    "end": "1397519"
  },
  {
    "text": "our processing topology in storm is very straightforward what we say is in our topology we're going to have three",
    "start": "1397519",
    "end": "1402950"
  },
  {
    "text": "components three bolts right we're going to have n1 spouts the spouts just generate random text random sentences we",
    "start": "1402950",
    "end": "1410720"
  },
  {
    "text": "call it a spout and then we start connecting processing classes to it whether class called splits and sentence",
    "start": "1410720",
    "end": "1417740"
  },
  {
    "text": "and I'll go into this and show you the code in just a sec but the name pretty much says it all it takes a sentence and splits it and bye-bye spaces right and",
    "start": "1417740",
    "end": "1424760"
  },
  {
    "text": "we connected to our spouts so anything that gunk comes out of the spout will go into our split sentence",
    "start": "1424760",
    "end": "1431360"
  },
  {
    "text": "class and notice we're actually going to be running this is the parallelism we're going to be running eight instances of this class so we have you know a million",
    "start": "1431360",
    "end": "1437990"
  },
  {
    "text": "messages coming in every second is going to be split between eight different classes and those will almost certainly",
    "start": "1437990",
    "end": "1443179"
  },
  {
    "text": "run on different machines right so we need to spin up a whole cluster of storm oisin and I'm going to bore you with how",
    "start": "1443179",
    "end": "1449809"
  },
  {
    "text": "it's actually done in the engineering sense but it's not easy I'm so I'm sorry",
    "start": "1449809",
    "end": "1454850"
  },
  {
    "text": "to say but it's much easier than writing it all the way all on your own which is the point so we're going to take the",
    "start": "1454850",
    "end": "1460669"
  },
  {
    "text": "split sentences which means are a small string of sentences as three most sentences is going to actually expand",
    "start": "1460669",
    "end": "1465710"
  },
  {
    "text": "every sentence is going to turn into a bunch of different events in the stream I'm going to split those and send",
    "start": "1465710",
    "end": "1473480"
  },
  {
    "text": "them to the word-count both which actually does the actual counting and stores the state of how",
    "start": "1473480",
    "end": "1479630"
  },
  {
    "text": "many times it's seen each bolt and the important here and a thing here is we're going to be shuffling it not just",
    "start": "1479630",
    "end": "1485690"
  },
  {
    "text": "randomly we're going to be consistently sending any particular word to a particular instance of this bolt right",
    "start": "1485690",
    "end": "1492380"
  },
  {
    "text": "so only so if the same word comes up it will go to the same instance of a bolt so it will actually be able to count it",
    "start": "1492380",
    "end": "1498140"
  },
  {
    "text": "right because each bolt we'll see in a second internally thread safe but if you same sent the same word different bolts",
    "start": "1498140",
    "end": "1503570"
  },
  {
    "text": "each one will have a partial count and that's not what we want and from there we just send it back send the actual",
    "start": "1503570",
    "end": "1508940"
  },
  {
    "text": "count to our printer bolt which all it does is print it doesn't actually emit anything it only collects data just look",
    "start": "1508940",
    "end": "1514700"
  },
  {
    "text": "inside the split sentence both just so we can actually see how it works the book itself is a very simple class right",
    "start": "1514700",
    "end": "1521990"
  },
  {
    "text": "it's a class which extends some kind of basic storm class that storm knows how to run it has exactly one method that",
    "start": "1521990",
    "end": "1528410"
  },
  {
    "text": "you can run it's called execute it gets an input which is a tuple it's just a class with a key in the value right and",
    "start": "1528410",
    "end": "1535730"
  },
  {
    "text": "it has a collector where you can write output and it can write as much or as little output as you want it doesn't",
    "start": "1535730",
    "end": "1541760"
  },
  {
    "text": "doesn't have to be one to one so we can get a tuple which is a sentence in as the input we split it and we get an",
    "start": "1541760",
    "end": "1549140"
  },
  {
    "text": "array of words from that sentence and then we omit each one of those as a new",
    "start": "1549140",
    "end": "1554510"
  },
  {
    "text": "tuple right so it's very simple that a sentence in we get a bunch of words out done this goes into our actual word",
    "start": "1554510",
    "end": "1561230"
  },
  {
    "text": "count boat which again doesn't do anything super complex all it takes is",
    "start": "1561230",
    "end": "1566320"
  },
  {
    "text": "every tuple and again in this case it's not a sentence anymore the word count both actually gets single words and it",
    "start": "1566320",
    "end": "1573740"
  },
  {
    "text": "just has a hash map which is a dictionary for you C sharp speakers where it just updates the count of every",
    "start": "1573740",
    "end": "1580400"
  },
  {
    "text": "how many times it C in the word and as you can see the boat itself is entirely thread-safe right we're using a local",
    "start": "1580400",
    "end": "1586760"
  },
  {
    "text": "variable we're guaranteeing that this thing is going to always execute on the same thread there's no way to quote this exact code",
    "start": "1586760",
    "end": "1593540"
  },
  {
    "text": "concurrently so storm actually guarantee is a very simple programming environment for us all the stuff you know about how",
    "start": "1593540",
    "end": "1600320"
  },
  {
    "text": "you need to do logging and thread safety and all that we don't need to worry about any of this because the framework takes care of the actual",
    "start": "1600320",
    "end": "1606260"
  },
  {
    "text": "execution environment and we just need to provide the user code which does the actual operations it's extremely",
    "start": "1606260",
    "end": "1612050"
  },
  {
    "text": "convenient especially when you start thinking about how you're going to synchronize concurrent code across a",
    "start": "1612050",
    "end": "1617630"
  },
  {
    "text": "cluster of 100 machines right so the fact that all of these frameworks actually take away the concern for",
    "start": "1617630",
    "end": "1623420"
  },
  {
    "text": "threading away from the developer is extremely convenient right so we didn't do here is we do all collect all the",
    "start": "1623420",
    "end": "1630830"
  },
  {
    "text": "counts right and then we omit the actual count itself so instead of just omitting the word we made a word and a number of",
    "start": "1630830",
    "end": "1637220"
  },
  {
    "text": "how many times we've seen that word right now this is obviously very unsafe who can think of why this thing is",
    "start": "1637220",
    "end": "1642530"
  },
  {
    "text": "actually a very bad implementation it's very unsafe anyone not at your question",
    "start": "1642530",
    "end": "1648890"
  },
  {
    "text": "let me give an example what happens if we shut down one of the machines in our hundred machine storm topology again now",
    "start": "1648890",
    "end": "1657110"
  },
  {
    "text": "two questions what happens we shut down the machine so any instances of this bolt that we're running in the memory of that machine",
    "start": "1657110",
    "end": "1662990"
  },
  {
    "text": "we're going to lose right so we're going to lose all the states which means in the machine will actually even a",
    "start": "1662990",
    "end": "1668450"
  },
  {
    "text": "shutdown storm will redistribute the bolts to different machines storm takes care of all that stuff but every one of",
    "start": "1668450",
    "end": "1674000"
  },
  {
    "text": "those will get a fresh new instance of our bolt which has no state storage it will just start counting from zero right",
    "start": "1674000",
    "end": "1680630"
  },
  {
    "text": "so we'll have some words with an existing account in some words with a new count so this is why in this case we'll actually have to manually take",
    "start": "1680630",
    "end": "1687050"
  },
  {
    "text": "care of State and usually the answer to that is put it somewhere else right where do you put state in database",
    "start": "1687050",
    "end": "1694280"
  },
  {
    "text": "anyone said the debate is exactly correct database is the place to store external state obviously a fast database",
    "start": "1694280",
    "end": "1700130"
  },
  {
    "text": "because you know sequel server probably isn't going to run at you know a hundred thousand events per second or anything",
    "start": "1700130",
    "end": "1705800"
  },
  {
    "text": "all right so the printer bolt actually that's exactly what it says in the cover it does exactly one line so we got a",
    "start": "1705800",
    "end": "1711800"
  },
  {
    "text": "tuple and we print it that would be the equivalent console console.writeline in c-sharp I know I didn't have a",
    "start": "1711800",
    "end": "1717680"
  },
  {
    "text": "translator and I just like just like it all right so let's run this and see if we get some word counts right so we can",
    "start": "1717680",
    "end": "1723740"
  },
  {
    "text": "run the topology itself now I will run it locally but in a real production environment there's actually a cluster",
    "start": "1723740",
    "end": "1729740"
  },
  {
    "text": "of storm machines running and they're doing nothing at the moment they only start doing stuff when you",
    "start": "1729740",
    "end": "1734870"
  },
  {
    "text": "create a topology which is just a library you built you compile and build and you submit your library with your",
    "start": "1734870",
    "end": "1740270"
  },
  {
    "text": "user code to the storm cluster and then it starts executing all the stuff you defined alright because normally when",
    "start": "1740270",
    "end": "1746270"
  },
  {
    "text": "you spin up a storm cluster or spark cluster or any of these things we're talking about it's not actually doing anything because it has no user code to",
    "start": "1746270",
    "end": "1752930"
  },
  {
    "text": "execute so you have to submit the user code and it will get executed so let's run this and unsurprisingly it will spin",
    "start": "1752930",
    "end": "1760850"
  },
  {
    "text": "out some text and we'll see that it actually produces counts for different words so the example itself is not very interesting it spins up a bunch of",
    "start": "1760850",
    "end": "1767570"
  },
  {
    "text": "different stuff in the background because again the clustering even if you do it locally is very complex but then we start seeing a bunch of word counts",
    "start": "1767570",
    "end": "1773840"
  },
  {
    "text": "appear if we pause this for a second we can see that yes indeed it's actually counting words so let's see see we had",
    "start": "1773840",
    "end": "1780080"
  },
  {
    "text": "at at 42 and then a second later we had at 43 as well just continue accumulating",
    "start": "1780080",
    "end": "1785660"
  },
  {
    "text": "counts for every word this is obviously not useful for anything but if you just",
    "start": "1785660",
    "end": "1790700"
  },
  {
    "text": "mentally map this to something useful like let's say you're doing an anomaly",
    "start": "1790700",
    "end": "1797780"
  },
  {
    "text": "detection and you're listening to your company Network you're attached to active directory this is a real example",
    "start": "1797780",
    "end": "1803150"
  },
  {
    "text": "by the way from a company I worked with they're attached to the active directory event they're scanning all the login",
    "start": "1803150",
    "end": "1809000"
  },
  {
    "text": "events all the network access anyone who opens the socket or calls the URL on the",
    "start": "1809000",
    "end": "1814280"
  },
  {
    "text": "front end and they're counting how many times any IP has tried to log in or",
    "start": "1814280",
    "end": "1819620"
  },
  {
    "text": "access the computer and they're looking for anomalies in their behavior right so they're counting how many logins for",
    "start": "1819620",
    "end": "1825290"
  },
  {
    "text": "every machine for a very P happens they're storing it in a separate database and any time logging happens",
    "start": "1825290",
    "end": "1831080"
  },
  {
    "text": "outside of normal parameter for example an IP which only connects during the week days connected tries to plan during",
    "start": "1831080",
    "end": "1836570"
  },
  {
    "text": "the weekend that will cause an alert and it's all implemented as a storm topology with a bunch of rules and just counters",
    "start": "1836570",
    "end": "1841940"
  },
  {
    "text": "which counts how many times everything happened and it runs on about ten different machines because they're purchasing a lot of events for a large",
    "start": "1841940",
    "end": "1847790"
  },
  {
    "text": "customer and this works very similarly to this just a little more complex in the actual execution logic all right so",
    "start": "1847790",
    "end": "1855920"
  },
  {
    "text": "let's switch over to spark which is an entirely different concept spark actually as I said came from was born",
    "start": "1855920",
    "end": "1865730"
  },
  {
    "text": "out of Hadoop and the needs to run very large batch processing operations in parallel and",
    "start": "1865730",
    "end": "1872330"
  },
  {
    "text": "then what happened was they took that concept and Maps it on 2 micro batches so it became much more real-time and",
    "start": "1872330",
    "end": "1879470"
  },
  {
    "text": "spark actually let's run this spark works in a entirely different way from",
    "start": "1879470",
    "end": "1886609"
  },
  {
    "text": "storm first of all you don't declare workers you actually declare your logic as a logical expression kind of like",
    "start": "1886609",
    "end": "1895309"
  },
  {
    "text": "link kind of like the way Java streams work so in this case we're creating a",
    "start": "1895309",
    "end": "1900859"
  },
  {
    "text": "stream of data coming in from my socket just slice it up I'm going to type in text it will listen to sockets on port",
    "start": "1900859",
    "end": "1907549"
  },
  {
    "text": "11 1811 and it will count the words coming in from that in real time and the",
    "start": "1907549",
    "end": "1913849"
  },
  {
    "text": "logic itself is exactly the same as we saw in storm right we're doing word count so we have our stream called lines and",
    "start": "1913849",
    "end": "1920779"
  },
  {
    "text": "we flat map it the same thing as we would do with you know dotnet or Java or anything any framework framework which",
    "start": "1920779",
    "end": "1927379"
  },
  {
    "text": "supports logical expressions with flat tappet by splitting the string itself by space",
    "start": "1927379",
    "end": "1934549"
  },
  {
    "text": "so we turn the small stream into a larger stream of words then we map every",
    "start": "1934549",
    "end": "1939590"
  },
  {
    "text": "word to account and every word has a count of 1 and then we're used to stream right we aggregate the stream by word",
    "start": "1939590",
    "end": "1946609"
  },
  {
    "text": "and then we get the count of how many times you seen every word and the important thing is you can't do this",
    "start": "1946609",
    "end": "1953090"
  },
  {
    "text": "infinitely right because then it would never finish right you can't just because the stream will actually wait for this for the data to finish before",
    "start": "1953090",
    "end": "1959629"
  },
  {
    "text": "it in aggregate and reduce it so we have to define some kind of window of how long to wait before you actually run",
    "start": "1959629",
    "end": "1964970"
  },
  {
    "text": "this calculation and so continuously run the calculation in intervals all right so we creates the stream with a 5 second",
    "start": "1964970",
    "end": "1972049"
  },
  {
    "text": "window all right so every 5 seconds it will take whatever it's accumulated until then it will execute the count logic and",
    "start": "1972049",
    "end": "1978529"
  },
  {
    "text": "split and all that stuff and spit out an answer in this answer can go somewhere you can go into the database or some",
    "start": "1978529",
    "end": "1984049"
  },
  {
    "text": "kind of other storage or it can go and be the basis of a different expression which does something so let's run this",
    "start": "1984049",
    "end": "1989269"
  },
  {
    "text": "and see it actually works well",
    "start": "1989269",
    "end": "1995470"
  },
  {
    "text": "yeah there we go",
    "start": "1998390",
    "end": "2002140"
  },
  {
    "text": "so it'll actually just listen for connections and sockets 1111 and there",
    "start": "2007740",
    "end": "2014710"
  },
  {
    "text": "we go let's go a a a b and c and we should start seeing counts every 5",
    "start": "2014710",
    "end": "2019960"
  },
  {
    "text": "seconds right and as you can see in the background or you can't see anything in the left corner so let's move this let's",
    "start": "2019960",
    "end": "2026410"
  },
  {
    "text": "move it over here and this over here",
    "start": "2026410",
    "end": "2031950"
  },
  {
    "text": "there you go much better so you can see that it's pretty out aggregations every",
    "start": "2031950",
    "end": "2037150"
  },
  {
    "text": "so often and if I actually give it some text to collect in the next 5 seconds it",
    "start": "2037150",
    "end": "2042340"
  },
  {
    "text": "will count the letter B five and three times right if we do this and continue doing stuff it will spit out",
    "start": "2042340",
    "end": "2048730"
  },
  {
    "text": "intermediate counts all right every five seconds this is actually the main",
    "start": "2048730",
    "end": "2055629"
  },
  {
    "text": "difference between the two approaches is twofold one they have different latency and throughput right storm is has much",
    "start": "2055630",
    "end": "2062260"
  },
  {
    "text": "lower latency because as soon as an event comes in we run it for the entire topology we do all the processing and we",
    "start": "2062260",
    "end": "2067690"
  },
  {
    "text": "get a result immediately usually within milliseconds and there's monitoring that can actually tell you how long the",
    "start": "2067690",
    "end": "2072820"
  },
  {
    "text": "processing takes for a single event which spark the minimum latency obviously is five seconds right we can't get a result in less than five seconds",
    "start": "2072820",
    "end": "2078820"
  },
  {
    "text": "or an average of that would say even if it even comes in on average will it will wait two and a half seconds right",
    "start": "2078820",
    "end": "2084580"
  },
  {
    "text": "but because spark can batch results it can reuse a lot of internal mechanism",
    "start": "2084580",
    "end": "2090340"
  },
  {
    "text": "for threading and memory allocation it can process data a lot faster at scale",
    "start": "2090340",
    "end": "2095770"
  },
  {
    "text": "so if you need to process a million events it will be a much more expensive to process them one at a time in storm",
    "start": "2095770",
    "end": "2101380"
  },
  {
    "text": "even though every result will come back quickly then in spark because it will take the whole million as a batch do processing",
    "start": "2101380",
    "end": "2108610"
  },
  {
    "text": "in the background and spit out a result so spark will actually have spark streaming rather will have much higher throughput per second in the storm and",
    "start": "2108610",
    "end": "2115120"
  },
  {
    "text": "this trade-off is what you have to play with right and companies which come which come at this from fresh right they",
    "start": "2115120",
    "end": "2122320"
  },
  {
    "text": "have no streaming topologies they have no data processing and they're building something which needs to handle all the",
    "start": "2122320",
    "end": "2127810"
  },
  {
    "text": "traffic that's coming from their event source whether it's a website or a mobile application that's sending back telemetry or whatever IOT is very hot",
    "start": "2127810",
    "end": "2134500"
  },
  {
    "text": "right now and obviously you have something which produces events on the edge device sends it back to the server and then",
    "start": "2134500",
    "end": "2139730"
  },
  {
    "text": "you have to do something with it right you have to do some kind of cool machine learning stuff to serve ads to the user",
    "start": "2139730",
    "end": "2144950"
  },
  {
    "text": "obviously because again that's what the internet is all about and one other thing which I'm not allowed to say but",
    "start": "2144950",
    "end": "2153100"
  },
  {
    "text": "the difference is again if you need real-time processing you go with something conceptual conceptually built",
    "start": "2153100",
    "end": "2159560"
  },
  {
    "text": "for processing single events at a time if you need something just high-throughput and you don't care about latency as much you go with storm and",
    "start": "2159560",
    "end": "2166280"
  },
  {
    "text": "you have the two approaches and all the other frameworks they fall somewhere on this continuum right we'll look at Kafka",
    "start": "2166280",
    "end": "2171470"
  },
  {
    "text": "which is also like at one events the time we look at flink which does batching and they're going to fall",
    "start": "2171470",
    "end": "2176600"
  },
  {
    "text": "somewhere in the middle all right so let's look at the weather we have next let's look at one more thing with spark",
    "start": "2176600",
    "end": "2183950"
  },
  {
    "text": "which is very cool and that's sparks equal on top of the normal spark execution engine we can actually run",
    "start": "2183950",
    "end": "2190840"
  },
  {
    "text": "queries spark has a language called spark sequel which lets you actually run",
    "start": "2190840",
    "end": "2196310"
  },
  {
    "text": "queries on top of your stream in real time right because you don't have to",
    "start": "2196310",
    "end": "2202340"
  },
  {
    "text": "have just one stream you can have multiple streams for example let's say I have a stream of user events coming in",
    "start": "2202340",
    "end": "2208100"
  },
  {
    "text": "from my website and I have a stream of geoip GIPS or some kind of something",
    "start": "2208100",
    "end": "2215030"
  },
  {
    "text": "that provides extra context for every user I can take both themes in spark and",
    "start": "2215030",
    "end": "2220190"
  },
  {
    "text": "I can join them right I can take for every user or for every IP in the user stream join with the matching IP and the",
    "start": "2220190",
    "end": "2227480"
  },
  {
    "text": "location from the Geo hash stream all right and the stream can actually be entirely cached in memory and that's",
    "start": "2227480",
    "end": "2233570"
  },
  {
    "text": "what spark will do and you can actually go and just use a regular sequel expression to join the streams in memory we can do the same thing which would",
    "start": "2233570",
    "end": "2240440"
  },
  {
    "text": "storm for example but then we'd have to manually write the join logic which is a bit more complicated so in this case we",
    "start": "2240440",
    "end": "2246020"
  },
  {
    "text": "have exactly the same thing with basically the same localhost to connect to the socket who listens for forwards",
    "start": "2246020",
    "end": "2251270"
  },
  {
    "text": "and then instead of batching them what we do is we store them in memory every",
    "start": "2251270",
    "end": "2257000"
  },
  {
    "text": "five seconds and then we run a query on them where we do where we define a schema on our data and the scheme is",
    "start": "2257000",
    "end": "2263810"
  },
  {
    "text": "very simple right it's a quiet table with two columns we have a column cord called word and a column called count",
    "start": "2263810",
    "end": "2269390"
  },
  {
    "text": "right and then we just select from the Select sorry : cool words an account is always",
    "start": "2269390",
    "end": "2275210"
  },
  {
    "text": "one because it's a single word and then we just run select word and count of everything as total right as we would in",
    "start": "2275210",
    "end": "2281749"
  },
  {
    "text": "sequel query and this will actually run every five seconds and take the aggregate data and run the query on the",
    "start": "2281749",
    "end": "2288529"
  },
  {
    "text": "aggregated data every five seconds so if you run this we'll see that it does basically the same thing I said let's",
    "start": "2288529",
    "end": "2293749"
  },
  {
    "text": "open our socket and listen to it and",
    "start": "2293749",
    "end": "2301069"
  },
  {
    "text": "take the while screen up because all of these frameworks actually have a bunch of stuff happening in the background",
    "start": "2301069",
    "end": "2306518"
  },
  {
    "text": "and I'm going to run a bunch of data we'll see it now we have a one and in",
    "start": "2307329",
    "end": "2313970"
  },
  {
    "text": "the next five seconds we'll see all the other letters I types again and now we can actually see them as a cycle table",
    "start": "2313970",
    "end": "2319819"
  },
  {
    "text": "right and you can do sequel operations on those which is very convenient especially when you are trying to do complicated analysis on them all right",
    "start": "2319819",
    "end": "2326930"
  },
  {
    "text": "and if link works small is the same it works like a storm we're defined both to",
    "start": "2326930",
    "end": "2332119"
  },
  {
    "text": "call them differently but it's the same idea we're defined operators you connect them in a graph one pointing downstream to",
    "start": "2332119",
    "end": "2339589"
  },
  {
    "text": "the next and you run your data through them the main difference is the link",
    "start": "2339589",
    "end": "2345979"
  },
  {
    "text": "comes with a bunch of different libraries which lets you do complicated stuff on top of the streaming and",
    "start": "2345979",
    "end": "2351650"
  },
  {
    "text": "streaming stuff for example link comes with library for CD complex event processing where complex event",
    "start": "2351650",
    "end": "2357829"
  },
  {
    "text": "processing deals more with finding patterns or doing more complicated",
    "start": "2357829",
    "end": "2363440"
  },
  {
    "text": "queries on top of your streaming data as an example let's say we we have our word stream coming in letters ABC and so on",
    "start": "2363440",
    "end": "2370039"
  },
  {
    "text": "I'm going to look for patterns I want to find every pattern where the letters a B and C repeat within a certain time frame",
    "start": "2370039",
    "end": "2377089"
  },
  {
    "text": "and if you think about this and look at actual real world examples where this originated think of for example stock",
    "start": "2377089",
    "end": "2382849"
  },
  {
    "text": "trades you have a stream of stocks updates coming in all the time and I want to look for patterns where a",
    "start": "2382849",
    "end": "2387920"
  },
  {
    "text": "certain stock behaves in a way it goes up down up and down and then you expect it maybe with some probability to go up",
    "start": "2387920",
    "end": "2394279"
  },
  {
    "text": "right and if it behaves this way within a certain time frame maybe you go and you adjust some positions and buy the",
    "start": "2394279",
    "end": "2399529"
  },
  {
    "text": "stock before it goes up again and again we're talking about microseconds here right because we want to adjust positions within microseconds buy the",
    "start": "2399529",
    "end": "2405200"
  },
  {
    "text": "stock and sell it on microseconds later if it goes up by you know I'm micro percent or you can think of other things",
    "start": "2405200",
    "end": "2412040"
  },
  {
    "text": "like user behavior again the same thing with the tracking user threats within the network if we want to look for a",
    "start": "2412040",
    "end": "2417860"
  },
  {
    "text": "certain pattern of events and I didn't identify them as a threat right we want to search for patterns where someone",
    "start": "2417860",
    "end": "2423560"
  },
  {
    "text": "tries to login and it fails three times and then immediately the same ip tries to login with a different user name",
    "start": "2423560",
    "end": "2428570"
  },
  {
    "text": "right so any time a pattern happens where one username fails three logs in and then the same IP logs in with a",
    "start": "2428570",
    "end": "2434810"
  },
  {
    "text": "different username we push out an alert and an alert right and you can also do this manually you can implement this",
    "start": "2434810",
    "end": "2440119"
  },
  {
    "text": "logic manually but it's very convenient if you can just define the pattern and listen for it all right and I think that's the same thing right we can see",
    "start": "2440119",
    "end": "2446240"
  },
  {
    "text": "we have exactly the same thing we had earlier with the word tokenizer we take in text with tokenize it by lines same",
    "start": "2446240",
    "end": "2453110"
  },
  {
    "text": "thing we had the storm but in this case we define a pattern and listen to that pattern in the stream so we're we want",
    "start": "2453110",
    "end": "2459470"
  },
  {
    "text": "as pattern which starts where the text is equal the letter A it has to continue",
    "start": "2459470",
    "end": "2464540"
  },
  {
    "text": "and be equal to a letter B and there can be other letters between those right because we're listening for a whole stream which one's a and then B and then",
    "start": "2464540",
    "end": "2471410"
  },
  {
    "text": "we want C to appear within one second right and if it's more than one second it's not interesting but it appears",
    "start": "2471410",
    "end": "2477770"
  },
  {
    "text": "within one second that's part of our pattern and we wants to listen to it and then we do something very interesting because flink lets us lets us actually",
    "start": "2477770",
    "end": "2486020"
  },
  {
    "text": "connect to its internal state externally it has a our PC mechanism we can connect",
    "start": "2486020",
    "end": "2491780"
  },
  {
    "text": "out form of tidal topology and ask it questions right which is a very interesting because then you can look at",
    "start": "2491780",
    "end": "2498590"
  },
  {
    "text": "your streaming process three processes in topology it's kind of a micro service which can serve requests right I can",
    "start": "2498590",
    "end": "2504440"
  },
  {
    "text": "always turn to it and say what's the latest pattern or what's your current internal state how many users have you",
    "start": "2504440",
    "end": "2509630"
  },
  {
    "text": "counted right and then instead of just having a very closed processing system",
    "start": "2509630",
    "end": "2515690"
  },
  {
    "text": "we have something that reserved intermediate data externally will actually see an example of that in just",
    "start": "2515690",
    "end": "2521690"
  },
  {
    "text": "a moment right so let's run this and again same thing we had earlier not you",
    "start": "2521690",
    "end": "2526850"
  },
  {
    "text": "I meant you open sockets run this thing",
    "start": "2526850",
    "end": "2532160"
  },
  {
    "text": "and now it will actually listen for events coming in on port whatever",
    "start": "2532160",
    "end": "2539580"
  },
  {
    "text": "there we go yes and if I just put in it",
    "start": "2539580",
    "end": "2546450"
  },
  {
    "text": "all does its print out the letter but if I happen to print in the exact pattern it will find the pattern right and if I",
    "start": "2546450",
    "end": "2552420"
  },
  {
    "text": "do this in any sort of order it will still detect the pattern within the",
    "start": "2552420",
    "end": "2558270"
  },
  {
    "text": "stream right and if I change if I wait more than a second between B and C then",
    "start": "2558270",
    "end": "2564390"
  },
  {
    "text": "that's not part of the pattern obviously right so if we do several of those please act Q of them right so now we",
    "start": "2564390",
    "end": "2573720"
  },
  {
    "text": "actually have the ability to not just take our events but actually do some very complex processing that's declared",
    "start": "2573720",
    "end": "2580050"
  },
  {
    "text": "in a very straightforward matter right you need to go and write some kind of logic which aggregates and keeps State",
    "start": "2580050",
    "end": "2585630"
  },
  {
    "text": "and memory because state as we saw is vulnerable vulnerable to failures to network failures and machine failures so",
    "start": "2585630",
    "end": "2591930"
  },
  {
    "text": "it's very hard to store state and it's very convenient when the framework does this for us right so let's move on let's",
    "start": "2591930",
    "end": "2598290"
  },
  {
    "text": "move on to our last contender which is Kefka and Kafka is interesting for two",
    "start": "2598290",
    "end": "2605880"
  },
  {
    "text": "things one it started out as I said as a messaging queue and then they tacked on the ability to actually execute code on",
    "start": "2605880",
    "end": "2614730"
  },
  {
    "text": "the events in in real time later and two they've they've completely separated out",
    "start": "2614730",
    "end": "2620100"
  },
  {
    "text": "the runtime environment from the actual processing environment one of the problems with all of the mechanisms I",
    "start": "2620100",
    "end": "2626970"
  },
  {
    "text": "talked about right support our storm spark link all of these is they implement their clustering solution on",
    "start": "2626970",
    "end": "2633300"
  },
  {
    "text": "their own right so they have to do a lot of work in a build a complex system which will do all the scaling and",
    "start": "2633300",
    "end": "2640170"
  },
  {
    "text": "processing and failover and load balancing between different cluster classes in the topology and essentially",
    "start": "2640170",
    "end": "2647430"
  },
  {
    "text": "it breaks the single responsibility principle by having a system which both runs user code and manages resources and",
    "start": "2647430",
    "end": "2655200"
  },
  {
    "text": "it tries to figure out where to run every piece of code where it's efficient where it has resources to spare and you",
    "start": "2655200",
    "end": "2661320"
  },
  {
    "text": "know do all the self-healing katka either by laziness or by lack of resources or by design I have decided",
    "start": "2661320",
    "end": "2667890"
  },
  {
    "text": "not to deal with any of that and they've separated out the execution part from",
    "start": "2667890",
    "end": "2673080"
  },
  {
    "text": "clustering part a catechist reams application just uses a library which handles all the logic of connecting to a",
    "start": "2673080",
    "end": "2679830"
  },
  {
    "text": "Kefka topic after q reading messages processing them in the application and putting them in a different queue it",
    "start": "2679830",
    "end": "2686370"
  },
  {
    "text": "does nothing to say to say where the code will run it's just a java application or you can actually",
    "start": "2686370",
    "end": "2692190"
  },
  {
    "text": "implement it and go there's a catgirl library for go and for dotnet as it happens and it's your responsibility to",
    "start": "2692190",
    "end": "2698100"
  },
  {
    "text": "create copies of this application and to run them which lets us do very",
    "start": "2698100",
    "end": "2703350"
  },
  {
    "text": "interesting things with scaling the application because it's up to the actual organization who uses them to",
    "start": "2703350",
    "end": "2710160"
  },
  {
    "text": "scale the application so for example anyone who's using currently Kafka streams most likely using some kind of",
    "start": "2710160",
    "end": "2715470"
  },
  {
    "text": "operations whether it's docker or you know kubernetes or one of the orchestrating frameworks which let it",
    "start": "2715470",
    "end": "2721950"
  },
  {
    "text": "let you run a bunch of copies of application and every application internally uses Kafka streams to",
    "start": "2721950",
    "end": "2727590"
  },
  {
    "text": "coordinate and read messages from Kafka process them put them in different Kefka queue right so separating out the actual",
    "start": "2727590",
    "end": "2735030"
  },
  {
    "text": "execution from resource management was actually a very wise decision for Kefka because it both let them write less code",
    "start": "2735030",
    "end": "2741360"
  },
  {
    "text": "and simplify the execution and also give the control of where to execute code to the actual user which which lets you",
    "start": "2741360",
    "end": "2749640"
  },
  {
    "text": "avoid trying to set up a very complex system right spinning up a cluster of storm or spark it's just a daunting task",
    "start": "2749640",
    "end": "2756480"
  },
  {
    "text": "on the engineering level to connect and configure everything correctly so they talk to each other alright so let's look",
    "start": "2756480",
    "end": "2762720"
  },
  {
    "text": "at cough cough before we move on and it's exactly the same exact example only",
    "start": "2762720",
    "end": "2768450"
  },
  {
    "text": "we're going to improve something this time instead of printing out the counts we want to expose it as some kind of API",
    "start": "2768450",
    "end": "2775080"
  },
  {
    "text": "that you can query and get the current count externally right and you can start",
    "start": "2775080",
    "end": "2781320"
  },
  {
    "text": "thinking of stream processing as kind of a small use case of micro services everyone knows about micro services right it's the new thing everyone's",
    "start": "2781320",
    "end": "2788340"
  },
  {
    "text": "doing micro services even if they don't have to so you can think of it as a micro service right we're going to do",
    "start": "2788340",
    "end": "2793680"
  },
  {
    "text": "what we're going to do is the code itself that does the processing is a micro service except now instead of",
    "start": "2793680",
    "end": "2800010"
  },
  {
    "text": "getting web request is getting events as requests does some kind of processing and push the response",
    "start": "2800010",
    "end": "2806410"
  },
  {
    "text": "synchronously somewhere else and on top of that it keeps state which we will expose as an actual micro service with",
    "start": "2806410",
    "end": "2812980"
  },
  {
    "text": "the REST API and everything so you can query the external state of our processing right so we're going to be",
    "start": "2812980",
    "end": "2818710"
  },
  {
    "text": "counting words because again stupid example but it's interesting from an engineering point of view and then we",
    "start": "2818710",
    "end": "2824440"
  },
  {
    "text": "want to expose the count as a rest service to anyone who wants to query it instead of just printing it out all",
    "start": "2824440",
    "end": "2830170"
  },
  {
    "text": "right so let's take a look at that this",
    "start": "2830170",
    "end": "2835420"
  },
  {
    "text": "one all right so our cafe application is",
    "start": "2835420",
    "end": "2847390"
  },
  {
    "text": "exactly the thing we had earlier right it takes messages don't have to read the",
    "start": "2847390",
    "end": "2852520"
  },
  {
    "text": "codes now I'm not there yet but what it does is it takes messages from a stream",
    "start": "2852520",
    "end": "2857980"
  },
  {
    "text": "from a catechol queue in fact and any anyone can put messages into a queue right they can come in from anywhere at",
    "start": "2857980",
    "end": "2864190"
  },
  {
    "text": "all it takes messages from one queue counts how many words there are puts them in a different queue but you can",
    "start": "2864190",
    "end": "2871630"
  },
  {
    "text": "look at a queue or other is at a stream as a very large change log of a table",
    "start": "2871630",
    "end": "2877180"
  },
  {
    "text": "right there's actually a duality between a change log or a stream and a regular table a key value table right because",
    "start": "2877180",
    "end": "2883390"
  },
  {
    "text": "what's a database table it's just the current state of all the changes accumulated so far so let's say we're",
    "start": "2883390",
    "end": "2889599"
  },
  {
    "text": "counting words and we all have two words in our dictionary a and B at a certain point we have a 1 then you have B 1 so",
    "start": "2889599",
    "end": "2895480"
  },
  {
    "text": "you have a 2 and a 3 and so on if you look at this from a stream point of view we'll see a bunch of A's and we can",
    "start": "2895480",
    "end": "2901270"
  },
  {
    "text": "count all of them if we coalesce this into a table we'll just have a table with 2 or 2 items a and count 4 and B",
    "start": "2901270",
    "end": "2908200"
  },
  {
    "text": "and the count 1 or something right so you can always expose the current state of a stream as a table and then you can",
    "start": "2908200",
    "end": "2914829"
  },
  {
    "text": "query this table to get a real-time look into what's going on in the stream all",
    "start": "2914829",
    "end": "2920260"
  },
  {
    "text": "right so this is what Kefka actually does and less you both process the stream and expose it as kind of a",
    "start": "2920260",
    "end": "2925869"
  },
  {
    "text": "virtual table on top of the stream which just duplicate all the values and gives you the latest snapshot of every value",
    "start": "2925869",
    "end": "2932220"
  },
  {
    "text": "so as I said all it does is exactly as we had earlier let's look at this",
    "start": "2932220",
    "end": "2940150"
  },
  {
    "text": "it takes a bunch of text lines from a queue it splits them and as a explicit",
    "start": "2940150",
    "end": "2948290"
  },
  {
    "text": "them by spaces it flat flat maps the values into words in a longer stream and",
    "start": "2948290",
    "end": "2955760"
  },
  {
    "text": "then it just groups them by the key and the word right and then it prints out",
    "start": "2955760",
    "end": "2961070"
  },
  {
    "text": "the counts into a topic called something else right so this part is not interesting we've seen it like five times so far but the instant part is we",
    "start": "2961070",
    "end": "2968930"
  },
  {
    "text": "can actually now spin up a rest service and I'm going to show you how to spin up a restaurant in Java because you know if",
    "start": "2968930",
    "end": "2974359"
  },
  {
    "text": "you have to do it you already know and if you don't well you're in luck and you're going to live much better lives",
    "start": "2974359",
    "end": "2979520"
  },
  {
    "text": "and happier not to know how to do rest services in Java but basically we spin",
    "start": "2979520",
    "end": "2984530"
  },
  {
    "text": "up a rest service which can respond to requests and it has a very simple API",
    "start": "2984530",
    "end": "2989780"
  },
  {
    "text": "you can go to slash counts and the word itself will tell you how many times it says seen the word right so if we run",
    "start": "2989780",
    "end": "2996650"
  },
  {
    "text": "this we need to produce some words let's run the example we'll produce some words",
    "start": "2996650",
    "end": "3009780"
  },
  {
    "text": "the ABA ABC yes enough I think if we",
    "start": "3013539",
    "end": "3020720"
  },
  {
    "text": "read the topic we can see that will actually have a bunch of words right so now we read it and it's all from the",
    "start": "3020720",
    "end": "3025910"
  },
  {
    "text": "command line because that's the way Kappa operate it's going to have a bunch",
    "start": "3025910",
    "end": "3031369"
  },
  {
    "text": "of words but now the interesting part is we have a rest service on top of it listening to how many events we had so if you go into",
    "start": "3031369",
    "end": "3037910"
  },
  {
    "text": "our web app I we can call it and get a response and say that we have a 25 times",
    "start": "3037910",
    "end": "3044599"
  },
  {
    "text": "writing to go and look for the count for B there's 10 B's right so now we can",
    "start": "3044599",
    "end": "3049640"
  },
  {
    "text": "actually take a stream topology and process a lot of data in a very scalable manner processing our thousands or",
    "start": "3049640",
    "end": "3056239"
  },
  {
    "text": "millions potential events per second and if we store States we can now query this state as an application and this is the",
    "start": "3056239",
    "end": "3062599"
  },
  {
    "text": "direction a lot of streaming frameworks go towards where they can let you actually build applications on top of",
    "start": "3062599",
    "end": "3068420"
  },
  {
    "text": "town on top of your stream processing topology and expose them as services that someone else can consume and this",
    "start": "3068420",
    "end": "3074690"
  },
  {
    "text": "first of all in this case we've just replaced the database right if we can query our streaming topology directly we",
    "start": "3074690",
    "end": "3080509"
  },
  {
    "text": "don't need to put the data in database which actually eliminates one very complicated component from a complicated",
    "start": "3080509",
    "end": "3086299"
  },
  {
    "text": "system because you can actually now talk to our data directly and get a snapshot right away and also we actually have a",
    "start": "3086299",
    "end": "3095180"
  },
  {
    "text": "much more real-time view of what the data is doing right because it updates in real time so now let's look at a much",
    "start": "3095180",
    "end": "3101539"
  },
  {
    "text": "more interesting example where we can actually do what I said earlier and that's we're going to take tweets and we're going to analyze them and do some",
    "start": "3101539",
    "end": "3107479"
  },
  {
    "text": "sentiment analysis I'm going to store them and do a dashboard that shows you tweets in real time and I have exactly",
    "start": "3107479",
    "end": "3112849"
  },
  {
    "text": "ten minutes to do this so hopefully we'll make it just in time and yeah I'll tell you it took a bit more than ten",
    "start": "3112849",
    "end": "3118369"
  },
  {
    "text": "minutes just to actually set up the demo but not by a lot right because you have you can connect all these things like",
    "start": "3118369",
    "end": "3123650"
  },
  {
    "text": "Legos Seth looks at our example and I purposefully mixed and matched different",
    "start": "3123650",
    "end": "3129309"
  },
  {
    "text": "frameworks here you know obviously you would never do this in production you'd use just a single framework but what I'm",
    "start": "3129309",
    "end": "3135140"
  },
  {
    "text": "doing is I'm going to be using storm to read tweets sorry sparked retweet from",
    "start": "3135140",
    "end": "3143730"
  },
  {
    "text": "the Twitter firehouse and Twitter by the",
    "start": "3143730",
    "end": "3148740"
  },
  {
    "text": "way has been just a massive boon to anyone who's doing demos because it's a this the best source for streaming demo",
    "start": "3148740",
    "end": "3154890"
  },
  {
    "text": "demos you can have right you should get data and it's free its own you have an internet connection you can get it the",
    "start": "3154890",
    "end": "3161280"
  },
  {
    "text": "problem with it is you have to provide some keywords to filter tweet and I've shared a lot of keywords and most of",
    "start": "3161280",
    "end": "3166680"
  },
  {
    "text": "them don't produce any tweets but I finally figure out the keyword which produces the best tweets and it's Trump",
    "start": "3166680",
    "end": "3172440"
  },
  {
    "text": "and I have no personal stake in the election in the US but they've been great for my demos so what you're going",
    "start": "3172440",
    "end": "3180000"
  },
  {
    "text": "to find a bunch of tweets with obviously politically loaded keywords because it gets you a lot of tweets and most",
    "start": "3180000",
    "end": "3185820"
  },
  {
    "text": "importantly they're beautiful for sentiment analysis because there's a lot of emotion and sentiment in political",
    "start": "3185820",
    "end": "3190980"
  },
  {
    "text": "tweets so all we do is we actually use the built in plugin for course for spark",
    "start": "3190980",
    "end": "3196740"
  },
  {
    "text": "streaming which knows how to connect Twitter and actually stream tweets to you and we're going to be doing two",
    "start": "3196740",
    "end": "3202079"
  },
  {
    "text": "things we're going to aggregate them by window by about 10 second window and count hashtags and find the most",
    "start": "3202079",
    "end": "3208290"
  },
  {
    "text": "trending hashtags every 10 seconds and then we're going to take all of the tweets throughout most of the data save",
    "start": "3208290",
    "end": "3215310"
  },
  {
    "text": "about 6 different fields as a JSON object and put it in a database wine and",
    "start": "3215310",
    "end": "3220349"
  },
  {
    "text": "database well I work with a Jewish I have to plug casually somewhere in my demo at least once I think it's Mike",
    "start": "3220349",
    "end": "3226380"
  },
  {
    "text": "it's in my contract somewhere and also because we want to query them right and I've seen I've shown you how you don't",
    "start": "3226380",
    "end": "3232440"
  },
  {
    "text": "have to use a database with Casca and I now I want to show the other way where you can actually put us in a datastore and query it as a regular database and",
    "start": "3232440",
    "end": "3239369"
  },
  {
    "text": "then we'll take all the data do some sentiment analysis on it and put it in a hopefully a real-time updating graph so",
    "start": "3239369",
    "end": "3245190"
  },
  {
    "text": "that's from this and by the way really really shouldn't is going to show all the Switch on the screen please don't",
    "start": "3245190",
    "end": "3250349"
  },
  {
    "text": "read them they're terrible tweets they don't I've made the mistake of reading them because I was trying to find the",
    "start": "3250349",
    "end": "3256290"
  },
  {
    "text": "ones with the most positive and negative sentiment and it ruined my life basically all right so we have tweets",
    "start": "3256290",
    "end": "3264089"
  },
  {
    "text": "coming in if we go into our database will see that we are indeed getting tweets let's look over here and yeah we",
    "start": "3264089",
    "end": "3273359"
  },
  {
    "text": "get tweets like this something-something what we're interested in is text right so now what",
    "start": "3273359",
    "end": "3279810"
  },
  {
    "text": "you want to do is actually take these tweets do some sentiment analysis enrich",
    "start": "3279810",
    "end": "3284970"
  },
  {
    "text": "them with data and then put them back in a database so we can query them somewhere right now why don't we do this",
    "start": "3284970",
    "end": "3290340"
  },
  {
    "text": "first we foo before we put them in database because the processing might be very complex it might take a lot of time",
    "start": "3290340",
    "end": "3295380"
  },
  {
    "text": "and as we know tweets aren't rewind able if you lose the tweet if the processing",
    "start": "3295380",
    "end": "3300420"
  },
  {
    "text": "framework fails and before you store the tweet you're going to lose it forever so the first thing we want to do is store it durably and then we want to do all",
    "start": "3300420",
    "end": "3307230"
  },
  {
    "text": "the other processing stuff so you can either put them in a queue like Kafka or Amazon Canisius or something like that or put them in a database and then",
    "start": "3307230",
    "end": "3313770"
  },
  {
    "text": "stream them from that place and do all the processing and stream them back so trim them back we're going to use a",
    "start": "3313770",
    "end": "3319470"
  },
  {
    "text": "storm because why not but you can do either any one of those frameworks in this case we have a very",
    "start": "3319470",
    "end": "3326490"
  },
  {
    "text": "simple topology we have a spout which the streams data from in this case",
    "start": "3326490",
    "end": "3331500"
  },
  {
    "text": "calibration the database but it could be Casca or any kind of cue it filters out",
    "start": "3331500",
    "end": "3336660"
  },
  {
    "text": "anything that's not a tweet just so we don't you know put garbage in a database and then it calls the sentiment bolt",
    "start": "3336660",
    "end": "3342270"
  },
  {
    "text": "which takes it sweet calculates the sentiment score it attaches it and then source it back in a database so the",
    "start": "3342270",
    "end": "3348420"
  },
  {
    "text": "sentiment bolt itself looks very simple like this there is all right it takes a",
    "start": "3348420",
    "end": "3354630"
  },
  {
    "text": "tweet it uses the stanford natural language processing library which is free and open source and you should",
    "start": "3354630",
    "end": "3359640"
  },
  {
    "text": "definitely play it if you're into that which calculates a sentiment score from",
    "start": "3359640",
    "end": "3364710"
  },
  {
    "text": "0 to 4 0 is very negative 4 is very positive and 2 is neutral and it just",
    "start": "3364710",
    "end": "3371700"
  },
  {
    "text": "gives us an integer right and then we attach this integer back to the JSON object and store it back in a database so now some of the if some of the",
    "start": "3371700",
    "end": "3378660"
  },
  {
    "text": "objects will start getting sentiment scores and we now have actually two streams running in parallel one is storing tweets and the other is trying",
    "start": "3378660",
    "end": "3385230"
  },
  {
    "text": "to catch up in attach scores to every tweet right and because calculating scores is much more complex the stream",
    "start": "3385230",
    "end": "3391170"
  },
  {
    "text": "is actually going to run at different speeds because tweets and storing twitch is very easy and likely in some",
    "start": "3391170",
    "end": "3396420"
  },
  {
    "text": "sentiments takes a lot longer so this is the second stream the session second processing is what we need to scale out",
    "start": "3396420",
    "end": "3401670"
  },
  {
    "text": "to much more much many more machines if we want to actually catch up to the first stream which is just saving tweets",
    "start": "3401670",
    "end": "3408569"
  },
  {
    "text": "so let's run this there we go and we'll start actually reading all the old",
    "start": "3408569",
    "end": "3415019"
  },
  {
    "text": "tweets right because we've started saving tweets like five minutes ago we have a backlog now we can choose for the",
    "start": "3415019",
    "end": "3420630"
  },
  {
    "text": "backlog and then we'll get to real-time and start getting the real line tweets and it's going to spin up and do a bunch of stuff and now we can go into our",
    "start": "3420630",
    "end": "3429299"
  },
  {
    "text": "database and actually do some queries on the tweets right so we can do something like select from tweets",
    "start": "3429299",
    "end": "3444739"
  },
  {
    "text": "come all right so let's say the app what the average sentiment score is for regular twitch which don't mention Trump",
    "start": "3451369",
    "end": "3457950"
  },
  {
    "text": "and if we look here we can see that it is in fact let's expand this a bit I",
    "start": "3457950",
    "end": "3463050"
  },
  {
    "text": "think like Cougars is about to explode I'm doing way too many things all two ones here all right",
    "start": "3463050",
    "end": "3469680"
  },
  {
    "text": "that's about 1.4 1.3 which is on the negative side but it's it's somewhere",
    "start": "3469680",
    "end": "3475800"
  },
  {
    "text": "view negative and neutral and if we just change this to or have to stop this",
    "start": "3475800",
    "end": "3481440"
  },
  {
    "text": "because things are going to explode less as I said natural language processing",
    "start": "3481440",
    "end": "3486510"
  },
  {
    "text": "takes a lot of CPU power and that's the part we need to actually scale so stop this and then go back to our queering",
    "start": "3486510",
    "end": "3495890"
  },
  {
    "text": "okay it's going to respond eventually vary there we go that's better",
    "start": "3498050",
    "end": "3504000"
  },
  {
    "text": "all right so let's run this and we can see that if we do include those then the tweets are obviously much more negative",
    "start": "3504000",
    "end": "3509910"
  },
  {
    "text": "which really shouldn't come as a surprise to anyone but it's interesting to be proven right all right so as it",
    "start": "3509910",
    "end": "3519210"
  },
  {
    "text": "happens and we don't have a lot of time so I'm going to show you how but I'm also replicating tweets in the background for my datastore to",
    "start": "3519210",
    "end": "3525180"
  },
  {
    "text": "elasticsearch who's heard of elasticsearch hopefully everyone like half the room ok good lesson church is",
    "start": "3525180",
    "end": "3530490"
  },
  {
    "text": "an engine for doing full text analysis and real-time queries on data and it has a plug-in called Cubana which does",
    "start": "3530490",
    "end": "3536940"
  },
  {
    "text": "real-time dashboards really nicely and very simple to setup so in the background I have a job running and",
    "start": "3536940",
    "end": "3542520"
  },
  {
    "text": "replicating data from our data store to my elasticsearch and I've set up a",
    "start": "3542520",
    "end": "3548310"
  },
  {
    "text": "dashboard in elasticsearch to actually show our tweets on the map if they have a location and show the number of tweets",
    "start": "3548310",
    "end": "3554520"
  },
  {
    "text": "on a nice histogram and the graph of sentiments and notice the top most prolific users and this is just binding",
    "start": "3554520",
    "end": "3561660"
  },
  {
    "text": "by way of sharing an example that streaming frameworks are actually excellent ETL tools right you can stream",
    "start": "3561660",
    "end": "3567599"
  },
  {
    "text": "data from one place do some kind of transformation in real-time and stream it to another place every framework that",
    "start": "3567599",
    "end": "3574020"
  },
  {
    "text": "I know of has at least three common plugins there's a plugin for reading and writing data to HDFS",
    "start": "3574020",
    "end": "3579660"
  },
  {
    "text": "Amazon s2 s3 and elastic search let's search is always in top three things where you want to put your data all",
    "start": "3579660",
    "end": "3585780"
  },
  {
    "text": "right that's the first thing most framework builders actually implement as you can see tweets are still coming in",
    "start": "3585780",
    "end": "3591630"
  },
  {
    "text": "right we have shown to it for the last 15 minutes and refreshes every five seconds and I can see the some of the",
    "start": "3591630",
    "end": "3597480"
  },
  {
    "text": "tweets happening and then you have bars coming in and the sentiment graph has stopped it's not actually moving on",
    "start": "3597480",
    "end": "3602580"
  },
  {
    "text": "because I stopped the topology which calculates sentiment but the apology that brings into it is still going right",
    "start": "3602580",
    "end": "3607950"
  },
  {
    "text": "and I cannot go back and rerun the sentiment analysis and this is one of the benefits of persistence queues and",
    "start": "3607950",
    "end": "3614160"
  },
  {
    "text": "persistent stores that I can go back and actually change the way I calculate things in parallel so let's say we have",
    "start": "3614160",
    "end": "3621270"
  },
  {
    "text": "the sentiment apology running all the time and I change the algorithm I now calculate sentiments differently but I don't want to stop showing my",
    "start": "3621270",
    "end": "3628140"
  },
  {
    "text": "graph right now I wanted to keep running and I can spin up a second copy of the topology with the new algorithm and run",
    "start": "3628140",
    "end": "3634560"
  },
  {
    "text": "from the beginning of my log over all the tweets stream all of them from the beginning recalculate and when the",
    "start": "3634560",
    "end": "3640710"
  },
  {
    "text": "second apology catches up to the real-time I can turn the first one off and then I will have seamlessly actually",
    "start": "3640710",
    "end": "3646110"
  },
  {
    "text": "transitions from one type of processing to another without any downtime and without losing actual real-time data all",
    "start": "3646110",
    "end": "3652830"
  },
  {
    "text": "right so in this case as you can see the end of this graph is actually moving further in time because we stopped processing it all right I'm done exactly",
    "start": "3652830",
    "end": "3660420"
  },
  {
    "text": "on time which is a new thing for me because I like demos and I have a lot of demos hopefully you've learned at least",
    "start": "3660420",
    "end": "3667380"
  },
  {
    "text": "something interesting and thank you very much for coming and enjoy the rest of the conference [Applause]",
    "start": "3667380",
    "end": "3673470"
  },
  {
    "text": "[Music] [Applause]",
    "start": "3673470",
    "end": "3679669"
  }
]