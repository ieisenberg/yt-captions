[
  {
    "text": "um thank you all for coming if any of you were here yesterday and saw me struggle with some technical",
    "start": "4680",
    "end": "11080"
  },
  {
    "text": "difficulties and came here today to see me struggle with the same technical difficulties I'm sorry we made it work",
    "start": "11080",
    "end": "19000"
  },
  {
    "text": "um so I'm Tess I work at Microsoft and I've been working with llms for about",
    "start": "19000",
    "end": "25039"
  },
  {
    "text": "the last year or so um and I'm sure that most of you probably have used an llm",
    "start": "25039",
    "end": "32078"
  },
  {
    "text": "everyone I would guess like at least 99% or something um and you probably know that",
    "start": "32079",
    "end": "39320"
  },
  {
    "text": "llms are very good by the way the subtitle of this if you don't know why you're here is ragging on rag we're",
    "start": "39320",
    "end": "45960"
  },
  {
    "text": "going to be talking about Rags if you don't know what a rag is you'll be acutely aware after this session if you",
    "start": "45960",
    "end": "52120"
  },
  {
    "text": "don't know what ragging is you'll also be very aware after this session what that means um but we all know that uh",
    "start": "52120",
    "end": "59280"
  },
  {
    "text": "llms are very very good at summarizing things um like summarizing a paper or",
    "start": "59280",
    "end": "66640"
  },
  {
    "text": "generating a story so in this case generating a story about a dog that gets on with a zebra and um appropriate for",
    "start": "66640",
    "end": "75000"
  },
  {
    "text": "age five because this is what language models do this is what language models have done before they were called large",
    "start": "75000",
    "end": "81159"
  },
  {
    "text": "language models they were kind of predicting the next word to put together in a sentence to make the sentence look",
    "start": "81159",
    "end": "87000"
  },
  {
    "text": "good and generally working with text but they're not all that good at data so",
    "start": "87000",
    "end": "94240"
  },
  {
    "text": "when I ask it a question like what are my hobbies which it won't know because I",
    "start": "94240",
    "end": "100360"
  },
  {
    "text": "haven't publicized them on the Internet it will make up a story both about who I",
    "start": "100360",
    "end": "106840"
  },
  {
    "text": "am and what my what my uh Hobbies or that it doesn't know my hobbies but it",
    "start": "106840",
    "end": "113200"
  },
  {
    "text": "will sometimes fabricate a little bit weirdly so in this case it says that I'm",
    "start": "113200",
    "end": "118920"
  },
  {
    "text": "a Microsoft MV which I'm not I'm I worked at Microsoft for the last 25 years but this is not a",
    "start": "118920",
    "end": "125479"
  },
  {
    "text": "huge oversight I mean a m an MVP versus an employee is not that big but I tried",
    "start": "125479",
    "end": "131879"
  },
  {
    "text": "to ask it again and it came up with this Tess fernandis who was a Microsoft",
    "start": "131879",
    "end": "137879"
  },
  {
    "text": "employee and a software developer passed away in 2014 so this startled me a little bit",
    "start": "137879",
    "end": "146280"
  },
  {
    "text": "actually when I read it um I will say the reports of my death have been greatly exaggerated I might have stopped",
    "start": "146280",
    "end": "153280"
  },
  {
    "text": "blogging maybe that was my problem but but I'm very much here and this is not a",
    "start": "153280",
    "end": "158680"
  },
  {
    "text": "hologram um so obviously it can come up with both things that are not correct",
    "start": "158680",
    "end": "165560"
  },
  {
    "text": "and maybe also not very tonally correct um but uh and and",
    "start": "165560",
    "end": "174400"
  },
  {
    "text": "also if we give an example of math and this is like a classic thing that everyone tries give it some math problem",
    "start": "174400",
    "end": "181840"
  },
  {
    "text": "if you give it 1 plus one it will obviously know because statistically in the data that it has been trained on",
    "start": "181840",
    "end": "187040"
  },
  {
    "text": "that shows up a lot but this random number that I'm putting Not So Random actually 1 2 3 4 5 6 Seven 8 nine but",
    "start": "187040",
    "end": "195239"
  },
  {
    "text": "this random number it will just answer confidently that this is the product but",
    "start": "195239",
    "end": "201599"
  },
  {
    "text": "actually it's not close but no cigar and this is a little bit a problem of llms",
    "start": "201599",
    "end": "208519"
  },
  {
    "text": "in general that they are not actually proper databases it's not like we're",
    "start": "208519",
    "end": "213640"
  },
  {
    "text": "actually extracting facts from them we're extracting or we're predicting the next word that could come out that may",
    "start": "213640",
    "end": "221280"
  },
  {
    "text": "or may not be factually correct in in real in reality an llm is a",
    "start": "221280",
    "end": "226959"
  },
  {
    "text": "probabilistic database and if you've never seen a probabilistic database before this is about how they work so",
    "start": "226959",
    "end": "233519"
  },
  {
    "text": "let's say we have a list of companies and and their Industries so this is a just a table of companies and Industries",
    "start": "233519",
    "end": "240439"
  },
  {
    "text": "and now you go through and you ask what industry does Scandinavian Airline belong",
    "start": "240439",
    "end": "246439"
  },
  {
    "text": "to I think most of us in here think it's an airline company flight",
    "start": "246439",
    "end": "252000"
  },
  {
    "text": "travel but this database would think it's software because statistically 75%",
    "start": "252000",
    "end": "258079"
  },
  {
    "text": "it would be software and this is how a probabilistic database work and this is also sort of sorry how um llms work it",
    "start": "258079",
    "end": "266000"
  },
  {
    "text": "will take whatever is statistically reasonable as the answer now",
    "start": "266000",
    "end": "271160"
  },
  {
    "text": "probabilistic databases they are used to clean data so that you don't put in sort of an age of 1902 or something like that",
    "start": "271160",
    "end": "279039"
  },
  {
    "text": "but they're not generally used for Gather like for taking or asking data",
    "start": "279039",
    "end": "285240"
  },
  {
    "text": "from so what we've learned is that we can go through and we can give it some",
    "start": "285240",
    "end": "291199"
  },
  {
    "text": "context so I can tell the LM in my query that I love uh pasta bers and cats can",
    "start": "291199",
    "end": "298680"
  },
  {
    "text": "you give me some recipes well hopefully not of past and",
    "start": "298680",
    "end": "304360"
  },
  {
    "text": "cats um and it gives me some some recipes that kind of resonate with",
    "start": "304360",
    "end": "311440"
  },
  {
    "text": "this so we can ground it with some data and we can get out some some other",
    "start": "311440",
    "end": "317039"
  },
  {
    "text": "information based on that data so this is something that uh Patrick Lewis and",
    "start": "317039",
    "end": "324360"
  },
  {
    "text": "his colleagues kind of took to and created this thing called retrieval augmented",
    "start": "324360",
    "end": "330199"
  },
  {
    "text": "generation how many people in here know what retrieval augmented generation or rag is yeah so fewer than I thought",
    "start": "330199",
    "end": "340319"
  },
  {
    "text": "actually but that's good because we're actually going to go through what uh retrieval augmented generation is and",
    "start": "340319",
    "end": "347479"
  },
  {
    "text": "also how we can optimize it and definitely when we shouldn't use it um",
    "start": "347479",
    "end": "353600"
  },
  {
    "text": "so the rest of the session will be how to create uh models that can actually reason about data",
    "start": "353600",
    "end": "360160"
  },
  {
    "text": "and when you shouldn't do that um so retrieval augmented generation Works a",
    "start": "360160",
    "end": "366560"
  },
  {
    "text": "little bit like this you have a question that comes from the user and then based",
    "start": "366560",
    "end": "371960"
  },
  {
    "text": "on that question you go through and you ask a database for some appropriate data",
    "start": "371960",
    "end": "377880"
  },
  {
    "text": "so you will have a database with data that it can use to then generate an answer from so essentially what I did",
    "start": "377880",
    "end": "385039"
  },
  {
    "text": "before where I said hi I'm test I love past use we're going to tell",
    "start": "385039",
    "end": "390160"
  },
  {
    "text": "the llm basically all the data it needs to be able to answer the question that the user",
    "start": "390160",
    "end": "395639"
  },
  {
    "text": "has so we searched the datab base we get some data out we create a prompt from from the",
    "start": "395639",
    "end": "403639"
  },
  {
    "text": "data um and the prompt may look very different depending on um where you get",
    "start": "403639",
    "end": "410120"
  },
  {
    "text": "it from or if you fabricate it yourself but the prompt will look something like this using the provided use the provided",
    "start": "410120",
    "end": "416080"
  },
  {
    "text": "context to answer the user's question you may not answer the user query unless it's specific uh unless the specific context",
    "start": "416080",
    "end": "424440"
  },
  {
    "text": "is in the following of text and if you don't know the answer or cannot answer please reply what I don't know so it",
    "start": "424440",
    "end": "429919"
  },
  {
    "text": "doesn't just hallucinate and come up with its own answer I'm using the word",
    "start": "429919",
    "end": "435160"
  },
  {
    "text": "hallucinate because this is something that you'll hear a lot about llms and hallucinate means basically making up a",
    "start": "435160",
    "end": "441800"
  },
  {
    "text": "fact that didn't exist and then we give it some data that we've gotten from the database along",
    "start": "441800",
    "end": "448560"
  },
  {
    "text": "with a question and then the llm will answer based on that and as you saw with my ping yes",
    "start": "448560",
    "end": "456160"
  },
  {
    "text": "thing it answered a lot more confident like a lot better than if I would would",
    "start": "456160",
    "end": "461280"
  },
  {
    "text": "have just asked it what I wanted for lunch without context",
    "start": "461280",
    "end": "466400"
  },
  {
    "text": "so um the way we do the searching is generally through something",
    "start": "466400",
    "end": "471800"
  },
  {
    "text": "called embeddings and I'm going to go through and talk about what embeddings are because this is sort of like at the",
    "start": "471800",
    "end": "477759"
  },
  {
    "text": "core of these rag systems and I would say rag is",
    "start": "477759",
    "end": "483599"
  },
  {
    "text": "currently probably like 75 of the applications that people do with llms are rag",
    "start": "483599",
    "end": "489680"
  },
  {
    "text": "systems I think that will change over the next year because of the problems that we're seeing with rag but that is",
    "start": "489680",
    "end": "496400"
  },
  {
    "text": "currently what's what we're using and we're doing something called a semantic search using these",
    "start": "496400",
    "end": "501479"
  },
  {
    "text": "embeddings so embeddings embeddings are like the secret source of deep learning",
    "start": "501479",
    "end": "507360"
  },
  {
    "text": "so this is a convolutional neural network you don't have to worry about knowing what a convolution or a pool is",
    "start": "507360",
    "end": "514360"
  },
  {
    "text": "or what this array is um but at the end what happens is um we will be able to",
    "start": "514360",
    "end": "521000"
  },
  {
    "text": "determine if something is a Chihuahua or a muffin if we create a neural network a",
    "start": "521000",
    "end": "526839"
  },
  {
    "text": "convolutional neuron Network that um figures this out convolutional neuron",
    "start": "526839",
    "end": "532120"
  },
  {
    "text": "networks are used for images if this was a deep Learning Network that wasn't used",
    "start": "532120",
    "end": "538279"
  },
  {
    "text": "for images instead would we wouldn't have convolutions we would have like a bunch of nodes um that we would",
    "start": "538279",
    "end": "544959"
  },
  {
    "text": "teach so this thing in the beginning like the convolutions and the pooling they're essentially extracting features",
    "start": "544959",
    "end": "551680"
  },
  {
    "text": "so they're learning filters to put on top of images and they will find filters",
    "start": "551680",
    "end": "557079"
  },
  {
    "text": "or features like this lines or colors or putting together patterns so um",
    "start": "557079",
    "end": "563680"
  },
  {
    "text": "something here for example we can see this looks like a little bit like a bird or a beehive or maybe a tire or",
    "start": "563680",
    "end": "571000"
  },
  {
    "text": "something like that and then depending on how much of this exists in the image",
    "start": "571000",
    "end": "576640"
  },
  {
    "text": "it then creates an array an array that might be um you choose the length",
    "start": "576640",
    "end": "582560"
  },
  {
    "text": "basically but each number in this array is between minus one and one and it kind",
    "start": "582560",
    "end": "588320"
  },
  {
    "text": "of tells you how much of a tire exists in this or how much of that beehive",
    "start": "588320",
    "end": "594880"
  },
  {
    "text": "exists in this so this what we see down here is the array which is sort of like",
    "start": "594880",
    "end": "600079"
  },
  {
    "text": "the embedding or the secret code or intermediate representation this is the",
    "start": "600079",
    "end": "605120"
  },
  {
    "text": "core of why we do deep learning this is what we want to figure out because this",
    "start": "605120",
    "end": "610160"
  },
  {
    "text": "is going to help us identify if it's a Chihuahua or a muffin and the way this works is if we picture that we want to",
    "start": "610160",
    "end": "617920"
  },
  {
    "text": "recommend a book and I have a large number of books and I have a daughter",
    "start": "617920",
    "end": "623920"
  },
  {
    "text": "and a husband and they might want a book recommendation from me so now I say okay",
    "start": "623920",
    "end": "629839"
  },
  {
    "text": "so in that case I might sort them like this some are for older people some are for younger people and maybe my daughter",
    "start": "629839",
    "end": "636240"
  },
  {
    "text": "wants something for the younger people and my husband wants something for the older people and maybe that's good or",
    "start": "636240",
    "end": "642360"
  },
  {
    "text": "maybe we want to kind of add another dimension to this and say is it fiction",
    "start": "642360",
    "end": "647639"
  },
  {
    "text": "or is it not fiction and we can now see in this coordinate system that these",
    "start": "647639",
    "end": "653000"
  },
  {
    "text": "books now get an array of numbers or actually a coordinate in this two-dimensional coordinate system",
    "start": "653000",
    "end": "660480"
  },
  {
    "text": "so we can also see that books that are very close together like Hunger Games",
    "start": "660480",
    "end": "665800"
  },
  {
    "text": "the Divergent they end up very close in this coordinate system and this is sort",
    "start": "665800",
    "end": "671880"
  },
  {
    "text": "of the key to embeddings that you can do similarity searches yes based on where they are in the coordinate system they",
    "start": "671880",
    "end": "678720"
  },
  {
    "text": "will either be similar or not similar so you can use them for recommendations or",
    "start": "678720",
    "end": "684160"
  },
  {
    "text": "or figuring out uh in a search so obviously these were two Dimensions but",
    "start": "684160",
    "end": "690320"
  },
  {
    "text": "in reality will add a lot more Dimensions like math references or if it's us Centric or shitet or funny or",
    "start": "690320",
    "end": "697480"
  },
  {
    "text": "sci-fi or lawyers or would Brad Pit play a character in a movie it could be any",
    "start": "697480",
    "end": "702800"
  },
  {
    "text": "sort of thing and in fact we don't know what it is because the Deep learning networks will come up with features that",
    "start": "702800",
    "end": "709519"
  },
  {
    "text": "are not humanly relatable it it's features that we would never come up",
    "start": "709519",
    "end": "714600"
  },
  {
    "text": "with ourselves but that the machine learning model or that a deep learning model learns",
    "start": "714600",
    "end": "720000"
  },
  {
    "text": "and this now becomes our embedding so we also do this not for IM",
    "start": "720000",
    "end": "726519"
  },
  {
    "text": "not only for images and items but also for faces for example so this is an",
    "start": "726519",
    "end": "731760"
  },
  {
    "text": "embedding of this picture of my face and this is something that was used in face net um in 2015 so this is quite old like",
    "start": "731760",
    "end": "740480"
  },
  {
    "text": "this embedding thing where um what you do is like you have the face net model",
    "start": "740480",
    "end": "747000"
  },
  {
    "text": "you extract your embedding you put your embedding in the database and then you take another picture of yourself and it",
    "start": "747000",
    "end": "754000"
  },
  {
    "text": "checks which one is it closer to and the person that or the hit it finds that",
    "start": "754000",
    "end": "759480"
  },
  {
    "text": "it's this image embedding is closest to that's the person it thinks it is if",
    "start": "759480",
    "end": "764800"
  },
  {
    "text": "it's close enough because you might also have determined that they should be",
    "start": "764800",
    "end": "770360"
  },
  {
    "text": "within a limit close to each other so here we can see for example I'm the San",
    "start": "770360",
    "end": "775760"
  },
  {
    "text": "person up here and you can see that all of my pictures neatly fit together in",
    "start": "775760",
    "end": "781800"
  },
  {
    "text": "this two-dimensional um projection of",
    "start": "781800",
    "end": "786880"
  },
  {
    "text": "128 dimensional embedding and we can also see that for example my features maybe are quite similar to Tom Cruz and",
    "start": "786880",
    "end": "794920"
  },
  {
    "text": "Sandra Bullock but not so much to Bill Clinton or Serena",
    "start": "794920",
    "end": "799959"
  },
  {
    "text": "Williams but this is something that's super useful for similarity seches so we",
    "start": "799959",
    "end": "805399"
  },
  {
    "text": "can do this with images faces um but we can also do this with with words and the",
    "start": "805399",
    "end": "810720"
  },
  {
    "text": "way it works with words is like this and by the way everyone who talks about word embeddings is going to talk about man",
    "start": "810720",
    "end": "817120"
  },
  {
    "text": "and woman and king and queen because this comes from a very old paper on embeddings I think from 2013 or",
    "start": "817120",
    "end": "825120"
  },
  {
    "text": "2011 so what we we can see from this image if we extracted the embeddings for",
    "start": "825120",
    "end": "830880"
  },
  {
    "text": "these words we can also see something interesting happening where if we look at the direction between man and woman",
    "start": "830880",
    "end": "838040"
  },
  {
    "text": "an uncle and Aunt and king and queen the directions between these arrows are very",
    "start": "838040",
    "end": "843800"
  },
  {
    "text": "very similar and this direction or this Vector between them kind of explains",
    "start": "843800",
    "end": "851360"
  },
  {
    "text": "female you know the longer you are along this Vector the more female the sentiment of the word probably is it's",
    "start": "851360",
    "end": "858920"
  },
  {
    "text": "not going to be exact and these dots are a little bit too exact because you know Queen may or may not be female uh and it",
    "start": "858920",
    "end": "866920"
  },
  {
    "text": "they may or may not be royal um but as far as like the statistical meaning of them they're probably female and same",
    "start": "866920",
    "end": "874600"
  },
  {
    "text": "thing if we go for uh man and King woman and queen we can also see that we can",
    "start": "874600",
    "end": "879880"
  },
  {
    "text": "extract the sentiment Royal from it so all of these is what the large",
    "start": "879880",
    "end": "886920"
  },
  {
    "text": "language model learns or like the embedding models learn they learn to extract the semantic meaning of a word",
    "start": "886920",
    "end": "895240"
  },
  {
    "text": "or of a piece of text so we can basically enter any any piece of text and it will extract things",
    "start": "895240",
    "end": "902800"
  },
  {
    "text": "um it will extract all these features and tell you how much of each they are so once we know what these",
    "start": "902800",
    "end": "910680"
  },
  {
    "text": "embeddings are the idea behind how this search happens is this we bring in a",
    "start": "910680",
    "end": "916279"
  },
  {
    "text": "number of documents so this is like our core data that we want to feed to the llm and then we Shunk them up because",
    "start": "916279",
    "end": "924519"
  },
  {
    "text": "llms don't necessarily have you can't necessarily put the whole do doucment in",
    "start": "924519",
    "end": "930000"
  },
  {
    "text": "the prompt for the llm specifically if you have very long documents so you might just want to put some very small",
    "start": "930000",
    "end": "937120"
  },
  {
    "text": "chunks very precise chunks of data that you pull out of the database and then we take the embedding",
    "start": "937120",
    "end": "944480"
  },
  {
    "text": "of this chunk so the semantic meaning of this and we store it in a vector",
    "start": "944480",
    "end": "949560"
  },
  {
    "text": "database so this is what we do as preparation and then on the other end we",
    "start": "949560",
    "end": "954680"
  },
  {
    "text": "go through the whole thing where we take the question and and we compare the question to the potential shunks in",
    "start": "954680",
    "end": "963160"
  },
  {
    "text": "here so let's see how this looks in in in an",
    "start": "963160",
    "end": "969720"
  },
  {
    "text": "example so here we have and I'm sorry if you you're not a python person but um",
    "start": "970319",
    "end": "977240"
  },
  {
    "text": "this is going to be python hopefully you can still read it um somewhat so I'm using Python and I'm using um a",
    "start": "977240",
    "end": "985839"
  },
  {
    "text": "framework called languishing there are multiple Frameworks for building llm applications and rag systems like",
    "start": "985839",
    "end": "992959"
  },
  {
    "text": "semantic kernel or llama index or a number of these but Lang Shain is a very",
    "start": "992959",
    "end": "998079"
  },
  {
    "text": "common one that's used for python so we're read in the docs and I'm actually",
    "start": "998079",
    "end": "1003319"
  },
  {
    "text": "also extracting some metadata so the data that I'm looking at here is actually this we have an engineering",
    "start": "1003319",
    "end": "1010560"
  },
  {
    "text": "fundamentals repository where we write things about agile",
    "start": "1010560",
    "end": "1017040"
  },
  {
    "text": "or um things like automated testing let's",
    "start": "1017040",
    "end": "1023120"
  },
  {
    "text": "see well anyways about automated testing and cicd and all sorts of things and",
    "start": "1023120",
    "end": "1028959"
  },
  {
    "text": "this is what I'm reading in here I have all of these files in markdown format so",
    "start": "1028959",
    "end": "1035280"
  },
  {
    "text": "if I go into something like machine learning I have like ml data exploration and",
    "start": "1035280",
    "end": "1041038"
  },
  {
    "text": "things so I'm reading this and I'm also um checking the structure and then",
    "start": "1041039",
    "end": "1046918"
  },
  {
    "text": "I'm going through and I'm doing shuning so this was the second part so for this for this specific example I'm using",
    "start": "1046919",
    "end": "1053320"
  },
  {
    "text": "something called a markdown text splitter and I'll go through what shuning how that works in in a second in",
    "start": "1053320",
    "end": "1060080"
  },
  {
    "text": "a PowerPoint but this basically then divides the documents up into parts and",
    "start": "1060080",
    "end": "1065840"
  },
  {
    "text": "then we go through and we embed the docs so for embedding we use an embedding",
    "start": "1065840",
    "end": "1071200"
  },
  {
    "text": "model in this case I'm using an open AI embedding model but there are many many different embedding models used for",
    "start": "1071200",
    "end": "1077640"
  },
  {
    "text": "different purposes so I'm using this and if we look at an embedding for example of the yes the",
    "start": "1077640",
    "end": "1083520"
  },
  {
    "text": "sentence hello world it will then spit out and embedding like an array that's",
    "start": "1083520",
    "end": "1089159"
  },
  {
    "text": "15 1,536 numbers long and looks something",
    "start": "1089159",
    "end": "1095000"
  },
  {
    "text": "like this so we saw it um similarly to when I was showing the embedding of my face and",
    "start": "1095000",
    "end": "1102799"
  },
  {
    "text": "then I'm adding all this to a vector database and there are plenty of vector",
    "start": "1102799",
    "end": "1107880"
  },
  {
    "text": "databases right right now I'm using Asher AI search but you can use in memory Vector databases and all sorts of",
    "start": "1107880",
    "end": "1114799"
  },
  {
    "text": "things so they all work pretty much the same they have a few different features",
    "start": "1114799",
    "end": "1120080"
  },
  {
    "text": "depending on which so this is what we do to to sort of get the data into the",
    "start": "1120080",
    "end": "1126520"
  },
  {
    "text": "system and then if we look at something like I ask a query of this like what are",
    "start": "1126520",
    "end": "1131760"
  },
  {
    "text": "the AG ceremonies it will then spit out the most like the shunks that are most",
    "start": "1131760",
    "end": "1138360"
  },
  {
    "text": "similar to the query what are the agile ceremonies um in fact if we go in and",
    "start": "1138360",
    "end": "1145760"
  },
  {
    "text": "look here at the this is the AI search database and I ask it something like",
    "start": "1145760",
    "end": "1152840"
  },
  {
    "text": "what tests should I run on an ml",
    "start": "1152840",
    "end": "1161039"
  },
  {
    "text": "model it will then spit back hopefully a number of documents um",
    "start": "1161039",
    "end": "1169400"
  },
  {
    "text": "see if so that contains something about model load predict let's see I wanted to",
    "start": "1169400",
    "end": "1175919"
  },
  {
    "text": "minimize this but see if we can scroll",
    "start": "1175919",
    "end": "1180840"
  },
  {
    "text": "down I think because I um it's so big I can't well anyways",
    "start": "1183799",
    "end": "1191520"
  },
  {
    "text": "trust me that this document is probably from the ml testing document and will it",
    "start": "1191520",
    "end": "1197720"
  },
  {
    "text": "will have sort of like 10 or depending on how many you want from your query it will have 10 or 20 shunks that it can",
    "start": "1197720",
    "end": "1206280"
  },
  {
    "text": "pull out from AI search and then if we go back so now we have the data in there and then to query",
    "start": "1206280",
    "end": "1212960"
  },
  {
    "text": "the data we basically set up the same things so we have the AI search and the",
    "start": "1212960",
    "end": "1218880"
  },
  {
    "text": "similarity and then we build something called here a conversation retrieval Shain this is something that exists in",
    "start": "1218880",
    "end": "1225559"
  },
  {
    "text": "Lang chain you have similar things in the other framework models and we put in the after AI search as the",
    "start": "1225559",
    "end": "1233039"
  },
  {
    "text": "retriever so the only thing we then so this is essentially how you set up like",
    "start": "1233039",
    "end": "1238280"
  },
  {
    "text": "the whole rag chain and it will automatically create the prompts and everything for you and then if I search",
    "start": "1238280",
    "end": "1245159"
  },
  {
    "text": "for what test should we run on machine learning code I get out an answer that looks like this where now the llm has",
    "start": "1245159",
    "end": "1253440"
  },
  {
    "text": "synthe synthesized all the information from the shunks that I used and and um",
    "start": "1253440",
    "end": "1259400"
  },
  {
    "text": "and giving them back to me in sort of like a natural language format I also like to do something a",
    "start": "1259400",
    "end": "1266559"
  },
  {
    "text": "little bit and and this is something I do suggest that you use if you're working in Python uh with these llm",
    "start": "1266559",
    "end": "1274120"
  },
  {
    "text": "things this is a tool called Shan bot so Shan bot is a library from in Python",
    "start": "1274120",
    "end": "1279600"
  },
  {
    "text": "that allows you basically to create a user interface for your llm experiments",
    "start": "1279600",
    "end": "1285640"
  },
  {
    "text": "and now just added the exact same code that we saw but to Shan lot just to show",
    "start": "1285640",
    "end": "1291240"
  },
  {
    "text": "you really quick how Shan lot Works um basically you just Implement an on",
    "start": "1291240",
    "end": "1298120"
  },
  {
    "text": "message so when you get a message from the user you just take it and send it off to the",
    "start": "1298120",
    "end": "1304039"
  },
  {
    "text": "Shain and you get this whole user interface for free so now I if I say um",
    "start": "1304039",
    "end": "1310520"
  },
  {
    "text": "what tests should we run on ML",
    "start": "1310520",
    "end": "1316400"
  },
  {
    "text": "models um it now goes through and the search and also synthesizing the the",
    "start": "1316400",
    "end": "1322960"
  },
  {
    "text": "results I'm using the gbt 35 model for the results for this and we",
    "start": "1322960",
    "end": "1329640"
  },
  {
    "text": "see it came back with approximately the same thing that we saw in the notebook and I'm also displaying like",
    "start": "1329640",
    "end": "1337159"
  },
  {
    "text": "the source documents here so we can see where it got all its information but I",
    "start": "1337159",
    "end": "1342720"
  },
  {
    "text": "find this is quite useful is to have real users test the system and you can",
    "start": "1342720",
    "end": "1348200"
  },
  {
    "text": "actually use chain lit in production if you want to as well",
    "start": "1348200",
    "end": "1353840"
  },
  {
    "text": "um so that was our little rag",
    "start": "1353840",
    "end": "1360600"
  },
  {
    "text": "demo um and these were the tools so now when we have the rag demo it's quite",
    "start": "1360600",
    "end": "1367360"
  },
  {
    "text": "tempting to go from this seemingly really nice question and answer machine",
    "start": "1367360",
    "end": "1375200"
  },
  {
    "text": "to um artificially general intelligence like you it's almost yeah there's no",
    "start": "1375200",
    "end": "1382919"
  },
  {
    "text": "step between there at all or at least production because I mean what else is there that we need to do but it turns",
    "start": "1382919",
    "end": "1390159"
  },
  {
    "text": "out that unless unlike sort of software products where we create a park and we",
    "start": "1390159",
    "end": "1395520"
  },
  {
    "text": "know it can be super super hard to figure out like how long it will take us from this park to an actual production",
    "start": "1395520",
    "end": "1402480"
  },
  {
    "text": "system we know that there's a whole lot we need to do but we at least have a general idea that if we can do it in",
    "start": "1402480",
    "end": "1408640"
  },
  {
    "text": "Park we can probably also do it in real life and we know that yeah setting up",
    "start": "1408640",
    "end": "1414159"
  },
  {
    "text": "the real infra setting up all the test and everything it will take some time but we have a ballpark of how long time",
    "start": "1414159",
    "end": "1421080"
  },
  {
    "text": "this will take in this case we have nothing like that we don't even know",
    "start": "1421080",
    "end": "1426159"
  },
  {
    "text": "from this from this small Park which took us five minutes to actually figuring out if this",
    "start": "1426159",
    "end": "1433200"
  },
  {
    "text": "is even doable in production and this is one of the problems that you kind of always have with machine learning and",
    "start": "1433200",
    "end": "1440320"
  },
  {
    "text": "and also with llms because the problem is that when we look at the real data or when we look at",
    "start": "1440320",
    "end": "1447240"
  },
  {
    "text": "the real sort of situation it might turn out that the data that we had for this",
    "start": "1447240",
    "end": "1452559"
  },
  {
    "text": "small Park was just you know nice data but if you went to Mal and Matilda's",
    "start": "1452559",
    "end": "1459279"
  },
  {
    "text": "talk about PL PR yesterday you saw that going through and waiting through PDFs",
    "start": "1459279",
    "end": "1466000"
  },
  {
    "text": "with tables where all the tables look different in all the different documents or or they might have images or",
    "start": "1466000",
    "end": "1472360"
  },
  {
    "text": "handwritten text and and all that sort of stuff it's a nightmare to even ingest the data or figuring out if the data you",
    "start": "1472360",
    "end": "1479120"
  },
  {
    "text": "have will actually answer the questions that that you want answered because if",
    "start": "1479120",
    "end": "1484919"
  },
  {
    "text": "you're not giving it the proper data it will never be able to answer the questions and then shuning which will",
    "start": "1484919",
    "end": "1492440"
  },
  {
    "text": "look at in a bit also matters a lot how big the shunks are um and how you Shunk",
    "start": "1492440",
    "end": "1500240"
  },
  {
    "text": "them and the embeddings the embeddings matter a lot and they depend a lot on",
    "start": "1500240",
    "end": "1505640"
  },
  {
    "text": "both the size of the shunks and and um for example if you if you don't have",
    "start": "1505640",
    "end": "1511200"
  },
  {
    "text": "English language or um or things like if you have um text that's maybe not",
    "start": "1511200",
    "end": "1518480"
  },
  {
    "text": "General maybe it's medical text or something like that that's a problem and",
    "start": "1518480",
    "end": "1523600"
  },
  {
    "text": "then we need to think about because we're now storing this data in a database in a vector database and this",
    "start": "1523600",
    "end": "1530159"
  },
  {
    "text": "data might have come from a system that was safe and secure and had privacy",
    "start": "1530159",
    "end": "1535399"
  },
  {
    "text": "because we all are really good at both security and privacy right um and now",
    "start": "1535399",
    "end": "1540919"
  },
  {
    "text": "suddenly we're copying all the data into a new data store and we're forgetting",
    "start": "1540919",
    "end": "1547559"
  },
  {
    "text": "about all that um so maybe Anne can now get to Pete data or or you know or maybe",
    "start": "1547559",
    "end": "1556679"
  },
  {
    "text": "people can get to data that they absolutely shouldn't um be allowed to see H and not only that but what if the",
    "start": "1556679",
    "end": "1563919"
  },
  {
    "text": "data is continuously updated how do you update this Vector store with that sort of thing so there",
    "start": "1563919",
    "end": "1569799"
  },
  {
    "text": "are a lot of considerations around the data that you don't have to worry about in the demo that are very very worrisome",
    "start": "1569799",
    "end": "1577480"
  },
  {
    "text": "in real life and then you have the question because when you're creating",
    "start": "1577480",
    "end": "1582720"
  },
  {
    "text": "this system you're probably going to ask it some really neat and nice questions based on what you're thinking but then a",
    "start": "1582720",
    "end": "1589799"
  },
  {
    "text": "user comes in and they ask something completely different so instead of asking some very precise questions about",
    "start": "1589799",
    "end": "1596320"
  },
  {
    "text": "what um what test should I run on machine learning models they say can you summarize all the problems I've had from",
    "start": "1596320",
    "end": "1602600"
  },
  {
    "text": "between 2001 and 2008 and suddenly there is no way that",
    "start": "1602600",
    "end": "1608360"
  },
  {
    "text": "your system is built for that it's no way your system is built for actually",
    "start": "1608360",
    "end": "1613559"
  },
  {
    "text": "accessing all the shunks that you needed uh to answer that question",
    "start": "1613559",
    "end": "1619559"
  },
  {
    "text": "um and also you might have people that do adversarial queries so they might go",
    "start": "1619559",
    "end": "1625480"
  },
  {
    "text": "in and try to jailbreak the system in different ways and then retrieval like this is um",
    "start": "1625480",
    "end": "1633360"
  },
  {
    "text": "a search problem that has existed since semantic search kind of started in",
    "start": "1633360",
    "end": "1639120"
  },
  {
    "text": "2013 it's it's not super trivial to do retrieval from a semantic database it's",
    "start": "1639120",
    "end": "1647240"
  },
  {
    "text": "not like it's going to give you 100% of all the documents that are relevant for this problem and then we have the",
    "start": "1647240",
    "end": "1653880"
  },
  {
    "text": "prompting and the prompting I I mean I'm sure you've heard in the name prompt engineering I think maybe not so much",
    "start": "1653880",
    "end": "1660120"
  },
  {
    "text": "engineering but prompt hacking maybe but it doesn't matter like even a word or",
    "start": "1660120",
    "end": "1665320"
  },
  {
    "text": "two in your in your prompt creates completely different",
    "start": "1665320",
    "end": "1670480"
  },
  {
    "text": "answers and then we have the generation which we already see how that can end up",
    "start": "1670480",
    "end": "1675720"
  },
  {
    "text": "telling people um that they should have done more stuff since",
    "start": "1675720",
    "end": "1681399"
  },
  {
    "text": "2014 so all of this might seem like a Ragtime nightmare",
    "start": "1681399",
    "end": "1687720"
  },
  {
    "text": "um and um but there are things that we can do about it so hopefully I'll I'll",
    "start": "1687720",
    "end": "1694120"
  },
  {
    "text": "introduce some good ways that you can optimize all of these things but before",
    "start": "1694120",
    "end": "1699919"
  },
  {
    "text": "we start optimizing anything we need to talk about evaluation because when you do experiments a lot of people will do",
    "start": "1699919",
    "end": "1707640"
  },
  {
    "text": "experiments and then evaluate them manually and that is probably the worst thing that you can",
    "start": "1707640",
    "end": "1714320"
  },
  {
    "text": "ever do because there is no good way then to tell if shuning it this way or",
    "start": "1714320",
    "end": "1720240"
  },
  {
    "text": "or parsing it this way will actually make a difference for for your whole system so a long long time ago back in",
    "start": "1720240",
    "end": "1728039"
  },
  {
    "text": "the day when we were just doing regular machine learning like last year or something we had these machine learning",
    "start": "1728039",
    "end": "1734480"
  },
  {
    "text": "models that would do classification for example it would figure out if if it was a cat or a dog and we knew that if we",
    "start": "1734480",
    "end": "1742159"
  },
  {
    "text": "got these answers so it didn't realized that this was a ragd doll it thought it",
    "start": "1742159",
    "end": "1748399"
  },
  {
    "text": "was a dog um we had 90% accuracy and",
    "start": "1748399",
    "end": "1753760"
  },
  {
    "text": "what we would worry about was if the test set was different enough from the training set so we didn't leak data uh",
    "start": "1753760",
    "end": "1761240"
  },
  {
    "text": "from the training set in the test set so I'm not saying that it was always",
    "start": "1761240",
    "end": "1766679"
  },
  {
    "text": "this easy uh to figure out what the accuracy was but generally we had",
    "start": "1766679",
    "end": "1771880"
  },
  {
    "text": "something called accuracy that we could deal with but now we have something completely different because now we're",
    "start": "1771880",
    "end": "1777799"
  },
  {
    "text": "generating text so if we're generating text about this image and the ground truth assuming that",
    "start": "1777799",
    "end": "1785039"
  },
  {
    "text": "we actually have ground truth because that's a problem too like we have to to evaluate something we actually have have",
    "start": "1785039",
    "end": "1792159"
  },
  {
    "text": "to have someone write down a number of questions and a number of answers that they would like to have so in this case",
    "start": "1792159",
    "end": "1799039"
  },
  {
    "text": "if I say this is a pick of a super cute kitten catching some seas and that's the correct answer but",
    "start": "1799039",
    "end": "1806320"
  },
  {
    "text": "how good is this answer an image of a sleeping feline raising her",
    "start": "1806320",
    "end": "1812000"
  },
  {
    "text": "paw obviously it's factually correct assuming that it's a yeah if assuming",
    "start": "1812000",
    "end": "1820240"
  },
  {
    "text": "it's a her but um but it might not be tonally correct and even and you can see",
    "start": "1820240",
    "end": "1825720"
  },
  {
    "text": "that the words are very different so might also be semantically correct but",
    "start": "1825720",
    "end": "1831399"
  },
  {
    "text": "we we have no real way of telling like is this a correct thing if this would",
    "start": "1831399",
    "end": "1836799"
  },
  {
    "text": "have to fit in in a book for a 5-year-old this might not be the perfect",
    "start": "1836799",
    "end": "1842640"
  },
  {
    "text": "answer here it might be very Incorrect and and often times it's not even factually",
    "start": "1842640",
    "end": "1848279"
  },
  {
    "text": "correct so it's very hard to say what the accuracy is here we have some",
    "start": "1848279",
    "end": "1854679"
  },
  {
    "text": "attempts to do this so um for example prompt flow in Asher has a number of",
    "start": "1854679",
    "end": "1861840"
  },
  {
    "text": "measurements that it can help you take and same thing with like Frameworks like Ras um that will help you evaluate",
    "start": "1861840",
    "end": "1869440"
  },
  {
    "text": "different things so on the retrieval side it might evaluate context precision and content context recall so what this",
    "start": "1869440",
    "end": "1877440"
  },
  {
    "text": "does is it basically say given the question context Precision is the the",
    "start": "1877440",
    "end": "1884279"
  },
  {
    "text": "shunks that I got back are they actually like do they fit the question and",
    "start": "1884279",
    "end": "1889760"
  },
  {
    "text": "context recall says did I get all the shunks that were necessary to answer the",
    "start": "1889760",
    "end": "1896080"
  },
  {
    "text": "question so that's on the retrieval side and then we have ground or faithfulness",
    "start": "1896080",
    "end": "1901559"
  },
  {
    "text": "on the generation side which means did the llm use the context to",
    "start": "1901559",
    "end": "1907720"
  },
  {
    "text": "generate the answer or did it just come up with Answers by itself so out of the",
    "start": "1907720",
    "end": "1914919"
  },
  {
    "text": "facts that it stated how many of those were actually ground in the context it was given and finally there is things",
    "start": "1914919",
    "end": "1921799"
  },
  {
    "text": "like answer relevancy which is did the question or did the answer actually 50",
    "start": "1921799",
    "end": "1927360"
  },
  {
    "text": "the question and so on so all of these we can now see that we we have four",
    "start": "1927360",
    "end": "1933240"
  },
  {
    "text": "different measurements and we all know that if we have four measurements and we try to tweak something and that tweak",
    "start": "1933240",
    "end": "1940120"
  },
  {
    "text": "fixes this but doesn't fix this then what did we do where like how how good",
    "start": "1940120",
    "end": "1946240"
  },
  {
    "text": "was that fix so becomes harder and harder the more things you have but the",
    "start": "1946240",
    "end": "1951679"
  },
  {
    "text": "other problem with this also is that this is typically based on asking an llm",
    "start": "1951679",
    "end": "1957519"
  },
  {
    "text": "to give an answer basically to tell it was it factually correct or was H was",
    "start": "1957519",
    "end": "1963880"
  },
  {
    "text": "the context relevant so we we literally have an llm watching an llm and if the",
    "start": "1963880",
    "end": "1970799"
  },
  {
    "text": "llm that we used to generate the answer was gbd4 and the llm that we're using to",
    "start": "1970799",
    "end": "1977279"
  },
  {
    "text": "watching G gbd4 is also gbd4 then do you think it will tell that",
    "start": "1977279",
    "end": "1983559"
  },
  {
    "text": "it's correct or incorrect so these are some problems that we have to wait through it will be a little bit better",
    "start": "1983559",
    "end": "1990760"
  },
  {
    "text": "if you have gbd4 for example watching gbd 35 because then you have a better",
    "start": "1990760",
    "end": "1996760"
  },
  {
    "text": "llm that can that can sort of uh tell you how Fally correct the um lower one",
    "start": "1996760",
    "end": "2004840"
  },
  {
    "text": "was but so these are all problems but this is what we have so at least we have something that we can use um to test",
    "start": "2004840",
    "end": "2012360"
  },
  {
    "text": "these things so now that we have something to test let's talk about like some of the optimizations we can make so the first",
    "start": "2012360",
    "end": "2019919"
  },
  {
    "text": "one is around data and and data ingestion and I think that in all",
    "start": "2019919",
    "end": "2025639"
  },
  {
    "text": "machine learning projects and the same thing with these llm and rag problems this is the most important part of the",
    "start": "2025639",
    "end": "2032320"
  },
  {
    "text": "problem which is actually making sure you have the right data and make making",
    "start": "2032320",
    "end": "2037960"
  },
  {
    "text": "sure the the data is ingestible so that it's not out of date that it's um",
    "start": "2037960",
    "end": "2045679"
  },
  {
    "text": "actually uh is complete will will answer the questions you need and also like",
    "start": "2045679",
    "end": "2052760"
  },
  {
    "text": "that when you extract it so if you extract a table deciding like how that will look and you might have other",
    "start": "2052760",
    "end": "2060000"
  },
  {
    "text": "problems in your IM in your PDFs for example with images or or all sorts of",
    "start": "2060000",
    "end": "2066240"
  },
  {
    "text": "things so one thing we can use is document intelligence so document",
    "start": "2066240",
    "end": "2071599"
  },
  {
    "text": "intelligence is um one of the tools that you can use to to do a little bit better parsing than normal if you have PDFs",
    "start": "2071599",
    "end": "2079000"
  },
  {
    "text": "there is also llama pars um which is something that came out now from llama",
    "start": "2079000",
    "end": "2084599"
  },
  {
    "text": "index but there are definitely better parsers that you can often use especially if you're working with",
    "start": "2084599",
    "end": "2091200"
  },
  {
    "text": "PDFs so I'm just bringing this up because this um before llama pars or",
    "start": "2091200",
    "end": "2096440"
  },
  {
    "text": "actually I think llama pars and document intelligence are kind of like on par but this used to be the state-ofthe-art for",
    "start": "2096440",
    "end": "2102800"
  },
  {
    "text": "for extracting things from PDFs it's not something that you use locally on your machine it's actually an API that that",
    "start": "2102800",
    "end": "2109800"
  },
  {
    "text": "you have to go through with models in the background and if we have multimodal",
    "start": "2109800",
    "end": "2115079"
  },
  {
    "text": "data multimodal meaning that we have images or we have tables or we have things that are not necessarily plain",
    "start": "2115079",
    "end": "2121640"
  },
  {
    "text": "text a couple of things that we can do are for example if you have an image in your PDF you can either ignore it and",
    "start": "2121640",
    "end": "2128960"
  },
  {
    "text": "say that I don't care about the the images but then you'll lose some data or you can do things like try to get a",
    "start": "2128960",
    "end": "2136520"
  },
  {
    "text": "caption for the data and you store that with your text junks instead or try to",
    "start": "2136520",
    "end": "2142359"
  },
  {
    "text": "extract and summarize your tables or extract them as Json or something H or",
    "start": "2142359",
    "end": "2149040"
  },
  {
    "text": "use a multimodal llm so now we have like shat shat 4V and 4 o that know uh",
    "start": "2149040",
    "end": "2157200"
  },
  {
    "text": "multiple modalities so it knows images Etc now we get to the",
    "start": "2157200",
    "end": "2163800"
  },
  {
    "text": "shuning so the the shuning basically means how we split up our data into",
    "start": "2163800",
    "end": "2170520"
  },
  {
    "text": "smaller parts and we want to make sure like that we pick a shuning method that's fitting",
    "start": "2170520",
    "end": "2177160"
  },
  {
    "text": "our data so if you have a long novel then the context that you need uh might",
    "start": "2177160",
    "end": "2183640"
  },
  {
    "text": "be sort of like a whole chapter or or something like that whereas if you have a Q&A document you might want to Shunk",
    "start": "2183640",
    "end": "2190560"
  },
  {
    "text": "them up in just a question and answer and that might be the correct shuning uh",
    "start": "2190560",
    "end": "2196000"
  },
  {
    "text": "part and you also have to think about um you know different embedding models have",
    "start": "2196000",
    "end": "2201720"
  },
  {
    "text": "different sizes that they work well with and not and different llms have",
    "start": "2201720",
    "end": "2207079"
  },
  {
    "text": "different context Windows like how much how many shunks you can actually put in or how much text you can actually put in",
    "start": "2207079",
    "end": "2213520"
  },
  {
    "text": "and the other thing you want to do is also try to balance because because",
    "start": "2213520",
    "end": "2218560"
  },
  {
    "text": "we're using in the general case which we'll see we can change we're using the question creating the embedding of that",
    "start": "2218560",
    "end": "2225359"
  },
  {
    "text": "and comparing that to the Chunk we want to make sure that they're approximately the same size so if you try to embed",
    "start": "2225359",
    "end": "2232720"
  },
  {
    "text": "something that's a page long that contains a lot of different topics then",
    "start": "2232720",
    "end": "2238319"
  },
  {
    "text": "the embedding for that is not going to be very precise it's going to be super General because it's going to have all these semantic meanings and it's going",
    "start": "2238319",
    "end": "2245200"
  },
  {
    "text": "to be very hard to kind of compare to that to a very crisp question so shuning works like this so",
    "start": "2245200",
    "end": "2253599"
  },
  {
    "text": "this is the most naive type of shuning and if you want to try out shuning there is this site called shank wh where you",
    "start": "2253599",
    "end": "2260079"
  },
  {
    "text": "can test out how shuning would work for your documents um but this one basically",
    "start": "2260079",
    "end": "2267800"
  },
  {
    "text": "says Shunk it up in to 400 characters at a time I don't care where that 400",
    "start": "2267800",
    "end": "2274640"
  },
  {
    "text": "characters take me so you can see here between the the blue and the yellow on top it just stopped in the middle of a",
    "start": "2274640",
    "end": "2281680"
  },
  {
    "text": "word obviously if we send in that Shunk to an",
    "start": "2281680",
    "end": "2286720"
  },
  {
    "text": "llm it will it will be missing some context at the end of that sentence it's",
    "start": "2286720",
    "end": "2292640"
  },
  {
    "text": "like if you would just tell your friend half a sentence and then ask it to give you an answer based on that qu on that",
    "start": "2292640",
    "end": "2299560"
  },
  {
    "text": "sentence so a little bit more refined way of doing this is by doing a",
    "start": "2299560",
    "end": "2305079"
  },
  {
    "text": "recursive sentence splitter or cursive uh character splitter and it will at",
    "start": "2305079",
    "end": "2310920"
  },
  {
    "text": "least give you paragraphs or it might look for uh sentences or things like",
    "start": "2310920",
    "end": "2317560"
  },
  {
    "text": "that so this is the typical thing that you'll see in most demos it will use this recursive character text",
    "start": "2317560",
    "end": "2325079"
  },
  {
    "text": "splitter but I think we should go a little bit further and do something like",
    "start": "2325079",
    "end": "2330280"
  },
  {
    "text": "this which is a recursive text splitter but with in this case since I'm using",
    "start": "2330280",
    "end": "2335520"
  },
  {
    "text": "markdown um it will know about markdown and I will say pick out chunks that are",
    "start": "2335520",
    "end": "2342319"
  },
  {
    "text": "like second level headings those are the shunks that I want and this recursive uh",
    "start": "2342319",
    "end": "2349560"
  },
  {
    "text": "text blader also has modes for HTML or for python or for a lot of different",
    "start": "2349560",
    "end": "2355480"
  },
  {
    "text": "things so depending on the context it often there is often like a splitter that will understand your type of",
    "start": "2355480",
    "end": "2361920"
  },
  {
    "text": "content and be able to split a little bit more intelligently based on that or you can create your own splitting",
    "start": "2361920",
    "end": "2368680"
  },
  {
    "text": "yourself but basically try to be as content aware as possible and then if we're talking about",
    "start": "2368680",
    "end": "2374680"
  },
  {
    "text": "the embedding and indexing uh I've already talked about how we want the shunks to be super small because the",
    "start": "2374680",
    "end": "2382319"
  },
  {
    "text": "shunks are are sort of like what we're going to be comparing to the question so we want to have a crisp topic we want to",
    "start": "2382319",
    "end": "2389400"
  },
  {
    "text": "have a crisp embedding that basically is very specific to that chunk but when we",
    "start": "2389400",
    "end": "2394720"
  },
  {
    "text": "give the context the context might need to be a lot bigger for the llm so as an",
    "start": "2394720",
    "end": "2400680"
  },
  {
    "text": "example if we have this document which is the the plot of Oppenheimer and we",
    "start": "2400680",
    "end": "2405880"
  },
  {
    "text": "have a question that is what was oppenheimer's relationship with Einstein it might have picked out this Shunk but",
    "start": "2405880",
    "end": "2412880"
  },
  {
    "text": "this Shunk might not tell the whole story this was because this shank contained Einstein and something like",
    "start": "2412880",
    "end": "2419560"
  },
  {
    "text": "that so it picked this out of the database but we send the llm this so we",
    "start": "2419560",
    "end": "2425520"
  },
  {
    "text": "send the llm the shunks before or after so this is a technique called small to",
    "start": "2425520",
    "end": "2431079"
  },
  {
    "text": "big and it's quite an easy technique that I think is is well worth",
    "start": "2431079",
    "end": "2436440"
  },
  {
    "text": "experimenting with there is also another version of this where we do the same thing but we're he sending back the",
    "start": "2436440",
    "end": "2442680"
  },
  {
    "text": "whole document Al together or we might have a summary of the page that we use",
    "start": "2442680",
    "end": "2448280"
  },
  {
    "text": "for indexing and then send back the whole page and how much we send back",
    "start": "2448280",
    "end": "2454240"
  },
  {
    "text": "depends on what llm we're using like how big context window is and and how many",
    "start": "2454240",
    "end": "2459560"
  },
  {
    "text": "shunks we might want to send back so this particular one is called parent child shunks",
    "start": "2459560",
    "end": "2466920"
  },
  {
    "text": "retrieval we also have a number of other techniques and and I will say that a lot of these techniques may or may not yield",
    "start": "2467040",
    "end": "2474119"
  },
  {
    "text": "results in your specific case but they are quite interesting to try out so the",
    "start": "2474119",
    "end": "2480119"
  },
  {
    "text": "next one is called hide um this was a paper that came out that talks about hypothetical document embeddings so when",
    "start": "2480119",
    "end": "2487640"
  },
  {
    "text": "we have the question instead of trying to embed the question what we do is we ask an llm can you give me a paragraph",
    "start": "2487640",
    "end": "2496640"
  },
  {
    "text": "based on this question and then we use an embedding of the paragraph instead",
    "start": "2496640",
    "end": "2501760"
  },
  {
    "text": "and the idea behind this is because you might have a question that goes can you",
    "start": "2501760",
    "end": "2506920"
  },
  {
    "text": "give me a passage from war in peace and then you want this which actually has",
    "start": "2506920",
    "end": "2512960"
  },
  {
    "text": "nothing to do with War and Peace like it doesn't contain the text of war and piece in any ways but it might be very",
    "start": "2512960",
    "end": "2519680"
  },
  {
    "text": "similar to a hypothetical Shunk that the llm SP",
    "start": "2519680",
    "end": "2524720"
  },
  {
    "text": "spits out but now we're kind of in the realm where we we're asking an llm to",
    "start": "2524720",
    "end": "2531200"
  },
  {
    "text": "guess what an answer might be and then try to retrieve something from a",
    "start": "2531200",
    "end": "2536319"
  },
  {
    "text": "database on what um what that answer might",
    "start": "2536319",
    "end": "2541839"
  },
  {
    "text": "be the alternative or the reverse of this is that when we embed all the",
    "start": "2541839",
    "end": "2548680"
  },
  {
    "text": "shunks instead of embedding the shunks we say can you please give me a question",
    "start": "2548680",
    "end": "2555680"
  },
  {
    "text": "that someone might ask about this chunk so the llm would then spit back a question and instead of instead of",
    "start": "2555680",
    "end": "2563559"
  },
  {
    "text": "giving the embedding of the Shunk in the index database we give the embedding of the question that the llm came up with",
    "start": "2563559",
    "end": "2570319"
  },
  {
    "text": "so now we're comparing questions and questions because it's quite important when you look at embeddings that you're comparing apples and apples and oranges",
    "start": "2570319",
    "end": "2577920"
  },
  {
    "text": "and oranges so this is one technique the other thing with",
    "start": "2577920",
    "end": "2583319"
  },
  {
    "text": "embeddings is this if we have a legal text like in this case we have uh words",
    "start": "2583319",
    "end": "2589599"
  },
  {
    "text": "like hearing after which yeah I've never used that word in real life actually but",
    "start": "2589599",
    "end": "2596400"
  },
  {
    "text": "uh or commencement date or if we have a medical text it might be something like",
    "start": "2596400",
    "end": "2601480"
  },
  {
    "text": "a wet lab or a solution and solution in medical cases or in chemical cases is a",
    "start": "2601480",
    "end": "2607800"
  },
  {
    "text": "lot different than a solution in general text so now if we're using um an or an",
    "start": "2607800",
    "end": "2614760"
  },
  {
    "text": "embedding model that's built for General text that might not find the sentiment",
    "start": "2614760",
    "end": "2622119"
  },
  {
    "text": "of solution in this case so we might want to is Trin um and fine-tune so",
    "start": "2622119",
    "end": "2627839"
  },
  {
    "text": "fine-tuning means that you take a model that already exists like in a meding model and then you train it a little bit",
    "start": "2627839",
    "end": "2634400"
  },
  {
    "text": "more with just a few more examples so that it learns what a solution means in",
    "start": "2634400",
    "end": "2641240"
  },
  {
    "text": "your type of text so you train it a little bit on your own",
    "start": "2641240",
    "end": "2646880"
  },
  {
    "text": "data then we have the indexing and retrieval so now we're going to get into some some other techniques that we can",
    "start": "2647040",
    "end": "2653680"
  },
  {
    "text": "use for that so this one is one that I will say if you do nothing else and if",
    "start": "2653680",
    "end": "2659520"
  },
  {
    "text": "you have the uh opportunity this is one that you absolutely do want to try",
    "start": "2659520",
    "end": "2665079"
  },
  {
    "text": "metadata filtering so we can picture a number of queries that we might be able to ask on on a number of",
    "start": "2665079",
    "end": "2673160"
  },
  {
    "text": "um sort of like if we have information or the plots of some",
    "start": "2673160",
    "end": "2678440"
  },
  {
    "text": "movies so we might want to say I want to watch a movie rated higher than 8.5 or",
    "start": "2678440",
    "end": "2683880"
  },
  {
    "text": "something that's directed by Greta Gerwig or or something like that where",
    "start": "2683880",
    "end": "2688920"
  },
  {
    "text": "in traditional means we would do filtering on this but they're not giving us a filter they're giving us sort of",
    "start": "2688920",
    "end": "2696079"
  },
  {
    "text": "like the question and playing text with kind of hidden filtering and questions",
    "start": "2696079",
    "end": "2701839"
  },
  {
    "text": "like for example has gr ger directed in movies about women that's actually uh",
    "start": "2701839",
    "end": "2707839"
  },
  {
    "text": "movies about women director equals gr GG that's how we probably would want to the",
    "start": "2707839",
    "end": "2713160"
  },
  {
    "text": "query to show up so the way we do this is we can now bring out the same shunks",
    "start": "2713160",
    "end": "2718839"
  },
  {
    "text": "and if we have the metadata around them and this might be in this case we have",
    "start": "2718839",
    "end": "2723960"
  },
  {
    "text": "very good metadata like what year or rating or genre or things like that but",
    "start": "2723960",
    "end": "2729720"
  },
  {
    "text": "you might have at least some metadata about where the PDF came from what you",
    "start": "2729720",
    "end": "2735760"
  },
  {
    "text": "know what machine it was talking about if it was talking about like how a machine works or something like that so",
    "start": "2735760",
    "end": "2742520"
  },
  {
    "text": "you can kind of like add metadata to your content and then you can tell um",
    "start": "2742520",
    "end": "2748480"
  },
  {
    "text": "also the vector database or I'm sorry not the vector database like you you can specify the",
    "start": "2748480",
    "end": "2754480"
  },
  {
    "text": "metadata Fields this is for the retriever to say this is the description",
    "start": "2754480",
    "end": "2759839"
  },
  {
    "text": "of the metadata Fields so when someone comes in and talks about genre then this",
    "start": "2759839",
    "end": "2765880"
  },
  {
    "text": "is what they might filter on and these are the possible answers that you might",
    "start": "2765880",
    "end": "2772040"
  },
  {
    "text": "have and then you go through and you uh retrieve the",
    "start": "2772040",
    "end": "2777920"
  },
  {
    "text": "content and you get out sort of things filtered so if we look at how this looks",
    "start": "2777920",
    "end": "2784760"
  },
  {
    "text": "in real life you'll see see um here I've done the same thing",
    "start": "2784760",
    "end": "2791960"
  },
  {
    "text": "here and the prompt that it uh actually asked the llm in order to do the",
    "start": "2791960",
    "end": "2799800"
  },
  {
    "text": "filtering was your goal is to structure the user's query to match the request uh",
    "start": "2799800",
    "end": "2805280"
  },
  {
    "text": "the request schema below with a query and a filter and it tells it how it should generate a filter so basically",
    "start": "2805280",
    "end": "2811839"
  },
  {
    "text": "we're asking the LM to create a filter for us into the in index database and",
    "start": "2811839",
    "end": "2818280"
  },
  {
    "text": "this works extremely well actually assuming that we have the metadata",
    "start": "2818280",
    "end": "2824400"
  },
  {
    "text": "for for the data we have",
    "start": "2824400",
    "end": "2831000"
  },
  {
    "text": "so um another thing we can do and and when you're doing some of these things",
    "start": "2832480",
    "end": "2837640"
  },
  {
    "text": "you have to be a little bit aware that some of these things take an extra call to an llm um in order to be able to",
    "start": "2837640",
    "end": "2845559"
  },
  {
    "text": "execute and you you will have to like consider if it's worth the cost and the",
    "start": "2845559",
    "end": "2850960"
  },
  {
    "text": "time it takes to to make that extra call so this is a query expansion you have a",
    "start": "2850960",
    "end": "2856599"
  },
  {
    "text": "question and you ask a machine learning model to or an llm to say give me five",
    "start": "2856599",
    "end": "2863640"
  },
  {
    "text": "questions based on this question and then I take all of these five questions",
    "start": "2863640",
    "end": "2868800"
  },
  {
    "text": "and I search the database and then I pull together sort of like I look at the unique shunks that",
    "start": "2868800",
    "end": "2875960"
  },
  {
    "text": "I get back and rerank them so reranking you often do with a reranking model and",
    "start": "2875960",
    "end": "2881920"
  },
  {
    "text": "then you ask um the llm to synthesize the answer based on that and the idea",
    "start": "2881920",
    "end": "2888440"
  },
  {
    "text": "behind this is that the user might have asked like a very W vague question like",
    "start": "2888440",
    "end": "2893680"
  },
  {
    "text": "maybe what are the approaches to task decomposition but if we can expand this",
    "start": "2893680",
    "end": "2899800"
  },
  {
    "text": "to these questions instead it might have a better chance to actually retrieve the",
    "start": "2899800",
    "end": "2905400"
  },
  {
    "text": "content from the V database or it might even sort of create like sub questions",
    "start": "2905400",
    "end": "2912040"
  },
  {
    "text": "so it might create um from one of these it might create a number of sub questions that together bring up all the",
    "start": "2912040",
    "end": "2919480"
  },
  {
    "text": "data that we need to answer the question and then we have hybrid search",
    "start": "2919480",
    "end": "2925720"
  },
  {
    "text": "uh hybrid search basically means that you both do semantic search but you do",
    "start": "2925720",
    "end": "2931000"
  },
  {
    "text": "some more traditional search or or NR search where engram is basically more",
    "start": "2931000",
    "end": "2936839"
  },
  {
    "text": "traditional NLP um searching so you can say I want",
    "start": "2936839",
    "end": "2943000"
  },
  {
    "text": "25% of my results to come from this type of search and 75 from this and you can",
    "start": "2943000",
    "end": "2949000"
  },
  {
    "text": "mix and match however you want and then take the shunks and reduce and rerank",
    "start": "2949000",
    "end": "2955280"
  },
  {
    "text": "and do this so this is also something to try out and most Vector databases have",
    "start": "2955280",
    "end": "2961280"
  },
  {
    "text": "this kind of capability and then when it comes to reranking you might think that if you've",
    "start": "2961280",
    "end": "2967920"
  },
  {
    "text": "gotten some something from a vector database it should already be ranked nicely like we get the best things first",
    "start": "2967920",
    "end": "2974799"
  },
  {
    "text": "in the worst like in in diminishing uh order but it's not necessarily always",
    "start": "2974799",
    "end": "2981480"
  },
  {
    "text": "true we might uh might use a rer ranker that's better at actually the ranking",
    "start": "2981480",
    "end": "2987359"
  },
  {
    "text": "portion and we also might have a problem like is described in this um paper",
    "start": "2987359",
    "end": "2992400"
  },
  {
    "text": "that's called a lost in the middle problem so it turns out that a lot of llms know things that are at the",
    "start": "2992400",
    "end": "2999000"
  },
  {
    "text": "beginning and at the end of The Prompt but they don't know like they kind of",
    "start": "2999000",
    "end": "3004400"
  },
  {
    "text": "forget about things that are in the middle of the prompt so you might want to rrang and put the important stuff",
    "start": "3004400",
    "end": "3011119"
  },
  {
    "text": "first and last like Claude 2 kind of like often listens more to the things",
    "start": "3011119",
    "end": "3017040"
  },
  {
    "text": "that are close to the questions so it depends also what llm you're using how",
    "start": "3017040",
    "end": "3023119"
  },
  {
    "text": "it's using the context and how it's doing this",
    "start": "3023119",
    "end": "3028280"
  },
  {
    "text": "and then when it comes to the generation there are a couple of things that we can do we can uh use something called Agent",
    "start": "3028280",
    "end": "3036640"
  },
  {
    "text": "so I'll show here um call this agent Mr",
    "start": "3036640",
    "end": "3044880"
  },
  {
    "text": "Anderson um so we saw from before that it wasn't very good at math and it's",
    "start": "3044880",
    "end": "3050520"
  },
  {
    "text": "also not good at actually searching for things or finding data that's very",
    "start": "3050520",
    "end": "3055839"
  },
  {
    "text": "recent so in this case what I want to like what",
    "start": "3055839",
    "end": "3061079"
  },
  {
    "text": "I want to do is I want to say how many more people live in",
    "start": "3061079",
    "end": "3068000"
  },
  {
    "text": "Stockholm versus Oslo so what does this going to do is",
    "start": "3068000",
    "end": "3073160"
  },
  {
    "text": "it's going to go first out and do a web search and then it's going to use a",
    "start": "3073160",
    "end": "3078280"
  },
  {
    "text": "calculator to to put things together and before I show the answers I'm just going to show",
    "start": "3078280",
    "end": "3085440"
  },
  {
    "text": "you how this a agent looks so the only thing I've done here is I've created a set of",
    "start": "3085440",
    "end": "3093280"
  },
  {
    "text": "tools um so one tool is a search and one tool is a calculator in this case I've",
    "start": "3093280",
    "end": "3098880"
  },
  {
    "text": "used a Bing search reason I used that was because I had the API key and then",
    "start": "3098880",
    "end": "3104160"
  },
  {
    "text": "I'm using um open AI to kind of synthesize the answer once it knows once",
    "start": "3104160",
    "end": "3111160"
  },
  {
    "text": "it's gotten the results of both of these and then you can see this agent what it does is it kind of uses the llm to",
    "start": "3111160",
    "end": "3118240"
  },
  {
    "text": "figure out how it should solve the problem based on what the question is",
    "start": "3118240",
    "end": "3123839"
  },
  {
    "text": "and one way it's doing this then it took five steps H so first it had a thought",
    "start": "3123839",
    "end": "3129640"
  },
  {
    "text": "that it should use the search tool for the current population in stockolm and Oslo and it goes through and that's the",
    "start": "3129640",
    "end": "3135680"
  },
  {
    "text": "search and then it used the results and it put them together and it used the",
    "start": "3135680",
    "end": "3141880"
  },
  {
    "text": "calculator tool which the calculator tool is basically is a function",
    "start": "3141880",
    "end": "3147680"
  },
  {
    "text": "like um that can calculate so this is not a guessing anymore like the",
    "start": "3147680",
    "end": "3152920"
  },
  {
    "text": "calculator tool is an actual calculator um and it Returns the answer",
    "start": "3152920",
    "end": "3161440"
  },
  {
    "text": "and all of this took one and then it comes together and and creates an an",
    "start": "3161440",
    "end": "3166880"
  },
  {
    "text": "answer for us that we can use and all of this took one step and and all of that",
    "start": "3166880",
    "end": "3171920"
  },
  {
    "text": "that's a part of of Shan lit this user interface that makes it also quite nice",
    "start": "3171920",
    "end": "3177760"
  },
  {
    "text": "to work with these agents and things yes because you can kind of like run a lot of queries and you can follow the whole",
    "start": "3177760",
    "end": "3184520"
  },
  {
    "text": "train of thought and everything so sometimes we can use these agents to figure out",
    "start": "3184520",
    "end": "3190319"
  },
  {
    "text": "first if you have very similar documents let's say you have documents that talk about different machines and how you",
    "start": "3190319",
    "end": "3196160"
  },
  {
    "text": "should operate them then you might have document agents that know first which",
    "start": "3196160",
    "end": "3202480"
  },
  {
    "text": "document you should run ragon and then use that instead of like having all the",
    "start": "3202480",
    "end": "3208319"
  },
  {
    "text": "shunks together in one rag system and then finally um with all of",
    "start": "3208319",
    "end": "3217200"
  },
  {
    "text": "these things there is still something very scary going on this is um an llm",
    "start": "3217200",
    "end": "3222480"
  },
  {
    "text": "leaderboard there is like a lot of benchmarking when it comes to to llms um",
    "start": "3222480",
    "end": "3228200"
  },
  {
    "text": "this is just one I just picked one the first one I could come up with when I searched for llm benchmarking but they",
    "start": "3228200",
    "end": "3234240"
  },
  {
    "text": "will all show about the same things that um on data sets that it's trained on it",
    "start": "3234240",
    "end": "3240440"
  },
  {
    "text": "will like it will find out the general accuracy of how well it answers",
    "start": "3240440",
    "end": "3246160"
  },
  {
    "text": "something is around 65 to 85% uh or 60 to 85% and they're getting",
    "start": "3246160",
    "end": "3253000"
  },
  {
    "text": "a like a little bit better uh I think GPT 40 is a little bit over 85% in in",
    "start": "3253000",
    "end": "3259559"
  },
  {
    "text": "general um so we're we're kind of getting there we're right now it sits at about 8",
    "start": "3259559",
    "end": "3267440"
  },
  {
    "text": "5% so we know that generation then um is",
    "start": "3267440",
    "end": "3273000"
  },
  {
    "text": "is definitely not 100% thing even given all the",
    "start": "3273000",
    "end": "3278079"
  },
  {
    "text": "context um in the correct order and then we have the retrieval",
    "start": "3278079",
    "end": "3283559"
  },
  {
    "text": "which itself might have an accuracy of maybe 60 to 85% yes Ball parking here",
    "start": "3283559",
    "end": "3288960"
  },
  {
    "text": "but these are quite reasonable numbers for retrieval and and",
    "start": "3288960",
    "end": "3294280"
  },
  {
    "text": "augmentation and if we look at this now we have two probabilistic systems that",
    "start": "3294280",
    "end": "3299640"
  },
  {
    "text": "we compound so we have 85 * 85 72 so that means that on a good day",
    "start": "3299640",
    "end": "3309720"
  },
  {
    "text": "it will answer correctly three out of four questions and that fourth question it",
    "start": "3309720",
    "end": "3316440"
  },
  {
    "text": "will also be very confidently incorrect you know so uh unlike something like",
    "start": "3316440",
    "end": "3324880"
  },
  {
    "text": "where we return a bunch of search results and we have the we give the user",
    "start": "3324880",
    "end": "3330720"
  },
  {
    "text": "you know 10 search results the user quite used to that it will have to sift",
    "start": "3330720",
    "end": "3337000"
  },
  {
    "text": "through the search results and if it's a good search then hopefully one of the three first things will be correct but",
    "start": "3337000",
    "end": "3345039"
  },
  {
    "text": "it it will itself like the user will themselves go through and say not relevant to me yeah my question was",
    "start": "3345039",
    "end": "3351559"
  },
  {
    "text": "probably not good enough to give me like you know but um but out of the one maybe",
    "start": "3351559",
    "end": "3358039"
  },
  {
    "text": "first three or first five it will find the answer in this case we're assuming that all of our five first guesses are",
    "start": "3358039",
    "end": "3365599"
  },
  {
    "text": "correct and we're using that as the facts for the llm to generate the answer",
    "start": "3365599",
    "end": "3371319"
  },
  {
    "text": "with so what happens here is that we might create a system and especially if",
    "start": "3371319",
    "end": "3376520"
  },
  {
    "text": "this is a sensitive system like we were experimenting with um yes really rough experiment",
    "start": "3376520",
    "end": "3384680"
  },
  {
    "text": "where we had a knowledge base and H our knowledge base and we want to create sort of like an FAQ on top of",
    "start": "3384680",
    "end": "3390359"
  },
  {
    "text": "it and it would confidently tell us that if your cat passed away you had three",
    "start": "3390359",
    "end": "3396280"
  },
  {
    "text": "days of vacation or not vacation but you could take three days of leave because of",
    "start": "3396280",
    "end": "3402480"
  },
  {
    "text": "this and when you're getting told this in natural language that is fact I mean",
    "start": "3402480",
    "end": "3409440"
  },
  {
    "text": "if you're asking an HR bot to um to give",
    "start": "3409440",
    "end": "3414880"
  },
  {
    "text": "you an answer you're going to trust what it says especially if it's um well you",
    "start": "3414880",
    "end": "3420039"
  },
  {
    "text": "might trust it but you're probably going to point to it and say you know told me",
    "start": "3420039",
    "end": "3425280"
  },
  {
    "text": "this um I should get these three days so if you have a sensitive use case where",
    "start": "3425280",
    "end": "3431039"
  },
  {
    "text": "someone like where what you're saying is legally binding or what you're saying is actually going to affect someone on how",
    "start": "3431039",
    "end": "3437520"
  },
  {
    "text": "they're going to process something like let's say uh you have something that returns how you should operate a machine",
    "start": "3437520",
    "end": "3444319"
  },
  {
    "text": "and it forgets to tell you one or two steps and those steps are quite crucial to how you should operate it maybe turn",
    "start": "3444319",
    "end": "3451400"
  },
  {
    "text": "the machine off before you clean it or something like that that might cause",
    "start": "3451400",
    "end": "3457440"
  },
  {
    "text": "some real actual problems in real life so be very careful if you're using this",
    "start": "3457440",
    "end": "3462480"
  },
  {
    "text": "sort of in in a real sensitive use case so that kind of post this um paper",
    "start": "3462480",
    "end": "3471559"
  },
  {
    "text": "says rag does not work for Enterprises and actually one of my uh",
    "start": "3471559",
    "end": "3477480"
  },
  {
    "text": "colleagues asked didn't test did you co-author this because I've kind of been a little bit on on the rant or rag for",
    "start": "3477480",
    "end": "3486079"
  },
  {
    "text": "this yeah no she didn't um so what we can do here instead",
    "start": "3486079",
    "end": "3492039"
  },
  {
    "text": "is is basically take off the augmented generation in the sensitive and by sensitive I mean sensitive in the widest",
    "start": "3492039",
    "end": "3498680"
  },
  {
    "text": "sense of the world yes do search maybe add data in templates or do highlighting",
    "start": "3498680",
    "end": "3505839"
  },
  {
    "text": "of the the search results when they come out instead of augmenting and actually generating an answer you know in natural",
    "start": "3505839",
    "end": "3513599"
  },
  {
    "text": "language and finally the last sort of uh thing that we need to take care of is",
    "start": "3513599",
    "end": "3519760"
  },
  {
    "text": "Guard railing so there is a lot of things that might happen where the user might have an adversarial question or",
    "start": "3519760",
    "end": "3526760"
  },
  {
    "text": "you know your llm might answer something that it shouldn't answer maybe the tone",
    "start": "3526760",
    "end": "3532480"
  },
  {
    "text": "is wrong and things like that so from this very naive rag system we should add a lot of",
    "start": "3532480",
    "end": "3540480"
  },
  {
    "text": "guard rails like you know check if it contains pii so we don't send pii to to",
    "start": "3540480",
    "end": "3547079"
  },
  {
    "text": "a language model that we don't want to send pii to or or if it's asking something that's completely off topics",
    "start": "3547079",
    "end": "3554559"
  },
  {
    "text": "off topic we need to understand if uh we need to give answers that are",
    "start": "3554559",
    "end": "3559799"
  },
  {
    "text": "appropriate or if they do a jailbreak attempt we need to stop that and on the other side also fix all the things that",
    "start": "3559799",
    "end": "3565880"
  },
  {
    "text": "can come out from the generation like hallucinations or or bad uh tone or or",
    "start": "3565880",
    "end": "3572039"
  },
  {
    "text": "you know if it supposed to return a piece of code testing that the code will actually work things like that that it",
    "start": "3572039",
    "end": "3579359"
  },
  {
    "text": "doesn't return Secrets Etc so all of this together um are things that we can",
    "start": "3579359",
    "end": "3585799"
  },
  {
    "text": "do to make these better but I want to just uh finish off with saying rack carefully out there friends because this",
    "start": "3585799",
    "end": "3593039"
  },
  {
    "text": "is um yeah what it looks uh shiny when you first try it out there's a lot of",
    "start": "3593039",
    "end": "3599359"
  },
  {
    "text": "work that needs to be done before you're finished so thank you so much for listening and I'll be here for a while",
    "start": "3599359",
    "end": "3605200"
  },
  {
    "text": "for questions",
    "start": "3605200",
    "end": "3608720"
  }
]