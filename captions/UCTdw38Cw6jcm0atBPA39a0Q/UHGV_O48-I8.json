[
  {
    "text": "hello everybody hey there good afternoon thank you for coming today um",
    "start": "4080",
    "end": "10840"
  },
  {
    "text": "I know the screen's outside it might have got a little confusing it's talking about all kinds of other people's talks but if you're here to talk about extreme",
    "start": "10840",
    "end": "18000"
  },
  {
    "text": "scale and Azure functions you're in the right place trust uh trust the website I",
    "start": "18000",
    "end": "23080"
  },
  {
    "text": "guess we could say um so anyway my name is Paul yovich I'm the the head uh",
    "start": "23080",
    "end": "28599"
  },
  {
    "text": "product person for Azure functions and serverless my team designs and operates",
    "start": "28599",
    "end": "34719"
  },
  {
    "text": "um some services at Microsoft Azure functions being the main one and we support the overall um serverless",
    "start": "34719",
    "end": "41680"
  },
  {
    "text": "efforts like container apps Legion and some of the other services that are under underneath uh what I want to do",
    "start": "41680",
    "end": "48199"
  },
  {
    "text": "today I want to do something a little bit different and thank you for uh coming to hear about with me I want to",
    "start": "48199",
    "end": "54399"
  },
  {
    "text": "double click and and really go under the hood of azure functions and talk about how we built it",
    "start": "54399",
    "end": "60480"
  },
  {
    "text": "and talk about how we made it scale um and when I talk about scale I'm talking about things like uh bursting of",
    "start": "60480",
    "end": "67320"
  },
  {
    "text": "instances let's say a thousand plus instances in under a minute or being able to handle over a million messages",
    "start": "67320",
    "end": "74119"
  },
  {
    "text": "or requests per second that's the kind of scale I'm talking about so today um we're going to go through a number of",
    "start": "74119",
    "end": "79920"
  },
  {
    "text": "things and hopefully give you a good idea of how it works you'll hear some of my opinions why did we do it this way",
    "start": "79920",
    "end": "87280"
  },
  {
    "text": "why are we thinking about this way and after this session I'd love to get your opinions too and I also hope to share",
    "start": "87280",
    "end": "93159"
  },
  {
    "text": "some tips and tricks that we've learned the hard way along the road and maybe you can apply that to your own apps uh",
    "start": "93159",
    "end": "100119"
  },
  {
    "text": "in your own Services as well does that sound like a good plan all right so um",
    "start": "100119",
    "end": "106479"
  },
  {
    "text": "this is about the only overview slide I had um you know Azure functions was",
    "start": "106479",
    "end": "111799"
  },
  {
    "text": "designed from the ground up to be all about event driven apps event driven",
    "start": "111799",
    "end": "117200"
  },
  {
    "text": "architectures event driven scale you can see a number of scenarios I have uh up on the slide you know I I",
    "start": "117200",
    "end": "124840"
  },
  {
    "text": "think serverless and functions probably started as being uh a Handler for when",
    "start": "124840",
    "end": "131680"
  },
  {
    "text": "blobs are uploaded run a little bit of code if I think way back to when I first saw um AWS Lambda um and I wasn't",
    "start": "131680",
    "end": "138959"
  },
  {
    "text": "working on serverless at the time I I admired how like now you can not only upload data into your buckets but you",
    "start": "138959",
    "end": "146319"
  },
  {
    "text": "could also run a bit of code and do some processing I think that's where it started what's evolved into is what you",
    "start": "146319",
    "end": "151720"
  },
  {
    "text": "see here so you're doing real-time data processing you're having event streams",
    "start": "151720",
    "end": "157040"
  },
  {
    "text": "coming in fire hosing you with information and you have the chance to ingest that data analyze it transform it",
    "start": "157040",
    "end": "164159"
  },
  {
    "text": "do all the things you need to do um and a lot of times a something like a function is maybe a companion to the",
    "start": "164159",
    "end": "170560"
  },
  {
    "text": "rest of your application so um I've seen it before I don't see too many applications that are let's say 100%",
    "start": "170560",
    "end": "177360"
  },
  {
    "text": "functions but I see an awful lot of applications that have functions that help get data in transform and send data",
    "start": "177360",
    "end": "183680"
  },
  {
    "text": "out it's also really handy for simple apis like if you want to have some back-end processing even some heavy",
    "start": "183680",
    "end": "190440"
  },
  {
    "text": "lifting some computation doing that with functions can be simpler than making full-blown API apps um and now uh youve",
    "start": "190440",
    "end": "199120"
  },
  {
    "text": "probably heard a little bit about AI from from us at Microsoft and uh from the whole Community maybe you're working",
    "start": "199120",
    "end": "204599"
  },
  {
    "text": "on it too but um we're seeing functions are really interesting to handle the different workflows and orchestrations",
    "start": "204599",
    "end": "211480"
  },
  {
    "text": "you do in AI pipelines so like I said um functions has always had its superpower",
    "start": "211480",
    "end": "217000"
  },
  {
    "text": "for taking data in and when you work on an AI app let's say with rag the very",
    "start": "217000",
    "end": "222280"
  },
  {
    "text": "first step you do is you take a bunch of data in and you embed it and so functions become really handy for that",
    "start": "222280",
    "end": "228560"
  },
  {
    "text": "use case and also um if you look on the top right we have the ability to do",
    "start": "228560",
    "end": "234799"
  },
  {
    "text": "actual proper workflows and orchestrations if you kind of take a step back most any enterprise process",
    "start": "234799",
    "end": "241040"
  },
  {
    "text": "where first you do this then you do that maybe you do a few things after that and you need it to be durable and reliable",
    "start": "241040",
    "end": "246760"
  },
  {
    "text": "those end up looking like a workflow and for those of you who've tried to build one of those yourself you can do it but",
    "start": "246760",
    "end": "253400"
  },
  {
    "text": "it's a bit tricky to keep it reliable right you have to do queuing and outbox patterns and journaling and um event",
    "start": "253400",
    "end": "261759"
  },
  {
    "text": "sourcing and all kinds of patterns to make that happen and the cool thing is with something like durable functions or",
    "start": "261759",
    "end": "267360"
  },
  {
    "text": "also Dapper workflow we actually have the technology where you write the kind of code you want to write and you get",
    "start": "267360",
    "end": "274120"
  },
  {
    "text": "that kind of orchestration and journaling built in and you even get sagas and compensation logic just by",
    "start": "274120",
    "end": "280840"
  },
  {
    "text": "doing try catch style code and doing your compensation logic in in the catch so um so anyway here's a bunch of",
    "start": "280840",
    "end": "287440"
  },
  {
    "text": "scenarios that I think about for functions and you know the bottom line is we've learned over the last several",
    "start": "287440",
    "end": "293440"
  },
  {
    "text": "years from you we had to scale a lot better and I'm really excited we've been working on uh azure um functions Flex",
    "start": "293440",
    "end": "301039"
  },
  {
    "text": "consumption plan folks heard about Flex consumption in the room well I'm going to talk a lot about it but that's a",
    "start": "301039",
    "end": "307160"
  },
  {
    "text": "completely rebooted plan um completely rebooted architecture where we rewrote",
    "start": "307160",
    "end": "313080"
  },
  {
    "text": "functions to hit the next you know 10 levels of scale above what we've been doing um so now you can get to that like",
    "start": "313080",
    "end": "319880"
  },
  {
    "text": "burst of thousands of instances in under a minute so that's the overview um a few",
    "start": "319880",
    "end": "327400"
  },
  {
    "text": "things this is the structure of how I'm going to approach things today um I'm going to do kind of things a little bit",
    "start": "327400",
    "end": "333199"
  },
  {
    "text": "in the opposite order I'm going to show you some resources so that if you want to try the CLI commands look at the code",
    "start": "333199",
    "end": "339240"
  },
  {
    "text": "yourself while you're sitting here that's cool um and you'll have it and then I'm going to do a demo and then I'm",
    "start": "339240",
    "end": "345880"
  },
  {
    "text": "going to do a bunch of kind of going back and forth between architecture you know here's some architectural decisions",
    "start": "345880",
    "end": "351759"
  },
  {
    "text": "we made and then when I can I'll try to illustrate it with a demo or something tangible um and then we'll do some tips",
    "start": "351759",
    "end": "357960"
  },
  {
    "text": "and tricks okay okay so getting right into it um hopefully this is the only thing you'd have to screenshot because",
    "start": "357960",
    "end": "364600"
  },
  {
    "text": "I'll I'll share things afterwards um if you go here everything about what I'm going to show you um notes CLI commands",
    "start": "364600",
    "end": "372120"
  },
  {
    "text": "it's all up there and if I forget something you tell me and I I pop into the gist and it just goes there uh so",
    "start": "372120",
    "end": "378240"
  },
  {
    "text": "hopefully that helps all right so let's talk about testing at scale and the way I think about this is we need to be able",
    "start": "378240",
    "end": "384560"
  },
  {
    "text": "to scale in all the way to zero where you're in a serverless billing model you're not paying anything if there's no",
    "start": "384560",
    "end": "390919"
  },
  {
    "text": "executions but we need to scale out I I exaggerate I say to Infinity but you",
    "start": "390919",
    "end": "396000"
  },
  {
    "text": "want that near Infinity feeling where as much as you want to scale as much as you're willing to pay you're going to",
    "start": "396000",
    "end": "402039"
  },
  {
    "text": "get that horsepower from the cloud um and then you it needs to be elastic so you need to be able to scale huge scale",
    "start": "402039",
    "end": "408720"
  },
  {
    "text": "back in scale out scale in over and over and over again and that gives you the good economics of using",
    "start": "408720",
    "end": "415840"
  },
  {
    "text": "serverless okay so a little bit of setup for this the way I've thought about scale um first it's",
    "start": "415840",
    "end": "423000"
  },
  {
    "text": "always I have this motto with the team measure measure measure and so the things I want to measure for scale the",
    "start": "423000",
    "end": "429440"
  },
  {
    "text": "number one thing I always measure is throughput so it's like how many requests or how many messages per second",
    "start": "429440",
    "end": "436080"
  },
  {
    "text": "can my app handle and I think about that from the out from from the external",
    "start": "436080",
    "end": "441520"
  },
  {
    "text": "clients the clients are pumping messages in the clients are pumping in HTTP calls",
    "start": "441520",
    "end": "446960"
  },
  {
    "text": "and how much can I process per unit time that's the most critical and I want to at a very bare minimum I want to be able",
    "start": "446960",
    "end": "453479"
  },
  {
    "text": "to handle tens of thousands of requests per second but with a lot of scale out I'd actually like to be able to handle",
    "start": "453479",
    "end": "460240"
  },
  {
    "text": "millions and so uh that's the Benchmark that we thought about for latencies uh",
    "start": "460240",
    "end": "465280"
  },
  {
    "text": "cold start has been one of the things Azure functions has had some challenges with honestly and we've worked a lot on",
    "start": "465280",
    "end": "472000"
  },
  {
    "text": "that so we want to measure those latencies also um you know and being a little bit scientific about it we want",
    "start": "472000",
    "end": "478080"
  },
  {
    "text": "to look at a median or 50th percentile latency we also want to look at the 99th",
    "start": "478080",
    "end": "483720"
  },
  {
    "text": "percentile and just see how we're doing for our end users um we want to keep",
    "start": "483720",
    "end": "489120"
  },
  {
    "text": "those error rates uh down obviously that hurts resiliency and it also affects",
    "start": "489120",
    "end": "494280"
  },
  {
    "text": "your performance um I'm going to keep an eye on the CPU and memory the reason why I do that is while I'm running at this",
    "start": "494280",
    "end": "500479"
  },
  {
    "text": "throughput I want to see if any part of my Hardware stack is going hot I want to see if my CPU is maxing out I want to",
    "start": "500479",
    "end": "507960"
  },
  {
    "text": "see if my memory is maxing out because any one of these uh ceilings could be the reason why your app won't scale and",
    "start": "507960",
    "end": "514959"
  },
  {
    "text": "I'm going to go into it a lot but kind of a rule of thumb is how do you get to about 70% utilization of your Hardware",
    "start": "514959",
    "end": "521680"
  },
  {
    "text": "70% CPU 70% memory in a steady state um",
    "start": "521680",
    "end": "527080"
  },
  {
    "text": "so that you've got a little bit of room left your your users are getting a good",
    "start": "527080",
    "end": "532360"
  },
  {
    "text": "experience but you're also getting the value out of what you're paying for right so you want that high throughput",
    "start": "532360",
    "end": "538160"
  },
  {
    "text": "with high utilization and you're going to balance the two um another thing that's super",
    "start": "538160",
    "end": "543640"
  },
  {
    "text": "important and whenever we partner with customers who are working through scale",
    "start": "543640",
    "end": "549320"
  },
  {
    "text": "um it's important to look at the Q depth so if your if your app is based on cues",
    "start": "549320",
    "end": "554640"
  },
  {
    "text": "if it's bent if it's based on messages I like to look at how many messages are",
    "start": "554640",
    "end": "559959"
  },
  {
    "text": "piling up in the queue uh going on processed and that's also a very good",
    "start": "559959",
    "end": "565760"
  },
  {
    "text": "observable metric to know if you're scaling properly right because if you're not processing your work that means your",
    "start": "565760",
    "end": "572079"
  },
  {
    "text": "users are getting a bad experience or maybe you've even just configured the uh",
    "start": "572079",
    "end": "577279"
  },
  {
    "text": "the functions in a in a non-desirable way so even though you're giving lots of instances lots of power still you're not",
    "start": "577279",
    "end": "584399"
  },
  {
    "text": "actually processing and getting the results so it's kind of like almost an output metric is how low can you keep",
    "start": "584399",
    "end": "590839"
  },
  {
    "text": "that Q depth while you're keeping the throughput extremely high uh make sense so far so far so good",
    "start": "590839",
    "end": "598560"
  },
  {
    "text": "does this kind of reson maybe those are things you look at okay all right so um now getting more tangible here are the",
    "start": "598560",
    "end": "605200"
  },
  {
    "text": "exact goals that that my team set um with a new plan which by the way is",
    "start": "605200",
    "end": "610839"
  },
  {
    "text": "public it's in public beta right now um so the flex consumption plan we set out",
    "start": "610839",
    "end": "616079"
  },
  {
    "text": "to do 1,000 RPS as just table Stakes",
    "start": "616079",
    "end": "621160"
  },
  {
    "text": "absolute Baseline 1,000 requests per second but we want to be able to do millions of requests per second through",
    "start": "621160",
    "end": "628160"
  },
  {
    "text": "through scaleout through heavy load so that's the goal um and we literally uh",
    "start": "628160",
    "end": "634120"
  },
  {
    "text": "we actually run over 80,000 tests a day on all aspects and all SKS uh testing",
    "start": "634120",
    "end": "641040"
  },
  {
    "text": "the heck out of functions and also watching its Trends so that we know every day how close are we getting to",
    "start": "641040",
    "end": "646880"
  },
  {
    "text": "these goals for you um so we we make a huge investment it's a really big deal the next thing is we want to look at",
    "start": "646880",
    "end": "652839"
  },
  {
    "text": "bursting um if you look at elastic premium which is kind of the most expensive plan the best plan we've",
    "start": "652839",
    "end": "658200"
  },
  {
    "text": "offered to date um it can take I mean if you want a hundred instances it can take many",
    "start": "658200",
    "end": "665440"
  },
  {
    "text": "minutes it can take a half hour um and we we don't want that anymore so we wanted to be able to scale to a thousand",
    "start": "665440",
    "end": "672440"
  },
  {
    "text": "instances in under a minute that's the Benchmark that we want to do and we want to do even hundreds of",
    "start": "672440",
    "end": "678120"
  },
  {
    "text": "instances in a few seconds um and then you know think about the message types",
    "start": "678120",
    "end": "683880"
  },
  {
    "text": "we have both HTTP and non-http message types and they have very different kinds",
    "start": "683880",
    "end": "689760"
  },
  {
    "text": "of scale requirements you know like think about HTP comes in through a load balancer messages are in the background",
    "start": "689760",
    "end": "696079"
  },
  {
    "text": "so we need to be able to handle both kinds of cases and uh we don't want to just do hello world we want to do a real",
    "start": "696079",
    "end": "702200"
  },
  {
    "text": "beefy kind of application and we need to be able to do it on a private Network so this new plan uh Flex consumption",
    "start": "702200",
    "end": "709079"
  },
  {
    "text": "actually works on v-net with private endpoints so it's got the complete access to the private Network you can do",
    "start": "709079",
    "end": "715560"
  },
  {
    "text": "things securely if you're in in an Enterprise um and I'm going to talk about concurrency",
    "start": "715560",
    "end": "721320"
  },
  {
    "text": "in a bit but concurrency is one of the other magic dials we have to to further optimize are you getting the most right",
    "start": "721320",
    "end": "729120"
  },
  {
    "text": "sizing your instances okay so that's the goals now let's talk about an actual",
    "start": "729120",
    "end": "735240"
  },
  {
    "text": "test that we're going to do right now so what I'm going to do I've got an endtoend it's simple but it's a real",
    "start": "735240",
    "end": "741480"
  },
  {
    "text": "endtoend application that's going to process Telemetry so I have a big",
    "start": "741480",
    "end": "746880"
  },
  {
    "text": "application let's say a big distributed web application it's creating all kinds of telemetry as",
    "start": "746880",
    "end": "751959"
  },
  {
    "text": "users interact with it and it's going to send the Telemetry as HTP post with Json",
    "start": "751959",
    "end": "758600"
  },
  {
    "text": "having a payload of you know here's the user uh anonymized here is um any uh you",
    "start": "758600",
    "end": "765880"
  },
  {
    "text": "know click metrics they have here's any feedback they have here's their experience and we send it through the system now what the HTP function is",
    "start": "765880",
    "end": "772880"
  },
  {
    "text": "going to do it's going to transform and convert those into vent Hub stream",
    "start": "772880",
    "end": "778000"
  },
  {
    "text": "messages and I have a number of Downstream analytics um and persistence",
    "start": "778000",
    "end": "784199"
  },
  {
    "text": "mechanisms that'll basically let me store and work with the Telemetry after it's done so it's kind of like data",
    "start": "784199",
    "end": "789600"
  },
  {
    "text": "ingestion with analytics that's what this function is doing and for a load",
    "start": "789600",
    "end": "794680"
  },
  {
    "text": "test I'm going to use the the new Azure load test which is a kind of a reboot of what we used to have um with Azure",
    "start": "794680",
    "end": "802600"
  },
  {
    "text": "devops but now there's a a new Azure native one and that one we're going to set up to do a th000 plus RPS with four",
    "start": "802600",
    "end": "810560"
  },
  {
    "text": "agents 250 users each we're going to do HTTP posts um I'm using Flex consumption",
    "start": "810560",
    "end": "816160"
  },
  {
    "text": "Flex consumption is inherently Linux I'm going to use the default skew which is 2 gigabytes um I just happen to use net 8",
    "start": "816160",
    "end": "824720"
  },
  {
    "text": "we uh we support all the languages that function supports so don't stress that I did net 8 but I wanted to show you that",
    "start": "824720",
    "end": "831079"
  },
  {
    "text": "one um and as I show like mechanically this is an HTTP trigger in and it's uh",
    "start": "831079",
    "end": "836759"
  },
  {
    "text": "event Hub bindings out so it's a it's an example of using multiple of the",
    "start": "836759",
    "end": "841800"
  },
  {
    "text": "triggers and bindings together which is often times what we see in functions all right um and like I said I'm going to",
    "start": "841800",
    "end": "848079"
  },
  {
    "text": "use the metrics that I'm looking before I want to look at that throughput I want to look at the CPU um I'm going to look at the scale logs and and all kinds of",
    "start": "848079",
    "end": "854880"
  },
  {
    "text": "stuff at the same time all right so that's the that's the approach enough PowerPoint let's go to a",
    "start": "854880",
    "end": "861079"
  },
  {
    "text": "demo so I've got this application let's make it a",
    "start": "861079",
    "end": "866720"
  },
  {
    "text": "little bigger",
    "start": "866720",
    "end": "869920"
  },
  {
    "text": "um so I have this end to end application that I described it's up on GitHub now",
    "start": "874360",
    "end": "880440"
  },
  {
    "text": "so we call this one HTTP vet event Hub so again it's htpn event hubs out everything on a",
    "start": "880440",
    "end": "887680"
  },
  {
    "text": "v-net and the core function code here if",
    "start": "887680",
    "end": "892759"
  },
  {
    "text": "I go down to the source um I have this process customer",
    "start": "892759",
    "end": "897839"
  },
  {
    "text": "feedback and it's incred inedibly simple but again I'm seeing that I've got an HTP trigger in um I'm going to process",
    "start": "897839",
    "end": "904160"
  },
  {
    "text": "the data I could do any more transforms I want and then I'm going to use the Tuple syntax in C so that I'm both going",
    "start": "904160",
    "end": "911920"
  },
  {
    "text": "to send a proper HTTP Response Code like a 200 and I'm also going to send an",
    "start": "911920",
    "end": "917079"
  },
  {
    "text": "event so it's a it's you can actually have multiple output bindings in functions I'm going to send multiple",
    "start": "917079",
    "end": "922440"
  },
  {
    "text": "outputs and on the output as you can see here I'm simply sending to an event hub",
    "start": "922440",
    "end": "929319"
  },
  {
    "text": "topic subscription and so any subscribers like uh databases other",
    "start": "929319",
    "end": "934440"
  },
  {
    "text": "analytic streams can now listen to this Telemetry streams bigot as it comes through so it's a pretty simple function",
    "start": "934440",
    "end": "940440"
  },
  {
    "text": "and the payload here's what it looks like so for my load test I'm going to",
    "start": "940440",
    "end": "946440"
  },
  {
    "text": "have feedback that kind of looks like this so we have both Telemetry and feedback users can submit feedback",
    "start": "946440",
    "end": "953199"
  },
  {
    "text": "telemetries collected by the system and this is a sampling of what that feedback will look like um before we process",
    "start": "953199",
    "end": "961199"
  },
  {
    "text": "it all right now I'm going to go over the portal and I've got all kinds of resource groups today to try to pull",
    "start": "961199",
    "end": "967199"
  },
  {
    "text": "things out of the oven for you um let's focus on this one first this is the deployed",
    "start": "967199",
    "end": "972319"
  },
  {
    "text": "app um and all the things you can kind of expect are here um I've got my",
    "start": "972319",
    "end": "978199"
  },
  {
    "text": "function itself um I've got my event hubs namespace my vent Hub is in there um",
    "start": "978199",
    "end": "985079"
  },
  {
    "text": "I've got all the monitoring and I've got even private networking so we're seeing where this this is all behind a v-net",
    "start": "985079",
    "end": "990560"
  },
  {
    "text": "and I've got a load test um let's go up to the load",
    "start": "990560",
    "end": "996160"
  },
  {
    "text": "test bear with me at this resolution it's a",
    "start": "996160",
    "end": "1000720"
  },
  {
    "text": "little a little tricky okay in this one it looks like I don't have it but that's no problem so",
    "start": "1002880",
    "end": "1008959"
  },
  {
    "text": "we made it easy in functions if I click on this Better",
    "start": "1008959",
    "end": "1016279"
  },
  {
    "text": "Together tab forgive us for the name we're working on on it but if you want to attach this this way I joke cuz the",
    "start": "1016279",
    "end": "1023040"
  },
  {
    "text": "the team was supposed to be like yeah like let's put some Services together that are better together that's the strategy but then all of a sudden it",
    "start": "1023040",
    "end": "1028760"
  },
  {
    "text": "showed up in the UI um anyway life's a journey so so",
    "start": "1028760",
    "end": "1034798"
  },
  {
    "text": "let's create a load test which is better together and we are going to create the",
    "start": "1034799",
    "end": "1040360"
  },
  {
    "text": "test um this actually pretty pretty cool like it already has the context of my function that I want to load test um so",
    "start": "1040360",
    "end": "1047839"
  },
  {
    "text": "when I go to add a request all I have to do is like give this thing a name so let's call it process",
    "start": "1047839",
    "end": "1055640"
  },
  {
    "text": "feedback and I pick the actual function that's deployed process customer feedback and you can see it's already",
    "start": "1055640",
    "end": "1062000"
  },
  {
    "text": "figured out the route um and it knows that it's a post and and then I can go to the body",
    "start": "1062000",
    "end": "1069720"
  },
  {
    "text": "and I can push the payload that I want to let's just use this sample that I already had",
    "start": "1069720",
    "end": "1076360"
  },
  {
    "text": "all right so I have an array of Json objects that'll get pushed in and that's my load test let's do I",
    "start": "1076360",
    "end": "1084679"
  },
  {
    "text": "mentioned I wanted to get over a th RPS I know just by kind of reading the docs",
    "start": "1084679",
    "end": "1090559"
  },
  {
    "text": "that if I have four instances with 250 users per I'm going",
    "start": "1090559",
    "end": "1097679"
  },
  {
    "text": "to get at least a th I'll I'll probably actually get more and so that'll be a pretty sweet",
    "start": "1097679",
    "end": "1102880"
  },
  {
    "text": "load test it also you can do these different like do you want linear do you want steps let's just do linear so but",
    "start": "1102880",
    "end": "1108880"
  },
  {
    "text": "that's just going to means over a function of seconds over 5 minutes it's going to do a ramp up and actually",
    "start": "1108880",
    "end": "1115720"
  },
  {
    "text": "that's 20 minutes which my boss might not love the cost that comes from that so let's dial it to",
    "start": "1115720",
    "end": "1122400"
  },
  {
    "text": "five he won't mind all right let's go and we'll do a",
    "start": "1122400",
    "end": "1128120"
  },
  {
    "text": "create so um that's going to create the load test and I'll probably have to find",
    "start": "1128120",
    "end": "1134080"
  },
  {
    "text": "some other things to talk about while I do that because this takes about two minutes so let's look at our clock",
    "start": "1134080",
    "end": "1139360"
  },
  {
    "text": "um 319 all right while we're doing that let's get set up on the other side so",
    "start": "1139360",
    "end": "1145159"
  },
  {
    "text": "let's look at how do we monitor this thing um I've got way too many resource",
    "start": "1145159",
    "end": "1151240"
  },
  {
    "text": "groups so this is the resource Group we were looking at right this flexvent hubs",
    "start": "1151240",
    "end": "1157200"
  },
  {
    "text": "Oslo flexvent hubs Oslo and let's go to app",
    "start": "1157200",
    "end": "1162760"
  },
  {
    "text": "insights um app insights just so you know this is the monitoring product that",
    "start": "1162760",
    "end": "1168280"
  },
  {
    "text": "I would say like is really built in and just super well integrated with functions you know if you ever want to",
    "start": "1168280",
    "end": "1173640"
  },
  {
    "text": "look at Telemetry you other want to look at logs uh distributed traces they're all in here and um this is kind of a",
    "start": "1173640",
    "end": "1181080"
  },
  {
    "text": "cool thing you probably know this but not only can I do things like see actual transactions that have",
    "start": "1181080",
    "end": "1187679"
  },
  {
    "text": "happened in in the last few minutes or the last few hours um I get visuals over",
    "start": "1187679",
    "end": "1193280"
  },
  {
    "text": "the topology I guess I need to send some traffic to it um and I get some live",
    "start": "1193280",
    "end": "1200360"
  },
  {
    "text": "metrics so as soon as I start producing traffic I'm going to see them",
    "start": "1200360",
    "end": "1205400"
  },
  {
    "text": "here and that's part of my setup so I don't see any yet but that's expected it's still provisioning my load",
    "start": "1205400",
    "end": "1212400"
  },
  {
    "text": "test um the load test actually I could I could talk about this for a second so I actually worked on starting the load",
    "start": "1212400",
    "end": "1218320"
  },
  {
    "text": "test service too and it's built on top of a kubernetes cluster in the kubernetes cluster is actually serving",
    "start": "1218320",
    "end": "1224440"
  },
  {
    "text": "up J meter you folks use or know J meter so um we wanted this load test to be a",
    "start": "1224440",
    "end": "1230280"
  },
  {
    "text": "lot more flexible and it just looked like the community had had bet pretty big on J meter throughout the years",
    "start": "1230280",
    "end": "1236360"
  },
  {
    "text": "there's other really good ones too like there's k6 and and other upand comers but we went with what most of the",
    "start": "1236360",
    "end": "1242600"
  },
  {
    "text": "community understood and that's J meter so when I run when I run this load test and I even set it up remember I kind of",
    "start": "1242600",
    "end": "1249240"
  },
  {
    "text": "it said here's your function here's the post here's the payload what that actually did is it outputed a jmx file",
    "start": "1249240",
    "end": "1254919"
  },
  {
    "text": "it created a j meter script and that's fully customizable so if you have your own jmeter that's done more of a like a",
    "start": "1254919",
    "end": "1262240"
  },
  {
    "text": "capture of live traffic or if you know how to customize J meter you can plug it in here too and this engine what it's",
    "start": "1262240",
    "end": "1269039"
  },
  {
    "text": "responsible for is like creating a fan out a huge scale of that load with a bunch of regional instances pounding",
    "start": "1269039",
    "end": "1275159"
  },
  {
    "text": "that endpoint um and then it does the monitoring of course so that's a little bit about the low test let's see how",
    "start": "1275159",
    "end": "1281120"
  },
  {
    "text": "we're doing here we're not we're not quite there all right so well I'll fill a little bit more time um I mentioned",
    "start": "1281120",
    "end": "1288000"
  },
  {
    "text": "some of those so the the request per second the big one I'm going to be looking at here is",
    "start": "1288000",
    "end": "1293360"
  },
  {
    "text": "the request rate and we're also going to keep an eye on the servers now this is a really fun one oh here we go watch this",
    "start": "1293360",
    "end": "1301000"
  },
  {
    "text": "looks like it's going so we're ramping up right",
    "start": "1301000",
    "end": "1306400"
  },
  {
    "text": "now so pay attention here and also watch the servers as it needs more servers",
    "start": "1306400",
    "end": "1312240"
  },
  {
    "text": "it's going to really quickly burst them look at that just went from 3 to 30 in just a few seconds",
    "start": "1312240",
    "end": "1319480"
  },
  {
    "text": "and now we're reaching a steady state of 2,000 requests per second um so this pretty awesome like this is um we were",
    "start": "1319480",
    "end": "1327720"
  },
  {
    "text": "we totally couldn't do this before like consumption um functions couldn't give",
    "start": "1327720",
    "end": "1333200"
  },
  {
    "text": "you this many servers and it couldn't give you this this fast and again even elastic premium really struggled like",
    "start": "1333200",
    "end": "1340039"
  },
  {
    "text": "you'd be getting you know maybe 10 instances per minute but now we can do a thousand so you're you're kind of seeing",
    "start": "1340039",
    "end": "1345799"
  },
  {
    "text": "this live um some other things I'm going to be looking at if I scroll down here I",
    "start": "1345799",
    "end": "1351360"
  },
  {
    "text": "want to make sure that not only am I ingesting those messages but also I'm making a good use of my hardware and the",
    "start": "1351360",
    "end": "1358039"
  },
  {
    "text": "and things are healthy so I see that my exception rate um is is zero which is",
    "start": "1358039",
    "end": "1363279"
  },
  {
    "text": "good so I'm not getting user exceptions I'm not getting functional exceptions and I'm not getting scaling exceptions",
    "start": "1363279",
    "end": "1368799"
  },
  {
    "text": "like I'm not seeing rate limiting you want to watch that one and make sure you're not getting rate limited all right and then here I'm looking at my",
    "start": "1368799",
    "end": "1375120"
  },
  {
    "text": "memory and my CPU percentage and um notice how it started with a lower",
    "start": "1375120",
    "end": "1381039"
  },
  {
    "text": "number of servers and the utilization was about 50% then as it added close to 89 servers",
    "start": "1381039",
    "end": "1388640"
  },
  {
    "text": "um the CPU went down let's call that what do you think 25 it's in the 25%",
    "start": "1388640",
    "end": "1393960"
  },
  {
    "text": "median um I'd say that's okay so like we accomplished one goal like we scaled",
    "start": "1393960",
    "end": "1399520"
  },
  {
    "text": "incredibly well no problem but the thing I'm going to ding us on right now is we're not being very efficient we're",
    "start": "1399520",
    "end": "1405880"
  },
  {
    "text": "only using 25% of the hardware we're already paying for for and I want to be as efficient as possible I want you guys",
    "start": "1405880",
    "end": "1411080"
  },
  {
    "text": "to save money um and only pay for what you need and that's going to be the magic of this thing so the way we're",
    "start": "1411080",
    "end": "1417559"
  },
  {
    "text": "going to address this is through concurrency and that's the very next topic but I just kind of want to place a mental note like we accomplish one goal",
    "start": "1417559",
    "end": "1424679"
  },
  {
    "text": "but we didn't accomplish every goal um but pretty cool that's Flex consumption",
    "start": "1424679",
    "end": "1430640"
  },
  {
    "text": "like that okay so definitely for for I would say in general when you want to do a vent driven on Azure start with flex",
    "start": "1430640",
    "end": "1437919"
  },
  {
    "text": "consumption functions um like I said it's in beta now it's going to go ga before the end of this calendar year and",
    "start": "1437919",
    "end": "1444120"
  },
  {
    "text": "it's just built from the ground up for this scenario all right so let's go back to",
    "start": "1444120",
    "end": "1451320"
  },
  {
    "text": "slides and all right so what we just",
    "start": "1451320",
    "end": "1458559"
  },
  {
    "text": "did that's not the right slides I'm sorry I've got even better more special",
    "start": "1458559",
    "end": "1464200"
  },
  {
    "text": "slides for you there we",
    "start": "1464200",
    "end": "1471080"
  },
  {
    "text": "go all right so we did the load test um what I'm going to show you here",
    "start": "1472159",
    "end": "1479080"
  },
  {
    "text": "this is a real example of a customer that we've been working with since the beginning of this calendar year so like",
    "start": "1479080",
    "end": "1484760"
  },
  {
    "text": "early about January February in 2024 we've been working with a little website",
    "start": "1484760",
    "end": "1490200"
  },
  {
    "text": "you might have heard of called GitHub and GitHub they did this exact scenario",
    "start": "1490200",
    "end": "1495720"
  },
  {
    "text": "that I just showed you they're doing Telemetry ingestion for all the GitHub sub apps that live on it right so um",
    "start": "1495720",
    "end": "1503120"
  },
  {
    "text": "let's say you know you're working with GitHub actions let's say you're working with uh the source code control you can",
    "start": "1503120",
    "end": "1510520"
  },
  {
    "text": "think of all the different apps and stuff they have that creates a whole bunch of telemetry and what they found",
    "start": "1510520",
    "end": "1516840"
  },
  {
    "text": "is that their message and Q ages got way too high they just couldn't keep up with it but they're hopeful to do this using",
    "start": "1516840",
    "end": "1523799"
  },
  {
    "text": "something like functions that's serverless and they didn't want to stand up kubernetes these clusters that they",
    "start": "1523799",
    "end": "1529360"
  },
  {
    "text": "really had to do a lot of operations just for Telemetry right because what they want to do is they want to run GitHub GitHub is their main business",
    "start": "1529360",
    "end": "1536399"
  },
  {
    "text": "they don't want to run Telemetry Telemetry is just a necessary evil to operate a service right so um so they",
    "start": "1536399",
    "end": "1544039"
  },
  {
    "text": "kind of took a bet early as we were new software as as you can see here they were able to achieve 1.6 million",
    "start": "1544039",
    "end": "1551440"
  },
  {
    "text": "requests per second as a steady state they're actually doing even a little bit higher now so it's cool they're a great",
    "start": "1551440",
    "end": "1557480"
  },
  {
    "text": "design partner because as we find issues we're finding them early with GitHub and we're able to get more scale and more",
    "start": "1557480",
    "end": "1563000"
  },
  {
    "text": "efficiency and another thing about this graph that's pretty fun is when you see that the the scale kicks in look at um",
    "start": "1563000",
    "end": "1569760"
  },
  {
    "text": "you know the y- AIS is basically showing you the message depth how many messages you have it's just boom it's like",
    "start": "1569760",
    "end": "1576399"
  },
  {
    "text": "plowing through that q and keeping them at a low Q age Q depth um as a steady",
    "start": "1576399",
    "end": "1583480"
  },
  {
    "text": "state so that's as you even measure your own performance this is the kind of graph that you'll want to see you want",
    "start": "1583480",
    "end": "1589039"
  },
  {
    "text": "to see that you can quickly take care of any backlog and then go steady as",
    "start": "1589039",
    "end": "1594120"
  },
  {
    "text": "messages are coming in you're knocking them down all right so let's talk about the architecture um my team might get a",
    "start": "1594120",
    "end": "1602399"
  },
  {
    "text": "little bad at me for this I'm going to like show a bunch of internal stuff to you but I think it's important um maybe",
    "start": "1602399",
    "end": "1608360"
  },
  {
    "text": "it'll increase your confidence in the service because I want you to know how this works and um you can kind of",
    "start": "1608360",
    "end": "1614399"
  },
  {
    "text": "understand some of the things we went through and maybe that'll help you as you build these apps as well okay so",
    "start": "1614399",
    "end": "1620120"
  },
  {
    "text": "let's go through the architecture um it's a bit of a what we call an ey chart there's a lot of stuff",
    "start": "1620120",
    "end": "1626120"
  },
  {
    "text": "going on but on the left hand side what we're seeing is our anies infrastructure",
    "start": "1626120",
    "end": "1633080"
  },
  {
    "text": "anies is our code name for anything that has to do with app services or anything",
    "start": "1633080",
    "end": "1639440"
  },
  {
    "text": "that has to do with functions over the years th this this is the compute infrastructure that runs app service and",
    "start": "1639440",
    "end": "1645840"
  },
  {
    "text": "that runs functions um like literally gives you the instances literally gives you the VMS and stuff like that and also",
    "start": "1645840",
    "end": "1652600"
  },
  {
    "text": "each like colored box represents a ro or a microservice um inside of the stamps",
    "start": "1652600",
    "end": "1659520"
  },
  {
    "text": "that actually make this thing run so it's interesting um we talk about microservices functions is an",
    "start": "1659520",
    "end": "1665399"
  },
  {
    "text": "alternative to microservices that makes your life a little bit easy but it's actually built on top of microservices",
    "start": "1665399",
    "end": "1671080"
  },
  {
    "text": "so we actually use that pattern ourselves and that's how we achieve some of the horizontal scale internally as",
    "start": "1671080",
    "end": "1678200"
  },
  {
    "text": "different parts of the system go hot all right so focusing on the left side um",
    "start": "1678200",
    "end": "1683320"
  },
  {
    "text": "this is this is effectively the set of controllers the set of bossy microservices that manage the rest of",
    "start": "1683320",
    "end": "1690000"
  },
  {
    "text": "the fleet and as I get to the right hand side the right hand side is where your workers worker in my vocabulary all that",
    "start": "1690000",
    "end": "1698120"
  },
  {
    "text": "means is your application your zip file that only you could write those run as",
    "start": "1698120",
    "end": "1703320"
  },
  {
    "text": "workers everything else is the host so the function service is the host your",
    "start": "1703320",
    "end": "1708440"
  },
  {
    "text": "code runs in workers so on the left side we'll focus on the host um we have this",
    "start": "1708440",
    "end": "1713760"
  },
  {
    "text": "microservice that we call the front end in the front end as you would expect that's the front door for your end users",
    "start": "1713760",
    "end": "1720519"
  },
  {
    "text": "when they make HTTP requests everything comes through that front door right when you saw that um you know even the web",
    "start": "1720519",
    "end": "1729120"
  },
  {
    "text": "browser let me come out of here for just a second just to kind of visualize it a",
    "start": "1729120",
    "end": "1734679"
  },
  {
    "text": "little bit um if I Peak at my function",
    "start": "1734679",
    "end": "1741640"
  },
  {
    "text": "itself uh let's go down here an HTTP function comes through a",
    "start": "1741640",
    "end": "1750480"
  },
  {
    "text": "URL right and this domain this azurewebsites.net everything goes",
    "start": "1750480",
    "end": "1755519"
  },
  {
    "text": "through azurewebsites.net that's our front door so it's all going through our load balancer and the good thing about",
    "start": "1755519",
    "end": "1761799"
  },
  {
    "text": "that kind of like any proxy since everything goes through that proxy we have a lot more data about what you're",
    "start": "1761799",
    "end": "1767720"
  },
  {
    "text": "end users are doing we can monitor it we can understand how much scale needs to go through it and we can do a lot of",
    "start": "1767720",
    "end": "1774000"
  },
  {
    "text": "placement right as traffic comes in we can route it we can place it based on the load and based on a number of things",
    "start": "1774000",
    "end": "1780840"
  },
  {
    "text": "um so come back here I'm going to keep doing this hopefully it doesn't irritate you but I'm going to try to show you",
    "start": "1780840",
    "end": "1787679"
  },
  {
    "text": "visuals as I talk through the architecture",
    "start": "1787679",
    "end": "1792919"
  },
  {
    "text": "um and sorry my hotkey didn't work okay so so that's the front end so",
    "start": "1792919",
    "end": "1799120"
  },
  {
    "text": "the front end it load balances the traffic and it monitors the traffic that's very important and then it it's",
    "start": "1799120",
    "end": "1806039"
  },
  {
    "text": "got a subcomponent within it called a scale controller and the scale controller basically is looking at the",
    "start": "1806039",
    "end": "1812600"
  },
  {
    "text": "traffic and also looking at your function app itself like oh it's an HTP app or it's an event Hub app and here's",
    "start": "1812600",
    "end": "1819600"
  },
  {
    "text": "the concurrency that it's expecting So based on that I'm going to make a decision and it actually votes the scale",
    "start": "1819600",
    "end": "1826279"
  },
  {
    "text": "controller votes should you have more instances should you have left less and it's the thing kind of orchestrating the",
    "start": "1826279",
    "end": "1833360"
  },
  {
    "text": "whole rest of the system so it's making votes then we have the data role that's",
    "start": "1833360",
    "end": "1838600"
  },
  {
    "text": "the next Green Box on the left the data Ro is responsible for placement it",
    "start": "1838600",
    "end": "1843960"
  },
  {
    "text": "places and schedules your work and it's going to do a few things like it's going to talk to the stamps on the right hand",
    "start": "1843960",
    "end": "1851600"
  },
  {
    "text": "side to see how much capacity you have and it's going to make a decision if you need more less or need to stay the same",
    "start": "1851600",
    "end": "1858000"
  },
  {
    "text": "so it's in constant communication with the fleet deciding how much compute is available to you and uh that's how we",
    "start": "1858000",
    "end": "1865240"
  },
  {
    "text": "keep you efficient that's how we keep you elastically scaling now being in the",
    "start": "1865240",
    "end": "1870440"
  },
  {
    "text": "business that I'm in is a little bit rough because to to make that kind of experience happened for you I'm managing",
    "start": "1870440",
    "end": "1876799"
  },
  {
    "text": "a huge pool and Fleet of VM servers on the right hand side a gigantic one",
    "start": "1876799",
    "end": "1882279"
  },
  {
    "text": "that's how we pull this all off um and that's why I wouldn't recommend that everyone here goes and builds a seress",
    "start": "1882279",
    "end": "1888279"
  },
  {
    "text": "paths because it's tricky these are the kinds of things you have to deal with um but it's worth it for us we we got there",
    "start": "1888279",
    "end": "1895000"
  },
  {
    "text": "um the next microservice is the controller and the functions pod service",
    "start": "1895000",
    "end": "1900440"
  },
  {
    "text": "or FPS you could kind of think of that as the data roll is being bossy it's saying hey give me some more servers",
    "start": "1900440",
    "end": "1907880"
  },
  {
    "text": "take some servers away and drain them it's being bossy requesting what it wants the controller is actually doing",
    "start": "1907880",
    "end": "1914320"
  },
  {
    "text": "the work on the right hand side to do those life cycles events it will create functions it will start functions it",
    "start": "1914320",
    "end": "1920960"
  },
  {
    "text": "will stop them it will drain them it will destroy them it could do all the actual actions so it's actually kind of",
    "start": "1920960",
    "end": "1928639"
  },
  {
    "text": "the uh technical implementation of how we pull your applications where the data",
    "start": "1928639",
    "end": "1935159"
  },
  {
    "text": "roll is more saying here's the desired state so data roll is saying here's the state I want the FPS is actually",
    "start": "1935159",
    "end": "1942360"
  },
  {
    "text": "implementing it and then at the bottom we have some databases historically when we started out we did All State",
    "start": "1942360",
    "end": "1948960"
  },
  {
    "text": "management using SQL server and SQL Server is a great backend um and when I say State Management there's things like",
    "start": "1948960",
    "end": "1955120"
  },
  {
    "text": "the life cycle like what State Should your your VMS and your functions be in",
    "start": "1955120",
    "end": "1960559"
  },
  {
    "text": "um is there any transaction that started well we need to journal it to keep it reliable so all that State Management",
    "start": "1960559",
    "end": "1966159"
  },
  {
    "text": "has been done in SQL but we decided to kind of pull off this kind of scale we",
    "start": "1966159",
    "end": "1971519"
  },
  {
    "text": "added Cosmos DB because it actually has the the best latency characteristics of",
    "start": "1971519",
    "end": "1977600"
  },
  {
    "text": "all the databases in our Fleet it almost behaves like a cache so when you're doing State Management working with",
    "start": "1977600",
    "end": "1983919"
  },
  {
    "text": "something like a cache is really desirable we could have used redus as well we evaluated it but uh Cosmos gave",
    "start": "1983919",
    "end": "1990159"
  },
  {
    "text": "us the experience we wanted and it did it regionally which is what we also wanted um so that's that side and then",
    "start": "1990159",
    "end": "1996600"
  },
  {
    "text": "on the right hand side I'm going to do another double click on this but we have something called Legion and Legion is",
    "start": "1996600",
    "end": "2004080"
  },
  {
    "text": "our custom compute that we built for you guys just for event driven in container",
    "start": "2004080",
    "end": "2009880"
  },
  {
    "text": "scenarios so you know if you kind of think of ordinary compute um you can create a VM or even a VM scale set which",
    "start": "2009880",
    "end": "2017519"
  },
  {
    "text": "gives you large units of virtual machines that are pretty static you know they take a few minutes to create um and",
    "start": "2017519",
    "end": "2025240"
  },
  {
    "text": "they're good for a set of workloads but in our case we want an event driven function an event to come in boom create",
    "start": "2025240",
    "end": "2030919"
  },
  {
    "text": "the container create the app process the request and then go away so it's much more ephemeral it's much faster and so",
    "start": "2030919",
    "end": "2038399"
  },
  {
    "text": "the way that we do that is we take a whole bunch of VM scale sets and we subdivide them and we chunk them into",
    "start": "2038399",
    "end": "2045480"
  },
  {
    "text": "nested VMS so we use hyperv nesting where you have hyperv running inside of",
    "start": "2045480",
    "end": "2050760"
  },
  {
    "text": "hyperv and we Chunk Up This Fleet of vmms and that microv VM now becomes your",
    "start": "2050760",
    "end": "2058679"
  },
  {
    "text": "pod that that's the place where your code can run and we actually literally do use containers as well just for our",
    "start": "2058679",
    "end": "2065158"
  },
  {
    "text": "own devops efficiency and for our own speed so um we'll literally carve up",
    "start": "2065159",
    "end": "2070599"
  },
  {
    "text": "microv VMS that can run PODS of containers and that in the end is where your function is going to run and we're",
    "start": "2070599",
    "end": "2078079"
  },
  {
    "text": "doing that kind of orchestration on your behalf of creating pods um and there's",
    "start": "2078079",
    "end": "2083480"
  },
  {
    "text": "some complexities here too I'm going to get into it more but like if you're running different run times if you're",
    "start": "2083480",
    "end": "2088878"
  },
  {
    "text": "running net 6 net 8 um uh python you",
    "start": "2088879",
    "end": "2094679"
  },
  {
    "text": "know node 22 Java all these things like they need different container images and",
    "start": "2094679",
    "end": "2100720"
  },
  {
    "text": "they need different specializations so we actually need to maintain pools of every permutation you can think of and",
    "start": "2100720",
    "end": "2107839"
  },
  {
    "text": "so and we call we'll I'll get into this we call these placeholders but one of the big tricks we do to get past cold",
    "start": "2107839",
    "end": "2114320"
  },
  {
    "text": "start um headaches is we create these placeholder pools that are like extremely close probably to what you're",
    "start": "2114320",
    "end": "2121040"
  },
  {
    "text": "going to run the only thing that's missing is your actual application so we can specialize it and and serve it to",
    "start": "2121040",
    "end": "2127520"
  },
  {
    "text": "you um so a lot of the game with functions and serverless it's not about making the",
    "start": "2127520",
    "end": "2133320"
  },
  {
    "text": "VM start quickly it's about having the VM already started and then specializing",
    "start": "2133320",
    "end": "2140839"
  },
  {
    "text": "it super quickly with your app that's actually the game that we're in and as you can see too there's a lot more going",
    "start": "2140839",
    "end": "2147359"
  },
  {
    "text": "on than just optimizing a VM because we have to optimize the load balancer we have to optimize um the concurrency we",
    "start": "2147359",
    "end": "2155200"
  },
  {
    "text": "have to optimize the number of instances you have we have to do State Management so there's so much more to it and and",
    "start": "2155200",
    "end": "2161520"
  },
  {
    "text": "that's really kind of the business that we're in so a little bit more vocabulary I'll spend just a little bit of time on",
    "start": "2161520",
    "end": "2167280"
  },
  {
    "text": "this so kind of explaining each thing I talked about you know the front end is responsible for all the HTP requests the",
    "start": "2167280",
    "end": "2173599"
  },
  {
    "text": "scale controller is doing the monitoring making the decisions the data role manages the life cycle of the workers",
    "start": "2173599",
    "end": "2180319"
  },
  {
    "text": "the controllers can create stop and start the the FPS or the function pod",
    "start": "2180319",
    "end": "2186280"
  },
  {
    "text": "service will actually man manage those placeholder pools and then we have State Management that's done with Cosmos DB",
    "start": "2186280",
    "end": "2192920"
  },
  {
    "text": "and we have Legion which is the actual container dispenser so now that we have a few of those kind of Primitives in our",
    "start": "2192920",
    "end": "2199000"
  },
  {
    "text": "mind let's see it come together in an endent I'm going to do two comparisons I'm going to do it before and after this",
    "start": "2199000",
    "end": "2205160"
  },
  {
    "text": "is before and a after I'll show you after so before we created Flex consumption the these are like if I use",
    "start": "2205160",
    "end": "2212599"
  },
  {
    "text": "a consumption plan today or if I use elastic premium today or app service today this is what happens so let's say",
    "start": "2212599",
    "end": "2218480"
  },
  {
    "text": "that HTTP traffic is coming in it's going to first on the left side it's",
    "start": "2218480",
    "end": "2223800"
  },
  {
    "text": "going to hit that load balancer it's going to hit that um that front end roll",
    "start": "2223800",
    "end": "2229560"
  },
  {
    "text": "and then the front end roll is going to basically ping the workers to see how many are there and to see if they're",
    "start": "2229560",
    "end": "2237040"
  },
  {
    "text": "healthy so it's basically trying to gauge the health of the cluster and if you have enough um and then based on",
    "start": "2237040",
    "end": "2243359"
  },
  {
    "text": "that it's going to make a scale decision using the pings now what could possibly go wrong having a a massive scale Fleet",
    "start": "2243359",
    "end": "2252040"
  },
  {
    "text": "where you're using pings as monitoring a lot could go wrong A lot",
    "start": "2252040",
    "end": "2258359"
  },
  {
    "text": "has gone wrong so that it turns out that was not the great greatest uh monitoring",
    "start": "2258359",
    "end": "2263400"
  },
  {
    "text": "mechanism and that's part of the reason why I think functions has had challenges",
    "start": "2263400",
    "end": "2268599"
  },
  {
    "text": "scaling and delivering the kind of instances you wanted it's because of this kind of a mechanism it got us you",
    "start": "2268599",
    "end": "2274760"
  },
  {
    "text": "know when you build a SAS service you kind of think about an architect sure they'll carry you a year or two but we definitely hit the point where we had to",
    "start": "2274760",
    "end": "2281119"
  },
  {
    "text": "rebuild it to to give it to give you the exact experience you want all right so",
    "start": "2281119",
    "end": "2286560"
  },
  {
    "text": "let's look at the latest approach this is flex consumption um so Flex consumption it's",
    "start": "2286560",
    "end": "2292079"
  },
  {
    "text": "a similar workflow traffic is going to come in through the load balancer it's going to hit the front ends and then L",
    "start": "2292079",
    "end": "2298319"
  },
  {
    "text": "logic instead of pinging what it's going to do is it's going to look at what's",
    "start": "2298319",
    "end": "2303599"
  },
  {
    "text": "the concurrency configuration for each of your function apps so basically how many events should it",
    "start": "2303599",
    "end": "2310000"
  },
  {
    "text": "be able to handle per instance and it's going to make a calculation of well here's how many",
    "start": "2310000",
    "end": "2316280"
  },
  {
    "text": "workers you should have based on the traffic I'm seeing and on what you've told me is the right siiz capacity of",
    "start": "2316280",
    "end": "2322760"
  },
  {
    "text": "your workers and there's no pings pings are are toast so it makes a much better",
    "start": "2322760",
    "end": "2327839"
  },
  {
    "text": "assessment through the control plane and then based on that um it can work with",
    "start": "2327839",
    "end": "2335240"
  },
  {
    "text": "the data role in the backend Fleet of placeholders to create more instances",
    "start": "2335240",
    "end": "2341760"
  },
  {
    "text": "and instead of doing things in Cal it actually can do things in batching as well so we could do batch Pub sub style",
    "start": "2341760",
    "end": "2348640"
  },
  {
    "text": "processing with the requests and we can build things a lot faster and as a",
    "start": "2348640",
    "end": "2353760"
  },
  {
    "text": "result we can create that burst effect of um knowing reliably what you need",
    "start": "2353760",
    "end": "2360040"
  },
  {
    "text": "right you stepped on the gas I have a lot of requests coming in based on oh here's how many requests per instance",
    "start": "2360040",
    "end": "2366560"
  },
  {
    "text": "you want you need way more instances let me create a batch for you boom and then it's constantly monitoring to decide",
    "start": "2366560",
    "end": "2373119"
  },
  {
    "text": "should I give you more should I give you less um it's a much more efficient system uh both for you and for",
    "start": "2373119",
    "end": "2382240"
  },
  {
    "text": "us so let's talk about concurrency I've been making a big deal about this um and I'll be honest when I first heard",
    "start": "2383400",
    "end": "2389480"
  },
  {
    "text": "concurrency I needed my team to explain it to me I was like can't we use the top 1,000 most common words in the language",
    "start": "2389480",
    "end": "2396400"
  },
  {
    "text": "to describe what it is they're like nope it's called concurrency okay so concurrency gives us control over the",
    "start": "2396400",
    "end": "2403200"
  },
  {
    "text": "number of messages per instance so um in parallel right so",
    "start": "2403200",
    "end": "2410400"
  },
  {
    "text": "let's look at a couple examples let's say that I've set the concurrency to one",
    "start": "2410400",
    "end": "2416079"
  },
  {
    "text": "what that's saying is I want each instance of a function to handle one in",
    "start": "2416079",
    "end": "2421720"
  },
  {
    "text": "only one message or request so let's take an example if I",
    "start": "2421720",
    "end": "2428000"
  },
  {
    "text": "have 10 executions coming in with a concurrency of one it's going to create 10 instances because it's a one: one",
    "start": "2428000",
    "end": "2434760"
  },
  {
    "text": "ratio if I cut the number of requests in half so I send five instances well the",
    "start": "2434760",
    "end": "2441359"
  },
  {
    "text": "Math's really easy or five requests I'm going to get five instances for those five requests so um this is incredibly",
    "start": "2441359",
    "end": "2448920"
  },
  {
    "text": "powerful if you have a high computation type workload like it really needs",
    "start": "2448920",
    "end": "2455240"
  },
  {
    "text": "dedicated resources to to compute and work on that data coming in it's really",
    "start": "2455240",
    "end": "2461160"
  },
  {
    "text": "good for that um and actually if you look at aws's architecture their entire architecture is based on concurrency of",
    "start": "2461160",
    "end": "2467480"
  },
  {
    "text": "one they're always giving you a new instance a new microv VM for every",
    "start": "2467480",
    "end": "2473280"
  },
  {
    "text": "Lambda call um and that's okay but we we're kind of proud of this one because we give you a flexible and controllable",
    "start": "2473280",
    "end": "2480960"
  },
  {
    "text": "concurrency and the whole system knows about it so let's say now I set a concurrency of two that means I every",
    "start": "2480960",
    "end": "2488960"
  },
  {
    "text": "instance will handle two events in parallel before it goes on to make a new one so let's let's rerun this whole",
    "start": "2488960",
    "end": "2494480"
  },
  {
    "text": "scenario so I'm going to send 10 requests and you can see it's doubling",
    "start": "2494480",
    "end": "2500440"
  },
  {
    "text": "up oh I'm going to put two messages in parallel in each instance and I won't",
    "start": "2500440",
    "end": "2505839"
  },
  {
    "text": "ask for more instances until I see more if I send five it's going to say oh okay",
    "start": "2505839",
    "end": "2511800"
  },
  {
    "text": "one and two got one executions three and four got another instance execution five",
    "start": "2511800",
    "end": "2517800"
  },
  {
    "text": "E even though it's underutilized it's going to get a whole instance so you can kind of see how the concurrency lets you have control over",
    "start": "2517800",
    "end": "2525440"
  },
  {
    "text": "what the right size is and we've debated and and I'll watch your feedback after",
    "start": "2525440",
    "end": "2530599"
  },
  {
    "text": "should we do automatic concurrency for you we've totally debated it we're worried that that's too magical and that",
    "start": "2530599",
    "end": "2537160"
  },
  {
    "text": "you know more about your workload than we do so we haven't done that yet the our our first step in this direction is",
    "start": "2537160",
    "end": "2543680"
  },
  {
    "text": "we give you controllable concurrency and what that also means you're have to do a bit of testing and probably load testing",
    "start": "2543680",
    "end": "2550440"
  },
  {
    "text": "just like I did before to see what's the right amount of messages and requests per instance hey when do I hit that",
    "start": "2550440",
    "end": "2556200"
  },
  {
    "text": "happy spot of CPU and memory of 70% that's kind of what you're looking for",
    "start": "2556200",
    "end": "2561720"
  },
  {
    "text": "um and I would encourage you rather than trying to do a thousand instances at once see if you can crank up that",
    "start": "2561720",
    "end": "2567880"
  },
  {
    "text": "concurrency and do more work in parallel if you do you're going to save money you're going to be more efficient um and",
    "start": "2567880",
    "end": "2574079"
  },
  {
    "text": "it'll also speed up the workload for your customers so a little bit more information about",
    "start": "2574079",
    "end": "2580800"
  },
  {
    "text": "our host um our host not only knows about your runtime and the version and",
    "start": "2580800",
    "end": "2586520"
  },
  {
    "text": "all that kind of stuff it knows about the kind of Hardware you want and this is a a change another change in Flex",
    "start": "2586520",
    "end": "2592720"
  },
  {
    "text": "consumption in consumption V1 the one that that's out before it was kind of a",
    "start": "2592720",
    "end": "2598760"
  },
  {
    "text": "black box you didn't really know uh what kind of memory and CPU you were using",
    "start": "2598760",
    "end": "2604839"
  },
  {
    "text": "and it it was just kind of magical but um in this case now you can actually say",
    "start": "2604839",
    "end": "2611319"
  },
  {
    "text": "what instance size you want just like you can for VMS so we'll default you to a 2 gigabyte um instance and it has the",
    "start": "2611319",
    "end": "2618319"
  },
  {
    "text": "right allocation of like one CPU to go with that you can go bigger to four",
    "start": "2618319",
    "end": "2623680"
  },
  {
    "text": "gigabytes you can go smaller even to half a gigabyte and by the way we're going to add more and more instances we",
    "start": "2623680",
    "end": "2630000"
  },
  {
    "text": "just have uh the three in beta but we're going to add a lot bigger ones we're",
    "start": "2630000",
    "end": "2635440"
  },
  {
    "text": "actually going to see if we need to add the smaller ones for us it's harder to do the smaller ones the economics for us",
    "start": "2635440",
    "end": "2641559"
  },
  {
    "text": "to subdivide the virtual machines into tiny sub submachines it's a little",
    "start": "2641559",
    "end": "2646960"
  },
  {
    "text": "tricky but we'll certainly um based on your feedback we'll do what you want",
    "start": "2646960",
    "end": "2652079"
  },
  {
    "text": "terms of the default concurrency um we've already run load tests and we've looked at real workloads that are",
    "start": "2652079",
    "end": "2658640"
  },
  {
    "text": "already running in functions and this is just our observation this is how we fine-tuned it for that run-ofthe-mill",
    "start": "2658640",
    "end": "2665119"
  },
  {
    "text": "function that's a 2 GB instant size we've set that to be a concurrency of 16",
    "start": "2665119",
    "end": "2671839"
  },
  {
    "text": "we've just seen most workloads run pretty well that way and you can kind of see it's kind of like powers of two so",
    "start": "2671839",
    "end": "2679119"
  },
  {
    "text": "as you double the memory um we can double the concurrency um but as as or",
    "start": "2679119",
    "end": "2685800"
  },
  {
    "text": "it's like powers of two basically and if you go down to 512 we we saw a concurrency of four work pretty well but",
    "start": "2685800",
    "end": "2691800"
  },
  {
    "text": "this is completely setable I think this is where workloads actually will vary is it C CPU intensive is it IO intensive",
    "start": "2691800",
    "end": "2699200"
  },
  {
    "text": "you have to kind of decide and you can set that concurrency but at least it's like one knob it's not a whole bunch of",
    "start": "2699200",
    "end": "2704800"
  },
  {
    "text": "knobs anymore um python has a concurrency of one by default we did",
    "start": "2704800",
    "end": "2711400"
  },
  {
    "text": "this because python has I won't use the word unfortunate it has its own point of view about threading and how many",
    "start": "2711400",
    "end": "2717559"
  },
  {
    "text": "threads can run per CPU so in a serverless environment that's pretty tricky and just to keep you out of",
    "start": "2717559",
    "end": "2723599"
  },
  {
    "text": "trouble we've set you to a concurrency of one and that way you won't run into any of those tricky issues waiting for",
    "start": "2723599",
    "end": "2729640"
  },
  {
    "text": "the CPU uh so Python's a little bit different um but you know if you take some small instances with a huge scale",
    "start": "2729640",
    "end": "2735960"
  },
  {
    "text": "out you can still run python workloads quite well and we have some super huge AI customers that are already doing that",
    "start": "2735960",
    "end": "2742200"
  },
  {
    "text": "right now so far so good makes sense all right",
    "start": "2742200",
    "end": "2747359"
  },
  {
    "text": "um so let's look at the concurrency control pop out of",
    "start": "2747359",
    "end": "2754078"
  },
  {
    "text": "PowerPoint um I have a kind of a demo for this one so here in vs",
    "start": "2754720",
    "end": "2762680"
  },
  {
    "text": "code um I have a demo that's basically data ingestion but the data ingestion is",
    "start": "2762680",
    "end": "2769040"
  },
  {
    "text": "going to um uh it's going to take prompts from the user and the prompts",
    "start": "2769040",
    "end": "2776359"
  },
  {
    "text": "will have actual files um like files that you want to analyze with AI and it",
    "start": "2776359",
    "end": "2783160"
  },
  {
    "text": "will it will ingest those big chunks of data and then it'll it'll do the processing on it and it'll do rag um to",
    "start": "2783160",
    "end": "2789800"
  },
  {
    "text": "give you kind of a a user example so we have a front end",
    "start": "2789800",
    "end": "2795880"
  },
  {
    "text": "and the front end will take the files from the user and if you've seen our AI demos this going to look similar to",
    "start": "2795880",
    "end": "2801680"
  },
  {
    "text": "probably every AI demo you've seen so in the rag scenario I'm going to take my own files I have all this Health Data",
    "start": "2801680",
    "end": "2808839"
  },
  {
    "text": "let's take 10 and I'm going to upload a batch of",
    "start": "2808839",
    "end": "2814040"
  },
  {
    "text": "them oh um did I take down my function am I looking the wrong one one",
    "start": "2814040",
    "end": "2819880"
  },
  {
    "text": "sec I did a deployment a second",
    "start": "2819880",
    "end": "2825559"
  },
  {
    "text": "ago uh I'm definitely keeping it real because stuff fails in",
    "start": "2825559",
    "end": "2833200"
  },
  {
    "text": "reality one second",
    "start": "2833200",
    "end": "2837520"
  },
  {
    "text": "so I I just created this one and I should have a static web",
    "start": "2853280",
    "end": "2858599"
  },
  {
    "text": "app that's the front",
    "start": "2858599",
    "end": "2862280"
  },
  {
    "text": "end is that the one I was Woody Meadow Woody Meadow all right let's take a quick look",
    "start": "2864119",
    "end": "2871240"
  },
  {
    "text": "we won't spend too much time debugging this",
    "start": "2871240",
    "end": "2875839"
  },
  {
    "text": "my functions are healthy that does not make sense let's just try it again it's the cloud",
    "start": "2884880",
    "end": "2890720"
  },
  {
    "text": "sometimes you just have to try things a couple times",
    "start": "2890720",
    "end": "2896240"
  },
  {
    "text": "um so take a file we'll ingest it no let's see if we can ask a question",
    "start": "2901040",
    "end": "2909359"
  },
  {
    "text": "does my plan cover ey",
    "start": "2909359",
    "end": "2914558"
  },
  {
    "text": "exams okay complete Paul is like crashing and burning that's okay uh",
    "start": "2915200",
    "end": "2920720"
  },
  {
    "text": "hopefully I've got a different kind of a demo forget you ever saw this I apologize um I don't know why it's not",
    "start": "2920720",
    "end": "2927040"
  },
  {
    "text": "working for you right now but I have it I have yet another example of concurrency that I could show",
    "start": "2927040",
    "end": "2933319"
  },
  {
    "text": "you because I was anticipating something could go wrong all right refres did I turn on refesh",
    "start": "2933319",
    "end": "2940839"
  },
  {
    "text": "hard refresh of the website itself that's a good idea let's",
    "start": "2940839",
    "end": "2947040"
  },
  {
    "text": "um let's do a full refresh it's making me off",
    "start": "2947040",
    "end": "2952240"
  },
  {
    "text": "again um",
    "start": "2952240",
    "end": "2956240"
  },
  {
    "text": "sir I salute you thank you I've used Azure before youve used Azure so you",
    "start": "2968880",
    "end": "2974799"
  },
  {
    "text": "know you have to try and refresh again thank",
    "start": "2974799",
    "end": "2979319"
  },
  {
    "text": "you I'm in a very forgiving mood so we'll take that one um okay so let's",
    "start": "2980200",
    "end": "2985400"
  },
  {
    "text": "actually try ingesting our data one more",
    "start": "2985400",
    "end": "2989240"
  },
  {
    "text": "time okay so things seem to be working end to end now let's um let's do that",
    "start": "2993799",
    "end": "3001760"
  },
  {
    "text": "trick so we're in AI Oslo and let's",
    "start": "3001760",
    "end": "3008720"
  },
  {
    "text": "watch our function through the lens of monitoring so we're going to go into",
    "start": "3008760",
    "end": "3014559"
  },
  {
    "text": "live",
    "start": "3014559",
    "end": "3016839"
  },
  {
    "text": "metrics on this tiny screen it's going to be interesting let's see if I can pull this off so we'll do this and then",
    "start": "3020359",
    "end": "3026280"
  },
  {
    "text": "I can do do like tile to the right and I can do like tile to the",
    "start": "3026280",
    "end": "3032000"
  },
  {
    "text": "left all right so let's let's see if we can force some spikes with the",
    "start": "3032000",
    "end": "3039599"
  },
  {
    "text": "concurrency so let's upload one file and watch",
    "start": "3042640",
    "end": "3047960"
  },
  {
    "text": "it I think in this resolution it's going to look not so awesome",
    "start": "3052839",
    "end": "3058640"
  },
  {
    "text": "awesome there we go so you saw one request one in and this is a i it's a",
    "start": "3060079",
    "end": "3067920"
  },
  {
    "text": "little bit of a bug but right now if app insights is monitoring your functions",
    "start": "3067920",
    "end": "3073359"
  },
  {
    "text": "there's like an inspection effect and it's causing instances to scale out so we it's a bug we got to work on that but",
    "start": "3073359",
    "end": "3080079"
  },
  {
    "text": "that's actually what's going on just notice that it's kind of a small number of servers okay so it cooled down we're",
    "start": "3080079",
    "end": "3085200"
  },
  {
    "text": "back at three it should be zero but I'm inspecting it with monitoring it's not zero right now sorry about that all",
    "start": "3085200",
    "end": "3091359"
  },
  {
    "text": "right so let's do it again and let's add 10",
    "start": "3091359",
    "end": "3098798"
  },
  {
    "text": "files all right watch here watch here here we",
    "start": "3099359",
    "end": "3105000"
  },
  {
    "text": "go okay that was a little counterintuitive but I",
    "start": "3114200",
    "end": "3120559"
  },
  {
    "text": "was expecting to see kind of a one for one ratio because what I've done is I've set the I thought I set the concurrency",
    "start": "3120559",
    "end": "3125680"
  },
  {
    "text": "to one let's actually double check that remember one a concurrency of one",
    "start": "3125680",
    "end": "3131640"
  },
  {
    "text": "means every one event gets its own server it gets its own instance so if I look at my function you probably know",
    "start": "3131640",
    "end": "3138599"
  },
  {
    "text": "this if you go to Json view you can see the actual like arm control plane view of the",
    "start": "3138599",
    "end": "3144880"
  },
  {
    "text": "resource and I can see how the heck did that happen a concurrency of",
    "start": "3144880",
    "end": "3151799"
  },
  {
    "text": "16 am I the wrong Resource Group that's a good question I am in the wrong Resource Group I think I'm trying to do",
    "start": "3152880",
    "end": "3159520"
  },
  {
    "text": "too much at once let's let's see let's go back",
    "start": "3159520",
    "end": "3164119"
  },
  {
    "text": "here I appreciate you bailing me out",
    "start": "3164760",
    "end": "3169160"
  },
  {
    "text": "though so we look at the function",
    "start": "3173720",
    "end": "3178440"
  },
  {
    "text": "all right now HTP scale is going to be set",
    "start": "3178920",
    "end": "3184640"
  },
  {
    "text": "here so this has a HTTP per instance concurrency of five so we have it lower",
    "start": "3187079",
    "end": "3193640"
  },
  {
    "text": "it's not all the way down to one to one but that was lower that's why we saw some instances but not a huge amount and",
    "start": "3193640",
    "end": "3199599"
  },
  {
    "text": "then this is an important thing for you need to know about how functions work HTP scaling is done different than",
    "start": "3199599",
    "end": "3205960"
  },
  {
    "text": "message scaling and it kind of makes sense if you think about it cuz HTTP",
    "start": "3205960",
    "end": "3211319"
  },
  {
    "text": "gets monitored by our front end in our load balancer that's how we know how much traffic is coming in there's really",
    "start": "3211319",
    "end": "3217319"
  },
  {
    "text": "no other way to know messages are different because we need a way to interact with the actual back end like",
    "start": "3217319",
    "end": "3223119"
  },
  {
    "text": "the message broker the Q uh Etc that you're working with and what actually",
    "start": "3223119",
    "end": "3228720"
  },
  {
    "text": "does that in functions is the Azure sdks like if I'm working with a service bus it's the Azure SDK for service bus that",
    "start": "3228720",
    "end": "3235880"
  },
  {
    "text": "functions in internally is using to keep a connection alive to that message CU if",
    "start": "3235880",
    "end": "3241760"
  },
  {
    "text": "it's a vent hubs which is the case here I'm using the Azure SDK for event hubs and you probably also know this but",
    "start": "3241760",
    "end": "3248359"
  },
  {
    "text": "Azure triggers and bindings and functions are actually just wrappers over the same sdks that you all have",
    "start": "3248359",
    "end": "3255000"
  },
  {
    "text": "they simplify and they give you context and if you use our triggers and bindings it knows things like concurrency it",
    "start": "3255000",
    "end": "3261920"
  },
  {
    "text": "knows things like the state of the function which give you some extra advantages for scaling but what what",
    "start": "3261920",
    "end": "3267640"
  },
  {
    "text": "that kind of means is we need to configure in a different way if we're going to work with messages and so the",
    "start": "3267640",
    "end": "3273839"
  },
  {
    "text": "way we do that this this came out with uh Target based scaling if you follow that a year ago but we have sections in",
    "start": "3273839",
    "end": "3280480"
  },
  {
    "text": "the host. Json so this is a configuration that goes with your app at deployment time right now that",
    "start": "3280480",
    "end": "3286960"
  },
  {
    "text": "configuration lives here and um event hubs you might know this too event Hub",
    "start": "3286960",
    "end": "3293000"
  },
  {
    "text": "is actually a specialized kind of service bus so the configuration for event hubs and service bus is the same",
    "start": "3293000",
    "end": "3298880"
  },
  {
    "text": "and it'll be in this block called service bus and that's where I can set the max concurrent calls to one so just",
    "start": "3298880",
    "end": "3305799"
  },
  {
    "text": "things to be aware of um you've got to like think about concurrency for HTTP and think about it separately from your",
    "start": "3305799",
    "end": "3312599"
  },
  {
    "text": "your messages um and so that's how I set it here now maybe we can do one more cool",
    "start": "3312599",
    "end": "3320240"
  },
  {
    "text": "example um I'm going to let this I'm going to let this one kind of Brew",
    "start": "3320240",
    "end": "3327760"
  },
  {
    "text": "because it takes two minutes and I'm going to run out of time all right so I'm going to switch",
    "start": "3327760",
    "end": "3334920"
  },
  {
    "text": "gears I made two more end to end demos so remember the first demo I did where we had the event hubs and it was like",
    "start": "3334920",
    "end": "3340839"
  },
  {
    "text": "2,000 requests per second so that has by default a concurrency of 16 if you take",
    "start": "3340839",
    "end": "3346880"
  },
  {
    "text": "the defaults so I have two experiments here I have one with a concurrency of eight so less messages per instance",
    "start": "3346880",
    "end": "3354799"
  },
  {
    "text": "means more instances so let's run the load test for this guy",
    "start": "3354799",
    "end": "3362440"
  },
  {
    "text": "um so I go to my load",
    "start": "3365599",
    "end": "3369599"
  },
  {
    "text": "test and I'm going to rerun it you can see I actually ran it right before this demo right before this",
    "start": "3372359",
    "end": "3379440"
  },
  {
    "text": "session all right",
    "start": "3379440",
    "end": "3383680"
  },
  {
    "text": "and I'm going to rinse and repeat and I'm going to do that for the concurrency of",
    "start": "3390039",
    "end": "3399039"
  },
  {
    "text": "um 32 here it",
    "start": "3399079",
    "end": "3405280"
  },
  {
    "text": "is we're going to go into the [Music] function we're going to go into the load",
    "start": "3405280",
    "end": "3412119"
  },
  {
    "text": "test",
    "start": "3412119",
    "end": "3415119"
  },
  {
    "text": "all right and these have a little bit of uh time that they need so let's do",
    "start": "3429319",
    "end": "3438760"
  },
  {
    "text": "let's go here let's go into the live metrics this",
    "start": "3449359",
    "end": "3455640"
  },
  {
    "text": "is the live metrics for",
    "start": "3455640",
    "end": "3458720"
  },
  {
    "text": "eight and we don't see it running yet still",
    "start": "3463799",
    "end": "3469160"
  },
  {
    "text": "provisioning and I'm going to do another side by side can you imagine amount of",
    "start": "3470760",
    "end": "3476760"
  },
  {
    "text": "infrastructure you used to have to have to do this kind of a thing all right so let's do was this the eight yeah that",
    "start": "3476760",
    "end": "3484200"
  },
  {
    "text": "was the eight so let's do the",
    "start": "3484200",
    "end": "3487200"
  },
  {
    "text": "32 I'm racing against the",
    "start": "3496880",
    "end": "3500838"
  },
  {
    "text": "clock but what I'm what I'm wanting to to see from this experiment is can I",
    "start": "3503240",
    "end": "3509520"
  },
  {
    "text": "keep the request rate as high as I want and if I force the concurrency down I'm",
    "start": "3509520",
    "end": "3515640"
  },
  {
    "text": "going to expect to see more instances and I'm going to expect to see less um strain and stress on my Hardware",
    "start": "3515640",
    "end": "3523720"
  },
  {
    "text": "whereas in this case the 32 case I'm going to try packing more concurrency or",
    "start": "3523720",
    "end": "3528960"
  },
  {
    "text": "more events in and I want to see if I can still right sizee it without blowing up the",
    "start": "3528960",
    "end": "3534240"
  },
  {
    "text": "machines and this is going to let me do that kind of real time cost analysis",
    "start": "3534240",
    "end": "3540319"
  },
  {
    "text": "um I'll tell you what let's flash away for a second cuz I literally am going to run out of time",
    "start": "3540319",
    "end": "3547119"
  },
  {
    "text": "uh so we'll Flash Forward um I think I wanted to tell you a little",
    "start": "3547119",
    "end": "3554000"
  },
  {
    "text": "bit about um placeholder pools so the way that we pull that",
    "start": "3554000",
    "end": "3560798"
  },
  {
    "text": "off oh shoot let's just keep it not zoomed in",
    "start": "3561319",
    "end": "3568640"
  },
  {
    "text": "mode so to again to keep this really fast we keep pools or fleets of servers",
    "start": "3570039",
    "end": "3575839"
  },
  {
    "text": "and for every application you have for every runtime you use we actually manage",
    "start": "3575839",
    "end": "3581920"
  },
  {
    "text": "different fleets and that way we can have a a ready warm pool available for",
    "start": "3581920",
    "end": "3587000"
  },
  {
    "text": "that kind of permutation of compute that you need and then we have a process called specialization I'll leave you",
    "start": "3587000",
    "end": "3592240"
  },
  {
    "text": "with the slides but specialization is the process of slamming your your application zip file on top of an",
    "start": "3592240",
    "end": "3598480"
  },
  {
    "text": "already warm VM and container and that's how we effectively give you the faster cold start times more in the hundreds of",
    "start": "3598480",
    "end": "3605000"
  },
  {
    "text": "milliseconds versus in minutes um and it there's some a lot of trickery we have",
    "start": "3605000",
    "end": "3610559"
  },
  {
    "text": "to do like I'm kind of trying to represent by the colors we need to keep track of all your apps and versions and",
    "start": "3610559",
    "end": "3616319"
  },
  {
    "text": "if your app cold start time gets slow one of the reasons could be that it's",
    "start": "3616319",
    "end": "3621760"
  },
  {
    "text": "you're deploying an app that didn't expect let's say you configured a net 8 app and you deploy to python app it's",
    "start": "3621760",
    "end": "3627599"
  },
  {
    "text": "like oh I can't find the placeholder pool for that one and it's going to be slow um so that's just that's a one of",
    "start": "3627599",
    "end": "3634880"
  },
  {
    "text": "the tips and tricks I'm going to leave with you okay let's come back here so here's our AB test um again to remember",
    "start": "3634880",
    "end": "3641039"
  },
  {
    "text": "this is concurrency of eight we're doing 40,000 requests per second not too shabby look at the servers we're up in",
    "start": "3641039",
    "end": "3646880"
  },
  {
    "text": "the hundreds and let's look at the CPU in memory uh I'm not so happy about that",
    "start": "3646880",
    "end": "3652760"
  },
  {
    "text": "look at that I'm like 10 to 20% utilization that's not good right that's we can do better than that so now let's",
    "start": "3652760",
    "end": "3659480"
  },
  {
    "text": "look at our AB test over here I've got a concurrency of 32",
    "start": "3659480",
    "end": "3665720"
  },
  {
    "text": "um I have to think about why the request rate is different but it should be the same depending on the point in time it",
    "start": "3665720",
    "end": "3672599"
  },
  {
    "text": "should ramp to the same number of users but look I'm using far less instances",
    "start": "3672599",
    "end": "3678079"
  },
  {
    "text": "and I'm getting closer to that 50% utilization so what this actually tells me for this particular workload which is",
    "start": "3678079",
    "end": "3684599"
  },
  {
    "text": "just light ETL light Telemetry ingestion I'm going to want a higher concurrency which will make my bursting faster",
    "start": "3684599",
    "end": "3691920"
  },
  {
    "text": "because I have less to burst to and it's going to make my cost slower because it's just less compute time in parallel",
    "start": "3691920",
    "end": "3698039"
  },
  {
    "text": "that I'm running so I think this is hopefully a good visualization of of what you want to do um and with that",
    "start": "3698039",
    "end": "3703720"
  },
  {
    "text": "said I'm going to in the last I guess I'm over by one minute apologize so I'm going to leave you with some tips on how",
    "start": "3703720",
    "end": "3710319"
  },
  {
    "text": "to scale um like if you want to achieve maximum maximum scale you do want that",
    "start": "3710319",
    "end": "3716119"
  },
  {
    "text": "one:1 ratio of function apps and functions within in a function plan that",
    "start": "3716119",
    "end": "3723079"
  },
  {
    "text": "means that every single function can have horizontal scale think about it this way if you have multiple functions",
    "start": "3723079",
    "end": "3729160"
  },
  {
    "text": "in a function app it's sharing it can have noisy neighbors um so that's",
    "start": "3729160",
    "end": "3734240"
  },
  {
    "text": "another element besides the concurrency um you're better off to choose small instance sizes and scale out versus big",
    "start": "3734240",
    "end": "3741520"
  },
  {
    "text": "instance sizes and scale up that's going to save you money chances are you're over izing your VMS that's not always",
    "start": "3741520",
    "end": "3748240"
  },
  {
    "text": "true but that's a lot of the times um watch out for your logging I didn't get to go into it today your logging can",
    "start": "3748240",
    "end": "3754039"
  },
  {
    "text": "actually be a reason why your scaling will fail so make sure that your steady state logging is the minimal logging you",
    "start": "3754039",
    "end": "3759480"
  },
  {
    "text": "need and rely on Trace level of information verbose or debug for the the",
    "start": "3759480",
    "end": "3766160"
  },
  {
    "text": "cases that you really need to troubleshoot something and the rest of the time keep it at a normal tracing level uh and have a cheeky joke like you",
    "start": "3766160",
    "end": "3773920"
  },
  {
    "text": "could order some coffee cuz you you won't have time to to pay attention to that um for cold start some other tricks",
    "start": "3773920",
    "end": "3780880"
  },
  {
    "text": "we actually I'll leave you with a whole article on it but um Flex consumption does this best practice but you can do",
    "start": "3780880",
    "end": "3786559"
  },
  {
    "text": "it for all functions make sure you deploy with the run from package equals one or run from package equals URL the",
    "start": "3786559",
    "end": "3794119"
  },
  {
    "text": "biggest determiner of cold start time since we make those placeholder pools is actually how fast we could load your",
    "start": "3794119",
    "end": "3800000"
  },
  {
    "text": "application zip so by using this mechanism You're simply loading a blob",
    "start": "3800000",
    "end": "3806279"
  },
  {
    "text": "and as long as your blob is small your app is going to load faster so I think this one can really really help you with",
    "start": "3806279",
    "end": "3812599"
  },
  {
    "text": "with cold start and that said um everything we went through here the source code the notes the CLI commands",
    "start": "3812599",
    "end": "3819440"
  },
  {
    "text": "and more is on this gist um and I'll make it better and better based on your",
    "start": "3819440",
    "end": "3824480"
  },
  {
    "text": "feedback with that said thank you so much for spending an hour talking about scale see you",
    "start": "3824480",
    "end": "3833558"
  }
]