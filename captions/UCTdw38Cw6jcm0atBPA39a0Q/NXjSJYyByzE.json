[
  {
    "text": "okay hi hello everybody uh my name is David I'm a software engineer at meta",
    "start": "840",
    "end": "7379"
  },
  {
    "text": "and before that I worked at a bunch of different companies mostly doing big data things because I'm a big data geek",
    "start": "7379",
    "end": "13320"
  },
  {
    "text": "uh and today we're going to be talking a lot about big data but this is not a big",
    "start": "13320",
    "end": "18420"
  },
  {
    "text": "data conference this is a software development conference and so we're going to be talking about data tools",
    "start": "18420",
    "end": "23699"
  },
  {
    "text": "from the perspective of software developers like I'm guessing most of you here are so let's do a quick check who",
    "start": "23699",
    "end": "29580"
  },
  {
    "text": "here actually writes software for a living yeah that's pretty much what I expected perfect",
    "start": "29580",
    "end": "35399"
  },
  {
    "text": "um so yeah so we're in the right place um so what we're going to be talking about is what are the problems different problems",
    "start": "35399",
    "end": "41340"
  },
  {
    "text": "that we solve with data tools how they evolved from 20 30 years ago to what we're actually using today and how if we",
    "start": "41340",
    "end": "48600"
  },
  {
    "text": "had to solve this problem right now in this room as software developers how we would actually build these tools",
    "start": "48600",
    "end": "53940"
  },
  {
    "text": "ourselves and see if it's actually close to what I don't really exists in the industry and generally just see what you",
    "start": "53940",
    "end": "60960"
  },
  {
    "text": "know what the different tools are and where you're likely to meet them in your day-to-day work if at all right and so",
    "start": "60960",
    "end": "67619"
  },
  {
    "text": "before we start we can talk about what is Big Data and there's actually exactly two jokes you're allowed to tell about",
    "start": "67619",
    "end": "72960"
  },
  {
    "text": "Big Data the first of course is a picture of Lieutenant data from Star Trek next to a slightly bigger picture of alternating data from Star Trek",
    "start": "72960",
    "end": "79500"
  },
  {
    "text": "hilarious um and the other joke is a Facebook post by psychologist Dana really from 20th",
    "start": "79500",
    "end": "88860"
  },
  {
    "text": "2013 and he wrote that big data is like teenage sex in that everybody thinks",
    "start": "88860",
    "end": "95040"
  },
  {
    "text": "everybody else is doing it everybody wants to be doing it and no one's actually doing it right and that was 10 years ago and that was",
    "start": "95040",
    "end": "102180"
  },
  {
    "text": "the peak hype for big data and at the time that was very funny the thing is now is 10 years later and most companies",
    "start": "102180",
    "end": "108479"
  },
  {
    "text": "actually have a lot of data and everybody's doing big data so I stopped telling that joke",
    "start": "108479",
    "end": "113520"
  },
  {
    "text": "so we're stuck with the only one official data joke now but really the big data is a field of",
    "start": "113520",
    "end": "119700"
  },
  {
    "text": "actually dealing with large enough or complex enough amounts of data that it's actually hard to process them with",
    "start": "119700",
    "end": "126840"
  },
  {
    "text": "traditional tools meaning databases relational databases like most of you I'm sure are familiar with let's do a",
    "start": "126840",
    "end": "132540"
  },
  {
    "text": "quick check who has ever written a SQL query perfect you're wearing the life plates",
    "start": "132540",
    "end": "137760"
  },
  {
    "text": "again good I'm just verifying that we're going in the right direction okay so like normal big data and normal",
    "start": "137760",
    "end": "143640"
  },
  {
    "text": "data tools are databases the stuff you use every day in your applications microservices whatever uh and at some",
    "start": "143640",
    "end": "148980"
  },
  {
    "text": "point we get to a place where either there's just so much data we have to query and process that normal databases",
    "start": "148980",
    "end": "154260"
  },
  {
    "text": "just don't cut it anymore or the speed at which data comes in and we have to ingest it and do something with it is so",
    "start": "154260",
    "end": "160379"
  },
  {
    "text": "high that again we can't just do that on a single instance even the most expensive and biggest database we can",
    "start": "160379",
    "end": "165900"
  },
  {
    "text": "buy and at that point we need to actually use specialized tools but these aren't some magical tools that only big",
    "start": "165900",
    "end": "172560"
  },
  {
    "text": "data people understand right all of those were written by software developers like all of us here to solve",
    "start": "172560",
    "end": "178920"
  },
  {
    "text": "a specific problem which wasn't solvable until that right and this is a cycle that happens in software development",
    "start": "178920",
    "end": "184019"
  },
  {
    "text": "very often right a new problem shows up that you have to solve in an engineering way and it's usually a combination of",
    "start": "184019",
    "end": "190200"
  },
  {
    "text": "economics and some different Trends in the industry which change the way you use data so for example the early 2000s",
    "start": "190200",
    "end": "197280"
  },
  {
    "text": "were a really good example of this uh we had the internet and search engines then a few years later we had social",
    "start": "197280",
    "end": "203220"
  },
  {
    "text": "media and a lot of people started generating just tons of data all the time and the economics of it combined with",
    "start": "203220",
    "end": "209819"
  },
  {
    "text": "the fact that storage got way cheaper generated a whole new generation of completely new tools which solves",
    "start": "209819",
    "end": "216000"
  },
  {
    "text": "problems that you couldn't actually solve before that right and so when we talk about problems and economics",
    "start": "216000",
    "end": "221879"
  },
  {
    "text": "we need to think about what is the value of data right and this is a slide which I really like it's from an article",
    "start": "221879",
    "end": "228239"
  },
  {
    "text": "written by by Scott Yar the CTO of all DB a long time ago and he talks about",
    "start": "228239",
    "end": "235140"
  },
  {
    "text": "the the fact that the value of data isn't constant right the value of data changes over time",
    "start": "235140",
    "end": "240599"
  },
  {
    "text": "and it's important to see what kind of data we're talking about when we look at an individual item of data right so one",
    "start": "240599",
    "end": "245940"
  },
  {
    "text": "record or one row in a database one event usually the value of this event",
    "start": "245940",
    "end": "251700"
  },
  {
    "text": "starts very high close to the moment when it was generated the best example would be for example let's see let's",
    "start": "251700",
    "end": "257400"
  },
  {
    "text": "we're working on e-commerce websites and our item of data is a user's shopping cart",
    "start": "257400",
    "end": "263340"
  },
  {
    "text": "so while we're browsing the website and the user does stuff and updates the shopping cart that object is very",
    "start": "263340",
    "end": "268919"
  },
  {
    "text": "valuable if you lose it we lose the customer it's going to be very upsetting but once the user checked out and used",
    "start": "268919",
    "end": "274259"
  },
  {
    "text": "the shopping cart and we log that in a database the value of that data item goes down very quickly five months later",
    "start": "274259",
    "end": "280860"
  },
  {
    "text": "no one actually cares about this one record in a database however the value of data in aggregate right so lots of",
    "start": "280860",
    "end": "288180"
  },
  {
    "text": "historical data for example all of the shopping carts for the past six months the value of data in aggregate starts",
    "start": "288180",
    "end": "293699"
  },
  {
    "text": "out usually very low because looking at statistics of shopping carts for the past five minutes isn't very informative",
    "start": "293699",
    "end": "299220"
  },
  {
    "text": "right but if you look at all the shopping carts for the past six months suddenly that data in aggregate becomes",
    "start": "299220",
    "end": "305639"
  },
  {
    "text": "very valuable because you can generate Trends you can get statistics analytics and business intelligence and you can",
    "start": "305639",
    "end": "311040"
  },
  {
    "text": "train machine learning models and all kinds of stuff with this data because we now have enough of it and it goes back long enough right and so there are",
    "start": "311040",
    "end": "319080"
  },
  {
    "text": "different tools to address each one of the sides of the slides there are specific tools usually the ones you normally use in your daily work which",
    "start": "319080",
    "end": "325979"
  },
  {
    "text": "are on the left side of the slide which is tools that are very good at dealing with small amounts of individual records",
    "start": "325979",
    "end": "332220"
  },
  {
    "text": "you know those as normal everyday databases right your SQL Server your",
    "start": "332220",
    "end": "337380"
  },
  {
    "text": "postgres SQL whatever they are and they're optimized to be very good at dealing with a small number of Records",
    "start": "337380",
    "end": "343380"
  },
  {
    "text": "very quickly and we don't really try to store very many of those and if we try to do",
    "start": "343380",
    "end": "349080"
  },
  {
    "text": "aggregate aggregative operations with those databases they kind of work but they're not really optimized for that",
    "start": "349080",
    "end": "355080"
  },
  {
    "text": "on the other side of the slide right when we deal with a lot of data and we want to store a lot of it for a long",
    "start": "355080",
    "end": "360120"
  },
  {
    "text": "time and then we want to get value out of it by aggregating getting insights or running analytical queries on it",
    "start": "360120",
    "end": "366419"
  },
  {
    "text": "we have completely a completely different set of tools to deal with those because Normal databases aren't",
    "start": "366419",
    "end": "371580"
  },
  {
    "text": "actually very good at dealing with queries that touch a lot of data all at once right and so this brings us to the",
    "start": "371580",
    "end": "379320"
  },
  {
    "text": "idea of oltp versus olap databases oltp stands for online transaction processing",
    "start": "379320",
    "end": "384600"
  },
  {
    "text": "databases these are the databases most of you know every day SQL Server Oracle whatever",
    "start": "384600",
    "end": "389699"
  },
  {
    "text": "all up from it stands for online analytical processing data and those are specific tools that are optimized not",
    "start": "389699",
    "end": "396660"
  },
  {
    "text": "for individual operations on records but rather on analytical operations which means a lot of data or a long history of",
    "start": "396660",
    "end": "403080"
  },
  {
    "text": "data going back and those have to be optimized very",
    "start": "403080",
    "end": "408720"
  },
  {
    "text": "differently and they have very different economic implications because it's okay to pay a lot of a lot of money for a",
    "start": "408720",
    "end": "415440"
  },
  {
    "text": "database which brings a lot of value out of all of your data and it's usually not as efficient to pay",
    "start": "415440",
    "end": "421620"
  },
  {
    "text": "a lot of a lot of money for database which just stores some data for a few hours or a few weeks",
    "start": "421620",
    "end": "427979"
  },
  {
    "text": "so when we think about what we do with analytical data let's look at a query",
    "start": "427979",
    "end": "433919"
  },
  {
    "text": "right this query I wrote a thousand versions of this kind of query I'm sure most of you did too when you were",
    "start": "433919",
    "end": "439080"
  },
  {
    "text": "looking at some data and trying to figure stuff out right this is a very typical analytical query of data you",
    "start": "439080",
    "end": "444660"
  },
  {
    "text": "select some data you aggregate it you know you count you aggregate you average your max mean whatever then you join a",
    "start": "444660",
    "end": "450360"
  },
  {
    "text": "few tables together because you want to touch a lot of data you subselect for some conditions in the new group when",
    "start": "450360",
    "end": "456479"
  },
  {
    "text": "you sort and do all kinds of stuff right when you look at it query as this query it looks very simple",
    "start": "456479",
    "end": "462180"
  },
  {
    "text": "I'll give you a sec to look at it while I take a sip",
    "start": "462180",
    "end": "466638"
  },
  {
    "text": "and the thing is this query has a lot of different engineering problems hidden",
    "start": "469560",
    "end": "475380"
  },
  {
    "text": "behind it so we'll go through them there's at least four really big very incompatible",
    "start": "475380",
    "end": "481620"
  },
  {
    "text": "problems going on all at once in this analytical query which is just an example most another queries kind of",
    "start": "481620",
    "end": "486720"
  },
  {
    "text": "have all those problems in them one is aggregation right the first one aggregating data is very expensive",
    "start": "486720",
    "end": "491940"
  },
  {
    "text": "computationally and in terms of i o right because unlike pulling out one record from a database if you want to go",
    "start": "491940",
    "end": "498000"
  },
  {
    "text": "and you want to have the sum of all the sales transactions in your entire table of data for six months there's there are",
    "start": "498000",
    "end": "506699"
  },
  {
    "text": "very few ways to do this efficiently you have to read all the data or you have to read it in advance generate indexes and that's also very expensive so this is a",
    "start": "506699",
    "end": "513240"
  },
  {
    "text": "big engineering problem uh joining joining the data sets is an incredibly difficult engineering engine engineering",
    "start": "513240",
    "end": "519599"
  },
  {
    "text": "challenge it's especially hard in distributed systems where some of your data is on one machine and some of data",
    "start": "519599",
    "end": "525180"
  },
  {
    "text": "is on another machine then you have to bring one side of it to the other so that's very expensive and very difficult to do and we spent the last 50 years",
    "start": "525180",
    "end": "532740"
  },
  {
    "text": "optimizing databases to do to do joins efficiently and we're still not done",
    "start": "532740",
    "end": "538440"
  },
  {
    "text": "um actually finding data within a data set so your wear conditions and subselects and sub queries that's a",
    "start": "538440",
    "end": "544740"
  },
  {
    "text": "separate engineering issue where you need to prepare your data in advance to be queries build indexes maintain them and finally grouping is a completely",
    "start": "544740",
    "end": "552240"
  },
  {
    "text": "separate engineering issue uh where you have where especially in a distributed system in order to group data together",
    "start": "552240",
    "end": "557519"
  },
  {
    "text": "you have to shuffle it between different machines over the network which is both very slow because you have to go over the network and also very complicated to",
    "start": "557519",
    "end": "564540"
  },
  {
    "text": "actually coordinate and so each one of these issues is a completely",
    "start": "564540",
    "end": "570420"
  },
  {
    "text": "separate branch of development and most tools can't really solve all of them all at once and so over time we've built",
    "start": "570420",
    "end": "577440"
  },
  {
    "text": "different Big Data Technologies which address a few of these issues really well and kind of ignore the others",
    "start": "577440",
    "end": "584100"
  },
  {
    "text": "because they're optimized to do something really well so we'll take a look at the first example",
    "start": "584100",
    "end": "589260"
  },
  {
    "text": "and I'll start with the story [Music] um let's say it's 1980 and we're in a big",
    "start": "589260",
    "end": "595860"
  },
  {
    "text": "company IBM something and at the time databases were not just a piece of software you installed on the",
    "start": "595860",
    "end": "601920"
  },
  {
    "text": "machine right it's a big closet appliance that you buy from some company for half a million dollars",
    "start": "601920",
    "end": "607680"
  },
  {
    "text": "you install it in the data center and you have a very very few people who can access it and it has a proprietary",
    "start": "607680",
    "end": "613440"
  },
  {
    "text": "interface and its own data format and a big enough company has a few of those and they're not necessarily all from the",
    "start": "613440",
    "end": "619680"
  },
  {
    "text": "same company and they're definitely not interoperable because none of the database vendors actually wants to",
    "start": "619680",
    "end": "624779"
  },
  {
    "text": "cooperate with other database vendors and your business analysts come to you and say we want to take the data that's",
    "start": "624779",
    "end": "630660"
  },
  {
    "text": "in the sales database and do a report that uses data from the user database which are in different actually",
    "start": "630660",
    "end": "637080"
  },
  {
    "text": "different cities and owned by different teams how do we do this an equivalent problem would actually be",
    "start": "637080",
    "end": "642600"
  },
  {
    "text": "today right if you you did your best architecture you you embraced microservices and your architect is very",
    "start": "642600",
    "end": "650339"
  },
  {
    "text": "happy because each microservice has its own database and you have your users microservice and you have your sales",
    "start": "650339",
    "end": "656100"
  },
  {
    "text": "micro service and your inventory microservice and now the bi people can come to you and say well we want to do a report across users and sales and",
    "start": "656100",
    "end": "662700"
  },
  {
    "text": "individually how do we do this like they're in different databases and it's the same problem because you",
    "start": "662700",
    "end": "668700"
  },
  {
    "text": "actually cannot do an efficient join across multiple data sources multiple",
    "start": "668700",
    "end": "673860"
  },
  {
    "text": "databases and in order to actually solve this problem the the new idea came about",
    "start": "673860",
    "end": "679500"
  },
  {
    "text": "in the early 1980s and it's still very popular today which is a data warehouse and the idea is very simple we can't",
    "start": "679500",
    "end": "686459"
  },
  {
    "text": "really efficiently join data and use data across multiple data sources let's just copy all of this data to one place",
    "start": "686459",
    "end": "692820"
  },
  {
    "text": "call it just a normal database but will not let anyone use it in production",
    "start": "692820",
    "end": "698459"
  },
  {
    "text": "because what happens if you let analysts run queries on the production database",
    "start": "698459",
    "end": "704339"
  },
  {
    "text": "oh thank you yes it's going to be very bad oh by the way just as a disclaimer I'm",
    "start": "704339",
    "end": "710700"
  },
  {
    "text": "going to be asking questions and they're not rhetorical questions when I ask a question my honest hope is that somebody",
    "start": "710700",
    "end": "715920"
  },
  {
    "text": "will actually answer it so don't be polite right I don't need you to give this exclaimer in other countries the further",
    "start": "715920",
    "end": "722279"
  },
  {
    "text": "south you go the less polite people get and so people just answer me sometimes without me asking anything",
    "start": "722279",
    "end": "727920"
  },
  {
    "text": "but you people are exceedingly polite so when I ask something I'm seriously asking a question they're not sure questions they're relatively",
    "start": "727920",
    "end": "733680"
  },
  {
    "text": "straightforward so please you know be brave so yeah what happens if you let analysts",
    "start": "733680",
    "end": "738720"
  },
  {
    "text": "run queries in production database it lags it's uh it can be overloaded it's going to compete for resources with your",
    "start": "738720",
    "end": "744899"
  },
  {
    "text": "day-to-day online queries and so we just copy all this data into a separate database we'll call it the data",
    "start": "744899",
    "end": "751260"
  },
  {
    "text": "warehouse because that's where we stole all the data forever and when I keep a long long history of",
    "start": "751260",
    "end": "756540"
  },
  {
    "text": "data in that data warehouse all of our data is going to be replicated and copied into one place we can run join",
    "start": "756540",
    "end": "762180"
  },
  {
    "text": "queries on it we can we don't need to compete with the production use cases so we can run very long queries without",
    "start": "762180",
    "end": "768180"
  },
  {
    "text": "affecting users and it's going to be wonderful of course you can just copy data easily",
    "start": "768180",
    "end": "774060"
  },
  {
    "text": "right you have to build some tools that allow you to extract data from one database and copy them to another continuously that's a process called ETL",
    "start": "774060",
    "end": "781260"
  },
  {
    "text": "extraction transform and load and over the past 50 years we've built I don't",
    "start": "781260",
    "end": "786300"
  },
  {
    "text": "know a million different ways to do that but overall this idea which is very old is actually still in",
    "start": "786300",
    "end": "793200"
  },
  {
    "text": "use today so the the first uh place where that idea actually appeared is a research",
    "start": "793200",
    "end": "798600"
  },
  {
    "text": "paper by uh Devlin and Murphy from 1899 which just explains the idea of a",
    "start": "798600",
    "end": "806339"
  },
  {
    "text": "business data warehouse and this was just revolutionary at the time no one has apparently thought that it's a good",
    "start": "806339",
    "end": "811860"
  },
  {
    "text": "idea to take all of your data put it in one place and then let analysts actually use it right",
    "start": "811860",
    "end": "817620"
  },
  {
    "text": "um and this is something that you can still find today if you go out to Cloud providers you'll find redshift in AWS",
    "start": "817620",
    "end": "824639"
  },
  {
    "text": "and there's a big query in Google and I can't remember the name of the thing in Azure but every cloud provider has their",
    "start": "824639",
    "end": "830579"
  },
  {
    "text": "own data warehouse or you can buy a very expensive proprietary data warehouse but",
    "start": "830579",
    "end": "836220"
  },
  {
    "text": "ultimately they're all the biggest benefit of having a data warehouse is it",
    "start": "836220",
    "end": "842639"
  },
  {
    "text": "gives you normal relational database semantics so you have your schema you have your relational data you have",
    "start": "842639",
    "end": "847680"
  },
  {
    "text": "queries in the same language in SQL which is very convenient because everybody speaks SQL",
    "start": "847680",
    "end": "852860"
  },
  {
    "text": "but it's optimized not to work on individual records it's optimized to actually run analytical queries a long",
    "start": "852860",
    "end": "859500"
  },
  {
    "text": "time but it's not optimized to run many queries side by side unlike an old GP",
    "start": "859500",
    "end": "865200"
  },
  {
    "text": "database which can optimize which is optimized to run a lot of queries all at once and so this is very convenient and very",
    "start": "865200",
    "end": "872760"
  },
  {
    "text": "flexible but there are a couple of problems with it queries just take a long time because",
    "start": "872760",
    "end": "878040"
  },
  {
    "text": "when you run an analytical queries on your past six months of data you have to go and actually physically touch six",
    "start": "878040",
    "end": "883440"
  },
  {
    "text": "months worth of data in terms of i o and this bandwidth and network bandwidth and all that stuff and people are not very",
    "start": "883440",
    "end": "889019"
  },
  {
    "text": "patient especially business people who want to get answers and so a different way of thinking about",
    "start": "889019",
    "end": "894779"
  },
  {
    "text": "solving some of the challenges in this analytical queries showed up a few years later after the",
    "start": "894779",
    "end": "900779"
  },
  {
    "text": "the idea of data warehouses and to explain this I'm going to take another sip and I'll tell you a story",
    "start": "900779",
    "end": "909500"
  },
  {
    "text": "so imagine you're a software developer should be very easy",
    "start": "912720",
    "end": "917180"
  },
  {
    "text": "and one day your CEO or some product manager comes to you and says where's",
    "start": "918839",
    "end": "923940"
  },
  {
    "text": "the stable with six months worth of sales transactions and I want you to build me an application which when I",
    "start": "923940",
    "end": "930180"
  },
  {
    "text": "click a button shows me the total number of transactions like a big dollar amount",
    "start": "930180",
    "end": "935399"
  },
  {
    "text": "amount on the screen and I want it to be really fast I don't want to wait I've used your data",
    "start": "935399",
    "end": "940440"
  },
  {
    "text": "warehouse and it's too slow so go do whatever it is you do in the",
    "start": "940440",
    "end": "945660"
  },
  {
    "text": "background with your software stuff but build me an application which when I click a button immediately shows me the result of the last six months of sales",
    "start": "945660",
    "end": "952440"
  },
  {
    "text": "and we're gonna we'll say we'll do it when I run it to the nearest day right so we'll Define a day as midnight to",
    "start": "952440",
    "end": "959160"
  },
  {
    "text": "midnight and I know some of you are thinking midnight in which time zone because you're smart asses I'm going to",
    "start": "959160",
    "end": "964440"
  },
  {
    "text": "ignore that so what do you do I'm seriously asking hopefully somebody will say okay",
    "start": "964440",
    "end": "971220"
  },
  {
    "text": "what would you suggest we do to solve this issue",
    "start": "971220",
    "end": "976699"
  },
  {
    "text": "you have you can build any kind of software in any language use any hardware whatever you want you",
    "start": "977820",
    "end": "983760"
  },
  {
    "text": "just need to make one thing one button you click it you get the result of total number of sales",
    "start": "983760",
    "end": "990860"
  },
  {
    "text": "schedule is a good direction what does the scheduler do",
    "start": "991199",
    "end": "995959"
  },
  {
    "text": "perfect that's a really good answer and aggregate perfect both those answer together",
    "start": "998519",
    "end": "1003920"
  },
  {
    "text": "are pretty much what actually exists so what we do is in advance because we only",
    "start": "1003920",
    "end": "1009380"
  },
  {
    "text": "need to press the button once before when the manager comes to work right we actually do the query in the background",
    "start": "1009380",
    "end": "1014839"
  },
  {
    "text": "when everyone's asleep and we store the number the query is very simple the result is just you know one float number",
    "start": "1014839",
    "end": "1021079"
  },
  {
    "text": "it's like 8 to 16 bytes maybe depending on which language and which year this is happening in",
    "start": "1021079",
    "end": "1027199"
  },
  {
    "text": "right and so you store it in your memory and you click it and you show the number perfect and everyone's happy you put this in",
    "start": "1027199",
    "end": "1032660"
  },
  {
    "text": "production and a week later the same manager comes to you and says software developing person that was really good remember when I said I just need the one",
    "start": "1032660",
    "end": "1039500"
  },
  {
    "text": "number so I lied here's some scope creep um I want you to actually do this not",
    "start": "1039500",
    "end": "1045020"
  },
  {
    "text": "just one number but separately for every country in the world you think okay well that shouldn't be",
    "start": "1045020",
    "end": "1052160"
  },
  {
    "text": "hard there's like 200 or so countries without getting getting political um okay no problem what do we do to show",
    "start": "1052160",
    "end": "1059360"
  },
  {
    "text": "the same number but for every country in the world",
    "start": "1059360",
    "end": "1063160"
  },
  {
    "text": "yes both of you are very correct you just while you're doing this this count you also group it by the whatever name",
    "start": "1064520",
    "end": "1072200"
  },
  {
    "text": "of the country for example and instead of storing a counter in memory you store a hash table right and the key of the",
    "start": "1072200",
    "end": "1078440"
  },
  {
    "text": "hash table is the country and their value is obviously how many sales were in that country and you keep the old",
    "start": "1078440",
    "end": "1084559"
  },
  {
    "text": "counter because why not you also have the total number perfect it takes you a while to write this and everybody is happy and in a week later the lying",
    "start": "1084559",
    "end": "1091880"
  },
  {
    "text": "manager comes to you and says haha scope creep and it goes okay I wanted this by",
    "start": "1091880",
    "end": "1097220"
  },
  {
    "text": "country and I want this by month and I also want this by day of the week and by category of product",
    "start": "1097220",
    "end": "1103220"
  },
  {
    "text": "can you do that it's obviously very easy right it's not a big deal and so it goes on and goes on and essentially what you're now building is a",
    "start": "1103220",
    "end": "1109940"
  },
  {
    "text": "multi-dimensional data structure in memory which every Dimension is one of these categories so one dimension is how",
    "start": "1109940",
    "end": "1115760"
  },
  {
    "text": "many countries are in the country per country how many sales are per for the category per day of the week and so on",
    "start": "1115760",
    "end": "1122059"
  },
  {
    "text": "and so on and so what we've actually built is an odd up Cube which is a multi-dimensional data",
    "start": "1122059",
    "end": "1128000"
  },
  {
    "text": "structure that holds aggregate results for every possible slicing and dicing of",
    "start": "1128000",
    "end": "1133580"
  },
  {
    "text": "the analytical query and we run this in advance because the thing we want to solve is the the biggest problem of data",
    "start": "1133580",
    "end": "1140840"
  },
  {
    "text": "warehouses which is queries are just very slow and this idea was published in 1993 by",
    "start": "1140840",
    "end": "1147919"
  },
  {
    "text": "Edgar Cod and C Sally they can remember what the C stands for but Edgar Cod just",
    "start": "1147919",
    "end": "1153380"
  },
  {
    "text": "as a side track is a well-known data scientist at Edgar code about 20 years before this wrote",
    "start": "1153380",
    "end": "1160580"
  },
  {
    "text": "the 12 rules for relational database management this was in the late 70s early 90s okay",
    "start": "1160580",
    "end": "1166880"
  },
  {
    "text": "so quick Story Time right before the 1980s databases were just wild west there were",
    "start": "1166880",
    "end": "1173840"
  },
  {
    "text": "no relational databases there was no standards no SQL query nothing like that every company had their own database and",
    "start": "1173840",
    "end": "1179059"
  },
  {
    "text": "it did its own completely different thing and then towards the late 70s",
    "start": "1179059",
    "end": "1184179"
  },
  {
    "text": "query languages became a thing and then the newest happiest thing was relational",
    "start": "1184179",
    "end": "1189860"
  },
  {
    "text": "database management which is you build your tables and schemas and you Define foreign keys to have relationships",
    "start": "1189860",
    "end": "1195080"
  },
  {
    "text": "between tables now it's a revolutionary idea at the time and then every database vendor who wanted to sell their software",
    "start": "1195080",
    "end": "1201980"
  },
  {
    "text": "slapped a very thin layer of relationality under a non-relational database called it the relational",
    "start": "1201980",
    "end": "1207919"
  },
  {
    "text": "database and sell and sold it because that's what vendors do I used to work for database vendor we're not uh never",
    "start": "1207919",
    "end": "1215120"
  },
  {
    "text": "mind um and so Edgar cudd who was a a purist in",
    "start": "1215120",
    "end": "1220700"
  },
  {
    "text": "terms of you know databases was very angry with this essentially",
    "start": "1220700",
    "end": "1227360"
  },
  {
    "text": "a disingenuous way of using the word relational database and he wrote a Manifesto talking about the 12 rules for",
    "start": "1227360",
    "end": "1235100"
  },
  {
    "text": "what makes a relational database and that's still followed today if you go and you look at postgres and MySQL and",
    "start": "1235100",
    "end": "1240860"
  },
  {
    "text": "SQL server and all those though all of those are actually relational databases according to the 12 rules of database",
    "start": "1240860",
    "end": "1246200"
  },
  {
    "text": "management so the same Edgar Cod a few years later was working for a company called Arbor",
    "start": "1246200",
    "end": "1251960"
  },
  {
    "text": "software and they created uh first of all published research paper explaining",
    "start": "1251960",
    "end": "1257660"
  },
  {
    "text": "what an all-up user analytics is and how the cube works and they built a something called",
    "start": "1257660",
    "end": "1264260"
  },
  {
    "text": "um our word Essex which stands for extended spreadsheet software essentially it was a multi-dimensional",
    "start": "1264260",
    "end": "1270860"
  },
  {
    "text": "Excel spreadsheet everyone can imagine in self-riched in 2D now imagine that in five dimensions",
    "start": "1270860",
    "end": "1278260"
  },
  {
    "text": "if you can congratulations but it looks kind of kind of something like this obviously I'm limited three",
    "start": "1278660",
    "end": "1284480"
  },
  {
    "text": "dimensions plus time so that's that's what I have but you can imagine a tester act or a hypercube or further Dimensions",
    "start": "1284480",
    "end": "1292220"
  },
  {
    "text": "above that then that's the idea of the of the olap Cube and you can slice this uh this multi-dimensional data structure",
    "start": "1292220",
    "end": "1300020"
  },
  {
    "text": "across any Dimension and get intermediate results of that Dimension so if you slice it by year as the",
    "start": "1300020",
    "end": "1306020"
  },
  {
    "text": "example here we'll get all the results for year 2004. and we slice it by category along this axis we'll get all",
    "start": "1306020",
    "end": "1313159"
  },
  {
    "text": "the results for every year for this category of product and this is an amazing optimization on",
    "start": "1313159",
    "end": "1319159"
  },
  {
    "text": "data warehouses because it actually gives us the query very quickly there's obviously a problem",
    "start": "1319159",
    "end": "1325460"
  },
  {
    "text": "can anyone spot the problem",
    "start": "1325460",
    "end": "1328658"
  },
  {
    "text": "that's first of all always true for every kind of software but in this case also yes you're exactly right what happens if we",
    "start": "1332000",
    "end": "1338600"
  },
  {
    "text": "have Dimensions with very hard cardinality which means there are very many unique values of that Dimension",
    "start": "1338600",
    "end": "1344419"
  },
  {
    "text": "imagine for example one of the dimensions is the user ID and there's let's say three billion users like in",
    "start": "1344419",
    "end": "1350360"
  },
  {
    "text": "Facebook what happens to the cube yeah because the number of number of",
    "start": "1350360",
    "end": "1358400"
  },
  {
    "text": "data points you have to store is the product the multiplication of the cardinality of every Dimension so if you",
    "start": "1358400",
    "end": "1364340"
  },
  {
    "text": "have very high cardinality Dimensions the cube becomes extremely inefficient it becomes not an optimization but",
    "start": "1364340",
    "end": "1370159"
  },
  {
    "text": "actually much worse than just using a data warehouse right and so this solution is actually very efficient but for a very narrow set",
    "start": "1370159",
    "end": "1377360"
  },
  {
    "text": "of use cases where you have low cardinality dimensions and and I I think some of you missed it you have to",
    "start": "1377360",
    "end": "1383720"
  },
  {
    "text": "declare your dimensions in advance because you have to pre-compute those you can't just change your mind and write a different query because the data",
    "start": "1383720",
    "end": "1390020"
  },
  {
    "text": "doesn't exist yet in late 90s sorry late yeah 97ish I was",
    "start": "1390020",
    "end": "1395419"
  },
  {
    "text": "working at Intel straight out of college and we had a an olive Cube it was a",
    "start": "1395419",
    "end": "1401539"
  },
  {
    "text": "Microsoft's all-up Cube server which by the way wasn't a Microsoft product they bought a company called called Panorama",
    "start": "1401539",
    "end": "1407059"
  },
  {
    "text": "software a year before took their software and slapped Microsoft label on it um and we were we would build this Cube",
    "start": "1407059",
    "end": "1414200"
  },
  {
    "text": "overnight it took about six hours to build the whole Cube and then in the morning we would come and discover whether it actually worked and half the",
    "start": "1414200",
    "end": "1420320"
  },
  {
    "text": "time it did and half the time it didn't and so every day we would go and see if the cube building succeeded if not that",
    "start": "1420320",
    "end": "1426740"
  },
  {
    "text": "was an entire day gone and if it did wonderful we didn't have to do anything so that's",
    "start": "1426740",
    "end": "1433340"
  },
  {
    "text": "that's two ways of solving some of the challenges of this analytical queries now those are very",
    "start": "1433340",
    "end": "1440179"
  },
  {
    "text": "interesting but there's something a problem that's very common to both of those and that is the fact",
    "start": "1440179",
    "end": "1446179"
  },
  {
    "text": "that dark data exists now what is dark data some of you might have jumped to dark",
    "start": "1446179",
    "end": "1451700"
  },
  {
    "text": "web and no it's not data about drugs and guns dark data is data which is written",
    "start": "1451700",
    "end": "1458780"
  },
  {
    "text": "but never ever used and there are many studies which look into you know how much dark data",
    "start": "1458780",
    "end": "1465260"
  },
  {
    "text": "actually exists and depending who you believe somewhere between 50 and 90 of",
    "start": "1465260",
    "end": "1470720"
  },
  {
    "text": "all data ever stored isn't actually used to provide any value nobody actually queries it now intuitively I'll give you",
    "start": "1470720",
    "end": "1477260"
  },
  {
    "text": "an example which makes it very clear think about application logs right every application has logs",
    "start": "1477260",
    "end": "1483020"
  },
  {
    "text": "how often do you go and read all of the logs of all the applications in your production uh whatever it is",
    "start": "1483020",
    "end": "1490940"
  },
  {
    "text": "sometimes it's extremely useful something breaks you have to go and you have to find a specific subset of logs for a specific time or specific service",
    "start": "1490940",
    "end": "1497480"
  },
  {
    "text": "most of the logs are never actually used most logs are actually direct data this is true for almost every category of",
    "start": "1497480",
    "end": "1504440"
  },
  {
    "text": "data it's especially true for machine generated data so sensors logs all kinds of stuff right the problem is you still",
    "start": "1504440",
    "end": "1511039"
  },
  {
    "text": "want to run analytics on data that includes a lot of dark data and what happens when you when you take all",
    "start": "1511039",
    "end": "1517760"
  },
  {
    "text": "this data and you put it in this very expensive piece of software and Hardware called the data warehouse or an olip",
    "start": "1517760",
    "end": "1524179"
  },
  {
    "text": "Cube first of all you pay a lot up front right because you have to pre-process the data and compute the cube and ingest",
    "start": "1524179",
    "end": "1530960"
  },
  {
    "text": "the data so you actually have to pay a very high cost in terms of resources up front to even put the data in",
    "start": "1530960",
    "end": "1537200"
  },
  {
    "text": "and then because it's an online piece of software the database has to continue working you keep paying maintenance on",
    "start": "1537200",
    "end": "1543020"
  },
  {
    "text": "it all the time even if nobody is actually querying it so actually it's very expensive to store",
    "start": "1543020",
    "end": "1548840"
  },
  {
    "text": "data in all app type Solutions into the warehouses even if no one's actually using him",
    "start": "1548840",
    "end": "1555620"
  },
  {
    "text": "so what can we do we can take another sip of water and think about this",
    "start": "1555620",
    "end": "1562720"
  },
  {
    "text": "so let me ask you a leading question what is the cheapest and most effective",
    "start": "1567080",
    "end": "1572720"
  },
  {
    "text": "way to store a bunch of data sorry say again",
    "start": "1572720",
    "end": "1578179"
  },
  {
    "text": "compressing is good but you're way ahead of us sir you're you're very Advanced before you even compress it",
    "start": "1578179",
    "end": "1585880"
  },
  {
    "text": "file hosted file you put it in the file yes perfect that's exactly that's the",
    "start": "1586640",
    "end": "1591740"
  },
  {
    "text": "easiest and cheapest and best way to store data you don't put it in database which is very expensive you just slap",
    "start": "1591740",
    "end": "1597140"
  },
  {
    "text": "all of it into a file in in a row format and files are that's what they exist a",
    "start": "1597140",
    "end": "1603020"
  },
  {
    "text": "file system is really the best database for just a bunch of data so you put your data in a file and you pay very little",
    "start": "1603020",
    "end": "1608840"
  },
  {
    "text": "upfront because writing data directly to a file in a row format is very cheap and at least these days storing data is",
    "start": "1608840",
    "end": "1615799"
  },
  {
    "text": "relatively cheap as well right storing data is I don't know if you go to AWS a terabyte of storage costs 20-ish dollars",
    "start": "1615799",
    "end": "1622760"
  },
  {
    "text": "per month right which is like my laptop has like two terabytes of data on it",
    "start": "1622760",
    "end": "1628520"
  },
  {
    "text": "um so storing data is not very expensive and the cheapest way to do that is just pour all of it there in just a you know",
    "start": "1628520",
    "end": "1633860"
  },
  {
    "text": "row format into a file of course that brings us to a problem why did we spend the last four years being building databases and optimizing stuff if we're",
    "start": "1633860",
    "end": "1640340"
  },
  {
    "text": "just going to put stuff in a file how do we actually run the query on the data and that brings us to data Lakes",
    "start": "1640340",
    "end": "1646880"
  },
  {
    "text": "so the idea of a data lake is to separate two parts of uh traditional",
    "start": "1646880",
    "end": "1653659"
  },
  {
    "text": "database software like every database you can think of it as two separate pieces of software one is the storage",
    "start": "1653659",
    "end": "1659059"
  },
  {
    "text": "layer right that manages the software and the data on disk the data format how it's actually written builds indexes and",
    "start": "1659059",
    "end": "1666200"
  },
  {
    "text": "the second part is the compute which is what executes the query run it reads the indexes reads the data and gives it the",
    "start": "1666200",
    "end": "1672260"
  },
  {
    "text": "result and in almost every database software they're part of the same application they're the same piece of software the network we could actually",
    "start": "1672260",
    "end": "1678919"
  },
  {
    "text": "split those apart and we move the storage to the cheapest form possible which is just bling files and we change",
    "start": "1678919",
    "end": "1686360"
  },
  {
    "text": "the compute part of the database to understand files and read them directly",
    "start": "1686360",
    "end": "1692000"
  },
  {
    "text": "so we'll flip the economics over the database upside down and until the early",
    "start": "1692000",
    "end": "1697340"
  },
  {
    "text": "2000s this wasn't actually very economical which is why it never happened before that",
    "start": "1697340",
    "end": "1703100"
  },
  {
    "text": "uh we had files for a long time right but nobody actually used files as a database or database system and what",
    "start": "1703100",
    "end": "1709820"
  },
  {
    "text": "changed in the early 2000s were two things one search engines were invented Because the Internet became big enough to require storing a lot of data right",
    "start": "1709820",
    "end": "1717020"
  },
  {
    "text": "so we're in the early 2000s we passed something like 1 billion web pages on the internet and this is just around the time of the",
    "start": "1717020",
    "end": "1724400"
  },
  {
    "text": "last round of search engine Wars is everyone is anyone old enough to remember search engine works besides",
    "start": "1724400",
    "end": "1730039"
  },
  {
    "text": "Google yes yes Yahoo has to visit all of those yeah um so search engines were a classic",
    "start": "1730039",
    "end": "1736400"
  },
  {
    "text": "example of something that has a bunch of dark data you ingest all of these URLs you index them and most of the time",
    "start": "1736400",
    "end": "1741860"
  },
  {
    "text": "nobody searches for the web page that you build for a cat right everyone's a small subset of web pages",
    "start": "1741860",
    "end": "1748159"
  },
  {
    "text": "and so this idea of let's put everything in the files and we will pay very little",
    "start": "1748159",
    "end": "1753740"
  },
  {
    "text": "upfront we pay very little in terms of maintenance on storage because storing files is cheap",
    "start": "1753740",
    "end": "1759320"
  },
  {
    "text": "because storage got much cheaper in the 2000s but will pay a much higher cost when you",
    "start": "1759320",
    "end": "1764899"
  },
  {
    "text": "actually want to use the data right so why do we pay a much higher cost I'll give you a an exercise this is the",
    "start": "1764899",
    "end": "1772820"
  },
  {
    "text": "most classic example of big data problems every Big Data course has this as the first example it's called word",
    "start": "1772820",
    "end": "1779779"
  },
  {
    "text": "count imagine you have a one terabyte file of text plain English text and I'm I come",
    "start": "1779779",
    "end": "1785720"
  },
  {
    "text": "to you and I say build an application which tells me how many files how many words are in the file",
    "start": "1785720",
    "end": "1791899"
  },
  {
    "text": "how do you do that hmm",
    "start": "1791899",
    "end": "1796820"
  },
  {
    "text": "perfect that's the perfect answer you open the file you read it sequentially and you count the why the change from",
    "start": "1803059",
    "end": "1810440"
  },
  {
    "text": "White space to non-white space or whatever it is depending on how complex you want to make it but ultimately",
    "start": "1810440",
    "end": "1815600"
  },
  {
    "text": "there's really only one way to do this and there are very few ways to optimize this for runtime right because you have",
    "start": "1815600",
    "end": "1821720"
  },
  {
    "text": "to read every byte because if you miss any of the bytes they could be a white space or another white space and you would miscount so you read every byte",
    "start": "1821720",
    "end": "1828860"
  },
  {
    "text": "and you and you actually get a result at the end of it and before we go into scaling out is there",
    "start": "1828860",
    "end": "1836059"
  },
  {
    "text": "any ways to optimize this for runtime",
    "start": "1836059",
    "end": "1841000"
  },
  {
    "text": "yes I mean okay depending on the business but generally yes",
    "start": "1845120",
    "end": "1851080"
  },
  {
    "text": "so just like one oh chunk the file up okay and then",
    "start": "1852559",
    "end": "1858340"
  },
  {
    "text": "it's a good direction and you were okay yes that's always the first thing people",
    "start": "1859100",
    "end": "1864140"
  },
  {
    "text": "think of it makes perfect sense uh the problem is especially if we're talking you know 20 years ago we're using",
    "start": "1864140",
    "end": "1870260"
  },
  {
    "text": "spinning disks uh it's actually not very efficient to try and parallelize data reads from a hard drive",
    "start": "1870260",
    "end": "1876500"
  },
  {
    "text": "because you're limited by uh the reads the sequentiality of the read head uh",
    "start": "1876500",
    "end": "1881779"
  },
  {
    "text": "and the bandwidth right so even under the most ideal conditions right the bigger the most bandwidth of for reading",
    "start": "1881779",
    "end": "1887419"
  },
  {
    "text": "you can get from a hard drive is about 150 megabits per second with an SSD you can get to 300 maybe maybe 500 megabytes",
    "start": "1887419",
    "end": "1894140"
  },
  {
    "text": "per second but if you have to read the terabyte of data your limiting factor is always going to be IO bandwidth right",
    "start": "1894140",
    "end": "1901159"
  },
  {
    "text": "there is no way around this and so the only way to solve the problem is just not solve it the only way to",
    "start": "1901159",
    "end": "1907520"
  },
  {
    "text": "solve this problem is to just split this file into a bunch of different Hardwares different machines and run part of the",
    "start": "1907520",
    "end": "1913640"
  },
  {
    "text": "problem on each one of them so obviously you still pay as much in total bandwidth and total CPU power and actually there's",
    "start": "1913640",
    "end": "1920480"
  },
  {
    "text": "some extra overhead but because we're only optimizing for runtime we don't really care as much",
    "start": "1920480",
    "end": "1926120"
  },
  {
    "text": "about resources this is really the only practical optimization you can do for runtime you just paralyze it to",
    "start": "1926120",
    "end": "1932240"
  },
  {
    "text": "different Hardware because the combined bandwidth of you know a thousand hard drives is a thousand times 150 megabytes",
    "start": "1932240",
    "end": "1938779"
  },
  {
    "text": "per second right and it is usually more than enough to compute more than fcpu and memory on each of those Hardwares to",
    "start": "1938779",
    "end": "1945200"
  },
  {
    "text": "do the task so the problem is mostly bandwidth now this is the the core idea",
    "start": "1945200",
    "end": "1950720"
  },
  {
    "text": "behind the Dead Lake instead of bringing the data to where your compute lives you",
    "start": "1950720",
    "end": "1956299"
  },
  {
    "text": "take your application the one you talked about we write the application and we copy a copy of this application to every",
    "start": "1956299",
    "end": "1961760"
  },
  {
    "text": "one of our machines where the data data lives and we run it locally we bring the compute to the data instead of the other",
    "start": "1961760",
    "end": "1968539"
  },
  {
    "text": "way around because usually the problem is the bandwidth of the data and not the amount of compute we have",
    "start": "1968539",
    "end": "1976460"
  },
  {
    "text": "right and this idea was revolutionary in the early 2000s and a whole giant giant",
    "start": "1976460",
    "end": "1984320"
  },
  {
    "text": "range of Technologies came out from just this idea and came about uh right around",
    "start": "1984320",
    "end": "1991000"
  },
  {
    "text": "2004-2003 that Google published two of their most famous white papers the first",
    "start": "1991000",
    "end": "1996980"
  },
  {
    "text": "was the white paper about the Google file system which is a distributed file system super efficient very resilient it",
    "start": "1996980",
    "end": "2002559"
  },
  {
    "text": "stores a multiple copies of every file across a network of machines and it",
    "start": "2002559",
    "end": "2008620"
  },
  {
    "text": "allows us to actually do the thing we said which is the first part of the solution right which is take a file and spread it across a lot of machines",
    "start": "2008620",
    "end": "2015940"
  },
  {
    "text": "and the second white paper talks about a new algorithm at the time called mapreduce which is the algorithm that",
    "start": "2015940",
    "end": "2022600"
  },
  {
    "text": "Google used to do precisely the thing we just invented five minutes ago which is take",
    "start": "2022600",
    "end": "2027760"
  },
  {
    "text": "our application send a copy of our application to every machine in the network run it locally on the subset of",
    "start": "2027760",
    "end": "2033220"
  },
  {
    "text": "data bring all the results back and combine them together those are the map and reduce steps right you send the",
    "start": "2033220",
    "end": "2038440"
  },
  {
    "text": "application out you map your data and you get the intermediate result and then",
    "start": "2038440",
    "end": "2043840"
  },
  {
    "text": "you reduce the intermediate results into the final result right and this idea is just amazing because it solves two of",
    "start": "2043840",
    "end": "2050800"
  },
  {
    "text": "the biggest problems that data warehouses and olaps had which is it's very expensive to scale them whereas if",
    "start": "2050800",
    "end": "2057158"
  },
  {
    "text": "you have you know a thousand very cheap machines with consumer grade drives in combination they are much cheaper",
    "start": "2057159",
    "end": "2064419"
  },
  {
    "text": "than you know a half a million dollar Data Warehouse from IBM and that colon Hadoop is the name of the",
    "start": "2064419",
    "end": "2072720"
  },
  {
    "text": "first open source system that implemented this idea outside of Google the two Engineers who actually wrote",
    "start": "2072720",
    "end": "2079118"
  },
  {
    "text": "Hadoop we're trying to solve something that's very interesting they were trying to build a search engine which could",
    "start": "2079119",
    "end": "2084520"
  },
  {
    "text": "index 1 billion pages which today doesn't sound like a lot one billion of anything isn't really that",
    "start": "2084520",
    "end": "2089800"
  },
  {
    "text": "much you know in today in terms of data but in 2002 indexing 1 billion web pages",
    "start": "2089800",
    "end": "2095940"
  },
  {
    "text": "was considered pretty much impossible in terms of economics right because they",
    "start": "2095940",
    "end": "2101380"
  },
  {
    "text": "did a research and they found that to index 1 billion web pages for a search engine they would need a system that",
    "start": "2101380",
    "end": "2107440"
  },
  {
    "text": "costs roughly half a million dollars up front and around fifty thousand dollars maintenance every month which is not",
    "start": "2107440",
    "end": "2113260"
  },
  {
    "text": "something you can afford is a startup who wants to build a search engine uh and so they took the ideas from these",
    "start": "2113260",
    "end": "2118960"
  },
  {
    "text": "two Google's uh white papers and they built a system called Hadoop and today",
    "start": "2118960",
    "end": "2124540"
  },
  {
    "text": "you wouldn't run into Hadoop anymore unless you work for like a very very conservative Bank but there are different systems that",
    "start": "2124540",
    "end": "2131440"
  },
  {
    "text": "take the same ideas and move it forward and Hadoop Quran the problem is",
    "start": "2131440",
    "end": "2137079"
  },
  {
    "text": "as it often happens with new technologies the first generation of a solution to a",
    "start": "2137079",
    "end": "2142540"
  },
  {
    "text": "problem that was considered impossible before that is usually written by social Developers for software developers which",
    "start": "2142540",
    "end": "2148900"
  },
  {
    "text": "means it's incredibly inconvenient to use for normal people uh you can think of it as there's plenty",
    "start": "2148900",
    "end": "2155079"
  },
  {
    "text": "of examples but in this case Hadoop required you to write a Java application using some apis",
    "start": "2155079",
    "end": "2161140"
  },
  {
    "text": "compiled into a jar and then use a very complicated configuration system to spread this jar across the network of",
    "start": "2161140",
    "end": "2167020"
  },
  {
    "text": "machines run it on the local file system bring back the results and the results would be written to a different file on a",
    "start": "2167020",
    "end": "2173380"
  },
  {
    "text": "different file system then you have to go and read the text file and that was your your API for the data processing",
    "start": "2173380",
    "end": "2179560"
  },
  {
    "text": "system no GUI no SQL Server SQL query is nothing like that now the second generation of those tools",
    "start": "2179560",
    "end": "2186579"
  },
  {
    "text": "as always happens with technology was optimized for non software developers so they had a nice query language like SQL",
    "start": "2186579",
    "end": "2193119"
  },
  {
    "text": "and they had a GUI and all kinds of nice things and",
    "start": "2193119",
    "end": "2198160"
  },
  {
    "text": "today we're actually going towards the third generation of these kind of tools and usually the third generation is actually",
    "start": "2198160",
    "end": "2205660"
  },
  {
    "text": "not even an improvement on the technology but rather a change in mindset set where instead of",
    "start": "2205660",
    "end": "2212020"
  },
  {
    "text": "let's think of it this way uh the second generation of every software is usually much more complicated and much better but it it requires a certain set of",
    "start": "2212020",
    "end": "2220900"
  },
  {
    "text": "skills which few people have and it creates like a new Mini profession it happens when databases Revelation",
    "start": "2220900",
    "end": "2226359"
  },
  {
    "text": "databases come around and dbas were invented it happened when devops came became a thing and devops people invent",
    "start": "2226359",
    "end": "2231880"
  },
  {
    "text": "were invented and became like a big profession and it happened around 2010 or so when data engineering became a",
    "start": "2231880",
    "end": "2239020"
  },
  {
    "text": "profession because people it was complicated and important enough to have like a separate mini profession just for",
    "start": "2239020",
    "end": "2245079"
  },
  {
    "text": "that and usually what ends up happening is a few years later",
    "start": "2245079",
    "end": "2251020"
  },
  {
    "text": "um companies get sick of hiring all these very expensive engineers and what they want is for one company",
    "start": "2251020",
    "end": "2257560"
  },
  {
    "text": "like Amazon like Microsoft like Google to hire all of these smart people and build something that's easy to use for",
    "start": "2257560",
    "end": "2263500"
  },
  {
    "text": "everybody else and this is more or less what's happening now all of the Big Data tools are now transforming to platforms",
    "start": "2263500",
    "end": "2269500"
  },
  {
    "text": "usually by big cloud providers or big vendors and normal people like us can go and just swipe your credit card and use",
    "start": "2269500",
    "end": "2275740"
  },
  {
    "text": "the platform because they have a nice UI and a nice query language and we don't need to maintain a very expensive",
    "start": "2275740",
    "end": "2281020"
  },
  {
    "text": "infrastructure on the back end right and this is this is where we're going towards like the third generation of data tools",
    "start": "2281020",
    "end": "2287260"
  },
  {
    "text": "but going back to the story remember the file we were reading um so I lied when I say there are very",
    "start": "2287260",
    "end": "2292839"
  },
  {
    "text": "few ways to optimize reading the file there's actually a lot of ways to optimize it but you have to ignore all",
    "start": "2292839",
    "end": "2298240"
  },
  {
    "text": "the things I had said before that so around 2010 or so",
    "start": "2298240",
    "end": "2304540"
  },
  {
    "text": "when people were trying to solve how to optimize reading files efficiently because that's",
    "start": "2304540",
    "end": "2309820"
  },
  {
    "text": "the cheapest way to store data um they decided that we don't it's it may",
    "start": "2309820",
    "end": "2316839"
  },
  {
    "text": "be okay to pay a little bit up front to do a little bit of pre-processing to",
    "start": "2316839",
    "end": "2322060"
  },
  {
    "text": "optimize the way we store data in files in order to make it easy to easier to query we're not going to do a full",
    "start": "2322060",
    "end": "2328000"
  },
  {
    "text": "database right we're not going to build the whole you know different kind of database here we're not going to reinvent indexes and memory caches and",
    "start": "2328000",
    "end": "2334300"
  },
  {
    "text": "all that but it's okay to do a little bit of work up front to make the Brute",
    "start": "2334300",
    "end": "2339400"
  },
  {
    "text": "Force queries afterwards slightly more more efficient and this is why columnar formats were",
    "start": "2339400",
    "end": "2344500"
  },
  {
    "text": "invented now what's a column in the format um Let's uh go back to another example",
    "start": "2344500",
    "end": "2351640"
  },
  {
    "text": "let's say that instead of saving raw text in a file we're allowed to change the data before",
    "start": "2351640",
    "end": "2358599"
  },
  {
    "text": "we store it in the file how can we optimize the word count example we had earlier",
    "start": "2358599",
    "end": "2364980"
  },
  {
    "text": "to make it easier and more efficient to count all the file all the words in the file",
    "start": "2364980",
    "end": "2372059"
  },
  {
    "text": "the only query we're ever going to do is count all the words in the file how can we change the application that thrives",
    "start": "2373420",
    "end": "2378820"
  },
  {
    "text": "the file to make it easy to count towards exactly that's that's an excellent way",
    "start": "2378820",
    "end": "2386020"
  },
  {
    "text": "to do it as we write the words we count them and we just store the account and at the end of the file just at the start",
    "start": "2386020",
    "end": "2392560"
  },
  {
    "text": "of the file some header and then we don't even need to read the file right we'll solve the biggest problem we don't need to use any bandwidth or any i o we",
    "start": "2392560",
    "end": "2399520"
  },
  {
    "text": "just read the header we get the count now we need to get if in the real world it needs to be slightly more complicated",
    "start": "2399520",
    "end": "2404619"
  },
  {
    "text": "because we don't just need to count it right we need to build a lot of different statistics and read them in advance and write them in advance and",
    "start": "2404619",
    "end": "2411820"
  },
  {
    "text": "then we might not need to read the entire file because we can read much less of it and answer the same question and this is what columnar formats",
    "start": "2411820",
    "end": "2418900"
  },
  {
    "text": "actually do the problem with this of course is if you're going to do pre-processing you cannot just directly write all of the",
    "start": "2418900",
    "end": "2425680"
  },
  {
    "text": "data to the file as it arrives need to buffer some data to pre-compute it right because in order to count it you have to",
    "start": "2425680",
    "end": "2431680"
  },
  {
    "text": "buffer some of it count it and then write everything you buffered so columnar formats and there's actually two standard formats today in the",
    "start": "2431680",
    "end": "2438040"
  },
  {
    "text": "industry one one is called optimized row columnar format or orc and the other is parquet parquet depending on your",
    "start": "2438040",
    "end": "2445359"
  },
  {
    "text": "pronunciation um and they both were more or less work similarly the way the work is when",
    "start": "2445359",
    "end": "2452020"
  },
  {
    "text": "you're writing to the file instead of spilling the data directly into the file we buffer by default about 10 000 rows",
    "start": "2452020",
    "end": "2459040"
  },
  {
    "text": "and on those 10 000 rows uh 10 000 records of data uh in memory we compute",
    "start": "2459040",
    "end": "2465220"
  },
  {
    "text": "statistics on every field on every column and we compute for example from the Merit columns we'll find the minimum and",
    "start": "2465220",
    "end": "2471339"
  },
  {
    "text": "the maximum and the average and the sum and count and pretty much all the statistics and for string columns we",
    "start": "2471339",
    "end": "2477640"
  },
  {
    "text": "might build a bloom filter or a dictionary of unique values and once we've done that we write the we write",
    "start": "2477640",
    "end": "2484180"
  },
  {
    "text": "the records into the file but we don't just write them like we would in a database so the way you write to a database is",
    "start": "2484180",
    "end": "2490240"
  },
  {
    "text": "every record is sequential because Normal databases deal with individual records and the most efficient way to",
    "start": "2490240",
    "end": "2496240"
  },
  {
    "text": "read the whole record is read it sequentially so in a standard database like SQL Server all every data record in",
    "start": "2496240",
    "end": "2502000"
  },
  {
    "text": "a table is written as a sequence of bytes in the file in a columnar format we flip that around",
    "start": "2502000",
    "end": "2507520"
  },
  {
    "text": "we take the values for one column and write all of the values for the column sequentially then we go to the next",
    "start": "2507520",
    "end": "2513160"
  },
  {
    "text": "column and write that column sequentially the next one the next the next one and then we write we close the row the stripe it's called and write the",
    "start": "2513160",
    "end": "2521200"
  },
  {
    "text": "footer which is which has the statistics for every column and the reason we do that is going back again to the",
    "start": "2521200",
    "end": "2526720"
  },
  {
    "text": "difference between olap and oltp databases in normal databases we usually want to get the whole record or update",
    "start": "2526720",
    "end": "2532119"
  },
  {
    "text": "the whole record in analytical applications we usually want to run aggregations and it's very rare when you",
    "start": "2532119",
    "end": "2538119"
  },
  {
    "text": "need to aggregate every single column in your data like if your data has 50 different fields you're not going to",
    "start": "2538119",
    "end": "2544119"
  },
  {
    "text": "group by every single field of those 50 right you will usually do something like select whatever name and count by three",
    "start": "2544119",
    "end": "2550839"
  },
  {
    "text": "fields or two fields or something and so it's very rare when you need to read all of the data but it's very very often",
    "start": "2550839",
    "end": "2557140"
  },
  {
    "text": "that you need to read the all the values for one column and so it makes the most sense to write all the values",
    "start": "2557140",
    "end": "2562300"
  },
  {
    "text": "sequentially for that one column and then the next one and next",
    "start": "2562300",
    "end": "2567339"
  },
  {
    "text": "um and so if we take now we should go back to the example of word count",
    "start": "2567339",
    "end": "2573160"
  },
  {
    "text": "how would we write a word count over a columnar format we wouldn't even count anything we would go to the index of the",
    "start": "2573160",
    "end": "2579819"
  },
  {
    "text": "file found how many find how many stripes there are go to the start of the stripe find the footer at the end of the",
    "start": "2579819",
    "end": "2585700"
  },
  {
    "text": "stripe read the statistics for that one column which gives us the count go to the next stripe go to the next",
    "start": "2585700",
    "end": "2591160"
  },
  {
    "text": "stripe some old accounts and that's our answer we would need to run to read maybe a kilobyte maybe two kilobyte of",
    "start": "2591160",
    "end": "2598780"
  },
  {
    "text": "data and it doesn't even matter how big the file is because you only read the actual statistics that are stored and",
    "start": "2598780",
    "end": "2604599"
  },
  {
    "text": "this bypasses the entire problem of bandwidth and you can also obviously do this very",
    "start": "2604599",
    "end": "2610119"
  },
  {
    "text": "very easily across a cluster of machines because each one of those will do this individually on the local files then",
    "start": "2610119",
    "end": "2616000"
  },
  {
    "text": "give you the result and we can sum the results from every different machine in the cluster into one one combined result",
    "start": "2616000",
    "end": "2623940"
  },
  {
    "text": "so all this stuff was actually invented by multiple different companies over the",
    "start": "2631540",
    "end": "2638200"
  },
  {
    "text": "course of very few years roughly from 2010 to 2012 and there are many reasons for that but",
    "start": "2638200",
    "end": "2644140"
  },
  {
    "text": "mainly are again economics and changes in the way people behave",
    "start": "2644140",
    "end": "2650619"
  },
  {
    "text": "2010 and 212 is roughly when social media became like unfortunately a part",
    "start": "2650619",
    "end": "2656079"
  },
  {
    "text": "of all of our daily lives I say unfortunately even while I'm working at Facebook but it's you know Facebook and and",
    "start": "2656079",
    "end": "2663640"
  },
  {
    "text": "whatever it was before Myspace and media and all that all the stuff that moved online became big enough that it",
    "start": "2663640",
    "end": "2670720"
  },
  {
    "text": "required all of these companies so Google and Microsoft and Facebook and a bunch of others to write their own",
    "start": "2670720",
    "end": "2676420"
  },
  {
    "text": "version of these big data tools and they all work very similarly all the ideas we",
    "start": "2676420",
    "end": "2681819"
  },
  {
    "text": "talked about columnar formats writing file and reading directly from files separating storage from compute each one",
    "start": "2681819",
    "end": "2688420"
  },
  {
    "text": "of them implemented a tool that does exactly the same thing with some variations they're optimized for slightly different things but it's",
    "start": "2688420",
    "end": "2694839"
  },
  {
    "text": "surprising how many companies invented basically the same thing and again for reasons for economics and",
    "start": "2694839",
    "end": "2701800"
  },
  {
    "text": "solving a problem which wasn't actually solvable until then",
    "start": "2701800",
    "end": "2706500"
  },
  {
    "text": "of course as with everything we looked at data Lakes have one big problem",
    "start": "2708220",
    "end": "2714599"
  },
  {
    "text": "and the problem is you write the data directly which means it doesn't really have a schema you can",
    "start": "2715420",
    "end": "2720640"
  },
  {
    "text": "write any kind of data and over time data Lakes which is just the repository of all the data you have in your",
    "start": "2720640",
    "end": "2726579"
  },
  {
    "text": "organization as just a collection of files usually it turns into a data swamp which is an official term",
    "start": "2726579",
    "end": "2733000"
  },
  {
    "text": "uh which is this is like the equivalent this is the data equivalent of spaghetti code right code goes from code to",
    "start": "2733000",
    "end": "2738700"
  },
  {
    "text": "speaker code data goes from a lake to a swamp uh which means no one actually knows",
    "start": "2738700",
    "end": "2744040"
  },
  {
    "text": "what's in the data Lake there's like a bunch of files no one is brave enough to delete those because they might be",
    "start": "2744040",
    "end": "2749260"
  },
  {
    "text": "useful uh and over time you just get more and more of it and the you'll forget what the format is and the file itself",
    "start": "2749260",
    "end": "2755980"
  },
  {
    "text": "doesn't really have a schema it's not like a normal database where you have a table with fields and rows and all that",
    "start": "2755980",
    "end": "2761380"
  },
  {
    "text": "uh and that's a big downside of data lakes and that's a big upside for data",
    "start": "2761380",
    "end": "2766720"
  },
  {
    "text": "warehouses and olaps because which is why they still exist today right because if data Lake was so Superior everybody",
    "start": "2766720",
    "end": "2772720"
  },
  {
    "text": "would just switch to that and abandon data warehouses and all the other Technologies but the fact is they didn't",
    "start": "2772720",
    "end": "2778119"
  },
  {
    "text": "because there are big advantages to actually structure it and schematize data it's much easier to work with it",
    "start": "2778119",
    "end": "2785140"
  },
  {
    "text": "and usually the people who pay the bills which is the business people listen to the analysts and analysts are",
    "start": "2785140",
    "end": "2791020"
  },
  {
    "text": "lazy and they went in and they wanted to be easy to work with data",
    "start": "2791020",
    "end": "2796200"
  },
  {
    "text": "I hope there's any business analysts here",
    "start": "2797079",
    "end": "2801119"
  },
  {
    "text": "I apologize some events um and so as usually happens with",
    "start": "2803560",
    "end": "2809140"
  },
  {
    "text": "when we solve a problem in very different ways optimizing for different uh directions eventually",
    "start": "2809140",
    "end": "2816819"
  },
  {
    "text": "the different implementations start borrowing features from the other if you think of it it happened with different",
    "start": "2816819",
    "end": "2823720"
  },
  {
    "text": "front-end Frameworks over time different Frontline Frameworks kind of borrowed features from each other",
    "start": "2823720",
    "end": "2829619"
  },
  {
    "text": "different data Technologies also borrow features from each other so modern implementations of data Lakes usually",
    "start": "2829619",
    "end": "2836859"
  },
  {
    "text": "steal a lot of the ideas a lot of the benefits from data warehouses and the other way around",
    "start": "2836859",
    "end": "2842260"
  },
  {
    "text": "which brings us to this idea of a data lake house which is a data Lake that",
    "start": "2842260",
    "end": "2848319"
  },
  {
    "text": "works like a data warehouse it's a stupid name I hate it but the idea behind it is actually very",
    "start": "2848319",
    "end": "2854859"
  },
  {
    "text": "interesting which is if we give up",
    "start": "2854859",
    "end": "2860079"
  },
  {
    "text": "uh if we pay a lot of complexity because usually when you have a hybrid technology you always paid pay with",
    "start": "2860079",
    "end": "2865960"
  },
  {
    "text": "increased complexity but if we're okay with having much more complicated systems we can have the benefit of both",
    "start": "2865960",
    "end": "2872200"
  },
  {
    "text": "the data warehouse which is structured data semantics which works kind of like a normal database and also the very",
    "start": "2872200",
    "end": "2879760"
  },
  {
    "text": "cheap storage of a data Lake uh and obviously it's something that not",
    "start": "2879760",
    "end": "2886000"
  },
  {
    "text": "every company will create for itself because again the level of complexity is exponentially higher but enough",
    "start": "2886000",
    "end": "2892000"
  },
  {
    "text": "companies wanted this ability to go and work with data easily but also not pay too much",
    "start": "2892000",
    "end": "2897460"
  },
  {
    "text": "money for it and so in the last three or four years this",
    "start": "2897460",
    "end": "2903579"
  },
  {
    "text": "idea of a data lake house became very prevalent in the industry and the way it's usually implemented is",
    "start": "2903579",
    "end": "2911800"
  },
  {
    "text": "you wouldn't normally build a data lake house on your own in a company a few companies like Facebook like Google or",
    "start": "2911800",
    "end": "2919240"
  },
  {
    "text": "Amazon have enough engineers and enough interest to build their own data lake house most normal companies would just",
    "start": "2919240",
    "end": "2926200"
  },
  {
    "text": "use it as a service because we're in the third generation of data Technologies and it's much more advantageous to",
    "start": "2926200",
    "end": "2932140"
  },
  {
    "text": "actually buy this as a service from a company like snowflake or databricks there's a bunch of them I'm not selling",
    "start": "2932140",
    "end": "2937839"
  },
  {
    "text": "any of those and just use that as a service instead of building your own data lake house",
    "start": "2937839",
    "end": "2943540"
  },
  {
    "text": "which is actually just combining a bunch of Open Source Technologies in very complicated ways",
    "start": "2943540",
    "end": "2949119"
  },
  {
    "text": "um and the advantage is still not clear which is better right if",
    "start": "2949119",
    "end": "2954579"
  },
  {
    "text": "you want just using something very simple where you can go and use a tool like redshift on AWS or build your own",
    "start": "2954579",
    "end": "2961060"
  },
  {
    "text": "data warehouse you will get advantage of having structured data you will have",
    "start": "2961060",
    "end": "2967240"
  },
  {
    "text": "asset semantics which is a database term for having for behaving like what you expect from a",
    "start": "2967240",
    "end": "2972819"
  },
  {
    "text": "database for being having atomicity and having consistent data and having isolation between data operations and",
    "start": "2972819",
    "end": "2979540"
  },
  {
    "text": "being durable uh which is something you expect in a normal relational database and data warehouses work the same way",
    "start": "2979540",
    "end": "2985839"
  },
  {
    "text": "data Lakes behave like files they're not consistent there's no protection for durability if somebody is modifying a",
    "start": "2985839",
    "end": "2991540"
  },
  {
    "text": "file while you're reading it it's probably going to cause data corruption and they're generally slower because",
    "start": "2991540",
    "end": "2997359"
  },
  {
    "text": "again they're files you're paying a Brute Force query over all of the files and it will take longer and they have no",
    "start": "2997359",
    "end": "3004079"
  },
  {
    "text": "schema so you have to actually validate the data while you're reading it which means you have to spend a lot more",
    "start": "3004079",
    "end": "3010200"
  },
  {
    "text": "effort in making sure that data is clean and validating it and so on but it's really really cheap",
    "start": "3010200",
    "end": "3015900"
  },
  {
    "text": "and it's very flexible because you can write any data before you know how you're planning to query it which is a",
    "start": "3015900",
    "end": "3022079"
  },
  {
    "text": "big downside of olaps and data warehouses where you have to declare the schema and create your cubes up front",
    "start": "3022079",
    "end": "3028560"
  },
  {
    "text": "you actually have to know what you're going to do with the data before you store any of the data with data",
    "start": "3028560",
    "end": "3033780"
  },
  {
    "text": "warehouses it's very attractive to companies who like to accumulate data and all companies like to accumulate",
    "start": "3033780",
    "end": "3038819"
  },
  {
    "text": "data which is they can store everything and figure out how to query it later and a lake house is kind of like a",
    "start": "3038819",
    "end": "3045540"
  },
  {
    "text": "hybridized version of both it's supposed to give you both and it's supposed to and there's a big asterisk because like every good",
    "start": "3045540",
    "end": "3051900"
  },
  {
    "text": "engineering question it starts with it depends how you use it and maybe it's going to be more expensive because if",
    "start": "3051900",
    "end": "3057599"
  },
  {
    "text": "you use a lake house as a service usually the services are are priced by",
    "start": "3057599",
    "end": "3064079"
  },
  {
    "text": "usage and by storage so you store more data you pay more you query it more often you pay more and it kind of",
    "start": "3064079",
    "end": "3070020"
  },
  {
    "text": "becomes a trap eventually right",
    "start": "3070020",
    "end": "3075740"
  },
  {
    "text": "so before we finish I want to talk a little bit about the future right and give you like a little glimpse into",
    "start": "3078240",
    "end": "3084180"
  },
  {
    "text": "what I think is going to happen this is purely my opinion there's no research behind this mostly just experience",
    "start": "3084180",
    "end": "3090260"
  },
  {
    "text": "and there's a couple of spoilers I want to talk about the first is what's probably going to happen uh for social",
    "start": "3090260",
    "end": "3096960"
  },
  {
    "text": "developers uh in terms of using data and the bigger Trend that's going to",
    "start": "3096960",
    "end": "3102660"
  },
  {
    "text": "happen I think in the next three to five years is something that's called it's called a data mesh architecture",
    "start": "3102660",
    "end": "3109040"
  },
  {
    "text": "which is this idea of instead of having data be subprofession for a very",
    "start": "3109040",
    "end": "3114540"
  },
  {
    "text": "specialized group of people data engineers um build data platforms which are",
    "start": "3114540",
    "end": "3119940"
  },
  {
    "text": "self-service which would move the ability to control data and manipulate",
    "start": "3119940",
    "end": "3124980"
  },
  {
    "text": "it from specialized groups like a data platform architecture Team every company has these days has like a data",
    "start": "3124980",
    "end": "3132540"
  },
  {
    "text": "infrastructure team or a data engineering team or one of those and those are the people who own the data because they're the only ones who can",
    "start": "3132540",
    "end": "3139079"
  },
  {
    "text": "manipulate and you know use these complicated tools and over time we're going to see I think a shift",
    "start": "3139079",
    "end": "3144540"
  },
  {
    "text": "in data ownership from specialized teams to normal software development teams this kind of thing happened",
    "start": "3144540",
    "end": "3151020"
  },
  {
    "text": "in the last 10 years in different domains as well a lot of companies now started building teams that have all the",
    "start": "3151020",
    "end": "3156900"
  },
  {
    "text": "all of the different subprofessions in them so you have a front-end engineer and back-end engineers and a devops",
    "start": "3156900",
    "end": "3162540"
  },
  {
    "text": "person and a quality or automation engineer all in the same team and a",
    "start": "3162540",
    "end": "3167880"
  },
  {
    "text": "product manager so they can actually build their own domain services on their own and not depend on anybody",
    "start": "3167880",
    "end": "3173880"
  },
  {
    "text": "and data is still the only exception that I very often see which is you'll have all of these people in one team and",
    "start": "3173880",
    "end": "3180599"
  },
  {
    "text": "a separate data team I think life companies are going to",
    "start": "3180599",
    "end": "3186720"
  },
  {
    "text": "recognize that it makes more sense to move data ownership into the normal integrated teams as well which means",
    "start": "3186720",
    "end": "3193380"
  },
  {
    "text": "they're going to start using uh self-service tools to actually give control of data creating tables moving",
    "start": "3193380",
    "end": "3199260"
  },
  {
    "text": "data around over to software develop developers we're really that actually belongs and this is one of the bigger Trends I",
    "start": "3199260",
    "end": "3206160"
  },
  {
    "text": "think is going to happen the other is what I call biggish data and this is",
    "start": "3206160",
    "end": "3212099"
  },
  {
    "text": "not my term I sold it from a good friend of mine um",
    "start": "3212099",
    "end": "3217559"
  },
  {
    "text": "so if you remember at the start of the stock we defined what is big data which is working with data sets that are too",
    "start": "3217559",
    "end": "3223680"
  },
  {
    "text": "complex or too large to be dealt with with traditional tools and the thing is what was Big Data 20",
    "start": "3223680",
    "end": "3230280"
  },
  {
    "text": "years ago and what was Big Data 10 years ago is very different what from what's considered Big Data today",
    "start": "3230280",
    "end": "3237059"
  },
  {
    "text": "because traditional tools didn't just go to sleep for 20 years SQL server and postgres and Microsoft or",
    "start": "3237059",
    "end": "3243059"
  },
  {
    "text": "SQL server and Oracle they had 20 years of Advent of advancement index and progress",
    "start": "3243059",
    "end": "3248819"
  },
  {
    "text": "so the the standard tools we have today are so much better at dealing with a lot of data and doing a lot of this work",
    "start": "3248819",
    "end": "3255300"
  },
  {
    "text": "quickly than they were 20 years ago when Big Data was essentially invented and so",
    "start": "3255300",
    "end": "3260540"
  },
  {
    "text": "the mindset of a lot of companies is lagging behind what actually exists",
    "start": "3260540",
    "end": "3265619"
  },
  {
    "text": "today what normally happens is companies overestimate how quickly",
    "start": "3265619",
    "end": "3271500"
  },
  {
    "text": "they're going to accumulate data some overly excited architect like myself will go and say we're going to start a",
    "start": "3271500",
    "end": "3277500"
  },
  {
    "text": "new project and we're going to advance and we're going to be prepared for hyper growth our data is going to grow",
    "start": "3277500",
    "end": "3283200"
  },
  {
    "text": "exponentially because our clients are going to grow exponentially and we need to build a data Lake and we need a data",
    "start": "3283200",
    "end": "3288720"
  },
  {
    "text": "warehouse and we need a lake house and we need all these seven other things uh before we actually experience hyper",
    "start": "3288720",
    "end": "3294599"
  },
  {
    "text": "growth because when we do it's actually going to be too late it's too hard to add those things and they overestimate just how much data",
    "start": "3294599",
    "end": "3300780"
  },
  {
    "text": "tooling they need where in reality most companies have very linear and gradual",
    "start": "3300780",
    "end": "3305880"
  },
  {
    "text": "growth in terms of how much data they have and how much their data needs evolve and today especially with Cloud",
    "start": "3305880",
    "end": "3312480"
  },
  {
    "text": "tools it's very easy to fulfill most companies needs with just relational databases and",
    "start": "3312480",
    "end": "3319740"
  },
  {
    "text": "normal tooling um so how many people know what an exabyte is exabyte",
    "start": "3319740",
    "end": "3326339"
  },
  {
    "text": "anybody want to say what it is yes it's a shitload of data",
    "start": "3326339",
    "end": "3334140"
  },
  {
    "text": "uh but how much exactly it's 1024 if you want to be exact",
    "start": "3334140",
    "end": "3342000"
  },
  {
    "text": "petabytes is which is 1 million terabytes is 1 billion gigabytes one trillion megabytes",
    "start": "3342000",
    "end": "3348599"
  },
  {
    "text": "and so on right so there are a few companies which actually have an exabyte of data",
    "start": "3348599",
    "end": "3354359"
  },
  {
    "text": "Facebook is one company uh Google Amazon maybe Microsoft there are a few",
    "start": "3354359",
    "end": "3360240"
  },
  {
    "text": "companies which actually have an excellent of data obviously don't query it all at once much more companies have",
    "start": "3360240",
    "end": "3366839"
  },
  {
    "text": "petabyte size data right the company I worked at before I came to Facebook had several terabytes of data there are",
    "start": "3366839",
    "end": "3373020"
  },
  {
    "text": "quite a few companies like that but in reality the vast majority of companies are in the terabyte scale one tens of",
    "start": "3373020",
    "end": "3379140"
  },
  {
    "text": "terabytes most normal companies don't actually go into a petabyte range and a terabyte is actually something I can do",
    "start": "3379140",
    "end": "3385680"
  },
  {
    "text": "on my laptop all right so I can actually put a terabyte of data at home I have I",
    "start": "3385680",
    "end": "3391200"
  },
  {
    "text": "think maybe terrible 10 terabytes of tors of storage across all my computers most people here have at least like a",
    "start": "3391200",
    "end": "3398160"
  },
  {
    "text": "few hundred gigabytes in on their phone alone right so terabit of data is not that much anymore you can put a terabyte",
    "start": "3398160",
    "end": "3405300"
  },
  {
    "text": "of data into pretty much any database that exists today you can query it all right you don't put it on your production database and run analytical",
    "start": "3405300",
    "end": "3411300"
  },
  {
    "text": "queries on it but it's okay to actually think of terabytes as not really very big data right the technology has moved",
    "start": "3411300",
    "end": "3418980"
  },
  {
    "text": "on so what is Big Data today is not what it was 20 years ago and we don't actually",
    "start": "3418980",
    "end": "3424020"
  },
  {
    "text": "need to run to Big Data tools to solve data problems most people in this room if I ask you to you know go build me a",
    "start": "3424020",
    "end": "3431400"
  },
  {
    "text": "data system which can handle one terabyte of data and query it you can just spin up a SQL server or whatever",
    "start": "3431400",
    "end": "3436559"
  },
  {
    "text": "it's going to be and use normal tools it's actually a really cool database called dacdb",
    "start": "3436559",
    "end": "3443400"
  },
  {
    "text": "collect the bird if you're interested you should Google it it's one of the new all up databases",
    "start": "3443400",
    "end": "3450059"
  },
  {
    "text": "it's an in-memory database which is designed to work on a single machine and",
    "start": "3450059",
    "end": "3455819"
  },
  {
    "text": "do what used to be a big data applications just on your laptop right it can read data from cloud providers",
    "start": "3455819",
    "end": "3462000"
  },
  {
    "text": "you can load data from S3 or from GitHub or from any web address or from local on",
    "start": "3462000",
    "end": "3467520"
  },
  {
    "text": "locally and we'll load data into memory and run all app queries analytical queries in real time very quickly",
    "start": "3467520",
    "end": "3475260"
  },
  {
    "text": "in at speeds and performance levels which were just completely impossible 10 years ago and so today if you just want",
    "start": "3475260",
    "end": "3481140"
  },
  {
    "text": "to build an analytical application you can download something like Doug DB or a similar database on your laptop put your",
    "start": "3481140",
    "end": "3486900"
  },
  {
    "text": "terabyte of data here and just run queries on it and it will actually work",
    "start": "3486900",
    "end": "3492420"
  },
  {
    "text": "all right so if you have any questions we have one minute and 30 seconds",
    "start": "3492420",
    "end": "3497579"
  },
  {
    "text": "uh otherwise I'll be out in the hole and I'm happy to chat about data wherever you want and if not thank you very much",
    "start": "3497579",
    "end": "3504330"
  },
  {
    "text": "[Applause]",
    "start": "3504330",
    "end": "3510510"
  }
]