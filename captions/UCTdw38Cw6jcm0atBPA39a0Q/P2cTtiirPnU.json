[
  {
    "text": "good morning and uh welcome to the session I'm delivering on GP inside GPT",
    "start": "5440",
    "end": "12440"
  },
  {
    "text": "large language models uh demystified this is probably about the sixth seventh",
    "start": "12440",
    "end": "17480"
  },
  {
    "text": "maybe eighth time I'm delivering uh this uh this session I've been doing it for uh probably about nine months I'm also",
    "start": "17480",
    "end": "24000"
  },
  {
    "text": "still working on the session uh I was adding and uh modifying and and changing slides so this is kind of a working",
    "start": "24000",
    "end": "29679"
  },
  {
    "text": "program ress as I learn more about how uh how GPT uh works and what I'm going to be going through uh is some quite",
    "start": "29679",
    "end": "36040"
  },
  {
    "text": "advanced stuff uh looking at kind of how language uh models work and the also the maths that goes on inside uh language",
    "start": "36040",
    "end": "42920"
  },
  {
    "text": "models bit about myself my name is Alan Smith um I work as a I describe myself as a developer trainer mentor and",
    "start": "42920",
    "end": "49480"
  },
  {
    "text": "evangelist I love writing code so ever since I started my it career way back in",
    "start": "49480",
    "end": "55359"
  },
  {
    "text": "1995 I've stuck with um you know working with code in various uh various languages these days I'm pretty much uh",
    "start": "55359",
    "end": "63000"
  },
  {
    "text": "programming in uh python uh most of the time working uh with a lot of AI and and machine learning uh projects I also",
    "start": "63000",
    "end": "69640"
  },
  {
    "text": "deliver um training courses uh both kind of um you know public courses on-site courses for companies I've also recorded",
    "start": "69640",
    "end": "76640"
  },
  {
    "text": "uh some uh courses for plural site I work as a mentor I'm mentoring uh some other teams uh to kind of help with with",
    "start": "76640",
    "end": "83320"
  },
  {
    "text": "AI uh projects and I like to um look at new technologies and talk about those Technologies Microsoft used to call this",
    "start": "83320",
    "end": "89079"
  },
  {
    "text": "Ro as an evangelist list and for some reason I've still got it in my slide deck I work for a company called active solution were based in Stockholm Sweden",
    "start": "89079",
    "end": "96280"
  },
  {
    "text": "at yob uh also works for active solution he's presenting uh later uh today and",
    "start": "96280",
    "end": "101799"
  },
  {
    "text": "I've had the Microsoft most valuable professional Awards since uh 2005 when I got awarded for bis talk",
    "start": "101799",
    "end": "108640"
  },
  {
    "text": "server and I've been in bis talk server connected systems integration AO uh before I actually got put into the AI",
    "start": "108640",
    "end": "115280"
  },
  {
    "text": "group I do a lot quite a lot of work with the community um mostly these days uh working on a YouTube channel so one",
    "start": "115280",
    "end": "121920"
  },
  {
    "text": "of the other sessions I do is um GPT versus Starcraft where I'm using GPT 4",
    "start": "121920",
    "end": "127960"
  },
  {
    "text": "to come up with strategy uh for Starcraft games I'd recommend watching the YouTube video uh unfortunately the",
    "start": "127960",
    "end": "133800"
  },
  {
    "text": "demos don't work too well on that um when I I try to do them in uh in presentations because of the randomness",
    "start": "133800",
    "end": "139040"
  },
  {
    "text": "of the game but there's stuff on YouTube I've edited that so that that TS to work well so I'd like to thank um the other",
    "start": "139040",
    "end": "146239"
  },
  {
    "text": "um YouTube uh presenters who've really helped me to get a grip with how language uh models uh work I think the",
    "start": "146239",
    "end": "152560"
  },
  {
    "text": "biggest shout out has to go to three blue one brown um for the last two or three videos that are looking inside GPT",
    "start": "152560",
    "end": "160239"
  },
  {
    "text": "covering kind of the same stuff that I'm uh covering today but with with his style of animation and explaining how",
    "start": "160239",
    "end": "165640"
  },
  {
    "text": "the maths works also Andre capath has done some some great uh videos explaining how language models work",
    "start": "165640",
    "end": "172080"
  },
  {
    "text": "they're well well worth watching so uh we've all been using uh GPT for a long time um we go in we ask a",
    "start": "172080",
    "end": "178959"
  },
  {
    "text": "question and it re basically generates this this text we see the generate uh",
    "start": "178959",
    "end": "184519"
  },
  {
    "text": "text being generated word by word it's actually being generated token by token and we'll talk about how that mechanism",
    "start": "184519",
    "end": "190599"
  },
  {
    "text": "uh Works a bit later on so we can go in and we and say how do I make cheese and it comes out with an actal description",
    "start": "190599",
    "end": "196640"
  },
  {
    "text": "of how to make trees I love to know how things work so how does this work what actually makes this tick how can it come",
    "start": "196640",
    "end": "203000"
  },
  {
    "text": "up with all of these different uh answers to all of these these questions uh that we can we can ask the models",
    "start": "203000",
    "end": "209280"
  },
  {
    "text": "themselves if we look at what GPT does the only thing that the models will do is that when we give it a sequence of",
    "start": "209280",
    "end": "216120"
  },
  {
    "text": "tokens um or an input input of text the GPT model will predict the probabilities",
    "start": "216120",
    "end": "222319"
  },
  {
    "text": "of each token being the next token in the sequence we'll look later on at the",
    "start": "222319",
    "end": "227519"
  },
  {
    "text": "model vocabulary uh there's a token vocabulary which in the GPT models is about 50,000 tokens so if we feed in a",
    "start": "227519",
    "end": "235480"
  },
  {
    "text": "sequence of these tokens it will just give this probability distribution of what it thinks uh the next token will be",
    "start": "235480",
    "end": "240959"
  },
  {
    "text": "in that sequence so if we say the cat at on there and we feed it into a gpt2",
    "start": "240959",
    "end": "247200"
  },
  {
    "text": "which I'm running on this machine it's going to come come out with this uh kind of series of predictions with flaw being",
    "start": "247200",
    "end": "253640"
  },
  {
    "text": "the most uh probable token at 7.64% now the reason that we see um when",
    "start": "253640",
    "end": "259959"
  },
  {
    "text": "we're in GPT these this text coming out kind of token by token is what the model will do then is it will take the token",
    "start": "259959",
    "end": "266280"
  },
  {
    "text": "for floor it adds it on to the input it sends that sequence into the model and",
    "start": "266280",
    "end": "271479"
  },
  {
    "text": "then the next thing that comes out is comma being uh the most probable at 26.1 4% so we put comma on the end we feed",
    "start": "271479",
    "end": "278919"
  },
  {
    "text": "that token sequence back into the model and it comes out with and at uh 4.78% and then we stick uh that token on",
    "start": "278919",
    "end": "286639"
  },
  {
    "text": "the end so this is basically what the models uh are doing we'll see that that's not optimal and we'll see uh",
    "start": "286639",
    "end": "293039"
  },
  {
    "text": "later on that there's other things that we can do to to generate uh better and cleaner uh outputs so this um sequence",
    "start": "293039",
    "end": "300199"
  },
  {
    "text": "prediction problem uh is very challenging when we're working with texts because these sequences can be of",
    "start": "300199",
    "end": "305400"
  },
  {
    "text": "variable length we can say the cat saton or the king of rock and roll is Elvis or",
    "start": "305400",
    "end": "310520"
  },
  {
    "text": "we can generate code by putting in a comment load training data.txt into a list and display the top 10 lines and",
    "start": "310520",
    "end": "317360"
  },
  {
    "text": "the probable tokens after this comment is going to be the code that actually executes what is described in the common",
    "start": "317360",
    "end": "324280"
  },
  {
    "text": "so that's why um we're using co-pilots and it's able to generate uh code for us",
    "start": "324280",
    "end": "329400"
  },
  {
    "text": "uh but all it's doing is really just uh giving out the next tokens in the sequence the sequence order is very",
    "start": "329400",
    "end": "335759"
  },
  {
    "text": "important if we say the film was not bad in fact it was very good versus the film was not good in fact it was very bad all",
    "start": "335759",
    "end": "342080"
  },
  {
    "text": "we're doing is really swapping two tokens in that sequence uh but the actual sentiment of that text is uh is",
    "start": "342080",
    "end": "348720"
  },
  {
    "text": "changing uh dramatically it's giving it pretty much the opposite uh opposite uh sentiment so the order of these tokens",
    "start": "348720",
    "end": "355600"
  },
  {
    "text": "uh is going to be very important also we have to deal with long-term dependency the cat sat on the floor it had just",
    "start": "355600",
    "end": "361720"
  },
  {
    "text": "been cleaned so then it had been cleaned so it is going to refer to the floor",
    "start": "361720",
    "end": "366880"
  },
  {
    "text": "versus the cat set on the floor it had just been fed well it had been fed so it in this case is going to refer to the",
    "start": "366880",
    "end": "373479"
  },
  {
    "text": "cat because we feed cats and we we clean floors so it needs to understand these long-term dependencies and this is a",
    "start": "373479",
    "end": "379919"
  },
  {
    "text": "simple example but when we're dealing with much more complex examples such as the poem The Boy stood on the burning",
    "start": "379919",
    "end": "385360"
  },
  {
    "text": "deck the boy is mentioned in the first line of this poem and in the second column it says upon his brow he felt",
    "start": "385360",
    "end": "391280"
  },
  {
    "text": "their breath so his is going to refer to the boy the father is also mentioned in the poem but uh the um language models",
    "start": "391280",
    "end": "398400"
  },
  {
    "text": "are able to understand that when we say his in that um particular verse of the poem we're referring uh to the boy and",
    "start": "398400",
    "end": "406360"
  },
  {
    "text": "then we have the concept of of prompt engineering prompt engineering is what we actually feed in uh to the large",
    "start": "406360",
    "end": "412800"
  },
  {
    "text": "language model so it can answer the question how do we make cheese we're not just sending in how do we make cheese",
    "start": "412800",
    "end": "418080"
  },
  {
    "text": "when we're using gp2 there's a whole bunch of other stuff so typically we tell uh the language model what it is",
    "start": "418080",
    "end": "424199"
  },
  {
    "text": "you are a friendly and good nature chatbot we also tell it things that it shouldn't do uh to prevent the users",
    "start": "424199",
    "end": "429440"
  },
  {
    "text": "from doing things like jailbreaking or getting it to generate inappropriate responses the the actual prompts are a",
    "start": "429440",
    "end": "434680"
  },
  {
    "text": "lot longer than that but that's basically uh what it's what it's saying and then we actually feed in the user's question and after all of that the next",
    "start": "434680",
    "end": "441639"
  },
  {
    "text": "tokens in that sequence is hopefully are going to be the answer to that question we also um have something called",
    "start": "441639",
    "end": "447960"
  },
  {
    "text": "retrieval augmented gener ation or rag uh you will have heard of this I think Tess fenes was was talking about this",
    "start": "447960",
    "end": "454120"
  },
  {
    "text": "earlier on in the in the conference and the general thing is that you know the the models have a cut off in their their",
    "start": "454120",
    "end": "460039"
  },
  {
    "text": "training data so they're trained on data and then we have this cut off in in the training data and they don't know anything after the actual uh training",
    "start": "460039",
    "end": "467080"
  },
  {
    "text": "data cut off so what we need to do is to provide information if we ask it um for",
    "start": "467080",
    "end": "472440"
  },
  {
    "text": "example who won the Eurovision Song Contest in 2024 the actual model uh doesn't know that so by retrieval",
    "start": "472440",
    "end": "479000"
  },
  {
    "text": "augmented generation uh we can go out to another system uh could be a search engine could be something else and then",
    "start": "479000",
    "end": "484199"
  },
  {
    "text": "we can actually feed in uh information into the prompt and then that goes into the the language model and the model is",
    "start": "484199",
    "end": "490639"
  },
  {
    "text": "able to use that information uh to be able to answer that specific question and we can also point this at our",
    "start": "490639",
    "end": "496599"
  },
  {
    "text": "internal search engines uh so we can use our internal data and we can build our own uh chat Bots that are going to",
    "start": "496599",
    "end": "502120"
  },
  {
    "text": "answer questions on our own data that's kind of the basics of how those those systems work trat history is also",
    "start": "502120",
    "end": "509000"
  },
  {
    "text": "important the models do not preserve any state so when we're having a conversation with GPT we ask it what is",
    "start": "509000",
    "end": "515800"
  },
  {
    "text": "the capital of Norway and then the chatbot is going to predict the next sequence of tokens and then uh we can",
    "start": "515800",
    "end": "521240"
  },
  {
    "text": "say is it nice there now if this was just a stateless thing request response if we send in is it nice there it",
    "start": "521240",
    "end": "527680"
  },
  {
    "text": "wouldn't know what we're talking about but because of the chat history saying what is the capital of Norway the capital of Norway is Oslo when I say the",
    "start": "527680",
    "end": "534600"
  },
  {
    "text": "user says is it nice there it assumes that we're referring to Oslo and it can say yes Oslo is a nice city what can you",
    "start": "534600",
    "end": "540399"
  },
  {
    "text": "do in Oslo you can do lots of things in uh in Oslo so it's able to uh you you'll",
    "start": "540399",
    "end": "545880"
  },
  {
    "text": "see why that's relevant a bit later on it's it's part of part of the demo so I've actually built uh a simple uh chat",
    "start": "545880",
    "end": "551959"
  },
  {
    "text": "model I'm using chat gpt2 and the reason I'm using that uh is because uh firstly",
    "start": "551959",
    "end": "557720"
  },
  {
    "text": "it's open source which means I can go into the code I can put breakpoints on the code and we can see what's happening",
    "start": "557720",
    "end": "563920"
  },
  {
    "text": "inside the model that's really good uh the second thing gpt2 will run on a computer that I can actually carry",
    "start": "563920",
    "end": "570399"
  },
  {
    "text": "around to a conference uh which isn't true of the larger language models which",
    "start": "570399",
    "end": "575560"
  },
  {
    "text": "would require a cluster uh with lots of really really expensive uh GP use so uh",
    "start": "575560",
    "end": "581000"
  },
  {
    "text": "I've got gpt2 here um and i' I've done a sound check so hopefully this is going to work let's try running this uh",
    "start": "581000",
    "end": "587600"
  },
  {
    "text": "without debugging I haven't tested it with Oslo um so let's just see where",
    "start": "587600",
    "end": "592680"
  },
  {
    "text": "it's uh what it's going to do",
    "start": "592680",
    "end": "597279"
  },
  {
    "text": "I'm chat gpt2 a friendly chat bot how can I help you what is the capital of",
    "start": "601640",
    "end": "610240"
  },
  {
    "text": "Norway the capital of Norway is Oslo is it nice",
    "start": "612000",
    "end": "619279"
  },
  {
    "text": "there yes it is nice why is it so nice in Oslo",
    "start": "621320",
    "end": "629240"
  },
  {
    "text": "it is nice in Oslo what can you do in",
    "start": "631640",
    "end": "637639"
  },
  {
    "text": "Oslo you can do anything in Oslo are there lions in",
    "start": "640600",
    "end": "648480"
  },
  {
    "text": "Oslo yes there are lions in Oslo what do they do",
    "start": "651480",
    "end": "659279"
  },
  {
    "text": "they do everything in",
    "start": "662480",
    "end": "665360"
  },
  {
    "text": "Oslo that's really really quaint that we we kind of um see this language model and we laugh at it uh when this this",
    "start": "667600",
    "end": "673920"
  },
  {
    "text": "came out um all of the data scientists got really really excited uh because it could generate text it wasn't very good",
    "start": "673920",
    "end": "681320"
  },
  {
    "text": "text it was kind of like talking uh with with a four-year-old but it was actually generating text that was syntactically",
    "start": "681320",
    "end": "686440"
  },
  {
    "text": "correct which was a major uh major milestone now obviously this wasn't something that the public would be",
    "start": "686440",
    "end": "692200"
  },
  {
    "text": "interested in if we published it um as an actual service it wouldn't be able to kind of help us do uh do anything uh",
    "start": "692200",
    "end": "698880"
  },
  {
    "text": "useful maybe entertain a four-year-old but um not much apart from that so but",
    "start": "698880",
    "end": "704440"
  },
  {
    "text": "what the um data scientists understood was that if we make this model bigger and if we give it more training data",
    "start": "704440",
    "end": "710880"
  },
  {
    "text": "it's going to be much much better at generating text so they could see that the potential of a small model could",
    "start": "710880",
    "end": "716959"
  },
  {
    "text": "just be Amplified and and scaled up with with larger models to be able to generate um better questions and answers",
    "start": "716959",
    "end": "723480"
  },
  {
    "text": "and that that kind of trend is continuing kind of like a bit like a MOS law thing uh where you know stuff will",
    "start": "723480",
    "end": "729399"
  },
  {
    "text": "get more powerful over time uh and we've got these models if we throw more data at it and if we have more gpus and give",
    "start": "729399",
    "end": "735440"
  },
  {
    "text": "it more time and have more dollars and uh use more electricity uh we're going to generate uh better uh models that can",
    "start": "735440",
    "end": "742000"
  },
  {
    "text": "generate uh generate text uh better so another quick demo of How It's actually doing this this sequence uh prediction",
    "start": "742000",
    "end": "747600"
  },
  {
    "text": "so what I'm doing here is uh a jupyter notebook uh where I've actually got uh the code that is is running this so I'm",
    "start": "747600",
    "end": "754680"
  },
  {
    "text": "importing torch because this version of gpt2 is implemented in pytorch I'm then from the Transformers which is from",
    "start": "754680",
    "end": "761040"
  },
  {
    "text": "hugging space I'm importing gpt2 to gpt2 tokenizer and LM head model and I'm also",
    "start": "761040",
    "end": "767160"
  },
  {
    "text": "using um uh M plot lid py plot uh to draw some uh some graphs I'm not going to take the survey",
    "start": "767160",
    "end": "775399"
  },
  {
    "text": "now so that's going to import that stuff uh we can then gener at our tokenizer",
    "start": "775399",
    "end": "780440"
  },
  {
    "text": "and generate our model I'll talk about that a bit later on um because the tokenizer is separate uh from the model",
    "start": "780440",
    "end": "786480"
  },
  {
    "text": "I've got uh various bits of texts I can use as the actual input which I can Define so we've got these various texts",
    "start": "786480",
    "end": "792680"
  },
  {
    "text": "so what I'm going to do is I'm going to take text zero and that is the cat set on there and then what we're going to do",
    "start": "792680",
    "end": "798800"
  },
  {
    "text": "is to print that out we're then going to get the input IDs so I'm call calling tokenizer doin code and giving in the",
    "start": "798800",
    "end": "804839"
  },
  {
    "text": "actual text and that is going to return the actual IDs uh that we're going to be at sending into the model and then we do",
    "start": "804839",
    "end": "811320"
  },
  {
    "text": "this outputs is equal to model and we pass in the input IDs and then if we run",
    "start": "811320",
    "end": "818760"
  },
  {
    "text": "this what we're getting out is uh a tensor uh which is a 3D Matrix uh which",
    "start": "818760",
    "end": "824720"
  },
  {
    "text": "is of size 1X 5x 50257 now one is basically the number of",
    "start": "824720",
    "end": "831880"
  },
  {
    "text": "batches uh that we've got when we're training um llms we typically send in or when we're training any neural network",
    "start": "831880",
    "end": "838480"
  },
  {
    "text": "we typically send in a batch uh of of um items and that speeds up the training process so in inference we can also send",
    "start": "838480",
    "end": "845480"
  },
  {
    "text": "in batches but we usually don't do that we normally just send in one string of text five because there are five tokens",
    "start": "845480",
    "end": "851880"
  },
  {
    "text": "the catat on the and what the actual model is doing in its predictions is is",
    "start": "851880",
    "end": "856959"
  },
  {
    "text": "it is predicting the next probable word after the after cat after sat after on",
    "start": "856959",
    "end": "862959"
  },
  {
    "text": "and after the and the reason it does this uh is because in the training process it makes sense to send in uh",
    "start": "862959",
    "end": "869639"
  },
  {
    "text": "sequences where you send in 100 tokens and it's predicting the next token of all of those tokens and then we can help",
    "start": "869639",
    "end": "875560"
  },
  {
    "text": "it in the training process to learn what the actual correct answers are when we do the the back propagation so what we need to do is to",
    "start": "875560",
    "end": "882079"
  },
  {
    "text": "actually filter through these what I'm doing is taking the last item and then um I'll set the temperature to be one",
    "start": "882079",
    "end": "888560"
  },
  {
    "text": "actually temperature should be uh 1.0 and we'll talk talk about this later on and what I'm doing is just getting the",
    "start": "888560",
    "end": "893639"
  },
  {
    "text": "10 most probable tokens and you can see that there we've got floor bedc ground",
    "start": "893639",
    "end": "899360"
  },
  {
    "text": "uh Edge uh bench table sofa other and back and you can see the actual probability distribution uh that we",
    "start": "899360",
    "end": "905639"
  },
  {
    "text": "we've got there now we can do this with a few others um because sometimes we'll",
    "start": "905639",
    "end": "911079"
  },
  {
    "text": "we'll actually see how the probability distribution varies if I do a control F5 on this it's going to do this with with",
    "start": "911079",
    "end": "917519"
  },
  {
    "text": "a few sentences so uh there we can see that",
    "start": "917519",
    "end": "923440"
  },
  {
    "text": "we've got the cat set on there and we've got all of these these probabilities now this is like when you go to the fairground and you've one of these spin",
    "start": "923440",
    "end": "929720"
  },
  {
    "text": "the wheel things and you spin this wheel and there's a tiny little wedge uh that is the the top price assuming uh the",
    "start": "929720",
    "end": "935360"
  },
  {
    "text": "wheel is not weighted you've got kind of an equal chance of landing on kind of each each section on on that wheel so",
    "start": "935360",
    "end": "941480"
  },
  {
    "text": "it's kind of like one of these things um if we're using a temperature of 1.0 there's a 7.64% chance of it choosing",
    "start": "941480",
    "end": "948519"
  },
  {
    "text": "floor and 6.53 of it choosing bed and then all of the other um 50, 158 tokens",
    "start": "948519",
    "end": "956160"
  },
  {
    "text": "there's a 28% chance it's going to land somewhere here so every token it could say the cat sat on the cheese it could",
    "start": "956160",
    "end": "963360"
  },
  {
    "text": "say the cat sat on the and then something very very inappropriate because uh there's probably tokens for",
    "start": "963360",
    "end": "969360"
  },
  {
    "text": "those inappropriate words and there's a very small chance that it could depict that so we'll look at how we can use um",
    "start": "969360",
    "end": "975079"
  },
  {
    "text": "parameters in the models to prevent it from choosing these these very rare tokens there we've got Helsinki as the",
    "start": "975079",
    "end": "980800"
  },
  {
    "text": "capital of Finland at 40 uh 42% and um also it understands code so if you",
    "start": "980800",
    "end": "987759"
  },
  {
    "text": "program in net using system dot there's a bunch of stuff that you'll probably put after that if you're programming in",
    "start": "987759",
    "end": "993440"
  },
  {
    "text": "Python uh you typically import numpy as mp uh you can see that that's the most probable token coming after that and",
    "start": "993440",
    "end": "1000120"
  },
  {
    "text": "then the king of rock and roll is Elvis tokens are not words tokens can be um",
    "start": "1000120",
    "end": "1005880"
  },
  {
    "text": "parts of words and we'll see uh kind of what they are later on so 64% chance that it's going to be Elvis press",
    "start": "1005880",
    "end": "1011839"
  },
  {
    "text": "probably going to be Lee after that and then we've got cost so it they're probably thinking Elvis costell there",
    "start": "1011839",
    "end": "1018120"
  },
  {
    "text": "and then if we feed in uh the king of rock and roll is Elvis press you can see that that's really high probability that",
    "start": "1018120",
    "end": "1023720"
  },
  {
    "text": "Lee is going to be the token following following that and then if it's something more um you know where there",
    "start": "1023720",
    "end": "1030798"
  },
  {
    "text": "can be many answers like my favorite animal is the there's all kinds of of different ones here so the probable most",
    "start": "1030799",
    "end": "1036798"
  },
  {
    "text": "probable one is only 2.43% uh there",
    "start": "1036799",
    "end": "1042000"
  },
  {
    "text": "okay so let's drop back back to the slides and uh and talk about tokens so one of the things is that um we're",
    "start": "1042000",
    "end": "1048919"
  },
  {
    "text": "making text we're feeding it into a model and we're getting out an answer um but neural networks don't really do text",
    "start": "1048919",
    "end": "1056039"
  },
  {
    "text": "they they deal with it with numbers so what we've got to do is to we've got to convert that text into numbers and most",
    "start": "1056039",
    "end": "1061480"
  },
  {
    "text": "people who've been uh looking at working with language models they understand the concept of tokenization how we can take",
    "start": "1061480",
    "end": "1068760"
  },
  {
    "text": "text and we can convert that into numbers so I really like this bit of text this was on a website which is the",
    "start": "1068760",
    "end": "1076880"
  },
  {
    "text": "um open AI tokenizer I think I've got it open uh here and what we can do is we",
    "start": "1076880",
    "end": "1082280"
  },
  {
    "text": "can put some text in here they've actually changed the text on their website um but I really liked the one that they had there originally so we can",
    "start": "1082280",
    "end": "1088840"
  },
  {
    "text": "put that in we can see how many tokens how many characters we can see the actual token IDs and you can switch to",
    "start": "1088840",
    "end": "1095799"
  },
  {
    "text": "different models as well uh and see uh uh which uh which tokens you get out with different models so that's good for",
    "start": "1095799",
    "end": "1101280"
  },
  {
    "text": "kind of uh comparing and looking at how prompts how how many tokens a prompt will cost because we we we pay for",
    "start": "1101280",
    "end": "1107600"
  },
  {
    "text": "tokens when we're working with these these models there three really important things the tokens are common",
    "start": "1107600",
    "end": "1112960"
  },
  {
    "text": "sequences of characters found in text now the generation of the token tokenization vocabulary is different a",
    "start": "1112960",
    "end": "1120400"
  },
  {
    "text": "different process to the training of the model so what they do is they take the training data uh and they do some uh",
    "start": "1120400",
    "end": "1126360"
  },
  {
    "text": "statistics on it to figure out given uh a token vocabulary of say 50,000 uh",
    "start": "1126360",
    "end": "1132159"
  },
  {
    "text": "round about 50,000 um what are the most optimal sequences of text that I can use um to",
    "start": "1132159",
    "end": "1138840"
  },
  {
    "text": "represent uh this this training data and because the majority of the training data is in English um what that should",
    "start": "1138840",
    "end": "1145919"
  },
  {
    "text": "really say is there common sequences of characters found in mostly English",
    "start": "1145919",
    "end": "1150960"
  },
  {
    "text": "text so it it will be dependent on the languages we'll see later how that uh that to the how efficient the",
    "start": "1150960",
    "end": "1157440"
  },
  {
    "text": "tokenization is the models understand the statistical relationships between these tokens this is really important I",
    "start": "1157440",
    "end": "1165440"
  },
  {
    "text": "would argue uh that GPT models don't understand text the they are basically",
    "start": "1165440",
    "end": "1171360"
  },
  {
    "text": "good uh one thing that they're good at the third really important point is they're really really good at predicting",
    "start": "1171360",
    "end": "1177159"
  },
  {
    "text": "the next token in the sequence of tokens or to put it more precisely uh they excel at generating the probability",
    "start": "1177159",
    "end": "1184280"
  },
  {
    "text": "distribution of each token being the next token in that particular sequence",
    "start": "1184280",
    "end": "1189520"
  },
  {
    "text": "but I would argue that they don't understand uh text they understand the statistical relationship between tokens",
    "start": "1189520",
    "end": "1194919"
  },
  {
    "text": "I'm currently working on a hobby project where I'm taking a whole big database of MIDI files I'm getting the notes out of",
    "start": "1194919",
    "end": "1200760"
  },
  {
    "text": "the MIDI files I'm building a tokenization language based on notes and then uh actually feeding those into GPT",
    "start": "1200760",
    "end": "1207799"
  },
  {
    "text": "and in that case GPT just sees a SE sequence of tokens and it predicts the next token in the sequence and in that",
    "start": "1207799",
    "end": "1214200"
  },
  {
    "text": "case if I train the model on that it would be predicting the next note uh in in a piece of music uh but here it's",
    "start": "1214200",
    "end": "1219799"
  },
  {
    "text": "predicting the next um kind of sequence of characters given uh the previous sequences of characters so I I would",
    "start": "1219799",
    "end": "1225280"
  },
  {
    "text": "argue that the models don't actually understand text they never actually see the text we do the tokenization and we give it a",
    "start": "1225280",
    "end": "1231280"
  },
  {
    "text": "sequence of integers and it gives out uh percentage distribution with a sequence of integers it never actually sees uh",
    "start": "1231280",
    "end": "1237679"
  },
  {
    "text": "the text but they're really good at being able to predict um the next uh tokens so this is what that will look",
    "start": "1237679",
    "end": "1243880"
  },
  {
    "text": "like um when we uh actually tokenize it and this is what the actual uh integer",
    "start": "1243880",
    "end": "1249120"
  },
  {
    "text": "integers will look like when we get the token sequence and that's going to be 45 tokens you can see that the word the is",
    "start": "1249120",
    "end": "1256000"
  },
  {
    "text": "represented differently most of the these uh or pretty much all of these tokens are most of the tokens are words",
    "start": "1256000",
    "end": "1263760"
  },
  {
    "text": "and they're usually prefix by a space because it wouldn't make sense to have the space as an actual token because",
    "start": "1263760",
    "end": "1269760"
  },
  {
    "text": "that would double uh the number of tokens in that actual piece of text so",
    "start": "1269760",
    "end": "1274960"
  },
  {
    "text": "we've got the um it's 464 then we've got cap the with a capital T with a space",
    "start": "1274960",
    "end": "1280159"
  },
  {
    "text": "before it it's 363 and then the lower case uh that uh we've got as uh",
    "start": "1280159",
    "end": "1286159"
  },
  {
    "text": "2262 okay so what's the relevance uh of those numbers there they didn't get this",
    "start": "1286159",
    "end": "1292159"
  },
  {
    "text": "in Finland they got it in Amsterdam can you get it in",
    "start": "1292159",
    "end": "1296640"
  },
  {
    "text": "Oslo maybe I'm old it's it's the asy character set I learned to program on a Vic 20 uh and uh Commodore pets and",
    "start": "1297440",
    "end": "1304400"
  },
  {
    "text": "things like that and we were dealing with the asy uh character I remember dealing with it in Doss as well and um",
    "start": "1304400",
    "end": "1310480"
  },
  {
    "text": "characters 11 and 13 are full stop and comma so the first 256 tokens is basically the asy character set and then",
    "start": "1310480",
    "end": "1317400"
  },
  {
    "text": "it goes into these kinds of sequences of of tokens there if we do this in Swedish",
    "start": "1317400",
    "end": "1322760"
  },
  {
    "text": "it comes out as 83 tokens and you can see that because the tokenization vocabulary is um really based on the",
    "start": "1322760",
    "end": "1329960"
  },
  {
    "text": "English language that in Swedish we kind of have to partition the words more if we do it in Norwegian slightly more",
    "start": "1329960",
    "end": "1335520"
  },
  {
    "text": "efficient uh 71 uh tokens Finnish kind of of the European language is is the",
    "start": "1335520",
    "end": "1341720"
  },
  {
    "text": "worst uh with 103 tokens and then we go to tuugo um which comes out as 620 2 tokens",
    "start": "1341720",
    "end": "1349840"
  },
  {
    "text": "uh that's the same the same tax you can see GPT appearing also you can see well you probably can't see because they're",
    "start": "1349840",
    "end": "1355720"
  },
  {
    "text": "really really small uh that all of these are basically under 256 they're asky characters and it's using character",
    "start": "1355720",
    "end": "1361720"
  },
  {
    "text": "codes to describe uh the actual um letters that are used in the the tulugu",
    "start": "1361720",
    "end": "1366880"
  },
  {
    "text": "alphabet so this is um the token optimization and I'm probably going to",
    "start": "1366880",
    "end": "1372480"
  },
  {
    "text": "be on popular here but I'm going to say English is the new programming language I'm from England I speak English I speak",
    "start": "1372480",
    "end": "1378000"
  },
  {
    "text": "Swedish very badly so English is really the language that I uh work with and um",
    "start": "1378000",
    "end": "1383080"
  },
  {
    "text": "that's really uh a statement that I've been hearing a lot because when we do prompt engineering we're actually",
    "start": "1383080",
    "end": "1388679"
  },
  {
    "text": "telling the uh language model what it should do uh for years and years and years since 1995 I've been telling",
    "start": "1388679",
    "end": "1395440"
  },
  {
    "text": "computers what to do by writing um C or basic or C or python or what have you",
    "start": "1395440",
    "end": "1402080"
  },
  {
    "text": "but now um we're pretty much using boiler plate code and the thing that we're modifying that we have to get",
    "start": "1402080",
    "end": "1407360"
  },
  {
    "text": "right is the prompt that we're typically uh writing in English so that's showing the um amount of tokens um that we're",
    "start": "1407360",
    "end": "1415080"
  },
  {
    "text": "going to be using uh to represent these these various uh various sequences of text and this kind of correlates with",
    "start": "1415080",
    "end": "1422520"
  },
  {
    "text": "the language understanding um for gp4 it's not a direct correlation but you can see tuuga is down there at the",
    "start": "1422520",
    "end": "1428760"
  },
  {
    "text": "bottom and tuuga is the least efficient uh with um with tokenization so uh four reasons English",
    "start": "1428760",
    "end": "1435720"
  },
  {
    "text": "is a new programming languages the GPT and probably all of the large language models uh unless they're specifically",
    "start": "1435720",
    "end": "1441600"
  },
  {
    "text": "trained on a language are most accurate uh with the English uh language and that's because the majority of the",
    "start": "1441600",
    "end": "1447520"
  },
  {
    "text": "training data is in English and also the tokenization vocabulary is in English billing is based on the number of tokens",
    "start": "1447520",
    "end": "1454200"
  },
  {
    "text": "um so it's going to be more expensive using other languages throughput is based on the number of tokens per second",
    "start": "1454200",
    "end": "1459240"
  },
  {
    "text": "so it's going to be faster to process the same sentiment using English and models have a maximum number of input",
    "start": "1459240",
    "end": "1465000"
  },
  {
    "text": "tokens so we can get more information in the prompt we developers right if we can make something uh that's 25% faster 25%",
    "start": "1465000",
    "end": "1473039"
  },
  {
    "text": "cheaper 25% more accurate and can process 25% more of the information",
    "start": "1473039",
    "end": "1478200"
  },
  {
    "text": "we're going to do that so I would recommend um writing your prompts in English even if you're kind of inputs",
    "start": "1478200",
    "end": "1484600"
  },
  {
    "text": "and your outputs in other languages you can write an English prompt and say that you must generate the answers in Norwegian and it will do that um but we",
    "start": "1484600",
    "end": "1492000"
  },
  {
    "text": "can make sure that we're working with prompts in English just to demo this um I did ask it how to make cheese in in",
    "start": "1492000",
    "end": "1498480"
  },
  {
    "text": "lugu um I I think I did that's what I got out of Google translate anyway and",
    "start": "1498480",
    "end": "1503600"
  },
  {
    "text": "it it did this retrieval augmented generation it searched for information about making cheese inugo and it",
    "start": "1503600",
    "end": "1509159"
  },
  {
    "text": "generated some answers um saying that I could use it uh with a main ingredients",
    "start": "1509159",
    "end": "1514440"
  },
  {
    "text": "being cheese powder um which I've never heard of so that wasn't as you can see that that isn't as good a description as",
    "start": "1514440",
    "end": "1520679"
  },
  {
    "text": "it in English then it gave up with tulugu uh and it says you're interested in something more creative I can",
    "start": "1520679",
    "end": "1526360"
  },
  {
    "text": "generate a poem or a story uh let me know if you want to do that also here is the image of the Frozen movie characters",
    "start": "1526360",
    "end": "1532799"
  },
  {
    "text": "Anna Elsa Kristoff Olaf and swen in an ancient Autumn bound Forest um of an",
    "start": "1532799",
    "end": "1538039"
  },
  {
    "text": "enchanted land that you requested I hope you like it I think this there's going to be several million people uh in",
    "start": "1538039",
    "end": "1544640"
  },
  {
    "text": "Southern India um who are just going what's everybody going on about this GPT thing it's rubbish you know it can't",
    "start": "1544640",
    "end": "1550600"
  },
  {
    "text": "even tell me how to make cheese it's drawing pictures of Frozen characters so it would be nice to see better support",
    "start": "1550600",
    "end": "1556559"
  },
  {
    "text": "in in languages now there is the this thing uh that has been buil in Sweden GPT s uh it's called and this is trained",
    "start": "1556559",
    "end": "1563440"
  },
  {
    "text": "on not just Swedish but um Swedish Norwegian Danish Icelandic English and",
    "start": "1563440",
    "end": "1569159"
  },
  {
    "text": "programming code and we have talked to customers who are experimenting this but their conclusion is that gp4 is just",
    "start": "1569159",
    "end": "1576320"
  },
  {
    "text": "better than um a language that's been uh trained on its own um you know custom",
    "start": "1576320",
    "end": "1581760"
  },
  {
    "text": "model trained on the specific language we can see that gp4 in many many languages all of those write down to",
    "start": "1581760",
    "end": "1588320"
  },
  {
    "text": "Punjabi outperforms GPT 3.5 in English because it's a bigger model but English",
    "start": "1588320",
    "end": "1594360"
  },
  {
    "text": "is probably the worst possible Choice uh because we have heteronyms does anybody know what a heteronym",
    "start": "1594360",
    "end": "1601360"
  },
  {
    "text": "is it's a homograph that is not a homophone uh according to Wikipedia but",
    "start": "1601360",
    "end": "1607120"
  },
  {
    "text": "basically a ven diagram helps us to to understand it so some words because of the English language being weird um",
    "start": "1607120",
    "end": "1613840"
  },
  {
    "text": "you've got the wind was too strong for him to wh the kite she shed a tear when she saw the te in her dress now these",
    "start": "1613840",
    "end": "1620240"
  },
  {
    "text": "are the same tokens um uh but they're different words they pronounced differently sometimes and sometimes",
    "start": "1620240",
    "end": "1626760"
  },
  {
    "text": "they're pronounced the same and they're spelled differently so English is is really really weird like that and kind",
    "start": "1626760",
    "end": "1632080"
  },
  {
    "text": "of a demo of that I asked it to give me a well well-known s song lyric with the word wind as in Winder watch in it and",
    "start": "1632080",
    "end": "1639360"
  },
  {
    "text": "uh it came out with blowing blowing in the Wind by Bob Dylan and then I",
    "start": "1639360",
    "end": "1644840"
  },
  {
    "text": "corrected it and says no I wanted winders in Winder watch not blowing in the says I'm sorry I misunderstood you and",
    "start": "1644840",
    "end": "1650399"
  },
  {
    "text": "it came out with Against The Wind by Bob Seager and this is a really uh",
    "start": "1650399",
    "end": "1656240"
  },
  {
    "text": "interesting one it came out with a Chain by Fleetwood ma that said listen to the wind blow watch the sun rise so it's got",
    "start": "1656240",
    "end": "1662399"
  },
  {
    "text": "wind and watching it close to each other but they're different meaning of wind different meaning of watch so remember",
    "start": "1662399",
    "end": "1668799"
  },
  {
    "text": "when I was saying that they don't understand language they understand um the statistical relationships in tokens",
    "start": "1668799",
    "end": "1675320"
  },
  {
    "text": "and because wind and wind and watch and watch are the um the same tokens is just thinking uh that's that's what it should",
    "start": "1675320",
    "end": "1681640"
  },
  {
    "text": "do this is just an algorithm uh that's generating those so just to um iterate",
    "start": "1681640",
    "end": "1687159"
  },
  {
    "text": "the tokens quickly and show you what that tokenization looks like I've got a demo called iterate tokens uh if I run",
    "start": "1687159",
    "end": "1693960"
  },
  {
    "text": "this uh without debugging it's important that you listen when we run this one so let's start without",
    "start": "1693960",
    "end": "1700679"
  },
  {
    "text": "debugging did you hear that one of the asky characters is the Bell character I forget which one so",
    "start": "1703399",
    "end": "1710120"
  },
  {
    "text": "when I do console right line of that character it does the bell sound in in",
    "start": "1710120",
    "end": "1715399"
  },
  {
    "text": "Windows so yeah if we whiz up to the top here you can see we've got the asky character set and then we've kind of got",
    "start": "1715399",
    "end": "1721679"
  },
  {
    "text": "these um small kind of uh bits of words that are used to build things together and if",
    "start": "1721679",
    "end": "1727600"
  },
  {
    "text": "we scroll down we're going to get to um words and you can see that most of those",
    "start": "1727600",
    "end": "1733000"
  },
  {
    "text": "words are prefixed by a space and we've also got like uh list uh which can be",
    "start": "1733000",
    "end": "1739320"
  },
  {
    "text": "stuck on the end of words and olution that can be put on the end of words so it's kind of the the efficient ways of",
    "start": "1739320",
    "end": "1744960"
  },
  {
    "text": "being able to actually build up uh words so we understand about tokens uh",
    "start": "1744960",
    "end": "1752559"
  },
  {
    "text": "however neural networks don't do integers uh they work with floating Point numbers so we get into the concept",
    "start": "1752559",
    "end": "1758960"
  },
  {
    "text": "of embedding and vectorization which has uh kind of used to be a really Niche thing uh that only the um kind of",
    "start": "1758960",
    "end": "1765559"
  },
  {
    "text": "natural language programming uh Geeks talked about but now it's kind of mainstream with everybody's talking",
    "start": "1765559",
    "end": "1770600"
  },
  {
    "text": "about vectorization and things so cheese is basically this 768 uh floating Point",
    "start": "1770600",
    "end": "1776600"
  },
  {
    "text": "numbers in um GPT 2 in in the actual vectorization so each token has an act",
    "start": "1776600",
    "end": "1783080"
  },
  {
    "text": "Vector uh which conveys its sentiment now a lot of research went into this with with word Tove and you're supposed",
    "start": "1783080",
    "end": "1789760"
  },
  {
    "text": "to be able to do things like this King minus man plus woman is equal to Queen and you can do maths uh with with",
    "start": "1789760",
    "end": "1796279"
  },
  {
    "text": "numbers Paris uh minus France France plus Poland is equal to Warsaw I've really tried to get this to work and I",
    "start": "1796279",
    "end": "1802080"
  },
  {
    "text": "haven't uh if you watch the three blue one brown he has some good examples of that and he talks about why King minus",
    "start": "1802080",
    "end": "1808880"
  },
  {
    "text": "man plus woman is equal to Queen should work but it doesn't uh because queen as",
    "start": "1808880",
    "end": "1813919"
  },
  {
    "text": "well as being say the Queen of England or the queen of Sweden uh you've also got the rock band Queen headed by Freddy",
    "start": "1813919",
    "end": "1819240"
  },
  {
    "text": "Mercury which kind of drags the meaning of Queen in in a completely different different direction uh and so that",
    "start": "1819240",
    "end": "1825960"
  },
  {
    "text": "that's kind of why it doesn't work but you know King minus man plus woman is going to be something closer to Queen",
    "start": "1825960",
    "end": "1831799"
  },
  {
    "text": "check out um the uh three blue one brown stuff cuz it gives a really good uh explanation of that we can't understand",
    "start": "1831799",
    "end": "1837440"
  },
  {
    "text": "768 Dimensions uh but we can understand three dimensions and we're developers so we know about red blue and green colors",
    "start": "1837440",
    "end": "1844200"
  },
  {
    "text": "and the color Azor is 0127 comma 255 as developers would see it as data",
    "start": "1844200",
    "end": "1849760"
  },
  {
    "text": "scientists it would be 0 0.51 and we can project colors into uh a",
    "start": "1849760",
    "end": "1855320"
  },
  {
    "text": "3D space uh and see that colors are Sim are closer uh closer together and um we",
    "start": "1855320",
    "end": "1862120"
  },
  {
    "text": "can also see um that we can kind of use this to to figure out colors that are similar now there's a few methods of",
    "start": "1862120",
    "end": "1868320"
  },
  {
    "text": "doing this there's ukian and and cosine similarity that you may hear about just to cover that quickly if we look at all",
    "start": "1868320",
    "end": "1874440"
  },
  {
    "text": "of the colors that are along that line there along that Vector you basically get darker versions of that particular",
    "start": "1874440",
    "end": "1881039"
  },
  {
    "text": "color so you could argue that dark Azor is similar to Azor because it's at the same angle the vectors are at the same",
    "start": "1881039",
    "end": "1887240"
  },
  {
    "text": "angle there's along that along that line whereas cyan is uh is quite close uh in",
    "start": "1887240",
    "end": "1892559"
  },
  {
    "text": "the uh Udan in the actual distance between those two but it's a different shade and all of the colors along that",
    "start": "1892559",
    "end": "1898679"
  },
  {
    "text": "cion will be uh um similar so that's kind of coine similarity look looking at",
    "start": "1898679",
    "end": "1903880"
  },
  {
    "text": "the Angles and then you've got the Udan uh distance how many people know what that means that",
    "start": "1903880",
    "end": "1909880"
  },
  {
    "text": "image a few of you do uh a few of you play play ball games it's a ball game um but takes about 45 minutes to play and I",
    "start": "1909880",
    "end": "1916679"
  },
  {
    "text": "think the AG is about nine or or 10 or something like that and this is uh Star Wars rebellion that takes about twice as",
    "start": "1916679",
    "end": "1923440"
  },
  {
    "text": "long to play and the recommended age is about 16 but the angle is very very similar but here cosine similarities is",
    "start": "1923440",
    "end": "1931000"
  },
  {
    "text": "too good we think about using eludium Ticket to Ride is much closer uh to to kazone I have this slide in my de",
    "start": "1931000",
    "end": "1937960"
  },
  {
    "text": "because I I was given this presentation in Stockholm and a coworker said he was trying to use vectorization to uh find",
    "start": "1937960",
    "end": "1943760"
  },
  {
    "text": "similar board games and I thought that's interesting let's uh let's just do something on that just to illustrate how that that works so we can do",
    "start": "1943760",
    "end": "1951360"
  },
  {
    "text": "dimensionality reduction uh with word to VC I was doing this yesterday with um",
    "start": "1951360",
    "end": "1957240"
  },
  {
    "text": "with news articles as well and finding similar uh news articles uh but what we can do is we've got 10,000 words in here",
    "start": "1957240",
    "end": "1963880"
  },
  {
    "text": "Dark theme always looks looks cooler and what I can do is search for the month uh J uh",
    "start": "1963880",
    "end": "1971200"
  },
  {
    "text": "January if I can type it and we can find say the",
    "start": "1972799",
    "end": "1980039"
  },
  {
    "text": "12 or the 11 similar most similar words there and what we've got is the months and you can see that they're in this 3D",
    "start": "1980039",
    "end": "1987080"
  },
  {
    "text": "space they're in different places now if I run the um TSN what this is going to",
    "start": "1987080",
    "end": "1992639"
  },
  {
    "text": "do is uh find the nearest neighbors and then it's going to um basically do what we call dimensionality reduction and",
    "start": "1992639",
    "end": "1999519"
  },
  {
    "text": "move uh vectors um closer to each other uh that have similar sentiments so words",
    "start": "1999519",
    "end": "2004600"
  },
  {
    "text": "that mean the same thing should uh be uh close together in this this 3D space now",
    "start": "2004600",
    "end": "2010360"
  },
  {
    "text": "the same thing is done with tokens in GPT so we understand that the tokens that have good meanings like um happy",
    "start": "2010360",
    "end": "2017200"
  },
  {
    "text": "and good and great and fantastic uh are going to have a certain type of sentiment they're going to be fairly close together and tokens like bad and",
    "start": "2017200",
    "end": "2023919"
  },
  {
    "text": "evil and nasty um are going to be have a different sentiment and they're going to be uh in a different um area within this",
    "start": "2023919",
    "end": "2031000"
  },
  {
    "text": "space but it means that um you know I had a bad experience I had a nasty experience are going to have similar",
    "start": "2031000",
    "end": "2036200"
  },
  {
    "text": "sentiment because those are those vectors are the same so the vectorization is really a way of being able to do maths with with sums and I",
    "start": "2036200",
    "end": "2044600"
  },
  {
    "text": "think we can see that um those months are are grouping grouping together",
    "start": "2044600",
    "end": "2049919"
  },
  {
    "text": "within that that 3D space bit of a slow demo to run on this laptop but hopefully",
    "start": "2049919",
    "end": "2055118"
  },
  {
    "text": "uh you get the uh the picture so um the the way that we've",
    "start": "2055119",
    "end": "2062118"
  },
  {
    "text": "weol through these models we had something called uh recurrent neural networks or RNN we then went into l ltsm",
    "start": "2062119",
    "end": "2068480"
  },
  {
    "text": "long shortterm memory and into Transformers I'm not going to spend too much time on this uh but basically um",
    "start": "2068480",
    "end": "2074720"
  },
  {
    "text": "the RNN was okay at doing sequence prediction uh but because it was only",
    "start": "2074720",
    "end": "2079839"
  },
  {
    "text": "storing the actual State between each word it would often forget uh what had",
    "start": "2079839",
    "end": "2085240"
  },
  {
    "text": "happened earlier on in the the actual sequence um so the ltsm uh attempted to",
    "start": "2085240",
    "end": "2092240"
  },
  {
    "text": "fix that problem by having uh long-term memory as well as short-term memory and what it's trying to do is remember more",
    "start": "2092240",
    "end": "2098280"
  },
  {
    "text": "about the sequence and predict the next the next word but all of these were kind of sequential it had to do kind of uh",
    "start": "2098280",
    "end": "2104320"
  },
  {
    "text": "feeding in one word at a time whereas a Transformer allows us to send in a large number say you know 4,000 or 32,000",
    "start": "2104320",
    "end": "2112160"
  },
  {
    "text": "tokens depending on on the size of the model that we are using so this came from the the attention is all you need",
    "start": "2112160",
    "end": "2117560"
  },
  {
    "text": "white paper um and originally this uh white paper was written about doing",
    "start": "2117560",
    "end": "2123560"
  },
  {
    "text": "translation so we'd have the source language and we'd have the target language and what the um Source language",
    "start": "2123560",
    "end": "2129440"
  },
  {
    "text": "was going through something called an um encoder and and then the uh it was",
    "start": "2129440",
    "end": "2135400"
  },
  {
    "text": "actually using uh the the other part of that to actually uh use the target language to actually generate text so we",
    "start": "2135400",
    "end": "2140599"
  },
  {
    "text": "feed in something in English and it generates text in German so the idea uh of doing generation is just to say well",
    "start": "2140599",
    "end": "2147440"
  },
  {
    "text": "forget about doing the trans the actual Transformer bit let's just have it do the generation and generate uh text in a",
    "start": "2147440",
    "end": "2153640"
  },
  {
    "text": "language based on the actual previous token so they've kind of adapted the transl with the encoder and the decoder uh to",
    "start": "2153640",
    "end": "2160280"
  },
  {
    "text": "just use the actual decoder uh and actually use it as as an actual generation algorithm so we're going to",
    "start": "2160280",
    "end": "2165720"
  },
  {
    "text": "be focusing on this bit so the first bit is to actually do the embedding and the positional embedding because sentiment",
    "start": "2165720",
    "end": "2172640"
  },
  {
    "text": "is important and also position is important and that's done with uh through through through this process so",
    "start": "2172640",
    "end": "2178960"
  },
  {
    "text": "we start off with tokenization uh we send in the cat set on the we get these tokens uh and in this case each word is",
    "start": "2178960",
    "end": "2185839"
  },
  {
    "text": "we've got 464 for the and it's really space cat space c sat space on Space the",
    "start": "2185839",
    "end": "2192160"
  },
  {
    "text": "for those actual actual tokens are there and then we do the token embedding now within the model we have something",
    "start": "2192160",
    "end": "2199040"
  },
  {
    "text": "called an embeddings table and this is just basically a two-dimensional array which is 768 across because there's 768",
    "start": "2199040",
    "end": "2207480"
  },
  {
    "text": "dimensions in the vectors and it's 50,2 56 down because that's the actual",
    "start": "2207480",
    "end": "2213960"
  },
  {
    "text": "size of the vocabulary so each token um basically Al has a lookup uh that we can",
    "start": "2213960",
    "end": "2219119"
  },
  {
    "text": "perform in that table to get the embeddings for that token and we just stick them in an array which is going to",
    "start": "2219119",
    "end": "2224440"
  },
  {
    "text": "be 768 by five in this case because we've got five tokens and that gives us our embedding vectors the next thing we",
    "start": "2224440",
    "end": "2232079"
  },
  {
    "text": "need is a positional vectors now when we go back to the",
    "start": "2232079",
    "end": "2238319"
  },
  {
    "text": "diagram you can see that there's kind of a sine wave in positional embedding there's another lookup table um in the",
    "start": "2238319",
    "end": "2246000"
  },
  {
    "text": "model which is the positional embeddings and instead of the um vectors being",
    "start": "2246000",
    "end": "2251119"
  },
  {
    "text": "generated on the sentiment of the word the vectors are just um actual positions relative to the actual positions so we",
    "start": "2251119",
    "end": "2257680"
  },
  {
    "text": "do a lookup of 0 1 2 3 4 in the positional vectors and these vectors are generated uh by a bunch of sine waves",
    "start": "2257680",
    "end": "2265319"
  },
  {
    "text": "and cosine waves of different frequencies just to give an indication of where that actual word is in the",
    "start": "2265319",
    "end": "2270480"
  },
  {
    "text": "sequence so the model can understand that and we perform the look upon that we add those two",
    "start": "2270480",
    "end": "2276119"
  },
  {
    "text": "together and it gives us the encoder input values so we can look at how that",
    "start": "2276119",
    "end": "2282040"
  },
  {
    "text": "works if I go into the GPT fundamentals and I do F5 because I've got break",
    "start": "2282040",
    "end": "2288640"
  },
  {
    "text": "points on the code let's just check on the um the actual word that I'm using because I",
    "start": "2288640",
    "end": "2294920"
  },
  {
    "text": "want to make sure I'm using the right word or was it next token prediction I",
    "start": "2294920",
    "end": "2302040"
  },
  {
    "text": "think it was this one actually yeah I'll comment this out and I'll use the same as I was using in the demo the cat set",
    "start": "2302040",
    "end": "2308359"
  },
  {
    "text": "on there so this is the startup file and hit",
    "start": "2308359",
    "end": "2313359"
  },
  {
    "text": "F5 it takes a while uh to load the the model weights into memory uh but it should actually kick off um fairly",
    "start": "2316760",
    "end": "2323680"
  },
  {
    "text": "quickly okay we've hit the first break point and what I've done is I put some",
    "start": "2323680",
    "end": "2328720"
  },
  {
    "text": "watches on that so self is kind of like this in uh in C and it's telling us the",
    "start": "2328720",
    "end": "2333960"
  },
  {
    "text": "actual name of the class so in the gpt2 model now and that's a construct structure uh for the gpt2 model the init",
    "start": "2333960",
    "end": "2340079"
  },
  {
    "text": "method there and if I continue we'll enter some text if I just",
    "start": "2340079",
    "end": "2345720"
  },
  {
    "text": "press enter it's going to default to the cat set on there and now we're in the gp2 LM head model and if I continue",
    "start": "2345720",
    "end": "2353599"
  },
  {
    "text": "again here um we're actually doing the uh the actual uh token embedding so you can see that we've got the input IDs 464",
    "start": "2353599",
    "end": "2362040"
  },
  {
    "text": "uh 3797 so that's the C tber the actual integers for those and then we've got",
    "start": "2362040",
    "end": "2367079"
  },
  {
    "text": "the put embeddings that we're using self. wte which is the um the token embedding and if I step over that we've",
    "start": "2367079",
    "end": "2374680"
  },
  {
    "text": "then set this actual tensor and if you look at the input embedding size that's 1X 5x 768 768 Dimensions five tokens at",
    "start": "2374680",
    "end": "2383040"
  },
  {
    "text": "one batch and then we're doing the same thing with the position embeddings but instead of the token embedding we're",
    "start": "2383040",
    "end": "2388119"
  },
  {
    "text": "using the position embeddings and uh we step over that and you can see we've got the position our vectors there and then",
    "start": "2388119",
    "end": "2394760"
  },
  {
    "text": "we're doing hidden States is equal to input ined ings plus position embeddings so I can step over that and what we've",
    "start": "2394760",
    "end": "2401160"
  },
  {
    "text": "done there uh with that actual tensor maths is this uh operation that we've",
    "start": "2401160",
    "end": "2406280"
  },
  {
    "text": "got on this particular slide for each token we're getting the embeddings and we're adding them up to generate the",
    "start": "2406280",
    "end": "2411680"
  },
  {
    "text": "actual input embeddings next comes the actual multi-head tension uh and this is",
    "start": "2411680",
    "end": "2416720"
  },
  {
    "text": "when we start doing maths and basically what we're doing uh is something that gpts are very uh extremely good at which",
    "start": "2416720",
    "end": "2423520"
  },
  {
    "text": "is Matrix uh maths we've got this query we've got the key and we've got uh the",
    "start": "2423520",
    "end": "2428760"
  },
  {
    "text": "uh the value and these are generated from a convolution layer in the Network",
    "start": "2428760",
    "end": "2434480"
  },
  {
    "text": "that has been trained uh to uh basically generate floating Point numbers which is",
    "start": "2434480",
    "end": "2439680"
  },
  {
    "text": "kind of a information about the actual words when we send in the encoder input values now this is done uh 144 times in",
    "start": "2439680",
    "end": "2448560"
  },
  {
    "text": "gpt2 there's 144 different attention uh blocks in there 12 layers of of 12 12",
    "start": "2448560",
    "end": "2455040"
  },
  {
    "text": "heads and what that will uh basically give is 1x 5x 2304 array and that's then",
    "start": "2455040",
    "end": "2461359"
  },
  {
    "text": "split into the query the key and the value uh these actual three arrays now",
    "start": "2461359",
    "end": "2467359"
  },
  {
    "text": "because we've got 12 attention heads in each layer uh we're going to take those",
    "start": "2467359",
    "end": "2472920"
  },
  {
    "text": "uh each of those 1X 5 by 768 and we're going to split it split this 768",
    "start": "2472920",
    "end": "2478119"
  },
  {
    "text": "Dimension by 12 which is going to give us 1X 12x 5x 64 so what it's doing is",
    "start": "2478119",
    "end": "2484920"
  },
  {
    "text": "splitting the 768 into 12 arrays because there's 12 uh attention blocks and then",
    "start": "2484920",
    "end": "2490160"
  },
  {
    "text": "each of those attention blocks will get its own query key and the value for the actual 12",
    "start": "2490160",
    "end": "2495480"
  },
  {
    "text": "blocks so the query is a current token that the model is focusing on so if I'm",
    "start": "2495480",
    "end": "2500640"
  },
  {
    "text": "the query uh in this particular option we do this for every token in the room so if we're all kind of a tokens that",
    "start": "2500640",
    "end": "2507240"
  },
  {
    "text": "are in in the actual uh word I can be the query uh in this example the key is",
    "start": "2507240",
    "end": "2512800"
  },
  {
    "text": "going to be how everybody relates to me maybe you've met me um maybe you work with me uh maybe uh you know we were at",
    "start": "2512800",
    "end": "2519359"
  },
  {
    "text": "a different conference and uh and we sat and had launch together you may may have some connection with me and uh the value",
    "start": "2519359",
    "end": "2525520"
  },
  {
    "text": "is information about that particular uh connection so what we're doing is we're kind of figuring out how words are",
    "start": "2525520",
    "end": "2531400"
  },
  {
    "text": "connected with other words and then the maths that we do is just modifying uh",
    "start": "2531400",
    "end": "2536920"
  },
  {
    "text": "these actual embedding vectors uh to actually find uh relations or find ATT tension uh between uh between actual",
    "start": "2536920",
    "end": "2543720"
  },
  {
    "text": "words and find how words are connected and this is when they say attention is all you need uh it means that we don't",
    "start": "2543720",
    "end": "2549520"
  },
  {
    "text": "need ltsm we don't need to have the um recurrent neural networks we're just",
    "start": "2549520",
    "end": "2554720"
  },
  {
    "text": "going to purely focus on the attention mechanism for being able to actually figure out uh how to do sequence",
    "start": "2554720",
    "end": "2561319"
  },
  {
    "text": "predictions in words and that's why that white paper was really revolutionary there so what they're doing is they're",
    "start": "2561319",
    "end": "2566880"
  },
  {
    "text": "performing this operation um and they're coming out uh with the actual attention",
    "start": "2566880",
    "end": "2572119"
  },
  {
    "text": "uh vectors there now it's a bit challenging to understand um remember",
    "start": "2572119",
    "end": "2577440"
  },
  {
    "text": "when I said there's these 768 dimensions and when you're dealing with colors there's three dimensions and it's much",
    "start": "2577440",
    "end": "2582559"
  },
  {
    "text": "much easier but there is a tool called the um visualizer uh visualizer uh which",
    "start": "2582559",
    "end": "2589160"
  },
  {
    "text": "is basically for the Bert model uh which is the one before a GPT and what we can see here is these um 12 attention heads",
    "start": "2589160",
    "end": "2598599"
  },
  {
    "text": "and the 12 layers in the network and what it's showing is graphically um the attention weights that are put between",
    "start": "2598599",
    "end": "2605480"
  },
  {
    "text": "different different words so you can go in here I think if we go into the um Layer Two there's one of these layers",
    "start": "2605480",
    "end": "2611920"
  },
  {
    "text": "where it's focusing on the next um the next actual word in the sequence",
    "start": "2611920",
    "end": "2617520"
  },
  {
    "text": "I forget which one it is but",
    "start": "2617520",
    "end": "2621599"
  },
  {
    "text": "um no I can't find it yeah this one here sorry uh the first one so if I switch all of those it's layer two and then",
    "start": "2632240",
    "end": "2638640"
  },
  {
    "text": "attention head one is just focusing that um each word is uh basically got",
    "start": "2638640",
    "end": "2645440"
  },
  {
    "text": "attention on the next word in the sequence so they links to cat cat to sat and so on and all of these calculations",
    "start": "2645440",
    "end": "2652240"
  },
  {
    "text": "that are done across all of these these layers are able uh to actually make that uh that next uh prediction and we can",
    "start": "2652240",
    "end": "2658680"
  },
  {
    "text": "drop back to the code and see how that maths works if I uh click on F5 you can",
    "start": "2658680",
    "end": "2663960"
  },
  {
    "text": "see here uh that we've got the actual hidden states and this is doing is feeding it first through that",
    "start": "2663960",
    "end": "2669040"
  },
  {
    "text": "convolution layer and then it's actually doing the split uh on that so if I do an F10 on that you can see that we've got",
    "start": "2669040",
    "end": "2676119"
  },
  {
    "text": "the qu the query the key and the value and we've got the query shape uh being",
    "start": "2676119",
    "end": "2682200"
  },
  {
    "text": "bit hard to see though it's just 1X 5x 768 and the same for the key and the same for the value and then we call",
    "start": "2682200",
    "end": "2687960"
  },
  {
    "text": "split heads because self. num heads should be equal to 12 and what that is then going to do is is split uh as I",
    "start": "2687960",
    "end": "2694440"
  },
  {
    "text": "mentioned in the slides the 768 dimension by 12 and that's giving us the 1X 12x 5x",
    "start": "2694440",
    "end": "2701160"
  },
  {
    "text": "64 size um that we're using uh for each of those as it goes into the heads and",
    "start": "2701160",
    "end": "2706880"
  },
  {
    "text": "because there's 12 layers in the network as we actually go through this we'll see that this happens 12 times before we",
    "start": "2706880",
    "end": "2713440"
  },
  {
    "text": "actually get to the output and through each of these layers as you know with CNN's when we're",
    "start": "2713440",
    "end": "2719680"
  },
  {
    "text": "dealing with images it's getting more and more detailed until it actually gets uh to the output and is able to give us",
    "start": "2719680",
    "end": "2725960"
  },
  {
    "text": "uh the the actual prediction and then it comes out with a with a prediction there so that's basically what's going on",
    "start": "2725960",
    "end": "2731640"
  },
  {
    "text": "inside those uh those those those networks okay uh so now we're good uh we",
    "start": "2731640",
    "end": "2736839"
  },
  {
    "text": "understand how this works we understand how we can actually do um the next uh token uh prediction so what we should be",
    "start": "2736839",
    "end": "2743839"
  },
  {
    "text": "able to do is to just put that in action and what we can do now is just take some text and we can just stick on um the uh",
    "start": "2743839",
    "end": "2750319"
  },
  {
    "text": "the next um the the next token in that sequence and we're good to go uh so let's just",
    "start": "2750319",
    "end": "2756640"
  },
  {
    "text": "see how that works um so what I'm going to do is I'll use the um next token",
    "start": "2756640",
    "end": "2761880"
  },
  {
    "text": "prediction but I'll take something a bit more creative such as the king of rock",
    "start": "2761880",
    "end": "2767000"
  },
  {
    "text": "and roll is Elvis and what we'll do is just run it without the debugging so we're not hitting all of those break",
    "start": "2767000",
    "end": "2774680"
  },
  {
    "text": "points so I just press enter and it say the king of rock and roll is Elvis and you can see that pre is the next uh",
    "start": "2777319",
    "end": "2783680"
  },
  {
    "text": "probable token at 64% so we stick that on and then we've got Lee at uh 99.76%",
    "start": "2783680",
    "end": "2791520"
  },
  {
    "text": "taking the most probable token we're always using the most probable token so if you go to a party and you say the",
    "start": "2817000",
    "end": "2822160"
  },
  {
    "text": "most predictable thing all of the time all night it's going to get really boring nobody's nobody's going to talk",
    "start": "2822160",
    "end": "2828040"
  },
  {
    "text": "to you so what we've got to do is to have some variation or some stochasticity as as data scientists",
    "start": "2828040",
    "end": "2834000"
  },
  {
    "text": "would would have us say we got to kind of have some Randomness in there now remember when I showed the other demo",
    "start": "2834000",
    "end": "2839599"
  },
  {
    "text": "and we had that that pie chart I was saying you know this is like the wheel in a Fairground you spin the wheel and the pointer lands on one of those things",
    "start": "2839599",
    "end": "2846559"
  },
  {
    "text": "we need to do that we need to be put in a bit of randomization uh into that that actual process and we do this uh by",
    "start": "2846559",
    "end": "2853920"
  },
  {
    "text": "using temperature so I'm kind of diving away from the slides but I'm going to jump into um Visual Studio code again",
    "start": "2853920",
    "end": "2861000"
  },
  {
    "text": "because here uh I've got the templat set to 1.0 and what I'm doing is I'm getting",
    "start": "2861000",
    "end": "2866960"
  },
  {
    "text": "the next token uh logits and I'm dividing them by the temperature and then I'm performing a soft Max operation",
    "start": "2866960",
    "end": "2874680"
  },
  {
    "text": "on that now people who've uh played around with P torch and tensor flow will",
    "start": "2874680",
    "end": "2879960"
  },
  {
    "text": "understand what softmax does it takes uh a sequence of floating Point numbers uh",
    "start": "2879960",
    "end": "2885200"
  },
  {
    "text": "that um are going to kind of have arbit arbitary values and it kind of soft Maxes them so that they have uh a sum of",
    "start": "2885200",
    "end": "2893359"
  },
  {
    "text": "1.0 and that means that they're all kind of percentage distributions of um uh",
    "start": "2893359",
    "end": "2899839"
  },
  {
    "text": "that so it kind of converts a series of numbers into a temperature into an actual percentage distribution so if",
    "start": "2899839",
    "end": "2905800"
  },
  {
    "text": "people have got different amount of money you can do a soft Max on that and it gives this kind of percentage Distribution on that however it is",
    "start": "2905800",
    "end": "2912240"
  },
  {
    "text": "nonlinear uh because we like to have nonlinear things in in neural networks and I think the formula for softmax is",
    "start": "2912240",
    "end": "2919599"
  },
  {
    "text": "somewhere around here um and it uses uses exponentials so if",
    "start": "2919599",
    "end": "2925599"
  },
  {
    "text": "we um divide it by the temperature uh and the temperature is very low then",
    "start": "2925599",
    "end": "2931680"
  },
  {
    "text": "we're actually increasing uh increasing the values um and also if I use use 0.0",
    "start": "2931680",
    "end": "2938280"
  },
  {
    "text": "as the temperature which uh sometimes happens you can see that this is a divide by0 and bad things are going to",
    "start": "2938280",
    "end": "2944440"
  },
  {
    "text": "happen now let's just not do that let's just do 0.01 as a temperature and run",
    "start": "2944440",
    "end": "2950000"
  },
  {
    "text": "that and you can see that oh yeah maybe that's a bit too uh too low so I'll try",
    "start": "2950000",
    "end": "2956400"
  },
  {
    "text": "0.01 so you can see that when I've got a very low temperature floor is going to",
    "start": "2956400",
    "end": "2961680"
  },
  {
    "text": "be uh have 100% probability as being the next token so when we're using a low temp temperature in the language models",
    "start": "2961680",
    "end": "2967799"
  },
  {
    "text": "they are more predictable they're taking the most probable token however as I",
    "start": "2967799",
    "end": "2973440"
  },
  {
    "text": "increase the the temperature and go to 0 2 you can see that the probability distribution has changed with a",
    "start": "2973440",
    "end": "2980200"
  },
  {
    "text": "temperature of 0.02 there's kind of a 50% chance that it will choose floor and",
    "start": "2980200",
    "end": "2985599"
  },
  {
    "text": "then uh 23% for bed and so on 0.005 uh for back it can still actually make",
    "start": "2985599",
    "end": "2991720"
  },
  {
    "text": "those those actual uh those actual choices there um and then as we increase",
    "start": "2991720",
    "end": "2997160"
  },
  {
    "text": "the temperature soyo to 0.5 it's going to 24% floor and then I",
    "start": "2997160",
    "end": "3005119"
  },
  {
    "text": "mentioned um with a temperature of one we're just dividing by one so we make no change uh",
    "start": "3005119",
    "end": "3011480"
  },
  {
    "text": "to the the numbers that are coming out of the network and we get kind of that distribution it's also possible to go above one for the actual temperature and",
    "start": "3011480",
    "end": "3019520"
  },
  {
    "text": "that tends to make things very unpredictable so the popular Tok most popular token is probably",
    "start": "3019520",
    "end": "3024559"
  },
  {
    "text": "0.0055 and if you do this with um the llms with say gp4 it can just generate",
    "start": "3024559",
    "end": "3030359"
  },
  {
    "text": "absolute garbage uh if you go go uh call it with a temperature of two so we",
    "start": "3030359",
    "end": "3035440"
  },
  {
    "text": "typically uh aren't uh going toh do that so um what we can see here in this demo",
    "start": "3035440",
    "end": "3042119"
  },
  {
    "text": "is the effect of temperature on the model output so if I select um I think",
    "start": "3042119",
    "end": "3048960"
  },
  {
    "text": "it's this temperature Theory and uh and run this guy without debugging I apologize in advance if anything",
    "start": "3048960",
    "end": "3054680"
  },
  {
    "text": "inappropriate comes out of this this model I've been doing this presentation I think in gothenberg and the whole",
    "start": "3054680",
    "end": "3059760"
  },
  {
    "text": "audience was in hysterics and I had no idea why but uh where it's doing here is",
    "start": "3059760",
    "end": "3065200"
  },
  {
    "text": "you can see with a temperature of uh 0.0 uh it's always predicting the same",
    "start": "3065200",
    "end": "3070240"
  },
  {
    "text": "thing my favorite animal is a dog it's a very good dog it's a very good dog it's a very good dog and so on however uh if",
    "start": "3070240",
    "end": "3075680"
  },
  {
    "text": "we put in a temperature of 0.02 it will actually start generating more um uh more kind of a meaningful uh",
    "start": "3075680",
    "end": "3083720"
  },
  {
    "text": "meaningful text also bear in mind that this is gpt2 uh which is pretty rubbish",
    "start": "3083720",
    "end": "3089799"
  },
  {
    "text": "in in um terms of of what the actual language models can do so experimenting",
    "start": "3089799",
    "end": "3097720"
  },
  {
    "text": "with temperature and seeing what actual responses you're getting is is very important as well we'll see that as we",
    "start": "3097720",
    "end": "3103480"
  },
  {
    "text": "increase the temperature we're getting more uh and more creative stuff coming out of the actual uh the actual Network",
    "start": "3103480",
    "end": "3109720"
  },
  {
    "text": "there it is running a bit slow actually I think I remember why I was doing some debugging and saving some stuff so",
    "start": "3109720",
    "end": "3117119"
  },
  {
    "text": "let's just see if I can remember where I did that um",
    "start": "3117119",
    "end": "3124000"
  },
  {
    "text": "uh so if I search for outputs in the",
    "start": "3128760",
    "end": "3133960"
  },
  {
    "text": "solution yeah I probably can't find it I was actually writing code that was saving the outputs down into uh into a",
    "start": "3138839",
    "end": "3144760"
  },
  {
    "text": "Json file um outputs underscore might get",
    "start": "3144760",
    "end": "3152640"
  },
  {
    "text": "it and as always I I forgot to actually take out the actual debug code so",
    "start": "3154280",
    "end": "3161480"
  },
  {
    "text": "um now it's not finding it unfortunately but but basically as we uh change the",
    "start": "3161480",
    "end": "3166799"
  },
  {
    "text": "temperature modify the temperature uh it's going to affect uh what words are chosen now temperature isn't the only",
    "start": "3166799",
    "end": "3173720"
  },
  {
    "text": "parameter uh that we've we've uh we've got to uh work with uh we've got this other thing which is called top P now a",
    "start": "3173720",
    "end": "3180200"
  },
  {
    "text": "lot of people understand what temperature is uh not many people understand top what top p is and how it",
    "start": "3180200",
    "end": "3185640"
  },
  {
    "text": "relates to temperature and when we should use temperature and when we should use use top P but it's really an",
    "start": "3185640",
    "end": "3190839"
  },
  {
    "text": "alternative uh to a temperature which is called nuclear nucleus sampling so the",
    "start": "3190839",
    "end": "3196280"
  },
  {
    "text": "idea of top p is if I choose a top P of 0.01 then we're only going to be able to",
    "start": "3196280",
    "end": "3201480"
  },
  {
    "text": "choose floor and bed uh because it's going to take the uh kind of a top 10%",
    "start": "3201480",
    "end": "3207520"
  },
  {
    "text": "of those and that means it can can never be ground or couch or so on so by varing",
    "start": "3207520",
    "end": "3212960"
  },
  {
    "text": "top PE we can kind of think about how much of this range can be um uh actually",
    "start": "3212960",
    "end": "3218640"
  },
  {
    "text": "chosen however temperature doesn't actually cut everything off it just shifts the the probabilities of of",
    "start": "3218640",
    "end": "3224040"
  },
  {
    "text": "things so um I wanted to kind of illustrate how that works so I put together an animation uh which should be",
    "start": "3224040",
    "end": "3231520"
  },
  {
    "text": "in the directory of this solution so if I right click here and open in file",
    "start": "3231520",
    "end": "3240040"
  },
  {
    "text": "explorer uh it was this templat top P so what I've done here um is I've taken the",
    "start": "3240119",
    "end": "3247760"
  },
  {
    "text": "catat on there and I'm going between 1.0 and zero with temperature and top P so",
    "start": "3247760",
    "end": "3255400"
  },
  {
    "text": "you can see when they're both at 1.0 it's exactly the same probability distribution however as uh we start to",
    "start": "3255400",
    "end": "3262920"
  },
  {
    "text": "move down the probabilities you can see that uh it's different probability distributions using temperature and top",
    "start": "3262920",
    "end": "3269440"
  },
  {
    "text": "P now as we get round about here you'll see that when we're using top P these",
    "start": "3269440",
    "end": "3275440"
  },
  {
    "text": "actual things like other is just going to disappear and then suur is going to disappear and they can never be chosen",
    "start": "3275440",
    "end": "3282040"
  },
  {
    "text": "whereas with temperature they're still there but they're just very very small uh slices there and remember that I said",
    "start": "3282040",
    "end": "3289319"
  },
  {
    "text": "um that it could say the cat sat on the and it could say something really inappropriate because uh if we've got",
    "start": "3289319",
    "end": "3295079"
  },
  {
    "text": "the temperature um even if it's set to to a fairly low value the inappropriate tokens will still be there somewhere and",
    "start": "3295079",
    "end": "3301760"
  },
  {
    "text": "there's a very minute chance that it could land on that so when we go in the",
    "start": "3301760",
    "end": "3307040"
  },
  {
    "text": "portal one of the things that we noticed when we're um basically uh developing stuff with um the large language models",
    "start": "3307040",
    "end": "3314480"
  },
  {
    "text": "in the Azo portal is you can see that we've got the top P there set to 095 and that basically means that those",
    "start": "3314480",
    "end": "3322280"
  },
  {
    "text": "5% of tokens that are going to be maybe uh bad or maybe not useful or maybe they're going to generate confusing",
    "start": "3322280",
    "end": "3328440"
  },
  {
    "text": "outputs can never be chosen and then the temperature there they've set that with a default value of 7 now here the slider",
    "start": "3328440",
    "end": "3335599"
  },
  {
    "text": "only goes between zero and one when you're calling through the API you can go above one and depending on the model",
    "start": "3335599",
    "end": "3341880"
  },
  {
    "text": "I think with gp4 you can go up to two uh but when I've been testing this it does get really creative a bit above one but",
    "start": "3341880",
    "end": "3349280"
  },
  {
    "text": "occasionally on 1.5 and because this is random it's this spinning wheel at a fair ground it can just suddenly just generate garbage it picks a few tokens",
    "start": "3349280",
    "end": "3356680"
  },
  {
    "text": "and then it's just generating like kind of just a mangled array of uh really weird characters and uh brackets and so",
    "start": "3356680",
    "end": "3363799"
  },
  {
    "text": "on it just kind of looks like it like a corrupted file but be once it generates one of these weird tokens then there's",
    "start": "3363799",
    "end": "3369240"
  },
  {
    "text": "no way back the actual next token in the sequence is just going to be weird with gibberish so garbage in uh garbage out",
    "start": "3369240",
    "end": "3375480"
  },
  {
    "text": "as um somebody said years years and years ago a few other parameters that we can use um there is the n which can",
    "start": "3375480",
    "end": "3382160"
  },
  {
    "text": "specify how many trat completions uh we're going to be generating for each output message M and um that's basically",
    "start": "3382160",
    "end": "3389119"
  },
  {
    "text": "used when uh we look at using um the the actual beam search which I'll mention",
    "start": "3389119",
    "end": "3394359"
  },
  {
    "text": "very quickly at the end we can specify a stop sequence maximum number of tokens to generate and then we've got the",
    "start": "3394359",
    "end": "3399559"
  },
  {
    "text": "presence and frequency penalty so the frequency penalty um is relating to",
    "start": "3399559",
    "end": "3405319"
  },
  {
    "text": "should it generate repetitive stuff should it say he's a rockstar and he's a rockstar and he's a rockstar and he's a",
    "start": "3405319",
    "end": "3410760"
  },
  {
    "text": "rockar um if you're singing a song about Elvis maybe that's going to be the fade out um so for song lyrics um you know",
    "start": "3410760",
    "end": "3418079"
  },
  {
    "text": "and poems and kids stories you can have a lot of repetition of groups of words for scientific texts and white papers",
    "start": "3418079",
    "end": "3424480"
  },
  {
    "text": "and maybe product manuals you probably don't want to have um uh repetitions of words so you can customize how",
    "start": "3424480",
    "end": "3429760"
  },
  {
    "text": "repetitive uh it will be will be allowed to be lastly uh beam search um it doeses",
    "start": "3429760",
    "end": "3436359"
  },
  {
    "text": "the or the I kind of am a bit in two minds as whether this is the actual",
    "start": "3436359",
    "end": "3441839"
  },
  {
    "text": "model or whether this is the code around the model but what a beam search is going to do",
    "start": "3441839",
    "end": "3447000"
  },
  {
    "text": "is when it's looking at these next tokens it's going to think about an actual combinations of tokens so if we",
    "start": "3447000",
    "end": "3453039"
  },
  {
    "text": "choose Flor then the next three probable tokens are comma and and of and then if we choose comma we've got and his and",
    "start": "3453039",
    "end": "3460599"
  },
  {
    "text": "her and uh if we choose and we've got the looked and started and if we choose the floor comma and then we can choose",
    "start": "3460599",
    "end": "3467640"
  },
  {
    "text": "the he his head and and so on so what it's going to do uh when we do a beam search is we can actually go in and",
    "start": "3467640",
    "end": "3473839"
  },
  {
    "text": "start looking at groups and combinations of words so if we were asking it to you know say something about Elvis and it",
    "start": "3473839",
    "end": "3479880"
  },
  {
    "text": "says the king of rock and roll is Elvis putting a full stop there is kind of like the end of that sentence we we",
    "start": "3479880",
    "end": "3486079"
  },
  {
    "text": "would uh basically finish the sentence it would be more creative um to say the king of rock and roll is Elvis Presley",
    "start": "3486079",
    "end": "3491599"
  },
  {
    "text": "and he did this and he did that and that's why he's the king of rock and roll full stop so by using beams it's",
    "start": "3491599",
    "end": "3497200"
  },
  {
    "text": "able to look at being able to generate more interesting stuff and uh more um kind of a creative stuff as well and",
    "start": "3497200",
    "end": "3503920"
  },
  {
    "text": "that goes on with as the inside uh the model I think I did have a quick demo that I can maybe round off on looking at",
    "start": "3503920",
    "end": "3511039"
  },
  {
    "text": "the actual beam search and what I was doing was just calculating uh lots of different beams",
    "start": "3511039",
    "end": "3517079"
  },
  {
    "text": "and looking at the uh the actual combinations of those so I think it's this one that I can fire up without",
    "start": "3517079",
    "end": "3524680"
  },
  {
    "text": "debugging again takes a few seconds to load the model and this what this is doing is going through the various",
    "start": "3527920",
    "end": "3533440"
  },
  {
    "text": "combinations of uh different um uh things that we can we can do with that and then basically putting a score on",
    "start": "3533440",
    "end": "3538960"
  },
  {
    "text": "those combinations and then it can actually calculate uh how this this stuff uh works so uh just uh to to round",
    "start": "3538960",
    "end": "3546119"
  },
  {
    "text": "off hopefully this has been uh giv you a bit of an insight into what's going on in these uh large uh language models",
    "start": "3546119",
    "end": "3553240"
  },
  {
    "text": "takeaways um is going to be that um think about the tokenization think about",
    "start": "3553240",
    "end": "3559599"
  },
  {
    "text": "um writing your prompts think about using English uh for for prompts as well and also um you can do a lot with the",
    "start": "3559599",
    "end": "3566240"
  },
  {
    "text": "models through prompt engineering modifying the temperature and modifying top P setting top P to say uh 0.9 or 95",
    "start": "3566240",
    "end": "3574720"
  },
  {
    "text": "is going to uh filter out any um other um any kind of these real really weird",
    "start": "3574720",
    "end": "3579799"
  },
  {
    "text": "world tokens that's typically done in the portal but it not might not be done in your code and always test and",
    "start": "3579799",
    "end": "3586200"
  },
  {
    "text": "evaluate and think about what the actual um correct setting for temperature is going to be on those outputs if you do",
    "start": "3586200",
    "end": "3593240"
  },
  {
    "text": "want to play around uh with the source code uh you can downlo download uh I'm just using the Transformers stuff from",
    "start": "3593240",
    "end": "3598400"
  },
  {
    "text": "hugging space it's updated uh fairly regularly you can play around with that my name's Alan Smith thank you very much",
    "start": "3598400",
    "end": "3603880"
  },
  {
    "text": "for listening and enjoy the rest of the [Applause]",
    "start": "3603880",
    "end": "3611599"
  },
  {
    "text": "conference yeah sorry",
    "start": "3611599",
    "end": "3619039"
  }
]