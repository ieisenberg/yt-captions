[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "hello everyone my name is erin gillard and i'll be talking about the parallel programming valves in c plus",
    "start": "11040",
    "end": "17520"
  },
  {
    "text": "plus this being the very last uh session of tech town i appreciate the fact that",
    "start": "17520",
    "end": "25039"
  },
  {
    "text": "you're still around i know you're all in a hurry to get back home",
    "start": "25039",
    "end": "30880"
  },
  {
    "text": "so thanks for staying with me",
    "start": "30880",
    "end": "34718"
  },
  {
    "start": "37000",
    "end": "37000"
  },
  {
    "text": "so a little bit about me i work i started my my first",
    "start": "37520",
    "end": "44000"
  },
  {
    "text": "job in a small company called idea where i was the only steeplesource",
    "start": "44000",
    "end": "50399"
  },
  {
    "text": "developer or the only developer that used c plus plus",
    "start": "50399",
    "end": "56000"
  },
  {
    "text": "and during that time quite a few years i developed a developed taste for uh",
    "start": "56000",
    "end": "63680"
  },
  {
    "text": "community events mostly user groups meetings um which were",
    "start": "63680",
    "end": "70560"
  },
  {
    "text": "actually the only way for me uh to meet other sql sus developers",
    "start": "70560",
    "end": "78000"
  },
  {
    "text": "so after a few years i went back to grad school uh obtained a phd from the technion",
    "start": "78000",
    "end": "84320"
  },
  {
    "text": "and institute of technology and towards the end of my phd i",
    "start": "84320",
    "end": "90799"
  },
  {
    "text": "co-founded the high fossee plus plus meetup group which was the first sql plus meetup",
    "start": "90799",
    "end": "96880"
  },
  {
    "text": "group in israel after graduate graduating i started",
    "start": "96880",
    "end": "102560"
  },
  {
    "text": "working at yao research and around that time i joined the organizing committee of the",
    "start": "102560",
    "end": "109920"
  },
  {
    "text": "core simplest conference core c plus plus is the",
    "start": "109920",
    "end": "116159"
  },
  {
    "text": "other meetup group in israel",
    "start": "116159",
    "end": "121920"
  },
  {
    "start": "123000",
    "end": "123000"
  },
  {
    "text": "so why do we care about parallel programming",
    "start": "123600",
    "end": "128720"
  },
  {
    "text": "first and foremost it gives us performance okay we can utilize more compute resources",
    "start": "128720",
    "end": "137200"
  },
  {
    "text": "we can also use parallelism to hide latencies introduced by io operations",
    "start": "137200",
    "end": "144480"
  },
  {
    "text": "and we can keep our uis more responsive why is parallel programming hard well it",
    "start": "144480",
    "end": "152239"
  },
  {
    "text": "produces a non-deterministic execution so subsequent and executions",
    "start": "152239",
    "end": "158720"
  },
  {
    "text": "might yield different results and it also introduces some issues such as data",
    "start": "158720",
    "end": "166400"
  },
  {
    "text": "races and deadlocks and and therefore having a structure in our",
    "start": "166400",
    "end": "173599"
  },
  {
    "text": "parallel programs can make things much easier",
    "start": "173599",
    "end": "178640"
  },
  {
    "text": "so a programming model provides some holistic structure",
    "start": "178879",
    "end": "185920"
  },
  {
    "text": "into our programs and that structure allows us to use",
    "start": "185920",
    "end": "192480"
  },
  {
    "text": "some very useful abstraction those abstractions allow the user",
    "start": "192480",
    "end": "199840"
  },
  {
    "text": "to better reason about the correctness of the program on one hand and on the other the library",
    "start": "199840",
    "end": "207920"
  },
  {
    "text": "which has a better idea about what the user is doing or trying to achieve",
    "start": "207920",
    "end": "214640"
  },
  {
    "text": "can provide useful features at runtime since the",
    "start": "214640",
    "end": "222959"
  },
  {
    "text": "the library has a better idea about what we are trying to do",
    "start": "222959",
    "end": "228319"
  },
  {
    "text": "and trying to achieve it can better optimize the scheduling and the execution",
    "start": "228319",
    "end": "233680"
  },
  {
    "text": "of our program so both ends the compile time",
    "start": "233680",
    "end": "239840"
  },
  {
    "text": "correctness and the runtime performance are crucial in parallel programming",
    "start": "239840",
    "end": "246159"
  },
  {
    "start": "246000",
    "end": "246000"
  },
  {
    "text": "now because offers a plethora of parallel programming",
    "start": "246799",
    "end": "252840"
  },
  {
    "text": "facilities i've listed just a few here and there are many more this",
    "start": "252840",
    "end": "261040"
  },
  {
    "text": "is neither a random or arbitrary collection nor a single api for parallel",
    "start": "261040",
    "end": "268639"
  },
  {
    "text": "programming the facilities can be categorized into",
    "start": "268639",
    "end": "274479"
  },
  {
    "text": "three different parallel programming models which are introduced in this talk",
    "start": "274479",
    "end": "280720"
  },
  {
    "start": "281000",
    "end": "281000"
  },
  {
    "text": "the first programming model is what i call a structure which is a kind of a contradiction but",
    "start": "281360",
    "end": "288400"
  },
  {
    "text": "bear with me and the unstructured model means that you implement your own uh",
    "start": "288400",
    "end": "296080"
  },
  {
    "text": "parallelism mechanisms in your program um using the lowest level mechanisms",
    "start": "296080",
    "end": "302479"
  },
  {
    "text": "such as thread mutex and atomics and this model was introduced in sql source 11.",
    "start": "302479",
    "end": "310639"
  },
  {
    "text": "the second model is task based which uses async promises in future introduced also",
    "start": "310639",
    "end": "318400"
  },
  {
    "text": "in sql source 11. and lastly the data parallelism which was",
    "start": "318400",
    "end": "325120"
  },
  {
    "text": "relies on the parallel algorithms facilities that were introduced in c plus 17.",
    "start": "325120",
    "end": "332800"
  },
  {
    "text": "those different models are not different abstraction levels they are a completely different way of",
    "start": "333120",
    "end": "338880"
  },
  {
    "text": "structuring your parallel program",
    "start": "338880",
    "end": "342880"
  },
  {
    "start": "343000",
    "end": "343000"
  },
  {
    "text": "so the outline of this talk after this brief introduction is as",
    "start": "344240",
    "end": "350720"
  },
  {
    "text": "follows first i'll describe three different uh programming models then i'll compare",
    "start": "350720",
    "end": "358639"
  },
  {
    "text": "the three and finally i'll discuss mixing and different models",
    "start": "358639",
    "end": "365120"
  },
  {
    "text": "this talk is about forest not about the trees namely i'm not going to deep dive into a",
    "start": "365120",
    "end": "371360"
  },
  {
    "text": "lot of the details of the different apis but rather try to",
    "start": "371360",
    "end": "376880"
  },
  {
    "text": "describe the model itself and how it can be used during this talk feel free to interrupt",
    "start": "376880",
    "end": "383680"
  },
  {
    "text": "me through the slack channel or or the webex chat and i'll try to",
    "start": "383680",
    "end": "391440"
  },
  {
    "text": "answer as we go or we can [Music] have a q a at the end too",
    "start": "391440",
    "end": "399440"
  },
  {
    "text": "so first model is the unstructured parallelism model this means we're using in this model",
    "start": "400240",
    "end": "408639"
  },
  {
    "text": "this ad hoc use of the parallelism facilities",
    "start": "408639",
    "end": "413759"
  },
  {
    "text": "namely the structure is not determined by the language level api and facilities but",
    "start": "413759",
    "end": "422560"
  },
  {
    "text": "but rather by the particular use that you you find appropriate in your program",
    "start": "422560",
    "end": "429919"
  },
  {
    "text": "it relies on the lowest level abstraction and this means you have or you can",
    "start": "429919",
    "end": "436080"
  },
  {
    "text": "have at least a minimal overhead and maximal hardware utilization but on the other hand",
    "start": "436080",
    "end": "444080"
  },
  {
    "text": "it provides a minimal amount of safety so you really need to know what you're doing",
    "start": "444080",
    "end": "450160"
  },
  {
    "text": "otherwise you'll get into trouble and this model i said was introduced in",
    "start": "450160",
    "end": "456000"
  },
  {
    "text": "sql source 11 and provided some of the central features",
    "start": "456000",
    "end": "461199"
  },
  {
    "text": "of this language version so the major components",
    "start": "461199",
    "end": "469440"
  },
  {
    "start": "467000",
    "end": "467000"
  },
  {
    "text": "of this model are at the lowest level the memory model itself which means which includes atomics and threadlocal",
    "start": "469440",
    "end": "479039"
  },
  {
    "text": "above that we have the threading and synchronization classes such as spread music",
    "start": "479039",
    "end": "484879"
  },
  {
    "text": "collisional variables and the newly introduced in sql source 20 74 latch barrier and others",
    "start": "484879",
    "end": "492479"
  },
  {
    "text": "and above them we have convenience repeals such as the raii lock wrappers",
    "start": "492479",
    "end": "500000"
  },
  {
    "text": "call ones and the newly introduced j threads",
    "start": "500000",
    "end": "504639"
  },
  {
    "text": "and and this forms the basic facilities that we can use for this model",
    "start": "505120",
    "end": "512560"
  },
  {
    "text": "some use cases that are exclusive for this kind of program model",
    "start": "512800",
    "end": "520479"
  },
  {
    "text": "first is construct higher level facilities for instance we can create a thread loop",
    "start": "520479",
    "end": "526399"
  },
  {
    "text": "or a spin lock second use case is concur implementing",
    "start": "526399",
    "end": "532240"
  },
  {
    "text": "concurrent data structure either thread safe but log based",
    "start": "532240",
    "end": "538880"
  },
  {
    "text": "using mutexes or even better log-free data structures",
    "start": "538880",
    "end": "545600"
  },
  {
    "text": "using the low-level atomics and we can also use threads for some long-running services in the",
    "start": "545600",
    "end": "552240"
  },
  {
    "text": "background the missing part in this programming",
    "start": "552240",
    "end": "558399"
  },
  {
    "start": "555000",
    "end": "555000"
  },
  {
    "text": "model is a safe shared state so when we launch a few threads uh",
    "start": "558399",
    "end": "566000"
  },
  {
    "text": "the way for them to communicate is via some some sort of shared state um",
    "start": "566000",
    "end": "572560"
  },
  {
    "text": "either global or you know one that is pointed by various threats um",
    "start": "572560",
    "end": "580000"
  },
  {
    "text": "we can manage that state using atomics but that is that or that can be a bit",
    "start": "580000",
    "end": "586160"
  },
  {
    "text": "too fine-grained and hard to get right we can also use locks to",
    "start": "586160",
    "end": "593120"
  },
  {
    "text": "make existing standard containers thread safe or for this kind of locking",
    "start": "593120",
    "end": "599760"
  },
  {
    "text": "can be too coarse grained and and reduce the the amount of parallelism we can obtain",
    "start": "599760",
    "end": "605519"
  },
  {
    "text": "in our program so what when we need is concurrent data structures",
    "start": "605519",
    "end": "611279"
  },
  {
    "text": "and this is being currently considered uh nothing ingredient but hopefully we'll",
    "start": "611279",
    "end": "617680"
  },
  {
    "text": "get in the upcoming versions um some facilities such as uh threads fqs",
    "start": "617680",
    "end": "625040"
  },
  {
    "text": "hash maps uh rcu data structures and so on um finger crossed",
    "start": "625040",
    "end": "633360"
  },
  {
    "text": "so the unstructured parallelism has a few advantages and disadvantages",
    "start": "633760",
    "end": "640000"
  },
  {
    "text": "first of all we get maximal control because we are using the lowest level api",
    "start": "640000",
    "end": "646560"
  },
  {
    "text": "and we can possibly get the maximum performance if we manage to get things",
    "start": "646560",
    "end": "652880"
  },
  {
    "text": "right on the other hand it relies on a very complicated memory model",
    "start": "652880",
    "end": "658720"
  },
  {
    "text": "and getting things correct is indeed pretty hard and when you don't get",
    "start": "658720",
    "end": "664079"
  },
  {
    "text": "things right then you experience fun stuff like data races",
    "start": "664079",
    "end": "669680"
  },
  {
    "text": "deadlocks and non-deterministic results",
    "start": "669680",
    "end": "675040"
  },
  {
    "start": "674000",
    "end": "674000"
  },
  {
    "text": "so the thread level api itself has a few shortcomings",
    "start": "675040",
    "end": "680640"
  },
  {
    "text": "a thread in general is an abstraction of a processing okay the thread gets",
    "start": "680640",
    "end": "687600"
  },
  {
    "text": "when it is launched uh an execution entry point um much like when you start a program",
    "start": "687600",
    "end": "696160"
  },
  {
    "text": "then you get a main that starts your program and the thread itself does not have any",
    "start": "696160",
    "end": "702079"
  },
  {
    "text": "functional semantics further any synchronization among",
    "start": "702079",
    "end": "708240"
  },
  {
    "text": "different threads or communication and scheduling they are all interleaved in the code that you're running with",
    "start": "708240",
    "end": "715440"
  },
  {
    "text": "that thread okay so this is up to the programmer to manage and it is interleaved within the",
    "start": "715440",
    "end": "724160"
  },
  {
    "text": "the parallel thread logic",
    "start": "724160",
    "end": "729440"
  },
  {
    "text": "what would have been nice is if we can have some higher level abstraction and some clever",
    "start": "729600",
    "end": "735600"
  },
  {
    "text": "runtime that will allow us to do less work and obviously have less bugs and potentially",
    "start": "735600",
    "end": "743760"
  },
  {
    "text": "gets better performance since this has done better and this",
    "start": "743760",
    "end": "752320"
  },
  {
    "start": "751000",
    "end": "751000"
  },
  {
    "text": "kind of solution is provided by class-based parallel model so tasks",
    "start": "752320",
    "end": "759760"
  },
  {
    "text": "are computations that are usually limited in scope and provide a single result or none",
    "start": "759760",
    "end": "768240"
  },
  {
    "text": "they can be functions lambda expressions loop iterations and so on and ideally",
    "start": "768240",
    "end": "775120"
  },
  {
    "text": "have inputs and or output but no side effects namely a as little",
    "start": "775120",
    "end": "782639"
  },
  {
    "text": "interaction with global state and the good thing about tasks is that they",
    "start": "782639",
    "end": "787760"
  },
  {
    "text": "decouple functionality from execution so you specify the task itself and",
    "start": "787760",
    "end": "794800"
  },
  {
    "text": "some runtime executes it in an abstracted way",
    "start": "794800",
    "end": "801519"
  },
  {
    "text": "so one thread can be used to run many tasks on one hand and on the other hand one",
    "start": "801519",
    "end": "808399"
  },
  {
    "text": "task can sometimes migrate between multiple threads",
    "start": "808399",
    "end": "814160"
  },
  {
    "text": "and this is uh not something that the model exposes to the programmer",
    "start": "814160",
    "end": "821360"
  },
  {
    "start": "822000",
    "end": "822000"
  },
  {
    "text": "tasks are asynchronous namely the creator creates a task uh task it is",
    "start": "823040",
    "end": "830160"
  },
  {
    "text": "then executed somewhere sometime on some uh other parallel or some",
    "start": "830160",
    "end": "837279"
  },
  {
    "text": "other compute resource and the creator can proceed do other stuff until it",
    "start": "837279",
    "end": "844639"
  },
  {
    "text": "actually needs the result of that task at which point it can wait and get that get the result",
    "start": "844639",
    "end": "851199"
  },
  {
    "text": "and communication between the creator and the task is done by a channel that has two ends",
    "start": "851199",
    "end": "858639"
  },
  {
    "text": "the creator end of that channel is called the future which is basically a handle to the",
    "start": "858639",
    "end": "864399"
  },
  {
    "text": "outcome of the task if one exists and on the task and the the",
    "start": "864399",
    "end": "872480"
  },
  {
    "text": "the end of the channel is uh called a promise um and this is where the",
    "start": "872480",
    "end": "879680"
  },
  {
    "text": "the task sends the results to",
    "start": "879680",
    "end": "885600"
  },
  {
    "text": "so in c plus five tasks are launched using stood async okay",
    "start": "885600",
    "end": "893440"
  },
  {
    "text": "when you make a call to async it returns future and via that future you can obtain the",
    "start": "893839",
    "end": "900240"
  },
  {
    "text": "result once the task is done it's computation by calling",
    "start": "900240",
    "end": "908399"
  },
  {
    "text": "future git okay um the the runtime",
    "start": "908399",
    "end": "915040"
  },
  {
    "text": "can assign the tasks that you create through async to different worker threads um",
    "start": "915040",
    "end": "923120"
  },
  {
    "text": "and this is something that can be a bit surprising for folks coming across the async at the",
    "start": "923120",
    "end": "930000"
  },
  {
    "text": "first time on the first time um and the somewhat surprising thing about",
    "start": "930000",
    "end": "936160"
  },
  {
    "text": "it is that the runtime here the c plus runtime which has to be as efficient as pro as possible",
    "start": "936160",
    "end": "942560"
  },
  {
    "text": "and can actually create and manage thread pools without the user having",
    "start": "942560",
    "end": "949920"
  },
  {
    "text": "a lot of control about it okay so if you call async 1",
    "start": "949920",
    "end": "956560"
  },
  {
    "text": "you might have a thread running in the background even after it returned",
    "start": "956560",
    "end": "962480"
  },
  {
    "start": "963000",
    "end": "963000"
  },
  {
    "text": "now the actual execution of the task is determined by the first argument",
    "start": "963680",
    "end": "968800"
  },
  {
    "text": "provided to recall to async",
    "start": "968800",
    "end": "974000"
  },
  {
    "text": "and that can be and there you have a few options the first",
    "start": "974000",
    "end": "979360"
  },
  {
    "text": "one is a pass launch async which means the task is going to be",
    "start": "979360",
    "end": "986079"
  },
  {
    "text": "going to be executed on a new thread or better put as if on a new thread async",
    "start": "986079",
    "end": "994560"
  },
  {
    "text": "does not necessarily launch a new thread okay and one should not use async",
    "start": "994560",
    "end": "1001199"
  },
  {
    "text": "to try to launch a new thread or get a new thread because it can use an existing thread",
    "start": "1001199",
    "end": "1008480"
  },
  {
    "text": "that's from an internally managed thread pool if one is available so it merely means",
    "start": "1008480",
    "end": "1016399"
  },
  {
    "text": "um the task is going to get to be executed on a separate thread from the one that",
    "start": "1016399",
    "end": "1022160"
  },
  {
    "text": "is uh from the one that on which async was called",
    "start": "1022160",
    "end": "1028480"
  },
  {
    "text": "the second option is launch deferred which means that the task is going to be",
    "start": "1028480",
    "end": "1034640"
  },
  {
    "text": "executed right before the result is needed on the very same thread that caused async so here",
    "start": "1034640",
    "end": "1044400"
  },
  {
    "text": "the task is not going to be executed until you actually meet the result and no other thread is involved in the",
    "start": "1044400",
    "end": "1051679"
  },
  {
    "text": "execution if you do not call a future get then the task itself will not be executed",
    "start": "1051679",
    "end": "1058000"
  },
  {
    "text": "so this is just deferring the execution of the task",
    "start": "1058000",
    "end": "1063600"
  },
  {
    "text": "and lastly you can combine asic and and deferred which is actually the",
    "start": "1063600",
    "end": "1069280"
  },
  {
    "text": "default when no policy is specified and in this case",
    "start": "1069280",
    "end": "1074720"
  },
  {
    "text": "the behavior is implementation defense dependent the runtime can decide whether it's more",
    "start": "1074720",
    "end": "1080720"
  },
  {
    "text": "appropriate to run or execute the task on a separate thread or just defer the execution until the",
    "start": "1080720",
    "end": "1088240"
  },
  {
    "text": "result is needed so i said uh st the the results of a",
    "start": "1088240",
    "end": "1098559"
  },
  {
    "start": "1092000",
    "end": "1092000"
  },
  {
    "text": "task are uh delivered through a channel that has to end on the caller side we have the future",
    "start": "1098559",
    "end": "1107840"
  },
  {
    "text": "which provide access to the task result and it's basically the pool end and on",
    "start": "1107840",
    "end": "1114640"
  },
  {
    "text": "the on the task end we have a promise which is the push end",
    "start": "1114640",
    "end": "1120640"
  },
  {
    "text": "and importantly you do not see the promise itself when you create a task using async and",
    "start": "1120640",
    "end": "1127840"
  },
  {
    "text": "as you saw you specify say a lambda x version that",
    "start": "1127840",
    "end": "1133679"
  },
  {
    "text": "returns some value and async stood async",
    "start": "1133679",
    "end": "1140160"
  },
  {
    "text": "moves that returned value into the encapsulated promise so there is a",
    "start": "1140160",
    "end": "1147280"
  },
  {
    "text": "promise within the channel but it's not exposed to the user that uses async and",
    "start": "1147280",
    "end": "1155440"
  },
  {
    "text": "features if you do want to use tasks but you do",
    "start": "1155440",
    "end": "1162960"
  },
  {
    "start": "1158000",
    "end": "1158000"
  },
  {
    "text": "not want to use async itself you can use a class called package task which allows you to",
    "start": "1162960",
    "end": "1170880"
  },
  {
    "text": "wrap some executable using a task object",
    "start": "1170880",
    "end": "1178640"
  },
  {
    "text": "and from that task you can get the future which will again be the end of the",
    "start": "1178640",
    "end": "1186400"
  },
  {
    "text": "channel that delivers the result of the task and at this point we're still working on",
    "start": "1186400",
    "end": "1191919"
  },
  {
    "text": "the same thread same color thread then we can um create a new stud thread",
    "start": "1191919",
    "end": "1200320"
  },
  {
    "text": "and pass it the task instead of using asic",
    "start": "1200320",
    "end": "1205760"
  },
  {
    "text": "some useful work and then again call the future gets and obtain the result so",
    "start": "1205760",
    "end": "1212480"
  },
  {
    "text": "this piece of code here is similar to async but without the async",
    "start": "1212480",
    "end": "1218559"
  },
  {
    "text": "itself execution so if you're not happy with",
    "start": "1218559",
    "end": "1223600"
  },
  {
    "text": "what async provides say you want to",
    "start": "1223600",
    "end": "1228880"
  },
  {
    "text": "implement your own thread tool or so you can use a package task",
    "start": "1228880",
    "end": "1235919"
  },
  {
    "text": "to to obtain at least part of the functionality and",
    "start": "1235919",
    "end": "1242880"
  },
  {
    "start": "1244000",
    "end": "1244000"
  },
  {
    "text": "so as c plus plus provide a task-based programming model",
    "start": "1244559",
    "end": "1250480"
  },
  {
    "text": "if we consider some well-designed test model to be this beautiful horse then what",
    "start": "1250480",
    "end": "1257440"
  },
  {
    "text": "would the sequels plus test model look like i say like half a camel or so",
    "start": "1257440",
    "end": "1264799"
  },
  {
    "text": "it is missing a lot of things it misses a thread pool size or more generally",
    "start": "1264799",
    "end": "1272559"
  },
  {
    "text": "an execution context that the programmer can control it misses composition of tasks and",
    "start": "1272559",
    "end": "1279840"
  },
  {
    "text": "it misses cooperative scheduling and so on",
    "start": "1279840",
    "end": "1285120"
  },
  {
    "start": "1285000",
    "end": "1285000"
  },
  {
    "text": "now the parallelism ts1 does provide some extensions to futures",
    "start": "1285600",
    "end": "1293440"
  },
  {
    "text": "first it provides future.then which allows you to chain continuations",
    "start": "1293440",
    "end": "1300640"
  },
  {
    "text": "to a certain task so you create a task using async you get a future",
    "start": "1300640",
    "end": "1305919"
  },
  {
    "text": "and then you chain some other tasks using uh future.then and that task",
    "start": "1305919",
    "end": "1313440"
  },
  {
    "text": "gets executed uh once the future completes and the result is ready it also provides",
    "start": "1313440",
    "end": "1321039"
  },
  {
    "text": "other facilities such as when all when any which allows you to chain tasks to a set",
    "start": "1321039",
    "end": "1326799"
  },
  {
    "text": "of tasks unfortunately those experimental",
    "start": "1326799",
    "end": "1333120"
  },
  {
    "text": "features are kind of on hold due to the following reason we have",
    "start": "1333120",
    "end": "1340000"
  },
  {
    "start": "1339000",
    "end": "1339000"
  },
  {
    "text": "coverteen and executors on the way so covert teams introduced in sql software",
    "start": "1340000",
    "end": "1348240"
  },
  {
    "text": "are in short a mechanism that allows tab and allows functions to be",
    "start": "1348240",
    "end": "1354640"
  },
  {
    "text": "paused and resumed from the same location or with the same state in which they",
    "start": "1354640",
    "end": "1360320"
  },
  {
    "text": "were paused and once that those functions are closed and you can do other things",
    "start": "1360320",
    "end": "1368880"
  },
  {
    "text": "and and this is a great feature to have for a task-based model let's say",
    "start": "1368880",
    "end": "1375360"
  },
  {
    "text": "if some task is uh is being is executing and it's now being stalled waiting for",
    "start": "1375360",
    "end": "1382799"
  },
  {
    "text": "say some lengthy i o operation instead of",
    "start": "1382799",
    "end": "1388720"
  },
  {
    "text": "keeping uh the thread busy uh processor busy uh while it's stalled it can just",
    "start": "1388720",
    "end": "1396400"
  },
  {
    "text": "notify the scheduler i'm stuck let someone else execute and resume my",
    "start": "1396400",
    "end": "1404000"
  },
  {
    "text": "execution when when when i'm ready um and that can be done using the covertly",
    "start": "1404000",
    "end": "1411039"
  },
  {
    "text": "mechanisms so the task crop where cooperatively",
    "start": "1411039",
    "end": "1418720"
  },
  {
    "text": "and gives up the execution context",
    "start": "1418720",
    "end": "1424000"
  },
  {
    "text": "moves aside let the scheduler execute some other task and when it's ready it",
    "start": "1424000",
    "end": "1429039"
  },
  {
    "text": "is resumed from the exact same state which is something core routines provide",
    "start": "1429039",
    "end": "1436799"
  },
  {
    "text": "so core teams provide one nice building blocks for tasks executors",
    "start": "1436799",
    "end": "1443360"
  },
  {
    "text": "which have not been approved yet and hopefully be added to the language in peoplesoft 23",
    "start": "1443360",
    "end": "1451360"
  },
  {
    "text": "and provide a way to define execution context so the",
    "start": "1451360",
    "end": "1458559"
  },
  {
    "text": "immediate example is some um some thread tool okay you as a user you",
    "start": "1458559",
    "end": "1465760"
  },
  {
    "text": "can say i have eight cores but i only want to the the mall the",
    "start": "1465760",
    "end": "1473360"
  },
  {
    "text": "task-based model to use four of them then you create a thread pool with four",
    "start": "1473360",
    "end": "1479120"
  },
  {
    "text": "and only four cores and and let the",
    "start": "1479120",
    "end": "1484960"
  },
  {
    "text": "i think use that threadboom okay so um",
    "start": "1484960",
    "end": "1490799"
  },
  {
    "text": "for a well-designed test-based model and since",
    "start": "1493919",
    "end": "1501360"
  },
  {
    "text": "both those features are either newly introduced or not yet introduced there's no great",
    "start": "1501360",
    "end": "1507600"
  },
  {
    "text": "point in extending async at this point of time",
    "start": "1507600",
    "end": "1512720"
  },
  {
    "text": "until other uh the other mechanisms are available",
    "start": "1512720",
    "end": "1520799"
  },
  {
    "start": "1521000",
    "end": "1521000"
  },
  {
    "text": "so uh to wrap this up uh let's compare phrase versus",
    "start": "1521279",
    "end": "1527440"
  },
  {
    "text": "tasks in general so first threads the use cases that can be",
    "start": "1527440",
    "end": "1534880"
  },
  {
    "text": "appropriate for threads are background services",
    "start": "1534880",
    "end": "1540400"
  },
  {
    "text": "long and complex parallel operations um and when you have an equal amount of",
    "start": "1540400",
    "end": "1547679"
  },
  {
    "text": "pair thread work okay because you want if you're using multiple threads for some",
    "start": "1547679",
    "end": "1555760"
  },
  {
    "text": "and you want them to complete their work roughly at the same time so you get decent load balancing",
    "start": "1555760",
    "end": "1564080"
  },
  {
    "text": "on the other hand you want to use tasks when you have many short and independent operations not",
    "start": "1564080",
    "end": "1571200"
  },
  {
    "text": "necessarily doing the same thing um when your those computations",
    "start": "1571200",
    "end": "1577039"
  },
  {
    "text": "are defined pretty much locally using say lambda expressions",
    "start": "1577039",
    "end": "1583760"
  },
  {
    "text": "and that provides dynamic load balancing so you can you can create a lot of them",
    "start": "1583760",
    "end": "1589679"
  },
  {
    "text": "and you can trust the uh the task scheduler to make sure that your",
    "start": "1589679",
    "end": "1595360"
  },
  {
    "text": "resources are utilized in a sensible way",
    "start": "1595360",
    "end": "1601200"
  },
  {
    "text": "as for stood threads and through the sink so studentsync provides",
    "start": "1602080",
    "end": "1609840"
  },
  {
    "text": "better resource utilization when the when the task scheduler",
    "start": "1609840",
    "end": "1618400"
  },
  {
    "text": "is properly used it provides a clean return value mechanism through",
    "start": "1618400",
    "end": "1624840"
  },
  {
    "text": "futures it provides exceptional safety okay if a task throws then that",
    "start": "1624840",
    "end": "1631120"
  },
  {
    "text": "exception is propagated to the future um it provides time waiting you can",
    "start": "1631120",
    "end": "1638640"
  },
  {
    "text": "wait on a future with a timeout unlike the split joints and it provides",
    "start": "1638640",
    "end": "1647520"
  },
  {
    "text": "multiple accesses to the future unlike join which we can you can't",
    "start": "1647520",
    "end": "1653039"
  },
  {
    "text": "recall multiple times threads on the other hand mostly provided deterministic execution",
    "start": "1653039",
    "end": "1660159"
  },
  {
    "text": "context which is a separate thread",
    "start": "1660159",
    "end": "1664480"
  },
  {
    "start": "1665000",
    "end": "1665000"
  },
  {
    "text": "okay so moving on to the third programming model which is data",
    "start": "1665520",
    "end": "1671200"
  },
  {
    "text": "parallelism so in this programming model",
    "start": "1671200",
    "end": "1676480"
  },
  {
    "text": "parallelism stems from the individual computation associated with every element in a collection okay",
    "start": "1676480",
    "end": "1684960"
  },
  {
    "text": "this means you have a collection or in general a lot of data that is",
    "start": "1684960",
    "end": "1692080"
  },
  {
    "text": "mostly non-dependent um and the algorithm can divide the work among different",
    "start": "1692080",
    "end": "1699360"
  },
  {
    "text": "processors and execute it this processing environment",
    "start": "1699360",
    "end": "1706880"
  },
  {
    "start": "1708000",
    "end": "1708000"
  },
  {
    "text": "the general structure of parallel algorithm includes the following components",
    "start": "1709360",
    "end": "1716480"
  },
  {
    "text": "first of all the algorithm itself is called from a sequential execution context okay you do not need to launch any",
    "start": "1716480",
    "end": "1724720"
  },
  {
    "text": "parallel threads or something",
    "start": "1724720",
    "end": "1731120"
  },
  {
    "text": "then you define the execution policy in a declarative way you just say the",
    "start": "1731520",
    "end": "1737760"
  },
  {
    "text": "desired policy and the algorithm itself takes care of the rest",
    "start": "1737760",
    "end": "1744240"
  },
  {
    "text": "and you provide the container with processing you'd like to accelerate by",
    "start": "1745520",
    "end": "1750720"
  },
  {
    "text": "doing it in parallel the parallel algorithms are a high level",
    "start": "1750720",
    "end": "1757200"
  },
  {
    "text": "abstraction okay so you have a few customization points that you can use you have",
    "start": "1757200",
    "end": "1765440"
  },
  {
    "text": "no control over the amount of parallelism the scheduling of different threads",
    "start": "1765440",
    "end": "1773120"
  },
  {
    "text": "and how work is being distributed on the other hand giving those",
    "start": "1773120",
    "end": "1778960"
  },
  {
    "text": "restrictions and abstractions the library and the runtime can be very efficient",
    "start": "1778960",
    "end": "1784240"
  },
  {
    "text": "um and when you use it properly you're free from data races deadlocks and all",
    "start": "1784240",
    "end": "1789679"
  },
  {
    "text": "those bad issues parallel algorithms were introduced in c",
    "start": "1789679",
    "end": "1796640"
  },
  {
    "text": "plus 17. their adoption has been slow in terms of component of support",
    "start": "1796640",
    "end": "1803840"
  },
  {
    "text": "they're currently only supported in gcc 9 and visual studio studio 2017",
    "start": "1803840",
    "end": "1812320"
  },
  {
    "text": "later editions and also available as library implementations",
    "start": "1812320",
    "end": "1818720"
  },
  {
    "text": "provided by intel or a coldplay by their sql library",
    "start": "1818720",
    "end": "1826559"
  },
  {
    "start": "1827000",
    "end": "1827000"
  },
  {
    "text": "so how do we use parallel algorithms um so first most",
    "start": "1827440",
    "end": "1833760"
  },
  {
    "text": "standard algorithms have a parallel overload and that's specified in the standard and",
    "start": "1833760",
    "end": "1840960"
  },
  {
    "text": "the first argument that you provide to a parallel algorithm is the execution",
    "start": "1840960",
    "end": "1847279"
  },
  {
    "text": "policy okay which i'll discuss in a minute",
    "start": "1847279",
    "end": "1852640"
  },
  {
    "text": "um and another difference between the regular algorithms and the parallel ones",
    "start": "1852640",
    "end": "1860000"
  },
  {
    "text": "is the type of the iterators that you provide okay so in the sequential algorithms you",
    "start": "1860000",
    "end": "1867519"
  },
  {
    "text": "provide inputs and output iterators while in the parallel variants you provide forward",
    "start": "1867519",
    "end": "1873200"
  },
  {
    "text": "iterators which differ in in being able to recall them or",
    "start": "1873200",
    "end": "1881200"
  },
  {
    "text": "store a forward iterator and re-access it even if it has moved",
    "start": "1881200",
    "end": "1888960"
  },
  {
    "text": "on which is important for distributed data among different threads",
    "start": "1888960",
    "end": "1897200"
  },
  {
    "text": "this is a very important because if you're trying to move from a sequential",
    "start": "1898320",
    "end": "1905760"
  },
  {
    "text": "algorithm to a parallel one and you just add an execution policy",
    "start": "1905760",
    "end": "1910960"
  },
  {
    "text": "then your code might not compile because of the type of iterators this is not",
    "start": "1910960",
    "end": "1918399"
  },
  {
    "text": "you know um this is not by accident it's it's something",
    "start": "1918399",
    "end": "1923440"
  },
  {
    "text": "something sometimes something that you have to really change your program for",
    "start": "1923440",
    "end": "1929919"
  },
  {
    "text": "as for the complexity requirements then they are a bit more relaxed on parallel algorithms because parallel",
    "start": "1930320",
    "end": "1937760"
  },
  {
    "text": "implementations might have to do more work for the parallel",
    "start": "1937760",
    "end": "1944640"
  },
  {
    "text": "processing however this a little bit of extra work allows",
    "start": "1944640",
    "end": "1951200"
  },
  {
    "text": "them to process the data in parallel so the overall",
    "start": "1951200",
    "end": "1956320"
  },
  {
    "text": "performance is expected to be better",
    "start": "1956320",
    "end": "1960720"
  },
  {
    "text": "the actual implementation of the parallel algorithm as the sequential algorithm",
    "start": "1961600",
    "end": "1969039"
  },
  {
    "text": "is not defined in this standard so in theory you can use different kind of",
    "start": "1969039",
    "end": "1974320"
  },
  {
    "text": "algorithms and you can use or sorry the",
    "start": "1974320",
    "end": "1979360"
  },
  {
    "text": "the the platform can use different processing resources such as a gpu if it finds it appropriate",
    "start": "1979440",
    "end": "1988000"
  },
  {
    "start": "1988000",
    "end": "1988000"
  },
  {
    "text": "so the first uh parallel processing process policy is called sequence policy",
    "start": "1988559",
    "end": "1995600"
  },
  {
    "text": "and what it means is that the parallel algorithm is actually going to be sequentially executed on the calling",
    "start": "1995600",
    "end": "2002720"
  },
  {
    "text": "thread okay and so using the sequence policy",
    "start": "2002720",
    "end": "2008080"
  },
  {
    "text": "differs from the uh non-parallel algorithms in two ways first exceptions",
    "start": "2008080",
    "end": "2016000"
  },
  {
    "text": "invoke should terminate",
    "start": "2016000",
    "end": "2020559"
  },
  {
    "text": "this is the case with any parallel algorithm and and even if it",
    "start": "2021440",
    "end": "2028240"
  },
  {
    "text": "is executed sequentially and the second difference is again the use of forward iterators",
    "start": "2028240",
    "end": "2036720"
  },
  {
    "text": "now to specify a policy you provide the algorithm with an object",
    "start": "2036720",
    "end": "2043360"
  },
  {
    "text": "which is an instance of the policy type so the instance of sequence policy is",
    "start": "2043360",
    "end": "2049760"
  },
  {
    "text": "called execution sick um and the idea is that",
    "start": "2049760",
    "end": "2057358"
  },
  {
    "text": "each object like that is has has a different type and this allows algorithms to be",
    "start": "2057359",
    "end": "2063520"
  },
  {
    "text": "overloaded according to the different policies they implement okay and this means that the",
    "start": "2063520",
    "end": "2072240"
  },
  {
    "text": "policy specification is a compile-time decision okay even though you pass an object you",
    "start": "2072240",
    "end": "2078720"
  },
  {
    "text": "can't make that a runtime decision because the type of that",
    "start": "2078720",
    "end": "2083919"
  },
  {
    "text": "object is defined in compiling",
    "start": "2083919",
    "end": "2089679"
  },
  {
    "text": "the second policy is called parallel policy and the execution in this policy",
    "start": "2089679",
    "end": "2097520"
  },
  {
    "text": "is done by multiple threads as determined by the runtime um scheduler",
    "start": "2097520",
    "end": "2105119"
  },
  {
    "text": "um here purse red processing is like the",
    "start": "2105119",
    "end": "2111520"
  },
  {
    "text": "sequence policy namely each thread and processes a single data element at every",
    "start": "2111520",
    "end": "2119359"
  },
  {
    "text": "given moment okay it does not process uh it does not interleave processing",
    "start": "2119359",
    "end": "2127119"
  },
  {
    "text": "of different data elements by switching among them",
    "start": "2127119",
    "end": "2134720"
  },
  {
    "text": "the the order of elements that is being processed by each thread is not specified but",
    "start": "2134720",
    "end": "2141200"
  },
  {
    "text": "again it is guaranteed that each element is being processed",
    "start": "2141200",
    "end": "2146240"
  },
  {
    "text": "in the full by a single thread okay",
    "start": "2146240",
    "end": "2151200"
  },
  {
    "text": "now within each thread if you execute your own code say by",
    "start": "2151359",
    "end": "2158640"
  },
  {
    "text": "using something like for each parallel databases are possible if you access",
    "start": "2158640",
    "end": "2166320"
  },
  {
    "text": "some external state that is not protected okay as with any kind of um",
    "start": "2166320",
    "end": "2174079"
  },
  {
    "text": "parallel execution that is unsafe however if you're not",
    "start": "2174079",
    "end": "2181280"
  },
  {
    "text": "accessing that kind of data any accesses to the processed container",
    "start": "2181280",
    "end": "2187680"
  },
  {
    "text": "is guaranteed to be safe",
    "start": "2187680",
    "end": "2191520"
  },
  {
    "text": "the third policy is called unsec parallel unsequenced and here",
    "start": "2192720",
    "end": "2200480"
  },
  {
    "text": "a few more restrictions are are provided or required and",
    "start": "2200480",
    "end": "2207760"
  },
  {
    "text": "so here they a single thread might process multiple",
    "start": "2208400",
    "end": "2216320"
  },
  {
    "text": "data elements at the same time or at least potentially do that",
    "start": "2216320",
    "end": "2221599"
  },
  {
    "text": "um and data elements might be processed by multiple",
    "start": "2221599",
    "end": "2227359"
  },
  {
    "text": "threads so this this poses some restrictions",
    "start": "2227359",
    "end": "2233680"
  },
  {
    "text": "on the code that you may write or may provide as the user for instance you may not use",
    "start": "2233680",
    "end": "2240560"
  },
  {
    "text": "any locks okay and you cannot assume",
    "start": "2240560",
    "end": "2245680"
  },
  {
    "text": "that a thread executes a single operation at any given time",
    "start": "2245680",
    "end": "2251040"
  },
  {
    "text": "those are some formal and requirements that are a bit difficult to understand but",
    "start": "2251040",
    "end": "2258240"
  },
  {
    "text": "what you need to have in mind here is the following using this policy or the restriction",
    "start": "2258240",
    "end": "2265200"
  },
  {
    "text": "imposed by those this policy allows victorization to be used okay so basically the goal of",
    "start": "2265200",
    "end": "2272880"
  },
  {
    "text": "those restrictions is to make it possible for the implementation to safely have a single",
    "start": "2272880",
    "end": "2279280"
  },
  {
    "text": "thread process multiple elements at the same time",
    "start": "2279280",
    "end": "2284480"
  },
  {
    "text": "using vector instruction okay",
    "start": "2284480",
    "end": "2291359"
  },
  {
    "text": "and the last execution policy is called unsequenced which is included",
    "start": "2293119",
    "end": "2300400"
  },
  {
    "text": "in c plus 20 and and according to this policy",
    "start": "2300400",
    "end": "2307359"
  },
  {
    "text": "an operation or execution is done on a single thread again like the",
    "start": "2307359",
    "end": "2313520"
  },
  {
    "text": "sequential execution however",
    "start": "2313520",
    "end": "2318160"
  },
  {
    "text": "operations can be interleaved namely a single thread can process multiple elements at the",
    "start": "2318880",
    "end": "2324880"
  },
  {
    "text": "same time and this again allows us or allows the implementation to use",
    "start": "2324880",
    "end": "2331280"
  },
  {
    "text": "vector instructions to accelerate processing even if it's done within the context of a single",
    "start": "2331280",
    "end": "2338839"
  },
  {
    "text": "thread the standard also allows",
    "start": "2338839",
    "end": "2344800"
  },
  {
    "text": "vendor-specific policies to be provided and those policies can use",
    "start": "2344800",
    "end": "2351920"
  },
  {
    "text": "different kinds of accelerators such as the gpu fpga and dedicated h basics",
    "start": "2351920",
    "end": "2361200"
  },
  {
    "text": "now one important thing to have in mind is that parallel algorithms do not mean",
    "start": "2362320",
    "end": "2367599"
  },
  {
    "text": "parallel containers okay the standard library clearly separates containers from",
    "start": "2367599",
    "end": "2374960"
  },
  {
    "text": "algorithms and the fact that algorithms can be parallel",
    "start": "2374960",
    "end": "2381200"
  },
  {
    "text": "does not mean that containers are made straight thread safe and the reason for that is",
    "start": "2381200",
    "end": "2388400"
  },
  {
    "text": "that thread safe containers by default at least would violate the zero overhead",
    "start": "2388400",
    "end": "2395040"
  },
  {
    "text": "principle i mean if you're not using uh parallelism in your program",
    "start": "2395040",
    "end": "2400240"
  },
  {
    "text": "why would you have to pay the price for being thread safe okay so",
    "start": "2400240",
    "end": "2407040"
  },
  {
    "text": "the uh parallel algorithms have to bear in mind",
    "start": "2407040",
    "end": "2412160"
  },
  {
    "text": "in their implementation that they are operating on containers that are not spread safe",
    "start": "2412160",
    "end": "2418319"
  },
  {
    "text": "and so processing have to be um have to to take that into consideration",
    "start": "2418319",
    "end": "2425839"
  },
  {
    "text": "so how is this or how can this be done so for instance if uh if you're",
    "start": "2426800",
    "end": "2433440"
  },
  {
    "text": "processing say a vector a regular vector which is",
    "start": "2433440",
    "end": "2439839"
  },
  {
    "text": "not thread safe in any way can still be accessed by multiple threads",
    "start": "2439839",
    "end": "2445119"
  },
  {
    "text": "as long as each thread accesses a different element in that vector okay this is safe according to the",
    "start": "2445119",
    "end": "2453599"
  },
  {
    "text": "simplest memory model so processing data structures such as",
    "start": "2453599",
    "end": "2460240"
  },
  {
    "text": "vectors is fairly easy on the other hand you cannot change",
    "start": "2460240",
    "end": "2465680"
  },
  {
    "text": "or it's not easy to change the structure of a container so replacing the",
    "start": "2465680",
    "end": "2472480"
  },
  {
    "text": "underlying array within a vector can",
    "start": "2472480",
    "end": "2478400"
  },
  {
    "text": "cause a data races and even worse updating dynamic data structures such as",
    "start": "2478400",
    "end": "2485680"
  },
  {
    "text": "sets maps and so on that are basically trees that are dynamically arranged according",
    "start": "2485680",
    "end": "2492160"
  },
  {
    "text": "to the data itself that is stored within them those data structures cannot be updated",
    "start": "2492160",
    "end": "2499760"
  },
  {
    "text": "in parallel because they are not thread safe so the algorithms",
    "start": "2499760",
    "end": "2505680"
  },
  {
    "text": "either cannot update them can cannot update them or need to find",
    "start": "2505680",
    "end": "2512720"
  },
  {
    "text": "some way to make it safe",
    "start": "2512720",
    "end": "2517200"
  },
  {
    "text": "so one other thing to have in mind that the fact that a container is being processed by a",
    "start": "2519200",
    "end": "2525280"
  },
  {
    "text": "parallel algorithm it does not make it again thread safe",
    "start": "2525280",
    "end": "2530640"
  },
  {
    "text": "during that processing namely if you have two threads one invokes some parallel algorithm",
    "start": "2530640",
    "end": "2539119"
  },
  {
    "text": "on a container and the other tries to access that parallel that container while it being it is being processed by",
    "start": "2539119",
    "end": "2546400"
  },
  {
    "text": "the parallel algorithm then that can potentially create a data race okay so safety is only",
    "start": "2546400",
    "end": "2554240"
  },
  {
    "text": "guaranteed within the execution of the parallel algorithm itself",
    "start": "2554240",
    "end": "2560720"
  },
  {
    "start": "2561000",
    "end": "2561000"
  },
  {
    "text": "okay moving on let's compare the different models we",
    "start": "2561680",
    "end": "2567520"
  },
  {
    "text": "discussed so far first in terms of the source of",
    "start": "2567520",
    "end": "2573440"
  },
  {
    "text": "parallelism in the unstructured model it's custom it depends on the",
    "start": "2573440",
    "end": "2579200"
  },
  {
    "text": "actual use case that or the the problem that your program is",
    "start": "2579200",
    "end": "2584560"
  },
  {
    "text": "trying to solve in the task based model parallelism stems from functional decomposition of",
    "start": "2584560",
    "end": "2592079"
  },
  {
    "text": "processing whereas in the data parallel model",
    "start": "2592079",
    "end": "2597119"
  },
  {
    "text": "parallelism stems from the data itself the state in a unstructured model",
    "start": "2597119",
    "end": "2604880"
  },
  {
    "text": "is basically shared among different threads potentially global in the task-based",
    "start": "2604880",
    "end": "2612720"
  },
  {
    "text": "model the state is delivered by using the promise to future a channel and in the",
    "start": "2612720",
    "end": "2620640"
  },
  {
    "text": "parallel model the state is the container that is being processed itself",
    "start": "2620640",
    "end": "2627760"
  },
  {
    "text": "the context in which we execute those models is again a costume uh",
    "start": "2628240",
    "end": "2635920"
  },
  {
    "text": "in the unstructured model uh in the task based model it would usually be this uh spawning",
    "start": "2635920",
    "end": "2643040"
  },
  {
    "text": "task will further or can further spawn other tasks",
    "start": "2643040",
    "end": "2649440"
  },
  {
    "text": "and in the data parallel model the parallel algorithm is invoked",
    "start": "2649440",
    "end": "2658079"
  },
  {
    "text": "from a sequential context scheduling in the unstructured model is",
    "start": "2658079",
    "end": "2664800"
  },
  {
    "text": "again custom it's up to the particular implementation in the task based model we have a",
    "start": "2664800",
    "end": "2671839"
  },
  {
    "text": "runtime scheduler that uses a task queue and in the data parallel model",
    "start": "2671839",
    "end": "2677920"
  },
  {
    "text": "we also have a scheduler that strives to spread the processing",
    "start": "2677920",
    "end": "2685280"
  },
  {
    "text": "among all possible compute resources",
    "start": "2685280",
    "end": "2690640"
  },
  {
    "text": "and lastly the status of the different models so the unstructured model is well",
    "start": "2690640",
    "end": "2695920"
  },
  {
    "text": "established since sql source 11. the task-based model",
    "start": "2695920",
    "end": "2701119"
  },
  {
    "text": "is again it also well established but is",
    "start": "2701119",
    "end": "2706559"
  },
  {
    "text": "will hopefully be redesigned and the data parallel model is being slowly adopted because um it is",
    "start": "2707119",
    "end": "2714400"
  },
  {
    "text": "uh it has limited support so the last thing i'd like to discuss is",
    "start": "2714400",
    "end": "2721119"
  },
  {
    "start": "2718000",
    "end": "2718000"
  },
  {
    "text": "missing mixing models so first let's assume we have some unstructured context",
    "start": "2721119",
    "end": "2727599"
  },
  {
    "text": "and we have a lot of reds that are executing various operations",
    "start": "2727599",
    "end": "2734400"
  },
  {
    "text": "say by implementing a server so within this context in which",
    "start": "2734400",
    "end": "2742400"
  },
  {
    "text": "most cores are used we can use tasks if we want to hide",
    "start": "2742400",
    "end": "2748720"
  },
  {
    "text": "io latency but we do not have compute resources",
    "start": "2748720",
    "end": "2755359"
  },
  {
    "text": "to invoke some parallel algorithm that just doesn't make sense um",
    "start": "2755359",
    "end": "2761839"
  },
  {
    "text": "at least if we unless we can use some additional resources such as gpu",
    "start": "2761839",
    "end": "2769359"
  },
  {
    "text": "on the other hand if within our unstructured context we are only using a",
    "start": "2769359",
    "end": "2776400"
  },
  {
    "text": "few or or some of the compute resources say we're using threads for some",
    "start": "2776400",
    "end": "2783280"
  },
  {
    "text": "background service and then we can certainly invoke tasks and parallel",
    "start": "2783280",
    "end": "2789680"
  },
  {
    "text": "algorithms however we do have to be careful with",
    "start": "2789680",
    "end": "2795359"
  },
  {
    "text": "shared state as with any parallel processing",
    "start": "2795359",
    "end": "2801359"
  },
  {
    "text": "one thing to have in mind in this kind of situation um the runtime schedulers whether",
    "start": "2801680",
    "end": "2808640"
  },
  {
    "text": "task-based or parallel programming parallel algorithms are not aware of or may not be aware of",
    "start": "2808640",
    "end": "2816079"
  },
  {
    "text": "threads created by the user okay if you create multiple threads the runtime scheduler",
    "start": "2816079",
    "end": "2823520"
  },
  {
    "text": "might still try to create all possible cores and thus over oversubscribe your compute",
    "start": "2823520",
    "end": "2831040"
  },
  {
    "text": "resources the next scenario is assuming we're",
    "start": "2831040",
    "end": "2838160"
  },
  {
    "start": "2835000",
    "end": "2835000"
  },
  {
    "text": "using a task based program which uses most of the course in its",
    "start": "2838160",
    "end": "2845520"
  },
  {
    "text": "execution namely it creates a lot of tasks and in this case within a certain task",
    "start": "2845520",
    "end": "2852720"
  },
  {
    "text": "it does not make sense to create a thread to thread um or to use any",
    "start": "2852720",
    "end": "2859280"
  },
  {
    "text": "parallel algorithms we just don't have enough compute resources for that on the other hand if we're",
    "start": "2859280",
    "end": "2865920"
  },
  {
    "text": "using tasks the just to hide io latency or",
    "start": "2865920",
    "end": "2871520"
  },
  {
    "text": "to uh to keep the ui responsive by using a ui worker uh then we can use",
    "start": "2871520",
    "end": "2880079"
  },
  {
    "text": "parallel algorithms within the within a task um creating threads from within a task",
    "start": "2880079",
    "end": "2887119"
  },
  {
    "text": "does not really make sense um you can instead spawn new tasks and also",
    "start": "2887119",
    "end": "2895359"
  },
  {
    "text": "using synchronization mechanisms such as mutexes um is possible but doesn't really fit",
    "start": "2895359",
    "end": "2902559"
  },
  {
    "text": "the the model the task-based model",
    "start": "2902559",
    "end": "2907040"
  },
  {
    "start": "2907000",
    "end": "2907000"
  },
  {
    "text": "lastly if our power processing is based on parallel algorithms then within a",
    "start": "2908160",
    "end": "2914400"
  },
  {
    "text": "parallel algorithm say your your own code",
    "start": "2914400",
    "end": "2919440"
  },
  {
    "text": "provided by a lambda there is no point in spawning async tasks or creating new threads",
    "start": "2919440",
    "end": "2927520"
  },
  {
    "text": "you can use synchronization mechanisms again mutex or some atomics",
    "start": "2927520",
    "end": "2934960"
  },
  {
    "text": "to access external state but you have to be careful not to kill the the",
    "start": "2934960",
    "end": "2941280"
  },
  {
    "text": "parallelism provided by the para algorithm",
    "start": "2941280",
    "end": "2945920"
  },
  {
    "text": "okay to summarize andrew tenenbaum once said the nice thing about standard",
    "start": "2946559",
    "end": "2952400"
  },
  {
    "start": "2947000",
    "end": "2947000"
  },
  {
    "text": "is that there are so many of them to choose from and the same thing applies for sql space",
    "start": "2952400",
    "end": "2958720"
  },
  {
    "text": "parallel fatigue facilities parallelism is hard",
    "start": "2958720",
    "end": "2964079"
  },
  {
    "text": "so in general prefer high-level models parallel algorithms that provide a",
    "start": "2964079",
    "end": "2971280"
  },
  {
    "text": "simple mental model they are slowly being supported but they have great potential with",
    "start": "2971280",
    "end": "2979200"
  },
  {
    "text": "future hardware and new accelerators that based model",
    "start": "2979200",
    "end": "2984319"
  },
  {
    "text": "is underutilized due to its unclear future",
    "start": "2984319",
    "end": "2990880"
  },
  {
    "text": "but future improvements are certainly to be expected um and",
    "start": "2990880",
    "end": "2997680"
  },
  {
    "text": "in general the task based model is key for asynchronous programming so hopefully",
    "start": "2997680",
    "end": "3003680"
  },
  {
    "text": "we'll see some significant improvements in the future",
    "start": "3003680",
    "end": "3009039"
  },
  {
    "text": "thank you for bearing with me thus far i'll have you to take questions either online or through the task very",
    "start": "3009359",
    "end": "3018000"
  },
  {
    "text": "slack channel",
    "start": "3018000",
    "end": "3027599"
  }
]