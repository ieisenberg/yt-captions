[
  {
    "start": "0",
    "end": "88000"
  },
  {
    "text": "i believe we can start welcome everybody i'm francesco tiziot developer advocate",
    "start": "2320",
    "end": "8000"
  },
  {
    "text": "at ivan which if you don't know it's originally a company based in finland",
    "start": "8000",
    "end": "13759"
  },
  {
    "text": "now we operate worldwide we provide open source data platforms as many services on top of the cloud of your choice",
    "start": "13759",
    "end": "21439"
  },
  {
    "text": "i'm here at nbc london very happy to be here in order to tell you about a kind of a",
    "start": "21439",
    "end": "27760"
  },
  {
    "text": "long title as i've been told and is basically how to track database",
    "start": "27760",
    "end": "33760"
  },
  {
    "text": "changes with apache kafka so i want this session to be as much interactive as",
    "start": "33760",
    "end": "38800"
  },
  {
    "text": "possible so i will ask you some questions along the way the first one being who is using kafka or who knows who what",
    "start": "38800",
    "end": "46480"
  },
  {
    "text": "kafka is okay quite some people the other question is who",
    "start": "46480",
    "end": "53680"
  },
  {
    "text": "works with a database i was expecting that so what we will",
    "start": "53680",
    "end": "59840"
  },
  {
    "text": "discover in this session is how to make all those two words working together",
    "start": "59840",
    "end": "65198"
  },
  {
    "text": "but before looking at how to make those two words working together let's do one step back let's try to understand why we",
    "start": "65199",
    "end": "72720"
  },
  {
    "text": "need to have those two technologies working together and uh as always in my talks i",
    "start": "72720",
    "end": "78560"
  },
  {
    "text": "try to come up with an example of a friend this case uh i believe you can understand the time italian so you will",
    "start": "78560",
    "end": "84880"
  },
  {
    "text": "get a lot of italian stuff in here as well the example is based on a friend who is",
    "start": "84880",
    "end": "90079"
  },
  {
    "start": "88000",
    "end": "445000"
  },
  {
    "text": "called luigi he owns a restaurant he is really forward-looking as person",
    "start": "90079",
    "end": "95840"
  },
  {
    "text": "and of course he stores the data of the restaurant in a database what kind of data",
    "start": "95840",
    "end": "102000"
  },
  {
    "text": "well he has waiters in the restaurant which goes to a certain table with an ipad or",
    "start": "102000",
    "end": "108960"
  },
  {
    "text": "a phone or whatever tablet they have and they will push one order to the",
    "start": "108960",
    "end": "114399"
  },
  {
    "text": "database or retrieve one one order so the type of operations that the database",
    "start": "114399",
    "end": "120799"
  },
  {
    "text": "is receiving are more or less transactions so my friend luigi spent a lot of time",
    "start": "120799",
    "end": "126399"
  },
  {
    "text": "optimizing the database for the type of transactions that the waiters are creating",
    "start": "126399",
    "end": "132560"
  },
  {
    "text": "and everything is working good like the transactions finish within few milliseconds the",
    "start": "132560",
    "end": "139680"
  },
  {
    "text": "waiters do their job they they're happy customers are happy but is this the end of the journey well",
    "start": "139680",
    "end": "146319"
  },
  {
    "text": "no because otherwise my talk will be like two minutes and all done we all know that",
    "start": "146319",
    "end": "152720"
  },
  {
    "text": "in all the companies the journey of the data doesn't stop here there will be other people interested in",
    "start": "152720",
    "end": "158400"
  },
  {
    "text": "the data for example the person doing the billing this person uses another technology",
    "start": "158400",
    "end": "165120"
  },
  {
    "text": "another database in order to do all the number crunching and you know the type of queries that this person generates is",
    "start": "165120",
    "end": "172480"
  },
  {
    "text": "not more or less transaction this person wants to know whatever happened yesterday whatever is happening",
    "start": "172480",
    "end": "179840"
  },
  {
    "text": "now how many orders are we getting are we making money out of our business",
    "start": "179840",
    "end": "186080"
  },
  {
    "text": "still like even if the type of queries are different we cannot refuse this person",
    "start": "186080",
    "end": "191200"
  },
  {
    "text": "to access the data but you know this person doesn't care if",
    "start": "191200",
    "end": "197680"
  },
  {
    "text": "she or he received the data today about orders being done yesterday all this person needs to make is",
    "start": "197680",
    "end": "204239"
  },
  {
    "text": "the counts that everything is okay that the business is running okay so how can we solve this problem well for example",
    "start": "204239",
    "end": "211440"
  },
  {
    "text": "we can allow this person to extract the data but only when we are sure that the",
    "start": "211440",
    "end": "216879"
  },
  {
    "text": "application is not in use so we don't make the database suffer we don't put pressure on the database so we allow",
    "start": "216879",
    "end": "224400"
  },
  {
    "text": "overnight extraction i believe this was a common theme in the previous years",
    "start": "224400",
    "end": "230480"
  },
  {
    "text": "but again the data in the restaurant doesn't stop there the data journey doesn't stop there there are other",
    "start": "230480",
    "end": "236480"
  },
  {
    "text": "people in the restaurant for example the chefs the chefs use a different technology",
    "start": "236480",
    "end": "243120"
  },
  {
    "text": "and you know they don't care about a single order of a single table they care about how many",
    "start": "243120",
    "end": "250640"
  },
  {
    "text": "linguine they need to produce how many pasta or many like whatever lasagna they",
    "start": "250640",
    "end": "255840"
  },
  {
    "text": "need to produce so the other difference between the chefs and the person doing the billing is that",
    "start": "255840",
    "end": "263759"
  },
  {
    "text": "the chef cannot wait one day in order to understand what dishes he has to prepare the chef",
    "start": "263759",
    "end": "269680"
  },
  {
    "text": "needs to know almost in real time what orders are coming from the restaurant",
    "start": "269680",
    "end": "275840"
  },
  {
    "text": "so you know how can we solve this problem well we need a different strategy we for example could create a",
    "start": "275840",
    "end": "281199"
  },
  {
    "text": "read-only replica of our source database and allow the chefs to query and extract",
    "start": "281199",
    "end": "286639"
  },
  {
    "text": "the data from the read-only replica still this kind of design allow us not to put too much pressure on the source",
    "start": "286639",
    "end": "292960"
  },
  {
    "text": "database and provide fresh data to the chefs but again i don't know about your",
    "start": "292960",
    "end": "299199"
  },
  {
    "text": "friends but my friend luigi is very smart he knows that if he wants to have a successful business he needs to hire a",
    "start": "299199",
    "end": "305120"
  },
  {
    "text": "data scientist and since he knows that data scientists are really smart",
    "start": "305120",
    "end": "310479"
  },
  {
    "text": "and really expensive doesn't want this data scientist to lose time into accessing the the database so what he",
    "start": "310479",
    "end": "317680"
  },
  {
    "text": "does it grants to the data scientist directly access to the source",
    "start": "317680",
    "end": "322840"
  },
  {
    "text": "database and you know this is the first scene",
    "start": "322840",
    "end": "328160"
  },
  {
    "text": "because i've been in a lot of companies and companies are a lot about data but",
    "start": "328160",
    "end": "333440"
  },
  {
    "text": "also a lot about politics so now that one person has access",
    "start": "333440",
    "end": "338800"
  },
  {
    "text": "to the source database a lot of other people start requesting access to the database",
    "start": "338800",
    "end": "346080"
  },
  {
    "text": "i don't know what went wrong basically a lot of other people are now",
    "start": "346080",
    "end": "352320"
  },
  {
    "text": "let's go on are now trying to get access to the database trying to pull data from the database itself",
    "start": "352320",
    "end": "360000"
  },
  {
    "text": "and the problem with this approach is that all these different people think about the people doing the delivery the people",
    "start": "360000",
    "end": "366639"
  },
  {
    "text": "doing the cleaning other people that are servicing the restaurant they will all have different query patterns",
    "start": "366639",
    "end": "373520"
  },
  {
    "text": "so now our database is suffering because it was optimized for transactions and now it receives different type of",
    "start": "373520",
    "end": "379440"
  },
  {
    "text": "queries is hammered by different type of queries the the waiter square is that the waiter's",
    "start": "379440",
    "end": "386160"
  },
  {
    "text": "transaction that was taking milliseconds to execute now takes seconds take minutes and your customers start",
    "start": "386160",
    "end": "391680"
  },
  {
    "text": "complaining even more all the errors that you see",
    "start": "391680",
    "end": "397120"
  },
  {
    "text": "here are now dependencies dependency means that",
    "start": "397120",
    "end": "403600"
  },
  {
    "text": "if before for example you wanted to do a small change in a database and you had only to talk with the waiters now you",
    "start": "403600",
    "end": "409919"
  },
  {
    "text": "have to talk and to agree on the change with all the people having an arrow going in and out of your database and",
    "start": "409919",
    "end": "416080"
  },
  {
    "text": "that basically slows down a lot the pace of change in the companies",
    "start": "416080",
    "end": "421680"
  },
  {
    "text": "again now another question how many of you have this situation in your company",
    "start": "421680",
    "end": "428639"
  },
  {
    "text": "how many of you regret having this situation in your company okay it's",
    "start": "428800",
    "end": "434400"
  },
  {
    "text": "a problem and again this is a situation about my friend luigi but we can take home a lot of lessons from this",
    "start": "434400",
    "end": "441759"
  },
  {
    "text": "story the first one is that a lot of people will be interested in",
    "start": "441759",
    "end": "447120"
  },
  {
    "start": "445000",
    "end": "675000"
  },
  {
    "text": "some data assets that we provide that we generate some of the use cases will be clear from day one",
    "start": "447120",
    "end": "453520"
  },
  {
    "text": "think about the chefs some of the use cases will be clear only two years later and also the related",
    "start": "453520",
    "end": "460160"
  },
  {
    "text": "patterns will be clear only two years later think about the delivery people it was a use case that five years ago",
    "start": "460160",
    "end": "466240"
  },
  {
    "text": "wasn't existing so we will need to create something that is able to cope with different type of",
    "start": "466240",
    "end": "473599"
  },
  {
    "text": "queries different type of interactions with our data the other lesson that we can take home",
    "start": "473599",
    "end": "479120"
  },
  {
    "text": "is people will use different technologies every person",
    "start": "479120",
    "end": "485039"
  },
  {
    "text": "every stakeholder of the data can use a different technology we cannot force all the people to use a certain technology",
    "start": "485039",
    "end": "491280"
  },
  {
    "text": "because it's the one that we choose initially if we force people using the same technology",
    "start": "491280",
    "end": "497360"
  },
  {
    "text": "well i have bad news for you people will probably take a copy of the data and use a different technology",
    "start": "497360",
    "end": "503039"
  },
  {
    "text": "themselves another lesson errors arrows are dependencies",
    "start": "503039",
    "end": "510240"
  },
  {
    "text": "errors are good because they showed that your company is data-driven as it should be but arrows are bad because other",
    "start": "510240",
    "end": "516880"
  },
  {
    "text": "dependencies so they will slow down the process of changing your company the last lesson that we can take home is",
    "start": "516880",
    "end": "524000"
  },
  {
    "text": "the fact that you know in the past it was fine to receive today yesterday data",
    "start": "524000",
    "end": "530160"
  },
  {
    "text": "but now this is not the reality anymore we cannot wait one day before analyzing if our customers are churning if our",
    "start": "530160",
    "end": "537440"
  },
  {
    "text": "sales are going down we need to act immediately",
    "start": "537440",
    "end": "542399"
  },
  {
    "text": "so how do we solve this problem well we need to change something but there is",
    "start": "542480",
    "end": "547519"
  },
  {
    "text": "one thing that we don't want to change we don't want to change the original setup",
    "start": "547519",
    "end": "553600"
  },
  {
    "text": "which is our application and our database why is that well because we spent a huge amount of time building the application",
    "start": "553600",
    "end": "560720"
  },
  {
    "text": "and a huge amount of time setting the database and we know that that solution works",
    "start": "560720",
    "end": "567279"
  },
  {
    "text": "so why should we change that but now what we can do is to create some",
    "start": "567279",
    "end": "572800"
  },
  {
    "text": "sort of intermediate layer between our source database and all the other technologies",
    "start": "572800",
    "end": "579360"
  },
  {
    "text": "using the data coming from the source database but since we are putting an intermediate",
    "start": "579360",
    "end": "585839"
  },
  {
    "text": "layer in the middle we need to have this kind of intermediate technology to have some",
    "start": "585839",
    "end": "591680"
  },
  {
    "text": "sort of the same qualities as how a good old database has",
    "start": "591680",
    "end": "596959"
  },
  {
    "text": "for example this intermediate technology needs to manage high volumes of data",
    "start": "596959",
    "end": "602640"
  },
  {
    "text": "because our database could cope with that this intermediate technology should be able to serve all the downstream",
    "start": "602640",
    "end": "609440"
  },
  {
    "text": "technology both in batch mode or in streaming mode depending on the need of those technologies",
    "start": "609440",
    "end": "616160"
  },
  {
    "text": "and because we need to serve different downstream technologies in different ways we need also",
    "start": "616160",
    "end": "622800"
  },
  {
    "text": "to be able to store the data within this interface technology",
    "start": "622800",
    "end": "627839"
  },
  {
    "text": "even more one additional thing if now we add a new downstream technology in the",
    "start": "627839",
    "end": "634800"
  },
  {
    "text": "overall figure we want the interface technology to take care of the additional load without",
    "start": "634800",
    "end": "640480"
  },
  {
    "text": "putting any other pressure on the source database and you know if you take all this",
    "start": "640480",
    "end": "647360"
  },
  {
    "text": "constraint into the picture well it's complex because you need",
    "start": "647360",
    "end": "652720"
  },
  {
    "text": "something that can speak multiple languages that can speak uh that can scale",
    "start": "652720",
    "end": "659040"
  },
  {
    "text": "that doesn't lose any data and there is only a little subset of",
    "start": "659040",
    "end": "664079"
  },
  {
    "text": "technologies that can solve this problem and the one that i'm going to talk about today is apache kafka",
    "start": "664079",
    "end": "670880"
  },
  {
    "text": "so the next question for some of you that don't know apache kafka what is apache kafka good question well done um",
    "start": "670880",
    "end": "678079"
  },
  {
    "start": "675000",
    "end": "820000"
  },
  {
    "text": "kafka is a beautiful technology and i believe you heard about about apache kafka being",
    "start": "678079",
    "end": "684000"
  },
  {
    "text": "used in a huge variety of use cases from like click stream data flowing to",
    "start": "684000",
    "end": "691040"
  },
  {
    "text": "transaction bank transaction flowing so the detail of apache kafka or the",
    "start": "691040",
    "end": "696320"
  },
  {
    "text": "basic of apache kafka is really a simple concept it's the concept of a log file",
    "start": "696320",
    "end": "703279"
  },
  {
    "text": "a file where you write to the log with one or more",
    "start": "703279",
    "end": "708480"
  },
  {
    "text": "applications called producers what do you write to the log you write events in the log in app and only and",
    "start": "708480",
    "end": "714959"
  },
  {
    "text": "immutable data format happen only means that you write event number zero then event number one then",
    "start": "714959",
    "end": "720639"
  },
  {
    "text": "event number two three and four up and only immutable means that",
    "start": "720639",
    "end": "727200"
  },
  {
    "text": "once you write event number zero in the log it's not like a record in a database",
    "start": "727200",
    "end": "733360"
  },
  {
    "text": "if something changes the reality of event number zero you cannot go in the log and change it you will need to write",
    "start": "733360",
    "end": "739440"
  },
  {
    "text": "the new reality as a new event in the log now",
    "start": "739440",
    "end": "746240"
  },
  {
    "text": "now that we have all these events in our log as soon as we start having the events we can also start consuming",
    "start": "746320",
    "end": "752880"
  },
  {
    "text": "because kafka is a streaming platform allows you to consume everything in stream and we can do that with one",
    "start": "752880",
    "end": "759120"
  },
  {
    "text": "or more consumers this is one of the main distinctions between kafka and for",
    "start": "759120",
    "end": "764160"
  },
  {
    "text": "example rabbit and q with ram and q you push an event and you read it only once",
    "start": "764160",
    "end": "769200"
  },
  {
    "text": "with kafka you can push an event and you can have one or more consumers reading it",
    "start": "769200",
    "end": "775279"
  },
  {
    "text": "and of course with kafka you are not limited to one unique log you can have multiple logs with multiple producers",
    "start": "775680",
    "end": "782560"
  },
  {
    "text": "multiple consumers and multiple type of events just think about our restaurant with at the top",
    "start": "782560",
    "end": "789760"
  },
  {
    "text": "the orders at the bottom for example you want to track the position of the delivery guy delivering your pizzas or",
    "start": "789760",
    "end": "797120"
  },
  {
    "text": "your food and in kafka terms d-logs are called topic so you can give a name for each",
    "start": "797120",
    "end": "803839"
  },
  {
    "text": "topic more or less like you used to give a name to each table in a database",
    "start": "803839",
    "end": "810879"
  },
  {
    "text": "one of the things that i said about apache kafka is that it needs to be able to scale",
    "start": "811760",
    "end": "817279"
  },
  {
    "text": "and that comes out by default with kafka because it's a distributed platform this",
    "start": "817279",
    "end": "822320"
  },
  {
    "start": "820000",
    "end": "910000"
  },
  {
    "text": "means that when you create a kafka instance you are actually most of the time creating a cluster of nodes",
    "start": "822320",
    "end": "828720"
  },
  {
    "text": "creating a cluster of nodes helps you because you could lose a node and still have the",
    "start": "828720",
    "end": "834880"
  },
  {
    "text": "cluster working in kafka terms the nodes are called brokers and now the log information that we were",
    "start": "834880",
    "end": "842480"
  },
  {
    "text": "talking about earlier on it's stored across the cluster",
    "start": "842480",
    "end": "848720"
  },
  {
    "text": "but if you notice here we are not storing the log only once in the cluster we are storing",
    "start": "848720",
    "end": "856480"
  },
  {
    "text": "it more than one following a parameter that is called replication factor that dictates how many copies of each topic",
    "start": "856480",
    "end": "864000"
  },
  {
    "text": "we have across the cluster in this case we have three copies of the sharp edges log and two copies of the round edges",
    "start": "864000",
    "end": "870399"
  },
  {
    "text": "log why do we do this well because we know",
    "start": "870399",
    "end": "875519"
  },
  {
    "text": "that computers are not entirely reliable so we know that we could lose a node but even if we lose a node we don't lose",
    "start": "875519",
    "end": "882320"
  },
  {
    "text": "any data this is the beauty of kafka now",
    "start": "882320",
    "end": "887839"
  },
  {
    "text": "we understood the basic of kafka but if you paid attention to what i said",
    "start": "887839",
    "end": "893199"
  },
  {
    "text": "initially i said that you right into the kafka logs which are",
    "start": "893199",
    "end": "899839"
  },
  {
    "text": "called topics events now if you remember what was the title",
    "start": "899839",
    "end": "904880"
  },
  {
    "text": "of my session was tracking database changes so",
    "start": "904880",
    "end": "910800"
  },
  {
    "start": "910000",
    "end": "1135000"
  },
  {
    "text": "we know that in database there are tables there are columns that are triggers rows",
    "start": "910800",
    "end": "917199"
  },
  {
    "text": "but what are events in a database i don't really know let's try to discover it",
    "start": "917199",
    "end": "924320"
  },
  {
    "text": "in order to understand this we need to understand the difference between a table-based approach into storing data",
    "start": "924320",
    "end": "931040"
  },
  {
    "text": "versus a log-based approach into storing data this is called usually the table versus logo table",
    "start": "931040",
    "end": "937680"
  },
  {
    "text": "versus stream duality is what i call the fridge dilemma",
    "start": "937680",
    "end": "944240"
  },
  {
    "text": "now before i go to the next slide i need to ask a question",
    "start": "944320",
    "end": "950880"
  },
  {
    "text": "how many of you rate pineapple pizza is good",
    "start": "950880",
    "end": "955839"
  },
  {
    "text": "okay let's ask this question again later okay so let's assume that i'm a friend",
    "start": "958480",
    "end": "965120"
  },
  {
    "text": "of luigi and because he is italian and runs an italian restaurant in italy",
    "start": "965120",
    "end": "971199"
  },
  {
    "text": "it's forbidden by low to put pineapple on pizza so how do we do it in italy in italy",
    "start": "971199",
    "end": "977120"
  },
  {
    "text": "what we do is we have a fridge where we store the pineapple because you could serve it as",
    "start": "977120",
    "end": "983120"
  },
  {
    "text": "within the dessert or like as fruit and what we do we store the",
    "start": "983120",
    "end": "989680"
  },
  {
    "text": "fridge information in a table or in a log depending on what technology you're using",
    "start": "989680",
    "end": "994880"
  },
  {
    "text": "and we check that no pineapple goes near the pizza oven",
    "start": "994880",
    "end": "999920"
  },
  {
    "text": "that's standard practice in italy okay so if we have the fridge information in",
    "start": "999920",
    "end": "1005519"
  },
  {
    "text": "a table we query the table and the table will tell us look that in your fridge there are five mushrooms",
    "start": "1005519",
    "end": "1011199"
  },
  {
    "text": "three salamis and six pineapples all fine correct",
    "start": "1011199",
    "end": "1017440"
  },
  {
    "text": "let's say that we check the fridge information every hour we see this",
    "start": "1018160",
    "end": "1023600"
  },
  {
    "text": "situation and everything seems to be fine if on the other side we store the same",
    "start": "1023600",
    "end": "1028640"
  },
  {
    "text": "information in a log and we query the log the log will tell the same thing but a little bit",
    "start": "1028640",
    "end": "1035360"
  },
  {
    "text": "different it will tell us that we purchase seven mushrooms and we use two of them so the",
    "start": "1035360",
    "end": "1042400"
  },
  {
    "text": "current state is five we purchase three salamis and we use none",
    "start": "1042400",
    "end": "1047839"
  },
  {
    "text": "and we purchase six pineapples if the situation was like this",
    "start": "1047839",
    "end": "1053200"
  },
  {
    "text": "paradise but if we have a devil's chef that knows that luigi checks the fridge every hour",
    "start": "1053200",
    "end": "1061360"
  },
  {
    "text": "and within an hour he goes to the fridge takes out five pineapples put them on pizzas and",
    "start": "1061360",
    "end": "1067200"
  },
  {
    "text": "replace them with other five that he purchased at this expense just to be devil",
    "start": "1067200",
    "end": "1072320"
  },
  {
    "text": "if he does all these changes within an hour luigi will not be able to find it out",
    "start": "1072320",
    "end": "1079600"
  },
  {
    "text": "basically what i'm saying here is that if you have a table the table contains the current state depending on how",
    "start": "1079600",
    "end": "1086160"
  },
  {
    "text": "frequently you will check the state you will be able to see all the changes or not on the other side the log contains all",
    "start": "1086160",
    "end": "1093200"
  },
  {
    "text": "the history of changes by replaying the log you will always arrive to the current state",
    "start": "1093200",
    "end": "1099360"
  },
  {
    "text": "this is how basically we have been doing data replication in many cases between the same technologies and we should do",
    "start": "1099360",
    "end": "1106559"
  },
  {
    "text": "something similar also to include all the changes happening in that database into kafka",
    "start": "1106559",
    "end": "1114080"
  },
  {
    "text": "who is with me until now everything clear are you satisfied are you still happy",
    "start": "1114240",
    "end": "1119360"
  },
  {
    "text": "with pineapple on pizza okay let's move on so we understood that",
    "start": "1119360",
    "end": "1127440"
  },
  {
    "text": "we need to put kafka in the picture we understood that we need to track all the little things",
    "start": "1127440",
    "end": "1132559"
  },
  {
    "text": "happening in our database now it's time to understand how do we integrate apache kafka with the existing tech",
    "start": "1132559",
    "end": "1139200"
  },
  {
    "start": "1135000",
    "end": "1245000"
  },
  {
    "text": "ecosystem which is our lovely database and you know we understood that kafka needs to fit",
    "start": "1139200",
    "end": "1144880"
  },
  {
    "text": "the picture we want to put kafka in the middle but still our data is somewhere else it's in cassandra in google pub sub",
    "start": "1144880",
    "end": "1151840"
  },
  {
    "text": "is in postgres or any other database we want to take the data from there into",
    "start": "1151840",
    "end": "1157360"
  },
  {
    "text": "kafka but we don't want to write that code by hand because as we know the database",
    "start": "1157360",
    "end": "1162640"
  },
  {
    "text": "contains huge volume of data and we don't have huge like java or c scales in",
    "start": "1162640",
    "end": "1168480"
  },
  {
    "text": "order to write the connector but we don't have to because",
    "start": "1168480",
    "end": "1173760"
  },
  {
    "text": "there is a pre-built framework that is called kafka connect that allow us to just specify where the",
    "start": "1173760",
    "end": "1180480"
  },
  {
    "text": "data is and it will take care of moving the data to kafka",
    "start": "1180480",
    "end": "1185840"
  },
  {
    "text": "even more kafka connect has kafka it's a distributed system so if we have",
    "start": "1185840",
    "end": "1191039"
  },
  {
    "text": "huge amount of data that we want to move we can possibly scale it up as needed",
    "start": "1191039",
    "end": "1197520"
  },
  {
    "text": "now if you remember my original drawing kafka usually is just a middle",
    "start": "1197520",
    "end": "1204080"
  },
  {
    "text": "layer you're taking data from point a and you're pushing to point b endpoint c and point d",
    "start": "1204080",
    "end": "1209679"
  },
  {
    "text": "and this is also usually the case for kafka where we want to take the data from kafka to",
    "start": "1209679",
    "end": "1215280"
  },
  {
    "text": "another set of technologies we want to bridge technologies with kafka",
    "start": "1215280",
    "end": "1220640"
  },
  {
    "text": "and again we don't want to write that connection ourselves but we can use kafka connect again to take the data",
    "start": "1220640",
    "end": "1226400"
  },
  {
    "text": "from a kafka topic to one or more technologies and we are using the source in order to",
    "start": "1226400",
    "end": "1231919"
  },
  {
    "text": "take data into kafka and the sync option to push the data to a target technology",
    "start": "1231919",
    "end": "1237919"
  },
  {
    "text": "okay also this is sorted we have a tool that we can use to move the data",
    "start": "1237919",
    "end": "1244640"
  },
  {
    "text": "next step how do i get the database changes in kafka",
    "start": "1244640",
    "end": "1250000"
  },
  {
    "start": "1245000",
    "end": "1535000"
  },
  {
    "text": "and you know if you're new to kafka but you have been using databases for",
    "start": "1250000",
    "end": "1255360"
  },
  {
    "text": "quite some time the usual approach in order to take data out of a database is by querying it",
    "start": "1255360",
    "end": "1262559"
  },
  {
    "text": "saying obvious and if you think this is a good solution well you're right",
    "start": "1262559",
    "end": "1268720"
  },
  {
    "text": "it doesn't it exists a kafka connect connector named the jdbc connector that",
    "start": "1268720",
    "end": "1273840"
  },
  {
    "text": "allows you to query a lot of databases via jdbc and take all the data into kafka",
    "start": "1273840",
    "end": "1279440"
  },
  {
    "text": "all you need to specify is the list of tables that you are interested into how frequently you want to poll in order",
    "start": "1279440",
    "end": "1286080"
  },
  {
    "text": "to check for new rows and also the query mode",
    "start": "1286080",
    "end": "1292000"
  },
  {
    "text": "the query mode is the obscure parameter here but basically tells kafka",
    "start": "1292000",
    "end": "1297840"
  },
  {
    "text": "how to identify which are the neurals compared to the previous ball",
    "start": "1297840",
    "end": "1304240"
  },
  {
    "text": "depending on the structure of of your table you can use different query modes so for example if",
    "start": "1304240",
    "end": "1311679"
  },
  {
    "text": "in your table you don't have a way to distinguish with which are the new rows compared to the previous poll what you",
    "start": "1311679",
    "end": "1317520"
  },
  {
    "text": "can do is you use debug mode so every polling interval every 30 seconds you",
    "start": "1317520",
    "end": "1322720"
  },
  {
    "text": "take all the data from the database table and you push it to kafka after 30 seconds what you do take all",
    "start": "1322720",
    "end": "1329200"
  },
  {
    "text": "the data from the table push it to kafka this works do you want to do that for a table",
    "start": "1329200",
    "end": "1336000"
  },
  {
    "text": "containing 5 trillion rows every 30 seconds i don't think so",
    "start": "1336000",
    "end": "1341760"
  },
  {
    "text": "so we need to be a little bit smarter so we can use other methods which rely on",
    "start": "1341760",
    "end": "1348080"
  },
  {
    "text": "having some extra information in the column for example if in the table sorry",
    "start": "1348080",
    "end": "1353919"
  },
  {
    "text": "you have an id which is always increasing well you can track which other neuros",
    "start": "1353919",
    "end": "1359760"
  },
  {
    "text": "based on that id and you can use the incremental mode for that if you don't have an id but you have a",
    "start": "1359760",
    "end": "1365679"
  },
  {
    "text": "timestamp well there is yet another method that allows you to use timestamps all pretty easy",
    "start": "1365679",
    "end": "1372880"
  },
  {
    "text": "let me show you how this works in practice so let's say that luigi has this beautiful table called",
    "start": "1372880",
    "end": "1379280"
  },
  {
    "text": "pasta with a call only one column containing all the pastas that he provides in the restaurant we have",
    "start": "1379280",
    "end": "1385120"
  },
  {
    "text": "fusilli panette farfalle in the table now we start the kafka connect connector in bulk mode in bike mode we are",
    "start": "1385120",
    "end": "1392000"
  },
  {
    "text": "extracting every polling interval all the data from kafka and moving it to sorry from the database and moving it to",
    "start": "1392000",
    "end": "1398559"
  },
  {
    "text": "kafka so the first query will extract fusilli penetefarfale and move it to kafka",
    "start": "1398559",
    "end": "1404480"
  },
  {
    "text": "after 30 seconds we do it again we do it again we do it again until we have a new row",
    "start": "1404480",
    "end": "1412159"
  },
  {
    "text": "appearing in our database table and in the next query we will have",
    "start": "1412159",
    "end": "1418080"
  },
  {
    "text": "the four values in kafka again for values again for values then",
    "start": "1418080",
    "end": "1424640"
  },
  {
    "text": "lasagna jumps in and in the next query we will see lasagna this works can track all the changes",
    "start": "1424640",
    "end": "1432480"
  },
  {
    "text": "but it's not optimal we are moving a huge amount of data this can be very expensive if you are in the cloud",
    "start": "1432480",
    "end": "1438320"
  },
  {
    "text": "because this is egress and ingress cost doesn't cost anything if you are working with ivan because recovery that we cover",
    "start": "1438320",
    "end": "1445360"
  },
  {
    "text": "all the network costs but in other solutions can be expensive let's do a little bit better if we have",
    "start": "1445360",
    "end": "1452080"
  },
  {
    "text": "a beautiful id like in this case always increasing we use incremental mode what the incremental mode does on",
    "start": "1452080",
    "end": "1458559"
  },
  {
    "text": "the first query loads all the data but also stores the maximum id which is three",
    "start": "1458559",
    "end": "1464720"
  },
  {
    "text": "all the following queries will have a where clause now saying give me all the rows with id greater than three so they",
    "start": "1464720",
    "end": "1471840"
  },
  {
    "text": "will move nothing until a new row appears with a d equal to four and that will be moved but only",
    "start": "1471840",
    "end": "1479600"
  },
  {
    "text": "that row not all the rows then nothing again will be moved until",
    "start": "1479600",
    "end": "1485760"
  },
  {
    "text": "a new row with a d5 pops in and we move 5 and we store 5 as",
    "start": "1485760",
    "end": "1491600"
  },
  {
    "text": "maximum works really good but what happens now if for example",
    "start": "1491600",
    "end": "1498840"
  },
  {
    "text": "luigi wants to get rid of panetta and wants to have pen instead",
    "start": "1498840",
    "end": "1504799"
  },
  {
    "text": "now we had an update in our table but because we didn't change the id",
    "start": "1504799",
    "end": "1511600"
  },
  {
    "text": "our incremental query is still looking for rows with a v greater than 5 it will not be able to track the update",
    "start": "1511600",
    "end": "1519039"
  },
  {
    "text": "well that's a pity because now we have a different version in kafka than we have",
    "start": "1519039",
    "end": "1524720"
  },
  {
    "text": "in the database so is there a way to track those updates well we need to create a",
    "start": "1524720",
    "end": "1531520"
  },
  {
    "text": "trigger more or less we can create a trigger based on a serial number like this or we can use",
    "start": "1531520",
    "end": "1538000"
  },
  {
    "start": "1535000",
    "end": "1720000"
  },
  {
    "text": "the timestamp approach where we have a creation date column and update that column and when we start kafka connector",
    "start": "1538000",
    "end": "1544720"
  },
  {
    "text": "again on the first run we'll load fuzzily penetrate then move nothing",
    "start": "1544720",
    "end": "1549760"
  },
  {
    "text": "until oh sorry it will stall the maximum timestamp 10 o'clock then nothing will be moved until we have",
    "start": "1549760",
    "end": "1556240"
  },
  {
    "text": "a new row in the database and in the new polling interval it will add spaghetti to the picture",
    "start": "1556240",
    "end": "1562240"
  },
  {
    "text": "and store 101 as maximum then we have lasagna that gets added to",
    "start": "1562240",
    "end": "1567440"
  },
  {
    "text": "kafka and store 102 as maximum and then when there is the update",
    "start": "1567440",
    "end": "1572480"
  },
  {
    "text": "now peneta is gone now we have pena but we have also an update date which is",
    "start": "1572480",
    "end": "1577679"
  },
  {
    "text": "1003. so our timestamp mode will be able to see okay there is some changes here i can",
    "start": "1577679",
    "end": "1584720"
  },
  {
    "text": "add pen just remember that we are dealing with a kafka topic which is an append only and",
    "start": "1584720",
    "end": "1590400"
  },
  {
    "text": "immutable data format so we are not removing panette we are just adding pen at the end of it",
    "start": "1590400",
    "end": "1596400"
  },
  {
    "text": "and we are able to track the updates and we also",
    "start": "1596400",
    "end": "1602480"
  },
  {
    "text": "update the maximum timestamp to 1003. so the updates",
    "start": "1602480",
    "end": "1608240"
  },
  {
    "text": "are solved but now let me show you another problem with this approach what happens if we",
    "start": "1608240",
    "end": "1614640"
  },
  {
    "text": "delete lasagna a query a query based approach will always",
    "start": "1614640",
    "end": "1620960"
  },
  {
    "text": "return rows only if they are existing if we are doing a deletion the query",
    "start": "1620960",
    "end": "1626799"
  },
  {
    "text": "based approach will not be able to return that row it will not be able to tell us that our row is missing",
    "start": "1626799",
    "end": "1634240"
  },
  {
    "text": "if we use the query based approach we are not able to track the lists we are not able to track hard elites",
    "start": "1634480",
    "end": "1640880"
  },
  {
    "text": "in order to track the leads we need to move from hard deletes to soft deletes to other column is deleted and set that",
    "start": "1640880",
    "end": "1647600"
  },
  {
    "text": "to true when we have a deletion this is something that you could do",
    "start": "1647600",
    "end": "1655360"
  },
  {
    "text": "but it's changing the shape of the table it's changing the",
    "start": "1655360",
    "end": "1661600"
  },
  {
    "text": "how applications are using the table because now they are they need to use soft deletes instead of harder deletes",
    "start": "1661600",
    "end": "1667760"
  },
  {
    "text": "now you have to talk with all the 20 200 developers that are working on the application to tell them",
    "start": "1667760",
    "end": "1673600"
  },
  {
    "text": "well you know you have to change the logic in some cases you can do that",
    "start": "1673600",
    "end": "1678880"
  },
  {
    "text": "because like the changes are small some other cases you have 30 applications written in the last 15 years",
    "start": "1678880",
    "end": "1685840"
  },
  {
    "text": "that really they don't want to change the way that they manage a table only because your jdbc connector can cope with hard",
    "start": "1685840",
    "end": "1693200"
  },
  {
    "text": "deletes you know this is possibly a problem",
    "start": "1693200",
    "end": "1699279"
  },
  {
    "text": "but let's say let's make it simple you know you could tell me",
    "start": "1699279",
    "end": "1704640"
  },
  {
    "text": "we are lucky francesco don't worry we have a table like this",
    "start": "1704640",
    "end": "1710480"
  },
  {
    "text": "we already do soft deletes you are like my friend luigi you think in the future",
    "start": "1710480",
    "end": "1716480"
  },
  {
    "text": "well good for you so let's let me show you how you can put the um",
    "start": "1716480",
    "end": "1722640"
  },
  {
    "start": "1720000",
    "end": "1843000"
  },
  {
    "text": "the query based approach in practice with kafka connect all you need is a json configuration file telling where",
    "start": "1722640",
    "end": "1729520"
  },
  {
    "text": "your data is coming from and where you want it to land so you can build this json configuration",
    "start": "1729520",
    "end": "1735279"
  },
  {
    "text": "file by saying first of all the name this is the name of the connector can you can give it any name even luigi if",
    "start": "1735279",
    "end": "1741440"
  },
  {
    "text": "you want then you say the connection details it's a gdbc connector pointing to a",
    "start": "1741440",
    "end": "1747440"
  },
  {
    "text": "postgres database using a very secure password123 that i've been told is the most secure in the world",
    "start": "1747440",
    "end": "1753919"
  },
  {
    "text": "and then you pass some additional parameters what table do i want to take the data from it's called pasta how often do i",
    "start": "1753919",
    "end": "1761360"
  },
  {
    "text": "want to to query the table every second or every 10 seconds in this case",
    "start": "1761360",
    "end": "1768240"
  },
  {
    "text": "then what else can we specify topic prefix uh what how i will call the topic in",
    "start": "1768240",
    "end": "1775520"
  },
  {
    "text": "kafka well it will be called pg source and then it i will add the name of the",
    "start": "1775520",
    "end": "1780559"
  },
  {
    "text": "table so i will have a topic pg source pasta then the mode i selected bulk but you",
    "start": "1780559",
    "end": "1787120"
  },
  {
    "text": "could use incremental or timestamp if you use one of those advanced methods you need also to specify which",
    "start": "1787120",
    "end": "1794320"
  },
  {
    "text": "columns are the serial number or the timestamp itself",
    "start": "1794320",
    "end": "1799919"
  },
  {
    "text": "and then one last thing that you can specify is the amount of parallel task that you",
    "start": "1799919",
    "end": "1805360"
  },
  {
    "text": "have in the connector to load the data and the good and bad news here is that",
    "start": "1805360",
    "end": "1811360"
  },
  {
    "text": "you can have at maximum one task per table",
    "start": "1811360",
    "end": "1817279"
  },
  {
    "text": "so i feel your disappointment i've told you initially that kafka is meant to",
    "start": "1817360",
    "end": "1823919"
  },
  {
    "text": "allow you to work with huge workloads and now what i'm saying is well you know you can have only one task",
    "start": "1823919",
    "end": "1830080"
  },
  {
    "text": "extracting the data and i know you have like a table generating trillion changes every second so this doesn't scale",
    "start": "1830080",
    "end": "1838240"
  },
  {
    "text": "well not entirely true because the table definition that you have here is what i",
    "start": "1838240",
    "end": "1843679"
  },
  {
    "text": "call a table you are querying the database via jdbc so there is nothing stopping you instead",
    "start": "1843679",
    "end": "1850320"
  },
  {
    "text": "of pointing to the huge table that you create to point a connector and create a",
    "start": "1850320",
    "end": "1856960"
  },
  {
    "text": "connector per partition you can create views on top of the table to limit only a certain part of the data",
    "start": "1856960",
    "end": "1863600"
  },
  {
    "text": "set to be retrieved for each connector so you can achieve parallelism even if",
    "start": "1863600",
    "end": "1868799"
  },
  {
    "text": "you don't have any uh if you just have one table in your",
    "start": "1868799",
    "end": "1873919"
  },
  {
    "text": "database so there are ways there are tricks that you can use in order to achieve a better parallelism if you have",
    "start": "1873919",
    "end": "1879519"
  },
  {
    "text": "the need for it so now let me show this in practice",
    "start": "1879519",
    "end": "1884720"
  },
  {
    "text": "i hope you can read it yes it's up there i'm connecting to a postgres database in here",
    "start": "1884720",
    "end": "1890480"
  },
  {
    "text": "and i'm creating a table called pasta timestamp which contains the creation add and modify that columns the creation",
    "start": "1890480",
    "end": "1897120"
  },
  {
    "text": "app works uh with the default now for the modified ad column i need to create a function and a",
    "start": "1897120",
    "end": "1903840"
  },
  {
    "text": "trigger that updates the column every time there is a change okay",
    "start": "1903840",
    "end": "1909120"
  },
  {
    "text": "i create the trigger now let me insert three data three rows spaghetti peneta",
    "start": "1909120",
    "end": "1915039"
  },
  {
    "text": "linguine i have the data in my source table there it is with",
    "start": "1915039",
    "end": "1920559"
  },
  {
    "text": "the non-empty creation at column now let me create the kafka connect connector",
    "start": "1920559",
    "end": "1926640"
  },
  {
    "text": "again i'm using ivan which is a way to have that in",
    "start": "1926640",
    "end": "1932159"
  },
  {
    "text": "apache kafka manage and also postgres managed so all i need to do is",
    "start": "1932159",
    "end": "1938158"
  },
  {
    "text": "create this json configuration file and pass it via cli to ivan to create the connector",
    "start": "1938240",
    "end": "1944240"
  },
  {
    "text": "i'm creating the connector pointing to pasta timestamp which is my table",
    "start": "1944240",
    "end": "1949279"
  },
  {
    "text": "does this work yes it's mold timestamp and i'm saying look that there are two columns the modify",
    "start": "1949279",
    "end": "1955279"
  },
  {
    "text": "that column and they created that column that you need to use and i'm polling the changes every two seconds in a topic",
    "start": "1955279",
    "end": "1962480"
  },
  {
    "text": "prefix called pg source and then it will be called pg source pasta timestamp",
    "start": "1962480",
    "end": "1968880"
  },
  {
    "text": "okay now let's check what's happening in kafka i'm using kcat which is a tool",
    "start": "1969039",
    "end": "1974080"
  },
  {
    "text": "that allows you to browse what's happening in a kafka topic and immediately what we see is that we",
    "start": "1974080",
    "end": "1979679"
  },
  {
    "text": "have the three rows that we have in the database also in kafka all working as expected",
    "start": "1979679",
    "end": "1985360"
  },
  {
    "text": "now let's try to do an insert tagliatelle within the next two seconds i have taliatelle in kafka",
    "start": "1985360",
    "end": "1992559"
  },
  {
    "text": "let's try to do an update i do instead of penete i use pena and now",
    "start": "1992559",
    "end": "2000799"
  },
  {
    "text": "i have depende the update also in kafka let's try to do a deletion",
    "start": "2000799",
    "end": "2008320"
  },
  {
    "text": "have the deletion and as i told you nothing happens in kafka",
    "start": "2008559",
    "end": "2014559"
  },
  {
    "text": "there is a problem here the query based approach has some limits",
    "start": "2014640",
    "end": "2021200"
  },
  {
    "start": "2019000",
    "end": "2218000"
  },
  {
    "text": "the first limit is about polling time the polling time when we when we use a",
    "start": "2021200",
    "end": "2028559"
  },
  {
    "text": "query based approach with a polling time we have to admit to the fits the first one is",
    "start": "2028559",
    "end": "2034320"
  },
  {
    "text": "do you remember the pineapple example that i told you before if changes happen",
    "start": "2034320",
    "end": "2040559"
  },
  {
    "text": "quicker than our polling interval we possibly are going to miss changes",
    "start": "2040559",
    "end": "2046080"
  },
  {
    "text": "the second thing that the polling interval the second problem about the polling interval is that we are",
    "start": "2046080",
    "end": "2051358"
  },
  {
    "text": "admitting a consistent delay between when an event is is happening in the database and when",
    "start": "2051359",
    "end": "2057760"
  },
  {
    "text": "we will keep track of it in kafka you know this could be two seconds this",
    "start": "2057760",
    "end": "2063599"
  },
  {
    "text": "could be five minutes depending on how frequently you pull the table still is always a delay for some use cases is",
    "start": "2063599",
    "end": "2070079"
  },
  {
    "text": "something acceptable in other use cases it's not another limit of the query based",
    "start": "2070079",
    "end": "2076079"
  },
  {
    "text": "approach is that we cannot really track all the things we saw with incremental one having problem with updates",
    "start": "2076079",
    "end": "2083760"
  },
  {
    "text": "if we are dealing with hard deletions well bad luck we cannot track them",
    "start": "2083760",
    "end": "2089118"
  },
  {
    "text": "then again even if we are putting kafka in the middle and we have now only kafka query",
    "start": "2089119",
    "end": "2094560"
  },
  {
    "text": "in the database still we have a load on the database we are still continuously",
    "start": "2094560",
    "end": "2099599"
  },
  {
    "text": "polling the database itself and even more this is kind of an",
    "start": "2099599",
    "end": "2105760"
  },
  {
    "text": "advanced topic what if the serial number that we thought it was always increasing is not always",
    "start": "2105760",
    "end": "2112640"
  },
  {
    "text": "increasing anymore if there is a dba that is doing a change",
    "start": "2112640",
    "end": "2117680"
  },
  {
    "text": "without telling us well all this query based approach with advanced ids",
    "start": "2117680",
    "end": "2123520"
  },
  {
    "text": "starts falling over but the basic problem of the query based",
    "start": "2123520",
    "end": "2129599"
  },
  {
    "text": "approach is that not for the bulk mode but for all the other smarter version we always",
    "start": "2129599",
    "end": "2135599"
  },
  {
    "text": "require some extra field to be present in the table if we want to use the incremental well",
    "start": "2135599",
    "end": "2141599"
  },
  {
    "text": "we need to have an always increasing id if we want to you use the timestamp based approach we need to have",
    "start": "2141599",
    "end": "2148000"
  },
  {
    "text": "timestamps and related triggers if we want to track the leads we need to have soft",
    "start": "2148000",
    "end": "2153839"
  },
  {
    "text": "deletes as i said before this can be possible or not depending on your use case",
    "start": "2153839",
    "end": "2160560"
  },
  {
    "text": "so we are 34 minutes in the talk who is happy with the solution",
    "start": "2160560",
    "end": "2168240"
  },
  {
    "text": "good because i have other content to tell you okay let's move on then in order we are still with the basic",
    "start": "2169119",
    "end": "2175599"
  },
  {
    "text": "problem we have our data in kafka and we don't seem to have a reliable option to take the data out of kafka look that the",
    "start": "2175599",
    "end": "2182480"
  },
  {
    "text": "query based approach is what etl tools have been using for long and long time",
    "start": "2182480",
    "end": "2188480"
  },
  {
    "text": "what is the change here etl tools have been doing batching have been extracting yesterday data they could start",
    "start": "2188480",
    "end": "2195200"
  },
  {
    "text": "extracting yesterday data today at 1 am just to be sure that all yesterday data was sorted",
    "start": "2195200",
    "end": "2202240"
  },
  {
    "text": "now what we are doing here is we are streaming so we cannot wait too much we need to",
    "start": "2202240",
    "end": "2208400"
  },
  {
    "text": "more or less track what's happening now and the query based approach i don't think it's the right approach",
    "start": "2208400",
    "end": "2214560"
  },
  {
    "text": "in order to solve this problem we need to change the paradigm we need to start using a log-based approach",
    "start": "2214560",
    "end": "2223200"
  },
  {
    "start": "2218000",
    "end": "2336000"
  },
  {
    "text": "who is happy with the solution are you wondering why i'm talking about the log base approach since my our",
    "start": "2224160",
    "end": "2231440"
  },
  {
    "text": "source data technology is still a database well",
    "start": "2231440",
    "end": "2236560"
  },
  {
    "text": "i have a news for you i believe a lot of you will already know this a database most of the time is a",
    "start": "2236560",
    "end": "2243440"
  },
  {
    "text": "database but also a log when you write changes into a database what the database does is it rises the",
    "start": "2243440",
    "end": "2250320"
  },
  {
    "text": "change in the internal state but also writes the change into log",
    "start": "2250320",
    "end": "2256400"
  },
  {
    "text": "do databases do that well because it's their way to prevent or to save the",
    "start": "2256400",
    "end": "2261839"
  },
  {
    "text": "their life if the state of the database takes fire so you know i've write an insert",
    "start": "2261839",
    "end": "2268560"
  },
  {
    "text": "the database received the insert says okay write it in the internal state but just to be sure let me write it also in",
    "start": "2268560",
    "end": "2275200"
  },
  {
    "text": "a log this is the way that a lot of databases are replicating the state",
    "start": "2275200",
    "end": "2281839"
  },
  {
    "text": "within the same technology if you think about postgres it's doing exactly this is writing to a log called the wall log",
    "start": "2281839",
    "end": "2288800"
  },
  {
    "text": "in order to do the replica to create a replica in another database this is a standard practice and i",
    "start": "2288800",
    "end": "2294640"
  },
  {
    "text": "mentioned postgres but it's not only about postgres if you check a lot of different technology they will",
    "start": "2294640",
    "end": "2301119"
  },
  {
    "text": "use a different name but it's always the same thing they write in their internal state",
    "start": "2301119",
    "end": "2306480"
  },
  {
    "text": "and in a file just to be sure to be able to replicate the states somewhere else",
    "start": "2306480",
    "end": "2313040"
  },
  {
    "text": "so it would be beautiful if we could use the information in the log because we",
    "start": "2313040",
    "end": "2318720"
  },
  {
    "text": "are sure that since the database needs to track all the changes itself in the law if we if we are able to read those",
    "start": "2318720",
    "end": "2325359"
  },
  {
    "text": "changes we are not going to miss a thing we are going to track all the changes",
    "start": "2325359",
    "end": "2330880"
  },
  {
    "text": "and i'm happy to tell you that we can do that with another type of connector which is called the division connector",
    "start": "2330880",
    "end": "2338079"
  },
  {
    "start": "2336000",
    "end": "2433000"
  },
  {
    "text": "the besiem is an open source project which is aimed at solving exactly this",
    "start": "2338079",
    "end": "2343680"
  },
  {
    "text": "problem tracking all the database changes happening by reading in the log or",
    "start": "2343680",
    "end": "2348880"
  },
  {
    "text": "depending on the technology we're reading in some specific views which contain the list of changes",
    "start": "2348880",
    "end": "2354880"
  },
  {
    "text": "and moving those changes into kafka with a standard format",
    "start": "2354880",
    "end": "2360320"
  },
  {
    "text": "how do you set it up it's again not too complex for postgres this is the case for",
    "start": "2360320",
    "end": "2366240"
  },
  {
    "text": "postgres you need to create a replication slot which is",
    "start": "2366240",
    "end": "2371280"
  },
  {
    "text": "if you ask your dba what he will or she will do to replicate the state from a primary node to a replica node",
    "start": "2371280",
    "end": "2379200"
  },
  {
    "text": "then once you create the replication slot you will have to define a publication",
    "start": "2379200",
    "end": "2384320"
  },
  {
    "text": "which is basically telling out of all this 200 tables that you have in the source database i care only about those",
    "start": "2384320",
    "end": "2391040"
  },
  {
    "text": "five and then you say the replicated entity the replicated",
    "start": "2391040",
    "end": "2396640"
  },
  {
    "text": "entity defines the columns for which you want to track the changes more or less",
    "start": "2396640",
    "end": "2402000"
  },
  {
    "text": "what is this stuff well i don't know how many of you are dbas but if you go to your dba and talk",
    "start": "2402000",
    "end": "2408000"
  },
  {
    "text": "this language there will be the happiest person in the room because this is what they have been",
    "start": "2408000",
    "end": "2414000"
  },
  {
    "text": "using to replicate the state between any service and any replica service so",
    "start": "2414000",
    "end": "2420560"
  },
  {
    "text": "you are including kafka in the picture and you're not making a special case for kafka you are allowing the dba to use",
    "start": "2420560",
    "end": "2426960"
  },
  {
    "text": "their tools in order to replicate the data to kafka so let's see now how we can",
    "start": "2426960",
    "end": "2434880"
  },
  {
    "start": "2433000",
    "end": "2552000"
  },
  {
    "text": "set up the division connector again it's a json configuration file as you can",
    "start": "2434880",
    "end": "2439920"
  },
  {
    "text": "tell it's probably a little bit longer than the other one so start with the name",
    "start": "2439920",
    "end": "2445920"
  },
  {
    "text": "then we define the class it's for postgres in this case but it's the",
    "start": "2445920",
    "end": "2451599"
  },
  {
    "text": "dibysium connector for postgres and then we define where is postgres hostname port user password database name and ssl",
    "start": "2451599",
    "end": "2459599"
  },
  {
    "text": "mode what else here we are speaking the language of the",
    "start": "2459599",
    "end": "2465440"
  },
  {
    "text": "dbas we're saying well start from the slot the slot name is the replication slot",
    "start": "2465440",
    "end": "2470960"
  },
  {
    "text": "name we can allow the dba to create the replication slot up front or we can allow if the user in the database has",
    "start": "2470960",
    "end": "2477920"
  },
  {
    "text": "enough privileges to create kafka connect to create the",
    "start": "2477920",
    "end": "2483680"
  },
  {
    "text": "replication slot itself on the first run we define the publication name",
    "start": "2483839",
    "end": "2489119"
  },
  {
    "text": "we define the plugin name which is basically saying to the postgres database which language it has to use in",
    "start": "2489119",
    "end": "2494880"
  },
  {
    "text": "order to write the changes down and then we define which table to include within the publication",
    "start": "2494880",
    "end": "2503200"
  },
  {
    "text": "then there is the most interesting parameter here database server name what is this",
    "start": "2503200",
    "end": "2508720"
  },
  {
    "text": "it's the same topic prefix parameter that we had in the jdbc connector just with a strange name",
    "start": "2508720",
    "end": "2515359"
  },
  {
    "text": "and then again maximum amount of parallel task in the museum you can have only one",
    "start": "2515359",
    "end": "2521920"
  },
  {
    "text": "parallel task per publication again i see your faces this is not going",
    "start": "2521920",
    "end": "2528720"
  },
  {
    "text": "to scale francesco come on you need to do better again it's",
    "start": "2528720",
    "end": "2534079"
  },
  {
    "text": "what you define in a publication you can have a publication per petition",
    "start": "2534079",
    "end": "2539440"
  },
  {
    "text": "you can split publications in order to have one publication only taking care of insert another publication only taking",
    "start": "2539440",
    "end": "2545520"
  },
  {
    "text": "care of updates another publication only taking care of deletions",
    "start": "2545520",
    "end": "2550720"
  },
  {
    "text": "in postgres this example in postgres 15 you will be able to create a publication",
    "start": "2550720",
    "end": "2556800"
  },
  {
    "text": "with a where clause so even if you don't have partition in the source table you can create several",
    "start": "2556800",
    "end": "2562480"
  },
  {
    "text": "publication for subsets of your data so you can scale this up you can achieve",
    "start": "2562480",
    "end": "2567760"
  },
  {
    "text": "better parallelism if you need again let me show you this in practice",
    "start": "2567760",
    "end": "2574880"
  },
  {
    "text": "i'm connecting to the postgres database and i'm creating a simple table always",
    "start": "2574880",
    "end": "2580160"
  },
  {
    "text": "the pasta table with a code and a name of the pasta i'm not creating",
    "start": "2580160",
    "end": "2585680"
  },
  {
    "text": "any trigger i don't have any additional columns i'm inserting three rows the",
    "start": "2585680",
    "end": "2591040"
  },
  {
    "text": "only thing that i'm doing here is replicate the entity identity is full so",
    "start": "2591040",
    "end": "2596160"
  },
  {
    "text": "i'm tracking all the columns the columns appearing in all the changes let me select from pasta and here i have",
    "start": "2596160",
    "end": "2602960"
  },
  {
    "text": "my three rows all good let me split and create the kafka connect connector",
    "start": "2602960",
    "end": "2610560"
  },
  {
    "text": "now the kafka connect connector this it's a little bit more complex is",
    "start": "2611359",
    "end": "2617200"
  },
  {
    "text": "all the settings that i told you before i'm doing an extra step by default if you create such a",
    "start": "2617200",
    "end": "2623599"
  },
  {
    "text": "connector it will stream the changes in json format",
    "start": "2623599",
    "end": "2629680"
  },
  {
    "text": "json is beautiful i believe we all agree but it has one benefit and one problem",
    "start": "2629680",
    "end": "2636079"
  },
  {
    "text": "jason is beautiful because you can read every json message and json is problematic because you can",
    "start": "2636079",
    "end": "2643680"
  },
  {
    "text": "read every json message with json the benefit and the problem is that it contains for each field the",
    "start": "2643680",
    "end": "2649920"
  },
  {
    "text": "field name and the field value which is beautiful because you can read it it's a problematic if you have huge",
    "start": "2649920",
    "end": "2655680"
  },
  {
    "text": "amount of data because the information that you contain is huge the packet that you are",
    "start": "2655680",
    "end": "2661440"
  },
  {
    "text": "generated is huge compared to the amount of information that you are passing through so what i'm doing here is",
    "start": "2661440",
    "end": "2668240"
  },
  {
    "text": "i'm using a value and a key converter to convert the json format into avro",
    "start": "2668240",
    "end": "2675520"
  },
  {
    "text": "avra is a way of taking the beautiful json taking out the schema",
    "start": "2675520",
    "end": "2680960"
  },
  {
    "text": "using sending the schema to a tool called schema registry that will contain that for us",
    "start": "2680960",
    "end": "2686400"
  },
  {
    "text": "and then only crunching the field values and passing only the field values over the wire so you have much better",
    "start": "2686400",
    "end": "2693040"
  },
  {
    "text": "throughput okay now let's create this",
    "start": "2693040",
    "end": "2699839"
  },
  {
    "text": "connector let me move on and use k-cut to browse what's in the",
    "start": "2699839",
    "end": "2705200"
  },
  {
    "text": "kafka topic okay what we have in the kafka topic if we",
    "start": "2705200",
    "end": "2711200"
  },
  {
    "text": "see here we have the same three rows we have spaghetti here panetta",
    "start": "2711200",
    "end": "2718319"
  },
  {
    "text": "and the same three rows over here spaghetti panetta linguine but check the status now we don't have",
    "start": "2718319",
    "end": "2724800"
  },
  {
    "text": "just the two columns because we now with the bezier we are looking at what is written in the log we",
    "start": "2724800",
    "end": "2731920"
  },
  {
    "text": "have a way richer set of information we have the timestamp of each operation",
    "start": "2731920",
    "end": "2737440"
  },
  {
    "text": "we have the logical sequence number the type of transaction",
    "start": "2737440",
    "end": "2742640"
  },
  {
    "text": "we have way way more information that we can propagate to downstream technologies if needed or we can just scrape all the",
    "start": "2742640",
    "end": "2749359"
  },
  {
    "text": "extra information and just pass the two columns if we want but still we have a lot more",
    "start": "2749359",
    "end": "2755520"
  },
  {
    "text": "that we can do so what's next",
    "start": "2755520",
    "end": "2760880"
  },
  {
    "text": "let's try to do an update or first of all an insert if we do an insert here",
    "start": "2760880",
    "end": "2766640"
  },
  {
    "text": "is this going to work probably insert let's insert farfalle we have",
    "start": "2766640",
    "end": "2773599"
  },
  {
    "text": "farfalle immediately available here we don't have to wait two seconds or five",
    "start": "2773599",
    "end": "2778640"
  },
  {
    "text": "seconds of pulling time as soon as the change is written in the log the visum will will be pulled will be",
    "start": "2778640",
    "end": "2786480"
  },
  {
    "text": "pushed the change and kafka were received",
    "start": "2786480",
    "end": "2791119"
  },
  {
    "text": "let's try to do an update",
    "start": "2792560",
    "end": "2796560"
  },
  {
    "text": "okay let's check the update now what's beautiful in here is that we have the situation before",
    "start": "2798800",
    "end": "2805040"
  },
  {
    "text": "before it was panetta and after is penelecia so we don't only get the",
    "start": "2805040",
    "end": "2810720"
  },
  {
    "text": "new status we also get what was before it's very interesting because we can propagate these changes to a target",
    "start": "2810720",
    "end": "2817440"
  },
  {
    "text": "system so we have all the",
    "start": "2817440",
    "end": "2823200"
  },
  {
    "text": "updates here and we have also the operation as update now let's delete we can track the leads",
    "start": "2823200",
    "end": "2831280"
  },
  {
    "text": "as well because we are looking in the log and the delete will be written in there we",
    "start": "2831280",
    "end": "2836800"
  },
  {
    "text": "have what is called tomstone message with the before value it was the spaghetti and the after value is a",
    "start": "2836800",
    "end": "2843760"
  },
  {
    "text": "beautiful now this is the standard way in kafka to say you deleted a row",
    "start": "2843760",
    "end": "2850000"
  },
  {
    "text": "let's now do one step more i told you that kafka usually is just a middle layer so as we saw until now we took the",
    "start": "2851599",
    "end": "2859359"
  },
  {
    "text": "data the changes from postgres into kafka let's do one step more let's try to bridge technologies by creating a",
    "start": "2859359",
    "end": "2867359"
  },
  {
    "text": "sync kafka connector taking the data from kafka to for example mysql",
    "start": "2867359",
    "end": "2873839"
  },
  {
    "text": "is this working let's try another click yes okay so we have",
    "start": "2876559",
    "end": "2883280"
  },
  {
    "text": "we are now creating a sync to my sequel it's always a different uh connector",
    "start": "2883280",
    "end": "2889440"
  },
  {
    "text": "jdbc sync connector taking the data to my sequel let's not now try to connect",
    "start": "2889440",
    "end": "2894480"
  },
  {
    "text": "to my sql i didn't do anything on the my sql side rather after creating the instance but",
    "start": "2894480",
    "end": "2901040"
  },
  {
    "text": "still by default with all the parameters that i set i have a table created for me called password sql with the same rows",
    "start": "2901040",
    "end": "2907920"
  },
  {
    "text": "that i have in postgres what i did here i have three rows in",
    "start": "2907920",
    "end": "2913200"
  },
  {
    "text": "postgres i have four rows in my in my sequel because i have the soft deletions",
    "start": "2913200",
    "end": "2918319"
  },
  {
    "text": "now if i do an insert i have that immediately available here if i do up an update",
    "start": "2918319",
    "end": "2924319"
  },
  {
    "text": "immediately available also in my sql i don't have to do anything kafka is able",
    "start": "2924319",
    "end": "2929760"
  },
  {
    "text": "to flow the data now",
    "start": "2929760",
    "end": "2934960"
  },
  {
    "text": "even the deletion is fusiloni is now deleted all correct",
    "start": "2935040",
    "end": "2940160"
  },
  {
    "text": "i want to do one step more i want to help you with one last strong message",
    "start": "2940160",
    "end": "2947599"
  },
  {
    "text": "i don't know for how many of you this will be a news but you cannot cook all the type of pasta",
    "start": "2947599",
    "end": "2954319"
  },
  {
    "text": "for 30 minutes every type of pasta has its own minutes so what i'm going to share with",
    "start": "2954319",
    "end": "2961200"
  },
  {
    "text": "you is in my postgres database i will add the column with the cooking minutes and i will set precise cooking minutes",
    "start": "2961200",
    "end": "2968000"
  },
  {
    "text": "for each of the type of pasta",
    "start": "2968000",
    "end": "2972240"
  },
  {
    "text": "i did that i didn't have to do anything i enabled in my connectors the fact that i could",
    "start": "2973760",
    "end": "2980800"
  },
  {
    "text": "change the shape of my data in the source table and reflect that change in the target table",
    "start": "2980800",
    "end": "2986640"
  },
  {
    "text": "and i have the cookie minutes available in my sql this can sound interesting or scary",
    "start": "2986640",
    "end": "2992960"
  },
  {
    "text": "depending on the use case it's an opportunity that you have in your kafka connector you can enable it",
    "start": "2992960",
    "end": "2998559"
  },
  {
    "text": "or not so let's review now as final thing the jdbc limits",
    "start": "2998559",
    "end": "3005040"
  },
  {
    "start": "3004000",
    "end": "3216000"
  },
  {
    "text": "or the query based limits and see how the business solves them",
    "start": "3005040",
    "end": "3010319"
  },
  {
    "text": "first of all updates and deletion we now can track all the changes happening because they",
    "start": "3010319",
    "end": "3015359"
  },
  {
    "text": "will be written in the log and we are sourcing all events from the log we don't have",
    "start": "3015359",
    "end": "3020559"
  },
  {
    "text": "the concept of polling time anymore we are in near real time we don't have to worry about fast events we can track",
    "start": "3020559",
    "end": "3027440"
  },
  {
    "text": "everything out of order events",
    "start": "3027440",
    "end": "3032640"
  },
  {
    "text": "it's a topic that we didn't cover but we read from the log so all events even",
    "start": "3032640",
    "end": "3038319"
  },
  {
    "text": "if they are out of order they will be written in the log and we can source them lot on the database we still have a",
    "start": "3038319",
    "end": "3044800"
  },
  {
    "text": "minimal load on the database but it's the same type of load that a dba would",
    "start": "3044800",
    "end": "3051760"
  },
  {
    "text": "expect if they were replicating the state to another postgres database to another database of the same type so we",
    "start": "3051760",
    "end": "3057839"
  },
  {
    "text": "are not doing something extra something new for them for kafka",
    "start": "3057839",
    "end": "3063040"
  },
  {
    "text": "and the other bit is required so extra fails we don't require any field we can work with any type of table of any shape",
    "start": "3063040",
    "end": "3071520"
  },
  {
    "text": "and this is a big plus i want to leave you with just a small table",
    "start": "3071520",
    "end": "3077359"
  },
  {
    "text": "that allows you overall comparison we have the query in bulk mode with aquarium",
    "start": "3077359",
    "end": "3082400"
  },
  {
    "text": "bulk mode we can track all the things more or less with a little asterisk",
    "start": "3082400",
    "end": "3087440"
  },
  {
    "text": "if changes are fast enough that's a problem",
    "start": "3087440",
    "end": "3092559"
  },
  {
    "text": "even more the bulk mode is really heavy because we are moving every polling interval all the rows from the database",
    "start": "3092559",
    "end": "3099119"
  },
  {
    "text": "to kafka then we have the timestamp or incremental mode for which",
    "start": "3099119",
    "end": "3104319"
  },
  {
    "text": "yeah we can track insert and maybe updates but deletions well bad luck that's not going to work",
    "start": "3104319",
    "end": "3110400"
  },
  {
    "text": "and then we have the log base approach with the busium that allows us to track everything and it really solves the day",
    "start": "3110400",
    "end": "3118480"
  },
  {
    "text": "i want to now to leave you with some references that you will find scanning this qr code",
    "start": "3118480",
    "end": "3125760"
  },
  {
    "text": "trust me it goes to my website basically you find links to",
    "start": "3125760",
    "end": "3130800"
  },
  {
    "text": "kafka documentation to the business documentation you find also two blog",
    "start": "3130800",
    "end": "3136640"
  },
  {
    "text": "posts that i wrote about showing you how you can replicate the examples that i was showing you today",
    "start": "3136640",
    "end": "3142160"
  },
  {
    "text": "both from the jb dbc and from the division point of view i was light",
    "start": "3142160",
    "end": "3148720"
  },
  {
    "text": "on the limits of the the query based approach the jdbc connector if you want",
    "start": "3148720",
    "end": "3153760"
  },
  {
    "text": "to dig more into it two weeks ago i was talking at kafka summit a conference dedicated to kafka and i had to talk the",
    "start": "3153760",
    "end": "3161599"
  },
  {
    "text": "about the jdbc connector what can go wrong so i had 45 minutes just dedicated",
    "start": "3161599",
    "end": "3167200"
  },
  {
    "text": "on showing what can go wrong with the jdbc connector and how the business solves all the problems",
    "start": "3167200",
    "end": "3172480"
  },
  {
    "text": "the last link is ivan d'teo is the company that they work for we offer the",
    "start": "3172480",
    "end": "3178160"
  },
  {
    "text": "kafka postgres my sequel flink click house a huge amount of open source data",
    "start": "3178160",
    "end": "3184000"
  },
  {
    "text": "platform as available as managed service please use them uh try to create",
    "start": "3184000",
    "end": "3189920"
  },
  {
    "text": "something try to destroy something let me know how it goes i'm very interested in your use cases",
    "start": "3189920",
    "end": "3197119"
  },
  {
    "text": "that's all for now i hope you enjoyed a little bit of kafka a little bit of database a lot of italian things help",
    "start": "3197119",
    "end": "3203280"
  },
  {
    "text": "you i change your mind on the way that you can integrate kafka within your database and hopefully also",
    "start": "3203280",
    "end": "3209280"
  },
  {
    "text": "on the pineapple on pizza if you have any questions i'm here for them",
    "start": "3209280",
    "end": "3214960"
  },
  {
    "text": "thank you very much",
    "start": "3214960",
    "end": "3218440"
  }
]