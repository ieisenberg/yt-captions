[
  {
    "text": "my name is Jennifer Lansky I work as a solution architect for Couchbase here in",
    "start": "9590",
    "end": "15650"
  },
  {
    "text": "Australia I also have a an open source",
    "start": "15650",
    "end": "21160"
  },
  {
    "text": "project that just made the Apache Incubator and most of my most of my life",
    "start": "21160",
    "end": "27619"
  },
  {
    "text": "for the last seven years were around dealing with data mostly big data and",
    "start": "27619",
    "end": "33350"
  },
  {
    "text": "and working on analytics projects but that's not what this talk is all about",
    "start": "33350",
    "end": "40370"
  },
  {
    "text": "so we're gonna talk about something slightly different we're gonna talk about micro services and when their data",
    "start": "40370",
    "end": "46789"
  },
  {
    "text": "is getting to be a little bit too big and how we can solve that so famous last",
    "start": "46789",
    "end": "53929"
  },
  {
    "text": "words let's build a micro service if if",
    "start": "53929",
    "end": "59899"
  },
  {
    "text": "you've been a part of this industry for the last few years and you have a pulse at a certain point in time you had to",
    "start": "59899",
    "end": "65540"
  },
  {
    "text": "say that to your boss let's build a micro service that's the way to go we'll build a micro service it will be",
    "start": "65540",
    "end": "72470"
  },
  {
    "text": "you know well-defined it will be",
    "start": "72470",
    "end": "77719"
  },
  {
    "text": "autonomous and it will be scalable so we have our micro service and our micro",
    "start": "77719",
    "end": "84350"
  },
  {
    "text": "service even persist data to a datastore and it's scalable you know has no dependencies autonomous well-defined",
    "start": "84350",
    "end": "92469"
  },
  {
    "text": "and even our datastore is hopefully scalable if your datastore is not",
    "start": "92469",
    "end": "98119"
  },
  {
    "text": "scalable talk to me after this I'll hook you up but we can scale out right we can have a",
    "start": "98119",
    "end": "107240"
  },
  {
    "text": "load balancer Microsoft service is very scalable and we're happy and we",
    "start": "107240",
    "end": "114409"
  },
  {
    "text": "celebrate right that's how it begins we're all very helpful about our micro",
    "start": "114409",
    "end": "120170"
  },
  {
    "text": "service it's a greenfield project or at least a Greenfield part of an existing project",
    "start": "120170",
    "end": "125240"
  },
  {
    "text": "and everything is good until that moment",
    "start": "125240",
    "end": "130399"
  },
  {
    "text": "of course where we have new requirements like another micro service that needs the data that we just processed in our",
    "start": "130399",
    "end": "136400"
  },
  {
    "text": "micro service or we need to put our data in another datastore right so we end up",
    "start": "136400",
    "end": "141860"
  },
  {
    "text": "with like this and then we're not really",
    "start": "141860",
    "end": "147780"
  },
  {
    "text": "happy and we don't celebrate anymore we need to go back and start working again",
    "start": "147780",
    "end": "153750"
  },
  {
    "text": "now a lot of systems get to this state",
    "start": "153750",
    "end": "159319"
  },
  {
    "text": "quite rapidly right we have multiple services that we need to hand off the data to we have additional data stores",
    "start": "159319",
    "end": "165930"
  },
  {
    "text": "like a search engine or our analytic store it HDFS or s3 or as your data like",
    "start": "165930",
    "end": "174000"
  },
  {
    "text": "thingy and we get to the point where there's a lot of load on our micro",
    "start": "174000",
    "end": "180989"
  },
  {
    "text": "services and that that is not an inbound load that's an outbound load where we",
    "start": "180989",
    "end": "186750"
  },
  {
    "text": "have to serve the data that we just processed in this micro service to",
    "start": "186750",
    "end": "192359"
  },
  {
    "text": "multiple consumers and there are various ways that we can you know deal with this",
    "start": "192359",
    "end": "200120"
  },
  {
    "text": "right so we have our micro service right now it is no longer autonomous it has",
    "start": "201469",
    "end": "208799"
  },
  {
    "text": "dependencies and we're not scalable anymore so different ways that we can",
    "start": "208799",
    "end": "218430"
  },
  {
    "text": "deal with this one of those is using our database as the integration point that",
    "start": "218430",
    "end": "225239"
  },
  {
    "text": "seems to be a pattern a lot of micro services take these days you have our",
    "start": "225239",
    "end": "231030"
  },
  {
    "text": "secondary service service be acquiring the database for the same database that",
    "start": "231030",
    "end": "237000"
  },
  {
    "text": "our micro service just processed and put there and we can have another micro",
    "start": "237000",
    "end": "244650"
  },
  {
    "text": "service that just kind of ETLs or even just scoops the data into our search engine and additional stores and there",
    "start": "244650",
    "end": "253199"
  },
  {
    "text": "are a lot of situation like that where you see those kind of loads hitting the database and that is not necessarily the",
    "start": "253199",
    "end": "263849"
  },
  {
    "text": "right use for your database don't get me wrong I earn my living by selling",
    "start": "263849",
    "end": "269880"
  },
  {
    "text": "database licenses but still for some things it's just too much for the",
    "start": "269880",
    "end": "275430"
  },
  {
    "text": "database are they bases usually have an additional overhead for querying for for indexing",
    "start": "275430",
    "end": "282990"
  },
  {
    "text": "for those kind of things and it's massive misuse of our database to use it",
    "start": "282990",
    "end": "288750"
  },
  {
    "text": "just as an integration point right and",
    "start": "288750",
    "end": "296330"
  },
  {
    "text": "another kind of aspect of that is that databases allow us to do things like",
    "start": "296330",
    "end": "302879"
  },
  {
    "text": "change your data right we keep update them and in that process you can miss",
    "start": "302879",
    "end": "309300"
  },
  {
    "text": "some of the updates our additional micro services might not get the correct version or the initial version of the",
    "start": "309300",
    "end": "315000"
  },
  {
    "text": "data they might me some of the change changes along the way that's not a",
    "start": "315000",
    "end": "320159"
  },
  {
    "text": "reliable way to store your data so another way to think about this whole",
    "start": "320159",
    "end": "327629"
  },
  {
    "text": "problem is to think about it like this what if we could have a data stream and",
    "start": "327629",
    "end": "336110"
  },
  {
    "text": "have different consumers running on top of that so this micro service can read",
    "start": "336110",
    "end": "342150"
  },
  {
    "text": "the data as it streams through the system other micro-services can get the",
    "start": "342150",
    "end": "347849"
  },
  {
    "text": "same data from the same data stream and we can even get the data either directly",
    "start": "347849",
    "end": "354080"
  },
  {
    "text": "to some of those stores because some of them need the raw data if you're doing",
    "start": "354080",
    "end": "359250"
  },
  {
    "text": "analytics if you're doing you know clickstream analysis on on Hadoop you",
    "start": "359250",
    "end": "365159"
  },
  {
    "text": "don't want the data to be processed by a macro service before you're doing your",
    "start": "365159",
    "end": "370319"
  },
  {
    "text": "analytics work so you can do that but you can also have your microservices processing the data storing it in the",
    "start": "370319",
    "end": "376830"
  },
  {
    "text": "database for what a database should be used right our data as it should be",
    "start": "376830",
    "end": "383550"
  },
  {
    "text": "stored and reflects the point in time that we're looking at and if you need to",
    "start": "383550",
    "end": "389360"
  },
  {
    "text": "integrate one data store with another you can still go through the same stream",
    "start": "389360",
    "end": "395810"
  },
  {
    "text": "so obviously this is not something that just our little micro Service had to",
    "start": "395810",
    "end": "402479"
  },
  {
    "text": "deal with it happened for to a lot of companies and and in different scales and at a certain point in time it",
    "start": "402479",
    "end": "410070"
  },
  {
    "text": "interlinking so LinkedIn around 2010 hit",
    "start": "410070",
    "end": "415560"
  },
  {
    "text": "a very similar problem and I'm gonna show you a couple of diagrams coming from LinkedIn and the problem that they",
    "start": "415560",
    "end": "421020"
  },
  {
    "text": "dealt with and the solution that they had was Apache kofta an Apache kafka is",
    "start": "421020",
    "end": "427920"
  },
  {
    "text": "a transaction log system I'm going to continue and repeat the words",
    "start": "427920",
    "end": "434550"
  },
  {
    "text": "transaction log and that's because transaction log usually is something",
    "start": "434550",
    "end": "440280"
  },
  {
    "text": "that we use in another part of the system which one is that someone said the relational database",
    "start": "440280",
    "end": "447230"
  },
  {
    "text": "databases I work for a none relational database we also have a transaction log",
    "start": "447230",
    "end": "453720"
  },
  {
    "text": "transaction log is the way that databases usually store their data okay",
    "start": "453720",
    "end": "458790"
  },
  {
    "text": "so we're talking about a pub/sub system that is based on the same concept the",
    "start": "458790",
    "end": "465300"
  },
  {
    "text": "same low level concept as databases do and it's very important in Kafka because",
    "start": "465300",
    "end": "470670"
  },
  {
    "text": "capcom has some of those attributes the databases have but it is designed to be",
    "start": "470670",
    "end": "479370"
  },
  {
    "text": "a pop subsystem a messaging platform a streaming platform if you may its",
    "start": "479370",
    "end": "485640"
  },
  {
    "text": "distributed it works in very large scale and as I said it was developed by",
    "start": "485640",
    "end": "491760"
  },
  {
    "text": "LinkedIn it is now an Apache open source project and a very successful one it's a",
    "start": "491760",
    "end": "498240"
  },
  {
    "text": "top-level project a lot of contributors a lot of organizations are using it at scale and you also have confluent which",
    "start": "498240",
    "end": "506280"
  },
  {
    "text": "is a commercial company supporting it so this is LinkedIn around 2010 and",
    "start": "506280",
    "end": "515030"
  },
  {
    "text": "LinkedIn found himself in a very similar situation to our micro services in a",
    "start": "515030",
    "end": "521190"
  },
  {
    "text": "larger scale right they had all these systems some of them are what we now",
    "start": "521190",
    "end": "526290"
  },
  {
    "text": "call micro services for mainly marketing",
    "start": "526290",
    "end": "532110"
  },
  {
    "text": "reasons but some of them are even data stores and what they found is that they",
    "start": "532110",
    "end": "538680"
  },
  {
    "text": "have all those integration points between the different parts of LinkedIn it's very hard to go",
    "start": "538680",
    "end": "544020"
  },
  {
    "text": "it's very hard to understand where data comes from and where it goes and there",
    "start": "544020",
    "end": "549120"
  },
  {
    "text": "are some additional attributes that are missing okay so they replace this mess",
    "start": "549120",
    "end": "557209"
  },
  {
    "text": "with this streaming data from different",
    "start": "557209",
    "end": "562740"
  },
  {
    "text": "systems into Kafka and having other systems like their their social graph",
    "start": "562740",
    "end": "569870"
  },
  {
    "text": "Hadoop and so on picking the data from Kafka this is the scale that Kafka was",
    "start": "569870",
    "end": "576300"
  },
  {
    "text": "designed to work and this is what LinkedIn did and they found along the",
    "start": "576300",
    "end": "581610"
  },
  {
    "text": "way some additional additional benefits as I said Kafka is a transaction log",
    "start": "581610",
    "end": "588410"
  },
  {
    "text": "based system and it's a persistent store so your data is stored there and",
    "start": "588410",
    "end": "595920"
  },
  {
    "text": "persisted for a certain period of time that you can configure one of the I will",
    "start": "595920",
    "end": "602790"
  },
  {
    "text": "treat it as an urban legend because I heard it a couple of times but I couldn't find anywhere in writing from",
    "start": "602790",
    "end": "608070"
  },
  {
    "text": "LinkedIn that this actually happened but there is an urban legend that LinkedIn had a crush which brought LinkedIn",
    "start": "608070",
    "end": "616980"
  },
  {
    "text": "completely down and what they did they brought their server our servers up and",
    "start": "616980",
    "end": "622320"
  },
  {
    "text": "what they could do using Kafka was to replay LinkedIn up to the point where it crashed and actually debug that on",
    "start": "622320",
    "end": "629100"
  },
  {
    "text": "addict on a dev environment so there are a lot of benefits that along the way LinkedIn found in Kafka and once we go",
    "start": "629100",
    "end": "639540"
  },
  {
    "text": "over how Kafka works and some of the attributes it will be very clear how that can help different systems so this",
    "start": "639540",
    "end": "645510"
  },
  {
    "text": "is basically Kafka metal at a very high level we have producers and producers",
    "start": "645510",
    "end": "652380"
  },
  {
    "text": "talk to a Kafka cluster made out of brokers and on the other side we have",
    "start": "652380",
    "end": "657870"
  },
  {
    "text": "consumers and our producers write the",
    "start": "657870",
    "end": "664529"
  },
  {
    "text": "data into topics topics are basically our streams of data inside our Kafka",
    "start": "664529",
    "end": "672630"
  },
  {
    "text": "clusters and they are made out of partitioned",
    "start": "672630",
    "end": "677850"
  },
  {
    "text": "transaction logs again I'm gonna repeat that it's a transaction log just like our database and those partitions can be",
    "start": "677850",
    "end": "687810"
  },
  {
    "text": "distributed across our cluster so we can scale out and they can also and should",
    "start": "687810",
    "end": "693810"
  },
  {
    "text": "be replicated so our Kafka cluster will be fault tolerant and our producers are quite",
    "start": "693810",
    "end": "704760"
  },
  {
    "text": "simple this is an example of the Java API I'm gonna use the Java API in this talk mainly because some of the more",
    "start": "704760",
    "end": "711060"
  },
  {
    "text": "interesting bits are not available in other languages at least not yet so",
    "start": "711060",
    "end": "717090"
  },
  {
    "text": "producers write directly into those partitions in the topic right we have",
    "start": "717090",
    "end": "726330"
  },
  {
    "text": "our producer Kafka producer when we initialize it we give it some properties",
    "start": "726330",
    "end": "731670"
  },
  {
    "text": "like a list of servers to connect to",
    "start": "731670",
    "end": "738200"
  },
  {
    "text": "timeouts and so on and we can create a producer record which is basically a key",
    "start": "738200",
    "end": "745200"
  },
  {
    "text": "value record so Kafka holds key values and we just use the producer send now as",
    "start": "745200",
    "end": "754740"
  },
  {
    "text": "I said the producer writes directly to the partition and this is being done based on the key by default what the",
    "start": "754740",
    "end": "763620"
  },
  {
    "text": "producer does it uses the default partitioner that just hashes the key and get a partition which is within the",
    "start": "763620",
    "end": "772890"
  },
  {
    "text": "range of the number of partitions we have but because there is some",
    "start": "772890",
    "end": "778380"
  },
  {
    "text": "implications to how we partition our data for example data is being is",
    "start": "778380",
    "end": "783480"
  },
  {
    "text": "ordered only within a partition ok this key should represent some sort of",
    "start": "783480",
    "end": "789030"
  },
  {
    "text": "business logic but sometimes we want to control the way it is partitioned we have an interface called partitioner",
    "start": "789030",
    "end": "797610"
  },
  {
    "text": "that we can implement and we can decide on our own strategy how to partition our data other than that the producer is",
    "start": "797610",
    "end": "806520"
  },
  {
    "text": "very lightweight and very straightforward so let's go back to our overall",
    "start": "806520",
    "end": "812390"
  },
  {
    "text": "architecture and let's talk about our consumers our consumers are slightly more complex and a little bit more more",
    "start": "812390",
    "end": "823430"
  },
  {
    "text": "powerful when we talk about consumers in Kafka we can actually have single",
    "start": "823430",
    "end": "829280"
  },
  {
    "text": "consumers but we can also have what we call consumer groups and our consumers",
    "start": "829280",
    "end": "836920"
  },
  {
    "text": "do a lot of the heavy lifting this is one of the things that makes Kafka very",
    "start": "836920",
    "end": "842150"
  },
  {
    "text": "scalable Kafka for example doesn't care about where the consumers are currently",
    "start": "842150",
    "end": "848840"
  },
  {
    "text": "at with the reading when we write into Kafka into our topics okay",
    "start": "848840",
    "end": "855950"
  },
  {
    "text": "our messages are being stores s bytes on disks with what we call an offset it's",
    "start": "855950",
    "end": "863090"
  },
  {
    "text": "just like an index in an array and while reading our consumers know they know",
    "start": "863090",
    "end": "870020"
  },
  {
    "text": "them or sorry manage their current offset so Kafka doesn't need to keep",
    "start": "870020",
    "end": "876080"
  },
  {
    "text": "trace of who's reading what Kafka just stores the data it has an offset seen think of it as a very large-scale array",
    "start": "876080",
    "end": "883490"
  },
  {
    "text": "that you can access by a position by an index right so our consumers can go to a",
    "start": "883490",
    "end": "890450"
  },
  {
    "text": "specific offset and read a batch of messages transfer that or sorry get that",
    "start": "890450",
    "end": "897860"
  },
  {
    "text": "it's being transferred to them over the network and process it that allows Kafka",
    "start": "897860",
    "end": "904250"
  },
  {
    "text": "to be very scalable when we initialize our consumer in our property we can give",
    "start": "904250",
    "end": "912320"
  },
  {
    "text": "it the group ID and the group ID basically associates that with a",
    "start": "912320",
    "end": "917630"
  },
  {
    "text": "consumer group so we can have our microservice all of our the instances of",
    "start": "917630",
    "end": "923990"
  },
  {
    "text": "our microservice initialize with the same group ID and they can read different parts of the stream right they",
    "start": "923990",
    "end": "929960"
  },
  {
    "text": "can balance the load amongst themselves so this is our consumer Kafka consumer",
    "start": "929960",
    "end": "936980"
  },
  {
    "text": "and we initialize it with the properties we call subscribes",
    "start": "936980",
    "end": "943390"
  },
  {
    "text": "to a list of topics right we can read more than one topics and this is very",
    "start": "943390",
    "end": "949510"
  },
  {
    "text": "common pattern we just loop and and pull",
    "start": "949510",
    "end": "955240"
  },
  {
    "text": "for new messages okay so we can either",
    "start": "955240",
    "end": "960250"
  },
  {
    "text": "manage to offset ourselves or let the API do that for us for consumer groups",
    "start": "960250",
    "end": "966490"
  },
  {
    "text": "what kafka dot what the API is do is is store the offset in a Kafka topic so",
    "start": "966490",
    "end": "974050"
  },
  {
    "text": "this is Kafka 101 now we have producers we have consumers we have consumers",
    "start": "974050",
    "end": "979360"
  },
  {
    "text": "group we can scale our micro-services we can actually solve some of the problems",
    "start": "979360",
    "end": "984640"
  },
  {
    "text": "we already talked about right we can stream all our data into Kafka feed from",
    "start": "984640",
    "end": "990010"
  },
  {
    "text": "from another macro service on or our client systems we can read we can read",
    "start": "990010",
    "end": "995800"
  },
  {
    "text": "in scalable manner so we solved we actually solve a big part of the",
    "start": "995800",
    "end": "1002610"
  },
  {
    "text": "problems that we already had but wait there's more I love that guy I still",
    "start": "1002610",
    "end": "1010800"
  },
  {
    "text": "haven't bought any of the renovator stuff but one day I will okay so we",
    "start": "1010800",
    "end": "1021690"
  },
  {
    "text": "actually have a much better or much more advanced consumer in Kafka debt I want to talk to you about and I think that",
    "start": "1021690",
    "end": "1028020"
  },
  {
    "text": "for applications like micro services this is actually very beneficial Kafka has for the last couple version a",
    "start": "1028020",
    "end": "1037439"
  },
  {
    "text": "new library called Kafka streams and Kafka streams basically brings a lot of the capabilities of stream processing",
    "start": "1037440",
    "end": "1045300"
  },
  {
    "text": "frameworks that we see a lot in the Hadoop ecosystem and in the big data landscape into our micro services so",
    "start": "1045300",
    "end": "1056030"
  },
  {
    "text": "Kafka stream is a part of Apache Kafka that means it's again an open source project you got the community you got",
    "start": "1056030",
    "end": "1062880"
  },
  {
    "text": "the Apache Software Foundation backing it and you can write stream processing",
    "start": "1062880",
    "end": "1069260"
  },
  {
    "text": "applications using it one of the biggest differences or one of few big",
    "start": "1069260",
    "end": "1076320"
  },
  {
    "text": "differences between kafka streams and other stream processing technologies and we're going to cover another major one is the fact",
    "start": "1076320",
    "end": "1082140"
  },
  {
    "text": "that it's self hosted anyone here used spark streaming storm any of the stream",
    "start": "1082140",
    "end": "1087840"
  },
  {
    "text": "processing frameworks no - excellent so",
    "start": "1087840",
    "end": "1092970"
  },
  {
    "text": "one thing about all these two stream processing frameworks like spark",
    "start": "1092970",
    "end": "1099030"
  },
  {
    "text": "streaming and so on and patchy flink is that they run on some sort of server",
    "start": "1099030",
    "end": "1105750"
  },
  {
    "text": "software right if you want to run spark streaming you either need a spark",
    "start": "1105750",
    "end": "1111420"
  },
  {
    "text": "cluster a mezzos cluster or a yarn cluster to run it same wood flink",
    "start": "1111420",
    "end": "1117150"
  },
  {
    "text": "all of them have dependencies in this big data ecosystem which is good it's very powerful it comes with a lot of",
    "start": "1117150",
    "end": "1123330"
  },
  {
    "text": "capabilities for analytics workloads but if we have our own web server our own micro services sorry Web Services is too",
    "start": "1123330",
    "end": "1130380"
  },
  {
    "text": "long the old way to go yes so if we're",
    "start": "1130380",
    "end": "1137400"
  },
  {
    "text": "running our own micro service stack and we run it on kubernetes with with docker",
    "start": "1137400",
    "end": "1145560"
  },
  {
    "text": "or however we want to run it we don't want to shift into another piece of",
    "start": "1145560",
    "end": "1151200"
  },
  {
    "text": "infrastructure spin up and a dupe cluster just to do some stream processing just to get the semantics of",
    "start": "1151200",
    "end": "1157700"
  },
  {
    "text": "processing a stream of data so that's the old way of running those framework",
    "start": "1157700",
    "end": "1164190"
  },
  {
    "text": "that basically gives us this continuous processing of data that runs through our system right we want something that we",
    "start": "1164190",
    "end": "1171510"
  },
  {
    "text": "can run within our micro services and this is one of the things that the spark",
    "start": "1171510",
    "end": "1177270"
  },
  {
    "text": "streaming brings to the table so we have a couple of interesting API zhing in",
    "start": "1177270",
    "end": "1183510"
  },
  {
    "text": "spark streaming yeah Kafka streams sorry too many frameworks so the first one and",
    "start": "1183510",
    "end": "1192270"
  },
  {
    "text": "and very important one is the Kay stream API Kay stream is basically a stream of",
    "start": "1192270",
    "end": "1199410"
  },
  {
    "text": "data that runs through our system it is built on top of our consumers so it has",
    "start": "1199410",
    "end": "1205560"
  },
  {
    "text": "the same restrictions or the same attributes as our consumers",
    "start": "1205560",
    "end": "1210780"
  },
  {
    "text": "right our consumers right read from different partitions messages are",
    "start": "1210780",
    "end": "1215910"
  },
  {
    "text": "ordered only within those partitions but other than that we just get this API",
    "start": "1215910",
    "end": "1223140"
  },
  {
    "text": "that allows to do things like this is just a stream that I get off Twitter and",
    "start": "1223140",
    "end": "1229250"
  },
  {
    "text": "our case stream just gets all these tweets and what we do now we take all",
    "start": "1229250",
    "end": "1236160"
  },
  {
    "text": "the tweets and we make an array of words out of them obviously this is just for",
    "start": "1236160",
    "end": "1244380"
  },
  {
    "text": "slide for the slide obviously we would like to do some better parsing maybe some regex",
    "start": "1244380",
    "end": "1249530"
  },
  {
    "text": "there are different white spaces that you can hit with tweets and flatmap is",
    "start": "1249530",
    "end": "1255810"
  },
  {
    "text": "also very common in stream processing and in fact in in data processing applications it basically take those",
    "start": "1255810",
    "end": "1262860"
  },
  {
    "text": "arrays and make one stream out of them so now we have our key that we stored",
    "start": "1262860",
    "end": "1270660"
  },
  {
    "text": "which is the same key that we partitioned and a word for each key we",
    "start": "1270660",
    "end": "1277080"
  },
  {
    "text": "can do things like map map takes one value in our stream and maps it to",
    "start": "1277080",
    "end": "1282870"
  },
  {
    "text": "another value so instead of a key of and the value because we're not interested",
    "start": "1282870",
    "end": "1288450"
  },
  {
    "text": "in our key anymore we can just omit the value in value she's obviously very good coding but I",
    "start": "1288450",
    "end": "1297710"
  },
  {
    "text": "almost forgot to say what I'm doing here the example that I'm showing here is is the hello world of every data processing",
    "start": "1297710",
    "end": "1304890"
  },
  {
    "text": "application nowadays it's word count we basically want to count how many times each word appears in our stream and we",
    "start": "1304890",
    "end": "1315690"
  },
  {
    "text": "can do count by key count by key is very interesting because it's an aggregation and we have this function that",
    "start": "1315690",
    "end": "1322260"
  },
  {
    "text": "aggregates now and because we have this function that aggregates we're going to",
    "start": "1322260",
    "end": "1328980"
  },
  {
    "text": "do two stream because we want to generate a case stream again so there's",
    "start": "1328980",
    "end": "1336150"
  },
  {
    "text": "a bit of a subtlety here when we did the aggregation we actually stopped returning streams and we returned",
    "start": "1336150",
    "end": "1342510"
  },
  {
    "text": "something a little bit I'm gonna talk about but basically what we have now is a stream that gives us a",
    "start": "1342510",
    "end": "1350620"
  },
  {
    "text": "word and it's count how many times it appeared so what is the thing that we've",
    "start": "1350620",
    "end": "1356590"
  },
  {
    "text": "created with count by key this is very",
    "start": "1356590",
    "end": "1361900"
  },
  {
    "text": "interesting and I think the most powerful concept in Kafka streams and it",
    "start": "1361900",
    "end": "1370090"
  },
  {
    "text": "comes from what confluent called the stream table duality so to understand we",
    "start": "1370090",
    "end": "1376300"
  },
  {
    "text": "have a stream here the stream is basically people and their location so",
    "start": "1376300",
    "end": "1381360"
  },
  {
    "text": "basically myself and my wife we both started in our stream in Melbourne we",
    "start": "1381360",
    "end": "1387460"
  },
  {
    "text": "came to Sydney and my wife is already back in Melbourne right this is stream when we look at this stream we just have",
    "start": "1387460",
    "end": "1394270"
  },
  {
    "text": "a chain of events we have your name in Melbourne marina in melbourne univ in sydney marine in sydney and more in",
    "start": "1394270",
    "end": "1401320"
  },
  {
    "text": "melbourne that's all we have but if we want to look at this point in time in",
    "start": "1401320",
    "end": "1409030"
  },
  {
    "text": "what we call a table view this is what happens right now right now your navies",
    "start": "1409030",
    "end": "1414310"
  },
  {
    "text": "in Sydney or NEADS in Melbourne and for some things this is what we're interested in right we're looking at",
    "start": "1414310",
    "end": "1420550"
  },
  {
    "text": "this point in time for other things we actually want the streams we actually want to see the event as they go through",
    "start": "1420550",
    "end": "1427660"
  },
  {
    "text": "the system so this is the duality between stream and table in the CAF",
    "start": "1427660",
    "end": "1434170"
  },
  {
    "text": "castrum terminology and this brings us to another api called k table k tables",
    "start": "1434170",
    "end": "1441490"
  },
  {
    "text": "are basically a table representation of our data and they allow us to see the",
    "start": "1441490",
    "end": "1446950"
  },
  {
    "text": "data as it is right now in our stream in our microservices and that's a very",
    "start": "1446950",
    "end": "1453580"
  },
  {
    "text": "unique capability so for example we saw count count gives us a count point count",
    "start": "1453580",
    "end": "1461470"
  },
  {
    "text": "of the instances right now but we can do",
    "start": "1461470",
    "end": "1467380"
  },
  {
    "text": "other thing so we can do reduced by key reduced by key basically for every key allow us to do an aggregation on the",
    "start": "1467380",
    "end": "1474070"
  },
  {
    "text": "values okay so if our value is something like a number that we want",
    "start": "1474070",
    "end": "1481299"
  },
  {
    "text": "a sum we can just add them to each other right now for this specific table that",
    "start": "1481299",
    "end": "1488590"
  },
  {
    "text": "we've just seen all I want to do is return the later location right I'm",
    "start": "1488590",
    "end": "1493690"
  },
  {
    "text": "interested in the latest value in my stream so this basically creates a new K table",
    "start": "1493690",
    "end": "1500649"
  },
  {
    "text": "and Mike a table can be or is being updated as the string goes through the system so why is that important it's",
    "start": "1500649",
    "end": "1511600"
  },
  {
    "text": "important because if we have you know different pieces of data let's say we have a list of items that we are",
    "start": "1511600",
    "end": "1521200"
  },
  {
    "text": "purchasing okay and we're also interested in were currently the person",
    "start": "1521200",
    "end": "1528519"
  },
  {
    "text": "doing the purchases is located in normal applications our consumers will get the",
    "start": "1528519",
    "end": "1534429"
  },
  {
    "text": "stream as it goes through the system and we'll have to get the data from some",
    "start": "1534429",
    "end": "1543700"
  },
  {
    "text": "data store like our database and this means they need to go over the network",
    "start": "1543700",
    "end": "1549750"
  },
  {
    "text": "they need to do a database query to get you know my location my current location",
    "start": "1549750",
    "end": "1556750"
  },
  {
    "text": "if they want to see at this point in time right this is a lot of what our web service our micro services are doing",
    "start": "1556750",
    "end": "1563169"
  },
  {
    "text": "they're getting the stream of data whether it's Kafka or just messages going through the systems they process",
    "start": "1563169",
    "end": "1569769"
  },
  {
    "text": "it they update the database with the current state what Kafka streams allows",
    "start": "1569769",
    "end": "1575289"
  },
  {
    "text": "us to do is to have both of those things in the same microservice so we can have",
    "start": "1575289",
    "end": "1581710"
  },
  {
    "text": "a K table being updated in memory and we can have the stream and what we can do",
    "start": "1581710",
    "end": "1588519"
  },
  {
    "text": "is join them and we can have a new stream that contains this in memory data",
    "start": "1588519",
    "end": "1594279"
  },
  {
    "text": "that is a current representation of what's going on in our application right now your navies in Sydney Mori Lee is in",
    "start": "1594279",
    "end": "1600340"
  },
  {
    "text": "Melbourne and this is why I can spend my money on a new MacBook obviously do not need so this is some of the power of",
    "start": "1600340",
    "end": "1609369"
  },
  {
    "text": "Kafka streams think about it as a in-memory cache that being updated by",
    "start": "1609369",
    "end": "1616840"
  },
  {
    "text": "your stream at all time so K tables K",
    "start": "1616840",
    "end": "1622720"
  },
  {
    "text": "tables basically bring us this distributed data store that states",
    "start": "1622720",
    "end": "1628360"
  },
  {
    "text": "within our micro-services and if we design it correctly because our",
    "start": "1628360",
    "end": "1635130"
  },
  {
    "text": "consumers read by read from the same partitions by the key we can have data",
    "start": "1635130",
    "end": "1643090"
  },
  {
    "text": "locality we know that if we do it correctly all the events were the key Z",
    "start": "1643090",
    "end": "1649900"
  },
  {
    "text": "and Eve are going to be in this instance and also the current location of gen√®ve",
    "start": "1649900",
    "end": "1655900"
  },
  {
    "text": "is going to be in this instance we do not shuffle data around the network anymore we have locality in this data source",
    "start": "1655900",
    "end": "1662530"
  },
  {
    "text": "stored is being updated by the stream so we can still query it it still is a sort",
    "start": "1662530",
    "end": "1670150"
  },
  {
    "text": "of a database again I sell databases for living and not this database but it",
    "start": "1670150",
    "end": "1679780"
  },
  {
    "text": "gives us a lot of the semantics of databases in memory based on our stream and that this is why it's very powerful",
    "start": "1679780",
    "end": "1686260"
  },
  {
    "text": "and and this is how we join ok this is basically a k.k stream which is the",
    "start": "1686260",
    "end": "1693250"
  },
  {
    "text": "result of the join between user products which is another case stream and user",
    "start": "1693250",
    "end": "1700900"
  },
  {
    "text": "locations and what we do is basically create a new key value and I just",
    "start": "1700900",
    "end": "1707620"
  },
  {
    "text": "created these products locations per object it's just an object so I have a",
    "start": "1707620",
    "end": "1714370"
  },
  {
    "text": "key and a new value that contains both of those data items it's very simple",
    "start": "1714370",
    "end": "1719679"
  },
  {
    "text": "very straightforward in other languages once this API will be available I'm sure you can do things like using tuples and",
    "start": "1719679",
    "end": "1725710"
  },
  {
    "text": "so on yes it's being done so I'll repeat",
    "start": "1725710",
    "end": "1733420"
  },
  {
    "text": "the question the joining is being was is the joining being done by Kafka it's been done by Kafka streams right",
    "start": "1733420",
    "end": "1740830"
  },
  {
    "text": "it's the API that does it within your microservice though those are two different basically what we're doing",
    "start": "1740830",
    "end": "1748550"
  },
  {
    "text": "here reading from two different topics one one is the locations topics one is the product topics right Kafka is unaware of",
    "start": "1748550",
    "end": "1756770"
  },
  {
    "text": "that but what happens is because both of them has the same key and consumers read",
    "start": "1756770",
    "end": "1763310"
  },
  {
    "text": "by key because they read a partition we know that the same key will be locally",
    "start": "1763310",
    "end": "1768560"
  },
  {
    "text": "on the same on the same instance of our",
    "start": "1768560",
    "end": "1773900"
  },
  {
    "text": "micro-service right so if we scale it and we have more consumers running we're reading more partitions then we know",
    "start": "1773900",
    "end": "1782240"
  },
  {
    "text": "that they still keep that locality one caveat in that is that sometimes you",
    "start": "1782240",
    "end": "1788300"
  },
  {
    "text": "cannot do that and there's a que global table which allows you to do the which",
    "start": "1788300",
    "end": "1795380"
  },
  {
    "text": "allows Kafka streams to do the shuffling if needed so you can actually have the",
    "start": "1795380",
    "end": "1800600"
  },
  {
    "text": "same table represented in multiple instances it's a bit more complex API you sacrifice performance but sometimes",
    "start": "1800600",
    "end": "1808040"
  },
  {
    "text": "you cannot partition to cater to those needs okay this yes you had another",
    "start": "1808040",
    "end": "1815150"
  },
  {
    "text": "question",
    "start": "1815150",
    "end": "1817480"
  },
  {
    "text": "you mentioned the Java is like where all the interesting stuff is yes mainly",
    "start": "1829880",
    "end": "1835679"
  },
  {
    "text": "mainly ice-creams anything happening especially was dreamed for the other language so I know that the consumer and",
    "start": "1835679",
    "end": "1843150"
  },
  {
    "text": "producer IP eyes are supported in multiple languages I don't work for confluent I'm not involved in this",
    "start": "1843150",
    "end": "1849690"
  },
  {
    "text": "project I'm just a heavy user of it but",
    "start": "1849690",
    "end": "1854870"
  },
  {
    "text": "it's definitely something you can you can find out on the Google Groups and",
    "start": "1854870",
    "end": "1860460"
  },
  {
    "text": "you can in them as far as I know they haven't published any plans right now",
    "start": "1860460",
    "end": "1866100"
  },
  {
    "text": "but it wouldn't surprise me they will support some other popular languages I would assume that at a certain point it",
    "start": "1866100",
    "end": "1874049"
  },
  {
    "text": "will get to Python because python is very very popular in this space yeah",
    "start": "1874049",
    "end": "1882919"
  },
  {
    "text": "okay so we've again solved a lot of the",
    "start": "1885260",
    "end": "1893039"
  },
  {
    "text": "problems that we had in in this slide we have another problem if we go beyond our",
    "start": "1893039",
    "end": "1900450"
  },
  {
    "text": "micro services and that's a problem that we have more than one data store that",
    "start": "1900450",
    "end": "1905909"
  },
  {
    "text": "needs our data and that's that's also a",
    "start": "1905909",
    "end": "1910980"
  },
  {
    "text": "very common thing especially today in the polyglot storage era right we have",
    "start": "1910980",
    "end": "1916590"
  },
  {
    "text": "multiple data stores that that needs the same data sometimes they need the data",
    "start": "1916590",
    "end": "1922980"
  },
  {
    "text": "after it was some transforms transformed but sometimes they just need the same",
    "start": "1922980",
    "end": "1928200"
  },
  {
    "text": "data so Kafka can also help with that",
    "start": "1928200",
    "end": "1936200"
  },
  {
    "text": "and sorry one point it I forgot to make",
    "start": "1936200",
    "end": "1942690"
  },
  {
    "text": "clear is that when you need the data transformed you also want one mastered a",
    "start": "1942690",
    "end": "1948870"
  },
  {
    "text": "lot of time one master data store that is the source of truth for your application so you might want to write",
    "start": "1948870",
    "end": "1954299"
  },
  {
    "text": "your database and then replicate to the different stores okay",
    "start": "1954299",
    "end": "1960720"
  },
  {
    "text": "so what we really want is a way to take data from our database our master",
    "start": "1960720",
    "end": "1966300"
  },
  {
    "text": "database and somehow stream it into additional data stores like our search",
    "start": "1966300",
    "end": "1971910"
  },
  {
    "text": "engine like our elastic core or solar or or whatever search engine we're using or",
    "start": "1971910",
    "end": "1978450"
  },
  {
    "text": "whatever additional data stores we're using so one way to do that would be to have just micro services shoveling data",
    "start": "1978450",
    "end": "1985920"
  },
  {
    "text": "from one store to another if and we can use Kafka for that Kafka gives us",
    "start": "1985920",
    "end": "1991200"
  },
  {
    "text": "additional durability it's a good way to transfer data from point to point especially if you wanna move it between",
    "start": "1991200",
    "end": "1998550"
  },
  {
    "text": "places but Kafka actually has another",
    "start": "1998550",
    "end": "2004130"
  },
  {
    "text": "sub project called Kafka Connect which does all that work for us so Kafka Connect allows us to set up connectors",
    "start": "2004130",
    "end": "2011080"
  },
  {
    "text": "that can read from data stores they they're called sources and write to",
    "start": "2011080",
    "end": "2017510"
  },
  {
    "text": "other data sources these are called sinks okay and Kafka Connect is",
    "start": "2017510",
    "end": "2023030"
  },
  {
    "text": "basically a tool to stream data from one data store or one data source to Kafka",
    "start": "2023030",
    "end": "2030710"
  },
  {
    "text": "and from Kafka to other destinations and that could be many to many one to many",
    "start": "2030710",
    "end": "2037429"
  },
  {
    "text": "whatever you like okay it's very scalable I'm gonna talk a little bit",
    "start": "2037429",
    "end": "2042860"
  },
  {
    "text": "about that basically it's an additional cluster software that runs can run on",
    "start": "2042860",
    "end": "2047870"
  },
  {
    "text": "the same machines or different machines you can play with that and gives you",
    "start": "2047870",
    "end": "2052878"
  },
  {
    "text": "some some management capabilities okay",
    "start": "2052879",
    "end": "2058220"
  },
  {
    "text": "it's very easy to replicate data from into Kafka using Kafka Connect so as I",
    "start": "2058220",
    "end": "2067100"
  },
  {
    "text": "said in order to do that we have the connectors connectors are basically implementation of a few interfaces that",
    "start": "2067100",
    "end": "2074210"
  },
  {
    "text": "deal with all of the technicalities that you need to do there is a really growing",
    "start": "2074210",
    "end": "2081290"
  },
  {
    "text": "list of connectors for almost any data store that you can think of that include things like sequel server Twitter",
    "start": "2081290",
    "end": "2090030"
  },
  {
    "text": "the Twitter firehose you just have a connector that you can stream data directly into Kafka without writing code",
    "start": "2090030",
    "end": "2096409"
  },
  {
    "text": "almost any database that you can think of I've I've looked into doing things",
    "start": "2096410",
    "end": "2104430"
  },
  {
    "text": "like db2 for a large financial client and really the the worst kind of data",
    "start": "2104430",
    "end": "2112590"
  },
  {
    "text": "stores that you can imagine they take there might be some sort of support some",
    "start": "2112590",
    "end": "2117840"
  },
  {
    "text": "of the support is commercial and you have to pay for it especially when you're dealing with the likes of IBM and some is open source and",
    "start": "2117840",
    "end": "2126540"
  },
  {
    "text": "and and a lot of good open source software out there but you can implement",
    "start": "2126540",
    "end": "2132030"
  },
  {
    "text": "yourself and our caveat here is that you should really know the data stores that",
    "start": "2132030",
    "end": "2138150"
  },
  {
    "text": "you're working with so for example I've seen some implementation that just query a database and just that just putting",
    "start": "2138150",
    "end": "2144570"
  },
  {
    "text": "additional load on the database there is one for example for couch base and couch",
    "start": "2144570",
    "end": "2149700"
  },
  {
    "text": "base and meet something we call DCP data change protocol it's basically the data",
    "start": "2149700",
    "end": "2154980"
  },
  {
    "text": "as it streams into couch base we also have an API audit publish it to whoever",
    "start": "2154980",
    "end": "2161760"
  },
  {
    "text": "wants to read and that's the way that we wrote the Kafka source for example so",
    "start": "2161760",
    "end": "2168109"
  },
  {
    "text": "but the technicalities of implementing a connector are very simple it really is",
    "start": "2168109",
    "end": "2175080"
  },
  {
    "text": "just a few api's that you need to implement as long as you know that the",
    "start": "2175080",
    "end": "2180960"
  },
  {
    "text": "data stores that you're working with so for example the Couchbase connector this",
    "start": "2180960",
    "end": "2186750"
  },
  {
    "text": "is literally all you need to do is to take the jar you put it in a pre-configured folder that's your",
    "start": "2186750",
    "end": "2194280"
  },
  {
    "text": "connector you have that now Kafka Connect can use that jar to to stream",
    "start": "2194280",
    "end": "2201119"
  },
  {
    "text": "data and you configure it so if you want to configure and Couchbase is a source and you can",
    "start": "2201119",
    "end": "2208960"
  },
  {
    "text": "see that our connector class is Couchbase source connector you configure",
    "start": "2208960",
    "end": "2215320"
  },
  {
    "text": "things like the number of tasks that you have and again tasks are just instances",
    "start": "2215320",
    "end": "2221650"
  },
  {
    "text": "that that run as consumers and those can be scaled across the cluster connection",
    "start": "2221650",
    "end": "2228850"
  },
  {
    "text": "to the cluster the bucket so bucket is basically a logical key space in cash",
    "start": "2228850",
    "end": "2235660"
  },
  {
    "text": "base it's it's the unit we store data that could be a table in other databases that could be collection in in other",
    "start": "2235660",
    "end": "2242800"
  },
  {
    "text": "databases whatever the database call and the topic that we stream the data to that's all you need in order to stream a",
    "start": "2242800",
    "end": "2251350"
  },
  {
    "text": "cart race bucket into Kafka literally that's that we got production sites",
    "start": "2251350",
    "end": "2256810"
  },
  {
    "text": "working with a little bit more configuration there's some SSL configuration that you can add but",
    "start": "2256810",
    "end": "2262570"
  },
  {
    "text": "literally not more than that and you have your data streaming from your database into Kafka very similarly yeah",
    "start": "2262570",
    "end": "2270700"
  },
  {
    "text": "you can connect on on the other side and just stream your data into your search engine draft database Hadoop cluster and",
    "start": "2270700",
    "end": "2279100"
  },
  {
    "text": "one of the initial workloads that that Kafka was built to to cater for was",
    "start": "2279100",
    "end": "2285250"
  },
  {
    "text": "actually offloading data into a dupe so Kafka has a lot of you know built-in",
    "start": "2285250",
    "end": "2290560"
  },
  {
    "text": "api's the old project is called kami after alberta me because you know when",
    "start": "2290560",
    "end": "2298300"
  },
  {
    "text": "you start with those type of names it's very hard to switch theme but there's a",
    "start": "2298300",
    "end": "2304900"
  },
  {
    "text": "new implementation of that so it's very easy so now I think we can do this we",
    "start": "2304900",
    "end": "2312700"
  },
  {
    "text": "can get from the point where our micro services are coupled to each other and",
    "start": "2312700",
    "end": "2318580"
  },
  {
    "text": "we need to do a lot of shuffling of data around into this world where we have a",
    "start": "2318580",
    "end": "2324100"
  },
  {
    "text": "stream of data we can have our micro services to actually a lot more than",
    "start": "2324100",
    "end": "2330250"
  },
  {
    "text": "what they've done before right we can use the K tables we can have these kind",
    "start": "2330250",
    "end": "2335350"
  },
  {
    "text": "of live caches within our applications we can you know do very expressive",
    "start": "2335350",
    "end": "2341140"
  },
  {
    "text": "processing on top of our data run it on kubernetes docker swarm wherever we want",
    "start": "2341140",
    "end": "2348040"
  },
  {
    "text": "to run it without you know changing anything in our code we can process the",
    "start": "2348040",
    "end": "2355060"
  },
  {
    "text": "data here put it in a data store and use Kafka to stream it to other systems or just stream the data directly from Kafka",
    "start": "2355060",
    "end": "2362560"
  },
  {
    "text": "so I think we're we're in pretty good shape but wait there's more",
    "start": "2362560",
    "end": "2368700"
  },
  {
    "text": "yes so so here for example and and we",
    "start": "2368700",
    "end": "2381460"
  },
  {
    "text": "can interpret that it's that's the beauty of art right so here I meant that",
    "start": "2381460",
    "end": "2387940"
  },
  {
    "text": "you know our microservices stored the data into our very scalable database who",
    "start": "2387940",
    "end": "2393220"
  },
  {
    "text": "we will not name and the database can just replicate itself to a search engine for example using cough go connect but",
    "start": "2393220",
    "end": "2401100"
  },
  {
    "text": "it might as well be that the search engine need the raw data and just picks",
    "start": "2401100",
    "end": "2406330"
  },
  {
    "text": "the same data streams as the micro services using Kafka connect right so we",
    "start": "2406330",
    "end": "2411820"
  },
  {
    "text": "can play around so those are the different yes exactly so you can do I",
    "start": "2411820",
    "end": "2419290"
  },
  {
    "text": "didn't repeat the question so the question was if the data goes directly",
    "start": "2419290",
    "end": "2425770"
  },
  {
    "text": "through Kafka connect to the database or not so so kind of the last thing I want",
    "start": "2425770",
    "end": "2437290"
  },
  {
    "text": "to go through with you is the conference platform because there is literally so",
    "start": "2437290",
    "end": "2443170"
  },
  {
    "text": "much more to Kafka than we can you know cover in one hour the confluent platform",
    "start": "2443170",
    "end": "2449920"
  },
  {
    "text": "is a commercial product by confluent it has two variation there's an open source one and a commercial one I'm not gonna",
    "start": "2449920",
    "end": "2456810"
  },
  {
    "text": "tell you more about the licensing because I don't know a lot and I don't represent confluent but confluent is a",
    "start": "2456810",
    "end": "2463540"
  },
  {
    "text": "company that was created but a lot of the original team members of Kafka",
    "start": "2463540",
    "end": "2468640"
  },
  {
    "text": "within LinkedIn they spawn confluent and they created this platform and this gives us some more capabilities that we",
    "start": "2468640",
    "end": "2476020"
  },
  {
    "text": "usually need for micro services and for other applications that might use Kafka so here we have the Kafka core right",
    "start": "2476020",
    "end": "2486099"
  },
  {
    "text": "Patrick Kafka contains Kafka itself Kafka streams and Kafka connects so",
    "start": "2486099",
    "end": "2492760"
  },
  {
    "text": "those are core Pachy projects don't need to pay anything you don't need to",
    "start": "2492760",
    "end": "2498130"
  },
  {
    "text": "download it from confluence you have everything available on the Apache repositories and so on there are",
    "start": "2498130",
    "end": "2506020"
  },
  {
    "text": "additional open source parts to it so there's the connector some of them are",
    "start": "2506020",
    "end": "2511690"
  },
  {
    "text": "being developed by confluence some are being developed by different data stores like cartridge did for the Couchbase",
    "start": "2511690",
    "end": "2517720"
  },
  {
    "text": "connector and some are by the community we have the client a lot of investment",
    "start": "2517720",
    "end": "2523270"
  },
  {
    "text": "by confluent in those clients including for example the c-sharp client another",
    "start": "2523270",
    "end": "2530020"
  },
  {
    "text": "very interesting bit is the scheme of registry and one thing we didn't talk about is when we use these pop",
    "start": "2530020",
    "end": "2537880"
  },
  {
    "text": "subsystems a lot of the time we sacrifice some of the capabilities of calling an API directly we sacrifice the",
    "start": "2537880",
    "end": "2544540"
  },
  {
    "text": "way the basically the capability of defining our API some way right when we",
    "start": "2544540",
    "end": "2550780"
  },
  {
    "text": "stream data to Kafka at the end of the day for Kafka it's key and few bytes",
    "start": "2550780",
    "end": "2558430"
  },
  {
    "text": "that it needs to store that all Kafka knows about our data the schema registry",
    "start": "2558430",
    "end": "2564160"
  },
  {
    "text": "allows us to use Kafka but also use a",
    "start": "2564160",
    "end": "2570310"
  },
  {
    "text": "format called opera anyone here ever used or no opera so a pro is a serialization format and if we see",
    "start": "2570310",
    "end": "2577450"
  },
  {
    "text": "realized to Avro and we configure through our producers the schema",
    "start": "2577450",
    "end": "2584800"
  },
  {
    "text": "registry what will happen is the first time oh we can either push a schema for",
    "start": "2584800",
    "end": "2590550"
  },
  {
    "text": "topic or the first time we see realize an object to a topic it will store the",
    "start": "2590550",
    "end": "2597430"
  },
  {
    "text": "schema for us and sorry there's an additional API call that you need to do",
    "start": "2597430",
    "end": "2603730"
  },
  {
    "text": "with your object for to send a schema but that allows us to basically have a schema for our topic",
    "start": "2603730",
    "end": "2610260"
  },
  {
    "text": "and Avro is a very powerful but also very complex way to manage schemas Avril",
    "start": "2610260",
    "end": "2617310"
  },
  {
    "text": "has different capabilities like backward compatibility forward compatibility and",
    "start": "2617310",
    "end": "2623960"
  },
  {
    "text": "and or you can decide not to be compatible at all bust but still have",
    "start": "2623960",
    "end": "2629880"
  },
  {
    "text": "your schema stored so this is one thing for example that Kafka does very well it brings us back to the whistle days right",
    "start": "2629880",
    "end": "2637710"
  },
  {
    "text": "where we have our schemas becoming monstrosities but still it's a fairly",
    "start": "2637710",
    "end": "2644190"
  },
  {
    "text": "common need in micro services and in services application we have a what they",
    "start": "2644190",
    "end": "2650339"
  },
  {
    "text": "call breast proxy which is a quasi REST API to store data into Kafka so if you",
    "start": "2650339",
    "end": "2655440"
  },
  {
    "text": "want to send directly from a client it only knows HTTP like Java Script you can",
    "start": "2655440",
    "end": "2662339"
  },
  {
    "text": "still do that I wouldn't necessarily recommend that but it still is a possibility and going into the confluent",
    "start": "2662339",
    "end": "2669089"
  },
  {
    "text": "platform we have things like the control center that allows us to monitor and govern our clusters another bit of cough",
    "start": "2669089",
    "end": "2678750"
  },
  {
    "text": "cough the open source project is a project called mirror maker mirror maker is basically Kafka - Kafka replication",
    "start": "2678750",
    "end": "2685980"
  },
  {
    "text": "that allows us to replicate data between two Kafka clusters",
    "start": "2685980",
    "end": "2691369"
  },
  {
    "text": "located for example in two data centers the problem with that is that mirror maker doesn't have a lot of the security",
    "start": "2691369",
    "end": "2697680"
  },
  {
    "text": "bits that you need to do that at scale and enterprise level so the conference platform does do that and they have a",
    "start": "2697680",
    "end": "2707580"
  },
  {
    "text": "lot of you know really interesting bits coming all the time they have enterprise",
    "start": "2707580",
    "end": "2714720"
  },
  {
    "text": "level support and that really allows us to use Kafka in in heavier loads right",
    "start": "2714720",
    "end": "2724220"
  },
  {
    "text": "so we actually are bit quicker than I",
    "start": "2724550",
    "end": "2729780"
  },
  {
    "text": "anticipated and that's all I wanted to show you today but but really I think",
    "start": "2729780",
    "end": "2735750"
  },
  {
    "text": "the main message here is that Kafka can really power and enable micro-services applications right",
    "start": "2735750",
    "end": "2743550"
  },
  {
    "text": "cover comes from or is best known for what it does in the big data space and",
    "start": "2743550",
    "end": "2748710"
  },
  {
    "text": "this is how I became familiar with Kafka the first time I used Kafka was to offload data into a Hadoop cluster using",
    "start": "2748710",
    "end": "2755640"
  },
  {
    "text": "chameleon MapReduce and and and that was a long time ago and as I worked more and",
    "start": "2755640",
    "end": "2762720"
  },
  {
    "text": "more with Kafka I realized that other applications that I'm doing like",
    "start": "2762720",
    "end": "2768150"
  },
  {
    "text": "services can really benefit free from Kafka so that was my journey it's it's a very powerful system it's a transaction",
    "start": "2768150",
    "end": "2775770"
  },
  {
    "text": "log based app subsystem remember that it has it's not a full-blown database but",
    "start": "2775770",
    "end": "2783030"
  },
  {
    "text": "it has some of the main concept of database stores and not an interesting bit that just came in the last couple of",
    "start": "2783030",
    "end": "2790770"
  },
  {
    "text": "version is exactly one delivery guarantee okay so we can have exactly",
    "start": "2790770",
    "end": "2797010"
  },
  {
    "text": "one delivery with Kafka obviously with those things and and for those of you",
    "start": "2797010",
    "end": "2802560"
  },
  {
    "text": "who remember the days of distributed transactions there's a lot of overhead for that and there's no black magic here",
    "start": "2802560",
    "end": "2808620"
  },
  {
    "text": "Kafka also I believe actually didn't use that in large scale just played around",
    "start": "2808620",
    "end": "2813930"
  },
  {
    "text": "with it but Kafka will probably suffer from the same pains of scaling transactions but it does have that it",
    "start": "2813930",
    "end": "2822210"
  },
  {
    "text": "has that power of true data store run in scale we have very powerful clients if",
    "start": "2822210",
    "end": "2829230"
  },
  {
    "text": "you're not using Kafka streams c-sharp the Python the Go api's are also",
    "start": "2829230",
    "end": "2835680"
  },
  {
    "text": "supported by confluent okay so you're not just sacrificing by not yeah using",
    "start": "2835680",
    "end": "2842280"
  },
  {
    "text": "Java it's just the streams ideas yes",
    "start": "2842280",
    "end": "2846920"
  },
  {
    "text": "no no the streaming is the only one only one that is not available so if you're using other languages and you're not",
    "start": "2851200",
    "end": "2857749"
  },
  {
    "text": "gonna use Kafka streams then you can there are fully supported by confluent in the community I've been involved with",
    "start": "2857749",
    "end": "2871729"
  },
  {
    "text": "people using Kafka from C sharp and even Avril seem like a problem like so many",
    "start": "2871729",
    "end": "2877459"
  },
  {
    "text": "versions of digitization civilization and - did you see this I think you",
    "start": "2877459",
    "end": "2887269"
  },
  {
    "text": "haven't tried it with C sharp what did you see similar issues with versioning and afro",
    "start": "2887269",
    "end": "2892700"
  },
  {
    "text": "yes I've seen I've seen issues with with opera versioning and and actually the avro jars outside of Kafka ended up Walt",
    "start": "2892700",
    "end": "2901339"
  },
  {
    "text": "so it is a common issue my only suggestion and the question was",
    "start": "2901339",
    "end": "2906349"
  },
  {
    "text": "regarding Averell breaks between versions sometimes it's actually common",
    "start": "2906349",
    "end": "2912380"
  },
  {
    "text": "for a lot of those formats I've seen that with pork as well so the problem is",
    "start": "2912380",
    "end": "2918049"
  },
  {
    "text": "sometimes that they're changing the underlying the actual underlying format",
    "start": "2918049",
    "end": "2924099"
  },
  {
    "text": "and that sometimes break my only suggestion for you is and in this world",
    "start": "2924099",
    "end": "2930469"
  },
  {
    "text": "you the open source world you have to be a little bit braver and go on the google groups and have those discussions and",
    "start": "2930469",
    "end": "2936440"
  },
  {
    "text": "get the support from the commuters so there are quite a few resources and all",
    "start": "2936440",
    "end": "2943819"
  },
  {
    "text": "of them available from through the confluent website so that that's the",
    "start": "2943819",
    "end": "2948859"
  },
  {
    "text": "only advice if some something is breaking they also support as I said and",
    "start": "2948859",
    "end": "2954529"
  },
  {
    "text": "this is one of the implication of having fully support Apache project you have a mailing list you have a lot of the time",
    "start": "2954529",
    "end": "2963499"
  },
  {
    "text": "Google Groups and those are the places to get an answer the mailing list people",
    "start": "2963499",
    "end": "2968509"
  },
  {
    "text": "will answer you okay so that's that's the way to go that",
    "start": "2968509",
    "end": "2973780"
  },
  {
    "text": "okay another very compelling thing for",
    "start": "2973780",
    "end": "2980560"
  },
  {
    "text": "Kafka is the unification of micro services and stream processing once you",
    "start": "2980560",
    "end": "2987220"
  },
  {
    "text": "start working in the stream processing world and you're used to doing those you know fluent API is one-liners that",
    "start": "2987220",
    "end": "2993580"
  },
  {
    "text": "really transform your data and do a lot of things in very few change functions I",
    "start": "2993580",
    "end": "3000660"
  },
  {
    "text": "will not use any any big words for that",
    "start": "3000660",
    "end": "3005840"
  },
  {
    "text": "it's very hard to go back to the to the times where you have to loop through data structures and and you know",
    "start": "3005840",
    "end": "3013470"
  },
  {
    "text": "transform your data the Kafka streams API does a great job of that so you can use that within your",
    "start": "3013470",
    "end": "3020790"
  },
  {
    "text": "micro services Kafka Connect for me it's",
    "start": "3020790",
    "end": "3027450"
  },
  {
    "text": "magic it takes so much work that I used to do as a data professional for setting",
    "start": "3027450",
    "end": "3033060"
  },
  {
    "text": "up those polyglot systems and just waters it down to a few configuration files and that's it I'm good to go",
    "start": "3033060",
    "end": "3040460"
  },
  {
    "text": "we can enforce schemas we can use the full-blown confluent platform if we'd",
    "start": "3040460",
    "end": "3045990"
  },
  {
    "text": "like to but we can keep it open source and really it's a great community so my",
    "start": "3045990",
    "end": "3053580"
  },
  {
    "text": "only advice is if you haven't already just install Kafka and take it for a spin you you won't regret it",
    "start": "3053580",
    "end": "3060410"
  },
  {
    "text": "yes so where's the authentication and",
    "start": "3060410",
    "end": "3067470"
  },
  {
    "text": "authorization concerns across the string managed in here",
    "start": "3067470",
    "end": "3074420"
  },
  {
    "text": "[Music]",
    "start": "3076380",
    "end": "3079669"
  },
  {
    "text": "what again again I'm sorry I so the",
    "start": "3082079",
    "end": "3087579"
  },
  {
    "text": "question is around how does the consuming service at the other side okay so it's not text okay sorry so it's not",
    "start": "3087579",
    "end": "3095710"
  },
  {
    "text": "the security implications it's you need to also have some context with your data",
    "start": "3095710",
    "end": "3101250"
  },
  {
    "text": "okay so there are two ways that you can do that right and that's a very good question the question is how can I have",
    "start": "3101250",
    "end": "3107290"
  },
  {
    "text": "the security context of some context of my call on the other side on the consuming side and there are two ways",
    "start": "3107290",
    "end": "3113290"
  },
  {
    "text": "that you can do that you can have it as part of an object serialized into Kafka and then you just read that and that's",
    "start": "3113290",
    "end": "3119619"
  },
  {
    "text": "one use case if that data data repeats itself right it's something like a session for a user and you want that",
    "start": "3119619",
    "end": "3126700"
  },
  {
    "text": "constantly available you can have that in an additional stream and you can join that using K tables and K streams and",
    "start": "3126700",
    "end": "3133780"
  },
  {
    "text": "that way you have very little data running through the second topic the topic that gives you the contact you",
    "start": "3133780",
    "end": "3140589"
  },
  {
    "text": "have that in memory within the instance of your micro service and as data comes",
    "start": "3140589",
    "end": "3146559"
  },
  {
    "text": "you can join those streams so that's that's a very good question shows the power of K tables yes let's do",
    "start": "3146559",
    "end": "3156910"
  },
  {
    "text": "the mic because I forget to repeat the questions yeah a few questions yes so for the cork of API I mean the hook of a",
    "start": "3156910",
    "end": "3165069"
  },
  {
    "text": "client is the dotnet client as cool as other language clients or there's a deficiency there",
    "start": "3165069",
    "end": "3171099"
  },
  {
    "text": "I see an O here and I haven't used it extensively I I have to admit I'm most",
    "start": "3171099",
    "end": "3176170"
  },
  {
    "text": "of the work I do is on the JVM but it is supported by confluent and if you raise",
    "start": "3176170",
    "end": "3182500"
  },
  {
    "text": "your voice and you say that you need it and you want it there is that's the way",
    "start": "3182500",
    "end": "3189040"
  },
  {
    "text": "to influence those type of projects right if people shout enough you get better support I'm not sure that it's",
    "start": "3189040",
    "end": "3195880"
  },
  {
    "text": "not as good as some of the other api's it's just that those type of projects",
    "start": "3195880",
    "end": "3202329"
  },
  {
    "text": "and the dependencies for example with Avro it's it dependencies on on",
    "start": "3202329",
    "end": "3207609"
  },
  {
    "text": "additional libraries it might break so and as I said I had and and this is an",
    "start": "3207609",
    "end": "3215310"
  },
  {
    "text": "enterprise-grade product right I was working with cloud era and and the version of pork a changed and broke",
    "start": "3215310",
    "end": "3223010"
  },
  {
    "text": "cluster processing I think 50 terabytes of data we just we had a an Impala",
    "start": "3223010",
    "end": "3231450"
  },
  {
    "text": "database it just one day one okay so so those kind of shifts sometimes happen in",
    "start": "3231450",
    "end": "3237840"
  },
  {
    "text": "those type of projects you the best way to deal with it is go on the mailing",
    "start": "3237840",
    "end": "3243780"
  },
  {
    "text": "list and and seek advice there and and obviously not just upgrade all the time",
    "start": "3243780",
    "end": "3249510"
  },
  {
    "text": "to the latest version yeah so what's the actual communication between the clients and Kafka is it like API or what's the",
    "start": "3249510",
    "end": "3255840"
  },
  {
    "text": "so yes you you have the the consumer and producer IP is for that Easter is I'm",
    "start": "3255840",
    "end": "3263520"
  },
  {
    "text": "asking many questions but I am very new to it is there a static Africa app available as a service from anybody but",
    "start": "3263520",
    "end": "3270510"
  },
  {
    "text": "without installing your own there there's been some announcement by confluent that's one option in Microsoft",
    "start": "3270510",
    "end": "3278280"
  },
  {
    "text": "Azure you have an HD inside HD inside kafka cluster okay so Kafka can be",
    "start": "3278280",
    "end": "3284850"
  },
  {
    "text": "managed either manually you can just install different instances of class or",
    "start": "3284850",
    "end": "3290910"
  },
  {
    "text": "nothing wrong with that you can run it as a framework on meses or you can run it on yarn in Hadoop okay so with",
    "start": "3290910",
    "end": "3297660"
  },
  {
    "text": "Microsoft there is HD inside kafka service with with confluent I've seen I",
    "start": "3297660",
    "end": "3308490"
  },
  {
    "text": "can commit exactly what it is but I've seen some announcement and some discussions regarding Kafka as a service",
    "start": "3308490",
    "end": "3314280"
  },
  {
    "text": "solution as well and just two more questions sorry there's a thing called Apache knife I how is that related to",
    "start": "3314280",
    "end": "3321900"
  },
  {
    "text": "Kafka good question really good questions computing products or know so",
    "start": "3321900",
    "end": "3328650"
  },
  {
    "text": "so Apache now if I basically gives you if anything it's competing with Kafka",
    "start": "3328650",
    "end": "3334080"
  },
  {
    "text": "Connect but it allows you to basically connect with data sources and destinations and do some transformations",
    "start": "3334080",
    "end": "3341220"
  },
  {
    "text": "in a UI environment and one of those day destinations can be",
    "start": "3341220",
    "end": "3347319"
  },
  {
    "text": "Kafka right knife I itself that is not",
    "start": "3347319",
    "end": "3353710"
  },
  {
    "text": "backed by any stores like like Kafka that gives you that you know transaction",
    "start": "3353710",
    "end": "3358810"
  },
  {
    "text": "log capability gives you that you know all those powerful guarantees about",
    "start": "3358810",
    "end": "3364750"
  },
  {
    "text": "delivery but but it's a very interesting project itself I will if you're using it",
    "start": "3364750",
    "end": "3372040"
  },
  {
    "text": "disc L my go-to approach and I haven't used it in this scale at all would",
    "start": "3372040",
    "end": "3379660"
  },
  {
    "text": "probably looking into streaming the data through nine fight two Kafka and what",
    "start": "3379660",
    "end": "3385480"
  },
  {
    "text": "did you use to create those slides they were really good it's an app called",
    "start": "3385480",
    "end": "3390849"
  },
  {
    "text": "paper and I used my daughter iPad for that she's here if you want to borrow a",
    "start": "3390849",
    "end": "3397900"
  },
  {
    "text": "ride any more questions those K streams",
    "start": "3397900",
    "end": "3408910"
  },
  {
    "text": "and K tables are they maintained in the memory of the client code that's right",
    "start": "3408910",
    "end": "3414430"
  },
  {
    "text": "that right like that has that code yes yes so the K table is maintained in the",
    "start": "3414430",
    "end": "3422050"
  },
  {
    "text": "instances of your Kafka streams application that can be any application right it doesn't it the one thing is",
    "start": "3422050",
    "end": "3429940"
  },
  {
    "text": "here is that it can do some other code outside that API if you need to but you",
    "start": "3429940",
    "end": "3435819"
  },
  {
    "text": "can also run that API within your java application right now hopefully some other languages and yes so the K table",
    "start": "3435819",
    "end": "3443980"
  },
  {
    "text": "is in that consumer being constantly updated in memory and it's up to you to",
    "start": "3443980",
    "end": "3451210"
  },
  {
    "text": "partition correctly so you don't blow up that instance and again with the K",
    "start": "3451210",
    "end": "3456460"
  },
  {
    "text": "stream yes it's basically consumer that gets the data continuously okay how do you handle",
    "start": "3456460",
    "end": "3463569"
  },
  {
    "text": "snapshotting so if your if your consumer restarts and they lose it cuz it's just in memory how's that okay probably I",
    "start": "3463569",
    "end": "3470290"
  },
  {
    "text": "want to consume for the beginning of the stream yes so that that is that is being",
    "start": "3470290",
    "end": "3475930"
  },
  {
    "text": "managed by the offset okay and and again I don't want to get into too deeply into the delivery",
    "start": "3475930",
    "end": "3481970"
  },
  {
    "text": "delivery guarantees but yes that is something that that with the offset that",
    "start": "3481970",
    "end": "3487100"
  },
  {
    "text": "is being persisted in a Kafka topic as well cough and potentially some additional",
    "start": "3487100",
    "end": "3494120"
  },
  {
    "text": "more complex mechanisms depending on your guarantees this is how it's being managed but yes this is definitely",
    "start": "3494120",
    "end": "3499550"
  },
  {
    "text": "something that Kafka streams manages yes",
    "start": "3499550",
    "end": "3505760"
  },
  {
    "text": "one second let's get you the mic yes they're gonna be available online okay",
    "start": "3505760",
    "end": "3518000"
  },
  {
    "text": "if that's all thank you very much [Applause]",
    "start": "3518000",
    "end": "3528030"
  }
]