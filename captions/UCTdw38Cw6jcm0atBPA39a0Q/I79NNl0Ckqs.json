[
  {
    "text": "my name is Michelle Frost um this is the elephant new data set um addressing bias",
    "start": "5759",
    "end": "11080"
  },
  {
    "text": "and machine learning um that's",
    "start": "11080",
    "end": "16800"
  },
  {
    "text": "Wilbur he's a good boy um much better up there than me uh by day I'm a senior",
    "start": "16800",
    "end": "22920"
  },
  {
    "text": "developer for Crema for a software um consultancy in Kansas City Missouri and",
    "start": "22920",
    "end": "29199"
  },
  {
    "text": "by night I'm a grad student at John's Hopkins um studying Ai and I graduate in four months so very excited to go back",
    "start": "29199",
    "end": "36600"
  },
  {
    "text": "to having one job it's a two clicker okay um so here's",
    "start": "36600",
    "end": "45079"
  },
  {
    "text": "our agenda today we're going to start off with a little bit of an exercise um and then we're going to talk about what is bias um what is it when we talk about",
    "start": "45079",
    "end": "52600"
  },
  {
    "text": "it in society versus um in a machine learning uh life cycle and what are the",
    "start": "52600",
    "end": "58039"
  },
  {
    "text": "different types of harmful bias how has bias and AI systems caused harm",
    "start": "58039",
    "end": "64518"
  },
  {
    "text": "how can we quantify bias how can we mitigate bias and then we're going to go through a demo um so a couple",
    "start": "64519",
    "end": "71400"
  },
  {
    "text": "disclaimers uh this is a lot to get through um it's we're going to go very uh wide but shallow know that every",
    "start": "71400",
    "end": "78960"
  },
  {
    "text": "single one of these um items of these questions could be a talk of its own um",
    "start": "78960",
    "end": "84880"
  },
  {
    "text": "so what I hope is that this kind of Sparks um some intention to you you know",
    "start": "84880",
    "end": "90400"
  },
  {
    "text": "go and do some further research um so the other thing uh the other disclaimer",
    "start": "90400",
    "end": "95840"
  },
  {
    "text": "um how has bias and AI systems caused harm we're going to have some examples uh and I don't take out the name of the",
    "start": "95840",
    "end": "102640"
  },
  {
    "text": "company because if I didn't if I did then you guys would just be Googling trying to figure out who did this um",
    "start": "102640",
    "end": "108479"
  },
  {
    "text": "there's some big companies here some of you may work for them um and that's okay they're not they're not to um this isn't",
    "start": "108479",
    "end": "113880"
  },
  {
    "text": "to call them out um and say what they've done is bad it's just to show that this can can happen if we're not intentional",
    "start": "113880",
    "end": "119399"
  },
  {
    "text": "about how um Carry On And so our goal today is for every",
    "start": "119399",
    "end": "125920"
  },
  {
    "text": "attendee to leave feeling empowered to answer these questions and be able to be an active participant um in discussion",
    "start": "125920",
    "end": "132840"
  },
  {
    "text": "of bias and AI systems so next",
    "start": "132840",
    "end": "139879"
  },
  {
    "text": "up we're going to do a little exercise a couple of you guys might be uncomfortable um if you become",
    "start": "140480",
    "end": "147319"
  },
  {
    "text": "uncomfortable I would challenge you to ask yourself why and maybe sit in that right um so",
    "start": "147319",
    "end": "156400"
  },
  {
    "text": "hopefully this this comes up here this is an intersectionality a wheel of privilege now these are privileges as",
    "start": "156400",
    "end": "162319"
  },
  {
    "text": "observed in the USA so this is going to uh you know differ a little bit for but for the most part I think that these are",
    "start": "162319",
    "end": "167599"
  },
  {
    "text": "pretty generalized um so as I go through here just kind of ask yourself if you",
    "start": "167599",
    "end": "174360"
  },
  {
    "text": "occupy this space of privilege okay and I'm not going to read through every single one of these cuz we'd be up here",
    "start": "174360",
    "end": "180159"
  },
  {
    "text": "the whole time but note that in the middle we have a space of power okay",
    "start": "180159",
    "end": "185400"
  },
  {
    "text": "that's the dominant and again observed in the USA in each of these subsections in the middle we have what we might say",
    "start": "185400",
    "end": "191720"
  },
  {
    "text": "erased so there's not a sense of power but there's also not um a sense of being",
    "start": "191720",
    "end": "197360"
  },
  {
    "text": "marginalized and then on that Outer Rim is uh a marginalized group so if we go",
    "start": "197360",
    "end": "202760"
  },
  {
    "text": "um clockwise skin",
    "start": "202760",
    "end": "207920"
  },
  {
    "text": "color neurodiversity are you neurotypical neurodiverse mental health are you",
    "start": "208000",
    "end": "216360"
  },
  {
    "text": "stable physical ability you able-bodied religion again varies from",
    "start": "216360",
    "end": "223120"
  },
  {
    "text": "country to Country in the US Christian wealth age education level housing are",
    "start": "223120",
    "end": "232519"
  },
  {
    "text": "you housed do you own a home Transportation do you have access to a car can you get wherever you need to",
    "start": "232519",
    "end": "239200"
  },
  {
    "text": "whenever you want to um political affiliation again this is going to vary across um different cultures different",
    "start": "239200",
    "end": "246280"
  },
  {
    "text": "um Geographic areas marital status citizenship",
    "start": "246280",
    "end": "252879"
  },
  {
    "text": "employment language and communication sexuality gender and body",
    "start": "252879",
    "end": "261160"
  },
  {
    "text": "size these are all spaces of privilege that we can occupy and it's important to",
    "start": "261160",
    "end": "268120"
  },
  {
    "text": "kind of understand where we sit here because if you have a privilege you might have a blind spot and if you have",
    "start": "268120",
    "end": "274840"
  },
  {
    "text": "a blind spot you can inject bias intentionally or",
    "start": "274840",
    "end": "280360"
  },
  {
    "text": "not what is bias well generally speaking when we talk about it it's an inclination or",
    "start": "281639",
    "end": "287199"
  },
  {
    "text": "Prejudice for or against one person or one group now in machine learning it's a",
    "start": "287199",
    "end": "293000"
  },
  {
    "text": "systematic error that occurs in the model itself due to assumptions made in the ml process",
    "start": "293000",
    "end": "301120"
  },
  {
    "text": "and a meme of course because we weren't going to get through this without one now bias is we talk about it in many",
    "start": "301120",
    "end": "309080"
  },
  {
    "text": "different ways um it's not always bad so I have to preface this talk saying that sometimes it's necessary um so we have",
    "start": "309080",
    "end": "315919"
  },
  {
    "text": "what's called inductive bias so machine learning is a search process right and a",
    "start": "315919",
    "end": "320960"
  },
  {
    "text": "search process needs a set of assumptions to guide the search otherwise it would be blind um so this",
    "start": "320960",
    "end": "326800"
  },
  {
    "text": "set of assumptions is what we refer to as inducted bias and this is a a more",
    "start": "326800",
    "end": "332800"
  },
  {
    "text": "formal quote from a 1997 paper by Tom Mitchell um the set of assumptions that",
    "start": "332800",
    "end": "338479"
  },
  {
    "text": "combined with observe training examples deductively entail subsequent instance classifications um made by the",
    "start": "338479",
    "end": "346759"
  },
  {
    "text": "learner the other bit we have to talk about is the Biance bias variance trade-off now in machine learning we",
    "start": "347039",
    "end": "352880"
  },
  {
    "text": "have a lot of different types of trade-offs I'm going to talk about another one very relevant to this talk a bit later um but",
    "start": "352880",
    "end": "360360"
  },
  {
    "text": "you have um I'm sure we've heard of overfitting we've heard of underfitting right so overfitting is when we have",
    "start": "360360",
    "end": "366199"
  },
  {
    "text": "very high variance um maybe your training uh your training data set is",
    "start": "366199",
    "end": "372120"
  },
  {
    "text": "performing very highly right because it's learning those examples in the training data um whereas when we have a",
    "start": "372120",
    "end": "377840"
  },
  {
    "text": "high bias we tend to",
    "start": "377840",
    "end": "381319"
  },
  {
    "text": "underfit okay one more prefacing slide before we get into the meat of this um some bias terminology",
    "start": "383840",
    "end": "390599"
  },
  {
    "text": "okay so we have a a protected class which is a category where bias is relevant and um a group is likely to be",
    "start": "390599",
    "end": "399160"
  },
  {
    "text": "differentially affected sensitive characteristics these are the features the algorithmic",
    "start": "399160",
    "end": "404880"
  },
  {
    "text": "decisions where bias could be a factor desperate treatment is when",
    "start": "404880",
    "end": "410319"
  },
  {
    "text": "members of the protected class are treated differently and then the desperate outcome or impact is the",
    "start": "410319",
    "end": "415560"
  },
  {
    "text": "effect of that treatment",
    "start": "415560",
    "end": "419160"
  },
  {
    "text": "so to introduce bias in machine learning um I'm going to use a paper that came",
    "start": "420680",
    "end": "426160"
  },
  {
    "text": "out in 2021 um by sesh and gut and what they",
    "start": "426160",
    "end": "431360"
  },
  {
    "text": "did is they identified seven different potential places where we could inject harmful bias in the m in the machine",
    "start": "431360",
    "end": "437840"
  },
  {
    "text": "learning life cycle um so this should look pretty familiar with to us uh the",
    "start": "437840",
    "end": "443120"
  },
  {
    "text": "first half up above is the data generation flow down below model building and implementation",
    "start": "443120",
    "end": "451319"
  },
  {
    "text": "here are the seven places that we could inject bias we're going to go through these one by one and talk about them and",
    "start": "456759",
    "end": "462759"
  },
  {
    "text": "then Show an example for each um but of course we have our historical uh bias",
    "start": "462759",
    "end": "468639"
  },
  {
    "text": "comes from data Generation Um we have representation bias we have measurement bias okay these",
    "start": "468639",
    "end": "475520"
  },
  {
    "text": "are all in the data Generation Um and then we start kind of uh let's say you're",
    "start": "475520",
    "end": "483680"
  },
  {
    "text": "11:00 counterclockwise aggregation bias and then learning bias evaluation bias",
    "start": "483680",
    "end": "491280"
  },
  {
    "text": "and lastly deployment bias historical bias um this is probably",
    "start": "491280",
    "end": "499039"
  },
  {
    "text": "the easiest for us to predict right garbage in garbage out um even if the data is measured perfectly sampled",
    "start": "499039",
    "end": "506919"
  },
  {
    "text": "perfectly if the world as it is as it was has Prejudice then it it's going to",
    "start": "506919",
    "end": "512959"
  },
  {
    "text": "lead to a model that has harmful outcomes if you don't mitigate",
    "start": "512959",
    "end": "518719"
  },
  {
    "text": "them gender bias and chat GPT um there's actually a lot of different biases in chat GPT but this is",
    "start": "520479",
    "end": "527040"
  },
  {
    "text": "a particular study um by uh Dr kodic and Dr um riton where they um we're",
    "start": "527040",
    "end": "534440"
  },
  {
    "text": "interrogating chat GPT to try and get it to say that a doctor could be a woman",
    "start": "534440",
    "end": "540800"
  },
  {
    "text": "and a nurse could be a male and so they set up these sentence um structures to where they could swap the pronouns out",
    "start": "540800",
    "end": "548040"
  },
  {
    "text": "and even when it did not make sense chat GPT still said that the nurse was female",
    "start": "548040",
    "end": "553720"
  },
  {
    "text": "and that the doctor was male you can do the same thing tell me a story about an engineer it's going to",
    "start": "553720",
    "end": "559519"
  },
  {
    "text": "tell you a story about a man um tell me a story about an engineer named Michelle and it's going to go and tell you how I",
    "start": "559519",
    "end": "565839"
  },
  {
    "text": "fought against the patriarchy um same thing tell me a story about a nurse who is a man it's going to",
    "start": "565839",
    "end": "572640"
  },
  {
    "text": "say how uh his family wasn't sure if nursing was um you know the right fit",
    "start": "572640",
    "end": "577959"
  },
  {
    "text": "for him and and this is within uh within chat",
    "start": "577959",
    "end": "582720"
  },
  {
    "text": "gbt so next we get into representation bias and this is when a sample under",
    "start": "583399",
    "end": "589480"
  },
  {
    "text": "represents some part of the population and it doesn't generalize well um so it",
    "start": "589480",
    "end": "594920"
  },
  {
    "text": "it's problematic when the target population does not reflect the Ed popul",
    "start": "594920",
    "end": "600519"
  },
  {
    "text": "or if that Target population contains under represented groups uh or if the",
    "start": "600519",
    "end": "605760"
  },
  {
    "text": "sample method is limited or uneven so you know it's easier for us to imagine",
    "start": "605760",
    "end": "611839"
  },
  {
    "text": "this you've got a sample you've got the pink dots you've got the blue dots uh",
    "start": "611839",
    "end": "617200"
  },
  {
    "text": "how do you think it's going to perform on the blue dots thank",
    "start": "617200",
    "end": "624760"
  },
  {
    "text": "you anyone remember this one",
    "start": "624959",
    "end": "629800"
  },
  {
    "text": "uh-huh Google started identifying African-Americans black",
    "start": "633800",
    "end": "640680"
  },
  {
    "text": "individuals as gorillas an engineer discovered this",
    "start": "640680",
    "end": "646600"
  },
  {
    "text": "posted it on Twitter does anyone know how they fixed",
    "start": "646600",
    "end": "652399"
  },
  {
    "text": "this they removed gorilla as a label",
    "start": "654240",
    "end": "659639"
  },
  {
    "text": "yeah measurement bias so when we're choosing collecting",
    "start": "664480",
    "end": "670079"
  },
  {
    "text": "or Computing features um as a proxy to represent some construct remember that",
    "start": "670079",
    "end": "677839"
  },
  {
    "text": "um we we're guessing here in some cases right it's easy for us to say an apple",
    "start": "677839",
    "end": "684320"
  },
  {
    "text": "is an apple a pair is a pair but what about when we're measuring things like creditworthiness",
    "start": "684320",
    "end": "690800"
  },
  {
    "text": "can we really say that we know which features will belong to that um and so this becomes problematic when we have an",
    "start": "690800",
    "end": "697440"
  },
  {
    "text": "over oversimplification of a more complex construct um if the method of",
    "start": "697440",
    "end": "703040"
  },
  {
    "text": "measurement varies across groups and if that accuracy of measurement varies across",
    "start": "703040",
    "end": "709680"
  },
  {
    "text": "groups who here has heard of compass so this is a compass",
    "start": "709880",
    "end": "717000"
  },
  {
    "text": "Correctional Fender management profiling for alternative sanctions okay this was developed by an Illinois company in the",
    "start": "717000",
    "end": "723880"
  },
  {
    "text": "late '90s called North Point and their goal was to help judges determine if",
    "start": "723880",
    "end": "730639"
  },
  {
    "text": "bail should be set um for offending criminals um",
    "start": "730639",
    "end": "738320"
  },
  {
    "text": "and so it's predicting recidivism right if if someone that's committed a crime",
    "start": "738320",
    "end": "744279"
  },
  {
    "text": "is going to go and commit another crime um does recidivism though truly indicate",
    "start": "744279",
    "end": "750639"
  },
  {
    "text": "the presence of a crime or does it indicate the presence of policing is it the right measurement tool so this went",
    "start": "750639",
    "end": "756880"
  },
  {
    "text": "into effect and and the last I checked it was being used by 49 out of 50 states by the way it wasn't public or was it",
    "start": "756880",
    "end": "764519"
  },
  {
    "text": "was not independently reviewed until 2017 when Pro publica um took its",
    "start": "764519",
    "end": "770720"
  },
  {
    "text": "findings they spent about a two threee study uh looking at the different data and what they found was though as North",
    "start": "770720",
    "end": "778639"
  },
  {
    "text": "Point report the average accuracy between white and black defendants was about the",
    "start": "778639",
    "end": "784760"
  },
  {
    "text": "same the types of errors were very different so labeled higher risk but",
    "start": "784760",
    "end": "791800"
  },
  {
    "text": "didn't reoffend see if I can right there um so White's 23.5% error compared to",
    "start": "791800",
    "end": "799399"
  },
  {
    "text": "44.9% error for black individuals label lower risk yet",
    "start": "799399",
    "end": "805839"
  },
  {
    "text": "did reoffend White 47.7% okay so our false negative for white",
    "start": "805839",
    "end": "813199"
  },
  {
    "text": "defendants was almost twice as high as black defendants and our false positive rate for Block defendants is almost",
    "start": "813199",
    "end": "819760"
  },
  {
    "text": "twice as high now it's worth noting that Pro uh that northp point rejected propublica's claims saying that they",
    "start": "819760",
    "end": "826680"
  },
  {
    "text": "interpreted the types of Errors incorrectly but there's been lots of studies done um in particular about comp",
    "start": "826680",
    "end": "832440"
  },
  {
    "text": "Compass since that time okay aggregation bias",
    "start": "832440",
    "end": "839880"
  },
  {
    "text": "um this is the assumption that a oniz fits-all model uh can map inputs to",
    "start": "839880",
    "end": "845040"
  },
  {
    "text": "labels across all subsets of the data right so um if a data set represents",
    "start": "845040",
    "end": "851880"
  },
  {
    "text": "people or groups from different backgrounds and cultures a given variable might mean something different",
    "start": "851880",
    "end": "857160"
  },
  {
    "text": "for them if we were running some sort of a text classification okay on my phone",
    "start": "857160",
    "end": "862839"
  },
  {
    "text": "maybe and um maybe we're trying to identify risky behavior and I've shot",
    "start": "862839",
    "end": "867920"
  },
  {
    "text": "out a text that says I'm going to be late I'm on LSD so you guys are probably making two",
    "start": "867920",
    "end": "874959"
  },
  {
    "text": "assumptions number one I'm on drugs and number two I've made bad decisions um has anyone been to",
    "start": "874959",
    "end": "881279"
  },
  {
    "text": "Chicago worst Street to drive on is Lakes Shore Drive we call it LSD I've still made a bad choice but I'm",
    "start": "881279",
    "end": "888959"
  },
  {
    "text": "probably not on drugs this also comes out in healthcare",
    "start": "888959",
    "end": "895279"
  },
  {
    "text": "okay um so uh there was a big study a couple years ago there's different levels of hemoglobin performs",
    "start": "895279",
    "end": "901680"
  },
  {
    "text": "differently across different ethnicities so if we don't account for that in our systems we're going to have varying",
    "start": "901680",
    "end": "908800"
  },
  {
    "text": "results across um different",
    "start": "908800",
    "end": "912720"
  },
  {
    "text": "subsets learning bias when our modeling choices prioritize one objective that",
    "start": "914680",
    "end": "920320"
  },
  {
    "text": "damages another remember we talked about that tradeoff so this comes across not",
    "start": "920320",
    "end": "925360"
  },
  {
    "text": "just bi bias various not just accuracy and fairness we'll talk about that but um you you know if you prioritize",
    "start": "925360",
    "end": "934399"
  },
  {
    "text": "accuracy you might take a hit in disperate Impact um so if we minimize for example",
    "start": "934399",
    "end": "941959"
  },
  {
    "text": "our cross entropy loss for classification um that might lead to a model that has more false",
    "start": "941959",
    "end": "948920"
  },
  {
    "text": "positives Amazon um this was big news in 2018 uh they",
    "start": "949680",
    "end": "956199"
  },
  {
    "text": "created an AI hiring tool and it immediately started saying that the the women weren't going to do the job well",
    "start": "956199",
    "end": "963279"
  },
  {
    "text": "um this was especially prevalent um across the technical and Engineering",
    "start": "963279",
    "end": "968720"
  },
  {
    "text": "roles so then we start to evaluate our models right we want to see how they're performing um but we do that often",
    "start": "973079",
    "end": "980880"
  },
  {
    "text": "against benchmarks right what if that Benchmark data doesn't represent the use",
    "start": "980880",
    "end": "987079"
  },
  {
    "text": "population things like below average average above average those are really",
    "start": "987079",
    "end": "992480"
  },
  {
    "text": "subjective right so if we don't start looking um Across The Wider group and",
    "start": "992480",
    "end": "998759"
  },
  {
    "text": "into individual subsets uh they're not going to perform very well um project",
    "start": "998759",
    "end": "1005160"
  },
  {
    "text": "gender Shades uh I highly recommend going there they have a beautiful website um and they did studies on",
    "start": "1005160",
    "end": "1011959"
  },
  {
    "text": "facial recognition performance um across three main um uh IBM Microsoft and then",
    "start": "1011959",
    "end": "1020160"
  },
  {
    "text": "face and so if you looked at this the biggest difference and error um again this website beautiful UI um and it'll",
    "start": "1020160",
    "end": "1026600"
  },
  {
    "text": "dig very specifically into the different types of error but there is a 34.4% difference between a white man and",
    "start": "1026600",
    "end": "1034798"
  },
  {
    "text": "a black woman from IBM that was the largest gap of",
    "start": "1034799",
    "end": "1040079"
  },
  {
    "text": "error last but not least deployment bias okay what if we intended to solve a",
    "start": "1041600",
    "end": "1048600"
  },
  {
    "text": "problem but somebody else is using it in another way we can also look back to that Compass right when they developed it um",
    "start": "1048600",
    "end": "1055320"
  },
  {
    "text": "they wanted to help judges determine if uh if and what bail should be set",
    "start": "1055320",
    "end": "1060760"
  },
  {
    "text": "at but then Court started using it to sentence that wasn't the original model",
    "start": "1060760",
    "end": "1066440"
  },
  {
    "text": "but as Engineers um and as stakeholders as anybody engaging with a product we have to think about how it can be",
    "start": "1066440",
    "end": "1073120"
  },
  {
    "text": "incorrectly used",
    "start": "1073120",
    "end": "1076760"
  },
  {
    "text": "I have a a slide for recommended reading at the very end but um specifically Virginia eubank's automating inequality",
    "start": "1080600",
    "end": "1088559"
  },
  {
    "text": "she looks at three different um cases where we built an AI system and it",
    "start": "1088559",
    "end": "1094120"
  },
  {
    "text": "failed poor families so in particular um a very alarming one was a child abuse",
    "start": "1094120",
    "end": "1099280"
  },
  {
    "text": "prediction model okay if you look at a you know a middle class family and a",
    "start": "1099280",
    "end": "1104720"
  },
  {
    "text": "child is being left in the cold okay you know maybe we should look into",
    "start": "1104720",
    "end": "1110880"
  },
  {
    "text": "that or if they're not being fed often enough but what about for families who can't afford that same",
    "start": "1110880",
    "end": "1117720"
  },
  {
    "text": "thing so what are we going to do about it well can't we just admit all the",
    "start": "1118960",
    "end": "1125080"
  },
  {
    "text": "protected attributes no you cannot um so we can",
    "start": "1125080",
    "end": "1131240"
  },
  {
    "text": "explain this through Simpson's Paradox uh which gets pretty technical it's a St",
    "start": "1131240",
    "end": "1136559"
  },
  {
    "text": "a statistical phenomenon um but basically what it comes down to",
    "start": "1136559",
    "end": "1141840"
  },
  {
    "text": "is that our features have been so bled in right even if we take something out",
    "start": "1141840",
    "end": "1147000"
  },
  {
    "text": "like let's say gender and income we have other features um that that attribute",
    "start": "1147000",
    "end": "1153120"
  },
  {
    "text": "has bled into right because our our data is entangled so we can't just take out",
    "start": "1153120",
    "end": "1158960"
  },
  {
    "text": "one feature or one column and expect to get unbiased",
    "start": "1158960",
    "end": "1164679"
  },
  {
    "text": "results we also have to talk about explainability here right because the more complex our model is um we might",
    "start": "1166000",
    "end": "1174679"
  },
  {
    "text": "gain accuracy but our explainability is going to decrease we're going to have a",
    "start": "1174679",
    "end": "1180039"
  },
  {
    "text": "harder time figuring out why we have bias in a model why certain groups are performing differently than others um",
    "start": "1180039",
    "end": "1187600"
  },
  {
    "text": "and we need to be able to explain them right especially in relation to human affected",
    "start": "1187600",
    "end": "1193400"
  },
  {
    "text": "applications so this brings us to fairness um my particular favorite area of",
    "start": "1195559",
    "end": "1202840"
  },
  {
    "text": "research um in responsible Ai and basically what we're doing is we're trying to study how to ensure that these",
    "start": "1202840",
    "end": "1208799"
  },
  {
    "text": "biases and model inaccuracies don't lead to models that treat individuals unfavorably due to sensitive",
    "start": "1208799",
    "end": "1216120"
  },
  {
    "text": "characteristics um another meme for you all 2017 2016 was like the year fairness",
    "start": "1216120",
    "end": "1226200"
  },
  {
    "text": "papers exploded um that trend has kept going there's new fairness papers coming",
    "start": "1226200",
    "end": "1232400"
  },
  {
    "text": "out all the time and there's some really fabulous work being done in the area so before we start getting a little",
    "start": "1232400",
    "end": "1239840"
  },
  {
    "text": "technical um we'll just do a brief review the different types of Errors",
    "start": "1239840",
    "end": "1247480"
  },
  {
    "text": "so we want to compare what our predicted values were right against what the",
    "start": "1247520",
    "end": "1253320"
  },
  {
    "text": "actual values are so we know true positive true negative we got it right",
    "start": "1253320",
    "end": "1258919"
  },
  {
    "text": "okay true positive or sorry false positive um we predicted positive it was",
    "start": "1258919",
    "end": "1265679"
  },
  {
    "text": "negative false negative we predicted negative it was positive okay um I think",
    "start": "1265679",
    "end": "1271400"
  },
  {
    "text": "this has become a little bit more public too since the covid test came out right um you can kind of start thinking about",
    "start": "1271400",
    "end": "1276640"
  },
  {
    "text": "which false you'd rather get depending on the circumstances so there are about",
    "start": "1276640",
    "end": "1283640"
  },
  {
    "text": "20 I'm going to misquote myself between 20 and 25 um different types of",
    "start": "1283640",
    "end": "1289120"
  },
  {
    "text": "measurements for fairness okay a lot of people have been working on this um but there's really three that we want to",
    "start": "1289120",
    "end": "1296159"
  },
  {
    "text": "understand so first statistical parody um 2012 or 2012 D work came up with this",
    "start": "1296159",
    "end": "1304039"
  },
  {
    "text": "definition um and this is achieved when the decisions or outcomes of a model are independent of a protected attribute now",
    "start": "1304039",
    "end": "1311960"
  },
  {
    "text": "something to um note here is that this is caring about the predictive",
    "start": "1311960",
    "end": "1319159"
  },
  {
    "text": "equality not the ground truth okay so we want to see that we're um accurately",
    "start": "1319159",
    "end": "1326960"
  },
  {
    "text": "favoring uh women and men for a positive prediction okay we want to have that to",
    "start": "1326960",
    "end": "1333279"
  },
  {
    "text": "be kind of balanced and the difference is going to be what that difference is we can also look at it in a",
    "start": "1333279",
    "end": "1340120"
  },
  {
    "text": "ratio then we're going to get into the most restrictive concept okay equalized odds now we do care We Care both about",
    "start": "1341919",
    "end": "1349559"
  },
  {
    "text": "our predictive equality and what that ground truth is and equalized odds can",
    "start": "1349559",
    "end": "1354880"
  },
  {
    "text": "only be achieved when the true um oh that's a tight bow there the true",
    "start": "1354880",
    "end": "1360919"
  },
  {
    "text": "positive rate and the false positive rate are equal across different groups okay so we can use this when um we want",
    "start": "1360919",
    "end": "1369200"
  },
  {
    "text": "to place a strong emphasis on predicting the positive outcome correctly or if we",
    "start": "1369200",
    "end": "1375200"
  },
  {
    "text": "want to minimize a costly false positive or if the target variable is not",
    "start": "1375200",
    "end": "1382440"
  },
  {
    "text": "subjective and heart and all 2016 came up with this definition as well as equal",
    "start": "1383159",
    "end": "1389760"
  },
  {
    "text": "opportunity um which is a very specific instant instantiation of equalized odds",
    "start": "1389760",
    "end": "1395200"
  },
  {
    "text": "that measures fairness in terms of the true positive rates okay so um if we",
    "start": "1395200",
    "end": "1400840"
  },
  {
    "text": "want to uh predict positive outcomes correctly okay so like that recidivism",
    "start": "1400840",
    "end": "1407679"
  },
  {
    "text": "task that might have been important um false positives are not costly Target variabl is not",
    "start": "1407679",
    "end": "1416440"
  },
  {
    "text": "subjective uh we can also there's another um predictive equality that then measures the true negative rates as well",
    "start": "1417640",
    "end": "1425480"
  },
  {
    "text": "um but that one's not used as heavily so what are we going to do about it mitigation strategies uh this in and",
    "start": "1425480",
    "end": "1434159"
  },
  {
    "text": "of itself could be a workshop okay um we can divide the mitigation strategies",
    "start": "1434159",
    "end": "1440320"
  },
  {
    "text": "into basically three different points okay pre-processing right so we want to",
    "start": "1440320",
    "end": "1446039"
  },
  {
    "text": "apply mitigation to the training data itself I don't have any examples up there because I'm going to walk through",
    "start": "1446039",
    "end": "1452159"
  },
  {
    "text": "several in a demo for you all um in processing so we want to do this during model training okay um so one example",
    "start": "1452159",
    "end": "1460840"
  },
  {
    "text": "adversarial debiasing um we all know what what Gans are right generative",
    "start": "1460840",
    "end": "1466480"
  },
  {
    "text": "adversarial networks it kind of performs the the same um you Loop two up together",
    "start": "1466480",
    "end": "1471919"
  },
  {
    "text": "and one learns to maximize the prediction accuracy and then the um and",
    "start": "1471919",
    "end": "1477200"
  },
  {
    "text": "you want to reduce that adversary's ability uh to determine the protected attribute from those",
    "start": "1477200",
    "end": "1483679"
  },
  {
    "text": "predictions um we can also put in what we call a Prejudice remover which is a discrimination aware regularization",
    "start": "1483679",
    "end": "1492158"
  },
  {
    "text": "term there are other methods as well like using multiobjective optimization",
    "start": "1492320",
    "end": "1498480"
  },
  {
    "text": "to look both not only at accuracy but also fairness",
    "start": "1498480",
    "end": "1503760"
  },
  {
    "text": "okay then in post-processing we want to mitigate after training equalize odds so",
    "start": "1503760",
    "end": "1510480"
  },
  {
    "text": "we could change the output labels we could classify the reject options and give favorable outcomes to",
    "start": "1510480",
    "end": "1516039"
  },
  {
    "text": "underprivileged groups um in my mind this is a little bit like trying to",
    "start": "1516039",
    "end": "1522600"
  },
  {
    "text": "retroactively fix the problem right we really want to do it in if we can at the data layer or during the model training",
    "start": "1522600",
    "end": "1529720"
  },
  {
    "text": "itself um and if we can get this right and we can get enough people to use it",
    "start": "1529720",
    "end": "1535200"
  },
  {
    "text": "you know then we can start deploying this in um highly used production",
    "start": "1535200",
    "end": "1540960"
  },
  {
    "text": "systems demo okay this is the part where we hope it works just",
    "start": "1543080",
    "end": "1548600"
  },
  {
    "text": "fine so we are going to pause this one",
    "start": "1548600",
    "end": "1558480"
  },
  {
    "text": "bring her",
    "start": "1559279",
    "end": "1562000"
  },
  {
    "text": "up and mirror and it's very small",
    "start": "1565919",
    "end": "1571600"
  },
  {
    "text": "great so let's do this and it's still kind of small",
    "start": "1571600",
    "end": "1580000"
  },
  {
    "text": "so still kind of small but we're going to walk through it together verbally",
    "start": "1584440",
    "end": "1590240"
  },
  {
    "text": "come on I was trying to minimize it just a little bit that's fine so um this is a data set it's on",
    "start": "1594000",
    "end": "1603080"
  },
  {
    "text": "the UCI um machine learning repository it's called the adult data set um perhaps some of you guys have heard",
    "start": "1603080",
    "end": "1608919"
  },
  {
    "text": "about it uh I would never use this thing in production because it's really Antiquated and it's just not great um",
    "start": "1608919",
    "end": "1616320"
  },
  {
    "text": "but it's used a lot in um studies on fairness because of um the differences",
    "start": "1616320",
    "end": "1622520"
  },
  {
    "text": "so the adult data set was trying to predict um High versus low income and",
    "start": "1622520",
    "end": "1628120"
  },
  {
    "text": "now they Define that as being uh at 50 50k USD um I think that this data set",
    "start": "1628120",
    "end": "1634640"
  },
  {
    "text": "was compiled in the 9s if I'm not mistaken um so obviously we would do",
    "start": "1634640",
    "end": "1639799"
  },
  {
    "text": "this a little bit differently but the reason it's used so frequently is because there are a lot of protected attributes in here okay there's race",
    "start": "1639799",
    "end": "1647520"
  },
  {
    "text": "there's gender there's education levels marital status um we could dissect each one of these",
    "start": "1647520",
    "end": "1653679"
  },
  {
    "text": "and see uh you know how bias performs against each",
    "start": "1653679",
    "end": "1658760"
  },
  {
    "text": "one um so you can kind of get a little look there",
    "start": "1658760",
    "end": "1664840"
  },
  {
    "text": "and so let's just kind of look at some of these different groupings",
    "start": "1664840",
    "end": "1669960"
  },
  {
    "text": "right our female to male data samples not good our uh bias example is",
    "start": "1669960",
    "end": "1677440"
  },
  {
    "text": "going to look at gender um but we're going to I'll show you guys a couple different other groups as",
    "start": "1677440",
    "end": "1683360"
  },
  {
    "text": "well white",
    "start": "1684200",
    "end": "1687480"
  },
  {
    "text": "observations United States countries we've got just barely any",
    "start": "1691279",
    "end": "1697000"
  },
  {
    "text": "representation from anywhere else um the next one right here is actually it's",
    "start": "1697000",
    "end": "1703159"
  },
  {
    "text": "Mexico so same with um education levels we have",
    "start": "1703159",
    "end": "1710120"
  },
  {
    "text": "a lot that are high school grads um we have some very few in preschool some",
    "start": "1710120",
    "end": "1717559"
  },
  {
    "text": "college um this varies a little bit better but it's still it's not",
    "start": "1717559",
    "end": "1723200"
  },
  {
    "text": "great different types of working and and then this is our Target",
    "start": "1724679",
    "end": "1732760"
  },
  {
    "text": "column got like a third in the positive column right",
    "start": "1732760",
    "end": "1739880"
  },
  {
    "text": "if we start looking at this by gender we have barely any females in the",
    "start": "1740640",
    "end": "1747640"
  },
  {
    "text": "positive outcome compare that you know just to the ratio of the male they're a little",
    "start": "1747640",
    "end": "1754760"
  },
  {
    "text": "bit more balanced right still still pretty off but a little bit",
    "start": "1754760",
    "end": "1759960"
  },
  {
    "text": "better same if we start looking at this by um race and gender",
    "start": "1763000",
    "end": "1768840"
  },
  {
    "text": "the how the different groups perform um and then I magically",
    "start": "1768840",
    "end": "1774240"
  },
  {
    "text": "pre-process this data set for you all because that's not what we're doing right okay so if we just run a general",
    "start": "1774240",
    "end": "1781080"
  },
  {
    "text": "logistic regression on this we get an A an average accuracy of about 84% okay um",
    "start": "1781080",
    "end": "1789200"
  },
  {
    "text": "so without debiasing pull this down here so our accuracy for female 91% almost",
    "start": "1789200",
    "end": "1796799"
  },
  {
    "text": "92% not bad right why is that CU it's pretty safe to say that",
    "start": "1796799",
    "end": "1804799"
  },
  {
    "text": "they're going to be in the negative column right um so here's what we're going to start",
    "start": "1804799",
    "end": "1811080"
  },
  {
    "text": "looking at through the rest of this demo these are our rates and I know it can be a little hard to see so remember",
    "start": "1811080",
    "end": "1817200"
  },
  {
    "text": "that orange is false positive and red is false negative okay",
    "start": "1817200",
    "end": "1823480"
  },
  {
    "text": "we hardly misclassified that false positive for the female group",
    "start": "1823480",
    "end": "1828640"
  },
  {
    "text": "right um our false positive for males is much higher which makes sense because",
    "start": "1828640",
    "end": "1833960"
  },
  {
    "text": "there's more males in the positive",
    "start": "1833960",
    "end": "1838360"
  },
  {
    "text": "class that's a note to Michelle to go to the dashboard because I forget it okay so um this is a",
    "start": "1841559",
    "end": "1850120"
  },
  {
    "text": "tool from Microsoft um on GitHub they have a responsible AI toolkit okay there's a",
    "start": "1850120",
    "end": "1857000"
  },
  {
    "text": "lot of different things to kind of explore and play around with in here um but what I have here is their fairness",
    "start": "1857000",
    "end": "1863480"
  },
  {
    "text": "dashboard which comes from their responsible AI widgets package and this is really cool because you can set it up",
    "start": "1863480",
    "end": "1870039"
  },
  {
    "text": "I will tell you that there's some like package difficulties with this so you have to run it on a separate virtual",
    "start": "1870039",
    "end": "1876279"
  },
  {
    "text": "environment um but it lets you upload your model select your sensitive features and then you can start looking",
    "start": "1876279",
    "end": "1882639"
  },
  {
    "text": "at different performance metrics um by either difference or ratio",
    "start": "1882639",
    "end": "1888360"
  },
  {
    "text": "so if I get started um so we're looking at",
    "start": "1888360",
    "end": "1893559"
  },
  {
    "text": "if we're male let's go ahead and look at balanced",
    "start": "1893559",
    "end": "1899880"
  },
  {
    "text": "accuracy and we talked about demographic",
    "start": "1899880",
    "end": "1905080"
  },
  {
    "text": "parity difference we'll click around through a few of these and then what I want is the false",
    "start": "1905080",
    "end": "1912159"
  },
  {
    "text": "positive and false negative rates okay so the top bar is our female",
    "start": "1912159",
    "end": "1919320"
  },
  {
    "text": "class Bottom bar is male orange is our false negative rates and blue is our",
    "start": "1919320",
    "end": "1925720"
  },
  {
    "text": "false positive rates Okay so we've got a pretty good big gap there and if we're",
    "start": "1925720",
    "end": "1931440"
  },
  {
    "text": "looking at the demographic parity difference it's",
    "start": "1931440",
    "end": "1935760"
  },
  {
    "text": "19% what are we going to do about it so if we're looking at pre-processing mitigation techniques um there's a",
    "start": "1937080",
    "end": "1944799"
  },
  {
    "text": "couple ways we can go about it now remember I told you all that awareness",
    "start": "1944799",
    "end": "1950120"
  },
  {
    "text": "unawareness doesn't really work right we can't really just remove that protected attribute and expect that it doesn't",
    "start": "1950120",
    "end": "1956799"
  },
  {
    "text": "know about it um so proof here for you",
    "start": "1956799",
    "end": "1962000"
  },
  {
    "text": "um our overall accuracy stayed about the same uh the accuracy for females I think dropped just a tad",
    "start": "1962000",
    "end": "1969799"
  },
  {
    "text": "and overall we're still looking quite the same right our false positive rate for females is still um much smaller",
    "start": "1969799",
    "end": "1976679"
  },
  {
    "text": "than it is for males false negative rate still higher for females than for",
    "start": "1976679",
    "end": "1983080"
  },
  {
    "text": "males okay so what if we balanced it right what if we made sure to take the",
    "start": "1983159",
    "end": "1989559"
  },
  {
    "text": "same number of males as we do",
    "start": "1989559",
    "end": "1994559"
  },
  {
    "text": "females we get a tensy bit better maybe but not by much right why is that",
    "start": "1994760",
    "end": "2002240"
  },
  {
    "text": "because the target is still the same right we had an imbalance in the in the",
    "start": "2002240",
    "end": "2007279"
  },
  {
    "text": "Target r ratio so what if we do",
    "start": "2007279",
    "end": "2013799"
  },
  {
    "text": "that okay we're starting to get better something to keep in mind here though",
    "start": "2015000",
    "end": "2020279"
  },
  {
    "text": "remember how small that female class was compared to the male remember how small",
    "start": "2020279",
    "end": "2025480"
  },
  {
    "text": "that positive female class was compared to what we had available for the male class uh if we do this we're limiting",
    "start": "2025480",
    "end": "2033720"
  },
  {
    "text": "our data set to the maximum of what we have right which which wasn't very much so we just took our data set and we took",
    "start": "2033720",
    "end": "2041080"
  },
  {
    "text": "a small percentage of it and that might not be realistic um when you're in a",
    "start": "2041080",
    "end": "2046279"
  },
  {
    "text": "production use case and you need a lot of data still we should know about these",
    "start": "2046279",
    "end": "2052520"
  },
  {
    "text": "things right so if we start looking at the um rates here so the false positive",
    "start": "2052520",
    "end": "2060079"
  },
  {
    "text": "rates for female 07 false positive rates for male .1 one",
    "start": "2060079",
    "end": "2068480"
  },
  {
    "text": "not perfect but we've gotten a little bit better right um false negative rates",
    "start": "2068480",
    "end": "2073560"
  },
  {
    "text": "are super close 08 difference um we're looking better maybe we'll take",
    "start": "2073560",
    "end": "2083760"
  },
  {
    "text": "this okay there's another uh there's another group that says we can do counterfactual augmentation because you",
    "start": "2084560",
    "end": "2090800"
  },
  {
    "text": "can't take out that attribute right but what if you flipped it so then what we're doing um is",
    "start": "2090800",
    "end": "2099800"
  },
  {
    "text": "instead of uh saying that you know that we have that Simpsons Paradox where it's bled into other other places we kind of",
    "start": "2099800",
    "end": "2107280"
  },
  {
    "text": "confuse the model because what it thinks it's learned about that gets flipped on its head and if you look at these",
    "start": "2107280",
    "end": "2114599"
  },
  {
    "text": "numbers they're pretty close right why would I not recommend using",
    "start": "2114599",
    "end": "2121240"
  },
  {
    "text": "this in production uh because you're flipping the labels on their heads so um it's great for a study just to prove",
    "start": "2121240",
    "end": "2127440"
  },
  {
    "text": "that can do it um but I wouldn't recommend doing this in a in a production system",
    "start": "2127440",
    "end": "2134400"
  },
  {
    "text": "um so let's go down here and check these",
    "start": "2134400",
    "end": "2139880"
  },
  {
    "text": "out so our ex's are our females our dots are our males um and we can kind of start to see",
    "start": "2139880",
    "end": "2148359"
  },
  {
    "text": "where we're mitigating those differences in the rates right so on the left that",
    "start": "2148359",
    "end": "2153640"
  },
  {
    "text": "was our original and um moving over to the last two we have our balanced ratio",
    "start": "2153640",
    "end": "2158960"
  },
  {
    "text": "and the counterfactual arguments and we're getting that Circle and the cross",
    "start": "2158960",
    "end": "2164599"
  },
  {
    "text": "very close together now aren't we um it's almost indistinguishable on that",
    "start": "2164599",
    "end": "2170040"
  },
  {
    "text": "counterfactual uh version but again I would say use caution if you're using that method on a real data",
    "start": "2170040",
    "end": "2178000"
  },
  {
    "text": "set okay so this is another inst instantiation of the fairness dashboard",
    "start": "2180359",
    "end": "2185880"
  },
  {
    "text": "and we're looking at the um balanced Target ratio uh again I wanted to show",
    "start": "2185880",
    "end": "2192880"
  },
  {
    "text": "you guys the counterfactual but we're going to look at something that would be a little bit safer in",
    "start": "2192880",
    "end": "2198599"
  },
  {
    "text": "production so we're going to hit balanced accuracy again for",
    "start": "2200160",
    "end": "2206640"
  },
  {
    "text": "parity and we'll look at these false positive and false negative",
    "start": "2206640",
    "end": "2212440"
  },
  {
    "text": "rates it's not perfect but it's a lot better than that okay our",
    "start": "2212560",
    "end": "2220280"
  },
  {
    "text": "difference went from 19% to 1.08% now demographic parody might not",
    "start": "2220280",
    "end": "2227599"
  },
  {
    "text": "be the right metric right we talked about these metrics we talked about when when they make sense to use them and",
    "start": "2227599",
    "end": "2233280"
  },
  {
    "text": "when you need to dig a little bit deeper um this fairness dashboard lets you look",
    "start": "2233280",
    "end": "2239359"
  },
  {
    "text": "at other things too so let's look at the equalized odds ratio for both of",
    "start": "2239359",
    "end": "2246119"
  },
  {
    "text": "these excuse me the difference so equalize odds difference in our original",
    "start": "2246119",
    "end": "2252240"
  },
  {
    "text": "un um changed Model",
    "start": "2252240",
    "end": "2257119"
  },
  {
    "text": "15.3% come on and it goes down to",
    "start": "2257520",
    "end": "2263079"
  },
  {
    "text": "5.16% so it's getting better",
    "start": "2264079",
    "end": "2269630"
  },
  {
    "text": "[Music] now this is the tricky part",
    "start": "2269630",
    "end": "2275960"
  },
  {
    "text": "this is where I have a lot of things for you guys to take photos of book",
    "start": "2294240",
    "end": "2300079"
  },
  {
    "text": "recommendations um I could probably add to this but these are my top six favorite so starting on um the top left",
    "start": "2300079",
    "end": "2307640"
  },
  {
    "text": "weapons of math destruction Cassie O'Neal if you haven't read this it's absolutely fabulous um it looks a little",
    "start": "2307640",
    "end": "2314160"
  },
  {
    "text": "bit more from a statistical perspective um but she talks about a lot of uh a lot of different use cases where um we've",
    "start": "2314160",
    "end": "2321880"
  },
  {
    "text": "created some sort of statistical model that uh caused harm biased again not super technical um",
    "start": "2321880",
    "end": "2330079"
  },
  {
    "text": "it's a quick read but it's uh something that we need to think about right we looked at our privileges we thought",
    "start": "2330079",
    "end": "2336520"
  },
  {
    "text": "about that um bias is going to dig a little bit deeper it also talks a lot about that Compass system that um I",
    "start": "2336520",
    "end": "2343440"
  },
  {
    "text": "mentioned the alignment problem was a lot of fun Brian Christian and he kind of talks about that um so you know we're",
    "start": "2343440",
    "end": "2349839"
  },
  {
    "text": "talking about AI systems how do we align um the need for a robust system and an",
    "start": "2349839",
    "end": "2356040"
  },
  {
    "text": "accurate system with how is it going to impact Society how is it going to impact individuals that um you know it's making",
    "start": "2356040",
    "end": "2363880"
  },
  {
    "text": "predictions on um and he talks a lot about reinforce learning too and the psychology behind reinforcement learning",
    "start": "2363880",
    "end": "2370400"
  },
  {
    "text": "so if you want a nerd out it's a good place to do it uh Invisible Woman you can kind of",
    "start": "2370400",
    "end": "2376880"
  },
  {
    "text": "guess what this is about data bi uh data bias um in a world designed for",
    "start": "2376880",
    "end": "2382640"
  },
  {
    "text": "men the big nine Amy web one of my personal favorites so she wrote this in",
    "start": "2382640",
    "end": "2387960"
  },
  {
    "text": "2019 um and it's super relevant she goes through the history of AI and kind of",
    "start": "2387960",
    "end": "2393079"
  },
  {
    "text": "talks about how we got to where we are today um and it's a lot of fun and then again I mentioned automating inequality",
    "start": "2393079",
    "end": "2399720"
  },
  {
    "text": "Virginia",
    "start": "2399720",
    "end": "2402160"
  },
  {
    "text": "Eubanks so again I work for Crema we're a design and technology consultancy that",
    "start": "2406160",
    "end": "2411680"
  },
  {
    "text": "exists to help Enterprise leaders discover understand and execute on your greatest opportunities that QR code will get you",
    "start": "2411680",
    "end": "2418839"
  },
  {
    "text": "a contact form in case you would like to talk with us I am also a member for the center for",
    "start": "2418839",
    "end": "2426960"
  },
  {
    "text": "practical bioethics ethical AI initiative Services we're based in Kansas City we're looking to expand um",
    "start": "2426960",
    "end": "2435119"
  },
  {
    "text": "we work with different healthc Care organizations on sort of a three-prong process the first is Education and",
    "start": "2435119",
    "end": "2441119"
  },
  {
    "text": "Training um for both Health Care Providers uh and developers in their",
    "start": "2441119",
    "end": "2446440"
  },
  {
    "text": "organizations um then we look over executive level supports how do we help seite Executives make the right",
    "start": "2446440",
    "end": "2453520"
  },
  {
    "text": "decisions um sometimes the word ethics scares people so we talk about risk",
    "start": "2453520",
    "end": "2459319"
  },
  {
    "text": "instead uh and then uh our process Improvement tools um what kind of",
    "start": "2459319",
    "end": "2464400"
  },
  {
    "text": "processes checkpoints uh life cycle mappings can we put out to help um",
    "start": "2464400",
    "end": "2469839"
  },
  {
    "text": "organizations uh understand these processes and if you'd like to get in",
    "start": "2469839",
    "end": "2476440"
  },
  {
    "text": "touch with us here's another QR code for you uh ethical AI",
    "start": "2476440",
    "end": "2483119"
  },
  {
    "text": "counil could be just second",
    "start": "2485760",
    "end": "2489760"
  },
  {
    "text": "there um and then if you want to talk with me that's me um appreciate you guys",
    "start": "2493040",
    "end": "2498880"
  },
  {
    "text": "all being here uh like a final comment before I open it up to questions which I think I did pretty good on time",
    "start": "2498880",
    "end": "2505839"
  },
  {
    "text": "so um fairness it's not just a byproduct of algorithms right um it's a choice",
    "start": "2505839",
    "end": "2512400"
  },
  {
    "text": "that we have to make it's sometimes it's akin to choosing a harder right over an easier you're wrong um but as developers",
    "start": "2512400",
    "end": "2520280"
  },
  {
    "text": "as Engineers as stakeholders we must go into the products that we're building and advocate for this and we saw that a",
    "start": "2520280",
    "end": "2527880"
  },
  {
    "text": "little bit of that accuracy fairness tradeoff um but in order to achieve Fair",
    "start": "2527880",
    "end": "2533680"
  },
  {
    "text": "systems we have to be willing to sit in the middle for a little bit",
    "start": "2533680",
    "end": "2539359"
  },
  {
    "text": "right questions comments concerns cheap shots",
    "start": "2540400",
    "end": "2546960"
  },
  {
    "text": "[Applause]",
    "start": "2548020",
    "end": "2552789"
  }
]