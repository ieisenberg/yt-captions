[
  {
    "start": "0",
    "end": "147000"
  },
  {
    "text": "okay all right thank you for coming to this talk for its building advanced and",
    "start": "6529",
    "end": "13660"
  },
  {
    "text": "native Rick's this is not the the meeting stock which I think is in room four and so hopefully you're in the",
    "start": "13660",
    "end": "20200"
  },
  {
    "text": "right room I so just to introduce myself I am lace Lou Franco I am a senior",
    "start": "20200",
    "end": "25330"
  },
  {
    "text": "software engineer at Microsoft as part of the commercial software engineering team and I'm based in Melbourne and I",
    "start": "25330",
    "end": "32258"
  },
  {
    "text": "focus more more slea round data specifically around spark so I'm here to present to you around spark specifically",
    "start": "32259",
    "end": "39280"
  },
  {
    "text": "as your data breaks so just a quick survey who here has actually used spark",
    "start": "39280",
    "end": "45210"
  },
  {
    "text": "okay we have a few folks yeah who has here has heard of data breaks okay you",
    "start": "45210",
    "end": "54520"
  },
  {
    "text": "said the bit played with it nah okay that's all right so I'm actually gonna start off with some basics first so you",
    "start": "54520",
    "end": "61330"
  },
  {
    "text": "should have some understanding of the key capabilities of spark and I sure",
    "start": "61330",
    "end": "66610"
  },
  {
    "text": "date the bricks specifically spark on your data bricks and I'm also gonna walk you through an advanced analytics",
    "start": "66610",
    "end": "72610"
  },
  {
    "text": "workload running on top of azure data bricks so here's the agenda so if you've",
    "start": "72610",
    "end": "78819"
  },
  {
    "text": "never done spark that's okay I'm actually gonna start from the basics but we just ten minutes talking about what",
    "start": "78819",
    "end": "83950"
  },
  {
    "text": "spark what did what the internals of spark and then I'll move into the meat of the talk which is add your data bricks and I'll be doing this in a way",
    "start": "83950",
    "end": "91329"
  },
  {
    "text": "where I'm actually walking you through an actual pipeline and I've built on top of the platform so without further ado",
    "start": "91329",
    "end": "99659"
  },
  {
    "text": "so if you've ever done some work in the big data space you probably recognize",
    "start": "99659",
    "end": "104740"
  },
  {
    "text": "some of these logos and the thing that I want you to notice is that it's a very heterogeneous set of tooling that in",
    "start": "104740",
    "end": "112119"
  },
  {
    "text": "order to get any end-to-end pipeline on the with the big data tooling set you",
    "start": "112119",
    "end": "117429"
  },
  {
    "text": "actually have to learn four or five six tools so if you wanted to do sequel on Hadoop you probably needed to do five if",
    "start": "117429",
    "end": "124209"
  },
  {
    "text": "you need it to some subscripting or ETL you needed to do big machine learning there was mahute for streaming there a",
    "start": "124209",
    "end": "130090"
  },
  {
    "text": "storm and flink for graph there was grf so in order for a data scientist and a",
    "start": "130090",
    "end": "135849"
  },
  {
    "text": "data engineer to actually get something substantially running they needed to actually write four or five different",
    "start": "135849",
    "end": "141810"
  },
  {
    "text": "frame right on top of four or five different frameworks so enter Apache",
    "start": "141810",
    "end": "146980"
  },
  {
    "text": "spark so Apache spark is this unified computing engine which could do",
    "start": "146980",
    "end": "152060"
  },
  {
    "start": "147000",
    "end": "147000"
  },
  {
    "text": "everything that you saw in the previous slide so when Apache spark came out in 2014 it",
    "start": "152060",
    "end": "158690"
  },
  {
    "text": "was fairly groundbreaking in its performance and also its functionality so you can see that in Apache spark it",
    "start": "158690",
    "end": "164720"
  },
  {
    "text": "gives you these libraries on top of a set of core unified libraries so you",
    "start": "164720",
    "end": "169820"
  },
  {
    "text": "have spark sequel it can do streaming it has a machine learning library built in",
    "start": "169820",
    "end": "174950"
  },
  {
    "text": "out of the box and even can do graph and on top of this it has a number of",
    "start": "174950",
    "end": "180500"
  },
  {
    "text": "language bindings so fork is actually written in scala so you'd expect that it has a scholar API and a Java API but it",
    "start": "180500",
    "end": "187340"
  },
  {
    "text": "also provides a Python in our API so for the data scientists in the crowd you'll",
    "start": "187340",
    "end": "193400"
  },
  {
    "text": "also notice that it's actually built on top of its you also know these are just a computing engine so compared to Hadoop",
    "start": "193400",
    "end": "200300"
  },
  {
    "text": "if you're familiar with Hadoop it's actually a computing engine and a file system so you have MapReduce and HDFS",
    "start": "200300",
    "end": "206780"
  },
  {
    "text": "SPARC is just the computing engine so lots of the times folks would actually",
    "start": "206780",
    "end": "212030"
  },
  {
    "text": "have Hadoop as in HDFS installed and they would install SPARC on top of it so",
    "start": "212030",
    "end": "217160"
  },
  {
    "text": "SPARC can reach into different file systems as you can see here blob storage s3 HBase and Cassandra and other and",
    "start": "217160",
    "end": "224930"
  },
  {
    "text": "many other things okay so spark when SPARC was developed and released around in 2014 so it's",
    "start": "224930",
    "end": "232220"
  },
  {
    "text": "actually quite relatively new it was almost pegged as it was it was built in",
    "start": "232220",
    "end": "239209"
  },
  {
    "text": "response to the limitations of hadoop mapreduce so the reason why MapReduce is",
    "start": "239209",
    "end": "244220"
  },
  {
    "text": "fairly slow is that every mapper actually has to write intermediary results to disk so every mapper and",
    "start": "244220",
    "end": "251299"
  },
  {
    "text": "reducer has to write intermediate results of this in which in the next stage actually picks it up now SPARC",
    "start": "251299",
    "end": "257180"
  },
  {
    "text": "actually tries to do most things in memory which is why it's much faster and so almost any so all narrow",
    "start": "257180",
    "end": "263750"
  },
  {
    "text": "transformations in SPARC is done exclusively in memory and the great",
    "start": "263750",
    "end": "268910"
  },
  {
    "text": "thing about SPARC is also it gives the ability to cache data the specific set so you'd imagine lots of the iterative",
    "start": "268910",
    "end": "275500"
  },
  {
    "text": "algorithms especially machine learning algorithms actually go through your data multiple times",
    "start": "275500",
    "end": "280910"
  },
  {
    "text": "so by cashing your data at specific steps in inside your pipeline your smart",
    "start": "280910",
    "end": "286460"
  },
  {
    "text": "doesn't have to reach back into disk and on top of all these improvements Park",
    "start": "286460",
    "end": "291800"
  },
  {
    "text": "has been benchmarked to be around 10 200 times faster than classic MapReduce ok",
    "start": "291800",
    "end": "300470"
  },
  {
    "start": "300000",
    "end": "300000"
  },
  {
    "text": "so digging deeper into the SPARC API so you're if you've ever done SPARC you've probably heard of the RDD which is the",
    "start": "300470",
    "end": "307610"
  },
  {
    "text": "resilient distributed data set that's the fundamental building blocks of SPARC and ultimately all of this API actually",
    "start": "307610",
    "end": "314210"
  },
  {
    "text": "compiles down to in an RDD in one shape or form now that was when spark came out the",
    "start": "314210",
    "end": "321170"
  },
  {
    "text": "version one point X of SPARC now at 2.0 2.0 and beyond do spark came out with",
    "start": "321170",
    "end": "329000"
  },
  {
    "text": "the dataframe and data set api which is a higher level abstraction on top of our",
    "start": "329000",
    "end": "334250"
  },
  {
    "text": "DDS now the advice moving forward is to use this higher level abstraction because you inherit all the optimization",
    "start": "334250",
    "end": "341780"
  },
  {
    "text": "that comes out of the box you don't get that with the RDD api and the only reason you would probably use already",
    "start": "341780",
    "end": "347180"
  },
  {
    "text": "the api is if you find this obscure use case in which case you can't use the higher-level ones",
    "start": "347180",
    "end": "354580"
  },
  {
    "start": "355000",
    "end": "355000"
  },
  {
    "text": "okay so here's a slice of a spark application the most important part of",
    "start": "355540",
    "end": "362510"
  },
  {
    "text": "an application a spark application is arguably the driver so this is your heart of your application this is where",
    "start": "362510",
    "end": "368870"
  },
  {
    "text": "your spark session lives and that's essentially your gateway to the spark cluster and that's also how you would",
    "start": "368870",
    "end": "375290"
  },
  {
    "text": "actually communicate with spark where you run your user code and so on now the driver is in charge of analyzing and",
    "start": "375290",
    "end": "382400"
  },
  {
    "text": "distributing your job across your cluster and giving you tasks to the executors the executors are all they do",
    "start": "382400",
    "end": "389150"
  },
  {
    "text": "is the tree get some fast food Driver execute report back to their status to the driver spark can also work with",
    "start": "389150",
    "end": "396140"
  },
  {
    "text": "different cluster managers so in the wild you probably find Hadoop yarn that's a very popular way of running",
    "start": "396140",
    "end": "403310"
  },
  {
    "text": "SPARC because usually customers really high already have a hadoop installation I just want to install its mark on top of it but SPARC also comes out of the",
    "start": "403310",
    "end": "410300"
  },
  {
    "text": "box with its own cluster manager so you don't have the hard dependencies on it also works on on top of Apache mesas",
    "start": "410300",
    "end": "417770"
  },
  {
    "text": "and the fun fact is the creator of spark is also a co-creator of Apache Mesa so",
    "start": "417770",
    "end": "422810"
  },
  {
    "text": "he's quite a smart dude now the other thing I want to draw your attention to",
    "start": "422810",
    "end": "428330"
  },
  {
    "text": "is the data structure so that's a spark that's essentially an RDD and/or spark",
    "start": "428330",
    "end": "434270"
  },
  {
    "text": "data frame depending which API you're using and if you're a new developer a very common pitfall is that you wanna",
    "start": "434270",
    "end": "441230"
  },
  {
    "text": "convey you have pandas or psych it or yeah and you're trying to manipulate",
    "start": "441230",
    "end": "446990"
  },
  {
    "text": "your spark data frame and you find that your libraries don't work with spark data frames so the first thing you try",
    "start": "446990",
    "end": "452150"
  },
  {
    "text": "to do is actually convert it to a panda's data frame that's not a not a good idea because you can see here your",
    "start": "452150",
    "end": "458630"
  },
  {
    "text": "panda's data frame is living in the driver that's one node and your spark data frame is literally spread across",
    "start": "458630",
    "end": "466130"
  },
  {
    "text": "your cluster so the moment you convert your spark mater frame into a panda's data frame you're literally collecting a",
    "start": "466130",
    "end": "472160"
  },
  {
    "text": "billion rows back into that one little tiny node so that's a very common pitfall when you start starting with",
    "start": "472160",
    "end": "479090"
  },
  {
    "text": "spark it might work with small data sets but the moment you deploy the production it almost guarantee will blow up okay so",
    "start": "479090",
    "end": "487250"
  },
  {
    "text": "that's pretty much spark in a nutshell I'm just gonna shift gears now to data brakes so what is data brakes data",
    "start": "487250",
    "end": "494120"
  },
  {
    "start": "493000",
    "end": "493000"
  },
  {
    "text": "bricks is essentially a managed apache spark platform with a number of tooling built out of the box it's a first party",
    "start": "494120",
    "end": "502070"
  },
  {
    "text": "service on Azure it's not something you would spin up from the azure marketplace and it comes with all kinds of",
    "start": "502070",
    "end": "508190"
  },
  {
    "text": "integrations with Azure as you would imagine so as your ad data connectors it",
    "start": "508190",
    "end": "514070"
  },
  {
    "text": "hooks up to your billing so you have a single pane of glass for your billing and of course you can also view data on",
    "start": "514070",
    "end": "520430"
  },
  {
    "text": "data breaks using power bi now instead of talking about beta breaks it's",
    "start": "520430",
    "end": "525530"
  },
  {
    "text": "actually easier to show it so I'll just gonna shift here okay so in order to",
    "start": "525530",
    "end": "533390"
  },
  {
    "text": "actually create add your data break so I'm just here in the azure portal and I just literally searched for Azure data",
    "start": "533390",
    "end": "539090"
  },
  {
    "text": "bricks I hit create here and it shows up this pane and I just literally I",
    "start": "539090",
    "end": "546470"
  },
  {
    "text": "literally entered my workspace name pick my subscription and resource group location",
    "start": "546470",
    "end": "552920"
  },
  {
    "text": "and there's a pricing tier of different tiers within data bricks so the standard",
    "start": "552920",
    "end": "558020"
  },
  {
    "text": "here the premium tier is essentially your standard Deerwood but with role-based access so that's pretty much",
    "start": "558020",
    "end": "563840"
  },
  {
    "text": "it so you hit create it creates a workspace pretty simply and once you get to your",
    "start": "563840",
    "end": "570170"
  },
  {
    "text": "workspace just say ok hmm internets",
    "start": "570170",
    "end": "580660"
  },
  {
    "text": "interestingly slow but you will find this button just launched workspace and",
    "start": "580660",
    "end": "586960"
  },
  {
    "text": "that will launch this workspace of azure data break so this is pretty much your data breaks portal the first thing you",
    "start": "586960",
    "end": "594830"
  },
  {
    "text": "want to do in native bricks is actually to create a cluster so to create a cluster you just go to clusters and",
    "start": "594830",
    "end": "600320"
  },
  {
    "text": "create cluster and you give your cluster a name so lace cluster you pick which",
    "start": "600320",
    "end": "605810"
  },
  {
    "text": "cluster type you want to actually create so the high concurrency cluster if you have a multi-user sort of scenario",
    "start": "605810",
    "end": "611410"
  },
  {
    "text": "standard is for single users so I'm just gonna click the standard and you can",
    "start": "611410",
    "end": "616580"
  },
  {
    "text": "pick different runtimes in your data bricks data bricks runtime version so",
    "start": "616580",
    "end": "621590"
  },
  {
    "text": "which Maps the different spark versions and different libraries that may come installed there is a GPU cluster just",
    "start": "621590",
    "end": "627800"
  },
  {
    "text": "cluster with GPUs and there's an CPU only cluster and in fact that beta class",
    "start": "627800",
    "end": "633440"
  },
  {
    "text": "version actually wasn't there yesterday so that's actually quite new yeah so you",
    "start": "633440",
    "end": "638750"
  },
  {
    "text": "pick your cluster version you pick your python version so python 3 is probably a better choice and here is when you have",
    "start": "638750",
    "end": "646460"
  },
  {
    "text": "to know a little bit about the SPARC architecture so you have your driver and your worker so remember it's the back to previous slides you pick your driver",
    "start": "646460",
    "end": "652580"
  },
  {
    "text": "size and you pick a specific VM your worker size and the great thing about",
    "start": "652580",
    "end": "657800"
  },
  {
    "text": "data breaks it has this auto scaling feature so which is very which is not available hdinsight if you're familiar",
    "start": "657800",
    "end": "663920"
  },
  {
    "text": "with that technology so spark spark native bricks will actually detect if",
    "start": "663920",
    "end": "669020"
  },
  {
    "text": "there's any back pressures in your in your spark job and it will automatically sail your cluster depending on the",
    "start": "669020",
    "end": "674870"
  },
  {
    "text": "parameters you set here and the other cool thing is that it has auto termination which will literally",
    "start": "674870",
    "end": "681240"
  },
  {
    "text": "kill your cluster if it detects it that it's idle for a set given amount of time which is a great feature if you've ever",
    "start": "681240",
    "end": "687560"
  },
  {
    "text": "forgot to turn off your hdinsight cluster you can also customize your",
    "start": "687560",
    "end": "694200"
  },
  {
    "text": "cluster by specifying some configuration inspark some environment variables you",
    "start": "694200",
    "end": "699780"
  },
  {
    "text": "can add your cluster for building purposes and you can ship your logs to a location within BBFS which i will talk",
    "start": "699780",
    "end": "707310"
  },
  {
    "text": "about in a second you can also customize your clusters further by specifying an init script which is basically a bash",
    "start": "707310",
    "end": "713580"
  },
  {
    "text": "script which will run every time your cluster starts so there's some you know obscure library that you want to install",
    "start": "713580",
    "end": "719280"
  },
  {
    "text": "this is probably the way to do it okay so I already have a cluster installed so",
    "start": "719280",
    "end": "726150"
  },
  {
    "text": "I'm not gonna go through I'm not actually gonna create a cluster there so the thing with data breaks is that it",
    "start": "726150",
    "end": "732810"
  },
  {
    "text": "has this thing called your workspace which is a list of your notebooks and if you're ever familiar with Jupiter",
    "start": "732810",
    "end": "738150"
  },
  {
    "text": "notebooks it's very similar so I have a notebook interface right here and the second thing that you want to do once",
    "start": "738150",
    "end": "744300"
  },
  {
    "text": "you open your notebook is actually attach it to a cluster so I already have a cluster running and it's attached this",
    "start": "744300",
    "end": "750630"
  },
  {
    "text": "notebook set that should that cluster now data breaks exposes the SPARC session already through the variable",
    "start": "750630",
    "end": "756870"
  },
  {
    "text": "called spark and you can see that's my spark session right there I can inspect the version of spark which is 2.3 and",
    "start": "756870",
    "end": "763400"
  },
  {
    "text": "Here I am just creating a simple spark data frame so and just displaying it on",
    "start": "763400",
    "end": "770400"
  },
  {
    "text": "data breaks so data breaks gives us this nice sort of table UI out of the box",
    "start": "770400",
    "end": "775650"
  },
  {
    "text": "which you don't necessarily get out some from other notebook technologies you can print the schema and the great thing",
    "start": "775650",
    "end": "782820"
  },
  {
    "text": "about data breaks is that you can also mix and match different languages so you can notice you know this this is a",
    "start": "782820",
    "end": "787860"
  },
  {
    "text": "Python notebook and here I'm actually gonna mix in some sparks equal so this",
    "start": "787860",
    "end": "792870"
  },
  {
    "text": "command literally just creates a temporary view on top of my data frame and then I can then use sequel to query",
    "start": "792870",
    "end": "799770"
  },
  {
    "text": "my data frame so if you're more familiar with sequel you can use spark sequel within your same notebook okay so the",
    "start": "799770",
    "end": "807480"
  },
  {
    "text": "other important thing about spark but you need to realize is how it actually executes spark has this thing",
    "start": "807480",
    "end": "814529"
  },
  {
    "text": "called transformation and actions so transformation so our TDS are actually immutable the only way you can change",
    "start": "814529",
    "end": "820889"
  },
  {
    "text": "your RDB is actually feed it to a transformation and it spits out a nutri RDD a brand new I did RTD so you're",
    "start": "820889",
    "end": "827189"
  },
  {
    "text": "effectively creating what they call a dag add a gov execution so directing directed a cyclic graph of executions",
    "start": "827189",
    "end": "833639"
  },
  {
    "text": "and then the moment you actually want to view your data smart will then execute the graph and then give you your data so",
    "start": "833639",
    "end": "839759"
  },
  {
    "text": "SPARC is essentially lazy in that sense so here I have my data frame and I'm",
    "start": "839759",
    "end": "845160"
  },
  {
    "text": "just doing a set of transformation so I'm just doing selects group wise and average and spark hasn't actually",
    "start": "845160",
    "end": "851369"
  },
  {
    "text": "executed it it just says yep I got peer transformation I know what I'm supposed to do and then when I actually do an",
    "start": "851369",
    "end": "857040"
  },
  {
    "text": "action where I am collecting back the results that's when spark actually executes now this is very important to",
    "start": "857040",
    "end": "863399"
  },
  {
    "text": "realize because sometimes you would execute you know five 10 cells and the bug that you the bug in your code is",
    "start": "863399",
    "end": "870269"
  },
  {
    "text": "actually in cell number one and you're ready at cell number 10 and then when you trigger the action you only find out that's it's already it's already passed",
    "start": "870269",
    "end": "876749"
  },
  {
    "text": "your bug so that's probably something probably one of the biggest hurdles when you're starting off developing in spark",
    "start": "876749",
    "end": "883279"
  },
  {
    "text": "ok so jumping back to my slides so that",
    "start": "883279",
    "end": "889709"
  },
  {
    "text": "was a nice sort of quick overview of spark and data breaks",
    "start": "889709",
    "end": "895009"
  },
  {
    "start": "895000",
    "end": "895000"
  },
  {
    "text": "ok so Google actually published this very interesting paper and it's it was",
    "start": "895009",
    "end": "901649"
  },
  {
    "text": "called the hidden technical depth in ml systems and the entire idea is that your",
    "start": "901649",
    "end": "907410"
  },
  {
    "text": "ml code is actually this tiny little piece of code in your greater ml production system and in order to",
    "start": "907410",
    "end": "913769"
  },
  {
    "text": "productionize machine learning models there's actually a lot of more work than just the sexy algorithm that your data",
    "start": "913769",
    "end": "919379"
  },
  {
    "text": "scientists produced so that's essentially the gap that data breaks is",
    "start": "919379",
    "end": "925139"
  },
  {
    "text": "trying to solve so it provides you this unified workspace that that that makes",
    "start": "925139",
    "end": "931980"
  },
  {
    "text": "the production izing of machine learning models much easier or at least with less friction so not only do you get a",
    "start": "931980",
    "end": "938850"
  },
  {
    "text": "optimize data breaks smart runtime you also get the ability to product",
    "start": "938850",
    "end": "943889"
  },
  {
    "text": "nice this notebooks to create multi-stage pipelines to do a job to",
    "start": "943889",
    "end": "948899"
  },
  {
    "text": "schedule it using a job scheduler and some notifications out of the box and as you saw in the workspace it gives you",
    "start": "948899",
    "end": "955109"
  },
  {
    "text": "this collaborative sort of environment where different data scientists engineers and business analysts could",
    "start": "955109",
    "end": "960449"
  },
  {
    "text": "actually use and not only that you also get if you step a little bit backwards",
    "start": "960449",
    "end": "967199"
  },
  {
    "start": "963000",
    "end": "963000"
  },
  {
    "text": "you could also see that later breaks rigs up with all the different adjure services so it can consume data from",
    "start": "967199",
    "end": "973949"
  },
  {
    "text": "Kafka and event hubs it can read data from cosmos eco data warehouse blob",
    "start": "973949",
    "end": "979410"
  },
  {
    "text": "storage and data Lake and you can even orchestrate your date urine matrix",
    "start": "979410",
    "end": "984480"
  },
  {
    "text": "notebooks using data factory which you might want to do if you have some other downstream components that you want to",
    "start": "984480",
    "end": "989759"
  },
  {
    "text": "run aside from your data bricks notebooks and of course you can also visualize using power bi okay so core",
    "start": "989759",
    "end": "998609"
  },
  {
    "start": "996000",
    "end": "996000"
  },
  {
    "text": "concepts so you've seen clusters workspaces and notebooks the other thing that you probably would want to know",
    "start": "998609",
    "end": "1004549"
  },
  {
    "text": "about this job's which is your data bricks job scheduler where he can schedule notebooks to run at a specific interval tables which I will go through",
    "start": "1004549",
    "end": "1012319"
  },
  {
    "text": "in second which is kind of like hive if you are familiar with hive where you can define essentially see sparks equal",
    "start": "1012319",
    "end": "1019160"
  },
  {
    "text": "tables and persist them in your data picks workspace you can save sensitive",
    "start": "1019160",
    "end": "1024199"
  },
  {
    "text": "keys in the form of data break secrets and you can upload third-party libraries in the form of libraries in data bricks",
    "start": "1024199",
    "end": "1032409"
  },
  {
    "text": "now the other thing that you also want to be aware of is the file system within",
    "start": "1032409",
    "end": "1037699"
  },
  {
    "text": "data bricks so this is essentially similar to HDFS but really it's just a layer on top of blob storage as far as",
    "start": "1037699",
    "end": "1044209"
  },
  {
    "text": "data bricks it's sorry it's it's implemented now the great thing about this is that it's not tightly coupled",
    "start": "1044209",
    "end": "1050539"
  },
  {
    "text": "with your cluster so that if you delete your cluster your data is still there so and spark data breaks gives you all of",
    "start": "1050539",
    "end": "1057799"
  },
  {
    "text": "these utilities to actually access data in dbfs you can use it using the different api's but it also gives you a",
    "start": "1057799",
    "end": "1065149"
  },
  {
    "text": "CLI and some utilities within your data breaks workspace okay so with that",
    "start": "1065149",
    "end": "1072250"
  },
  {
    "text": "usually the first thing that I do when I spin up a workspace is I actually mount my blob storage",
    "start": "1072250",
    "end": "1077890"
  },
  {
    "text": "out-of-the-box it actually comes with a blob storage with your workspace but",
    "start": "1077890",
    "end": "1084100"
  },
  {
    "text": "guidance is that you want to actually attach a separate blob storage or it may be your date that's already in this",
    "start": "1084100",
    "end": "1090290"
  },
  {
    "text": "other blob storage that's not that came with data bricks okay so I have here a",
    "start": "1090290",
    "end": "1098180"
  },
  {
    "text": "notebook and as you can see the first few two lines of code I'm just retrieving data break brick secret which",
    "start": "1098180",
    "end": "1105440"
  },
  {
    "text": "I've defined earlier so this is my storage account and key and if I let me",
    "start": "1105440",
    "end": "1110480"
  },
  {
    "text": "just first attach that and if I try to execute this you'll notice that the",
    "start": "1110480",
    "end": "1116390"
  },
  {
    "text": "storage key is actually gonna be redacted there so it so when it prints",
    "start": "1116390",
    "end": "1121670"
  },
  {
    "text": "out in you know your logs you want see your key printed out in your logs now this is the command of actually mounting",
    "start": "1121670",
    "end": "1129350"
  },
  {
    "text": "data bricks so remember DB details that was the utility command in the slides that's available out of the box in your",
    "start": "1129350",
    "end": "1135770"
  },
  {
    "text": "notebook and I'm just calling the FS mount command and I'm passing data bricks my storage account and all of",
    "start": "1135770",
    "end": "1143120"
  },
  {
    "text": "this configuration the mount path could be just something like mount that blob storage which is basically a file for",
    "start": "1143120",
    "end": "1150530"
  },
  {
    "text": "file folder within your dbfs environment and then I'm just passing my storage key",
    "start": "1150530",
    "end": "1157010"
  },
  {
    "text": "and I'm not gonna run this because I've already mounted it and the rest of the notebook is just me downloading the data",
    "start": "1157010",
    "end": "1163520"
  },
  {
    "text": "for the demo so that's pretty much how easy it is to mount a filesystem",
    "start": "1163520",
    "end": "1170330"
  },
  {
    "text": "in data bricks and in order to see that you actually made of bricks actually come provides you with a command-line",
    "start": "1170330",
    "end": "1176420"
  },
  {
    "text": "dbfs so I'm just doing DFS LS and the where I mounted my blob storage and that",
    "start": "1176420",
    "end": "1182570"
  },
  {
    "text": "should show me the files that are living in blob storage once it returns but I'll",
    "start": "1182570",
    "end": "1193580"
  },
  {
    "text": "just show you blob storage which is has data and models so this should return",
    "start": "1193580",
    "end": "1200740"
  },
  {
    "text": "okay I think the Internet's struggling at this point perhaps someone's",
    "start": "1205940",
    "end": "1211890"
  },
  {
    "text": "downloading a gigantic docker in image in the other room but that should return",
    "start": "1211890",
    "end": "1217920"
  },
  {
    "text": "basically and it will list out the files that you see here that's pretty much it okay that's me typing okay so moving on",
    "start": "1217920",
    "end": "1229039"
  },
  {
    "start": "1235000",
    "end": "1235000"
  },
  {
    "text": "okay so for this for the rest of the slides I'm actually gonna be focusing on the specific data set",
    "start": "1235520",
    "end": "1240720"
  },
  {
    "text": "it's the kdd 1999 intrusion network intrusion data set is a very common data",
    "start": "1240720",
    "end": "1246450"
  },
  {
    "text": "set used for anomaly detection intrusion Network detection so I'm essentially",
    "start": "1246450",
    "end": "1252510"
  },
  {
    "text": "going to build an anomaly detection pipeline or natural data brakes now the thing with this data set I just want to",
    "start": "1252510",
    "end": "1257670"
  },
  {
    "text": "throw it out there at the start is that it's actually not super representative of the real world because nobody's in",
    "start": "1257670",
    "end": "1263970"
  },
  {
    "text": "the data set this actually has more has more anomalies than the actual normal data and it was it's a bit synthetic and",
    "start": "1263970",
    "end": "1271260"
  },
  {
    "text": "synthetic in that way but I just chose anomaly detection because it's quite easy to understand intuitively and",
    "start": "1271260",
    "end": "1277200"
  },
  {
    "text": "without me further explaining it okay so here's the architecture that I'm actually building so I will go through",
    "start": "1277200",
    "end": "1284280"
  },
  {
    "start": "1280000",
    "end": "1280000"
  },
  {
    "text": "all of these steps one by one so you obviously have to load your data explore your data train your model and then you",
    "start": "1284280",
    "end": "1289380"
  },
  {
    "text": "want to actually operationalize your model on data bricks so in order to load",
    "start": "1289380",
    "end": "1295800"
  },
  {
    "text": "your model usually lots of people use spark spark sequel for the ETL jobs so",
    "start": "1295800",
    "end": "1300960"
  },
  {
    "text": "spark sequel is essentially the interface to schwarz interface to working with structured and",
    "start": "1300960",
    "end": "1306570"
  },
  {
    "text": "semi-structured data so it it's built on top of your data frames and data sets",
    "start": "1306570",
    "end": "1311940"
  },
  {
    "text": "API and it provides you ODBC and JDBC access as you would imagine so where",
    "start": "1311940",
    "end": "1319290"
  },
  {
    "text": "that lives in my demo it's around is here and I will just jump back to my workspace and I'll open that notebook",
    "start": "1319290",
    "end": "1329000"
  },
  {
    "text": "okay so here as you can see I am just",
    "start": "1329000",
    "end": "1334050"
  },
  {
    "text": "creating Oh",
    "start": "1334050",
    "end": "1337100"
  },
  {
    "text": "excuse me doesn't seem to be shooing",
    "start": "1339960",
    "end": "1347039"
  },
  {
    "text": "sorry I'll die okay okay there you go",
    "start": "1348150",
    "end": "1357330"
  },
  {
    "text": "all right sorry okay so basically okay",
    "start": "1357330",
    "end": "1363520"
  },
  {
    "text": "I'm just here in my spark EPL notebook so I'm just literally creating a set of",
    "start": "1363520",
    "end": "1369430"
  },
  {
    "text": "tables using the create tables syntax and you'll notice that it's very similar to sequel so here I'm just specifying on",
    "start": "1369430",
    "end": "1377470"
  },
  {
    "text": "my table definition and I'm saying it's a CSV file living in blob storage and it's living in this specific location in",
    "start": "1377470",
    "end": "1384670"
  },
  {
    "text": "blob storage and you'll notice that that's my mountain path by specifying a",
    "start": "1384670",
    "end": "1389740"
  },
  {
    "text": "location in my create table statement it says that this is an external table just",
    "start": "1389740",
    "end": "1394960"
  },
  {
    "text": "you create a pointer to that specific location now after creating the pointer",
    "start": "1394960",
    "end": "1400360"
  },
  {
    "text": "or an unmanaged table by spark sequel I can then create my actual table and then I'll just load my my pointer into that",
    "start": "1400360",
    "end": "1407050"
  },
  {
    "text": "actual table using this statement and then I'm just dropping my temporary table and refreshing so it's a fairly",
    "start": "1407050",
    "end": "1413290"
  },
  {
    "text": "simple pattern within data bricks and I'm doing exactly the same thing with my other data set so a fairly simple table",
    "start": "1413290",
    "end": "1420400"
  },
  {
    "text": "just to see your standard C test at a execution and if I go to the data tab",
    "start": "1420400",
    "end": "1426930"
  },
  {
    "text": "you could see that all my tables are now loaded so this is persisted in spark",
    "start": "1426930",
    "end": "1432220"
  },
  {
    "text": "data bricks and you can see that you can very view your your table your data here",
    "start": "1432220",
    "end": "1438580"
  },
  {
    "text": "and I just want to show you that",
    "start": "1438580",
    "end": "1443970"
  },
  {
    "text": "okay I just want to show you that okay I",
    "start": "1449840",
    "end": "1454750"
  },
  {
    "text": "think there's just some network issues",
    "start": "1457360",
    "end": "1463299"
  },
  {
    "text": "okay I just wanted to show you that it actually supports handling erase within tables so you can actually embed Jason",
    "start": "1465790",
    "end": "1472550"
  },
  {
    "text": "within your values so it it does handle semi structured data okay",
    "start": "1472550",
    "end": "1478880"
  },
  {
    "text": "so jumping back to the slides okay so",
    "start": "1478880",
    "end": "1490310"
  },
  {
    "text": "this is actually the meat of the top which is the machine learning librarian table bar called ml Dib now Emma Lib",
    "start": "1490310",
    "end": "1496910"
  },
  {
    "text": "comes out of the box with different ml algorithms as you would imagine so clustering classification regression and",
    "start": "1496910",
    "end": "1503720"
  },
  {
    "text": "even collaborative filtering and it also provides you out of the box with different feature extracting feature",
    "start": "1503720",
    "end": "1510140"
  },
  {
    "text": "extractor and transformation methods and also probably the most powerful feature",
    "start": "1510140",
    "end": "1515270"
  },
  {
    "text": "is your mo pipelines which is very similar to the API that you find inside kit so digging deeper into the concepts",
    "start": "1515270",
    "end": "1524510"
  },
  {
    "text": "of MLM so you start off with your data frame so it's built on top of your spark data frame API and then you have these",
    "start": "1524510",
    "end": "1531950"
  },
  {
    "text": "set of transformers and estimators so these are higher level abstractions of the different operations that you can do",
    "start": "1531950",
    "end": "1539240"
  },
  {
    "text": "with your data frame with an ml Lib and idea is you can also it also has a standard way of passing parameters using",
    "start": "1539240",
    "end": "1545900"
  },
  {
    "text": "a parameter map and idea is is that you can wrap a set of transformers and estimators in what we call a pipeline",
    "start": "1545900",
    "end": "1552740"
  },
  {
    "text": "and this pipeline you can take as a unit and then use it to transform your data and compose really fairly complex",
    "start": "1552740",
    "end": "1560150"
  },
  {
    "text": "transformations fairly simply now you might be wondering what's the difference",
    "start": "1560150",
    "end": "1565730"
  },
  {
    "text": "between a transformer and estimator so a transformer as you would imagine is very simple you would just it feeds it picks",
    "start": "1565730",
    "end": "1572090"
  },
  {
    "start": "1567000",
    "end": "1567000"
  },
  {
    "text": "in a data frame and transform it transforms into a new data frame so probably the most simplest transformer",
    "start": "1572090",
    "end": "1577460"
  },
  {
    "text": "you could think of it's a binary server you gets a column of values it's normally not a normally and it",
    "start": "1577460",
    "end": "1582990"
  },
  {
    "text": "turns it to zeros and once an estimator slightly more complex because it takes a",
    "start": "1582990",
    "end": "1588630"
  },
  {
    "text": "data frame and it spits out a transformer and the way to think about it this that's actually your model so you could think you fit a an estimator",
    "start": "1588630",
    "end": "1596880"
  },
  {
    "text": "it spits out a trained model which is now a transformer which you can use to make predictions on your data so that's",
    "start": "1596880",
    "end": "1604050"
  },
  {
    "text": "that's how you would think about it now almost everything within sparked Emma lived is either a transformer or an",
    "start": "1604050",
    "end": "1611220"
  },
  {
    "text": "estimator where we like 90 percent of it so you can see that there's a fairly feature-rich",
    "start": "1611220",
    "end": "1616460"
  },
  {
    "text": "library but let's say you had some obscure logic or some specific model",
    "start": "1616460",
    "end": "1622410"
  },
  {
    "text": "that wasn't available it's also fairly extensible so you can write your own custom transformers and estimators in",
    "start": "1622410",
    "end": "1629610"
  },
  {
    "start": "1623000",
    "end": "1623000"
  },
  {
    "text": "fact Microsoft did exactly that with FML spark which is a set of custom transformers and estimators written for",
    "start": "1629610",
    "end": "1637830"
  },
  {
    "text": "for data science so it provides deep learning support on spark and also other",
    "start": "1637830",
    "end": "1642929"
  },
  {
    "text": "data science tools that I thought was quite useful which was not available in built in MLM and there's this driving",
    "start": "1642929",
    "end": "1649320"
  },
  {
    "text": "community of third-party spark packages not just MLM but also for the other api's which essentially is extend spark",
    "start": "1649320",
    "end": "1656790"
  },
  {
    "text": "which I encourage you to check out so okay so for the demo I'm just gonna show",
    "start": "1656790",
    "end": "1662730"
  },
  {
    "text": "you how to explore data and actually train your model with spark MLM",
    "start": "1662730",
    "end": "1668690"
  },
  {
    "text": "okay so I am back here in data bricks and I'm just gonna load my explore data",
    "start": "1675740",
    "end": "1683009"
  },
  {
    "text": "notebook okay so I've already run this notebook but",
    "start": "1683009",
    "end": "1691889"
  },
  {
    "text": "I'll just walk you through it so I'm just reading my data my kdd data and I'm just displaying my data so and",
    "start": "1691889",
    "end": "1698899"
  },
  {
    "text": "investigating it I can print the schema checking all the columns you can do a",
    "start": "1698899",
    "end": "1704039"
  },
  {
    "text": "count it's a fairly relatively small data set around 1.7 million rows and here I'm just doing summary statistics",
    "start": "1704039",
    "end": "1711450"
  },
  {
    "text": "so count mean standard deviation 50 25 50 75 percentile and so on and I'm",
    "start": "1711450",
    "end": "1719460"
  },
  {
    "text": "simply just doing a group by between the anomalous and none of the most data",
    "start": "1719460",
    "end": "1724649"
  },
  {
    "text": "points so you can see that normally the data points are actually larger than the actual normal their disappoints so it's",
    "start": "1724649",
    "end": "1730409"
  },
  {
    "text": "a fairly synthetic data set it was actually created using real TCP connections but they literally have over",
    "start": "1730409",
    "end": "1739190"
  },
  {
    "text": "created the anomalous data points as you can see here and I'm just breaking down",
    "start": "1739190",
    "end": "1745049"
  },
  {
    "text": "the different types of anomalous data set data points into their pipes because",
    "start": "1745049",
    "end": "1751590"
  },
  {
    "text": "it's actually broken down it's actually labeled two different types of anomalies so the normal data set it's actually the",
    "start": "1751590",
    "end": "1758100"
  },
  {
    "text": "highest and the second one is smurf attack which I'm not a network guru but",
    "start": "1758100",
    "end": "1763529"
  },
  {
    "text": "apparently that's something it's got like a DDoS attack for for intrusion",
    "start": "1763529",
    "end": "1768870"
  },
  {
    "text": "detection trojan gurus out there so yeah",
    "start": "1768870",
    "end": "1774299"
  },
  {
    "text": "so and the rest of them are fairly small data points so it's a fairly skewed data",
    "start": "1774299",
    "end": "1779580"
  },
  {
    "text": "set in that regard now I'm just gonna shift through the training of training",
    "start": "1779580",
    "end": "1785610"
  },
  {
    "text": "actual the model so here I am I've loaded my different spark ml libraries",
    "start": "1785610",
    "end": "1792000"
  },
  {
    "text": "and just specifying a bunch of variables on the top and I'm loading my data like",
    "start": "1792000",
    "end": "1799440"
  },
  {
    "text": "here now I am actually defining now my features estimators and transformer so",
    "start": "1799440",
    "end": "1806309"
  },
  {
    "text": "if you're not much of a if you're not the data signs that's okay so but if you are",
    "start": "1806309",
    "end": "1811950"
  },
  {
    "text": "you probably recognize a number of these so the one hot encoding scaling your",
    "start": "1811950",
    "end": "1817440"
  },
  {
    "text": "feature so that it falls within a specific range and probably labeling",
    "start": "1817440",
    "end": "1823139"
  },
  {
    "text": "you're changing your feature from a from strings into into categories and",
    "start": "1823139",
    "end": "1829950"
  },
  {
    "text": "indexing it back to strings so that's all of these feature create feature",
    "start": "1829950",
    "end": "1836029"
  },
  {
    "text": "estimators and transformers and you'll notice that I'm actually just defining them I'm actually not using them yet now",
    "start": "1836029",
    "end": "1843179"
  },
  {
    "text": "I can actually compose my pipeline here so you can see that I have this this new",
    "start": "1843179",
    "end": "1850379"
  },
  {
    "text": "pipeline statement and I'm setting the stages given the different features and estimators and transformers that I",
    "start": "1850379",
    "end": "1857729"
  },
  {
    "text": "defined up top and Here I am calling the fit function so the pipeline is actually",
    "start": "1857729",
    "end": "1863549"
  },
  {
    "text": "in an estimator itself which is an fit function which then return and then",
    "start": "1863549",
    "end": "1868710"
  },
  {
    "text": "gives me a transformer which then I can call transform on so that's all what this is doing so I'm transforming my",
    "start": "1868710",
    "end": "1874950"
  },
  {
    "text": "clean DF into this transform DF and then I'm splitting my data between the between my my train and test set and",
    "start": "1874950",
    "end": "1882529"
  },
  {
    "text": "that gives me a good sort of training set to start with to start training my model so the first model that I'm",
    "start": "1882529",
    "end": "1890190"
  },
  {
    "text": "actually training is a gbt or gradient boosted tree binary classification which will classify if it's anomalous or not",
    "start": "1890190",
    "end": "1896460"
  },
  {
    "text": "the anomalous I'm defining my estimator aka gbt classifier here and I'm just",
    "start": "1896460",
    "end": "1902729"
  },
  {
    "text": "calling fit given the data and now I have my fitted model and I'm just doing your usual evaluation which is your area",
    "start": "1902729",
    "end": "1911429"
  },
  {
    "text": "under the curve and so on and that actually did quite well I think it did",
    "start": "1911429",
    "end": "1916529"
  },
  {
    "text": "like 99% accuracy so it's this data set it's actually it's a fairly trivial data set to fit on the",
    "start": "1916529",
    "end": "1925379"
  },
  {
    "text": "other thing you can do is actually chain a fairly complex pipeline like you see here so the previous models I was",
    "start": "1925379",
    "end": "1931589"
  },
  {
    "text": "actually setting out the data transformation and the model training you can train a chain that entire thing",
    "start": "1931589",
    "end": "1936929"
  },
  {
    "text": "into one gigantic pipeline like like so and he I'm just using random forests and here",
    "start": "1936929",
    "end": "1943920"
  },
  {
    "text": "is where I'm defining all my pipeline stages fitting the pipeline and doing my motor class evaluation the other thing",
    "start": "1943920",
    "end": "1953190"
  },
  {
    "text": "you can do is cross validation so which is basically looping through different",
    "start": "1953190",
    "end": "1959160"
  },
  {
    "text": "polls within your data to actually tune your parameters and Here I am you can",
    "start": "1959160",
    "end": "1964590"
  },
  {
    "text": "also use cross-validation within pipelines so that's that so that's supported now once you've once",
    "start": "1964590",
    "end": "1975720"
  },
  {
    "text": "the data scientist is finished working and are happy with your model you can now then save your model into disk using",
    "start": "1975720",
    "end": "1982230"
  },
  {
    "text": "spark persistence and here you can see that I'm saving all the models back into",
    "start": "1982230",
    "end": "1987840"
  },
  {
    "text": "blob storage now the great thing about this is that data scientists could train on a smaller subset of your data or a",
    "start": "1987840",
    "end": "1993750"
  },
  {
    "text": "smaller cluster and then they could move give that date that persistent model to a data engineer to run in a different",
    "start": "1993750",
    "end": "2000110"
  },
  {
    "text": "cluster so that's it's probably another use case or it's also great for production Ising models okay",
    "start": "2000110",
    "end": "2008720"
  },
  {
    "text": "so now I will just swap to to the slides",
    "start": "2008720",
    "end": "2015850"
  },
  {
    "start": "2020000",
    "end": "2020000"
  },
  {
    "text": "okay so we have a train model sitting in blob storage now we're ready to",
    "start": "2020710",
    "end": "2025760"
  },
  {
    "text": "productionize it there's multiple ways of actually production izing models one",
    "start": "2025760",
    "end": "2031700"
  },
  {
    "text": "is in spark where you can choose between that batch or streaming inference and",
    "start": "2031700",
    "end": "2037370"
  },
  {
    "text": "another very common way of production icing machine learning models is actually out of spark where you're",
    "start": "2037370",
    "end": "2043190"
  },
  {
    "text": "actually container izing your model and potentially wrapping a rest service on top of this and then so that other",
    "start": "2043190",
    "end": "2050200"
  },
  {
    "text": "components can actually call into it we're going to be talking about the in spark",
    "start": "2050200",
    "end": "2055790"
  },
  {
    "text": "way of production Ising models for this is actually a completely separate talk altogether the only thing that I would",
    "start": "2055790",
    "end": "2062300"
  },
  {
    "text": "point out is that if you are interested in this sort of way of production models take a look at ml leap and M will flow",
    "start": "2062300",
    "end": "2068240"
  },
  {
    "text": "which are great libraries on how you would actually do that like exporting models in Spore",
    "start": "2068240",
    "end": "2074350"
  },
  {
    "text": "without any sport dependencies and running it without your spark libraries so yeah",
    "start": "2074350",
    "end": "2081090"
  },
  {
    "text": "okay so you've seen ml persistence how you would actually save the model into",
    "start": "2081090",
    "end": "2087330"
  },
  {
    "text": "persistent into blob storage the great feature about this is that a data scientist could write their code in",
    "start": "2087330",
    "end": "2094000"
  },
  {
    "text": "Python and a data engineer can Road the model in Scala so in the real world a",
    "start": "2094000",
    "end": "2100300"
  },
  {
    "text": "lot of the times they designed this would write their code in either Python and R and a data engineer would have to",
    "start": "2100300",
    "end": "2105400"
  },
  {
    "text": "re-implement everything in a separate language because that's what their production environment is is written in",
    "start": "2105400",
    "end": "2110470"
  },
  {
    "text": "but the great thing about ml persistence is that it's uniform across all the API so if a data engineer starting in our",
    "start": "2110470",
    "end": "2116410"
  },
  {
    "text": "smart car specifically then they can save it and the data engineer who's reading it in the Scala API of spark can",
    "start": "2116410",
    "end": "2123520"
  },
  {
    "text": "read read it back in read back the Train model the other thing that spark gives",
    "start": "2123520",
    "end": "2131740"
  },
  {
    "start": "2129000",
    "end": "2129000"
  },
  {
    "text": "you out of the box is streaming so structured streaming is the API that you",
    "start": "2131740",
    "end": "2136870"
  },
  {
    "text": "get from spark it's a successor of the D streams API if you've worked with spark for a while and",
    "start": "2136870",
    "end": "2143590"
  },
  {
    "text": "the great thing about structured streaming is that the code that you write for batch is almost identical to",
    "start": "2143590",
    "end": "2149110"
  },
  {
    "text": "the code that you write for streaming and the way to think about it is that in batch you have a finite beta frame while",
    "start": "2149110",
    "end": "2157000"
  },
  {
    "text": "in streaming you have an infinitely long data frame or a data frame a piece that gets appended to so that's how logically",
    "start": "2157000",
    "end": "2164020"
  },
  {
    "text": "you would think about it when you're writing code for structured streaming which i think is pretty cool there's",
    "start": "2164020",
    "end": "2170440"
  },
  {
    "text": "obviously some caveats so not not everything that works in batch will work in streaming but predominantly you will",
    "start": "2170440",
    "end": "2176350"
  },
  {
    "text": "notice that your code is actually quite similar okay so so I'm just the demo",
    "start": "2176350",
    "end": "2183850"
  },
  {
    "text": "model operational part which I'm going to be demoing",
    "start": "2183850",
    "end": "2188940"
  },
  {
    "text": "okay so so first I actually would want",
    "start": "2198000",
    "end": "2205810"
  },
  {
    "text": "to show you the job so here I have a batch and streaming job already scheduled and in order to schedule jobs",
    "start": "2205810",
    "end": "2212470"
  },
  {
    "text": "in spark you just do create job and specify a number of parameters but I'll just actually investigate my current job",
    "start": "2212470",
    "end": "2219070"
  },
  {
    "text": "here which is my batch scoring job",
    "start": "2219070",
    "end": "2223710"
  },
  {
    "text": "a K once it actually loads",
    "start": "2231170",
    "end": "2237369"
  },
  {
    "text": "I might shift to my phone Wi-Fi if this keeps sorry for that",
    "start": "2245500",
    "end": "2260099"
  },
  {
    "text": "does anyone have questions so far well I'm transferring my connection great",
    "start": "2278540",
    "end": "2290660"
  },
  {
    "text": "question so as your studio I would say is great if you're still starting out because it has that very simple",
    "start": "2290660",
    "end": "2297590"
  },
  {
    "text": "interface the drag-and-drop GUI the thing with spark is that you almost have to write code so you you have to be",
    "start": "2297590",
    "end": "2304580"
  },
  {
    "text": "familiar with the API is out of the box so it's fairly advanced",
    "start": "2304580",
    "end": "2309950"
  },
  {
    "text": "I guess you advanced ish sort of data scientists would use spark and the thing",
    "start": "2309950",
    "end": "2315290"
  },
  {
    "text": "would as your studio it has has a limitation of the amount of data sets you can actually train on I think it's",
    "start": "2315290",
    "end": "2320510"
  },
  {
    "text": "around a gigabyte so you won't be able to train over terabytes of data using the mo studio service so it's a great",
    "start": "2320510",
    "end": "2330620"
  },
  {
    "text": "starting point though so yeah okay so let me just try to reload my job okay",
    "start": "2330620",
    "end": "2340370"
  },
  {
    "text": "that must have been the thing I clicked on okay so here I have my batch job",
    "start": "2340370",
    "end": "2346310"
  },
  {
    "text": "that's already scheduled and you can see that I'm specifying a specific cluster size so the great thing about spark jobs",
    "start": "2346310",
    "end": "2352850"
  },
  {
    "text": "is that you can have a cluster tailor fit to that specific job which is very",
    "start": "2352850",
    "end": "2358130"
  },
  {
    "text": "hard to do if you you're running on premises and you can see that I've scheduled a specific notebook and I can",
    "start": "2358130",
    "end": "2363410"
  },
  {
    "text": "even pass parameters to that notebook but for this I'm not actually passing any parameters and you have a number of",
    "start": "2363410",
    "end": "2369530"
  },
  {
    "text": "different Advanced Options AI concurrency and that sort of stuff and timeout so I'll just load this specific batch",
    "start": "2369530",
    "end": "2377150"
  },
  {
    "text": "scoring job it's actually quite simple all I'm doing is just loading some",
    "start": "2377150",
    "end": "2383150"
  },
  {
    "text": "unlabeled data points for production you probably want to filter this by when was",
    "start": "2383150",
    "end": "2388220"
  },
  {
    "text": "the last run of this job but for this demo I'm just loading it as is I'm",
    "start": "2388220",
    "end": "2393230"
  },
  {
    "text": "loading my pipeline model for this I'm just using my random forest pipeline which I think was the best pipeline",
    "start": "2393230",
    "end": "2399110"
  },
  {
    "text": "model and I'm just calling it transform method and saving my results so super simple notebooks don't be afraid of",
    "start": "2399110",
    "end": "2405860"
  },
  {
    "text": "batch job like batch jobs are fairly trivial to do and",
    "start": "2405860",
    "end": "2411650"
  },
  {
    "text": "usually streaming it's only when you really have that specific use case um that low latency inference",
    "start": "2411650",
    "end": "2419529"
  },
  {
    "text": "so that's batch and I'm just gonna jump to my streaming demo but before that",
    "start": "2419529",
    "end": "2426710"
  },
  {
    "text": "I'll just trigger my actual data generator so which will actually start streaming so this one's I'm just",
    "start": "2426710",
    "end": "2433219"
  },
  {
    "text": "literally feeding the data from that unlabeled data frame into event hubs and",
    "start": "2433219",
    "end": "2439119"
  },
  {
    "text": "my streaming job should then pick that up so I'll just open this streaming job",
    "start": "2439119",
    "end": "2445160"
  },
  {
    "text": "and this specific job is actually a continuous job it doesn't have an end point and it you can see that I ran it",
    "start": "2445160",
    "end": "2452210"
  },
  {
    "text": "last night and if I open my notebook so here I'm",
    "start": "2452210",
    "end": "2458450"
  },
  {
    "text": "actually cooking up into Azure event hubs so the one the key thing to know about this is that you actually need to",
    "start": "2458450",
    "end": "2465140"
  },
  {
    "text": "load a third-party library and in order to do that you actually go here workspace and I like to put my libraries",
    "start": "2465140",
    "end": "2472579"
  },
  {
    "text": "in the shared folder and you know create library and this library you can specify a specific repository that they're gonna",
    "start": "2472579",
    "end": "2480950"
  },
  {
    "text": "pull from so for event hubs you need this specific library which is your",
    "start": "2480950",
    "end": "2486319"
  },
  {
    "text": "event hub spark library and then you just say create library and the once you",
    "start": "2486319",
    "end": "2491569"
  },
  {
    "text": "have that library created you have to attach that to your cluster where your streaming job is gonna run so flipping",
    "start": "2491569",
    "end": "2500150"
  },
  {
    "text": "back to my job so here I you can see",
    "start": "2500150",
    "end": "2507890"
  },
  {
    "text": "that I'm actually loading up that specific library and I am retrieving a set of secrets so my event hub",
    "start": "2507890",
    "end": "2514069"
  },
  {
    "text": "essentially connection details I'm loading my pipeline model and I'm",
    "start": "2514069",
    "end": "2520369"
  },
  {
    "text": "setting up my connection string to event hubs finally I just connect to my event",
    "start": "2520369",
    "end": "2527089"
  },
  {
    "text": "hubs by using this command it's called spark dot read stream and if you're familiar spark to read a non streaming",
    "start": "2527089",
    "end": "2533719"
  },
  {
    "text": "data data source you just do spark don't read and that gives you a data frame",
    "start": "2533719",
    "end": "2540349"
  },
  {
    "text": "called incoming stream which is literally a spark data frame but it's a dreaming spark dataframe so and I'm just",
    "start": "2540349",
    "end": "2548160"
  },
  {
    "text": "using all the different commands that you would use almost exactly the same as batch so I'm just adding a bunch of",
    "start": "2548160",
    "end": "2554310"
  },
  {
    "text": "columns I'm extracting the body field and adding a watermark and so on and",
    "start": "2554310",
    "end": "2560730"
  },
  {
    "text": "here I'm just extracting some JSON objects and I'm actually doing a static",
    "start": "2560730",
    "end": "2565770"
  },
  {
    "text": "join between a data frame that's sitting on my sparks equal table and the",
    "start": "2565770",
    "end": "2572610"
  },
  {
    "text": "incoming stream so I'd imagine the incoming stream would just probably have three fields but for this specific",
    "start": "2572610",
    "end": "2579060"
  },
  {
    "text": "scenario I actually extracted the fields out and just match on a specific ID so",
    "start": "2579060",
    "end": "2584490"
  },
  {
    "text": "that I can actually do an in-stream join between static and streaming data frames",
    "start": "2584490",
    "end": "2590000"
  },
  {
    "text": "now I just use my model to use and call the transform and I'm just filtering on",
    "start": "2590000",
    "end": "2595830"
  },
  {
    "text": "prediction where it is anomalous and here I'm just outputting my anomalous",
    "start": "2595830",
    "end": "2601350"
  },
  {
    "text": "data frames data points into another event hub in which case the downstream",
    "start": "2601350",
    "end": "2606600"
  },
  {
    "text": "process can probably act on it like send an alert or whatever you need to do so",
    "start": "2606600",
    "end": "2613410"
  },
  {
    "text": "that's pretty much it so it's fairly simple to actually wire up spark",
    "start": "2613410",
    "end": "2619530"
  },
  {
    "text": "streaming to event hubs",
    "start": "2619530",
    "end": "2622730"
  },
  {
    "text": "okay so the data bricks comes out of the box with a number of developer tooling",
    "start": "2633690",
    "end": "2640230"
  },
  {
    "start": "2635000",
    "end": "2635000"
  },
  {
    "text": "so you probably if for the guys who don't like repeating the same thing",
    "start": "2640230",
    "end": "2645329"
  },
  {
    "text": "three times using the GUI don't worry there's a CLI which you can script everything out that I've just showed you",
    "start": "2645329",
    "end": "2651599"
  },
  {
    "text": "and it also comes with the rest API if you if you can't use the CLI for whatever reason so you can control your",
    "start": "2651599",
    "end": "2658650"
  },
  {
    "text": "data bricks workspace with with this tooling now in fact I've actually scripted out this entire demo so if you",
    "start": "2658650",
    "end": "2665970"
  },
  {
    "start": "2663000",
    "end": "2663000"
  },
  {
    "text": "if you want to deploy this demo literally and authorized the deployment so literally just run docker run IT and",
    "start": "2665970",
    "end": "2672900"
  },
  {
    "text": "pass it that container and this will literally spin up the demo completely from scratch it will prompt you for your",
    "start": "2672900",
    "end": "2680039"
  },
  {
    "text": "login details so it'll prompt you for your subscription obviously and tell you where ask you where the research script",
    "start": "2680039",
    "end": "2686430"
  },
  {
    "text": "you want to deploy to and if you're logged into a number of subscriptions you can you can point to a specific",
    "start": "2686430",
    "end": "2692270"
  },
  {
    "text": "subscription and it will just literally walk through all the scripts to deploy this solution and I've actually also",
    "start": "2692270",
    "end": "2701250"
  },
  {
    "text": "given a different top and this which is the recommendation engine this is",
    "start": "2701250",
    "end": "2707010"
  },
  {
    "text": "similar similar talk but it's let's focus on ml Lib and you can also spin up",
    "start": "2707010",
    "end": "2712200"
  },
  {
    "text": "this demo as well where I'm actually it's an end-to-end recommendation engine built on top of azure data bricks so and",
    "start": "2712200",
    "end": "2720000"
  },
  {
    "text": "if you're not familiar docker you can also go to the github link and you can deploy it locally but if if you don't",
    "start": "2720000",
    "end": "2726990"
  },
  {
    "text": "know docker I highly recommend that you go try it out okay so more resources",
    "start": "2726990",
    "end": "2734569"
  },
  {
    "text": "just the apache spark website your as your native bricks documentation and",
    "start": "2734569",
    "end": "2740119"
  },
  {
    "text": "this is a great book by the creator of apache spark mathias area which it which",
    "start": "2740119",
    "end": "2747029"
  },
  {
    "text": "basically goes around goes deeper into the spark details i highly recommend this book",
    "start": "2747029",
    "end": "2752450"
  },
  {
    "text": "it was recently released this year I believe I just read the pre-release so",
    "start": "2752450",
    "end": "2758750"
  },
  {
    "text": "and with that that's that's pretty much it there's any wanted questions",
    "start": "2758750",
    "end": "2765170"
  },
  {
    "text": "yep",
    "start": "2768120",
    "end": "2771120"
  },
  {
    "text": "okay so you don't get intellisense it's",
    "start": "2774780",
    "end": "2782920"
  },
  {
    "text": "not that great you can also write in your favorite IDE so the thing with spark native for data breaks is that you",
    "start": "2782920",
    "end": "2790180"
  },
  {
    "text": "have that CLI endpoints so you can submit jars or Python packages from your",
    "start": "2790180",
    "end": "2796090"
  },
  {
    "text": "favorite ID like vs code so you're not confined to writing code in the notebook",
    "start": "2796090",
    "end": "2801280"
  },
  {
    "text": "so you can and by that regard you can get debugging out of the box depending",
    "start": "2801280",
    "end": "2806350"
  },
  {
    "text": "on what's what ID are using oh sorry the question was can you do debugging in",
    "start": "2806350",
    "end": "2814330"
  },
  {
    "text": "notebooks and the first one was intellisense yeah anyone else have questions yeah",
    "start": "2814330",
    "end": "2827099"
  },
  {
    "text": "that's a great question in fact a lot of our clients that we find is struggling",
    "start": "2832710",
    "end": "2838120"
  },
  {
    "text": "with exactly that Pingdom it's not necessarily with spark but ml in general like C ICD with an ml there's actually a",
    "start": "2838120",
    "end": "2844810"
  },
  {
    "text": "great top that's coming up on Friday called DevOps for ml or ml for DevOps which is the last top of the conference",
    "start": "2844810",
    "end": "2851940"
  },
  {
    "text": "so I definitely encourage you to go to that but for C ICD because you have your",
    "start": "2851940",
    "end": "2858310"
  },
  {
    "text": "CLI in your s and endpoint there's no reason why you can't script out anything that you need to do via the GUI so you",
    "start": "2858310",
    "end": "2865330"
  },
  {
    "text": "can certainly build AC ICD pipeline where it automatically trains your model",
    "start": "2865330",
    "end": "2870910"
  },
  {
    "text": "or let's say it builds a docker container upon check-in in fact the github repos that I have has a very",
    "start": "2870910",
    "end": "2876940"
  },
  {
    "text": "simple CI CD we're literally rebuilds the docker container my deployment docker container upon every check in so",
    "start": "2876940",
    "end": "2883360"
  },
  {
    "text": "that docker container that you saw up front is always up-to-date and yeah so",
    "start": "2883360",
    "end": "2890050"
  },
  {
    "text": "you can see that I have my little CI CD pipeline there it's actually using Azure DevOps pipelines which is free unlimited",
    "start": "2890050",
    "end": "2895570"
  },
  {
    "text": "build minutes for public repos so anyone else have questions awesome so",
    "start": "2895570",
    "end": "2910060"
  },
  {
    "text": "I guess if no one has questions that's that's my top thank you [Applause]",
    "start": "2910060",
    "end": "2917859"
  }
]