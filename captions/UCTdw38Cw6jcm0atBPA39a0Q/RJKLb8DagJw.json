[
  {
    "text": "all right so let's get started hello everyone my name is metl I'm a developer",
    "start": "5160",
    "end": "10360"
  },
  {
    "text": "Advocate at Google based in London and today I'm joined by Mark hian I'm Mark happy to be here today",
    "start": "10360",
    "end": "18720"
  },
  {
    "text": "cool um so today we're going to be talking about Lessons Learned Building A J gen AI powered",
    "start": "18720",
    "end": "24279"
  },
  {
    "text": "application but before we get there uh we want to take you through the Journey of how this app came about out and and",
    "start": "24279",
    "end": "31039"
  },
  {
    "text": "how it started and then how J helped and then we'll talk about Lessons Learned so we because just giving you the lessons",
    "start": "31039",
    "end": "37520"
  },
  {
    "text": "without this story doesn't make sense so we want to lead you through the Journey and then",
    "start": "37520",
    "end": "43280"
  },
  {
    "text": "the the rest of the talk will be about Lessons Learned and if you want the slides uh this is my speaker deck and",
    "start": "43280",
    "end": "49399"
  },
  {
    "text": "the slides is already there a version of it but this version also will be there soon so if you want to get the slides",
    "start": "49399",
    "end": "54760"
  },
  {
    "text": "later feel free all right so what we're going to do is as I said first we're going to talk",
    "start": "54760",
    "end": "60559"
  },
  {
    "text": "about the idea of this app before gen and actually goes back a while and might",
    "start": "60559",
    "end": "66119"
  },
  {
    "text": "we'll talk about that then we'll talk about how gen came and kind of restarted our app and then I will get into the",
    "start": "66119",
    "end": "73840"
  },
  {
    "text": "architecture of the app and talk about how we built it what we use just to give you an idea and then we'll talk about",
    "start": "73840",
    "end": "79720"
  },
  {
    "text": "after gen so what did gen do for us um but also what kind of problems it",
    "start": "79720",
    "end": "84960"
  },
  {
    "text": "introduced even though it helped us with a lot of things it also introduced new problems so we'll talk talk about that a",
    "start": "84960",
    "end": "90439"
  },
  {
    "text": "little bit and then the the real part of the talk is this lesson learned so after",
    "start": "90439",
    "end": "95960"
  },
  {
    "text": "all of this what we learned we just want to share with you so that you aware in case you want to use gen uh in your",
    "start": "95960",
    "end": "102000"
  },
  {
    "text": "applications all right so with that um I'll hand it over to Mark now where he will talk about the app because he's the",
    "start": "102000",
    "end": "107280"
  },
  {
    "text": "one who started this app years ago so I'll take it away thanks Mana um so I have a theory that every",
    "start": "107280",
    "end": "115759"
  },
  {
    "text": "great thing we know about in Computing started as someone's weekend project project and to a certain extent this",
    "start": "115759",
    "end": "123360"
  },
  {
    "text": "talk is about my weekend project or what started out as my weekend project and",
    "start": "123360",
    "end": "128679"
  },
  {
    "text": "has grown to something bigger uh back in 2016 I was trying to come up with",
    "start": "128679",
    "end": "136120"
  },
  {
    "text": "ideas to highlight some new Chrome features specifically something called",
    "start": "136120",
    "end": "141440"
  },
  {
    "text": "Progressive web apps which are a way of making uh web apps that look and feel like a native uh app on a mobile device",
    "start": "141440",
    "end": "149400"
  },
  {
    "text": "and I came up with this idea of creating a trivia quiz application uh and my plan was I feel",
    "start": "149400",
    "end": "157519"
  },
  {
    "text": "like the biggest challenge was sourcing the trivia content and my plan was to",
    "start": "157519",
    "end": "162840"
  },
  {
    "text": "find an open-source trivia database which I looked around for and",
    "start": "162840",
    "end": "168599"
  },
  {
    "text": "and did find uh there's something called the open Trivia database and you can go",
    "start": "168599",
    "end": "174480"
  },
  {
    "text": "there and specify your inputs and make a restful public API call",
    "start": "174480",
    "end": "180599"
  },
  {
    "text": "and get back a nice uh Json document summarizing the uh the trivia content that you",
    "start": "180599",
    "end": "186640"
  },
  {
    "text": "want so that was all fine um and I was able to craft an app",
    "start": "186640",
    "end": "194640"
  },
  {
    "text": "initially that essentially gave you access to all of the content in the open Trivia",
    "start": "194640",
    "end": "200519"
  },
  {
    "text": "database but um there were some problems with this approach um I'm a bit of a perfectionist",
    "start": "200519",
    "end": "207879"
  },
  {
    "text": "and I didn't really like how this was turning out um there was a limited list of topics if you noticed in that",
    "start": "207879",
    "end": "213920"
  },
  {
    "text": "previous slide there was maybe 25 or 30 categories and that was it uh and I",
    "start": "213920",
    "end": "220640"
  },
  {
    "text": "wanted a much broader scope um there was limited set of questions and answers you kind of get what you get it's a pretty",
    "start": "220640",
    "end": "226920"
  },
  {
    "text": "decent sized collection of questions on the order of a thousand or more few",
    "start": "226920",
    "end": "232079"
  },
  {
    "text": "thousand maybe uh but it's not huge by uh web",
    "start": "232079",
    "end": "237400"
  },
  {
    "text": "standards uh there's a Limited Format just all multiple choice questions with with four answers it's all English so",
    "start": "237400",
    "end": "244920"
  },
  {
    "text": "sort of uh us or English-speaking Centric content which I I didn't like I",
    "start": "244920",
    "end": "252599"
  },
  {
    "text": "wanted more more um generality there there was no imagery uh and the main problem was if",
    "start": "252599",
    "end": "260560"
  },
  {
    "text": "you want to expand that content you've got to do work you've got to do the tedious work of actually handcrafting good trivia questions which is going to",
    "start": "260560",
    "end": "267759"
  },
  {
    "text": "slow you down and maybe prevent you from ever really realizing this application then in March 2023 we all",
    "start": "267759",
    "end": "275360"
  },
  {
    "text": "saw this sort of explosion that came about with large language models and I sort",
    "start": "275360",
    "end": "282400"
  },
  {
    "text": "of uh immediately when I saw these language models I immediately thought",
    "start": "282400",
    "end": "287759"
  },
  {
    "text": "about this app and I thought that's the thing I've been waiting for because with",
    "start": "287759",
    "end": "293120"
  },
  {
    "text": "those tools I can create unlimited content",
    "start": "293120",
    "end": "298160"
  },
  {
    "text": "unlimited um topic areas uh all sorts of possibilities become available to me",
    "start": "298160",
    "end": "305440"
  },
  {
    "text": "that were never Poss possible before so I created this app called quic it rhymes",
    "start": "305440",
    "end": "311440"
  },
  {
    "text": "with Mosaic and the best way to explain this app is to show it to you so let's go",
    "start": "311440",
    "end": "318800"
  },
  {
    "text": "ahead and do a quick demonstration of",
    "start": "318800",
    "end": "324680"
  },
  {
    "text": "quic so it looks like this you get just a friendly little brief homepage",
    "start": "324680",
    "end": "330360"
  },
  {
    "text": "uh you can then browse the quizzes if you uh hit this toggle here you can uh",
    "start": "330360",
    "end": "335960"
  },
  {
    "text": "oscillate between seeing all the quizzes or just your quizzes and then once you",
    "start": "335960",
    "end": "341400"
  },
  {
    "text": "have a quiz you can do things within the quiz like host it um you can edit it",
    "start": "341400",
    "end": "346840"
  },
  {
    "text": "copy it and delete it of course so let's go ahead and create a quiz uh let's call this",
    "start": "346840",
    "end": "355560"
  },
  {
    "text": "uh test quiz yeah maybe you can use open uh trivia data set first just to",
    "start": "355560",
    "end": "362720"
  },
  {
    "text": "show yeah so if I select open Trivia the quiz topic I get is a that fixed list",
    "start": "362720",
    "end": "370599"
  },
  {
    "text": "that you saw before which is fine but it's way too limited if I then switch",
    "start": "370599",
    "end": "375840"
  },
  {
    "text": "the quiz generator to a large language model and I'm going to select Gemini Ultra because that's Google's latest and",
    "start": "375840",
    "end": "381639"
  },
  {
    "text": "greatest model and the one that we're finding gives us the best quality quizzes and here I can enter a free form",
    "start": "381639",
    "end": "388880"
  },
  {
    "text": "uh topic now I can make this this is the great thing it's is unlimited I can make it as",
    "start": "388880",
    "end": "395639"
  },
  {
    "text": "specific or as general as I want if I'm a teacher who wants to generate a quiz for your class that's about exactly the",
    "start": "395639",
    "end": "402720"
  },
  {
    "text": "thing we covered this week I can do that or if I'm having a party or something and I want to have like a general thing",
    "start": "402720",
    "end": "408160"
  },
  {
    "text": "world history trivia literature whatever I can do that so I want to ask some",
    "start": "408160",
    "end": "413880"
  },
  {
    "text": "Brave person in the audience to give me a quiz topic and I'll we'll try it",
    "start": "413880",
    "end": "422240"
  },
  {
    "text": "out did I hear metal oh go ahead",
    "start": "422639",
    "end": "429800"
  },
  {
    "text": "say methal metal metal like the mineral okay",
    "start": "429800",
    "end": "435080"
  },
  {
    "text": "I wasn't sure if you meant like music like metal music or or okay let's try",
    "start": "435080",
    "end": "442840"
  },
  {
    "text": "Metals um we could get specific about certain types of metals and all that stuff but let's just go with metal",
    "start": "444319",
    "end": "449520"
  },
  {
    "text": "medals I can select the difficulty level the number of questions let's do five questions and let's do it in English um",
    "start": "449520",
    "end": "456080"
  },
  {
    "text": "I'll do another quick test in a minute where I'm going to do it in a different language but for now let's just do it in",
    "start": "456080",
    "end": "461919"
  },
  {
    "text": "English the first thing you'll notice is you get a placeholder for the quiz and",
    "start": "461919",
    "end": "467240"
  },
  {
    "text": "that's because these llms are slow and you need to build something into your user interface to condition",
    "start": "467240",
    "end": "474199"
  },
  {
    "text": "the user to understand you are working on it and it's going to take a little bit of time but notice uh two things one",
    "start": "474199",
    "end": "481360"
  },
  {
    "text": "is that um you know I I gave progress and as soon as it was ready I I I gave",
    "start": "481360",
    "end": "486479"
  },
  {
    "text": "an indication that it was ready and at the same time I generated a unique image again using uh generative AI That's",
    "start": "486479",
    "end": "493879"
  },
  {
    "text": "based on the topic so this is an image I guess of metal um so if I want to actually play",
    "start": "493879",
    "end": "501720"
  },
  {
    "text": "this quiz I can select host here I get an option to do a synchronous or an asynchronous quiz",
    "start": "501720",
    "end": "508800"
  },
  {
    "text": "what's the difference an asynchronous quiz means you can uh players can play it any time they want and can advance",
    "start": "508800",
    "end": "516000"
  },
  {
    "text": "the questions from their player experience the use case for this again go back to a teacher it's Friday at the",
    "start": "516000",
    "end": "522760"
  },
  {
    "text": "end of the week we just studied metals and which you do and uh we want",
    "start": "522760",
    "end": "531800"
  },
  {
    "text": "to give our students a little quiz uh during the weekend so we do an",
    "start": "531800",
    "end": "537200"
  },
  {
    "text": "asynchronous quiz and the students can do the quiz whenever they want and we'll tally their results and they can advance",
    "start": "537200",
    "end": "543000"
  },
  {
    "text": "through it as they like but today we're going to do a synchronous quiz because we want to have a competition okay so",
    "start": "543000",
    "end": "548120"
  },
  {
    "text": "I'll say start hosting and I want you to take out your phones this is one of those rare talks",
    "start": "548120",
    "end": "554959"
  },
  {
    "text": "where the speaker wants you to be on your phone um go ahead and scan",
    "start": "554959",
    "end": "562320"
  },
  {
    "text": "that it should take you to a place where you identify yourself with a name",
    "start": "562800",
    "end": "570279"
  },
  {
    "text": "and who let's see who's going to be first in AK wherever you are good good speed there why do you have to name",
    "start": "570279",
    "end": "577920"
  },
  {
    "text": "yourself right shouldn't the system know who you are I wanted the player experience to be anonymous I don't want",
    "start": "577920",
    "end": "584200"
  },
  {
    "text": "you to have to have a Google login or any other login and I don't want you to have to authenticate yourself just to",
    "start": "584200",
    "end": "589760"
  },
  {
    "text": "have fun playing a quiz so that's why you need to give a little bit of ID okay we now have",
    "start": "589760",
    "end": "597480"
  },
  {
    "text": "60 can we get to 7 75 wow okay we'll get a good good count",
    "start": "597480",
    "end": "606440"
  },
  {
    "text": "of how many people are in this room this way 88 slowing down 90 can we get to 100",
    "start": "606440",
    "end": "613600"
  },
  {
    "text": "how cool would that be come on oh the QR is gone there we",
    "start": "613600",
    "end": "621640"
  },
  {
    "text": "go that's a bug by the way I shouldn't have had to do that but 93 are there is",
    "start": "621640",
    "end": "627440"
  },
  {
    "text": "there anyone who hasn't done it yet okay okay I won't Hound you 93 players that's perfectly fine now what you'll notice is",
    "start": "627440",
    "end": "633920"
  },
  {
    "text": "when I start the quiz it's going to immediately pop up the question the first question on your phone and uh time",
    "start": "633920",
    "end": "642079"
  },
  {
    "text": "as well as accuracy counts toward your score so let's start the",
    "start": "642079",
    "end": "647120"
  },
  {
    "text": "quiz which metal is a key component of the production of stainless steel",
    "start": "647120",
    "end": "652240"
  },
  {
    "text": "offering corrosion resistance so you'll notice at the top of the page I'm getting a running count of how many of",
    "start": "652240",
    "end": "659880"
  },
  {
    "text": "the participants have responded so that gives me a clue as to when would be a good time to reveal the answer and move",
    "start": "659880",
    "end": "666200"
  },
  {
    "text": "on to the next question I think I gave you each 15",
    "start": "666200",
    "end": "671920"
  },
  {
    "text": "seconds so you're going to run out of time if you haven't responded but um seems like we're stuck at 75 so if",
    "start": "671920",
    "end": "680639"
  },
  {
    "text": "that's because you're having trouble you're seeing some problem with the app please come down after this session and",
    "start": "680639",
    "end": "687320"
  },
  {
    "text": "talk with me if it's because you got a little tired of playing already that's fine let's show the results we get a",
    "start": "687320",
    "end": "694480"
  },
  {
    "text": "little histogram showing uh what was the most popular answer which was Zinc apparently the correct answer was",
    "start": "694480",
    "end": "701399"
  },
  {
    "text": "chromium and JJ is in first place another thing if you know about",
    "start": "701399",
    "end": "708360"
  },
  {
    "text": "metals and you decide one of these questions or more is are wrong please come see me as well and part of the talk",
    "start": "708360",
    "end": "716800"
  },
  {
    "text": "later on we're going to talk about how we try to figure out out whether this these quizzes are accurate and reliable",
    "start": "716800",
    "end": "722959"
  },
  {
    "text": "okay next question probably move a little more quickly brass is an alloy primarily",
    "start": "722959",
    "end": "730480"
  },
  {
    "text": "composed of which two",
    "start": "730480",
    "end": "733920"
  },
  {
    "text": "metals okay we're getting more responses this time 83 let's go ahead and reveal the",
    "start": "739320",
    "end": "746920"
  },
  {
    "text": "results the correct answer was copper and zinc and that was the most popular answer and JJ retains their position in",
    "start": "746920",
    "end": "755160"
  },
  {
    "text": "first place which metal known for its high con conductivity is commonly used in electric",
    "start": "755160",
    "end": "762720"
  },
  {
    "text": "wiring by the way I love trivia quizzes I love going to Pub quizzes and that's",
    "start": "766680",
    "end": "772240"
  },
  {
    "text": "my secret use cases I want to make this available to quiz Masters around the world who who hold Pub quizzes or other",
    "start": "772240",
    "end": "779680"
  },
  {
    "text": "quizes uh we're at 58 of 103 let's go ahead and reveal the most common answer was copper",
    "start": "779680",
    "end": "788920"
  },
  {
    "text": "and that was correct St has jumped into first place so this",
    "start": "788920",
    "end": "794240"
  },
  {
    "text": "is getting pretty exciting next question number four due to its lightweight and",
    "start": "794240",
    "end": "799320"
  },
  {
    "text": "high strength which metal is heavily utilized in the Aerospace industry",
    "start": "799320",
    "end": "805480"
  },
  {
    "text": "okay I'm going to reveal a little bit earlier just so we don't use lose too much time on the quiz but the answer is",
    "start": "813600",
    "end": "819880"
  },
  {
    "text": "titanium and most people got that jock is now in first place okay we're coming to the last",
    "start": "819880",
    "end": "825199"
  },
  {
    "text": "question amalgam a Mercury based alloy has historically been used in which",
    "start": "825199",
    "end": "831920"
  },
  {
    "text": "application now after this question if you're one of the top three finishers you will get a fireworks to display on",
    "start": "832639",
    "end": "839600"
  },
  {
    "text": "your phone so look for that and 68 of 104 oh we did get over",
    "start": "839600",
    "end": "846160"
  },
  {
    "text": "100 players that's awesome let's show the results the most popular result was",
    "start": "846160",
    "end": "851759"
  },
  {
    "text": "Dental fillings and that was the correct answer our top three finishers are os s and Wall-E did did you where are",
    "start": "851759",
    "end": "860959"
  },
  {
    "text": "you all did you see your phones give a little fireworks",
    "start": "860959",
    "end": "866000"
  },
  {
    "text": "display yes okay let's give a round of applause for our three three",
    "start": "866000",
    "end": "872079"
  },
  {
    "text": "winners hopefully that gives you a sense of what the app is all about um I said",
    "start": "872519",
    "end": "878639"
  },
  {
    "text": "that I would try something with a different language I'm going to do a quick test how many of you speak",
    "start": "878639",
    "end": "886720"
  },
  {
    "text": "Norwegian okay I'm not surprised I'm glad that's comforting to know I'm in",
    "start": "888480",
    "end": "894079"
  },
  {
    "text": "the country I thought I was in",
    "start": "894079",
    "end": "898199"
  },
  {
    "text": "is it iie okay I've already learned something",
    "start": "900600",
    "end": "908120"
  },
  {
    "text": "that's an embarrassing thing to not know but thank you for helping me with that",
    "start": "908120",
    "end": "913279"
  },
  {
    "text": "let's do three and let's do a different um should we do Oslo trivia or something",
    "start": "913279",
    "end": "920480"
  },
  {
    "text": "local Oslo history let's save it and um while it's",
    "start": "920600",
    "end": "928399"
  },
  {
    "text": "thinking I'll tell you just the backstory on this language feature I was giving this talk or variation of it in",
    "start": "928399",
    "end": "934480"
  },
  {
    "text": "Sweden and I thought Here I Am The Ugly American everything I'm doing is in English I should really make this talk",
    "start": "934480",
    "end": "940759"
  },
  {
    "text": "in Swedish and then I thought llm I just have to add the words in Swedish to my prompt and so got up in",
    "start": "940759",
    "end": "949079"
  },
  {
    "text": "front of the audience and I added in Swedish and I put it in the wrong place in the sentence I said generate a quiz",
    "start": "949079",
    "end": "957639"
  },
  {
    "text": "based on this topic fill in the topic in Swedish",
    "start": "957639",
    "end": "963079"
  },
  {
    "text": "and it interpreted in Swedish as applying to the topic and my topic was in English and so it just threw that",
    "start": "963079",
    "end": "968720"
  },
  {
    "text": "part away going back after the talk I put in Swedish right after quiz generate",
    "start": "968720",
    "end": "974240"
  },
  {
    "text": "a quiz in Swedish based on the following topic and it works so the lesson is these things are very finicky and",
    "start": "974240",
    "end": "980279"
  },
  {
    "text": "they're very literal whatever you tell them to do they're going to do uh so be careful what you ask for I",
    "start": "980279",
    "end": "987440"
  },
  {
    "text": "am not going to run this quiz I'm just going to show you the questions and you",
    "start": "987440",
    "end": "993920"
  },
  {
    "text": "tell me oh there is a feature here I wanted to explain you see all these people registered I didn't have to give",
    "start": "993920",
    "end": "1000079"
  },
  {
    "text": "you another link I've made these uh quizzes in this Paradigm of a session so",
    "start": "1000079",
    "end": "1006959"
  },
  {
    "text": "the same exact uh entry can excuse me can give you access to multiple",
    "start": "1006959",
    "end": "1012720"
  },
  {
    "text": "successive quizzes which is kind of a nice feature anyway we're not going to compete on this one I just want to show",
    "start": "1012720",
    "end": "1018079"
  },
  {
    "text": "you that does this make any sense because obviously I'm not able to read it and the right answer is",
    "start": "1018079",
    "end": "1025880"
  },
  {
    "text": "1814 sound right I show you one more that's wrong that's",
    "start": "1025880",
    "end": "1031640"
  },
  {
    "text": "wrong okay back to the drawing board how about this one does that make sense",
    "start": "1031640",
    "end": "1038319"
  },
  {
    "text": "linguistically is it good is it proper",
    "start": "1038319",
    "end": "1042558"
  },
  {
    "text": "Norwegian and slow no okay oh okay one more so far we're",
    "start": "1047079",
    "end": "1054840"
  },
  {
    "text": "zero for two how about this one is it proper or Norwegian is it",
    "start": "1054840",
    "end": "1061880"
  },
  {
    "text": "readable and let's see the right make sense okay not a great",
    "start": "1061880",
    "end": "1070360"
  },
  {
    "text": "example I think I maybe got one out of three right but I have the feeling you're being generous with me but the",
    "start": "1070360",
    "end": "1077039"
  },
  {
    "text": "point is I got a mult multilingual feature and we can argue about how well",
    "start": "1077039",
    "end": "1083159"
  },
  {
    "text": "I got it but I got a multilingual capability by adding a couple of words",
    "start": "1083159",
    "end": "1088679"
  },
  {
    "text": "to a prompt and tweaking my user interface to let me select a language that's a pretty different way of",
    "start": "1088679",
    "end": "1094880"
  },
  {
    "text": "operating than than I've ever had in the past um in my past experience with software development",
    "start": "1094880",
    "end": "1103200"
  },
  {
    "text": "okay that's our demo and we will now move on to tell you",
    "start": "1104280",
    "end": "1109360"
  },
  {
    "text": "a bit about the architecture and design of the application okay um so let's just go",
    "start": "1109360",
    "end": "1115120"
  },
  {
    "text": "through like the design of the app and then we'll get into the Gen part well",
    "start": "1115120",
    "end": "1120360"
  },
  {
    "text": "first of all this is what we have for the app um we have two tier like so we",
    "start": "1120360",
    "end": "1125640"
  },
  {
    "text": "have a UI server that um host the UI and we use flutter which I'll talk about",
    "start": "1125640",
    "end": "1131000"
  },
  {
    "text": "then we have a separate API server uh it's a python flask application running on cloud run um and that python is",
    "start": "1131000",
    "end": "1139159"
  },
  {
    "text": "talking to the llm um on the llm site we are using Gemini as a model uh so Gemini",
    "start": "1139159",
    "end": "1145280"
  },
  {
    "text": "is the model we use to generate the quizzes uh and imag gen is the model we use to generate the image on that topic",
    "start": "1145280",
    "end": "1152400"
  },
  {
    "text": "uh and this is all running on Google Google Cloud um weex AI um which is the which is the AI platform of Google cloud",
    "start": "1152400",
    "end": "1160039"
  },
  {
    "text": "and for the back end we are using fir store um which is a Nole database that I'll talk about um and fire store has",
    "start": "1160039",
    "end": "1167400"
  },
  {
    "text": "this nice feature of real time updates so it basically gives you free real time",
    "start": "1167400",
    "end": "1172520"
  },
  {
    "text": "updates to your browser and we use that in the application to to give you updates uh about the state so that's",
    "start": "1172520",
    "end": "1178799"
  },
  {
    "text": "kind of the architecture of the small app um we chose to use flatter for the client because we want to have a mobile",
    "start": "1178799",
    "end": "1185840"
  },
  {
    "text": "app and also a web application uh and flutter for those of you who might not know it's a it's it's a platform for",
    "start": "1185840",
    "end": "1194720"
  },
  {
    "text": "multiplatform applications from Google and it uses a language called Dart which is similar to C and Java if you're used",
    "start": "1194720",
    "end": "1200760"
  },
  {
    "text": "to it so I like that it's a strongly type language but for the client side so it work pretty well for us um for",
    "start": "1200760",
    "end": "1207880"
  },
  {
    "text": "hosting the UI and API servers we are using Cloud run which is the um container service on Google cloud and",
    "start": "1207880",
    "end": "1214120"
  },
  {
    "text": "it's the default place that I always deploy my code to because it gives you flexibility uh with the containers but",
    "start": "1214120",
    "end": "1221080"
  },
  {
    "text": "it's also very easy to use you can actually go from your source code to a running service pretty quickly even",
    "start": "1221080",
    "end": "1227280"
  },
  {
    "text": "without having to create a Docker file so that's what we use as well and it gives you all the features of autoscaling and and monitoring and",
    "start": "1227280",
    "end": "1234480"
  },
  {
    "text": "logging and all that kind of stuff within Google cloud and for fire store and for backend we use fir store because",
    "start": "1234480",
    "end": "1240400"
  },
  {
    "text": "it's a no SQL database it's a document kind of um store and this goes well with",
    "start": "1240400",
    "end": "1246919"
  },
  {
    "text": "um quizzes because they quizzes are kind of like documents anyway and we also have this um almost free feature or",
    "start": "1246919",
    "end": "1253919"
  },
  {
    "text": "real-time update so anything we change on the server site this state gets updated in the browser automatically",
    "start": "1253919",
    "end": "1258960"
  },
  {
    "text": "without having without us having to do anything so that was also a good choice for us so that's very quick explanation",
    "start": "1258960",
    "end": "1265520"
  },
  {
    "text": "of what we use then we have the typical data structures like we have admins and",
    "start": "1265520",
    "end": "1271200"
  },
  {
    "text": "generators and quizzes and sessions and results uh we have four key personas",
    "start": "1271200",
    "end": "1276320"
  },
  {
    "text": "like the admin Persona the Creator host and player and then",
    "start": "1276320",
    "end": "1282400"
  },
  {
    "text": "um different um different personas have access to different fire store databases",
    "start": "1282400",
    "end": "1289320"
  },
  {
    "text": "so this is kind of what they can do in terms of read and write so I won't go through in too much detail but just to",
    "start": "1289320",
    "end": "1295240"
  },
  {
    "text": "give you an overview of like uh what they can do and where and then from here like we have",
    "start": "1295240",
    "end": "1301960"
  },
  {
    "text": "the typical API that you expect like getting the quizzes creating the quizzes deleting the quizzes uh starting um a a",
    "start": "1301960",
    "end": "1309279"
  },
  {
    "text": "game things like that and then we use vertex AI for",
    "start": "1309279",
    "end": "1316720"
  },
  {
    "text": "generating the quiz and for that um we had to play with it a little bit because",
    "start": "1316720",
    "end": "1322000"
  },
  {
    "text": "you need you need to do a a little bit of prompt engineering you need to kind of get your prompt right get your responses right and play with the",
    "start": "1322000",
    "end": "1328720"
  },
  {
    "text": "parameters a little bit to get it right so actually I will show you this so if you are um let me close this one we",
    "start": "1328720",
    "end": "1335760"
  },
  {
    "text": "don't need it anymore close this one okay so this is where text AI in Google Cloud um and here you have access to",
    "start": "1335760",
    "end": "1344440"
  },
  {
    "text": "different models so the Gemini 1.5 flash is the latest one there's some experimental ones here um then this is",
    "start": "1344440",
    "end": "1352320"
  },
  {
    "text": "where you kind of try different prompts so normally we started with something",
    "start": "1352320",
    "end": "1357559"
  },
  {
    "text": "simple like give me a trivia quiz on this topic um but then we don't want",
    "start": "1357559",
    "end": "1363000"
  },
  {
    "text": "free text so we add it like give me in Json and give me this category so you have to play with it to get the results",
    "start": "1363000",
    "end": "1369840"
  },
  {
    "text": "you want but in the end this is the kind of prompt that we have you know we are a trivia expert and generate a set of",
    "start": "1369840",
    "end": "1376360"
  },
  {
    "text": "multiple choice questions this is the category this is the difficulty this is the number of questions this is the responses per question and the language",
    "start": "1376360",
    "end": "1383480"
  },
  {
    "text": "that we added later so this almost came for free and then we also added some rules you know we we care about accuracy",
    "start": "1383480",
    "end": "1390919"
  },
  {
    "text": "because we don't want fake quizzes even though we still get them and we'll talk about that uh we don't want questions",
    "start": "1390919",
    "end": "1396919"
  },
  {
    "text": "with multiple correct answers so you have to be you have to tell llm exactly what you need and even then sometimes it",
    "start": "1396919",
    "end": "1403200"
  },
  {
    "text": "doesn't give you the right thing um so what and at some point when we were using different models uh we had to give",
    "start": "1403200",
    "end": "1409840"
  },
  {
    "text": "some examples of what we expected but then as models progressed we realized",
    "start": "1409840",
    "end": "1415279"
  },
  {
    "text": "that we don't need to give examples anymore by just giving some rules it was giving us the the output that we needed",
    "start": "1415279",
    "end": "1420960"
  },
  {
    "text": "so once you play with this um you can change the temperature which is the creativity of the model you can change",
    "start": "1420960",
    "end": "1428480"
  },
  {
    "text": "the output to token limit which means how where boss you want your output um and then you can just hit",
    "start": "1428480",
    "end": "1434480"
  },
  {
    "text": "submit and just play with it and see what you get through the console and and I think yeah if we go down here we",
    "start": "1434480",
    "end": "1441240"
  },
  {
    "text": "should see a response um and then yeah you see that we get a Json response with",
    "start": "1441240",
    "end": "1446480"
  },
  {
    "text": "a question responses and the correct response like like that and once you're happy you can go to get code and then",
    "start": "1446480",
    "end": "1454080"
  },
  {
    "text": "this gives you the code in Python U but also other languages not just Java and",
    "start": "1454080",
    "end": "1459400"
  },
  {
    "text": "it's not in the in here but if you are C person we actually have some C examples",
    "start": "1459400",
    "end": "1466360"
  },
  {
    "text": "in on GitHub where you can actually this kind of thing from C so you create a prediction client and set your",
    "start": "1466360",
    "end": "1473480"
  },
  {
    "text": "parameters uh with your prompt and then just get a response so it's all of this",
    "start": "1473480",
    "end": "1478799"
  },
  {
    "text": "is also possible in C it doesn't have to be python I mean you know python is the language of gen but it's actually not",
    "start": "1478799",
    "end": "1486799"
  },
  {
    "text": "true nowadays like you can use whatever language you want like we have SDK and",
    "start": "1486799",
    "end": "1492039"
  },
  {
    "text": "uh liaries for that all right um so that's how we got our um",
    "start": "1492039",
    "end": "1499600"
  },
  {
    "text": "and quiz generation working and if if I go",
    "start": "1499600",
    "end": "1505600"
  },
  {
    "text": "back so in terms of quiz generation so initially as I said we had the open Tri trivia quiz generator which was static",
    "start": "1506039",
    "end": "1512919"
  },
  {
    "text": "so you only had like certain topics and you only use open three by data set so it was very limited then we initially",
    "start": "1512919",
    "end": "1519919"
  },
  {
    "text": "started using pal which was the model that came out from Google in a in March",
    "start": "1519919",
    "end": "1525240"
  },
  {
    "text": "2023 um this kind of worked but it wasn't that that great in the sense that it wasn't as accurate as other models",
    "start": "1525240",
    "end": "1532120"
  },
  {
    "text": "then Gemini came along with Pro and Ultra versions and that's the latest one that we are using right now but we still",
    "start": "1532120",
    "end": "1538240"
  },
  {
    "text": "kept the palm and other ones just in case we want to test with it and um The",
    "start": "1538240",
    "end": "1543440"
  },
  {
    "text": "Prompt uh so how do you get quizzes I show you this already so this is the prompt that works for us today uh but we",
    "start": "1543440",
    "end": "1549640"
  },
  {
    "text": "always um kind of tweak it as we go along or as we try different models um",
    "start": "1549640",
    "end": "1555120"
  },
  {
    "text": "and sometimes you need even though we think this works you still need to do a little bit of post processing afterwards",
    "start": "1555120",
    "end": "1561360"
  },
  {
    "text": "for example sometimes we get Json and it's correct Json but sometimes we actually",
    "start": "1561360",
    "end": "1567240"
  },
  {
    "text": "get Json like not not in markdown or something like that so it's not proper",
    "start": "1567240",
    "end": "1572840"
  },
  {
    "text": "Json so we have to do a little bit of post processing but for the most part this seems to work and the code is very simple I mean",
    "start": "1572840",
    "end": "1579799"
  },
  {
    "text": "this is python but it's as simple in C it's literally just Define your model uh",
    "start": "1579799",
    "end": "1585720"
  },
  {
    "text": "Define your parameters and passing passing your prompt um here and then you just get a response that un need to",
    "start": "1585720",
    "end": "1591760"
  },
  {
    "text": "parse so it's it's just one one call for image generation uh we started",
    "start": "1591760",
    "end": "1598120"
  },
  {
    "text": "with image gen one version one um and then we switched to version two um when",
    "start": "1598120",
    "end": "1604279"
  },
  {
    "text": "it came out um and it version two is basically the better version of version one uh and to use that again it's a",
    "start": "1604279",
    "end": "1612799"
  },
  {
    "text": "single call so you define your model that you want to use so this is the version two",
    "start": "1612799",
    "end": "1618960"
  },
  {
    "text": "um model and then you have your prompt in this case our prompt is very simple photo realistic image about topic and",
    "start": "1618960",
    "end": "1627039"
  },
  {
    "text": "then you can specify number of images you you get back in this case just one and you also have this negative prompt",
    "start": "1627039",
    "end": "1632600"
  },
  {
    "text": "where you you tell the model what you don't want so in this case we just said blurry uh we could probably add more things here to make it more appropriate",
    "start": "1632600",
    "end": "1640480"
  },
  {
    "text": "for quizzes but this seems to work pretty well as well all right um so so after we",
    "start": "1640480",
    "end": "1649399"
  },
  {
    "text": "implemented this now I want to revisit the problems that we initially had with this application so if you remember we",
    "start": "1649399",
    "end": "1656360"
  },
  {
    "text": "had limited set of topics now with Gen we can do pretty much any",
    "start": "1656360",
    "end": "1661559"
  },
  {
    "text": "topic we had limited questions and answers now again everything like it's pretty much like unlimited questions and",
    "start": "1661559",
    "end": "1668919"
  },
  {
    "text": "answers we had Limited Format uh multiple choice um for questions uh in",
    "start": "1668919",
    "end": "1675080"
  },
  {
    "text": "this case you can have different formats you just need to change your prompt um we are still using multiple choice uh",
    "start": "1675080",
    "end": "1682039"
  },
  {
    "text": "but we don't have to we can have more than four answers we can have free form we can we can have pretty much any",
    "start": "1682039",
    "end": "1687840"
  },
  {
    "text": "format we want it used to be English only now it's any language um there were",
    "start": "1687840",
    "end": "1693519"
  },
  {
    "text": "no images now we can generate images on demand they might not be great but at least it's it's an image um that we can",
    "start": "1693519",
    "end": "1701360"
  },
  {
    "text": "use and expanding quiz content was very difficult before we had open Trivia and",
    "start": "1701360",
    "end": "1707679"
  },
  {
    "text": "we couldn't really expanded beyond that and that's what kept the app in its state for like a few years but now with",
    "start": "1707679",
    "end": "1714279"
  },
  {
    "text": "J expanding the quiz content is really easy but with Gen uh we now have new",
    "start": "1714279",
    "end": "1722399"
  },
  {
    "text": "problems um first of all the learning curve so you know being software",
    "start": "1722399",
    "end": "1727440"
  },
  {
    "text": "Engineers we are used to our tools and our languages and stuff like that but with Gen you're entering to a new",
    "start": "1727440",
    "end": "1733880"
  },
  {
    "text": "landscape where you have different models different platforms different frame works and you need to kind of",
    "start": "1733880",
    "end": "1739960"
  },
  {
    "text": "figure it out like what I use can I use C here can I use this model or for image generation which model should I use for",
    "start": "1739960",
    "end": "1747039"
  },
  {
    "text": "or should I use Gemini Nano or Gemini Ultra so there's a lot of things you need to kind of figure it out and there's a little bit of learning curve",
    "start": "1747039",
    "end": "1754440"
  },
  {
    "text": "there and the landscape is changing quite a bit uh just within one year we went from Palm to Gemini um Pro to",
    "start": "1754440",
    "end": "1763039"
  },
  {
    "text": "Gemini Ultra and now we will probably use Gemini flash um Palm had it's own",
    "start": "1763039",
    "end": "1768519"
  },
  {
    "text": "API and libraries and then when we switched to Gemini we had to use new libraries um so it's a very fast",
    "start": "1768519",
    "end": "1774399"
  },
  {
    "text": "changing landscape not not just within Google but also outside Google as well and you need to kind of keep up to dat",
    "start": "1774399",
    "end": "1781080"
  },
  {
    "text": "and kind of figure it out like how like what should what I should use which libraries and and make sure you are",
    "start": "1781080",
    "end": "1787480"
  },
  {
    "text": "using the right things one of the biggest problems with llms is you will have inconsistent or no",
    "start": "1787480",
    "end": "1795519"
  },
  {
    "text": "output sometimes for no reason that just the way they work like it's not like if",
    "start": "1795519",
    "end": "1801360"
  },
  {
    "text": "you pass in the exact same input twice you might get you might get different outputs and you need to be comfortable",
    "start": "1801360",
    "end": "1807279"
  },
  {
    "text": "with that with that idea you know again as software Engineers we're used to",
    "start": "1807279",
    "end": "1812399"
  },
  {
    "text": "having consistency so if you call something you expect the same response but with llms that's not",
    "start": "1812399",
    "end": "1818600"
  },
  {
    "text": "true and llm calls are slow so it's not like normal API call I mean sometimes",
    "start": "1818600",
    "end": "1824399"
  },
  {
    "text": "it's really quick but sometimes you need to wait a little bit to get an answer sometimes doesn't respond sometimes it",
    "start": "1824399",
    "end": "1829519"
  },
  {
    "text": "responds with something that that's not true or or no output so you need to deal",
    "start": "1829519",
    "end": "1835240"
  },
  {
    "text": "with that hallucinations it's a big problem as you seen in the Norwegian um",
    "start": "1835240",
    "end": "1841159"
  },
  {
    "text": "quiz sometimes it tells you the wrong thing and maybe it's okay sometimes but",
    "start": "1841159",
    "end": "1846640"
  },
  {
    "text": "sometimes it's not okay so again you need to deal with that as well and I think the most the biggest",
    "start": "1846640",
    "end": "1853519"
  },
  {
    "text": "and most difficult problem is how do you check the accuracy and quality of llm",
    "start": "1853519",
    "end": "1859480"
  },
  {
    "text": "outputs okay and you know it's easy to get content out of llms you can get text",
    "start": "1859480",
    "end": "1867320"
  },
  {
    "text": "out you can get images out but then how do you know that that text is what you want it how do you measure it how do you",
    "start": "1867320",
    "end": "1873760"
  },
  {
    "text": "know that the image is actually what you want what's the quality of that image like it's that's very hard to check and",
    "start": "1873760",
    "end": "1881080"
  },
  {
    "text": "you need to again do things to kind of measure that and we'll talk more about that",
    "start": "1881080",
    "end": "1886440"
  },
  {
    "text": "shortly all right so so that was a very quick overview of the app where it came from um and how it works and what it",
    "start": "1886440",
    "end": "1894000"
  },
  {
    "text": "solves and what it introduces now we want to switch to the real me of the",
    "start": "1894000",
    "end": "1899519"
  },
  {
    "text": "talk and talk about what what we actually learned um throughout this exercise so I'll hand it over to Mark",
    "start": "1899519",
    "end": "1905720"
  },
  {
    "text": "and yeah let's go through that thanks MAA so I'll start with a couple of",
    "start": "1905720",
    "end": "1912200"
  },
  {
    "text": "General things one is it's surprisingly easy to do what used to be hard with Gen",
    "start": "1912200",
    "end": "1918600"
  },
  {
    "text": "AI um I guess the the language capability is is a example of of that",
    "start": "1918600",
    "end": "1927279"
  },
  {
    "text": "um but uh you know I've been developing",
    "start": "1927279",
    "end": "1932639"
  },
  {
    "text": "application since I started my career in 1982 so a little over four",
    "start": "1932639",
    "end": "1937840"
  },
  {
    "text": "decades and this has really changed how I think about building apps which is",
    "start": "1937840",
    "end": "1943240"
  },
  {
    "text": "pretty pretty significant um some of you may have seen this tweet from Andre carpey where he",
    "start": "1943240",
    "end": "1950159"
  },
  {
    "text": "said the hottest new programming language is English and I would now say it's Norwegian of",
    "start": "1950159",
    "end": "1958120"
  },
  {
    "text": "course but the point being natural language programming has arrived but with that power and convenience comes a",
    "start": "1958120",
    "end": "1964840"
  },
  {
    "text": "lot of um kind of things you need to to deal with to worry about um quiz image",
    "start": "1964840",
    "end": "1972080"
  },
  {
    "text": "generation with a single API call is another example of how powerful this is",
    "start": "1972080",
    "end": "1977639"
  },
  {
    "text": "on on the other hand it's hard to do things well and consistently so um good results require very specific",
    "start": "1977639",
    "end": "1985840"
  },
  {
    "text": "prompt engineering and then once you've optimized your prompt you may find it needs you need to keep keep working on",
    "start": "1985840",
    "end": "1991399"
  },
  {
    "text": "it that's almost like your software like life cycle where you're finding and fixing bugs all the time and you will",
    "start": "1991399",
    "end": "1997760"
  },
  {
    "text": "get inconsistent outputs no matter how hard you try and so you need to adopt a belt and suspenders kind of approach um",
    "start": "1997760",
    "end": "2005440"
  },
  {
    "text": "where you kind of codify in the prompt exactly what you want but then you don't",
    "start": "2005440",
    "end": "2011600"
  },
  {
    "text": "assume you're going to get it you sort of still code to deal with what might happen if you don't get exactly what you",
    "start": "2011600",
    "end": "2018320"
  },
  {
    "text": "want a good example of this is early on I was asking for the quiz in",
    "start": "2018320",
    "end": "2023679"
  },
  {
    "text": "Json and it was working great and then a version of the model came out where it would say it would prefix the Json with",
    "start": "2023679",
    "end": "2031039"
  },
  {
    "text": "here's your Json like thank you you just blew up my app and then I realized it's",
    "start": "2031039",
    "end": "2037440"
  },
  {
    "text": "not its fault I need to be sure I deal with the",
    "start": "2037440",
    "end": "2043399"
  },
  {
    "text": "fact that I might not get perfect Json so I strip out anything that comes",
    "start": "2043399",
    "end": "2048919"
  },
  {
    "text": "before the beginning of the Json and then uh of course I got something after",
    "start": "2048919",
    "end": "2055280"
  },
  {
    "text": "the document I won't keep going with this story but um you can imagine uh why",
    "start": "2055280",
    "end": "2060560"
  },
  {
    "text": "it can be frustrating and you need to keep worrying about coding defensively basically um the other hard part we've",
    "start": "2060560",
    "end": "2068200"
  },
  {
    "text": "already mentioned is hard to measure the output quality by the way it's easy for a human",
    "start": "2068200",
    "end": "2073240"
  },
  {
    "text": "being to measure the output quality but that's not good enough right we need Automation and Engineering practices so",
    "start": "2073240",
    "end": "2078560"
  },
  {
    "text": "we're going to talk more about how we're doing that except the uncertainty of an llm this is a bit repetitive so I'm",
    "start": "2078560",
    "end": "2084760"
  },
  {
    "text": "going to kind of go through this quickly but the same prompt same model can give you different output the same prompt uh",
    "start": "2084760",
    "end": "2092118"
  },
  {
    "text": "same model can the model can get updated to give you different output and so on so just expect the unexpected when",
    "start": "2092119",
    "end": "2098480"
  },
  {
    "text": "you're working with these things the the good side of that story",
    "start": "2098480",
    "end": "2103640"
  },
  {
    "text": "is that when a new model comes out suddenly your app gets better and you didn't have to do any work quote unquote",
    "start": "2103640",
    "end": "2110560"
  },
  {
    "text": "other than test and verify and adopt that new model but we saw that in",
    "start": "2110560",
    "end": "2116400"
  },
  {
    "text": "practice when we started with pal and the quizzes were so so and then we moved to GE Gemini Pro and the quizzes got",
    "start": "2116400",
    "end": "2122640"
  },
  {
    "text": "better but they still weren't great in my opinion and then we went to Gemini Ultra and they're even better so every",
    "start": "2122640",
    "end": "2128079"
  },
  {
    "text": "time these models get better and I'm talking across the industry not just Google uh it's you know the high TI T",
    "start": "2128079",
    "end": "2135480"
  },
  {
    "text": "tide raises all boats kind of a thing every app that's depending on these tools is getting better which is pretty",
    "start": "2135480",
    "end": "2140520"
  },
  {
    "text": "cool same with image generation and got all that value with little or no code",
    "start": "2140520",
    "end": "2146119"
  },
  {
    "text": "changes another thing to ask yourself is do you even need an llm one sort of Pitfall I fell into was I I sort of fell",
    "start": "2146119",
    "end": "2152960"
  },
  {
    "text": "in love a little too hard with these things like I I was like wow I can do anything with these things and you have",
    "start": "2152960",
    "end": "2160400"
  },
  {
    "text": "to remember the costs and you don't want to do everything with them a good example was at one point I was working",
    "start": "2160400",
    "end": "2166200"
  },
  {
    "text": "on uh free form quizzes so this is where it says you know fill in the answer and you don't get the choices you just have",
    "start": "2166200",
    "end": "2172240"
  },
  {
    "text": "to specify the input grading that can be very tricky right humans have very fuzzy",
    "start": "2172240",
    "end": "2178040"
  },
  {
    "text": "linguistic ways of expressing a given answer and you have to decide is this answer correct or not and I started",
    "start": "2178040",
    "end": "2183720"
  },
  {
    "text": "going down the road of I know I'll ask the llm bad idea because number one",
    "start": "2183720",
    "end": "2188960"
  },
  {
    "text": "that's a lot of propagation delay and latency in the user interface uh it's very inefficient but number two there's",
    "start": "2188960",
    "end": "2195640"
  },
  {
    "text": "a much easier way to solve it that's fast and understood which would be like a a python library or whatever language",
    "start": "2195640",
    "end": "2201200"
  },
  {
    "text": "you're working on which does Levenstein distance or fuzzy matching things like that it may not be as good as the",
    "start": "2201200",
    "end": "2208480"
  },
  {
    "text": "solution you want but it's probably going to be good enough um another example was just this this",
    "start": "2208480",
    "end": "2215800"
  },
  {
    "text": "little um sort of logo that you see I spent weeks",
    "start": "2215800",
    "end": "2221440"
  },
  {
    "text": "trying to generate that with just the A and the i in red I can't tell you how",
    "start": "2221440",
    "end": "2227400"
  },
  {
    "text": "how bruised my forehead is from banging it against the wall trying to get that to happen if you know how to do that",
    "start": "2227400",
    "end": "2232720"
  },
  {
    "text": "tell me because I've tried everything I was talking to meta and meta said why don't you just use Photoshop or",
    "start": "2232720",
    "end": "2239400"
  },
  {
    "text": "something like that I was like yes why don't I so sometimes there's a good",
    "start": "2239400",
    "end": "2245599"
  },
  {
    "text": "oldfashioned way to do things you don't have to do everything with the new cool stuff and that avoids an expensive call",
    "start": "2245599",
    "end": "2252640"
  },
  {
    "text": "to uh to what is probably a little bit on the slow side uh functionality",
    "start": "2252640",
    "end": "2257839"
  },
  {
    "text": "prompting be very specific as I've already mentioned more detailed prompts doesn't always lead to better results",
    "start": "2257839",
    "end": "2264520"
  },
  {
    "text": "manage your prompts like code it is critical part of your application now um version your prompts of course uh",
    "start": "2264520",
    "end": "2272280"
  },
  {
    "text": "the output parsing and the prompting kind of go hand inand so you want to think of those two as a that you you're",
    "start": "2272280",
    "end": "2278359"
  },
  {
    "text": "matching up can I say something about the previous part so uh in terms of the",
    "start": "2278359",
    "end": "2283839"
  },
  {
    "text": "more detailed prompts not being better results um this comes from me because as I was doing prompt engineering I thought",
    "start": "2283839",
    "end": "2290640"
  },
  {
    "text": "okay I can give a lot of rules and the more rules I give it will get get better right that's what you think but actually",
    "start": "2290640",
    "end": "2295960"
  },
  {
    "text": "I realized like in some cases the more the more where was your prompt gets then",
    "start": "2295960",
    "end": "2301760"
  },
  {
    "text": "the results get worse as well so just because you can give more rules doesn't mean that things will get better",
    "start": "2301760",
    "end": "2308160"
  },
  {
    "text": "and and that's a little bit unintuitive for me U so what you will really want is figure out the minimal prompt that works",
    "start": "2308160",
    "end": "2314560"
  },
  {
    "text": "and kind of build on that but making sure that every little thing you add to your prompt actually makes things better",
    "start": "2314560",
    "end": "2321200"
  },
  {
    "text": "and for that to happen you need to be able to measure things and we'll get to that later but yeah so this is one thing",
    "start": "2321200",
    "end": "2326680"
  },
  {
    "text": "and the second thing is that um so there's two parts to the llms one is the prompt the other is the output you get",
    "start": "2326680",
    "end": "2333960"
  },
  {
    "text": "from from The Prompt right and you want to be able to Cur at them because as you",
    "start": "2333960",
    "end": "2339119"
  },
  {
    "text": "iterate on your prompts you probably want to version them so you can keep track but then as as the prompts change",
    "start": "2339119",
    "end": "2346839"
  },
  {
    "text": "the outputting how you parse it will also change so it's good to kind of have it in your code as well the same kind of",
    "start": "2346839",
    "end": "2352800"
  },
  {
    "text": "versioning so that when you change your prompt you can you know how to parse it because it will change depending on the promp so I just want to make sure that",
    "start": "2352800",
    "end": "2358640"
  },
  {
    "text": "we cover that because it's it's it will save you a lot of frustration when you're uh iterating on your prompt right",
    "start": "2358640",
    "end": "2365079"
  },
  {
    "text": "thanks mana and then the the example I gave earlier about where it gave me information I didn't expect I didn't",
    "start": "2365079",
    "end": "2370160"
  },
  {
    "text": "mention this but um I fine-tuned The Prompt that was my first attempt and",
    "start": "2370160",
    "end": "2375599"
  },
  {
    "text": "what I said to it was give me a Json document and only a Json documents I was",
    "start": "2375599",
    "end": "2381400"
  },
  {
    "text": "very explicit in my prompt I don't want anything but a but a Json document and that fixed it like it it seemed to work",
    "start": "2381400",
    "end": "2388440"
  },
  {
    "text": "perfectly and what I found was one out of every like 50 times it still did the",
    "start": "2388440",
    "end": "2393599"
  },
  {
    "text": "wrong thing and so then I realized okay even though I got the prompt to be 98%",
    "start": "2393599",
    "end": "2399880"
  },
  {
    "text": "accurate I still need to add the code to to make sure it does the right",
    "start": "2399880",
    "end": "2405040"
  },
  {
    "text": "thing which brings us to code defensively I've already hit that pretty hard so hopefully that's that's clear",
    "start": "2405040",
    "end": "2411160"
  },
  {
    "text": "but the llm can fail and in addition to not getting exactly the output you want",
    "start": "2411160",
    "end": "2417240"
  },
  {
    "text": "and dealing with that also deal with the fact when it just Flatout fails you get something that's just complete garbage",
    "start": "2417240",
    "end": "2424880"
  },
  {
    "text": "you need to have a strategy for that right so that could be retrying some number of times of course you want to",
    "start": "2424880",
    "end": "2431240"
  },
  {
    "text": "keep the user informed of what's going on so they just don't don't see just a blank screen but you're raising the ante",
    "start": "2431240",
    "end": "2436760"
  },
  {
    "text": "I mean you don't you do this in a traditional app right you make a database call and something goes horribly wrong you might not have",
    "start": "2436760",
    "end": "2442680"
  },
  {
    "text": "worried about it as much unless you're a bank because it would be so infrequent but it will happen and you need to code",
    "start": "2442680",
    "end": "2449440"
  },
  {
    "text": "defensively now you're dealing with something that's probably more temperamental than anything you've used",
    "start": "2449440",
    "end": "2455119"
  },
  {
    "text": "before and so the the defensive code and the recovery strategies are even more important than they were in the",
    "start": "2455119",
    "end": "2461520"
  },
  {
    "text": "past uh yeah you get some empty results can you live with no quizzes or no image",
    "start": "2461520",
    "end": "2468520"
  },
  {
    "text": "these are questions you have to ask yourself as an application developer my answer would be no I can't live without",
    "start": "2468520",
    "end": "2474359"
  },
  {
    "text": "a quiz I can live without an image so if the image generation fails I put the",
    "start": "2474359",
    "end": "2479760"
  },
  {
    "text": "quic logo there as the image for the quiz is it perfect no do do you get the",
    "start": "2479760",
    "end": "2485839"
  },
  {
    "text": "functionality without having to lose the whole workflow yes so that's a good that's a good um compromise I",
    "start": "2485839",
    "end": "2493240"
  },
  {
    "text": "think sometimes you'll find the llm is too cautious a lot of the commercial llms are the providers are struggling",
    "start": "2493240",
    "end": "2501640"
  },
  {
    "text": "with making sure they don't do the wrong thing however you define the wrong thing and so you will see that you'll get um",
    "start": "2501640",
    "end": "2508720"
  },
  {
    "text": "failed returns because the topic the subject you asked caused it to generate",
    "start": "2508720",
    "end": "2514920"
  },
  {
    "text": "something that it felt was maybe questionable so you want to look look carefully at the safety settings in your application or your API calls um and",
    "start": "2514920",
    "end": "2522960"
  },
  {
    "text": "decide for yourself what's the you know tolerable level of of safety for your",
    "start": "2522960",
    "end": "2528119"
  },
  {
    "text": "application pinning model versions is really important because if you just use",
    "start": "2528119",
    "end": "2533440"
  },
  {
    "text": "a name that refers to whatever the latest model is then you leave your application open to the fact that it's",
    "start": "2533440",
    "end": "2539800"
  },
  {
    "text": "suddenly suddenly changes overnight or in the middle of the day and your application suddenly behaves differently",
    "start": "2539800",
    "end": "2546119"
  },
  {
    "text": "maybe differently better maybe differently worse and so you want to do this the same way we would do any other",
    "start": "2546119",
    "end": "2553280"
  },
  {
    "text": "software engineering application deployment job you want to test and make sure you're getting exactly what you",
    "start": "2553280",
    "end": "2558680"
  },
  {
    "text": "want and only advance and pin it to the version you specifically want and then advance that when you're",
    "start": "2558680",
    "end": "2566160"
  },
  {
    "text": "ready uh consider a higher level abstraction layer like Lang chain um there are different apis",
    "start": "2567119",
    "end": "2574040"
  },
  {
    "text": "available from Google or any other supplier of a large Lang language model you can embed those into your",
    "start": "2574040",
    "end": "2579920"
  },
  {
    "text": "application in vertex AI we have different libraries for or had different libraries as as met to mention for Palm",
    "start": "2579920",
    "end": "2586640"
  },
  {
    "text": "and Gemini and that's just within Google and now if you want to support other llms you're looking at a constellation",
    "start": "2586640",
    "end": "2593520"
  },
  {
    "text": "of interfaces across the industry so you want to be able to not have to spend a lot of time Reinventing the wheel every",
    "start": "2593520",
    "end": "2601160"
  },
  {
    "text": "time a new llm is available yeah and I should I should add here I'm a library purist meaning I like to use use the",
    "start": "2601160",
    "end": "2607400"
  },
  {
    "text": "library of the service without having to use something like Lang chain for example because I feel like that",
    "start": "2607400",
    "end": "2613359"
  },
  {
    "text": "introduces another level of interaction so that's why we started with the actual libraries of Vex AI um but then when I",
    "start": "2613359",
    "end": "2619880"
  },
  {
    "text": "see that different models they had they end up having different libraries within",
    "start": "2619880",
    "end": "2625119"
  },
  {
    "text": "Google and then if you want to use open AI stuff then they have their own libraries but you're at the end of the",
    "start": "2625119",
    "end": "2630440"
  },
  {
    "text": "day you're doing similar things then that's when I started like switching to Lang chain and because it gives you like",
    "start": "2630440",
    "end": "2635760"
  },
  {
    "text": "this high level kind of interface that works with bunch of different places so my idea changed",
    "start": "2635760",
    "end": "2642720"
  },
  {
    "text": "and now I kind of prefer Lang chain because they have the right level of abstractions right uh but it's up to",
    "start": "2642720",
    "end": "2648920"
  },
  {
    "text": "what you prefer yeah and by the way um there's no right answer here I'm not advocating everyone here should use Lang",
    "start": "2648920",
    "end": "2655640"
  },
  {
    "text": "chain there's always a trade-off when you use an abstraction layer you lose a certain amount of control over what's",
    "start": "2655640",
    "end": "2662280"
  },
  {
    "text": "happening at the lower levels and hopefully the benefit you get from that abst raction and not having to code to",
    "start": "2662280",
    "end": "2668920"
  },
  {
    "text": "specific interfaces it outweighs the loss of control but every use case is",
    "start": "2668920",
    "end": "2674960"
  },
  {
    "text": "unique and it's something you want to consider all the good old software",
    "start": "2674960",
    "end": "2681559"
  },
  {
    "text": "engineering tricks we have still apply so don't forget you can do things like",
    "start": "2681559",
    "end": "2687119"
  },
  {
    "text": "batch your prompts wherever it makes sense to get away with you know sort of minimizing the round trips to the slow",
    "start": "2687119",
    "end": "2694599"
  },
  {
    "text": "thing which is the llm right now you can parallelize calls like we don't wait",
    "start": "2694599",
    "end": "2700079"
  },
  {
    "text": "till the quiz is generated before we start asking for the image we can start them both going at the same time uh we",
    "start": "2700079",
    "end": "2706760"
  },
  {
    "text": "can cache common responses so all those things we've been doing for a long time",
    "start": "2706760",
    "end": "2711880"
  },
  {
    "text": "uh still apply and you should still use them um let's sort of switch gears to testing and validation so unit and",
    "start": "2711880",
    "end": "2718599"
  },
  {
    "text": "functional tests are as important as ever as we've discussed uh it's easy to check the",
    "start": "2718599",
    "end": "2724520"
  },
  {
    "text": "existence or the format of a quiz so that's pretty straightforward is it a parsable Json document which of course",
    "start": "2724520",
    "end": "2730960"
  },
  {
    "text": "you have you want to do you don't want to blindly store this quote unquote uh Json output in a database and start",
    "start": "2730960",
    "end": "2738040"
  },
  {
    "text": "serving it only to find out that the user gets you know a very weird experience so you want to parse the",
    "start": "2738040",
    "end": "2743839"
  },
  {
    "text": "output check it's it exists it looks reasonable and it has the right format is this a quiz with five questions and",
    "start": "2743839",
    "end": "2750400"
  },
  {
    "text": "four answers is it is does the image exist or not but that's just giving you like",
    "start": "2750400",
    "end": "2755960"
  },
  {
    "text": "technically did I get back something that sort of makes syntactic sense but then there's a semantic question is this",
    "start": "2755960",
    "end": "2762400"
  },
  {
    "text": "a good quiz is this an accurate quiz and that's much more difficult to check for",
    "start": "2762400",
    "end": "2768200"
  },
  {
    "text": "example the person entered history is this quiz actually about history is the answer it tells me is",
    "start": "2768200",
    "end": "2774559"
  },
  {
    "text": "correct actually correct we saw a case or may maybe more where that was it it",
    "start": "2774559",
    "end": "2780000"
  },
  {
    "text": "it failed and we want to understand how to tell if that's happening is a generated image appropriate for this",
    "start": "2780000",
    "end": "2785839"
  },
  {
    "text": "quiz that's an even harder problem which we haven't frankly tried to to solve yet",
    "start": "2785839",
    "end": "2791359"
  },
  {
    "text": "but if we try to focus on good engineering practices we need a way to",
    "start": "2791359",
    "end": "2796440"
  },
  {
    "text": "measure llm outputs so here's my idea we should use an llm to evaluate",
    "start": "2796440",
    "end": "2804000"
  },
  {
    "text": "the llm output as far as its accuracy goes everybody okay with",
    "start": "2804000",
    "end": "2810440"
  },
  {
    "text": "that most people kind of laugh when I say that because if you think about it it's like asking a student to grade",
    "start": "2810440",
    "end": "2816480"
  },
  {
    "text": "their own uh exam paper right it kind of doesn't make sense if you can't believe or trust",
    "start": "2816480",
    "end": "2822920"
  },
  {
    "text": "the quiz that it generates why would you believe its ability to to judge its own quiz well it turns out it does work and",
    "start": "2822920",
    "end": "2830079"
  },
  {
    "text": "I'll explain more um but the key is to automate this once we're able to do that",
    "start": "2830079",
    "end": "2837640"
  },
  {
    "text": "to automate it and to use it as a benchmark as we go forward so here's the vision I have for this what I'm calling",
    "start": "2837640",
    "end": "2844240"
  },
  {
    "text": "a validator and we've got it working it's not integrated into the app yet but we've got the Standalone component",
    "start": "2844240",
    "end": "2850319"
  },
  {
    "text": "working so the the the vision is the generator builds uh makes a call to the",
    "start": "2850319",
    "end": "2856160"
  },
  {
    "text": "llm and generates a quiz as you've seen twice we take that quiz and we send it",
    "start": "2856160",
    "end": "2861400"
  },
  {
    "text": "to the validator and the validator asks the LM to validate the quiz accuracy and based on that result we get",
    "start": "2861400",
    "end": "2869160"
  },
  {
    "text": "back something to attach to the quiz so think of this as a confidence rating",
    "start": "2869160",
    "end": "2874800"
  },
  {
    "text": "we're not going to know with 100% certainty that quiz is 100% accurate but we will be able to say something uh",
    "start": "2874800",
    "end": "2881200"
  },
  {
    "text": "statistically significant that quiz is 90% likely to be accurate or 60% likely",
    "start": "2881200",
    "end": "2887920"
  },
  {
    "text": "to be accurate something along those lines so we're never going to have perfect knowledge but we're going to be",
    "start": "2887920",
    "end": "2893040"
  },
  {
    "text": "able to uh infer statistically the quality of of our quizzes with this",
    "start": "2893040",
    "end": "2899040"
  },
  {
    "text": "approach um and by the way I'm not going to",
    "start": "2899040",
    "end": "2905000"
  },
  {
    "text": "Advocate that we do this process interactively because I don't want to make the user wait for the evaluation my",
    "start": "2905000",
    "end": "2911280"
  },
  {
    "text": "my um idea in my mind is we'll store the quiz as it is fire off this validation",
    "start": "2911280",
    "end": "2917160"
  },
  {
    "text": "as as a background thread again using our good old software techniques that",
    "start": "2917160",
    "end": "2922319"
  },
  {
    "text": "we've been using for 50 40 years and then once the validation is done we can",
    "start": "2922319",
    "end": "2927640"
  },
  {
    "text": "attach the little confidence score to the",
    "start": "2927640",
    "end": "2931880"
  },
  {
    "text": "quiz okay how do we know if the validator works I'm going to let you take this yeah so one question is like okay like",
    "start": "2932760",
    "end": "2940920"
  },
  {
    "text": "we can use llm for the validator but how how do we know that it if it actually works and one idea we had was um we have",
    "start": "2940920",
    "end": "2947720"
  },
  {
    "text": "the open Trivia as as the questions um data right so why don't we",
    "start": "2947720",
    "end": "2953760"
  },
  {
    "text": "take that and ask llm to find the correct answer and see how many times it",
    "start": "2953760",
    "end": "2960599"
  },
  {
    "text": "actually does that so if we do that with with the right number of questions we should get a score so that's one idea",
    "start": "2960599",
    "end": "2967839"
  },
  {
    "text": "um and I actually yeah so basically take your quiz which is open Trivia um maybe",
    "start": "2967839",
    "end": "2973559"
  },
  {
    "text": "change it a little bit and I'll show you what what I mean by this then ask the llm given this question and these",
    "start": "2973559",
    "end": "2979599"
  },
  {
    "text": "answers which one is correct and then get the results and compare that with open Trivia and and see what you get so",
    "start": "2979599",
    "end": "2985640"
  },
  {
    "text": "I actually have this um small GitHub repo where I did this like open Trivia",
    "start": "2985640",
    "end": "2990799"
  },
  {
    "text": "llm testing and I I won't go through it in too much detail but I just want to show you um what we have",
    "start": "2990799",
    "end": "2998599"
  },
  {
    "text": "um so basically it's just a python script where you pass in your Google",
    "start": "2998599",
    "end": "3003839"
  },
  {
    "text": "Cloud project ID and your model and you want to do this in iterations because you want to llms they give you different",
    "start": "3003839",
    "end": "3012000"
  },
  {
    "text": "results right so you want to do multiple times with a multiple number of questions um and see what you get and",
    "start": "3012000",
    "end": "3019440"
  },
  {
    "text": "then I run this against Gemini 1.0 Pro 1.5 and 1.5 Flash and just to look at",
    "start": "3019440",
    "end": "3024960"
  },
  {
    "text": "the the python script just I won't go through the code in detail but just just to give you an idea what it does it",
    "start": "3024960",
    "end": "3031680"
  },
  {
    "text": "basically gets the questions from open Trivia then it filters this information",
    "start": "3031680",
    "end": "3037680"
  },
  {
    "text": "it doesn't need so it only gets the question the correct answer and the incorrect answers and then you don't",
    "start": "3037680",
    "end": "3043640"
  },
  {
    "text": "want to bias llm with the answer so you don't want to give the correct answer to the llm so what I do here is that I take",
    "start": "3043640",
    "end": "3050480"
  },
  {
    "text": "the correct answer and the incorrect answers and then combine them together into one field called answers",
    "start": "3050480",
    "end": "3057200"
  },
  {
    "text": "and then I ask llm now given these questions in Json with the question and the answers um can you tell me which",
    "start": "3057200",
    "end": "3065319"
  },
  {
    "text": "question is correct and then um I get the answer back um and then you",
    "start": "3065319",
    "end": "3073079"
  },
  {
    "text": "see I have this remove Json markers so in case I get M form Json in mark down I remove that um and then I do a",
    "start": "3073079",
    "end": "3079720"
  },
  {
    "text": "comparison basically and say okay this is what llm says this is this is what we have what's the percentage right um and",
    "start": "3079720",
    "end": "3087920"
  },
  {
    "text": "what we found out is",
    "start": "3087920",
    "end": "3091280"
  },
  {
    "text": "that what we found out is that Palm the model that we use initially uh it was",
    "start": "3093680",
    "end": "3098720"
  },
  {
    "text": "around 80% accuracy so it knew 80% of the time um if something was correct so",
    "start": "3098720",
    "end": "3105040"
  },
  {
    "text": "that wasn't that great uh now the Gemini Ultra the latest one we use now it's it's around 94% so it got better so it's",
    "start": "3105040",
    "end": "3112599"
  },
  {
    "text": "not 100% but it's not bad either so it gets most of the time so you can take a",
    "start": "3112599",
    "end": "3119400"
  },
  {
    "text": "look at my uh GitHub report to see what what kind of things you can do but the point is that you need to have something",
    "start": "3119400",
    "end": "3125520"
  },
  {
    "text": "like this where you can basically measure things in whatever way whatever you need to measure so that when you",
    "start": "3125520",
    "end": "3131960"
  },
  {
    "text": "change your proms you can kind of see if it made things better or worse that's that's the key Point here",
    "start": "3131960",
    "end": "3140240"
  },
  {
    "text": "and there's also something called grounding so grounding is um basically",
    "start": "3140240",
    "end": "3146839"
  },
  {
    "text": "trying to make your llms grounded into some data this can be either your own private data or it can be Google search",
    "start": "3146839",
    "end": "3154280"
  },
  {
    "text": "and actually it's very easy to use so you can all you need to do is you create a tool and you just say my tool is um",
    "start": "3154280",
    "end": "3160280"
  },
  {
    "text": "Google search retrieval and you pass that tool to your uh model and then the",
    "start": "3160280",
    "end": "3166119"
  },
  {
    "text": "model uses Google search to ground its answers so initially my idea was okay",
    "start": "3166119",
    "end": "3172720"
  },
  {
    "text": "since we have Google search maybe if I ground my answers to Google search I will get 100% accuracy because you know",
    "start": "3172720",
    "end": "3178160"
  },
  {
    "text": "it's it's going to use search but in our case it actually didn't help um and my guess is that it's because we are",
    "start": "3178160",
    "end": "3185400"
  },
  {
    "text": "already using open Trivia data which is already out there so maybe the model is already trained on it so if you have",
    "start": "3185400",
    "end": "3191559"
  },
  {
    "text": "Google search on top like it doesn't really help but in cases where you need real-time data like we the data from",
    "start": "3191559",
    "end": "3198359"
  },
  {
    "text": "today or or stock data from today or things like that where you you know that the data is not in models training data",
    "start": "3198359",
    "end": "3205680"
  },
  {
    "text": "then for those kind of things Google search actually helps quite a bit so you can actually get more answers and more accurate answers using grounding so it's",
    "start": "3205680",
    "end": "3212880"
  },
  {
    "text": "something to keep in mind as well and if you have any private data where you want to ground it with your private documents",
    "start": "3212880",
    "end": "3218799"
  },
  {
    "text": "again you can use that you can create a tool that uses um um vertex AI search to",
    "start": "3218799",
    "end": "3225040"
  },
  {
    "text": "search your own data and give you more accurate answers and then Mark also work on this a little bit in a different way so I'll",
    "start": "3225040",
    "end": "3231720"
  },
  {
    "text": "let you explain this one yeah so it's it's sort of easy to get a little conf confused here because there's a level of",
    "start": "3231720",
    "end": "3237720"
  },
  {
    "text": "indirection going on so what did meta just share it wasn't uh assessing the",
    "start": "3237720",
    "end": "3244440"
  },
  {
    "text": "quality of quizzes it was assessing the quality of an llm to judge a quiz and",
    "start": "3244440",
    "end": "3251280"
  },
  {
    "text": "what we found was uh Gemini Ultra is about 94% accurate if you give it a",
    "start": "3251280",
    "end": "3258440"
  },
  {
    "text": "random quiz and it tells you an answer for for it it judges its accuracy it's",
    "start": "3258440",
    "end": "3264880"
  },
  {
    "text": "about 94% correct so that gives us the first part of the puzzle which is that Gemini Ultra is a",
    "start": "3264880",
    "end": "3271960"
  },
  {
    "text": "very good pretty good I would say uh judger of quizzes but now we want to",
    "start": "3271960",
    "end": "3277240"
  },
  {
    "text": "take our generated content and run it through that assessor and so the way we",
    "start": "3277240",
    "end": "3283040"
  },
  {
    "text": "do that is we take a given quiz that we've generated so here's an example who was the first US president you can think",
    "start": "3283040",
    "end": "3289319"
  },
  {
    "text": "of any multiple choice question if you remember that uh game two truths and a lie any multiple choice question is",
    "start": "3289319",
    "end": "3296400"
  },
  {
    "text": "three lies and a truth okay so if you decompose the multiple choice into four",
    "start": "3296400",
    "end": "3302280"
  },
  {
    "text": "assertions only one will be true and the other three will be false so you take those four",
    "start": "3302280",
    "end": "3308520"
  },
  {
    "text": "assertions batch them because your quiz is going to have multiple questions and again you get the benefit of getting",
    "start": "3308520",
    "end": "3314720"
  },
  {
    "text": "everything done in one API call and then you can ask the assessor or the",
    "start": "3314720",
    "end": "3320200"
  },
  {
    "text": "validator that meta just described what do you think of this so in this case here you batchet together",
    "start": "3320200",
    "end": "3326760"
  },
  {
    "text": "these four assumptions in reality for a quiz a 10 question quiz you'd have 40 assertions and you'd probably shuffle",
    "start": "3326760",
    "end": "3332280"
  },
  {
    "text": "them to avoid locality bias and the llm if you prompt it correctly it'll say",
    "start": "3332280",
    "end": "3337880"
  },
  {
    "text": "false false true false it'll it'll uh give you its reading on these things and you can compare them to the known values",
    "start": "3337880",
    "end": "3345839"
  },
  {
    "text": "and decide if the uh quiz makes sense or not whether the quiz is accurate or not",
    "start": "3345839",
    "end": "3351680"
  },
  {
    "text": "and what we found out was that uh running this on Gemini Pro gave us only",
    "start": "3351680",
    "end": "3358400"
  },
  {
    "text": "a 70% uh sorry I need to be careful generating quizzes on Gemini Pro",
    "start": "3358400",
    "end": "3365839"
  },
  {
    "text": "and then validating them with the best known assessor the the generator Gemini",
    "start": "3365839",
    "end": "3371960"
  },
  {
    "text": "Pro was roughly 70% accurate in terms of its quizzes Which was disappointing and",
    "start": "3371960",
    "end": "3377760"
  },
  {
    "text": "meta found that the as a as an assessor as a validator it was it was 80% but the",
    "start": "3377760",
    "end": "3383200"
  },
  {
    "text": "actual quality of its quizzes was only 70% so it's two different quantities we're talking about the good news is",
    "start": "3383200",
    "end": "3389039"
  },
  {
    "text": "looking at this same problem with quizzes generated by Gemini Ultra has",
    "start": "3389039",
    "end": "3394079"
  },
  {
    "text": "jumped up to 91% now don't take these uh numbers too literally because they",
    "start": "3394079",
    "end": "3399640"
  },
  {
    "text": "change every day but the point is Gemini Ultra has proven to be a very good judge",
    "start": "3399640",
    "end": "3404839"
  },
  {
    "text": "of quiz quality and a very good pretty good generator of accurate quizzes so",
    "start": "3404839",
    "end": "3411280"
  },
  {
    "text": "what we want to do is operationalize all this into the dynamically test in of",
    "start": "3411280",
    "end": "3416599"
  },
  {
    "text": "quizzes as well as doing uh kind of unit or regression test type work where we",
    "start": "3416599",
    "end": "3422039"
  },
  {
    "text": "can after we have a new version of the model or new version of the app we can run a test suite and it'll use uh the",
    "start": "3422039",
    "end": "3428960"
  },
  {
    "text": "the preferred model to generate thousands of quizzes and give us a report and you know automate things in the way we always want to do for um well",
    "start": "3428960",
    "end": "3436480"
  },
  {
    "text": "well- defined software engineering processes so to summarize all this um we",
    "start": "3436480",
    "end": "3443079"
  },
  {
    "text": "went from this is a little bit of an exaggeration I'm kind of implying what what would have taken seven years took",
    "start": "3443079",
    "end": "3448599"
  },
  {
    "text": "me seven weeks or took us s weeks that's not really true but there is a kernel of Truth there because the reason it took",
    "start": "3448599",
    "end": "3454200"
  },
  {
    "text": "me seven years to realize this app was that it was too painful or maybe not",
    "start": "3454200",
    "end": "3459319"
  },
  {
    "text": "even possible to build it the way I was dreaming of it being and once I had",
    "start": "3459319",
    "end": "3465079"
  },
  {
    "text": "access to this large language model I didn't come up with the version you're looking at today but I came up",
    "start": "3465079",
    "end": "3470599"
  },
  {
    "text": "with a pretty respectable prototype in seven weeks I didn't know Dart and I had to pick up flutter on the Fly so that",
    "start": "3470599",
    "end": "3476680"
  },
  {
    "text": "all slowed me down a little bit but the point is I went from something I just didn't know how to solve to something",
    "start": "3476680",
    "end": "3483160"
  },
  {
    "text": "that I could solve in a satisfying way in a few weeks I think that's a lot to",
    "start": "3483160",
    "end": "3488839"
  },
  {
    "text": "do with what uh generative AP AI means to me it makes new things possible that",
    "start": "3488839",
    "end": "3495119"
  },
  {
    "text": "weren't possible or were were very difficult as we use this technology we",
    "start": "3495119",
    "end": "3500480"
  },
  {
    "text": "just have to keep our eyes open to the fact that it's not free it comes with a cost as a developer as a system some",
    "start": "3500480",
    "end": "3506240"
  },
  {
    "text": "integrator as a tester all those things we have to keep worrying about so thank you very much for your",
    "start": "3506240",
    "end": "3513520"
  },
  {
    "text": "attention we're just right up to the last minute we might have time for one question or yeah maybe yeah anyone have",
    "start": "3513520",
    "end": "3519400"
  },
  {
    "text": "any question for us yes",
    "start": "3519400",
    "end": "3525799"
  },
  {
    "text": "right seems like you're taking the user input text input free form and placing",
    "start": "3525799",
    "end": "3531400"
  },
  {
    "text": "it straight into the prompt you have any recommendations ORS",
    "start": "3531400",
    "end": "3538039"
  },
  {
    "text": "toate yes yes so the question was are we taking",
    "start": "3538760",
    "end": "3545359"
  },
  {
    "text": "the user input and putting it straight into the model not exactly I have a I'm using a profanity",
    "start": "3545359",
    "end": "3552240"
  },
  {
    "text": "filter uh which is minimal but you want to do that kind of thing and I haven't bullet hardened it you know to be",
    "start": "3552240",
    "end": "3559400"
  },
  {
    "text": "perfect but yes there's a whole school of of things you need to worry about and",
    "start": "3559400",
    "end": "3564440"
  },
  {
    "text": "that falls into that category of the Stu not free and by the way the profanity filters in that same category I said",
    "start": "3564440",
    "end": "3570520"
  },
  {
    "text": "earlier my first you can guess what my first way to do the profanity filter was and it was wrong it was let me ask the",
    "start": "3570520",
    "end": "3576520"
  },
  {
    "text": "llm and then I found lots of good easy to use libraries to do",
    "start": "3576520",
    "end": "3582359"
  },
  {
    "text": "that any last question was there another one",
    "start": "3582359",
    "end": "3588160"
  },
  {
    "text": "yeah sorry there's a lot of noise behind me can you speak",
    "start": "3591480",
    "end": "3597760"
  },
  {
    "text": "temper have we played with the temperature parameter yeah we play with it and we we try to make it zero",
    "start": "3599359",
    "end": "3606119"
  },
  {
    "text": "expecting that we'll get like perfect quizzes same quizzes every every single time it's not true you still get",
    "start": "3606119",
    "end": "3611640"
  },
  {
    "text": "different quizzes and then we try to move it so that we get more creative quizzes and they do get more creative",
    "start": "3611640",
    "end": "3616720"
  },
  {
    "text": "but then you get also inaccurate quizzes so yeah we play with it did it help no",
    "start": "3616720",
    "end": "3623400"
  },
  {
    "text": "yeah I mean I my take on that is we're we're kind of going with a very simple approach of keeping it at zero because",
    "start": "3623400",
    "end": "3629319"
  },
  {
    "text": "it's the most reproducible repeatable thing but ideally the end user should be able to specify the attent the the uh um",
    "start": "3629319",
    "end": "3637640"
  },
  {
    "text": "temperature value because you might not care if if it's not as accurate but you",
    "start": "3637640",
    "end": "3643280"
  },
  {
    "text": "get these kind of crazy or interesting questions so ultimately I'd like to pass that through to the end user and for our",
    "start": "3643280",
    "end": "3650920"
  },
  {
    "text": "testing we should be able to vary the temperature and run different tests and we should actually see like you know the",
    "start": "3650920",
    "end": "3657839"
  },
  {
    "text": "this is the curve accuracy curve we get as we tweak the the temperature so I",
    "start": "3657839",
    "end": "3664119"
  },
  {
    "text": "think we'll call yeah I was going to say that so we will be here outside the room if you have more questions so feel free",
    "start": "3664119",
    "end": "3670039"
  },
  {
    "text": "to stop by and we'll be happy to answer thank you thank you",
    "start": "3670039",
    "end": "3676680"
  }
]