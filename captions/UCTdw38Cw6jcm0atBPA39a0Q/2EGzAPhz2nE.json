[
  {
    "text": "hello good afternoon keep keep coming in um so",
    "start": "6520",
    "end": "12639"
  },
  {
    "text": "welcome uh thank you for coming along to this session my name is Steve i'm a Microsoft MVP Plurite author and",
    "start": "12639",
    "end": "18880"
  },
  {
    "text": "engineer at Elastic uh you can find me online i'm Stevej Gordon on most social platforms uh I blog at",
    "start": "18880",
    "end": "25640"
  },
  {
    "text": "stevejordon.co.uk some of the stuff that I'll be going through today is kind of built off of longer form blog posts so",
    "start": "25640",
    "end": "31279"
  },
  {
    "text": "if you want to dive a bit deeper uh check out those articles this bit.ly link here um if you want access to the",
    "start": "31279",
    "end": "38000"
  },
  {
    "text": "slides the code and things after the session uh just grab that link or the QR code there and you'll be able to kind of",
    "start": "38000",
    "end": "44320"
  },
  {
    "text": "review those i'll share that link again at the end so today we're going to be",
    "start": "44320",
    "end": "49360"
  },
  {
    "text": "talking about high performance C code um and some of the I call them new they're",
    "start": "49360",
    "end": "54640"
  },
  {
    "text": "not new anymore but newer features uh available to us in the language and the base class libraries that allow us to do",
    "start": "54640",
    "end": "61199"
  },
  {
    "text": "high performance code so just to set the agenda and make sure we're all on the same page uh what we'll be covering is",
    "start": "61199",
    "end": "68240"
  },
  {
    "text": "we're going to begin with just a quick discussion around what is performance and what is it we're actually aiming to uh improve in our code thank you um",
    "start": "68240",
    "end": "76640"
  },
  {
    "text": "we'll then spend a little bit of time talking about how to actually measure the performance of code so that we know",
    "start": "76640",
    "end": "81920"
  },
  {
    "text": "where we're starting from and where we're actually moving towards as we do some optimizations on some code we'll",
    "start": "81920",
    "end": "87600"
  },
  {
    "text": "then spend a bit of time with these types uh so span of T read only span and memory of T we'll talk a bit about array",
    "start": "87600",
    "end": "94880"
  },
  {
    "text": "pools we'll talk a little bit about uh pipelines system IO pipelines and a type called read only sequence and hopefully",
    "start": "94880",
    "end": "102240"
  },
  {
    "text": "there'll be time for me to race through some stuff on system text JSON at the end and just show some demos of using",
    "start": "102240",
    "end": "107360"
  },
  {
    "text": "high performance stuff there so let's begin really with the aspects of performance so there's three main sort",
    "start": "107360",
    "end": "113360"
  },
  {
    "text": "of categories or criteria of uh performance that I tend to think about when I'm optimizing code the first is",
    "start": "113360",
    "end": "119600"
  },
  {
    "text": "execution time so how long does a unit of code take to run um that might be you",
    "start": "119600",
    "end": "125840"
  },
  {
    "text": "know the entire operation of handling a request for example or it might be that you're looking at smaller units of code",
    "start": "125840",
    "end": "131360"
  },
  {
    "text": "individual methods or even lines of code and how long those take to execute typically if we can make our code",
    "start": "131360",
    "end": "137040"
  },
  {
    "text": "execute more quickly then we can give a better response to the user the customer that's visiting the site or visiting",
    "start": "137040",
    "end": "143120"
  },
  {
    "text": "using the application because we're able to return results or information to them more quickly",
    "start": "143120",
    "end": "148800"
  },
  {
    "text": "throughput is a a sort of related measure um but it's a better way of thinking about this in terms of what a",
    "start": "148800",
    "end": "154800"
  },
  {
    "text": "system or a service can actually achieve so throughput is more a measure of what we can actually do with a given amount",
    "start": "154800",
    "end": "160239"
  },
  {
    "text": "of resource on a machine so requests per second in aspet core or maybe messages processed off a cube per second per",
    "start": "160239",
    "end": "166160"
  },
  {
    "text": "minute however we're measuring that and this gives us a nice way of measuring sort of the actual performance of an",
    "start": "166160",
    "end": "172879"
  },
  {
    "text": "application in production because it's an easy measure that we can take using metrics based tools to collect this",
    "start": "172879",
    "end": "178480"
  },
  {
    "text": "information to see how our applications are performing these two are quite closely linked uh typically if we can",
    "start": "178480",
    "end": "184319"
  },
  {
    "text": "reduce the execution time of our code then we might see that we're able to increase the throughput of the system as well it's not always an exact",
    "start": "184319",
    "end": "190480"
  },
  {
    "text": "correlation different things can affect throughput there's different sort of bottlenecks that you might encounter on your system around IO and and memory",
    "start": "190480",
    "end": "197840"
  },
  {
    "text": "that may sort of play a part in that measure as well um but typically if we get the execution time we see",
    "start": "197840",
    "end": "203440"
  },
  {
    "text": "improvements in throughput and then the final one that not everyone tends to think about is around memory allocations",
    "start": "203440",
    "end": "209760"
  },
  {
    "text": "so allocations inn net are extremely cheap because we already have the heap memory sort of ready to go and so when",
    "start": "209760",
    "end": "215920"
  },
  {
    "text": "we create a new object it's usually just bumping a pointer um logically within that sort of heap memory the cost for",
    "start": "215920",
    "end": "222480"
  },
  {
    "text": "memory allocations comes later at some point in time those objects go out of scope and that memory can be reclaimed",
    "start": "222480",
    "end": "228319"
  },
  {
    "text": "so at some point the garbage collection process is going to need to kick in and clean up those unused objects and that",
    "start": "228319",
    "end": "235360"
  },
  {
    "text": "tends to have a small but sometimes significant uh performance impact in high throughput systems and so if we can",
    "start": "235360",
    "end": "242319"
  },
  {
    "text": "avoid allocating objects on the heap and allocating memory at all then we avoid",
    "start": "242319",
    "end": "247439"
  },
  {
    "text": "that potential penalty later on and a lot of the stuff you see if you read any of the posts by like Steven Ta from",
    "start": "247439",
    "end": "253200"
  },
  {
    "text": "Microsoft when they talk about like the high performance features that they've introduced they'll often be",
    "start": "253200",
    "end": "258239"
  },
  {
    "text": "demonstrating how they've reduced allocations as one of the ways that they've made performance overall uh",
    "start": "258239",
    "end": "265080"
  },
  {
    "text": "better now I do like to caution that performance is contextual there's various different ways of saying this",
    "start": "265080",
    "end": "272080"
  },
  {
    "text": "but I just sort of summarize this in this this form because a lot of what I'm going to show you today isn't necessarily stuff that you need to race",
    "start": "272080",
    "end": "278400"
  },
  {
    "text": "to your office on Monday and start implementing but in certain situations",
    "start": "278400",
    "end": "283440"
  },
  {
    "text": "where you've got high throughput systems or you're already encountering certain bottlenecks in your system then you",
    "start": "283440",
    "end": "289199"
  },
  {
    "text": "might want to start looking at optimizing some of the code to reduce those those problems that you're facing but maybe for 95% of applications some",
    "start": "289199",
    "end": "296880"
  },
  {
    "text": "of the stuff I'm going to show you today might not be necessary although what I do hope to demonstrate is that some of",
    "start": "296880",
    "end": "302160"
  },
  {
    "text": "the techniques to for example reduce allocations don't necessarily require a lot of deep advanced code uh to to write",
    "start": "302160",
    "end": "309199"
  },
  {
    "text": "them sometimes we can actually achieve those gains with relatively simple changes that said there is a bit of a",
    "start": "309199",
    "end": "316400"
  },
  {
    "text": "trade-off when it comes to high performance code typically the high performance code tends to get a little",
    "start": "316400",
    "end": "322080"
  },
  {
    "text": "bit more verbose it's a little less easy maybe to read or reason about because it's written in a certain way that we",
    "start": "322080",
    "end": "328400"
  },
  {
    "text": "know optimizes for certain scenarios and that can mean that it's harder to maintain the code that we've moved to",
    "start": "328400",
    "end": "334479"
  },
  {
    "text": "this higher performance uh environment and so you do have to have this trade-off like if you're working on a",
    "start": "334479",
    "end": "340080"
  },
  {
    "text": "system that's regularly changing it has lots of developers touching that code on a regular basis then you might want to",
    "start": "340080",
    "end": "346160"
  },
  {
    "text": "move more towards readability and maintainability as your priority because actually getting out features shipping",
    "start": "346160",
    "end": "351680"
  },
  {
    "text": "code uh is more important than sort of milliseconds of execution time or small amounts of memory gains that you might",
    "start": "351680",
    "end": "357759"
  },
  {
    "text": "make but there will always be a certain set of applications where if you can reduce the memory footprint for example",
    "start": "357759",
    "end": "363280"
  },
  {
    "text": "of the application you might be able to reduce costs in terms of how you have to scale that application in production",
    "start": "363280",
    "end": "369199"
  },
  {
    "text": "environments so you need to weigh this up in your organization service by service to see where you should be sort",
    "start": "369199",
    "end": "375520"
  },
  {
    "text": "of treading across this line are you moving more to the high performance or are you needing to just keep that code",
    "start": "375520",
    "end": "381120"
  },
  {
    "text": "easy to work on and and easy to read",
    "start": "381120",
    "end": "386360"
  },
  {
    "text": "um so when we go around optimizing code I'm going to be sort of demonstrating what I refer to as the optimization",
    "start": "386800",
    "end": "392639"
  },
  {
    "text": "cycle today and the most important first step in optimizing code is actually making measurements first it's very",
    "start": "392639",
    "end": "399919"
  },
  {
    "text": "dangerous when you're doing optimizations to make assumptions about how code might operate if we switch to a",
    "start": "399919",
    "end": "405039"
  },
  {
    "text": "higher performance uh code that we've already used maybe elsewhere it doesn't always pay off in the way we expect so",
    "start": "405039",
    "end": "411440"
  },
  {
    "text": "we start by measuring our code to get an understanding of what the system is currently doing and and the very first",
    "start": "411440",
    "end": "416560"
  },
  {
    "text": "set of measurements should be ideally production measurements if you can so how is your system currently function if",
    "start": "416560",
    "end": "422080"
  },
  {
    "text": "it's already deployed what is your throughput of a web server for example or how many messages can your processing",
    "start": "422080",
    "end": "428000"
  },
  {
    "text": "service manage off a queue in a certain given amount of time uh you want to make sure that any changes you're making aren't negatively affecting those",
    "start": "428000",
    "end": "434560"
  },
  {
    "text": "production metrics then when we start actually looking at where we should be spending our time in our code we want to",
    "start": "434560",
    "end": "440560"
  },
  {
    "text": "start with sort of higher level tools that allow us to profile CPU and memory to start to give us a sort of guidance",
    "start": "440560",
    "end": "446720"
  },
  {
    "text": "of where we want to start optimizing code so we'll talk about a few of the tools in a moment but the basic idea is",
    "start": "446720",
    "end": "452560"
  },
  {
    "text": "we want to know what are the hot paths in the application so that we can go and focus on where we could approve that",
    "start": "452560",
    "end": "458000"
  },
  {
    "text": "code base on those hot pass because that's going to give us the most gain if it's code that's executed most often and",
    "start": "458000",
    "end": "463599"
  },
  {
    "text": "we can use memory profilers for example to then help us narrow down within that hot path where are the potential gains",
    "start": "463599",
    "end": "469360"
  },
  {
    "text": "for maybe reducing allocations coming from and then we go down to even lower level measurements and we'll have a look",
    "start": "469360",
    "end": "475520"
  },
  {
    "text": "at an example of benchmarking with a tool called benchmark.net as we move through the session and this is what",
    "start": "475520",
    "end": "480720"
  },
  {
    "text": "allows us to get very high precision measurements for small units of code as we go about optimizing it so we measure",
    "start": "480720",
    "end": "488000"
  },
  {
    "text": "first we get our sort of baselines and then we can begin to optimize the most important thing about this step is that",
    "start": "488000",
    "end": "493520"
  },
  {
    "text": "we don't just make all of the possible changes that we can think of in one go i prefer to do this in small iterations",
    "start": "493520",
    "end": "500080"
  },
  {
    "text": "small changes after we optimize maybe a few lines of code with a high performance technique maybe like one of",
    "start": "500080",
    "end": "506160"
  },
  {
    "text": "the ones I'll show you today we want to measure again to validate that that change uh is actually represented in the",
    "start": "506160",
    "end": "512399"
  },
  {
    "text": "data that we're seeing if you change a whole bunch of things together you might improve the overall performance of your",
    "start": "512399",
    "end": "517839"
  },
  {
    "text": "code but you don't know if all of those changes were actually positive and so by doing things in small increments you can",
    "start": "517839",
    "end": "523120"
  },
  {
    "text": "get very scientific about validating each theory that you have and each change that you're making is pushing you",
    "start": "523120",
    "end": "528800"
  },
  {
    "text": "towards whatever goal it is you're working toward and then this cycle just continues there's no real specific end",
    "start": "528800",
    "end": "535360"
  },
  {
    "text": "point for this that you have to determine within your organization within your team what is the point to stop optimizing sometimes you'll have",
    "start": "535360",
    "end": "542240"
  },
  {
    "text": "reached your given requirement you may have reduced your memory footprint by half and you're happy and that's fine",
    "start": "542240",
    "end": "547600"
  },
  {
    "text": "that's a good place to stop if that was your objective sometimes you might just hit the ceiling of what you think you",
    "start": "547600",
    "end": "552640"
  },
  {
    "text": "can reasonably achieve you've gone through all of the techniques that you've read about or seen today and",
    "start": "552640",
    "end": "557680"
  },
  {
    "text": "actually you don't think there's any more real gains you're talking sort of nanoseconds or maybe a few bites here and there but there's not a real gain to",
    "start": "557680",
    "end": "564240"
  },
  {
    "text": "be had so you don't want to continue optimizing too far past that sort of optimum point of time spent versus the",
    "start": "564240",
    "end": "570640"
  },
  {
    "text": "gains that you're making so how do we measure high performance code how do we sort of start",
    "start": "570640",
    "end": "576480"
  },
  {
    "text": "to analyze some of this stuff so the first tool I like to highlight just because it's often overlooked is if",
    "start": "576480",
    "end": "582160"
  },
  {
    "text": "you're using Visual Studio and I do um then the diagnostic tools that pop up",
    "start": "582160",
    "end": "587200"
  },
  {
    "text": "when you start debugging an application um can be quite useful they can give you a bit of a representative view of how",
    "start": "587200",
    "end": "593839"
  },
  {
    "text": "the memory profile of that application is looking you can see where the GCs are occurring you can take snapshots of that",
    "start": "593839",
    "end": "599440"
  },
  {
    "text": "memory if you want to uh understand a little bit more about it and so just by sort of debugging our application we",
    "start": "599440",
    "end": "605200"
  },
  {
    "text": "have access to some rich data the caution here is that you are in a debug mode you're not running this against",
    "start": "605200",
    "end": "610560"
  },
  {
    "text": "highly optimized uh code so there might be some differences to what you would measure in a production application so",
    "start": "610560",
    "end": "616640"
  },
  {
    "text": "it's a good indicator but it isn't necessarily a final measure for that what you want to do is start going a",
    "start": "616640",
    "end": "622079"
  },
  {
    "text": "little bit deeper on the um the build release build code and actually use tools like the Visual Studio profiling",
    "start": "622079",
    "end": "629279"
  },
  {
    "text": "suite um or Perfw or some of the Jet Brains products to actually narrow down further visual Studio profiling tools",
    "start": "629279",
    "end": "635920"
  },
  {
    "text": "are actually pretty rich today we have a lot of options in there we can look at all of the memory allocations um",
    "start": "635920",
    "end": "640959"
  },
  {
    "text": "tracking those across time to see what's being allocated we can profile the CPU and see where the method times are being",
    "start": "640959",
    "end": "647200"
  },
  {
    "text": "spent um and we can also dig into things like the async and the um the task parallel library um parts as well so you",
    "start": "647200",
    "end": "654720"
  },
  {
    "text": "can actually get quite rich information just from Visual Studio the other end of the scale I",
    "start": "654720",
    "end": "661600"
  },
  {
    "text": "would say is perfw this is a very rich tool it's a very advanced tool um I",
    "start": "661600",
    "end": "666880"
  },
  {
    "text": "don't get along with it particularly well i've dipped in and out of it but it is quite difficult to get to grips with and so I don't recommend it as a kind of",
    "start": "666880",
    "end": "673519"
  },
  {
    "text": "the first thing you go to unless you really feel you need to or you've had some familiarity with it in the past",
    "start": "673519",
    "end": "678800"
  },
  {
    "text": "where I tend to push most of my time uh for performance stuff is into the Jet Brains tools i'm not paid to say that i",
    "start": "678800",
    "end": "684560"
  },
  {
    "text": "don't work for them i just prefer them um tracememory are both very easy tools",
    "start": "684560",
    "end": "690079"
  },
  {
    "text": "to get started with in terms of taking CPU profiles of an application as well as understanding where the memor is",
    "start": "690079",
    "end": "695920"
  },
  {
    "text": "being allocated and and diving into that information so pick the tool that you like um sometimes not always it can be",
    "start": "695920",
    "end": "703680"
  },
  {
    "text": "useful to go quite quite deep and this is where maybe you might want to start even looking at the intermediate",
    "start": "703680",
    "end": "708959"
  },
  {
    "text": "language code that gets generated when you compile your C application um sometimes this can give you a bit of an",
    "start": "708959",
    "end": "715360"
  },
  {
    "text": "indicator in terms of the C# you're writing how many ILIL intermediate language instructions is that actually",
    "start": "715360",
    "end": "720560"
  },
  {
    "text": "compiling down to and typically if you can make your code smaller and have less instructions then it's going to execute",
    "start": "720560",
    "end": "726880"
  },
  {
    "text": "more quickly other things we can sometimes spot in IL based tools is we can see where boxing instructions for",
    "start": "726880",
    "end": "732720"
  },
  {
    "text": "example are being um occurring so we can maybe identify cases where we're we're boxing value types as well so not always",
    "start": "732720",
    "end": "740240"
  },
  {
    "text": "needed but there are various free tools out there that you can use just to go and inspect that I code and don't",
    "start": "740240",
    "end": "746399"
  },
  {
    "text": "overlook the importance of having production metrics and monitoring in place before you start doing this as well again testing this stuff locally",
    "start": "746399",
    "end": "753760"
  },
  {
    "text": "even if you you try your hardest you might not always get a representative example of how an application performs",
    "start": "753760",
    "end": "758959"
  },
  {
    "text": "in a real production environment because our developer machines are a bit different there's no real contention going on um the loads that we're putting",
    "start": "758959",
    "end": "766079"
  },
  {
    "text": "through them are typically not necessarily going to be the same as we would see in production and the the patterns of usage might differ as well",
    "start": "766079",
    "end": "772560"
  },
  {
    "text": "so having monitoring in production for existing applications before you start this journey is pretty useful um just",
    "start": "772560",
    "end": "779120"
  },
  {
    "text": "basic things like maybe measures of memory CPU uh usage u throughput of an",
    "start": "779120",
    "end": "784560"
  },
  {
    "text": "application if it's an aspet core type app uh can be really useful because you can see if you're positively or",
    "start": "784560",
    "end": "790000"
  },
  {
    "text": "negatively affecting them in the direction that you hope you are",
    "start": "790000",
    "end": "795200"
  },
  {
    "text": "so the tool that we'll spend a little bit of time with today just because less people are familiar with it is benchmark.net so this is a library for",
    "start": "795200",
    "end": "802800"
  },
  {
    "text": "typically microbenchmarking our code so this is where we'll want to measure small chunks of code maybe individual",
    "start": "802800",
    "end": "808880"
  },
  {
    "text": "methods maybe even individual lines of code and compare them to how they sort of working after we've optimized them",
    "start": "808880",
    "end": "816560"
  },
  {
    "text": "it's a highly precise tool so it does a lot of things to give us accurate measurements technically you could just",
    "start": "816560",
    "end": "821839"
  },
  {
    "text": "use like a stopwatch or a timer around some code and that would give you a rough measure but what benchmark.net is",
    "start": "821839",
    "end": "827200"
  },
  {
    "text": "doing is it's going to run many different stages as it goes through benchmarking it does a warm-up phase to",
    "start": "827200",
    "end": "832639"
  },
  {
    "text": "make sure that the code you're testing is pre-jitted uh so the just in time compiler has run through its vari",
    "start": "832639",
    "end": "839040"
  },
  {
    "text": "compilation levels and your code is the most optimal that it would be in in a real scenario it runs its own overhead",
    "start": "839040",
    "end": "845279"
  },
  {
    "text": "measurements so it checks how its overhead of actually inspecting and monitoring uh your application while",
    "start": "845279",
    "end": "850720"
  },
  {
    "text": "it's running might take into effect and it can remove that uh overhead and then it will run each of your uh benchmark",
    "start": "850720",
    "end": "857040"
  },
  {
    "text": "methods many tens hundreds of thousands of times to make sure it's giving you good statistical averages for the data",
    "start": "857040",
    "end": "863519"
  },
  {
    "text": "that you're measuring so when we start getting down to those kind of nancond tiny measurements of time we need that",
    "start": "863519",
    "end": "869360"
  },
  {
    "text": "highly precise um uh measurement process to make sure that what we're getting is accurate",
    "start": "869360",
    "end": "876240"
  },
  {
    "text": "there's very other various other pieces of data that we can collect in the form of diagnosis uh so we'll see one in",
    "start": "876240",
    "end": "882000"
  },
  {
    "text": "action shortly but these are things like uh like GC and memory um so we can",
    "start": "882000",
    "end": "889440"
  },
  {
    "text": "change the GC mode that we're running our benchmarks under we can look at different JIT uh modes and things um and",
    "start": "889440",
    "end": "896240"
  },
  {
    "text": "this can be useful sometimes we can then replicate more closely what would be a production system when we're doing our",
    "start": "896240",
    "end": "903079"
  },
  {
    "text": "benchmarking um we can compare performance on all these the different platforms as well so you might be",
    "start": "903079",
    "end": "908160"
  },
  {
    "text": "developing on a Windows machine but you might be running your benchmarks um against Linux mode as well and you can",
    "start": "908160",
    "end": "913519"
  },
  {
    "text": "run run these then in CI and compare how those things run this tool is the tool that's very",
    "start": "913519",
    "end": "919440"
  },
  {
    "text": "extensively used by the Microsoft teams uh you'll see it in all of Steven Ta's kind of annual performance blog posts um",
    "start": "919440",
    "end": "926399"
  },
  {
    "text": "and uh it's been used heavily through the .NET runtime codebase as well as ASP.NET core as the teams have been",
    "start": "926399",
    "end": "932079"
  },
  {
    "text": "optimizing those so this is kind of the hello world of benchmarking so up the top here we",
    "start": "932079",
    "end": "938399"
  },
  {
    "text": "start with a a main method and that's just invoking this benchmark runner.run method and in this case what we're doing",
    "start": "938399",
    "end": "944480"
  },
  {
    "text": "is passing in a type that contains some benchmarks now there's various different ways that you can configure benchmark.net to run um you can have it",
    "start": "944480",
    "end": "951360"
  },
  {
    "text": "in kind of an interactive mode where it discover all of your benchmarks in a system and let you choose what you run",
    "start": "951360",
    "end": "956720"
  },
  {
    "text": "you can have it in a more CLI mode where you if you're doing it sort of CI environments and you want to run",
    "start": "956720",
    "end": "961759"
  },
  {
    "text": "specific benchmarks um you can do that by passing in the name of those as arguments but in this mode we're just",
    "start": "961759",
    "end": "967680"
  },
  {
    "text": "going to run everything that's in that class below the name paraser benchmarks and what I've added here is that memory",
    "start": "967680",
    "end": "973199"
  },
  {
    "text": "diagnoser attribute which allows us to say that on in addition to execution time measurements we want to do a run",
    "start": "973199",
    "end": "979360"
  },
  {
    "text": "that measures the uh allocation overhead of this code as well i have a little bit",
    "start": "979360",
    "end": "984639"
  },
  {
    "text": "of setup code here so these are set up as constant or static pieces of data they're not things that I'm actually",
    "start": "984639",
    "end": "989920"
  },
  {
    "text": "measuring as part of my benchmark but they're things I need to do the benchmark so I need some input data and",
    "start": "989920",
    "end": "995519"
  },
  {
    "text": "I need an instance of a name paraser and then my actual benchmark is below so it's just a regular method with the",
    "start": "995519",
    "end": "1001360"
  },
  {
    "text": "benchmark attribute applied to it and in this case we're calling the get last name method of that paraser passing in a",
    "start": "1001360",
    "end": "1007920"
  },
  {
    "text": "full name and we're going to then see what the um execution time and memory overhead of that particular method is",
    "start": "1007920",
    "end": "1014800"
  },
  {
    "text": "we're going to run this it's going to take some time to run you want to do it on a release build you also ideally want",
    "start": "1014800",
    "end": "1019839"
  },
  {
    "text": "to do it on a system that's not running other stuff so in an ideal world you disable every other application even",
    "start": "1019839",
    "end": "1025438"
  },
  {
    "text": "maybe antivirus because all of those things could kick in and affect nancond level measurements um but generally uh",
    "start": "1025439",
    "end": "1032880"
  },
  {
    "text": "as long as you've turned off most of your other apps that could consume memory or maybe jump in and affect uh",
    "start": "1032880",
    "end": "1038079"
  },
  {
    "text": "CPUuling and things uh you'll get fairly accurate results and after a few minutes",
    "start": "1038079",
    "end": "1043199"
  },
  {
    "text": "because of all those warm-up phases all of those many executions that it will do we end up with this basic summary",
    "start": "1043199",
    "end": "1049120"
  },
  {
    "text": "results and there's a much richer set of results that you can pull from uh the output folder but in this case what it's",
    "start": "1049120",
    "end": "1055039"
  },
  {
    "text": "giving is a quick summary of that particular method takes 116 nanconds to",
    "start": "1055039",
    "end": "1060240"
  },
  {
    "text": "run and we don't know if that's good bad it's pretty fast but could we make it quicker do we need to make it quicker",
    "start": "1060240",
    "end": "1066320"
  },
  {
    "text": "that's a decision for us to decide later on because we added the memory diagnos we have a few extra columns on the end",
    "start": "1066320",
    "end": "1072640"
  },
  {
    "text": "so in this case we have a single column for gen zero which is a rough approximation as best it can kind of",
    "start": "1072640",
    "end": "1078080"
  },
  {
    "text": "estimate of how many uh per 1000 operations um how many gen zero",
    "start": "1078080",
    "end": "1083120"
  },
  {
    "text": "collections would be incurred so in this case we're saying that you'd have to run that method roughly 29,000 times to have",
    "start": "1083120",
    "end": "1089280"
  },
  {
    "text": "induced enough pressure of gen zero objects for that to then need freeing um",
    "start": "1089280",
    "end": "1094400"
  },
  {
    "text": "it's not an exact measure because the GC uses a bunch of huristics and sort of um",
    "start": "1094400",
    "end": "1099520"
  },
  {
    "text": "uh adaptive technologies behind the scenes to try and make it as efficient as possible um but it gives you a bit of",
    "start": "1099520",
    "end": "1105440"
  },
  {
    "text": "an indication and the fact that we're not seeing a gen one or gen two column here shows us that none of our objects are longived or particularly large",
    "start": "1105440",
    "end": "1112360"
  },
  {
    "text": "necessarily we can see how many bytes was actually allocated um as a result of calling that method again we don't know",
    "start": "1112360",
    "end": "1118400"
  },
  {
    "text": "if this is good or bad but we do know that we've got 144 bytes allocated by this particular method so this would be",
    "start": "1118400",
    "end": "1124160"
  },
  {
    "text": "a good starting point for us so now that we've had a brief introduction to sort of uh benchmark",
    "start": "1124160",
    "end": "1131280"
  },
  {
    "text": "code let's look at some high performance code and then I'll hopefully show you some uh demonstrations of where I've",
    "start": "1131280",
    "end": "1136480"
  },
  {
    "text": "applied that in the past and how I've made those gains uh pay off for me so the first type I I want to spend",
    "start": "1136480",
    "end": "1142880"
  },
  {
    "text": "probably most time on is span of tea i'm always curious how many people in the room would say that they've heard of this span of tea concept already by show",
    "start": "1142880",
    "end": "1150160"
  },
  {
    "text": "of hands maybe halfish how many people have used span of te in their own codebase so far it's not bad but far",
    "start": "1150160",
    "end": "1157360"
  },
  {
    "text": "fewer and that's I've been doing this talk on and off for probably four or five years now um and it's pretty much",
    "start": "1157360",
    "end": "1163440"
  },
  {
    "text": "always the same proportion a lot of people have heard of spanity or a reasonable number because it's always",
    "start": "1163440",
    "end": "1168559"
  },
  {
    "text": "mentioned in things like those annual blog posts by Steven Ta is always mentioned quite heavily in any post that",
    "start": "1168559",
    "end": "1174559"
  },
  {
    "text": "discusses performance in relation to .NET and the runtime baseclass libraries um because it was quite a a",
    "start": "1174559",
    "end": "1180760"
  },
  {
    "text": "revolutionary addition to .NET that has led to many improvements to the runtime that we've seen in since net sort of",
    "start": "1180760",
    "end": "1187440"
  },
  {
    "text": "pre1 sort of time frame um but I also find that not many people have actually",
    "start": "1187440",
    "end": "1192480"
  },
  {
    "text": "used it in code partly because they may not need to but also partly because I think Microsoft tended to caution away",
    "start": "1192480",
    "end": "1198640"
  },
  {
    "text": "from using it um and I've always been a little bit less on the caution side it's not hugely complex to get started with",
    "start": "1198640",
    "end": "1205440"
  },
  {
    "text": "span of t and you can apply it quite easily in a code bases to reduce allocations as we'll see so this was um",
    "start": "1205440",
    "end": "1212720"
  },
  {
    "text": "built in net core 21 uh and since then so we've we've got it on all latest versions of net sort of runtime um it's",
    "start": "1212720",
    "end": "1220720"
  },
  {
    "text": "also available as a system memory package so you can actually use this with net framework applications and net standard libraries um the there is a",
    "start": "1220720",
    "end": "1228159"
  },
  {
    "text": "subtle difference in the level of performance so in .NET core what they did is actually make some runtime changes to optimize for this type um in",
    "start": "1228159",
    "end": "1235679"
  },
  {
    "text": "the best way possible we don't tend to see runtime changes in .NET framework partly because it's mostly maintenance",
    "start": "1235679",
    "end": "1241200"
  },
  {
    "text": "mode and partly because of fears around how that ships but with net core we have a much easier system of upgrading and",
    "start": "1241200",
    "end": "1247440"
  },
  {
    "text": "running sidebyside versions and so they felt safe in actually optimizing this with actual runtime",
    "start": "1247440",
    "end": "1255039"
  },
  {
    "text": "changes at its heart what it does is provide a readr view over some contiguous region of memory the nice",
    "start": "1255559",
    "end": "1262159"
  },
  {
    "text": "thing about span of t is it's kind of agnostic to what that memory is so we can actually use this over traditional",
    "start": "1262159",
    "end": "1268799"
  },
  {
    "text": "heap um allocated memory so things like strings or arrays which are contiguous",
    "start": "1268799",
    "end": "1273840"
  },
  {
    "text": "blocks of memory Mor off the heap but we can also use it to reference memory that's allocated directly on the stack",
    "start": "1273840",
    "end": "1279840"
  },
  {
    "text": "um and even native and unmanaged memory if you're working in those types of environments but we work with this one",
    "start": "1279840",
    "end": "1285760"
  },
  {
    "text": "type that's both type safe and memory safe in all of those scenarios uh so one of the key things it",
    "start": "1285760",
    "end": "1292640"
  },
  {
    "text": "offers to us is this ability to avoid allocations and data copying which can be quite significant in reducing uh the",
    "start": "1292640",
    "end": "1300080"
  },
  {
    "text": "overhead of application code so we can work with it uh in a very similar way to traditional array based",
    "start": "1300080",
    "end": "1306400"
  },
  {
    "text": "data structures we can index into it we can iterate through that data um there's",
    "start": "1306400",
    "end": "1311520"
  },
  {
    "text": "pretty much almost no overhead versus using a native array um and all of the operations on here have been highly",
    "start": "1311520",
    "end": "1316880"
  },
  {
    "text": "optimized to be as efficient as possible which is why it's used throughout the runtime",
    "start": "1316880",
    "end": "1322120"
  },
  {
    "text": "libraries one of the key operations on span that's um the most valuable really is this idea of slicing data or slicing",
    "start": "1322120",
    "end": "1329120"
  },
  {
    "text": "memory so here what we do is we start by creating a new integer array of nine elements and then we call as span on",
    "start": "1329120",
    "end": "1335520"
  },
  {
    "text": "there to get a span of t a span of um int in this case over that data and so",
    "start": "1335520",
    "end": "1341120"
  },
  {
    "text": "these two local variables here would point to the same portion of memory the first thing you worry might worry about",
    "start": "1341120",
    "end": "1347039"
  },
  {
    "text": "is this creation of this span of int that we're creating as a result of that new um as span call the thing about span",
    "start": "1347039",
    "end": "1354080"
  },
  {
    "text": "is it isn't ever allocated on the heap so there's never any uh GC overhead from creating an instance of a span it's a",
    "start": "1354080",
    "end": "1360799"
  },
  {
    "text": "specialized type that only ever appears on the stack and we'll see later why that introduces some limitations but it",
    "start": "1360799",
    "end": "1366480"
  },
  {
    "text": "also offers this guarantee that actually it's a very cheap thing to do there's never any heap cost that we should be worrying about now once we have that",
    "start": "1366480",
    "end": "1373360"
  },
  {
    "text": "span based view over the data any point in time we can call slice on there to give us a different view over that same",
    "start": "1373360",
    "end": "1379679"
  },
  {
    "text": "block of memory so in this case we're slicing into a starting position of two and a length of five and so this new",
    "start": "1379679",
    "end": "1386400"
  },
  {
    "text": "span two here references a different portion of the existing memory we haven't done any data copying we've just",
    "start": "1386400",
    "end": "1392799"
  },
  {
    "text": "changed our view and this is extremely powerful if you're processing bytes that maybe are streaming off the wire or",
    "start": "1392799",
    "end": "1398400"
  },
  {
    "text": "you're processing through large uh strings which are just you know ultimately just characters um in a",
    "start": "1398400",
    "end": "1404080"
  },
  {
    "text": "system because you can just change your view over that data very quickly the nice thing about slicing is it's a",
    "start": "1404080",
    "end": "1410159"
  },
  {
    "text": "constant time constant cost operation as well so it doesn't matter if this array is nine elements or 9 million the cost",
    "start": "1410159",
    "end": "1416080"
  },
  {
    "text": "of slicing into it for any length is uh pretty much exactly the same each time",
    "start": "1416080",
    "end": "1421520"
  },
  {
    "text": "um and that makes it very powerful because we're never copying anything we're basically just holding a pointer within that memory portion um and then",
    "start": "1421520",
    "end": "1428799"
  },
  {
    "text": "the length that's pretty easy to maintain the analogy I use for slicing",
    "start": "1428799",
    "end": "1433840"
  },
  {
    "text": "if it hasn't quite sort of fit into your mental model yet is of maybe taking some photographs so I've been out in Porto",
    "start": "1433840",
    "end": "1440159"
  },
  {
    "text": "we've been taking some photographs around the city um and sometimes there might be a building or some piece of",
    "start": "1440159",
    "end": "1445600"
  },
  {
    "text": "architecture that we want to get a nice close-up picture of and one option we have is to walk closer to the object and",
    "start": "1445600",
    "end": "1451840"
  },
  {
    "text": "once we're close to it we can take our photo and depending on how far away that object is it might have taken us a fair",
    "start": "1451840",
    "end": "1457039"
  },
  {
    "text": "amount of time a fair amount of energy to get there but eventually we will get our close-up the alternative on our",
    "start": "1457039",
    "end": "1462640"
  },
  {
    "text": "phone is that we just pinch zoom and ignoring the fact that we have you know essentially some degragation of image",
    "start": "1462640",
    "end": "1468000"
  },
  {
    "text": "quality that doesn't um come across in this analogy the the key thing there is a pinch zoom is pretty much a constant",
    "start": "1468000",
    "end": "1474080"
  },
  {
    "text": "time constant cost operation we can very quickly change our field of view over a scene and then we have a different view",
    "start": "1474080",
    "end": "1479840"
  },
  {
    "text": "of that same scene from the same position with very low effort and that's kind of the idea with",
    "start": "1479840",
    "end": "1485640"
  },
  {
    "text": "slicing so I'm going to begin with a a brief uh overview of what it looks like to optimize some code now this",
    "start": "1485640",
    "end": "1491919"
  },
  {
    "text": "particular example I do caution is very contrived very trivial um this is just to get us started and then we'll look at",
    "start": "1491919",
    "end": "1498080"
  },
  {
    "text": "some more real world code uh later on so let's imagine on Monday you return to",
    "start": "1498080",
    "end": "1504080"
  },
  {
    "text": "work and your manager tells you that if there's somehow that they found a way that if you could have a method that",
    "start": "1504080",
    "end": "1510400"
  },
  {
    "text": "takes an array of elements even number of elements and then starts taking a quarter of those elements from the",
    "start": "1510400",
    "end": "1515840"
  },
  {
    "text": "middle you're going to have some major business advantage over your competitor um so let's imagine that's the scenario",
    "start": "1515840",
    "end": "1521919"
  },
  {
    "text": "you're given um one pretty reasonable way that you could achieve that without thinking too hard is okay well we get",
    "start": "1521919",
    "end": "1527760"
  },
  {
    "text": "the length we'll skip halfway into that array we'll take the number of elements or quarter of the elements and then",
    "start": "1527760",
    "end": "1533360"
  },
  {
    "text": "we'll return the the new array and that's perfectly valid code it will do the job but then maybe your manager",
    "start": "1533360",
    "end": "1539520"
  },
  {
    "text": "comes to you and says actually if we can make this code really really fast um maybe we can make a huge amount of money",
    "start": "1539520",
    "end": "1544880"
  },
  {
    "text": "so I told you it was contrived but let's go with this so the first thing we should do in this scenario is measure",
    "start": "1544880",
    "end": "1550000"
  },
  {
    "text": "before we even begin optimizing we want to start with measuring our current assumption and so what we do here in",
    "start": "1550000",
    "end": "1556159"
  },
  {
    "text": "this this benchmark is there's a few new elements that I'll introduce again it's still a class um but this time we've got",
    "start": "1556159",
    "end": "1562799"
  },
  {
    "text": "this size property on here with uh the pram attribute with three different values what this lets us say to",
    "start": "1562799",
    "end": "1568320"
  },
  {
    "text": "benchmark.net is that we want to run these benchmarks with these three different values for that size property",
    "start": "1568320",
    "end": "1574480"
  },
  {
    "text": "so run these benchmarks three times essentially with those different sizes and this allows us to kind of test edge",
    "start": "1574480",
    "end": "1580480"
  },
  {
    "text": "cases sometimes testing your kind of expected value is fine but maybe there are edge cases to what your system",
    "start": "1580480",
    "end": "1586799"
  },
  {
    "text": "accepts that actually might result in different measures of performance and so by by testing your kind of middle case",
    "start": "1586799",
    "end": "1593200"
  },
  {
    "text": "plus your edge cases uh you can get make sure that you've got a representative view that the changes apply uh in all",
    "start": "1593200",
    "end": "1599080"
  },
  {
    "text": "scenarios because we're setting this up we now need an array of these different lengths as our input and so what we do",
    "start": "1599080",
    "end": "1605679"
  },
  {
    "text": "in this global setup um method here which is just a regular method with the global setup attribute we can pre-create",
    "start": "1605679",
    "end": "1611919"
  },
  {
    "text": "that array of the right size and populate it um this runs once for before",
    "start": "1611919",
    "end": "1616960"
  },
  {
    "text": "the benchmark run actually kicks in so there's no measurement overhead here this isn't part of what you're measuring as part of your benchmark it's just",
    "start": "1616960",
    "end": "1623360"
  },
  {
    "text": "setup code and then to do actual benchmark we can just run that piece of code that we saw",
    "start": "1623360",
    "end": "1629240"
  },
  {
    "text": "earlier the only difference from what we looked at earlier is that we're now putting this baseline equals true to say",
    "start": "1629240",
    "end": "1634559"
  },
  {
    "text": "this is our starting point and once we run this we see that okay for an input array of 100 elements it's about 100",
    "start": "1634559",
    "end": "1641039"
  },
  {
    "text": "nanoseconds 224 bytes allocated and we see what we'd kind of expect given the",
    "start": "1641039",
    "end": "1646400"
  },
  {
    "text": "code if we reason about it as we increase the input array size then by factors the execution time and the",
    "start": "1646400",
    "end": "1653120"
  },
  {
    "text": "allocations are going up because we're creating a new array at the end of the day without new data in it and so that's",
    "start": "1653120",
    "end": "1658640"
  },
  {
    "text": "giving us our starting point our baseline so maybe the first theory we have is well we've heard that link has",
    "start": "1658640",
    "end": "1664640"
  },
  {
    "text": "some overhead to it and maybe we should avoid using that and that will make our lives better so in this case what we do",
    "start": "1664640",
    "end": "1670000"
  },
  {
    "text": "is create a new array pres we then use array copy um to copy the data from one",
    "start": "1670000",
    "end": "1675840"
  },
  {
    "text": "array to the other and uh then we see if this works better so we run our benchmarks again now with our new",
    "start": "1675840",
    "end": "1681760"
  },
  {
    "text": "version added and we can see that for 100 elements it looks pretty good we've we've reduced our um execution time by",
    "start": "1681760",
    "end": "1687919"
  },
  {
    "text": "about 86% and it looks like our allocation ratio is improving massively as well by 43 the good thing is we've",
    "start": "1687919",
    "end": "1694799"
  },
  {
    "text": "tested other sizes here and we can see that while the execution time is still a good improvement the allocation ratio is",
    "start": "1694799",
    "end": "1701760"
  },
  {
    "text": "getting less and less valuable and that's because all we've done is remove the small 96 bytes of overhead of using",
    "start": "1701760",
    "end": "1707520"
  },
  {
    "text": "that link expression versus what we're doing with the array copy approach but we're still creating a new array we're",
    "start": "1707520",
    "end": "1712799"
  },
  {
    "text": "still copying data in there so in this final example let's assume that we can",
    "start": "1712799",
    "end": "1717840"
  },
  {
    "text": "change the return type so I'm changing the return type here to a span of int um so let's assume that there's further",
    "start": "1717840",
    "end": "1723520"
  },
  {
    "text": "processing that happens after this occurs and we our callers can accept a span of int back so in this case we now",
    "start": "1723520",
    "end": "1730000"
  },
  {
    "text": "use the as span and then we can slice in you can also use the range operator here if you prefer the syntax so you don't",
    "start": "1730000",
    "end": "1735360"
  },
  {
    "text": "need to slice you can just do uh square braces dot.length um and that's quite nice as well or starting position and",
    "start": "1735360",
    "end": "1742000"
  },
  {
    "text": "end position and so in this example we now moved to kind of a span based approach and we'll see that now the",
    "start": "1742000",
    "end": "1748960"
  },
  {
    "text": "execution time is 99% improved so we're we're less than a nancond which I think most people will agree is pretty",
    "start": "1748960",
    "end": "1754559"
  },
  {
    "text": "reasonable um and the allocation overhead now is is zero we've not got any bytes allocated because all we're",
    "start": "1754559",
    "end": "1760320"
  },
  {
    "text": "doing is returning a new view over that existing array from that start position halfway in for that length of a quarter",
    "start": "1760320",
    "end": "1766080"
  },
  {
    "text": "of the elements and we can see that as we move through the different sizes this constant time constant cost operation of",
    "start": "1766080",
    "end": "1772320"
  },
  {
    "text": "slicing doesn't matter how big that input array is um the savings are the same and so this has now massively",
    "start": "1772320",
    "end": "1778159"
  },
  {
    "text": "reduced that portion of code's overhead as I say a little contrived but we'll",
    "start": "1778159",
    "end": "1784480"
  },
  {
    "text": "get to some better examples soon now the other nice thing we can do with span is we can work with strings um so on a",
    "start": "1784480",
    "end": "1790960"
  },
  {
    "text": "string literal or a string variable we can call as span we get back a read only span of char so char is fairly obvious",
    "start": "1790960",
    "end": "1798320"
  },
  {
    "text": "strings are made up of characters the readonly span um because the runtime will enforce the safety here that",
    "start": "1798320",
    "end": "1804640"
  },
  {
    "text": "strings are immutable so it's never going to allow us to get a read write view with a span over the memory",
    "start": "1804640",
    "end": "1810159"
  },
  {
    "text": "occupied by a string because that would let us effectively mutate that string's data and that would break pretty much",
    "start": "1810159",
    "end": "1815520"
  },
  {
    "text": "every assumption we have in our codebase about safety of strings so we get back a readonly view and so this still is",
    "start": "1815520",
    "end": "1821919"
  },
  {
    "text": "useful because we can parse through the existing strings finding the pieces of data that we want so in this again very",
    "start": "1821919",
    "end": "1827919"
  },
  {
    "text": "basic example maybe we just want to get the portion of that string that represents the surname so that we can do some further analysis on it in this case",
    "start": "1827919",
    "end": "1834960"
  },
  {
    "text": "we could say let's find the index of the last space character and then slice from that position onwards and now we're",
    "start": "1834960",
    "end": "1841200"
  },
  {
    "text": "still working with the same block of memory but we're able to do it on strings as well now there are some",
    "start": "1841200",
    "end": "1847360"
  },
  {
    "text": "limitations of um span of t that are worth pointing out um and as I go through these you might get more and",
    "start": "1847360",
    "end": "1853279"
  },
  {
    "text": "more concerned about where this can actually be applied but don't worry it is fine so the first thing is I",
    "start": "1853279",
    "end": "1859039"
  },
  {
    "text": "mentioned earlier that the guarantee that we have that a uh span is never going to end up on the heap so we can",
    "start": "1859039",
    "end": "1865600"
  },
  {
    "text": "use it safely at any point in our code without worrying about heap allocations and that's enforced by the fact that",
    "start": "1865600",
    "end": "1871120"
  },
  {
    "text": "it's not just a normal strruct it's actually got this refstruct keyword that was introduced in C 7.2 that enables a",
    "start": "1871120",
    "end": "1878480"
  },
  {
    "text": "kind of guarantee that the runtime will enforce that this time can never end up u on the heap because there's various",
    "start": "1878480",
    "end": "1884240"
  },
  {
    "text": "ways that a value type would end up there and so what this effectively means is we can't ever let a span of bite or a",
    "start": "1884240",
    "end": "1890720"
  },
  {
    "text": "span of int or whatever it is be boxed because that would be then stored on the heap one of the main reasons for this",
    "start": "1890720",
    "end": "1896720"
  },
  {
    "text": "limitation this kind of stack only um uh limitation is that it can be pointing at",
    "start": "1896720",
    "end": "1902399"
  },
  {
    "text": "stack allocated memory so we can never allow this span of t thing to live longer than the stack frame that it is",
    "start": "1902399",
    "end": "1908320"
  },
  {
    "text": "is built is used on because it might then be pointing at data that's no longer valid if it outlives that stack",
    "start": "1908320",
    "end": "1914399"
  },
  {
    "text": "frame so that's one of the reasons we can't do boxing it also means that it can't be used as a field in a class or a",
    "start": "1914399",
    "end": "1920760"
  },
  {
    "text": "non-refstruct because again that data is living on the heap this is the one that tends to trip",
    "start": "1920760",
    "end": "1926799"
  },
  {
    "text": "people up um because of the way that async works it can't be used as an argument or a local variable in async",
    "start": "1926799",
    "end": "1932480"
  },
  {
    "text": "methods because async methods behind the scenes eventually get compiled down to that state machine that you may or may",
    "start": "1932480",
    "end": "1937760"
  },
  {
    "text": "not sort of be aware of but on uh most code that will be a strct but it's still not a strct that's guaranteed to be",
    "start": "1937760",
    "end": "1944320"
  },
  {
    "text": "stack only and so for the same reasons that it can't be a field in a class um it can't be used in those syntax in",
    "start": "1944320",
    "end": "1950640"
  },
  {
    "text": "those environments because those basically get hoisted up as fields on that state machine object that gets",
    "start": "1950640",
    "end": "1955880"
  },
  {
    "text": "created um for very similar reason it can't be captured by lambda expressions because of the closure that would be",
    "start": "1955880",
    "end": "1961440"
  },
  {
    "text": "created around those um and up until this version of that's coming u sort of",
    "start": "1961440",
    "end": "1968080"
  },
  {
    "text": "in November it couldn't be used as a generic type argument now that's actually being that limitation is being",
    "start": "1968080",
    "end": "1973519"
  },
  {
    "text": "removed u because we're getting this new sort of now new combination of keywords as a aware constraint on generics where",
    "start": "1973519",
    "end": "1979679"
  },
  {
    "text": "we can say we do allow a refstruct um as long as we know that our method is never going to let that escape the safety uh",
    "start": "1979679",
    "end": "1986399"
  },
  {
    "text": "the safe context of being within the same stack frame um and so in C# 13 you",
    "start": "1986399",
    "end": "1991519"
  },
  {
    "text": "can start to use it in certain new environments and there's other limitations I'm not even starting to",
    "start": "1991519",
    "end": "1998000"
  },
  {
    "text": "hint at here that get much more uh sort of deeper about how this this functions um but the key thing is that there are",
    "start": "1998000",
    "end": "2004320"
  },
  {
    "text": "some limitations to it fortunately at the same time as introducing span the team introduced this other type called",
    "start": "2004320",
    "end": "2010799"
  },
  {
    "text": "memory of T which has very similar um semantics to span the main difference",
    "start": "2010799",
    "end": "2016080"
  },
  {
    "text": "though is it can live on the heap if it needs to so it's defined um uh in in a",
    "start": "2016080",
    "end": "2022080"
  },
  {
    "text": "different way which does make it a little less versatile because you can't necessarily point it at stack allocated",
    "start": "2022080",
    "end": "2027760"
  },
  {
    "text": "memory anymore um and the performance is not quite as good there's certain guarantees about the stack only nature",
    "start": "2027760",
    "end": "2034080"
  },
  {
    "text": "of span that have allowed the runtime to be modified to make it highly efficient and we don't get all of those same",
    "start": "2034080",
    "end": "2039360"
  },
  {
    "text": "guarantees with this memory t uh version so it's defined uh in the code as a read",
    "start": "2039360",
    "end": "2044880"
  },
  {
    "text": "only strct but not a refstruct so we we still generally don't see it being copied too often hopefully um but it",
    "start": "2044880",
    "end": "2050560"
  },
  {
    "text": "might eventually end up being boxed in certain scenarios uh because of that if we're working with it directly then it's",
    "start": "2050560",
    "end": "2056158"
  },
  {
    "text": "going to be slightly slower if we do things like slicing into it um but fortunately at any point in time we can",
    "start": "2056159",
    "end": "2061520"
  },
  {
    "text": "call span on there and get the span property which represents the spanbased portion of the same block of memory and",
    "start": "2061520",
    "end": "2068079"
  },
  {
    "text": "so this kind of comes into effect if we're trying to do something in say an async method and we'd really like to take some data in in this this new sort",
    "start": "2068079",
    "end": "2074960"
  },
  {
    "text": "of highly optimized form if we try and pass a span of bite here then we get this compiler error because we're now",
    "start": "2074960",
    "end": "2080560"
  },
  {
    "text": "trying to create this parameter of a type span but what we can do instead is switch our code to accept a memory of",
    "start": "2080560",
    "end": "2087200"
  },
  {
    "text": "bite and we're still fine and things like a bite array etc will sort of implicitly convert to this quite happily",
    "start": "2087200",
    "end": "2094158"
  },
  {
    "text": "and that means that we could just work with that memory directly if we're not sort of worried about the the sort of the final nancond of performance then",
    "start": "2094159",
    "end": "2100880"
  },
  {
    "text": "usually working with memory is reasonable but if you want to what you can do is create uh a non-async method",
    "start": "2100880",
    "end": "2106880"
  },
  {
    "text": "where you can accept a span and then switch to that at any point in your code that you can do so typically we're only",
    "start": "2106880",
    "end": "2112480"
  },
  {
    "text": "in an async context during some IO operation we're getting some bytes off the wire uh through a stream or we're",
    "start": "2112480",
    "end": "2118960"
  },
  {
    "text": "reading in a file or whatever it might be but once we've done that sort of initial read and we've got some data buffered into memory somewhere at that",
    "start": "2118960",
    "end": "2125760"
  },
  {
    "text": "point we can switch to non-async code while we're processing and so in this codebase one option you might be tempted",
    "start": "2125760",
    "end": "2131680"
  },
  {
    "text": "to do is try and create an instance of a span sliced at a certain point that you're going to pass into that async",
    "start": "2131680",
    "end": "2136880"
  },
  {
    "text": "non-async method which again you can't do because it can't be a local but what we can do is pass in that slice",
    "start": "2136880",
    "end": "2143040"
  },
  {
    "text": "expression directly and the compiler is totally happy that it can make all the right guarantees about this now and then",
    "start": "2143040",
    "end": "2148320"
  },
  {
    "text": "that non-async method can do all of the processing using the high performance span based approach um and we can switch",
    "start": "2148320",
    "end": "2154160"
  },
  {
    "text": "back to the async method when when we're done so I'm going to start putting this into practice with a a more real world",
    "start": "2154160",
    "end": "2161640"
  },
  {
    "text": "example um and this is based on something for a project I worked on at a previous company um so I'm going to",
    "start": "2161640",
    "end": "2167760"
  },
  {
    "text": "switch over to Visual Studio which looks okay there is the code okay at the back for everyone good cool um so what I'm",
    "start": "2167760",
    "end": "2174720"
  },
  {
    "text": "going to do is show you um I'll give you the quick summary of what this is doing actually I'll show you this slide so the",
    "start": "2174720",
    "end": "2181359"
  },
  {
    "text": "the context for this service which is a micros service I used to work on is that we were reading messages off of a queue",
    "start": "2181359",
    "end": "2187440"
  },
  {
    "text": "in this case it was Amazon SQS we des serialize that JSON data from the queue and we wanted then to store into the",
    "start": "2187440",
    "end": "2193440"
  },
  {
    "text": "Amazon S3 blob store a copy of that message basically as a backup process and in order to ensure that we were only",
    "start": "2193440",
    "end": "2199680"
  },
  {
    "text": "storing one copy we derived a a unique object key basically a file name for that blob from some of the properties on",
    "start": "2199680",
    "end": "2206000"
  },
  {
    "text": "the message and so the existing code that uh we had looked a bit like this this is sort of simplified down a bit",
    "start": "2206000",
    "end": "2212320"
  },
  {
    "text": "but we have this event context which you can imagine represents the desialized message coming in in this case it's got",
    "start": "2212320",
    "end": "2217760"
  },
  {
    "text": "five properties in the real system it had uh a few hundred um but the key thing is we wanted to use just a few of",
    "start": "2217760",
    "end": "2224160"
  },
  {
    "text": "those for the object key um so if the date was present we'd use the date but it wasn't always guaranteed hence why",
    "start": "2224160",
    "end": "2229680"
  },
  {
    "text": "this code deals with that possibility but the main way this code worked is that we basically created a string array",
    "start": "2229680",
    "end": "2235599"
  },
  {
    "text": "to hold all of the parts that were going to be used in that object key and then for each index of that string array we",
    "start": "2235599",
    "end": "2242160"
  },
  {
    "text": "use this get part method to um take the input string so in this case for example this is coming from the product or the",
    "start": "2242160",
    "end": "2248800"
  },
  {
    "text": "site key that's coming off that object um that would come in if it was null or empty then we would just fill um that",
    "start": "2248800",
    "end": "2255520"
  },
  {
    "text": "particular element with just unknown because for some reason we didn't have the data otherwise we'd call remove",
    "start": "2255520",
    "end": "2260640"
  },
  {
    "text": "spaces which just did a replace of spaces with underscores and then once we've done that we did this is valid",
    "start": "2260640",
    "end": "2266240"
  },
  {
    "text": "check basically using reax to make sure that it matched a particular set of characters we knew was safe for uh S3",
    "start": "2266240",
    "end": "2272320"
  },
  {
    "text": "object keys so we did that with all of the main uh pieces of data coming in we",
    "start": "2272320",
    "end": "2277359"
  },
  {
    "text": "then used uh the date time two string just to format out the date if it was present and then we put the message ID",
    "start": "2277359",
    "end": "2284160"
  },
  {
    "text": "and a suffix on that file so once we've got all of the individual pieces we just use string join to join them together uh",
    "start": "2284160",
    "end": "2290400"
  },
  {
    "text": "lowercased everything and that then became the object key so a few of you might have already spotted where that",
    "start": "2290400",
    "end": "2296079"
  },
  {
    "text": "might allocate and why that could potentially be optimized and so I'll show you the new code up here so there's a few",
    "start": "2296079",
    "end": "2302640"
  },
  {
    "text": "differences uh so this code has been updated for like .NET 8 um and so one of the things I did in here was move to the",
    "start": "2302640",
    "end": "2308560"
  },
  {
    "text": "new source generated reax stuff which basically premputes the the reax",
    "start": "2308560",
    "end": "2313599"
  },
  {
    "text": "expressions and the code needed to uh do a reax match in an optimal way and it",
    "start": "2313599",
    "end": "2318640"
  },
  {
    "text": "does it at compile time rather than runtime so you can remove some of the runtime cost of reax u by using it the",
    "start": "2318640",
    "end": "2326000"
  },
  {
    "text": "other thing it does here is use this kind of special um sort of technique for basically defining these readonly span",
    "start": "2326000",
    "end": "2333040"
  },
  {
    "text": "static properties and what this does is the compiler knows how to handle this and basically treats this as specialized",
    "start": "2333040",
    "end": "2339119"
  },
  {
    "text": "data that gets stored into the actual binary uh data for the DLL and so at any time that we're using these in code it's",
    "start": "2339119",
    "end": "2345839"
  },
  {
    "text": "just pointing to an existing memory location within that DL uh which lets us be very optimal and avoid allocating",
    "start": "2345839",
    "end": "2352240"
  },
  {
    "text": "those when we're comparing the main bulk of the code then",
    "start": "2352240",
    "end": "2359040"
  },
  {
    "text": "uh used this new string create feature so basically what string create lets us",
    "start": "2359040",
    "end": "2364160"
  },
  {
    "text": "do is to pre-allocate some uh memory on the heap that's going to be used to uh",
    "start": "2364160",
    "end": "2369440"
  },
  {
    "text": "create a string but it lets us mutate uh that memory directly and work with it as",
    "start": "2369440",
    "end": "2374960"
  },
  {
    "text": "a span while we're creating the string so once it's created it becomes a regular immutable string but during that",
    "start": "2374960",
    "end": "2381280"
  },
  {
    "text": "creation method the code actually can mutate the various bytes or various characters within there so in order for",
    "start": "2381280",
    "end": "2387599"
  },
  {
    "text": "that to work it has to premp compute the length because we need to know the right length that we need um for that string",
    "start": "2387599",
    "end": "2392800"
  },
  {
    "text": "to occupy um I won't show you that code it's fairly boring um but then we do string create and what string create",
    "start": "2392800",
    "end": "2398560"
  },
  {
    "text": "takes is this key builder action which is this special span action type that basically gives us um uh lets us pass in",
    "start": "2398560",
    "end": "2406480"
  },
  {
    "text": "our state which is our event context and it gives us access to a span here a span of characters which is that um",
    "start": "2406480",
    "end": "2413119"
  },
  {
    "text": "pre-allocated heat memory for the string and so in here what we do is for each uh of those um pieces of data that we want",
    "start": "2413119",
    "end": "2420240"
  },
  {
    "text": "to use in this the string key we call build part and we pass in a current position as we do this which is what's",
    "start": "2420240",
    "end": "2425680"
  },
  {
    "text": "going to allow us to slice through that memory as we're adding new elements to it and so we take the input string um",
    "start": "2425680",
    "end": "2432560"
  },
  {
    "text": "and we do basically what the original code is but we're now doing a more sort of basic check to see if it's null or",
    "start": "2432560",
    "end": "2438160"
  },
  {
    "text": "empty by checking if it's a length of zero or whites space um we then run the",
    "start": "2438160",
    "end": "2443599"
  },
  {
    "text": "source generated reax for the validation piece we then call this memory extensions to lower invariant which lets",
    "start": "2443599",
    "end": "2449280"
  },
  {
    "text": "us take a string copy the characters out and lowerase them into our span pre-allocated memory in one operation um",
    "start": "2449280",
    "end": "2456800"
  },
  {
    "text": "every time we do something where we're writing we're incrementing our position by how much we've written so we're slowly moving through that span of",
    "start": "2456800",
    "end": "2463440"
  },
  {
    "text": "memory as we add more elements to it and basically we do this for each item then",
    "start": "2463440",
    "end": "2469280"
  },
  {
    "text": "up here we do a string rep sorry a span replace so this new replace operation added in net 8 is very optimized way of",
    "start": "2469280",
    "end": "2475599"
  },
  {
    "text": "replacing certain characters within a span uh so we replace all the spaces with underscores for dates we can use",
    "start": "2475599",
    "end": "2481680"
  },
  {
    "text": "this new try format or newer tri format method which lets us format a date directly into again a span of of",
    "start": "2481680",
    "end": "2487680"
  },
  {
    "text": "characters um and gives us back how many bytes were written as doing that we then",
    "start": "2487680",
    "end": "2493520"
  },
  {
    "text": "lowerase the message ID and we copy on that JSON suffix and in this case we're just slicing uh in this case using a",
    "start": "2493520",
    "end": "2500560"
  },
  {
    "text": "rangebased operator for the last five characters where we put the suffix uh in there so what this is doing is basically",
    "start": "2500560",
    "end": "2506880"
  },
  {
    "text": "moving us to a span based um way of building up that string now I promise we benchmarked and we did all the right",
    "start": "2506880",
    "end": "2512640"
  },
  {
    "text": "things as we went ahead with this and I'll just show you the results so you can kind of see the gains so with the new code we were about",
    "start": "2512640",
    "end": "2520000"
  },
  {
    "text": "26% quicker um actually our improvements were like three times faster when we did this back in the free one days i've",
    "start": "2520000",
    "end": "2526000"
  },
  {
    "text": "updated this code a few times to move to new versions of .NET and Microsoft keep improving even those built-in operations",
    "start": "2526000",
    "end": "2532079"
  },
  {
    "text": "like string replace for example so the the gain is going down but there's still a gain to be had by manually doing this",
    "start": "2532079",
    "end": "2538640"
  },
  {
    "text": "and the key thing is that we then removed about 74% of the allocation so we're down to 192 bytes and when we",
    "start": "2538640",
    "end": "2544160"
  },
  {
    "text": "profile the memory that 192 bytes was the actual object key string that we we were creating and we had to create to",
    "start": "2544160",
    "end": "2550240"
  },
  {
    "text": "pass into the Amazon S3 um API so that means that we've got rid of all the overhead of creating that we're just",
    "start": "2550240",
    "end": "2556480"
  },
  {
    "text": "down to the actual cost of the string creation that we actually want now on its own this might not seem like it's",
    "start": "2556480",
    "end": "2561839"
  },
  {
    "text": "worthwhile it's a few hundred bytes right but this service was doing 18 million messages a day so this one",
    "start": "2561839",
    "end": "2567040"
  },
  {
    "text": "subtle change to the codebase in how we manage this object key creation saved us 10 gig of daily allocations and that's",
    "start": "2567040",
    "end": "2573280"
  },
  {
    "text": "going to contribute to a number of GCs that we're no longer having to do in that application as a result so we got",
    "start": "2573280",
    "end": "2578880"
  },
  {
    "text": "quite a nice gain there so the next type I want to talk about is called array pool um the name kind of gives this one",
    "start": "2578880",
    "end": "2585119"
  },
  {
    "text": "away it's a pool of arrays um and this can be used anywhere in your codebase where you need those kind of like",
    "start": "2585119",
    "end": "2591119"
  },
  {
    "text": "temporary buffers of either bytes or characters as you're you're doing some processing typically where we see this",
    "start": "2591119",
    "end": "2596160"
  },
  {
    "text": "is if we're doing anything within an iteration where we might be working with streambased data where we have to pass",
    "start": "2596160",
    "end": "2601359"
  },
  {
    "text": "it a temporary buffer to work with um and what we can use the array pool to do is instead of incurring the cost of",
    "start": "2601359",
    "end": "2607839"
  },
  {
    "text": "those temporary arrays each time we're doing that processing work we can use a rented array which is uh coming from",
    "start": "2607839",
    "end": "2614160"
  },
  {
    "text": "this shared pool to avoid that sort of cost so over time we sort of amatize away the cost of uh creating those",
    "start": "2614160",
    "end": "2620720"
  },
  {
    "text": "temporary buffers so it's found in the system buffers name space um renting is pretty straightforward we access the",
    "start": "2620720",
    "end": "2626880"
  },
  {
    "text": "array pool of T so whatever type we want uh an array to hold we you gen generally",
    "start": "2626880",
    "end": "2632079"
  },
  {
    "text": "use this shared version so this is a pre-built sort of in the box implementation of an array pool that",
    "start": "2632079",
    "end": "2638400"
  },
  {
    "text": "uses a predefined set of buckets for the sizes of the arrays that it can um store and then we just rent the one that we",
    "start": "2638400",
    "end": "2644880"
  },
  {
    "text": "want with the length we need the key thing is that we might and generally will get an array that's larger than what we ask for and the reason is for a",
    "start": "2644880",
    "end": "2652160"
  },
  {
    "text": "a pool to be useful it has to bucket into particular sizes it can't have every possible size array that we might",
    "start": "2652160",
    "end": "2658000"
  },
  {
    "text": "ever need because the chances of us needing the same exact size every time isn't necessarily um consistent so it's",
    "start": "2658000",
    "end": "2664640"
  },
  {
    "text": "going to give us the closest array size it has that's at least big enough to hold what we've asked for which means",
    "start": "2664640",
    "end": "2670480"
  },
  {
    "text": "that when we're working with an array pool based um piece of data we're typically then going to need to track the length of the data that we've",
    "start": "2670480",
    "end": "2676720"
  },
  {
    "text": "actually written into that temporary buffer so that we can always slice it later on and just view the portion that",
    "start": "2676720",
    "end": "2682000"
  },
  {
    "text": "we know we've written when we're done we return it uh we return the array and we optionally can",
    "start": "2682000",
    "end": "2687920"
  },
  {
    "text": "pass this clear array um switch here as well so by default the clear away array is set to false which does mean that by",
    "start": "2687920",
    "end": "2695280"
  },
  {
    "text": "default when you return an array the data is still left in there as it was when you worked with it before um so for",
    "start": "2695280",
    "end": "2701599"
  },
  {
    "text": "sensitive systems that might be something you need to think about the reason this is done is that zeroing out an array has a cost and so by default",
    "start": "2701599",
    "end": "2708319"
  },
  {
    "text": "for this to be as optimal as possible it doesn't zero out those pieces of data um",
    "start": "2708319",
    "end": "2713520"
  },
  {
    "text": "generally that's not going to be a problem but again if you need to switch to the kind of safer mode you can do so",
    "start": "2713520",
    "end": "2719280"
  },
  {
    "text": "so what this looks like is here for example imagine we have a method it's going to be called by some tight loop as",
    "start": "2719280",
    "end": "2725119"
  },
  {
    "text": "processing is being done and it needs a temporary buffer that it's going to pass into another method so here we're",
    "start": "2725119",
    "end": "2730720"
  },
  {
    "text": "allocating each time a thousand uh bytes for this buffer we can very easily",
    "start": "2730720",
    "end": "2736000"
  },
  {
    "text": "switch that to instead prefer using the shared array pool and renting a thousand",
    "start": "2736000",
    "end": "2741280"
  },
  {
    "text": "now the main difference here is that when we then want to work with that buffer we now have to pass in the actual",
    "start": "2741280",
    "end": "2746640"
  },
  {
    "text": "length so that we know we only get we're going to only work with a thousand elements which is the piece that we know we're going to have populated because",
    "start": "2746640",
    "end": "2753200"
  },
  {
    "text": "typically if we rent 1,000 the minimum size that array pool can give us is 1024 bytes um and it might even be larger if",
    "start": "2753200",
    "end": "2759839"
  },
  {
    "text": "there's none of those available and it moves a size up and so it does mean that when we're working with the data below",
    "start": "2759839",
    "end": "2765440"
  },
  {
    "text": "we have to then slice into it for the length that we're actually know we've populated in the previous method um we",
    "start": "2765440",
    "end": "2772079"
  },
  {
    "text": "do need to remember to return the array for the pool to have any real benefit um I'm showing it here with a try finally",
    "start": "2772079",
    "end": "2778720"
  },
  {
    "text": "microsoft have actually moved away from this guidance fairly recently and and tend not to bother with it so the idea",
    "start": "2778720",
    "end": "2784000"
  },
  {
    "text": "of doing it in a try finally is you always guarantee if there's an exception in that other method we're returning the",
    "start": "2784000",
    "end": "2789440"
  },
  {
    "text": "array to the pool but actually the cost of doing a try finally and the operations needed is",
    "start": "2789440",
    "end": "2794520"
  },
  {
    "text": "usually not necessary for the number of times that might actually cause an exception so unless that method could",
    "start": "2794520",
    "end": "2800000"
  },
  {
    "text": "really cause an exception very often um it's better to just do uh the method that you want and then return it",
    "start": "2800000",
    "end": "2806160"
  },
  {
    "text": "afterwards and on the few cases where an exception were to occur it doesn't matter if that uh buffer doesn't go back",
    "start": "2806160",
    "end": "2812079"
  },
  {
    "text": "into the pool it will eventually uh through its final finalizer be collected by GC anyway so um it's up to you it",
    "start": "2812079",
    "end": "2819599"
  },
  {
    "text": "doesn't really matter either way um I still sometimes do it this way um just to protect",
    "start": "2819599",
    "end": "2825000"
  },
  {
    "text": "myself so the other type I want to show is um system IO",
    "start": "2825000",
    "end": "2830760"
  },
  {
    "text": "pipelines this one's good if you ever work with like streaming data or or or sort of data coming across um uh the",
    "start": "2830760",
    "end": "2837920"
  },
  {
    "text": "wire and so this was originally created by the ASP.NET team i think it was pretty much David Fowler's kind of child",
    "start": "2837920",
    "end": "2844240"
  },
  {
    "text": "um that he sort of built into the .NET system so it was originally in the ASP.NET codebase and then has been",
    "start": "2844240",
    "end": "2849359"
  },
  {
    "text": "shifted into the runtime codebase and the key reason it was kind of introduced was around Kestrel so Kestrel is the web",
    "start": "2849359",
    "end": "2855680"
  },
  {
    "text": "server in ASP.NET Core it's obviously dealing with lots of bites coming off a wire that then go through the ASP.NET",
    "start": "2855680",
    "end": "2860880"
  },
  {
    "text": "Core pipeline internally and one of the things the team wanted to do was try and avoid all of those costs of switching",
    "start": "2860880",
    "end": "2867040"
  },
  {
    "text": "between different streams as that data gets processed through the pipeline so they found that by moving to this new",
    "start": "2867040",
    "end": "2873280"
  },
  {
    "text": "sort of pipelines library they could roughly improve their performance by about 2x in the kind of kestrel scenario",
    "start": "2873280",
    "end": "2879200"
  },
  {
    "text": "and typically it's just done by removing what would otherwise be code you could write yourself to manage streams and",
    "start": "2879200",
    "end": "2884400"
  },
  {
    "text": "temporary buffers and use the array pool uh to try and avoid allocations but it would actually have a lot of boilerplate",
    "start": "2884400",
    "end": "2890720"
  },
  {
    "text": "and edge cases that you'd have to consider so it just wraps that all up for you and does all of the heavy lifting so the main difference is that",
    "start": "2890720",
    "end": "2897440"
  },
  {
    "text": "unlike when you're using streams yourself um with pipelines it's managing those buffers for you so you don't",
    "start": "2897440",
    "end": "2902640"
  },
  {
    "text": "create the the buffers that you would pass in as you're processing the stream you just work with this pipe concept and",
    "start": "2902640",
    "end": "2909040"
  },
  {
    "text": "internally it's using what's known as a memory pool which basically is backed by an array pool behind the scenes to",
    "start": "2909040",
    "end": "2914640"
  },
  {
    "text": "actually manage those buffers without allocating uh two ends to a pipe as you might guess so there's a writing end and",
    "start": "2914640",
    "end": "2921119"
  },
  {
    "text": "when we write uh there's a few ways we can do it there are sort of uh shortcut methods for writing small blocks of data",
    "start": "2921119",
    "end": "2926960"
  },
  {
    "text": "directly with write or write async or in most scenarios what you might want to do is actually just ask the pipe for a",
    "start": "2926960",
    "end": "2933680"
  },
  {
    "text": "block of memory in this case it comes back as a memory of bite that you can then populate so it's a memory of bite",
    "start": "2933680",
    "end": "2939280"
  },
  {
    "text": "typically because pipelines is going to be used in these async scenarios where you're getting data off of uh IO network",
    "start": "2939280",
    "end": "2944880"
  },
  {
    "text": "connections etc um but of course you can switch from that memory of bite to a non",
    "start": "2944880",
    "end": "2950079"
  },
  {
    "text": "async method at any point and start working with that same data as a span but once you've worked with some of the",
    "start": "2950079",
    "end": "2955440"
  },
  {
    "text": "data you just advance the pipe writer by how much you've written and then flush it and as soon as you flush it that",
    "start": "2955440",
    "end": "2961440"
  },
  {
    "text": "awakens the reading end that will be sitting there waiting so on the reader we called read async and that will then",
    "start": "2961440",
    "end": "2967200"
  },
  {
    "text": "just asynchronously await data being available from the pipe so it's non-blocking and eventually we get back",
    "start": "2967200",
    "end": "2972640"
  },
  {
    "text": "a read result once some data is flushed in for us we can access the buffer on that read result which comes back as",
    "start": "2972640",
    "end": "2977680"
  },
  {
    "text": "this readonly sequence of bite so why sequence down here in a memory on the way in and the reason is ahead of time",
    "start": "2977680",
    "end": "2983760"
  },
  {
    "text": "the pipe doesn't know how much data is going to fill it um typically if you're reading off of the wire for example then",
    "start": "2983760",
    "end": "2989599"
  },
  {
    "text": "you might be dealing with chunk data and you don't know the content length yet and so what it does internally is it",
    "start": "2989599",
    "end": "2995200"
  },
  {
    "text": "will rent a block of memory and start filling it for you and if you're keeping up with the the reading and the writing",
    "start": "2995200",
    "end": "3001119"
  },
  {
    "text": "then that might be sufficient but if at any point the the reading end slows down and isn't actually processing all of",
    "start": "3001119",
    "end": "3006480"
  },
  {
    "text": "that block in time then it will just rent another block of memory and then another block of memory and keep filling",
    "start": "3006480",
    "end": "3011520"
  },
  {
    "text": "it and then on the reading end what it gives you is this essentially a linked list of those blocks of memory as this",
    "start": "3011520",
    "end": "3018079"
  },
  {
    "text": "read only sequence and gives us a way to view that data in the correct sequence um so the example that sort of",
    "start": "3018079",
    "end": "3024720"
  },
  {
    "text": "demonstrates usage of this um again similar to the last one same sort of micros service uh we were dealing with",
    "start": "3024720",
    "end": "3031040"
  },
  {
    "text": "retrieving objects from S3 in this case it was a tab separated file so we wanted to get three columns of data from",
    "start": "3031040",
    "end": "3038079"
  },
  {
    "text": "separated file um and uh index them into elastic search uh so I should show you",
    "start": "3038079",
    "end": "3044000"
  },
  {
    "text": "the the before code I didn't write this I'm going to",
    "start": "3044000",
    "end": "3049359"
  },
  {
    "text": "tell you that in advance because you might spot the problems Um so this is roughly what the original code was doing",
    "start": "3049359",
    "end": "3054960"
  },
  {
    "text": "so here I'm using a file stream in this demo app but it was basically an S3 stream from the S3 um API instead um it",
    "start": "3054960",
    "end": "3063040"
  },
  {
    "text": "needed to be decompressed because it was zipped uh gzipped um for decompression and then this is where the",
    "start": "3063040",
    "end": "3069200"
  },
  {
    "text": "code gets a bit interesting so it then after decompressing it creates a new memory stream and it copies from the",
    "start": "3069200",
    "end": "3075119"
  },
  {
    "text": "decompress stream into the memory stream it then calls two array to get an entire the entire bytes from that uh memory",
    "start": "3075119",
    "end": "3081839"
  },
  {
    "text": "stream and then called get string to then get the string representation of that file um and then it was passing it",
    "start": "3081839",
    "end": "3088559"
  },
  {
    "text": "into this paraser thing now the reason it was doing that I think at the time is that it was using this library called tiny CSV paraser that I think only took",
    "start": "3088559",
    "end": "3095440"
  },
  {
    "text": "string inputs at one point um at least that's the explanation I was given and I will trust the engineer did this for a",
    "start": "3095440",
    "end": "3101440"
  },
  {
    "text": "good reason but if you're looking at this code you can probably spot where this might allocate we're copying memory",
    "start": "3101440",
    "end": "3106480"
  },
  {
    "text": "we are we're creating a new array which again is copying we're getting a string that represents the entire contents of a",
    "start": "3106480",
    "end": "3112000"
  },
  {
    "text": "large file so all of these allocations that occur here also occur on the large object heap because typically these files are about a thousand row 10,000",
    "start": "3112000",
    "end": "3119200"
  },
  {
    "text": "rows each time so the new code that we switched to um slightly more complex if you look at",
    "start": "3119200",
    "end": "3125520"
  },
  {
    "text": "the length compared to the last one but what we did instead was um we still need",
    "start": "3125520",
    "end": "3130800"
  },
  {
    "text": "the decompress stream but as soon as we've got that we get use this pipe reader create here to just create a reader over that stream so that gives us",
    "start": "3130800",
    "end": "3137760"
  },
  {
    "text": "a pipe reader that's accessing the stream as the data becomes available and so we do the read async we access the",
    "start": "3137760",
    "end": "3143760"
  },
  {
    "text": "buffer and as soon as we've got the buffer we can call pars lines to see what lines we've got and so this is",
    "start": "3143760",
    "end": "3149119"
  },
  {
    "text": "dealing with that readon sequence that was coming off the pipe reader and it uses a sequence reader as a helper type",
    "start": "3149119",
    "end": "3155040"
  },
  {
    "text": "that allows us to more easily parse through these sequences because we have to deal with a scenario that we might have one block of memory or multiple",
    "start": "3155040",
    "end": "3161440"
  },
  {
    "text": "blocks that we have to work through so the sequence reader deals with that and so it gives us this convenience that we",
    "start": "3161440",
    "end": "3166880"
  },
  {
    "text": "can just loop through uh the reader until we've read everything um we can try and read out an entire line so what",
    "start": "3166880",
    "end": "3173280"
  },
  {
    "text": "we want to essentially get is one line of the file delimited by the new line so if it finds a new line give me the span",
    "start": "3173280",
    "end": "3179200"
  },
  {
    "text": "of that file or of that line um that represents the data that I can then process with pars line down below and",
    "start": "3179200",
    "end": "3186000"
  },
  {
    "text": "then pars line is just manually doing what that tiny CSV parser library was doing where it's basically accessing the",
    "start": "3186000",
    "end": "3193280"
  },
  {
    "text": "free um tabs or columns of data that we actually care about so it basically",
    "start": "3193280",
    "end": "3198480"
  },
  {
    "text": "finds the index of the tab character uh checks if that current tab count that we're at is a piece of data that we care",
    "start": "3198480",
    "end": "3205200"
  },
  {
    "text": "about and if it is at that point it does get the string by slicing the line from the current position to where that tab",
    "start": "3205200",
    "end": "3210880"
  },
  {
    "text": "appears and that gets us our string that we need to populate our object to go into Elastic Search so it only does that",
    "start": "3210880",
    "end": "3216960"
  },
  {
    "text": "for the three columns we care about 0 1 and 10 and as soon as we've reached that we actually then just break out of this",
    "start": "3216960",
    "end": "3222480"
  },
  {
    "text": "loop because we're not going to go past um the the columns that we care about and when we're reading that line um",
    "start": "3222480",
    "end": "3228640"
  },
  {
    "text": "that's pretty much what it does um and I'll show you the before and after of that so that's just switching to those",
    "start": "3228640",
    "end": "3234200"
  },
  {
    "text": "pipelines and again span based optimizations so the original code um",
    "start": "3234200",
    "end": "3239839"
  },
  {
    "text": "versus the new code about 81% quicker we weren't going for speed the reason we investigated this service was that it",
    "start": "3239839",
    "end": "3246079"
  },
  {
    "text": "was running as a a container we had memory limits set on it and quite regularly it was hitting those memory",
    "start": "3246079",
    "end": "3251839"
  },
  {
    "text": "limits and killing the container and so we constantly seem to find ourselves increasing the the container limits for",
    "start": "3251839",
    "end": "3257119"
  },
  {
    "text": "that container and that's why we looked at it we saw that code and we then realized that actually yeah by switching",
    "start": "3257119",
    "end": "3262240"
  },
  {
    "text": "to this new version we could remove uh 97% of the allocations just by switching",
    "start": "3262240",
    "end": "3267520"
  },
  {
    "text": "to something that wasn't allocating as much and when I did a profile on that it was about 2.85 of that with the data",
    "start": "3267520",
    "end": "3274480"
  },
  {
    "text": "that we actually needed the strings that we did want to push into elastic search so there was about 045 meg of overhead",
    "start": "3274480",
    "end": "3281040"
  },
  {
    "text": "some of that comes from the pipe reader types and the sequence reader types i did I still do want to check and see if",
    "start": "3281040",
    "end": "3287040"
  },
  {
    "text": "I can get that down any lower but this is a good example of like sometimes stopping when you've done enough is perfectly fine we'd solved the main",
    "start": "3287040",
    "end": "3294000"
  },
  {
    "text": "problem that 045 meg of overhead is insignificant compared to the problem we started with and so we didn't feel the",
    "start": "3294000",
    "end": "3300160"
  },
  {
    "text": "need to to go any further with this one so we've got a few minutes left um so",
    "start": "3300160",
    "end": "3305599"
  },
  {
    "text": "I'll very quickly try and go over the system textjer stuff with a quick demo of what that uh can do for your code so",
    "start": "3305599",
    "end": "3311520"
  },
  {
    "text": "these have been in the bot since net core 3 so um traditionally most people",
    "start": "3311520",
    "end": "3317359"
  },
  {
    "text": "probably familiar with Newtonoft JSON for doing JSON parsing um Microsoft",
    "start": "3317359",
    "end": "3322559"
  },
  {
    "text": "introduced this as an in in the box API in Freo uh to start giving us a new",
    "start": "3322559",
    "end": "3328240"
  },
  {
    "text": "version of JSON parsing that could take advantage of these new high performance techniques and so it gives us three",
    "start": "3328240",
    "end": "3334640"
  },
  {
    "text": "levels to work with we can work at a very low level with um directly with kind of UTF8 bytes for the JSON reader",
    "start": "3334640",
    "end": "3340720"
  },
  {
    "text": "and writer we can work at a middle level when we're reading using this JSON document object model that we can then",
    "start": "3340720",
    "end": "3346480"
  },
  {
    "text": "process our way through in a kind of read forward fashion um and then we also have a regular kind of serializer where",
    "start": "3346480",
    "end": "3352720"
  },
  {
    "text": "you can just say give give me an object and serialize it or give me some JSON and deserialize it into an object even",
    "start": "3352720",
    "end": "3358880"
  },
  {
    "text": "that new higher level one is going to be more optimal than using Newton soft JSON just because it tries to avoid",
    "start": "3358880",
    "end": "3364400"
  },
  {
    "text": "allocations behind the scenes as much as possible but the real gains come if you really need them from those low-level",
    "start": "3364400",
    "end": "3369599"
  },
  {
    "text": "types so the example I have for this again similar scenario to the last",
    "start": "3369599",
    "end": "3375280"
  },
  {
    "text": "one i was working on a scenario where we were bulk indexing some data into elastic search when you do that with",
    "start": "3375280",
    "end": "3380319"
  },
  {
    "text": "elastic search it gives you a response back that tells you did any of the maybe 100 10,000 whatever it is you're sending",
    "start": "3380319",
    "end": "3387280"
  },
  {
    "text": "in um operations the indexing operations uh have an error and if and then it also",
    "start": "3387280",
    "end": "3392640"
  },
  {
    "text": "gives you a summary for each operation as to what the status codes were for them and so what we wanted to do is just",
    "start": "3392640",
    "end": "3398079"
  },
  {
    "text": "find out if any of the data we tried to index was failing so that we could track the IDs and essentially dead letter",
    "start": "3398079",
    "end": "3403119"
  },
  {
    "text": "those items and deal with them later so I'll give you a quick tour of the before and after code",
    "start": "3403119",
    "end": "3409520"
  },
  {
    "text": "so the original code is really tur it was new using Newton source JSON that's all the code there is um ultimately it",
    "start": "3409520",
    "end": "3416319"
  },
  {
    "text": "was getting to the point where we had a a JSON serializer des serializing the uh",
    "start": "3416319",
    "end": "3421440"
  },
  {
    "text": "JSON response that comes back into this type which has the length of time it took whether or not there were errors",
    "start": "3421440",
    "end": "3427200"
  },
  {
    "text": "and then for each item each indexing operation what was the result",
    "start": "3427200",
    "end": "3432640"
  },
  {
    "text": "uh and then what we do is check that errors property and if there's no errors it's given us a false on that errors",
    "start": "3432640",
    "end": "3437680"
  },
  {
    "text": "property then we can to say that yes this was successful and there's no ids to return otherwise we have to process",
    "start": "3437680",
    "end": "3443119"
  },
  {
    "text": "those those items to find the ids of the ones that failed so that is pretty easy code now the new code I'll grant you is",
    "start": "3443119",
    "end": "3450480"
  },
  {
    "text": "a good example of where moving to the higher performance code is less tur um",
    "start": "3450480",
    "end": "3456079"
  },
  {
    "text": "and this would be less maintainable right um but if you need the performance then this is the kind of thing you might",
    "start": "3456079",
    "end": "3461680"
  },
  {
    "text": "end up doing so what this does that's different is we read a stream in this case directly and we go into this par",
    "start": "3461680",
    "end": "3468240"
  },
  {
    "text": "errors um where we've read some data off the stream uh we just use the rented array uh from the array pool for that",
    "start": "3468240",
    "end": "3474480"
  },
  {
    "text": "buffer for the streaming in this case and so what we do here is we switch that low-level JSON uh reader type and at",
    "start": "3474480",
    "end": "3482720"
  },
  {
    "text": "that point what we can do is basically ask it to read through the the bytes that it's received um giving us each",
    "start": "3482720",
    "end": "3488960"
  },
  {
    "text": "JSON token as we as we reach it so this reads forward a JSON token at a time but",
    "start": "3488960",
    "end": "3494480"
  },
  {
    "text": "we track a bunch of state so you can see I'm passing in a bunch of my own state in here mostly as by reference um as",
    "start": "3494480",
    "end": "3501119"
  },
  {
    "text": "well as this JSON reader state object which allows us to make that uh UTF JSON reader re-enterable so the state tracks",
    "start": "3501119",
    "end": "3508480"
  },
  {
    "text": "where it is in that JSON structure so if you're getting data in chunks off of the wire you can start processing the JSON",
    "start": "3508480",
    "end": "3514720"
  },
  {
    "text": "and then return to this method later on to continue with the next chunk and the basic premise of what this code does is",
    "start": "3514720",
    "end": "3520640"
  },
  {
    "text": "it switches on the JSON token and so I can kind of track my way through that that expected um JSON data that I'm",
    "start": "3520640",
    "end": "3527280"
  },
  {
    "text": "getting so am I into into the start object am I into the array have I reached certain properties so does the",
    "start": "3527280",
    "end": "3533680"
  },
  {
    "text": "token uh value text so the string match this um predefined in this case this U8",
    "start": "3533680",
    "end": "3540319"
  },
  {
    "text": "here basically converts this string literal into a UTF8 JSON bite as a",
    "start": "3540319",
    "end": "3546000"
  },
  {
    "text": "readonly span again using that same technique to kind of cache those in the binary um and so what this does is",
    "start": "3546000",
    "end": "3552480"
  },
  {
    "text": "basically track when we've read the ID properties and the status properties off that item checks if we've read like a",
    "start": "3552480",
    "end": "3558240"
  },
  {
    "text": "true or false result for example and the key really is down here that when once we've checked that errors property on",
    "start": "3558240",
    "end": "3564799"
  },
  {
    "text": "the message and if it's false i.e there's no errors we can stop processing the rest of this JSON data and so even",
    "start": "3564799",
    "end": "3571440"
  },
  {
    "text": "further up the code uh here we do another check and we break out of this um other while loop here as",
    "start": "3571440",
    "end": "3579200"
  },
  {
    "text": "we're reading that data through the off the stream so we can short circuit in this the happy path where we generally",
    "start": "3579200",
    "end": "3584480"
  },
  {
    "text": "expect all items to have been indexed successfully and that's kind of the crux of the improvement there um I'll show",
    "start": "3584480",
    "end": "3591280"
  },
  {
    "text": "you the numbers before we switch to the last couple of slides so in the unhappy path where there are",
    "start": "3591280",
    "end": "3597280"
  },
  {
    "text": "errors on the response then we still see that there's a 67% reduction in execution time and still a very good",
    "start": "3597280",
    "end": "3604480"
  },
  {
    "text": "reduction in the overhead of the JSON bars we still have some allocations because we need to capture all of those",
    "start": "3604480",
    "end": "3610160"
  },
  {
    "text": "ids of the failed messages or the failed operations and so that that means we've saved 84% but not a not completely saved",
    "start": "3610160",
    "end": "3617280"
  },
  {
    "text": "our allocations but for the 99% case where we expect everything to be indexed successfully because of that early",
    "start": "3617280",
    "end": "3623280"
  },
  {
    "text": "exiting as we've read the first few tokens of JSON which is roughly the first I think 20 something bytes of JSON",
    "start": "3623280",
    "end": "3628480"
  },
  {
    "text": "data we exit really early so now we've we've exited in 200 nonds from that processing loop and we've got zero",
    "start": "3628480",
    "end": "3635280"
  },
  {
    "text": "allocations now because on that that sort of success path there's nothing to gather we don't actually need to allocate any instances of objects to",
    "start": "3635280",
    "end": "3641680"
  },
  {
    "text": "track that data and so very quickly we can get some real gains with that UTF8 JSON reading approach so to wrap up in",
    "start": "3641680",
    "end": "3648640"
  },
  {
    "text": "the last minute or two uh maybe you like what you've seen today and you want to go and get business buying how do you do",
    "start": "3648640",
    "end": "3653839"
  },
  {
    "text": "that i recommend starting with quick wins so think of services you know have performance issues within your",
    "start": "3653839",
    "end": "3658960"
  },
  {
    "text": "organization people complain how long they take or how much how many instances you have to run to process your data",
    "start": "3658960",
    "end": "3664799"
  },
  {
    "text": "that kind of thing then use a scientific approach to work out the gains um and",
    "start": "3664799",
    "end": "3670160"
  },
  {
    "text": "then change that into a monetary value so the main problem of going to a business owner and saying \"Hey I can remove all byte allocations on this code",
    "start": "3670160",
    "end": "3676799"
  },
  {
    "text": "path is that business owners probably don't really care what that means or might not understand but if you can show",
    "start": "3676799",
    "end": "3682240"
  },
  {
    "text": "them monetary value it tends to make uh a bigger difference so give them a cost to benefit ratio.\" So all of the stuff",
    "start": "3682240",
    "end": "3688000"
  },
  {
    "text": "I've shown you today plus other techniques went into one key microser we had that one that was dealing with 18",
    "start": "3688000",
    "end": "3693920"
  },
  {
    "text": "million messages a day and through various experiments with these small changes worked out we could save about",
    "start": "3693920",
    "end": "3699520"
  },
  {
    "text": "half of the allocation and roughly double the per instance throughput of those processing services and what that",
    "start": "3699520",
    "end": "3705359"
  },
  {
    "text": "led to is reducing one VM a year that we needed to run that particular microser as a container now $1,700 on its own",
    "start": "3705359",
    "end": "3712720"
  },
  {
    "text": "might not be worth the engineering effort that it took to introduce this but we had hundreds of microservices",
    "start": "3712720",
    "end": "3717760"
  },
  {
    "text": "doing very similar things and we could reapply this pattern and so this then can start to scale so these kind of",
    "start": "3717760",
    "end": "3723119"
  },
  {
    "text": "gains can really have an impact as you move to multiple instances where you're applying these same kind of techniques",
    "start": "3723119",
    "end": "3728720"
  },
  {
    "text": "over and over again so in summary um when you're doing this kind of stuff uh",
    "start": "3728720",
    "end": "3733920"
  },
  {
    "text": "don't assume don't make assumptions measure with profiling tools measure with benchmarking before and after doing",
    "start": "3733920",
    "end": "3739280"
  },
  {
    "text": "things be scientific so small changes each time have a theory validate the theory and move on uh focus on the hot",
    "start": "3739280",
    "end": "3746480"
  },
  {
    "text": "paths because they're the areas that you're going to get the most gain from uh don't copy memory so using things",
    "start": "3746480",
    "end": "3751760"
  },
  {
    "text": "like span to just slice your way into existing memory is a very good way of reducing um allocation overhead do use",
    "start": "3751760",
    "end": "3758240"
  },
  {
    "text": "array pools where it's appropriate to do so to avoid those shortlive buffer type allocations if you're doing anything",
    "start": "3758240",
    "end": "3763920"
  },
  {
    "text": "with sort of regular streaming of data then u off no I'm on um and uh yeah consider",
    "start": "3763920",
    "end": "3771440"
  },
  {
    "text": "system text JSON APIs as a quick swap out for Newtonoft JSON for some wins if you want to go deeper this book by",
    "start": "3771440",
    "end": "3777280"
  },
  {
    "text": "Conrad there's a new edition coming this year i highly recommend it it's about that thick it's about thousand pages in",
    "start": "3777280",
    "end": "3782720"
  },
  {
    "text": "English it's uh fantastic to read um it's also great to carry around as a weapon and uh it has all of the stuff",
    "start": "3782720",
    "end": "3790000"
  },
  {
    "text": "I've shown you today and more so that's it i'll give you the link to the slides",
    "start": "3790000",
    "end": "3795039"
  },
  {
    "text": "i don't have time for questions but I'm happy to kind of answer them off to the side afterwards thank you",
    "start": "3795039",
    "end": "3802119"
  }
]