[
  {
    "text": "well uh welcome everyone my name is salmanik bah if you're here for the f-sharp talk",
    "start": "8160",
    "end": "14480"
  },
  {
    "text": "this ain't it because this we're going to talk about kubernetes and kubernetes scheduler the the speaker was supposed",
    "start": "14480",
    "end": "19840"
  },
  {
    "text": "to do i know you're a bit disappointed but hopefully you won't be at the end of this session uh the speaker was supposed to be here i can't can't make it so i",
    "start": "19840",
    "end": "25920"
  },
  {
    "text": "just jumped in last minute to do this talk uh so if there's any mistakes or anything like that please forgive me but",
    "start": "25920",
    "end": "31840"
  },
  {
    "text": "hopefully we'll all have a good time i do have another talk are we good to share the screen",
    "start": "31840",
    "end": "39360"
  },
  {
    "text": "okay uh i do have an actual talk that i was supposed to do on in uh",
    "start": "42960",
    "end": "48079"
  },
  {
    "text": "in ndc on friday it's about running machine learning stuff in in the cloud at scale and how can you do it in a",
    "start": "48079",
    "end": "54640"
  },
  {
    "text": "cloud agnostic way and that's what i'll be talking about on friday so if you are into",
    "start": "54640",
    "end": "59920"
  },
  {
    "text": "devops cloud machine learning or one again to machine learning data science uh that talk you'll find useful",
    "start": "59920",
    "end": "67760"
  },
  {
    "text": "so first of all i'd like to say thank you all for coming and if you're not here for the f sharp talk if you're here",
    "start": "67760",
    "end": "73280"
  },
  {
    "text": "for the f sharp talk this is not it but hopefully so what we're going to talk about is kubernetes and how kubernetes",
    "start": "73280",
    "end": "80080"
  },
  {
    "text": "places its workloads in the cloud if you have to leave at any time feel free to leave but hopefully we",
    "start": "80080",
    "end": "85600"
  },
  {
    "text": "can we can uh we can go through it i'm not going to take the full hour so uh i",
    "start": "85600",
    "end": "90880"
  },
  {
    "text": "think it should be good uh let me just quickly play this and we'll start",
    "start": "90880",
    "end": "96400"
  },
  {
    "text": "how's everybody's conference so far first day good yeah i see some thumbs ups and nods is",
    "start": "96400",
    "end": "102320"
  },
  {
    "text": "about to get better well so yeah so i know i said so the title of",
    "start": "102320",
    "end": "108240"
  },
  {
    "text": "the actual talk is how does kubernetes play tetris with your containers",
    "start": "108240",
    "end": "113280"
  },
  {
    "text": "just as a show of hands just for for my interest how many people in this room have run",
    "start": "113280",
    "end": "119040"
  },
  {
    "text": "containers even if just like okay oh so pretty much pretty much everybody pretty much everybody awesome",
    "start": "119040",
    "end": "125680"
  },
  {
    "text": "how about played around with kubernetes yeah cool so we've got half the room and",
    "start": "125680",
    "end": "131200"
  },
  {
    "text": "if you've not played around with kubernetes great because this is we're going to talk about what kubernetes is we we won't go into much detail but if",
    "start": "131200",
    "end": "137680"
  },
  {
    "text": "you have any questions afterwards or during a talk feel free to ask i will answer and i think it will be even",
    "start": "137680",
    "end": "143040"
  },
  {
    "text": "better if we if you have any questions if something doesn't make sense please ask and i will answer during the talk i",
    "start": "143040",
    "end": "148319"
  },
  {
    "text": "think it's uh we'll keep interactive that way so my name is salman iqbal the best way to find me",
    "start": "148319",
    "end": "154239"
  },
  {
    "text": "is on twitter so you can search me a soulmanic bar uh people think because i can sing that's why my handle is soul",
    "start": "154239",
    "end": "160640"
  },
  {
    "text": "man but i cannot sing i just couldn't get a handle someone as well so i stuck with saw my neckbar and i work as a",
    "start": "160640",
    "end": "167599"
  },
  {
    "text": "kubernetes instructor with uh with learn k8 so that's a is the company that we provide kubernetes training and",
    "start": "167599",
    "end": "174160"
  },
  {
    "text": "i also work as an embellops engineer with a company called appia and envelops is all about data science for devops",
    "start": "174160",
    "end": "182080"
  },
  {
    "text": "and i i am part of community so i come from wales in within in uk anybody from wales here",
    "start": "182080",
    "end": "188239"
  },
  {
    "text": "yeah i thought so just me uh yeah so we run a cloud native community in wales with my friend and",
    "start": "188239",
    "end": "194319"
  },
  {
    "text": "you can find me on twitter i do actually have a youtube channel so you know what people say like and subscribe so if you",
    "start": "194319",
    "end": "200800"
  },
  {
    "text": "want to learn a bit more about uh kubernetes stuff i do post kubernetes stuff uh please forgive the",
    "start": "200800",
    "end": "206640"
  },
  {
    "text": "uh the cheesy thumbnails but you gotta do it for the for the for the titles uh yeah if you want to",
    "start": "206640",
    "end": "212959"
  },
  {
    "text": "learn a bit more about kubernetes that's it that's the place to check out uh before i start i just want to thank",
    "start": "212959",
    "end": "219280"
  },
  {
    "text": "my friend daniel palencic who's helped me put this together he's i worked with him in learn kate's",
    "start": "219280",
    "end": "225599"
  },
  {
    "text": "uh he's a bit of a legend well so since you've come to this unexpected talk i thought i should reward you not",
    "start": "225599",
    "end": "231599"
  },
  {
    "text": "just with knowledge but perhaps with something else i have one of these it's a retro game",
    "start": "231599",
    "end": "238000"
  },
  {
    "text": "that apparently you can plug into the computer and play 200 games on it so that's so this is this is up for grabs",
    "start": "238000",
    "end": "244000"
  },
  {
    "text": "just because you come to this talk i think i should give this to you but the question is in front of you without",
    "start": "244000",
    "end": "249680"
  },
  {
    "text": "googling can anybody tell when this ozo spectrum open",
    "start": "249680",
    "end": "255280"
  },
  {
    "text": "roughly get have a guess have a crack whoever's closest gets this",
    "start": "255280",
    "end": "261280"
  },
  {
    "text": "92 any any other takers go for it 95 anything else 80 anything else 88 88 what what are we",
    "start": "261280",
    "end": "269520"
  },
  {
    "text": "saying 88 boy 88 which month august 92 what month",
    "start": "269520",
    "end": "275759"
  },
  {
    "text": "yeah february oh it's december 1990 so who's the closest",
    "start": "275759",
    "end": "281680"
  },
  {
    "text": "i think you are right because you said august",
    "start": "281680",
    "end": "286720"
  },
  {
    "text": "and you said yeah there you go so you should not be disappointed that you came to this talk you won this i'm",
    "start": "286720",
    "end": "291840"
  },
  {
    "text": "happy ronda flows for our winner here yeah we're getting somewhere awesome",
    "start": "291840",
    "end": "297759"
  },
  {
    "text": "so here's the agenda i'll quickly explain what kubernetes is and",
    "start": "297759",
    "end": "303600"
  },
  {
    "text": "why perhaps it's useful and i'll just say if you have any questions anyone please do ask this talk is specifically about how does",
    "start": "303600",
    "end": "309840"
  },
  {
    "text": "kubernetes place your workloads in the cluster so kubernetes has a cluster of machines they you can run your",
    "start": "309840",
    "end": "315360"
  },
  {
    "text": "containerized workloads on so we'll talk about that and what you can do to",
    "start": "315360",
    "end": "320639"
  },
  {
    "text": "move your workloads in a more efficient manner so that's it's a little bit more advanced but hopefully we'll all go",
    "start": "320639",
    "end": "326080"
  },
  {
    "text": "through it through this together and we'll cover a lot of stuff if you have any questions i'll put q a in here but i",
    "start": "326080",
    "end": "332800"
  },
  {
    "text": "prefer if you ask questions while we're going through it so that's what we'll do but you might start",
    "start": "332800",
    "end": "337919"
  },
  {
    "text": "to think that uh why is this important for me i don't even use kubernetes",
    "start": "337919",
    "end": "344000"
  },
  {
    "text": "if you don't use kubernetes hopefully some there's some information for you to learn but if you do use kubernetes open",
    "start": "344000",
    "end": "349520"
  },
  {
    "text": "ai people heard of open eye open eye it's like yeah it's a it's a it's an ai",
    "start": "349520",
    "end": "355440"
  },
  {
    "text": "institution they do a lot of work uh in in the ai field so they fixed the",
    "start": "355440",
    "end": "361360"
  },
  {
    "text": "performance issues they have a cluster with 2500 nodes so that's a that's pretty beefy cluster of machines of",
    "start": "361360",
    "end": "368319"
  },
  {
    "text": "anything to be honest and they have performance issues and they fixed it by using kubernetes scheduler and how to",
    "start": "368319",
    "end": "374240"
  },
  {
    "text": "influence it so well before we start i know most of you in this room said you already use kubernetes but i think it's it's",
    "start": "374240",
    "end": "380080"
  },
  {
    "text": "beneficial for everyone to give a 30 second recap on on what cuban on what containers are so uh i'm sorry i meant",
    "start": "380080",
    "end": "387360"
  },
  {
    "text": "containers what containers are um containers i've just seen have you know basically in the last few years in",
    "start": "387360",
    "end": "394240"
  },
  {
    "text": "the five six or seven or the last six seven years we've seen a lot of uptake in containers a lot of people using",
    "start": "394240",
    "end": "400000"
  },
  {
    "text": "containers to run their workloads in on the locally and local machines in in dev in production wherever they want",
    "start": "400000",
    "end": "407600"
  },
  {
    "text": "they might want to run their workloads predominantly because it gives you reproducibility if the i5 a container",
    "start": "407600",
    "end": "413039"
  },
  {
    "text": "that i can run on my laptop i can take the same container that runs on my laptop i can run it on a different",
    "start": "413039",
    "end": "418160"
  },
  {
    "text": "machine operating system dependent of course and if there's any",
    "start": "418160",
    "end": "423280"
  },
  {
    "text": "dependencies i can include it in so but in order to run something uh i need to have runtime",
    "start": "423280",
    "end": "429280"
  },
  {
    "text": "in in our case i just put because these slides are not for ndc that's why there's a java virtual machine i would",
    "start": "429280",
    "end": "435440"
  },
  {
    "text": "have put a net framework a.net core whatever it was if it was for if i knew i was going to give",
    "start": "435440",
    "end": "440800"
  },
  {
    "text": "this slide by the way i wasn't supposed to give this presentation but we're here which is good so imagine there's a",
    "start": "440800",
    "end": "446000"
  },
  {
    "text": "runtime any runtime uh you could have python you could have jvm you could have.net",
    "start": "446000",
    "end": "451120"
  },
  {
    "text": "whatever it might be and what you do is you take your artifacts take all your files that you have you know all your",
    "start": "451120",
    "end": "456960"
  },
  {
    "text": "dotnet uh files and and all the artifacts and you put it alongside this uh runtime and you stick",
    "start": "456960",
    "end": "464560"
  },
  {
    "text": "your any external dependencies so any packages that you want to install using new get uh you put them all together and",
    "start": "464560",
    "end": "471520"
  },
  {
    "text": "you you and you create this image which you can run over and over again so this is what is now called a container so jce",
    "start": "471520",
    "end": "478080"
  },
  {
    "text": "is java cryptographic extension for uh for for for linux canter windows container linux container outside it",
    "start": "478080",
    "end": "485599"
  },
  {
    "text": "doesn't make any difference the outside the daemon the docker or anything that runs it outside it doesn't look any",
    "start": "485599",
    "end": "491919"
  },
  {
    "text": "different but when you peek inside it looks a bit different but now we have a container in the container we have",
    "start": "491919",
    "end": "497280"
  },
  {
    "text": "everything so i can take a container and i can create these containers using what's called a docker file if you already use container you know and it's",
    "start": "497280",
    "end": "503919"
  },
  {
    "text": "just a set of instructions on how to build the application and run it and once it's built i can take that and run",
    "start": "503919",
    "end": "510400"
  },
  {
    "text": "the container i can do docker run on my laptop i can do docker run in a production server i can do it in the cloud whatever i want to do everywhere",
    "start": "510400",
    "end": "517200"
  },
  {
    "text": "it works if i have one container it's easy for me to manage i can start i can stop i can",
    "start": "517200",
    "end": "522640"
  },
  {
    "text": "delete i can do whatever i want imagine there's your applications nowadays have microservices",
    "start": "522640",
    "end": "528959"
  },
  {
    "text": "and microservices typically not typically you know there's a lot of services that you have to look",
    "start": "528959",
    "end": "534560"
  },
  {
    "text": "after and you have a lot of services then you have a lot of containers and if there's a lot of containers",
    "start": "534560",
    "end": "540320"
  },
  {
    "text": "they will crash at some point i don't know there's a if you're like me perhaps you left a bug",
    "start": "540320",
    "end": "545360"
  },
  {
    "text": "in the code and it will crash and if it crashes you have to start the container back up again managing one container is okay but",
    "start": "545360",
    "end": "551920"
  },
  {
    "text": "managing multiple containers is kind of hard i mean we wouldn't want to wake up at 3am in the morning and restart a",
    "start": "551920",
    "end": "557200"
  },
  {
    "text": "container right would you want to wake up at 3am no so why don't we let something else deal",
    "start": "557200",
    "end": "565200"
  },
  {
    "text": "with restarting of the containers and this is where the the container schedulers uh container orchestrators",
    "start": "565200",
    "end": "570560"
  },
  {
    "text": "come in kubernetes being one of them and there's other other ones as well hashicorp nomad",
    "start": "570560",
    "end": "575600"
  },
  {
    "text": "and whatnot and what i can what we can kubernetes allows us to do is basically automate",
    "start": "575600",
    "end": "581839"
  },
  {
    "text": "this looking after of the container so if it crashes restart it if i need to scale up scale it up based on the",
    "start": "581839",
    "end": "588399"
  },
  {
    "text": "traffic that's coming in so i can define some metrics and i say if people can't uh if the traffic isn't getting served",
    "start": "588399",
    "end": "594560"
  },
  {
    "text": "within i don't know 15 seconds from a container scale out and instead of running two containers",
    "start": "594560",
    "end": "600000"
  },
  {
    "text": "run four five six seven containers depending on what the traffic is that's that's really good and also what",
    "start": "600000",
    "end": "605920"
  },
  {
    "text": "kubernetes allows you to do is run uh your resources quite well as in it will",
    "start": "605920",
    "end": "611040"
  },
  {
    "text": "take a machine and will try and fit as much application as as many containers as it can inside there inside that machine so",
    "start": "611040",
    "end": "618399"
  },
  {
    "text": "what is kubernetes is anybody happy with that so far are we good right excellent that container can be an f sharp",
    "start": "618399",
    "end": "624320"
  },
  {
    "text": "container as well so if you're here for f sharp this also applies right so that container can be an f sharp container",
    "start": "624320",
    "end": "630800"
  },
  {
    "text": "so imagine you have a cluster a cluster of machines and in kubernetes what you",
    "start": "630800",
    "end": "635920"
  },
  {
    "text": "do is you when you have a cluster of machines you make one in charge of everything else and the one on the right",
    "start": "635920",
    "end": "642720"
  },
  {
    "text": "or to your right is the is the kubernetes uh you can see the kubernetes logo is there that's called the control",
    "start": "642720",
    "end": "648480"
  },
  {
    "text": "plane that's the thing that's the brains of the of kubernetes that controls everything and the other nodes are just",
    "start": "648480",
    "end": "654959"
  },
  {
    "text": "worker nodes that's what the worker nodes are called and we run containers in those nodes and these nodes could be",
    "start": "654959",
    "end": "660959"
  },
  {
    "text": "anything to be honest they could be a linux machine there could be a windows server there could be a raspberry pi as",
    "start": "660959",
    "end": "666480"
  },
  {
    "text": "long as it matches the uh the requirement that the minimum requirement for kubernetes we can run anything inside it",
    "start": "666480",
    "end": "675120"
  },
  {
    "text": "and these machines don't have to be the same size you can have a bigger machine or smaller machine",
    "start": "675120",
    "end": "680640"
  },
  {
    "text": "so basically i can create a cluster in which i can deploy the containers now",
    "start": "680640",
    "end": "686000"
  },
  {
    "text": "if you're running your application currently uh and sometimes you probably have to deploy an application where you know",
    "start": "686000",
    "end": "692959"
  },
  {
    "text": "container or non-container whatever it might be you might have to uh rdp onto a server",
    "start": "692959",
    "end": "698560"
  },
  {
    "text": "people already be on to servers here like remote desktop connection right so you rdp run the thing in run the",
    "start": "698560",
    "end": "704640"
  },
  {
    "text": "application and then if you if you're doing a release okay where's the credentials try and figure out let me",
    "start": "704640",
    "end": "710160"
  },
  {
    "text": "just go and log into that server and deploy it in kubernetes we don't do any of that stuff we treat the whole thing",
    "start": "710160",
    "end": "716399"
  },
  {
    "text": "as one virtual machine because right now you might have like 15 or 20 or 30 however many virtual",
    "start": "716399",
    "end": "722000"
  },
  {
    "text": "machines you have there's one specific specific server for database there's one",
    "start": "722000",
    "end": "727040"
  },
  {
    "text": "specific server for where you run your message queues there's one specific server where you run your web apps so",
    "start": "727040",
    "end": "732800"
  },
  {
    "text": "there's a lot happening and you have to kind of figure out okay in this box i deploy this in that box i deploy that and that all that's happening but in",
    "start": "732800",
    "end": "739440"
  },
  {
    "text": "kubernetes we say nope everything is just one virtual machine so i'm just going to submit",
    "start": "739440",
    "end": "744720"
  },
  {
    "text": "my container to kubernetes and i'll let kubernetes decide where to place the container so",
    "start": "744720",
    "end": "750079"
  },
  {
    "text": "kubernetes is playing tetris so i submit this request and i'll show you what the request looks like looks",
    "start": "750079",
    "end": "756480"
  },
  {
    "text": "like in a second because i think it's uh might beneficial using yaml files",
    "start": "756480",
    "end": "761519"
  },
  {
    "text": "so all i have is a container and that container is not it's got nothing in it it's just a simple web app right it's a",
    "start": "761519",
    "end": "767639"
  },
  {
    "text": "net.net core web app we're just running on there and all i do is submit to the kubernetes",
    "start": "767639",
    "end": "773279"
  },
  {
    "text": "and say humanities can you deploy this on your cluster kubernetes takes that request and it goes and it scans",
    "start": "773279",
    "end": "779600"
  },
  {
    "text": "all the nodes to see which node are best fit now which nodes are best fit is what",
    "start": "779600",
    "end": "786800"
  },
  {
    "text": "this talk is about how does it decide which nodes are best fit to deploy now this comes in it gets deployed let's",
    "start": "786800",
    "end": "793920"
  },
  {
    "text": "say on the the let's call this one node one right one on the left is called node one now let me ask you a question",
    "start": "793920",
    "end": "800480"
  },
  {
    "text": "if some if we want to deploy another replica because you know we've got a lot of traffic because we've done a release",
    "start": "800480",
    "end": "806480"
  },
  {
    "text": "uh people are excited to buy our products so we'll come to our website",
    "start": "806480",
    "end": "811519"
  },
  {
    "text": "and then we say actually you know what we should scale up kubernetes says we need to scale up we need to run replicas we we don't want to run the same thing",
    "start": "811519",
    "end": "818320"
  },
  {
    "text": "we run multiple replicas if you were the kubernetes scheduler so that's the bit that decides where",
    "start": "818320",
    "end": "824800"
  },
  {
    "text": "the container gets deployed would you deploy this on node one or would you deploy this on node two what do you",
    "start": "824800",
    "end": "830240"
  },
  {
    "text": "think shout out node one who thinks node one node two",
    "start": "830240",
    "end": "835440"
  },
  {
    "text": "oh yeah so a lot of people think node two and nobody thinks node one which is excellent so i mean there's a reason why",
    "start": "835440",
    "end": "841279"
  },
  {
    "text": "kubernetes does that because if one of the nodes fails and both of the replicas on the same",
    "start": "841279",
    "end": "847040"
  },
  {
    "text": "node we lose both replicas so it tries to see okay can i does it do have enough resources can i deploy it should i",
    "start": "847040",
    "end": "853360"
  },
  {
    "text": "spread it out and that's what it does it spreads it out and puts it in and then another request comes in this",
    "start": "853360",
    "end": "859040"
  },
  {
    "text": "thing requires not enough memory as much memory as cpu by the way the size of the of the box represents how much memory",
    "start": "859040",
    "end": "865199"
  },
  {
    "text": "and cpu they require you know all apps have different requirements that's what the box represents so the next request",
    "start": "865199",
    "end": "870880"
  },
  {
    "text": "comes in all right next request comes in and then kubernetes looks around now where would you deploy this node one",
    "start": "870880",
    "end": "878720"
  },
  {
    "text": "yeah no two well they're both similar so what kubernetes has to decide and we'll",
    "start": "878720",
    "end": "883760"
  },
  {
    "text": "see how it decides and eventually deploys another one comes again we check see what's going on and it keeps",
    "start": "883760",
    "end": "889199"
  },
  {
    "text": "deploying and another one comes it just basically keeps paying tetris the aim of kubernetes scheduler is to pack all the",
    "start": "889199",
    "end": "896160"
  },
  {
    "text": "containers on every bit of resource as tightly as possible because what kubernetes promise is that",
    "start": "896160",
    "end": "903760"
  },
  {
    "text": "if you have your res if you have your resources as in your virtual machines or actual machines or or servers",
    "start": "903760",
    "end": "909519"
  },
  {
    "text": "we don't want to waste any resource i know there's a lot of research out there and people say that you typically",
    "start": "909519",
    "end": "917199"
  },
  {
    "text": "depending on where the research comes from 70 of servers are idle most of the time would you",
    "start": "917199",
    "end": "923600"
  },
  {
    "text": "agree with that some people are nodding their heads now that's a bit of a waste of money isn't it like seventy percent of the service",
    "start": "923600",
    "end": "930000"
  },
  {
    "text": "so kubernetes says no i'm not gonna let you waste that money uh let's save some money and then and it does that now how",
    "start": "930000",
    "end": "937440"
  },
  {
    "text": "does kubernetes how do i submit this stuff to kubernetes what does it look like what does the request actually look like",
    "start": "937440",
    "end": "944079"
  },
  {
    "text": "how do i tell kubernetes i want you to run this much replicas of this application",
    "start": "944079",
    "end": "950160"
  },
  {
    "text": "and uh this is the name of the container that i want you to run how do we do that now if you already use kubernetes you've",
    "start": "950160",
    "end": "955759"
  },
  {
    "text": "come across this but if you haven't i'm going to show you a file let's just go in here",
    "start": "955759",
    "end": "962000"
  },
  {
    "text": "sorry this is a very last minute because this is a very last minute talk",
    "start": "962000",
    "end": "968079"
  },
  {
    "text": "now in kubernetes we use yaml files people use yaml files here have everybody use yammerfalls yeah key value",
    "start": "968079",
    "end": "974720"
  },
  {
    "text": "pairs all we got is uh key value pairs so you can see there's a lot of stuff a lot of information in here and there's a lot",
    "start": "974720",
    "end": "982320"
  },
  {
    "text": "and this is one of the reasons why kubernetes is a bit complex because you have to create all these files from uh",
    "start": "982320",
    "end": "987600"
  },
  {
    "text": "for your application to be deployed and we'll just ignore most of the stuff in",
    "start": "987600",
    "end": "992639"
  },
  {
    "text": "here and we'll just focus on the important bits the kind of resource we want to create and in our",
    "start": "992639",
    "end": "998880"
  },
  {
    "text": "case we say kind is deployment a deployment is a recipe of what your application looks like yeah how many",
    "start": "998880",
    "end": "1004800"
  },
  {
    "text": "replicas you want to run so this kind could be a different kind it could be a secret so kubernetes has some built-in kinds and you can actually customize it",
    "start": "1004800",
    "end": "1012480"
  },
  {
    "text": "to create your own kinds if you wanted to you can create a sql server so if you submit that request to kubernetes it",
    "start": "1012480",
    "end": "1018720"
  },
  {
    "text": "will create you a sql server but that's that's just an example um well the other bit in here is",
    "start": "1018720",
    "end": "1024160"
  },
  {
    "text": "replicas how many replicas i want to run i want to run three replicas so if there's a lot of traffic instead of",
    "start": "1024160",
    "end": "1029199"
  },
  {
    "text": "sending it to one container i've sent it to multiple containers now this works well if the application is stateless if",
    "start": "1029199",
    "end": "1035839"
  },
  {
    "text": "the application has state in it it's kind of hard to do because where do you make sure the request goes in so",
    "start": "1035839",
    "end": "1040880"
  },
  {
    "text": "kubernetes works well with uh with stateless stuff but that's a different topic altogether these labels are just",
    "start": "1040880",
    "end": "1046959"
  },
  {
    "text": "like information that other things in kubernetes use to root the traffic the important bit in here is this bit called",
    "start": "1046959",
    "end": "1054000"
  },
  {
    "text": "the image that's the image that you and i built you know the docker container images that we built and then this got some",
    "start": "1054000",
    "end": "1060000"
  },
  {
    "text": "versioning and it says this is nginx nginx is a web server think is a very uh very very lightweight like linux",
    "start": "1060000",
    "end": "1068000"
  },
  {
    "text": "based uh linux based container and there's a version in it 1142 or whatever it is and",
    "start": "1068000",
    "end": "1074320"
  },
  {
    "text": "basically this is how we take this file and we can submit this to kubernetes using this command line tool called cube",
    "start": "1074320",
    "end": "1080480"
  },
  {
    "text": "ctl or queue control or some people call the cube cuddle but uh yeah you take you take this file and you submit that to",
    "start": "1080480",
    "end": "1086880"
  },
  {
    "text": "kubernetes cluster kubernetes cluster receives their request and he sees oh this is the container you want to run",
    "start": "1086880",
    "end": "1092080"
  },
  {
    "text": "these are the replicas you want to run so that's any questions on this is everybody happy with that so far cool",
    "start": "1092080",
    "end": "1099200"
  },
  {
    "text": "so let me just go back if i can find my slides where they go yeah there there they are",
    "start": "1099200",
    "end": "1107720"
  },
  {
    "text": "so so file that i showed you we'll take that you and i let's say we'll pick one of us",
    "start": "1110559",
    "end": "1116000"
  },
  {
    "text": "right one of us wrote that file and says we we want to run this application",
    "start": "1116000",
    "end": "1121200"
  },
  {
    "text": "and what we say is we create this deployment file that i just showed you and we say we want to run two replicas",
    "start": "1121200",
    "end": "1127200"
  },
  {
    "text": "not one we run two replicas three whatever it might be so we send this request to kubernetes",
    "start": "1127200",
    "end": "1132720"
  },
  {
    "text": "control plane as i was showing in earlier on kubernetes receives that request and the control plane is made up of different",
    "start": "1132720",
    "end": "1139360"
  },
  {
    "text": "components and all the components are here so the request goes to what's called the",
    "start": "1139360",
    "end": "1145760"
  },
  {
    "text": "api server again it's it's uh it's a component and kubernetes runs inside a container",
    "start": "1145760",
    "end": "1151520"
  },
  {
    "text": "most all the kubernetes component also runs inside a container in kubernetes so that's running in in this called the api",
    "start": "1151520",
    "end": "1158400"
  },
  {
    "text": "server so when you from command line you submit the request to the api server it receives a request and it takes that",
    "start": "1158400",
    "end": "1164880"
  },
  {
    "text": "request and it stores all the information that i showed you in the yammer file the deployment the replicas",
    "start": "1164880",
    "end": "1171039"
  },
  {
    "text": "and all that in the bit on the right that's that's a logo there it's called the fcd this bit here it's called etcd it's a",
    "start": "1171039",
    "end": "1177280"
  },
  {
    "text": "database it's a key value database think real readers but it works very well in distributed",
    "start": "1177280",
    "end": "1182960"
  },
  {
    "text": "fashion so you can run uh three or four of them or five of them so if one crashes uh it's called etcd as lcd or",
    "start": "1182960",
    "end": "1191600"
  },
  {
    "text": "some people call it etc so that's that's the bit that stores all the information now that is all the state of your",
    "start": "1191600",
    "end": "1198960"
  },
  {
    "text": "kubernetes cluster as in what's running uh what images are running how many replicas do i have uh",
    "start": "1198960",
    "end": "1204960"
  },
  {
    "text": "what services do i have what ingresses do i have you know all this stuff the idea being that if you take a backup",
    "start": "1204960",
    "end": "1211039"
  },
  {
    "text": "off that lcd which is just key value pairs and for some some reason your cluster",
    "start": "1211039",
    "end": "1216400"
  },
  {
    "text": "gets deleted but you have a backup of the lcd somewhere you can go and restore from that and you get everything back",
    "start": "1216400",
    "end": "1223520"
  },
  {
    "text": "so the point is the kubernetes try to do is every time all the time it's just",
    "start": "1223520",
    "end": "1228640"
  },
  {
    "text": "reconciliating against what's written in xcd so every time we change the state kubernetes says i have to",
    "start": "1228640",
    "end": "1235600"
  },
  {
    "text": "act on that change and make it happen so anyway so that's fcd on the right and on the bottom you can see there's a",
    "start": "1235600",
    "end": "1241679"
  },
  {
    "text": "component called controller manager and there's another component called scheduler so if you ever wanted to learn all the main",
    "start": "1241679",
    "end": "1248480"
  },
  {
    "text": "major components there's there's more components but just like main components in kubernetes control plane we have four",
    "start": "1248480",
    "end": "1254799"
  },
  {
    "text": "main components in here i'm talking about api server this is control plane by the way uh and at cd",
    "start": "1254799",
    "end": "1260960"
  },
  {
    "text": "as you can imagine api server all it does is it receives requests on from everyone and then we have a controller",
    "start": "1260960",
    "end": "1266240"
  },
  {
    "text": "manager controller managers at the point for controller manager and you can you can see now all these bits are also",
    "start": "1266240",
    "end": "1272159"
  },
  {
    "text": "based on microservices way of thinking right so this control manager does one and one thing only there's different",
    "start": "1272159",
    "end": "1278480"
  },
  {
    "text": "types of controller manager but in our case we'll say it looks at that request and says oh how many replicas do you want or three replicas and it goes and",
    "start": "1278480",
    "end": "1285520"
  },
  {
    "text": "creates those replicas in fcd actually writes them out one by one part one part two and part three",
    "start": "1285520",
    "end": "1292159"
  },
  {
    "text": "so imagine we want to deploy this bit called the scheduler oh sorry this this application this this red page it's got",
    "start": "1292159",
    "end": "1298080"
  },
  {
    "text": "nothing in it it's just an html page it's just completely red there's nothing there's nothing specific nothing special just red page but this",
    "start": "1298080",
    "end": "1305360"
  },
  {
    "text": "could be anything this could be your application f sharp c sharp whatever it might be now",
    "start": "1305360",
    "end": "1310640"
  },
  {
    "text": "the contour controller manager is basically listening to the changes so we submit the request the request goes to",
    "start": "1310640",
    "end": "1316480"
  },
  {
    "text": "the api server api server stores their request in ncd and controller manager is listening to",
    "start": "1316480",
    "end": "1322320"
  },
  {
    "text": "xcd all the time and it says oh right there is a deployment for me with two",
    "start": "1322320",
    "end": "1327840"
  },
  {
    "text": "pods in it pod by the way is uh i should explain",
    "start": "1327840",
    "end": "1334000"
  },
  {
    "text": "you and i talk about containers kubernetes does an abstraction it says i'm not going to talk about containers",
    "start": "1334000",
    "end": "1339520"
  },
  {
    "text": "i'm going to talk about pod and pod could be just one container or collection of containers so in in a pod",
    "start": "1339520",
    "end": "1345840"
  },
  {
    "text": "you can run multiple containers or you can run one container if you like most of the times you run one container",
    "start": "1345840",
    "end": "1351440"
  },
  {
    "text": "but there are some use cases when you want to run multiple containers how are we doing by the way is everybody",
    "start": "1351440",
    "end": "1356640"
  },
  {
    "text": "okay yeah are we good cool nobody missing f-sharp i hope yeah",
    "start": "1356640",
    "end": "1361840"
  },
  {
    "text": "excellent let's carry on so the request comes in the controller manager goes okay i know what i need to do i will",
    "start": "1361840",
    "end": "1367039"
  },
  {
    "text": "create you these two these two pods i'll create those parts for you but actually it doesn't create the pods it",
    "start": "1367039",
    "end": "1373039"
  },
  {
    "text": "just writes the information in that cd just says okay i'm going to write this information in xcd because at",
    "start": "1373039",
    "end": "1379120"
  },
  {
    "text": "cd is what we said is where all the information about this data or cluster is",
    "start": "1379120",
    "end": "1384480"
  },
  {
    "text": "and then what happens is once you can see we say there's two parts part one and part two and they",
    "start": "1384480",
    "end": "1390159"
  },
  {
    "text": "have a status they could be running they could be pending they could be whatever and as",
    "start": "1390159",
    "end": "1395360"
  },
  {
    "text": "soon as the status changes depending uh there's the other component called the scheduler is listening in to for that",
    "start": "1395360",
    "end": "1401440"
  },
  {
    "text": "specific change for that specific status and it goes oh there's a pot that's going pod that's",
    "start": "1401440",
    "end": "1407039"
  },
  {
    "text": "gone pending it's my job now to figure out where these pods have to be run on the cluster",
    "start": "1407039",
    "end": "1414480"
  },
  {
    "text": "so the scheduler then goes around and scans the cluster and says okay i'll deploy this uh in this part or that part",
    "start": "1414480",
    "end": "1421200"
  },
  {
    "text": "or the we'll try and schedule it now so what we'll do is we'll focus on what the scheduler does",
    "start": "1421200",
    "end": "1426640"
  },
  {
    "text": "just just uh information by the way i stuck my uh twitter handle in saw manic bar if you if you have any",
    "start": "1426640",
    "end": "1432880"
  },
  {
    "text": "questions best way to find me is there or the comment section in youtube",
    "start": "1432880",
    "end": "1439600"
  },
  {
    "text": "so there's two phases the scheduler does scheduler's got two things it's got the scheduling phase",
    "start": "1439840",
    "end": "1445840"
  },
  {
    "text": "and a binding phase uh we're going in a bit of a bit of detail but hopefully it's going to be useful for you",
    "start": "1445840",
    "end": "1451520"
  },
  {
    "text": "we'll play a game as well in a little while we'll come we'll come there's a game that comes up we can all play together",
    "start": "1451520",
    "end": "1456960"
  },
  {
    "text": "so um there's in the scheduling phase the scheduler decides the best node for the",
    "start": "1456960",
    "end": "1462799"
  },
  {
    "text": "pod and the binding phase that's where he says i'm going to assign that part to the node pod is a container or a",
    "start": "1462799",
    "end": "1468640"
  },
  {
    "text": "collection of containers that's what a pod is now as soon as the pod is created",
    "start": "1468640",
    "end": "1474799"
  },
  {
    "text": "by the controller manager so the first component that we talked about before um it also is added to the scheduler's",
    "start": "1474799",
    "end": "1482000"
  },
  {
    "text": "queue so everything gets into added to queue kind of like you know how we have this queuing stuff so pod gets added to",
    "start": "1482000",
    "end": "1488000"
  },
  {
    "text": "the scheduler queue so it picks up the first one and starts doing what it needs to do and then to decide if",
    "start": "1488000",
    "end": "1495039"
  },
  {
    "text": "a node a worker node so i could have two i could have five i could have 2500 like the open api people did",
    "start": "1495039",
    "end": "1502559"
  },
  {
    "text": "to decide if a node is suitable for a pod the scheduler scans and filters",
    "start": "1502559",
    "end": "1507840"
  },
  {
    "text": "the nodes and that's important in the second i'll and i'll explain but filtering sometimes is not enough in",
    "start": "1507840",
    "end": "1515200"
  },
  {
    "text": "most times filtering is not enough because you could end up with multiple nodes multiple worker nodes where you",
    "start": "1515200",
    "end": "1520799"
  },
  {
    "text": "can deploy this pod so how do you decide and this is where the scoring comes in",
    "start": "1520799",
    "end": "1525840"
  },
  {
    "text": "so each node then gets given a score and we'll look at what scoring is",
    "start": "1525840",
    "end": "1532240"
  },
  {
    "text": "and after scoring is basically the the notifier and the binding",
    "start": "1532240",
    "end": "1537360"
  },
  {
    "text": "policies the deal is pretty basically done by that after that time so score and once the scoring is done you just",
    "start": "1537360",
    "end": "1543200"
  },
  {
    "text": "need to notify some components to say okay this is where it's going and and then basically we you know bind bind",
    "start": "1543200",
    "end": "1550400"
  },
  {
    "text": "the container to the bind the pod to that node",
    "start": "1550400",
    "end": "1555120"
  },
  {
    "text": "and then this is this is where the binding happens binding is yet another object all that says if you look at it",
    "start": "1555679",
    "end": "1561679"
  },
  {
    "text": "it says pod one uh runs on node worker one or worker two whatever it might be that's all that",
    "start": "1561679",
    "end": "1567760"
  },
  {
    "text": "does again just like everything else information about your application about the state of your cluster is all stored",
    "start": "1567760",
    "end": "1574559"
  },
  {
    "text": "in this where do you think it's all stored let me ask you a question",
    "start": "1574559",
    "end": "1580159"
  },
  {
    "text": "in the in the lcd database right so at cd database is the bit uh that stores all the information so every information",
    "start": "1580159",
    "end": "1587120"
  },
  {
    "text": "about your not the application information because that's that belongs to application about the infrastructure",
    "start": "1587120",
    "end": "1592159"
  },
  {
    "text": "where your containers are running what what you know what are they listening to that gets stored in ncd database",
    "start": "1592159",
    "end": "1598480"
  },
  {
    "text": "so now what we've done is we have scheduled the pod we've scheduled the port and we said",
    "start": "1598480",
    "end": "1604159"
  },
  {
    "text": "okay this this this part is going to run on worker node one or worker node two",
    "start": "1604159",
    "end": "1611279"
  },
  {
    "text": "but so far nothing has actually been created we actually haven't run the pod the pod is",
    "start": "1611279",
    "end": "1616480"
  },
  {
    "text": "not running yet we've just done we've just updated lcd cluster all we've done is taken actually",
    "start": "1616480",
    "end": "1622720"
  },
  {
    "text": "say okay uh there was a pod that was pending but now it's scheduled because we figure out we",
    "start": "1622720",
    "end": "1627919"
  },
  {
    "text": "update this information and say okay pod one you're going to work at one port two you're going to work a two",
    "start": "1627919",
    "end": "1633520"
  },
  {
    "text": "so who actually creates the pod well you know i said we don't really log in and start or stop the containers",
    "start": "1633520",
    "end": "1640559"
  },
  {
    "text": "so every time there's a cluster in the cluster you have the control plane in which all the",
    "start": "1640559",
    "end": "1646080"
  },
  {
    "text": "other components were and then we have the the worker nodes and worker nodes",
    "start": "1646080",
    "end": "1651760"
  },
  {
    "text": "there's a component called cubelet again it's just a it's just a",
    "start": "1651760",
    "end": "1657200"
  },
  {
    "text": "process it's it's a binary um it's just a bunch of code that runs in a",
    "start": "1657200",
    "end": "1662960"
  },
  {
    "text": "container on every on every worker node and all the tasks for cubelet is to",
    "start": "1662960",
    "end": "1669520"
  },
  {
    "text": "speak to the control plane and say is there any part for me to be run on this worker node that's all it",
    "start": "1669520",
    "end": "1675679"
  },
  {
    "text": "asks so all he's asking is is any parts for me to run so if there's",
    "start": "1675679",
    "end": "1680720"
  },
  {
    "text": "three if if one of them gets deployed if one of the pod is assigned to worker one basically we'll take that request",
    "start": "1680720",
    "end": "1687760"
  },
  {
    "text": "and it will cubelet will call container uh runtime a container runtime",
    "start": "1687760",
    "end": "1693039"
  },
  {
    "text": "could be anything it could be docker it could be container d but right now let's say it's docker",
    "start": "1693039",
    "end": "1698399"
  },
  {
    "text": "and all docker container runtime in our case is docker and all docker well it's a docker demon",
    "start": "1698399",
    "end": "1704320"
  },
  {
    "text": "and all it does is does what we do with the containers docker run it just runs the container all he's doing is running",
    "start": "1704320",
    "end": "1709360"
  },
  {
    "text": "the container so now the container is up and running it's all good but let's take an example",
    "start": "1709360",
    "end": "1715600"
  },
  {
    "text": "why is this important right what am i talking about imagine we created deployment right you",
    "start": "1715600",
    "end": "1720640"
  },
  {
    "text": "know a deployment file i showed you before information and deployment file is how many replicas i want to run and uh",
    "start": "1720640",
    "end": "1727760"
  },
  {
    "text": "and what container am i running and you know some some extra information that we need to fill in so we just we write that",
    "start": "1727760",
    "end": "1733360"
  },
  {
    "text": "yaml file in our case we give it some requirements we say okay i need to run one replicas",
    "start": "1733360",
    "end": "1739120"
  },
  {
    "text": "and i wanted to use a gpu you know gpu we run like machine learning stuff or your gaming stuff runs on gpus i don't",
    "start": "1739120",
    "end": "1745279"
  },
  {
    "text": "want to deploy in cpu uh i want to deploy on a gpu and this is the memory and and",
    "start": "1745279",
    "end": "1750640"
  },
  {
    "text": "and the uh the cpu requirement for for the container that needs to be run",
    "start": "1750640",
    "end": "1756480"
  },
  {
    "text": "and this is our cluster i've got 12 worker nodes right i've got 12 worker nodes some nodes are already full so you",
    "start": "1756480",
    "end": "1763520"
  },
  {
    "text": "can see the ones with yellow and the if you see in the in the top row the green one is half half full the yellow one is",
    "start": "1763520",
    "end": "1770000"
  },
  {
    "text": "half full the bottom ones there's like pretty much full there's two full some are most of them are cpus but there are",
    "start": "1770000",
    "end": "1777440"
  },
  {
    "text": "some which are gpus you can see that it says their gpus so they support gpus",
    "start": "1777440",
    "end": "1782559"
  },
  {
    "text": "and we our requirement is to run it on a gpu so the first phase of the scheduler as i",
    "start": "1782559",
    "end": "1788559"
  },
  {
    "text": "said in the beginning is filtering you filter out the nodes and i'm sure you could we can all agree",
    "start": "1788559",
    "end": "1794559"
  },
  {
    "text": "this the requirement is to run it on a gpu so we get rid of the cpu scheduler goes yeah you're out you are not off no",
    "start": "1794559",
    "end": "1801919"
  },
  {
    "text": "use so it takes the cpus out and then uh we got four gpus right",
    "start": "1801919",
    "end": "1810559"
  },
  {
    "text": "and then what it says uh hopefully when you go four left which one should you pick uh the scheduler then ranks and",
    "start": "1810559",
    "end": "1816399"
  },
  {
    "text": "prioritizes the nodes and there's an empty one right so you can see at the bottom there's an empty one uh why not",
    "start": "1816399",
    "end": "1821760"
  },
  {
    "text": "why don't we pick that because it's empty let's just deploy it on there but it's got this ranking there one two three four",
    "start": "1821760",
    "end": "1827679"
  },
  {
    "text": "you might see three and four they're quite similar it's got two two pods running in there how did it rank one",
    "start": "1827679",
    "end": "1833039"
  },
  {
    "text": "with number three and one with number four that's kind of what did it do how do we do that and we'll just look at all of this now in a few seconds so",
    "start": "1833039",
    "end": "1840000"
  },
  {
    "text": "what it does is then it basically takes that and runs that pod on that node and everything is good people can send",
    "start": "1840000",
    "end": "1845919"
  },
  {
    "text": "requests to this pod well it's running on a gpu or whatever it is and everything gets um",
    "start": "1845919",
    "end": "1852480"
  },
  {
    "text": "traffic gets served everything's good any questions so far is everybody okay yeah are we good",
    "start": "1852480",
    "end": "1858640"
  },
  {
    "text": "cool i know it's the last again at the end of the day and but it's the first day so you know it's",
    "start": "1858640",
    "end": "1863760"
  },
  {
    "text": "uh hopefully we're all good now filtering we said filtering happens so how do we",
    "start": "1863760",
    "end": "1869519"
  },
  {
    "text": "do how do we do the filtering well filtering happens in lots of stages",
    "start": "1869519",
    "end": "1874559"
  },
  {
    "text": "what kubernetes has these these things that it goes through what it calls predicates some checks that it does",
    "start": "1874559",
    "end": "1880000"
  },
  {
    "text": "there's a lot of checks right so it's doing some checks um and it has a it has quite a few",
    "start": "1880000",
    "end": "1885120"
  },
  {
    "text": "checks 12 of them i'm not going to go in in detail but let's say if it's a web server a web",
    "start": "1885120",
    "end": "1892320"
  },
  {
    "text": "server it needs to be run on the server if you need it needs to be run on a server you know we've all done this hopefully if you if you're on a server",
    "start": "1892320",
    "end": "1898880"
  },
  {
    "text": "or whatever we need to make sure the port that it runs on is empty on the virtual machine right we've done that we just make sure the port is a port is",
    "start": "1898880",
    "end": "1905360"
  },
  {
    "text": "free otherwise it's running on a different port right it's a web server or running a different port and that's kind of what kubernetes does it checks",
    "start": "1905360",
    "end": "1912480"
  },
  {
    "text": "if i deploy this container on this node does it have that port free the one that's required by this web server",
    "start": "1912480",
    "end": "1918880"
  },
  {
    "text": "if it doesn't it doesn't give us you know he just says yeah you don't have a port free so i can kick",
    "start": "1918880",
    "end": "1924559"
  },
  {
    "text": "you out but if it does it picks that and then basically sometimes you can say",
    "start": "1924559",
    "end": "1930480"
  },
  {
    "text": "a pod needs to be deployed on a specific host you can see like a specific machine with this name if you wanted to you can do",
    "start": "1930480",
    "end": "1936480"
  },
  {
    "text": "that and there's there's tons more right so i'm i'm not going to go through all of them there's like memory pressure",
    "start": "1936480",
    "end": "1941760"
  },
  {
    "text": "check just checks to see if there's enough space on the if there's enough space on the on the node there's some tolerations we'll talk",
    "start": "1941760",
    "end": "1948480"
  },
  {
    "text": "about tolerations and also looks at like can i bind any volumes sometimes the pods might need uh information from a",
    "start": "1948480",
    "end": "1955440"
  },
  {
    "text": "disk or something so if you can apply that the point is it's not important to to know",
    "start": "1955440",
    "end": "1960880"
  },
  {
    "text": "each one of them and what they are but it's just kubernetes does all these checks to see if it can",
    "start": "1960880",
    "end": "1966880"
  },
  {
    "text": "fit the pod on that node but it's not done yet we've just done the filtering it's a lot of checks but",
    "start": "1966880",
    "end": "1973279"
  },
  {
    "text": "we have we have one more stage to go which is called the storing stage scoring stage",
    "start": "1973279",
    "end": "1978559"
  },
  {
    "text": "and uh is basically similar when it needs to score so we you know we did the filtering and now we're going to do the",
    "start": "1978559",
    "end": "1984559"
  },
  {
    "text": "scoring again it does the similar things it's got a list of 12 things it goes through",
    "start": "1984559",
    "end": "1989760"
  },
  {
    "text": "and you know basically spreads across the hosts uh same thing again i'm not going to go through them all",
    "start": "1989760",
    "end": "1996159"
  },
  {
    "text": "just make sure that the the nodes are the pods are spread across the all the nodes because we don't want to",
    "start": "1996159",
    "end": "2001600"
  },
  {
    "text": "deploy them on the same node if there's replicas or because if we lose that one machine we lose all our applications we",
    "start": "2001600",
    "end": "2007600"
  },
  {
    "text": "don't want that that's not good and affinity stuff but again there's",
    "start": "2007600",
    "end": "2012640"
  },
  {
    "text": "tons of checks it goes through and it just and it starts giving them a score",
    "start": "2012640",
    "end": "2018960"
  },
  {
    "text": "now the scheduler actually is optimized to make the best placements about your",
    "start": "2018960",
    "end": "2024640"
  },
  {
    "text": "application that's what it's optimized for right and that's that's the that's the promise the kubernetes will say okay",
    "start": "2024640",
    "end": "2030960"
  },
  {
    "text": "you send me the request and i'll figure out where it needs to run so you don't have to worry about this",
    "start": "2030960",
    "end": "2037120"
  },
  {
    "text": "but sometimes you might know your application better than kubernetes does and we look at some of the use cases",
    "start": "2037120",
    "end": "2044720"
  },
  {
    "text": "that way there's four mechanisms in kubernetes that help you influence the the behavior of the scheduler",
    "start": "2044720",
    "end": "2051760"
  },
  {
    "text": "because the scheduler is doing its thing you know we saw those two filtering and scoring but we can say we can we can you",
    "start": "2051760",
    "end": "2057599"
  },
  {
    "text": "know schedule we can uh basically influence the scheduler's behavior and",
    "start": "2057599",
    "end": "2062878"
  },
  {
    "text": "there's four checks we can for four hints we can use one is called the note selector",
    "start": "2062879",
    "end": "2067919"
  },
  {
    "text": "hopefully that kind of makes sense you can select a node there's something called the node affinity and part affinity anti-affinity",
    "start": "2067919",
    "end": "2076000"
  },
  {
    "text": "taints and tolerations these terms might be new but we'll break them down and explain them so if anybody asks you and",
    "start": "2076000",
    "end": "2082638"
  },
  {
    "text": "when you go back to your office to your teams and your zoom call how do i deploy this pod you can tell you can tell them tell them",
    "start": "2082639",
    "end": "2088878"
  },
  {
    "text": "all about it so note selector is the simplest one and i",
    "start": "2088879",
    "end": "2094320"
  },
  {
    "text": "actually have used it quite a lot of times because there's some cases where you might want to use it what nodes have is labels and labels are",
    "start": "2094320",
    "end": "2101760"
  },
  {
    "text": "just key value pairs random key values you can say server key server is the key",
    "start": "2101760",
    "end": "2106880"
  },
  {
    "text": "and the value is web you know just random key value pairs you can attach them to the nodes you can go cube ctl",
    "start": "2106880",
    "end": "2113359"
  },
  {
    "text": "well i'll show you the command in a second and pods you know the labels i was showing you in",
    "start": "2113359",
    "end": "2118800"
  },
  {
    "text": "the beginning in in the yaml file they have what were called selectors again they're also labels",
    "start": "2118800",
    "end": "2124400"
  },
  {
    "text": "so what you can do is you can get these selectors to target the nodes that's what it is",
    "start": "2124400",
    "end": "2130079"
  },
  {
    "text": "when you write the you say oh just select the select this node node selector you write in the podium you say node selector target the labels",
    "start": "2130079",
    "end": "2137599"
  },
  {
    "text": "so let's take an example imagine i have four nodes",
    "start": "2137599",
    "end": "2143040"
  },
  {
    "text": "here and we can deploy the apps then apps on them and what i do is i assign a label i say",
    "start": "2143040",
    "end": "2148720"
  },
  {
    "text": "cube cdl label nodes and i i say worker one which is uh cube ctl",
    "start": "2148720",
    "end": "2154000"
  },
  {
    "text": "is a command line two worker one is is the is the node and then app equals red is the label i'll just attach that label",
    "start": "2154000",
    "end": "2159359"
  },
  {
    "text": "it's just simple label nothing special just attach the label to it in this case we assign the label up",
    "start": "2159359",
    "end": "2164800"
  },
  {
    "text": "equals red to the first and the third node in the cluster you can see the first one and the third one right i just say",
    "start": "2164800",
    "end": "2170880"
  },
  {
    "text": "worker one and work at three and here's my deployment file did we we saw this before right you know a",
    "start": "2170880",
    "end": "2177200"
  },
  {
    "text": "deployment file uh came in late but i can i can we can quickly look at it deploying file it says i need to run",
    "start": "2177200",
    "end": "2182800"
  },
  {
    "text": "this this the three replicas and halfway through and then in at the end there's",
    "start": "2182800",
    "end": "2188880"
  },
  {
    "text": "this thing the image that i'm running and uh and the node selector the box should be a little bit lower than that",
    "start": "2188880",
    "end": "2195119"
  },
  {
    "text": "line but you can see node selectors app equals red so we're saying this part needs to be deployed on the label or app",
    "start": "2195119",
    "end": "2202480"
  },
  {
    "text": "on the node which has the label articles right now there's three replicas let me ask you all a question",
    "start": "2202480",
    "end": "2209119"
  },
  {
    "text": "where do you think the pods will get deployed let's start from the left one two three",
    "start": "2209119",
    "end": "2214240"
  },
  {
    "text": "and four where do you think they will get deployed just shout out it's okay there's no one",
    "start": "2214240",
    "end": "2219599"
  },
  {
    "text": "in three very good very good we have a we have a first answer excellent stuff there's there's a few more so you'll",
    "start": "2219599",
    "end": "2225119"
  },
  {
    "text": "feel free to shout out the answers so one and three i can we can deploy to one and three right and there's space for two two containers",
    "start": "2225119",
    "end": "2232000"
  },
  {
    "text": "we just deploy them on there on the first one there's not enough space left so we go and deploy it on the on the third one hopefully that's clear so this",
    "start": "2232000",
    "end": "2238880"
  },
  {
    "text": "thing has to have an exact match if the the labels don't match",
    "start": "2238880",
    "end": "2244000"
  },
  {
    "text": "no good it's a hard requirement so if there's no not enough space your",
    "start": "2244000",
    "end": "2249200"
  },
  {
    "text": "pod will not run so consider you have this existing cluster and i scale the replicas from",
    "start": "2249200",
    "end": "2256640"
  },
  {
    "text": "three to five right three to five i go from three to five and the node selector is red",
    "start": "2256640",
    "end": "2263599"
  },
  {
    "text": "so um it went from three to five where do you think the pod will run",
    "start": "2263599",
    "end": "2270480"
  },
  {
    "text": "any any guesses we've got five replicas any guesses",
    "start": "2270480",
    "end": "2275520"
  },
  {
    "text": "go on yeah one is one is already full do you agree one is four what about three can we deploy a",
    "start": "2275520",
    "end": "2282320"
  },
  {
    "text": "part on three yeah we can deploy one part and three and the other one just stays pending because there's a hard requirement and",
    "start": "2282320",
    "end": "2288720"
  },
  {
    "text": "you you'll say oh that's terrible because i have two worker nodes which are completely empty",
    "start": "2288720",
    "end": "2294000"
  },
  {
    "text": "agreed they're completely empty why don't we use that and that's why kubernetes says okay if you want a bit",
    "start": "2294000",
    "end": "2299599"
  },
  {
    "text": "more flexibility you can use no dfinity so we're building up now so we're building up a little bit more i'm going to flash this next slide and it's going",
    "start": "2299599",
    "end": "2306560"
  },
  {
    "text": "to be a big label on there but let's ignore the label because whatever it says require during scheduling or during",
    "start": "2306560",
    "end": "2312720"
  },
  {
    "text": "execution i don't even know what that means that means hard or soft requirement right so there's two things that's how i remember them i don't",
    "start": "2312720",
    "end": "2318880"
  },
  {
    "text": "remember it that way because that's too long so hard or soft requirement so you can with no dfinity have a hard",
    "start": "2318880",
    "end": "2324000"
  },
  {
    "text": "requirement or software for example hard requirement will be only run the pods",
    "start": "2324000",
    "end": "2329680"
  },
  {
    "text": "with on nodes which which have gpus that makes sense because if a pod requires gpus you can't really",
    "start": "2329680",
    "end": "2335680"
  },
  {
    "text": "run on cpu it's a lot longer it won't even run a soft one is something like this run",
    "start": "2335680",
    "end": "2341119"
  },
  {
    "text": "pods in let's say uk first you have a con you have a node that runs in uk run it there",
    "start": "2341119",
    "end": "2346320"
  },
  {
    "text": "otherwise run it somewhere else so otherwise run it somewhere else that's what that is it's a hard and soft requirement so again we'll do another",
    "start": "2346320",
    "end": "2353040"
  },
  {
    "text": "example and uh and the same cluster i tested labels we've",
    "start": "2353040",
    "end": "2359119"
  },
  {
    "text": "got two labels and it looks a little bit weird sorry we're going we're going into a lot of yammer files here but this is this is",
    "start": "2359119",
    "end": "2365680"
  },
  {
    "text": "basically required during scheduling ignoring execution this is a hard requirement and",
    "start": "2365680",
    "end": "2371040"
  },
  {
    "text": "you can see the key is app and operator is in and the values is read",
    "start": "2371040",
    "end": "2376960"
  },
  {
    "text": "all it says is if it has app equals red that's what what our uh what our",
    "start": "2376960",
    "end": "2383040"
  },
  {
    "text": "affinity is so that's where we wanted to run the container on and now this is very similar to what we saw before",
    "start": "2383040",
    "end": "2390000"
  },
  {
    "text": "because it's a hard requirement which is exactly the same scenario that we saw previously",
    "start": "2390000",
    "end": "2395040"
  },
  {
    "text": "and i can i can take them and you know like it will basically go in i've got three replicas running if i scale up to",
    "start": "2395040",
    "end": "2401119"
  },
  {
    "text": "five basically the same thing will happen right everything will be the same and one node will another part will run",
    "start": "2401119",
    "end": "2406720"
  },
  {
    "text": "there and one will stay pending so nothing is actually changed nothing has actually changed but",
    "start": "2406720",
    "end": "2412800"
  },
  {
    "text": "i can do this node affinity with preferred during scheduling and ignore during execution sorry the the highlight",
    "start": "2412800",
    "end": "2418800"
  },
  {
    "text": "is a little bit higher than it should be i'm not sure why but it's there but it should be on that long long label there",
    "start": "2418800",
    "end": "2424319"
  },
  {
    "text": "so this is a soft requirement what do you think will happen to the pods now",
    "start": "2424319",
    "end": "2430400"
  },
  {
    "text": "it's soft requirement they might end up on part node 2 note 3",
    "start": "2430400",
    "end": "2435680"
  },
  {
    "text": "node 4. do you agree might end up on it yeah that's that's exactly what happens but if it's scheduled you kind of have",
    "start": "2435680",
    "end": "2440800"
  },
  {
    "text": "to delete it but the point is if you use uh if you use the soft requirement you can basically go ahead and you'll say",
    "start": "2440800",
    "end": "2447680"
  },
  {
    "text": "okay there's no there's no nodes left i'm gonna deploy it on there i know this is boring stuff but",
    "start": "2447680",
    "end": "2453040"
  },
  {
    "text": "some people have this issue they might figure this out so uh there's another thing",
    "start": "2453040",
    "end": "2459839"
  },
  {
    "text": "sometimes you want some pods to run together or not you might not want to run them",
    "start": "2459839",
    "end": "2465680"
  },
  {
    "text": "together and if you want to run them together for example you you have a trading app i",
    "start": "2465680",
    "end": "2470800"
  },
  {
    "text": "don't know something that requires uh mark services we've got market services imagine you've got all these nodes uh",
    "start": "2470800",
    "end": "2476720"
  },
  {
    "text": "some nodes are deployed in in finland some diplo nodes are deployed in norway some nodes are deployed in in usa and",
    "start": "2476720",
    "end": "2484240"
  },
  {
    "text": "the pods could be deployed anywhere right they could be deployed anywhere or any other nodes if that happens how do",
    "start": "2484240",
    "end": "2490000"
  },
  {
    "text": "we reduce the latency between the traffic that goes between all these containers and",
    "start": "2490000",
    "end": "2495839"
  },
  {
    "text": "this is where we use pod affinity and anti-infinity you can get pods to stick together or repel that's basically what",
    "start": "2495839",
    "end": "2501839"
  },
  {
    "text": "this is stick together and uh affinity and infinity is based on",
    "start": "2501839",
    "end": "2507119"
  },
  {
    "text": "labels so we've got labels uh specific for namespaces but you know that's uh that's basically we just keep using the",
    "start": "2507119",
    "end": "2513280"
  },
  {
    "text": "same labels as before now imagine i have this pod with app equals red so",
    "start": "2513280",
    "end": "2518960"
  },
  {
    "text": "app equals right is the pod and i'm going to show you another another file here uh",
    "start": "2518960",
    "end": "2524319"
  },
  {
    "text": "we've got labels app equals red with the with the label that we attached",
    "start": "2524319",
    "end": "2529599"
  },
  {
    "text": "and we know we deploy all the red apps like we would and everything gets deployed correctly",
    "start": "2529599",
    "end": "2535040"
  },
  {
    "text": "and what we want to do is we want to deploy this green app right and this is with a different label",
    "start": "2535040",
    "end": "2540720"
  },
  {
    "text": "and part affinity and we define basically port affinity in this part definition we just take that part",
    "start": "2540720",
    "end": "2546720"
  },
  {
    "text": "definition again we define that and here you can see this part definition and we say okay this this pod",
    "start": "2546720",
    "end": "2553119"
  },
  {
    "text": "uh has to be run next to the pods which has app equals red label right that's",
    "start": "2553119",
    "end": "2558319"
  },
  {
    "text": "that's what we're saying so this this green application needs to run with the pod which has the red label",
    "start": "2558319",
    "end": "2563920"
  },
  {
    "text": "so i'm sure you're all thinking it gets deployed here so again we can get these parts to talk to each other work deploy",
    "start": "2563920",
    "end": "2570640"
  },
  {
    "text": "them close to one another and that's what happens there and it's the same thing i've got i've",
    "start": "2570640",
    "end": "2575920"
  },
  {
    "text": "got this yellow one and in this yellow one i just tell it uh running next to the red pod and this yellow one goes in",
    "start": "2575920",
    "end": "2582560"
  },
  {
    "text": "and it goes again the same same kind of thing you can imagine it will get it will get deployed to the to the red pod",
    "start": "2582560",
    "end": "2589599"
  },
  {
    "text": "uh when there is if there's a if there's a requirement for it right i'm just going to skip that",
    "start": "2589599",
    "end": "2595119"
  },
  {
    "text": "now um just like uh node affinity pod affinity",
    "start": "2595119",
    "end": "2600560"
  },
  {
    "text": "and anti-affinity can also be hard and soft requirements that's just the way kubernetes scheduling is done and you",
    "start": "2600560",
    "end": "2607200"
  },
  {
    "text": "know you can you can decide to change the scheduling and say okay i'm going to put a hard requirement or i'm going to put a soft",
    "start": "2607200",
    "end": "2612640"
  },
  {
    "text": "requirement and if this is just like before just like before if there's no space next to it it'll go and deploy",
    "start": "2612640",
    "end": "2618240"
  },
  {
    "text": "somewhere else but if there is space next to the container it'll run next to it if this is a soft requirement if it's",
    "start": "2618240",
    "end": "2623760"
  },
  {
    "text": "a hard requirement it'll just repel away and not deploy there so that's what this is saying same same kind of concept we",
    "start": "2623760",
    "end": "2630079"
  },
  {
    "text": "won't play the game but i'm sure you understand we'll basically go ahead and get run and get deployed there",
    "start": "2630079",
    "end": "2635839"
  },
  {
    "text": "and anti-affinity works in the opposite way all we tell it is to repel any of",
    "start": "2635839",
    "end": "2642240"
  },
  {
    "text": "the labels that we wanted to repel we wanted to repel app because red our app equals green and so this is this is the",
    "start": "2642240",
    "end": "2648720"
  },
  {
    "text": "yellow app and we we tell it to basically repel it in this case we said app equals red and green we don't have",
    "start": "2648720",
    "end": "2655119"
  },
  {
    "text": "anything with a red and green label so it just goes ahead and deploys on the last last last node um there's some",
    "start": "2655119",
    "end": "2663520"
  },
  {
    "text": "there's some constraints where if you use undefinity uh you're doing a lot of calculations at least it's trying to",
    "start": "2663520",
    "end": "2670000"
  },
  {
    "text": "figure out oh can i run here can i run there it takes a lot of time so it could eat up your cpu and memory and you only",
    "start": "2670000",
    "end": "2676720"
  },
  {
    "text": "you can basically it's useful when you want to schedule the pods together and",
    "start": "2676720",
    "end": "2682079"
  },
  {
    "text": "sometimes when you don't want to run the pots together you know maybe is for some",
    "start": "2682079",
    "end": "2687280"
  },
  {
    "text": "compliance sake so you can basically repel them using this and dfinity now this last bit is this most confusing",
    "start": "2687280",
    "end": "2694079"
  },
  {
    "text": "bit but i'm going to try and do do it in the least possible the least confusing way hopefully it's",
    "start": "2694079",
    "end": "2700640"
  },
  {
    "text": "just the way the the the api is written is a little bit weird but",
    "start": "2700640",
    "end": "2705920"
  },
  {
    "text": "you know i'm recording i'm getting recorded but i don't mind saying that because it's kind of hard to understand",
    "start": "2705920",
    "end": "2711200"
  },
  {
    "text": "um but once you get it it makes sense but in this case what you can do is you can get nodes",
    "start": "2711200",
    "end": "2717200"
  },
  {
    "text": "to repel the pods that's what we're saying sometimes you can say a node if you need to upgrade a node",
    "start": "2717200",
    "end": "2724319"
  },
  {
    "text": "let's say you're upgrading a node but you want zero downtime and you don't want anybody to deploy anything on there",
    "start": "2724319",
    "end": "2729680"
  },
  {
    "text": "so you can apply these apply these labels apply these taints to repel all the pods so nobody can schedule a node",
    "start": "2729680",
    "end": "2736400"
  },
  {
    "text": "nobody can schedule a pod once everything's set up you can you can start deploying it it's basically the same as a node",
    "start": "2736400",
    "end": "2742480"
  },
  {
    "text": "selector but the label is slightly different now instead of adding a label you add a",
    "start": "2742480",
    "end": "2748640"
  },
  {
    "text": "label with an effect you say this is a label and this is the effect and when you do that and there's three",
    "start": "2748640",
    "end": "2754880"
  },
  {
    "text": "effects you can have no schedule prefer no schedule and no execute which is basically hard soft and",
    "start": "2754880",
    "end": "2760400"
  },
  {
    "text": "evict kind of like what we had before pretty much the same apart from we added one more",
    "start": "2760400",
    "end": "2765760"
  },
  {
    "text": "uh behavior which is evict so if the hard and soft don't match up just evict the pod if it's running in",
    "start": "2765760",
    "end": "2772000"
  },
  {
    "text": "there just you know like if it's sorry if if it doesn't tolerate that evict the pod that's what it does",
    "start": "2772000",
    "end": "2779440"
  },
  {
    "text": "so this is perhaps the most confusing bit and we'll be done uh we don't have much long to go",
    "start": "2779440",
    "end": "2785520"
  },
  {
    "text": "and what you do is what you do is you taint the nodes so you have all the nodes what i'm doing is tainting the",
    "start": "2785520",
    "end": "2791040"
  },
  {
    "text": "node using this command called cube ctl paint nodes and app equals yellow no",
    "start": "2791040",
    "end": "2796240"
  },
  {
    "text": "schedule and what the no schedule is the evict taint",
    "start": "2796240",
    "end": "2802800"
  },
  {
    "text": "now i'm gonna i'm gonna do the first one and hopefully we'll see it so we have cluster with two taints you can see app",
    "start": "2802800",
    "end": "2808960"
  },
  {
    "text": "equals yellow no schedule and app equals green prefer no schedule prefers the",
    "start": "2808960",
    "end": "2814240"
  },
  {
    "text": "soft one and no schedules is a hard requirement right so it prefers the soft and the hard requirement you can see we get this hard and soft in different use",
    "start": "2814240",
    "end": "2820960"
  },
  {
    "text": "cases we decided to create a deployment of a red pod and what we do is we create a",
    "start": "2820960",
    "end": "2826400"
  },
  {
    "text": "deployment with a toleration of applicables red right we just say this thought it can tolerate articles right",
    "start": "2826400",
    "end": "2833200"
  },
  {
    "text": "and the the thing with the labels which is the weird bit is if you tolerate",
    "start": "2833200",
    "end": "2839040"
  },
  {
    "text": "that label the pot tolerance that label that's there which is that because yellow app equals green then the effect that's there doesn't",
    "start": "2839040",
    "end": "2845280"
  },
  {
    "text": "apply to that pod it's a bit weird it's just the way the the apis if it doesn't make sense don't",
    "start": "2845280",
    "end": "2850559"
  },
  {
    "text": "worry it is it's just the way it is but in our case we don't actually have applicable's",
    "start": "2850559",
    "end": "2855920"
  },
  {
    "text": "yellow articles red label here there's one node which doesn't have any labels it'll go ahead basically and deploy it",
    "start": "2855920",
    "end": "2861920"
  },
  {
    "text": "there and if we increase it to three replicas it'll basically deploy one in the last",
    "start": "2861920",
    "end": "2868160"
  },
  {
    "text": "one which is empty and then if you see the one in the middle that says app equals green prefer no schedule so it only actually",
    "start": "2868160",
    "end": "2875440"
  },
  {
    "text": "prefers the middle one only prefers to deploy pods which have app equals green",
    "start": "2875440",
    "end": "2880559"
  },
  {
    "text": "node it prefers if it doesn't have app equals green label it will deploy it",
    "start": "2880559",
    "end": "2886400"
  },
  {
    "text": "if there's no other space for it to go the one on the left will only deploy the pods which have apical yellow label",
    "start": "2886400",
    "end": "2893280"
  },
  {
    "text": "that's just the way their api is written it's a bit weird but kind of works in in those use cases so in our case we we",
    "start": "2893280",
    "end": "2900000"
  },
  {
    "text": "scale it from three to five and the middle one gets another pod and there's one space pending so that's what it is",
    "start": "2900000",
    "end": "2906400"
  },
  {
    "text": "and what you can do is if you let's say you wanna upgrade your uh worker because with with zero downtime you can add app",
    "start": "2906400",
    "end": "2914480"
  },
  {
    "text": "equals green no execute which is worker 3 and all it will do is it will evict all the pods does that make sense",
    "start": "2914480",
    "end": "2921599"
  },
  {
    "text": "if it doesn't blame the kubernetes people wrote kubernetes",
    "start": "2921599",
    "end": "2926960"
  },
  {
    "text": "but you know if you have any questions afterwards you can ask us again same thing with this that's what this is so",
    "start": "2926960",
    "end": "2932640"
  },
  {
    "text": "why is this uh uh when is it useful um all the containers are usually scheduled on worker nodes",
    "start": "2932640",
    "end": "2940720"
  },
  {
    "text": "the worker nodes that with the nodes that we deploy the pos that we deployed they don't get scheduled on on master or",
    "start": "2940720",
    "end": "2946480"
  },
  {
    "text": "on the on the control plane so this is where on the control plane when you check it has a has this uh this",
    "start": "2946480",
    "end": "2953440"
  },
  {
    "text": "this taint which says no schedule for everything so nothing it just says it says control",
    "start": "2953440",
    "end": "2959040"
  },
  {
    "text": "plane key control plane node control plane no schedule so it doesn't deploy that",
    "start": "2959040",
    "end": "2964559"
  },
  {
    "text": "so let me just finish up we've got a couple more we've got five more minutes uh so if node selector node affinity",
    "start": "2964559",
    "end": "2971920"
  },
  {
    "text": "port affinity taint and toleration aren't enough you can influence the scheduler by writing your own plug-in",
    "start": "2971920",
    "end": "2977599"
  },
  {
    "text": "again the bits that i showed you in here in the beginning you remember this diagram uh this the filtering is basically in",
    "start": "2977599",
    "end": "2986079"
  },
  {
    "text": "there's more more stages to it rather than the simplified version that i showed you and scoring as well you can",
    "start": "2986079",
    "end": "2991599"
  },
  {
    "text": "see this there's more stages there just you know every bit does its own thing and uh the blocks at the boy and all the",
    "start": "2991599",
    "end": "2998000"
  },
  {
    "text": "bits that you see in the top uh you can you can you can modify them you can write your own custom logic to it the",
    "start": "2998000",
    "end": "3004720"
  },
  {
    "text": "bit at the bottom the binding you can't really change that it is that's what it is and uh if you want to customize any of",
    "start": "3004720",
    "end": "3010720"
  },
  {
    "text": "the plugins you can write your own logic and if you look at in the middle one this q sort plug-in all it does is just",
    "start": "3010720",
    "end": "3016480"
  },
  {
    "text": "picks one part and returns it you can write your own plugins for it and if none of the options work for you",
    "start": "3016480",
    "end": "3022160"
  },
  {
    "text": "you can actually write your own scheduler so you can say forget about the kubernetes scheduler i know the logic better you can actually write it",
    "start": "3022160",
    "end": "3029280"
  },
  {
    "text": "as a pod like your own logic you have to read your information from xcd and you can write that and uh i know i'm",
    "start": "3029280",
    "end": "3035119"
  },
  {
    "text": "bombarding you with a yammer files but unfortunately kubernetes is all about yammer files um",
    "start": "3035119",
    "end": "3040880"
  },
  {
    "text": "and then you can you can tell a port to use a specific scheduler that you wrote so you can do that",
    "start": "3040880",
    "end": "3046640"
  },
  {
    "text": "so kubernetes if anybody if any pod has that scheduler it ignores everything by by def ignores the default scheduler",
    "start": "3046640",
    "end": "3053920"
  },
  {
    "text": "and you just have to kind of watch everything from an api and write your own logic and this is the pseudocode an",
    "start": "3053920",
    "end": "3060800"
  },
  {
    "text": "example of that's written here all it does is it gets all the gets all the all the pods that are there that needs to be",
    "start": "3060800",
    "end": "3067119"
  },
  {
    "text": "deployed and it basically uh picks picks a node at random you can see uh we just",
    "start": "3067119",
    "end": "3073280"
  },
  {
    "text": "do a bit of random logic here and we then assign the part to the node and this is what the simon looks like again",
    "start": "3073280",
    "end": "3079440"
  },
  {
    "text": "it doesn't really matter too much what it looks like but all we've done now is basically deployed our container",
    "start": "3079440",
    "end": "3085520"
  },
  {
    "text": "there now you might say what are the use cases right so this is the last bit what are the use cases why are you talking",
    "start": "3085520",
    "end": "3091680"
  },
  {
    "text": "instead of talking about f sharp why are you talking about this crap well there is this is this is why it's useful",
    "start": "3091680",
    "end": "3096960"
  },
  {
    "text": "so there's quite a few use cases imagine you have a gpu",
    "start": "3096960",
    "end": "3102079"
  },
  {
    "text": "and you have a load that you want to run on gpu so i can use a node selector and no dfinity that's the kind of thing i can use right so that's that's a good",
    "start": "3102079",
    "end": "3108319"
  },
  {
    "text": "use case and i can use taints and tolerations where the node control what actually gets",
    "start": "3108319",
    "end": "3114880"
  },
  {
    "text": "deployed on that pod on that on on that node itself so maybe you want to upgrade a node with zero",
    "start": "3114880",
    "end": "3121599"
  },
  {
    "text": "downtime you can you can basically taint that node so nothing you know nothing is deployed or we use control plane we",
    "start": "3121599",
    "end": "3128480"
  },
  {
    "text": "deploy like with to attain some toleration it should be an s there but there's thing why there's a bit of a",
    "start": "3128480",
    "end": "3135839"
  },
  {
    "text": "mistake there and you can use pod affinity to group parts together and",
    "start": "3135839",
    "end": "3143039"
  },
  {
    "text": "and if you want to reduce latency between two pods so you know",
    "start": "3143200",
    "end": "3148240"
  },
  {
    "text": "let's say you have an application you want to reduce latency uh you can use your pod affinity and dfinity or as open",
    "start": "3148240",
    "end": "3154480"
  },
  {
    "text": "ai i talked about that in the beginning find out you make sure that not all pods get deployed on the same node because",
    "start": "3154480",
    "end": "3161920"
  },
  {
    "text": "that's what they came across so openly i found out this issue where they had a node with 2 500 they",
    "start": "3161920",
    "end": "3168720"
  },
  {
    "text": "had a cluster with 2 500 nodes and there's a service in kubernetes again it",
    "start": "3168720",
    "end": "3174160"
  },
  {
    "text": "runs as a pod basically tries to figure out where all the pods are running so if a pod wants",
    "start": "3174160",
    "end": "3180079"
  },
  {
    "text": "to talk down the pot if a container wants to talk from one container to another container this is a service called cube dns that figures out oh this",
    "start": "3180079",
    "end": "3187680"
  },
  {
    "text": "part is in this node that part is in that node but that one crashed so now it's somewhere here so this is that",
    "start": "3187680",
    "end": "3192720"
  },
  {
    "text": "service is very very important for kubernetes to work otherwise it doesn't work what they found was that um",
    "start": "3192720",
    "end": "3199040"
  },
  {
    "text": "that they found that on some nodes there were more than 10 instances of cube dns running cube dns",
    "start": "3199040",
    "end": "3205040"
  },
  {
    "text": "running on more than 10 instances and this was actually running in azure and in 2018 uh each virtual machine in azure",
    "start": "3205040",
    "end": "3212800"
  },
  {
    "text": "um had to request the query per second request of of 200. so you can only",
    "start": "3212800",
    "end": "3218160"
  },
  {
    "text": "request in the virtual machine you can only request like 200 queries per second but",
    "start": "3218160",
    "end": "3223359"
  },
  {
    "text": "because we had 10 instances of cube dns running on each node more than 10 instances and they're constantly talking",
    "start": "3223359",
    "end": "3228640"
  },
  {
    "text": "to the api way more than 200 requests per second and what happened was everything was getting",
    "start": "3228640",
    "end": "3234720"
  },
  {
    "text": "everything was getting throttled so that none of the stuff was getting deployed so they looked at it and go oh crap that's what's happening then they",
    "start": "3234720",
    "end": "3240559"
  },
  {
    "text": "started they used part 90 affinity to make sure the pod repelled themselves so instead of",
    "start": "3240559",
    "end": "3246960"
  },
  {
    "text": "basically at 2500 nodes they re they use part and dfinity part of anti-affinity to repel all the pods and",
    "start": "3246960",
    "end": "3253440"
  },
  {
    "text": "just run them on all the nodes instead of like more than 10 running on on one node so that's what they did now",
    "start": "3253440",
    "end": "3260000"
  },
  {
    "text": "any questions no okay so last bit the recap",
    "start": "3260000",
    "end": "3266720"
  },
  {
    "text": "sorry go for it",
    "start": "3266720",
    "end": "3269280"
  },
  {
    "text": "is that is that like dapper no it's basically uh it's it's not dapper it's just it's just a logic that",
    "start": "3273599",
    "end": "3279680"
  },
  {
    "text": "cube dns logic all it does is updates ip tables in in the machine there's no it's not like",
    "start": "3279680",
    "end": "3284960"
  },
  {
    "text": "that right it's literally all it's doing is just updating ip tables that says oh this pod has this ip address that part",
    "start": "3284960",
    "end": "3290559"
  },
  {
    "text": "has the ip address all that's doing but that's a good question but it's just a it's a logic that runs in the cluster",
    "start": "3290559",
    "end": "3297280"
  },
  {
    "text": "cube dns sure",
    "start": "3297280",
    "end": "3302760"
  },
  {
    "text": "assuming",
    "start": "3314480",
    "end": "3317480"
  },
  {
    "text": "so the question is that i said one of the things that kubernetes checks in the beginning is is a port is the",
    "start": "3330400",
    "end": "3336799"
  },
  {
    "text": "port free right that's the question and then the question your question is which is quite good it says",
    "start": "3336799",
    "end": "3341839"
  },
  {
    "text": "why can't cube dns be a bit more clever and it says oh that port doesn't exist so maybe i'll rename the re-change the",
    "start": "3341839",
    "end": "3349040"
  },
  {
    "text": "port to this port and deploy is that right yeah so the reason why it doesn't change the port is because that port definition",
    "start": "3349040",
    "end": "3356400"
  },
  {
    "text": "that we write in deployment is written by you and i so that's why kubernetes doesn't change that because otherwise you have to like",
    "start": "3356400",
    "end": "3362720"
  },
  {
    "text": "track one more thing to change it and then root it so basically the yaml file that we wrote that has the",
    "start": "3362720",
    "end": "3368720"
  },
  {
    "text": "port does that make sense to answer your question okay basically what it doesn't do anything",
    "start": "3368720",
    "end": "3375280"
  },
  {
    "text": "fancy with the port it just takes that information it says yeah that's the port running on i'm just going to deploy with that port it doesn't it it doesn't try",
    "start": "3375280",
    "end": "3382160"
  },
  {
    "text": "to do anything fancy because that's not the cube dns is that's not its main job that's not the main job to like figure",
    "start": "3382160",
    "end": "3388240"
  },
  {
    "text": "out all it does is just basically updates where the pods are what is ip addresses that's the cube in the dns major",
    "start": "3388240",
    "end": "3394240"
  },
  {
    "text": "but perhaps you can write a component to do that you know maybe like update it or figure out but that's a very good",
    "start": "3394240",
    "end": "3399680"
  },
  {
    "text": "question any other questions okay last slide the scheduler is in",
    "start": "3399680",
    "end": "3405440"
  },
  {
    "text": "charge of deciding where the pod is deployed in the cluster and the schedule goes to two phases scheduling and",
    "start": "3405440",
    "end": "3410799"
  },
  {
    "text": "binding phase and there's you know there's like a lot of filters and predictors that we talked about and then",
    "start": "3410799",
    "end": "3416960"
  },
  {
    "text": "there's the simplest one node selector you can select a node or you can do node affinity",
    "start": "3416960",
    "end": "3422799"
  },
  {
    "text": "uh so you know you can you can figure out uh if a pod can be",
    "start": "3422799",
    "end": "3428160"
  },
  {
    "text": "placed on a particular node and you can do pod affinity and dfinity you can repel them together you can",
    "start": "3428160",
    "end": "3433200"
  },
  {
    "text": "repel away uh if you wanted to entain centaurus and get nodes to repel the pods you can",
    "start": "3433200",
    "end": "3438799"
  },
  {
    "text": "update your logic if you wanted to and you can write your custom scheduler but if you want more information on this",
    "start": "3438799",
    "end": "3444319"
  },
  {
    "text": "stuff you can always message me on on link on twitter i'm still around you can you can",
    "start": "3444319",
    "end": "3449680"
  },
  {
    "text": "catch me you can go to the websites we've got learncase.org we've got some really sick blogs on there you know",
    "start": "3449680",
    "end": "3455119"
  },
  {
    "text": "we've got some really good blogs as well you can go to kubernetes documentation uh if you have any questions you can you",
    "start": "3455119",
    "end": "3461280"
  },
  {
    "text": "can uh find me on twitter i saw minecraft i also have a youtube channel but i'd like to the thanks all of you",
    "start": "3461280",
    "end": "3467839"
  },
  {
    "text": "for turning up to this unexpected session i hope you found it of any use i've been told that after 5 pm the only",
    "start": "3467839",
    "end": "3474799"
  },
  {
    "text": "card you can use is green is that right all right so when you i'm joking feel free to give a",
    "start": "3474799",
    "end": "3480319"
  },
  {
    "text": "non honest uh honest feedback but any questions are we good",
    "start": "3480319",
    "end": "3485760"
  },
  {
    "text": "okay so thank you all for joining i really appreciate it hopefully you found it useful thank you",
    "start": "3485760",
    "end": "3492950"
  },
  {
    "text": "[Applause]",
    "start": "3492950",
    "end": "3496560"
  },
  {
    "text": "you",
    "start": "3502960",
    "end": "3505040"
  }
]