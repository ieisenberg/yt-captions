[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "hi everyone just before we start I'll get a show of hands who's played with",
    "start": "5880",
    "end": "13210"
  },
  {
    "text": "your networks it's so a few of you have had some experience so most of you",
    "start": "13210",
    "end": "18910"
  },
  {
    "text": "haven't really done much with them at all I'll talk quickly about what what it",
    "start": "18910",
    "end": "25029"
  },
  {
    "start": "23000",
    "end": "79000"
  },
  {
    "text": "actually is and probably the best way to start is is to compare it with with",
    "start": "25029",
    "end": "30850"
  },
  {
    "text": "traditional programming which is we basically do something like this this is what we do most of the time when we're",
    "start": "30850",
    "end": "37600"
  },
  {
    "text": "programming and if you want to say write a web page to do something like this",
    "start": "37600",
    "end": "42610"
  },
  {
    "text": "calculate body mass index at the heart of this is it's an algorithm that this",
    "start": "42610",
    "end": "52660"
  },
  {
    "text": "code works out your body mass index and essentially we write that algorithm we",
    "start": "52660",
    "end": "58570"
  },
  {
    "text": "figure out what's the formula for this",
    "start": "58570",
    "end": "63989"
  },
  {
    "text": "we write it and then we test it and if it doesn't work that means there's a bug in there so we",
    "start": "63989",
    "end": "70119"
  },
  {
    "text": "go back and think what have I done wrong we change it we fix the bug we test it again but some kind of problems are",
    "start": "70119",
    "end": "77710"
  },
  {
    "text": "really not conducive to that so if you were to write a program to say well what digit what handwritten digit is that you",
    "start": "77710",
    "end": "86710"
  },
  {
    "text": "could divide all that up the pixels and now you've got a big array of pixels and how do you write an algorithm to figure",
    "start": "86710",
    "end": "92830"
  },
  {
    "text": "out what what number that is and this is where machine learning can",
    "start": "92830",
    "end": "97900"
  },
  {
    "text": "can make this a whole lot easier so that the way a neural network works is give a",
    "start": "97900",
    "end": "103240"
  },
  {
    "text": "high level overview you've got something that looks like this so on the input side these are all the",
    "start": "103240",
    "end": "110770"
  },
  {
    "text": "pixels they would all end up in the input boxes here and then we feed it into a network of artificial neurons",
    "start": "110770",
    "end": "118780"
  },
  {
    "text": "that the design is loosely inspired by the way our brain works and the idea",
    "start": "118780",
    "end": "124719"
  },
  {
    "text": "with this is to begin with all of the internal weights and biases are set to",
    "start": "124719",
    "end": "130479"
  },
  {
    "text": "small random values so it won't work at the beginning but we start out with a",
    "start": "130479",
    "end": "137200"
  },
  {
    "text": "large set of training data and may be 50,000 or more images and we feed",
    "start": "137200",
    "end": "143260"
  },
  {
    "text": "those through one at a time so let's say we feed a fall through that that neural network it's going to come out with an",
    "start": "143260",
    "end": "148810"
  },
  {
    "text": "answer and it's going to be wrong because it's not being trained so then we say what can we do to make it very",
    "start": "148810",
    "end": "155920"
  },
  {
    "text": "slightly less wrong can we modify those weights and biases to make it a little",
    "start": "155920",
    "end": "161050"
  },
  {
    "text": "bit more likely to be correct then we repeat the process with all of our",
    "start": "161050",
    "end": "167040"
  },
  {
    "text": "50,000 or so images and we'll have done that we shuffle the images changed the",
    "start": "167040",
    "end": "172660"
  },
  {
    "text": "order that was called an epoch and we do it again we do another epoch and we keep doing this over and over gradually those",
    "start": "172660",
    "end": "179680"
  },
  {
    "text": "weights and biases start to start to take effect and then this can correctly",
    "start": "179680",
    "end": "186040"
  },
  {
    "text": "infer what the numbers are so we're going to build this in the next hour but",
    "start": "186040",
    "end": "193830"
  },
  {
    "start": "192000",
    "end": "273000"
  },
  {
    "text": "the question is why what's the advantage of building why don't we just use a library because there are a lot of",
    "start": "193830",
    "end": "199690"
  },
  {
    "text": "libraries out there - already to do machine learning and those libraries there's nothing wrong with them they're",
    "start": "199690",
    "end": "205300"
  },
  {
    "text": "a great place to end up they're a lot more sophisticated than what you could write yourself and they're also faster",
    "start": "205300",
    "end": "212050"
  },
  {
    "text": "because they use the graphics processing unit the problem though is if you really want to understand the underlying",
    "start": "212050",
    "end": "217900"
  },
  {
    "text": "principles it's not a good place to start because that they're all black boxes with lots of complex knobs and",
    "start": "217900",
    "end": "225310"
  },
  {
    "text": "levers you move a lever and it doesn't work anymore you don't know what you don't know why so it's not a good good",
    "start": "225310",
    "end": "231130"
  },
  {
    "text": "way to really understand them from the bottom up it's rather like if you've ever flown a flight simulator first",
    "start": "231130",
    "end": "237430"
  },
  {
    "text": "thing you do is you start with something like this right a big passenger jet but then you follow a tutorial on how to fly",
    "start": "237430",
    "end": "244390"
  },
  {
    "text": "it you make one little you get one thing wrong and it crashes and you don't know why so you end up going back to",
    "start": "244390",
    "end": "250150"
  },
  {
    "text": "something like this and really learning the basic stick and rudder skills and and how to take off a land and then when",
    "start": "250150",
    "end": "257049"
  },
  {
    "text": "you got the hang of that this makes a lot more sense because when you go through the tutorial you've got these",
    "start": "257049",
    "end": "262330"
  },
  {
    "text": "conceptual hooks on which to hang your knowledge so we're going to not only fly",
    "start": "262330",
    "end": "267880"
  },
  {
    "text": "this we're going to actually build this as well so so really get to understand it",
    "start": "267880",
    "end": "272999"
  },
  {
    "text": "before we proceed I should make it one word of warning which is that we're",
    "start": "272999",
    "end": "280289"
  },
  {
    "start": "273000",
    "end": "349000"
  },
  {
    "text": "actually going to be be training to neural networks right so the the an",
    "start": "280289",
    "end": "285719"
  },
  {
    "text": "artificial neural network and all the the real human neural networks in your",
    "start": "285719",
    "end": "291389"
  },
  {
    "text": "brain so for the next half an hour or so I've managed to break this down into it into something that's a series of fairly",
    "start": "291389",
    "end": "299129"
  },
  {
    "text": "simple concepts training neural networks is actually fiendishly difficult to really get your head round but I've",
    "start": "299129",
    "end": "305069"
  },
  {
    "text": "broken this down into a series of simple concepts but it's essential if you want to understand that that you pay",
    "start": "305069",
    "end": "311069"
  },
  {
    "text": "attention for about the next half an hour where we go over those slides and don't get distracted otherwise you'll kind of get lost quite",
    "start": "311069",
    "end": "317999"
  },
  {
    "text": "quickly so let's start with it with the basic concept which is a neuron and what",
    "start": "317999",
    "end": "324179"
  },
  {
    "text": "that is so essentially a neuron it has it has one or more inputs usually at",
    "start": "324179",
    "end": "331709"
  },
  {
    "text": "least to all those inputs have weights there's a bias and all of that stuff on",
    "start": "331709",
    "end": "340289"
  },
  {
    "text": "the Left combines to form a total input goes through an active or scored an activation function and gives you an",
    "start": "340289",
    "end": "347789"
  },
  {
    "text": "output now take its simplest example is called a perceptron apperception has a",
    "start": "347789",
    "end": "353550"
  },
  {
    "start": "349000",
    "end": "405000"
  },
  {
    "text": "comparator as the activation function and it asks a simple question is the",
    "start": "353550",
    "end": "358919"
  },
  {
    "text": "total input greater than zero if so then it fires a 1 otherwise it outputs a zero",
    "start": "358919",
    "end": "365899"
  },
  {
    "text": "so let's give an example let's imagine this comparator has three inputs and the",
    "start": "365899",
    "end": "373139"
  },
  {
    "text": "weights are set as as such and the bias is set to minus 1.5 so let's run some",
    "start": "373139",
    "end": "381059"
  },
  {
    "text": "numbers through that and see what we get so imagine we have an eight five in there - if we run that through then we",
    "start": "381059",
    "end": "387389"
  },
  {
    "text": "do that multiplication but the inputs times the weights we add in the bias and we can see there we get 0.5 which is",
    "start": "387389",
    "end": "395789"
  },
  {
    "text": "greater than 0 therefore this would Apple - one like that's pretty much as",
    "start": "395789",
    "end": "401550"
  },
  {
    "text": "as simple as it gets for up for a perceptron so let's look at the state of an Iran",
    "start": "401550",
    "end": "408070"
  },
  {
    "start": "405000",
    "end": "420000"
  },
  {
    "text": "there's two aspects to the state there's the long term state which comprises the",
    "start": "408070",
    "end": "413140"
  },
  {
    "text": "bias and the weights the long term state determines its behavior and then there's",
    "start": "413140",
    "end": "420580"
  },
  {
    "start": "420000",
    "end": "432000"
  },
  {
    "text": "the transient state so when the neuron actually fires it has an output and then",
    "start": "420580",
    "end": "426070"
  },
  {
    "text": "there are the intermediate variables we have in determining the output such as the the total input so let's see what we",
    "start": "426070",
    "end": "434080"
  },
  {
    "start": "432000",
    "end": "448000"
  },
  {
    "text": "can do with this let's let's try and make a NAND gate a simple logic gate",
    "start": "434080",
    "end": "439210"
  },
  {
    "text": "which has that the following truth table if you assign off to zero and on to one",
    "start": "439210",
    "end": "445080"
  },
  {
    "text": "so naught and gate let's see if we can do that and if we set the bias to three",
    "start": "445080",
    "end": "450160"
  },
  {
    "start": "448000",
    "end": "563000"
  },
  {
    "text": "and the weights to both two - - without labouring through all the examples that",
    "start": "450160",
    "end": "456190"
  },
  {
    "text": "will work perfectly which means if we can build a NAND gate with a perceptron you can build any other kind of logic",
    "start": "456190",
    "end": "463180"
  },
  {
    "text": "gate just from NAND gates so this is functionally complete we can do anything just with perceptrons but we could do it",
    "start": "463180",
    "end": "470440"
  },
  {
    "text": "anything before anyway the interesting bit is is how do we actually how do we",
    "start": "470440",
    "end": "476170"
  },
  {
    "text": "get this thing to learn because I put those biases and weights in myself I worked it out but imagine we didn't know that we're",
    "start": "476170",
    "end": "482500"
  },
  {
    "text": "not they were the interesting bit is can this learn can it learn from examples what the correct bias and weights is in",
    "start": "482500",
    "end": "489730"
  },
  {
    "text": "this case so what we do is we start by initializing these values just to small",
    "start": "489730",
    "end": "495910"
  },
  {
    "text": "random numbers and these are actually quite big random numbers but for the sake of the example this this works well",
    "start": "495910",
    "end": "502750"
  },
  {
    "text": "so initial izing as follows and then what we're going to do is we're going to put some some training data through",
    "start": "502750",
    "end": "509350"
  },
  {
    "text": "there so I'm going to put a 0 and a 1 in there let's see what we get we get a 0 on the output because when we take the",
    "start": "509350",
    "end": "516460"
  },
  {
    "text": "combined input when we multiply zero by point two and add it to 1 times minus",
    "start": "516460",
    "end": "522039"
  },
  {
    "text": "point 3 and then adding the bias we get a combined input of minus put 4 in there",
    "start": "522040",
    "end": "529000"
  },
  {
    "text": "and that's that's less than 0 so this outputs are 0 but the output we want in",
    "start": "529000",
    "end": "534730"
  },
  {
    "text": "this case 0 NAND 1 is a 1 so we've got the wrong result so what we do",
    "start": "534730",
    "end": "540930"
  },
  {
    "text": "is we ask ourselves if we made a little increase or decrease to those weights of",
    "start": "540930",
    "end": "547350"
  },
  {
    "text": "bias how would that change the output would it make it a little bit less wrong or and the problem we've got here is if",
    "start": "547350",
    "end": "555240"
  },
  {
    "text": "we make any small changes nothing is going to change in the output right we have to make a really big change for",
    "start": "555240",
    "end": "561300"
  },
  {
    "text": "anything to happen so the problem is that comparative function looks like this it's either on or off 1 or 0 it's",
    "start": "561300",
    "end": "568320"
  },
  {
    "start": "563000",
    "end": "683000"
  },
  {
    "text": "not conducive to learning so what we can do instead we can we can kind of soften that function we can turn it into",
    "start": "568320",
    "end": "574380"
  },
  {
    "text": "something like this which is called a logistic sigmoid which has sort of add values in between it has a gray area so",
    "start": "574380",
    "end": "583070"
  },
  {
    "text": "the other advantage is that when we start building networks of neurons and connecting them together this outputs",
    "start": "583070",
    "end": "589830"
  },
  {
    "text": "real numbers right fractional numbers so that makes a network when we send fractional numbers to other neurons a",
    "start": "589830",
    "end": "596010"
  },
  {
    "text": "bit more rich and expressive so this this will work quite well there is a",
    "start": "596010",
    "end": "602220"
  },
  {
    "text": "problem though because if you look at what the formula is for that that's relatively expensive to calculate now",
    "start": "602220",
    "end": "609000"
  },
  {
    "text": "remember that we need to be training this with tens of thousands and sometimes up to millions and of of",
    "start": "609000",
    "end": "616110"
  },
  {
    "text": "training samples and we may have thousands and Euron so this isn't isn't",
    "start": "616110",
    "end": "621750"
  },
  {
    "text": "the best kind of formula for something that you want to calculate quickly so",
    "start": "621750",
    "end": "626850"
  },
  {
    "text": "over the years people have experimented with simpler formulas to see what works and they found that something remarkably",
    "start": "626850",
    "end": "633480"
  },
  {
    "text": "simple does a pretty good job called a rectified linear unit or a Lu and it's",
    "start": "633480",
    "end": "639990"
  },
  {
    "text": "actually much easier to calculate them to pronounce so with this if X is greater than zero its X otherwise it's",
    "start": "639990",
    "end": "647430"
  },
  {
    "text": "zero now they've also found that in most cases we can we can improve this we can",
    "start": "647430",
    "end": "654030"
  },
  {
    "text": "make a better version of the r lu called a league here le by having having it leaked on the left hand side so if X is",
    "start": "654030",
    "end": "661260"
  },
  {
    "text": "less than zero it's x over a hundred that's still really easy fast quick to",
    "start": "661260",
    "end": "666630"
  },
  {
    "text": "calculate that works really well so now we've got a neuron which outputs",
    "start": "666630",
    "end": "672820"
  },
  {
    "text": "fractional numbers real numbers instead of whole numbers so in the case of our",
    "start": "672820",
    "end": "678910"
  },
  {
    "text": "NAND gate we still need to interpret the answer in the end is it up on run off and to do that is really easy so let's",
    "start": "678910",
    "end": "686050"
  },
  {
    "start": "683000",
    "end": "749000"
  },
  {
    "text": "say it outputs the point two we just say well point two is closer to a zero than",
    "start": "686050",
    "end": "691420"
  },
  {
    "text": "it is to a one so we'll interpret this as off rather than on if it was a point",
    "start": "691420",
    "end": "696610"
  },
  {
    "text": "eight would interpret that as on so we just look at whatever it's closest to but the important thing now is that",
    "start": "696610",
    "end": "701740"
  },
  {
    "text": "we've actually we can measure the rightness or wrongness of this so if it outputs 0.2 and we wanted it to an",
    "start": "701740",
    "end": "709420"
  },
  {
    "text": "output a zero we can say the error is zero point two the error is the actual",
    "start": "709420",
    "end": "714520"
  },
  {
    "text": "output minus desired output if it outputted a if the desired output was a one the error would be a minus 0.8 right",
    "start": "714520",
    "end": "723400"
  },
  {
    "text": "so it seems reasonable that we want to minimize the error across as many of the",
    "start": "723400",
    "end": "729550"
  },
  {
    "text": "samples as we can right but that there is a complication in doing this if we if",
    "start": "729550",
    "end": "736209"
  },
  {
    "text": "we simply say let's minimize the total error across all samples we're not going",
    "start": "736209",
    "end": "741730"
  },
  {
    "text": "to get a very good result imagine we've got full training samples all of them have an error of point to all of them it",
    "start": "741730",
    "end": "750850"
  },
  {
    "start": "749000",
    "end": "816000"
  },
  {
    "text": "is got it right remember point if the error is less than 0.5 it's basically going to get it right since it's not a",
    "start": "750850",
    "end": "757270"
  },
  {
    "text": "big deal that's a good result very good outcome imagine now instead we do this",
    "start": "757270",
    "end": "763150"
  },
  {
    "text": "the same thing the first three have an error of 0 the last one has an error of",
    "start": "763150",
    "end": "768250"
  },
  {
    "text": "0.6 that that's now got the last one wrong an error of greater than 0.5 we'll",
    "start": "768250",
    "end": "773800"
  },
  {
    "text": "get it wrong but the total error is lower right with machine learning we",
    "start": "773800",
    "end": "780010"
  },
  {
    "text": "make the rules it follows them if we say minimize the total error it's going to",
    "start": "780010",
    "end": "785170"
  },
  {
    "text": "favor that bad result over the over the good result right so what we need to do",
    "start": "785170",
    "end": "790839"
  },
  {
    "text": "is something to to punish those big errors like 0.6 more than the small",
    "start": "790839",
    "end": "796930"
  },
  {
    "text": "errors like point two and the easiest thing to do is to square the error first",
    "start": "796930",
    "end": "802720"
  },
  {
    "text": "if we square the before for adding them up then we're going to fix that problem we're going to",
    "start": "802720",
    "end": "808449"
  },
  {
    "text": "punish the big errors more than the small errors so we're not minimizing the total error we're minimizing something",
    "start": "808449",
    "end": "815290"
  },
  {
    "text": "else which we call the loss the loss is the variable trying to minimize and we",
    "start": "815290",
    "end": "821170"
  },
  {
    "start": "816000",
    "end": "875000"
  },
  {
    "text": "can define the loss function however we like but usually for categorizations",
    "start": "821170",
    "end": "826829"
  },
  {
    "text": "issues what we're doing now it's the error squared summed across all training",
    "start": "826829",
    "end": "832209"
  },
  {
    "text": "samples losses are some of the error squared where the error is the actual minus a desired output now a couple of",
    "start": "832209",
    "end": "840160"
  },
  {
    "text": "nuances we could write this instead of half some error squared identical right",
    "start": "840160",
    "end": "847240"
  },
  {
    "text": "if you try to minimize the sum of the error squared it's you're going to get the same result as if you try and",
    "start": "847240",
    "end": "853180"
  },
  {
    "text": "minimize half of the sum of the earthquake the reason we favor the second one it creates a nice",
    "start": "853180",
    "end": "859560"
  },
  {
    "text": "mathematical symmetry that we'll see soon another point is that instead of",
    "start": "859560",
    "end": "865060"
  },
  {
    "text": "putting Sun we could put average and again it would do exactly the same job loss is also sometimes called cost cost",
    "start": "865060",
    "end": "873040"
  },
  {
    "text": "loss same thing now here's an interesting point we don't usually calculate the loss",
    "start": "873040",
    "end": "879670"
  },
  {
    "start": "875000",
    "end": "1269000"
  },
  {
    "text": "which might seem weird if we want to minimize something surely the first thing you do is you work out what it is",
    "start": "879670",
    "end": "886029"
  },
  {
    "text": "you're minimizing but you imagine you know if you think of yourself as a good driver you're constantly trying to do",
    "start": "886029",
    "end": "893470"
  },
  {
    "text": "things to minimize the chance of an accident and you might be quite good at that but if you were if I was to ask you",
    "start": "893470",
    "end": "899889"
  },
  {
    "text": "what is the chance of you having an accident one in what you probably not",
    "start": "899889",
    "end": "905589"
  },
  {
    "text": "going to be able to tell me right so you can take actions to minimize something without actually working it out and if",
    "start": "905589",
    "end": "912730"
  },
  {
    "text": "you look in through a neural network for the code that squares the error to the",
    "start": "912730",
    "end": "917889"
  },
  {
    "text": "loss you won't usually find it so let's",
    "start": "917889",
    "end": "923350"
  },
  {
    "text": "go back now and plug that leak you Lu it into our NAND gate so again we start out",
    "start": "923350",
    "end": "928629"
  },
  {
    "text": "with small random values and this time I'm going to plug a 1 and a 0 in there",
    "start": "928629",
    "end": "934149"
  },
  {
    "text": "so the combined input is now 0 point three right now in this case the",
    "start": "934149",
    "end": "942790"
  },
  {
    "text": "desired output is one the actual output is also point three because when we put",
    "start": "942790",
    "end": "948489"
  },
  {
    "text": "point three through that value because it's greater than zero then it's simply point three it's a one-to-one so now the",
    "start": "948489",
    "end": "955089"
  },
  {
    "text": "error is minus zero point seven so we can see just from the sign of the error",
    "start": "955089",
    "end": "961779"
  },
  {
    "text": "that we need to increase the output to reduce the error and the loss a small",
    "start": "961779",
    "end": "968170"
  },
  {
    "text": "increase to the output will have a good effect you see that so simplest thing we",
    "start": "968170",
    "end": "975399"
  },
  {
    "text": "can possibly say in this case is we need to increase the output to reduce the error and the loss so that means we also",
    "start": "975399",
    "end": "982449"
  },
  {
    "text": "need to increase the input why because we're on the positive side of the rail",
    "start": "982449",
    "end": "987790"
  },
  {
    "text": "you right we're aware so the input and output in a one-to-one correspondence so we also need to increase the input which",
    "start": "987790",
    "end": "994660"
  },
  {
    "text": "we could do by increasing that of those biases and weights a little so what we",
    "start": "994660",
    "end": "1000089"
  },
  {
    "text": "could do is now that we've worked out what we need to do for this training sample we could take all of the training",
    "start": "1000089",
    "end": "1007619"
  },
  {
    "text": "samples and repeat the same calculation and get votes we're saying how many",
    "start": "1007619",
    "end": "1013049"
  },
  {
    "text": "training samples want to increase the input how many training samples want to decrease the input and whichever vote",
    "start": "1013049",
    "end": "1020429"
  },
  {
    "text": "wins then we make a little adjustment to the weights and biases and then we repeat the process again now that would",
    "start": "1020429",
    "end": "1028798"
  },
  {
    "text": "kind of work you can see intuitively that would that would help us gradually improve but what we will be doing is",
    "start": "1028799",
    "end": "1036899"
  },
  {
    "text": "minimizing the error the total error not the total loss right we would not be",
    "start": "1036899",
    "end": "1043860"
  },
  {
    "text": "punishing the big errors more than the small errors and I remember how important that is to get a good result",
    "start": "1043860",
    "end": "1049980"
  },
  {
    "text": "we've got to punish the big errors more than the small errors so how might we do that well it's quite simple really",
    "start": "1049980",
    "end": "1057539"
  },
  {
    "text": "instead of simply getting a vote to increase or decrease the output we get a",
    "start": "1057539",
    "end": "1063870"
  },
  {
    "text": "variable number of votes the number of votes you get is proportional to the error so in this case the error is minus",
    "start": "1063870",
    "end": "1071370"
  },
  {
    "text": "foot so we get zero point seven votes to increase the output all right if the",
    "start": "1071370",
    "end": "1077940"
  },
  {
    "text": "error was only minus point one would only get point one votes to increase the",
    "start": "1077940",
    "end": "1083790"
  },
  {
    "text": "output right so that way we were the big errors are waiting much more than the",
    "start": "1083790",
    "end": "1089640"
  },
  {
    "text": "small errors right and we flipped the sign because we're trying to minimize the loss on the error not it not to",
    "start": "1089640",
    "end": "1096570"
  },
  {
    "text": "maximize it right so that means again we can say that the input also gets point",
    "start": "1096570",
    "end": "1104250"
  },
  {
    "text": "seven votes so what we could do now is we could take all of those training",
    "start": "1104250",
    "end": "1109950"
  },
  {
    "text": "samples work out the number of votes and add those votes together and then based",
    "start": "1109950",
    "end": "1115770"
  },
  {
    "text": "on that we can now make adjustments to the weights and bias now that would work",
    "start": "1115770",
    "end": "1122280"
  },
  {
    "text": "and that that's called batched learning right we collate all the votes for a",
    "start": "1122280",
    "end": "1127500"
  },
  {
    "text": "large batch or all of our training samples and once we've got the the total net votes we may then make the",
    "start": "1127500",
    "end": "1134550"
  },
  {
    "text": "adjustments but there is actually a slightly more efficient way we can do this called stochastic learning",
    "start": "1134550",
    "end": "1141720"
  },
  {
    "text": "so rather than collating all the votes up for all the training samples we can update those biases and weights as we go",
    "start": "1141720",
    "end": "1149700"
  },
  {
    "text": "along as long as we make really small changes we can make a change after every",
    "start": "1149700",
    "end": "1155580"
  },
  {
    "text": "single training sample that avoids having to do that collation all right so with stochastic learning what we do is",
    "start": "1155580",
    "end": "1162120"
  },
  {
    "text": "we take those point seven votes and we multiply it by little factor called the",
    "start": "1162120",
    "end": "1167190"
  },
  {
    "text": "learning rate all right in this case it's 0.01 so when we adjust that we get put double-oh-seven so now what we do is",
    "start": "1167190",
    "end": "1174930"
  },
  {
    "text": "we we need to increase the bias by that amount so the bias becomes put 107 and",
    "start": "1174930",
    "end": "1181500"
  },
  {
    "text": "we also increase the weight so the first weight becomes point 207 what about the",
    "start": "1181500",
    "end": "1187020"
  },
  {
    "text": "second way we've not increased that at all right why have we not adjusted the second way rights because the the inputs",
    "start": "1187020",
    "end": "1197280"
  },
  {
    "text": "0 for the second one there's no point doing anything to that because it had no influence on it right so you could only",
    "start": "1197280",
    "end": "1204120"
  },
  {
    "text": "do harm we don't touch that right so before increasing or decreasing the weight we multiply by the value of",
    "start": "1204120",
    "end": "1212549"
  },
  {
    "text": "the input right that also takes care of the scenario of this being negative",
    "start": "1212549",
    "end": "1217710"
  },
  {
    "text": "imagine that was negative one that input we will need to do the opposite we would need to decrease the weight in order to",
    "start": "1217710",
    "end": "1224309"
  },
  {
    "text": "increase the total input why does that make sense okay so that in a nutshell is",
    "start": "1224309",
    "end": "1232429"
  },
  {
    "text": "stochastic learning this is how you train a neuron we've almost covered",
    "start": "1232429",
    "end": "1237630"
  },
  {
    "text": "everything right there there's one thing though that I want to touch on which is that by doing this we are actually",
    "start": "1237630",
    "end": "1245899"
  },
  {
    "text": "minimizing the total loss the square of the error and even though we never",
    "start": "1245899",
    "end": "1253620"
  },
  {
    "text": "worked it out we didn't do any squaring we're actually doing that I'm going to for the sake of of interest I'm going to",
    "start": "1253620",
    "end": "1260010"
  },
  {
    "text": "explain to you how it is that we're actually now minimizing the square of the error so if you can't follow this",
    "start": "1260010",
    "end": "1266309"
  },
  {
    "text": "it's not critical but it is interesting to see how it works so let's plot on a",
    "start": "1266309",
    "end": "1271770"
  },
  {
    "start": "1269000",
    "end": "1453000"
  },
  {
    "text": "graph the input versus the error so we had an input of 0.3 and an error of",
    "start": "1271770",
    "end": "1278659"
  },
  {
    "text": "minus 0.7 but you can see from this graph that if we increase the input a",
    "start": "1278659",
    "end": "1284250"
  },
  {
    "text": "little bit like we did the error is going to go down so now let's also plot",
    "start": "1284250",
    "end": "1289830"
  },
  {
    "text": "this imaginary loss curve this this graph of the square of the error the",
    "start": "1289830",
    "end": "1295350"
  },
  {
    "text": "thing we never calculated the thing which won and minimize that's what it would look like if we took plot it now",
    "start": "1295350",
    "end": "1301049"
  },
  {
    "text": "there's a very interesting coincidence here because if we look at what the slope of that is at that point it",
    "start": "1301049",
    "end": "1306990"
  },
  {
    "text": "exactly matches the error it does that at every point in there and we can prove",
    "start": "1306990",
    "end": "1314309"
  },
  {
    "text": "this mathematically by differentiating the loss function right then if you",
    "start": "1314309",
    "end": "1319799"
  },
  {
    "text": "differentiate y equals 1/2 x squared you just get y equals x differentiate that",
    "start": "1319799",
    "end": "1325169"
  },
  {
    "text": "blue line you get the grey line so we know that the slope is always the same as the error and that means remember we",
    "start": "1325169",
    "end": "1332039"
  },
  {
    "text": "took a little step to the right proportional to that point 7 what that",
    "start": "1332039",
    "end": "1337559"
  },
  {
    "text": "means is that we travel down that blue slope at a rate that's proportional to the slope right imagine",
    "start": "1337559",
    "end": "1345150"
  },
  {
    "text": "that that's a slide that blue line and you're sitting on that slide you're going to start sliding down at a force",
    "start": "1345150",
    "end": "1351900"
  },
  {
    "text": "proportional to how steep it is this is called gradient descent we keep taking a",
    "start": "1351900",
    "end": "1358260"
  },
  {
    "text": "little step in proportion to the slope now if we were doing batched learning we",
    "start": "1358260",
    "end": "1364950"
  },
  {
    "text": "batched gradient descent we will just keep going along that line till we got to the bottom but because we're doing",
    "start": "1364950",
    "end": "1371690"
  },
  {
    "text": "stochastic gradient descent we change that those graphs for every training",
    "start": "1371690",
    "end": "1379320"
  },
  {
    "text": "sample so we switched to a new training sample though those are going to look different now right the error is going",
    "start": "1379320",
    "end": "1384360"
  },
  {
    "text": "to be in a different place so now let's say we happen to for the next training sample the error happened to be z+ 0.35",
    "start": "1384360",
    "end": "1393030"
  },
  {
    "text": "that would mean we can we would know that the square of that the loss function would have a slope of also 0.35",
    "start": "1393030",
    "end": "1401790"
  },
  {
    "text": "so that means that we would take a little step to the left not as big as our last step only half as much right",
    "start": "1401790",
    "end": "1408420"
  },
  {
    "text": "because we've only got half the error all right so the votes corresponds to this to the",
    "start": "1408420",
    "end": "1415340"
  },
  {
    "text": "negative error which corresponds to the that the slope of the loss function so",
    "start": "1415340",
    "end": "1422100"
  },
  {
    "text": "you can see we've taken half of that step back because it was only 0.35 so we",
    "start": "1422100",
    "end": "1427500"
  },
  {
    "text": "haven't quite ended up where we started in fact the position that we've ended up in is very much the same as the position",
    "start": "1427500",
    "end": "1434760"
  },
  {
    "text": "we would have ended up in had we done a gradient descent on the combined loss",
    "start": "1434760",
    "end": "1440280"
  },
  {
    "text": "for both of those training samples if we added those two blue lines together and",
    "start": "1440280",
    "end": "1446040"
  },
  {
    "text": "did a gradient descent on that we would end up in pretty much the same place okay so to summarize stochastic gradient",
    "start": "1446040",
    "end": "1454740"
  },
  {
    "start": "1453000",
    "end": "1472000"
  },
  {
    "text": "descent for each training sample determine the slope of the loss function at the current position but the error",
    "start": "1454740",
    "end": "1461100"
  },
  {
    "text": "tells a slope that gives us a number of votes and we step the following amount minus the slope times our learning rate",
    "start": "1461100",
    "end": "1467850"
  },
  {
    "text": "and that that's pretty much it to gradient descent so let's build it now there's two",
    "start": "1467850",
    "end": "1477590"
  },
  {
    "text": "samples in this and you can download those and work through those yourself so from link pad if you go to the samples",
    "start": "1477590",
    "end": "1485270"
  },
  {
    "text": "in link pad and click download import more samples then if you scroll to the",
    "start": "1485270",
    "end": "1490580"
  },
  {
    "text": "bottom you'll see that the samples for this for this training session so",
    "start": "1490580",
    "end": "1495880"
  },
  {
    "text": "they're all they're all in there so you can work through this yourself now there's two samples the first one is",
    "start": "1495880",
    "end": "1500930"
  },
  {
    "text": "this NAND gate for the neuron so this is how I've created a neuron we've got two",
    "start": "1500930",
    "end": "1506810"
  },
  {
    "text": "weights in there hard-coded this year on to have two weights it has a bias and we",
    "start": "1506810",
    "end": "1511880"
  },
  {
    "text": "start by initializing these two small random numbers that's it that's all I've",
    "start": "1511880",
    "end": "1516920"
  },
  {
    "text": "got for the neuron now you're probably wondering why I've done this but I've separated the neuron into two classes",
    "start": "1516920",
    "end": "1524150"
  },
  {
    "text": "it's good programming practice in general to separate your long-term state",
    "start": "1524150",
    "end": "1529730"
  },
  {
    "text": "from your transient state so I've created another class called firing",
    "start": "1529730",
    "end": "1534950"
  },
  {
    "text": "neuron which captures the transient state of a neuron firing references the neuron and has the output and that",
    "start": "1534950",
    "end": "1542180"
  },
  {
    "text": "intermediate variable total input in there and this is a method that fires a",
    "start": "1542180",
    "end": "1548300"
  },
  {
    "text": "neuron we take two inputs the total input we work that out it's each input times the corresponding",
    "start": "1548300",
    "end": "1555320"
  },
  {
    "text": "weight it's exactly what we were doing to fire it we add in the bias now we apply the activation function right",
    "start": "1555320",
    "end": "1562250"
  },
  {
    "text": "there Elliot so all we do is we say if the input is greater than zero then we",
    "start": "1562250",
    "end": "1567800"
  },
  {
    "text": "just take the input otherwise we divide it by hundred that's all it is it's a simple activation function and this is",
    "start": "1567800",
    "end": "1574370"
  },
  {
    "text": "how we learn the second one first to learn we fire the input so we know what",
    "start": "1574370",
    "end": "1580010"
  },
  {
    "text": "the output is and the total input and then we work out those output votes",
    "start": "1580010",
    "end": "1585080"
  },
  {
    "text": "write the output votes is the negative of the error right so it's just the",
    "start": "1585080",
    "end": "1590990"
  },
  {
    "text": "expected output minus the output that tells us how wrong we are and how much the output needs to change you'll have",
    "start": "1590990",
    "end": "1598130"
  },
  {
    "text": "the outputs influence on the loss so then remember we need to multiply by the",
    "start": "1598130",
    "end": "1605000"
  },
  {
    "text": "slope of the a function so I just want to go back now because there's something I didn't",
    "start": "1605000",
    "end": "1610039"
  },
  {
    "text": "wasn't quite explicit about which is this situation so the output here got",
    "start": "1610039",
    "end": "1617390"
  },
  {
    "text": "0.7 votes and the input also got 0.7 votes now imagine that this input here",
    "start": "1617390",
    "end": "1625020"
  },
  {
    "text": "was actually a negative number so we're now on the leaky side of the rail you right so what that would mean is the",
    "start": "1625020",
    "end": "1632460"
  },
  {
    "text": "input would get how many votes exactly",
    "start": "1632460",
    "end": "1640980"
  },
  {
    "text": "we divide it by 100 the input will get point double-oh-seven votes so to work",
    "start": "1640980",
    "end": "1646049"
  },
  {
    "text": "out how many votes to give the input we take the output votes we multiply it by",
    "start": "1646049",
    "end": "1651480"
  },
  {
    "text": "the slope of the activation function this is the chain rule in calculus if",
    "start": "1651480",
    "end": "1657480"
  },
  {
    "text": "you didn't understand the chain rule before you understand it now okay so",
    "start": "1657480",
    "end": "1667649"
  },
  {
    "text": "that there we are we're multiplying the by the slope of the activation function now timidly adjustment we multiplied by",
    "start": "1667649",
    "end": "1675360"
  },
  {
    "text": "the learning rate that's small that small point oh one and then we can now increase the bias by that adjustment and",
    "start": "1675360",
    "end": "1681870"
  },
  {
    "text": "we can also adjust these weights remember we multiply the adjustment by the input so if it's zero it doesn't do",
    "start": "1681870",
    "end": "1688799"
  },
  {
    "text": "anything if it's minus one it flips it the other way around so that's it that's how we learn only other thing here is",
    "start": "1688799",
    "end": "1695580"
  },
  {
    "text": "the the code to do the training we need to manufacture some testers and some",
    "start": "1695580",
    "end": "1701010"
  },
  {
    "text": "TARDIS so I won't go into the code that I do that with a link query it's probably easiest just to just to dump it",
    "start": "1701010",
    "end": "1707370"
  },
  {
    "text": "out so we can have a look at what that looks like what the training data looks like so the data simply we've generated",
    "start": "1707370",
    "end": "1713880"
  },
  {
    "text": "a whole lot of random numbers here and these are the inputs and this is the",
    "start": "1713880",
    "end": "1720510"
  },
  {
    "text": "desired output sort of the NAND gate is functioning properly this is how it should work we can manufacture as much",
    "start": "1720510",
    "end": "1726450"
  },
  {
    "text": "training data as we want here really easy so let's get rid of that so that's",
    "start": "1726450",
    "end": "1734039"
  },
  {
    "text": "first step is to to manufacture the data second step is we divide it up into a",
    "start": "1734039",
    "end": "1739140"
  },
  {
    "text": "training set and a test set so I'm saying let's take 80% of that data and we're going to",
    "start": "1739140",
    "end": "1745279"
  },
  {
    "text": "use that for training we're going to put the remaining 20% aside for testing",
    "start": "1745279",
    "end": "1751370"
  },
  {
    "text": "because once we've trained it we don't really know how accurate it is if we",
    "start": "1751370",
    "end": "1756799"
  },
  {
    "text": "simply rely on the training data it might got a very good with that training data but not so good at generalizing to",
    "start": "1756799",
    "end": "1762919"
  },
  {
    "text": "data it hasn't seen so we have to we have to put a separate set of data it hasn't seen the testing data to know how",
    "start": "1762919",
    "end": "1769580"
  },
  {
    "text": "good it is so then we train it for each sample in that training set we call the",
    "start": "1769580",
    "end": "1775669"
  },
  {
    "text": "learn method on that and you notice I'm only going through that training set once I'm only doing one epoch because",
    "start": "1775669",
    "end": "1785110"
  },
  {
    "text": "normally we would do lots of epochs we would shuffle the data and do that again but in this case we have a limitless",
    "start": "1785110",
    "end": "1792320"
  },
  {
    "text": "amount of training data we don't need to do that we could if we want more epochs we could just make more training data",
    "start": "1792320",
    "end": "1799120"
  },
  {
    "text": "finally I'm doing a report on how accurate it was so we're firing the",
    "start": "1799120",
    "end": "1804169"
  },
  {
    "text": "neurons over the testing set and then we'll just see how many successes and failures who have got and how that the",
    "start": "1804169",
    "end": "1810799"
  },
  {
    "text": "error was so we're going to run this now with a thousand sample so it's 800 for",
    "start": "1810799",
    "end": "1816379"
  },
  {
    "text": "training 200 for testing and I've set the learning rate 2.0 - so let's run",
    "start": "1816379",
    "end": "1821659"
  },
  {
    "text": "this and see what we get now that's the final state of the neuron it's automatically worked out from all these",
    "start": "1821659",
    "end": "1827509"
  },
  {
    "text": "training cycles that these are the other right weights and biases and it's got",
    "start": "1827509",
    "end": "1833299"
  },
  {
    "text": "all 200 correct so that hasn't worked perfectly in there we can also look at",
    "start": "1833299",
    "end": "1838820"
  },
  {
    "text": "I've also imported on the total error magnitude and that's a histogram of all the errors that were been getting and",
    "start": "1838820",
    "end": "1844970"
  },
  {
    "text": "because none of them are greater than put 5 it means none of them it's got the wrong the wrong result so you can have a",
    "start": "1844970",
    "end": "1852409"
  },
  {
    "text": "lot of fun with this and play with this and that because this is so simple and this captures really everything there is to pretty much - training a neuron you",
    "start": "1852409",
    "end": "1859399"
  },
  {
    "text": "can learn a lot from this so if we reduce the total number of samples to a hundred we'll find that this will no",
    "start": "1859399",
    "end": "1866690"
  },
  {
    "text": "longer work so we've got 12 correct and ain't wrong which simply don't have enough training samples to make the",
    "start": "1866690",
    "end": "1873799"
  },
  {
    "text": "work we need more than a hundred we could also see if we if we changed the learning rate to to say point five",
    "start": "1873799",
    "end": "1879919"
  },
  {
    "text": "increased the learning rate too much what happens if the learning rates too big is it it will oscillate right it",
    "start": "1879919",
    "end": "1887480"
  },
  {
    "text": "will it will keep going imagine you're on that slide and you're bouncing you're sliding down that that's s-curve you get",
    "start": "1887480",
    "end": "1893899"
  },
  {
    "text": "to the bottom you're going to go up the other side right so we ended we oscillate it doesn't work if the learning rate is too high so you can you",
    "start": "1893899",
    "end": "1900499"
  },
  {
    "text": "can experiment with all of this and see how well it works you can also we can this is a NAND gate",
    "start": "1900499",
    "end": "1907999"
  },
  {
    "text": "that we're training it to do but we could quite easily make it do other kinds of functions if I change this an",
    "start": "1907999",
    "end": "1915859"
  },
  {
    "text": "to an or it's now going to become an or gate so we can see whether we can train it to do an or gate and we can that",
    "start": "1915859",
    "end": "1921799"
  },
  {
    "text": "works just as well as as an angei but what about if I try to do an XOR in",
    "start": "1921799",
    "end": "1927859"
  },
  {
    "text": "there can it do that I put an X or in this case it all doesn't work right",
    "start": "1927859",
    "end": "1934100"
  },
  {
    "text": "we've got we've got it's not doesn't seem to be working at all and this we try this it will not it will not train and there's no there's nothing wrong",
    "start": "1934100",
    "end": "1941149"
  },
  {
    "text": "with our training the problem here is that you cannot do an exclusive all with just one year on we need we need a",
    "start": "1941149",
    "end": "1947960"
  },
  {
    "text": "number of neurons to make this work right which brings us on to the next",
    "start": "1947960",
    "end": "1953119"
  },
  {
    "text": "point because right now I did promise that we would we would build a Cessna but we're really only built a",
    "start": "1953119",
    "end": "1959659"
  },
  {
    "text": "hang-glider at this point right because it's there's there's nothing really very exciting about her and NAND gate so what",
    "start": "1959659",
    "end": "1966679"
  },
  {
    "start": "1965000",
    "end": "2178000"
  },
  {
    "text": "we need to do now is to is is to turn this neuron into a whole network of",
    "start": "1966679",
    "end": "1972080"
  },
  {
    "text": "neurons so this is the the architecture for a neural network we have the inputs on the left on the far right we have a",
    "start": "1972080",
    "end": "1979970"
  },
  {
    "text": "layer of output neurons so in the case of say doing character recognition each",
    "start": "1979970",
    "end": "1988850"
  },
  {
    "text": "of those those neurons on the right corresponds to whether it thinks it's that number so if whichever one gives us",
    "start": "1988850",
    "end": "1996889"
  },
  {
    "text": "the highest output that's the answer it thinks it is so that's the output layer on the right hand side in the middle",
    "start": "1996889",
    "end": "2002470"
  },
  {
    "text": "anything in the middle we call those the input layers so the idea is that the",
    "start": "2002470",
    "end": "2007660"
  },
  {
    "text": "laya connects to to the layer to the right of it so it's hooked up as a as a",
    "start": "2007660",
    "end": "2014820"
  },
  {
    "text": "directed acyclic graph so the IMP the",
    "start": "2014820",
    "end": "2020410"
  },
  {
    "text": "outputs of one year on feed the inputs of another now there is almost nothing else that we need to do that we haven't",
    "start": "2020410",
    "end": "2027130"
  },
  {
    "text": "already done and the process of training this how do we train a network rather",
    "start": "2027130",
    "end": "2032350"
  },
  {
    "text": "what's different about training a network over a single neuron we just train each neuron so let's start with",
    "start": "2032350",
    "end": "2039490"
  },
  {
    "text": "that with the firing of the network if we want to fire the net the whole neural network we go through each of these",
    "start": "2039490",
    "end": "2045790"
  },
  {
    "text": "neurons in the hidden layer and we fire them one at a time then we move to the",
    "start": "2045790",
    "end": "2051100"
  },
  {
    "text": "right and go to the next hidden layer or the output layer in this case and we fire each of these neurons in sequence",
    "start": "2051100",
    "end": "2058780"
  },
  {
    "text": "and we're moving from left to right when we're firing the network that's called feed-forward when we're firing the whole",
    "start": "2058780",
    "end": "2064060"
  },
  {
    "text": "neural network and imagine we end up with these outputs we look for whichever one is the highest which is an eighth",
    "start": "2064060",
    "end": "2070659"
  },
  {
    "text": "that means it thinks it's an eighth so we can see from this that big arrows are",
    "start": "2070660",
    "end": "2076270"
  },
  {
    "text": "going to be worse than small errors just as with our NAND gate so we would use the same loss function of an arrow",
    "start": "2076270",
    "end": "2083050"
  },
  {
    "text": "squared if we wanted to train this so how do we do the training how do we do the the training cycle and again very",
    "start": "2083050",
    "end": "2090310"
  },
  {
    "text": "similar to what we were doing before we start by with all our output neurons we go through the monitor time and we run",
    "start": "2090310",
    "end": "2096490"
  },
  {
    "text": "that same calculation we did before we work out the output votes right the arm",
    "start": "2096490",
    "end": "2102430"
  },
  {
    "text": "out which is effectively that the slope of the loss function so work out the number of output votes which is the the",
    "start": "2102430",
    "end": "2108930"
  },
  {
    "text": "output minus the desired output for each of these then we work out the input",
    "start": "2108930",
    "end": "2114520"
  },
  {
    "text": "votes exactly the same way but it's the same as the output votes if the inputs positive if it's negative we divide by",
    "start": "2114520",
    "end": "2121060"
  },
  {
    "text": "hundred that's the slope of the rally now there's one extra step we need to",
    "start": "2121060",
    "end": "2126760"
  },
  {
    "text": "now go to the hidden layer we go backwards right this is called back propagation when you're training a when",
    "start": "2126760",
    "end": "2133240"
  },
  {
    "text": "you're training a neural network the process are going from right to left it's called back propagation so how do",
    "start": "2133240",
    "end": "2139030"
  },
  {
    "text": "we work out the number of out pop votes in the hidden layer on these hidden neurons how do we do that and",
    "start": "2139030",
    "end": "2145180"
  },
  {
    "text": "it's the simplest thing that could possibly plausibly work the the number",
    "start": "2145180",
    "end": "2150430"
  },
  {
    "text": "of output votes you get on a a neuron in the hidden layer is simply the number of",
    "start": "2150430",
    "end": "2157740"
  },
  {
    "text": "input votes for all of the the layer to the right weighted",
    "start": "2157740",
    "end": "2163500"
  },
  {
    "text": "it's the weighted sum of all of these input votes here gives you the output",
    "start": "2163500",
    "end": "2169210"
  },
  {
    "text": "votes for each of these neurons here that's the only extra bit of information",
    "start": "2169210",
    "end": "2174520"
  },
  {
    "text": "you need to build a neural net right so we're going to do this now",
    "start": "2174520",
    "end": "2180370"
  },
  {
    "text": "and we're going to and I'm going to run through that example now I won't i won't",
    "start": "2180370",
    "end": "2186730"
  },
  {
    "text": "labor through the source code there's a lot more code here but it's just plumbing it's just hooking all the",
    "start": "2186730",
    "end": "2192340"
  },
  {
    "text": "neurons up there's only one piece of extra code here which which we've not",
    "start": "2192340",
    "end": "2197380"
  },
  {
    "text": "seen really in the next example that's actually related to the training of the",
    "start": "2197380",
    "end": "2202620"
  },
  {
    "text": "the training process which is when we work out the number of output votes so",
    "start": "2202620",
    "end": "2208900"
  },
  {
    "text": "here we are so when we're working out the number of output votes in the hidden neurons we do that weighted sum so we go",
    "start": "2208900",
    "end": "2216130"
  },
  {
    "text": "to the layer to the right that we just processed and we do a weighted sum of all of its input votes times the weight",
    "start": "2216130",
    "end": "2224380"
  },
  {
    "text": "of that neuron that's the only extra bit of code related to the actual process of",
    "start": "2224380",
    "end": "2230460"
  },
  {
    "text": "learning that so I'm going to - we're going to start running this now I will",
    "start": "2230460",
    "end": "2238050"
  },
  {
    "text": "make one note on that the fact that with because we separated the fight that the",
    "start": "2238050",
    "end": "2244000"
  },
  {
    "text": "the transient state from the long term states what that meant is that we were able to easily parallel eyes",
    "start": "2244000",
    "end": "2250300"
  },
  {
    "text": "so this actually uses all the calls when your processor and it didn't take much code to do that at all and there's just",
    "start": "2250300",
    "end": "2256990"
  },
  {
    "text": "one place where we needed to lock foot for thread safety so okay so let's run this now the first thing we need to do",
    "start": "2256990",
    "end": "2263170"
  },
  {
    "text": "is get some training data there is a public domain database of training images there's around 50 with this",
    "start": "2263170",
    "end": "2271510"
  },
  {
    "text": "50,000 training images of hand written digits and another 10,000 which are for testing called the M NIST",
    "start": "2271510",
    "end": "2278230"
  },
  {
    "text": "database this sample automatically downloads them from the internet see you're all ready to go so we creating",
    "start": "2278230",
    "end": "2284980"
  },
  {
    "text": "your net now the number of input neurons it has they're not actually neurons",
    "start": "2284980",
    "end": "2290050"
  },
  {
    "text": "they're just inputs it's the because it's a square image it's the image width times the height right we're going to",
    "start": "2290050",
    "end": "2297790"
  },
  {
    "text": "create one with 20 neurons in the hidden layer and 10 neurons in the output layer",
    "start": "2297790",
    "end": "2305580"
  },
  {
    "text": "right then I'm going to train this and use a learning rate of point O one and",
    "start": "2305580",
    "end": "2310950"
  },
  {
    "text": "we're going to run this for ten a pox and then we're going to report on the",
    "start": "2310950",
    "end": "2316270"
  },
  {
    "text": "failure see how well we've done so let's run this so this is a first epoch it's",
    "start": "2316270",
    "end": "2324310"
  },
  {
    "text": "going through it for the first time ninety-three percent so far 95 percent with the second one ninety five point",
    "start": "2324310",
    "end": "2330640"
  },
  {
    "text": "two ninety five point five it keeps going up with each epoch and eventually",
    "start": "2330640",
    "end": "2336369"
  },
  {
    "text": "it kind of settles it but that highest working again so we're up to now after",
    "start": "2336369",
    "end": "2341830"
  },
  {
    "text": "ninety pox we're up to ninety six point one percent with the training data",
    "start": "2341830",
    "end": "2347230"
  },
  {
    "text": "that's the accuracy now with the testing data it's a bit lower because when we",
    "start": "2347230",
    "end": "2352540"
  },
  {
    "text": "give it some data it hasn't actually seen it doesn't usually do quite as well but in this case it still got ninety",
    "start": "2352540",
    "end": "2359260"
  },
  {
    "text": "five point four percent accuracy which is pretty good for thirty brain cells right this is a home-baked solution with",
    "start": "2359260",
    "end": "2366730"
  },
  {
    "text": "30 brain cells and we've got ninety five point four percent accuracy with this this is all a white box this uses no",
    "start": "2366730",
    "end": "2373570"
  },
  {
    "text": "libraries you can go and have a look understand it and play with it so let's look at the failures with the highest",
    "start": "2373570",
    "end": "2379930"
  },
  {
    "text": "loss these so these are the ones that",
    "start": "2379930",
    "end": "2386470"
  },
  {
    "text": "had the most trouble with right these are the digit you can see that is you can see why that's quite hard to pick up",
    "start": "2386470",
    "end": "2393779"
  },
  {
    "text": "that one had a dot in it that confused it okay so that gives you an idea I",
    "start": "2394230",
    "end": "2400960"
  },
  {
    "text": "don't even what that is that's supposed to be that's a six I think it's done",
    "start": "2400960",
    "end": "2406330"
  },
  {
    "text": "pretty well so what we're going to do now is I'm gonna run this again I've written as a",
    "start": "2406330",
    "end": "2413110"
  },
  {
    "text": "link pad visualizer for the trip for the trainer which means that you can dump it and it dumps out a WPF control so we can",
    "start": "2413110",
    "end": "2419980"
  },
  {
    "text": "watch it learn I'm gonna run this again and we're going to watch it as it learns",
    "start": "2419980",
    "end": "2425340"
  },
  {
    "text": "so this now is showing you the color",
    "start": "2425340",
    "end": "2430390"
  },
  {
    "text": "indicates the bias you can see the bias taking effect in the color of the of all of these connectors indicates the",
    "start": "2430390",
    "end": "2436840"
  },
  {
    "text": "weights so we can watch the weights and biases it learns right now this is",
    "start": "2436840",
    "end": "2442420"
  },
  {
    "text": "showing you the hidden layers and the output layer it doesn't show you all of",
    "start": "2442420",
    "end": "2447760"
  },
  {
    "text": "the inputs because there's too many it doesn't show you all of the weights between the inputs and the hidden layer",
    "start": "2447760",
    "end": "2453490"
  },
  {
    "text": "because there's too many of those but it still shows you most of what's interesting so we can what we can now",
    "start": "2453490",
    "end": "2459070"
  },
  {
    "text": "see what this kind of neural net looks like enough also I've got the D I've put in a pad where we can actually write a",
    "start": "2459070",
    "end": "2465100"
  },
  {
    "text": "number and then see whether it works because you know it's one thing just to run it over these these test images but",
    "start": "2465100",
    "end": "2470860"
  },
  {
    "text": "it's another thing to actually write yourself so I'm gonna put it I'm gonna draw one in there Oh thinks it's a seven",
    "start": "2470860",
    "end": "2477780"
  },
  {
    "text": "let's try that again put another one in there it thinks it's a zero now when",
    "start": "2477780",
    "end": "2483190"
  },
  {
    "text": "something like this happens this is when you really learn what's going on right how is it it did so well with that test",
    "start": "2483190",
    "end": "2490390"
  },
  {
    "text": "data and so poorly but when we actually use it in practice and when you look at",
    "start": "2490390",
    "end": "2495550"
  },
  {
    "text": "all of those those training images and test images what you noticed is that all of those images are perfectly centered",
    "start": "2495550",
    "end": "2503490"
  },
  {
    "text": "right so and when they scanned it in part of the scanning the handwritten was",
    "start": "2503490",
    "end": "2508510"
  },
  {
    "text": "centering this is not centered so it's got completely confused right you can",
    "start": "2508510",
    "end": "2515740"
  },
  {
    "text": "see there's no danger of this neural network taking over your jobs so there",
    "start": "2515740",
    "end": "2522730"
  },
  {
    "text": "are two ways that we can fix this one of them is called data augmentation what we",
    "start": "2522730",
    "end": "2529270"
  },
  {
    "text": "can do is take our training set and deliberately shift them all around the place scale them rotate them add some",
    "start": "2529270",
    "end": "2537400"
  },
  {
    "text": "noise to them and train those as well and then it will how to deal with that that's what",
    "start": "2537400",
    "end": "2542810"
  },
  {
    "text": "approached the other simpler approach in this case is we could do exactly the same thing to this to this canvas that",
    "start": "2542810",
    "end": "2549500"
  },
  {
    "text": "we that they did to the training data and we can Center it before feeding into the network and that's what I'm going to",
    "start": "2549500",
    "end": "2555080"
  },
  {
    "text": "do so I already did that I just I just commented it out so you could see it",
    "start": "2555080",
    "end": "2560480"
  },
  {
    "text": "fail so now we're gonna run this again I don't actually need to retrain it but I",
    "start": "2560480",
    "end": "2565820"
  },
  {
    "text": "haven't written the code to save the the training data so it's only done three",
    "start": "2565820",
    "end": "2572030"
  },
  {
    "text": "epochs but we should just be a it should now work if I put a 1 in its that's going to get that right and let's put it",
    "start": "2572030",
    "end": "2578930"
  },
  {
    "text": "let's put something else put a 2 in its got that put a 5 it's already got that",
    "start": "2578930",
    "end": "2586220"
  },
  {
    "text": "even though we haven't finished the 5 all right so we've got up two fairly good training and test accuracy in there",
    "start": "2586220",
    "end": "2595390"
  },
  {
    "text": "but you know we can do better so what we could do there's a number of ways we can",
    "start": "2595390",
    "end": "2601130"
  },
  {
    "text": "improve this so one thing we could do is add an extra hidden layer so I could put",
    "start": "2601130",
    "end": "2606620"
  },
  {
    "text": "30 creeps up thirteen-year arms in there and what another layer with 13 Euron's",
    "start": "2606620",
    "end": "2613160"
  },
  {
    "text": "and then let's see watch what happens what we are going to get a better result but what you tend to find is that when",
    "start": "2613160",
    "end": "2620030"
  },
  {
    "text": "you add more layers to this kind of network with this kind of problem while",
    "start": "2620030",
    "end": "2626180"
  },
  {
    "text": "in theory adding layers should always make it more expressive and powerful that it makes the learning much more",
    "start": "2626180",
    "end": "2632920"
  },
  {
    "text": "likely to go wrong or to be suboptimal so usually you find that for every kind",
    "start": "2632920",
    "end": "2638720"
  },
  {
    "text": "of scenario where you create an extra layer here you can probably do at least as well by putting all of those neurons",
    "start": "2638720",
    "end": "2645170"
  },
  {
    "text": "into one hidden layer now this is not true in all cases but with this kind of this image recognition this character",
    "start": "2645170",
    "end": "2651500"
  },
  {
    "text": "recognition you can probably do just as well with with just getting getting it",
    "start": "2651500",
    "end": "2656630"
  },
  {
    "text": "into two layers instead of three so but you can see we're up to 97.7% now just",
    "start": "2656630",
    "end": "2663950"
  },
  {
    "text": "with our training school 98 now and let's see what the actual accuracy is",
    "start": "2663950",
    "end": "2669290"
  },
  {
    "text": "with the test data we're 97.1 so that's actually not that's not too bad",
    "start": "2669290",
    "end": "2674600"
  },
  {
    "text": "we've got it we've got an even even improved it even more but we would probably do just as well by putting all",
    "start": "2674600",
    "end": "2679910"
  },
  {
    "text": "those 16 Euron's into the into the input layer the other thing I want to show you",
    "start": "2679910",
    "end": "2685370"
  },
  {
    "text": "is what happens if we if we set that learning rate too high so if I bump that",
    "start": "2685370",
    "end": "2690440"
  },
  {
    "text": "up to say porno over five you can actually watch it watch the the",
    "start": "2690440",
    "end": "2695660"
  },
  {
    "text": "oscillation can you see the oscillation in the colors so that's really useful that visualization because it",
    "start": "2695660",
    "end": "2701330"
  },
  {
    "text": "immediately tells you if you've got a problem with the learning rate being being too high what I've actually done",
    "start": "2701330",
    "end": "2706670"
  },
  {
    "text": "in this code is I've set it so that the learning rate automatically gets a",
    "start": "2706670",
    "end": "2711860"
  },
  {
    "text": "little bit lower with every epoch so as it becomes more accurate it reduces the",
    "start": "2711860",
    "end": "2717350"
  },
  {
    "text": "learning rate to prevent overshooting that the ID or values in there and the",
    "start": "2717350",
    "end": "2724040"
  },
  {
    "text": "other thing I'm going to do is we'll store that learning rate we're going to back this up to let's make it 100",
    "start": "2724040",
    "end": "2734030"
  },
  {
    "text": "neurons and in so still two layers imma put a hundred and one of the things I've",
    "start": "2734030",
    "end": "2740870"
  },
  {
    "text": "done with this they can think of this as a toolkit for learning is that instead of hard-coding",
    "start": "2740870",
    "end": "2746360"
  },
  {
    "text": "the rail you activation function here have created a pluggable architecture",
    "start": "2746360",
    "end": "2751790"
  },
  {
    "text": "I've created an activator class and a subclass fact earlier activator to a",
    "start": "2751790",
    "end": "2757790"
  },
  {
    "text": "logistic sigmoid activator that other one we saw before the hyperbolic tangent",
    "start": "2757790",
    "end": "2763040"
  },
  {
    "text": "a soft Max and I've got an exotic one in there too which changes the loss function so you can experiment with",
    "start": "2763040",
    "end": "2769250"
  },
  {
    "text": "different activation function to see how that affects the learning and there is one which is quite cool which I'm going",
    "start": "2769250",
    "end": "2775280"
  },
  {
    "text": "to put in there now which is an exotic one called a soft max activator with",
    "start": "2775280",
    "end": "2782300"
  },
  {
    "text": "cross-entropy loss this actually modifies the the mathematics is really",
    "start": "2782300",
    "end": "2788360"
  },
  {
    "text": "complicated but the the algorithm is actually quite simple so that this actually modifies the loss function so",
    "start": "2788360",
    "end": "2794780"
  },
  {
    "text": "no when we were squaring the loss to punish the big errors more it turns out",
    "start": "2794780",
    "end": "2799820"
  },
  {
    "text": "you can actually go kind of further with that you can go even more elaborate in how you you accurately measure the loss",
    "start": "2799820",
    "end": "2806990"
  },
  {
    "text": "so that to give it the best results so I'm going to run this through now hundred neurons in and and",
    "start": "2806990",
    "end": "2813650"
  },
  {
    "text": "while this runs I'm gonna I'm going to touch on another point which is about",
    "start": "2813650",
    "end": "2819830"
  },
  {
    "text": "gradient descent so if we if we do a gradient descent on this curve we're",
    "start": "2819830",
    "end": "2825650"
  },
  {
    "text": "going to end up in that that bottom that that bottom well now the the the curves",
    "start": "2825650",
    "end": "2831830"
  },
  {
    "text": "don't normally look like that in that if you imagine you're doing a gradient",
    "start": "2831830",
    "end": "2837230"
  },
  {
    "text": "descent on the combined loss for every training sample it's going to be a complex Wiggly curve it might end up",
    "start": "2837230",
    "end": "2845359"
  },
  {
    "text": "looking like that so we can see if we came in from the left we get stuck in a",
    "start": "2845359",
    "end": "2850849"
  },
  {
    "text": "local minima here and it wouldn't end up with a very good result right so this is",
    "start": "2850849",
    "end": "2856820"
  },
  {
    "text": "one of the reasons why I said we initialize the weights and values to the weights and biases to small random",
    "start": "2856820",
    "end": "2864349"
  },
  {
    "text": "values because by running it again each time you run it it's going to start from a different place so that way you can",
    "start": "2864349",
    "end": "2871220"
  },
  {
    "text": "partly get around this problem by re running your training over and over and each",
    "start": "2871220",
    "end": "2877190"
  },
  {
    "text": "time it will have a different starting place having said that if you do keep",
    "start": "2877190",
    "end": "2882890"
  },
  {
    "text": "your your networks relatively simple with your activation functions and the",
    "start": "2882890",
    "end": "2889190"
  },
  {
    "text": "number of layers what tends to happen is that all those minimas are quite close",
    "start": "2889190",
    "end": "2894650"
  },
  {
    "text": "to each other so you end up with a situation like this where if you started from the right",
    "start": "2894650",
    "end": "2899750"
  },
  {
    "text": "you'd only get a slightly better outcome than if you started from the left so",
    "start": "2899750",
    "end": "2906290"
  },
  {
    "text": "let's look back to to the training and see what's about how well it's doing so",
    "start": "2906290",
    "end": "2913010"
  },
  {
    "text": "you can see now it's up to ninety-nine point eight percent accuracy so are we",
    "start": "2913010",
    "end": "2918830"
  },
  {
    "text": "going to hit a hundred this is a really good good and active activation function",
    "start": "2918830",
    "end": "2924619"
  },
  {
    "text": "and it is a loss it is slower than a rally but we got here we are we got ninety-nine point eight percent with a",
    "start": "2924619",
    "end": "2931520"
  },
  {
    "text": "training data but we only got ninety seven point nine with the actual test data and that's no better than we would",
    "start": "2931520",
    "end": "2938119"
  },
  {
    "text": "do if we use a straight value so this exotic activation function and loss calculation",
    "start": "2938119",
    "end": "2943760"
  },
  {
    "text": "has done a lot to bump up our training accuracy but it hasn't actually helped",
    "start": "2943760",
    "end": "2948950"
  },
  {
    "text": "with the real-world test accuracy and this condition is called overfit it's",
    "start": "2948950",
    "end": "2954590"
  },
  {
    "text": "when it's done a really good job with the training data and it's locked onto a lot of specific features of that",
    "start": "2954590",
    "end": "2961130"
  },
  {
    "text": "training data but it's unable to generalize well to data it hasn't seen that's called over fit and that's what",
    "start": "2961130",
    "end": "2967940"
  },
  {
    "text": "we've got here we would actually be just as well off with a straight rally and",
    "start": "2967940",
    "end": "2978470"
  },
  {
    "text": "now one other point I'll mention is what",
    "start": "2978470",
    "end": "2987530"
  },
  {
    "text": "to do if you don't have much training data so I said that we need tens of thousands of rep to millions",
    "start": "2987530",
    "end": "2993140"
  },
  {
    "text": "I already we already touched on that that trick of data augmentation where we can kind of scale and shift and add some",
    "start": "2993140",
    "end": "2999320"
  },
  {
    "text": "noise to the data before putting it in but there's something else that you can do when you don't have much data so",
    "start": "2999320",
    "end": "3005010"
  },
  {
    "text": "imagine we didn't have that big set of digits we mention we only had a few thousand of those but imagine that we",
    "start": "3005010",
    "end": "3011350"
  },
  {
    "text": "did have a big set of letters from A to Z what we could do we could train it on",
    "start": "3011350",
    "end": "3017560"
  },
  {
    "text": "the letters from A to Z if we had 50,000 of those and then once we've got that",
    "start": "3017560",
    "end": "3022960"
  },
  {
    "text": "nicely trained we can then take that and we can use that as a starting point to",
    "start": "3022960",
    "end": "3029050"
  },
  {
    "text": "train the digits which we've only got a few off and the fact it because letters",
    "start": "3029050",
    "end": "3034600"
  },
  {
    "text": "and digits are kind of got things in common it will actually that's called",
    "start": "3034600",
    "end": "3039910"
  },
  {
    "text": "pre training it will actually get a better result so rather than starting off with random weights and biases you start off with a",
    "start": "3039910",
    "end": "3046510"
  },
  {
    "text": "network that's already been trained on some other data that is kind of similar that's got stuff in common because a lot",
    "start": "3046510",
    "end": "3053590"
  },
  {
    "text": "of those weights and biases encode kind of features like you know that you can",
    "start": "3053590",
    "end": "3058930"
  },
  {
    "text": "think of it as encoding the curves or the all the straight lines and so on and these things are in common that's a",
    "start": "3058930",
    "end": "3064300"
  },
  {
    "text": "really good trick as pre-training you can use there's something else I should also mention which is well what we want",
    "start": "3064300",
    "end": "3073150"
  },
  {
    "start": "3068000",
    "end": "3278000"
  },
  {
    "text": "to do if we want to take this to the next level because to get above 99% it is quite difficult with the architecture of God",
    "start": "3073150",
    "end": "3079610"
  },
  {
    "text": "and also the architecture we've got wouldn't be so conducive to really large complex images so if we had something",
    "start": "3079610",
    "end": "3086330"
  },
  {
    "text": "like a photograph with a furry animal in",
    "start": "3086330",
    "end": "3091760"
  },
  {
    "text": "it and we want to train it to identify find the furry animal and tell us if it's a cat or a dog we need something",
    "start": "3091760",
    "end": "3098480"
  },
  {
    "text": "that's more powerful than this and essentially what we need to do is with",
    "start": "3098480",
    "end": "3104030"
  },
  {
    "text": "this neural network this is a general-purpose neural network that could solve any kind of problem but what",
    "start": "3104030",
    "end": "3110420"
  },
  {
    "text": "we can we can do to make it work better is say look we're just going to make this work only for images so if we're",
    "start": "3110420",
    "end": "3117200"
  },
  {
    "text": "going to make design this network just for images we can do tricks like we can put filters and feature extraction",
    "start": "3117200",
    "end": "3125420"
  },
  {
    "text": "filters into the start of that neural network so it does things like edge",
    "start": "3125420",
    "end": "3130430"
  },
  {
    "text": "detection or it does things like find curves or straight lines and then it feeds those features into the inputs of",
    "start": "3130430",
    "end": "3139370"
  },
  {
    "text": "the neural network rather than feeding the raw pixels so the question is then",
    "start": "3139370",
    "end": "3145070"
  },
  {
    "text": "how do we write those feature extractors and then how do we decide on what features to extract and this is where it",
    "start": "3145070",
    "end": "3152270"
  },
  {
    "text": "kind of gets quite cool because what you can do is you can you can write a an",
    "start": "3152270",
    "end": "3158240"
  },
  {
    "text": "image processor but by writing a matrix of numbers and then scanning that matrix",
    "start": "3158240",
    "end": "3166370"
  },
  {
    "text": "over the image like doing a dot product you can then do a transformation on it in other words you can describe a",
    "start": "3166370",
    "end": "3174230"
  },
  {
    "text": "transformation purely as a bunch of numbers now given that you can describe",
    "start": "3174230",
    "end": "3180320"
  },
  {
    "text": "an image transformation or feature extraction just as numbers we don't have",
    "start": "3180320",
    "end": "3186020"
  },
  {
    "text": "to come up with the numbers we can get the neural network to optimize those numbers right we can get that back",
    "start": "3186020",
    "end": "3192500"
  },
  {
    "text": "propagation algorithm to work out what are the ideal numbers to extract features so it will work out what",
    "start": "3192500",
    "end": "3199400"
  },
  {
    "text": "features to extract with only need to decide on how many layers we want we",
    "start": "3199400",
    "end": "3204680"
  },
  {
    "text": "don't need to decide on what those layers will extract right that's called a convolutional network",
    "start": "3204680",
    "end": "3210880"
  },
  {
    "text": "convolutional layer does a feature extraction and image processing and then feeds that that extracted layer into the",
    "start": "3210880",
    "end": "3218570"
  },
  {
    "text": "into the next layer so if you're using the libraries one of the more powerful features you'll see a options for",
    "start": "3218570",
    "end": "3224750"
  },
  {
    "text": "convolutional layers and they're really aimed at image processing and they cut they're also conducive to having lots of",
    "start": "3224750",
    "end": "3230930"
  },
  {
    "text": "layers and they're very conducive to pre-training so you can you can or you",
    "start": "3230930",
    "end": "3236090"
  },
  {
    "text": "can actually load up pre-trained convolutional image networks which have",
    "start": "3236090",
    "end": "3241640"
  },
  {
    "text": "been trained with millions of images over the Internet and use that as a starting point for doing image recognition so I've covered I think I've",
    "start": "3241640",
    "end": "3251330"
  },
  {
    "text": "covered pretty much everything now and and are there any questions",
    "start": "3251330",
    "end": "3257200"
  },
  {
    "text": "okay so when we started out with 20/20 neurons not bring that up again I'll put",
    "start": "3268000",
    "end": "3274190"
  },
  {
    "text": "20 back in there so there's there's a couple of points here but the this is",
    "start": "3274190",
    "end": "3279680"
  },
  {
    "start": "3278000",
    "end": "3600000"
  },
  {
    "text": "this is just showing you the hidden layer on the on the left and the output layer on the right so what we can't see",
    "start": "3279680",
    "end": "3288140"
  },
  {
    "text": "on here is that there are around 780 input cells they're not neurons s cells",
    "start": "3288140",
    "end": "3293750"
  },
  {
    "text": "and they're also all the interconnections between those and the hidden layer because if we drew them there just be too much clutter",
    "start": "3293750",
    "end": "3301390"
  },
  {
    "text": "okay that's a good question how do you define the optimal learning rate and that there is kind of no silver bullet",
    "start": "3309630",
    "end": "3317380"
  },
  {
    "text": "you just experiment you try and different values and see how it works",
    "start": "3317380",
    "end": "3322510"
  },
  {
    "text": "and and what you discover is also that the same value doesn't work well in every layer so in fact with this network",
    "start": "3322510",
    "end": "3329829"
  },
  {
    "text": "I I increase the learning rate with every layer as you go to the right and that's actually commonly done you need a",
    "start": "3329829",
    "end": "3335829"
  },
  {
    "text": "higher learning rate as you go to the right and also I in decrease its slightly with every epoch I as it",
    "start": "3335829",
    "end": "3343510"
  },
  {
    "text": "becomes more accurate and so you have to be careful with a learning rate because when you start modifying other aspects",
    "start": "3343510",
    "end": "3350800"
  },
  {
    "text": "of that so you change for instance the activation function then you might also find you need to change the learning",
    "start": "3350800",
    "end": "3356589"
  },
  {
    "text": "rate if you don't do that it can you get a really bad result and you assume it was that new activation function was no",
    "start": "3356589",
    "end": "3363490"
  },
  {
    "text": "good when in fact you just needed to change the learning rate to match that function but yes it's just kind of trial",
    "start": "3363490",
    "end": "3370359"
  },
  {
    "text": "and error and there are you know in the heuristics so some of the more advanced",
    "start": "3370359",
    "end": "3377559"
  },
  {
    "text": "networks will kind of work it out themselves that they've got heuristics to work it out and there even our",
    "start": "3377559",
    "end": "3383230"
  },
  {
    "text": "adaptive learning rates I believe where it will actually train itself to into",
    "start": "3383230",
    "end": "3388630"
  },
  {
    "text": "the right learning rate yes when you're",
    "start": "3388630",
    "end": "3397119"
  },
  {
    "text": "doing rather than just feeding it okay this so",
    "start": "3397119",
    "end": "3406690"
  },
  {
    "text": "that you you mean by up here mean to the left to the little there okay you could",
    "start": "3406690",
    "end": "3412540"
  },
  {
    "text": "try that I don't see how that could improve it though because but by having",
    "start": "3412540",
    "end": "3419319"
  },
  {
    "text": "by waiting the working out the output votes as a weighted sum of the input",
    "start": "3419319",
    "end": "3425680"
  },
  {
    "text": "votes that seems to capture everything that I mean know what it would do is it would kind of league some dirty data",
    "start": "3425680",
    "end": "3432369"
  },
  {
    "text": "back towards towards the end but because the the the when your feed from the output to the first hidden layer the",
    "start": "3432369",
    "end": "3439359"
  },
  {
    "text": "hidden layer itself will give the best possible feedback as to what the layer to the left of that would need so I'm I'm not",
    "start": "3439359",
    "end": "3446859"
  },
  {
    "text": "sure that that would help but that's just from a you know a really quick ten-second evaluation because there",
    "start": "3446859",
    "end": "3454240"
  },
  {
    "text": "might be some subtle reasons why that could work well I I do know that when it comes to feeding forward it can actually",
    "start": "3454240",
    "end": "3461680"
  },
  {
    "text": "sometimes help to to allow stuff to leak forwards because it can help when you went with training very deep networks",
    "start": "3461680",
    "end": "3468940"
  },
  {
    "text": "that would otherwise not not qui s so I mean there are subtleties like that",
    "start": "3468940",
    "end": "3474400"
  },
  {
    "text": "which often that they are discovered by trial and error",
    "start": "3474400",
    "end": "3478500"
  },
  {
    "text": "so the input layer has intrinsic meaning the output layer ends intrinsic meaning the hidden layers in the in the middle",
    "start": "3490780",
    "end": "3497660"
  },
  {
    "text": "do those have any intrinsic meaning yes yeah that's also a good question and no",
    "start": "3497660",
    "end": "3503510"
  },
  {
    "text": "they don't they well they do they mean something to the neural network right but they don't it's very unlikely that",
    "start": "3503510",
    "end": "3509240"
  },
  {
    "text": "it means much to us it's very hard for us to look at what's in that layer and",
    "start": "3509240",
    "end": "3514340"
  },
  {
    "text": "and try and correlate that that's that's got some something that's meaningful to",
    "start": "3514340",
    "end": "3520340"
  },
  {
    "text": "us it's unlikely because it's it does represent some kind of extraction so if",
    "start": "3520340",
    "end": "3526070"
  },
  {
    "text": "you think about the the digits it could be capturing the concept of a loop or a concept of a of a straight line or",
    "start": "3526070",
    "end": "3533270"
  },
  {
    "text": "something but that's way too simplistic it's probably it's the most complex and",
    "start": "3533270",
    "end": "3538430"
  },
  {
    "text": "subtle and wheel is unlikely that we would find much at least with this style of network is possible with",
    "start": "3538430",
    "end": "3544940"
  },
  {
    "text": "convolutional networks that we might see me see something that was more meaningful to us but yes it does have",
    "start": "3544940",
    "end": "3551210"
  },
  {
    "text": "meaning but not I don't think it's something that we would would mean anything to us as humans",
    "start": "3551210",
    "end": "3557950"
  },
  {
    "text": "you mentioned earlier that you'd be better off having a toilet yes in what",
    "start": "3562599",
    "end": "3570700"
  },
  {
    "text": "scenarios does it best have well this",
    "start": "3570700",
    "end": "3576239"
  },
  {
    "text": "scenario certainly is better to be taller the if you're using convolutional",
    "start": "3576239",
    "end": "3582069"
  },
  {
    "text": "layers that's very conducive to having more layers rather than than just being taller and also with convolutional",
    "start": "3582069",
    "end": "3588309"
  },
  {
    "text": "layers and this may also apply to other architectures they're not fully",
    "start": "3588309",
    "end": "3593979"
  },
  {
    "text": "connected so that's one of the advantages is that with those that kind of image processing you're doing is",
    "start": "3593979",
    "end": "3600539"
  },
  {
    "text": "before it feeds into the network in a sense is that if you think of the whole",
    "start": "3600539",
    "end": "3606069"
  },
  {
    "text": "thing as one your network the neurons in in the input layer won't work all of",
    "start": "3606069",
    "end": "3612220"
  },
  {
    "text": "them might all be kicked - every neuron in the next layer and that makes the convolutional networks more efficient and I think that also probably my",
    "start": "3612220",
    "end": "3619150"
  },
  {
    "text": "intuition is that helps with with with the depth allowing for more layers but",
    "start": "3619150",
    "end": "3624789"
  },
  {
    "text": "they're really the best way is to where this kind of thing is to try it and see and if don't give up too soon if it",
    "start": "3624789",
    "end": "3631210"
  },
  {
    "text": "doesn't work try do a different activation function in there or changing the learning rate or but it's the best",
    "start": "3631210",
    "end": "3638589"
  },
  {
    "text": "way really is to is to experiment and see you mean in across layers yeah yeah",
    "start": "3638589",
    "end": "3653170"
  },
  {
    "text": "definitely what we just did we we had we were using the softmax activation function in the in the output layer so",
    "start": "3653170",
    "end": "3661299"
  },
  {
    "text": "this and then and then a Darrell you in the hidden layer and that's quite often",
    "start": "3661299",
    "end": "3666309"
  },
  {
    "text": "done is that you have different activation functions in different layers that that's quite very common",
    "start": "3666309",
    "end": "3673798"
  },
  {
    "text": "you have different data for the input legs not during one okay so in this case",
    "start": "3682050",
    "end": "3689550"
  },
  {
    "text": "well actually it was it was fractional to err or yes it was numbers than the input data in this case it was between 0",
    "start": "3689550",
    "end": "3695790"
  },
  {
    "text": "and 255 because that was how was representing the the pixels right there was a grayscale so you basically convert",
    "start": "3695790",
    "end": "3704280"
  },
  {
    "text": "it to a number a real number and then that becomes your input yes you need to",
    "start": "3704280",
    "end": "3713670"
  },
  {
    "text": "I believe it has to be at come kind of number before that and also that there are the way in which you convert it to",
    "start": "3713670",
    "end": "3720120"
  },
  {
    "text": "the number can have an influence on how successful it is because you're effectively doing some pre-processing",
    "start": "3720120",
    "end": "3725970"
  },
  {
    "text": "which can make it much easier harder from the new it for the network depending on how you do that okay",
    "start": "3725970",
    "end": "3735960"
  },
  {
    "text": "believe what time maybe for one more question and then call it quits",
    "start": "3735960",
    "end": "3742490"
  },
  {
    "text": "I suggest you download that and just have a play with it",
    "start": "3762560",
    "end": "3768210"
  },
  {
    "text": "fiddle with it or even you could just write that first example again from scratch that will be the first thing I",
    "start": "3768210",
    "end": "3774630"
  },
  {
    "text": "would suggest you do is take this the simple the simple example we had with one year on to do the NAND gate and",
    "start": "3774630",
    "end": "3782750"
  },
  {
    "text": "start again so I'll see whether you can reproduce that because if you can you pretty much understand the the learning",
    "start": "3782750",
    "end": "3789210"
  },
  {
    "text": "algorithm if you can do that it's not that hard but that's the first stage because what you'll probably find is if you do a cleaner and implementation of",
    "start": "3789210",
    "end": "3795900"
  },
  {
    "text": "that it won't work you'll get some basic thing wrong and that's that's how you learn that's how you get that you know I",
    "start": "3795900",
    "end": "3802710"
  },
  {
    "text": "did this myself I thought it was simple I wrote it it didn't work and I realized I didn't really understand how it worked",
    "start": "3802710",
    "end": "3808200"
  },
  {
    "text": "and that that billet I believe is a first step now the next step if you want to have some more fun with us is you",
    "start": "3808200",
    "end": "3813870"
  },
  {
    "text": "could say you could experiment with different activation functions you could it'll be really cool if you wrote a",
    "start": "3813870",
    "end": "3819690"
  },
  {
    "text": "convolutional layer for this if you do that please send it to me okay thanks",
    "start": "3819690",
    "end": "3827670"
  },
  {
    "text": "everyone so that's all we've got time for thank you for coming [Applause]",
    "start": "3827670",
    "end": "3834289"
  }
]