[
  {
    "text": "all right this is going to be probably one of the most full-on talks you've seen because I've tried to fit in as much value as possible at the same time",
    "start": "5240",
    "end": "12719"
  },
  {
    "text": "you are not required to Remember Everything by all means we're going to give you a list at the end of all the things you should be considering there",
    "start": "12719",
    "end": "19000"
  },
  {
    "text": "will be some things in this talk that you go that is so basic I can't believe people aren't doing that there'll be",
    "start": "19000",
    "end": "24480"
  },
  {
    "text": "some things in this talk that you probably haven't considered before and if there's something in this talk that you've seen that I haven't mentioned",
    "start": "24480",
    "end": "31800"
  },
  {
    "text": "please come up and talk talk to me I said patterns at the very beginning in",
    "start": "31800",
    "end": "37000"
  },
  {
    "text": "terms of getting to production these are not only patterns but considerations but also things you should know as",
    "start": "37000",
    "end": "43039"
  },
  {
    "text": "developers as machine learning Engineers this is kind of a starting point if you",
    "start": "43039",
    "end": "48600"
  },
  {
    "text": "will this will make you sleep a little bit better at night knowing that you have generative AI in production with",
    "start": "48600",
    "end": "54120"
  },
  {
    "text": "the number of leaks and whatnot that have happened over the past few years this should be your default stance",
    "start": "54120",
    "end": "60760"
  },
  {
    "text": "quick housekeeping I will do some questions at the end and I will stick around for as long as there are questions um a writeup will be published",
    "start": "60760",
    "end": "67240"
  },
  {
    "text": "after the talk and what is with the black bar on the right side of this presentation well there is so much",
    "start": "67240",
    "end": "73080"
  },
  {
    "text": "technical information in this presentation I was going to put like just a little video game on the side like they do on Tik Tok just to keep",
    "start": "73080",
    "end": "79280"
  },
  {
    "text": "everyone's attention and energy but then I realized we're developers what we actually want to see is like a",
    "start": "79280",
    "end": "84439"
  },
  {
    "text": "successful compilation running again and again with no errors that's what really",
    "start": "84439",
    "end": "90159"
  },
  {
    "text": "really keeps attention but no in reality we're actually going to have a live demo that",
    "start": "90159",
    "end": "95799"
  },
  {
    "text": "you can participate in throughout the talk 99% of what we're going to be talking about today I've actually",
    "start": "95799",
    "end": "102560"
  },
  {
    "text": "implemented in a live demo that funnily enough is running live now wh let me",
    "start": "102560",
    "end": "110040"
  },
  {
    "text": "move that around a little bit so it's not looking the slide cool you can scan the QR code you get a nice little co-pilot everything we're going to talk",
    "start": "110040",
    "end": "116920"
  },
  {
    "text": "about feel free to try it against the bot feel free to attack it feel free to do whatever you want with it throw",
    "start": "116920",
    "end": "123360"
  },
  {
    "text": "whatever prompt injection you've learned over the past few days at it it'll be up for a little while afterwards as well if",
    "start": "123360",
    "end": "129200"
  },
  {
    "text": "you can get the secret word the file name that's used for any of the reference metad dat or the tool name",
    "start": "129200",
    "end": "134319"
  },
  {
    "text": "that's used behind the scenes you will get bonus points and if someone can get all three I might organize a prize but",
    "start": "134319",
    "end": "141800"
  },
  {
    "text": "getting straight into things what do we actually need to get production scale what do we need to actually be safe",
    "start": "141800",
    "end": "147959"
  },
  {
    "text": "running AI Solutions in production and it all starts with a pipeline it starts with an app it starts with recognizing",
    "start": "147959",
    "end": "155200"
  },
  {
    "text": "that we should not be talking to large language models directly from consumers but we should be building an API this",
    "start": "155200",
    "end": "162440"
  },
  {
    "text": "sounds really simple really straightforward but the number of customers that I've worked with that",
    "start": "162440",
    "end": "167640"
  },
  {
    "text": "have gone from a Jupiter notebook to then just refactoring that all into client side code leaking Secrets",
    "start": "167640",
    "end": "173680"
  },
  {
    "text": "everywhere is kind of wild so when you're looking at any form of generative",
    "start": "173680",
    "end": "179480"
  },
  {
    "text": "AI whether it be public co-pilots or private internal usage build a pipeline",
    "start": "179480",
    "end": "184879"
  },
  {
    "text": "that you can extend and add on to what do I mean by pipeline literally an API",
    "start": "184879",
    "end": "190080"
  },
  {
    "text": "request in and out in a nonfixed format it's just an API second thing which everyone here has",
    "start": "190080",
    "end": "197799"
  },
  {
    "text": "probably heard way too much about learn prompt engineering spend an afternoon playing around with Azure I Studio or",
    "start": "197799",
    "end": "204200"
  },
  {
    "text": "any of the llm tools actually play with it even if you're a developer and you go I don't want to mck around with natural",
    "start": "204200",
    "end": "210439"
  },
  {
    "text": "language prompt engineering is important to know it's actually quite interesting",
    "start": "210439",
    "end": "215720"
  },
  {
    "text": "how much it changes the way we use tools like GitHub co-pilot or other sorts of co-pilots or agents themselves when you",
    "start": "215720",
    "end": "223280"
  },
  {
    "text": "actually understand the language to use be clear and if the one takeaway you get",
    "start": "223280",
    "end": "229040"
  },
  {
    "text": "from this slide other than anything else assume the agent that you're building has zero knowledge treat them like a",
    "start": "229040",
    "end": "236120"
  },
  {
    "text": "brand new employee give them every bit of information they'll actually need now",
    "start": "236120",
    "end": "241480"
  },
  {
    "text": "I'm not going to hop on this because there's a million one fantastic talks on EXA exactly this what you should be",
    "start": "241480",
    "end": "248000"
  },
  {
    "text": "doing though and what I hope people are doing actively especially if you are running generative AI in production",
    "start": "248000",
    "end": "253560"
  },
  {
    "text": "today is prompt injection is attacking your own Services um we'll talk about",
    "start": "253560",
    "end": "259880"
  },
  {
    "text": "some of the patterns to defend against it a little bit later on but I'm actually the big a big fan of writing",
    "start": "259880",
    "end": "265600"
  },
  {
    "text": "not only user stories but abuser stories I want the stories of the user that",
    "start": "265600",
    "end": "271320"
  },
  {
    "text": "submits a thousand requests in 5 seconds I want the user that submits massive",
    "start": "271320",
    "end": "276880"
  },
  {
    "text": "long contexts multiple times those sort of pattern and injection attempts is",
    "start": "276880",
    "end": "283560"
  },
  {
    "text": "what we're actually seeing in the wild these aren't theoretical anymore these are real attacks and automated testing",
    "start": "283560",
    "end": "290320"
  },
  {
    "text": "is beginning to catch up when it comes to things like prompt injection you should be attacking",
    "start": "290320",
    "end": "296240"
  },
  {
    "text": "everything you build not just generative AI applications now before we go any deeper I'm going to pause here to say",
    "start": "296240",
    "end": "303199"
  },
  {
    "text": "who has used a co-pilot whether it be GitHub co-pilot or Bing chat or any sort",
    "start": "303199",
    "end": "308360"
  },
  {
    "text": "of llm at all believe majority of the audience cool how many people have built",
    "start": "308360",
    "end": "313680"
  },
  {
    "text": "like a proof of concept or a demo using a foundational model something like open AI",
    "start": "313680",
    "end": "319560"
  },
  {
    "text": "GPT couple of people cool you're going to get maximum value from today how many people just haven't touched the",
    "start": "319560",
    "end": "326120"
  },
  {
    "text": "fundament foundational models at all cool I'm going to give some background",
    "start": "326120",
    "end": "331520"
  },
  {
    "text": "as we go along you will find that a lot of these patterns if you look at it from a developer lens will be like yeah du",
    "start": "331520",
    "end": "338400"
  },
  {
    "text": "why don't we do that so taking it a step further for those",
    "start": "338400",
    "end": "344080"
  },
  {
    "text": "that haven't used foundational models before funnily enough there's different types of messages we can use different",
    "start": "344080",
    "end": "349680"
  },
  {
    "text": "messages we can send this is the thing that well first and foremost presents",
    "start": "349680",
    "end": "354960"
  },
  {
    "text": "the most risk if someone is able to modify your system message introduce",
    "start": "354960",
    "end": "360280"
  },
  {
    "text": "a change in Behavior through context through any data they control it's",
    "start": "360280",
    "end": "365759"
  },
  {
    "text": "basically as good as their model now at that point if you lose control of the system message it's theirs but at the",
    "start": "365759",
    "end": "372800"
  },
  {
    "text": "same time we should actually be cognant of what is marked as user versus what is marked as assistant user is the data",
    "start": "372800",
    "end": "379720"
  },
  {
    "text": "that we feed in assistant is the data that comes out if you're misrepresenting",
    "start": "379720",
    "end": "385160"
  },
  {
    "text": "who said what in a conversation large language models are great at following patents what this means is if you've",
    "start": "385160",
    "end": "392199"
  },
  {
    "text": "marked some data some untrusted data as the assistant saying it EG data that's",
    "start": "392199",
    "end": "398240"
  },
  {
    "text": "come back guess what the model is going to repeat that as part of the conversation so be very careful the",
    "start": "398240",
    "end": "405360"
  },
  {
    "text": "types of messages you're actually including and as a general rule of thumb",
    "start": "405360",
    "end": "410720"
  },
  {
    "text": "make some assertions as part of your tests make sure you only have one system message that it's most commonly",
    "start": "410720",
    "end": "416879"
  },
  {
    "text": "assistant user assistant user these sort of patents will give you a level of protection that to be perfectly honest a",
    "start": "416879",
    "end": "423240"
  },
  {
    "text": "lot of people skip over all right this is going to be the",
    "start": "423240",
    "end": "428960"
  },
  {
    "text": "no people don't do that in production moment for a lot of people hands up what's wrong with this line of",
    "start": "428960",
    "end": "436280"
  },
  {
    "text": "code something in production oh all good we will continue",
    "start": "436280",
    "end": "442720"
  },
  {
    "text": "on for the sake of time having state in memory is the worst idea ever when we're",
    "start": "442720",
    "end": "448560"
  },
  {
    "text": "looking at generative AI it doesn't allow you to scale but more importantly",
    "start": "448560",
    "end": "453599"
  },
  {
    "text": "we just said before there are different types of messages user messages assistant messages system messages if",
    "start": "453599",
    "end": "460039"
  },
  {
    "text": "you're just storing everything as a list of strings in memory you've lost context",
    "start": "460039",
    "end": "465759"
  },
  {
    "text": "you can't reconstruct those messages properly one of the biggest things to keep in mind and this is a real gotcha",
    "start": "465759",
    "end": "471680"
  },
  {
    "text": "moment for the people moving from a Jupiter notebook to an actual production scale application is that when we look",
    "start": "471680",
    "end": "478879"
  },
  {
    "text": "at our large language models that we're calling they are entirely stateless they do not store data they do not store",
    "start": "478879",
    "end": "486159"
  },
  {
    "text": "conversations you have to construct the history yourself and pass it through if",
    "start": "486159",
    "end": "491560"
  },
  {
    "text": "you're doing that in memory and a user hits another instance or it's you scale",
    "start": "491560",
    "end": "496680"
  },
  {
    "text": "in or out you're going to lose context this is as I say it's wild the number of",
    "start": "496680",
    "end": "502919"
  },
  {
    "text": "times I've seen basically this exact line used in production gen AI systems that have migrated from proof of",
    "start": "502919",
    "end": "508879"
  },
  {
    "text": "Concepts simple things from a developers perspective Major Impact to user",
    "start": "508879",
    "end": "514880"
  },
  {
    "text": "experience though all right what does this actually look like in reality throw a database in",
    "start": "514880",
    "end": "521839"
  },
  {
    "text": "there as we're having a conversation write out what messages come in write",
    "start": "521839",
    "end": "527160"
  },
  {
    "text": "out what responses go back look them up for every single request that way where",
    "start": "527160",
    "end": "533560"
  },
  {
    "text": "abstracting State away putting it in something like we for Microsoft Cosmos DB you no SQL database of choice even a",
    "start": "533560",
    "end": "541160"
  },
  {
    "text": "postgress table will do to be honest abstracting State gives you the ability to scale properly but be cognant if",
    "start": "541160",
    "end": "549000"
  },
  {
    "text": "you're constantly retrieving large amounts of messages from a database your bottlenecks no longer your nice",
    "start": "549000",
    "end": "554959"
  },
  {
    "text": "stateless service that can scale to Infinity it's now your backend database",
    "start": "554959",
    "end": "560040"
  },
  {
    "text": "so consider how you partition in turn all right this is one of those",
    "start": "560040",
    "end": "565760"
  },
  {
    "text": "other ones that everyone forgets going to production but this is what you need to actually be production ready basic",
    "start": "565760",
    "end": "573720"
  },
  {
    "text": "things like rate limiting are often overlooked in reality tokens messages",
    "start": "573720",
    "end": "579320"
  },
  {
    "text": "that are coming in content that's generated and sent back costs dollars whether that be in terms of a pay as",
    "start": "579320",
    "end": "585880"
  },
  {
    "text": "youro model where you're paying for each individual token or whether it be paying for performance that's being wasted by",
    "start": "585880",
    "end": "592160"
  },
  {
    "text": "people that are just spamming messages at you put in rate limiting have it done early and have a standard across your",
    "start": "592160",
    "end": "598959"
  },
  {
    "text": "organiz ation for all generative AI components whether it be internal use or",
    "start": "598959",
    "end": "604279"
  },
  {
    "text": "external use rate limiting is not something you should be sleeping on as I say simple simple patterns but the",
    "start": "604279",
    "end": "611560"
  },
  {
    "text": "number of solutions that I've seen missing this is kind of wild the other thing that people often",
    "start": "611560",
    "end": "617920"
  },
  {
    "text": "Overlook and I have a real life use case where exactly this happened and someone was like what what happened to our",
    "start": "617920",
    "end": "623959"
  },
  {
    "text": "system overnight is not restricting the amount of data we can actually pass into these systems",
    "start": "623959",
    "end": "630040"
  },
  {
    "text": "when we talk about large language models we have a context window how much information we can throw in at any one",
    "start": "630040",
    "end": "636240"
  },
  {
    "text": "point in time the more that context window we use the more you normally pay",
    "start": "636240",
    "end": "641839"
  },
  {
    "text": "because it's more tokens you're using if you're allowing users to submit 10,000",
    "start": "641839",
    "end": "647639"
  },
  {
    "text": "30,000 character inputs into your large language model execution it is going to",
    "start": "647639",
    "end": "653040"
  },
  {
    "text": "cost you a fortune and chances are they're trying to do something like prompt injection one funny by product",
    "start": "653040",
    "end": "659720"
  },
  {
    "text": "the more information you include the easier it is to change some of the behavior of a large language model by",
    "start": "659720",
    "end": "666360"
  },
  {
    "text": "doing something as simple as a substring 150 200 characters will protect you",
    "start": "666360",
    "end": "673160"
  },
  {
    "text": "against like 50 to 60% of prompt injection attacks just simply because they don't fit into that size window so",
    "start": "673160",
    "end": "681880"
  },
  {
    "text": "simple patent add basic basic Max payload size doesn't have to be token",
    "start": "681880",
    "end": "687000"
  },
  {
    "text": "based literally just go 100 car and that is it the other thing to keep in mind is",
    "start": "687000",
    "end": "693399"
  },
  {
    "text": "it's not just content in that matters but content out that matters I have seen",
    "start": "693399",
    "end": "698440"
  },
  {
    "text": "exhaustion attacks done against large language models where the prompt data that was put in was actually saying",
    "start": "698440",
    "end": "704920"
  },
  {
    "text": "generate pages and pages of content not just generate a singular response you",
    "start": "704920",
    "end": "711240"
  },
  {
    "text": "need to be thinking about not only what tokens you pass in but what you're asking to generate now for those that",
    "start": "711240",
    "end": "717399"
  },
  {
    "text": "haven't played with foundational models before for you actually get this really nice Insight when you ask for something",
    "start": "717399",
    "end": "724519"
  },
  {
    "text": "that's bigger than your maximum number of tokens if I ask it to write a couple of paragraphs and I give it 800 tokens",
    "start": "724519",
    "end": "731800"
  },
  {
    "text": "worth of space for those playing at home a token is like three to four characters",
    "start": "731800",
    "end": "737120"
  },
  {
    "text": "easiest way to think about it if I give it 800 tokens it's going to use up as much as it needs to answer the question",
    "start": "737120",
    "end": "744440"
  },
  {
    "text": "if as a user I'm saying right five paragraphs 10 paragraphs 15 parag RS",
    "start": "744440",
    "end": "749519"
  },
  {
    "text": "worth of content I'm chewing up those tokens if I only expect this thing to",
    "start": "749519",
    "end": "754800"
  },
  {
    "text": "give back one sentence answers then by all means I should be using a smaller number of tokens if you go too small",
    "start": "754800",
    "end": "762639"
  },
  {
    "text": "guess what we actually get a finish reason saying the length rather than getting to the end of the content we",
    "start": "762639",
    "end": "769320"
  },
  {
    "text": "were generated it comes back and says no sorry I couldn't fit it in what you requested either retry this or increase",
    "start": "769320",
    "end": "777600"
  },
  {
    "text": "the limit another thing that a lot of people Overlook but incredibly important to actually get costs under control as you",
    "start": "777600",
    "end": "785199"
  },
  {
    "text": "head towards production itself the other big one this is a little bit specific to Azure open Ai and",
    "start": "785199",
    "end": "791920"
  },
  {
    "text": "funnily enough if you played with chat GPT if you played with GitHub co-pilot if you played with any of our F",
    "start": "791920",
    "end": "798360"
  },
  {
    "text": "Microsoft's foundational models you've seen content filters in action these stop it from generating sensitive",
    "start": "798360",
    "end": "805480"
  },
  {
    "text": "derogatory information hateful violent information if you've ever been using",
    "start": "805480",
    "end": "810959"
  },
  {
    "text": "GitHub co-pilot and say write me XY Z and it stops halfway through and says",
    "start": "810959",
    "end": "816600"
  },
  {
    "text": "sorry I can't generate this content that's a Content filter off going off in the background these content filters are",
    "start": "816600",
    "end": "823519"
  },
  {
    "text": "designed to be as easy as possible but if you're running llama yourself on a GPU guess what you don't necessarily get",
    "start": "823519",
    "end": "830959"
  },
  {
    "text": "these sort of controls we'll talk about compensating measures a little bit later on but if you're building a production",
    "start": "830959",
    "end": "837680"
  },
  {
    "text": "level system for the Enterprise set up content filters day one I don't care whether it's external or internal",
    "start": "837680",
    "end": "844480"
  },
  {
    "text": "this is on the to-do list they've actually been expanded and",
    "start": "844480",
    "end": "849600"
  },
  {
    "text": "this is really interesting because previously the content filters were focused around sensitive topics and",
    "start": "849600",
    "end": "854759"
  },
  {
    "text": "information that we wouldn't want generated and you get control over what that actually looks like it now includes",
    "start": "854759",
    "end": "861079"
  },
  {
    "text": "three very interesting actually pretty hard to read from the back I'll read them out um filters a jailbreak filter",
    "start": "861079",
    "end": "868199"
  },
  {
    "text": "so if someone's trying to change the behavior of your generative AI agent or the prompt you've constructed there's a",
    "start": "868199",
    "end": "874920"
  },
  {
    "text": "Content filter to protect against that there is also restricted text and",
    "start": "874920",
    "end": "880519"
  },
  {
    "text": "material code basically giving you checks if you're reproducing copyrighted work as part of the response why does",
    "start": "880519",
    "end": "889120"
  },
  {
    "text": "this matter well if your agent is generating code that's then used by end",
    "start": "889120",
    "end": "894279"
  },
  {
    "text": "users they need to be aware of the licensing implications of that this is something something that GitHub co-pilot",
    "start": "894279",
    "end": "900240"
  },
  {
    "text": "uses quite heavily to make sure the code it generates is ideally license free or",
    "start": "900240",
    "end": "905279"
  },
  {
    "text": "permittable to use but if you're building your own large language model systems that generate code this is",
    "start": "905279",
    "end": "910959"
  },
  {
    "text": "something you need to look into it's a legal battle that you don't want to be having and if you can turn it on with",
    "start": "910959",
    "end": "916279"
  },
  {
    "text": "just a single tick box it makes life a million times easier um funnily enough our internal",
    "start": "916279",
    "end": "923000"
  },
  {
    "text": "retries down the bottom there in the bottom right we've got some live stats of people as they're interacting with the bot internal retries there will be",
    "start": "923000",
    "end": "930560"
  },
  {
    "text": "made up from rate limiting so where I've run out of quota but also content filter",
    "start": "930560",
    "end": "935600"
  },
  {
    "text": "violations so I'm going to read that as so far we've had about 105 messages that",
    "start": "935600",
    "end": "940880"
  },
  {
    "text": "have been stopped or blocked by the content buil up rather than even interacting with our business",
    "start": "940880",
    "end": "947839"
  },
  {
    "text": "logic this is the secret source to production scale if you will I could probably spend 60 minutes talking about",
    "start": "947839",
    "end": "954079"
  },
  {
    "text": "this Slide by itself message analytics are gold the that we can capture as part",
    "start": "954079",
    "end": "960519"
  },
  {
    "text": "of a conversation the messages that come in and the responses that go back is",
    "start": "960519",
    "end": "965680"
  },
  {
    "text": "fantastic finnally enough on that slide before where we're externalizing State",
    "start": "965680",
    "end": "970800"
  },
  {
    "text": "you've already built the basis of a message analytic storage mechanism but don't stop at just who said it and what",
    "start": "970800",
    "end": "978240"
  },
  {
    "text": "was said include things like token counts timestamps give each message its",
    "start": "978240",
    "end": "983600"
  },
  {
    "text": "own unique ID and store that in here this is great first and foremost for",
    "start": "983600",
    "end": "988959"
  },
  {
    "text": "abuse monitoring if a response has come back and someone in the public has posted a screenshot of your app saying",
    "start": "988959",
    "end": "995560"
  },
  {
    "text": "something really really bad being able to go to a message analytics database look through and see was that response",
    "start": "995560",
    "end": "1002399"
  },
  {
    "text": "actually generated and what was the prompt that went in is gold likewise",
    "start": "1002399",
    "end": "1007480"
  },
  {
    "text": "being able to actually run analytics over the top of this asking things like was a conversation",
    "start": "1007480",
    "end": "1013959"
  },
  {
    "text": "resolved based on these messages of a large language model gives you the ability to actually customize not oh",
    "start": "1013959",
    "end": "1021480"
  },
  {
    "text": "sorry classify what a conversation was about but if there are any followup",
    "start": "1021480",
    "end": "1026520"
  },
  {
    "text": "actions if there are any missing data spots this is basically everything we wanted from chatbot and chatbot",
    "start": "1026520",
    "end": "1032360"
  },
  {
    "text": "analytics like three or four years ago but actually starting to be done right",
    "start": "1032360",
    "end": "1037558"
  },
  {
    "text": "the other thing I'll note down the bottom there is store your content filter results store when things have",
    "start": "1037559",
    "end": "1043480"
  },
  {
    "text": "triggered or stopped early too many tokens generated it is valuable data and valuable insights these are things that",
    "start": "1043480",
    "end": "1050559"
  },
  {
    "text": "you should be going back and reviewing we'll talk about monitoring a little bit later on but if you're seeing that hey",
    "start": "1050559",
    "end": "1056600"
  },
  {
    "text": "one in every five requests is hitting maximum length you either need to look at upping that limit or consider how",
    "start": "1056600",
    "end": "1063559"
  },
  {
    "text": "much information your prompts actually generating as I say secret Source behind actually going to production and",
    "start": "1063559",
    "end": "1070360"
  },
  {
    "text": "understanding what your users are doing the only thing I will say is please be transparent if you are capturing data in",
    "start": "1070360",
    "end": "1077039"
  },
  {
    "text": "a non in a non onized format make sure you're telling users about it at the end",
    "start": "1077039",
    "end": "1082880"
  },
  {
    "text": "of the day if you're using a public facing website there'll be a privacy policy there'll be data retention",
    "start": "1082880",
    "end": "1087960"
  },
  {
    "text": "policies have the exact same thing for your large language models and your message",
    "start": "1087960",
    "end": "1094520"
  },
  {
    "text": "retention the other thing I'll mention in this once again is something that everyone is sleeping on but we haven't",
    "start": "1094520",
    "end": "1100240"
  },
  {
    "text": "actually seen too many attacks so far session IDs or conversation IDs are the",
    "start": "1100240",
    "end": "1106360"
  },
  {
    "text": "building blocks of how we relate messages to each other how we actually resume a conversation a great example is",
    "start": "1106360",
    "end": "1113000"
  },
  {
    "text": "when you open up chat GPT you have your conversations sitting there on the side",
    "start": "1113000",
    "end": "1118280"
  },
  {
    "text": "ready to resume all of that is backed by a unique identifier if you've got a",
    "start": "1118280",
    "end": "1123480"
  },
  {
    "text": "public system rather than a private system what happens if someone picks up a session ID that was already used what",
    "start": "1123480",
    "end": "1130919"
  },
  {
    "text": "if someone retrieves history using a session ID or even if you've designed the system not to retrieve history what",
    "start": "1130919",
    "end": "1137960"
  },
  {
    "text": "if someone picks up a session ID and asks what have you talked about so far",
    "start": "1137960",
    "end": "1143440"
  },
  {
    "text": "to the large language model session IDs unless they're locked down scoped or",
    "start": "1143440",
    "end": "1148600"
  },
  {
    "text": "possibly even expire are a little bit dangerous to have round without user",
    "start": "1148600",
    "end": "1153760"
  },
  {
    "text": "authentication if you can orth your users this makes things simple if you're a public facing site consider time to a",
    "start": "1153760",
    "end": "1161760"
  },
  {
    "text": "degree expiry times on session IDs that way someone can't resume a conversation",
    "start": "1161760",
    "end": "1167000"
  },
  {
    "text": "that was had six months ago all right this is also one of the hidden",
    "start": "1167000",
    "end": "1173280"
  },
  {
    "text": "features that not many people know about but everyone sees the property when they're using the open Ai and the Azure",
    "start": "1173280",
    "end": "1179039"
  },
  {
    "text": "open Ai sdks and they have no idea what it means personal opinion worst named",
    "start": "1179039",
    "end": "1185400"
  },
  {
    "text": "property ever user does not actually it isn't what it says this is a",
    "start": "1185400",
    "end": "1191640"
  },
  {
    "text": "reference or a metadata property that allows you to track who triggered what content filters so if you have a user",
    "start": "1191640",
    "end": "1199039"
  },
  {
    "text": "that is continually triggering your content filters you can get feedback from Microsoft saying hey you need to go",
    "start": "1199039",
    "end": "1205240"
  },
  {
    "text": "and look at this particular user this particular ID if you will if you're",
    "start": "1205240",
    "end": "1210400"
  },
  {
    "text": "building an anonymous system throw session ID in that have it something that's actually catable against your",
    "start": "1210400",
    "end": "1216520"
  },
  {
    "text": "message analytics otherwise throw an actual user ID in there the last place",
    "start": "1216520",
    "end": "1221679"
  },
  {
    "text": "you want to be is having no idea who is using your system who is triggering content filters and potentially abusing",
    "start": "1221679",
    "end": "1228679"
  },
  {
    "text": "getting abusive results back have those analytics propagate them end to end this",
    "start": "1228679",
    "end": "1234720"
  },
  {
    "text": "is the generative AI version of a correlation ID all right this is also something that",
    "start": "1234720",
    "end": "1241679"
  },
  {
    "text": "has popped up and was made popular back in the chatbot days slightly less technical but just as valuable you",
    "start": "1241679",
    "end": "1248480"
  },
  {
    "text": "should be giving people the ability to thumbs up or thumbs down responses you've probably seen this in GitHub",
    "start": "1248480",
    "end": "1254440"
  },
  {
    "text": "co-pilot you've probably seen this on chatbots for years this is a great way together whether a response is actually",
    "start": "1254440",
    "end": "1261080"
  },
  {
    "text": "useful and once again if you begin to pair it with your message analytics database and wrap all of that with a",
    "start": "1261080",
    "end": "1268679"
  },
  {
    "text": "large language model you can ask questions like why didn't the user find",
    "start": "1268679",
    "end": "1274120"
  },
  {
    "text": "this message helpful and actually gain insights via generative AI this is",
    "start": "1274120",
    "end": "1279440"
  },
  {
    "text": "something that as I say a lot of organizations are beginning to do well but actively asking for feedback is in",
    "start": "1279440",
    "end": "1286039"
  },
  {
    "text": "my opinion a must especially for any public system but please do not stop at",
    "start": "1286039",
    "end": "1292200"
  },
  {
    "text": "active feedback just because people can thumbs up and thumbs down doesn't mean they always do you should also be",
    "start": "1292200",
    "end": "1298120"
  },
  {
    "text": "monitoring things like question follow-up rate how long between messages whether a user abandons a conversation",
    "start": "1298120",
    "end": "1305080"
  },
  {
    "text": "or follows a click through to another part of the site especially when we're looking at things like retail assistants",
    "start": "1305080",
    "end": "1311120"
  },
  {
    "text": "that are giving active product suggestions use those insights to determine if the messages coming back",
    "start": "1311120",
    "end": "1317679"
  },
  {
    "text": "are relevant or not this is to a degree basic user tracking and analytics but",
    "start": "1317679",
    "end": "1323799"
  },
  {
    "text": "when we apply it to generative AI all of a sudden we can begin to identify what parts of a conversation Drive user",
    "start": "1323799",
    "end": "1331400"
  },
  {
    "text": "Behavior not a pattern that you'll see on every single system but really helpful to understand if users are",
    "start": "1331400",
    "end": "1337559"
  },
  {
    "text": "getting value full stop we're going to get it slightly more",
    "start": "1337559",
    "end": "1343279"
  },
  {
    "text": "advanced for like three or four slides and I know we're going at a pretty good Pace but there's some fun things that I",
    "start": "1343279",
    "end": "1348760"
  },
  {
    "text": "want to get to towards the end why are we just using large language models to process user requests to",
    "start": "1348760",
    "end": "1356600"
  },
  {
    "text": "process user prompts why not actually begin to use the prompts the responses",
    "start": "1356600",
    "end": "1363880"
  },
  {
    "text": "the requests that come through as input to further large language models say for",
    "start": "1363880",
    "end": "1369480"
  },
  {
    "text": "example a large language model with a prompt designed to identify is this a relevant response or not does this",
    "start": "1369480",
    "end": "1377360"
  },
  {
    "text": "response actually solve what the user is asking for it's kind of Turtles the",
    "start": "1377360",
    "end": "1382440"
  },
  {
    "text": "whole way down at this point but funnily enough I'll pick on GitHub copilot again this is exactly what they do with their",
    "start": "1382440",
    "end": "1389159"
  },
  {
    "text": "sort of pipelines they begin to use multiple large language models to decide what files should or shouldn't be",
    "start": "1389159",
    "end": "1395159"
  },
  {
    "text": "included how we actually decide what model to use to answer the question and",
    "start": "1395159",
    "end": "1400600"
  },
  {
    "text": "then pass it on from there the whole idea idea of doing it asynchronously doing it off a change feed based around",
    "start": "1400600",
    "end": "1407080"
  },
  {
    "text": "your message Analytics means you're not slowing the user down if anyone started to send messages to the bot you'll",
    "start": "1407080",
    "end": "1413400"
  },
  {
    "text": "notice it's coming back pretty quickly but we're analyzing every message in the background to determine is it relevant",
    "start": "1413400",
    "end": "1420799"
  },
  {
    "text": "is it garbage does it solve the user's problem but to be perfectly honest the patent goes above and beyond just",
    "start": "1420799",
    "end": "1427760"
  },
  {
    "text": "background processing to actually blocking responses as well we shouldn't",
    "start": "1427760",
    "end": "1433240"
  },
  {
    "text": "just be using this pattern of evaluating is a response or does a response look correct",
    "start": "1433240",
    "end": "1438960"
  },
  {
    "text": "asynchronously we can also use this to stop bad responses from going out I",
    "start": "1438960",
    "end": "1444520"
  },
  {
    "text": "don't recommend this patent for everyone but if you're working with legal documents product disclosure statements",
    "start": "1444520",
    "end": "1450320"
  },
  {
    "text": "anywhere where you want an extra level of validation over the top consider having an evaluation step we'll talk",
    "start": "1450320",
    "end": "1458039"
  },
  {
    "text": "about evaluation methodology a little bit further down but if you need confidence in answers drive it through",
    "start": "1458039",
    "end": "1465799"
  },
  {
    "text": "analytics drive it through additional prompt exec utions FN enough this is this is a",
    "start": "1465799",
    "end": "1472880"
  },
  {
    "text": "really common pattern in Enterprise systems where it doesn't matter whether it's half a second or one second for the",
    "start": "1472880",
    "end": "1478720"
  },
  {
    "text": "result to come back all",
    "start": "1478720",
    "end": "1484080"
  },
  {
    "text": "right this is probably one of my most favorite favorite items because everyone",
    "start": "1484080",
    "end": "1489279"
  },
  {
    "text": "looks past IT band word lists the power of string. contains is huge there are",
    "start": "1489279",
    "end": "1497000"
  },
  {
    "text": "certain topics that we never want to interact with a large language model using and this is a great example of how",
    "start": "1497000",
    "end": "1503320"
  },
  {
    "text": "we can actually drive that behavior not relying on a prompt but catch Things",
    "start": "1503320",
    "end": "1509360"
  },
  {
    "text": "Early I'd always recommend logging these messages and funnily enough if you're actually chatting to the bot you mention",
    "start": "1509360",
    "end": "1514760"
  },
  {
    "text": "a couple of different keywords it actually won't even hit the large language model itself it'll exit out",
    "start": "1514760",
    "end": "1521120"
  },
  {
    "text": "early and give a preand response back to you because that's a topic that has nothing to do with what you're meant to",
    "start": "1521120",
    "end": "1527679"
  },
  {
    "text": "be asking two biggest recommendations here first and foremost generate a relevant word",
    "start": "1527679",
    "end": "1534200"
  },
  {
    "text": "list and constantly monitor users ask the weirdest things and to be perfectly",
    "start": "1534200",
    "end": "1540000"
  },
  {
    "text": "honest there are some times where terminology can be used multiple ways so don't just set a bandw list and leave it",
    "start": "1540000",
    "end": "1546760"
  },
  {
    "text": "constantly monitor the second one is don't return straight away this is my",
    "start": "1546760",
    "end": "1553279"
  },
  {
    "text": "favorite ux trick whenever we're shortcutting generative or short circuiting generative I add in",
    "start": "1553279",
    "end": "1559799"
  },
  {
    "text": "artificial delay make a user think that it has actually completed a full round",
    "start": "1559799",
    "end": "1565640"
  },
  {
    "text": "trip from an attacker's perspective they now have no idea whether they've hit a",
    "start": "1565640",
    "end": "1571200"
  },
  {
    "text": "foundational model or whether they've just got a response back straight away likewise generate pre-canned responses",
    "start": "1571200",
    "end": "1577640"
  },
  {
    "text": "ahead of time in the demo bot I've got about 10 or 12 preann sorry I can't help",
    "start": "1577640",
    "end": "1583240"
  },
  {
    "text": "you with that messages that are randomly returned if you have a banned word popup",
    "start": "1583240",
    "end": "1588320"
  },
  {
    "text": "from a user experience perspective you see no difference between a banned word list or a prompt coming back saying you",
    "start": "1588320",
    "end": "1595840"
  },
  {
    "text": "can't talk about that as I say simple things Major Impact where I've seen this",
    "start": "1595840",
    "end": "1601679"
  },
  {
    "text": "works really well is I've worked with a government customer who had a public facing chatbot that would had a very",
    "start": "1601679",
    "end": "1607640"
  },
  {
    "text": "specific topic it was talking about by using a banned word list they actually got approval to go to production in",
    "start": "1607640",
    "end": "1614240"
  },
  {
    "text": "basically two weeks from ideation of proof of concept to live on the site",
    "start": "1614240",
    "end": "1620039"
  },
  {
    "text": "because they had that extra layer of security that anything coming in or anything responding with one of these",
    "start": "1620039",
    "end": "1626200"
  },
  {
    "text": "potentially banned words would be blocked keep it simple sometimes the simple techniques work",
    "start": "1626200",
    "end": "1632960"
  },
  {
    "text": "best other thing I'll mention very quickly getting back to our pipeline we've seen we've been building out a",
    "start": "1632960",
    "end": "1638039"
  },
  {
    "text": "pipeline as we go don't just do this on the information coming in do this on",
    "start": "1638039",
    "end": "1643320"
  },
  {
    "text": "responses going back as well it is Trivial unless you have your prompt con instructed correctly to get bots to",
    "start": "1643320",
    "end": "1650440"
  },
  {
    "text": "translate or agents to translate between different languages if you have a band words list that's only in English and",
    "start": "1650440",
    "end": "1656919"
  },
  {
    "text": "someone puts something rude in Spanish and says translate to English chances are that won't get picked out on the way",
    "start": "1656919",
    "end": "1663799"
  },
  {
    "text": "in you should be picking it up on the way out we will continue to build this out as we go as you can see this is why",
    "start": "1663799",
    "end": "1670720"
  },
  {
    "text": "I call it a pipeline it is very much step by step what should happen for every single message that flows through",
    "start": "1670720",
    "end": "1677399"
  },
  {
    "text": "the system traditional text analytics once again",
    "start": "1677399",
    "end": "1682679"
  },
  {
    "text": "kind of basic but I I do like it I don't see it used in every single use case but",
    "start": "1682679",
    "end": "1688080"
  },
  {
    "text": "doing things like language detection stops translation injection and",
    "start": "1688080",
    "end": "1693120"
  },
  {
    "text": "translation-based prompt attacks if you will actually detecting that it has a",
    "start": "1693120",
    "end": "1698640"
  },
  {
    "text": "positive sentiment rather than an angry or negative sentiment can be helpful if we're doing customer support traditional",
    "start": "1698640",
    "end": "1705720"
  },
  {
    "text": "text analytics in NLP isn't something you should throw to the side just because we have generative AI they're",
    "start": "1705720",
    "end": "1712320"
  },
  {
    "text": "best used together and funnily enough there are some amazing open- Source NLP",
    "start": "1712320",
    "end": "1717440"
  },
  {
    "text": "models with Bann and sensitive word lists that are really good at picking up",
    "start": "1717440",
    "end": "1722840"
  },
  {
    "text": "if someone's trying to trigger abusive contexts it doesn't have to be a fancy",
    "start": "1722840",
    "end": "1728080"
  },
  {
    "text": "cloud service to do these things you can run it all on premise or in your own environment if you want to just with all",
    "start": "1728080",
    "end": "1734360"
  },
  {
    "text": "of this consider latency everything we add to the pipeline we increase latency slightly we'll talk about latency a",
    "start": "1734360",
    "end": "1741640"
  },
  {
    "text": "little bit later on this is probably one of the most interesting adoptions to security that",
    "start": "1741640",
    "end": "1749320"
  },
  {
    "text": "I've seen for rag or retrieval augmented generation I'm not going to do a rag",
    "start": "1749320",
    "end": "1754640"
  },
  {
    "text": "talk because we'll be here for another hour on top of the hour it basically means grab some documents and include it",
    "start": "1754640",
    "end": "1761039"
  },
  {
    "text": "in as part of the conversation so if I ask about dogs it's going to do a quick search for what it knows about dogs and",
    "start": "1761039",
    "end": "1768200"
  },
  {
    "text": "include that as part of the context if you're doing that search yourself if you're looking up relevant documents to",
    "start": "1768200",
    "end": "1774919"
  },
  {
    "text": "include and you get zero documents back chances are the user isn't asking about",
    "start": "1774919",
    "end": "1780720"
  },
  {
    "text": "anything they should be asking about short circuit that early you don't even need to send it through the large",
    "start": "1780720",
    "end": "1786480"
  },
  {
    "text": "language model to ask a question funnily enough this is actually one of the fundamental principles of uh bring your",
    "start": "1786480",
    "end": "1793159"
  },
  {
    "text": "own data for Azure open AI for people that have worked with bring your own Data before which is is like hey here's",
    "start": "1793159",
    "end": "1799440"
  },
  {
    "text": "a search index here's a large language model put them together done no code to",
    "start": "1799440",
    "end": "1804480"
  },
  {
    "text": "write you'll probably be familiar with this message this message basically means there were no relevant documents",
    "start": "1804480",
    "end": "1811440"
  },
  {
    "text": "found so I'm not even going to try doing anything I'm just going to Short Circuit early stop return a message saying hey I",
    "start": "1811440",
    "end": "1818600"
  },
  {
    "text": "can't find anything to do with it the faster you can get back to the user the better but also the less work we do",
    "start": "1818600",
    "end": "1825240"
  },
  {
    "text": "especially for a potentially dangerous payload the better as well Fally enough",
    "start": "1825240",
    "end": "1830399"
  },
  {
    "text": "as your open AI you see the thumbs up thumbs down in the right hand corner of saying whether it was a good or bad response we're implementing our own",
    "start": "1830399",
    "end": "1837159"
  },
  {
    "text": "pattern which is nice all right getting a little bit more",
    "start": "1837159",
    "end": "1842320"
  },
  {
    "text": "exciting if you haven't used these foundational models as well there is a little bit of a light bulb moment that",
    "start": "1842320",
    "end": "1847600"
  },
  {
    "text": "everyone goes through when you realize that hey wait a second Jason is natural",
    "start": "1847600",
    "end": "1853480"
  },
  {
    "text": "language so I can get these foundational models to go from English written text",
    "start": "1853480",
    "end": "1859519"
  },
  {
    "text": "that email that I can't be bothered reading to a structured object Jason out",
    "start": "1859519",
    "end": "1864600"
  },
  {
    "text": "the other end I'm seeing a number of people and actually working with quite a few companies that are doing help desk",
    "start": "1864600",
    "end": "1869720"
  },
  {
    "text": "and ticket automation automatic classification and summarization and then looking up next steps based around",
    "start": "1869720",
    "end": "1876320"
  },
  {
    "text": "this fundamental idea that we can have natural language in and structured data",
    "start": "1876320",
    "end": "1881559"
  },
  {
    "text": "out but guess what you should not trust that structured data at all the data",
    "start": "1881559",
    "end": "1887799"
  },
  {
    "text": "that that comes out treat it like user data treat it like someone has just found your API and is throwing data at",
    "start": "1887799",
    "end": "1894559"
  },
  {
    "text": "it left right and Center you should be validating schema you should be validating individual Fields basically",
    "start": "1894559",
    "end": "1900639"
  },
  {
    "text": "treat every property that comes out of a generated response as a potential",
    "start": "1900639",
    "end": "1907039"
  },
  {
    "text": "attack I wish it wasn't this way and I wish we could trust and S safely use",
    "start": "1907039",
    "end": "1912159"
  },
  {
    "text": "those values but please validate the output make no assumptions that the properties will even be there now",
    "start": "1912159",
    "end": "1920120"
  },
  {
    "text": "companies are trying to make these better by actually fine tuning on Json",
    "start": "1920120",
    "end": "1925159"
  },
  {
    "text": "responses and making sure that the the data that actually comes back is in adjacent format funnily enough tick",
    "start": "1925159",
    "end": "1932440"
  },
  {
    "text": "boxes like this do not save you all they're doing is checking to say is this",
    "start": "1932440",
    "end": "1937760"
  },
  {
    "text": "a valid jent object and that's it it's not checking for properties it's not checking for values send it once send it",
    "start": "1937760",
    "end": "1945880"
  },
  {
    "text": "another time you'll get two completely different objects back but how do we begin to get around this whole idea of",
    "start": "1945880",
    "end": "1952720"
  },
  {
    "text": "structured data out in a format that we expect well examples examples give as",
    "start": "1952720",
    "end": "1959480"
  },
  {
    "text": "many examples of the format as you can whether it be defining a schema as part",
    "start": "1959480",
    "end": "1965080"
  },
  {
    "text": "of your prompt whether it be describing what the properties contain one by one or whether it actually be doing what we",
    "start": "1965080",
    "end": "1971720"
  },
  {
    "text": "call few shot learning and including potential examples as part of the conversation it's it's funny because we",
    "start": "1971720",
    "end": "1979200"
  },
  {
    "text": "are seeing these massively long context lengths where we can put in as much information as we want having the",
    "start": "1979200",
    "end": "1984960"
  },
  {
    "text": "ability to pre-stage examples of what the data should look like means that it's no longer having to think up and",
    "start": "1984960",
    "end": "1992279"
  },
  {
    "text": "imagine what this Json object is it's just following the pattern but I got you",
    "start": "1992279",
    "end": "1998960"
  },
  {
    "text": "that everyone will run into it some stage when they're doing few shot learning or examples if you have poor",
    "start": "1998960",
    "end": "2004519"
  },
  {
    "text": "data in your examples guess what the language model will actually use that as part of its response I've seen",
    "start": "2004519",
    "end": "2011799"
  },
  {
    "text": "hallucinations where it's used sample or dummy data in an example as part of the",
    "start": "2011799",
    "end": "2017840"
  },
  {
    "text": "actual final response that comes back at the end of the day every example that you give every part of the context in",
    "start": "2017840",
    "end": "2024919"
  },
  {
    "text": "the conversation can potentially be used to generate output so if you're building",
    "start": "2024919",
    "end": "2030480"
  },
  {
    "text": "examples build examples that are correct and ideally reference data that is",
    "start": "2030480",
    "end": "2036000"
  },
  {
    "text": "relevant okay we're officially at the halfway mark things are going to get fun from",
    "start": "2036000",
    "end": "2042799"
  },
  {
    "text": "here and as I said a little bit of tidal wave of information especially for those that haven't played with a foundational",
    "start": "2042799",
    "end": "2047879"
  },
  {
    "text": "model we're going to get a little bit into the to a degree the research area",
    "start": "2047879",
    "end": "2053040"
  },
  {
    "text": "of the world things that we don't see as often but are kind of changing the way we think about generative Ai and",
    "start": "2053040",
    "end": "2060079"
  },
  {
    "text": "generative AI security we're going to come back to some base fundamentals after that um this is a very interesting",
    "start": "2060079",
    "end": "2067480"
  },
  {
    "text": "one people have figured out and we actually figured out pretty early on that hey if you include references with",
    "start": "2067480",
    "end": "2073280"
  },
  {
    "text": "a name guess what the generative AI the large language model can actually say XY",
    "start": "2073280",
    "end": "2078720"
  },
  {
    "text": "Z because of source one giving you the ability to actually provide references",
    "start": "2078720",
    "end": "2084280"
  },
  {
    "text": "and callbacks to where the data came from my big theme for the second half of",
    "start": "2084280",
    "end": "2089440"
  },
  {
    "text": "2024 is validate your responses or rather make it transparent how a user",
    "start": "2089440",
    "end": "2095878"
  },
  {
    "text": "can validate them we can't guarantee that an answer will be 100% correct but if you present an answer and the",
    "start": "2095879",
    "end": "2102880"
  },
  {
    "text": "references of where that data came from the user can now not just trust your",
    "start": "2102880",
    "end": "2108280"
  },
  {
    "text": "response but actually validate that it's correct but you need to be very careful here if you've pulled back 20 30 40",
    "start": "2108280",
    "end": "2116720"
  },
  {
    "text": "documents as part of your initial rag or your initial search you may not want to include references to all of these why",
    "start": "2116720",
    "end": "2124920"
  },
  {
    "text": "you actually have information leakage via this there is an attack pattern where you can actually get it to include irrelevant",
    "start": "2124920",
    "end": "2132119"
  },
  {
    "text": "documents or sensitive documents that might not be related and if you're not filtering to see what is actually",
    "start": "2132119",
    "end": "2138119"
  },
  {
    "text": "relevant all of a sudden anything can come back now this is dependent on how much information you actually have in",
    "start": "2138119",
    "end": "2144599"
  },
  {
    "text": "your search index what you're actually using to drive a lot of these or a lot of the",
    "start": "2144599",
    "end": "2150040"
  },
  {
    "text": "context but all I ask is if you're doing referencing actually validate to make sure the references are used and only in",
    "start": "2150040",
    "end": "2157640"
  },
  {
    "text": "include valid references don't include everything um other thing that I've",
    "start": "2157640",
    "end": "2164160"
  },
  {
    "text": "found and this is once again like very very light information leakage nothing to be too concerned about um if you're",
    "start": "2164160",
    "end": "2171280"
  },
  {
    "text": "using um nons sanitized naming for your references your model will very happily",
    "start": "2171280",
    "end": "2177640"
  },
  {
    "text": "use those names as part of the conversation and it will actually say as per external document pdf1 do blah blah",
    "start": "2177640",
    "end": "2185680"
  },
  {
    "text": "blah blah blah as part of its response so where possible sanitize resource uh",
    "start": "2185680",
    "end": "2191800"
  },
  {
    "text": "reference names ahead of time um as I say it's like light information disclosure but we should be thinking",
    "start": "2191800",
    "end": "2198359"
  },
  {
    "text": "about all these things when we're passing things through this is a little bit more",
    "start": "2198359",
    "end": "2204440"
  },
  {
    "text": "Cutting Edge and something that I haven't seen that many organizations do but I think we'll begin to change the",
    "start": "2204440",
    "end": "2210000"
  },
  {
    "text": "way we think about relevant requests I'm not going to go into vectors and embedding so the next like",
    "start": "2210000",
    "end": "2216359"
  },
  {
    "text": "sentence we probably go over few people's heads but when we begin to look at distance between vectors we're",
    "start": "2216359",
    "end": "2223839"
  },
  {
    "text": "talking about relevancy if we know what an example response should look like and",
    "start": "2223839",
    "end": "2229359"
  },
  {
    "text": "we know what a bad response should look like why aren't we using these same patterns to check is this answer",
    "start": "2229359",
    "end": "2236400"
  },
  {
    "text": "relevant or bad is this answer or rather this request that's coming in a question",
    "start": "2236400",
    "end": "2242920"
  },
  {
    "text": "that I expect to see or someone trying to do prompt injection all of a sudden we begin to move away",
    "start": "2242920",
    "end": "2249839"
  },
  {
    "text": "from keyword matching and begin to actually look at relevancy to expected data or potentially bad data now my",
    "start": "2249839",
    "end": "2257720"
  },
  {
    "text": "favorite part about this is there is large large libraries of prompt injection attacks out there online",
    "start": "2257720",
    "end": "2264280"
  },
  {
    "text": "there's one GitHub Reaper with like 600 prompt injection attacks you can take all of those convert them to embeddings",
    "start": "2264280",
    "end": "2271040"
  },
  {
    "text": "so giving you a vector and you can now compare to see is what the user asking",
    "start": "2271040",
    "end": "2276720"
  },
  {
    "text": "related to any prompt injection that is publicly available a little bit of a novel approach to detecting is the user",
    "start": "2276720",
    "end": "2284200"
  },
  {
    "text": "asking something relevant or not but the nice thing here no proprietary apis no",
    "start": "2284200",
    "end": "2290280"
  },
  {
    "text": "cloud services this is something you can run on your local machine with any sort of embedding model be it open source or",
    "start": "2290280",
    "end": "2297680"
  },
  {
    "text": "commercial it's a great way to begin looking at content filtering as I said bit of a novel",
    "start": "2297680",
    "end": "2303520"
  },
  {
    "text": "approach chunking in context is one of those other things that I will not talk about because we'll be here for like 2",
    "start": "2303520",
    "end": "2309200"
  },
  {
    "text": "or 3 hours but when we're breaking up data to only include like parts of the",
    "start": "2309200",
    "end": "2314400"
  },
  {
    "text": "data be careful where you break up the data if you break it by Page all of a",
    "start": "2314400",
    "end": "2320119"
  },
  {
    "text": "sudden we lose context when we include one page if you're going to break things up introduce overlap if you're working",
    "start": "2320119",
    "end": "2328240"
  },
  {
    "text": "with a data scientist or a machine learning expert this will be their bread and butter how much context overlap we",
    "start": "2328240",
    "end": "2334400"
  },
  {
    "text": "actually do but to be perfectly honest I don't see them Des designing the next generation of intelligent apps of",
    "start": "2334400",
    "end": "2339680"
  },
  {
    "text": "productionize systems it's developers the tools are available to everyone on",
    "start": "2339680",
    "end": "2345119"
  },
  {
    "text": "top of that be thinking about where you can actually begin to structure your reference data don't just have hey this",
    "start": "2345119",
    "end": "2351920"
  },
  {
    "text": "is page one this is page two this is Page Three actually put some structure and format to each of those references",
    "start": "2351920",
    "end": "2359119"
  },
  {
    "text": "if you've got a HTML page if you've got something in markdown this is really easy to do and funnily enough it's just",
    "start": "2359119",
    "end": "2365800"
  },
  {
    "text": "string processing at the end of the day so should all feel relatively comfortable with",
    "start": "2365800",
    "end": "2371400"
  },
  {
    "text": "that the other thing that I'll mention and this comes from an actual customer use case because they were trying to",
    "start": "2371400",
    "end": "2376800"
  },
  {
    "text": "abuse the tools they were given um if you have a massive large index of all these documents and only one particular",
    "start": "2376800",
    "end": "2383880"
  },
  {
    "text": "document relates to a user please don't try using Vector search or hybrid search",
    "start": "2383880",
    "end": "2389000"
  },
  {
    "text": "or semantic search to find that one particular document just put in a filter",
    "start": "2389000",
    "end": "2394440"
  },
  {
    "text": "keep things simple if you're only looking for documents for a particular year put a hard and fast filter in if",
    "start": "2394440",
    "end": "2401480"
  },
  {
    "text": "you're only looking for documents that a user has access to use group ID",
    "start": "2401480",
    "end": "2407480"
  },
  {
    "text": "filters we often get caught up in the world of let's add some additional metadata some additional context to each",
    "start": "2407480",
    "end": "2413240"
  },
  {
    "text": "of the documents to make sure I can retrieve them and find them funnily enough a filter is the simplest approach",
    "start": "2413240",
    "end": "2419800"
  },
  {
    "text": "to ensure you actually get what you want to see this is just a generic call out and",
    "start": "2419800",
    "end": "2426520"
  },
  {
    "text": "a 5-second rant please don't fine-tune models early there are specific use cases where fine-tuning makes sense and",
    "start": "2426520",
    "end": "2433200"
  },
  {
    "text": "if you do not know what fine-tuning is then you do not need to fine-tune your models everything that we've talked up",
    "start": "2433200",
    "end": "2439400"
  },
  {
    "text": "to this point should be tried tested implemented and you have a specific",
    "start": "2439400",
    "end": "2445280"
  },
  {
    "text": "reason why that doesn't solve the problem before you ever reach for fine tuning for those that are wondering what",
    "start": "2445280",
    "end": "2452400"
  },
  {
    "text": "do I mean by fine-tuning this kind of moves towards building your own model changing the actual underlying weights",
    "start": "2452400",
    "end": "2459280"
  },
  {
    "text": "if you really want to have a look at it and Management's like no we need our own GPT we need our own model navigate them",
    "start": "2459280",
    "end": "2466079"
  },
  {
    "text": "to chat GPT open ai's website and look at custom models there models start from a$1 to $3 million investment point for",
    "start": "2466079",
    "end": "2474040"
  },
  {
    "text": "your first iteration that is not something that every organization is going to say yes",
    "start": "2474040",
    "end": "2479480"
  },
  {
    "text": "let's do that straight away before we've seen value how many people here have seen a",
    "start": "2479480",
    "end": "2487319"
  },
  {
    "text": "429 response from a HTTP request so I've sent something off and get a 429 back",
    "start": "2487319",
    "end": "2493400"
  },
  {
    "text": "cool this is going to be the hot Response Code of generative AI it is too",
    "start": "2493400",
    "end": "2499359"
  },
  {
    "text": "many requests funnily enough we're limited by compute how how many gpus we can cram",
    "start": "2499359",
    "end": "2505440"
  },
  {
    "text": "into a Data Center and you won't always have access to all the infinite capacity",
    "start": "2505440",
    "end": "2512000"
  },
  {
    "text": "in the world that hey Cloud was kind of meant to deliver no we live in reality Data Center have limits you need to be",
    "start": "2512000",
    "end": "2519560"
  },
  {
    "text": "looking for and understanding too many request response codes when they come back you need to be retrying in a",
    "start": "2519560",
    "end": "2525920"
  },
  {
    "text": "meaningful Manner and actually waiting not just slamming these systems as hard as you can and where possible look at",
    "start": "2525920",
    "end": "2532680"
  },
  {
    "text": "introducing circuit breaker early there's no sense in sitting there waiting to get a 429 back if you've had",
    "start": "2532680",
    "end": "2538839"
  },
  {
    "text": "a 429 for the past two minutes beforehand the other thing I'll mention",
    "start": "2538839",
    "end": "2545280"
  },
  {
    "text": "is we actually give some really nice quota controls in Azure open AI so if you are like hey I want to make sure",
    "start": "2545280",
    "end": "2551079"
  },
  {
    "text": "this thing doesn't run away from me that I don't get million dollar costs overnight you can actually Force 429s to",
    "start": "2551079",
    "end": "2558160"
  },
  {
    "text": "happen by lowering your quota this is super cool both for test environments where you want to make sure that you",
    "start": "2558160",
    "end": "2563839"
  },
  {
    "text": "handle this Behavior correctly but also to make sure that you don't have Bill shock at the end of the",
    "start": "2563839",
    "end": "2568920"
  },
  {
    "text": "month there is nothing wrong from setting your quota down low and then increasing it as you",
    "start": "2568920",
    "end": "2575160"
  },
  {
    "text": "need cool you should also be thinking about retry logic retry Logic for 429s",
    "start": "2575160",
    "end": "2580359"
  },
  {
    "text": "retry Logic for content filtering retry Logic for everything that comes back",
    "start": "2580359",
    "end": "2585760"
  },
  {
    "text": "pulling back the magical curtain all of this new generative AI stuff is just simply an API call it's",
    "start": "2585760",
    "end": "2593079"
  },
  {
    "text": "not actually that new from like an integration perspective it's just another API you integrate with so don't",
    "start": "2593079",
    "end": "2600520"
  },
  {
    "text": "throw out things like retry policies have them as part of your design from day one",
    "start": "2600520",
    "end": "2607800"
  },
  {
    "text": "other thing I'll mention which This is highly topical depending on who you talk to is hey one data center might not have",
    "start": "2607800",
    "end": "2614119"
  },
  {
    "text": "capacity which you know what it's the us right now and they're not a sorry it's the US nighttime right now and they're",
    "start": "2614119",
    "end": "2620480"
  },
  {
    "text": "not awake so let's go use some of their capacity as well in scenarios where we",
    "start": "2620480",
    "end": "2625559"
  },
  {
    "text": "can actually have data that leaves Australian Shores but the whole idea of actually load balancing between multiple",
    "start": "2625559",
    "end": "2631480"
  },
  {
    "text": "regions multiple resources means that you can serve requests in as low latency as possible just consider whether it is",
    "start": "2631480",
    "end": "2639640"
  },
  {
    "text": "your application that does the load balancing or if you have an organizational strategy to do load balancing I'm going to do a quick shout",
    "start": "2639640",
    "end": "2646280"
  },
  {
    "text": "out here Graham Foster one of my co-workers um developed a really cool project called AI Central which is",
    "start": "2646280",
    "end": "2651640"
  },
  {
    "text": "focused around giving abstractions across multiple llms multiple regions multiple data centers giving you one",
    "start": "2651640",
    "end": "2658720"
  },
  {
    "text": "endpoint to call in an Enterprise ready way all right let's get into a couple of",
    "start": "2658720",
    "end": "2665319"
  },
  {
    "text": "fun things monitoring who here likes monitoring who here doesn't know they",
    "start": "2665319",
    "end": "2670880"
  },
  {
    "text": "like monitoring because they don't have monitoring I was expecting one or two hands there but that's okay you don't",
    "start": "2670880",
    "end": "2677079"
  },
  {
    "text": "need to put your hand up um monitor everything the first place I often talk",
    "start": "2677079",
    "end": "2683280"
  },
  {
    "text": "about especially when we look at user experience is do you understand the timing of every single request that's",
    "start": "2683280",
    "end": "2689800"
  },
  {
    "text": "happening do you know how long you're spelling spending in llm inference verse how much time you're spending preparing",
    "start": "2689800",
    "end": "2696400"
  },
  {
    "text": "the data before beforehand we're going to talk about streaming in a second but if it's taking",
    "start": "2696400",
    "end": "2703440"
  },
  {
    "text": "2 to 3 seconds to get to that first bite of the large language model coming back",
    "start": "2703440",
    "end": "2708760"
  },
  {
    "text": "it doesn't matter how fast we stream data out to the user you're always going to be bound by that first or time to",
    "start": "2708760",
    "end": "2715119"
  },
  {
    "text": "first bite if you will understand your timings log your timings Monitor and",
    "start": "2715119",
    "end": "2721119"
  },
  {
    "text": "ideally keep an eye on them if you're noticing your inference times are spiking and going super high that's",
    "start": "2721119",
    "end": "2727240"
  },
  {
    "text": "something to look into could be an attack could be that you just need to change compute",
    "start": "2727240",
    "end": "2732760"
  },
  {
    "text": "resources also monitor your failures and this is what we're actually doing down the bottom right there we're monitoring",
    "start": "2732760",
    "end": "2738440"
  },
  {
    "text": "how many failures come back how many times people are attacking it this is helpful to understand the behavior of",
    "start": "2738440",
    "end": "2744200"
  },
  {
    "text": "your system um if you are constantly having content filters being triggered off and blocking actual user responses",
    "start": "2744200",
    "end": "2751760"
  },
  {
    "text": "it shouldn't be the users that are telling you about it you should know ahead of time when you're hitting those",
    "start": "2751760",
    "end": "2757119"
  },
  {
    "text": "Val your limits those retry limits and also to be perfectly honest log who's",
    "start": "2757119",
    "end": "2762400"
  },
  {
    "text": "using what session ID even if you don't have users logged in that you can",
    "start": "2762400",
    "end": "2767559"
  },
  {
    "text": "actually link a session ID back to capture IP address capture basic fingerprinting data it is helpful if",
    "start": "2767559",
    "end": "2773839"
  },
  {
    "text": "someone tries to abuse the system you can actually begin to correlate between",
    "start": "2773839",
    "end": "2779720"
  },
  {
    "text": "sessions this is the one big slide that has the things that I wish everyone would capture but absolutely no one",
    "start": "2779720",
    "end": "2785040"
  },
  {
    "text": "captures and we always have to do it after the fact fact monitor utilization not just from number of tokens but",
    "start": "2785040",
    "end": "2791119"
  },
  {
    "text": "number of conversations number of messages per conversation number of conversations per",
    "start": "2791119",
    "end": "2796520"
  },
  {
    "text": "user understand how your users use your application I've had one or two retail",
    "start": "2796520",
    "end": "2803680"
  },
  {
    "text": "assistants now that have been changed the way they're designed the whole user experience because of the fact people",
    "start": "2803680",
    "end": "2808920"
  },
  {
    "text": "are only asking one question and then closing it off if they weren't getting their answer within that first question",
    "start": "2808920",
    "end": "2815119"
  },
  {
    "text": "they were just abandoning they were just going off so the system was changed to actually have more of a single answer",
    "start": "2815119",
    "end": "2821839"
  },
  {
    "text": "response with more detail rather than necessarily expecting everyone will answer or ask follow-up",
    "start": "2821839",
    "end": "2830400"
  },
  {
    "text": "questions the last one which is slightly more advance but I really want to throw it in as an explicit one because so many",
    "start": "2830400",
    "end": "2835960"
  },
  {
    "text": "organizations being caught out by this when it comes to that pipeline that we're building that endtoend API monitor",
    "start": "2835960",
    "end": "2843040"
  },
  {
    "text": "any external calls as well if you're using tooling if if you're having a large language model query a SQL",
    "start": "2843040",
    "end": "2849599"
  },
  {
    "text": "database know how much time it spends in the database versus generating the query",
    "start": "2849599",
    "end": "2855440"
  },
  {
    "text": "versus analyzing the results these very simple patterns for actually understanding timing means that when it",
    "start": "2855440",
    "end": "2862000"
  },
  {
    "text": "comes back and says hey this takes 20 seconds to execute if 15 seconds of that",
    "start": "2862000",
    "end": "2867200"
  },
  {
    "text": "is your SQL database there's no sense in trying to optimize the foundational model",
    "start": "2867200",
    "end": "2873800"
  },
  {
    "text": "itself add in a global kill switch this is something that I see especially for public resources for public chat Bots",
    "start": "2873800",
    "end": "2880800"
  },
  {
    "text": "especially Anonymous ones have a way to shut things down seriously it's actually",
    "start": "2880800",
    "end": "2886240"
  },
  {
    "text": "really nice to have rather than necessarily having to go in change code",
    "start": "2886240",
    "end": "2891800"
  },
  {
    "text": "pull out or have a bad user experience expect that the API will be down",
    "start": "2891800",
    "end": "2897040"
  },
  {
    "text": "gracefully recover from it I'm going to go on a 30- second rant",
    "start": "2897040",
    "end": "2902079"
  },
  {
    "text": "with this one you do not need streaming when you get started what do I mean by stre streaming those characters popping",
    "start": "2902079",
    "end": "2908400"
  },
  {
    "text": "in one by one as they're generated for those that have opened up the bot looked at the app that does not use streaming",
    "start": "2908400",
    "end": "2915000"
  },
  {
    "text": "at all did you feel that it was not Snappy that it didn't come back straight away no the characters popping up on",
    "start": "2915000",
    "end": "2922240"
  },
  {
    "text": "screen there are done entirely client side it receives the requests and it just starts typing them one by one as",
    "start": "2922240",
    "end": "2927960"
  },
  {
    "text": "quickly as it can there are cases where you will know you will need streaming you don't always",
    "start": "2927960",
    "end": "2934119"
  },
  {
    "text": "need it let's get into testing very quickly because this is an area that I'm quite",
    "start": "2934119",
    "end": "2939240"
  },
  {
    "text": "passionate about and would like to actually talk in a little bit more detail you should be testing your",
    "start": "2939240",
    "end": "2944760"
  },
  {
    "text": "generative AI generate sample inputs and outputs here we've got four to be",
    "start": "2944760",
    "end": "2950000"
  },
  {
    "text": "perfectly honest I'd be happy with 400 have examples of what users are asking and actually every time you make",
    "start": "2950000",
    "end": "2957480"
  },
  {
    "text": "a prompt revision or every time you change your code run through them all your first iteration can be a human",
    "start": "2957480",
    "end": "2964200"
  },
  {
    "text": "reviewing it checking are these results actually relevant your second iteration will be",
    "start": "2964200",
    "end": "2970119"
  },
  {
    "text": "using relevancy rather than having to have a human read through all the results one by one there are some really",
    "start": "2970119",
    "end": "2975599"
  },
  {
    "text": "nice Frameworks to actually allow you to check if results are relevant if they're not relevant using very basic NLP um if",
    "start": "2975599",
    "end": "2984079"
  },
  {
    "text": "you're looking at embedding based birth score super cool funnily enough it's that pattern we talked about before I",
    "start": "2984079",
    "end": "2990480"
  },
  {
    "text": "get one answer back I get another answer back how similar are these two things are we at least in the ballpark",
    "start": "2990480",
    "end": "2997920"
  },
  {
    "text": "what you will see organizations go to over time though is using generative AI not just for inference but for test",
    "start": "2997920",
    "end": "3005359"
  },
  {
    "text": "validation as well actually using a system prompt to say given that this was",
    "start": "3005359",
    "end": "3010839"
  },
  {
    "text": "the question this was the answer that was produced and this was the expected answer how far off the mark are we",
    "start": "3010839",
    "end": "3018319"
  },
  {
    "text": "beginning to actually use understanding of the responses and requests to test",
    "start": "3018319",
    "end": "3023799"
  },
  {
    "text": "the nice thing about this is it is incredibly fast to run run can be done at scale and can not only be done on",
    "start": "3023799",
    "end": "3030240"
  },
  {
    "text": "test stter as part of test pipelines but you can run this in prod as well you can actually use this as a live smoke test",
    "start": "3030240",
    "end": "3037760"
  },
  {
    "text": "constantly evaluating the quality of the answers that come back where this is really handy is if you have constantly",
    "start": "3037760",
    "end": "3043960"
  },
  {
    "text": "changing reference metadata if someone goes in and deletes all your reference metadata this is a great way to find out",
    "start": "3043960",
    "end": "3050960"
  },
  {
    "text": "because it's the a it's a large language model telling you there are no relevant results rather than a user script",
    "start": "3050960",
    "end": "3056599"
  },
  {
    "text": "screaming at you saying hey I couldn't find what I was looking for this is another one once again that",
    "start": "3056599",
    "end": "3063040"
  },
  {
    "text": "I'm quite passionate about test prompt injection in the same way we did all of those tests before now multiply them by",
    "start": "3063040",
    "end": "3069319"
  },
  {
    "text": "all of the different prompt injection attacks that are available today basically do a matrix of here are all of",
    "start": "3069319",
    "end": "3075640"
  },
  {
    "text": "my tests here is all of my prompt injection techniques and test every one of them you'll probably hit a point",
    "start": "3075640",
    "end": "3081720"
  },
  {
    "text": "where it will be too expensive to test every single iteration but it's a great place to start",
    "start": "3081720",
    "end": "3087160"
  },
  {
    "text": "if you as part of a testing pipeline can validate that prompt injection has changed the behavior of my application",
    "start": "3087160",
    "end": "3095000"
  },
  {
    "text": "that's fantastic that means you've actually tested before you've released and validated that you are protected as",
    "start": "3095000",
    "end": "3101559"
  },
  {
    "text": "I say there's a repo with 600 plus um like prompt injection examples if you've",
    "start": "3101559",
    "end": "3107240"
  },
  {
    "text": "heard of like chat GPT jail breaks or do anything now or Dan those patterns are",
    "start": "3107240",
    "end": "3113280"
  },
  {
    "text": "the basics for modern prompt injection you use them take advantage of them",
    "start": "3113280",
    "end": "3118559"
  },
  {
    "text": "they're out there in the open if you're not testing against them trust me users will be they love getting Bots to say",
    "start": "3118559",
    "end": "3125160"
  },
  {
    "text": "random things all right we're on the home stretch got a couple more um decide",
    "start": "3125160",
    "end": "3131079"
  },
  {
    "text": "early whether prompts are code or prompts of data I see a lot of a lot of",
    "start": "3131079",
    "end": "3137599"
  },
  {
    "text": "developers reaching for prompts as code which I personally like having a release",
    "start": "3137599",
    "end": "3143599"
  },
  {
    "text": "that not only represents the code and the logic but the system prompt that will be used means that it's very simple",
    "start": "3143599",
    "end": "3150040"
  },
  {
    "text": "to roll out roll back to know exactly what revision is being used but makes it harder to iterate on the flip side if we",
    "start": "3150040",
    "end": "3157079"
  },
  {
    "text": "have prompts as data you need to keep track of exactly what prompt is in what environment at any point in time and if",
    "start": "3157079",
    "end": "3163720"
  },
  {
    "text": "all of a sudden you're changing your resp expected result vers what the prompt says to do you're going to be in",
    "start": "3163720",
    "end": "3170319"
  },
  {
    "text": "a world of trouble so prompters code simpler harder to iterate on promp data",
    "start": "3170319",
    "end": "3177359"
  },
  {
    "text": "easier to iterate more chance to shoot you shoot yourself in the foot um AB your prompts once again if",
    "start": "3177359",
    "end": "3185400"
  },
  {
    "text": "you're doing prompts as code and having it compiled as part of your application this means two revisions sitting side by",
    "start": "3185400",
    "end": "3191960"
  },
  {
    "text": "side allow your testers to actually trigger a particular prompt revision",
    "start": "3191960",
    "end": "3197079"
  },
  {
    "text": "nothing new here AB testing is kind of just standard practice these days if you're not playing with it now",
    "start": "3197079",
    "end": "3202559"
  },
  {
    "text": "generative AI is a great place to get started because testing the differ between two prompts is actually really",
    "start": "3202559",
    "end": "3208880"
  },
  {
    "text": "really useful finally you remember that big metadata table we built up earlier on",
    "start": "3208880",
    "end": "3215000"
  },
  {
    "text": "everything that we've just talked about every bit of monitoring data should be part of that metadata table you should",
    "start": "3215000",
    "end": "3221319"
  },
  {
    "text": "know what prompt was used what version of your application what code release what rag options were used to actually",
    "start": "3221319",
    "end": "3227839"
  },
  {
    "text": "include relevant metadata trust me this will help you when you have a production issue",
    "start": "3227839",
    "end": "3233400"
  },
  {
    "text": "wondering why a bot responded in a particular way if you can take the message that was passed in the data that",
    "start": "3233400",
    "end": "3240240"
  },
  {
    "text": "was provided and try and reproduce it gives you a it makes your life",
    "start": "3240240",
    "end": "3246880"
  },
  {
    "text": "simpler I've said it a few times all of this is just an API like seriously when",
    "start": "3246880",
    "end": "3253119"
  },
  {
    "text": "you're getting started with generative AI do not treat it as machine learning in that context just think of it as",
    "start": "3253119",
    "end": "3259400"
  },
  {
    "text": "another API call that calls another API that's it we want to orchestrate how",
    "start": "3259400",
    "end": "3264520"
  },
  {
    "text": "that happens we want to make sure that results are relevant we never want to trust the data coming in nor do we never",
    "start": "3264520",
    "end": "3270119"
  },
  {
    "text": "want to trust the data coming back out from the large language mod model validate validate validate and log",
    "start": "3270119",
    "end": "3276559"
  },
  {
    "text": "everything if there's nothing else you take away it's just please log",
    "start": "3276559",
    "end": "3281880"
  },
  {
    "text": "more all right last four and these are the fun",
    "start": "3282160",
    "end": "3287359"
  },
  {
    "text": "ones we've talked about prompt injection but did you know you can actually get generative AI to begin",
    "start": "3287359",
    "end": "3295559"
  },
  {
    "text": "attacking your your own system you can get generative AI to generate cross-site scripting attacks as part of responses",
    "start": "3295559",
    "end": "3302680"
  },
  {
    "text": "you can get it to generate SQL injection attacks without it even knowing and if you're building SQL queries of what",
    "start": "3302680",
    "end": "3309040"
  },
  {
    "text": "comes back from these models you could possibly be in a world of hurt and the funniest thing is it doesn't even need",
    "start": "3309040",
    "end": "3315400"
  },
  {
    "text": "to be complex things literally asking as I say for the first example is a great one because I actually saw someone use",
    "start": "3315400",
    "end": "3321920"
  },
  {
    "text": "this against one of my own demos and luckily I was using inner text not in a HTML but only enough nothing to do with",
    "start": "3321920",
    "end": "3328760"
  },
  {
    "text": "Gen just basic developer practices this catches people out",
    "start": "3328760",
    "end": "3334480"
  },
  {
    "text": "seriously vulnerability scanning I will tell a very fun story about this if you are not vulnerability scanning the apis",
    "start": "3334480",
    "end": "3340920"
  },
  {
    "text": "that you have someone else is um I deployed a demo for a customer at 9:30",
    "start": "3340920",
    "end": "3346400"
  },
  {
    "text": "p.m. went to bed for the night came back in the morning and had 200,000 conversations each with a couple of",
    "start": "3346400",
    "end": "3352839"
  },
  {
    "text": "hundred messages someone decided to run a Zed and a nessus automated scan",
    "start": "3352839",
    "end": "3359280"
  },
  {
    "text": "against my API end points so I had conversations messages with every type",
    "start": "3359280",
    "end": "3364839"
  },
  {
    "text": "of vulnerability scanner attack you can think of the tricky thing is because you",
    "start": "3364839",
    "end": "3370079"
  },
  {
    "text": "can pass in some information and the large language model might say back sorry I can't help you with XYZ that",
    "start": "3370079",
    "end": "3376200"
  },
  {
    "text": "looks like a reflection attack to every vulnerability scanner out there so they will just go into turbo mode and start",
    "start": "3376200",
    "end": "3382400"
  },
  {
    "text": "just absolutely throwing as much as they can against your",
    "start": "3382400",
    "end": "3387520"
  },
  {
    "text": "points because you're doing safe AI practices because you've got examples and Json validation and you're doing",
    "start": "3387520",
    "end": "3393839"
  },
  {
    "text": "everything we've talked about so far doesn't mean your application safe you still need to do proper security",
    "start": "3393839",
    "end": "3399960"
  },
  {
    "text": "practices from the ground up this is just a personal call out and",
    "start": "3399960",
    "end": "3405680"
  },
  {
    "text": "funnily enough it is one of the reasons that I threw it in here was the last example that I gave if you're building a",
    "start": "3405680",
    "end": "3410880"
  },
  {
    "text": "proof of concept put a North Key put North key on it I've actually got some samples that I'll be releasing along",
    "start": "3410880",
    "end": "3417160"
  },
  {
    "text": "with kind of references for this talk of how to do basic code based authentication for pretty much every",
    "start": "3417160",
    "end": "3422799"
  },
  {
    "text": "single language if you have an endpoint that can get to a large language model",
    "start": "3422799",
    "end": "3428039"
  },
  {
    "text": "chances are people will find it how will they find it Well if you use let's encrypt someone watch the certificate",
    "start": "3428039",
    "end": "3434119"
  },
  {
    "text": "transparency logs and they now know your host name and they're now calling your apis a million times in a night bam",
    "start": "3434119",
    "end": "3441440"
  },
  {
    "text": "you've now stuck with the bill so put authentication authorization or even just an API key around the proof of",
    "start": "3441440",
    "end": "3448640"
  },
  {
    "text": "Concepts you built um I will publish and release that I don't think that's public yet so not",
    "start": "3448640",
    "end": "3455720"
  },
  {
    "text": "even worth taking a photo of um last two be transparent please say when something",
    "start": "3455720",
    "end": "3462960"
  },
  {
    "text": "is AI generated say when it is generated by generative AI be honest with your",
    "start": "3462960",
    "end": "3469039"
  },
  {
    "text": "users you can do personalization and personification of",
    "start": "3469039",
    "end": "3474200"
  },
  {
    "text": "AI but make sure the user actually knows that hey this is not a human generated message it could potentially be",
    "start": "3474200",
    "end": "3480480"
  },
  {
    "text": "incorrect and give them the ability to validate use AI responsibly please it's just kind of a plea from myself and",
    "start": "3480480",
    "end": "3486720"
  },
  {
    "text": "Microsoft has this whole big responsible AI thing that I highly recommend everyone should read it is very very",
    "start": "3486720",
    "end": "3492960"
  },
  {
    "text": "long but very important especially over the next few years but before we wrap up I will leave",
    "start": "3492960",
    "end": "3500720"
  },
  {
    "text": "you with pattern number 45 and this to me is the most important pattern",
    "start": "3500720",
    "end": "3506760"
  },
  {
    "text": "because imagine every one of your users has seen this talk every one of your",
    "start": "3506760",
    "end": "3512039"
  },
  {
    "text": "users is now trying to do prompt injection attacks spam you with messages fill your contexts with as many tokens",
    "start": "3512039",
    "end": "3518680"
  },
  {
    "text": "as possible is trying to do cross-site scripting and sequal injection attacks on every single request they send",
    "start": "3518680",
    "end": "3526200"
  },
  {
    "text": "through treat every user this way treat them as Scott because hey you know what if I find a public facing bot you can",
    "start": "3526200",
    "end": "3533839"
  },
  {
    "text": "normally in one or two prompt injection attacks to determine what model it's using and from there the world's your",
    "start": "3533839",
    "end": "3539960"
  },
  {
    "text": "oyster so the big question is do you have the patterns to go to production here is the big slide of all",
    "start": "3539960",
    "end": "3547119"
  },
  {
    "text": "the patterns everything we've covered today who remembers everything that's on there no no one does I mean I keep",
    "start": "3547119",
    "end": "3553720"
  },
  {
    "text": "having to look to the slide to remember what's actually coming up next this has just been a tital wave of information",
    "start": "3553720",
    "end": "3559680"
  },
  {
    "text": "and this has been a talk that I've wanted to give for so long because we need everyone to understand the",
    "start": "3559680",
    "end": "3566240"
  },
  {
    "text": "importance of all aspects of generative AI when it comes to building intelligent apps not just the security tick boxes",
    "start": "3566240",
    "end": "3573240"
  },
  {
    "text": "and the security theater not just throwing in private networking because it needs a private IP address but",
    "start": "3573240",
    "end": "3579079"
  },
  {
    "text": "actually understanding what you need to get a solution to production now hasn't",
    "start": "3579079",
    "end": "3585000"
  },
  {
    "text": "been too many architectural diagrams there's fantastic open source ones out there I'm not going to repeat that",
    "start": "3585000",
    "end": "3590599"
  },
  {
    "text": "things like Azure chat for example are a great code first example of how to implement pretty much all of this so",
    "start": "3590599",
    "end": "3596480"
  },
  {
    "text": "these patterns do exist but this is a nice tick list to begin to go through and say hey have we considered XYZ for",
    "start": "3596480",
    "end": "3602880"
  },
  {
    "text": "our new application so with that we have officially hit an hour so I going to",
    "start": "3602880",
    "end": "3608760"
  },
  {
    "text": "finish up there thank you for listening and we'll do questions a little bit later thank",
    "start": "3608760",
    "end": "3613920"
  },
  {
    "text": "you and what is that uh 2.5 million prompt tokens seeing as you can only put",
    "start": "3617839",
    "end": "3624160"
  },
  {
    "text": "in like 100 to 250 characters that's pretty wild over the course of an hour and this is a couple of people in a room",
    "start": "3624160",
    "end": "3631400"
  },
  {
    "text": "um we had what is that 74k completion tokens I'm glad I set that limit low otherwise you would have just ruined",
    "start": "3631400",
    "end": "3638440"
  },
  {
    "text": "ruined the budget that I have there um hey 2.8 th000 messages sent for people",
    "start": "3638440",
    "end": "3645280"
  },
  {
    "text": "that were meant to be paying attention to a presentation I'm pretty pretty happy with but the number that I really",
    "start": "3645280",
    "end": "3651319"
  },
  {
    "text": "like of the 2,800 messages that were sent there were five",
    "start": "3651319",
    "end": "3657079"
  },
  {
    "text": "84 errors content filter validations or retries that we caught and gave a good",
    "start": "3657079",
    "end": "3664559"
  },
  {
    "text": "user experience because of everything we've seen in this talk so it does actually work and most importantly I",
    "start": "3664559",
    "end": "3671480"
  },
  {
    "text": "haven't seen a little notification pop up there to say that someone has managed to exfiltrate all three items so I'll",
    "start": "3671480",
    "end": "3678039"
  },
  {
    "text": "ask the question did anyone manage to get the secret word back at all no good",
    "start": "3678039",
    "end": "3683880"
  },
  {
    "text": "it means these pattern work uh the tool name or file name by any chance awesome that is fantastic it",
    "start": "3683880",
    "end": "3690480"
  },
  {
    "text": "means that some of these patterns actually work I'll keep it up for a little bit longer if you want to keep on playing around with it but I'm also",
    "start": "3690480",
    "end": "3696119"
  },
  {
    "text": "releasing all the code open source so you can actually see what's driving the whole experience from",
    "start": "3696119",
    "end": "3701880"
  },
  {
    "text": "today really quick yeah and it's funny because it's doing all of that in the",
    "start": "3701880",
    "end": "3707480"
  },
  {
    "text": "background the other fun thing that and this is why I love to blow people's minds that doesn't use streaming it's",
    "start": "3707480",
    "end": "3714079"
  },
  {
    "text": "actually waiting for the entire resp response to come back from the large language model before we send it down to",
    "start": "3714079",
    "end": "3719240"
  },
  {
    "text": "the user the streaming experience you see is entirely client side when the message pops up it already knows exactly",
    "start": "3719240",
    "end": "3725920"
  },
  {
    "text": "what it wants to say but funnily enough putting the characters on screen faster",
    "start": "3725920",
    "end": "3732000"
  },
  {
    "text": "once you get them feels faster than waiting for something to be typed out step by step so little cool ux tricks",
    "start": "3732000",
    "end": "3739599"
  },
  {
    "text": "but I'll stop there because we've gone over the hour thank you so much and I'll Stick Around for questions at the end thanks",
    "start": "3739599",
    "end": "3747319"
  }
]