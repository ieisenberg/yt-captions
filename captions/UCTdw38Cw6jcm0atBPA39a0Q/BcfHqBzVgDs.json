[
  {
    "text": "so today this session I will talk about analyzing Stack Exchange with Azure data",
    "start": "9599",
    "end": "15090"
  },
  {
    "text": "Lake so first I'm Tom Kharkov I'm from Belgium I'm an azure consultant at codit",
    "start": "15090",
    "end": "21119"
  },
  {
    "text": "which is a company in Europe I'm based in in Belgium so I'm also an agile MVP",
    "start": "21119",
    "end": "27210"
  },
  {
    "text": "and an advisor and I'm part of the agile user group in Belgium I wrote a little",
    "start": "27210",
    "end": "32250"
  },
  {
    "text": "white paper on the Internet of Things if you're interested you can find it over there so what's on the agenda first a",
    "start": "32250",
    "end": "40920"
  },
  {
    "text": "quick introduction to Azure data Lake and then we'll dive into what is Azure data Lake store and analytics and then",
    "start": "40920",
    "end": "48510"
  },
  {
    "text": "I'll use the stack exchange to show you how you can combine them and how easy it",
    "start": "48510",
    "end": "54059"
  },
  {
    "text": "is for me as a developer another data scientists to write queries that will",
    "start": "54059",
    "end": "59340"
  },
  {
    "text": "run on larger data sets so first big",
    "start": "59340",
    "end": "65039"
  },
  {
    "text": "data it's a big buzzword and if you start looking into it you have Hadoop",
    "start": "65039",
    "end": "70619"
  },
  {
    "text": "with all these other kind of technologies which are open source and",
    "start": "70619",
    "end": "76710"
  },
  {
    "text": "it's a rapidly evolving ecosystem so first the no-brain that will be let's",
    "start": "76710",
    "end": "85020"
  },
  {
    "text": "go open source and use these components but as a c-sharp and sequel developer",
    "start": "85020",
    "end": "90290"
  },
  {
    "text": "working with the Microsoft stack that brings a couple of challenges so the first is most of them support sequel",
    "start": "90290",
    "end": "99630"
  },
  {
    "text": "like languages but not the sequel language we're used to and also the",
    "start": "99630",
    "end": "105000"
  },
  {
    "text": "custom code well there's no c-sharp support so I need to learn a new language need to maintain that language",
    "start": "105000",
    "end": "112619"
  },
  {
    "text": "as well so it's another challenge it's rapidly evolving and also I need to spin",
    "start": "112619",
    "end": "119549"
  },
  {
    "text": "a plus this operators clusters need to know how to set them up it's also an",
    "start": "119549",
    "end": "125399"
  },
  {
    "text": "extra burden that I as a developer don't want last but not least it's typically",
    "start": "125399",
    "end": "130769"
  },
  {
    "text": "with Linux which is also not something a lot of Microsoft developers use as of",
    "start": "130769",
    "end": "138359"
  },
  {
    "text": "today it's evolving more towards Linux as well but not everyone is used to it so it's yet another hurdle",
    "start": "138359",
    "end": "146560"
  },
  {
    "text": "to jump so Microsoft has an alternative so they have what's called Azure data",
    "start": "146560",
    "end": "153640"
  },
  {
    "text": "Lake which is a kind of umbrella of three services on the left you have data",
    "start": "153640",
    "end": "159189"
  },
  {
    "text": "Lake store which is in web HDFS data store where basically there is no size",
    "start": "159189",
    "end": "166900"
  },
  {
    "text": "limitations you can put whatever data in it that you want then you have data like",
    "start": "166900",
    "end": "172359"
  },
  {
    "text": "analytics which is which is an analytic job service where I can just write scripts give it to data like analytics",
    "start": "172359",
    "end": "180639"
  },
  {
    "text": "and they'll run it for me I don't need to provision anything and then last but not least you have hdinsight where you",
    "start": "180639",
    "end": "186849"
  },
  {
    "text": "can provision those open source technologies if you want on a cluster so",
    "start": "186849",
    "end": "191919"
  },
  {
    "text": "this is what was available before where you still have to maintain the cluster",
    "start": "191919",
    "end": "197230"
  },
  {
    "text": "shut it up operate it and you use the open source but for me it's it's a big",
    "start": "197230",
    "end": "203799"
  },
  {
    "text": "jump to go to Big Data so let's dive into the details so data like store like",
    "start": "203799",
    "end": "210579"
  },
  {
    "text": "I said is built on web HDFS compatible and it can store any sizes of the files",
    "start": "210579",
    "end": "218530"
  },
  {
    "text": "and it has no limitation on the total size so it's basically an infinite",
    "start": "218530",
    "end": "223840"
  },
  {
    "text": "storage if you will but you know infinite doesn't exist you can also",
    "start": "223840",
    "end": "230979"
  },
  {
    "text": "store any formats it's a schema less data store so you don't need to define the schema out",
    "start": "230979",
    "end": "237129"
  },
  {
    "text": "front just store the files and it's a write once and read many principle so",
    "start": "237129",
    "end": "243639"
  },
  {
    "text": "it's optimized for running analytics chops on top of it it also has enterprise-grade security which we'll",
    "start": "243639",
    "end": "250090"
  },
  {
    "text": "see later it's built on Azure ad so not on fancy authentication keys just using",
    "start": "250090",
    "end": "255849"
  },
  {
    "text": "the security we already have so it's basically the big data store and a",
    "start": "255849",
    "end": "262690"
  },
  {
    "text": "if you don't want to run your own cluster so it's fairly easy and as you",
    "start": "262690",
    "end": "270370"
  },
  {
    "text": "can see it's the center of your big data ecosystem if you are working with the",
    "start": "270370",
    "end": "275919"
  },
  {
    "text": "cloud where on the left you have all these other services which you can use to ingest data on top you",
    "start": "275919",
    "end": "281830"
  },
  {
    "text": "can have multiple services to process the data so we'll use Azure data like",
    "start": "281830",
    "end": "287020"
  },
  {
    "text": "analytics then you can visualize it with other services where we will use power bi and then on the right you can also",
    "start": "287020",
    "end": "294070"
  },
  {
    "text": "download it or move it to another data store if you need so basically it's all",
    "start": "294070",
    "end": "299350"
  },
  {
    "text": "built around data like store a small caveat what is the data like and how",
    "start": "299350",
    "end": "306280"
  },
  {
    "text": "does it compare with the data warehouse because it's not the same a data warehouse is typically a warehouse which",
    "start": "306280",
    "end": "312370"
  },
  {
    "text": "is also a central place to store your data but it's a central place with",
    "start": "312370",
    "end": "317560"
  },
  {
    "text": "structured data so this means that you need to define the schema of your data up front you need to create those",
    "start": "317560",
    "end": "324540"
  },
  {
    "text": "extract transform and load pipelines to make sure that the data is in the format",
    "start": "324540",
    "end": "331840"
  },
  {
    "text": "that is required so that's it's been around for a while a lot of companies",
    "start": "331840",
    "end": "337360"
  },
  {
    "text": "use this and then on the other side you have data like where you just basically dump all the data in the raw format and",
    "start": "337360",
    "end": "345460"
  },
  {
    "text": "the data lake itself this can be unstructured structured and semi-structured and you don't need to",
    "start": "345460",
    "end": "352810"
  },
  {
    "text": "define any schema okay and then the data scientists just go in the data they can",
    "start": "352810",
    "end": "358960"
  },
  {
    "text": "select the data they are interested in check what the schema and then process",
    "start": "358960",
    "end": "364150"
  },
  {
    "text": "it this has the advantage that you don't get rid of the of certain data",
    "start": "364150",
    "end": "369940"
  },
  {
    "text": "attributes when you're storing it because with the data warehouse you need to define the schema and it can happen",
    "start": "369940",
    "end": "376960"
  },
  {
    "text": "that as of today you don't need a certain data field within two years she might but since you had to transform the",
    "start": "376960",
    "end": "384040"
  },
  {
    "text": "data set you already lost all that information so you can't use it anymore",
    "start": "384040",
    "end": "389400"
  },
  {
    "text": "but the downside of the data like is since it doesn't have any structure by",
    "start": "389400",
    "end": "395380"
  },
  {
    "text": "nature it can be a data swamp which is not nice so if we see this picture of",
    "start": "395380",
    "end": "401919"
  },
  {
    "text": "Martin Fowler which wrote a very nice article about this you can see on the",
    "start": "401919",
    "end": "407050"
  },
  {
    "text": "Left data warehouse where you basically clean the data before you store it and then the",
    "start": "407050",
    "end": "412570"
  },
  {
    "text": "analysis is known based on that schema so basically you prepare the data for",
    "start": "412570",
    "end": "419620"
  },
  {
    "text": "the analysts where on the right you have the data lake where you just have everything in one big data store and",
    "start": "419620",
    "end": "427120"
  },
  {
    "text": "they just transform the data they need for their report visualizations but",
    "start": "427120",
    "end": "432880"
  },
  {
    "text": "whatsoever you can basically compare it with fishing where on the Left if you",
    "start": "432880",
    "end": "440080"
  },
  {
    "text": "want to eat a trout you go to the data warehouse and you buy the trout it's",
    "start": "440080",
    "end": "445270"
  },
  {
    "text": "already prepared and everything well on the right you can still go fishing in",
    "start": "445270",
    "end": "450880"
  },
  {
    "text": "the data Lake where you can select the fish you want and cook it how you want and prepare it how you want but if if",
    "start": "450880",
    "end": "461410"
  },
  {
    "text": "the lake is full and you create a data swamp your fish will die and the data",
    "start": "461410",
    "end": "467950"
  },
  {
    "text": "will be useless it's a small comparison you can make so a security for data Lake",
    "start": "467950",
    "end": "475540"
  },
  {
    "text": "story can use role based access control for the store itself and you can grant",
    "start": "475540",
    "end": "482140"
  },
  {
    "text": "users and group access to certain folders and files so you can really lock down who has access to what and really",
    "start": "482140",
    "end": "489760"
  },
  {
    "text": "control this it also has a firewall which is off by default unfortunately",
    "start": "489760",
    "end": "495330"
  },
  {
    "text": "but this allows you to really protect your data set from the inside and only",
    "start": "495330",
    "end": "502030"
  },
  {
    "text": "allow the certain IP addresses to get access to it and you can also add encryption at rest where either",
    "start": "502030",
    "end": "508600"
  },
  {
    "text": "Microsoft controls the keys or you provide your own with Azure key vault ok",
    "start": "508600",
    "end": "515080"
  },
  {
    "text": "and pricing here's a big overview but basically you use a pay-as-you-go model",
    "start": "515080",
    "end": "521289"
  },
  {
    "text": "or you use a monthly commitment package which makes it up to 33% cheaper but",
    "start": "521290",
    "end": "529960"
  },
  {
    "text": "then you need to pay those packages you",
    "start": "529960",
    "end": "536410"
  },
  {
    "text": "basically paper to reside per transaction for read and write and as you can see read is cheaper why because",
    "start": "536410",
    "end": "542950"
  },
  {
    "text": "it's built for analytic and they want to motivate you to less frequently you write to the data store",
    "start": "542950",
    "end": "550020"
  },
  {
    "text": "okay and then a small comparison when I should blob storage because you can use",
    "start": "550020",
    "end": "555940"
  },
  {
    "text": "that as well you can see that while data like store doesn't have any limitations",
    "start": "555940",
    "end": "562209"
  },
  {
    "text": "as if storage has them they're very big so I think you can have up to 200 now",
    "start": "562209",
    "end": "568690"
  },
  {
    "text": "100 accounts per subscription up to hundreds of terabytes so it also very",
    "start": "568690",
    "end": "574149"
  },
  {
    "text": "big but you need to separate them across blob storages then security it has the",
    "start": "574149",
    "end": "581200"
  },
  {
    "text": "ad support like I mentioned while in blob storage II need to use SAS tokens",
    "start": "581200",
    "end": "587140"
  },
  {
    "text": "which is less secure but the pricing for data like stored is more expensive I",
    "start": "587140",
    "end": "593910"
  },
  {
    "text": "think the only storage option that is more expensive is a globally redundant",
    "start": "593910",
    "end": "603220"
  },
  {
    "text": "storage with read access so that's the most expensive one redundancy is a",
    "start": "603220",
    "end": "611529"
  },
  {
    "text": "different topic because data like story don't have access you don't control how",
    "start": "611529",
    "end": "616630"
  },
  {
    "text": "redundant it is I think it's only locally redundant so if one region goes down you currently lose your data which",
    "start": "616630",
    "end": "625029"
  },
  {
    "text": "is not a not really nice well with blob storage you can basically control it",
    "start": "625029",
    "end": "631750"
  },
  {
    "text": "locally zone geo read access scalability",
    "start": "631750",
    "end": "638350"
  },
  {
    "text": "well data like store is built for analytics so it's really optimized for reads less for writes while blob storage",
    "start": "638350",
    "end": "646300"
  },
  {
    "text": "basically is built for the two and they both have integration with data factory",
    "start": "646300",
    "end": "652810"
  },
  {
    "text": "data catalog and hdinsight so from that perspective there's the same okay we're",
    "start": "652810",
    "end": "660160"
  },
  {
    "text": "almost through the introduction data like analytics is the most interesting",
    "start": "660160",
    "end": "665380"
  },
  {
    "text": "one to me because you basically have a serverless kind of analytic service",
    "start": "665380",
    "end": "671560"
  },
  {
    "text": "where you write the jobs you write the scripts you give it to the service the service interpret",
    "start": "671560",
    "end": "677680"
  },
  {
    "text": "them optimizes them shares them across multiple notes and they run it all for",
    "start": "677680",
    "end": "683290"
  },
  {
    "text": "you you basically just give them the script and to do that you can use you sequel",
    "start": "683290",
    "end": "688660"
  },
  {
    "text": "which is a mix of T sequel language and c-sharp language so basically basically",
    "start": "688660",
    "end": "695649"
  },
  {
    "text": "I can just write the sequel scripts if I need to have some extensibility I can",
    "start": "695649",
    "end": "701500"
  },
  {
    "text": "just use my c-sharp scripts you provision a certain amount of analytics",
    "start": "701500",
    "end": "708040"
  },
  {
    "text": "units once you submit the job which basically defines how many nodes you want and then you pay per processing",
    "start": "708040",
    "end": "716890"
  },
  {
    "text": "time per analytics unit so the more units you use the faster the job will be",
    "start": "716890",
    "end": "724089"
  },
  {
    "text": "completed but you will pay a lot more which makes sense but we'll see that later on so basically",
    "start": "724089",
    "end": "732339"
  },
  {
    "text": "I have these kinds of data stores that are supported so data like store obviously which is also a required one",
    "start": "732339",
    "end": "740529"
  },
  {
    "text": "because they store metadata for the jobs and then you can also use blob storage for query and write and then any kind of",
    "start": "740529",
    "end": "748930"
  },
  {
    "text": "sequel datastore and Azure for reads and data like analytics also has built-in",
    "start": "748930",
    "end": "756850"
  },
  {
    "text": "partition tables if you need those which can come in very handy so basically you",
    "start": "756850",
    "end": "763209"
  },
  {
    "text": "just use the correct data store you want if you for example have a lot of",
    "start": "763209",
    "end": "768510"
  },
  {
    "text": "telemetry data and data Lake but you need to correlate them with a different data set which is in another database",
    "start": "768510",
    "end": "775060"
  },
  {
    "text": "you can just do that as well you're not limited to data Lake store and then this",
    "start": "775060",
    "end": "783430"
  },
  {
    "text": "is an example of a UC cool script where at the top you basically extract the",
    "start": "783430",
    "end": "789070"
  },
  {
    "text": "data from a data set where you also define the schema of that data set so",
    "start": "789070",
    "end": "794560"
  },
  {
    "text": "this is where you need to do it not when you ingest the data between once you want to extract it and you could say I",
    "start": "794560",
    "end": "801579"
  },
  {
    "text": "want to use CSV TSV or you can write your own which we will do then you can",
    "start": "801579",
    "end": "807820"
  },
  {
    "text": "just transform and analyze the data how you using see a sequel syntax in line for",
    "start": "807820",
    "end": "815470"
  },
  {
    "text": "example over here or over here or you can also use method calls however you",
    "start": "815470",
    "end": "823300"
  },
  {
    "text": "want and then at the bottom you basically output the result of your scripts to another location so here it",
    "start": "823300",
    "end": "831760"
  },
  {
    "text": "is a data Lake store but you can also do it to to blob storage and then you can order by or whatever and then you can",
    "start": "831760",
    "end": "838810"
  },
  {
    "text": "choose how you want to output it here it's with the CSV but again you can use",
    "start": "838810",
    "end": "844630"
  },
  {
    "text": "TSV or create around output so this is a very basic example script but you can",
    "start": "844630",
    "end": "853090"
  },
  {
    "text": "really go as crazy as you want but this is the way you need to do it basically",
    "start": "853090",
    "end": "859540"
  },
  {
    "text": "extract transform analyze and output it so extensibility like I said you can use",
    "start": "859540",
    "end": "867100"
  },
  {
    "text": "n9c sharp oregon creatives custom defined functions operators and",
    "start": "867100",
    "end": "874710"
  },
  {
    "text": "aggregators depending on your scenario and those operators are also a couple of",
    "start": "874710",
    "end": "882520"
  },
  {
    "text": "them so we will create our own extractive to extract it from the data store then you can have processes",
    "start": "882520",
    "end": "889180"
  },
  {
    "text": "reduces combiner suppliers and output this all with their own news case but",
    "start": "889180",
    "end": "894700"
  },
  {
    "text": "they all come in handy this is a big overview of the metadata model i won't",
    "start": "894700",
    "end": "900940"
  },
  {
    "text": "go in detail because of the time but you can basically see that you have an as a data like analytics catalog which is",
    "start": "900940",
    "end": "907180"
  },
  {
    "text": "your instance in Azure under which you can have multiple databases basically",
    "start": "907180",
    "end": "914110"
  },
  {
    "text": "like you have with the database server you can also have multiple databases and then limit who has access to which",
    "start": "914110",
    "end": "920020"
  },
  {
    "text": "database and then under there you have also data sources that are linked schema stables etc and also c-sharp assemblies",
    "start": "920020",
    "end": "928480"
  },
  {
    "text": "that can be used in the scripts for that extensibility okay",
    "start": "928480",
    "end": "934600"
  },
  {
    "text": "if you provision you seek to see you sequel script it's basically compiled and then they define how the processing",
    "start": "934600",
    "end": "942460"
  },
  {
    "text": "should look like and then they optimize the script for you by creating the optimized plan they",
    "start": "942460",
    "end": "947920"
  },
  {
    "text": "use the job scheduler to keep the job for you then the job manager will divide the work across",
    "start": "947920",
    "end": "955509"
  },
  {
    "text": "multiple vortexes so that's basically your data set which they divide in smaller pieces and then they will just",
    "start": "955509",
    "end": "963129"
  },
  {
    "text": "distribute it across all the nodes once they are all completed they will come",
    "start": "963129",
    "end": "968589"
  },
  {
    "text": "back to the job manager and then your job is finished and the output is in data like stored if you want to have a",
    "start": "968589",
    "end": "974829"
  },
  {
    "text": "more detailed overview of this Michael Royce did a very good presentation on tuning and optimizing new sequel scripts",
    "start": "974829",
    "end": "982060"
  },
  {
    "text": "where he goes more into depth on this topic so if you have more advanced",
    "start": "982060",
    "end": "987819"
  },
  {
    "text": "scripts this is really useful to have ok and this is an example of how you can",
    "start": "987819",
    "end": "994569"
  },
  {
    "text": "write a simple script and how they optimize it into all these pieces for you and you don't need to know how it",
    "start": "994569",
    "end": "1001980"
  },
  {
    "text": "all works just submit it and they do the magic for you then you can see the all",
    "start": "1001980",
    "end": "1008009"
  },
  {
    "text": "the states of a job basically you create a new one where they compile the script they cue it and they schedule it and it",
    "start": "1008009",
    "end": "1014370"
  },
  {
    "text": "starts processing the transient and it ends so we can all follow this along in",
    "start": "1014370",
    "end": "1019379"
  },
  {
    "text": "the tooling so almost there security we also have our box support so you can",
    "start": "1019379",
    "end": "1025709"
  },
  {
    "text": "support the instance in Azure where you",
    "start": "1025709",
    "end": "1030959"
  },
  {
    "text": "can select who can manage the instance itself it also has a firewall and you",
    "start": "1030959",
    "end": "1036298"
  },
  {
    "text": "can also have access control on the catalog or the database like I mentioned",
    "start": "1036299",
    "end": "1042270"
  },
  {
    "text": "so we can lock down one database for user a what a user B only has access to the other one and you can also use",
    "start": "1042270",
    "end": "1050730"
  },
  {
    "text": "resource management because it's not that cheap you can also make sure that",
    "start": "1050730",
    "end": "1056280"
  },
  {
    "text": "if somebody new starts they don't provision 100 analytics units while they",
    "start": "1056280",
    "end": "1062370"
  },
  {
    "text": "only need 10 so you can use these groups where you can say ok these are new",
    "start": "1062370",
    "end": "1067620"
  },
  {
    "text": "developers can only provision 5 analytics units well if it's for production we want to have more",
    "start": "1067620",
    "end": "1073409"
  },
  {
    "text": "analytics units and a higher priority and the job service",
    "start": "1073409",
    "end": "1079110"
  },
  {
    "text": "and then on an account level you can also assign the maximum analytics units maximum concurrent jobs and how long the",
    "start": "1079110",
    "end": "1086039"
  },
  {
    "text": "metadata of the job should be available",
    "start": "1086039",
    "end": "1091549"
  },
  {
    "text": "and for pricing you basically buy pay 28",
    "start": "1091549",
    "end": "1099570"
  },
  {
    "text": "pennies per minute of an analytics unit which is not that much but like I said",
    "start": "1099570",
    "end": "1106289"
  },
  {
    "text": "the more you sign the more expensive it gets and it's not because you assign",
    "start": "1106289",
    "end": "1111750"
  },
  {
    "text": "more units that script will be faster it all depends on on how the script works",
    "start": "1111750",
    "end": "1118559"
  },
  {
    "text": "the tooling also provides an optimizer where you can see okay I've provisioned 20 Analytics units",
    "start": "1118559",
    "end": "1125880"
  },
  {
    "text": "but basically the service only use 10 so if I submit it again we maybe should",
    "start": "1125880",
    "end": "1132030"
  },
  {
    "text": "optimize to 10 units because we overpaid for 10 units and here we also have the",
    "start": "1132030",
    "end": "1138990"
  },
  {
    "text": "monthly commitment packages okay a lot of talk so let's show it in action",
    "start": "1138990",
    "end": "1145860"
  },
  {
    "text": "so what we will do is Stack Exchange has over twenty two hundred and eighty",
    "start": "1145860",
    "end": "1152490"
  },
  {
    "text": "websites including the meta websites of which they have 150 gigabytes of data",
    "start": "1152490",
    "end": "1159240"
  },
  {
    "text": "they publish so we'll use that data set as an example where we will get the data",
    "start": "1159240",
    "end": "1167330"
  },
  {
    "text": "by downloading it I actually already did that because we also need to extract the",
    "start": "1167330",
    "end": "1173970"
  },
  {
    "text": "zip files upload those and data like so",
    "start": "1173970",
    "end": "1179280"
  },
  {
    "text": "I did that already but I'll show you the script I used it's fairly easy and then",
    "start": "1179280",
    "end": "1185010"
  },
  {
    "text": "once it's there we will start merging all the datasets for each website and to",
    "start": "1185010",
    "end": "1190559"
  },
  {
    "text": "an aggregated file and then based on that aggregated file we will try to",
    "start": "1190559",
    "end": "1196380"
  },
  {
    "text": "determine how many people filled in the location on their profile which actually",
    "start": "1196380",
    "end": "1202020"
  },
  {
    "text": "matches to a country and then we will visualize that in power bi so this is a",
    "start": "1202020",
    "end": "1209820"
  },
  {
    "text": "small setup of what we used so we have archive.org which is the",
    "start": "1209820",
    "end": "1215220"
  },
  {
    "text": "website where they publish this so I provision the VM and North Europe where",
    "start": "1215220",
    "end": "1220500"
  },
  {
    "text": "I download all the the zip files I extract them on an SSD of one terabyte",
    "start": "1220500",
    "end": "1227520"
  },
  {
    "text": "so I had a lot of XML files and then I use the PowerShell script to upload it",
    "start": "1227520",
    "end": "1233670"
  },
  {
    "text": "to data Lake store then I used two scripts to aggregate them and run the",
    "start": "1233670",
    "end": "1241050"
  },
  {
    "text": "business logic on top of it and then power bi uses uses the data of data like",
    "start": "1241050",
    "end": "1246120"
  },
  {
    "text": "store ok so let's not even do it first",
    "start": "1246120",
    "end": "1253320"
  },
  {
    "text": "this is the website for those are interested its archive.org but somehow",
    "start": "1253320",
    "end": "1262040"
  },
  {
    "text": "this thing is in front of it where you basically have every I think it's six",
    "start": "1262040",
    "end": "1267600"
  },
  {
    "text": "months a new data set of all the data and if you have a look they basically have one zip file app or website",
    "start": "1267600",
    "end": "1275730"
  },
  {
    "text": "except for Stack Exchange because it's way too big so if you have a look over",
    "start": "1275730",
    "end": "1281190"
  },
  {
    "text": "here you can see for example this guy",
    "start": "1281190",
    "end": "1288230"
  },
  {
    "text": "all the posts on Stack Overflow is 18 gigabytes gigabytes and a compressed",
    "start": "1288230",
    "end": "1294540"
  },
  {
    "text": "version so it's not that small so what",
    "start": "1294540",
    "end": "1301380"
  },
  {
    "text": "did I do I wrote a small PowerShell script which",
    "start": "1301380",
    "end": "1307790"
  },
  {
    "text": "basically authenticates with Azure ad big with as your research manager sorry",
    "start": "1307790",
    "end": "1314660"
  },
  {
    "text": "yes how do you do any Visual Studio codes",
    "start": "1314660",
    "end": "1322159"
  },
  {
    "text": "no I don't work on code ah yes it does sorry okay",
    "start": "1325920",
    "end": "1333270"
  },
  {
    "text": "so first we will authenticate and assign the subscription we want which basically",
    "start": "1333270",
    "end": "1340170"
  },
  {
    "text": "just authenticates what as a resource manager and then it will just use the",
    "start": "1340170",
    "end": "1346050"
  },
  {
    "text": "azure data like store commandlets to say okay sorry wrong script that was to",
    "start": "1346050",
    "end": "1355890"
  },
  {
    "text": "download because I actually had to move one from a data Lake to another data like so I first had to download it",
    "start": "1355890",
    "end": "1364370"
  },
  {
    "text": "okay the correct script so basically I just authenticate and then I want to",
    "start": "1364370",
    "end": "1370500"
  },
  {
    "text": "upload them all so what I did was I specified the data like store account name the local folder on my VM and then",
    "start": "1370500",
    "end": "1378540"
  },
  {
    "text": "the destination folder on my data Lake which was basically the routes so if we",
    "start": "1378540",
    "end": "1384870"
  },
  {
    "text": "have a look I basically just clean up the parameters first and then for each",
    "start": "1384870",
    "end": "1391710"
  },
  {
    "text": "file I just loop over all the folders then",
    "start": "1391710",
    "end": "1397650"
  },
  {
    "text": "remove slashes because data Lake doesn't like them and then we start uploading so",
    "start": "1397650",
    "end": "1403050"
  },
  {
    "text": "if it's a folder I just check okay does it already already exist if it doesn't I",
    "start": "1403050",
    "end": "1410070"
  },
  {
    "text": "just create a new a Jairam data like store item where I say okay it's a",
    "start": "1410070",
    "end": "1415950"
  },
  {
    "text": "folder and this is the destination if it isn't and then I just do an import of an",
    "start": "1415950",
    "end": "1421500"
  },
  {
    "text": "item where I say okay this is the account again this is the path to my",
    "start": "1421500",
    "end": "1426510"
  },
  {
    "text": "file this is where I where it should end up and then I force it and that's all I",
    "start": "1426510",
    "end": "1433410"
  },
  {
    "text": "had to do so basically just iterate over all the files if it's a folder create a folder and data like if it's not just",
    "start": "1433410",
    "end": "1441330"
  },
  {
    "text": "import a file that's all I had to do then you can see I imported three data",
    "start": "1441330",
    "end": "1450060"
  },
  {
    "text": "set which result in 620 gigabytes and if",
    "start": "1450060",
    "end": "1455610"
  },
  {
    "text": "I go over here you can see I have a three data Lake store accounts where the last",
    "start": "1455610",
    "end": "1463730"
  },
  {
    "text": "one contains all my stock exchange data actually if I have a look at that one",
    "start": "1463730",
    "end": "1469510"
  },
  {
    "text": "you can see that there is a data Explorer where you can basically just",
    "start": "1469510",
    "end": "1475100"
  },
  {
    "text": "browse the whole data store and download all the files get previews etc so here",
    "start": "1475100",
    "end": "1483530"
  },
  {
    "text": "you can see all the websites etc I can if somebody needs to gain access I can",
    "start": "1483530",
    "end": "1488630"
  },
  {
    "text": "do it over here okay so this is the data set we will be working on so for this we",
    "start": "1488630",
    "end": "1496520"
  },
  {
    "text": "need to create a new data like store accounts to publish our result for power",
    "start": "1496520",
    "end": "1506210"
  },
  {
    "text": "bi because I don't want to use the other one so you just give it a name for",
    "start": "1506210",
    "end": "1511310"
  },
  {
    "text": "example NDC demo use the resource group",
    "start": "1511310",
    "end": "1519070"
  },
  {
    "text": "that's better so any research group the region I want",
    "start": "1519820",
    "end": "1525500"
  },
  {
    "text": "to do and then you can select the encryption settings so either I can choose data like store or key vault and",
    "start": "1525500",
    "end": "1532690"
  },
  {
    "text": "because I like keyboard volt more I just then select the keyboard I want to use i",
    "start": "1532690",
    "end": "1540770"
  },
  {
    "text": "select the encryption key so let's use this one I select encryption key I want to use",
    "start": "1540770",
    "end": "1549310"
  },
  {
    "text": "and then I hit OK but because it takes a while I'm going to skip this and use",
    "start": "1551350",
    "end": "1557810"
  },
  {
    "text": "this one for example and once you provision it it gives me this warning why because I should keyboard is using",
    "start": "1557810",
    "end": "1565160"
  },
  {
    "text": "Azure ad as an authentication mechanism and basically over here I first need to",
    "start": "1565160",
    "end": "1570550"
  },
  {
    "text": "say okay this data like store account I grant this access to my date to my a",
    "start": "1570550",
    "end": "1579200"
  },
  {
    "text": "jerky vault so over here you can see my vault my subscription and the key I want to",
    "start": "1579200",
    "end": "1584810"
  },
  {
    "text": "use and then you basically grant the permissions okay but I'm not going to do that",
    "start": "1584810",
    "end": "1591220"
  },
  {
    "text": "because I already have one over here and as I mentioned it has a day it has a",
    "start": "1591220",
    "end": "1598600"
  },
  {
    "text": "firewall which is off by default so this is something I would typically turn on",
    "start": "1598600",
    "end": "1604030"
  },
  {
    "text": "but for the sake of the demo we don't care for the rest there's nothing really",
    "start": "1604030",
    "end": "1609520"
  },
  {
    "text": "fancy in here we have a data Explorer that I showed we have some metrics which",
    "start": "1609520",
    "end": "1614560"
  },
  {
    "text": "we which we found on the dashboards and that's basically it so no alerts that's",
    "start": "1614560",
    "end": "1624370"
  },
  {
    "text": "it so if you need to operate this the only thing you have is an overview of the amount of data",
    "start": "1624370",
    "end": "1630570"
  },
  {
    "text": "okay so aggregating the data our data is",
    "start": "1630570",
    "end": "1636490"
  },
  {
    "text": "in data lake we can now start creating a you sequel script that will run on top",
    "start": "1636490",
    "end": "1641620"
  },
  {
    "text": "of all these websites and that will output the result set in this data Lake",
    "start": "1641620",
    "end": "1646810"
  },
  {
    "text": "store for that we need a data like analytics instance which I'll I already",
    "start": "1646810",
    "end": "1654100"
  },
  {
    "text": "created oh sorry which are already created as this one",
    "start": "1654100",
    "end": "1660120"
  },
  {
    "text": "and if I go over here I can see that",
    "start": "1660120",
    "end": "1669310"
  },
  {
    "text": "there's a default data source why is this every data like analytics unit",
    "start": "1669310",
    "end": "1675100"
  },
  {
    "text": "needs a data like store account because it needs to store sure metadata about the service and this is c-sharp",
    "start": "1675100",
    "end": "1683580"
  },
  {
    "text": "assemblies or also the history about jobs files basically just the data store",
    "start": "1683580",
    "end": "1690340"
  },
  {
    "text": "for your data like analytics but we can also add another data source which",
    "start": "1690340",
    "end": "1696460"
  },
  {
    "text": "allows us to use it in in the you sequel scripts so if you want to use one and",
    "start": "1696460",
    "end": "1701860"
  },
  {
    "text": "you reduce equal scripts you need to add it here first because otherwise it will fail so we",
    "start": "1701860",
    "end": "1707200"
  },
  {
    "text": "will add one by account name and I just select the data like the stock exchange",
    "start": "1707200",
    "end": "1712950"
  },
  {
    "text": "data like store the one we saw so I click add and now I can add this to my",
    "start": "1712950",
    "end": "1721390"
  },
  {
    "text": "use sequel scripts so this guy is ready to go now we go to",
    "start": "1721390",
    "end": "1728080"
  },
  {
    "text": "visual studio sorry I forgot to show you",
    "start": "1728080",
    "end": "1733900"
  },
  {
    "text": "the data set so as you can see this is",
    "start": "1733900",
    "end": "1739030"
  },
  {
    "text": "an XML for data a big data processing",
    "start": "1739030",
    "end": "1744059"
  },
  {
    "text": "xml's are not really nice why because they have a certain structure if I want",
    "start": "1744059",
    "end": "1749860"
  },
  {
    "text": "to tear this file apart into multiple pieces to separate it across multiple",
    "start": "1749860",
    "end": "1755200"
  },
  {
    "text": "notes this is actually a problem why because if I separate it into two pieces",
    "start": "1755200",
    "end": "1760299"
  },
  {
    "text": "my route notes will be not in the same file as the the ending root note so",
    "start": "1760299",
    "end": "1767140"
  },
  {
    "text": "basically once we the first step and the aggregation is also transformation to a",
    "start": "1767140",
    "end": "1774130"
  },
  {
    "text": "CSV file so what we will do is we will write a custom extractor",
    "start": "1774130",
    "end": "1779620"
  },
  {
    "text": "because this is not supported by default and then in that custom extracted we",
    "start": "1779620",
    "end": "1786370"
  },
  {
    "text": "will also say each file should be processed in the same format so that we",
    "start": "1786370",
    "end": "1795040"
  },
  {
    "text": "can keep the whole structure of the XML so here it's just a root node with all rows in it but imagine if you have",
    "start": "1795040",
    "end": "1801700"
  },
  {
    "text": "multiple sub structures that would also be broken so basically what you need to",
    "start": "1801700",
    "end": "1807850"
  },
  {
    "text": "do is you just need to just pull in a",
    "start": "1807850",
    "end": "1816280"
  },
  {
    "text": "nougat package which is this guy and you basically just derive implement I",
    "start": "1816280",
    "end": "1823270"
  },
  {
    "text": "extracted and then you need to provide implement this extract methods and also",
    "start": "1823270",
    "end": "1830110"
  },
  {
    "text": "notice over here this is the flag telling data like analytics not to",
    "start": "1830110",
    "end": "1837000"
  },
  {
    "text": "separate file into multiple pieces because of the structure this is",
    "start": "1837000",
    "end": "1842380"
  },
  {
    "text": "actually a very funny one to find so don't ask me how I know so basically",
    "start": "1842380",
    "end": "1848440"
  },
  {
    "text": "what you get in is just an input and an output where we will just use an XM",
    "start": "1848440",
    "end": "1856150"
  },
  {
    "text": "read from donut where we load the stream of the input and then we just do the XML",
    "start": "1856150",
    "end": "1863680"
  },
  {
    "text": "parsing like we do and that's where we check okay if it's a row then we know",
    "start": "1863680",
    "end": "1869380"
  },
  {
    "text": "it's an entry for our outputs and then we just loop over all the over the",
    "start": "1869380",
    "end": "1876340"
  },
  {
    "text": "schema and then we interpret the data okay and then if it's null then we just",
    "start": "1876340",
    "end": "1883780"
  },
  {
    "text": "set a default value because we need to have a default for each row because",
    "start": "1883780",
    "end": "1889780"
  },
  {
    "text": "otherwise our CSV is messed up and then we check if it's a string then we",
    "start": "1889780",
    "end": "1897570"
  },
  {
    "text": "simplify it why do we do this because we read it what is it",
    "start": "1897570",
    "end": "1903070"
  },
  {
    "text": "okay we remove these because they don't like it and certainly if it's a CSV that",
    "start": "1903070",
    "end": "1909460"
  },
  {
    "text": "uses a comma I don't want to use a comma in the data fields I just change it to a",
    "start": "1909460",
    "end": "1916350"
  },
  {
    "text": "you know the English word a comma with a dot on top of it semicolon thank you okay so basically we",
    "start": "1916350",
    "end": "1924220"
  },
  {
    "text": "just clean the data a bit and there's a byte boundary so if it's exceeds that we",
    "start": "1924220",
    "end": "1932200"
  },
  {
    "text": "just adapt that and then we just say okay output this is in the name of the column and this is the value if it's not",
    "start": "1932200",
    "end": "1939340"
  },
  {
    "text": "a string for example an int so here this column basically refers to the script",
    "start": "1939340",
    "end": "1947050"
  },
  {
    "text": "where we will define the scheme of the fowl and then we just convert it to the",
    "start": "1947050",
    "end": "1953140"
  },
  {
    "text": "value okay and then we just return every row okay if you want to play with this",
    "start": "1953140",
    "end": "1961300"
  },
  {
    "text": "later on it will be available so I'm going pretty quick but you will be able",
    "start": "1961300",
    "end": "1968050"
  },
  {
    "text": "to do this afterwards so for example this guy if we want to aggregate all the",
    "start": "1968050",
    "end": "1976630"
  },
  {
    "text": "users which we will need for the demo we start first with referencing to the",
    "start": "1976630",
    "end": "1983470"
  },
  {
    "text": "assembly which contains our custom extractor which will be deployed in data",
    "start": "1983470",
    "end": "1988510"
  },
  {
    "text": "lake and a letter and there we say okay go to this data",
    "start": "1988510",
    "end": "1994330"
  },
  {
    "text": "lake store account where I can use these kinds of place holders to loop over all",
    "start": "1994330",
    "end": "2001110"
  },
  {
    "text": "the folders instead of having to add all of them I just say okay let's call this",
    "start": "2001110",
    "end": "2006690"
  },
  {
    "text": "dump name and I can use this mi output as well which is actually very nice and then the source is the name of the",
    "start": "2006690",
    "end": "2013740"
  },
  {
    "text": "website and my folder structure which will also be included in in the result set and then I just referred to my",
    "start": "2013740",
    "end": "2022740"
  },
  {
    "text": "custom extractor and captured this in a",
    "start": "2022740",
    "end": "2027929"
  },
  {
    "text": "variable and then I just output this one to a CSV file on my default data like",
    "start": "2027929",
    "end": "2035429"
  },
  {
    "text": "store so that's why it's a shorthand notation well over here I need to prefix",
    "start": "2035429",
    "end": "2040679"
  },
  {
    "text": "the whole name that's the difference okay for the sake of the demo we will",
    "start": "2040679",
    "end": "2047220"
  },
  {
    "text": "not use the three data sets why because we will have a lot of duplicate users",
    "start": "2047220",
    "end": "2052378"
  },
  {
    "text": "because it's the same data but just a slice of six months added to this so I",
    "start": "2052379",
    "end": "2058349"
  },
  {
    "text": "have another one where I don't use the dump aim but basically fix the June data",
    "start": "2058349",
    "end": "2064740"
  },
  {
    "text": "set but before we want to do this we need to deploy our custom code first so",
    "start": "2064740",
    "end": "2072118"
  },
  {
    "text": "there's one option where you can run au sequel script where you say okay in",
    "start": "2072119",
    "end": "2079260"
  },
  {
    "text": "database master drop the old version first because this is the new version where we do create assembly and give it",
    "start": "2079260",
    "end": "2086940"
  },
  {
    "text": "a name and then we can point to a data like store folder where we uploaded the",
    "start": "2086940",
    "end": "2095368"
  },
  {
    "text": "DLL an alternative is we go to visual studio and we just say okay register",
    "start": "2095369",
    "end": "2101849"
  },
  {
    "text": "this DLL and then I don't need to upload it come on wake up there.this so I",
    "start": "2101849",
    "end": "2111599"
  },
  {
    "text": "choose my data like store a yeah and the italic analytics account sorry and I",
    "start": "2111599",
    "end": "2116819"
  },
  {
    "text": "used my user and it will ask me which database so for the sake of",
    "start": "2116819",
    "end": "2123030"
  },
  {
    "text": "the demo I'll just use the master database but if you want to use another one you select another one here and then",
    "start": "2123030",
    "end": "2129420"
  },
  {
    "text": "I give it a name if there was already another one I could select this this",
    "start": "2129420",
    "end": "2136260"
  },
  {
    "text": "checkbox to override it and then I choose the amount of data lakes units I want to use for this but because it's",
    "start": "2136260",
    "end": "2143280"
  },
  {
    "text": "just registering a DLL I could provision five but it will only use one so then I",
    "start": "2143280",
    "end": "2150690"
  },
  {
    "text": "hit submit and let's hope the Internet is very nice okay",
    "start": "2150690",
    "end": "2156750"
  },
  {
    "text": "so now what you can see on the left is actually the flow of the of the job so",
    "start": "2156750",
    "end": "2162960"
  },
  {
    "text": "first it's preparing it what is it doing now so it's compiling the script",
    "start": "2162960",
    "end": "2168540"
  },
  {
    "text": "checking theories in custom codes well is that custom code deployed on on the",
    "start": "2168540",
    "end": "2174090"
  },
  {
    "text": "data like analytics account you are referring to a certain folder do I know that datasource etc so once everything",
    "start": "2174090",
    "end": "2181260"
  },
  {
    "text": "is compiled it will queued the job for processing and then it will be run and",
    "start": "2181260",
    "end": "2186720"
  },
  {
    "text": "then it's finalized so by now you see it's finalized but it's only registering",
    "start": "2186720",
    "end": "2194790"
  },
  {
    "text": "the DLL so I don't have a job graph what you can also see over here is the",
    "start": "2194790",
    "end": "2201090"
  },
  {
    "text": "duration of the compilation of how long it queued basically not and then how",
    "start": "2201090",
    "end": "2206340"
  },
  {
    "text": "long it took to run it's only three seconds but also over here you can see again the parallelism which is the",
    "start": "2206340",
    "end": "2212700"
  },
  {
    "text": "analytics units and then the priority so the priority is how higher the amount",
    "start": "2212700",
    "end": "2218540"
  },
  {
    "text": "the less Heights and priority actually so priority one is the most important",
    "start": "2218540",
    "end": "2225630"
  },
  {
    "text": "one okay so now my custom code is there now I can run the aggregate script so I can",
    "start": "2225630",
    "end": "2233250"
  },
  {
    "text": "just select NDC Sydney I can submit it",
    "start": "2233250",
    "end": "2239330"
  },
  {
    "text": "so now it's compiling that script",
    "start": "2239750",
    "end": "2244490"
  },
  {
    "text": "actually let me go to the data explorer to see the results so now and come on",
    "start": "2250190",
    "end": "2260320"
  },
  {
    "text": "and the data Explorer we see that we have a reference data folder which",
    "start": "2260320",
    "end": "2266780"
  },
  {
    "text": "contains a foul we'll use during the the business logic let's say then we have",
    "start": "2266780",
    "end": "2272060"
  },
  {
    "text": "the catalog and the system folder which is the metadata for the service itself this is only because this is the default",
    "start": "2272060",
    "end": "2278990"
  },
  {
    "text": "data store and then once the job is done [Music]",
    "start": "2278990",
    "end": "2285209"
  },
  {
    "text": "see now it's running I use parallelism 5 which was not a good idea I should have",
    "start": "2285340",
    "end": "2292160"
  },
  {
    "text": "used more ok ok so now you see we have",
    "start": "2292160",
    "end": "2298810"
  },
  {
    "text": "328 stages and 328 vertices so like I",
    "start": "2298810",
    "end": "2304100"
  },
  {
    "text": "mentioned four dishes are smaller pieces of data sets but because in my custom",
    "start": "2304100",
    "end": "2310250"
  },
  {
    "text": "extractor I said I want to use a Tomic processing it just uses one for each",
    "start": "2310250",
    "end": "2316040"
  },
  {
    "text": "file so basically each stage as one file and now if I do expand all expand oh",
    "start": "2316040",
    "end": "2326660"
  },
  {
    "text": "yeah you can see this why because we have all of these files but if i zoom in",
    "start": "2326660",
    "end": "2334990"
  },
  {
    "text": "yeah you can see that there's just one aggregate shape to aggregate all these",
    "start": "2335650",
    "end": "2342530"
  },
  {
    "text": "files and then it's outputting it to my user CSV okay this will take a while",
    "start": "2342530",
    "end": "2353110"
  },
  {
    "text": "let's cancel it and provision more analytics units",
    "start": "2353110",
    "end": "2359860"
  },
  {
    "text": "yeah okay so it's canceled let's try again okay in the meantime",
    "start": "2365910",
    "end": "2380500"
  },
  {
    "text": "let's go through the next script so we don't need this one anymore no this one",
    "start": "2380500",
    "end": "2387190"
  },
  {
    "text": "not okay so after that we will create a small script - so what will we do we",
    "start": "2387190",
    "end": "2397990"
  },
  {
    "text": "will take all the users in the aggregated file check their profile there's a location in there and then we",
    "start": "2397990",
    "end": "2404740"
  },
  {
    "text": "will check doesn't match the name of the country so it needs to exactly match it",
    "start": "2404740",
    "end": "2411130"
  },
  {
    "text": "for the sake of the demo but in reality you would check does it match the country and sent three or if it has more",
    "start": "2411130",
    "end": "2418890"
  },
  {
    "text": "information in the field for example I live in Belgium it should also detect Belgium and link it about but for the",
    "start": "2418890",
    "end": "2426250"
  },
  {
    "text": "sake of the demo we won't do this but that means we need to have some reference data that contains all the",
    "start": "2426250",
    "end": "2431890"
  },
  {
    "text": "files and all the countries in the world and for that there's a data set based on",
    "start": "2431890",
    "end": "2439660"
  },
  {
    "text": "the ISO 3166 that contains all the",
    "start": "2439660",
    "end": "2445289"
  },
  {
    "text": "countries in the world where I also added a field friendly name why because",
    "start": "2445289",
    "end": "2452140"
  },
  {
    "text": "in that standard the official name for the USA and the UK are hold on a second",
    "start": "2452140",
    "end": "2458980"
  },
  {
    "text": "United States of America where I added United States and then for the UK United",
    "start": "2458980",
    "end": "2467259"
  },
  {
    "text": "Kingdom of Great Britain and Northern Ireland so there I just added a United",
    "start": "2467259",
    "end": "2472960"
  },
  {
    "text": "Kingdom as well and basically I should also add Ireland but okay",
    "start": "2472960",
    "end": "2478539"
  },
  {
    "text": "because I don't take a lot of people using that naming when they need to provide a location so we will use these",
    "start": "2478539",
    "end": "2485890"
  },
  {
    "text": "friendly names map it to the user location if we have a match we will add",
    "start": "2485890",
    "end": "2491710"
  },
  {
    "text": "it to that bucket if we can't resolve the country we will add it to the not found bucket",
    "start": "2491710",
    "end": "2497800"
  },
  {
    "text": "if they don't specify it we added to the not specified bucket okay now we can",
    "start": "2497800",
    "end": "2503680"
  },
  {
    "text": "create a certain of visualization and then we also do averages and certain things so for that we first need to",
    "start": "2503680",
    "end": "2512170"
  },
  {
    "text": "again extract all the aggregated users from our dataset so it's basically the",
    "start": "2512170",
    "end": "2518020"
  },
  {
    "text": "output of the previous job and then we also load all the countries from our",
    "start": "2518020",
    "end": "2525310"
  },
  {
    "text": "reference data and as you can see it's also on data like store in this folder",
    "start": "2525310",
    "end": "2531700"
  },
  {
    "text": "with his name etc and that's a CSV so then we have two variables containing",
    "start": "2531700",
    "end": "2537460"
  },
  {
    "text": "our datasets and we can perform some logic on top of it now first have a look",
    "start": "2537460",
    "end": "2544920"
  },
  {
    "text": "moving on so because it's case sensitive",
    "start": "2551910",
    "end": "2558690"
  },
  {
    "text": "too to match the fields I also created a",
    "start": "2558690",
    "end": "2564550"
  },
  {
    "text": "cleansing method let's say where I just add the locations and I just lowercase",
    "start": "2564550",
    "end": "2569800"
  },
  {
    "text": "everything so I can match the friendly name with the location in a lower case I",
    "start": "2569800",
    "end": "2576390"
  },
  {
    "text": "could also use that existing c-sharp assembly but each script also has a code",
    "start": "2576390",
    "end": "2583060"
  },
  {
    "text": "behind so if I go to the code-behind of my script you can see I have a class",
    "start": "2583060",
    "end": "2588850"
  },
  {
    "text": "with a small method in here which just takes a string if it's null or",
    "start": "2588850",
    "end": "2594070"
  },
  {
    "text": "whitespace I just remove it I just give it back because I'm not interested if it",
    "start": "2594070",
    "end": "2599110"
  },
  {
    "text": "isn't then I move all the the trailing and just remove all the spaces at the",
    "start": "2599110",
    "end": "2606970"
  },
  {
    "text": "beginning or the end and then I do just to a to logger so I'm sure that the data is the same before we match it I do this",
    "start": "2606970",
    "end": "2615700"
  },
  {
    "text": "for all the users and I do it only for the location because I only need it there and then for the countries I do it",
    "start": "2615700",
    "end": "2624130"
  },
  {
    "text": "for the name and the friendly name the region and the Cyprian well actually I just need a friendly name",
    "start": "2624130",
    "end": "2630890"
  },
  {
    "text": "whatever okay so my data set is as clean",
    "start": "2630890",
    "end": "2637380"
  },
  {
    "text": "note that I cleaned the data sets which I already extracted so I'm not cleaning",
    "start": "2637380",
    "end": "2643380"
  },
  {
    "text": "it on the persistent data I'm doing it in the let's call it in memory data okay",
    "start": "2643380",
    "end": "2649619"
  },
  {
    "text": "so I'm not adapting the original data and then I can just use sequel like",
    "start": "2649619",
    "end": "2655829"
  },
  {
    "text": "language with c-sharp well over here I'm using inline c-sharp if there's no region sorry if the region is null then",
    "start": "2655829",
    "end": "2663660"
  },
  {
    "text": "I add not applicable and otherwise I use the region I do account I use an average while",
    "start": "2663660",
    "end": "2670680"
  },
  {
    "text": "average is not the best fit but okay then I just do a join on both the users and the countries where the location and",
    "start": "2670680",
    "end": "2679020"
  },
  {
    "text": "the friendly name are the same and I drew by region sup region and name so",
    "start": "2679020",
    "end": "2684630"
  },
  {
    "text": "here I'm basically creating an overview of all the countries per region per",
    "start": "2684630",
    "end": "2692339"
  },
  {
    "text": "Cyprian to determine how many users live in Australia for example and what the",
    "start": "2692339",
    "end": "2697589"
  },
  {
    "text": "average reputation is okay and then the second one is where I divide them in the",
    "start": "2697589",
    "end": "2703319"
  },
  {
    "text": "buckets so this one will include all the users then this one so I don't filter",
    "start": "2703319",
    "end": "2711720"
  },
  {
    "text": "over here and then the ones that are not found I do a left join where I also",
    "start": "2711720",
    "end": "2716819"
  },
  {
    "text": "check if the friendly name is no and the location is different from an empty",
    "start": "2716819",
    "end": "2723299"
  },
  {
    "text": "string and to make sure that I don't match the not specified ones as well",
    "start": "2723299",
    "end": "2728309"
  },
  {
    "text": "which I have over here which I also Union by the way and here I also have an",
    "start": "2728309",
    "end": "2734490"
  },
  {
    "text": "inline c-sharp expression and then I Union with all the found users where",
    "start": "2734490",
    "end": "2741510"
  },
  {
    "text": "they are actually the same so no rocket science but also shows you",
    "start": "2741510",
    "end": "2746549"
  },
  {
    "text": "that I can just write a small script on top of the big data big data set and",
    "start": "2746549",
    "end": "2752400"
  },
  {
    "text": "then I just output it to two CS fees problem is I need is this guy to finish",
    "start": "2752400",
    "end": "2759260"
  },
  {
    "text": "okay so really there so then I just mmm",
    "start": "2759260",
    "end": "2766580"
  },
  {
    "text": "that was not smart if you just click Submit it uses the previous yes 20",
    "start": "2766580",
    "end": "2773640"
  },
  {
    "text": "should be fine so now it's running that one and over here so now this this one",
    "start": "2773640",
    "end": "2781050"
  },
  {
    "text": "is done but you can see that you can just I need to load the profile first",
    "start": "2781050",
    "end": "2789440"
  },
  {
    "text": "and then I can come and then I can just",
    "start": "2790850",
    "end": "2795920"
  },
  {
    "text": "replay the whole processing so now it's just yeah all of these boxes better than",
    "start": "2795920",
    "end": "2801090"
  },
  {
    "text": "the next up I'll show it as well where you can see okay this step was taking a lot of processing and then you can see",
    "start": "2801090",
    "end": "2807840"
  },
  {
    "text": "okay this is because we're aggregating after blah blah blah and then you can see where the bottlenecks are in your",
    "start": "2807840",
    "end": "2814620"
  },
  {
    "text": "script also I mentioned resources I can",
    "start": "2814620",
    "end": "2820200"
  },
  {
    "text": "see how many of the allocated analytics units that were used so we can see the",
    "start": "2820200",
    "end": "2826080"
  },
  {
    "text": "most used ones were 20 so actually I offered proficient 5 analytics units if",
    "start": "2826080",
    "end": "2831390"
  },
  {
    "text": "I schedule this kind of job in the future I can only use 20 because we can",
    "start": "2831390",
    "end": "2836700"
  },
  {
    "text": "save money with that which is actually important and then you have this modeler so let's say I had 10 units",
    "start": "2836700",
    "end": "2844110"
  },
  {
    "text": "it shows you a prediction it's not not certain that you will get this result",
    "start": "2844110",
    "end": "2849630"
  },
  {
    "text": "but that's what they think will be the outcome so this is actually very interesting if you need to model your",
    "start": "2849630",
    "end": "2856410"
  },
  {
    "text": "script to optimize with the cost ok so",
    "start": "2856410",
    "end": "2861930"
  },
  {
    "text": "let's see what this one is it's refreshing",
    "start": "2861930",
    "end": "2867710"
  },
  {
    "text": "ok we're already at half and you can see that it's already a more fancy graph",
    "start": "2867710",
    "end": "2875130"
  },
  {
    "text": "well I don't need to know how to model this I just need to write the script they do logic for me don't need to",
    "start": "2875130",
    "end": "2882300"
  },
  {
    "text": "profession any clusters just write the business logic and run it okay",
    "start": "2882300",
    "end": "2891500"
  },
  {
    "text": "okay it's finished and now you can see",
    "start": "2894900",
    "end": "2903040"
  },
  {
    "text": "that on the left side how many bytes it's Brett and how many were written and",
    "start": "2903040",
    "end": "2910600"
  },
  {
    "text": "also that we separated the one big aggregated user file across 37 vertices",
    "start": "2910600",
    "end": "2917440"
  },
  {
    "text": "okay and if I use this one I need to",
    "start": "2917440",
    "end": "2923830"
  },
  {
    "text": "load profile' first but instead of progress you also have these others - to",
    "start": "2923830",
    "end": "2930970"
  },
  {
    "text": "view the processing so if I use computation time for example you can now",
    "start": "2930970",
    "end": "2936550"
  },
  {
    "text": "see what the bottlenecks are so extracting really helpful if you need to",
    "start": "2936550",
    "end": "2954460"
  },
  {
    "text": "troubleshoot certain things and then if",
    "start": "2954460",
    "end": "2959710"
  },
  {
    "text": "we go back to the resource usage you see I provision 20 well actually I",
    "start": "2959710",
    "end": "2966070"
  },
  {
    "text": "could have used 10 or less see 12 would have been perfect but that means if we",
    "start": "2966070",
    "end": "2973600"
  },
  {
    "text": "go back to our datastore we have the aggregated data folder where we have",
    "start": "2973600",
    "end": "2980590"
  },
  {
    "text": "this aggregated users so it's only three 0.66 gigabytes but if you if I would",
    "start": "2980590",
    "end": "2989020"
  },
  {
    "text": "have run on all the data sets or used other files it would be bigger because",
    "start": "2989020",
    "end": "2994510"
  },
  {
    "text": "the users is actually pretty small but here I can also have a small overview so",
    "start": "2994510",
    "end": "2999820"
  },
  {
    "text": "on the left was the source so the name of the of the website and you can see",
    "start": "2999820",
    "end": "3005160"
  },
  {
    "text": "that they have these test users across the same all the websites which all live",
    "start": "3005160",
    "end": "3011640"
  },
  {
    "text": "on the on the server farm so you can get it I won't download the file but that's how",
    "start": "3011640",
    "end": "3019380"
  },
  {
    "text": "you can also check the result of your data set and then",
    "start": "3019380",
    "end": "3024780"
  },
  {
    "text": "output of the reports is now over here and I can just use power bi so I'll just",
    "start": "3024780",
    "end": "3032790"
  },
  {
    "text": "quickly first I need you where I and",
    "start": "3032790",
    "end": "3038400"
  },
  {
    "text": "then I can just go over here select data like store I actually need to speed up a",
    "start": "3038400",
    "end": "3045390"
  },
  {
    "text": "bit and then you need to authenticate",
    "start": "3045390",
    "end": "3050820"
  },
  {
    "text": "why because it's again using Azure ad for the security to know if I have",
    "start": "3050820",
    "end": "3056340"
  },
  {
    "text": "permissions to view this",
    "start": "3056340",
    "end": "3059570"
  },
  {
    "text": "okay and then I can connect to the data like store account so now it gives me",
    "start": "3080050",
    "end": "3088940"
  },
  {
    "text": "this so basically it's just a folder structure which is not the data set I want I can edit it and it goes to this",
    "start": "3088940",
    "end": "3098480"
  },
  {
    "text": "modeler come on",
    "start": "3098480",
    "end": "3108670"
  },
  {
    "text": "Australian Internet okay I'll let it load there Wow I'll proceed with the",
    "start": "3110859",
    "end": "3118040"
  },
  {
    "text": "slides first okay so operations which is important",
    "start": "3118040",
    "end": "3123400"
  },
  {
    "text": "data like stored like I mentioned we have the graphs for storage read/write",
    "start": "3123400",
    "end": "3128930"
  },
  {
    "text": "ingress/egress but we don't have metrics and we don't have alerts which can be a problem and",
    "start": "3128930",
    "end": "3137530"
  },
  {
    "text": "then we also have all the time request logs which can be interesting but",
    "start": "3137530",
    "end": "3143890"
  },
  {
    "text": "another crucial for certain scenarios and then we have data like analytics where we also have the graphs where you",
    "start": "3143890",
    "end": "3151640"
  },
  {
    "text": "can see the amount of failed jobs succeeded jobs Council jobs etc and the",
    "start": "3151640",
    "end": "3157010"
  },
  {
    "text": "evolution of the used analytics unit time and we also have metrics of those",
    "start": "3157010",
    "end": "3163180"
  },
  {
    "text": "but we don't have a lurch on it which is not perfect in my opinion because I want",
    "start": "3163180",
    "end": "3170060"
  },
  {
    "text": "to have an alert if a job fails for example if I schedule certain jobs on the daily basis I don't want to log in",
    "start": "3170060",
    "end": "3177440"
  },
  {
    "text": "and check it every morning so tooling I showed Visual Studio where",
    "start": "3177440",
    "end": "3182900"
  },
  {
    "text": "you have also a store explorer where you can just connect to your store and",
    "start": "3182900",
    "end": "3188450"
  },
  {
    "text": "browse it over there so you don't need to log in the portal and I also showed",
    "start": "3188450",
    "end": "3194330"
  },
  {
    "text": "you the job visualizer where you can just troubleshoot your jobs and to a",
    "start": "3194330",
    "end": "3200420"
  },
  {
    "text": "certain extent you can do the same in the portal but the visual studio tooling as provides a more richer capabilities",
    "start": "3200420",
    "end": "3208190"
  },
  {
    "text": "if you like visual studio codes you can also use it to run the unit tests on the",
    "start": "3208190",
    "end": "3214049"
  },
  {
    "text": "extensibility for example which I didn't show you or just use the local local",
    "start": "3214049",
    "end": "3219210"
  },
  {
    "text": "execution because yeah if you want to save money you'd want to test your",
    "start": "3219210",
    "end": "3225119"
  },
  {
    "text": "scripts first locally which you can actually also do in plain Visual Studio",
    "start": "3225119",
    "end": "3230839"
  },
  {
    "text": "an integration with other services the most important one is data factory where",
    "start": "3230839",
    "end": "3236009"
  },
  {
    "text": "you can create these pipelines and schedule them as well so one scenario would be that you copy",
    "start": "3236009",
    "end": "3243569"
  },
  {
    "text": "data over into data Lake and then after that it triggers a script to run on top",
    "start": "3243569",
    "end": "3249359"
  },
  {
    "text": "of data Lake store does something with the output etc you can also use Azure",
    "start": "3249359",
    "end": "3257099"
  },
  {
    "text": "data catalog to annotate your data sets to kind of document all the data sets",
    "start": "3257099",
    "end": "3263339"
  },
  {
    "text": "you have so that you are certain that it's not becoming a data swamp and then",
    "start": "3263339",
    "end": "3269640"
  },
  {
    "text": "let's see how power bi is doing not good okay so I want to have my report so I go",
    "start": "3269640",
    "end": "3278969"
  },
  {
    "text": "to this table and let's use the location",
    "start": "3278969",
    "end": "3288049"
  },
  {
    "text": "and then I can just use all this information so that's not supposed to",
    "start": "3288049",
    "end": "3298380"
  },
  {
    "text": "happen okay I am NOT interested for all the for all the results because we know",
    "start": "3298380",
    "end": "3304529"
  },
  {
    "text": "that but let's use this data set normally you would give them nice names but for the sake of the time I'll skip",
    "start": "3304529",
    "end": "3311249"
  },
  {
    "text": "that I'll also include an example and in",
    "start": "3311249",
    "end": "3321839"
  },
  {
    "text": "the github repo so you can see how you can just use them but let's say I want",
    "start": "3321839",
    "end": "3327180"
  },
  {
    "text": "to use a map which is over here and then I know by heart that this is",
    "start": "3327180",
    "end": "3332729"
  },
  {
    "text": "the location I should have used them the names okay",
    "start": "3332729",
    "end": "3341328"
  },
  {
    "text": "they should be the location and then no I'm using the wrong dataset no back yeah",
    "start": "3344070",
    "end": "3369580"
  },
  {
    "text": "this is the one I want skip the names again because I like living on the edge",
    "start": "3369580",
    "end": "3378840"
  },
  {
    "text": "okay so location here we go here we go",
    "start": "3378840",
    "end": "3387870"
  },
  {
    "text": "and then I know this is the continent",
    "start": "3387870",
    "end": "3392920"
  },
  {
    "text": "and then this is the subcontinent",
    "start": "3392920",
    "end": "3396839"
  },
  {
    "text": "actually this is the amount of total users so I can adapt the size or use the",
    "start": "3398940",
    "end": "3408520"
  },
  {
    "text": "reputation and then I can add some tooltips okay so if I go over over the",
    "start": "3408520",
    "end": "3415390"
  },
  {
    "text": "u.s. really bad names where maximum reputation is 612 and the maximum views",
    "start": "3415390",
    "end": "3422710"
  },
  {
    "text": "is 35 you can see that in in India",
    "start": "3422710",
    "end": "3428200"
  },
  {
    "text": "there's a lot more people using Stack Exchange for example okay I'll just",
    "start": "3428200",
    "end": "3436960"
  },
  {
    "text": "leave the power bi part now but if you're interested I can show you after the session if you want to know more",
    "start": "3436960",
    "end": "3444220"
  },
  {
    "text": "about Azure data Lake they are preparing best practices for now it's not public",
    "start": "3444220",
    "end": "3451720"
  },
  {
    "text": "yet but if you want contact me then you can get a preview of this this is a book",
    "start": "3451720",
    "end": "3457030"
  },
  {
    "text": "I can really recommend by showing at each other about all the things about",
    "start": "3457030",
    "end": "3462750"
  },
  {
    "text": "data analytics and asset he gives you a whole overview of the data stores you can use the kinds of analytics you can",
    "start": "3462750",
    "end": "3470710"
  },
  {
    "text": "use how you can visualize it prepare it etc we nice one then they also have a Microsoft",
    "start": "3470710",
    "end": "3476760"
  },
  {
    "text": "Virtual Academy introducing Azure data Lake they have a github repo with all",
    "start": "3476760",
    "end": "3482400"
  },
  {
    "text": "the release notes and the announcements and the you sequel documentation okay",
    "start": "3482400",
    "end": "3489000"
  },
  {
    "text": "and then a small summary yeah big data",
    "start": "3489000",
    "end": "3494250"
  },
  {
    "text": "is not a hype anymore but that's obvious because data is the new currency if your",
    "start": "3494250",
    "end": "3500670"
  },
  {
    "text": "data like store you can use to analyze today and tomorrow because it's data",
    "start": "3500670",
    "end": "3506790"
  },
  {
    "text": "Lake not the data warehouse so you're not throwing away your valuable data and",
    "start": "3506790",
    "end": "3512599"
  },
  {
    "text": "beware of the data swamps you don't want to have that and then data Lake",
    "start": "3512599",
    "end": "3517650"
  },
  {
    "text": "analytics you don't have the cluster management which you also don't have for store but you can just write your",
    "start": "3517650",
    "end": "3524730"
  },
  {
    "text": "scripts and give it so in a certain sense it's also server less to include",
    "start": "3524730",
    "end": "3529950"
  },
  {
    "text": "the buzzwords I can reuse my existing skills to write these scripts I don't",
    "start": "3529950",
    "end": "3535500"
  },
  {
    "text": "need to learn high4 whatsoever don't need to learn Java because I don't want to do that and I just pay for what",
    "start": "3535500",
    "end": "3542940"
  },
  {
    "text": "I use so I don't need to pay for all the VMS of the cluster as well I just pay",
    "start": "3542940",
    "end": "3548490"
  },
  {
    "text": "for the computes so my advice would be if you want to use big data and a",
    "start": "3548490",
    "end": "3553980"
  },
  {
    "text": "and you're a c-sharp or sequel developer use data like store and analytics thank",
    "start": "3553980",
    "end": "3559500"
  },
  {
    "text": "you [Applause]",
    "start": "3559500",
    "end": "3565429"
  }
]