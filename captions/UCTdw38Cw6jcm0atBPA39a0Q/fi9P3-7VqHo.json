[
  {
    "text": "so good morning everybody um my name is Aaron standard and today we're going to be talking about uh",
    "start": "1800",
    "end": "7859"
  },
  {
    "text": "learning.net systems programming the hard way and a little bit about uh sort of my discoveries and experiences along",
    "start": "7859",
    "end": "14400"
  },
  {
    "text": "the way welcome everyone who's coming in there should be plenty of seats around here still",
    "start": "14400",
    "end": "19500"
  },
  {
    "text": "uh it's a little bit about me I've been a net developer since 2005. I started",
    "start": "19500",
    "end": "24539"
  },
  {
    "text": "learning.net I think it was done at 2.0 had just come out I was a college intern working for a company called the fair",
    "start": "24539",
    "end": "31980"
  },
  {
    "text": "Isaac Corporation they use machine learning to compute everybody's credit worthiness in the United States",
    "start": "31980",
    "end": "38940"
  },
  {
    "text": "um I also have built a large-scale software as a service application on top of.net it was",
    "start": "38940",
    "end": "46200"
  },
  {
    "text": "basically a multi-tenant real-time analytics and marketing automation system for Developers for building apps",
    "start": "46200",
    "end": "51660"
  },
  {
    "text": "for the Windows store and as part of working on that company I",
    "start": "51660",
    "end": "57960"
  },
  {
    "text": "co-created akka.net and have been the maintainer of it since about 2013 so this November academic will turn 10 and",
    "start": "57960",
    "end": "65100"
  },
  {
    "text": "I've been working on it continuously more or less and what I'm responsible for in the well",
    "start": "65100",
    "end": "71460"
  },
  {
    "text": "in terms of what octodon that does if you're not familiar with it it's an implementation of the erling style actor",
    "start": "71460",
    "end": "76500"
  },
  {
    "text": "model but on.net so it's sort of a canonical implementation of the actor model",
    "start": "76500",
    "end": "82020"
  },
  {
    "text": "um this is designed for handling highly concurrent and distributed workloads so we deal with all sorts of system issues",
    "start": "82020",
    "end": "88080"
  },
  {
    "text": "such as working with with threads working with network i o serialization uh context switching and these are all",
    "start": "88080",
    "end": "95579"
  },
  {
    "text": "Concepts we're going to be touching on in the presentation today uh the primary thing that akka.net is used for is",
    "start": "95579",
    "end": "102479"
  },
  {
    "text": "building Mission critical real-time applications so I'd give you a real world example of that we have a lot of",
    "start": "102479",
    "end": "109200"
  },
  {
    "text": "customers and spaces like real-time Fleet and Logistics tracking for cargo networks trains ships that type of thing",
    "start": "109200",
    "end": "116399"
  },
  {
    "text": "we have a lot of customers that use awka.net for running real-time Financial systems so being able to do things like",
    "start": "116399",
    "end": "122640"
  },
  {
    "text": "automatically trade currency or be able to manage positions in a big equities portfolio and we've been used for some",
    "start": "122640",
    "end": "128819"
  },
  {
    "text": "more fun stuff like multiplayer video games so given the space that we're in performance is an essential feature of",
    "start": "128819",
    "end": "135840"
  },
  {
    "text": "our work on the aqua.net project and this is sort of what I've learned while working on that feature",
    "start": "135840",
    "end": "141900"
  },
  {
    "text": "the first place will kind of start talking about systems programming is garbage collection in.net uh the garbage",
    "start": "141900",
    "end": "148319"
  },
  {
    "text": "collector is going to be one of the biggest sources of potential CPU consumption inside your system",
    "start": "148319",
    "end": "153480"
  },
  {
    "text": "potentially if you're not very efficient about how you allocate objects but on top of that the garbage collector in.net",
    "start": "153480",
    "end": "160739"
  },
  {
    "text": "is highly configurable you can go ahead and reduce the penalty that garbage collection imposes on your application",
    "start": "160739",
    "end": "166379"
  },
  {
    "text": "by adjusting a couple of settings and I'm going to show you one very powerful setting in just a moment or by maybe",
    "start": "166379",
    "end": "172800"
  },
  {
    "text": "changing the way you manage the lifespan of some of your objects so the way.net garbage collection",
    "start": "172800",
    "end": "179400"
  },
  {
    "text": "typically works is let's say for instance we have this set of memory on our left initially right where we go",
    "start": "179400",
    "end": "185580"
  },
  {
    "text": "ahead and have a total let's say seven objects in memory and then the application forms Roots",
    "start": "185580",
    "end": "191099"
  },
  {
    "text": "those objects these are active references for other objects that are not yet marked for deletion so as long",
    "start": "191099",
    "end": "196739"
  },
  {
    "text": "as an object is rooted it will be retained in memory by the garbage collector an object that is not rooted",
    "start": "196739",
    "end": "202440"
  },
  {
    "text": "will be collected and its memory will be freed and released back to the application that's what the garbage",
    "start": "202440",
    "end": "207900"
  },
  {
    "text": "collection process does this tries to identify those unrooted objects and free up that memory",
    "start": "207900",
    "end": "213780"
  },
  {
    "text": "so after the garbage collector runs there's one more important thing that you'll kind of see on the diagram here",
    "start": "213780",
    "end": "220140"
  },
  {
    "text": "which is that those objects that are arrayed in memory all get compressed together again in memory as well this",
    "start": "220140",
    "end": "226980"
  },
  {
    "text": "sort of compaction of free memory is designed to help accelerate the allocation of new objects in the future",
    "start": "226980",
    "end": "233640"
  },
  {
    "text": "so the garbage collector in addition to freeing objects that are no longer rooted is also responsible for trying to",
    "start": "233640",
    "end": "241019"
  },
  {
    "text": "compact memory together that compaction process is actually really essential for helping make your applications fast",
    "start": "241019",
    "end": "247260"
  },
  {
    "text": "imagine if you had let's say a collection of different objects that you were working with inside some key",
    "start": "247260",
    "end": "253560"
  },
  {
    "text": "business context well if they're all available sequentially together in memory that makes it more likely that",
    "start": "253560",
    "end": "258959"
  },
  {
    "text": "you can take advantage of caching at the hardware layer inside your computer it also makes this access much quicker",
    "start": "258959",
    "end": "265380"
  },
  {
    "text": "because it's all sort of sequential increments of a pointer that way whereas if that memory was highly fragmented you",
    "start": "265380",
    "end": "271620"
  },
  {
    "text": "wouldn't be able to take advantage of that and you'd see a noticeable performance hit doing it so the garbage collector does all this for you more or",
    "start": "271620",
    "end": "278100"
  },
  {
    "text": "less we have four generations of garbage collection in.net essentially first",
    "start": "278100",
    "end": "285720"
  },
  {
    "text": "as what we call gen zero garbage collection the general the more the higher up the the generation is the more",
    "start": "285720",
    "end": "292440"
  },
  {
    "text": "expensive it is to run so gen zero garbage collection is the cheapest and these are for objects that are",
    "start": "292440",
    "end": "297479"
  },
  {
    "text": "essentially ephemeral so for instance if you allocate a string inside the scope",
    "start": "297479",
    "end": "302940"
  },
  {
    "text": "of a function do some work with it uh maybe you write it out to the console and then that function exits that's",
    "start": "302940",
    "end": "309060"
  },
  {
    "text": "going to be gen zero collected more than likely and we're going to see a little example of that in a second so that's",
    "start": "309060",
    "end": "314460"
  },
  {
    "text": "the sort of the cheapest form of garbage collection and the reason why it's cheap is that we go and allocate that object",
    "start": "314460",
    "end": "320100"
  },
  {
    "text": "at the top of the Heap and then immediately take it off the Heap again without necessarily needing to do a",
    "start": "320100",
    "end": "326580"
  },
  {
    "text": "complex rooting sort of calculation inside the GC system so it's really easy to get",
    "start": "326580",
    "end": "332280"
  },
  {
    "text": "stuff off the top of the Heap and uh back again doesn't also doesn't require any compaction when we do that",
    "start": "332280",
    "end": "339380"
  },
  {
    "text": "one garbage collection is slightly more expensive these are for objects that survive at least one uh gen zero garbage",
    "start": "339380",
    "end": "346380"
  },
  {
    "text": "collection attempts they get promoted into a higher generation so these might be objects that maybe get assigned as a",
    "start": "346380",
    "end": "353039"
  },
  {
    "text": "property to a class and that class gets passed around across multiple methods and over you know a handful garbage",
    "start": "353039",
    "end": "360000"
  },
  {
    "text": "collection runs uh that object is still in use still rooted so we don't we don't collect it right away but generally it's",
    "start": "360000",
    "end": "366539"
  },
  {
    "text": "going to get collected after a fairly short period of time in gen 1. so gen 1 is only incrementally more expensive",
    "start": "366539",
    "end": "372060"
  },
  {
    "text": "than gen zero Gen 2 is where things get really interesting uh these are the long-lived objects inside your",
    "start": "372060",
    "end": "378539"
  },
  {
    "text": "application that have survived multiple gen 1 garbage collection attempts these tend to be things like",
    "start": "378539",
    "end": "385380"
  },
  {
    "text": "for instance if you have a static property inside your application that's always going to live in Gen 2 garbage",
    "start": "385380",
    "end": "391740"
  },
  {
    "text": "collection and in fact we'll probably never be collected or another example this might be if you have some objects",
    "start": "391740",
    "end": "398039"
  },
  {
    "text": "that are extremely long-lived so a good example with you know akka.net and the actorspace if you have an actor that's",
    "start": "398039",
    "end": "404039"
  },
  {
    "text": "running in the background and they can run for months or days at a time that actor in all of its state will all be in",
    "start": "404039",
    "end": "410520"
  },
  {
    "text": "the Gen 2 garbage collection category and then finally we have sort of our fourth generation here which is kind of",
    "start": "410520",
    "end": "416400"
  },
  {
    "text": "a special case this is the large object Heap I'm blinking on actually no I'm not blinking I have a",
    "start": "416400",
    "end": "422460"
  },
  {
    "text": "note that tells me here um in order to be on the large object Heap it has to be a object that is",
    "start": "422460",
    "end": "427680"
  },
  {
    "text": "bigger than 85k is how big it needs to be so if you have a really large buffer maybe you've done something like load uh",
    "start": "427680",
    "end": "434520"
  },
  {
    "text": "binary file from disk or maybe you have a super big object graph that goes in",
    "start": "434520",
    "end": "439680"
  },
  {
    "text": "the large object Heap and that gets created a little bit differently because the cost of memory compaction is a lot",
    "start": "439680",
    "end": "445139"
  },
  {
    "text": "higher for doing that you're compacting a lot more stuff all at once when that occurs",
    "start": "445139",
    "end": "450960"
  },
  {
    "text": "all right there we go okay let me turn my mouse on all right",
    "start": "450960",
    "end": "457740"
  },
  {
    "text": "so the.net memory model let's take a look at this little piece of code right here so I have a this is inside a class",
    "start": "457740",
    "end": "464759"
  },
  {
    "text": "and I have a private read-only field that accesses this random.shared property a random.shared is a static",
    "start": "464759",
    "end": "471780"
  },
  {
    "text": "type it's built into I think this is.net6 so that is going to be a Gen 2",
    "start": "471780",
    "end": "476819"
  },
  {
    "text": "garbage collected object inside this method we are going to use",
    "start": "476819",
    "end": "481860"
  },
  {
    "text": "that random to generate two integers integers are value types so they belong",
    "start": "481860",
    "end": "487080"
  },
  {
    "text": "on the stack they're not even going to get garbage collected they're just simply going to be released as this",
    "start": "487080",
    "end": "492240"
  },
  {
    "text": "function exits the stack once it completes but down below if I go ahead and let's say add up that",
    "start": "492240",
    "end": "498599"
  },
  {
    "text": "integer so I you know sum those two integers together and I convert that result to a string and write that out to",
    "start": "498599",
    "end": "504660"
  },
  {
    "text": "the console here's what's going to happen I have my stack which basically consists",
    "start": "504660",
    "end": "509759"
  },
  {
    "text": "of everything that's happening inside this function while it's executing and some of these are going to be local",
    "start": "509759",
    "end": "515219"
  },
  {
    "text": "instructions that are all local to the stack again that's not going to touch the Heap which is what dotnet uses for",
    "start": "515219",
    "end": "521880"
  },
  {
    "text": "allocating you know reference types like classes on the Heap I'm going to go ahead and have a reference to the method",
    "start": "521880",
    "end": "528600"
  },
  {
    "text": "itself I'm going to have a reference that static field and then further down on the Heap I'm going to have the string",
    "start": "528600",
    "end": "534720"
  },
  {
    "text": "that I allocated while I executed this method once what's going to end up happening is that",
    "start": "534720",
    "end": "540180"
  },
  {
    "text": "this string is going to get garbage collected via gen zero that's uh random.shared will be in gen 2. all",
    "start": "540180",
    "end": "546720"
  },
  {
    "text": "those integers are on the stack they're value types so we don't garbage collect them at all they're just automatically freed and this method belongs to a class",
    "start": "546720",
    "end": "554220"
  },
  {
    "text": "that could either be gen 0 gen 1 or Gen 2 depending on how long it lives",
    "start": "554220",
    "end": "559980"
  },
  {
    "text": "so for generally speaking for garbage collection purposes from a performance standpoint",
    "start": "559980",
    "end": "565700"
  },
  {
    "text": "we know that basic garbage collection costs increases with the size of the",
    "start": "565700",
    "end": "571440"
  },
  {
    "text": "generation a Gen 2 garbage collection is going to involve a lot more compaction and a lot more memory basically reducing",
    "start": "571440",
    "end": "577560"
  },
  {
    "text": "a lot more memory fragmentation from a lot more places and the garbage collector will introduce what's called a",
    "start": "577560",
    "end": "582720"
  },
  {
    "text": "garbage collection pause potentially inside possibly your foreground thread or depending on how you configure IT",
    "start": "582720",
    "end": "589080"
  },
  {
    "text": "background threads we're going to talk about that configuration in just a second so the ability to not have as",
    "start": "589080",
    "end": "596100"
  },
  {
    "text": "many of those expensive Gen 2 garbage collection Pro attempts will actually help improve the overall throughput and",
    "start": "596100",
    "end": "602580"
  },
  {
    "text": "efficiency of our application so if we can we want to try to keep our allocations limited to either the stack",
    "start": "602580",
    "end": "608940"
  },
  {
    "text": "so value types or we want to try to stay inside gen 0 gen 1. that's going to",
    "start": "608940",
    "end": "614160"
  },
  {
    "text": "ultimately results in the cheapest garbage collection overall and is going to help keep our memory less fragmented",
    "start": "614160",
    "end": "620880"
  },
  {
    "text": "again like I mentioned earlier in gen zero for allocating something at the top of the Heap and we'll talk about how the",
    "start": "620880",
    "end": "626220"
  },
  {
    "text": "heaps organized in a minute if we're allocating stuff at the top of the Heap because that's just you know the code that's executing right here in",
    "start": "626220",
    "end": "632760"
  },
  {
    "text": "this moment everything lower on the Heap is all the older Gen 2 gen 1 stuff that survive multiple garbage collection",
    "start": "632760",
    "end": "638459"
  },
  {
    "text": "attempts we can allocate that string on the Heap and then pop it right off when that function exits and that's very very",
    "start": "638459",
    "end": "644399"
  },
  {
    "text": "inexpensive to do doesn't involve moving other objects around in memory so that's one of the reasons why it's so",
    "start": "644399",
    "end": "650220"
  },
  {
    "text": "cheap now if you have Gen 2 objects the right approach for maybe trying to help manage",
    "start": "650220",
    "end": "656700"
  },
  {
    "text": "performance there is keep gensu objects available in memory forever no garbage",
    "start": "656700",
    "end": "662459"
  },
  {
    "text": "collection for those objects will occur if they're still rooted this is where and we're going to get into some",
    "start": "662459",
    "end": "667740"
  },
  {
    "text": "examples of this Concepts like memory pooling and object pooling kind of enter the picture where maybe rather than",
    "start": "667740",
    "end": "674040"
  },
  {
    "text": "having a bunch of new stream Builders allocated continuously behind the scenes maybe it's better to have a shared pool",
    "start": "674040",
    "end": "679980"
  },
  {
    "text": "of a thousand of them that get rented periodically throughout the course your application running that might actually",
    "start": "679980",
    "end": "686220"
  },
  {
    "text": "result in a lot less memory fragmentation and a lot less garbage collection over the course of your",
    "start": "686220",
    "end": "691740"
  },
  {
    "text": "application's lifetime so yeah this is where we get into things like object pools",
    "start": "691740",
    "end": "697500"
  },
  {
    "text": "um so I think Daniel and his talk yesterday mentioned um the array pool for being able to go",
    "start": "697500",
    "end": "703320"
  },
  {
    "text": "ahead and get uh essentially chunks of memory you could use for working through uh buffers and that sort of thing",
    "start": "703320",
    "end": "709320"
  },
  {
    "text": "Microsoft also supports arbitrary pools of objects as well where rather than it just being bytes it actually might be a",
    "start": "709320",
    "end": "716820"
  },
  {
    "text": "functional object you can go ahead and reuse over and over again and the best candidates for these things are reusable",
    "start": "716820",
    "end": "722820"
  },
  {
    "text": "types such as string Builder is the example we're going to take a look at I think byte arrays also work as well but",
    "start": "722820",
    "end": "729300"
  },
  {
    "text": "there's more specialized uh basically pools for that like the memory pool type and you might also be able to",
    "start": "729300",
    "end": "736800"
  },
  {
    "text": "um have a couple of other reusable types for doing things like serialization potentially but string build is probably",
    "start": "736800",
    "end": "741959"
  },
  {
    "text": "a really good candidate because it's an object that can be inherently sort of expensive to spin up and create but you",
    "start": "741959",
    "end": "747959"
  },
  {
    "text": "can also clear all of its state when you return it to the pool that's what makes it reusable",
    "start": "747959",
    "end": "753420"
  },
  {
    "text": "so to give you an example this is some code from the aqua.net project here where we are doing a Json",
    "start": "753420",
    "end": "760380"
  },
  {
    "text": "serialization inside akka.net this is like our default serializer now one of",
    "start": "760380",
    "end": "766800"
  },
  {
    "text": "the things that we decided to do is rather than basically allocating a brand new string Builder each and every time",
    "start": "766800",
    "end": "773399"
  },
  {
    "text": "we were going to allocate up front when the process first began a fixed number of them I think 1024 string Builders is",
    "start": "773399",
    "end": "781320"
  },
  {
    "text": "what we decided by default so so we're going to go ahead and get a reference to a string Builder pool and just grab one",
    "start": "781320",
    "end": "789060"
  },
  {
    "text": "copy of a string Builder that we're going to use right here inside this newtonsoft.json operation",
    "start": "789060",
    "end": "795120"
  },
  {
    "text": "so we go ahead and pass in that string Builder into the Json text writer down",
    "start": "795120",
    "end": "800220"
  },
  {
    "text": "below we call serialize and then we convert this to a byte array and then at",
    "start": "800220",
    "end": "806100"
  },
  {
    "text": "the very end in our little finally block we return that string Builder we just used back to the pool all over again",
    "start": "806100",
    "end": "813480"
  },
  {
    "text": "well to give you a sense of what the performance impact of this looked like I have here on screen you know some",
    "start": "813480",
    "end": "819600"
  },
  {
    "text": "benchmark.net results from when we had pooling no pooling and then multi-threaded versions of both of these",
    "start": "819600",
    "end": "825540"
  },
  {
    "text": "benchmarks this is a theme you're going to see show up over and over again is that our Benchmark numbers start to",
    "start": "825540",
    "end": "831360"
  },
  {
    "text": "change when we start making them concurrent that's something that we're going to focus on as well",
    "start": "831360",
    "end": "836700"
  },
  {
    "text": "so here I can see that our pooled implementation used about",
    "start": "836700",
    "end": "841820"
  },
  {
    "text": "49.7 megabytes whereas our unpooled implementation used about 71 megabytes",
    "start": "841820",
    "end": "847019"
  },
  {
    "text": "that's a 30 memory savings on top of that we completely eliminated gen 1",
    "start": "847019",
    "end": "852480"
  },
  {
    "text": "garbage collection uh period by using the pooling system instead so that helped quite a bit and in terms of our",
    "start": "852480",
    "end": "859440"
  },
  {
    "text": "Peak latency here we went from I don't remember how many objects we were serializing I think it must have been a",
    "start": "859440",
    "end": "865260"
  },
  {
    "text": "hundred thousand or so but we basically went from doing it in about 44 milliseconds to 42 in the single",
    "start": "865260",
    "end": "871139"
  },
  {
    "text": "threaded scenario but in the multi-threaded scenario if I go down here we actually have a 28",
    "start": "871139",
    "end": "877500"
  },
  {
    "text": "throughput Improvement akka.net is extremely multi-threaded in terms of its inherent nature so that was a really",
    "start": "877500",
    "end": "883620"
  },
  {
    "text": "nice win for us the pooling ultimately helped akka.net improve its throughput for its",
    "start": "883620",
    "end": "889620"
  },
  {
    "text": "sort of normal standard use Case by about 28 percent now this is where we start getting into",
    "start": "889620",
    "end": "895500"
  },
  {
    "text": "things like garbage collection settings uh the way you configure your garbage collector can actually have a big impact",
    "start": "895500",
    "end": "901260"
  },
  {
    "text": "on how these changes will affect your application um so there's basically this sort of",
    "start": "901260",
    "end": "906720"
  },
  {
    "text": "little Matrix here of the three different fundamental modes of garbage collection uh there's background garbage",
    "start": "906720",
    "end": "913199"
  },
  {
    "text": "collection which has been enabled by default for quite a long time in.net now and what this basically does is it",
    "start": "913199",
    "end": "919740"
  },
  {
    "text": "allows a background thread to be responsible for doing most Gen 2 garbage collection that way you don't have a big",
    "start": "919740",
    "end": "926040"
  },
  {
    "text": "GC pause that will affect your app so good example if you were building a Maui application or a WPF app prior to having",
    "start": "926040",
    "end": "934560"
  },
  {
    "text": "a background GC enabled you might get if your application was allocating quite a large number of objects a visible pause",
    "start": "934560",
    "end": "941100"
  },
  {
    "text": "or stutter on screen while the GC operation went through that's not a good user experience so beginning I think",
    "start": "941100",
    "end": "947760"
  },
  {
    "text": "around.net framework when they enable this this must have been like.net framework 4.5 or 4.6 so quite a while",
    "start": "947760",
    "end": "954540"
  },
  {
    "text": "ago they implemented this to kind of move all that work on the background thread however most audit applications by",
    "start": "954540",
    "end": "960720"
  },
  {
    "text": "default run with background GC enabled and with workstation GC enabled where",
    "start": "960720",
    "end": "966060"
  },
  {
    "text": "what occurs with workstation garbage collection is that gen 0 and gen 1 garbage collection typically happens on",
    "start": "966060",
    "end": "972480"
  },
  {
    "text": "your foreground thread meaning the application threads that your app is using to actually do work this creates a",
    "start": "972480",
    "end": "979260"
  },
  {
    "text": "garbage collection pause that while it tends to be a lot smaller because it's gen zero and gen 1 garbage collection",
    "start": "979260",
    "end": "984779"
  },
  {
    "text": "which we know is relatively cheap it still does cause a pause inside your system so this is kind of like a",
    "start": "984779",
    "end": "990899"
  },
  {
    "text": "blocking garbage collection implementation and this is the default that most applications use and it is",
    "start": "990899",
    "end": "997199"
  },
  {
    "text": "also if you don't specify the default all brand new data applications still used today",
    "start": "997199",
    "end": "1003380"
  },
  {
    "text": "this is where we take a look at server-side garbage collection now this is going to be there's going to be two",
    "start": "1003380",
    "end": "1009019"
  },
  {
    "text": "pieces of advice during this presentation that'll give you the most bang for your buck you can implement it in five minutes and it might make a huge",
    "start": "1009019",
    "end": "1015980"
  },
  {
    "text": "performance difference for your app this is the first piece of advice like that is enabling server-side garbage",
    "start": "1015980",
    "end": "1021500"
  },
  {
    "text": "collection what server-side garbage collection does is kind of by definition ideal for let's",
    "start": "1021500",
    "end": "1027020"
  },
  {
    "text": "say asp.net or grpc or signalr applications what it essentially does is it fragments",
    "start": "1027020",
    "end": "1033798"
  },
  {
    "text": "the Heap on a per core basis so rather than having one big Heap for your process that's managed by a single Gen 2",
    "start": "1033799",
    "end": "1041120"
  },
  {
    "text": "garbage collection thread and the background GC system now you have a heap per core inside your your application so",
    "start": "1041120",
    "end": "1048140"
  },
  {
    "text": "there's if you have a 16 core machine you're going to have 16 little heaps that are all managed by the CLR",
    "start": "1048140",
    "end": "1054200"
  },
  {
    "text": "what server-side GC will do is it will essentially run a dedicated thread per",
    "start": "1054200",
    "end": "1059299"
  },
  {
    "text": "core for being able to garbage collect each of those independently from each other so you're no longer going to have",
    "start": "1059299",
    "end": "1064640"
  },
  {
    "text": "blocking Our Garbage Collection that causes let's say your entire front-end threads to potentially pause it's all",
    "start": "1064640",
    "end": "1071660"
  },
  {
    "text": "going to happen on a per core basis and each Heap can be managed independently by its own thread and its own set of",
    "start": "1071660",
    "end": "1077780"
  },
  {
    "text": "memory that it uses for that CPU so this is going to greatly improve throughput",
    "start": "1077780",
    "end": "1083000"
  },
  {
    "text": "but the cost you're going to get from it is that memory usage will be a lot higher on average and on top of that you",
    "start": "1083000",
    "end": "1089539"
  },
  {
    "text": "might notice that the actual amount of CPU time used by the garbage collection process is also higher in the grand",
    "start": "1089539",
    "end": "1095660"
  },
  {
    "text": "scheme of things though the juice is worth the squeeze so let's take a look at an example",
    "start": "1095660",
    "end": "1101299"
  },
  {
    "text": "to enable whoops my it looks like my annotation is missing a little bit to enable server-side garbage collection",
    "start": "1101299",
    "end": "1107900"
  },
  {
    "text": "in case you can't see it might be a little too small back there um",
    "start": "1107900",
    "end": "1113000"
  },
  {
    "text": "but in case you can't see it it's this property right here server garbage collection is true when you enable that",
    "start": "1113000",
    "end": "1119120"
  },
  {
    "text": "that'll go ahead and cause when your.net process launches it to run with that you know per core based mode instead of",
    "start": "1119120",
    "end": "1126080"
  },
  {
    "text": "doing it as sort of like a workstation mode let's take a look at some performance figures again apologize if these are too",
    "start": "1126080",
    "end": "1132860"
  },
  {
    "text": "small back there so this is I believe our Aqua dot remote ping pong Benchmark",
    "start": "1132860",
    "end": "1138380"
  },
  {
    "text": "that we use this is how we test the end-to-end throughput of like a single Connection in akkadot remote",
    "start": "1138380",
    "end": "1144260"
  },
  {
    "text": "so I have workstation GC on my left and server DC on the right",
    "start": "1144260",
    "end": "1149419"
  },
  {
    "text": "on the left I'm doing about 140 to 150 000 messages a second",
    "start": "1149419",
    "end": "1156559"
  },
  {
    "text": "on the right I am doing roughly twice that I'm doing about 295 293 291 a",
    "start": "1156559",
    "end": "1164120"
  },
  {
    "text": "thousand messages a second there all by changing one XML setting in my project",
    "start": "1164120",
    "end": "1169220"
  },
  {
    "text": "that's a lot of bang for your buck right there so rather than having your engineers spending an inordinate amount",
    "start": "1169220",
    "end": "1174860"
  },
  {
    "text": "of time trying to Performance optimize things maybe just configuring your garbage collector differently can make a huge difference",
    "start": "1174860",
    "end": "1181039"
  },
  {
    "text": "I've got one more easy win like this for you guys a little bit later in the presentation where again it's just changing one XML",
    "start": "1181039",
    "end": "1187880"
  },
  {
    "text": "setting but this is the first one so enable server side GC if throughput is important uh the one big trade-off here",
    "start": "1187880",
    "end": "1194360"
  },
  {
    "text": "is that if you're running let's say a really densely packed server Farm where you might have hundreds of multi-tenant",
    "start": "1194360",
    "end": "1200240"
  },
  {
    "text": "applications all running on the same box this is probably cost prohibitive to enable server-side GC for all of them",
    "start": "1200240",
    "end": "1206980"
  },
  {
    "text": "but that's kind of an unusual hosting setup you don't see very often anymore for most of your sort of application",
    "start": "1206980",
    "end": "1213980"
  },
  {
    "text": "workloads server-side GC is probably worth the trade-off now you got a question yeah",
    "start": "1213980",
    "end": "1222279"
  },
  {
    "text": "how big of so the question was how big of a memory increase do you see when you go from workstation GC to server GC",
    "start": "1224360",
    "end": "1232700"
  },
  {
    "text": "um basically if we were allocating on our heat before let's say it was uh 10",
    "start": "1232700",
    "end": "1238039"
  },
  {
    "text": "megabytes for our um workstation GC right out of the gate you might see that the average memory",
    "start": "1238039",
    "end": "1244820"
  },
  {
    "text": "usage might be something closer to 40 or 50 megabytes in there so not necessarily A 5x increase because I'm also not",
    "start": "1244820",
    "end": "1251780"
  },
  {
    "text": "getting into things like page sizes and all that the the amount of total amount of memory increase I'd say as a",
    "start": "1251780",
    "end": "1257360"
  },
  {
    "text": "percentage it's going to be using the 30 or 40 percent more memory probably than it did before and that's not necessarily",
    "start": "1257360",
    "end": "1263600"
  },
  {
    "text": "that the memory is actively being utilized is that the application is going to demand that much more capacity",
    "start": "1263600",
    "end": "1269600"
  },
  {
    "text": "in order to have Heap space available on each core right so that's sort of like requested memory not necessarily used",
    "start": "1269600",
    "end": "1275720"
  },
  {
    "text": "memory does that make sense yeah",
    "start": "1275720",
    "end": "1279940"
  },
  {
    "text": "would I enable this for UI applications well as long as it's not running on a super low powered machine uh yeah I'll",
    "start": "1282020",
    "end": "1288440"
  },
  {
    "text": "definitely go ahead and enable it um I don't quite Recall why server side GC is not enabled by default to be",
    "start": "1288440",
    "end": "1294860"
  },
  {
    "text": "honest with you um but you know I don't I don't think it would hurt again if throughput's",
    "start": "1294860",
    "end": "1300020"
  },
  {
    "text": "important I would probably enable it if you're more concerned with let's say keeping resource utilization as low as",
    "start": "1300020",
    "end": "1306740"
  },
  {
    "text": "possible let's say you're building a application that's going to run on a thin client that has a really low amount",
    "start": "1306740",
    "end": "1313100"
  },
  {
    "text": "of CPU and memory I'd probably stick with workstation GC for that",
    "start": "1313100",
    "end": "1318220"
  },
  {
    "text": "so the question is if you're running on containers is server GC a problem",
    "start": "1322400",
    "end": "1328000"
  },
  {
    "text": "so the question was if I tell me if I got this right that if you're deploying",
    "start": "1346159",
    "end": "1352840"
  },
  {
    "text": "maybe we'll need to get back to that one at the end real quick um now in terms of technique so we've",
    "start": "1354740",
    "end": "1361100"
  },
  {
    "text": "talked a bit about how the garbage collector works and how it can affect the performance characteristics of your machine and we've seen a couple of",
    "start": "1361100",
    "end": "1366919"
  },
  {
    "text": "little examples of like how we can uh reduce our our usage of the garbage",
    "start": "1366919",
    "end": "1372740"
  },
  {
    "text": "collector through techniques like pooling well the next thing we kind of want to talk about is from a coding standpoint what are some little things",
    "start": "1372740",
    "end": "1379880"
  },
  {
    "text": "you might end up doing on the hot path of your application that can result in the garbage collector having to work a",
    "start": "1379880",
    "end": "1385340"
  },
  {
    "text": "lot harder than it needs to a really good example of this and Daniel talked about this in his excellent talk",
    "start": "1385340",
    "end": "1390440"
  },
  {
    "text": "yesterday as well is delegates and closures inside our application so",
    "start": "1390440",
    "end": "1396260"
  },
  {
    "text": "if we have this little bit of code right here and again I apologize for the small size",
    "start": "1396260",
    "end": "1401299"
  },
  {
    "text": "this is the run method of the akkadona actors mailbox this is like the hottest of hot pads inside akka.net this is",
    "start": "1401299",
    "end": "1408260"
  },
  {
    "text": "designed to run hundreds of millions of times per second potentially well we were doing a delegate call down here",
    "start": "1408260",
    "end": "1417980"
  },
  {
    "text": "on this method we accidentally closed over the this keyword which is something that you know we sharper jetbrain's",
    "start": "1417980",
    "end": "1423740"
  },
  {
    "text": "writer will warn you about when you're doing that so we closed over this in order to base the execute a little bit",
    "start": "1423740",
    "end": "1429320"
  },
  {
    "text": "of processing and cleanup code inside this method so I went ahead and I benchmarked this",
    "start": "1429320",
    "end": "1435620"
  },
  {
    "text": "and said okay what does our performance look like as sort of a bit of a baseline",
    "start": "1435620",
    "end": "1441320"
  },
  {
    "text": "here and so when we're processing 10 000 messages we could do that in about 216",
    "start": "1441320",
    "end": "1446780"
  },
  {
    "text": "microseconds today and we allocate 385k worth of memory that's just queuing up",
    "start": "1446780",
    "end": "1452000"
  },
  {
    "text": "messages and then when we schedule the actor to actually run and process those messages it takes about uh 2.3",
    "start": "1452000",
    "end": "1458299"
  },
  {
    "text": "milliseconds roughly and we consume 21 kilobits worth of information uh the enqueue performance on the mailbox is",
    "start": "1458299",
    "end": "1464780"
  },
  {
    "text": "kind of a step that happens before this well one thing we did was we simply",
    "start": "1464780",
    "end": "1470000"
  },
  {
    "text": "inlined the delegate so rather than um trying to basically rewrite the",
    "start": "1470000",
    "end": "1475340"
  },
  {
    "text": "delegate to pass in a bunch of additional uh stat properties then trying to make that delegate cash and",
    "start": "1475340",
    "end": "1481280"
  },
  {
    "text": "make it static we just took a look at what the delegate was doing and said you know what it's not so sophisticated that",
    "start": "1481280",
    "end": "1486860"
  },
  {
    "text": "it's kind of worth it for us to and plus this code's not callable from the outside it's all internals",
    "start": "1486860",
    "end": "1492559"
  },
  {
    "text": "we should just go ahead and in line with that delegates doing and just simplify this code not avoid having the delegate",
    "start": "1492559",
    "end": "1498080"
  },
  {
    "text": "in the first place so we moved some of the code the delegate was doing and just kind of copied it right here into the hot path",
    "start": "1498080",
    "end": "1505220"
  },
  {
    "text": "and here's what the performance characteristics look like here we went from allocating when the actor",
    "start": "1505220",
    "end": "1511940"
  },
  {
    "text": "processes messages 21 kilobits when we did 10 000 and 203",
    "start": "1511940",
    "end": "1517039"
  },
  {
    "text": "kilobits when we did a hundred thousand all the way down to just a single kilobit going forward and that one",
    "start": "1517039",
    "end": "1522260"
  },
  {
    "text": "kilobit probably corresponds to something innate with the actor itself has nothing to do with the volume of",
    "start": "1522260",
    "end": "1527960"
  },
  {
    "text": "messages it's processing and on top of that we had a bit of a throughput Improvement of around 10 percent uh",
    "start": "1527960",
    "end": "1534260"
  },
  {
    "text": "which really helps uh speed up your actors by default as well so just simply getting rid of allocating this delegate",
    "start": "1534260",
    "end": "1540919"
  },
  {
    "text": "that was probably a gen zero allocation it's probably where it was showing up in garbage collection getting rid of that",
    "start": "1540919",
    "end": "1547700"
  },
  {
    "text": "um on its own right kind of really helped speed up our application reduce memory pressure and just got kind of",
    "start": "1547700",
    "end": "1553580"
  },
  {
    "text": "waste out of the picture for this really critical area of our framework",
    "start": "1553580",
    "end": "1558620"
  },
  {
    "text": "now there's a number of other different ways you can go about getting rid of delegates other than inlining them inlining is probably what I would do if",
    "start": "1558620",
    "end": "1565279"
  },
  {
    "text": "it's really simple and it's in a really critical area where it's being used but if you have like a public API and you",
    "start": "1565279",
    "end": "1570679"
  },
  {
    "text": "can't necessarily easily get rid of delegates because end user code relies on having a callback or some other",
    "start": "1570679",
    "end": "1576380"
  },
  {
    "text": "configuration method there's some better techniques you can use here um see Daniel and his talk showed using",
    "start": "1576380",
    "end": "1583279"
  },
  {
    "text": "static delegates that's a new keyword they added in c-sharp nine you can go ahead and declare a delegate as static",
    "start": "1583279",
    "end": "1588380"
  },
  {
    "text": "that means that it's not going to be able to close over any local context that it's running inside of you're going",
    "start": "1588380",
    "end": "1594020"
  },
  {
    "text": "to have to pass in anything any data you want to use inside that delegate from the outside using parameters so that's",
    "start": "1594020",
    "end": "1600679"
  },
  {
    "text": "going to use the compiler to help prevent you from having these types of implicit closures and other sources of",
    "start": "1600679",
    "end": "1607400"
  },
  {
    "text": "waste so static delegates are a good idea another thing you can potentially do is a value delegate this is a",
    "start": "1607400",
    "end": "1612919"
  },
  {
    "text": "technique from Bartosh Adam chesky that I really like which is a way of basically of kind of having delegates",
    "start": "1612919",
    "end": "1619159"
  },
  {
    "text": "Without Really writing them as Lambda expressions and let me show you an example of that real quick",
    "start": "1619159",
    "end": "1625000"
  },
  {
    "text": "this is a private read-only struct this request worker task right here so this",
    "start": "1625000",
    "end": "1632179"
  },
  {
    "text": "is a value type hence the term value delegate implements an interface called I runnable I runnable is basically",
    "start": "1632179",
    "end": "1639140"
  },
  {
    "text": "something that's going to get scheduled onto the occodine net dispatcher typically it's a mailbox processing run",
    "start": "1639140",
    "end": "1644360"
  },
  {
    "text": "but it could also be something like a scheduled task could also end up on there so we implement this I runnable",
    "start": "1644360",
    "end": "1649940"
  },
  {
    "text": "interface and that's this little run method down below here and this run method will run just the same as a",
    "start": "1649940",
    "end": "1657080"
  },
  {
    "text": "reference type what we can do in places where we are going to consume this delegate",
    "start": "1657080",
    "end": "1663799"
  },
  {
    "text": "is we basically can go ahead and pass in into this pool you know queue user work",
    "start": "1663799",
    "end": "1669020"
  },
  {
    "text": "item we can go ahead and pull in a new instance of this value type here now you'll note that I make a a note here",
    "start": "1669020",
    "end": "1676159"
  },
  {
    "text": "that we can potentially create a boxing allocation by accident if this method takes an object or some sort of",
    "start": "1676159",
    "end": "1683480"
  },
  {
    "text": "reference type down here we're going to go ahead and undo the work of passing in a value delegate reason why is that",
    "start": "1683480",
    "end": "1689960"
  },
  {
    "text": "that'll get converted or boxed into a reference type and then executed inside",
    "start": "1689960",
    "end": "1695240"
  },
  {
    "text": "of there and we're still going to be creating allocations so what you can't see on this particular piece of code is",
    "start": "1695240",
    "end": "1700880"
  },
  {
    "text": "that this actually takes a generic of type I runnable and that is not going to create any boxing for us we're going to",
    "start": "1700880",
    "end": "1706520"
  },
  {
    "text": "be able to pass in this particular struct and that's going to get run behind the scenes without creating",
    "start": "1706520",
    "end": "1711559"
  },
  {
    "text": "boxing one other thing we're going to talk about is some potential sources of boxing that might have happened in",
    "start": "1711559",
    "end": "1717080"
  },
  {
    "text": "earlier versions of C sharp can by and large get eliminated now through Dynamic profile guided optimization which is the",
    "start": "1717080",
    "end": "1724340"
  },
  {
    "text": "second big win we're going to talk about at the very end of the presentation so value delegates are another Technique",
    "start": "1724340",
    "end": "1729620"
  },
  {
    "text": "we can use to get rid of delegate allocations now another source of potential memory",
    "start": "1729620",
    "end": "1737840"
  },
  {
    "text": "waste that can build up inside our system is empty Collections and here's something that I noticed while I was",
    "start": "1737840",
    "end": "1743179"
  },
  {
    "text": "using jetbrain's Dynamic profiler was I noticed that our finite State",
    "start": "1743179",
    "end": "1749240"
  },
  {
    "text": "machine actor which basically sends around some little State objects to kind",
    "start": "1749240",
    "end": "1754279"
  },
  {
    "text": "of describe here's the current state and position of our actor at any given time I noticed that this thing when we were",
    "start": "1754279",
    "end": "1760760"
  },
  {
    "text": "running our remoting Benchmark allocated according to writer on here about 360",
    "start": "1760760",
    "end": "1766539"
  },
  {
    "text": "megabytes I'm thinking to myself this is like a stupid Poco class that doesn't",
    "start": "1766539",
    "end": "1772039"
  },
  {
    "text": "have any innate data on its own how could we possibly be allocating that much memory on here",
    "start": "1772039",
    "end": "1778340"
  },
  {
    "text": "so I decided to look under the covers you know what let's see if I can zoom in a little bit",
    "start": "1778340",
    "end": "1784220"
  },
  {
    "text": "um yeah you know what I'm going to go ahead and call an audible here",
    "start": "1784220",
    "end": "1790299"
  },
  {
    "text": "take that 4x3 resolutions hold on",
    "start": "1796340",
    "end": "1801559"
  },
  {
    "text": "yeah not not letting not letting my Boomer PowerPoint skills get the better of us",
    "start": "1801559",
    "end": "1807200"
  },
  {
    "text": "today all right so here's what I found inside the state",
    "start": "1807200",
    "end": "1812299"
  },
  {
    "text": "class is some probably me I decided to go ahead and allocate a new",
    "start": "1812299",
    "end": "1819860"
  },
  {
    "text": "list if the in the event that the replies was blank and you know this is back in",
    "start": "1819860",
    "end": "1825100"
  },
  {
    "text": "2014-13 when things were a lot more innocent you know back then we didn't know any better we were just having the",
    "start": "1825100",
    "end": "1831440"
  },
  {
    "text": "time of our lives well this allocates a new non-empty array every single time I",
    "start": "1831440",
    "end": "1837320"
  },
  {
    "text": "create one of these State objects ironically enough it's creating it when there's nothing for us to actually do",
    "start": "1837320",
    "end": "1843140"
  },
  {
    "text": "it's creating a bunch of empty lists for no apparent reason and so that is what is resulting in this 360 megabytes worth",
    "start": "1843140",
    "end": "1852020"
  },
  {
    "text": "of whoops well apparently I should not have taunted PowerPoint",
    "start": "1852020",
    "end": "1858620"
  },
  {
    "text": "all right okay so apparently this is basically what the source of our allocations was so every",
    "start": "1858620",
    "end": "1865220"
  },
  {
    "text": "single time you go and allocate one of these new lists you're going to be create putting 32 bytes on the Heap so",
    "start": "1865220",
    "end": "1871700"
  },
  {
    "text": "this is a really easy fix in newer versions of.net because the.net Base Class library now supports having a",
    "start": "1871700",
    "end": "1879740"
  },
  {
    "text": "static sort of generic property oops there we go",
    "start": "1879740",
    "end": "1885919"
  },
  {
    "text": "having a static generic property called array.empty and you can pass in the type there that's going to allocate a 32-bit",
    "start": "1885919",
    "end": "1892460"
  },
  {
    "text": "object once per type and I can go and reuse that every single time I spin up one of these new state classes or every",
    "start": "1892460",
    "end": "1899480"
  },
  {
    "text": "single time I need to use an empty object anywhere throughout my application so this completely",
    "start": "1899480",
    "end": "1904760"
  },
  {
    "text": "eliminated that accidental source of uh of allocations there",
    "start": "1904760",
    "end": "1910419"
  },
  {
    "text": "next in terms of something that is a little counter-intuitive value types versus reference types",
    "start": "1910520",
    "end": "1918080"
  },
  {
    "text": "the thing we have probably heard over and over and over again from people like",
    "start": "1918080",
    "end": "1923659"
  },
  {
    "text": "David Fowler and other folks who are working on asp.net performance is that value types are the greatest things in",
    "start": "1923659",
    "end": "1930320"
  },
  {
    "text": "sliced bread you should use them everywhere where you have sort of Val you copy by value sort of semantics",
    "start": "1930320",
    "end": "1935480"
  },
  {
    "text": "that's going to result in a lot less allocations a lot less garbage collection pressure going to result in",
    "start": "1935480",
    "end": "1940760"
  },
  {
    "text": "Faster software I am generally also a proponent of that but it can get a",
    "start": "1940760",
    "end": "1945980"
  },
  {
    "text": "little nuanced and that's what we're going to take a look at this is from that same class we're",
    "start": "1945980",
    "end": "1951020"
  },
  {
    "text": "looking at anaka.net this is an event that gets emitted every time our finite State machine has work to do so every",
    "start": "1951020",
    "end": "1956360"
  },
  {
    "text": "time someone wants to send a message to a finite State machine actor it's going to get encapsulated inside this event",
    "start": "1956360",
    "end": "1962299"
  },
  {
    "text": "type so we allocate uh millions of these per second in busy applications here and",
    "start": "1962299",
    "end": "1969799"
  },
  {
    "text": "this shows up on our profiler when we do that so if we take a look here let me zoom in",
    "start": "1969799",
    "end": "1975860"
  },
  {
    "text": "on my little little graph there we go so we've benchmarked our",
    "start": "1975860",
    "end": "1981080"
  },
  {
    "text": "finite State machine actor in terms of how much memory and how much time does it take to process a million messages",
    "start": "1981080",
    "end": "1986860"
  },
  {
    "text": "compared to an untyped actor which does not use these event types well the total amount of memory we",
    "start": "1986860",
    "end": "1993320"
  },
  {
    "text": "allocated for our FSM actor was about 287 megabytes versus our untyped actor",
    "start": "1993320",
    "end": "1998720"
  },
  {
    "text": "which used roughly a fifth of that 56 megabytes okay well that's a little disconcerting",
    "start": "1998720",
    "end": "2005559"
  },
  {
    "text": "so what if I change that sealed class that we saw earlier to a read-only",
    "start": "2005559",
    "end": "2011620"
  },
  {
    "text": "struct functionally equivalent to what we were doing before doesn't meaningfully change the semantics of how",
    "start": "2011620",
    "end": "2017679"
  },
  {
    "text": "this actor works what does our memory allocation look like now change it to a value type",
    "start": "2017679",
    "end": "2024100"
  },
  {
    "text": "and we see a reduction of roughly 30 megabytes on our message processing here doesn't reduce it all the way to zero",
    "start": "2024100",
    "end": "2031360"
  },
  {
    "text": "because there's other stuff that the finite State machine actor is doing but basically reduces the overhead of",
    "start": "2031360",
    "end": "2037120"
  },
  {
    "text": "allocating just the um the event class by roughly it's like",
    "start": "2037120",
    "end": "2042640"
  },
  {
    "text": "32 bits a pop basically so for a million events we get 32 megabytes total shaved",
    "start": "2042640",
    "end": "2048158"
  },
  {
    "text": "out of the allocations during that Benchmark that's how that sort of gets calculated so that's a little bit of",
    "start": "2048159",
    "end": "2054220"
  },
  {
    "text": "memory savings there and a minor throughput Improvement well one thing we decided to do is take",
    "start": "2054220",
    "end": "2061540"
  },
  {
    "text": "a look at where we are currently using value types but we're not using them very well inside our application this is",
    "start": "2061540",
    "end": "2067960"
  },
  {
    "text": "that boxing terminology I mentioned briefly earlier boxing is what happens when you accidentally cast a value type",
    "start": "2067960",
    "end": "2074800"
  },
  {
    "text": "back into a reference type it completely undoes all the undoes all the great work",
    "start": "2074800",
    "end": "2079898"
  },
  {
    "text": "you've done uh trying to go ahead and use value types inside your system when you do this so",
    "start": "2079899",
    "end": "2085780"
  },
  {
    "text": "if I take a look here yeah boxing is basically when you explicitly cast back and forth here's an",
    "start": "2085780",
    "end": "2091898"
  },
  {
    "text": "example of a method right here if I zoom in some this is inside that finite State machine",
    "start": "2091899",
    "end": "2098800"
  },
  {
    "text": "class again again we found just a lot of stuff wrong with this class when I when we started looking under the covers",
    "start": "2098800",
    "end": "2104619"
  },
  {
    "text": "this is a quality method right here state name equals next state",
    "start": "2104619",
    "end": "2110560"
  },
  {
    "text": "and so quick show of hands for everybody who believes that a referential check",
    "start": "2110560",
    "end": "2115960"
  },
  {
    "text": "for equality should return allocations or produce allocations show of hands",
    "start": "2115960",
    "end": "2122740"
  },
  {
    "text": "all hands are down you all pass the test good this is code smell when you see this showing up on here it means someone",
    "start": "2122740",
    "end": "2129160"
  },
  {
    "text": "someone made a boo-boo as we say inside our team so 496 Megs allocated there's trouble",
    "start": "2129160",
    "end": "2136119"
  },
  {
    "text": "Brewing so what exactly is causing this well the issue here in fact I'll zoom in",
    "start": "2136119",
    "end": "2142839"
  },
  {
    "text": "one more time the issue here in this case is that the state name is typically an enum which is a value type in.net and",
    "start": "2142839",
    "end": "2150640"
  },
  {
    "text": "we are actually doing an object that equals call right here which means we are boxing our enum every single time we",
    "start": "2150640",
    "end": "2157480"
  },
  {
    "text": "call the equals but if the other thing we're looking at is also an enum we're also boxing that",
    "start": "2157480",
    "end": "2163660"
  },
  {
    "text": "as well so we're actually boxing both both sides of this equation at a given time",
    "start": "2163660",
    "end": "2168880"
  },
  {
    "text": "whoops so really simple mistake we were just using the the nor the standard equals method and just assuming that",
    "start": "2168880",
    "end": "2175540"
  },
  {
    "text": "that would be safe but no it actually was not in this case and thankfully uh yeah jetbrain's Dynamic profiler helped",
    "start": "2175540",
    "end": "2182140"
  },
  {
    "text": "us find that so we rewrote this code and I'll zoom in a little bit to make that a little easier to read we rewrote",
    "start": "2182140",
    "end": "2189099"
  },
  {
    "text": "this code to use the equality compare and we pass in the type the generic type that users",
    "start": "2189099",
    "end": "2194920"
  },
  {
    "text": "use to create these FSM actors so quality compare.default.equals and this actually",
    "start": "2194920",
    "end": "2200020"
  },
  {
    "text": "performs a non-boxing check on both of these enums here it's basically doing an",
    "start": "2200020",
    "end": "2205420"
  },
  {
    "text": "integer comparison on both of them now that way we don't allocate anything anymore",
    "start": "2205420",
    "end": "2210700"
  },
  {
    "text": "so if I take a look at my note here this removed 100 of boxing allocations at this call site and helped speed up this",
    "start": "2210700",
    "end": "2217660"
  },
  {
    "text": "comparison operation really significantly by an order of magnitude or more so again a really simple type of mistake",
    "start": "2217660",
    "end": "2224859"
  },
  {
    "text": "that's really easy for developers to make and a really good tool like the dynamic profiler and Rider can help you",
    "start": "2224859",
    "end": "2230800"
  },
  {
    "text": "find this stuff uh when you're running your applications here's where things get a little tricky",
    "start": "2230800",
    "end": "2236859"
  },
  {
    "text": "which is we actually have a built-in value type in akka.net that's used really heavily today called an envelope",
    "start": "2236859",
    "end": "2243040"
  },
  {
    "text": "this is what we use to do in-memory message passing from one actor to another so we produce",
    "start": "2243040",
    "end": "2248859"
  },
  {
    "text": "potentially hundreds of millions of these a second depending on how busy your system is and this today already",
    "start": "2248859",
    "end": "2254200"
  },
  {
    "text": "exists as a read-only struct well what happens when a value type",
    "start": "2254200",
    "end": "2260920"
  },
  {
    "text": "passes from one method to another and you're not using a ref keyword on there well the answer is that you copy it",
    "start": "2260920",
    "end": "2267160"
  },
  {
    "text": "right this basic result if you're passing an integer around through multiple methods you're copying it by value rather than copying by reference",
    "start": "2267160",
    "end": "2273520"
  },
  {
    "text": "this is something you might have learned you know in a introductory course an object-oriented programming with C plus",
    "start": "2273520",
    "end": "2279280"
  },
  {
    "text": "plus C sharp or Java maybe those are sort of the semantics for how value types in C sharp work well this envelope",
    "start": "2279280",
    "end": "2287020"
  },
  {
    "text": "is highly concurrent and gets passed around between lots of different actors and appears in lots of different",
    "start": "2287020",
    "end": "2292839"
  },
  {
    "text": "mailboxes and there's going to be a lot of copying pressure happening throughout the system on here so one Theory we had",
    "start": "2292839",
    "end": "2299740"
  },
  {
    "text": "was what happens if we go and change this back to a reference type could that",
    "start": "2299740",
    "end": "2305020"
  },
  {
    "text": "actually be faster in this case because we're not constantly having to make copies of this data structure over and",
    "start": "2305020",
    "end": "2310720"
  },
  {
    "text": "over again implicitly by moving messages throughout our pipeline so we thought okay we're super mega Geniuses let's go",
    "start": "2310720",
    "end": "2318460"
  },
  {
    "text": "ahead and see if this works so what our current sort of Baseline performance looks like in aka.net with",
    "start": "2318460",
    "end": "2325180"
  },
  {
    "text": "this read-only struct is if we have an actor processing a million messages and this is using the other performance",
    "start": "2325180",
    "end": "2331119"
  },
  {
    "text": "improvements you saw earlier it takes us about um you know for Su for ten thousand messages it's about 2.2 milliseconds and",
    "start": "2331119",
    "end": "2339160"
  },
  {
    "text": "then for a hundred thousand messages it's about 9.8 milliseconds okay",
    "start": "2339160",
    "end": "2344260"
  },
  {
    "text": "let's see what happens when we change this back to a reference type so if we",
    "start": "2344260",
    "end": "2349420"
  },
  {
    "text": "zoom in here oops sorry about that I've made this back into a uh yeah a sealed",
    "start": "2349420",
    "end": "2355540"
  },
  {
    "text": "class and what do the numbers look like well this actually reduce allocations",
    "start": "2355540",
    "end": "2361960"
  },
  {
    "text": "and if I zoom out over here in fact it sure did reduce allocations specifically when we're enqueuing",
    "start": "2361960",
    "end": "2368500"
  },
  {
    "text": "messages inside the mailbox so memory pressure actually went down as a result",
    "start": "2368500",
    "end": "2373839"
  },
  {
    "text": "of moving from a value type to a reference type since we're not doing all that copying pressure over and over",
    "start": "2373839",
    "end": "2380020"
  },
  {
    "text": "again so we went from let's say about yeah 394 kilobits to 264 or from 3.15",
    "start": "2380020",
    "end": "2387099"
  },
  {
    "text": "megabytes to 2.1 megabytes great this feels like an amazing win I'm going to",
    "start": "2387099",
    "end": "2392440"
  },
  {
    "text": "go ahead and write a blog post and get an MVP award for sure but wait there's more",
    "start": "2392440",
    "end": "2399220"
  },
  {
    "text": "it's not quite that simple member of my team",
    "start": "2399220",
    "end": "2405040"
  },
  {
    "text": "you know who understands all this stuff says yeah copy by value has its pitfalls it can result in a lot of pressure when",
    "start": "2405040",
    "end": "2411460"
  },
  {
    "text": "you're moving that same data structure through a lot of different contexts he says yes",
    "start": "2411460",
    "end": "2417099"
  },
  {
    "text": "that is all true Aaron and you have proved that proven this using this single threaded Benchmark very well done",
    "start": "2417099",
    "end": "2424300"
  },
  {
    "text": "however have you considered have you considered",
    "start": "2424300",
    "end": "2429880"
  },
  {
    "text": "what happens when we start running this the way we really do in aka.net which is across a lot of threads where we're",
    "start": "2429880",
    "end": "2436780"
  },
  {
    "text": "going to be passing these value types around not just on a single thread in a benchmark.net benchmark but across",
    "start": "2436780",
    "end": "2442780"
  },
  {
    "text": "potentially you know in a really large application it might be a hundred threads or it might be 30 or 40. what",
    "start": "2442780",
    "end": "2449800"
  },
  {
    "text": "happens to that same Benchmark when we significantly increase cross thread message traffic",
    "start": "2449800",
    "end": "2455320"
  },
  {
    "text": "and I said all right game on I'll go ahead and test this I'm sure my MVP award is good so I go ahead and run this",
    "start": "2455320",
    "end": "2462220"
  },
  {
    "text": "little Benchmark here uh but we're putting let's see about a hundred thousand messages through and",
    "start": "2462220",
    "end": "2468339"
  },
  {
    "text": "I'm keeping track of my gen zero and gen 1 allocations here and so it looks like we allocated with um my c-sharp uh",
    "start": "2468339",
    "end": "2476140"
  },
  {
    "text": "reference types here about 104 megabytes when we were doing let's say 10 total",
    "start": "2476140",
    "end": "2481780"
  },
  {
    "text": "actors and when I had 100 actors we did you know 10 roughly 10x that about a gigabyte of memory okay that looks good",
    "start": "2481780",
    "end": "2488619"
  },
  {
    "text": "I can see my total throughput here uh was about on average took about 1.2",
    "start": "2488619",
    "end": "2493720"
  },
  {
    "text": "seconds for 10 actors to process a hundred thousand messages each and it's like 100 actors roughly 10x that roughly",
    "start": "2493720",
    "end": "2501400"
  },
  {
    "text": "13 seconds now what happens if we convert an envelope back into a struct again",
    "start": "2501400",
    "end": "2507339"
  },
  {
    "text": "well if we take a look at the memory usage memory usage is way down now that we're back into a concurrent context we",
    "start": "2507339",
    "end": "2514420"
  },
  {
    "text": "went from 104 megabytes back to 71 from about a gig back into 689 again it's",
    "start": "2514420",
    "end": "2521079"
  },
  {
    "text": "that 32-bit allocation disappearing again that's sort of what happened here and on top of that",
    "start": "2521079",
    "end": "2527619"
  },
  {
    "text": "if I take a look at our performance our performance was essentially unaffected by changing this from a value type to a",
    "start": "2527619",
    "end": "2534520"
  },
  {
    "text": "reference type either way it was only the amount of memory utilization that really got affected by it so the sort of",
    "start": "2534520",
    "end": "2541180"
  },
  {
    "text": "lesson here if I zoom back out sorry for my AV my Boomer AV challenges today is",
    "start": "2541180",
    "end": "2547180"
  },
  {
    "text": "guys this is uh all kind of doing it on the Fly here um the issue in this case is that",
    "start": "2547180",
    "end": "2553599"
  },
  {
    "text": "something that looks like it works in a single threaded context may not necessarily be so in a multi-threaded",
    "start": "2553599",
    "end": "2560020"
  },
  {
    "text": "context because the way memory gets copied between cores and the way context switching works also has an impact on",
    "start": "2560020",
    "end": "2567099"
  },
  {
    "text": "the ultimate throughput and the ultimate so let's say sort of effectiveness of some of these Solutions so lesson",
    "start": "2567099",
    "end": "2572680"
  },
  {
    "text": "learned changing a value type into a reference type well it worked great on a single",
    "start": "2572680",
    "end": "2578260"
  },
  {
    "text": "core absolutely sucked when we did it on a multi-core system and so we move on to",
    "start": "2578260",
    "end": "2583359"
  },
  {
    "text": "the next section threads hate you and your code",
    "start": "2583359",
    "end": "2588299"
  },
  {
    "text": "so let's go ahead and talk a little bit about memory management in a multi-threaded environment",
    "start": "2588640",
    "end": "2593920"
  },
  {
    "text": "um c-sharp has these Concepts called thread static and thread local variables and there's also async local which is a",
    "start": "2593920",
    "end": "2600640"
  },
  {
    "text": "little different and we'll talk about that what these Concepts kind of have in common the thread local and thread",
    "start": "2600640",
    "end": "2606880"
  },
  {
    "text": "static is they allocate objects directly into what's known as thread local storage so this is data that's being",
    "start": "2606880",
    "end": "2612339"
  },
  {
    "text": "allocated inside a special area reserved for that thread and that's really useful",
    "start": "2612339",
    "end": "2617680"
  },
  {
    "text": "for storing things like potentially a bit of work you know that one thread",
    "start": "2617680",
    "end": "2623619"
  },
  {
    "text": "is going to need to refer to over and over again so an example in akka.net where we use thread static values is we",
    "start": "2623619",
    "end": "2630819"
  },
  {
    "text": "have a a last recently at least recently used cash lru cash for storing a",
    "start": "2630819",
    "end": "2637180"
  },
  {
    "text": "deserialized actor paths and we use that rather than a concurrent dictionary",
    "start": "2637180",
    "end": "2642460"
  },
  {
    "text": "because we can avoid having to synchronize cash values between all the different cores that are currently doing",
    "start": "2642460",
    "end": "2648880"
  },
  {
    "text": "work on processing those remoting workloads at any given time having a thread local construct is inherently",
    "start": "2648880",
    "end": "2655599"
  },
  {
    "text": "thread safe because no one else can access those values although in theory you can but it's super awkward and weird",
    "start": "2655599",
    "end": "2661780"
  },
  {
    "text": "so it's probably very unlikely to happen and then on top of that all the data and work that's being performed while that",
    "start": "2661780",
    "end": "2667960"
  },
  {
    "text": "thread is running are kind of in an adjacent registers and memory for that stack so the downside of thread local data",
    "start": "2667960",
    "end": "2675700"
  },
  {
    "text": "structures is that they can't be synchronized uh by Def by definition you could create a a thread static",
    "start": "2675700",
    "end": "2682240"
  },
  {
    "text": "concurrent dictionary if you wanted to but that concurrent dictionary couldn't very easily be referenced by any other thread so these sort of thread static",
    "start": "2682240",
    "end": "2689380"
  },
  {
    "text": "constructs are useful in context where you're you know you're going to be performing work on the same",
    "start": "2689380",
    "end": "2695260"
  },
  {
    "text": "thread over and over again and that works all going to look quite similar so this kind of brings us to dotnet's",
    "start": "2695260",
    "end": "2703180"
  },
  {
    "text": "threading model and the impact this might have on your application's performance reference types that get passed between",
    "start": "2703180",
    "end": "2710079"
  },
  {
    "text": "let's say uh activities running on separate threads they're almost always going to end up in gen 2. not always",
    "start": "2710079",
    "end": "2716579"
  },
  {
    "text": "sometimes you might end up in gen 1 but typically it's going to end up in gen 2. so if you",
    "start": "2716579",
    "end": "2721900"
  },
  {
    "text": "invoke an a weights and you passed in some data into that and that object is",
    "start": "2721900",
    "end": "2727060"
  },
  {
    "text": "going to get used inside that awaitable method more often than not that bit of data you passed in there is going to get",
    "start": "2727060",
    "end": "2732819"
  },
  {
    "text": "promoted to Gen 2 garbage collection value types that are passed between threads are simply copied into the stack",
    "start": "2732819",
    "end": "2739540"
  },
  {
    "text": "of the new thread that's there that's why the memory allocation pressure from our multi-threaded Benchmark actually",
    "start": "2739540",
    "end": "2745540"
  },
  {
    "text": "went down is because basically we were making these sort of uh stack stack",
    "start": "2745540",
    "end": "2750760"
  },
  {
    "text": "based copies of those objects over and over again versus allocating new ones on the Heap and then trying to move that",
    "start": "2750760",
    "end": "2756880"
  },
  {
    "text": "stuff across different uh different CPUs where the performance sort of",
    "start": "2756880",
    "end": "2763300"
  },
  {
    "text": "characteristics for multi-threading start to become really relevant is the higher i o levels that your computer can",
    "start": "2763300",
    "end": "2769359"
  },
  {
    "text": "run at namely taking advantage of L1 and L2 caching these are the the built-in",
    "start": "2769359",
    "end": "2774760"
  },
  {
    "text": "memory caches that your actual physical CPUs themselves have access to and they tend to be quite small so maybe it might",
    "start": "2774760",
    "end": "2781540"
  },
  {
    "text": "be the L2 cache might have 16 or maybe on a really big processor maybe 32 and",
    "start": "2781540",
    "end": "2787599"
  },
  {
    "text": "then your L1 cache is going to be an order magnitude smaller than that so maybe four megabytes or so but these",
    "start": "2787599",
    "end": "2793359"
  },
  {
    "text": "have much faster access speeds than even dram does on your machine one of the things that we can typically",
    "start": "2793359",
    "end": "2800140"
  },
  {
    "text": "take advantage of is that if we have the same types of work being performed by the same thread and that same thread is",
    "start": "2800140",
    "end": "2806740"
  },
  {
    "text": "getting scheduled on the same CPU over and over again the hardware will begin to optimize the memory access to start",
    "start": "2806740",
    "end": "2814180"
  },
  {
    "text": "moving some of those frequently accessed objects or those frequently accessed methods into those cache lines and that",
    "start": "2814180",
    "end": "2821020"
  },
  {
    "text": "will allow your application to execute many orders of magnitude faster for that little bit of code but that kind of",
    "start": "2821020",
    "end": "2826720"
  },
  {
    "text": "relies on a little bit of what's called we call sort of thread locality where the thread is being scheduled onto the",
    "start": "2826720",
    "end": "2832900"
  },
  {
    "text": "same core over and over again most operating systems and also the.net runtime will by default try to do that",
    "start": "2832900",
    "end": "2839619"
  },
  {
    "text": "for you so the underlying infrastructure is going to try to do a good job out of the game however it's still possible for your",
    "start": "2839619",
    "end": "2847000"
  },
  {
    "text": "application to experience context switching context switching is what happens when work was executing on one",
    "start": "2847000",
    "end": "2854140"
  },
  {
    "text": "thread let's say you have a long IAC enumerable you're iterating through you",
    "start": "2854140",
    "end": "2859180"
  },
  {
    "text": "might go ahead and process a couple of your a-weighted operations on thread a which is running on",
    "start": "2859180",
    "end": "2864940"
  },
  {
    "text": "um one CPU and then when you a weight maybe that thread has to go service some other work that's more urgent and then",
    "start": "2864940",
    "end": "2871599"
  },
  {
    "text": "another thread running on another CPU is going to be the one that gets scheduled to pick up uh the next a weight in your",
    "start": "2871599",
    "end": "2877359"
  },
  {
    "text": "chain that is called a context switch when that occurs basically",
    "start": "2877359",
    "end": "2883300"
  },
  {
    "text": "what's going to happen is that each thread in and this is kind of scoped more or less to Windows but the Linux",
    "start": "2883300",
    "end": "2890020"
  },
  {
    "text": "sort of runtime works very similar to this where each thread is given by the operating system a what's called a",
    "start": "2890020",
    "end": "2895960"
  },
  {
    "text": "Quantum of time to work with and that's usually bounded to about 30 milliseconds of execution time this value is",
    "start": "2895960",
    "end": "2902440"
  },
  {
    "text": "configurable and also can be different between something like Windows desktop and Windows Server so it kind of can",
    "start": "2902440",
    "end": "2907780"
  },
  {
    "text": "vary a little bit but this is generally speaking how all preemptive operating systems are designed where each thread",
    "start": "2907780",
    "end": "2914440"
  },
  {
    "text": "is given a finite amount of time and we do this in order to prevent starvation so one really long running task crowding",
    "start": "2914440",
    "end": "2920920"
  },
  {
    "text": "out shorter ones so that thread is going to get scheduled onto the CPU",
    "start": "2920920",
    "end": "2927099"
  },
  {
    "text": "and after it completes its work or after it basically consumes it's allotted time that thread will get moved to the back",
    "start": "2927099",
    "end": "2933880"
  },
  {
    "text": "of the operating system's work queue and the next thread that is scheduled to run on that CPU will get its Quantum of time",
    "start": "2933880",
    "end": "2939880"
  },
  {
    "text": "and the operating system is going to be constantly kind of managing this and linearizing work where it's possible and",
    "start": "2939880",
    "end": "2945819"
  },
  {
    "text": "it's going to try to make sure the same threads get scheduled onto the same CPU but there's no guarantees it's a little",
    "start": "2945819",
    "end": "2951880"
  },
  {
    "text": "arbitrary in terms of how it can work context switching is when thread 0 moves",
    "start": "2951880",
    "end": "2958000"
  },
  {
    "text": "onto a different CPU or in this case your a weighted unit of work was being",
    "start": "2958000",
    "end": "2963520"
  },
  {
    "text": "processed on thread zero but now it's being processed on thread four those types those are both examples of context",
    "start": "2963520",
    "end": "2969579"
  },
  {
    "text": "switching we're essentially the runtime execution context of your piece of code is being changed up from under you",
    "start": "2969579",
    "end": "2975819"
  },
  {
    "text": "usually through arbitrary things like whichever core whichever thread was available you don't necessarily have a",
    "start": "2975819",
    "end": "2981700"
  },
  {
    "text": "lot of control over that as a software developer well as it turns out we actually do have some",
    "start": "2981700",
    "end": "2989619"
  },
  {
    "text": "control over this beginning like dot net core 3.3.0 and I believe um I believe the actual task work queue",
    "start": "2989619",
    "end": "2997660"
  },
  {
    "text": "for scheduling uh TPL tasks actually does use this under the covers now but",
    "start": "2997660",
    "end": "3002940"
  },
  {
    "text": "we're getting a net core three they added a new interface called an i thread pool work item this was an interface",
    "start": "3002940",
    "end": "3009720"
  },
  {
    "text": "that basically allowed us to go ahead and schedule work directly on the thread pool without needing to create a bunch",
    "start": "3009720",
    "end": "3015060"
  },
  {
    "text": "of weird little callback delegates and stuff like that it could basically just be in any arbitrary class that",
    "start": "3015060",
    "end": "3020460"
  },
  {
    "text": "implements this interface can now be scheduled directly onto the.net thread pool well there's also another new API they",
    "start": "3020460",
    "end": "3027839"
  },
  {
    "text": "added in.net6 and I'll go ahead and use my Boomer Zoom here let's get this up and running",
    "start": "3027839",
    "end": "3033599"
  },
  {
    "text": "all right there we go maybe a little bigger a little bigger all right there we go",
    "start": "3033599",
    "end": "3039180"
  },
  {
    "text": "this is a relatively new API they added in.net6 thread pool to unsafe Q user",
    "start": "3039180",
    "end": "3044940"
  },
  {
    "text": "work item that's like that that method's been around for a long time what's new is this little parameter right here",
    "start": "3044940",
    "end": "3050460"
  },
  {
    "text": "which is totally unreadable with me being zoomed in here but basically what this parameter does is it specifies",
    "start": "3050460",
    "end": "3057960"
  },
  {
    "text": "for this particular I runnable work item I would prefer to reschedule it onto the same CPU that I am running on right now",
    "start": "3057960",
    "end": "3065040"
  },
  {
    "text": "if possible uh I don't think I have a diagram on this but essentially every single thread",
    "start": "3065040",
    "end": "3070859"
  },
  {
    "text": "in the.net thread pool has like a local work queue and threads have the ability to steal work items from each other in",
    "start": "3070859",
    "end": "3076680"
  },
  {
    "text": "order to try to keep the system um moving smoothly at all times so that way tasks don't wait a long time to get",
    "start": "3076680",
    "end": "3083220"
  },
  {
    "text": "executed what this actual uh call does is it cues this work item right now onto",
    "start": "3083220",
    "end": "3089880"
  },
  {
    "text": "the thread that's currently executing and guess what if you have something like an actor that constantly schedules",
    "start": "3089880",
    "end": "3094980"
  },
  {
    "text": "itself over and over again to keep processing messages in its mailbox that means that actor is going to have a lot",
    "start": "3094980",
    "end": "3101040"
  },
  {
    "text": "less context switching as a result of this it's going to always essentially provide a hint to the runtime that I",
    "start": "3101040",
    "end": "3107640"
  },
  {
    "text": "want to execute on the same thread that I'm currently running on right now and so long as there isn't some major",
    "start": "3107640",
    "end": "3112920"
  },
  {
    "text": "starvation or crowding issue or some other um arbitrary semantic run time issue",
    "start": "3112920",
    "end": "3118920"
  },
  {
    "text": "that would prevent this from working what we're going to end up experiencing is a pretty significant impact on",
    "start": "3118920",
    "end": "3124859"
  },
  {
    "text": "performance so if we take a look here at our little performance comparison",
    "start": "3124859",
    "end": "3131400"
  },
  {
    "text": "in fact I got to use the zoom again pull this out this is awka.net's in-memory messaging",
    "start": "3131400",
    "end": "3138420"
  },
  {
    "text": "Benchmark and I think this is running on.net6 um before we added that API call and",
    "start": "3138420",
    "end": "3144900"
  },
  {
    "text": "after we added the API call so we have two different types of actors we're benchmarking here let's just take a look at the actor base on the left we're",
    "start": "3144900",
    "end": "3152520"
  },
  {
    "text": "doing at about Peak throughput and that throughput value by the way on the left is basically saying how many birth like",
    "start": "3152520",
    "end": "3159240"
  },
  {
    "text": "how big of a burst of messages can an actor process at any given time so when an actor gets scheduled to run we have",
    "start": "3159240",
    "end": "3164940"
  },
  {
    "text": "the ability to configure how many messages an actor can process the default value is 30 which is a pretty",
    "start": "3164940",
    "end": "3170760"
  },
  {
    "text": "good handles like 99 of use cases so a throughput of one means this actor is",
    "start": "3170760",
    "end": "3176160"
  },
  {
    "text": "going to have the highest possible context switching a throughput of 900 means while this actor is not going to",
    "start": "3176160",
    "end": "3183119"
  },
  {
    "text": "have very much context switching at all it's also going to hog threads for a really long time so it's kind of a",
    "start": "3183119",
    "end": "3189059"
  },
  {
    "text": "little bit of a balance to strike there so let's take a look at the throughput value of 30 which is our system default",
    "start": "3189059",
    "end": "3194700"
  },
  {
    "text": "we're doing about 41 million messages a second on you",
    "start": "3194700",
    "end": "3199940"
  },
  {
    "text": "know.net.net6 before we added that API life once we add that API we jump from about",
    "start": "3199940",
    "end": "3207300"
  },
  {
    "text": "40 million messages a second to 65 million messages a second so that's um",
    "start": "3207300",
    "end": "3214079"
  },
  {
    "text": "now I'm a little jet lagged and I had a lot of beer last night but I think that's about a 50 increase in overall",
    "start": "3214079",
    "end": "3219900"
  },
  {
    "text": "memory throughput if my mental math serves me well that is a tremendous free",
    "start": "3219900",
    "end": "3225000"
  },
  {
    "text": "lunch for an aqua.net user that they didn't really have to do a lot of work for all we had to do was find a way from",
    "start": "3225000",
    "end": "3231240"
  },
  {
    "text": "an infrastructure point of view to reduce context switching um there's apis you can go ahead and",
    "start": "3231240",
    "end": "3238079"
  },
  {
    "text": "call directly just like the ones I showed you for doing this but generally speaking the way a lot of the TPL stuff",
    "start": "3238079",
    "end": "3243839"
  },
  {
    "text": "is designed in newer versions of.net will automatically help do a lot of this for you now make sure that your tasks",
    "start": "3243839",
    "end": "3250140"
  },
  {
    "text": "get rescheduled back under the thread that's currently servicing them it's not a guarantee but it's a preference you",
    "start": "3250140",
    "end": "3255660"
  },
  {
    "text": "can go up that will be set automatically whenever you go ahead and a weight a task usually so if you haven't upgraded",
    "start": "3255660",
    "end": "3261660"
  },
  {
    "text": "to newer versions of.net you should definitely consider doing that in order to take advantage of it uh let's see",
    "start": "3261660",
    "end": "3268920"
  },
  {
    "text": "move on to the next slide here thread locality without context",
    "start": "3268920",
    "end": "3273960"
  },
  {
    "text": "switching yeah we kind of covered that all right data structures and synchronization so",
    "start": "3273960",
    "end": "3281339"
  },
  {
    "text": "one thing we decided that we thought we could do being the super mega Geniuses that we are was maybe we could",
    "start": "3281339",
    "end": "3287339"
  },
  {
    "text": "outperform the.net runtime at managing a concurrent Cube sure that seems like a",
    "start": "3287339",
    "end": "3292920"
  },
  {
    "text": "trivial problem I could hack together in an afternoon right yeah I'm being sarcastic the concurrent",
    "start": "3292920",
    "end": "3298859"
  },
  {
    "text": "cue is very very well done um so I I thought to myself you know uh we could in theory maybe use a linked",
    "start": "3298859",
    "end": "3305760"
  },
  {
    "text": "list and reduce some of the internal allocations that happen with lots of messages are being queued into the same",
    "start": "3305760",
    "end": "3310859"
  },
  {
    "text": "actor at the same time so why don't we go ahead and try using a linked list with a lock and see how well that works",
    "start": "3310859",
    "end": "3317880"
  },
  {
    "text": "well if we take a look at the performance numbers for what this looked like earlier",
    "start": "3317880",
    "end": "3323400"
  },
  {
    "text": "we can see that you know our and Q performance we're allocating about 385k and it takes us about 200 microseconds",
    "start": "3323400",
    "end": "3330540"
  },
  {
    "text": "to encue 10 000 messages into a single mailbox okay all right that's sort of our Baseline got it",
    "start": "3330540",
    "end": "3336839"
  },
  {
    "text": "well what happens if I go and change it to a synchronized linked list with a",
    "start": "3336839",
    "end": "3341880"
  },
  {
    "text": "lock like this locks are relatively cheap from a a synchronization standpoint they're not free but they're",
    "start": "3341880",
    "end": "3348540"
  },
  {
    "text": "not tremendously expensive either so I'm going to go ahead and use the built-in linked list and",
    "start": "3348540",
    "end": "3354500"
  },
  {
    "text": "system.collections.generic and I'm going to place a lock around all the sort of mutable operations on it internally",
    "start": "3354500",
    "end": "3361079"
  },
  {
    "text": "and I'm thinking this should offer better performance than a concurrent queue because I can go ahead and easily",
    "start": "3361079",
    "end": "3366420"
  },
  {
    "text": "append new items to the end of the list and easily consume items from the front of the list without having to resize the",
    "start": "3366420",
    "end": "3373440"
  },
  {
    "text": "internal Collections and the internal memory segments that are concurrent queue uses let's see how well that",
    "start": "3373440",
    "end": "3379140"
  },
  {
    "text": "worked out in theory well um if I zoom in here",
    "start": "3379140",
    "end": "3385319"
  },
  {
    "text": "we got our asses kicked more or less um not only did we double the amount of",
    "start": "3385319",
    "end": "3391200"
  },
  {
    "text": "memory usage that we had before we also more than doubled the amount of latency that we had so if I were a Ruby",
    "start": "3391200",
    "end": "3398520"
  },
  {
    "text": "programmer I consider this to be a job well done and check it in and move on",
    "start": "3398520",
    "end": "3403579"
  },
  {
    "text": "but instead we went ahead and rolled that back and decided you know what we're probably better off just waiting",
    "start": "3403579",
    "end": "3410579"
  },
  {
    "text": "for the very smart people the.net runtime to make keep working on concurrent queue and improving that for",
    "start": "3410579",
    "end": "3416099"
  },
  {
    "text": "us each time so what went wrong there well basically the concurrent queue is",
    "start": "3416099",
    "end": "3421980"
  },
  {
    "text": "actually a lock free data structure this is the sort of stuff that you can go ahead and write for fun if you want to",
    "start": "3421980",
    "end": "3427500"
  },
  {
    "text": "but it's very difficult to get right in practice thankfully concurrent cues got a ton of people using it therefore a lot",
    "start": "3427500",
    "end": "3433800"
  },
  {
    "text": "of regression testing a lot of really smart computer science type people looking at it it uses basically volatile",
    "start": "3433800",
    "end": "3439740"
  },
  {
    "text": "memory to go ahead and swap out sections of its internal sort of buffers internally and it uses some Lock Free",
    "start": "3439740",
    "end": "3446220"
  },
  {
    "text": "synchronization methods like interlock stock compare exchange to do this it's very optimized for lots of quick reads",
    "start": "3446220",
    "end": "3453240"
  },
  {
    "text": "and writes from both the back of the queue and the front of the queue happening concurrently it is",
    "start": "3453240",
    "end": "3458400"
  },
  {
    "text": "significantly less expensive even on a single thread to use this than it is to use a lock inside of there on top of",
    "start": "3458400",
    "end": "3464940"
  },
  {
    "text": "that I'm pretty sure.net's built-in linked list is utter crap and you would",
    "start": "3464940",
    "end": "3470460"
  },
  {
    "text": "have been way better off writing your own like linked list implementation using pointers basically than doing that",
    "start": "3470460",
    "end": "3476160"
  },
  {
    "text": "so I'm convinced that the linked list data structure on its own was probably a bad choice uh for us but nevertheless",
    "start": "3476160",
    "end": "3483000"
  },
  {
    "text": "the fact that the Locking overhead was so high and we observed that in our tests meant that we should probably just",
    "start": "3483000",
    "end": "3489180"
  },
  {
    "text": "stick with concurrent queue for the time being now the last thing I'm going to touch on before we run out of time here is a",
    "start": "3489180",
    "end": "3496319"
  },
  {
    "text": "brand new feature that kind of made it into.net 7 and will be available by default in.net 8 going forward which is",
    "start": "3496319",
    "end": "3503520"
  },
  {
    "text": "dynamic profile guided optimization this is a just-in-time compiler setting for net",
    "start": "3503520",
    "end": "3509099"
  },
  {
    "text": "uh this is the second big win I told you about pgo which you'll see This Acronym appear",
    "start": "3509099",
    "end": "3515579"
  },
  {
    "text": "in like your configuration and also in some of the literature on.net profile guided optimization is basically where",
    "start": "3515579",
    "end": "3521940"
  },
  {
    "text": "the compiler or sorry the just-in-time compiler analyzes how your program runs",
    "start": "3521940",
    "end": "3526980"
  },
  {
    "text": "while it's running and makes decisions about how can we speed up some of these frequently used areas of code",
    "start": "3526980",
    "end": "3533040"
  },
  {
    "text": "essentially the the just-in-time compiler will make a bet and see how it goes and if that bet results in higher",
    "start": "3533040",
    "end": "3539760"
  },
  {
    "text": "throughput it'll keep that byte code around the next time through there's two different types of profile",
    "start": "3539760",
    "end": "3546119"
  },
  {
    "text": "guided optimization there's static pgo which has been around I think this is.net five they've had some flavor of",
    "start": "3546119",
    "end": "3552240"
  },
  {
    "text": "this this analyzes the program when it's not running and makes some optimization decisions ahead of time this is that",
    "start": "3552240",
    "end": "3558119"
  },
  {
    "text": "cold start reduction you've heard about so this was designed to kind of help things like serverless functions launch",
    "start": "3558119",
    "end": "3563940"
  },
  {
    "text": "more quickly on Azure that type of thing that's what static pgo does and that's kind of a jit function that takes",
    "start": "3563940",
    "end": "3570119"
  },
  {
    "text": "runtime data and makes it available offline to help speed up the reduced cold start times but the really exciting",
    "start": "3570119",
    "end": "3576839"
  },
  {
    "text": "feature is dynamic pgo this is new in.net6 or sorry.net seven and will be",
    "start": "3576839",
    "end": "3582299"
  },
  {
    "text": "made available by turned on by default in.net 8. this is the ability for the jit to optimize your program while it's",
    "start": "3582299",
    "end": "3589380"
  },
  {
    "text": "running based on real usage patterns from live interactions with users or",
    "start": "3589380",
    "end": "3594599"
  },
  {
    "text": "other systems let's take a look at it um",
    "start": "3594599",
    "end": "3600200"
  },
  {
    "text": "the cost of turning on profile guided optimization nothing is free in the",
    "start": "3600200",
    "end": "3605579"
  },
  {
    "text": "software development industry um the cost of turning on profile guided optimization is that the amount of",
    "start": "3605579",
    "end": "3611700"
  },
  {
    "text": "resources your just-in-time compiler is going to use will probably go up by a factor of three so in this case I used a",
    "start": "3611700",
    "end": "3617579"
  },
  {
    "text": "perf view to go ahead and measure this I was able to compute the total amount of time my application spent in jit so when",
    "start": "3617579",
    "end": "3625020"
  },
  {
    "text": "I had pgo turned off I think I spent a total of 0.5 percent of CPU running the",
    "start": "3625020",
    "end": "3632819"
  },
  {
    "text": "just-in-time compiler so out of all the things my application was doing half a percent of it was just-in-time",
    "start": "3632819",
    "end": "3638099"
  },
  {
    "text": "compilation when I turned on Dynamic pgo that value jumped from 0.5 to 1.1",
    "start": "3638099",
    "end": "3644460"
  },
  {
    "text": "percent so you know more than 100 increase in just-in-time compiling time",
    "start": "3644460",
    "end": "3649740"
  },
  {
    "text": "in terms of raw milliseconds of you know computation time that used that's going",
    "start": "3649740",
    "end": "3654900"
  },
  {
    "text": "from like roughly two seconds to 3.1 seconds is sort of roughly held that sort of in terms of uh practical terms",
    "start": "3654900",
    "end": "3661680"
  },
  {
    "text": "and this is for an application that ran for many minutes so it's even though that percent increase sounds big in the",
    "start": "3661680",
    "end": "3668040"
  },
  {
    "text": "grand scheme of things it practically speaking was not noticeable but the performance impact this had in",
    "start": "3668040",
    "end": "3674460"
  },
  {
    "text": "our application was tremendous okay take us take a photograph of this Implement both of these settings in your server",
    "start": "3674460",
    "end": "3680099"
  },
  {
    "text": "apps when you get home this is the money slide this by itself is worth it's worth the time time to attend",
    "start": "3680099",
    "end": "3686280"
  },
  {
    "text": "this will turn on uh Dynamic profile guided optimization the setting above that turns on server-side garbage",
    "start": "3686280",
    "end": "3692339"
  },
  {
    "text": "collection that's the money shot um you won't need to do the tiered pgo setting anymore starting with DOT net a",
    "start": "3692339",
    "end": "3699119"
  },
  {
    "text": "it'll just be enabled by default going forward now we went ahead and benchmarked some",
    "start": "3699119",
    "end": "3705480"
  },
  {
    "text": "of our code with pgo turned on and off so we actually compared the performance of two actors uh the dotted line is our",
    "start": "3705480",
    "end": "3712859"
  },
  {
    "text": "receive actor which uses all sorts of fancy expression compiler magic to try to be fast this is like clever 2014 era",
    "start": "3712859",
    "end": "3721319"
  },
  {
    "text": "like performance optimizations whereas the untyped actor which is the solid line you can see up there uses c-sharp",
    "start": "3721319",
    "end": "3728700"
  },
  {
    "text": "pattern matching essentially just built-in language features nothing fancy at all so on.net6 the performance of",
    "start": "3728700",
    "end": "3735540"
  },
  {
    "text": "these two over let's say a large range of messages is roughly equal not there's not no real clear Divergence in",
    "start": "3735540",
    "end": "3741720"
  },
  {
    "text": "performance here and in.net 7 again the performance is roughly equal there's a little bit of a",
    "start": "3741720",
    "end": "3748319"
  },
  {
    "text": "outlier at the very end down there but we can kind of disregard that I wouldn't count that as statistically significant",
    "start": "3748319",
    "end": "3754799"
  },
  {
    "text": "but when I turn on profile guided optimization there's a really clear Trend that the untyped actor that's",
    "start": "3754799",
    "end": "3761400"
  },
  {
    "text": "using switch expressions and pattern matching versus our super duper smart",
    "start": "3761400",
    "end": "3766680"
  },
  {
    "text": "receive actor that uses delegates and caching and the expression compiler you can actually see the r code that is",
    "start": "3766680",
    "end": "3773640"
  },
  {
    "text": "simple is significantly faster than our code smart code this is because the",
    "start": "3773640",
    "end": "3780780"
  },
  {
    "text": "dynamic pgo system is able to actually interpret and understand the simple code",
    "start": "3780780",
    "end": "3786119"
  },
  {
    "text": "that we wrote and is able to make that much faster at runtime whereas our quote smart unquote optimizations and the",
    "start": "3786119",
    "end": "3794339"
  },
  {
    "text": "receive actor are things that the dynamic profile guided optimization system looks at and goes",
    "start": "3794339",
    "end": "3800940"
  },
  {
    "text": "well these dudes are pretty weird we're going to skip that code and because we don't know what we're there's too much",
    "start": "3800940",
    "end": "3806819"
  },
  {
    "text": "going on here we're not going to try to optimize it so this kind of changes The Meta for how we want to do Performance",
    "start": "3806819",
    "end": "3813359"
  },
  {
    "text": "Management and Dot net to some extent simpler constructs that use built-in language features are going to be made a",
    "start": "3813359",
    "end": "3819839"
  },
  {
    "text": "lot faster as a result of dynamic pgo and the types of optimizations this includes are things like removing",
    "start": "3819839",
    "end": "3826140"
  },
  {
    "text": "interface dispatching overhead devirtualizing methods so essentially there's less function tables and things",
    "start": "3826140",
    "end": "3833339"
  },
  {
    "text": "like that for figuring out which implementation of a method should be called and all sorts of things and we",
    "start": "3833339",
    "end": "3839040"
  },
  {
    "text": "can see an example of what the cumulative impact of that looks like on our performance this is our Benchmark",
    "start": "3839040",
    "end": "3844559"
  },
  {
    "text": "for aqued.net 1.5's in-memory message processing without pgo turned on on.net",
    "start": "3844559",
    "end": "3850319"
  },
  {
    "text": "seven and we're peeking out around 70 million messages per second here",
    "start": "3850319",
    "end": "3855359"
  },
  {
    "text": "bear in mind we're taking advantage of that prefer local API we saw earlier to",
    "start": "3855359",
    "end": "3861900"
  },
  {
    "text": "reduce context switching so we're doing about 70 million messages a second well if I turn Dynamic pgo on now we're",
    "start": "3861900",
    "end": "3869160"
  },
  {
    "text": "doing about 100 million messages per second Peak Performance and you can kind of see that around let's say yeah 40 uh",
    "start": "3869160",
    "end": "3876839"
  },
  {
    "text": "one of the throughput is set to about 40 or 90. you can see that show up so this is a free lunch for us as.net developers",
    "start": "3876839",
    "end": "3883740"
  },
  {
    "text": "all I have to do is hand over a few more CPU Cycles to the Justin Time compiler",
    "start": "3883740",
    "end": "3888960"
  },
  {
    "text": "and enable one XML setting and I get a you know 30 40 performance Improvement",
    "start": "3888960",
    "end": "3895020"
  },
  {
    "text": "that sounds like a pretty good deal um on top of that I didn't I didn't",
    "start": "3895020",
    "end": "3900480"
  },
  {
    "text": "include this Benchmark on here just in the interest of time but our remoting pipeline where we're doing Network i o is actually twice as fast what dynamic",
    "start": "3900480",
    "end": "3907680"
  },
  {
    "text": "pgo turned on so we go from about 300 000 messages a second to about six or seven hundred thousand messages a second",
    "start": "3907680",
    "end": "3914220"
  },
  {
    "text": "with Dynamic pgo enabled so this is quite the free lunch for net developers",
    "start": "3914220",
    "end": "3919380"
  },
  {
    "text": "this will be enabled by default.net 8 which should come out uh November this year probably but you can go ahead and",
    "start": "3919380",
    "end": "3925680"
  },
  {
    "text": "turn it on and start reaping some of the benefits and measuring it in your own applications in.net 7 today using that",
    "start": "3925680",
    "end": "3931619"
  },
  {
    "text": "slide I showed you so I know we just ran a few minutes over but I'll Stick Around Here For questions",
    "start": "3931619",
    "end": "3937859"
  },
  {
    "text": "for a couple minutes and then I'll make way for the next speaker and head outside but otherwise I want to thank",
    "start": "3937859",
    "end": "3943200"
  },
  {
    "text": "everyone for attending my talk today really appreciate your time and I hope you enjoyed it thank you",
    "start": "3943200",
    "end": "3948540"
  },
  {
    "text": "[Applause]",
    "start": "3948540",
    "end": "3955420"
  }
]