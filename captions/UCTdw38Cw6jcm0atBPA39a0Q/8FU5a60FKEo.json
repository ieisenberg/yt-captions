[
  {
    "text": "uh my name is Kevin pilch uh I work at Microsoft on the Azure Cosmos DB team um",
    "start": "5480",
    "end": "11280"
  },
  {
    "text": "I've worked at Microsoft for gosh uh 21 and a half years or so",
    "start": "11280",
    "end": "16760"
  },
  {
    "text": "now um I spent the first 20 years or so working on C andn Net the language and",
    "start": "16760",
    "end": "22119"
  },
  {
    "text": "the runtime and the tooling and visual studio uh aspnet core for a little while",
    "start": "22119",
    "end": "27679"
  },
  {
    "text": "um a bunch of that stuff so I'm a an engineer in manager so uh I manage a group of about 30 people um so for the",
    "start": "27679",
    "end": "34719"
  },
  {
    "text": "last year and a half I went looking for a little bit of a change um after being a a compiler guy for 20 years I decided",
    "start": "34719",
    "end": "40920"
  },
  {
    "text": "to see uh what databases are about and and switched over to work on Cosmos DB",
    "start": "40920",
    "end": "47559"
  },
  {
    "text": "uh right now I manage our developer experience team so the people who make the client side sdks that you use for",
    "start": "47559",
    "end": "54039"
  },
  {
    "text": "interacting with Cosmos TB the net one the Java one the JavaScript one the python one the go one all the different",
    "start": "54039",
    "end": "61440"
  },
  {
    "text": "languages um kind of how you interact with uh Cosmos DB this is my first time",
    "start": "61440",
    "end": "67320"
  },
  {
    "text": "giving a talk since I switch teams so this is new material for me um so please",
    "start": "67320",
    "end": "72640"
  },
  {
    "text": "uh definitely give me feedback I'd love to uh kind of hear more um about what I could do better talking about this",
    "start": "72640",
    "end": "78920"
  },
  {
    "text": "subject uh moving on let's talk well first of all how many people uh currently use Cosmos DB uh in the crowd",
    "start": "78920",
    "end": "85720"
  },
  {
    "text": "a bunch of people okay good um so this is going to talk a little bit more about",
    "start": "85720",
    "end": "90840"
  },
  {
    "text": "kind of what our backend architecture is how we uh how we make some of the guarantees that we talk about operate in",
    "start": "90840",
    "end": "98680"
  },
  {
    "text": "in practice um and so let's kind of jump in we'll start with a little bit of a",
    "start": "98680",
    "end": "103920"
  },
  {
    "text": "history of Cosmos DB um so the project started in 2010 uh it started when one of the",
    "start": "103920",
    "end": "111040"
  },
  {
    "text": "Microsoft technical fellows uh someone named Dharma Shukla uh he was looking at",
    "start": "111040",
    "end": "116840"
  },
  {
    "text": "how to build a cloud native multitenant shared nothing database from the ground up uh one of the things that is a little",
    "start": "116840",
    "end": "124200"
  },
  {
    "text": "piece of trivia you might not know uh he built an initial proof of concept of some of this while he was on vacation in",
    "start": "124200",
    "end": "130360"
  },
  {
    "text": "Florence and so for a long time uh the project was codenamed Project Florence",
    "start": "130360",
    "end": "135560"
  },
  {
    "text": "internally at Microsoft um so like I said it got started in 2010 by 2014 it was already",
    "start": "135560",
    "end": "143040"
  },
  {
    "text": "in production powering critical services within Azure and across the rest of Microsoft um but as we got closer to",
    "start": "143040",
    "end": "149519"
  },
  {
    "text": "launching it for public availability in 2017 uh we decided that we needed a",
    "start": "149519",
    "end": "154959"
  },
  {
    "text": "better name than document DB which is what it have been called internally and why you still see a bunch of stuff in",
    "start": "154959",
    "end": "160519"
  },
  {
    "text": "the resource provider and and stuff like that called document DB uh so we decided to to rename it to Cosmos DB to kind of",
    "start": "160519",
    "end": "166920"
  },
  {
    "text": "reflect what our Ambitions are uh there and uh another fun piece of trivia on",
    "start": "166920",
    "end": "173440"
  },
  {
    "text": "this uh it turns out that there is already an existing product for capturing Telemetry inside Microsoft",
    "start": "173440",
    "end": "179239"
  },
  {
    "text": "that has been coded named Cosmos for years and years and years and so there is plenty of confusion inside Microsoft",
    "start": "179239",
    "end": "187040"
  },
  {
    "text": "about whether people are talking about Cosmos or Cosmos DB um one is the distributed database and one is the",
    "start": "187040",
    "end": "193760"
  },
  {
    "text": "Telemetry uh uh data warehouse anyway um so yeah we went public in 2017 and then",
    "start": "193760",
    "end": "200680"
  },
  {
    "text": "we added some multi- API support for Gremlin and Cassandra in 2018 um and so",
    "start": "200680",
    "end": "206000"
  },
  {
    "text": "we're going to talk a little bit about how all those things come together um you all use Cosmos DB so most of the",
    "start": "206000",
    "end": "212680"
  },
  {
    "text": "rest is probably well known to you um but there's a bunch of kind of features of Cosmos DB that we tend to think about",
    "start": "212680",
    "end": "219799"
  },
  {
    "text": "as falling into three different buckets right we have this kind of core extensible feature set that talks about",
    "start": "219799",
    "end": "226599"
  },
  {
    "text": "multi- API schema free uh automatic indexing you know we have a rich query",
    "start": "226599",
    "end": "232120"
  },
  {
    "text": "language for things all those sorts of things are kind of a core feature set and then we have really kind of the",
    "start": "232120",
    "end": "237439"
  },
  {
    "text": "fundamentals the scalability the performance the high availability right we support multi- region multi- right",
    "start": "237439",
    "end": "244280"
  },
  {
    "text": "systems that you know can have up to 5 NES of availability we support low",
    "start": "244280",
    "end": "249519"
  },
  {
    "text": "latency with financially backed slas on what that means for Point reads and and",
    "start": "249519",
    "end": "254760"
  },
  {
    "text": "that sort of thing and finally we have a bunch of Enterprise ready features things like uh you know well one is kind",
    "start": "254760",
    "end": "263040"
  },
  {
    "text": "of a a demonstration of Faith right we're available we're considered a ring zero Azure service we're available in",
    "start": "263040",
    "end": "269880"
  },
  {
    "text": "every region in Azure tons of other Microsoft Services depend on Cosmos DB which kind of can give you some",
    "start": "269880",
    "end": "276479"
  },
  {
    "text": "confidence that it's a a pretty reliable service um and then specific features we have things like continuous backup uh",
    "start": "276479",
    "end": "283520"
  },
  {
    "text": "with point in time restores um you can do private links Network isolation you",
    "start": "283520",
    "end": "290280"
  },
  {
    "text": "can encrypt we always encrypt all of your data at rest uh with a system managed key but you can also bring your",
    "start": "290280",
    "end": "296600"
  },
  {
    "text": "own key so that you know um we don't have access to what the key is so that",
    "start": "296600",
    "end": "302160"
  },
  {
    "text": "everything is always encrypted and things like ro-based access control and and so on so I've kind of mentioned",
    "start": "302160",
    "end": "308479"
  },
  {
    "text": "already some Microsoft services using Cosmos DB but who else uses it and what",
    "start": "308479",
    "end": "313840"
  },
  {
    "text": "are the what scale kind of like um here's an example of some of the customers that we have some people who",
    "start": "313840",
    "end": "320360"
  },
  {
    "text": "are using Cosmos DB at scale you might have heard of some of those companies down there and some of the products",
    "start": "320360",
    "end": "325840"
  },
  {
    "text": "within Microsoft so uh Microsoft inra Microsoft or Azure DNS Microsoft teams",
    "start": "325840",
    "end": "333800"
  },
  {
    "text": "Etc um companies like Walmart uh chat gpt's chat history is kind of stored in",
    "start": "333800",
    "end": "339479"
  },
  {
    "text": "Cosmos DB now um talking about scale you know that",
    "start": "339479",
    "end": "345680"
  },
  {
    "text": "second bullet point um we have a single customer who operates at the scale of",
    "start": "345680",
    "end": "352720"
  },
  {
    "text": "100 million requests per second over pedabytes of storage with their data available in over 40 regions right and",
    "start": "352720",
    "end": "360000"
  },
  {
    "text": "so you know that's one of our customers it's not kind of the entirety of the the service we provide so how do we go about",
    "start": "360000",
    "end": "367599"
  },
  {
    "text": "doing that um let's talk first about kind of a very high level architecture",
    "start": "367599",
    "end": "374960"
  },
  {
    "text": "so how do different requests operate in Cosmos DB this is uh you know a pretty",
    "start": "374960",
    "end": "381120"
  },
  {
    "text": "simple diagram but you know we have a bunch of backend storage replicas",
    "start": "381120",
    "end": "386720"
  },
  {
    "text": "storage nodes we call them right and in between that and the customer there's",
    "start": "386720",
    "end": "392240"
  },
  {
    "text": "kind of the Azure Cloud where typically you're running VMS or uh containers or",
    "start": "392240",
    "end": "398599"
  },
  {
    "text": "kubernetes pods or or whatever and from there typically you reach out and you",
    "start": "398599",
    "end": "403880"
  },
  {
    "text": "talk to this Cosmos DB API Gateway and the API Gateway sits",
    "start": "403880",
    "end": "409960"
  },
  {
    "text": "between uh your application and the backend storage nodes and that's really",
    "start": "409960",
    "end": "415360"
  },
  {
    "text": "one of the ways that we leverage or or that we enable this multi API support",
    "start": "415360",
    "end": "421319"
  },
  {
    "text": "the Gateway is able to build up a query do the translation from that the API",
    "start": "421319",
    "end": "428520"
  },
  {
    "text": "specific semantics to our backend semantics distribute the query across all the backend nodes aggregate the",
    "start": "428520",
    "end": "435120"
  },
  {
    "text": "results and finally return uh those results to the user we uh we'll talk a little bit more",
    "start": "435120",
    "end": "442199"
  },
  {
    "text": "about what that Gateway looks like later but first let's dive into kind of what the backend storage engines look like",
    "start": "442199",
    "end": "448080"
  },
  {
    "text": "and how they provide some of those avail a ability guarantees that we talked",
    "start": "448080",
    "end": "453199"
  },
  {
    "text": "about so again probably review for most of you who already use Cosmos DB um but",
    "start": "453680",
    "end": "458919"
  },
  {
    "text": "let's talk about the resource hierarchy a little bit right we have the idea of an account uh this is like a and then",
    "start": "458919",
    "end": "465919"
  },
  {
    "text": "within an account you can have multiple databases and within a database you can have multiple",
    "start": "465919",
    "end": "471400"
  },
  {
    "text": "containers containers are provisioned with the idea of these resource units",
    "start": "471400",
    "end": "476960"
  },
  {
    "text": "right this is how you decide how much you want to pay us um and how much",
    "start": "476960",
    "end": "482919"
  },
  {
    "text": "traffic you think you're going to have it's also how we know uh how much capacity to reserve for you so that we",
    "start": "482919",
    "end": "488319"
  },
  {
    "text": "can ensure that we meet those those resources",
    "start": "488319",
    "end": "493800"
  },
  {
    "text": "um and then uh kind of each operation that you do consumes some of those",
    "start": "493800",
    "end": "498960"
  },
  {
    "text": "resources from the container and finally the way that we do horizontal scaling is through this idea",
    "start": "498960",
    "end": "506720"
  },
  {
    "text": "of partitioning so we partition your container uh through the this idea of a partition",
    "start": "506720",
    "end": "512479"
  },
  {
    "text": "key Like A Shard key and in most other things um and they can be either a",
    "start": "512479",
    "end": "519518"
  },
  {
    "text": "single partition or they can be hierarchical or they can be composite for things like the Cassandra API let's",
    "start": "519519",
    "end": "525279"
  },
  {
    "text": "talk a little bit about kind of what one of those might look like so there's this idea of a difference",
    "start": "525279",
    "end": "532920"
  },
  {
    "text": "between a logical partition and a physical partition so when we're talking about the backend storage we often talk",
    "start": "532920",
    "end": "540240"
  },
  {
    "text": "about physical partitions although in a lot of ways they're kind of an implementation detail or they're supposed to be an implementation detail",
    "start": "540240",
    "end": "546680"
  },
  {
    "text": "as far as uh customers are concerned but a lot of times kind of the performance",
    "start": "546680",
    "end": "552040"
  },
  {
    "text": "implications of multiple physical partitions can come to bear in your application and and so we'll talk about",
    "start": "552040",
    "end": "557800"
  },
  {
    "text": "that a little bit so let's talk about let's imagine you know we have a we have a partition key that is based on the",
    "start": "557800",
    "end": "563920"
  },
  {
    "text": "city property and from that we can route data to different",
    "start": "563920",
    "end": "570000"
  },
  {
    "text": "backend storage nodes and so this has an example where Seattle and London uh they're different logical partitions",
    "start": "570000",
    "end": "577200"
  },
  {
    "text": "right they have different values of the partition key but they might get routed to the same physical partition they",
    "start": "577200",
    "end": "583279"
  },
  {
    "text": "might end up on the same actual backend storage",
    "start": "583279",
    "end": "587800"
  },
  {
    "text": "partition uh partitioning it's great for horizontal scale right it's how we actually are",
    "start": "591440",
    "end": "597240"
  },
  {
    "text": "able to distribute this data and provide these guarantees but it comes with some",
    "start": "597240",
    "end": "602680"
  },
  {
    "text": "uh complications right uh it in data modeling it's important to do a good job",
    "start": "602680",
    "end": "608320"
  },
  {
    "text": "of figuring out what your partition key is going to be you can run into problems like uh what we call Hot partitions",
    "start": "608320",
    "end": "615360"
  },
  {
    "text": "right where uh most of your reads and writes all go to the same partition key",
    "start": "615360",
    "end": "620760"
  },
  {
    "text": "which means that even though you you provision a bunch of capacity that capacity is kind of evenly distributed",
    "start": "620760",
    "end": "627560"
  },
  {
    "text": "among each of your partitions and so if everything ends up going to the same partition anyways then you have a hard",
    "start": "627560",
    "end": "634200"
  },
  {
    "text": "time scaling on the reverse side a lot of people get around that they say well great I'll use you know a guid or a",
    "start": "634200",
    "end": "641440"
  },
  {
    "text": "random number or something like that for my partition key you get good distribution of your data across all of",
    "start": "641440",
    "end": "647360"
  },
  {
    "text": "your partitions but if you want to use the rich querying facilities that we have that means that that query then has",
    "start": "647360",
    "end": "654480"
  },
  {
    "text": "to distribute across every single one of those partitions and collect the results and so there often uh you know one of",
    "start": "654480",
    "end": "661160"
  },
  {
    "text": "the the challenging Parts about working with a a horizontally scalable system like this is doing a good job of picking",
    "start": "661160",
    "end": "667959"
  },
  {
    "text": "the partition key we have some features to help alleviate some of these so we have like the this idea of Hier",
    "start": "667959",
    "end": "674920"
  },
  {
    "text": "hierarchical partitioning can help with some of this uh hot hot partition uh",
    "start": "674920",
    "end": "681120"
  },
  {
    "text": "actually it can help with hot partitions it can also help by trying to group uh",
    "start": "681120",
    "end": "688760"
  },
  {
    "text": "related different partitions together in the same physical partition so that those cross partition queries are a little bit",
    "start": "688760",
    "end": "695480"
  },
  {
    "text": "less expensive so if this is something that you've been running into I would suggest taking a look at uh at this idea",
    "start": "695480",
    "end": "702079"
  },
  {
    "text": "of hierarchical partitioning moving on um let's talk a little bit about replicas and partitions",
    "start": "702079",
    "end": "708680"
  },
  {
    "text": "I've talked a little bit about it so far but this idea we the terminology that we",
    "start": "708680",
    "end": "715399"
  },
  {
    "text": "use all the time replica sets and partition sets replicas and partitions what's kind of the difference what do they look like well a replica set is the",
    "start": "715399",
    "end": "724560"
  },
  {
    "text": "set of replicas for a given piece of data within a single region and so",
    "start": "724560",
    "end": "730920"
  },
  {
    "text": "typically uh in Cosmos DB we'll have four replicas of every piece of data that you store and we're going to get",
    "start": "730920",
    "end": "736959"
  },
  {
    "text": "into the details of how that's laid out a little bit uh in the next slide um but kind of each region each partition of",
    "start": "736959",
    "end": "745040"
  },
  {
    "text": "your data each piece of your data is stored in four replicas one of those on",
    "start": "745040",
    "end": "750720"
  },
  {
    "text": "the slide it's called leader uh sometimes we call it primary sometimes we call it right um but basically each",
    "start": "750720",
    "end": "757800"
  },
  {
    "text": "replica set has one copy that is the the Master Copy where all the rights happen",
    "start": "757800",
    "end": "764639"
  },
  {
    "text": "and then it has a variety of secondary uh replicas or for or followers or uh",
    "start": "764639",
    "end": "771040"
  },
  {
    "text": "read replicas we call them and then if you have a multi- region",
    "start": "771040",
    "end": "776160"
  },
  {
    "text": "account that data is also replicated in each different region and So within",
    "start": "776160",
    "end": "782000"
  },
  {
    "text": "each region you have kind of one of those copies of a replica set so you'll have four independent stored copies of",
    "start": "782000",
    "end": "789079"
  },
  {
    "text": "your data in each region that the data lives in and so let's dig into that a",
    "start": "789079",
    "end": "795680"
  },
  {
    "text": "little bit more so this is kind of a a",
    "start": "795680",
    "end": "801079"
  },
  {
    "text": "representation of what this looks like inside a data center for us um we have",
    "start": "801079",
    "end": "806279"
  },
  {
    "text": "this idea of fault domains in Azure and so it's the idea that within a data",
    "start": "806279",
    "end": "812040"
  },
  {
    "text": "center um something can happen to one machine",
    "start": "812040",
    "end": "817880"
  },
  {
    "text": "it shouldn't necessarily impact other machines within the same Data Center and so typically this is stuff like uh",
    "start": "817880",
    "end": "824440"
  },
  {
    "text": "machines are guaranteed to be in different racks or on different Power Systems or that sort of thing and so uh",
    "start": "824440",
    "end": "831000"
  },
  {
    "text": "one of the things that we do is we distribute each of those replicas across different fault",
    "start": "831000",
    "end": "836639"
  },
  {
    "text": "domains and each machine can host multiple customer pieces of",
    "start": "836639",
    "end": "842480"
  },
  {
    "text": "data and so the right example there or or the left example with the red arrow",
    "start": "842480",
    "end": "848600"
  },
  {
    "text": "you can imagine partition one for one customer is spread across multiple different fault domains right and then",
    "start": "848600",
    "end": "855680"
  },
  {
    "text": "you can imagine uh for a given customer the different partitions are sped among",
    "start": "855680",
    "end": "862560"
  },
  {
    "text": "different clusters that's kind of spreading among different clusters is how we get the the",
    "start": "862560",
    "end": "870199"
  },
  {
    "text": "elasticity and the scale and the spreading across different fault domains for a given uh rep partition for a given",
    "start": "870199",
    "end": "878199"
  },
  {
    "text": "set of replicas is how we get some of the durability and uh and availability",
    "start": "878199",
    "end": "883480"
  },
  {
    "text": "that we talk about one thing that's kind of an additional Point here is you can",
    "start": "883480",
    "end": "888560"
  },
  {
    "text": "get even better availability with the idea of availability zones which is kind of a relatively New Concept in Azure um",
    "start": "888560",
    "end": "895519"
  },
  {
    "text": "and this is the idea of like an even more uh independent sub region within a",
    "start": "895519",
    "end": "902600"
  },
  {
    "text": "region so you can imagine kind of two buildings across the street from each other might be different availability",
    "start": "902600",
    "end": "908199"
  },
  {
    "text": "zones and so even if something happens to one of the buildings uh if your data",
    "start": "908199",
    "end": "913399"
  },
  {
    "text": "is replicated across the two availability zones if some of your replicas are in each of the two different availability zones you can",
    "start": "913399",
    "end": "920040"
  },
  {
    "text": "still be available even though part of that region has gone down I'm not going to talk much more anymore about",
    "start": "920040",
    "end": "926920"
  },
  {
    "text": "availability zones but that's something else to to kind of think about there any questions so far am I going",
    "start": "926920",
    "end": "935160"
  },
  {
    "text": "too fast too slow is it making sense",
    "start": "935160",
    "end": "939879"
  },
  {
    "text": "y sing Everything yeah we're gonna talk",
    "start": "944639",
    "end": "950319"
  },
  {
    "text": "about that in a little bit um yeah all right um let's look next at",
    "start": "950319",
    "end": "957560"
  },
  {
    "text": "kind of what a what the request flow looks like for a document WR how do we preserve kind of the semantics that you",
    "start": "957560",
    "end": "963959"
  },
  {
    "text": "think about what what's inside one of these replicas so a single backend replica um",
    "start": "963959",
    "end": "971040"
  },
  {
    "text": "actually let's talk about the the kind of contents of a backend replica what do we have in there so we have um a b tree",
    "start": "971040",
    "end": "978920"
  },
  {
    "text": "we have an inverted index it's uh this is kind of used for query terms the index query terms that's a B+ tree",
    "start": "978920",
    "end": "985920"
  },
  {
    "text": "there's um a really interesting paper about how that indexing works there's a",
    "start": "985920",
    "end": "991720"
  },
  {
    "text": "link there um I'll copy it to the resources section at the end um honestly",
    "start": "991720",
    "end": "997839"
  },
  {
    "text": "uh kind of some of the lock free code for building that inverted index um you someone could probably not me because",
    "start": "997839",
    "end": "1004480"
  },
  {
    "text": "I'm not an expert in it but someone could probably do an hourong talk in kind of just how that b tree uh B+ tree",
    "start": "1004480",
    "end": "1011120"
  },
  {
    "text": "inverted index works but it needless to say it's kind of an important part of what powers our uh indexing and and",
    "start": "1011120",
    "end": "1018319"
  },
  {
    "text": "query abil within Cosmos DB then we have the query engine um this is kind of uh you know",
    "start": "1018319",
    "end": "1025640"
  },
  {
    "text": "going back to my compiler days uh you write a query in our our nosql language it gets translated into query iil the",
    "start": "1025640",
    "end": "1032880"
  },
  {
    "text": "query engine has a little virtual machine that knows how to interpret that iil and uh filter the results and do",
    "start": "1032880",
    "end": "1039558"
  },
  {
    "text": "projections and all of that stuff um and so that lives in each one of our back backend replicas and then finally we",
    "start": "1039559",
    "end": "1046640"
  },
  {
    "text": "have a replication CU this is like a log of rights that happen and we'll talk about what that's used for in a second",
    "start": "1046640",
    "end": "1054039"
  },
  {
    "text": "so what does it look like when you write a single document uh what happens",
    "start": "1054039",
    "end": "1061440"
  },
  {
    "text": "so the so you've got the secondary replicas right and a primary replica so",
    "start": "1062160",
    "end": "1067880"
  },
  {
    "text": "we talked about this idea of primary secondary also write replica read replicas so writing a",
    "start": "1067880",
    "end": "1075200"
  },
  {
    "text": "document the first thing that happens is the primary replica kind of collects a a",
    "start": "1075200",
    "end": "1084440"
  },
  {
    "text": "package of data about that gets ready to commit it but it doesn't actually commit",
    "start": "1084440",
    "end": "1089480"
  },
  {
    "text": "it into its storage yet first it packages all that up and it distributes",
    "start": "1089480",
    "end": "1096799"
  },
  {
    "text": "it to each of its secondary replicas concurrently each of the secondary",
    "start": "1096799",
    "end": "1102240"
  },
  {
    "text": "replicas takes that commits it to its storage its index and its replication q",
    "start": "1102240",
    "end": "1110159"
  },
  {
    "text": "and then acknowledges the result back meanwhile the primary replica the the",
    "start": "1110159",
    "end": "1116240"
  },
  {
    "text": "right replica Waits until it has a quorum of the read replicas acknowledged",
    "start": "1116240",
    "end": "1123280"
  },
  {
    "text": "before it actually commits the transaction and so this way knowing that one is that that this is the primary",
    "start": "1123280",
    "end": "1131400"
  },
  {
    "text": "replica means that it doesn't actually reflect the results until it we know",
    "start": "1131400",
    "end": "1137159"
  },
  {
    "text": "that at least two other replica have stored that data and acknowledged it back and then it commits and returns to",
    "start": "1137159",
    "end": "1144520"
  },
  {
    "text": "the user the replication cue is there for the cases where you know that third replica didn't actually acknowledge the",
    "start": "1144520",
    "end": "1151600"
  },
  {
    "text": "right the replication cue can be used to get that other replica up to date later on but this is kind of how we get some",
    "start": "1151600",
    "end": "1158880"
  },
  {
    "text": "of those uh uh consistent results across rights because you're guaranteed that",
    "start": "1158880",
    "end": "1167039"
  },
  {
    "text": "before a control returns to to the user for a right it's present in the right",
    "start": "1167039",
    "end": "1172679"
  },
  {
    "text": "region or the right replica and at least two of the read replicas so three out of",
    "start": "1172679",
    "end": "1178000"
  },
  {
    "text": "the four replicas are guaranteed to have the data committed to disk flush to disk",
    "start": "1178000",
    "end": "1183120"
  },
  {
    "text": "before that right document ends up returning make",
    "start": "1183120",
    "end": "1188880"
  },
  {
    "text": "sense all right uh so this is the two replies coming back then we write flush",
    "start": "1188880",
    "end": "1195720"
  },
  {
    "text": "I always forget that I have a builder slide for this then we write commit everything on the uh on the right",
    "start": "1195720",
    "end": "1202960"
  },
  {
    "text": "replica all right so then what happens if we're talking about multiple regions right that was all for within a single",
    "start": "1202960",
    "end": "1209159"
  },
  {
    "text": "region there's four replicas in a single region so what happens when I have multiple",
    "start": "1209159",
    "end": "1215240"
  },
  {
    "text": "regions well the first thing that happens is the",
    "start": "1215240",
    "end": "1220960"
  },
  {
    "text": "same thing that happened in the last case right client issues a right the",
    "start": "1220960",
    "end": "1226000"
  },
  {
    "text": "right goes to the primary the primary issues it to each of the",
    "start": "1226000",
    "end": "1231280"
  },
  {
    "text": "secondaries one of the secondaries is designated as the forwarder or the the",
    "start": "1231280",
    "end": "1239440"
  },
  {
    "text": "replication uh secondary and so that one secondary on your behalf issues a right",
    "start": "1239440",
    "end": "1247320"
  },
  {
    "text": "to the other region and",
    "start": "1247320",
    "end": "1252760"
  },
  {
    "text": "then because one of the features that that Cosmos DB supports is the idea of",
    "start": "1252760",
    "end": "1258679"
  },
  {
    "text": "multiple right regions of course the same thing can happen in the other region so a client in region B can do",
    "start": "1258679",
    "end": "1267679"
  },
  {
    "text": "the same thing and of course the whole process Flows In Reverse",
    "start": "1267679",
    "end": "1274760"
  },
  {
    "text": "yes the primary will not commit it if it doesn't yeah that's correct",
    "start": "1282240",
    "end": "1288679"
  },
  {
    "text": "doesn't what would be wrong if it did try to commit it and then try to again",
    "start": "1289520",
    "end": "1295840"
  },
  {
    "text": "um the the problem is if the so I first I'll repete the question um what happens",
    "start": "1295840",
    "end": "1303000"
  },
  {
    "text": "if two of the the read replicas don't acknowledge that they've written a right",
    "start": "1303000",
    "end": "1308720"
  },
  {
    "text": "um so let's go back over here um great question so what happens",
    "start": "1308720",
    "end": "1314240"
  },
  {
    "text": "if two if if only one acknowledges the right or zero of them acknowledge the",
    "start": "1314240",
    "end": "1319440"
  },
  {
    "text": "right well if you return to the customer and say I've written that data fine what happens if the reason why",
    "start": "1319440",
    "end": "1328120"
  },
  {
    "text": "those two didn't acknowledge the right is because something is happening and the data and power is failing in that",
    "start": "1328120",
    "end": "1334760"
  },
  {
    "text": "data center and so those two machines are down your idea is well you know we've got the replication log we'll hold",
    "start": "1334760",
    "end": "1341240"
  },
  {
    "text": "on to it and and we'll catch them up later well what if we crash at exactly",
    "start": "1341240",
    "end": "1346679"
  },
  {
    "text": "that point as well right right then you've told the customer you've returned",
    "start": "1346679",
    "end": "1352120"
  },
  {
    "text": "to the to to the customer hey I've written your data it's safe you can trust me um but it's actually not on any",
    "start": "1352120",
    "end": "1359200"
  },
  {
    "text": "of the machines that exist right and so when power comes back up that data is",
    "start": "1359200",
    "end": "1364480"
  },
  {
    "text": "gone right and so this idea of having a quorum having it be on multiple uh machines flush to dis is one of the ways",
    "start": "1364480",
    "end": "1372120"
  },
  {
    "text": "that we provide the the consistency guarantees that I was talking about does that make sense great",
    "start": "1372120",
    "end": "1379159"
  },
  {
    "text": "okay so multi- region rights things can go in",
    "start": "1379159",
    "end": "1384919"
  },
  {
    "text": "reverse two people can write the same document turns out getting time right",
    "start": "1384919",
    "end": "1391840"
  },
  {
    "text": "across the internet uh you know if we didn't allow for multiple people to write the same document across the",
    "start": "1391840",
    "end": "1398600"
  },
  {
    "text": "internet that wouldn't be a very useful system but what this can mean is that as",
    "start": "1398600",
    "end": "1404400"
  },
  {
    "text": "that secondary is replicating the data back to region a someone else may have",
    "start": "1404400",
    "end": "1410600"
  },
  {
    "text": "already written the data in region a and so what do we do in that",
    "start": "1410600",
    "end": "1416799"
  },
  {
    "text": "case right somebody says we do conflict resolution right so we have this idea of",
    "start": "1417400",
    "end": "1423400"
  },
  {
    "text": "uh conflict resolution we have tunable ways we can do that uh each document has the has a time stamp uh fi within it and",
    "start": "1423400",
    "end": "1431760"
  },
  {
    "text": "so you can say whoever wrote to it last wins um you can populate a stored",
    "start": "1431760",
    "end": "1438120"
  },
  {
    "text": "procedure to run whenever a conf conflict happens that can decide how to how to deal with the results uh you can",
    "start": "1438120",
    "end": "1445520"
  },
  {
    "text": "have it pop into a que to be dealt with later or uh you know there's some API",
    "start": "1445520",
    "end": "1451039"
  },
  {
    "text": "specific things Cassandra does like per property conflict resolution with time stamps per property and and that sort of",
    "start": "1451039",
    "end": "1457240"
  },
  {
    "text": "thing so there's kind of a variety of different ways uh but it is something you have to think about if you're going",
    "start": "1457240",
    "end": "1462600"
  },
  {
    "text": "to have a multi- region multi- right region account often this doesn't come up very",
    "start": "1462600",
    "end": "1469760"
  },
  {
    "text": "much because a lot of the time uh people kind of focus the rights on the nearest",
    "start": "1469760",
    "end": "1478279"
  },
  {
    "text": "region to where the customer is um and so often like if you're if your",
    "start": "1478279",
    "end": "1483880"
  },
  {
    "text": "partition key is by customer or by region or something like that then you're unlikely to have you know",
    "start": "1483880",
    "end": "1489919"
  },
  {
    "text": "multiple people in different regions writing to the same document but it is something that we have to be prepared to",
    "start": "1489919",
    "end": "1495480"
  },
  {
    "text": "deal with",
    "start": "1495480",
    "end": "1498799"
  },
  {
    "text": "so those are rights every right that happens in Cosmos DB is consistently",
    "start": "1502760",
    "end": "1509080"
  },
  {
    "text": "written to the database before it's acknowledged before you get a success back from the API call but that's not",
    "start": "1509080",
    "end": "1516399"
  },
  {
    "text": "necessarily true of reads for reads we offer five variations",
    "start": "1516399",
    "end": "1524880"
  },
  {
    "text": "of consistency models starting so first of all um this",
    "start": "1524880",
    "end": "1530279"
  },
  {
    "text": "is you can set a default for each account each container um and then on",
    "start": "1530279",
    "end": "1535520"
  },
  {
    "text": "every read request you can choose to have a different one if you need to for that particular read request there's a",
    "start": "1535520",
    "end": "1541320"
  },
  {
    "text": "tradeoff in these between latency availability and throughput right uh you",
    "start": "1541320",
    "end": "1547000"
  },
  {
    "text": "know latency is a great example if I say that I want globally strongly consistent",
    "start": "1547000",
    "end": "1553120"
  },
  {
    "text": "reads then that means every time I issue a read a write I can't read from that",
    "start": "1553120",
    "end": "1559679"
  },
  {
    "text": "database until that has been replicated to all the other regions that are part of my account and",
    "start": "1559679",
    "end": "1566480"
  },
  {
    "text": "all those other regions have confirmed back to the to the region that you're doing the read from that everything is",
    "start": "1566480",
    "end": "1571880"
  },
  {
    "text": "there right so that obviously implies at least a round cross region round trip as",
    "start": "1571880",
    "end": "1578600"
  },
  {
    "text": "part of your latency for every read that you do right um and so like I said so",
    "start": "1578600",
    "end": "1584640"
  },
  {
    "text": "moving from the the left of the slide globally strong means every right shows",
    "start": "1584640",
    "end": "1589760"
  },
  {
    "text": "up in every read across the world right and then you can get down to bounded",
    "start": "1589760",
    "end": "1596039"
  },
  {
    "text": "staleness which is uh typically the best way to think of it is",
    "start": "1596039",
    "end": "1603760"
  },
  {
    "text": "it's consistent within a region right so within a region every time you write",
    "start": "1603760",
    "end": "1608840"
  },
  {
    "text": "something every read after that is going to see it and there's kind of this the",
    "start": "1608840",
    "end": "1614279"
  },
  {
    "text": "bounded part of it is and don't let me do any more rights if the replication to",
    "start": "1614279",
    "end": "1620640"
  },
  {
    "text": "other regions have fallen too far behind right if it takes more than 10 minutes",
    "start": "1620640",
    "end": "1625960"
  },
  {
    "text": "to replicate to other regions then maybe slow down my ability to write to this region so that I don't get too far out",
    "start": "1625960",
    "end": "1632760"
  },
  {
    "text": "of date and have an issue if this region goes down where I have lost data again that that sort of thing right",
    "start": "1632760",
    "end": "1640120"
  },
  {
    "text": "um session this is kind of our most common this is the default consistency",
    "start": "1640120",
    "end": "1645880"
  },
  {
    "text": "level uh this is the idea that if I on my machine write something and then I",
    "start": "1645880",
    "end": "1653039"
  },
  {
    "text": "issue a read I can see the thing I just read wrote right that's pretty convenient it's pretty hard to deal with",
    "start": "1653039",
    "end": "1660159"
  },
  {
    "text": "some of the the ones to the right around eventual consistency if you have this model where you need to be able to to",
    "start": "1660159",
    "end": "1667080"
  },
  {
    "text": "look up something after you wrote it right uh if you can't guarantee that you see it right after you wrote it it's a",
    "start": "1667080",
    "end": "1673840"
  },
  {
    "text": "programming problem that is hard for a lot of people to wrap their head around unless you're doing you know queuing",
    "start": "1673840",
    "end": "1679600"
  },
  {
    "text": "typee things where you actually don't need to write it from the same machine as long as it shows up at some point",
    "start": "1679600",
    "end": "1685519"
  },
  {
    "text": "right and so there's different use cases for all these different consistency levels um and like and like I said it's",
    "start": "1685519",
    "end": "1691760"
  },
  {
    "text": "kind of a trade-off among your latency your availability your throughput the one I didn't put on the slide is uh like",
    "start": "1691760",
    "end": "1698279"
  },
  {
    "text": "I said your your uh com comprehend ability of the the problem space right",
    "start": "1698279",
    "end": "1705399"
  },
  {
    "text": "uh globally strong is easy to understand it's like the way we tend to think about databases a lot of the time right once I",
    "start": "1705399",
    "end": "1710760"
  },
  {
    "text": "write something to the database it's there every read it shows up but it's pretty expensive and so for a lot of uh",
    "start": "1710760",
    "end": "1718320"
  },
  {
    "text": "problem spaces we want to be able to slide down further down to the the right on this scale of of",
    "start": "1718320",
    "end": "1725360"
  },
  {
    "text": "consistency um just wanted to highlight",
    "start": "1725360",
    "end": "1731320"
  },
  {
    "text": "again this has nothing to do with rights right like a right to a given",
    "start": "1731320",
    "end": "1739760"
  },
  {
    "text": "replica is entirely consistent before it returns to the the customer right does",
    "start": "1739760",
    "end": "1746279"
  },
  {
    "text": "that make sense okay I said entirely consistent but",
    "start": "1746279",
    "end": "1752760"
  },
  {
    "text": "that's actually not necessarily true uh it's only it's consistent with at least",
    "start": "1752760",
    "end": "1757840"
  },
  {
    "text": "two of the reads right so three of the four replicas are consistent but that's how you can get eventual consistency",
    "start": "1757840",
    "end": "1764600"
  },
  {
    "text": "where even if you read from one of the secondaries you may not see the right",
    "start": "1764600",
    "end": "1770080"
  },
  {
    "text": "that you just did so let's talk about this a little bit um reads with tunable consistency how was this implemented on",
    "start": "1770080",
    "end": "1777480"
  },
  {
    "text": "our side um it's not super complicated",
    "start": "1777480",
    "end": "1783399"
  },
  {
    "text": "right if you do uh an eventual or session consistency",
    "start": "1783399",
    "end": "1788880"
  },
  {
    "text": "read we pick a read replica at random it's not quite at",
    "start": "1788880",
    "end": "1795039"
  },
  {
    "text": "random but close enough um and we assure a read to it and we give you the data that's there uh we check the session",
    "start": "1795039",
    "end": "1802120"
  },
  {
    "text": "token if in session consistency but basically we read from one of your read",
    "start": "1802120",
    "end": "1810080"
  },
  {
    "text": "replicas on the other hand if you do bounded stenness or",
    "start": "1810080",
    "end": "1815760"
  },
  {
    "text": "strong we pick any two and we only return the result if",
    "start": "1815760",
    "end": "1823399"
  },
  {
    "text": "there's a quorum between those two if they agree on what the value that you're trying to read is because we know that a",
    "start": "1823399",
    "end": "1832960"
  },
  {
    "text": "right is guaranteed to be in at least two of the three replica WR read replicas before",
    "start": "1832960",
    "end": "1839120"
  },
  {
    "text": "the right finishes we know that if we read from two of those read",
    "start": "1839120",
    "end": "1844399"
  },
  {
    "text": "replicas and they agree on the result that they have the latest",
    "start": "1844399",
    "end": "1850360"
  },
  {
    "text": "result if they don't agree then maybe we got one that is behind right and so then",
    "start": "1850360",
    "end": "1856960"
  },
  {
    "text": "what my team does and the SDK is it might have to reach out to the third replica and ask it hey have you seen",
    "start": "1856960",
    "end": "1863440"
  },
  {
    "text": "this or not right and so this is how the SDK handles Quorum",
    "start": "1863440",
    "end": "1869559"
  },
  {
    "text": "right questions does this make sense yep does mean that the",
    "start": "1870360",
    "end": "1877639"
  },
  {
    "text": "founded consistency is going to be more expensive in terms of yes so uh again question is does this",
    "start": "1877639",
    "end": "1887039"
  },
  {
    "text": "mean that uh um Global strong or bounded stess is going to be more expensive in",
    "start": "1887039",
    "end": "1892279"
  },
  {
    "text": "terms of our use um the answer is yes it will you know we had say typically a",
    "start": "1892279",
    "end": "1898840"
  },
  {
    "text": "point read of a 1K document is one Ru um a point read in strong consistency is",
    "start": "1898840",
    "end": "1905399"
  },
  {
    "text": "two to three Rus U",
    "start": "1905399",
    "end": "1909799"
  },
  {
    "text": "definitely any other questions all right we're moving along",
    "start": "1912039",
    "end": "1919240"
  },
  {
    "text": "um ensuring High availability so this is going to get into some of those upgrade questions",
    "start": "1919880",
    "end": "1926360"
  },
  {
    "text": "kind of what happens uh uh as we're doing upgrades but not just what happens as we're doing upgrades what happens",
    "start": "1926360",
    "end": "1933200"
  },
  {
    "text": "with machine failures and hard drive failures and all those things right like one of the reasons why we want to have",
    "start": "1933200",
    "end": "1939279"
  },
  {
    "text": "four replicas of your data is we want it to be highly available we want it to be durable right we want to give you back",
    "start": "1939279",
    "end": "1945399"
  },
  {
    "text": "the results that you've written and I mentioned earlier kind of the scale",
    "start": "1945399",
    "end": "1950960"
  },
  {
    "text": "that we're running at right however many million requests per second across pedabytes of",
    "start": "1950960",
    "end": "1956840"
  },
  {
    "text": "data all of that data as we're going to talk about in a little bit is stored on directly connected live ssds to machines",
    "start": "1956840",
    "end": "1964200"
  },
  {
    "text": "in data centers you can imagine how many ssds that is and given the failure rate",
    "start": "1964200",
    "end": "1969679"
  },
  {
    "text": "of ssds you can imagine that sometimes one of these replicas has an SSD fail or",
    "start": "1969679",
    "end": "1977039"
  },
  {
    "text": "sometimes the power goes out or sometimes we need to take a machine down to run upgrades on it and so how do we",
    "start": "1977039",
    "end": "1983240"
  },
  {
    "text": "deal with that case um well we have kind of pretty",
    "start": "1983240",
    "end": "1989080"
  },
  {
    "text": "robust monitoring of all the systems they all kind of send health checks back to uh our Central SI our our coordinator",
    "start": "1989080",
    "end": "1996440"
  },
  {
    "text": "systems they report a bunch of different metrics about how they're doing what the disc Q length is what the memory usage",
    "start": "1996440",
    "end": "2003000"
  },
  {
    "text": "is all those sorts of things and our Central system can",
    "start": "2003000",
    "end": "2008639"
  },
  {
    "text": "side that secondary is no longer available so let me find a machine that",
    "start": "2008639",
    "end": "2015399"
  },
  {
    "text": "has capacity and I will just build a new secondary so I'll basically copy the",
    "start": "2015399",
    "end": "2021159"
  },
  {
    "text": "data from the primary and give it to the secondary and what happens during that",
    "start": "2021159",
    "end": "2028720"
  },
  {
    "text": "time depends on the consistency level you've chosen and and all that sort of thing but because we know that the data",
    "start": "2028720",
    "end": "2035919"
  },
  {
    "text": "is on the right region or the right replica we know that we can copy it and it's on at least one of the other uh um",
    "start": "2035919",
    "end": "2044159"
  },
  {
    "text": "read replicas right that's going to be important in a second so basically we",
    "start": "2044159",
    "end": "2049520"
  },
  {
    "text": "transparently to you bring up a new machine with the data on it we copy the",
    "start": "2049520",
    "end": "2055320"
  },
  {
    "text": "partition over and bring it up and this is done kind of live with the replication log or replication cue so",
    "start": "2055320",
    "end": "2062158"
  },
  {
    "text": "you by the time the machine is available you don't see any impact to your application",
    "start": "2062159",
    "end": "2068638"
  },
  {
    "text": "um any impact to your application the the reality is what you",
    "start": "2068639",
    "end": "2074679"
  },
  {
    "text": "typically see if you're looking at all of your requests and you're looking at kind of the metrics of what's Happening",
    "start": "2074679",
    "end": "2081280"
  },
  {
    "text": "you'll see a response come back with a something like a 409 or I mean a 410",
    "start": "2081280",
    "end": "2088040"
  },
  {
    "text": "gone response right and that's an indication of a a gracefully shutting",
    "start": "2088040",
    "end": "2093240"
  },
  {
    "text": "down replica it says I don't have your data anymore it's gone um and so in that case the SDK will",
    "start": "2093240",
    "end": "2102520"
  },
  {
    "text": "we're going to get to this flow in a little bit but it'll go back to a central system and say Hey where's the",
    "start": "2102520",
    "end": "2107839"
  },
  {
    "text": "data for this partition and it'll get a new address and it'll try on that other address and so what you'll actually see",
    "start": "2107839",
    "end": "2114440"
  },
  {
    "text": "is some impact to your latency because your request will fail and you'll have to go try on a different partition",
    "start": "2114440",
    "end": "2120960"
  },
  {
    "text": "somewhere right so you you might see some failure D failure Network calls uh",
    "start": "2120960",
    "end": "2127240"
  },
  {
    "text": "that's by design it's part of building a a resilient distributed",
    "start": "2127240",
    "end": "2133040"
  },
  {
    "text": "system okay what happens if the right the primary storage primary goes",
    "start": "2133359",
    "end": "2139960"
  },
  {
    "text": "out well in that case we use something called",
    "start": "2139960",
    "end": "2145320"
  },
  {
    "text": "paxos um this is kind of a well researched well documented algorithm in",
    "start": "2145320",
    "end": "2150800"
  },
  {
    "text": "the distributed system space um for electing a new leader and so we take the",
    "start": "2150800",
    "end": "2157200"
  },
  {
    "text": "three remaining read replicas and say let's elect a leader among those three",
    "start": "2157200",
    "end": "2164079"
  },
  {
    "text": "I'm not going to uh explain all of the details of paxos it's complicated um I",
    "start": "2164079",
    "end": "2169720"
  },
  {
    "text": "will just say uh we'll pick a new leader and then we will follow the same process that we did before to pull in another",
    "start": "2169720",
    "end": "2177480"
  },
  {
    "text": "secondary and copy the data to it so we will again have a write and three",
    "start": "2177480",
    "end": "2183599"
  },
  {
    "text": "reads um part of the paos implementation guarantees that the the winner of the",
    "start": "2183599",
    "end": "2191359"
  },
  {
    "text": "election has the latest rights that have been told back to customers right so",
    "start": "2191359",
    "end": "2196839"
  },
  {
    "text": "again if one of those other read regions hasn't acknowledged the right yet hasn't caught up from the replication then it",
    "start": "2196839",
    "end": "2203960"
  },
  {
    "text": "will not be elected the leader and so you don't have this kind of right ahead problem so that's uh you know something",
    "start": "2203960",
    "end": "2211920"
  },
  {
    "text": "that we had to deal with uh so that so that you don't have to deal with that yes",
    "start": "2211920",
    "end": "2217839"
  },
  {
    "text": "if have a lot coming in will they like take bre",
    "start": "2217839",
    "end": "2224560"
  },
  {
    "text": "from it's like you're a plant uh so the question is what happens if a lot of right requests are coming in will they",
    "start": "2224560",
    "end": "2230560"
  },
  {
    "text": "take a break um how do we how do we handle kind of load scenarios like that",
    "start": "2230560",
    "end": "2236200"
  },
  {
    "text": "and the answer is this is kind of uh baked into our Ru philosophy right and",
    "start": "2236200",
    "end": "2242640"
  },
  {
    "text": "so we have this idea of resource governance and so the idea of resource",
    "start": "2242640",
    "end": "2248400"
  },
  {
    "text": "governance is that when a request comes in to our back end uh we decide whether",
    "start": "2248400",
    "end": "2256680"
  },
  {
    "text": "we have the capacity to deal with it or not right and typically the way we",
    "start": "2256680",
    "end": "2261760"
  },
  {
    "text": "measure whether we have the capacity or not is by those request units that we talked about at the beginning and so for",
    "start": "2261760",
    "end": "2268359"
  },
  {
    "text": "simple crud requests like reads and writes um that sort of thing we will",
    "start": "2268359",
    "end": "2275920"
  },
  {
    "text": "um uh we we know ahead of time how much those are going to cost and so we just",
    "start": "2275920",
    "end": "2282359"
  },
  {
    "text": "don't let a request start if we're already out of capacity and so uh to",
    "start": "2282359",
    "end": "2288839"
  },
  {
    "text": "your point your your your question was kind of like what happens if there's a lot of Rights happening at the time",
    "start": "2288839",
    "end": "2295440"
  },
  {
    "text": "while we're electing a new leader right well electing the new leader will",
    "start": "2295440",
    "end": "2300640"
  },
  {
    "text": "typically happen during one of those right requests right like that one right request will either time out or by the",
    "start": "2300640",
    "end": "2306640"
  },
  {
    "text": "time it wait until we've elected a new leader right and from then those",
    "start": "2306640",
    "end": "2311839"
  },
  {
    "text": "requests will continue they might be slightly they might have slightly higher latency as we're doing more work on the",
    "start": "2311839",
    "end": "2318240"
  },
  {
    "text": "machine to replicate to to the other read replicas um but they'll continue so",
    "start": "2318240",
    "end": "2323520"
  },
  {
    "text": "so typically we won't uh block requests in that case unless you start sending a lot of requests in which case our",
    "start": "2323520",
    "end": "2330280"
  },
  {
    "text": "capacity gets overloaded we get to your your uh limit",
    "start": "2330280",
    "end": "2336040"
  },
  {
    "text": "of request units what you've provis visioned and then we typically return uh",
    "start": "2336040",
    "end": "2342119"
  },
  {
    "text": "a 429 retry later result that says you know come back in a second and a half or",
    "start": "2342119",
    "end": "2348560"
  },
  {
    "text": "so um so I guess there's there's two things that can happen right so we so I",
    "start": "2348560",
    "end": "2355040"
  },
  {
    "text": "mentioned kind of Point operations cred operations are pretty simple we know how much they're going to cost um the thing",
    "start": "2355040",
    "end": "2361040"
  },
  {
    "text": "where we don't know what it's going to cost is queries and so the whole query VM that I talked about uh with queries",
    "start": "2361040",
    "end": "2368400"
  },
  {
    "text": "we typically start running a query and within the query VM as it's chugging",
    "start": "2368400",
    "end": "2373800"
  },
  {
    "text": "through collecting results it'll periodically check and say how many Rus",
    "start": "2373800",
    "end": "2379280"
  },
  {
    "text": "have I used is it more than I was given as a budget at the start and if it is it",
    "start": "2379280",
    "end": "2385040"
  },
  {
    "text": "will do one of two things uh it'll either return a successful query with",
    "start": "2385040",
    "end": "2391760"
  },
  {
    "text": "partial results uh with a continuation token that says hey here's the results I've got so far",
    "start": "2391760",
    "end": "2397920"
  },
  {
    "text": "I'm out of time to calculate more results for you come back later with this continuation token and we can",
    "start": "2397920",
    "end": "2404319"
  },
  {
    "text": "continue from where we where we were right that's if you get kind of close to",
    "start": "2404319",
    "end": "2409800"
  },
  {
    "text": "your capacity um there's also cases where you can write queries for example if they happen to",
    "start": "2409800",
    "end": "2416400"
  },
  {
    "text": "not use the index like if you tune your index and remove something from it and we end up having to load every document",
    "start": "2416400",
    "end": "2422839"
  },
  {
    "text": "and scan through all the properties of every document those can be very expensive queries so you can churn",
    "start": "2422839",
    "end": "2427880"
  },
  {
    "text": "through kind of all of your uh your Ru budget very quickly in those cases",
    "start": "2427880",
    "end": "2434560"
  },
  {
    "text": "you'll end up with this kind of 429 where we say actually you didn't even just use this one second's budget you Ed",
    "start": "2434560",
    "end": "2441400"
  },
  {
    "text": "the next 5 Seconds budget so come back and try that query again in 5 seconds when I have some time to to handle it",
    "start": "2441400",
    "end": "2449000"
  },
  {
    "text": "right does that make sense so that's this idea of of request units and this",
    "start": "2449000",
    "end": "2454520"
  },
  {
    "text": "is kind of a key part of how we manage to build a multi-tenant system right one",
    "start": "2454520",
    "end": "2460760"
  },
  {
    "text": "of the big problems in multi-tenant systems like ours is I said at the",
    "start": "2460760",
    "end": "2466760"
  },
  {
    "text": "beginning one physical machine might have a bunch of different customers",
    "start": "2466760",
    "end": "2471880"
  },
  {
    "text": "partitions on them right and one thing you don't want as a customer with your",
    "start": "2471880",
    "end": "2477480"
  },
  {
    "text": "data on that partition is for your neighbor to run an expensive query and",
    "start": "2477480",
    "end": "2483800"
  },
  {
    "text": "have that expensive query bring that server to its knees and mean that your queries start taking",
    "start": "2483800",
    "end": "2489920"
  },
  {
    "text": "longer having it impact your latency right and this is one of the reasons why",
    "start": "2489920",
    "end": "2494960"
  },
  {
    "text": "those request units are so important for us is because we have this idea of",
    "start": "2494960",
    "end": "2500319"
  },
  {
    "text": "resource governance right we know that a given machine can handle n 100,000 n",
    "start": "2500319",
    "end": "2507760"
  },
  {
    "text": "million Rus right and so we know how many customers we can put on it if every",
    "start": "2507760",
    "end": "2514160"
  },
  {
    "text": "customer has a th000 Rus and a machine can handle 100,000 Rus then we can put",
    "start": "2514160",
    "end": "2519599"
  },
  {
    "text": "100 customers on that machine and as long as we actually stop running the",
    "start": "2519599",
    "end": "2525160"
  },
  {
    "text": "results when a customer exceeds its budget of our use we know that we can maintain our slas for every other",
    "start": "2525160",
    "end": "2531800"
  },
  {
    "text": "customer with that machine and so this is why we kind of throttle it's why we have all of this R",
    "start": "2531800",
    "end": "2538599"
  },
  {
    "text": "it's why we normalize everything to Rus is so that we can have this Ru capacity",
    "start": "2538599",
    "end": "2543760"
  },
  {
    "text": "planning for us uh and resource governance does that make",
    "start": "2543760",
    "end": "2549319"
  },
  {
    "text": "sense great I was worried at the beginning we",
    "start": "2549319",
    "end": "2555160"
  },
  {
    "text": "were going pretty fast but uh but we're slowing down so that's good um elasticity so this is we're going to",
    "start": "2555160",
    "end": "2561160"
  },
  {
    "text": "talk more about what I what I was just kind of talking about this idea of of provision throughput and elasticity so",
    "start": "2561160",
    "end": "2567720"
  },
  {
    "text": "um we have a variety of different models because customers have a variety of",
    "start": "2567720",
    "end": "2573280"
  },
  {
    "text": "different types of traffic right um many",
    "start": "2573280",
    "end": "2578640"
  },
  {
    "text": "products say teams right teams uses Us stores all of the chat history in in",
    "start": "2578640",
    "end": "2584079"
  },
  {
    "text": "Microsoft teams is stored in Cosmos CB it turns out the teams has a kind of",
    "start": "2584079",
    "end": "2590200"
  },
  {
    "text": "follow the sun model right most people use teams during their workday and so if",
    "start": "2590200",
    "end": "2595520"
  },
  {
    "text": "you look around the world at our transactions from teams they go you know",
    "start": "2595520",
    "end": "2600880"
  },
  {
    "text": "in the US during one period of time then they are writing in a different region during a different period of time right",
    "start": "2600880",
    "end": "2607400"
  },
  {
    "text": "and so that people want to be able to have different uh ways of provisioning the",
    "start": "2607400",
    "end": "2612680"
  },
  {
    "text": "system depending on their usage patterns sometimes things happen in batch right so you know maybe one account doesn't do",
    "start": "2612680",
    "end": "2620119"
  },
  {
    "text": "very much during the week but every weekend it goes and you know does a big batch job or or every night at 2 a. does",
    "start": "2620119",
    "end": "2627079"
  },
  {
    "text": "a big batch job right and so you want to be able to have different provisioning for for different things like this and",
    "start": "2627079",
    "end": "2632839"
  },
  {
    "text": "so we offer um two simple ones right provisioned right",
    "start": "2632839",
    "end": "2641119"
  },
  {
    "text": "we offer you provisioned we guarantee that you can use 30,000 Rus right and we",
    "start": "2641119",
    "end": "2647240"
  },
  {
    "text": "pay you a fixed cost you pay us a fixed cost we don't pay you that'd be that'd",
    "start": "2647240",
    "end": "2652359"
  },
  {
    "text": "be nice for you if we did um you pay us a fixed cost for those 30,000 Rus right",
    "start": "2652359",
    "end": "2659000"
  },
  {
    "text": "and the way the math breaks down for us is we give you um one physical partition",
    "start": "2659000",
    "end": "2665680"
  },
  {
    "text": "for every 10,000 ru that you have um uh provisioned and",
    "start": "2665680",
    "end": "2672280"
  },
  {
    "text": "so that's fine if you have a steady workload right that you know kind of continuously is using that amount of",
    "start": "2672280",
    "end": "2679559"
  },
  {
    "text": "data the second is this idea of autoscaling throughput right and so you can say well my workload is going to use",
    "start": "2679559",
    "end": "2688000"
  },
  {
    "text": "somewhere between 1500 and 15,000 Rus but it depends it's V it's variable",
    "start": "2688000",
    "end": "2694240"
  },
  {
    "text": "right and so in that case um we kind of provision you up to what the maximum of",
    "start": "2694240",
    "end": "2700920"
  },
  {
    "text": "that is number of of physical partitions but we charge you for what",
    "start": "2700920",
    "end": "2708000"
  },
  {
    "text": "the peak number of Rus you used in any given hour is so for each 24-hour",
    "start": "2708000",
    "end": "2715119"
  },
  {
    "text": "period we pick the second within that period that had the high highest number of Rus and that's what you get build for",
    "start": "2715119",
    "end": "2722760"
  },
  {
    "text": "that hour right so if you have a spiky workload this can be a way to save a lot of money because most of the time you're",
    "start": "2722760",
    "end": "2729000"
  },
  {
    "text": "kind of s sitting at that idle lower level and then when you have a spike we",
    "start": "2729000",
    "end": "2734559"
  },
  {
    "text": "can instantly scale up to that level because we've already actually provisioned you the machine resources to do it um and you pay for the spiky",
    "start": "2734559",
    "end": "2744119"
  },
  {
    "text": "workload when you have the spiky workload make sense um not shown on this",
    "start": "2744119",
    "end": "2749880"
  },
  {
    "text": "slide we also have a serverless uh model where you actually just pay for consumption right uh you",
    "start": "2749880",
    "end": "2756559"
  },
  {
    "text": "pay each time an Ru happens you pay per Ru and so if you have a very low usage",
    "start": "2756559",
    "end": "2764000"
  },
  {
    "text": "workload occasional workload that can be better than provisioning ahead of time",
    "start": "2764000",
    "end": "2770040"
  },
  {
    "text": "uh what the uh what the pro the ru budget is going to be it does mean you",
    "start": "2770040",
    "end": "2777480"
  },
  {
    "text": "know you have somewhat less control over uh instantly scaling to a large number",
    "start": "2777480",
    "end": "2783000"
  },
  {
    "text": "of Rus um and so it's something to think about whether whether you want to use serverless or not but is great for",
    "start": "2783000",
    "end": "2790559"
  },
  {
    "text": "getting started small projects experimentations that sort of thing or like I said kind of those really",
    "start": "2790559",
    "end": "2796280"
  },
  {
    "text": "occasional workloads um and then the last thing I'll mention um new in",
    "start": "2796280",
    "end": "2801960"
  },
  {
    "text": "preview we announced at ignite uh we are also now offering this Auto scale idea but",
    "start": "2801960",
    "end": "2809640"
  },
  {
    "text": "autoscaling per partition or per region so instead of saying I want to scale my",
    "start": "2809640",
    "end": "2814680"
  },
  {
    "text": "whole account in every region I can say actually I I know that this region is",
    "start": "2814680",
    "end": "2819720"
  },
  {
    "text": "busy right now and actually I even know that this partition key within that region is super busy so I'll pay more",
    "start": "2819720",
    "end": "2825920"
  },
  {
    "text": "for that one to have better uh better provisioning right so that's kind of a",
    "start": "2825920",
    "end": "2831240"
  },
  {
    "text": "new feature so uh a lot of flexibility in kind of how you you provision things",
    "start": "2831240",
    "end": "2837280"
  },
  {
    "text": "but what does it actually look like how do we manage elastic scaling in the case where you don't have the provision the",
    "start": "2837280",
    "end": "2844160"
  },
  {
    "text": "the capacity already so let's imagine you have a container that's provisioned",
    "start": "2844160",
    "end": "2849319"
  },
  {
    "text": "with 30,000 Rus like I said before that means you'll get three physical",
    "start": "2849319",
    "end": "2855119"
  },
  {
    "text": "partitions Each of which is able to serve 10,000 Rus and",
    "start": "2855119",
    "end": "2861000"
  },
  {
    "text": "then you decide to bump it to 60,000 Rus what do we",
    "start": "2861000",
    "end": "2868559"
  },
  {
    "text": "do well the reason why we say this can take a little bit is because we go",
    "start": "2868559",
    "end": "2874559"
  },
  {
    "text": "find six places to put partitions and we copy the data from",
    "start": "2874559",
    "end": "2881880"
  },
  {
    "text": "your three partitions into six partitions so we do a bunch of work to",
    "start": "2881880",
    "end": "2887520"
  },
  {
    "text": "kind of optimally split these so that half your data from each partition goes into one of the partitions below we copy",
    "start": "2887520",
    "end": "2895160"
  },
  {
    "text": "everything once we finish copying we go through the replication cue and we copy down everything until we're at a point",
    "start": "2895160",
    "end": "2901960"
  },
  {
    "text": "where it's live and then we update the containers topology",
    "start": "2901960",
    "end": "2907720"
  },
  {
    "text": "to say hey forget about those those old container those old replicas these are",
    "start": "2907720",
    "end": "2912800"
  },
  {
    "text": "the replicas now and this is another case where like I was saying before you would get one of those uh 410 gone",
    "start": "2912800",
    "end": "2920079"
  },
  {
    "text": "exceptions or 410 gone responses because now those old replicas are gone because",
    "start": "2920079",
    "end": "2926240"
  },
  {
    "text": "they were at a lower capacity and we we want you to redirect and use the ones with high with higher",
    "start": "2926240",
    "end": "2932160"
  },
  {
    "text": "capacity does that make sense to everybody",
    "start": "2932160",
    "end": "2937200"
  },
  {
    "text": "so no downtime happens as part of this um this slide for Simplicity showed kind",
    "start": "2938559",
    "end": "2945880"
  },
  {
    "text": "of the three initial replicas going away all at the same time um that doesn't",
    "start": "2945880",
    "end": "2951559"
  },
  {
    "text": "actually happen as soon as the copying is done for a given pair beneath it we'll update the topology for each of",
    "start": "2951559",
    "end": "2958359"
  },
  {
    "text": "those other containers at a time right in a three to six partition split that's",
    "start": "2958359",
    "end": "2963520"
  },
  {
    "text": "less interesting in our customers that have got hundreds or thousands of of partitions you know if you had to wait",
    "start": "2963520",
    "end": "2969200"
  },
  {
    "text": "for every partition within a thousand to split um that would be a big deal right and so the the fact that you can split",
    "start": "2969200",
    "end": "2976359"
  },
  {
    "text": "individual partitions uh is important all right we're going to fly",
    "start": "2976359",
    "end": "2983839"
  },
  {
    "text": "through some of the rest of this stuff um multi- API support I said we come back to this right uh we support",
    "start": "2983839",
    "end": "2989680"
  },
  {
    "text": "multiple apis the best thing about Cosmos CB and the best thing about the rest of this",
    "start": "2989680",
    "end": "2995920"
  },
  {
    "text": "talk the reason why I can fly through it um is that everything that I've talked about so far applies to all of our",
    "start": "2995920",
    "end": "3002680"
  },
  {
    "text": "different apis right all of our different Ru based apis right the idea",
    "start": "3002680",
    "end": "3008400"
  },
  {
    "text": "is they all share everything that I just talked about um so when we make",
    "start": "3008400",
    "end": "3013480"
  },
  {
    "text": "investments in any of those things that we talked about they benefit all of our apis it also means you know we don't",
    "start": "3013480",
    "end": "3019839"
  },
  {
    "text": "have to think about all of those durability and availability decisions",
    "start": "3019839",
    "end": "3024960"
  },
  {
    "text": "for every one of the apis that we do it means that if we optimize something it gets optimized for all of",
    "start": "3024960",
    "end": "3031640"
  },
  {
    "text": "them um and then so sometimes you know",
    "start": "3031640",
    "end": "3036839"
  },
  {
    "text": "the the nosql back end which is kind of the the default one uh sometimes it",
    "start": "3036839",
    "end": "3042280"
  },
  {
    "text": "doesn't have a feature that is needed to support and one of these apis and so the idea there is we build extensibility",
    "start": "3042280",
    "end": "3048599"
  },
  {
    "text": "hooks into that back end to allow us to support the apis rather than duplicating",
    "start": "3048599",
    "end": "3053920"
  },
  {
    "text": "a bunch of code and and stuff like that it you know just Sound Engineering but it it's worth pointing",
    "start": "3053920",
    "end": "3060720"
  },
  {
    "text": "out um so back to this slide that we saw before um each of these is kind of the",
    "start": "3060720",
    "end": "3069960"
  },
  {
    "text": "an idea of a set of microservices right for each API we have a microservice um and those",
    "start": "3069960",
    "end": "3078799"
  },
  {
    "text": "run uh on those compute gateways and again",
    "start": "3078799",
    "end": "3085960"
  },
  {
    "text": "we have to kind of think about this idea of of blast raist and noisy neighbors",
    "start": "3085960",
    "end": "3091000"
  },
  {
    "text": "and uh and how we maintain throughput in availability for those it's a it's a difficult space um to build this compute",
    "start": "3091000",
    "end": "3098720"
  },
  {
    "text": "Gateway and and to build it well in a multi-tenant way uh and to build it cheaply uh I it's currently super",
    "start": "3098720",
    "end": "3105280"
  },
  {
    "text": "expensive for us to run the API Gateway and so um working on ways to to improve",
    "start": "3105280",
    "end": "3110880"
  },
  {
    "text": "that definitely um and so we'll talk a little bit about that so uh one of",
    "start": "3110880",
    "end": "3116760"
  },
  {
    "text": "things that we want to do there is we want to make sure that we have flexibility in scaling it so uh I talked",
    "start": "3116760",
    "end": "3122640"
  },
  {
    "text": "about all those fault domains and clusters and stuff like that when we deploy the API Gateway we can do that",
    "start": "3122640",
    "end": "3130200"
  },
  {
    "text": "independently of any of the backend deployments and scale them independently of each other so depending on how many",
    "start": "3130200",
    "end": "3136240"
  },
  {
    "text": "people are using multi- API stuff uh we can choose to how we want to scale it um",
    "start": "3136240",
    "end": "3143839"
  },
  {
    "text": "that shared platform layer like I said both in the the back end and then within a compute Gateway we have a we kind of",
    "start": "3143839",
    "end": "3150200"
  },
  {
    "text": "try to normalize stuff into one platform layer so that we can optimize it once for all the different",
    "start": "3150200",
    "end": "3157040"
  },
  {
    "text": "apis um resource governance is a challenging problem with",
    "start": "3157040",
    "end": "3163640"
  },
  {
    "text": "these API gateways it's very hard to know how difficult it's going to be to translate a query into a nosql",
    "start": "3163640",
    "end": "3170319"
  },
  {
    "text": "query and back ahead of time and so it's hard to figure out exactly what on the",
    "start": "3170319",
    "end": "3176839"
  },
  {
    "text": "API Gateway is going to end up being a Noisy Neighbor problem uh and so",
    "start": "3176839",
    "end": "3182559"
  },
  {
    "text": "typically today we do that through load balancing um rather than through direct",
    "start": "3182559",
    "end": "3188640"
  },
  {
    "text": "uh resource governance like with the ru budgets that that we talked about in the back end and finally we do uh tie things",
    "start": "3188640",
    "end": "3197240"
  },
  {
    "text": "pretty tightly to the underlying Hardware so um we'll talk about this in the next slide but like we use Numa",
    "start": "3197240",
    "end": "3203839"
  },
  {
    "text": "boundaries and and processor affinity and and that sort of thing so you can imagine uh the way that this compute",
    "start": "3203839",
    "end": "3210079"
  },
  {
    "text": "Gateway API Gateway works is a bunch of small processes uh one process for each",
    "start": "3210079",
    "end": "3217000"
  },
  {
    "text": "API that we might be doing or multiple copies of the same process for each API",
    "start": "3217000",
    "end": "3222119"
  },
  {
    "text": "to scale we run on large machines in Azure so those are often multiple socket",
    "start": "3222119",
    "end": "3228880"
  },
  {
    "text": "machines and so they have multiple Numa nodes Numa for those who aren't aware is",
    "start": "3228880",
    "end": "3233960"
  },
  {
    "text": "non-uniform memory access as when you have a computer where depending on where your",
    "start": "3233960",
    "end": "3241400"
  },
  {
    "text": "process is running it can take different amounts of time to access different parts of memory right um and to",
    "start": "3241400",
    "end": "3249559"
  },
  {
    "text": "synchronize memory between different threads and and that sort of thing and so we do a bunch of work to kind of tie",
    "start": "3249559",
    "end": "3256079"
  },
  {
    "text": "make small processes that can run on a small number of cores and then",
    "start": "3256079",
    "end": "3261640"
  },
  {
    "text": "we affinitize them to those cores so they don't have to cross and of those",
    "start": "3261640",
    "end": "3266799"
  },
  {
    "text": "boundaries and and do those expensive calls we also do a bunch of work kind of",
    "start": "3266799",
    "end": "3273359"
  },
  {
    "text": "uh optimizing the uh the process itself",
    "start": "3273359",
    "end": "3278400"
  },
  {
    "text": "that we'll talk about in a second um but I mentioned before load balancing um we",
    "start": "3278400",
    "end": "3284079"
  },
  {
    "text": "have multiple levels of load balancing to deal with this idea of of noisy neighbors and and if something goes",
    "start": "3284079",
    "end": "3290000"
  },
  {
    "text": "wrong on one node to be able to kind of contain it to that one node So within a given VM we even can rewrote connections",
    "start": "3290000",
    "end": "3296880"
  },
  {
    "text": "between different processes right um we have uh the Azure SLB does load",
    "start": "3296880",
    "end": "3303559"
  },
  {
    "text": "balancing we tell it to take VMS out of rotation if they get busy right so some customer picks a crazy random query that",
    "start": "3303559",
    "end": "3311160"
  },
  {
    "text": "makes the CPU spin up the Azure SLB will take it out of rotation right and not",
    "start": "3311160",
    "end": "3316200"
  },
  {
    "text": "wrote anything else to it um we also um are able to like at our level through",
    "start": "3316200",
    "end": "3323720"
  },
  {
    "text": "monitoring say hey uh this cluster is getting very busy let's take all the",
    "start": "3323720",
    "end": "3329039"
  },
  {
    "text": "requests from this given account from now on and put them into a different cluster so that you know this cluster",
    "start": "3329039",
    "end": "3334559"
  },
  {
    "text": "can can cool down and then finally uh this kind of work in progress we're working on a a more active load",
    "start": "3334559",
    "end": "3341160"
  },
  {
    "text": "balancing um where we can kind of look at live Health Data from all of the",
    "start": "3341160",
    "end": "3346920"
  },
  {
    "text": "different uh different nodes pick the best one uh we've got some interesting",
    "start": "3346920",
    "end": "3352280"
  },
  {
    "text": "prototypes using historical insight and ml models of what traffic is likely do for a given cluster and and do load",
    "start": "3352280",
    "end": "3358799"
  },
  {
    "text": "balancing there uh finally system",
    "start": "3358799",
    "end": "3363839"
  },
  {
    "text": "performance um you know we talked a bunch about kind of the back end before",
    "start": "3363839",
    "end": "3369079"
  },
  {
    "text": "and what we do to optimize across the whole back end it's super important um",
    "start": "3369079",
    "end": "3374559"
  },
  {
    "text": "you know we have a bunch of uh checking gates in in Azure devops that run performance tests on every check-in uh",
    "start": "3374559",
    "end": "3382200"
  },
  {
    "text": "even a regression of less than 1% in any of our benchmarks is not allowed to be",
    "start": "3382200",
    "end": "3389000"
  },
  {
    "text": "merged into our code base um and so oftentimes people have to go find",
    "start": "3389000",
    "end": "3394079"
  },
  {
    "text": "somewhere else to optimize if they need to do some more Logic for a given operation right and so we kind of hold",
    "start": "3394079",
    "end": "3400359"
  },
  {
    "text": "ourselves to a consistently High bar uh around performance but it is something",
    "start": "3400359",
    "end": "3405839"
  },
  {
    "text": "that has to be kind of designed for and enforced and thought about and uh like",
    "start": "3405839",
    "end": "3411680"
  },
  {
    "text": "it it's an entire culture to build a highly performant system like this",
    "start": "3411680",
    "end": "3417280"
  },
  {
    "text": "um on the API Gateway itself I I should have mentioned um all the backend stuff",
    "start": "3417280",
    "end": "3423160"
  },
  {
    "text": "just about uh is that I've talked about is written in C++ um the API Gateway is",
    "start": "3423160",
    "end": "3428480"
  },
  {
    "text": "written in net uh either net 6 oret 8 it was donet 6 but I saw the pr foret 8 so",
    "start": "3428480",
    "end": "3436240"
  },
  {
    "text": "I haven't checked my mail today if it got merged then maybe it's running onet 8 now um but um we work closely with the",
    "start": "3436240",
    "end": "3443839"
  },
  {
    "text": "net team my said I spent 20 years in the net team various other people on the team also uh came from theet team and so",
    "start": "3443839",
    "end": "3451559"
  },
  {
    "text": "we work closely with them both to provide data about where they could help",
    "start": "3451559",
    "end": "3456760"
  },
  {
    "text": "us um but also for them to help us optune and and optimize things so we",
    "start": "3456760",
    "end": "3463520"
  },
  {
    "text": "work hard to to do a good job there um let's see what else I wanted to talk",
    "start": "3463520",
    "end": "3470000"
  },
  {
    "text": "about I think that's the main things I wanted to talk about um does anyone have any questions for",
    "start": "3470000",
    "end": "3478559"
  },
  {
    "text": "the last two minutes yes you mentioned that thek also",
    "start": "3478559",
    "end": "3484920"
  },
  {
    "text": "does a lot of he lifting in terms of selecting the replica to make a call for what is the",
    "start": "3484920",
    "end": "3492400"
  },
  {
    "text": "rough can you talk more about thek part of this sure uh conveniently it's my",
    "start": "3492400",
    "end": "3500039"
  },
  {
    "text": "next slide wow um you guys are doing a great",
    "start": "3500039",
    "end": "3505280"
  },
  {
    "text": "job of asking me the questions that I want to answer um so for those of you who are",
    "start": "3505280",
    "end": "3510559"
  },
  {
    "text": "using the net SDK um you may have seen that we have this idea of connection mode direct connection versus uh versus",
    "start": "3510559",
    "end": "3518160"
  },
  {
    "text": "uh Gateway connection um the direct connection mode uh",
    "start": "3518160",
    "end": "3525960"
  },
  {
    "text": "basically allows you to skip talking to the routing Gateway for most of your",
    "start": "3525960",
    "end": "3532680"
  },
  {
    "text": "operations and so what the SDK actually does is the first time you make an operation um it will call into what we",
    "start": "3532680",
    "end": "3541359"
  },
  {
    "text": "call the rting Gateway and say hey I need to get some data but I don't know",
    "start": "3541359",
    "end": "3546400"
  },
  {
    "text": "where any of the data lives so for this account can you give me the topology can",
    "start": "3546400",
    "end": "3552599"
  },
  {
    "text": "tell can you tell me where all the backend replicas are for each partition key range um what's the address what's",
    "start": "3552599",
    "end": "3559760"
  },
  {
    "text": "the port for all the different replicas and then it can directly",
    "start": "3559760",
    "end": "3566200"
  },
  {
    "text": "connect to those backend replicas so we do that over a custom TCP protocol with",
    "start": "3566200",
    "end": "3572720"
  },
  {
    "text": "custom TCP serialization uh it makes it difficult for firewall administrators because we",
    "start": "3572720",
    "end": "3579640"
  },
  {
    "text": "can reach out on any random Port um it makes it difficult because we connect to",
    "start": "3579640",
    "end": "3586240"
  },
  {
    "text": "all four replicas for each partition um and so the number of outbound network",
    "start": "3586240",
    "end": "3592319"
  },
  {
    "text": "connections that we can make can be huge um I also talked at the beginning about",
    "start": "3592319",
    "end": "3600079"
  },
  {
    "text": "cross partition queries you notice everything we talked about today with the backend nodes was one partition",
    "start": "3600079",
    "end": "3606640"
  },
  {
    "text": "right and so the other thing the sdks have to do when they're in this direct mode is like Stitch together all the",
    "start": "3606640",
    "end": "3612480"
  },
  {
    "text": "results across all those different partitions if I say like select",
    "start": "3612480",
    "end": "3618480"
  },
  {
    "text": "something and order by some field right well that order bu can only happen on",
    "start": "3618480",
    "end": "3625520"
  },
  {
    "text": "the client as part of the SDK right so a bunch of that logic ends up having to happen in the SDK um does that kind of",
    "start": "3625520",
    "end": "3633079"
  },
  {
    "text": "answer your question does that help um so I mentioned before all those kind of",
    "start": "3633079",
    "end": "3638200"
  },
  {
    "text": "gone exceptions that sort of thing they kind of re-trigger this flow if you try to read from a read replica you get a",
    "start": "3638200",
    "end": "3644400"
  },
  {
    "text": "gone exception they re-trigger the SDK to go ask for new topology information",
    "start": "3644400",
    "end": "3650440"
  },
  {
    "text": "uh in the net SDK we also do it implicitly every five minutes just in",
    "start": "3650440",
    "end": "3655640"
  },
  {
    "text": "case to try to preempt the the idea of the topology changing we kind of reach out and say hey has anything changed in",
    "start": "3655640",
    "end": "3660960"
  },
  {
    "text": "the last 5 minutes have any of my partitions moved around um but there the reason why I have a team as big as I do",
    "start": "3660960",
    "end": "3668079"
  },
  {
    "text": "is because a bunch of this stuff ends up having to happen um directly in the SDK",
    "start": "3668079",
    "end": "3673880"
  },
  {
    "text": "and with five and maybe soon to be six or seven languages to support there's uh there's a lot of work to to support",
    "start": "3673880",
    "end": "3680599"
  },
  {
    "text": "those things in in the SDK all right I think",
    "start": "3680599",
    "end": "3686599"
  },
  {
    "text": "we are one minute over I want to thank everyone very very much um this has been",
    "start": "3686599",
    "end": "3692000"
  },
  {
    "text": "a great talk um I have some uh information here pointers to docs and",
    "start": "3692000",
    "end": "3698799"
  },
  {
    "text": "that sort of thing um please uh feel free to reach out to me on Twitter I'm at pil threads P.D",
    "start": "3698799",
    "end": "3706839"
  },
  {
    "text": "somebody hadd already taken my handle um you can email me it's Kevin pi@",
    "start": "3706839",
    "end": "3712279"
  },
  {
    "text": "microsoft.com um definitely feel free to reach out if you have any",
    "start": "3712279",
    "end": "3717960"
  },
  {
    "text": "questions all right thanks a lot",
    "start": "3717960",
    "end": "3722640"
  }
]