[
  {
    "text": "all right uh good day everyone uh today I want to share with you how you can use serverless functions to do big data",
    "start": "4720",
    "end": "11719"
  },
  {
    "text": "applications and how to think about things that are not necessarily you know I've got a web server I've got like a",
    "start": "11719",
    "end": "18080"
  },
  {
    "text": "set of Step functions but instead how can we actually put serverless to work in in larger data analytics",
    "start": "18080",
    "end": "24960"
  },
  {
    "text": "applications So today we're going to have talk about four things first of all we're going to talk about how server",
    "start": "24960",
    "end": "30519"
  },
  {
    "text": "lless can be helpful to kind of these uh Big Data workflows secondly because I",
    "start": "30519",
    "end": "36040"
  },
  {
    "text": "come from a reliability engineering background we're going to talk about all the things that go wrong when you do that naively without uh Consulting",
    "start": "36040",
    "end": "42600"
  },
  {
    "text": "anyone or bumping into the uh bumping into all the obstacles head first thirdly we'll talk about how to do this",
    "start": "42600",
    "end": "49239"
  },
  {
    "text": "in a safe way that makes it possible for you to safely experiment and then fourth we'll talk about uh some of the",
    "start": "49239",
    "end": "55640"
  },
  {
    "text": "experiments I've done I and my company have done over the past couple of years to improve price performance",
    "start": "55640",
    "end": "60680"
  },
  {
    "text": "by applying some uh some arm 64 processors to our servus",
    "start": "60680",
    "end": "66960"
  },
  {
    "text": "workload so let's talk first about what is serverless for like both traditionally and kind of what's our new",
    "start": "66960",
    "end": "73759"
  },
  {
    "text": "application of it and maybe just to quickly Define serverless because I know the definition of serverless can be",
    "start": "73759",
    "end": "79960"
  },
  {
    "text": "contentious what I'm going to say serverless is um I am not an expert in this I don't necessarily go around",
    "start": "79960",
    "end": "86159"
  },
  {
    "text": "telling people you know you should toally do serverless um but my definition of serverless is that it is a",
    "start": "86159",
    "end": "93320"
  },
  {
    "text": "runtime environment where I can set my application running I don't have to think necessarily about what Hardware is",
    "start": "93320",
    "end": "99600"
  },
  {
    "text": "it running on uh I don't have to think about manually provisioning it because the cloud provider takes care of that",
    "start": "99600",
    "end": "105360"
  },
  {
    "text": "for me and in Honeycombs case uh in the kind of case study that I'm demonstrating today uh we use AWS Lambda",
    "start": "105360",
    "end": "112560"
  },
  {
    "text": "I understand some of you may use Azure functions and that's okay too right like you know basically the core idea is",
    "start": "112560",
    "end": "118439"
  },
  {
    "text": "whichever cloud provider using you are offloading that kind of work of maintaining and scaling the runtime to",
    "start": "118439",
    "end": "124240"
  },
  {
    "text": "your cloud provider so specifically in our case we started wondering could we use AWS",
    "start": "124240",
    "end": "131720"
  },
  {
    "text": "Lambda for optimizing our custom data store but you know again like who is the",
    "start": "131720",
    "end": "137480"
  },
  {
    "text": "serving what's the purpose of it so why are we running a custom data store um so",
    "start": "137480",
    "end": "142640"
  },
  {
    "text": "what is what is retriever why does it why does that matter so retriever is a",
    "start": "142640",
    "end": "148040"
  },
  {
    "text": "colum Dory that we wrote that is very similar to click house um if click house had been around 5 years earlier we might",
    "start": "148040",
    "end": "154360"
  },
  {
    "text": "have chosen to use it instead but the core idea is that retri is a distributed column store that enables us to do",
    "start": "154360",
    "end": "161760"
  },
  {
    "text": "realtime event aggregation and Analysis so think you know something like Google",
    "start": "161760",
    "end": "167000"
  },
  {
    "text": "analytics think something like uh Splunk think something where you are trying to",
    "start": "167000",
    "end": "172360"
  },
  {
    "text": "process data uh and also because all of our services at honeycomb are named after dogs um here's a picture of my",
    "start": "172360",
    "end": "178920"
  },
  {
    "text": "golden retriever uh mix so that is flurry uh",
    "start": "178920",
    "end": "184840"
  },
  {
    "text": "a so uh what are we using this Real Time Event aggregation 4 because you know",
    "start": "185080",
    "end": "190120"
  },
  {
    "text": "right like you could be analyzing clicks on your website you could be analyzing",
    "start": "190120",
    "end": "195159"
  },
  {
    "text": "uh the performance of your store sales but in our case we are doing this for the observability purpose we are trying",
    "start": "195159",
    "end": "201560"
  },
  {
    "text": "to interactively query Trace bands and logs so um this probably is an opportune",
    "start": "201560",
    "end": "208040"
  },
  {
    "text": "moment for me to introduce myself um I'm Liz I'm the field CTO at honeycomb and",
    "start": "208040",
    "end": "213120"
  },
  {
    "text": "I'm also an AWS Community hero and I hail from uh Sydney New South Wales in",
    "start": "213120",
    "end": "218319"
  },
  {
    "text": "Australia and Vancouver British Columbia in Canada so you've actually caught me kind of in the middle between those",
    "start": "218319",
    "end": "223840"
  },
  {
    "text": "between those two places right now so at honeycomb we help people",
    "start": "223840",
    "end": "229519"
  },
  {
    "text": "understand how their systems are behaving and what's going on inside of their",
    "start": "229519",
    "end": "234879"
  },
  {
    "text": "systems and you know historically we've thought about kind of you know monitoring as you know you look at your",
    "start": "234879",
    "end": "240680"
  },
  {
    "text": "application and you know it keeps your system keeps track of what's going on is everything normal or are there",
    "start": "240680",
    "end": "246560"
  },
  {
    "text": "situations that are abnormal that we need to send alerts on which is well and good except for today we live in a much",
    "start": "246560",
    "end": "252959"
  },
  {
    "text": "more complex world and maybe you might want to see things that you didn't anticipate in advance maybe you might",
    "start": "252959",
    "end": "260120"
  },
  {
    "text": "not want to have everything pre-aggregated but you might want to post aggregate to figure out what",
    "start": "260120",
    "end": "265400"
  },
  {
    "text": "happened after the fact rather than having a predefined rule saying I only want to up my HD 500s or I only want to",
    "start": "265400",
    "end": "271840"
  },
  {
    "text": "group things by build ID and instead to have the flexibility to choose to to group anything",
    "start": "271840",
    "end": "278000"
  },
  {
    "text": "up so what do I mean by this well again you know this is just to give you an idea of the use case that we are uh that",
    "start": "278000",
    "end": "285360"
  },
  {
    "text": "we using serverless functions for so I have an application here",
    "start": "285360",
    "end": "290759"
  },
  {
    "text": "um let's see roster do so this is uh this is my E online Alliance and uh",
    "start": "290759",
    "end": "298039"
  },
  {
    "text": "because we are an eonline alliance and we like to have fun by writing",
    "start": "298039",
    "end": "303160"
  },
  {
    "text": "software um we have uh written our own HR software and our own HR software is",
    "start": "303160",
    "end": "310360"
  },
  {
    "text": "measured and instrumented uh with uh with this technology so I can go in and",
    "start": "310360",
    "end": "315720"
  },
  {
    "text": "I can ask you know hey what's going on uh you know what's going on with this",
    "start": "315720",
    "end": "321680"
  },
  {
    "text": "page load um you know why is it potentially why is it so so I can go ahead and run those queries and because",
    "start": "321680",
    "end": "329080"
  },
  {
    "text": "I want to get super meta here so this is a set of queries that I just this is a",
    "start": "329080",
    "end": "335919"
  },
  {
    "text": "set of requests that just happened because I wouldn't accessed the uh access this demo application as it were",
    "start": "335919",
    "end": "343240"
  },
  {
    "text": "but let's go one level deeper and let's introspect into what happened when I",
    "start": "343240",
    "end": "349080"
  },
  {
    "text": "clicked that runquery button or what happened when I went and viewed this Trace so I can go here and I can look",
    "start": "349080",
    "end": "355919"
  },
  {
    "text": "and I can see specifically for queries that I issued what's going on with those um so I might",
    "start": "355919",
    "end": "362840"
  },
  {
    "text": "be able to to ask you know what's going on with the performance of that specific time that I clicked run query and sure",
    "start": "362840",
    "end": "370199"
  },
  {
    "text": "enough we can see that there's a lot of stuff that happens when you run a query so that's kind of the core idea that",
    "start": "370199",
    "end": "377919"
  },
  {
    "text": "we're getting at here is that if you have stored all this raw data you can go",
    "start": "377919",
    "end": "383080"
  },
  {
    "text": "ahead and look at it uh after the fact and it doesn't matter whether it's observability data it could also be you",
    "start": "383080",
    "end": "388400"
  },
  {
    "text": "know your shop sales it could also be uh could also be information on who's accessing government",
    "start": "388400",
    "end": "394639"
  },
  {
    "text": "benefits but the core idea here is that the faster you make this feedback loop",
    "start": "394639",
    "end": "400680"
  },
  {
    "text": "the easier it is to do kind of hypothesis driven development uh because this there there was this talk this",
    "start": "400680",
    "end": "406639"
  },
  {
    "text": "morning right in which there was a discussion of uh understanding your software systems using scientific method",
    "start": "406639",
    "end": "413360"
  },
  {
    "text": "and that you know you do 80% of your job is writing software writing features and then the other 80% of he's in the he's",
    "start": "413360",
    "end": "420240"
  },
  {
    "text": "on the audience great and then the other 80% of your job is debugging those features when they go wrong right so",
    "start": "420240",
    "end": "427560"
  },
  {
    "text": "this is the kind of thing where if that 80 other 80% of your job that you're doing in your 160% time uh winds up",
    "start": "427560",
    "end": "435560"
  },
  {
    "text": "taking forever and being a giant pain then you're going to have a much less happy time at work so it's important to",
    "start": "435560",
    "end": "443960"
  },
  {
    "text": "have observability if you have complex systems because you have to be able to",
    "start": "443960",
    "end": "449720"
  },
  {
    "text": "understand what's going on with those complex systems and be able to very quickly iterate and hone in on no my",
    "start": "449720",
    "end": "455520"
  },
  {
    "text": "hypothesis about it being in this part of the system is wrong it's actually in that part of the",
    "start": "455520",
    "end": "460759"
  },
  {
    "text": "system so at the end of the day uh the fundamental data type that we're ingesting to generate all this data that",
    "start": "460759",
    "end": "466960"
  },
  {
    "text": "we're using serverless on is we're ingesting a bunch of open Telemetry data and we're storing it onto into our CER",
    "start": "466960",
    "end": "474319"
  },
  {
    "text": "data store and you know hopefully people are asking questions and getting valuable insights and of course uh",
    "start": "474319",
    "end": "480639"
  },
  {
    "text": "because this is the year 2024 of course you know there is AI sprinkled on this",
    "start": "480639",
    "end": "486159"
  },
  {
    "text": "too because our investors have told us that we have to do that so why did I say speed right why",
    "start": "486159",
    "end": "493400"
  },
  {
    "text": "did I say that it has to be fast well how many of you have ever sat there and",
    "start": "493400",
    "end": "498879"
  },
  {
    "text": "um run a Prometheus query or run a Splunk query okay yes but how many okay",
    "start": "498879",
    "end": "504360"
  },
  {
    "text": "other the side of people who have run like a Prometheus query or a spun query or data dog query did it take 30 seconds",
    "start": "504360",
    "end": "510280"
  },
  {
    "text": "60 seconds or it come yeah that hands up right like it it took 30 seconds 60 seconds what if you could click the",
    "start": "510280",
    "end": "517240"
  },
  {
    "text": "runquery button take a sip of water by the time you take the step of",
    "start": "517240",
    "end": "523399"
  },
  {
    "text": "water you have the answers to your question that you just asked right if you get jerked out of that state of",
    "start": "523399",
    "end": "529600"
  },
  {
    "text": "developer experience and flow every single time that you have to wait to for the system to tell you what's going on",
    "start": "529600",
    "end": "535560"
  },
  {
    "text": "that's going to really make you feel not very happy as a developer right",
    "start": "535560",
    "end": "540720"
  },
  {
    "text": "so there's a lot of magic that we wound up trying to do under the hood to to make this feel as seamless as possible",
    "start": "540720",
    "end": "547760"
  },
  {
    "text": "um we're a go shop primarily we don't really use that much net um that's kind",
    "start": "547760",
    "end": "553200"
  },
  {
    "text": "of the a function of being a business uh in in North America that started in",
    "start": "553200",
    "end": "559240"
  },
  {
    "text": "2017 but kind of the core thing that that our users expect us to do is they",
    "start": "559240",
    "end": "564519"
  },
  {
    "text": "expect to kind of ask this kind of arbitrary question for instance like you know hey show me all the requests that",
    "start": "564519",
    "end": "573160"
  },
  {
    "text": "um let's group this by URL right so show me all of the requests",
    "start": "573160",
    "end": "579560"
  },
  {
    "text": "that are to this particular dashboard page which says uh Q summary right show",
    "start": "579560",
    "end": "585880"
  },
  {
    "text": "me that that specific thing right so I'm kind of formulating these queries on the Fly and expecting the query engine to",
    "start": "585880",
    "end": "592200"
  },
  {
    "text": "act on it really fast so this originally was inspired by A system that the",
    "start": "592200",
    "end": "597959"
  },
  {
    "text": "developers at Facebook which is now called meta came up with and that system is called scuba so scuba was a system",
    "start": "597959",
    "end": "605120"
  },
  {
    "text": "that was tremendously complex and also that was so expensive only Facebook",
    "start": "605120",
    "end": "610560"
  },
  {
    "text": "could run it uh specifically what it was doing was it was storing all this data in memory so that it could give people",
    "start": "610560",
    "end": "616399"
  },
  {
    "text": "Lightning Fast results the other thing that the developers at scuba did was they didn't really prioritize developer",
    "start": "616399",
    "end": "622600"
  },
  {
    "text": "experience they just said you know we're just going to put a command line UI on it you can write a SQL query on the command line and that's good enough",
    "start": "622600",
    "end": "628000"
  },
  {
    "text": "right like you know no one needs developer experience um so we decided that we",
    "start": "628000",
    "end": "633320"
  },
  {
    "text": "wanted to uh to to try to expand the system and to make it something that",
    "start": "633320",
    "end": "639079"
  },
  {
    "text": "would be affordable to mere mortals so initially the way that we decided we're going to do this was we said okay we're",
    "start": "639079",
    "end": "646320"
  },
  {
    "text": "going to buy a bunch of ec2 nodes and we are specifically going to buy a bunch of",
    "start": "646320",
    "end": "652440"
  },
  {
    "text": "ec2 nodes that have local uh nvme solid state drives on them and surely that's",
    "start": "652440",
    "end": "658399"
  },
  {
    "text": "going to be several of magnitude of cost lower than storing all the data in memory right like that's that's going to be cheap enough for users",
    "start": "658399",
    "end": "665120"
  },
  {
    "text": "right so there is still some sharting there is so still some capability for horizontal scaling right because you",
    "start": "665120",
    "end": "672240"
  },
  {
    "text": "can't fit everything on one single machine right like machines only scale vertically so big so the way that we",
    "start": "672240",
    "end": "677920"
  },
  {
    "text": "decided we were going to approach this was that we were going to use Kafka to farm out the data horizontally and just",
    "start": "677920",
    "end": "683440"
  },
  {
    "text": "splash it out across as many nodes as we needed whether it be you know 12 or 30 or 50 I think that Fleet grew to",
    "start": "683440",
    "end": "690760"
  },
  {
    "text": "something like 70 different nodes um and the idea was you know you would send",
    "start": "690760",
    "end": "696200"
  },
  {
    "text": "along the specification of what is the user trying to read what's the data that they're trying to access and you would",
    "start": "696200",
    "end": "703399"
  },
  {
    "text": "have each node go and query its local nvme storage and return a result and then all this would get merged back",
    "start": "703399",
    "end": "710000"
  },
  {
    "text": "together glorious right and the other trick that we thought was going to be really helpful which we borrowed from",
    "start": "710000",
    "end": "716240"
  },
  {
    "text": "scuba was the idea of the colar data store right the idea that instead of",
    "start": "716240",
    "end": "721800"
  },
  {
    "text": "being row based where you store each log line of data kind of one at a time serly so that you can only access each",
    "start": "721800",
    "end": "729399"
  },
  {
    "text": "individual row and each value within the row by looking up that Row first we said",
    "start": "729399",
    "end": "734720"
  },
  {
    "text": "we're going to split up all the data so if there is a given key value pair that's in in each log line we're going",
    "start": "734720",
    "end": "741079"
  },
  {
    "text": "to store all the values for each key together because chances are there's some duplication there maybe there's an",
    "start": "741079",
    "end": "746639"
  },
  {
    "text": "opportunity to do some compression because maybe you know the same HTP URL is accessed multiple",
    "start": "746639",
    "end": "752880"
  },
  {
    "text": "times uh by different users who are uh and therefore you can say okay this URL is repeated many times we're just going",
    "start": "752880",
    "end": "759160"
  },
  {
    "text": "to condense it into a single identifier but the other thing besides the compression benefit of this is that",
    "start": "759160",
    "end": "764800"
  },
  {
    "text": "it allows you to say you know let's go ahead and um and and make it so that I",
    "start": "764800",
    "end": "770440"
  },
  {
    "text": "can separately access all the HTP URLs and I can separately access all of the",
    "start": "770440",
    "end": "775680"
  },
  {
    "text": "build IDs but that I don't necessarily need to keep the data on user agent uh",
    "start": "775680",
    "end": "782040"
  },
  {
    "text": "right next to all the data on the HP URLs or the bill so if someone comes along and says I am specifically",
    "start": "782040",
    "end": "787720"
  },
  {
    "text": "interested in Bill IDs you don't have to sift through all that extraneous data and that really really helps",
    "start": "787720",
    "end": "794240"
  },
  {
    "text": "improve the speed because then you're not scanning data that's extraneous the other crucial Insight is",
    "start": "794240",
    "end": "801360"
  },
  {
    "text": "that you want to be able to reduce the amount of data that you're querying based off of the time range right it",
    "start": "801360",
    "end": "807560"
  },
  {
    "text": "doesn't make sense to read data that is between 59 and 60 days old if I've just",
    "start": "807560",
    "end": "812800"
  },
  {
    "text": "asked a question that's only about the most recent hour of data right I only need to find any particular files on",
    "start": "812800",
    "end": "818160"
  },
  {
    "text": "disk that might overlap with that one hour uh time range that I'm looking at so those are kind of some of the",
    "start": "818160",
    "end": "825199"
  },
  {
    "text": "tricks that we initially did to try to make this a commercially viable product and this really kind of enabled",
    "start": "825199",
    "end": "832360"
  },
  {
    "text": "people to uh to ask questions they that they couldn't before and today there's actually this really wide plethora of",
    "start": "832360",
    "end": "839839"
  },
  {
    "text": "solutions out there that are based off of Click house um because the folks at endex open sourced it and people found",
    "start": "839839",
    "end": "847160"
  },
  {
    "text": "it very attractive and useful for being able to do this kind of workload but we actually encountered an",
    "start": "847160",
    "end": "853320"
  },
  {
    "text": "interesting problem uh in in 2019 which is that more and more and more data was coming in and that data stopped fitting",
    "start": "853320",
    "end": "861199"
  },
  {
    "text": "on the individual nvme solid state drives of each of each box and the data that was uh we were",
    "start": "861199",
    "end": "869320"
  },
  {
    "text": "aging out the data that would no longer fit on disk and it turns out that it's not quite as useful if your tool says",
    "start": "869320",
    "end": "876199"
  },
  {
    "text": "for data that's 2 hours old oh I don't have that anymore like it expired it's like great like what use is it if I",
    "start": "876199",
    "end": "881639"
  },
  {
    "text": "can't even query back you know 24 hours so the first thing that we did",
    "start": "881639",
    "end": "887519"
  },
  {
    "text": "here was we said okay maybe we don't need to keep all this on local mvme solid state disc why don't we go ahead",
    "start": "887519",
    "end": "893639"
  },
  {
    "text": "and Implement data tearing let's just go ahead and tear all of that older data S3 because you know chances are",
    "start": "893639",
    "end": "899880"
  },
  {
    "text": "someone's not going to look at the data that's 60 days old very often but people are more likely to look at the data",
    "start": "899880",
    "end": "905040"
  },
  {
    "text": "that's 2 hours old or 3 hours old and you know we can kind of prioritize uh keeping the fresher data on local disk",
    "start": "905040",
    "end": "911639"
  },
  {
    "text": "and if we need to scrape data back from S3 that's okay so yay you know people can now keep 60 days of data rather than",
    "start": "911639",
    "end": "918959"
  },
  {
    "text": "us saying sorry we ate your data it's it's it's uh you have too much data um so you know yay people can run",
    "start": "918959",
    "end": "926720"
  },
  {
    "text": "queries over 60 days except for the reliability engineer in me is cringing because uh oh no people can run queries",
    "start": "926720",
    "end": "933800"
  },
  {
    "text": "over 60 days and suddenly we've gone from you know I can finish running a query in less than a time it takes to",
    "start": "933800",
    "end": "939560"
  },
  {
    "text": "take a drink of water so it's sitting there and spinning",
    "start": "939560",
    "end": "944680"
  },
  {
    "text": "and spinning and spinning and spinning so fundamentally the problem",
    "start": "944680",
    "end": "950639"
  },
  {
    "text": "here is a mismatch between the amount of compute that and network bandwith that",
    "start": "950639",
    "end": "956519"
  },
  {
    "text": "was available on each on each of our local nodes versus the amount of storage in all of",
    "start": "956519",
    "end": "962880"
  },
  {
    "text": "S3 that we could potentially pile up from our customers and fundamentally this is a problem that I encounter in my",
    "start": "962880",
    "end": "968880"
  },
  {
    "text": "household as well which is that my dog really wants to play with all of her toys at once simultaneously when she's",
    "start": "968880",
    "end": "975560"
  },
  {
    "text": "like romping around and being like you know I'm going to go and Chase the thing oops I forgot to bring it back to you right like and you just throw more and",
    "start": "975560",
    "end": "980920"
  },
  {
    "text": "more and more toys out of the basket into your into your house and then the dog is is tired and she goes to sleep",
    "start": "980920",
    "end": "987560"
  },
  {
    "text": "and she's like actually I don't want to play anymore right and now you're stuck with all these toys scattered all over",
    "start": "987560",
    "end": "992600"
  },
  {
    "text": "your floor so fundamentally we knew that we couldn't just keep on scaling out our",
    "start": "992600",
    "end": "999440"
  },
  {
    "text": "compute Fleet because people are quering that data a lot less often than uh than",
    "start": "999440",
    "end": "1007120"
  },
  {
    "text": "they're writing the data so the answer to this turned out to be serverless for us that serverless was",
    "start": "1007120",
    "end": "1013920"
  },
  {
    "text": "something where the cloud provider has compute lying around and they're willing to Le it you for a fee and only charge",
    "start": "1013920",
    "end": "1020959"
  },
  {
    "text": "you for the time that you actually use so we were able to say okay if the",
    "start": "1020959",
    "end": "1026280"
  },
  {
    "text": "data has aged out and it's stored on S3 rather than stored in local disk then why don't we go ahead and farm",
    "start": "1026280",
    "end": "1033480"
  },
  {
    "text": "out that work that's querying across all of these files that are stored on S3 there's no reason that it has to be",
    "start": "1033480",
    "end": "1039438"
  },
  {
    "text": "queried by the node that originally wrote those files to S3 but instead you can just have an arbitrary compute task",
    "start": "1039439",
    "end": "1046199"
  },
  {
    "text": "that's running somewhere I don't care where go ahead and pick up the list of files to read and to pick up the uh kind",
    "start": "1046199",
    "end": "1053280"
  },
  {
    "text": "of query expression that you want to run against it and go ahead and and ask and",
    "start": "1053280",
    "end": "1058400"
  },
  {
    "text": "and and then merge those those answers together so the only data that really matters for locality is the most recent",
    "start": "1058400",
    "end": "1064240"
  },
  {
    "text": "data that hasn't been tiered to uh to solid state yet or tiered to S3 yet so",
    "start": "1064240",
    "end": "1070000"
  },
  {
    "text": "this has a really beautiful property of suddenly instead of a linear or superlinear pattern of of coral lency",
    "start": "1070000",
    "end": "1077520"
  },
  {
    "text": "now suddenly it's sublinear that you can query an order of magnitude more more",
    "start": "1077520",
    "end": "1082760"
  },
  {
    "text": "stuff and and have it take less than an order of magnitude more time and amazingly enough I don't",
    "start": "1082760",
    "end": "1090640"
  },
  {
    "text": "understand exactly you know how why they decided to let us get away with this but um they are willing to rent compute to",
    "start": "1090640",
    "end": "1098240"
  },
  {
    "text": "you in one millisecond units which means that if you don't actually use that compute for any given time range you",
    "start": "1098240",
    "end": "1104480"
  },
  {
    "text": "don't you don't pay for it even though they're keeping your your workers nice and warmed up for you",
    "start": "1104480",
    "end": "1110440"
  },
  {
    "text": "so that's kind of the in a nutshell story of how we used serverless",
    "start": "1110440",
    "end": "1116240"
  },
  {
    "text": "Computing to to try to make our query engine run faster so there are some caveats and",
    "start": "1116240",
    "end": "1122799"
  },
  {
    "text": "asterisks here um so this now we're getting into part two of The Talk which is let's talk about some of the",
    "start": "1122799",
    "end": "1128440"
  },
  {
    "text": "asterisks in this uh in this beautiful slide that says oh it's only three to four times as expensive and it typically",
    "start": "1128440",
    "end": "1135159"
  },
  {
    "text": "returns within 2.5 seconds well uh what happens what happens when that when that goes a little bit um so we experienced",
    "start": "1135159",
    "end": "1143960"
  },
  {
    "text": "very very uh large growth uh from 2019 to 20 uh carrying over to the current",
    "start": "1143960",
    "end": "1149520"
  },
  {
    "text": "year in 2024 um so we've grown by more than order of magnitude in that time uh",
    "start": "1149520",
    "end": "1154760"
  },
  {
    "text": "both in terms of how much data we're ingesting which impacts the scale of the system and also how much people are",
    "start": "1154760",
    "end": "1159919"
  },
  {
    "text": "querying that data so I actually ran the numbers and yes we are actually petabyte scale uh we passed one PAB of data under",
    "start": "1159919",
    "end": "1167960"
  },
  {
    "text": "management um about a month ago which is which is pretty",
    "start": "1167960",
    "end": "1173120"
  },
  {
    "text": "exciting and here's here's kind of where things break down when you are starting to use serverless functions to to look",
    "start": "1173120",
    "end": "1180600"
  },
  {
    "text": "at p a paby of data so first let's talk about the scaling so yes it turns out you know we",
    "start": "1180600",
    "end": "1188240"
  },
  {
    "text": "would love it if AWS Lambda kept our workers warmed up 100% of the time across their entire fleet but guess what",
    "start": "1188240",
    "end": "1195039"
  },
  {
    "text": "they have other clients besides us too right so in General your serverless",
    "start": "1195039",
    "end": "1200400"
  },
  {
    "text": "provider wants to keep an appropriate number of workers that are pre-scaled",
    "start": "1200400",
    "end": "1205559"
  },
  {
    "text": "and available to you and they might let you cold start more workers and kind of boot up more workers if your load",
    "start": "1205559",
    "end": "1212120"
  },
  {
    "text": "increases but their kind of operating theory is based off of you know web services or things where you're using an",
    "start": "1212120",
    "end": "1218760"
  },
  {
    "text": "API Gateway to invoke a Lambda where the load is a little bit more predictable so their idea is essentially",
    "start": "1218760",
    "end": "1225159"
  },
  {
    "text": "you know they will raise your concurrency limit if they see that you're repeatedly hitting hitting your limits but that if you burst over your",
    "start": "1225159",
    "end": "1232880"
  },
  {
    "text": "limit they have they reserve the right to send you an error message that says you know slow down try again later like",
    "start": "1232880",
    "end": "1238159"
  },
  {
    "text": "you know we're not ready for you quite yet but when you have a situation where",
    "start": "1238159",
    "end": "1244360"
  },
  {
    "text": "someone comes along like me right like someone comes along like me and runs and runs a query and it tries to invoke 10,000",
    "start": "1244360",
    "end": "1252039"
  },
  {
    "text": "different tasks right that might breach our burst Limit but only for maybe 5",
    "start": "1252039",
    "end": "1257760"
  },
  {
    "text": "seconds and then that load goes away and uh originally it took uh it was one",
    "start": "1257760",
    "end": "1263880"
  },
  {
    "text": "minute of sustained load that was required for the concurrency limit to be bumped but the good news is at least on",
    "start": "1263880",
    "end": "1270480"
  },
  {
    "text": "the Amazon side I cannot speak to the Azure side um they changed this recently to be like 10 seconds that if you are",
    "start": "1270480",
    "end": "1277240"
  },
  {
    "text": "exceeding your limit for 10 seconds they'll give you a th work concurrent workers and if you're still exceeding your limit 10 seconds later after that",
    "start": "1277240",
    "end": "1282760"
  },
  {
    "text": "they'll give you another thousand concurrent workers so it actually turns out to be less of a problem for this",
    "start": "1282760",
    "end": "1287799"
  },
  {
    "text": "kind of workload today than it was when we originally started doing this 5 years ago and one of the fun things is uh",
    "start": "1287799",
    "end": "1294760"
  },
  {
    "text": "because we dog food uh we can actually verify how much real world concurrency",
    "start": "1294760",
    "end": "1300240"
  },
  {
    "text": "are we actually seeing from the cloud provider and does it match the negotiations and expectations that we have with our cloud provider about what",
    "start": "1300240",
    "end": "1306720"
  },
  {
    "text": "they what they've said they've capacity planned for us and that by the way is a little bit of an asterisk right the",
    "start": "1306720",
    "end": "1312799"
  },
  {
    "text": "whole theory of serverless is you walk up to your cloud provider and you say you know one compute please and it just",
    "start": "1312799",
    "end": "1318159"
  },
  {
    "text": "gives it you right but here we are having to manually capacity plan with our cloud provider because we are this",
    "start": "1318159",
    "end": "1325159"
  },
  {
    "text": "odd shape that is very bursty so they want to understand when are we scaling how are we",
    "start": "1325159",
    "end": "1330360"
  },
  {
    "text": "scaling so there are limits um but the key thing that you can do if your",
    "start": "1330360",
    "end": "1336360"
  },
  {
    "text": "workload is something that is you know that that is a retri a retriable",
    "start": "1336360",
    "end": "1341480"
  },
  {
    "text": "workload which a read only workload is fundamentally retryable you can just keep on retrying until the request",
    "start": "1341480",
    "end": "1347360"
  },
  {
    "text": "eventually succeeds because because your your rate limit has been raised high enough and yes uh talk to the people at",
    "start": "1347360",
    "end": "1353120"
  },
  {
    "text": "your lovely cloud provider if you're if you're going to be using serverless functions to do big data",
    "start": "1353120",
    "end": "1358480"
  },
  {
    "text": "analytics okay let's talk about the next thing cold starts so as I",
    "start": "1358480",
    "end": "1363799"
  },
  {
    "text": "mentioned most of the time um if you are running a fairly predictable workload",
    "start": "1363799",
    "end": "1369559"
  },
  {
    "text": "your cloud provider will keep a pre-warmed copy of your code that's running and ready to receive a request",
    "start": "1369559",
    "end": "1375120"
  },
  {
    "text": "at any given time but that's not always guaranteed so sometimes you have to do a cold start and when you do a cold start",
    "start": "1375120",
    "end": "1383080"
  },
  {
    "text": "you are paying some amount of time to do static initialization so people invest a lot of",
    "start": "1383080",
    "end": "1389440"
  },
  {
    "text": "effort into trying to reduce their the impact of of a cold start because you",
    "start": "1389440",
    "end": "1394520"
  },
  {
    "text": "cannot guarantee that you're not going to have a cold start so the best thing that you can do is kind of frontload as much initialization as possible maybe",
    "start": "1394520",
    "end": "1401880"
  },
  {
    "text": "that means you know serializing a bunch of state and just loading it into memory right away as opposed to fetching it",
    "start": "1401880",
    "end": "1407080"
  },
  {
    "text": "from a remote host maybe it can mean deferring some of that execution until you actually need it right like there",
    "start": "1407080",
    "end": "1413360"
  },
  {
    "text": "are a number of techniques that can be really helpful but at the end of the day it is still even in this kind of worst",
    "start": "1413360",
    "end": "1419760"
  },
  {
    "text": "case scenario of 13 milliseconds 50 milliseconds for a cold start that is blazing fast compared to the cost of",
    "start": "1419760",
    "end": "1426760"
  },
  {
    "text": "spinning up a new ec2 instance or spinning up a new uh Azure Azure Compu instance right that if you have to wait",
    "start": "1426760",
    "end": "1433360"
  },
  {
    "text": "for Linux to boot that's going to take on the order of seconds at least right",
    "start": "1433360",
    "end": "1438799"
  },
  {
    "text": "whereas we can get this compute fresh delivered to us and ready to execute a request most of the time within 50",
    "start": "1438799",
    "end": "1445600"
  },
  {
    "text": "milliseconds and the compute on the back end right like because again like serverless functions abstract this away",
    "start": "1445600",
    "end": "1452559"
  },
  {
    "text": "from us they'll go ahead and allow some of these workers to fall back asleep",
    "start": "1452559",
    "end": "1458039"
  },
  {
    "text": "rather than killing them after each request is done which means in an optimal situation you wind up with a",
    "start": "1458039",
    "end": "1463400"
  },
  {
    "text": "worker being hot and available to run a request whenever and I actually went and pulled this this morning um so you can",
    "start": "1463400",
    "end": "1470200"
  },
  {
    "text": "actually see uh that roughly it looks like we have at at any given time there",
    "start": "1470200",
    "end": "1475399"
  },
  {
    "text": "are 50,000 workers that are kind of pre-warmed for us and they'll be kind of pressed into Service uh when when",
    "start": "1475399",
    "end": "1482240"
  },
  {
    "text": "there's a a query workload and then if there's not a sustained query workload for a while then they'll suddenly shut",
    "start": "1482240",
    "end": "1488240"
  },
  {
    "text": "them down right they'll say okay like you know we're no longer allowing you to keep these 50 50,000 workers suspended",
    "start": "1488240",
    "end": "1495520"
  },
  {
    "text": "you know because it still is consuming memory right like it's even if it's not actively consuming CPU it's still",
    "start": "1495520",
    "end": "1500960"
  },
  {
    "text": "sitting there consuming resources so they manage this whole process of capacity planning to shut down workers",
    "start": "1500960",
    "end": "1506840"
  },
  {
    "text": "that we're not using and sure enough you can see right like you know after um after it's been like 5 minutes without",
    "start": "1506840",
    "end": "1512919"
  },
  {
    "text": "someone running a query they'll cut the number of workers down to 10,000 and then 2,000 until until someone starts",
    "start": "1512919",
    "end": "1519399"
  },
  {
    "text": "quering again okay the second kind of uh pedantic systems operator thing 90% of",
    "start": "1519399",
    "end": "1527480"
  },
  {
    "text": "our queries return return within 2 uh 90% of our queries against ads Lambda returned within 2.5 seconds turns out",
    "start": "1527480",
    "end": "1534880"
  },
  {
    "text": "P90 is not really a great metric to use because it means 10% of your users can be having a worse experience than your",
    "start": "1534880",
    "end": "1541200"
  },
  {
    "text": "P90 so you know it's one thing for me to say you know our P90 is great everything is you know f as fast as taking a sip of",
    "start": "1541200",
    "end": "1547960"
  },
  {
    "text": "water but what happens about the 10% that are slower than that so it turns out",
    "start": "1547960",
    "end": "1554760"
  },
  {
    "text": "that sometimes you will stand up the Lambda worker and tell it go read this",
    "start": "1554760",
    "end": "1559799"
  },
  {
    "text": "collection of S3 files and the Lambda worker will go ahead and talk to S3 and say please give me these files so I can",
    "start": "1559799",
    "end": "1565080"
  },
  {
    "text": "run this compute on them and S3 will say okay I got your",
    "start": "1565080",
    "end": "1570919"
  },
  {
    "text": "request and it'll never get back to you but the answer there is uh we have this",
    "start": "1570919",
    "end": "1576640"
  },
  {
    "text": "idea of impatience right so we when when 99% of the requests have finished we'll",
    "start": "1576640",
    "end": "1582679"
  },
  {
    "text": "just automatically choose to retry the slowest 1% of requests because our",
    "start": "1582679",
    "end": "1587799"
  },
  {
    "text": "experience with with cloudbased block storage or object storage is that a request that failed the first time is",
    "start": "1587799",
    "end": "1594120"
  },
  {
    "text": "not necessarily guaranteed to fail a second time more often than not the problem is with whichever server is",
    "start": "1594120",
    "end": "1599840"
  },
  {
    "text": "handling the read rather than the object on itself on the back and being",
    "start": "1599840",
    "end": "1606320"
  },
  {
    "text": "inaccessible the other little caveat here is uh again this is cloud provider dependent but we have been really really",
    "start": "1606320",
    "end": "1612679"
  },
  {
    "text": "frustrated with uh the fact that Amazon has designed uh it was Lambda to work",
    "start": "1612679",
    "end": "1619559"
  },
  {
    "text": "with API Gateway to work with web request to work with httv request response so every request has to in",
    "start": "1619559",
    "end": "1624960"
  },
  {
    "text": "their view serialized to either an HTTP request or serialized to Json but it",
    "start": "1624960",
    "end": "1630520"
  },
  {
    "text": "turns out in our case what we have is a bunch of Proto Buffs that say you know here's the encapsulation of the query",
    "start": "1630520",
    "end": "1636240"
  },
  {
    "text": "specification here's the encapsulation of the list of files why am I spending all this effort",
    "start": "1636240",
    "end": "1641840"
  },
  {
    "text": "writing it into Json and then reading it out of Json to serialize it Brack into protuff right like Json is not a great",
    "start": "1641840",
    "end": "1647559"
  },
  {
    "text": "TR High bandwidth High throughput transport mechanism um so in some cases we wind up sending the data uh by",
    "start": "1647559",
    "end": "1654320"
  },
  {
    "text": "literally using S3 as a as a uh as a wire protocol so we'll write the uh pay",
    "start": "1654320",
    "end": "1661240"
  },
  {
    "text": "the the request payload into an S3 file and send a link to that instead but it's it's a little bit frustrating and janky",
    "start": "1661240",
    "end": "1668039"
  },
  {
    "text": "similarly the return value let's suppose someone asks a really complex query that returns you know a thousand different",
    "start": "1668039",
    "end": "1674159"
  },
  {
    "text": "groups and you know has has 100 or or 200 or 500 different uh time time granularity buckets well great now that",
    "start": "1674159",
    "end": "1681840"
  },
  {
    "text": "is more than 6 megabytes and um Lambda will truncate that response so again we",
    "start": "1681840",
    "end": "1687240"
  },
  {
    "text": "have to use S3 as a message passing interface and finally I think there's",
    "start": "1687240",
    "end": "1692760"
  },
  {
    "text": "this kind of question of cost um so yes per millisecond of compute and per per",
    "start": "1692760",
    "end": "1700080"
  },
  {
    "text": "CPU millisecond of compute uh Lambda is about three to four times as expensive as",
    "start": "1700080",
    "end": "1705840"
  },
  {
    "text": "ec2 and this may seem like it's a horrible deal right like why would you ever voluntarily pay three to four times",
    "start": "1705840",
    "end": "1712399"
  },
  {
    "text": "as much except for if you remember that slide that I was showing you back here right of our",
    "start": "1712399",
    "end": "1718399"
  },
  {
    "text": "utilization our average utilization is less than 5% right so effectively AWS is only",
    "start": "1718399",
    "end": "1726320"
  },
  {
    "text": "charging us for the seconds that we're running so they're kind of effectively giving us a 20x discount on this compute",
    "start": "1726320",
    "end": "1733600"
  },
  {
    "text": "so the fact that they're charging us four times as much per second that we're actually running is vast outweighed by",
    "start": "1733600",
    "end": "1739519"
  },
  {
    "text": "the fact that they don't charge us when things are not running but it is still something that",
    "start": "1739519",
    "end": "1744799"
  },
  {
    "text": "we keep an eye on because um if you've ever how many of you have ever accidentally set off a Lambda that set",
    "start": "1744799",
    "end": "1750720"
  },
  {
    "text": "off another Lambda that then called the first Lambda right like how many of you have ever kind of created created a Lambda Loop yeah those are no fun right",
    "start": "1750720",
    "end": "1757640"
  },
  {
    "text": "so we don't have Lambda Loops because our lambdas are not capable of invoking other lambas but we do have the",
    "start": "1757640",
    "end": "1763679"
  },
  {
    "text": "phenomenon of someone I don't know something might go wrong and someone might run query and they'll be like oh",
    "start": "1763679",
    "end": "1769000"
  },
  {
    "text": "that's taking a while I'm going to press run query again I'm going to press run query again and suddenly that starts",
    "start": "1769000",
    "end": "1774440"
  },
  {
    "text": "really really adding up so we've had to set alerts that are based off of the Lambda cost and it",
    "start": "1774440",
    "end": "1779919"
  },
  {
    "text": "turns out Computing the Lambda cost is pretty straightforward because you multiply the duration of the invoke by the price per per millisecond of the",
    "start": "1779919",
    "end": "1786360"
  },
  {
    "text": "invoke and it's glorious the other interesting thing uh I was actually talking at with the",
    "start": "1786360",
    "end": "1791880"
  },
  {
    "text": "lovely Folks at at the seek Booth uh yesterday about this is even if we",
    "start": "1791880",
    "end": "1797640"
  },
  {
    "text": "shrunk our Lambda bill by you know 50% or 90% that still does not get rid of the",
    "start": "1797640",
    "end": "1804919"
  },
  {
    "text": "kind of fixed costs regardless of the amount of of time that your Lambda runs for right if you're sending a body that",
    "start": "1804919",
    "end": "1812039"
  },
  {
    "text": "says here are the files to read and here's the payload to execute if that payload is too large or you have really",
    "start": "1812039",
    "end": "1818919"
  },
  {
    "text": "really really tiny files and you have very many of them suddenly the cost of telling Lambda you know that I want to",
    "start": "1818919",
    "end": "1826720"
  },
  {
    "text": "invoke this this payload if your Lambda payload is you know becomes a megabyte or two",
    "start": "1826720",
    "end": "1832000"
  },
  {
    "text": "megabytes they're going to charge you for that they're going to charge you for the network cost to actually send them",
    "start": "1832000",
    "end": "1837919"
  },
  {
    "text": "that request and that winds up being not ins substantial so that's the green cost here and then there's the S3 request",
    "start": "1837919",
    "end": "1844840"
  },
  {
    "text": "cost so if your S3 files are not large enough then you're going to have a",
    "start": "1844840",
    "end": "1850039"
  },
  {
    "text": "situation where you're paying more to access the files for the privilege of executing a get request against set file",
    "start": "1850039",
    "end": "1856240"
  },
  {
    "text": "in S3 then you for the actual computation involved however it's too it's easy of course to go the other way",
    "start": "1856240",
    "end": "1862679"
  },
  {
    "text": "right and if you create files that are a gigaby in size and you only need 50 megabytes of that file congratulations",
    "start": "1862679",
    "end": "1868880"
  },
  {
    "text": "now you're burning a bunch of CPU to decompress the file to pick out only the parts that you need right so there's",
    "start": "1868880",
    "end": "1874159"
  },
  {
    "text": "this kind of interesting Balancing Act and I don't necessarily see people talk about this a lot because people talk",
    "start": "1874159",
    "end": "1879320"
  },
  {
    "text": "about the idea of you know I'm going to use a Lambda power tuner right like I'm going to optimize the size of my lambdas to right siiz the amount of CPU and",
    "start": "1879320",
    "end": "1885440"
  },
  {
    "text": "memory according to the workload which is well and good but there's another dimension that you have to think about",
    "start": "1885440",
    "end": "1890679"
  },
  {
    "text": "when you are doing big data analytics workloads which is if you are using a",
    "start": "1890679",
    "end": "1896039"
  },
  {
    "text": "scalable data storage solution like S3 the cost of that S3 also matters you kind of have to manage that trade-off of",
    "start": "1896039",
    "end": "1902480"
  },
  {
    "text": "size of each object versus the cost to process each object but one dimension that we do have",
    "start": "1902480",
    "end": "1907720"
  },
  {
    "text": "control over is the kind of compute that we use um so again this is kind of a",
    "start": "1907720",
    "end": "1913120"
  },
  {
    "text": "little bit of a violation of the serverless abstraction barrier right like theoretically",
    "start": "1913120",
    "end": "1918600"
  },
  {
    "text": "you know if I had a Python program I could just stick the Python program up on AWS and tell them you know hey you",
    "start": "1918600",
    "end": "1924960"
  },
  {
    "text": "make this Python program run I don't care where I don't care how I don't care what architecture it uses but in reality",
    "start": "1924960",
    "end": "1931919"
  },
  {
    "text": "um it turns out that even python programs you might have native C",
    "start": "1931919",
    "end": "1937200"
  },
  {
    "text": "bindings for those python programs right like you actually need to tell AWS no like you know my Python program that",
    "start": "1937200",
    "end": "1944000"
  },
  {
    "text": "expecting you to run on Lambda only runs on x86 or this python has been built",
    "start": "1944000",
    "end": "1949440"
  },
  {
    "text": "only for arm so there is kind of this control knob of what Hardware do you use",
    "start": "1949440",
    "end": "1956320"
  },
  {
    "text": "that your cloud provider May expose to you in the course of running your serverless functions and some of those knobs for which Hardware you you want to",
    "start": "1956320",
    "end": "1963200"
  },
  {
    "text": "use might offer Better Price performance but the devil's always in the details of",
    "start": "1963200",
    "end": "1968399"
  },
  {
    "text": "testing to your specific workload right like there's this glorious marketing release from ad us that on September",
    "start": "1968399",
    "end": "1973960"
  },
  {
    "text": "29th 2021 that says you can get up to 34% Better Price performance so we decided we were going",
    "start": "1973960",
    "end": "1981120"
  },
  {
    "text": "to give it a go um especially because we'd previously found that uh when we",
    "start": "1981120",
    "end": "1987240"
  },
  {
    "text": "were migrating some of our ec2 workload to run on arm 64 processors that it was",
    "start": "1987240",
    "end": "1993240"
  },
  {
    "text": "about 30 it was about 30% more cost efficient so we were like yeah okay that that smells about right up to 34% Better",
    "start": "1993240",
    "end": "1999440"
  },
  {
    "text": "Price performance so we tried it um and again we were able to try this because",
    "start": "1999440",
    "end": "2006080"
  },
  {
    "text": "you know if you're using a language like c.net go right like there's the ability",
    "start": "2006080",
    "end": "2011360"
  },
  {
    "text": "to specify just with an environment variable or with like a compile time command that just just says what Target",
    "start": "2011360",
    "end": "2017360"
  },
  {
    "text": "architecture do you want to Target if you are using a language that doesn't have that or where you're kind of using",
    "start": "2017360",
    "end": "2023399"
  },
  {
    "text": "native dependencies that may or may not be compiled for the architecture well you might be a little bit more out of luck um but in our case we were able to",
    "start": "2023399",
    "end": "2031440"
  },
  {
    "text": "tweak an environment variable produce artifacts for both architectures and and try it uh but then you run the problem",
    "start": "2031440",
    "end": "2039039"
  },
  {
    "text": "of verifying you know the cloud vendor will can promise you the sun and the moon but do they actually deliver for",
    "start": "2039039",
    "end": "2045279"
  },
  {
    "text": "your specific workload so we went ahead and we tried",
    "start": "2045279",
    "end": "2050720"
  },
  {
    "text": "uh you know we we previously had success with some of our fixed workloads that were using ec2 instances or eks but we",
    "start": "2050720",
    "end": "2057560"
  },
  {
    "text": "wanted to try this with with Lambda so we uploaded a zip file with our code",
    "start": "2057560",
    "end": "2063000"
  },
  {
    "text": "that was compiled for for arm 64 so we could run an AB test of arm vers versus",
    "start": "2063000",
    "end": "2068240"
  },
  {
    "text": "versus x86 and it turns out that the cloud",
    "start": "2068240",
    "end": "2073320"
  },
  {
    "text": "provider can say you know we're going to Discount the price of this by 20% and they can say you know in theory your",
    "start": "2073320",
    "end": "2079040"
  },
  {
    "text": "stuff should finish running 10% faster because the processor is 10% faster so that's there's where they get that 34%",
    "start": "2079040",
    "end": "2085240"
  },
  {
    "text": "performance number our experience was not nearly as rosy at least and optimized for this",
    "start": "2085240",
    "end": "2090919"
  },
  {
    "text": "workload so we uploaded the two zip files you know compiled with different options and it turns out that the arm 64",
    "start": "2090919",
    "end": "2098320"
  },
  {
    "text": "option took literally twice as long to execute the same data analytics workload",
    "start": "2098320",
    "end": "2104640"
  },
  {
    "text": "as as the x86 uh as the x86 version so that was a little bit of a head scratcher for us we were like why did we",
    "start": "2104640",
    "end": "2111520"
  },
  {
    "text": "see such good results with one workload and see such bad results on Lambda right like surely they're using the same",
    "start": "2111520",
    "end": "2117960"
  },
  {
    "text": "processors right like you know it's it's it's all this new graviton chip that they put out so the first thing to do",
    "start": "2117960",
    "end": "2125359"
  },
  {
    "text": "obviously is you don't just sit there with degraded customer experience while you're trying to figure it out right",
    "start": "2125359",
    "end": "2131800"
  },
  {
    "text": "like you know you have feature Flags you have control knobs to say I'm going to limit the blast radius of this feature",
    "start": "2131800",
    "end": "2138400"
  },
  {
    "text": "because otherwise you know you're you're going to set users up for having a really bad experience so I tweaked the",
    "start": "2138400",
    "end": "2145079"
  },
  {
    "text": "control knob and immediately was shut it off and uh and our customers were back to seeing good results so now kind of",
    "start": "2145079",
    "end": "2151680"
  },
  {
    "text": "came the performance engineering exercise of why was this so slow so turn turn out that uh again like",
    "start": "2151680",
    "end": "2159440"
  },
  {
    "text": "you know serverless functions believe it or not they run on real physical",
    "start": "2159440",
    "end": "2165240"
  },
  {
    "text": "servers so uh AWS had had launched this in September of 2021 but they hadn't",
    "start": "2165240",
    "end": "2172079"
  },
  {
    "text": "necessarily built out all that capacity to the same levels as they built out x86 Lambda because x86 Lambda has existed",
    "start": "2172079",
    "end": "2178960"
  },
  {
    "text": "forever in a day right it's it was one of the earliest products that that Amazon put out but this kind of pool of",
    "start": "2178960",
    "end": "2185359"
  },
  {
    "text": "resources available to execute arm request meant that we were a large enough when we just you know naively",
    "start": "2185359",
    "end": "2191280"
  },
  {
    "text": "flipped that feature flag to 50% suddenly we became the single largest customer of that of that new product",
    "start": "2191280",
    "end": "2197720"
  },
  {
    "text": "that they'd put out so that meant we were contending with ourselves every time we would say you know I would like",
    "start": "2197720",
    "end": "2203200"
  },
  {
    "text": "to run 1 million Lambda invokes right now please we would contend ourselves and that would slow it",
    "start": "2203200",
    "end": "2208240"
  },
  {
    "text": "down the other factors were that um we were using an earlier version of the go run time that did not have as optimized",
    "start": "2208240",
    "end": "2215400"
  },
  {
    "text": "of a support for for m64 in particular um this is kind of a little bit of",
    "start": "2215400",
    "end": "2220480"
  },
  {
    "text": "compiler inside baseball but like it's a lot faster to pass arguments between",
    "start": "2220480",
    "end": "2226319"
  },
  {
    "text": "your function calls in the registers rather than expecting to pop them to the stack and to and and to push them back",
    "start": "2226319",
    "end": "2232000"
  },
  {
    "text": "out right like it it just it just makes a lot a lot better sense to store the data in the registers and go hadn't",
    "start": "2232000",
    "end": "2237920"
  },
  {
    "text": "implemented that optimization yet for for arm and another thing that we found was",
    "start": "2237920",
    "end": "2242960"
  },
  {
    "text": "that uh some of the libraries that we're using because we have this tradeoff of you know again going to the cost of the",
    "start": "2242960",
    "end": "2249599"
  },
  {
    "text": "compute and the cost of the storage we had decided you know we wanted to compress our data before writing it to",
    "start": "2249599",
    "end": "2257160"
  },
  {
    "text": "S3 but the problem with compressing your data before writing it to S3 is now you need to pay the tax of decompressing the",
    "start": "2257160",
    "end": "2263000"
  },
  {
    "text": "data every time you want to read it and if you happen to be using a library that does not have a native implementation",
    "start": "2263000",
    "end": "2269280"
  },
  {
    "text": "and you are just running naive compiled code it's not necessarily going to be super performant and",
    "start": "2269280",
    "end": "2275839"
  },
  {
    "text": "fast so we able to schedule doing uh you know turning on and off our feature",
    "start": "2275839",
    "end": "2281240"
  },
  {
    "text": "flags and kind of profiling things as we went in order to determine when was safe for us to proceed and eventually we were",
    "start": "2281240",
    "end": "2287960"
  },
  {
    "text": "able to hit 99% of our uh of our Lambda executions running against the new processor architecture and more",
    "start": "2287960",
    "end": "2294760"
  },
  {
    "text": "importantly right like we are able to actually save that money that right like that 20 to",
    "start": "2294760",
    "end": "2300319"
  },
  {
    "text": "30% and interestingly the way that we chose to optimize that given the constraints of you know it takes a",
    "start": "2300319",
    "end": "2307079"
  },
  {
    "text": "certain amount of time to read a file off of S3 each Lambda has a specific amount of network bandwith available to",
    "start": "2307079",
    "end": "2312480"
  },
  {
    "text": "it the way that we achieved that savings was not by trying to shave down the amount of time that each Lambda took to",
    "start": "2312480",
    "end": "2318520"
  },
  {
    "text": "run on arm but instead we said we're going to make our arm lambdas and our",
    "start": "2318520",
    "end": "2324200"
  },
  {
    "text": "x86 lambdas take about the same amount of time to execute any given request but we'll shave off the amount of CPU and",
    "start": "2324200",
    "end": "2331119"
  },
  {
    "text": "memory allocated to each Lambda worker because they charge you by the V virtual CPU second so",
    "start": "2331119",
    "end": "2338000"
  },
  {
    "text": "that's how we broke what was otherwise shaping up to be a really really bad cost curve right like we are on track to",
    "start": "2338000",
    "end": "2344319"
  },
  {
    "text": "spend so much money on Lambda if we' continued on our trajectory through uh through 2022 but instead we're actually just now",
    "start": "2344319",
    "end": "2352599"
  },
  {
    "text": "in 2024 starting to see the levels of expenditure on Lambda that we were seeing in 2022 now would it have been",
    "start": "2352599",
    "end": "2359400"
  },
  {
    "text": "cheaper to run all of this you know to have our product be slower and just run all of this sequentially on on on Raw on",
    "start": "2359400",
    "end": "2366880"
  },
  {
    "text": "Raw VM sure but developer experience on Flow are worth spending money on right so",
    "start": "2366880",
    "end": "2374800"
  },
  {
    "text": "anytime someone asks me a question like you know hey is it worth it spending money on um on build kite or Circle C",
    "start": "2374800",
    "end": "2381200"
  },
  {
    "text": "right like my answer is yes right like there is a huge difference between your build time taking 15 minutes and your",
    "start": "2381200",
    "end": "2387920"
  },
  {
    "text": "build time taking two hours right that spending that right like throwing money",
    "start": "2387920",
    "end": "2393319"
  },
  {
    "text": "at making your tests run faster is very much worth it so that you don't have to worry about oh I'm going to turn off",
    "start": "2393319",
    "end": "2399200"
  },
  {
    "text": "these tests or oh like you know I can't afford to pay for good observability so I'm just going to go and use the shitty",
    "start": "2399200",
    "end": "2404359"
  },
  {
    "text": "thing that sits there and Spins for for three minutes right your developer time and your flow",
    "start": "2404359",
    "end": "2410800"
  },
  {
    "text": "is so much more valuable than any amount of money really like almost any amount of money that you could spend on on ABS",
    "start": "2410800",
    "end": "2416440"
  },
  {
    "text": "Lambda like this and the same thing goes for you know your for your data analyst the same thing goes for your data",
    "start": "2416440",
    "end": "2421800"
  },
  {
    "text": "scientist right like this is not just an observability thing this is anytime that you have the ability to throw compute at",
    "start": "2421800",
    "end": "2428119"
  },
  {
    "text": "a problem to make a a engineer or developer or analyst happier you should do",
    "start": "2428119",
    "end": "2433319"
  },
  {
    "text": "that so uh you know here are some some charts and graphs illustrating that phenomenon of you know sure the arm",
    "start": "2433319",
    "end": "2440880"
  },
  {
    "text": "processors right when we starve them of CPU to the bare minimum they might run",
    "start": "2440880",
    "end": "2446319"
  },
  {
    "text": "slightly slower than the x86 workers at the way with the way that we tuned it",
    "start": "2446319",
    "end": "2451560"
  },
  {
    "text": "but we are using less compute per worker to achieve that result and also",
    "start": "2451560",
    "end": "2457800"
  },
  {
    "text": "each CPU each CPU millisecond was costing about 20% less so this this was a good outcome for for us and a good",
    "start": "2457800",
    "end": "2464319"
  },
  {
    "text": "outcome for our users because it meant that we have not needed to say you know hey by the way you're quering too much",
    "start": "2464319",
    "end": "2470400"
  },
  {
    "text": "knock that off right like but it's also for us it's not just about the money",
    "start": "2470400",
    "end": "2475800"
  },
  {
    "text": "because it turns out um I'm really gutted that I missed the talk about sustainability in the previous walk like",
    "start": "2475800",
    "end": "2481640"
  },
  {
    "text": "it turns out that when you have uh information Computing and Technology",
    "start": "2481640",
    "end": "2486680"
  },
  {
    "text": "when you have data centers that are emitting as much much CO2 ASE as single",
    "start": "2486680",
    "end": "2493440"
  },
  {
    "text": "plane in the world right like that's that's a lot of that is a lot of of carbon emission that is a lot of energy",
    "start": "2493440",
    "end": "2500079"
  },
  {
    "text": "that is being spent potentially inefficiently and we're making that problem worse with AI where we're just throwing gpus at it but this is kind of",
    "start": "2500079",
    "end": "2507119"
  },
  {
    "text": "one of our little ways of saying you know these arm processors right running",
    "start": "2507119",
    "end": "2512400"
  },
  {
    "text": "this this serverless workload against against arm it isn't just cheaper for us it also Al means that the data centers",
    "start": "2512400",
    "end": "2519240"
  },
  {
    "text": "are consuming less power and it means that the carbon emission associated with clicking that runquery button is is a",
    "start": "2519240",
    "end": "2524280"
  },
  {
    "text": "lot lower so my message to you if you are thinking about applying these lessons at home is yes do this if it is",
    "start": "2524280",
    "end": "2532920"
  },
  {
    "text": "appropriate to your workload so most bulk workloads where someone is waiting",
    "start": "2532920",
    "end": "2538560"
  },
  {
    "text": "for the response to come back will benefit from doing this first thing that you have to do is you have to move that",
    "start": "2538560",
    "end": "2546119"
  },
  {
    "text": "state from local machines on optic storage right if the data that you're trying to analyze is stuck on an ec2",
    "start": "2546119",
    "end": "2551319"
  },
  {
    "text": "instance or an Azure VM somewhere right no amount of parallelizing is going to help because it's going to be blocked on",
    "start": "2551319",
    "end": "2557720"
  },
  {
    "text": "the network trying to talk to that individual machine to pull to suck the data off of it so we have to first make",
    "start": "2557720",
    "end": "2563160"
  },
  {
    "text": "sure that we are uh that we are tearing that data onto object storage so that any worker running anywhere in your CL",
    "start": "2563160",
    "end": "2569480"
  },
  {
    "text": "in your cloud provider service environment can can pull that data secondly um again like do not run",
    "start": "2569480",
    "end": "2576240"
  },
  {
    "text": "one invoke per file right you have to think about okay you know what are my",
    "start": "2576240",
    "end": "2581559"
  },
  {
    "text": "work units what's the size per batch that kind of optimizes the startup cost",
    "start": "2581559",
    "end": "2586720"
  },
  {
    "text": "associated with with an invoke call versus the cost of potentially you know you're running uh 10 you know you're",
    "start": "2586720",
    "end": "2592760"
  },
  {
    "text": "you're quering 10 things in cial 20 things in Cal right like what happens if one of them gets slow and it blocks returning the entire",
    "start": "2592760",
    "end": "2599200"
  },
  {
    "text": "batch uh thirdly you need to make sure that you that every object processed is",
    "start": "2599200",
    "end": "2604640"
  },
  {
    "text": "independent of all the others right you need to make sure that you have a clean separation of kind of the mapping and reducing processes because if you do not",
    "start": "2604640",
    "end": "2611240"
  },
  {
    "text": "have a clean separation of mapping and and reducing right if your workload is highly interdependent upon previous",
    "start": "2611240",
    "end": "2616720"
  },
  {
    "text": "results you're not going to get good results because the whole theory behind massively Fanning out and using uh",
    "start": "2616720",
    "end": "2623160"
  },
  {
    "text": "serverless workers as your executors is that they shouldn't have to cross talk with each other they should only need to",
    "start": "2623160",
    "end": "2629520"
  },
  {
    "text": "talk to object storage and they should only have to report their results and finally you do need like some sort of",
    "start": "2629520",
    "end": "2634839"
  },
  {
    "text": "efficiency in in your reducer function right if you are calling out a million invoke requests you'd better have an efficient",
    "start": "2634839",
    "end": "2641000"
  },
  {
    "text": "way of of getting a million responses back and aggregating them together so here are the dragons um do",
    "start": "2641000",
    "end": "2648319"
  },
  {
    "text": "not apply this if you have workloads that are that are latency insensitive right like if you have a bulk analytics",
    "start": "2648319",
    "end": "2654640"
  },
  {
    "text": "report of you know here's the past quarters Financial results maybe don't run that against Lambda right like it",
    "start": "2654640",
    "end": "2659960"
  },
  {
    "text": "user expectations is that that might take a few hours to run that's okay like you know that is a cost issue um again",
    "start": "2659960",
    "end": "2667960"
  },
  {
    "text": "if you were right like if I were to change this time range to um right so if I were to change this",
    "start": "2667960",
    "end": "2674200"
  },
  {
    "text": "time range to 10 minutes right this 10-minute query should not wind up",
    "start": "2674200",
    "end": "2679680"
  },
  {
    "text": "executing against against Lambda because that would be a waste to query zero or maybe one or two files off of S3 but not",
    "start": "2679680",
    "end": "2686640"
  },
  {
    "text": "like a whole bunch of them right that's better just computed locally uh third because this is a workload that",
    "start": "2686640",
    "end": "2693839"
  },
  {
    "text": "kind of Blends serverless and server full Computing it's it's important to coordinate with your cloud provider",
    "start": "2693839",
    "end": "2698920"
  },
  {
    "text": "because they may have capacity planning considerations around if you expect to suddenly burst to a concurrency of",
    "start": "2698920",
    "end": "2704800"
  },
  {
    "text": "10,000 or 20,000 or 30,000 and then fourth and finally um",
    "start": "2704800",
    "end": "2710000"
  },
  {
    "text": "you know whether you be adopting something like arm 64 whether you be migrating your workload to serverless for the first time you really need to do",
    "start": "2710000",
    "end": "2718480"
  },
  {
    "text": "profiling of your workload where you really have a situation where time is money where every millisecond of",
    "start": "2718480",
    "end": "2724319"
  },
  {
    "text": "execution results in you being sent the bill for that millisecond it's important to understand like you know are you making the most of every",
    "start": "2724319",
    "end": "2731640"
  },
  {
    "text": "single scrap of CPU time that you're getting especially given this is actually a really interesting thing that",
    "start": "2731640",
    "end": "2737400"
  },
  {
    "text": "we found one thing that we found is that Amazon actually will give you the at",
    "start": "2737400",
    "end": "2744359"
  },
  {
    "text": "least on x86 they'll give you the worst CPUs they have in their entire fleet right they are selling you their spare",
    "start": "2744359",
    "end": "2750960"
  },
  {
    "text": "capacity so you will find literally like 2.5 GHz zons from 10 years that are run",
    "start": "2750960",
    "end": "2757640"
  },
  {
    "text": "that are running Lambda executions it's TR it's it's truly truly strange let me let me show you what I mean uh server",
    "start": "2757640",
    "end": "2764760"
  },
  {
    "text": "name is Lambda and I'm going to group it by uh CPU",
    "start": "2764760",
    "end": "2770040"
  },
  {
    "text": "info uh let's see CPU info CPU megahertz uh let's do",
    "start": "2770040",
    "end": "2776240"
  },
  {
    "text": "that so yeah literally right like you know I",
    "start": "2776240",
    "end": "2781400"
  },
  {
    "text": "can see okay roughly rounded right like a majority of this of these executions oh my God",
    "start": "2781400",
    "end": "2787920"
  },
  {
    "text": "that that that's wild a third of these executions are happening on 2.2 GHz",
    "start": "2787920",
    "end": "2793920"
  },
  {
    "text": "machines right like literally like 10-year-old Hardware so the behavior that you are seeing running your data",
    "start": "2793920",
    "end": "2799079"
  },
  {
    "text": "processing workload on the latest and greatest instances may not be the performance that you actually see",
    "start": "2799079",
    "end": "2805119"
  },
  {
    "text": "running in parallel on Lambda so tune everything properly right you know tune the items for invoke tune",
    "start": "2805119",
    "end": "2811640"
  },
  {
    "text": "your CPU Ram ratios tune your batch sizes um of the amount of data per per",
    "start": "2811640",
    "end": "2818880"
  },
  {
    "text": "file stored in S3 um make sure that you're doing optimization make sure you're profiling make sure that you are",
    "start": "2818880",
    "end": "2825079"
  },
  {
    "text": "looking for opportunities to save on waste make sure that you're measuring everything as you go um right like you",
    "start": "2825079",
    "end": "2832000"
  },
  {
    "text": "have to make sure especially if you're paying for it that you are getting the results that you expect and that if",
    "start": "2832000",
    "end": "2837960"
  },
  {
    "text": "there is something that is causing you more cold starts you want to know about it if your cold starts are taking longer there's some kind of regression you want",
    "start": "2837960",
    "end": "2843760"
  },
  {
    "text": "to be able to find out about it and that's where again like having that metric of hey by the way did you know",
    "start": "2843760",
    "end": "2848839"
  },
  {
    "text": "you just spent $10,000 in one day right like having an alert go off is is really helpful to kind of provide that break",
    "start": "2848839",
    "end": "2855599"
  },
  {
    "text": "against against uh something getting out of control and if you do wind up choosing",
    "start": "2855599",
    "end": "2861119"
  },
  {
    "text": "to adopt arm 64 um you can save a lot of money and make the planet a better place um but it does require a little of el a",
    "start": "2861119",
    "end": "2867760"
  },
  {
    "text": "little bit of elbow grease right you have to make sure that your build pipeline is producing artifacts for the right architecture that your clusters or",
    "start": "2867760",
    "end": "2874400"
  },
  {
    "text": "that your Lambda functions are configured to be be able to run either architecture's configuration and that",
    "start": "2874400",
    "end": "2880400"
  },
  {
    "text": "you have the control knobs to flip back and forth between them to AB test to kind of measure and tune it in before",
    "start": "2880400",
    "end": "2886520"
  },
  {
    "text": "you roll out to 100% of everything so at the end of the day uh nothing matters unless developers are happy and also",
    "start": "2886520",
    "end": "2894000"
  },
  {
    "text": "that uh your CFO is not mad at you because of the cloud Bill uh so that's what I have to share with you today uh",
    "start": "2894000",
    "end": "2899400"
  },
  {
    "text": "if you're interested in more details about kind of this uh about the system that we built um that is kind of was a",
    "start": "2899400",
    "end": "2904760"
  },
  {
    "text": "little bit of a predecessor developer parallel with click house um you can check out the over book that I and my",
    "start": "2904760",
    "end": "2911440"
  },
  {
    "text": "co-authors wrote and here's a final picture of flurry who is an adorable",
    "start": "2911440",
    "end": "2917000"
  },
  {
    "text": "Golden Retriever and that I love very much uh and that I want to be happy as well thank you so much",
    "start": "2917000",
    "end": "2924300"
  },
  {
    "text": "[Applause]",
    "start": "2924300",
    "end": "2929619"
  }
]