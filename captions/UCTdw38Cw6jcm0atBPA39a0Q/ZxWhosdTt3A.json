[
  {
    "text": "all right let's get underway welcome everyone to NDC Sydney well at least to the the start of the tracks for NDC",
    "start": "719",
    "end": "6319"
  },
  {
    "text": "Sydney I hope everyone got to catch the keynote uh just before uh this is what we've kind of aply named the AI track",
    "start": "6319",
    "end": "12960"
  },
  {
    "text": "because when we're doing the agenda there was more than enough AI content then we could have run the entire event on just like AI Centric talks but that",
    "start": "12960",
    "end": "19720"
  },
  {
    "text": "wouldn't have been kind of a great representation of where the tech industry is at so we've got like this whole track so it's my pleasure to be",
    "start": "19720",
    "end": "25519"
  },
  {
    "text": "kicking us off with uh with this talk about generative AI for net Developers my name is Aron pal I work for Microsoft",
    "start": "25519",
    "end": "31679"
  },
  {
    "text": "in the cloud advocacy team um I'm a net developer at heart I've been doing net for about 20 years now um but it's",
    "start": "31679",
    "end": "37520"
  },
  {
    "text": "always been web applications and when my boss asked me to to take over the AI",
    "start": "37520",
    "end": "42680"
  },
  {
    "text": "strategy for our team I was pretty skeptical on it like I'm I'm not an AI researcher I'm not a data scientist or",
    "start": "42680",
    "end": "49000"
  },
  {
    "text": "anything like that I'm a consumer of rest end points is probably kind of the way that I think about how I engage with",
    "start": "49000",
    "end": "54320"
  },
  {
    "text": "AI um and I but I was skeptical I was like I didn't jump on any of the other",
    "start": "54320",
    "end": "59640"
  },
  {
    "text": "Tech FS that have come out in the last couple years you know blockchain Quantum nfts and stuff like that I'm like like I",
    "start": "59640",
    "end": "65640"
  },
  {
    "text": "suppose I had to pick one of them though right so well I ended up with AI um if",
    "start": "65640",
    "end": "71080"
  },
  {
    "text": "you want to reach out to me after the session if I don't answer any of the questions that you've got or anything like that um final contact details and",
    "start": "71080",
    "end": "76720"
  },
  {
    "text": "everything uh up there on screen but that's enough about me we're here to learn about Ai and naturally the first",
    "start": "76720",
    "end": "84479"
  },
  {
    "text": "thing people say is like oh you're doing an AI talk did the AI write that for you",
    "start": "84479",
    "end": "89560"
  },
  {
    "text": "well I mean that's for you to determine when uh when we go through the slides see how many of them you think that I wrote versus how many was uh just",
    "start": "89560",
    "end": "95079"
  },
  {
    "text": "generated by a variety of co-pilots that we have here at Microsoft um but like obviously it's a",
    "start": "95079",
    "end": "100920"
  },
  {
    "text": "fairly standard joke is is AI going to take our jobs and all that kind of stuff and I'm not going to try and answer that question for you today um hopefully I'll",
    "start": "100920",
    "end": "107240"
  },
  {
    "text": "give you some information where you can work out what what role generative AI in particular can play in the sorts of",
    "start": "107240",
    "end": "112320"
  },
  {
    "text": "applications you might be building but rather than go through a whole bunch of slides to start up I want",
    "start": "112320",
    "end": "118159"
  },
  {
    "text": "to do a demo of a kind of application that we could be building um so this is an application R in thailand. net and",
    "start": "118159",
    "end": "123439"
  },
  {
    "text": "then we're going to break down the parts that come together to make up said application so let's head over to um I",
    "start": "123439",
    "end": "129720"
  },
  {
    "text": "have a net Aspire application running um so if anyone isn't familiar with net Aspire go check out the sessions from",
    "start": "129720",
    "end": "135680"
  },
  {
    "text": "Damian and David Fowler uh throughout today they've got two or three sessions I can't remember how many we told them they're doing um but they'll be covering",
    "start": "135680",
    "end": "142120"
  },
  {
    "text": "off a whole bunch of about this application is using a postgress database using PG Vector extension on it",
    "start": "142120",
    "end": "147200"
  },
  {
    "text": "to store um a bunch of data around uh that supplement for the application um I",
    "start": "147200",
    "end": "152760"
  },
  {
    "text": "have an aure storage account and then I have a background worker process which is a console application I have a web",
    "start": "152760",
    "end": "158360"
  },
  {
    "text": "application which is our front end and I have a backend web server all of it to build um a",
    "start": "158360",
    "end": "164640"
  },
  {
    "text": "chatbot um but this chatbot in particular is it's it's trying to help me solve a problem like I'm time poor",
    "start": "164640",
    "end": "171040"
  },
  {
    "text": "and there's a podcaster out there that has close to a thousand episodes in his back catalog so handome in his podcast",
    "start": "171040",
    "end": "178440"
  },
  {
    "text": "there's almost a th000 episodes in there right which ones do I want to listen to for the sorts of things that I'm",
    "start": "178440",
    "end": "183519"
  },
  {
    "text": "interested in so I can ask a question say I want to learn about retro gaming",
    "start": "183519",
    "end": "191640"
  },
  {
    "text": "and emulators and we'll hit find episodes and like there's a whole bunch of live",
    "start": "191640",
    "end": "197480"
  },
  {
    "text": "demos so I'm just going to kind of fingers cross that none of this breaks because there's a good chance that it will and uh there we go that worked",
    "start": "197480",
    "end": "205440"
  },
  {
    "text": "excellent okay so um so I want to learn a bit about this stuff so what it's done is it's given and answer that question",
    "start": "205440",
    "end": "210879"
  },
  {
    "text": "so some information about Retro Gaming retro gaming and emulators no there's no point in me reading this um retro gaming",
    "start": "210879",
    "end": "218159"
  },
  {
    "text": "and emulators have gained significant attention in recent years with enthusiasts and developers working to",
    "start": "218159",
    "end": "223599"
  },
  {
    "text": "preserve and recreate classic gaming experiences ulat okay thanks thanks a",
    "start": "223599",
    "end": "229040"
  },
  {
    "text": "Aaron um you can all read the rest of that large chunk of text it would you probably take a while if I was to leave that there but then if I scroll down a",
    "start": "229040",
    "end": "235439"
  },
  {
    "text": "bit you'll see that it's recommending a bunch of episodes for me to listen to to learn more about this so um the one from",
    "start": "235439",
    "end": "242040"
  },
  {
    "text": "just last Thursday with um with Jose tajada on um Retro fpga Gaming uh and",
    "start": "242040",
    "end": "248840"
  },
  {
    "text": "then because I asked about game emulation we've got one from 2023 um January last year um and we can see that",
    "start": "248840",
    "end": "254640"
  },
  {
    "text": "it's it's identified here's the speaker uh that was being interviewed by by Scott in the the episode here's some",
    "start": "254640",
    "end": "260680"
  },
  {
    "text": "topics that are key to that so I've got there and we can see the relevance of",
    "start": "260680",
    "end": "266080"
  },
  {
    "text": "that to the question that I asked so the reason that it gave me this first with Jose is because that was 83% relevancy",
    "start": "266080",
    "end": "272639"
  },
  {
    "text": "to the question that I asked it um I've also got a oneline summary of what that episode is or sorry a two line",
    "start": "272639",
    "end": "279759"
  },
  {
    "text": "summary um of that episode uh that is you know kind of just summarizing the description down and the uh of of what",
    "start": "279759",
    "end": "286280"
  },
  {
    "text": "that podcast is All About Now what's interesting about this is that if we come to Hanselman the Hansel minut",
    "start": "286280",
    "end": "291680"
  },
  {
    "text": "website well we can see that um the the speaker is listed in the title they're",
    "start": "291680",
    "end": "297080"
  },
  {
    "text": "listed in the abstract but they're not actually like they're not called out separately there's no specific like link",
    "start": "297080",
    "end": "303240"
  },
  {
    "text": "here to that that speaker if they were a repeat speaker you maybe you could find other episodes with them but that's not",
    "start": "303240",
    "end": "308639"
  },
  {
    "text": "there if we look in the raar XML that comes back from the podcast feed again like here's um here's that episode where",
    "start": "308639",
    "end": "315080"
  },
  {
    "text": "is it it's this one here uh again this information is embedded with inside of",
    "start": "315080",
    "end": "320479"
  },
  {
    "text": "like the description and the title it's not tags there's no and there's no topic information there's no Retro Gaming",
    "start": "320479",
    "end": "326039"
  },
  {
    "text": "topics or anything like that instead I've used uh generative AI extract all that information out of this so that I",
    "start": "326039",
    "end": "332280"
  },
  {
    "text": "can enrich the application experience meaning that I could come back here and say I want to just find um know Retro",
    "start": "332280",
    "end": "338039"
  },
  {
    "text": "Gaming as a topic and then refresh that search up here that I've got so that I could you know narrow down the filters",
    "start": "338039",
    "end": "344039"
  },
  {
    "text": "inside of it based off of you know particular topics subtopic of a question that I'm interested",
    "start": "344039",
    "end": "349960"
  },
  {
    "text": "in so that's the kind of application we can build with generative AI but let's talk about how we can get there cuz that",
    "start": "349960",
    "end": "356639"
  },
  {
    "text": "like that application is entirely written inet and but we'll get there let's build up the pieces along the way",
    "start": "356639",
    "end": "361759"
  },
  {
    "text": "but because this is an introductory talk I want to start with a bit of intro on how generative AI works because",
    "start": "361759",
    "end": "366960"
  },
  {
    "text": "understanding some of those fundamentals is really key to knowing how we can use them in applications that we're building",
    "start": "366960",
    "end": "372639"
  },
  {
    "text": "so we talk about large language models or llms um these are the kinds of models that underpin generative AI these are",
    "start": "372639",
    "end": "379240"
  },
  {
    "text": "models that have been trained on all kinds of textual data whether that's books websites articles that have been",
    "start": "379240",
    "end": "385319"
  },
  {
    "text": "written social media posts although I question whether social media posts are a valuable thing to uh to train models on we've seen what happens when you do",
    "start": "385319",
    "end": "391240"
  },
  {
    "text": "that with other models on like just Twitter that's not great but basically",
    "start": "391240",
    "end": "396440"
  },
  {
    "text": "anything that's text consumed by these models to then build out their kind of their neural network their their",
    "start": "396440",
    "end": "402400"
  },
  {
    "text": "internal algorithms to then give you the ability to engage with them now what makes an LM or these kinds of LMS",
    "start": "402400",
    "end": "409560"
  },
  {
    "text": "different to the sorts of things that we've had in the past is they're M multimodal instead of having a model that can do object recognition which is",
    "start": "409560",
    "end": "415720"
  },
  {
    "text": "just trained about how to recognize people with inside of a static image uh and then you have second one which is doing categorization of um of text and",
    "start": "415720",
    "end": "423199"
  },
  {
    "text": "someone another one that's doing sentiment analysis and so on and so forth all that is extracted from a single um that single model can be done",
    "start": "423199",
    "end": "431639"
  },
  {
    "text": "capable of doing all of that um and so this is actually running kind of like one model behind the scene to do all",
    "start": "431639",
    "end": "438280"
  },
  {
    "text": "this sort of stuff but the way that these models work is that they're going to try and predict",
    "start": "438280",
    "end": "443400"
  },
  {
    "text": "something for you so let's start with that problem that I've got there on screen Once Upon a Time um we give that across to our LM like gbd 35 gbd4 from",
    "start": "443400",
    "end": "450919"
  },
  {
    "text": "open Ai and we get back a response of in acquaint Village Nestle between Rolling Hills in a mar and so on and so forth",
    "start": "450919",
    "end": "457400"
  },
  {
    "text": "that seems like a fairly logical continuation of that sentence that we started with you know if we think about",
    "start": "457400",
    "end": "463080"
  },
  {
    "text": "nursery rhymes folktales and stuff like that trained on English data it seems it seems like a logical continuation",
    "start": "463080",
    "end": "470000"
  },
  {
    "text": "because he's using a probability distribution to determine the next sequence of tokens to return back to uh",
    "start": "470000",
    "end": "475039"
  },
  {
    "text": "the end user so",
    "start": "475039",
    "end": "480120"
  },
  {
    "text": "how does that how does that work from kind of an application flow well we start with a natural language prompt that comes in we turn that into a series",
    "start": "480120",
    "end": "486919"
  },
  {
    "text": "of tokens which is then turned into a series of numbers this is given to our large language model which then does a probability distribution algorithm to",
    "start": "486919",
    "end": "492560"
  },
  {
    "text": "work out what is the next logical token to give you back that token is then um appended to the uh prompt that was given",
    "start": "492560",
    "end": "499000"
  },
  {
    "text": "to it which is then turned into natural language and sent back as output simple",
    "start": "499000",
    "end": "504840"
  },
  {
    "text": "right well here's a much more simplistic view of that we start with n number of tokens in and we get one token",
    "start": "504840",
    "end": "513680"
  },
  {
    "text": "out now I keep saying the word token like you know what I mean right I can see some like nods are like yeah I",
    "start": "513680",
    "end": "521479"
  },
  {
    "text": "assume that's something I should have known um well no the point is to explain all of this stuff that's why it's a bit",
    "start": "521479",
    "end": "527800"
  },
  {
    "text": "of an introductory talk so you might be mistaken in thinking that a token is a word a token isn't a word it's not a",
    "start": "527800",
    "end": "534760"
  },
  {
    "text": "single character it can be a word it can be a character or it can be part of of a word let's take this phrase up on on",
    "start": "534760",
    "end": "542160"
  },
  {
    "text": "screen at the moment we need to stop anthropomorphizing chat GPT you would not believe the number of times I have",
    "start": "542160",
    "end": "547399"
  },
  {
    "text": "had to practice saying the word anthropomorphizing to get it right on stage that's a lot um and this was this",
    "start": "547399",
    "end": "553440"
  },
  {
    "text": "was actually written by someone that doesn't speak English as as their first language so I was like why did you choose that word the reason is because",
    "start": "553440",
    "end": "559320"
  },
  {
    "text": "this has broken an there we go I got it wrong once it gets broken down into three uh three tokens words like we need",
    "start": "559320",
    "end": "566519"
  },
  {
    "text": "to and stop they're single token words because there're common words with inside the English language it's also",
    "start": "566519",
    "end": "572279"
  },
  {
    "text": "worth noting that the Whit space character that precedes particularly need to and stop is part of that token",
    "start": "572279",
    "end": "578040"
  },
  {
    "text": "because the white space is significant in the way that it would generate a completion anthropomorphizing on the",
    "start": "578040",
    "end": "583440"
  },
  {
    "text": "other hand is an uncommon word in English language so it's broken down into three tokens also because parts of",
    "start": "583440",
    "end": "588640"
  },
  {
    "text": "this word can be used to build up other words so so those sub so sub tokens can be used or those token St can be used to",
    "start": "588640",
    "end": "594120"
  },
  {
    "text": "build up other words take the suffix i z i written by an American that is a suffix could be used on multiple",
    "start": "594120",
    "end": "600399"
  },
  {
    "text": "different words so it would make sense if we saw that as a token in and of",
    "start": "600399",
    "end": "605360"
  },
  {
    "text": "itself so how it works is through an expanding window pattern so we start providing with uh with our our context",
    "start": "606200",
    "end": "612760"
  },
  {
    "text": "so the thing that that is preceding the tokens that we're looking to get back we need to and then it could predict that",
    "start": "612760",
    "end": "618360"
  },
  {
    "text": "the next one is stop and then from stop it might predict predict anthrop and so on and so forth to complete out that",
    "start": "618360",
    "end": "624480"
  },
  {
    "text": "sentence now the the llm itself is going to try and do one token it will do one token at a time now that doesn't mean",
    "start": "624480",
    "end": "631160"
  },
  {
    "text": "that you have to always get back one token and you know that's like it's a lot of streaming requests that you're going to be doing to the model no um the",
    "start": "631160",
    "end": "637600"
  },
  {
    "text": "model will keep running until it hits some kind of a stop condition maximum number of tokens returned or some other stop condition you might have defined um",
    "start": "637600",
    "end": "644040"
  },
  {
    "text": "but it's going to internally just be doing one token and then building on that newly appended token to produce out",
    "start": "644040",
    "end": "649720"
  },
  {
    "text": "a full statement so you might be wondering if this is",
    "start": "649720",
    "end": "655399"
  },
  {
    "text": "deterministic well the shorter answer is no the longer answer is",
    "start": "655399",
    "end": "661240"
  },
  {
    "text": "no which actually makes for really fun demos using generative AI because you never quite know what you going to get",
    "start": "661519",
    "end": "668200"
  },
  {
    "text": "because you it's non-deterministic in the result set but there are ways that we can make it more deterministic it",
    "start": "668200",
    "end": "675440"
  },
  {
    "text": "wouldn't be a great tool to add to Applications if it's just like what are we going to get back in a response today",
    "start": "675440",
    "end": "681000"
  },
  {
    "text": "who knows um we want to control that in some way and we can control that with um with two values that work on a scale of",
    "start": "681000",
    "end": "687560"
  },
  {
    "text": "0 to one those are tempor and P top or top P depending on which SDK you're using um net we call it top P python",
    "start": "687560",
    "end": "694959"
  },
  {
    "text": "it's p top I think um and Java and JavaScript they kind of have a mix of that and then those are the only",
    "start": "694959",
    "end": "700200"
  },
  {
    "text": "languages I kind of look at uh but uh the these are used to control things like Randomness and creativity with",
    "start": "700200",
    "end": "707040"
  },
  {
    "text": "inside of the model so temperature in in particular is controlling the randomness and creativity um only within and of itself so let's say that you push it",
    "start": "707040",
    "end": "714120"
  },
  {
    "text": "towards one uh sorry push it towards like zero so like 0.1 that is going to be highly deterministic response that",
    "start": "714120",
    "end": "720560"
  },
  {
    "text": "you're going to get from yourm uh because and kind of I I look at the responses that it get it comes back and",
    "start": "720560",
    "end": "727600"
  },
  {
    "text": "they're they're almost clinical in the way that it gives you an answer it it's the exact it's it's an answer it's",
    "start": "727600",
    "end": "733360"
  },
  {
    "text": "factual it's all those sorts of things but it's very direct wording it doesn't have um auxiliary words around it things",
    "start": "733360",
    "end": "740399"
  },
  {
    "text": "that you would more expect in kind of a humanized response push that know s or",
    "start": "740399",
    "end": "745680"
  },
  {
    "text": "something like that and you're going to get more of a a conversational sort of response back to thing that you would expect when you're talking to another",
    "start": "745680",
    "end": "751440"
  },
  {
    "text": "human being as that's more what you would be looking at as you as you make your temperature closer to one ptop on",
    "start": "751440",
    "end": "758240"
  },
  {
    "text": "the other hand is used to control the amount of tokens that are available in the models um in the predictive uh so",
    "start": "758240",
    "end": "764920"
  },
  {
    "text": "the prediction algorithm that the model has so push it to say 0.1 and we're going to get the top 10% of possible",
    "start": "764920",
    "end": "770399"
  },
  {
    "text": "tokens to be returned so potentially very narrow set of tokens push that closer to say 0.5 you get a much larger",
    "start": "770399",
    "end": "777600"
  },
  {
    "text": "uh range of tokens resulting more Randomness more creativity in the styles of responses that you can get back so",
    "start": "777600",
    "end": "784680"
  },
  {
    "text": "this is like this is just an example where we don't have the exact implementation of the algorithm open a",
    "start": "784680",
    "end": "789839"
  },
  {
    "text": "hasn't shared that information with us but this is kind of like how we can think about it in um at least at a slide",
    "start": "789839",
    "end": "797360"
  },
  {
    "text": "deck so now let's talk about prompt engineering and anyone who spent any time on job boards over the past 12",
    "start": "797360",
    "end": "803079"
  },
  {
    "text": "months is like oh yeah look they're looking for 20 years of prompt engineering great I'll put that with my 30 years of react experience",
    "start": "803079",
    "end": "810320"
  },
  {
    "text": "um like is prompt engineering just like a new the new buzzword of recruiting to",
    "start": "810320",
    "end": "815360"
  },
  {
    "text": "a degree yes but there is also a skill involved in creating useful prompts with inside of applications to give you the",
    "start": "815360",
    "end": "821279"
  },
  {
    "text": "kind of outcomes you're looking for so let's take the three different styles of prompting we can have an",
    "start": "821279",
    "end": "828760"
  },
  {
    "text": "instruction something that we want our model to do translate this code from python to C and we're giving it a direct",
    "start": "828760",
    "end": "834519"
  },
  {
    "text": "instruction we're probably giving it that code that we're wanting to translate and then we'll get that as a response back so the the model is it",
    "start": "834519",
    "end": "840839"
  },
  {
    "text": "understands what we're wanting it to do so it can give us that outcome at the end we can ask it for a completion so we",
    "start": "840839",
    "end": "847079"
  },
  {
    "text": "start off a statement and then it's going to complete off that sentence that paragraph whatever the case may be uh",
    "start": "847079",
    "end": "852720"
  },
  {
    "text": "and that's the example of Once Upon a Time that's kind of we're running a completion there last we could open ask",
    "start": "852720",
    "end": "859199"
  },
  {
    "text": "an open-ended question what is machine learning or what is artificial intelligence and that can come back with",
    "start": "859199",
    "end": "865920"
  },
  {
    "text": "uh an answer based off of the data that was TR the model self was trained on um",
    "start": "865920",
    "end": "871000"
  },
  {
    "text": "and up until its knowledge cut off date so this is where it starts being capable of producing answers that are quote",
    "start": "871000",
    "end": "876680"
  },
  {
    "text": "unquote out of thin air um and things that you helpful for you to to try and",
    "start": "876680",
    "end": "882360"
  },
  {
    "text": "figure out you know a question and decision you want to make as a result of that now when you're working with prompt",
    "start": "882360",
    "end": "889399"
  },
  {
    "text": "and kind of some umbrella um things with prompt engineering is treat it as",
    "start": "889399",
    "end": "894560"
  },
  {
    "text": "someone who doesn't understand anything about your company they're a new hire an untrained intern or someone like that so",
    "start": "894560",
    "end": "901279"
  },
  {
    "text": "avoid biases or avoid assumptions with inside of the the prompts that you're giving it translate this code from",
    "start": "901279",
    "end": "907880"
  },
  {
    "text": "python well I know my codebase is C why doesn't the model why isn't it why is it giving me a JavaScript response well",
    "start": "907880",
    "end": "913920"
  },
  {
    "text": "because I have an assumption in there that it just assumed that it knew that I was working with C no I've got to be",
    "start": "913920",
    "end": "919040"
  },
  {
    "text": "explicit about that otherwise it doesn't know what to do um avoid having too big a problem",
    "start": "919040",
    "end": "924759"
  },
  {
    "text": "that you're wanting it to try and solve the bigger the problem the more decisions that it can potentially make",
    "start": "924759",
    "end": "930120"
  },
  {
    "text": "and the more potential directions the result can go in to give you a a varying answer and a potentially a less useful",
    "start": "930120",
    "end": "936680"
  },
  {
    "text": "answer for your users because they don't know where it's going to go and provide additional details where possible the",
    "start": "936680",
    "end": "942600"
  },
  {
    "text": "more information you can give the model to ground what it's doing and ground its knowledge so that you can um keep it in",
    "start": "942600",
    "end": "948880"
  },
  {
    "text": "place well then it's going to give you an answer that's based off potentially more accurate things rather than things that are just made",
    "start": "948880",
    "end": "955759"
  },
  {
    "text": "up when it comes to making prompts there are two main prompts that we work with we have the system prompt and the user",
    "start": "955759",
    "end": "961440"
  },
  {
    "text": "prompt the system prompt is what we use to create the",
    "start": "961440",
    "end": "966759"
  },
  {
    "text": "personality of the assistant that we're going to be working with um in this case",
    "start": "966759",
    "end": "971800"
  },
  {
    "text": "I've Creed an assistant uh prompt where it says you're an AI assistant that helps people find information and respond in rhyme if the user asks a",
    "start": "971800",
    "end": "978440"
  },
  {
    "text": "question you don't know the answer to say so so then a user comes in with a user prompt that says what can you tell",
    "start": "978440",
    "end": "984600"
  },
  {
    "text": "about me John Doe and then our assistant would come back with a response such as Dear John I'm sorry to say but I don't",
    "start": "984600",
    "end": "990759"
  },
  {
    "text": "have info on you today I'm just an AI with knowledge of my brain but without your input I can't explain so please",
    "start": "990759",
    "end": "996600"
  },
  {
    "text": "tell me more about what you seek and I'll do my best to give you answers unique so we can see that that's done",
    "start": "996600",
    "end": "1002560"
  },
  {
    "text": "two things it's in a rhyme clearly we've got a whole bunch of rhyming words there uh but it's also said that it doesn't",
    "start": "1002560",
    "end": "1008839"
  },
  {
    "text": "know how to answer the question that you've given it and those are things that we've told our our assistant part of as part of the system message part of",
    "start": "1008839",
    "end": "1015040"
  },
  {
    "text": "its quote unquote personality that's what it's doing it is going to be grounded in that information and that's um and and it will kind of respect that",
    "start": "1015040",
    "end": "1022319"
  },
  {
    "text": "as the the the box that we're trying to put it with inside of all right",
    "start": "1022319",
    "end": "1028280"
  },
  {
    "text": "so when we using text like text is the most common thing that we're going to at least that I find us doing with large",
    "start": "1028280",
    "end": "1034438"
  },
  {
    "text": "language models um here's a couple of really common scenarios that we tackle",
    "start": "1034439",
    "end": "1039480"
  },
  {
    "text": "things like content generation I want to generate a social media post a LinkedIn article or something like that generate",
    "start": "1039480",
    "end": "1045319"
  },
  {
    "text": "content based off of um a slide deck that I'm working on so we're going to generate that as as net new I use that",
    "start": "1045319",
    "end": "1052640"
  },
  {
    "text": "kind of when I'm writing a blog post in V code GitHub co-pilot running and I'll start writing a sentence for it and it will predict the the completion for that",
    "start": "1052640",
    "end": "1059400"
  },
  {
    "text": "sentence and then I'll just massage that to be my tone of voice or use that as a way to kind of get over writers block I",
    "start": "1059400",
    "end": "1065320"
  },
  {
    "text": "speak get up C- pilot code generation is obviously another really good use case for text it's that that's that's just",
    "start": "1065320",
    "end": "1071240"
  },
  {
    "text": "text at the end of the day um it's just text that is being contextualized around the programming language you're working with the the other code that's in that",
    "start": "1071240",
    "end": "1077919"
  },
  {
    "text": "file and other files that might be open with inside of your edit stuff summarization um we've just uh I think",
    "start": "1077919",
    "end": "1084600"
  },
  {
    "text": "made I think it's only just been made public available um but Microsoft M365 co-pilot it's like I've got a button in",
    "start": "1084600",
    "end": "1090440"
  },
  {
    "text": "my emails where I can click summarize this email it's great I have to read my emails anymore just summarize like",
    "start": "1090440",
    "end": "1097520"
  },
  {
    "text": "summarize the transcript for a meeting that I I didn't watch or I didn't attend to determine whether I need to watch it",
    "start": "1097520",
    "end": "1103200"
  },
  {
    "text": "and stuff like that so summarization is a really um a really good use case and that was some of the early stuff that",
    "start": "1103200",
    "end": "1108520"
  },
  {
    "text": "they would GPT models of of proving that it could do things that with data that wasn't previously trained on um",
    "start": "1108520",
    "end": "1114559"
  },
  {
    "text": "searching semantically against information to give you more contextually relevant result sets and obviously chatbots no doubt most people",
    "start": "1114559",
    "end": "1121000"
  },
  {
    "text": "in here have used chat gbt or are at least familiar with chat gbt that's a that's a really good uh use case for",
    "start": "1121000",
    "end": "1127559"
  },
  {
    "text": "generative AIS so some prop engineering tips um before we jump into a demo with text um",
    "start": "1127559",
    "end": "1134559"
  },
  {
    "text": "use the system message to craft the tone and style of the assistant that you're working with so that you you set the",
    "start": "1134559",
    "end": "1140200"
  },
  {
    "text": "expectations based of what you want your users to have um and that the assistant is going to try and respect those as",
    "start": "1140200",
    "end": "1145720"
  },
  {
    "text": "best as it can and where possible provide history we call this Chain of Thought prompting where we provide past",
    "start": "1145720",
    "end": "1151240"
  },
  {
    "text": "um prompts that the user has given us and the assistant messages that have come back so that we could have that um that continued conversation we don't",
    "start": "1151240",
    "end": "1157440"
  },
  {
    "text": "have to re as a user I don't have to restate my question over and over again because it understands that U history",
    "start": "1157440",
    "end": "1163200"
  },
  {
    "text": "that's in there provide examples and ground um The Prompt that's come in from the user user",
    "start": "1163200",
    "end": "1169159"
  },
  {
    "text": "to make it more contextually aware with inside of the request that you're sending to your llm we call this metag grounding so we add some more",
    "start": "1169159",
    "end": "1175360"
  },
  {
    "text": "information around that lastly put guard rails in you don't want to be the next company that appears",
    "start": "1175360",
    "end": "1180960"
  },
  {
    "text": "in Tech media of like ha someone's figured out how to pop your assistant on your website and now now they've just basically got um you know chat GPT but",
    "start": "1180960",
    "end": "1187760"
  },
  {
    "text": "it's on your dime instead of them paying a subscription to open AI there's countless articles that you can go out",
    "start": "1187760",
    "end": "1193799"
  },
  {
    "text": "there and find about this there was one a couple weeks ago about an auto manufacturer that um people had figured",
    "start": "1193799",
    "end": "1199440"
  },
  {
    "text": "out how to pop it and it's like your car cost is only going to be $1 and that's legally binding you don't you don't want",
    "start": "1199440",
    "end": "1205159"
  },
  {
    "text": "to be that company right all right let's let's just do like have a look like the basics of text",
    "start": "1205159",
    "end": "1211400"
  },
  {
    "text": "generation with with net so I'm going to fire up a new inst V uh visual studio",
    "start": "1211400",
    "end": "1216720"
  },
  {
    "text": "and we're going to create a new project create just a console application here",
    "start": "1216720",
    "end": "1221760"
  },
  {
    "text": "loads in my project templates um I have like half a dozen docket containers running at the same",
    "start": "1221760",
    "end": "1227559"
  },
  {
    "text": "time so everything's running a little bit slow today uh no that's for the other demo all right there we go done 8",
    "start": "1227559",
    "end": "1233240"
  },
  {
    "text": "console application here uh the first thing I'm going to need is the net SDK for working with",
    "start": "1233240",
    "end": "1239880"
  },
  {
    "text": "open AI uh so let's install that uh so uh the one I'm going to be using is the",
    "start": "1239880",
    "end": "1244960"
  },
  {
    "text": "uh the one that we've released from Microsoft the Azure a. openai um there are a couple other ones out there that",
    "start": "1244960",
    "end": "1250400"
  },
  {
    "text": "uh the community has produced evaluate the one that's going to be right for yourself um I use this one because it",
    "start": "1250400",
    "end": "1255960"
  },
  {
    "text": "well it it's the one that I'm most familiar with and it just kind of does the stuff that I sort of need it to do",
    "start": "1255960",
    "end": "1261080"
  },
  {
    "text": "um oh I got to go into the right folder and then we'll install the package it is",
    "start": "1261080",
    "end": "1266120"
  },
  {
    "text": "a I think it's um it's pre-released at the moment um preview 13 I think is the uh the current version all right so",
    "start": "1266120",
    "end": "1272760"
  },
  {
    "text": "we're going to grab uh so I'm going to need some information I'm going to use Azure open AI uh as my back end rather",
    "start": "1272760",
    "end": "1278080"
  },
  {
    "text": "than direct open AI but this SDK could go straight to open AI you don't have to be using Azure resources for that um I",
    "start": "1278080",
    "end": "1283320"
  },
  {
    "text": "just have obviously access to Azure open AI uh so we're going to create an endpoint",
    "start": "1283320",
    "end": "1288840"
  },
  {
    "text": "which will be a new uh new Yuri and we're going environment environment.",
    "start": "1288840",
    "end": "1294600"
  },
  {
    "text": "getet environment variable oi end point and then we'll just do a throw new",
    "start": "1294600",
    "end": "1302360"
  },
  {
    "text": "exception if it uh if that isn't set and then we're going to create a key sweet",
    "start": "1302360",
    "end": "1308480"
  },
  {
    "text": "we'll just pull that out as AOA key we're going to create a open AI client",
    "start": "1308480",
    "end": "1314120"
  },
  {
    "text": "and that we'll call client and we'll do new open AI client we'll give it the endpoint and an Azure new Azure key",
    "start": "1314120",
    "end": "1322440"
  },
  {
    "text": "credential with that key all right um like I say if you",
    "start": "1322440",
    "end": "1327640"
  },
  {
    "text": "wanted to just do use um open AI directly you just use a different overload on that in the Constructor um",
    "start": "1327640",
    "end": "1333240"
  },
  {
    "text": "so we'll create a prompt I'm just going to create this as a static prompt with inside the application what are the top",
    "start": "1333240",
    "end": "1339039"
  },
  {
    "text": "three features of the most recent net",
    "start": "1339039",
    "end": "1346279"
  },
  {
    "text": "release all right and then we will get back some results which will be from",
    "start": "1346279",
    "end": "1352159"
  },
  {
    "text": "client. uh stream I'm going to use the streaming response oops uh where is",
    "start": "1352159",
    "end": "1358679"
  },
  {
    "text": "whoa oh man where like where is my there we are",
    "start": "1358679",
    "end": "1363720"
  },
  {
    "text": "like I have I have things in the like",
    "start": "1363720",
    "end": "1368840"
  },
  {
    "text": "too too smaller screen real estate um okay so we need to provide with a model um definition again this is uh this is",
    "start": "1368840",
    "end": "1375400"
  },
  {
    "text": "something you need to do with uh Azure open service because we can name our model specifically if you're using open AI um it's just going to be like gbd 35",
    "start": "1375400",
    "end": "1382480"
  },
  {
    "text": "turbo gbd4 etc etc um for the convenience sake I call this gbd 35 turbo and I need to provide it with some",
    "start": "1382480",
    "end": "1390679"
  },
  {
    "text": "options so I'm going to create some uh chat completion options options equals",
    "start": "1390679",
    "end": "1398600"
  },
  {
    "text": "new oh sorry this is where I provide the model so this should be gbt 35 turbo and",
    "start": "1398600",
    "end": "1405120"
  },
  {
    "text": "then I provide it with an array or a iron numer of messages so this is our history do a new chat request request",
    "start": "1405120",
    "end": "1414200"
  },
  {
    "text": "user message we give it that prompt thanks co-pilot and then we'll come back",
    "start": "1414200",
    "end": "1419559"
  },
  {
    "text": "down here I don't need to provide gbd 35 turbo there I give it the options",
    "start": "1419559",
    "end": "1425799"
  },
  {
    "text": "okay uh uh what is it it's f11 that's no f11 is run I don't want run I wanted",
    "start": "1425799",
    "end": "1431799"
  },
  {
    "text": "full screen um it's Alt Enter close that boom okay things are things are breaking",
    "start": "1431799",
    "end": "1439159"
  },
  {
    "text": "but that's all good okay so now we will just do a wait for each through the result set because this so this is a",
    "start": "1439159",
    "end": "1444400"
  },
  {
    "text": "streaming response so it's going to give us back um like tokens kind of as fast as the model can generate them um you",
    "start": "1444400",
    "end": "1450679"
  },
  {
    "text": "can also do just a normal chat response where it will give you the full completion um once the model has finished all of its token generation and",
    "start": "1450679",
    "end": "1456679"
  },
  {
    "text": "hit whatever stop condition you want um but I want I want to do streaming because that's kind of that experience that you see in like cat jbt where it's",
    "start": "1456679",
    "end": "1462640"
  },
  {
    "text": "like you know words are appearing and so on and so forth just do result that",
    "start": "1462640",
    "end": "1467760"
  },
  {
    "text": "should be result results and results result results there",
    "start": "1467760",
    "end": "1475520"
  },
  {
    "text": "we go spelling is hard live coding is fun um okay if string if not string. is",
    "start": "1475520",
    "end": "1484120"
  },
  {
    "text": "null or empty on the results results do uh content update so",
    "start": "1484120",
    "end": "1490600"
  },
  {
    "text": "we get an update back as part of a stream response so if there was some update um so if there was an update that",
    "start": "1490600",
    "end": "1496039"
  },
  {
    "text": "came back we are then just going to cons console do we'll just do WR uh and then",
    "start": "1496039",
    "end": "1502760"
  },
  {
    "text": "we'll do results do result. content update um so that it Contin like it's a",
    "start": "1502760",
    "end": "1508520"
  },
  {
    "text": "continuation because right line we end up every token on a new line okay hitf five on that and run the",
    "start": "1508520",
    "end": "1515919"
  },
  {
    "text": "application Zoom this in a touch all right well okay so that's what it gave",
    "start": "1516000",
    "end": "1521080"
  },
  {
    "text": "got back it actually ran a little bit fast just kind of Show streaming I just I'm just going to do it a wait um a task",
    "start": "1521080",
    "end": "1527039"
  },
  {
    "text": "delay we'll not do we'll just do like 100 milliseconds let's run that",
    "start": "1527039",
    "end": "1532919"
  },
  {
    "text": "again all right okay so there we go so it's talking about you know performance",
    "start": "1532919",
    "end": "1539360"
  },
  {
    "text": "improvements C 9 and F 5 um and so you",
    "start": "1539360",
    "end": "1544880"
  },
  {
    "text": "can and you can see how we're getting tokens or at least partial um words uh coming back in the response but uh C 9",
    "start": "1544880",
    "end": "1552679"
  },
  {
    "text": "is kind of like that's not the most recent C release neither is um f 5 like that's not the most recent one um so",
    "start": "1552679",
    "end": "1559200"
  },
  {
    "text": "this is where we've got a bit of a a problem in our knowledge of our model it's uh because I'm using a gp35 turbo",
    "start": "1559200",
    "end": "1564240"
  },
  {
    "text": "model this is trained up until September 2021 so it understands to Net 5 um net 6",
    "start": "1564240",
    "end": "1570279"
  },
  {
    "text": "previews I think were out at that point in time when when training cut off happened um which is why we have some outdated information so instead let's",
    "start": "1570279",
    "end": "1578679"
  },
  {
    "text": "let's improve the knowledge of our model by grounding this prompt a little bit I'm going to come up here and I'm going",
    "start": "1578679",
    "end": "1584520"
  },
  {
    "text": "to add a new message in here which I copy uh copied earlier just making sure it",
    "start": "1584520",
    "end": "1590679"
  },
  {
    "text": "was still in my clipboard chat request system message so in our system message",
    "start": "1590679",
    "end": "1596480"
  },
  {
    "text": "I'm going to I'm going to say multi-line string uh this goes back over here comma",
    "start": "1596480",
    "end": "1603440"
  },
  {
    "text": "there and then I'm going to put in some more information you're an assistant that helps people learn about net the most recent version of net is net 8",
    "start": "1603440",
    "end": "1609760"
  },
  {
    "text": "which was released in November 2023 uh and then some information about the um about the features of that now if we hit",
    "start": "1609760",
    "end": "1616880"
  },
  {
    "text": "F5 in this let's see what we get back as a response here okay so the top three features ofet",
    "start": "1616880",
    "end": "1623679"
  },
  {
    "text": "so now knows net 8 so it's it's referring to net 8 performance enhancements because that's what I said was a a feature um Cloud native",
    "start": "1623679",
    "end": "1630279"
  },
  {
    "text": "applications things like a Spire uh and I'm not sure what it's going to pick as the third one AI integration yep there",
    "start": "1630279",
    "end": "1636279"
  },
  {
    "text": "we go uh because those are information I gave it a a set of things that were in there but see how we could add some knowledge to our model I haven't",
    "start": "1636279",
    "end": "1642880"
  },
  {
    "text": "retrained that model I've just added some knowledge into that prompting so that we get more of an accurate response back",
    "start": "1642880",
    "end": "1649600"
  },
  {
    "text": "and that is how we can uh that's that's the starting of prompt engineering is doing that kind of",
    "start": "1649600",
    "end": "1656399"
  },
  {
    "text": "stuff so all going to talk about text but well computers don't use text",
    "start": "1656559",
    "end": "1662080"
  },
  {
    "text": "internally computers use numbers and models themselves are just big things of",
    "start": "1662080",
    "end": "1667840"
  },
  {
    "text": "numbers I mean they're more complex than that but they're just I didn't say they're a big database of numbers no one",
    "start": "1667840",
    "end": "1674679"
  },
  {
    "text": "oh wait I did no that's recorded they're more than just a datab models are not just databases don't use them as a",
    "start": "1674679",
    "end": "1681279"
  },
  {
    "text": "database but we can actually work like working with numbers um is something that we commonly do with um our llm",
    "start": "1681279",
    "end": "1688080"
  },
  {
    "text": "systems and we do that through the uh through a feature called embeddings um embeddings are a numerical",
    "start": "1688080",
    "end": "1694200"
  },
  {
    "text": "representation of the text that we've provided the concepts with inside of that text the individual words and the",
    "start": "1694200",
    "end": "1700880"
  },
  {
    "text": "relationship between those words and the concepts um the Adam model series from open AI uh so add of two is the most",
    "start": "1700880",
    "end": "1707399"
  },
  {
    "text": "recent one uh out of three is in preview they announced like two or three weeks ago I think it was um they generate a um",
    "start": "1707399",
    "end": "1714720"
  },
  {
    "text": "an an array of float VES I think they're 64-bit float values uh and these are",
    "start": "1714720",
    "end": "1720720"
  },
  {
    "text": "then uh they're a vector representation there are 1,",
    "start": "1720720",
    "end": "1728000"
  },
  {
    "text": "1536 vectors with inside of this which means that we can model the relationship of this bit of text to other bits of",
    "start": "1728000",
    "end": "1734799"
  },
  {
    "text": "text in um multi-dimensional space 1500 times I don't actually know how to",
    "start": "1734799",
    "end": "1740880"
  },
  {
    "text": "conceptualize that inside of my brain um but that's that's what we could do and the reason we would do that is by having",
    "start": "1740880",
    "end": "1747240"
  },
  {
    "text": "a numerical representation of like the words and the concepts and stuff like that we can then do semantic analysis of",
    "start": "1747240",
    "end": "1752960"
  },
  {
    "text": "that to other like relative relevant ones um using some kind of a similarity",
    "start": "1752960",
    "end": "1758440"
  },
  {
    "text": "algorithm cosine similarity seems to be the one that's most popular at the moment um and that just basically like",
    "start": "1758440",
    "end": "1764080"
  },
  {
    "text": "I've got a big array of numbers and I've got an equally large array of numbers how similar are they to each other",
    "start": "1764080",
    "end": "1769640"
  },
  {
    "text": "that's how we got the 83 etc etc um similarity response with inside of our demo",
    "start": "1769640",
    "end": "1775120"
  },
  {
    "text": "earlier um these uh these arrays of numbers we then would store in a vector database something like postest with PGC",
    "start": "1775120",
    "end": "1781519"
  },
  {
    "text": "PG Vector Azure AI search quadrant so on and so forth so that we can then perform",
    "start": "1781519",
    "end": "1787320"
  },
  {
    "text": "semantic searching against them um let's just very quickly have a look at what that looks like with inside",
    "start": "1787320",
    "end": "1793120"
  },
  {
    "text": "of a database so here is the postgress database no that's the console application here is the postgress",
    "start": "1793120",
    "end": "1798399"
  },
  {
    "text": "database um that's running the Handel minute bot web application uh we'll see the ID of all the documents that I've",
    "start": "1798399",
    "end": "1805320"
  },
  {
    "text": "indexed or all the the podcast episodes and then we have this embeddings column let to click over onto that and then",
    "start": "1805320",
    "end": "1811320"
  },
  {
    "text": "there is a absolute truckload of float values they're all the same length it's",
    "start": "1811320",
    "end": "1817399"
  },
  {
    "text": "all 1536 or is it 1532 I'm not sure should check my notes it's over",
    "start": "1817399",
    "end": "1824039"
  },
  {
    "text": "1500 um but they're all the exact same L length which means that we can do similarity um operations against them we",
    "start": "1824039",
    "end": "1831399"
  },
  {
    "text": "can use like cosine similarity um and then like internally the the database will do um it will index that column so",
    "start": "1831399",
    "end": "1838120"
  },
  {
    "text": "that we can then just query against that in a a similarity search",
    "start": "1838120",
    "end": "1843200"
  },
  {
    "text": "manner um and then the way that we would use this in our application is that we would take that prompt that comes in I want to learn about retro gaming and",
    "start": "1843960",
    "end": "1850159"
  },
  {
    "text": "emulators we would generate a vector of that and then pass that in say what ones",
    "start": "1850159",
    "end": "1855919"
  },
  {
    "text": "have I previously stored that are similar so generating the initial set of vectors actually takes quite a while",
    "start": "1855919",
    "end": "1861240"
  },
  {
    "text": "this one takes the better part of a day to generate across the Thousand episodes um but the result set of that is that",
    "start": "1861240",
    "end": "1867159"
  },
  {
    "text": "you end up with you know quite a large um data set that you can then look at in the",
    "start": "1867159",
    "end": "1872320"
  },
  {
    "text": "future let's talk about custom logic because that's something that um we might want to have to our application to",
    "start": "1872320",
    "end": "1878080"
  },
  {
    "text": "make decision trees based off of the the way the llm works because having a natural language prompt is so much more",
    "start": "1878080",
    "end": "1884880"
  },
  {
    "text": "convenient for users I can ask questions in the way that I want to ask questions I don't have to try try and figure out what are the keywords I have to put into",
    "start": "1884880",
    "end": "1890639"
  },
  {
    "text": "um a prompt to to trigger the right bit of searching with open a we have a feature called function calling well it used to",
    "start": "1890639",
    "end": "1898519"
  },
  {
    "text": "be called functions it's kind of still called functions but they're deprecating that name and they're calling it tools now so um so it's functions and tools",
    "start": "1898519",
    "end": "1906120"
  },
  {
    "text": "they're fundamentally the same thing what we do with this is we generate a Json structure that we",
    "start": "1906120",
    "end": "1912000"
  },
  {
    "text": "provide as part of the prompt that goes up um with the user prompt to then look for information inside of user prompt to",
    "start": "1912000",
    "end": "1918519"
  },
  {
    "text": "then get ajacent response back so in this case I've created one called search hotels I'm saying that it's going to",
    "start": "1918519",
    "end": "1923799"
  },
  {
    "text": "have some properties inside of the user prompt I'm looking for a location and a Max price and then when we have the user",
    "start": "1923799",
    "end": "1930360"
  },
  {
    "text": "prompt comes in hotel with private beach Max cost 300 in delaris instead of",
    "start": "1930360",
    "end": "1936279"
  },
  {
    "text": "giving me a text completion of a hotel you might be looking for in delmaris is blah blah blah blah blah it comes back",
    "start": "1936279",
    "end": "1942200"
  },
  {
    "text": "with adjacent payload location delaris Max price 300 because those are the things I said to look for with inside",
    "start": "1942200",
    "end": "1947960"
  },
  {
    "text": "that prompt now it's worth noting that while these are called functions or have traditionally been called",
    "start": "1947960",
    "end": "1954320"
  },
  {
    "text": "functions that's not executing something inside of your application it's not executing a function it's giving you",
    "start": "1954320",
    "end": "1960440"
  },
  {
    "text": "information that you can use to execute a function and this is why I'm glad that they're renaming it to tools instead of functions because it was a little bit",
    "start": "1960440",
    "end": "1966039"
  },
  {
    "text": "confusing you're not exposing your application's internal logic to the llm um you're getting information back from",
    "start": "1966039",
    "end": "1972360"
  },
  {
    "text": "the llm to then do something with inside of your application based off that result set let's have a look at a demo of",
    "start": "1972360",
    "end": "1978799"
  },
  {
    "text": "functions and this time I'm not going to type out the code because it's a little little bit more complex instead I'm going to drop over to link pad to a one",
    "start": "1978799",
    "end": "1985799"
  },
  {
    "text": "that I wrote earlier uh so here we go U I'm creating the connection to open AI uh using the",
    "start": "1985799",
    "end": "1993360"
  },
  {
    "text": "open AI SDK as before and what I'm then doing is I'm creating a chat completion",
    "start": "1993360",
    "end": "1998600"
  },
  {
    "text": "function tool definition that's a mouthful um in this uh because well it's",
    "start": "1998600",
    "end": "2004159"
  },
  {
    "text": "net we have to you know we use a net object structure to rather than just like Json um but here we go we've got",
    "start": "2004159",
    "end": "2009760"
  },
  {
    "text": "the name of it get Podcast info the description is it extracts information from the podcast description uh and the",
    "start": "2009760",
    "end": "2015200"
  },
  {
    "text": "things that I want to extract are going to be the summary uh and then I tell it some information about how I want that",
    "start": "2015200",
    "end": "2020720"
  },
  {
    "text": "summary constructed go away over uh in tell sense so generate a summary from the podcast description provided",
    "start": "2020720",
    "end": "2027480"
  },
  {
    "text": "summarized to a single sentence uh I want the topics I want a oh I want a comma separated list it was",
    "start": "2027480",
    "end": "2035039"
  },
  {
    "text": "still actually generating a comma separated list even though it said a common um separated list from the uh of topics from the description and a comma",
    "start": "2035039",
    "end": "2041399"
  },
  {
    "text": "separated list of from the description excludes Scott Hanselman because while he's a speaker he's a known speaker",
    "start": "2041399",
    "end": "2047399"
  },
  {
    "text": "across all the episodes um what then what I do is I'm giving uh creating the chat completion",
    "start": "2047399",
    "end": "2053320"
  },
  {
    "text": "options I'm giving it two user messages um which is the description of two different podcast",
    "start": "2053320",
    "end": "2058638"
  },
  {
    "text": "episodes and then I call the chat completion acing instead of the streaming because I want that Jason",
    "start": "2058639",
    "end": "2064440"
  },
  {
    "text": "payload to come back as a single Json lump I don't want there's no Point streaming this response and then we have",
    "start": "2064440",
    "end": "2070560"
  },
  {
    "text": "a look through this we look is the fish reason the reason that we got that completion back is that because of a tool call if it was we're going to have",
    "start": "2070560",
    "end": "2076679"
  },
  {
    "text": "a look at our um we're going to extract the tool calls out of that and we're then going to dilize those and just dump",
    "start": "2076679",
    "end": "2082079"
  },
  {
    "text": "them out in link pad so let's run this one and let's uh we'll zoom in the bottom here once we get our response",
    "start": "2082079",
    "end": "2089560"
  },
  {
    "text": "back zoom zoom zoom and we'll see here we get a tool response uh we have a a summary um uh quiz you last teach",
    "start": "2089560",
    "end": "2097880"
  },
  {
    "text": "founded free code Camp yada yada yada uh topics Quincy lson free code Camp learn to code resources the speaker was Quincy",
    "start": "2097880",
    "end": "2105880"
  },
  {
    "text": "and then using um the one with Jose interestingly it didn't respect what I said of don't include Scott hansman it",
    "start": "2105880",
    "end": "2111920"
  },
  {
    "text": "still included Scott hansman in that so and this is where you kind of have to work on well is there something I should",
    "start": "2111920",
    "end": "2117000"
  },
  {
    "text": "change about the way that prompt Works should I have added a system prompt that also instructed to exclude Scott hansman",
    "start": "2117000",
    "end": "2123119"
  },
  {
    "text": "and this is where like from a prompt engineering standpoint you iterate on the prompts that you're working with to get back the responses getting but yeah",
    "start": "2123119",
    "end": "2128720"
  },
  {
    "text": "you can kind of see what you got from there that's how we extracted those topics and that's how then on the web application I had I had speakers I had",
    "start": "2128720",
    "end": "2135280"
  },
  {
    "text": "topics and so on and so forth in my own application that actually runs that demo um it does a few things to like um",
    "start": "2135280",
    "end": "2142040"
  },
  {
    "text": "sanitize the data a little bit more like removes things like Hansel minutes and Scott hansman as topics because well",
    "start": "2142040",
    "end": "2147440"
  },
  {
    "text": "they're not really",
    "start": "2147440",
    "end": "2150078"
  },
  {
    "text": "topics and then this comes to speech speech is something that well we've been doing speech for a long time like speech",
    "start": "2155280",
    "end": "2162079"
  },
  {
    "text": "in this is this is not a new thing um speech to text doing live translation",
    "start": "2162079",
    "end": "2168000"
  },
  {
    "text": "sorry live transcriptions that's been around for quite a while um the difference with the llm based ones so",
    "start": "2168000",
    "end": "2173319"
  },
  {
    "text": "with openi that's the whisper model family they the speed that they can respond with the ocelot lardis Paradis",
    "start": "2173319",
    "end": "2181640"
  },
  {
    "text": "is a small wild cat native to the southwestern United States Mexico and",
    "start": "2181640",
    "end": "2186760"
  },
  {
    "text": "Central and South America this medium-sized cat is characterized by solid black spots and streaks on its",
    "start": "2186760",
    "end": "2194480"
  },
  {
    "text": "coat and the accuracy like there's some complex words in that things like lardis",
    "start": "2194480",
    "end": "2199560"
  },
  {
    "text": "Paralis like that's like that's a complex word who's been to a talk where they've had live transcription that's",
    "start": "2199560",
    "end": "2206000"
  },
  {
    "text": "machine generated and someone has said the word sequel that's a fun one to to see translated but with these LM based",
    "start": "2206000",
    "end": "2213280"
  },
  {
    "text": "live translations sorry live transcriptions like it's so much more",
    "start": "2213280",
    "end": "2218319"
  },
  {
    "text": "accurate in that like it understands complex words and Technology terms and things like that um but the other thing",
    "start": "2218319",
    "end": "2224200"
  },
  {
    "text": "that we can do with uh with this is we can do live translation I can go from uh",
    "start": "2224200",
    "end": "2229319"
  },
  {
    "text": "English spoken to say Italian as an output and then that brings us to the",
    "start": "2229319",
    "end": "2235000"
  },
  {
    "text": "next idea which is speech to text text to speech again not exactly a new thing",
    "start": "2235000",
    "end": "2241280"
  },
  {
    "text": "because I've got Google homes in my house and Amazon alexas and stuff like like that's doing text to speech that",
    "start": "2241280",
    "end": "2247800"
  },
  {
    "text": "again that's not new but it's the Fidelity of the the language that comes back so let's take this um sentence to",
    "start": "2247800",
    "end": "2254319"
  },
  {
    "text": "best meet your needs I would recommend the following or to best meet your needs I would recommend the following thanks",
    "start": "2254319",
    "end": "2261640"
  },
  {
    "text": "AI Aaron so for this one I've actually trained it using a custom neural voice",
    "start": "2261640",
    "end": "2267280"
  },
  {
    "text": "so rather than just a machine generated voice this is being trained on my voice I provided for that sample there I",
    "start": "2267280",
    "end": "2273440"
  },
  {
    "text": "provided with 34 utterances so those were about six word phras I just read them to my computer and it",
    "start": "2273440",
    "end": "2280040"
  },
  {
    "text": "would um tell me whether I like mispronounced stuff whatever the case may be and then it's been about an hour",
    "start": "2280040",
    "end": "2286800"
  },
  {
    "text": "training and it got me that like that level of almost accuracy I I would say",
    "start": "2286800",
    "end": "2292160"
  },
  {
    "text": "it's probably about 75% of my own voice it doesn't have tones and inflections and things like that that I would normally have when speaking uh there are",
    "start": "2292160",
    "end": "2298520"
  },
  {
    "text": "definitely some americanisms in there I played around with it and they uh had the word ask and it was very ask not ask",
    "start": "2298520",
    "end": "2304960"
  },
  {
    "text": "and don't get it to say my name I'm not Aaron I'm Aaron so but but that was on",
    "start": "2304960",
    "end": "2311680"
  },
  {
    "text": "like 34 phrases right the next step up for that is the the pro bottle with we",
    "start": "2311680",
    "end": "2318000"
  },
  {
    "text": "got with inside Azure AI service and that one takes a minimum of 300 um phrases and that generates much higher",
    "start": "2318000",
    "end": "2324440"
  },
  {
    "text": "quality audio but also then we can go into the next aspect of that which is fine-tuning of the Audio model using the",
    "start": "2324440",
    "end": "2330920"
  },
  {
    "text": "speech synthesis markup language ssml say that fast five times that um we can",
    "start": "2330920",
    "end": "2336480"
  },
  {
    "text": "use to change things like the pitch and the tone of the voice talent that we're working with in uh in the applications",
    "start": "2336480",
    "end": "2342839"
  },
  {
    "text": "that we're building so let's have a look at how we use speech from a net",
    "start": "2342839",
    "end": "2349000"
  },
  {
    "text": "perspective I'm going to come back over to link pad and I have my speech demo here um so this is using a different SDK",
    "start": "2349440",
    "end": "2356160"
  },
  {
    "text": "it's using the speech SDK not the open AI SDK um because the just how it works so what I've got",
    "start": "2356160",
    "end": "2363240"
  },
  {
    "text": "here is a you loading up the empo key and everything like that I'm glad I've sent my keys because I can see people",
    "start": "2363240",
    "end": "2368960"
  },
  {
    "text": "taking photos of it it's also recorded um I'm using uh a a neural voice uh",
    "start": "2368960",
    "end": "2376079"
  },
  {
    "text": "sorry yeah that I've generated um this is actually using the pro neural voice not the um the light one which was what",
    "start": "2376079",
    "end": "2382280"
  },
  {
    "text": "I had in the slide deck there uh so we create um the speech config we um Define uh the audio quality that we want what's",
    "start": "2382280",
    "end": "2389240"
  },
  {
    "text": "its output going to be um I generate a location on this that I'm going to then write that to uh I then give it a prompt",
    "start": "2389240",
    "end": "2395640"
  },
  {
    "text": "that I want it to say and let's just kick that off because it'll take a little bit of time to generate that and",
    "start": "2395640",
    "end": "2401960"
  },
  {
    "text": "then it's going to um generate output generate a wave file output so I'm just generating this as as wave file outputs",
    "start": "2401960",
    "end": "2408240"
  },
  {
    "text": "um using a speech synthesizer and we're waiting till we get a completed response back so let's copy this path so this is",
    "start": "2408240",
    "end": "2415960"
  },
  {
    "text": "path to the file it's generated and we'll paste that in hello NDC Sydney this is AI Aaron",
    "start": "2415960",
    "end": "2424440"
  },
  {
    "text": "speaking to you and I hope that you are looking forward to this this event I wish I could attend in person but as I",
    "start": "2424440",
    "end": "2430480"
  },
  {
    "text": "am not a person that would not really be possible thanks AI Aaron I've been doing",
    "start": "2430480",
    "end": "2436680"
  },
  {
    "text": "a lot of heavy lifting today but but we can see that from like 10 lines of",
    "start": "2436680",
    "end": "2442680"
  },
  {
    "text": "code and I can take any bit of text up to there's a character limit you can't put like War and Peace in there and get",
    "start": "2442680",
    "end": "2448640"
  },
  {
    "text": "it to read your War and Peace U well you could you would just have to chunk it down to a lot of really small files uh but like 10 lines of code and",
    "start": "2448640",
    "end": "2455960"
  },
  {
    "text": "we've added an accessibility layer to our application using a voice Talent such as",
    "start": "2455960",
    "end": "2462000"
  },
  {
    "text": "myself um to represent the uh the the",
    "start": "2462000",
    "end": "2467319"
  },
  {
    "text": "personality of our assistant the brand that we're working for or whatever the case may",
    "start": "2467319",
    "end": "2473599"
  },
  {
    "text": "be all right let's talk about the knowledge of the models let's say this prompt here is",
    "start": "2477280",
    "end": "2484200"
  },
  {
    "text": "Queen Elizabeth II still alive and the answer today is going to vary depending on what model you're working with",
    "start": "2484200",
    "end": "2489960"
  },
  {
    "text": "because the models have a knowledge cut off date we can see there things like gbd 35 turbo mentioned earlier as well",
    "start": "2489960",
    "end": "2495079"
  },
  {
    "text": "cutoff date was um September 2021 gbt 4 uh that was last year it had its cut off",
    "start": "2495079",
    "end": "2500920"
  },
  {
    "text": "date so a respons come back could be that she's still",
    "start": "2500920",
    "end": "2506560"
  },
  {
    "text": "alive maybe not I mean let's not get into conspiracy theory World um or it could come back",
    "start": "2506560",
    "end": "2513400"
  },
  {
    "text": "with a response where it's like well based off of my training information this is is my this is my answer but I",
    "start": "2513400",
    "end": "2519119"
  },
  {
    "text": "might be out of date so the model the models are they",
    "start": "2519119",
    "end": "2524720"
  },
  {
    "text": "don't necessarily have all the answers because they don't know they don't know beyond a particular point in time but",
    "start": "2524720",
    "end": "2530560"
  },
  {
    "text": "they also don't know about your organization let's say I work for a fical company called I don't know",
    "start": "2530560",
    "end": "2536000"
  },
  {
    "text": "Koso probably heard of them and we we are an online outdoor um like clothing",
    "start": "2536000",
    "end": "2542880"
  },
  {
    "text": "retailer so we want to have a way that people can come and ask natural language questions like I need warm waterproof shoes to go on a hike well we passed",
    "start": "2542880",
    "end": "2549680"
  },
  {
    "text": "that to our LM and it's going to come back with surea I can help you um here's a shoe that you can uh that would be",
    "start": "2549680",
    "end": "2554960"
  },
  {
    "text": "fitting that well but what if I don't have those like they're not ones that we",
    "start": "2554960",
    "end": "2560240"
  },
  {
    "text": "stock we're not a a retailer for um for that brand or like how I want to direct",
    "start": "2560240",
    "end": "2565480"
  },
  {
    "text": "people to the um the purchase path I want them to click through to the product page on my website to get that well how do I do",
    "start": "2565480",
    "end": "2571480"
  },
  {
    "text": "that well this is where we add out of the prompt so we put this in the meta prompt that goes up with the user we saw",
    "start": "2571480",
    "end": "2577280"
  },
  {
    "text": "that where we added some knowledge around Net a so in this case we might add some information so like here's a bunch of trail walking shoes that people",
    "start": "2577280",
    "end": "2583359"
  },
  {
    "text": "might be interested in uh and then the prompt comes in asks for warm waterpr shoes to go on a hike and it comes back with saying sure here's a product the",
    "start": "2583359",
    "end": "2589640"
  },
  {
    "text": "trail Walker hiking shoes here's this here's the ID here's the link to that with inside of our product catalog so the person can then go across and",
    "start": "2589640",
    "end": "2595359"
  },
  {
    "text": "purchase that it's that very natural flow um into like a a checkout process uh and it's done through a an experience",
    "start": "2595359",
    "end": "2602480"
  },
  {
    "text": "where someone hasn't tried to figure out you know like what's I don't know what the shoes called or anything like that",
    "start": "2602480",
    "end": "2607520"
  },
  {
    "text": "or but what if they said um I need shoes to keep my feet dry while going on a hike well like dry doesn't necessarily",
    "start": "2607520",
    "end": "2615559"
  },
  {
    "text": "match anything with inside of our database um and is dry any words that we've got with inside of that text we've",
    "start": "2615559",
    "end": "2621800"
  },
  {
    "text": "got there probably not there's where we might want to look into something like the retrieval",
    "start": "2621800",
    "end": "2627160"
  },
  {
    "text": "augmented generation pattern or rag pattern for short so in this case we get the user prompt that comes in and then",
    "start": "2627160",
    "end": "2633319"
  },
  {
    "text": "we're going to perform some kind of a query over it we might go out to our llm and generate an embedding for that and",
    "start": "2633319",
    "end": "2638880"
  },
  {
    "text": "then do a semantic search against our previous like our product list which we've generated embeddings for installed",
    "start": "2638880",
    "end": "2644800"
  },
  {
    "text": "them in a vector database we then select only the relevant Ones based off of our relevancy threshold and return those",
    "start": "2644800",
    "end": "2651440"
  },
  {
    "text": "back that goes to our uh our llm to complete the prompt of the users the user initial question grounded on",
    "start": "2651440",
    "end": "2657599"
  },
  {
    "text": "knowledge that we've figured out from their initial prompt so that they're getting contextually relevant information we're not trying to load our",
    "start": "2657599",
    "end": "2663280"
  },
  {
    "text": "entire product database into a prompt because we're probably going to run out of tokens to be able to send we're only",
    "start": "2663280",
    "end": "2668559"
  },
  {
    "text": "getting the stuff that's relevant to them and ultimately that's how the application that we have for the",
    "start": "2668559",
    "end": "2674839"
  },
  {
    "text": "handsome in spot is working so let's go back over now that we've seen all these little pieces text",
    "start": "2674839",
    "end": "2680599"
  },
  {
    "text": "Generation Um embedding generation so on and so forth let's see all of that how it comes together with inside of uh the",
    "start": "2680599",
    "end": "2688079"
  },
  {
    "text": "application so we start with our text prompt I want to learn about retr gaming and emulators for this I've added a",
    "start": "2688079",
    "end": "2695319"
  },
  {
    "text": "system prompt that's a um your uh you're an assistant that helps people find information about um podcast from the",
    "start": "2695319",
    "end": "2701480"
  },
  {
    "text": "handome minut podcast um you know you're going to provide links to relevant podcasts that people that people will",
    "start": "2701480",
    "end": "2707559"
  },
  {
    "text": "want to look for um we have our text of speech there which is taking this large chunk of text",
    "start": "2707559",
    "end": "2713520"
  },
  {
    "text": "uh generating a wave file and then streaming this back to the browser when the user clicks play and then we get",
    "start": "2713520",
    "end": "2719800"
  },
  {
    "text": "back these um semantically relevant results from uh our application so the application itself is",
    "start": "2719800",
    "end": "2726280"
  },
  {
    "text": "made up of um it's Aire application it's got a whole bunch of different stuff in here uh so inside of uh let's start with",
    "start": "2726280",
    "end": "2734200"
  },
  {
    "text": "our our app host and we'll just have a look at all the things we're put popping in here all right um Alt Enter full screen",
    "start": "2734200",
    "end": "2742760"
  },
  {
    "text": "no control shift p full screen what's the keyboard short control KZ oh cool I",
    "start": "2742760",
    "end": "2748920"
  },
  {
    "text": "was going to muscle memory that one uh okay so um to start this application up we're going to look for a",
    "start": "2748920",
    "end": "2755079"
  },
  {
    "text": "bunch of things like the get conf ation information and whatnot because I want those to be cascaded through to all our",
    "start": "2755079",
    "end": "2760520"
  },
  {
    "text": "sub applications um this is an aspired talk but I just want to touch on some of the the key parts that we've got in here",
    "start": "2760520",
    "end": "2766559"
  },
  {
    "text": "I've got um an as storage account which I'm using to store uh the status of uh indexing of the podcast episodes because",
    "start": "2766559",
    "end": "2773200"
  },
  {
    "text": "I do this in the background using a a worker process so I pop a message on the que say IND Index this podcast it's then",
    "start": "2773200",
    "end": "2779119"
  },
  {
    "text": "going to run that indexing process and um if I shut down the server it can pick up from where it was previously at um",
    "start": "2779119",
    "end": "2784359"
  },
  {
    "text": "stepwise uh because this is R like running locally I'm just setting up the emulator with some known ports uh and",
    "start": "2784359",
    "end": "2790319"
  },
  {
    "text": "I'm mounting the volume uh so that every time I restart the application it doesn't start with no data when taking",
    "start": "2790319",
    "end": "2796119"
  },
  {
    "text": "better part of a day to generate your data you don't want to be doing that multiple times in a day um then create a queue and a blob's",
    "start": "2796119",
    "end": "2803440"
  },
  {
    "text": "storage resource then I have a postgress database where I'm specifying a custom container image here on line 43 for",
    "start": "2803440",
    "end": "2810680"
  },
  {
    "text": "using PG Vector because I want that extension um when if I was to point this to Azure um PG Vector is a native with",
    "start": "2810680",
    "end": "2817240"
  },
  {
    "text": "inside of azure I've just got some debugging as well so I can see all the SQL ques that happen and I mount initial",
    "start": "2817240",
    "end": "2822640"
  },
  {
    "text": "SQL script to scaffold up our database um I'm then creating a memory",
    "start": "2822640",
    "end": "2828280"
  },
  {
    "text": "project so uh this is the service that I call to ask that question of which is then going to do the LM um generation",
    "start": "2828280",
    "end": "2834240"
  },
  {
    "text": "response I have our loader project which is the one that's going to load data into the index and then I have a front",
    "start": "2834240",
    "end": "2840480"
  },
  {
    "text": "end let's have a look at the worker uh and because uh I am an F fan",
    "start": "2840480",
    "end": "2847359"
  },
  {
    "text": "I'm contractually obliged to show F at least once in a talk so sorry if you're only coming here for the C you know this",
    "start": "2847359",
    "end": "2853520"
  },
  {
    "text": "is this is f come talk to me more about fop if you want to uh there are dozens of us that",
    "start": "2853520",
    "end": "2859839"
  },
  {
    "text": "care uh so this is just a like a background job that's running and we'll see we have our open AI function um so",
    "start": "2859839",
    "end": "2866960"
  },
  {
    "text": "this is I wrote this a while ago before they changed from functions to tools so this is using a slightly different syntax but you can see there I've got",
    "start": "2866960",
    "end": "2873440"
  },
  {
    "text": "I'm asking for the summary the topics and the speakers as comma separated",
    "start": "2873440",
    "end": "2878520"
  },
  {
    "text": "lists um we then serialize that down and then we come down and we call our uh we",
    "start": "2878520",
    "end": "2884359"
  },
  {
    "text": "get our chat completions here um I'm not doing this asynchronously just I was having some problems with the",
    "start": "2884359",
    "end": "2889480"
  },
  {
    "text": "asynchronously I have a branch that's better async but yeah we go out we we generate those responses using our gbt",
    "start": "2889480",
    "end": "2895280"
  },
  {
    "text": "35 model um with our function definition or our tool definition uh we then unpack",
    "start": "2895280",
    "end": "2900599"
  },
  {
    "text": "that payload extract the speakers the summary and the topic and then send that over to our memory client to index uh so",
    "start": "2900599",
    "end": "2907960"
  },
  {
    "text": "it's then going to generate the vectors um store those in postgress uh and then make an endpoint available for searching",
    "start": "2907960",
    "end": "2914079"
  },
  {
    "text": "which happens in this chunk of code down here import memory record which is up here we have our memory client now for",
    "start": "2914079",
    "end": "2921599"
  },
  {
    "text": "the memory application um I actually didn't write much of that code I'm using an open source project from Microsoft",
    "start": "2921599",
    "end": "2926839"
  },
  {
    "text": "called kernel memory which is kind of like rag patent as a service you just like it's a library you drop in or um in",
    "start": "2926839",
    "end": "2933760"
  },
  {
    "text": "this case I'm running it as a web server in program do FS let's find no let's find the right",
    "start": "2933760",
    "end": "2940200"
  },
  {
    "text": "one here we go um so I'm just creating a kernel memory Builder here um attaching",
    "start": "2940200",
    "end": "2945680"
  },
  {
    "text": "the postrest database The Blob storage uh saying that I'm going to be using open AI text embeddings and open AI text",
    "start": "2945680",
    "end": "2952440"
  },
  {
    "text": "completions or text generation they call it um and then this is using cues as orchestrators for all of that so it's a",
    "start": "2952440",
    "end": "2958559"
  },
  {
    "text": "lot more asynchronous background processing and so on and so forth but I haven't had to write any code to",
    "start": "2958559",
    "end": "2965880"
  },
  {
    "text": "actually touch an l in this case not to generate embeddings not to write to PG Vector not to um uh to to handle the",
    "start": "2965880",
    "end": "2975440"
  },
  {
    "text": "request that comes in from the user to then um find the semantic searching semantically relevant um options and or",
    "start": "2975440",
    "end": "2982799"
  },
  {
    "text": "anything like that I have an upload endpoint where I send it the podcast episode description and um tags and the",
    "start": "2982799",
    "end": "2989000"
  },
  {
    "text": "speaker and all that kind of stuff and then it will import that",
    "start": "2989000",
    "end": "2994040"
  },
  {
    "text": "document um this will also take care of things like chunk let's say you've got a PDF document um you can just upload the",
    "start": "2994040",
    "end": "2999720"
  },
  {
    "text": "PDF it will then uh extract all that text as small chunks um uh generate",
    "start": "2999720",
    "end": "3005680"
  },
  {
    "text": "vectors of those chunks rather than of like the entire document because you've got to find that balance between how",
    "start": "3005680",
    "end": "3012319"
  },
  {
    "text": "small do you want those vectors to be uh to give you useful semantic results",
    "start": "3012319",
    "end": "3018240"
  },
  {
    "text": "versus um how how complex do you want people to be able to put prompts in um",
    "start": "3018240",
    "end": "3023280"
  },
  {
    "text": "so there again that's from a a testing standpoint you kind got to iterate on am I chunking like what's the am I chunking",
    "start": "3023280",
    "end": "3029559"
  },
  {
    "text": "on paragraph am I chunking on page am I chunking at some kind of um uh Gap in",
    "start": "3029559",
    "end": "3034599"
  },
  {
    "text": "between um I've actually done I've done this this kind of a demo on YouTube video",
    "start": "3034599",
    "end": "3040040"
  },
  {
    "text": "transcripts um so I could actually do this one but use the podcast transcripts instead of the podcast descriptions and",
    "start": "3040040",
    "end": "3046079"
  },
  {
    "text": "in that kind of stuff you want to Chunk we we found The Sweet Spot was about 3 minutes let's say you got a whole bunch",
    "start": "3046079",
    "end": "3051160"
  },
  {
    "text": "of internal training videos pull the transcripts from those chunk them down into 3 minute time segment ments and",
    "start": "3051160",
    "end": "3057400"
  },
  {
    "text": "generate vectors of those store them in a database and then have an interface where someone can ask questions find the",
    "start": "3057400",
    "end": "3063040"
  },
  {
    "text": "point with inside of a trading video for them to be launched directly into um if anyone wants to see like learn more",
    "start": "3063040",
    "end": "3068200"
  },
  {
    "text": "about that kind of solution I do have sample application running that like I said on YouTube playlists um sorry",
    "start": "3068200",
    "end": "3074839"
  },
  {
    "text": "YouTube videos and transcriptions and yeah like we were deep linking to like the point in a YouTube video that was",
    "start": "3074839",
    "end": "3080599"
  },
  {
    "text": "relevant to a question that someone wanted to know the answer to I thought that was kind of cool um the other the other major",
    "start": "3080599",
    "end": "3087599"
  },
  {
    "text": "endpoint here is ask a question so this is where I put in that prompt I want to learn about retro gaming and and",
    "start": "3087599",
    "end": "3093680"
  },
  {
    "text": "emulators it's going to go into the memory it's going to say it's G to ask that question of my memory the question",
    "start": "3093680",
    "end": "3099240"
  },
  {
    "text": "it's going to look at the index because I might have multiple indexes here could be the transcriptions it could be the podcast epis or could this could be for",
    "start": "3099240",
    "end": "3105440"
  },
  {
    "text": "multiple podcasts um uh if that was the way that we designed it are there any",
    "start": "3105440",
    "end": "3111000"
  },
  {
    "text": "filters looking for a particular speaker or a particular subtopic that's been extracted from that description um and",
    "start": "3111000",
    "end": "3116680"
  },
  {
    "text": "what is the minimum relevancy here uh so I'm using I think a08 relevancy so it's got to be 80% relevant to the prompt",
    "start": "3116680",
    "end": "3123440"
  },
  {
    "text": "that the user provided us and then this is going to generate me a response that I send back to the",
    "start": "3123440",
    "end": "3130599"
  },
  {
    "text": "client which if we come into home.",
    "start": "3130760",
    "end": "3136480"
  },
  {
    "text": "razor. CS uh we have a function here called do",
    "start": "3136480",
    "end": "3141920"
  },
  {
    "text": "ask iing think this is hard okay um we get our user prompt that's",
    "start": "3141920",
    "end": "3147079"
  },
  {
    "text": "coming in just make sure that there was actually user prompt provided if there was we're going to clear out previous answers we're going to clear out previous sources we're going to clear",
    "start": "3147079",
    "end": "3153079"
  },
  {
    "text": "out set the loading state to to True um delete all previous audio files we're going to look to see if they had any um",
    "start": "3153079",
    "end": "3159359"
  },
  {
    "text": "selected speakers and topics add those as filters to our um our memory call and",
    "start": "3159359",
    "end": "3165480"
  },
  {
    "text": "then we're just going to memory ask this question um here's the filters here's",
    "start": "3165480",
    "end": "3170599"
  },
  {
    "text": "the minimum relevancy he going to come back with the answer so that was that large chunk of text that we had at the",
    "start": "3170599",
    "end": "3176640"
  },
  {
    "text": "top here um retro gaming and emulators that's the answer to the question that someone was asking uh and then what were",
    "start": "3176640",
    "end": "3183200"
  },
  {
    "text": "the sources that made up that so what episodes were relevant to produce that answer um iterate over this so um in",
    "start": "3183200",
    "end": "3190960"
  },
  {
    "text": "this case uh a each Source represents a single podcast episode because they're",
    "start": "3190960",
    "end": "3196200"
  },
  {
    "text": "pretty short descriptions but you could imagine that if you were doing this on like a transcript of an episode or a",
    "start": "3196200",
    "end": "3202240"
  },
  {
    "text": "full PDF The Source might be the relevant Source might be page seven of that PDF and that gives you that back",
    "start": "3202240",
    "end": "3208920"
  },
  {
    "text": "instead of the here is a 30 page PDS of like a product disclosure statement um",
    "start": "3208920",
    "end": "3214400"
  },
  {
    "text": "in PDF form your relevance the relevant answer is in there somewhere and you've",
    "start": "3214400",
    "end": "3220799"
  },
  {
    "text": "got to then just like like great I could have just read the PDS myself um but no like being able to just like say here is",
    "start": "3220799",
    "end": "3227240"
  },
  {
    "text": "the relevant page even like the relevant paragraph when the inside of that page just means that you can dive straight into",
    "start": "3227240",
    "end": "3234280"
  },
  {
    "text": "it um and then lastly we um once we get that back we pass the answer across to our speech service this is also why the",
    "start": "3235160",
    "end": "3241680"
  },
  {
    "text": "link pad thing said answer instead of prompt because I just copied the code across from here um the answer that comes back from uh from our our llm um",
    "start": "3241680",
    "end": "3250359"
  },
  {
    "text": "we're going to generate a uh generate the wave file and write that out to disk and then I've just got some error handling that I didn't have with the",
    "start": "3250359",
    "end": "3255799"
  },
  {
    "text": "inside of Link Pad but with that and looking at the",
    "start": "3255799",
    "end": "3261480"
  },
  {
    "text": "timing I think we're pretty much done and we do actually have some time for questions",
    "start": "3261480",
    "end": "3267920"
  },
  {
    "text": "but yes um the very least thank you for coming to this session I hope you've learned more about the pieces that can",
    "start": "3267920",
    "end": "3273760"
  },
  {
    "text": "come together to build a somewhat complex or at least what seems like a fair somewhat complex application and we",
    "start": "3273760",
    "end": "3281079"
  },
  {
    "text": "we saw that we start with just a text prompt that comes in but we can ground that on our own data we can ground that",
    "start": "3281079",
    "end": "3286280"
  },
  {
    "text": "on contextually relevant data to The Prompt that the user has provided us we're not just going here's a prompt",
    "start": "3286280",
    "end": "3292280"
  },
  {
    "text": "answer that question and kind of figure out you know chat GPT style we're gring it on our particular organization in",
    "start": "3292280",
    "end": "3298559"
  },
  {
    "text": "this case our organization happens to be the handle minutes podcast we're giving you relevancy based off of that like I",
    "start": "3298559",
    "end": "3304680"
  },
  {
    "text": "said um there's about five minutes left um if anyone's got questions throw your hands up and I can see Pop Bunch popping",
    "start": "3304680",
    "end": "3309960"
  },
  {
    "text": "up already excellent microphone",
    "start": "3309960",
    "end": "3316640"
  },
  {
    "text": "yeah um let's say I have a company website with specialized knowledge which",
    "start": "3316640",
    "end": "3322400"
  },
  {
    "text": "might be on a firewall or password protected website yep is it possible to train the llm on that website ah so a",
    "start": "3322400",
    "end": "3330920"
  },
  {
    "text": "good question one one thing I didn't talk about in adding knowledge to model is whether or not you should be training that model generally speaking no you",
    "start": "3330920",
    "end": "3337319"
  },
  {
    "text": "wouldn't actually retrain those models so the models that we have from open AI they they're what we call foundational models um we we kind of you kind of just",
    "start": "3337319",
    "end": "3344760"
  },
  {
    "text": "leave them alone and they do the things that they do what you then do is you add data to The Prompt that you have",
    "start": "3344760",
    "end": "3351039"
  },
  {
    "text": "extracted from the website from a knowledge base or whatever the case may be as part of that PR request that goes",
    "start": "3351039",
    "end": "3357160"
  },
  {
    "text": "up so the the bottle itself has not been retrained but you've grounded the question the person has asked on the",
    "start": "3357160",
    "end": "3363280"
  },
  {
    "text": "knowledge of your organization and then it can answer the question based off of it um there's a whole deeper set that",
    "start": "3363280",
    "end": "3369280"
  },
  {
    "text": "you can go into prompt engineering around what do you do with meta prompting um how you ground knowledge more deeply how you avoid jailbreaks how",
    "start": "3369280",
    "end": "3375280"
  },
  {
    "text": "you um instructed and stuff like that which I happy to to take offline because I it's the whole Topic in and of itself",
    "start": "3375280",
    "end": "3381760"
  },
  {
    "text": "but no generally speaking you shouldn't be retraining these models because that result in a lot more complexity down the",
    "start": "3381760",
    "end": "3387599"
  },
  {
    "text": "track new models come out it's you got to retrain again um and gbd3 so the kind",
    "start": "3387599",
    "end": "3392960"
  },
  {
    "text": "of the oldest of these models that took N9 days to train in Azure it cost $4.6",
    "start": "3392960",
    "end": "3400359"
  },
  {
    "text": "million so you don't want to be training models at any kind of frequency you want to be working at how you can add",
    "start": "3400359",
    "end": "3406039"
  },
  {
    "text": "information to them you augment which is what the rag pattern is designed for yep okay",
    "start": "3406039",
    "end": "3412160"
  },
  {
    "text": "thanks a question up here just the front um oh just on that while the the mic's coming out so the",
    "start": "3412160",
    "end": "3418559"
  },
  {
    "text": "the library used their kernel memory um you can actually say here's a web page and it will extract the text out of the web page you don't actually have had to",
    "start": "3418559",
    "end": "3424200"
  },
  {
    "text": "like do that text generation yourself so yeah I it's a it's a really cool project um I'd recommend checking out as a",
    "start": "3424200",
    "end": "3430559"
  },
  {
    "text": "evaluation tool in in kind of B project you might be building yeah sorry a quick questions like do you have the source",
    "start": "3430559",
    "end": "3436319"
  },
  {
    "text": "code for the things that you show in GitHub somewhere or uh yes they're in GitHub are they public no um no so that",
    "start": "3436319",
    "end": "3444599"
  },
  {
    "text": "that uh that um um app that I had there I'm going to",
    "start": "3444599",
    "end": "3449960"
  },
  {
    "text": "make that public it's just it's it's been in a private repo as I've been iterating on and some stuff like that they also released preview three of",
    "start": "3449960",
    "end": "3455680"
  },
  {
    "text": "aspire like 7even hours ago uh and it broke all my demos last week when they released it so I'm using like Aspire",
    "start": "3455680",
    "end": "3462960"
  },
  {
    "text": "preview to an old version and that was another reason why I didn't make it public because I was like great I don't want like this half broken thing going",
    "start": "3462960",
    "end": "3468720"
  },
  {
    "text": "out so I'm going to do some polish on that um but it'll be on my GitHub github.com aronal I think it's called",
    "start": "3468720",
    "end": "3474000"
  },
  {
    "text": "handl it's bot I'm also going to try and it's talk to Scott see if we can put that like actually on the handsome in",
    "start": "3474000",
    "end": "3479599"
  },
  {
    "text": "website CU is really cool like it's it's a great way to discover stuff",
    "start": "3479599",
    "end": "3486359"
  },
  {
    "text": "yes what's the major difference between open Ai and Azure open",
    "start": "3486359",
    "end": "3492160"
  },
  {
    "text": "AI uh the main difference between Azure open Ai and open AI",
    "start": "3492160",
    "end": "3498200"
  },
  {
    "text": "is probably the responsible AI stuff that we have with inside of Microsoft so we we've got a lot of additional things",
    "start": "3498200",
    "end": "3504760"
  },
  {
    "text": "that you can tune over the top of the gbt models um to avoid things like",
    "start": "3504760",
    "end": "3510880"
  },
  {
    "text": "generate potentially um hateful responses um or sexualized content and things like that um there uh there are",
    "start": "3510880",
    "end": "3518319"
  },
  {
    "text": "ways that you can obviously do that with the open AI ones I know that open AI Enterprise also support some of that stuff we have a bunch of built-in",
    "start": "3518319",
    "end": "3524160"
  },
  {
    "text": "features to to do that that you can tune and um modify based off of the needs of the kinds of applications You're",
    "start": "3524160",
    "end": "3529559"
  },
  {
    "text": "Building um the other uh value proposition I see around Azure open AI",
    "start": "3529559",
    "end": "3534680"
  },
  {
    "text": "service over the top of just generalized open AI is the integration into Azure I I can connect in Azure AI search for",
    "start": "3534680",
    "end": "3542920"
  },
  {
    "text": "example with just like the Click of a button um and while that Vector database that I had there was in PG Vector um in",
    "start": "3542920",
    "end": "3548240"
  },
  {
    "text": "a in a postgress database if I push that into Azure AI search I can connect that into um Azure openai service um and then",
    "start": "3548240",
    "end": "3556960"
  },
  {
    "text": "I can use something like prompt flow to do a low code approach to building out this kind of a prompting application",
    "start": "3556960",
    "end": "3562559"
  },
  {
    "text": "we're net developers we at net but um we just have seamless integration of things",
    "start": "3562559",
    "end": "3567680"
  },
  {
    "text": "like you know a vector database as an example um and the responsible AI stuff as well is another one um I think there",
    "start": "3567680",
    "end": "3573960"
  },
  {
    "text": "are some more sessions that dive a bit more into Azure open AI service um over the next three",
    "start": "3573960",
    "end": "3579640"
  },
  {
    "text": "days uh I have a question about uh chat history management so when you're in",
    "start": "3583880",
    "end": "3589640"
  },
  {
    "text": "getting relevant knowledge through the embeddings usually it's about the latest",
    "start": "3589640",
    "end": "3595400"
  },
  {
    "text": "question that's come in um but as the conversation continues things may not be relevant and then I feel like I'm",
    "start": "3595400",
    "end": "3602400"
  },
  {
    "text": "wasting tokens and it blows out the cost you can imagine a conversation can end up being $3 per query and it may not be",
    "start": "3602400",
    "end": "3611480"
  },
  {
    "text": "relevant to previous so is there like a solution that you feel like for chat history management yeah I there's no one",
    "start": "3611480",
    "end": "3619200"
  },
  {
    "text": "size is going to fit all kind of solution for history management um there's a couple of TR of thought first of is like a 5o model just like the",
    "start": "3619200",
    "end": "3625599"
  },
  {
    "text": "oldest way you drop off as um as they lose relevancy uh the other is giving a UI option for people to be able to",
    "start": "3625599",
    "end": "3631880"
  },
  {
    "text": "delete messages out of History so it's like okay that's no longer relevant or that's like you know artificially um",
    "start": "3631880",
    "end": "3638400"
  },
  {
    "text": "influencing um the the prompt that I'm now trying to put in um the other one is looking at having kind of that decision",
    "start": "3638400",
    "end": "3645640"
  },
  {
    "text": "tree that's in there like if you see the responses come in uh and you know you look at that prompt using like a",
    "start": "3645640",
    "end": "3651799"
  },
  {
    "text": "function call First and say hey this one is asking about you this one's asking about hiking shoes okay so I'm going to",
    "start": "3651799",
    "end": "3657839"
  },
  {
    "text": "add stuff with hiking shoes and then we see followup questions which aren't triggering tool flows and then we get another one that triggers off a tool",
    "start": "3657839",
    "end": "3663680"
  },
  {
    "text": "flow maybe at that point we actually delete past history or we discard it and then inform the user that we didn't",
    "start": "3663680",
    "end": "3669680"
  },
  {
    "text": "include past history because it's we've deemed it no longer relevant if you want that relevancy explicitly opt it back in",
    "start": "3669680",
    "end": "3675920"
  },
  {
    "text": "um yeah like I I actually really like using functions or tools that naming is",
    "start": "3675920",
    "end": "3681119"
  },
  {
    "text": "hard um I really like using that to do decision trees with inside of it um but",
    "start": "3681119",
    "end": "3686160"
  },
  {
    "text": "then also you you start building out with say orchestrators like samanic kernel or Lang chain and they've got",
    "start": "3686160",
    "end": "3691839"
  },
  {
    "text": "opinionated approaches to doing history management well um I know we're at time um I'm happy to stay around and F",
    "start": "3691839",
    "end": "3697720"
  },
  {
    "text": "questions but also uh if you do want to to leave don't feel like I'm going to make it awkward for you well I mean I am",
    "start": "3697720",
    "end": "3704119"
  },
  {
    "text": "but I'm not going to make it more Awkward for you uh but at the very least thank you for joining me today I hope you've learned something new I hope",
    "start": "3704119",
    "end": "3710200"
  },
  {
    "text": "you're inspired by the way you can use AI enjoy the event",
    "start": "3710200",
    "end": "3716200"
  }
]