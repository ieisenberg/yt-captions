[
  {
    "start": "0",
    "end": "94000"
  },
  {
    "text": "mine so my name is daniel marbach and you're here in track one the evolutionary history of revitamq dot",
    "start": "6640",
    "end": "12639"
  },
  {
    "text": "net client towards currency so i work for a company called particular software and we're the makers",
    "start": "12639",
    "end": "18960"
  },
  {
    "text": "of and service bus and that's part of my job at particular but also in my free time",
    "start": "18960",
    "end": "24800"
  },
  {
    "text": "i worked a lot on the rabbitmq.net client so for multiple years since the version",
    "start": "24800",
    "end": "31679"
  },
  {
    "text": "4.1 i have been contributing to the revitmq.net client and helped to gradually evolve the",
    "start": "31679",
    "end": "37760"
  },
  {
    "text": "client into more into more modern and concurrency-enabled library together with the vmware team at that",
    "start": "37760",
    "end": "43680"
  },
  {
    "text": "time they were still all pivotal but now they're owned by vmware as well as",
    "start": "43680",
    "end": "48960"
  },
  {
    "text": "other stellar community members have helped me together to actually involve the client",
    "start": "48960",
    "end": "54079"
  },
  {
    "text": "so in this talk i will teach you i think very valuable lessons about what i have learned personally about",
    "start": "54079",
    "end": "60640"
  },
  {
    "text": "asic and concurrency and how you also can evolve your own business code bases or your",
    "start": "60640",
    "end": "65680"
  },
  {
    "text": "infrastructure frameworks and libraries into a more modern concurrency-enabled",
    "start": "65680",
    "end": "71200"
  },
  {
    "text": "paradigms so i guess many of these lessons can be translated into your daily work as well and of course on",
    "start": "71200",
    "end": "78799"
  },
  {
    "text": "top of that you will get deep knowledge about async the tpl the task para",
    "start": "78799",
    "end": "84000"
  },
  {
    "text": "library and i also will recap throughout this this talk some fundamentals of",
    "start": "84000",
    "end": "90000"
  },
  {
    "text": "asynchronous and current programming so",
    "start": "90000",
    "end": "95280"
  },
  {
    "start": "94000",
    "end": "155000"
  },
  {
    "text": "the goals of this talk is that i can't assume that everybody is familiar with the rabbit mq",
    "start": "95280",
    "end": "100560"
  },
  {
    "text": "broker and the client so we'll briefly recap that so that everyone is on the same baseline",
    "start": "100560",
    "end": "105759"
  },
  {
    "text": "and then i will go through why the client makes it particularly hard to",
    "start": "105759",
    "end": "110799"
  },
  {
    "text": "actually achieve concurrency with the existing client and then i will walk you through",
    "start": "110799",
    "end": "116479"
  },
  {
    "text": "the experience that i had together with the with the vmware team and the community members to evolve",
    "start": "116479",
    "end": "123360"
  },
  {
    "text": "gradually declined the net clients towards a more modern concurrency-enabled client",
    "start": "123360",
    "end": "130239"
  },
  {
    "text": "to so that you get a brief overview what i will be talking about i quickly tell you the high-level things",
    "start": "130239",
    "end": "136400"
  },
  {
    "text": "that i'm going to talk about so first of all what we did there the process was the following we",
    "start": "136400",
    "end": "141440"
  },
  {
    "text": "we tried to understand the synchronous path in the rabbit mq client and then we tried to optimize the",
    "start": "141440",
    "end": "147680"
  },
  {
    "text": "synchronous path and then we introduced the redundant asyc code path",
    "start": "147680",
    "end": "153040"
  },
  {
    "text": "into the rabbitmq client and then",
    "start": "153040",
    "end": "158239"
  },
  {
    "text": "and that is basically already the things that that were the most important stuff so in",
    "start": "158400",
    "end": "165040"
  },
  {
    "text": "this talk what i'm going to do i i figured like okay instead of going through thousands",
    "start": "165040",
    "end": "170319"
  },
  {
    "text": "of boring slides and make you all fall asleep because you're probably already on low on low sugar uh because it's almost uh",
    "start": "170319",
    "end": "177280"
  },
  {
    "text": "lunch time i figured why not go through some demos and and real code in the id",
    "start": "177280",
    "end": "182800"
  },
  {
    "text": "so i'll be spending most of my time in the id by the way should you have any questions feel free to shoot the",
    "start": "182800",
    "end": "188720"
  },
  {
    "text": "questions into into the chat of the webpacks please do not use a slack right now because i will",
    "start": "188720",
    "end": "194800"
  },
  {
    "text": "not be monitoring slack because there is a limit to my brain capacity how much concurrent things i can do",
    "start": "194800",
    "end": "200640"
  },
  {
    "text": "even though i'm talking about concurrency but um yeah feel free to uh shoot things into",
    "start": "200640",
    "end": "206720"
  },
  {
    "text": "the channel should also be the font size a little bit too small and you want me to zoom in feel free to shout out",
    "start": "206720",
    "end": "212000"
  },
  {
    "text": "as well i will be watching during the talk and and try to anticipate the questions as i can",
    "start": "212000",
    "end": "218879"
  },
  {
    "start": "218000",
    "end": "267000"
  },
  {
    "text": "okay so um like i said uh we will first of all go into uh what is the",
    "start": "218879",
    "end": "224799"
  },
  {
    "text": "client itself so and before we do that we actually i actually want to talk about what is",
    "start": "224799",
    "end": "230080"
  },
  {
    "text": "arabic revit mq itself well rabbitmq is a messaging uh an open source message broker and how",
    "start": "230080",
    "end": "236640"
  },
  {
    "text": "vmware puts it is the most widely deployed open source message broker so",
    "start": "236640",
    "end": "241840"
  },
  {
    "text": "what does it mean it's basically it's like a server it can be clustered and clients that are producing messages",
    "start": "241840",
    "end": "248720"
  },
  {
    "text": "can can send messages to the server they will be durably or non-durably stored depending on the configuration",
    "start": "248720",
    "end": "254959"
  },
  {
    "text": "and they will be reliably or not so reliably depending on the configuration as well delivered to all delivered to all the",
    "start": "254959",
    "end": "261759"
  },
  {
    "text": "consumers that are waiting on the other side and they're also connected to the broker the revit mq um",
    "start": "261759",
    "end": "269199"
  },
  {
    "start": "267000",
    "end": "328000"
  },
  {
    "text": "the broker itself for historic reasons it implements the aimqp091 protocol",
    "start": "269199",
    "end": "275919"
  },
  {
    "text": "the mqp091 protocol uh was later then on broken and entirely broken and it was re",
    "start": "275919",
    "end": "282400"
  },
  {
    "text": "re-developed into am key mqp 1.0 specification and it's a",
    "start": "282400",
    "end": "287440"
  },
  {
    "text": "completely different protocol mq-p 1.0 amqp 1.0 is supported by revit",
    "start": "287440",
    "end": "293919"
  },
  {
    "text": "mq although you have to have a plugin on the broker side to actually enable",
    "start": "293919",
    "end": "299280"
  },
  {
    "text": "clients that primarily talk mqp so that they can send and receive messages from the broker so what we can say today",
    "start": "299280",
    "end": "306400"
  },
  {
    "text": "is that the mqp091 protocol has become de facto",
    "start": "306400",
    "end": "311600"
  },
  {
    "text": "the rabbit mq protocol and it primarily defines exchanges and bindings and cues",
    "start": "311600",
    "end": "317199"
  },
  {
    "text": "and how these things actually interact together and as far as i know nobody else in the",
    "start": "317199",
    "end": "323280"
  },
  {
    "text": "industry right now implements the mqp091 protocol if we are pulling down the rabbitmq",
    "start": "323280",
    "end": "330320"
  },
  {
    "text": "client into our net solution we're getting we're going to nougat to a",
    "start": "330320",
    "end": "335360"
  },
  {
    "text": "search for the rabbitmq.net client and then what we're getting is the first entry point is the connection factory",
    "start": "335360",
    "end": "342160"
  },
  {
    "text": "here i'm running uh i'm running docker so i have a localhost this is port 555672",
    "start": "342160",
    "end": "350080"
  },
  {
    "text": "i'm also telling the connection factory to use background threads for io so that it has named threads behind the",
    "start": "350080",
    "end": "356479"
  },
  {
    "text": "scenes so that they know what's uh what's going on the connection factory itself is the one",
    "start": "356479",
    "end": "361600"
  },
  {
    "start": "359000",
    "end": "401000"
  },
  {
    "text": "that i connect that connects to the broker and creates uh connection uh objects connection objects can be",
    "start": "361600",
    "end": "369280"
  },
  {
    "text": "created here with great connections you can give them a name so that you can see uh",
    "start": "369280",
    "end": "374560"
  },
  {
    "text": "what's going on and connections they represent long-lived mqp09 objects",
    "start": "374560",
    "end": "381120"
  },
  {
    "text": "that then what they do is they for example observe the connection between the client of the broker",
    "start": "381120",
    "end": "386319"
  },
  {
    "text": "and if you for example lose the connection the tcp connection there it does the actual connection recovery",
    "start": "386319",
    "end": "392479"
  },
  {
    "text": "it also maps the frame the protocol frames that are underneath to to the headers and commands and",
    "start": "392479",
    "end": "398720"
  },
  {
    "text": "everything else of the underlying protocol now when you have a connection the most important piece there is and",
    "start": "398720",
    "end": "406479"
  },
  {
    "text": "that's the thing that actually enables you to interact uh with with the broker itself is what they call imodel",
    "start": "406479",
    "end": "413759"
  },
  {
    "text": "sometimes it's also referred in the documentation to as channels so that's a little bit",
    "start": "413759",
    "end": "418800"
  },
  {
    "text": "confusing so these two words are are basically used together so model and channel",
    "start": "418800",
    "end": "424000"
  },
  {
    "text": "i'm trying to stick to more to the to the notion of model because later on i will",
    "start": "424000",
    "end": "429199"
  },
  {
    "text": "be using channel in a different in a different uh term so that it doesn't get too confusing",
    "start": "429199",
    "end": "434720"
  },
  {
    "start": "434000",
    "end": "648000"
  },
  {
    "text": "okay so the model what it is it basically represents this mqp09 uh piece that is long lived uh",
    "start": "434720",
    "end": "442880"
  },
  {
    "text": "a long lift interaction over the connection uh with the broker and the broker",
    "start": "442880",
    "end": "448639"
  },
  {
    "text": "protocol methods applications that are using multiple",
    "start": "448639",
    "end": "453919"
  },
  {
    "text": "threads and and multiple processes they actually should be opening more or less let's say one one",
    "start": "453919",
    "end": "460960"
  },
  {
    "text": "channel per thread and one channel per process and threats should not be sharing",
    "start": "460960",
    "end": "466879"
  },
  {
    "text": "uh the models uh the models in in between and so what you have to do is you have",
    "start": "466879",
    "end": "472879"
  },
  {
    "text": "to establish basically a clear notion of how you mapping your application or server model",
    "start": "472879",
    "end": "478800"
  },
  {
    "text": "how you map your code uh to the models that's that you're creating uh via the connection most importantly",
    "start": "478800",
    "end": "486639"
  },
  {
    "text": "this though it's on the publishing publishing site so you should never ever mix multi-threaded publishes on the same",
    "start": "486639",
    "end": "493919"
  },
  {
    "text": "model the problem is if you start doing that what's going to happen is on the protocol",
    "start": "493919",
    "end": "499520"
  },
  {
    "text": "on the protocol level there will be frame interleaving happening and then when into leaving happens",
    "start": "499520",
    "end": "506160"
  },
  {
    "text": "because you're sharing the model with with multiple threads then things are going wild so what it means is the broker will",
    "start": "506160",
    "end": "513680"
  },
  {
    "text": "not be receiving what you just sent it will be receiving some binary chunk garbage",
    "start": "513680",
    "end": "519200"
  },
  {
    "text": "essentially a single connection that you're creating against the broker can have up to a max",
    "start": "519200",
    "end": "525040"
  },
  {
    "text": "100 channels that's though a setting that can be configured on the server side",
    "start": "525040",
    "end": "530480"
  },
  {
    "text": "but as you can probably imagine if you have to manage multiple uh models on on on",
    "start": "530480",
    "end": "536720"
  },
  {
    "text": "on your sides it can it can become quite complex and the other hand what's also possible is the more models",
    "start": "536720",
    "end": "543279"
  },
  {
    "text": "you're creating on the same connection the more the performance and throughput will will go down for your client",
    "start": "543279",
    "end": "549360"
  },
  {
    "text": "as well as the more models that the server needs to deal with the more the throughput can go down as",
    "start": "549360",
    "end": "555680"
  },
  {
    "text": "well so you need to carefully balance basically your applications needs in",
    "start": "555680",
    "end": "560720"
  },
  {
    "text": "terms of scalability towards the the client interaction model that you have with the",
    "start": "560720",
    "end": "566160"
  },
  {
    "text": "with the broker and then once you have this this model you need to create a an eventing basic",
    "start": "566160",
    "end": "572240"
  },
  {
    "text": "consumer that you pass in the receive model or the model that you're using and then",
    "start": "572240",
    "end": "578160"
  },
  {
    "text": "on this eventing basic consumer you have events delegates and these event delegates will fire whenever there is",
    "start": "578160",
    "end": "585120"
  },
  {
    "text": "a message on the broker and then you can you can do your codes so here um",
    "start": "585120",
    "end": "593120"
  },
  {
    "text": "in here what we can see is we have this consumer received event handler delegate where we're",
    "start": "593120",
    "end": "598320"
  },
  {
    "text": "getting these basic delivery event arcs what we do here is we just print out",
    "start": "598320",
    "end": "603440"
  },
  {
    "text": "something we can do is a thread sleep to simulate any kind of work and then what we do is we call on",
    "start": "603440",
    "end": "609120"
  },
  {
    "text": "the receive model the basic act with delivery tech for uh conceptually the delivery attack you",
    "start": "609120",
    "end": "615519"
  },
  {
    "text": "can imagine this is almost like a good it's basically a unique anti identifier that identifies your incoming",
    "start": "615519",
    "end": "622800"
  },
  {
    "text": "message and you can then with that unique identifier you can then say to the broker hey i actually successfully",
    "start": "622800",
    "end": "629040"
  },
  {
    "text": "handled this message or i did not successfully handle this message then you're naking the message",
    "start": "629040",
    "end": "634240"
  },
  {
    "text": "and then the broker will redeliver it again to the client or to another competing consumer that is",
    "start": "634240",
    "end": "640000"
  },
  {
    "text": "connected um on that same queue or exchange for example that that you're having on on your server",
    "start": "640000",
    "end": "648079"
  },
  {
    "start": "648000",
    "end": "712000"
  },
  {
    "text": "and then what we have here is we also have we can send messages i'm using a",
    "start": "648079",
    "end": "653279"
  },
  {
    "text": "confirms aware model or channel and what this is this is basically a reliable way to transfer",
    "start": "653279",
    "end": "659519"
  },
  {
    "text": "messages over the broker what this is going to do is it sends a message to the broker",
    "start": "659519",
    "end": "665120"
  },
  {
    "text": "and then it waits until the broker has actually acknowledged that the message is durably stored on",
    "start": "665120",
    "end": "671360"
  },
  {
    "text": "the broker side and when that happens that the broker then sends over the uh over connection protocol over mqp091",
    "start": "671360",
    "end": "680079"
  },
  {
    "text": "it sends back to me a message that says hey by the way i actually successfully handled the",
    "start": "680079",
    "end": "685279"
  },
  {
    "text": "message with this consumer tech that you just sent me why is that important well by default",
    "start": "685279",
    "end": "690480"
  },
  {
    "text": "you don't get these delivery guarantees so what that means is you might be sending to the",
    "start": "690480",
    "end": "695600"
  },
  {
    "text": "broker messages but then and the broker says yep okay i got it right but it hasn't really",
    "start": "695600",
    "end": "701760"
  },
  {
    "text": "durably stored it so if the light goes out and your broker is not highly reliable it might be that the",
    "start": "701760",
    "end": "706959"
  },
  {
    "text": "message is is gone so that's why we're using this confirms aware channel to actually send messages and as you can see here",
    "start": "706959",
    "end": "713360"
  },
  {
    "start": "712000",
    "end": "755000"
  },
  {
    "text": "i'm sending every 450 milliseconds i'm sending uh messages to the broker so let",
    "start": "713360",
    "end": "718800"
  },
  {
    "text": "me let me show you how this uh how this demo works um these are the client basic client bits",
    "start": "718800",
    "end": "726399"
  },
  {
    "text": "cleaning up my screen.net run so now i'm running this this sample as you can see on the left",
    "start": "726399",
    "end": "733120"
  },
  {
    "text": "side we're we're sending a value on the right side it's the current state of the queue uh on the broker uh represent with the",
    "start": "733120",
    "end": "739920"
  },
  {
    "text": "letter q column and because we're sending more messages than we're actually receiving right",
    "start": "739920",
    "end": "745200"
  },
  {
    "text": "because we all can only handle one messages at a time and we're doing this red sleep the messages will be basically piling up",
    "start": "745200",
    "end": "752880"
  },
  {
    "text": "on on the broker okay so what we can say is right because um we're only handing one",
    "start": "752880",
    "end": "759920"
  },
  {
    "start": "755000",
    "end": "813000"
  },
  {
    "text": "messages at the time the more we're sending messages and the more we're increasing this descending speed things start piling up",
    "start": "759920",
    "end": "766720"
  },
  {
    "text": "and of course we could say well one way of speeding up processing of messages would be if we",
    "start": "766720",
    "end": "772639"
  },
  {
    "text": "for example horizontally scale out the consumer so we would be copying",
    "start": "772639",
    "end": "777680"
  },
  {
    "text": "basically this process and connect it to the broker and then we would have many copies of that",
    "start": "777680",
    "end": "784079"
  },
  {
    "text": "running that receive messages from the broker but on the other hand well wouldn't it be nice if",
    "start": "784079",
    "end": "790480"
  },
  {
    "text": "we could for example make sure that we are efficiently using the resources that a single client can",
    "start": "790480",
    "end": "796560"
  },
  {
    "text": "actually have so not just basically scale out but first of all we want to scale up",
    "start": "796560",
    "end": "802639"
  },
  {
    "text": "right so we want to basically introduce concurrency um into into the client application so that",
    "start": "802639",
    "end": "808880"
  },
  {
    "text": "we can re receive multiple messages at the same time of course what you can do is normally",
    "start": "808880",
    "end": "816480"
  },
  {
    "start": "813000",
    "end": "839000"
  },
  {
    "text": "for example we just saw it up here right you could create multiple models but we talked about how that can",
    "start": "816480",
    "end": "822720"
  },
  {
    "text": "get complex especially with the restrictions on the server side or you could be creating multiple",
    "start": "822720",
    "end": "827839"
  },
  {
    "text": "eventing basic consumers to try to consume multiple",
    "start": "827839",
    "end": "833040"
  },
  {
    "text": "messages but dealing with that code can also get extremely complex so",
    "start": "833040",
    "end": "840000"
  },
  {
    "start": "839000",
    "end": "963000"
  },
  {
    "text": "what have most clients that are using the rabbit mq a client what have they been uh doing let me show you um how",
    "start": "840000",
    "end": "848000"
  },
  {
    "text": "you can achieve uh concurrency so the basic bits are still the same",
    "start": "848000",
    "end": "853120"
  },
  {
    "text": "we're creating a connection factory and all that stuff and uh in order to when we go into",
    "start": "853120",
    "end": "859440"
  },
  {
    "text": "concurrency land right one of the most important factors we want to take into account is we want",
    "start": "859440",
    "end": "864480"
  },
  {
    "text": "to make sure that we throttle requests at some point right because if you're just going and saying hey just",
    "start": "864480",
    "end": "871040"
  },
  {
    "text": "give me everything you have on the server well if you just have three messages on the server well that's not going to have a big",
    "start": "871040",
    "end": "876320"
  },
  {
    "text": "impact right but let's say if you have a db connection pool of 100 db connections that you can",
    "start": "876320",
    "end": "881920"
  },
  {
    "text": "currently create and you have a thousand messages on the server and you're going without",
    "start": "881920",
    "end": "886959"
  },
  {
    "text": "concurrency limitations the server will deliver you a thousand messages right then you're trying to spawn up a",
    "start": "886959",
    "end": "893519"
  },
  {
    "text": "thousand concurrent consumers and they will be overwhelming the db connection pool and things start timing out so we want",
    "start": "893519",
    "end": "899680"
  },
  {
    "text": "to make sure we limit the concurrency and we do that by having a symbol for slim which is going",
    "start": "899680",
    "end": "904880"
  },
  {
    "text": "to basically gatekeep things so that things are not running in unbounded",
    "start": "904880",
    "end": "910959"
  },
  {
    "text": "concurrency fashion then we're going to again wire up the received",
    "start": "910959",
    "end": "916720"
  },
  {
    "text": "delegate and then we we start kicking off now the problematic part is that the",
    "start": "916720",
    "end": "922880"
  },
  {
    "text": "consumer delegate here it's a regular.net event handler delegate so what we return",
    "start": "922880",
    "end": "929360"
  },
  {
    "text": "here is void so now if you're accustomed to do a tpl task based programming you're like okay",
    "start": "929360",
    "end": "936240"
  },
  {
    "text": "boyd ah kind of evil because well how can we actually introduce concurrency well one way is",
    "start": "936240",
    "end": "942880"
  },
  {
    "text": "that you start going into custom off loading by using task dot run or any of your preferred",
    "start": "942880",
    "end": "949519"
  },
  {
    "text": "worker thread pool offloading mechanics that you usually use another one is well",
    "start": "949519",
    "end": "955759"
  },
  {
    "text": "um you use async void right but as you probably remember from the async best practices asic void",
    "start": "955759",
    "end": "961920"
  },
  {
    "text": "is kind of evil why is it evil well the thing is first of all async void when you",
    "start": "961920",
    "end": "967279"
  },
  {
    "start": "963000",
    "end": "1013000"
  },
  {
    "text": "enter the thread that enters the method will synchronously execute to the point where the first await",
    "start": "967279",
    "end": "973279"
  },
  {
    "text": "statement comes that truly yields into an eye completion port",
    "start": "973279",
    "end": "978480"
  },
  {
    "text": "so that means everything here is executed synchronously and everything that throws up to that path will actually",
    "start": "978480",
    "end": "985279"
  },
  {
    "text": "synchronously throw up so that's fine right so for example if the if the the the client",
    "start": "985279",
    "end": "990480"
  },
  {
    "text": "bits assume that if an exception is thrown that they're throwing some other event handler delegates then that will work but as",
    "start": "990480",
    "end": "996959"
  },
  {
    "text": "soon as we have an await statement in an async void code that will no longer happen because",
    "start": "996959",
    "end": "1002560"
  },
  {
    "text": "from the perspective of the caller or the invoker of this event event handler the call is already",
    "start": "1002560",
    "end": "1010240"
  },
  {
    "text": "over and it will continue to do stuff so now that also means if you're doing uh if",
    "start": "1010240",
    "end": "1016480"
  },
  {
    "start": "1013000",
    "end": "1061000"
  },
  {
    "text": "you're using async void and even for example if you're using casted run what you would have to do",
    "start": "1016480",
    "end": "1022000"
  },
  {
    "text": "is we have to first uh copy the buffer that we're getting from from the client why do we have to copy",
    "start": "1022000",
    "end": "1028000"
  },
  {
    "text": "the buffer well if you don't copy the buffer it is possible that as soon as we are in going into a",
    "start": "1028000",
    "end": "1034240"
  },
  {
    "text": "concurrency land because we return back to the client but we're still processing the bytes",
    "start": "1034240",
    "end": "1040880"
  },
  {
    "text": "payload that we got from the server what's going to happen is the the client will start",
    "start": "1040880",
    "end": "1047678"
  },
  {
    "text": "will start collecting the allocated buffers and clean them away and if we don't copy it we run into into creating",
    "start": "1049120",
    "end": "1057520"
  },
  {
    "text": "garbage because of reused buffers and then again we need to acquire this",
    "start": "1057520",
    "end": "1063440"
  },
  {
    "start": "1061000",
    "end": "1076000"
  },
  {
    "text": "mf4 and to limit the currency but now we have another problem",
    "start": "1063440",
    "end": "1068640"
  },
  {
    "text": "because uh regularly what happens if and we said we want to handle let me go back up",
    "start": "1068640",
    "end": "1074080"
  },
  {
    "text": "here we have a concurrency of four we want to handle up to a max",
    "start": "1074080",
    "end": "1079200"
  },
  {
    "start": "1076000",
    "end": "1126000"
  },
  {
    "text": "for four messages at the same time so what happens is here if you're if the",
    "start": "1079200",
    "end": "1085039"
  },
  {
    "text": "semaphore still has slots available clients can enter the semaphore and the",
    "start": "1085039",
    "end": "1090720"
  },
  {
    "text": "semaphore will synchronously complete so that means when we run through here and the semaphore still has",
    "start": "1090720",
    "end": "1097600"
  },
  {
    "text": "slots available what's going to happen is that down here we will be still on the threat that is",
    "start": "1097600",
    "end": "1103760"
  },
  {
    "text": "invoking us so that means that is the rabbitmq client driver threats that are doing stuff so",
    "start": "1103760",
    "end": "1110880"
  },
  {
    "text": "if that happens we are actually not really concurrently doing anything because we are still blocking",
    "start": "1110880",
    "end": "1117039"
  },
  {
    "text": "uh the threat so we are not on unleashing it so what it means is he cannot pump more messages",
    "start": "1117039",
    "end": "1122960"
  },
  {
    "text": "to the client duplication at that specific time so now what we have to do is we basically have to do advanced",
    "start": "1122960",
    "end": "1128960"
  },
  {
    "start": "1126000",
    "end": "1161000"
  },
  {
    "text": "trickery like for example this one and that's just one way of achieving it when we enter the method we store the",
    "start": "1128960",
    "end": "1135760"
  },
  {
    "text": "current managed thread id and then once we exited the the weight",
    "start": "1135760",
    "end": "1140799"
  },
  {
    "text": "async and if we synchronously complete it we are still on the same managed thread and",
    "start": "1140799",
    "end": "1146000"
  },
  {
    "text": "if that happens we force the current threat to go back by doing in the weight tasked with yield testosterone if you",
    "start": "1146000",
    "end": "1153440"
  },
  {
    "text": "have never heard of it it's basically just a very quick way to tell the current threat hey go away",
    "start": "1153440",
    "end": "1160480"
  },
  {
    "text": "it's similar to threat yield but just for for the tpl apis",
    "start": "1160480",
    "end": "1166160"
  },
  {
    "text": "and once we have done that the continuation which is this part of the code will be executed and now we're in a",
    "start": "1166160",
    "end": "1172880"
  },
  {
    "text": "truly uh three threaded environments and we're not blocking any threat and especially",
    "start": "1172880",
    "end": "1178240"
  },
  {
    "text": "important we're no longer blocking the worker threat that is pumping messages to us",
    "start": "1178240",
    "end": "1183600"
  },
  {
    "start": "1183000",
    "end": "1218000"
  },
  {
    "text": "but now comes the next problem i told you at the beginning that from a channel interaction",
    "start": "1183600",
    "end": "1188960"
  },
  {
    "text": "perspective it is not possible to interleave calls to the broker so but since we are",
    "start": "1188960",
    "end": "1197039"
  },
  {
    "text": "here in a three-threaded environment if we want to do a basic acknowledgement",
    "start": "1197039",
    "end": "1202480"
  },
  {
    "text": "of the message what we have to do is we have to make sure that these calls here on this line",
    "start": "1202480",
    "end": "1208280"
  },
  {
    "text": "128 would never actually interleave with each other because now we have up to four things that are going on",
    "start": "1208280",
    "end": "1214320"
  },
  {
    "text": "concurrently so we need to make sure they're sequentially executed all during the free free",
    "start": "1214320",
    "end": "1220320"
  },
  {
    "start": "1218000",
    "end": "1296000"
  },
  {
    "text": "flowing free threaded environment let me show you how that works and first of all",
    "start": "1220320",
    "end": "1225600"
  },
  {
    "text": "we're going to use the pass.factory start new normally you should always be using pass.run",
    "start": "1225600",
    "end": "1231440"
  },
  {
    "text": "because that's the the api that is almost 99 of the case is the right api to use",
    "start": "1231440",
    "end": "1237520"
  },
  {
    "text": "except when you wanted to advance trickery like saving allocations or in our case",
    "start": "1237520",
    "end": "1242720"
  },
  {
    "text": "saving allocations as well as making sure we control the task scheduler that",
    "start": "1242720",
    "end": "1248000"
  },
  {
    "text": "is going to execute this method so what we do is we offload to the work thread pool why do",
    "start": "1248000",
    "end": "1254159"
  },
  {
    "text": "we offload well because the async as there is no asyncravid mqa",
    "start": "1254159",
    "end": "1259360"
  },
  {
    "text": "client api right so we're still doing synchronous i o and you should probably know that's not",
    "start": "1259360",
    "end": "1265039"
  },
  {
    "text": "the most efficient way to do i o but at least with offloading it to the worker threat to although it's costly operation",
    "start": "1265039",
    "end": "1272080"
  },
  {
    "text": "we can make sure we have a proper concurrency enabled and then what we do as well is we save",
    "start": "1272080",
    "end": "1278240"
  },
  {
    "text": "the state to save our locations here and then the most important piece we're passing in",
    "start": "1278240",
    "end": "1283520"
  },
  {
    "text": "the task scheduler from the outside and we're making sure that nobody can actually attach",
    "start": "1283520",
    "end": "1288880"
  },
  {
    "text": "as a continuation as a child test to this current task so if you're not running into",
    "start": "1288880",
    "end": "1294080"
  },
  {
    "text": "weird tasks dependencies okay but here we need to actually have",
    "start": "1294080",
    "end": "1300720"
  },
  {
    "start": "1296000",
    "end": "1371000"
  },
  {
    "text": "something that you might have heard of or maybe not i'm going to explain to you so in the net framework there is this",
    "start": "1300720",
    "end": "1307280"
  },
  {
    "text": "concurrent exclusive scheduler pair what this is it's basically a task scheduler that has two sides",
    "start": "1307280",
    "end": "1314080"
  },
  {
    "text": "one side of it is well do whatever you want to do and concurrently execute and interleave",
    "start": "1314080",
    "end": "1320480"
  },
  {
    "text": "calls and then there is the exclusive scheduler and the exclusive scheduler will make sure",
    "start": "1320480",
    "end": "1325679"
  },
  {
    "text": "that anything that is scheduled on that exclusive scheduler will always be a executed in the order",
    "start": "1325679",
    "end": "1333520"
  },
  {
    "text": "where they were or were added well order is not entirely guaranteed",
    "start": "1333520",
    "end": "1339039"
  },
  {
    "text": "there's no strict ordering in there but at least somewhat ordered and definitely not",
    "start": "1339039",
    "end": "1345600"
  },
  {
    "text": "interleaved right so and with that by by providing this uh exclusive scheduler down here to the",
    "start": "1345600",
    "end": "1352640"
  },
  {
    "text": "basic single to the receive model all the basic acts will never be executed um in interleaved",
    "start": "1352640",
    "end": "1360400"
  },
  {
    "text": "and then at the end we just release the semaphore so that we have um enough so that we can release",
    "start": "1360400",
    "end": "1367200"
  },
  {
    "text": "the semaphore slot so that we can concurrently process messages well okay let me show you how that looks",
    "start": "1367200",
    "end": "1373840"
  },
  {
    "start": "1371000",
    "end": "1417000"
  },
  {
    "text": "like let's go back let's go to the",
    "start": "1373840",
    "end": "1379440"
  },
  {
    "text": "concurrency example let me clear the screen right now so that you can better see it i'm running now this demo",
    "start": "1379440",
    "end": "1386960"
  },
  {
    "text": "and as you can see here we're getting four messages and we had to four times",
    "start": "1386960",
    "end": "1392640"
  },
  {
    "text": "the yield uh at the beginning because we're still on the on the synchronous path of the semaphore",
    "start": "1392640",
    "end": "1398159"
  },
  {
    "text": "and now with all the trickery in place we can concurrently handle up to four messages",
    "start": "1398159",
    "end": "1403360"
  },
  {
    "text": "things are still piling up on the server because we are still sending sending faster than we can more",
    "start": "1403360",
    "end": "1410720"
  },
  {
    "text": "or less receive but we have at least achieved some kind of concurrency and now that we",
    "start": "1410720",
    "end": "1418240"
  },
  {
    "start": "1417000",
    "end": "1469000"
  },
  {
    "text": "have concurrency i also want to briefly show you why we have to buffer copy so if i don't",
    "start": "1418240",
    "end": "1424000"
  },
  {
    "text": "buffer copy and if i remove here the two array where we where i force a copy of the memory",
    "start": "1424000",
    "end": "1430080"
  },
  {
    "text": "and then i'm passing the span to getstring so that's the the current",
    "start": "1430080",
    "end": "1435520"
  },
  {
    "text": "buffer what's going to happen is the following let me show you the demo again",
    "start": "1435520",
    "end": "1440799"
  },
  {
    "text": "exactly same code now i'm executing it the first ones are are working and now",
    "start": "1440799",
    "end": "1447520"
  },
  {
    "text": "we're getting hearts and smileys and everything wow this looks pretty cool in the demo right because who doesn't love to see",
    "start": "1447520",
    "end": "1454320"
  },
  {
    "text": "hearts on screen i mean in your production system that would be very terrible because you're",
    "start": "1454320",
    "end": "1460400"
  },
  {
    "text": "probably not going to send hearts around because you're probably going to send business garbage around that nobody can",
    "start": "1460400",
    "end": "1466000"
  },
  {
    "text": "receive right and you you don't want to deal with that okay um so so that was the",
    "start": "1466000",
    "end": "1473679"
  },
  {
    "start": "1469000",
    "end": "1655000"
  },
  {
    "text": "that was the demo of showing how we actually need uh to buffer copy so let me do a brief uh",
    "start": "1473679",
    "end": "1479840"
  },
  {
    "text": "recap of what we have just seen here even though rabbitmq",
    "start": "1479840",
    "end": "1484960"
  },
  {
    "text": "is is io bound so it interacts with tcp sockets and underlying mqp protocol and",
    "start": "1484960",
    "end": "1491440"
  },
  {
    "text": "everything like that it's mostly purely sync right we saw that the event handlers they returned void and in order to bring",
    "start": "1491440",
    "end": "1499120"
  },
  {
    "text": "true asynchronicity where we can actually work with tasks to make sure we're not blocking any threat",
    "start": "1499120",
    "end": "1506080"
  },
  {
    "text": "from the consumer perspective we have to go through a lot of hassle and it requires a lot of advanced",
    "start": "1506080",
    "end": "1512240"
  },
  {
    "text": "trickery to get things working so really the question at the time was well",
    "start": "1512240",
    "end": "1517440"
  },
  {
    "text": "why does someone that is working with the rabbit mq driver why do they have to deal with all this stuff",
    "start": "1517440",
    "end": "1523039"
  },
  {
    "text": "with all this crazy stuff and how could we achieve better concurrent currency and asynchronicity",
    "start": "1523039",
    "end": "1528640"
  },
  {
    "text": "to basically support the first class concurrency on the client side well we start",
    "start": "1528640",
    "end": "1535760"
  },
  {
    "text": "we started initiating and primarily i started initiating the conversation with the vmware team",
    "start": "1535760",
    "end": "1540799"
  },
  {
    "text": "at that time and well we talked to them well at that time for them it wasn't really",
    "start": "1540799",
    "end": "1545840"
  },
  {
    "text": "an option to basically go async all the way and touching all the the model methods",
    "start": "1545840",
    "end": "1553679"
  },
  {
    "text": "so i mean that's clear right it's also if you have an existing application that has um",
    "start": "1553679",
    "end": "1560000"
  },
  {
    "text": "hundreds of thousands of lines of code and you're going to your boss and saying hey a boss i'm going",
    "start": "1560000",
    "end": "1565840"
  },
  {
    "text": "into into into the dark mode for the next six months and i'm going to refactor everything and redesign",
    "start": "1565840",
    "end": "1571919"
  },
  {
    "text": "everything without delivering any business valuable codes you're probably not going to succeed in that conversation with",
    "start": "1571919",
    "end": "1578159"
  },
  {
    "text": "your boss and or i'm not sure if there's if you're still in business right if",
    "start": "1578159",
    "end": "1584240"
  },
  {
    "text": "you're the only one if you're your boss yourself and you you haven't actually delivered any meaningful business features within six",
    "start": "1584240",
    "end": "1590400"
  },
  {
    "text": "months right then we had to do the same there so we had to try to find a way how we can gradually",
    "start": "1590400",
    "end": "1595679"
  },
  {
    "text": "evolve and the decision at that time was well we start from the receiver side so the high life high level idea is that",
    "start": "1595679",
    "end": "1603360"
  },
  {
    "text": "we we need to find the best entry point into the client to introduce a",
    "start": "1603360",
    "end": "1608440"
  },
  {
    "text": "asynchronicity without breaking all the consumers that are currently using the rabbitmq client because like i said",
    "start": "1608440",
    "end": "1614960"
  },
  {
    "text": "in the abstract there are more than 30 million downloads of the nugets worldwide right",
    "start": "1614960",
    "end": "1620159"
  },
  {
    "text": "quite a few quite a few people are using this client um and once we have an asynchronous path in",
    "start": "1620159",
    "end": "1626480"
  },
  {
    "text": "there we want to be able to continuously and parallel gradually evolve the client to introduce async",
    "start": "1626480",
    "end": "1632000"
  },
  {
    "text": "support and before we did that we had to understand the synchronous path completely and let",
    "start": "1632000",
    "end": "1639440"
  },
  {
    "text": "me uh let me walk you through that through that scenario so if so the first step that i usually do",
    "start": "1639440",
    "end": "1647039"
  },
  {
    "text": "is when i go to an existing code base where i have to introduce asynchronousity is i have to fully",
    "start": "1647039",
    "end": "1652240"
  },
  {
    "text": "understand what's going on on the synchronous path so we",
    "start": "1652240",
    "end": "1658159"
  },
  {
    "start": "1655000",
    "end": "1709000"
  },
  {
    "text": "let's have a look at the consumer work service that is the consumer work service that we had at the time i'm showing you",
    "start": "1658159",
    "end": "1664320"
  },
  {
    "text": "a lot of code i'm not going to walk through everything and it's also not really important that you",
    "start": "1664320",
    "end": "1669919"
  },
  {
    "text": "understand every bit and piece that i'm showing right now um it's just as you can see here",
    "start": "1669919",
    "end": "1676320"
  },
  {
    "text": "the client has the worker service and that's the service that is responsible to pump messages towards",
    "start": "1676320",
    "end": "1683279"
  },
  {
    "text": "your code has a action delegates internally that will get a lot of closure allocation so a lot",
    "start": "1683279",
    "end": "1690640"
  },
  {
    "text": "of memory pressure chain 0 memory pressure a lot of custom scheduling on tasks scheduler for legacy",
    "start": "1690640",
    "end": "1699039"
  },
  {
    "text": "purposes and so on and so forth so what i did there was we i ran i ran a",
    "start": "1699039",
    "end": "1705600"
  },
  {
    "text": "test to actually see how does the current code behave and surprisingly what we found out",
    "start": "1705600",
    "end": "1711440"
  },
  {
    "start": "1709000",
    "end": "1825000"
  },
  {
    "text": "is that it didn't behave as you would expect so at that time i have used multiple machines to",
    "start": "1711440",
    "end": "1718320"
  },
  {
    "text": "actually test it we had like the core duo",
    "start": "1718320",
    "end": "1723840"
  },
  {
    "text": "intel processor as you can see with one consumer we're able to pipe through six six thousand nine",
    "start": "1723840",
    "end": "1729600"
  },
  {
    "text": "hundred roughly messages a second with 32 consumer we get seven thousand uh",
    "start": "1729600",
    "end": "1735279"
  },
  {
    "text": "six hundred and some some more and with the sandy bridge cpu um we got with 32 consumers we got a",
    "start": "1735279",
    "end": "1742320"
  },
  {
    "text": "nice performance improvement up to 11 000 messages a second and now comes the funny part and by the way",
    "start": "1742320",
    "end": "1749039"
  },
  {
    "text": "yes i'm that old no well in cpu cpu years i'm probably not uh not that",
    "start": "1749039",
    "end": "1756480"
  },
  {
    "text": "old because um cp is evolved quite quite quickly right but yes we were using cordu and sandy bridge and stuff",
    "start": "1756480",
    "end": "1763279"
  },
  {
    "text": "like that and the most modern that we had was the was this uh sky was the sky",
    "start": "1763279",
    "end": "1769360"
  },
  {
    "text": "league processor and there was a there was a comment in the chat about the concurrency example especially uh",
    "start": "1769360",
    "end": "1776320"
  },
  {
    "text": "i will try to i will try to deal with the question a little bit later where it nicely fits into the",
    "start": "1776320",
    "end": "1782960"
  },
  {
    "text": "conversation flow but here um as we're using as the most model was kalec and as you can see",
    "start": "1782960",
    "end": "1791039"
  },
  {
    "text": "well you just bought a precious new cpu and you're like yes i'm going to test",
    "start": "1791039",
    "end": "1796240"
  },
  {
    "text": "the revit mq client and it must be faster surely it's going to be faster right and you're fully excited you're on your",
    "start": "1796240",
    "end": "1802799"
  },
  {
    "text": "benchmarks and see like oh no it's actually not what a terrible day right we're",
    "start": "1802799",
    "end": "1808159"
  },
  {
    "text": "we're only getting 2 200 messages a second so what we can say is the more modern the cpu becomes more or less the",
    "start": "1808159",
    "end": "1815679"
  },
  {
    "text": "more gradually the the performance of the client uh went uh went down that's a pretty pretty",
    "start": "1815679",
    "end": "1823279"
  },
  {
    "text": "terrible situation to be in so the the reason why this was was this",
    "start": "1823279",
    "end": "1828320"
  },
  {
    "start": "1825000",
    "end": "1979000"
  },
  {
    "text": "uh so-called batching work pool that is used internally so the batching work pool is used by the",
    "start": "1828320",
    "end": "1834559"
  },
  {
    "text": "worker service and it uses as you can see here a global lock per batching work pool",
    "start": "1834559",
    "end": "1842080"
  },
  {
    "text": "and what we can say is what you could observe at that time if",
    "start": "1842080",
    "end": "1847200"
  },
  {
    "text": "you use the profiler of your choice you could actually observe that the client the more modern",
    "start": "1847200",
    "end": "1853200"
  },
  {
    "text": "the cpu is the more lock contentions you're getting because multiple concurrent operations are",
    "start": "1853200",
    "end": "1860240"
  },
  {
    "text": "trying to acquire the lock and reacquire the lock and will go into the monitoring and can't exit",
    "start": "1860240",
    "end": "1867039"
  },
  {
    "text": "until another one is released and multiple will try to again to acquire the lock so the lock contention was was",
    "start": "1867039",
    "end": "1873519"
  },
  {
    "text": "extremely uh extremely high so what you have to look for if you",
    "start": "1873519",
    "end": "1880480"
  },
  {
    "text": "are going into the concurrency land is first of all look at your synchronous path like we",
    "start": "1880480",
    "end": "1886159"
  },
  {
    "text": "did in in rabbit mq and try to see if they're locked in there well first of all we said yes lock",
    "start": "1886159",
    "end": "1891600"
  },
  {
    "text": "contention is bad uh well because it actually pulls down uh pulls down uh orders of magnitude in",
    "start": "1891600",
    "end": "1898880"
  },
  {
    "text": "throughput uh on on your code because multiple threats will be waiting and have to basically try to reacquire the locks but",
    "start": "1898880",
    "end": "1906080"
  },
  {
    "text": "it's even what's even worse is if we are wants to introduce into a call path",
    "start": "1906080",
    "end": "1912080"
  },
  {
    "text": "asynchronicity so the async awaits keywords and tasks we cannot really we can never do that",
    "start": "1912080",
    "end": "1918000"
  },
  {
    "text": "inside the lock statement the compiler is going to tell you you are crazy nobody does that",
    "start": "1918000",
    "end": "1923519"
  },
  {
    "text": "why is that not possible well maybe you remember from the fundamentals",
    "start": "1923519",
    "end": "1929039"
  },
  {
    "text": "of c sharp and locking the thread that actually enters the lock acquires a lock is the",
    "start": "1929039",
    "end": "1934559"
  },
  {
    "text": "threat that has to release the lock so of course when you have an upgrade statement where we are actually in a",
    "start": "1934559",
    "end": "1940640"
  },
  {
    "text": "free threaded environment it cannot be guaranteed that with your",
    "start": "1940640",
    "end": "1945679"
  },
  {
    "text": "try finally block in the final block when the when the lock is released so",
    "start": "1945679",
    "end": "1950799"
  },
  {
    "text": "basically that is on this line that is still the same threat that that acquired um the lock right so",
    "start": "1950799",
    "end": "1957120"
  },
  {
    "text": "you have to also get rid lock get rid of locks when you want to truly use async await and now i'm going to",
    "start": "1957120",
    "end": "1965679"
  },
  {
    "text": "get to the question there's one question would one use this approach to throttling when you have multiple cues",
    "start": "1965679",
    "end": "1970720"
  },
  {
    "text": "being read on the channel and trying to see when this should be ideal versus setting the prefetch count",
    "start": "1970720",
    "end": "1976000"
  },
  {
    "text": "to say four in your example okay yes so um let me quickly go back to",
    "start": "1976000",
    "end": "1982000"
  },
  {
    "start": "1979000",
    "end": "2166000"
  },
  {
    "text": "concurrency um you could definitely",
    "start": "1982000",
    "end": "1987120"
  },
  {
    "text": "use the semaphore slim to also throttle requests from multiple cues that is possible the",
    "start": "1987120",
    "end": "1992640"
  },
  {
    "text": "prefetch count i have not talked about this this is this part here what the prefetch count basically just",
    "start": "1992640",
    "end": "1998399"
  },
  {
    "text": "does it just says well go and fetch in one network call up to the number of messages from the broker",
    "start": "1998399",
    "end": "2005519"
  },
  {
    "text": "and buffer them locally and then start and then dispatch those so what we're doing with the prefetch",
    "start": "2005519",
    "end": "2011360"
  },
  {
    "text": "we're only making sure that we don't have to continuously reach out to the broker to get more messages right but that's really",
    "start": "2011360",
    "end": "2018559"
  },
  {
    "text": "more or less independent a setting that is independent from the currency",
    "start": "2018559",
    "end": "2023679"
  },
  {
    "text": "semantics that we have on the client side right the only thing the thing that we can do is we can optimize for more fetching and for less",
    "start": "2023679",
    "end": "2030960"
  },
  {
    "text": "latency in the interaction with the broker from pulling in the messages i would still use this ml4",
    "start": "2030960",
    "end": "2037360"
  },
  {
    "text": "slim uh that that controls the concurrency that i want to have in my application",
    "start": "2037360",
    "end": "2042799"
  },
  {
    "text": "code hopefully that answers that that's that question okay so what we did is well we try to",
    "start": "2042799",
    "end": "2051280"
  },
  {
    "text": "get rid get rid of these locks because even like i said when you have locks um and",
    "start": "2051280",
    "end": "2057520"
  },
  {
    "text": "lock problems not even async is going to help you at all right so we need to first optimize",
    "start": "2057520",
    "end": "2062878"
  },
  {
    "text": "the sync path so we introduced a new uh work pool and let me scroll up a",
    "start": "2062879",
    "end": "2069760"
  },
  {
    "text": "little bit i'm sorry so here we have the consumer work service that was revamped it uses a concurrent dictionary",
    "start": "2069760",
    "end": "2076000"
  },
  {
    "text": "internally to handle multiple work pools but the most important part is not this one of course we're already",
    "start": "2076000",
    "end": "2082398"
  },
  {
    "text": "getting a lot of nice benefits just by handling multiple models and worker pools but the best",
    "start": "2082399",
    "end": "2089118"
  },
  {
    "text": "thing is here in the worker pool so we're using now a concurrent queue",
    "start": "2089119",
    "end": "2094158"
  },
  {
    "text": "with action delegates still some closure allocations and auto reset events to make sure",
    "start": "2094159",
    "end": "2099440"
  },
  {
    "text": "that we're only actually trying to get stuff uh from that concurrent queue when things are actually",
    "start": "2099440",
    "end": "2105040"
  },
  {
    "text": "uh ready and then when we're starting we're starting either a custom uh threat on the background or",
    "start": "2105040",
    "end": "2111520"
  },
  {
    "text": "using the tpl with tpel we cannot name threats right so that's not possible um",
    "start": "2111520",
    "end": "2116800"
  },
  {
    "text": "we start the background threat if we if we're not using.net core so that we have a nice benefit of having me",
    "start": "2116800",
    "end": "2123440"
  },
  {
    "text": "named threats that i didn't make that choice that was at the time already there so we just made sure that still",
    "start": "2123440",
    "end": "2128880"
  },
  {
    "text": "supported and then what we do is when we enqueue a new action we just throw it into",
    "start": "2128880",
    "end": "2134079"
  },
  {
    "text": "the concurrent queue and then we set the message arrived auto reset event to a set state what",
    "start": "2134079",
    "end": "2139760"
  },
  {
    "text": "that means is the loop that is running in the background most of the time if there is nothing to consume it will",
    "start": "2139760",
    "end": "2146560"
  },
  {
    "text": "be sitting here on the wait one and it will wait until the wait time is over or someone sets the auto reset event",
    "start": "2146560",
    "end": "2153200"
  },
  {
    "text": "and then once we have something and we're not canceled we're trying to as efficiently as we can getting stuff from",
    "start": "2153200",
    "end": "2158960"
  },
  {
    "text": "the concurrent queue try to dequeue the actions and then we execute uh the uh the the actions there",
    "start": "2158960",
    "end": "2166800"
  },
  {
    "start": "2166000",
    "end": "2310000"
  },
  {
    "text": "and that's the basic gist just the first iteration of making this the sync code uh better and what we got here is um",
    "start": "2166800",
    "end": "2175119"
  },
  {
    "text": "all these really nice improvements by the way i'm showing you the the benchmark results that we did",
    "start": "2175119",
    "end": "2180880"
  },
  {
    "text": "from the high level perspective from answer response using the rabbit mq client",
    "start": "2180880",
    "end": "2186320"
  },
  {
    "text": "the newer versions of it so the actual raw numbers by just using the client might be even",
    "start": "2186320",
    "end": "2192640"
  },
  {
    "text": "different um but here when we uh under for let's focus on the right side here the receive",
    "start": "2192640",
    "end": "2200480"
  },
  {
    "text": "throughput improvements just by on our side just by using async await was 1.69",
    "start": "2200480",
    "end": "2206960"
  },
  {
    "text": "once we removed the lock contentions we got up to 6.66 times more receive",
    "start": "2206960",
    "end": "2214000"
  },
  {
    "text": "throughput just by introducing this smarter consumer work service that",
    "start": "2214000",
    "end": "2219280"
  },
  {
    "text": "doesn't have the same lock contention problems so if we go back here",
    "start": "2219280",
    "end": "2225200"
  },
  {
    "text": "right to this example where we looked at where we looked at the skylake",
    "start": "2225200",
    "end": "2230720"
  },
  {
    "text": "processor so we can actually say that uh on any cpu we're getting up to 6.6",
    "start": "2230720",
    "end": "2237599"
  },
  {
    "text": "times more six six times more more more receive throughputs and that scales very",
    "start": "2237599",
    "end": "2244079"
  },
  {
    "text": "well also with even more modern cpus so now we are at the stage where we can say at least from a synchronous path",
    "start": "2244079",
    "end": "2250079"
  },
  {
    "text": "perspective buying new cpus actually makes sense right previously it wasn't a good investment",
    "start": "2250079",
    "end": "2256560"
  },
  {
    "text": "with the revit mq client okay so the key takeaways that i want to give",
    "start": "2256560",
    "end": "2262400"
  },
  {
    "text": "you here if you can go into your synchronous path and and remove the locks that's a very",
    "start": "2262400",
    "end": "2267440"
  },
  {
    "text": "liberating procedures for your code and you get orders of magnitudes of improvement just by doing that",
    "start": "2267440",
    "end": "2274320"
  },
  {
    "text": "and we haven't even talked about basic right so now we're going into the meat now we're going into the async",
    "start": "2274320",
    "end": "2280000"
  },
  {
    "text": "stuff okay so like i said because rabbitmq had a had a large user base and they couldn't",
    "start": "2280000",
    "end": "2286560"
  },
  {
    "text": "just break everything we had to gradually change things so one way like most software problems get solved",
    "start": "2286560",
    "end": "2294400"
  },
  {
    "text": "right is to hack stuff into your existing code uh i'm sorry but that's that's how things",
    "start": "2294400",
    "end": "2300079"
  },
  {
    "text": "sometimes work right so we introduced on the ace we introduced a new interface it was called ai async connection",
    "start": "2300079",
    "end": "2306240"
  },
  {
    "text": "factory where you have dispatch consumer async and once you have that enabled uh what's",
    "start": "2306240",
    "end": "2312720"
  },
  {
    "start": "2310000",
    "end": "2383000"
  },
  {
    "text": "going to happen is the connection will basically do an internal uh type",
    "start": "2312720",
    "end": "2317920"
  },
  {
    "text": "save cost and we'll check well is the dispatch consumer async flag set to true",
    "start": "2317920",
    "end": "2324000"
  },
  {
    "text": "and once that is in there it hooks in a new async consumer work service",
    "start": "2324000",
    "end": "2329040"
  },
  {
    "text": "that is geared towards the async path only right so that is the how we introduced a",
    "start": "2329040",
    "end": "2334400"
  },
  {
    "text": "redundant async path and then um well from a client perspective what you",
    "start": "2334400",
    "end": "2340560"
  },
  {
    "text": "have to do now is you can no longer use the eventing basic consumer you basically",
    "start": "2340560",
    "end": "2346320"
  },
  {
    "text": "use the async variants so there is the async eventing basic consumer or the async default basic consumer and the most",
    "start": "2346320",
    "end": "2353200"
  },
  {
    "text": "important part here is they're using this async event handler which is now a delegate that no longer",
    "start": "2353200",
    "end": "2360160"
  },
  {
    "text": "returns a void and now returns a task right so the requirement is once you set the",
    "start": "2360160",
    "end": "2366839"
  },
  {
    "text": "flag on this uh dispatch consumer async to true you have to use",
    "start": "2366839",
    "end": "2372079"
  },
  {
    "text": "the other worker service and the client will tell you unfortunately by throwing a runtime",
    "start": "2372079",
    "end": "2377359"
  },
  {
    "text": "exception that was the trade-off we had to make in order to not break break everyone okay",
    "start": "2377359",
    "end": "2384160"
  },
  {
    "start": "2383000",
    "end": "2950000"
  },
  {
    "text": "and the consumer works service a few a bunch of improvements like for example better caching better",
    "start": "2384160",
    "end": "2391920"
  },
  {
    "text": "caching things stuff but the most important part is here so at that time we were still on",
    "start": "2391920",
    "end": "2399839"
  },
  {
    "text": "the net 4.5 framework um so we had to do a few advanced trickeries to to make",
    "start": "2399839",
    "end": "2406240"
  },
  {
    "text": "sure everything properly works so we use the concurrent queue a still",
    "start": "2406240",
    "end": "2411520"
  },
  {
    "text": "action and then we have a cancellation token to properly support cancellation",
    "start": "2411520",
    "end": "2417280"
  },
  {
    "text": "and then when we enqueue a new action what you do is we use a task completion source a task completion source",
    "start": "2417280",
    "end": "2423280"
  },
  {
    "text": "it's just a mechanism of basically have as a task where you control the state of the task",
    "start": "2423280",
    "end": "2428319"
  },
  {
    "text": "you control the outcome and because we were using the.net 4.5",
    "start": "2428319",
    "end": "2433520"
  },
  {
    "text": "we had to use this nasty trickery to basically depending whether you are",
    "start": "2433520",
    "end": "2438640"
  },
  {
    "text": "on the net framework or not uh that has a specific version we had to basically grab with reflection into",
    "start": "2438640",
    "end": "2444160"
  },
  {
    "text": "the internals of the of the of the the apis and then flip flip and internal flag to make sure",
    "start": "2444160",
    "end": "2450880"
  },
  {
    "text": "we always con asynchronously complete the task completion source why is that important",
    "start": "2450880",
    "end": "2457040"
  },
  {
    "text": "by default if you're uh below dotnet 4.6 the task completion source",
    "start": "2457040",
    "end": "2462079"
  },
  {
    "text": "will always uh the the task that is representative will always complete",
    "start": "2462079",
    "end": "2467280"
  },
  {
    "text": "on the thread that actually did the tri-set result and as you can imagine that would be",
    "start": "2467280",
    "end": "2472400"
  },
  {
    "text": "very evil because if someone if the client enqueues",
    "start": "2472400",
    "end": "2477440"
  },
  {
    "text": "a dispatch operation of a message and then it sets try set result the continuation of the loop which is",
    "start": "2477440",
    "end": "2485359"
  },
  {
    "text": "what happens when here the task completion source completes would be executed",
    "start": "2485359",
    "end": "2490640"
  },
  {
    "text": "on the threads that did that so that means the while loop that we have down here will be executed on that thread and then we will be",
    "start": "2490640",
    "end": "2496800"
  },
  {
    "text": "blocking the one that gives us the messages which might be some um underlying thread that",
    "start": "2496800",
    "end": "2503680"
  },
  {
    "text": "observes the tcp frames and everything that would be not not a good approach and then um we're",
    "start": "2503680",
    "end": "2511040"
  },
  {
    "text": "waiting here uh as in contrast to the to the synchronous version and we're not",
    "start": "2511040",
    "end": "2516319"
  },
  {
    "text": "blocking any thread right because if we because uh it's async once we're getting",
    "start": "2516319",
    "end": "2521440"
  },
  {
    "text": "out we're resetting the task completion source uh there might be races on that field but this turned out to be a non-issue",
    "start": "2521440",
    "end": "2527839"
  },
  {
    "text": "uh due to the async fencing sim fencing semantics and everything so that worked",
    "start": "2527839",
    "end": "2533200"
  },
  {
    "text": "extremely well and then once we uh once we are down here once we know something is in",
    "start": "2533200",
    "end": "2538960"
  },
  {
    "text": "there again we're trying to dequeue everything that we have and then execute uh execute",
    "start": "2538960",
    "end": "2546240"
  },
  {
    "text": "these these action [Music]",
    "start": "2546240",
    "end": "2551280"
  },
  {
    "text": "action delay delegates okay so so the key takeaways",
    "start": "2551280",
    "end": "2559280"
  },
  {
    "text": "that that you have to that you have to think about here is that and that was",
    "start": "2559280",
    "end": "2564880"
  },
  {
    "text": "the consumer work service and then the async consumer work service is exactly the same",
    "start": "2564880",
    "end": "2570560"
  },
  {
    "text": "except except that here what we're doing is we use exactly the same technique we do a tri-set result",
    "start": "2570560",
    "end": "2576880"
  },
  {
    "text": "and we await the task but now as you can see here is when we take the stuff we're no",
    "start": "2576880",
    "end": "2582960"
  },
  {
    "text": "longer getting actually action delegates we're getting something that returns a task and then we just await and execute and",
    "start": "2582960",
    "end": "2590000"
  },
  {
    "text": "then we're no longer blocking even when we're executing the async consumer",
    "start": "2590000",
    "end": "2595839"
  },
  {
    "text": "the the async consumers that are attached to this uh worker worker service and by doing so",
    "start": "2595839",
    "end": "2602960"
  },
  {
    "text": "we free up all the threats threads that are in place and we can achieve a",
    "start": "2602960",
    "end": "2608319"
  },
  {
    "text": "very high i o saturation and freeing up the threats is crucial for modern",
    "start": "2608319",
    "end": "2613720"
  },
  {
    "text": "concurrency-enabled libraries and the task-based apis they're they are the key",
    "start": "2613720",
    "end": "2618960"
  },
  {
    "text": "to achieve this high i o saturations or value tasks even if you're using",
    "start": "2618960",
    "end": "2624480"
  },
  {
    "text": "even more modern approaches and there is no really no way how you can avoid it and of course we",
    "start": "2624480",
    "end": "2632079"
  },
  {
    "text": "were quite lucky because we were able to basically bring in over this consumer",
    "start": "2632079",
    "end": "2637680"
  },
  {
    "text": "and async consumer work service a redundant code path for client could hook in so that was that was great but maybe in",
    "start": "2637680",
    "end": "2644960"
  },
  {
    "text": "your business code that might not always be uh such a clear cut right but still",
    "start": "2644960",
    "end": "2650800"
  },
  {
    "text": "if you use the same techniques that we learned so far basically by lock looking for locks starting to",
    "start": "2650800",
    "end": "2656480"
  },
  {
    "text": "optimize the sync path then trying to find an io bion path where you plug in",
    "start": "2656480",
    "end": "2661599"
  },
  {
    "text": "an async a redundant async path like we just did you might be able to get a lot of additional gains without having to",
    "start": "2661599",
    "end": "2668079"
  },
  {
    "text": "rework everything that you have in place so once uh once we we did that the next",
    "start": "2668079",
    "end": "2674400"
  },
  {
    "text": "client version was um was that we did some cleanups because we",
    "start": "2674400",
    "end": "2680720"
  },
  {
    "text": "bumped the net framework support um so we were able to just remove the",
    "start": "2680720",
    "end": "2686880"
  },
  {
    "text": "the hex the task completion source factory we did some other cleanups as well so no nothing really uh fancy but",
    "start": "2686880",
    "end": "2695359"
  },
  {
    "text": "the core of the logic um is still still the same one thing that came",
    "start": "2695359",
    "end": "2701520"
  },
  {
    "text": "up because there was a lot of time passing was uh someone from the community asked",
    "start": "2701520",
    "end": "2706640"
  },
  {
    "text": "well why are you using a task completion source here wouldn't it be much more efficient to actually have a semaphore in place",
    "start": "2706640",
    "end": "2713839"
  },
  {
    "text": "and make sure things are waiting here so basically this semantics we have a semaphore that we release when someone calls",
    "start": "2713839",
    "end": "2720240"
  },
  {
    "text": "enqueue instead of a triset result and then we do a weight semaphore weight async",
    "start": "2720240",
    "end": "2726000"
  },
  {
    "text": "here until we're cancelled and then and then iterate again well that was a",
    "start": "2726000",
    "end": "2733839"
  },
  {
    "text": "very interesting uh question and we was like okay well well if you can get rid of this",
    "start": "2733839",
    "end": "2739440"
  },
  {
    "text": "complexity with the task completion source why not right so we then did some um performance bench",
    "start": "2739440",
    "end": "2745920"
  },
  {
    "text": "benchmarks uh around this and uh surprisingly what we found out even though the semi for",
    "start": "2745920",
    "end": "2752240"
  },
  {
    "text": "slim would potentially be the more uh intention revealing design or the more",
    "start": "2752240",
    "end": "2757440"
  },
  {
    "text": "intuitive way to achieve this was that what we could see is the task completion source",
    "start": "2757440",
    "end": "2762720"
  },
  {
    "text": "is actually orders of magnitudes faster so the baseline is the task completion source",
    "start": "2762720",
    "end": "2768000"
  },
  {
    "text": "and as you can see is for example semaphore slim with a lot a lot a lot of",
    "start": "2768000",
    "end": "2773119"
  },
  {
    "text": "inserts and elements that we're processing we're getting we're up to 10 times faster",
    "start": "2773119",
    "end": "2780720"
  },
  {
    "text": "up to 10 times faster with the task completion source in comparison to december for slim and of course",
    "start": "2781839",
    "end": "2789280"
  },
  {
    "text": "here on this side if you um if you pay attention you see that chain zero",
    "start": "2789280",
    "end": "2794560"
  },
  {
    "text": "garbage is slightly higher with the task completion source approach because we have to replace",
    "start": "2794560",
    "end": "2799760"
  },
  {
    "text": "the field whenever um we are going to loop right so that is a bit more problematic",
    "start": "2799760",
    "end": "2805680"
  },
  {
    "text": "but in this specific case it was actually okay to to have a bit more chain zero garbage",
    "start": "2805680",
    "end": "2812160"
  },
  {
    "text": "because we wanted to make sure on the enqueue path where we actually where the client actually throws in work",
    "start": "2812160",
    "end": "2817680"
  },
  {
    "text": "we're just as fast as we can that was the thing that we're measuring so we're making we're making the",
    "start": "2817680",
    "end": "2823119"
  },
  {
    "text": "performance through the necessary performance training so that means the task completion source was uh was still",
    "start": "2823119",
    "end": "2829280"
  },
  {
    "text": "the most efficient way to do it and we roll back those changes so the key takeaway is",
    "start": "2829280",
    "end": "2834319"
  },
  {
    "text": "even if you're going from sync optimizing to async measure measure measure your assumptions",
    "start": "2834319",
    "end": "2840640"
  },
  {
    "text": "because your assumptions might not always be true or yours things might even be invalidated",
    "start": "2840640",
    "end": "2847599"
  },
  {
    "text": "performance things that you introduce at a certain point in time if you do not capture these performance",
    "start": "2847599",
    "end": "2853200"
  },
  {
    "text": "metrics in your code someone else or maybe even you because you forgot right you have more gray hair and they're coming back and",
    "start": "2853200",
    "end": "2859839"
  },
  {
    "text": "you don't remember all the details and now you change the the code and suddenly things are getting uh getting slower",
    "start": "2859839",
    "end": "2867359"
  },
  {
    "text": "and now with the introduction of the major version six of the client um we we did a uh more cleanup so",
    "start": "2867359",
    "end": "2875839"
  },
  {
    "text": "we were aware that there is this assisted threading channel that came with the dot net framework it's also if",
    "start": "2875839",
    "end": "2881520"
  },
  {
    "text": "you if you're supporting dot net framework previous versions and you're not running on.net core you can",
    "start": "2881520",
    "end": "2887680"
  },
  {
    "text": "actually pull in a nuget package that represents the system threading channel and we started rewriting the async",
    "start": "2887680",
    "end": "2895040"
  },
  {
    "text": "consumer work service to use the system threading channel apis",
    "start": "2895040",
    "end": "2900319"
  },
  {
    "text": "and we're able to do that right because we're doing a major version uh we're assembling compliant now you",
    "start": "2900319",
    "end": "2905760"
  },
  {
    "text": "can pull in another additional library because you know you're upgrading from a previous major to a new major version so now what we're doing is",
    "start": "2905760",
    "end": "2912559"
  },
  {
    "text": "we're trading internally and unbound the channel um that allows us to to store as many as",
    "start": "2912559",
    "end": "2918160"
  },
  {
    "text": "many things as we want without blocking there is guaranteed to be a single reader we tell it so that it",
    "start": "2918160",
    "end": "2924160"
  },
  {
    "text": "can optimize internally its data structure um there is there are multiple writers",
    "start": "2924160",
    "end": "2929520"
  },
  {
    "text": "potentially multiple concurrent writers and this is also a crucial piece we need to tell it",
    "start": "2929520",
    "end": "2934640"
  },
  {
    "text": "we we cannot allow synchronous continuation and the example is the same we cannot allow that",
    "start": "2934640",
    "end": "2941200"
  },
  {
    "text": "the one that enqueues something to the system threading channel is also the threat that actually is going to",
    "start": "2941200",
    "end": "2946400"
  },
  {
    "text": "execute stuff because that would be not good for the performance and then when we enqueue we just do",
    "start": "2946400",
    "end": "2952240"
  },
  {
    "start": "2950000",
    "end": "3230000"
  },
  {
    "text": "use the channel the writer try right and the loop has become tremendously uh tremendously simple we're just",
    "start": "2952240",
    "end": "2959200"
  },
  {
    "text": "waiting here until this the channel tells us hey there is actually something to read",
    "start": "2959200",
    "end": "2964319"
  },
  {
    "text": "and once we have it we read as much as we can as before and then we did some further",
    "start": "2964319",
    "end": "2969680"
  },
  {
    "text": "optimizations so we execute the worker uh the worker and",
    "start": "2969680",
    "end": "2975119"
  },
  {
    "text": "many client applications started using the async the async consumer the asic eventing",
    "start": "2975119",
    "end": "2982079"
  },
  {
    "text": "basic consumer but sometimes returns task completed and in such a case if the task is",
    "start": "2982079",
    "end": "2988160"
  },
  {
    "text": "already completed we can actually optimize by checking if it's completed and then we don't need to go into the a",
    "start": "2988160",
    "end": "2995200"
  },
  {
    "text": "bit more expensive allocation heavy path where we do where we use the async state machine",
    "start": "2995200",
    "end": "3000640"
  },
  {
    "text": "and that's a further optimization of course every time because there could be something happening here",
    "start": "3000640",
    "end": "3005760"
  },
  {
    "text": "even if we synchronously complete it we then need to do get away to get result to actually materialize any exceptions",
    "start": "3005760",
    "end": "3012559"
  },
  {
    "text": "that could have happened so that we are still um compliant with the previous behavior",
    "start": "3012559",
    "end": "3017760"
  },
  {
    "text": "um of of the code and then we we we asked us ourselves an",
    "start": "3017760",
    "end": "3023839"
  },
  {
    "text": "interesting question so well now we introduce this the threading channel how does this from a performance",
    "start": "3023839",
    "end": "3029200"
  },
  {
    "text": "perspective behave towards the task completion source and i was at that time i was like",
    "start": "3029200",
    "end": "3034319"
  },
  {
    "text": "yeah sure a library for microsoft it must be faster right did again i told you measure",
    "start": "3034319",
    "end": "3041520"
  },
  {
    "text": "measure measure right because your assumptions might not be true so it did that ran through a benchmarking",
    "start": "3041520",
    "end": "3049599"
  },
  {
    "text": "using benchmark.net and what's interesting as you can see here is we are actually the task completion",
    "start": "3049599",
    "end": "3056079"
  },
  {
    "text": "source is um faster up to 2.8 times",
    "start": "3056079",
    "end": "3061119"
  },
  {
    "text": "faster than system threading channel but i have to say bot first of all for",
    "start": "3061119",
    "end": "3068000"
  },
  {
    "text": "every benchmark result that you're seeing you also have to understand the context in which the benchmark was done",
    "start": "3068000",
    "end": "3073599"
  },
  {
    "text": "and in this context we only i only checked the enqueue path so on the enqueue path i'm actually faster using the task",
    "start": "3073599",
    "end": "3080240"
  },
  {
    "text": "completion source but then on the other hand on the if you would basically",
    "start": "3080240",
    "end": "3085359"
  },
  {
    "text": "check everything from from top to bottom from enqueue also from receive um uh i am not showing you these results",
    "start": "3085359",
    "end": "3092640"
  },
  {
    "text": "but then system threading channel is is much faster faster and what's also important",
    "start": "3092640",
    "end": "3098960"
  },
  {
    "text": "as you can see here now things are getting quite interesting because by using",
    "start": "3098960",
    "end": "3104480"
  },
  {
    "text": "system threading channel but we might be slightly slower but not very terribly slower on the enq",
    "start": "3104480",
    "end": "3111440"
  },
  {
    "text": "path we're actually less allocation heavy because we're not using task completion source and system threading channel",
    "start": "3111440",
    "end": "3117359"
  },
  {
    "text": "doesn't produce any gen zero garbage garbage anymore so that's very nice right so so re yes it's not just because",
    "start": "3117359",
    "end": "3125599"
  },
  {
    "text": "a performance benchmark told you something is is lower than something else it doesn't mean you have to",
    "start": "3125599",
    "end": "3131359"
  },
  {
    "text": "backtrack and throw out everything in your code right you have to understand the context you're operating in",
    "start": "3131359",
    "end": "3136480"
  },
  {
    "text": "you have to compare and contrast the things and then make the best decision and on top of that what's also really",
    "start": "3136480",
    "end": "3142319"
  },
  {
    "text": "crucial is i think any code that you can replace that any custom code that you wrote or",
    "start": "3142319",
    "end": "3149760"
  },
  {
    "text": "let's say i wrote i know that i'm a terrible developer and i make a lot of mistakes right so",
    "start": "3149760",
    "end": "3154960"
  },
  {
    "text": "any custom code that i can replace divert that i can replace with a base class library that",
    "start": "3154960",
    "end": "3160720"
  },
  {
    "text": "comes from microsoft that is going to be battle tested is going to be built developed by",
    "start": "3160720",
    "end": "3166559"
  },
  {
    "text": "a lot of runtime engineers this battle test by thousand thousand people worldwide",
    "start": "3166559",
    "end": "3171760"
  },
  {
    "text": "is worth replacing by something that come from microsoft right so not always the fastest solution",
    "start": "3171760",
    "end": "3178880"
  },
  {
    "text": "is the best solutions all factors have to have to be taken into account and in the",
    "start": "3178880",
    "end": "3184720"
  },
  {
    "text": "latest iteration um of of uh of the of the client which is the 6.2",
    "start": "3184720",
    "end": "3191920"
  },
  {
    "text": "version it's the current released version i introduced the notion of concurrency so what you can do now",
    "start": "3191920",
    "end": "3198160"
  },
  {
    "text": "is scrolling up quickly excuse me um you can pass in to uh to the library um",
    "start": "3198160",
    "end": "3204880"
  },
  {
    "text": "you can say hey i want the concurrency of four and what it does then internally it then basically depending",
    "start": "3204880",
    "end": "3211440"
  },
  {
    "text": "on the concurrency level it either spins spins up a loop that just does a single execution sequential",
    "start": "3211440",
    "end": "3218400"
  },
  {
    "text": "execution or it spins up a dedicated currency loop that then operates in a different way",
    "start": "3218400",
    "end": "3225200"
  },
  {
    "text": "and makes different assumptions about how your code is going to behave let's have a quick look how it looks",
    "start": "3225200",
    "end": "3231440"
  },
  {
    "start": "3230000",
    "end": "3400000"
  },
  {
    "text": "like this is the concurrency loop everything else by the way i'm not showing right now because it's exactly the same so again we're using",
    "start": "3231440",
    "end": "3238960"
  },
  {
    "text": "system threading channel we're still on there and then we do a try read then we just check whether we have uh quickly",
    "start": "3238960",
    "end": "3246079"
  },
  {
    "text": "synchronously check in order to not allocate the same machine if the semaphore",
    "start": "3246079",
    "end": "3251200"
  },
  {
    "text": "still has slots available if not we then wait here and then we just kick off the handle",
    "start": "3251200",
    "end": "3256720"
  },
  {
    "text": "concurrent and what handle current does exactly the same as it did before",
    "start": "3256720",
    "end": "3262000"
  },
  {
    "text": "but now it's basically offloaded right so we execute we do the same checks we materialize the exceptions",
    "start": "3262000",
    "end": "3268559"
  },
  {
    "text": "and the most important piece of the puzzle is um we do then limiter uh release so we",
    "start": "3268559",
    "end": "3275040"
  },
  {
    "text": "make sure that the acquired semaphore semaphore slots are then released again",
    "start": "3275040",
    "end": "3282319"
  },
  {
    "text": "and now how does this look like if you use the the newest incarnation of the rabbitmq",
    "start": "3282319",
    "end": "3288160"
  },
  {
    "text": "client well super cool i think you can now say dispatch consumers async you set that to",
    "start": "3288160",
    "end": "3295440"
  },
  {
    "text": "true you say consumer dispatch concurrency you set it to the number uh you want right and then you start",
    "start": "3295440",
    "end": "3303119"
  },
  {
    "text": "start going you have this asic eventing basic consumer that i've already showed you and now you just attach the delegate",
    "start": "3303119",
    "end": "3311680"
  },
  {
    "text": "that can return a task you do proper async awaits no advanced trickery",
    "start": "3311680",
    "end": "3317520"
  },
  {
    "text": "nothing and you're getting dispatch concurrently this method and as you can see here on",
    "start": "3317520",
    "end": "3323200"
  },
  {
    "text": "top of that we have done further optimizations in the client so there is no longer a need for passing",
    "start": "3323200",
    "end": "3330000"
  },
  {
    "text": "an exclusive concurrent scheduler pair because underneath because we already have a dependency on system threading",
    "start": "3330000",
    "end": "3336160"
  },
  {
    "text": "channels for the protocol interactions the client also uses system threading channels to",
    "start": "3336160",
    "end": "3342079"
  },
  {
    "text": "making sure that there is never an interleave happening on the underlying protocol",
    "start": "3342079",
    "end": "3347119"
  },
  {
    "text": "level so we can get rid of that we still if we want to offload we still need to use",
    "start": "3347119",
    "end": "3352559"
  },
  {
    "text": "task factory start new because we are not yet fully async enabled we are still doing a synchronous io",
    "start": "3352559",
    "end": "3358799"
  },
  {
    "text": "on some of the model interaction methods unfortunately but it's already much much better and",
    "start": "3358799",
    "end": "3365760"
  },
  {
    "text": "let's do a quick demo here",
    "start": "3365760",
    "end": "3370079"
  },
  {
    "text": "don't hit run and now as you can see we're running we're running and we're",
    "start": "3372400",
    "end": "3378640"
  },
  {
    "text": "concurrently fetching things are not piling up and as you probably saw uh because",
    "start": "3378640",
    "end": "3385200"
  },
  {
    "text": "the we can directly use this fan here there is no more buffer copying anymore",
    "start": "3385200",
    "end": "3390319"
  },
  {
    "text": "so we're like concurrent highly efficient and we don't need to waste memory",
    "start": "3390319",
    "end": "3397200"
  },
  {
    "text": "anymore and the client is also much faster what does the future hold well the future will hold that",
    "start": "3397200",
    "end": "3403359"
  },
  {
    "start": "3400000",
    "end": "3593000"
  },
  {
    "text": "with the version seven and eight we will get all asynchronous operations on the model",
    "start": "3403359",
    "end": "3409839"
  },
  {
    "text": "so it will be truly uh top to bottom async all the way and there will be more and more",
    "start": "3409839",
    "end": "3415520"
  },
  {
    "text": "allocation reductions on the client to make this an extremely fast",
    "start": "3415520",
    "end": "3420559"
  },
  {
    "text": "net client and let me do a brief recap of what i've just told you",
    "start": "3420559",
    "end": "3428880"
  },
  {
    "text": "so what you can learn here from this talk is that if you go into your code and find the",
    "start": "3428880",
    "end": "3434319"
  },
  {
    "text": "outbound paths in your application code your business code in your libraries that you maintain and if you know",
    "start": "3434319",
    "end": "3440480"
  },
  {
    "text": "where io is going through and you're doing synchronous io make sure that you analyze this code for",
    "start": "3440480",
    "end": "3446799"
  },
  {
    "text": "lock contentions and making sure that you're getting rid of lock that's the first preparation step",
    "start": "3446799",
    "end": "3451920"
  },
  {
    "text": "you want to do and then if possible in order to not break everything in order to not",
    "start": "3451920",
    "end": "3457440"
  },
  {
    "text": "have months of work for not much added value introduce a redundant code path that",
    "start": "3457440",
    "end": "3462480"
  },
  {
    "text": "goes async all the way and then from there you can start gradually extending uh",
    "start": "3462480",
    "end": "3468240"
  },
  {
    "text": "asynchronicity and concurrency on the i o bomb paths important never block threats right",
    "start": "3468240",
    "end": "3473599"
  },
  {
    "text": "blocking threats is evil and it's uh not modern concurrency practices and",
    "start": "3473599",
    "end": "3478960"
  },
  {
    "text": "then only custom offloads were really necessary like we saw because revit mq client interaction is",
    "start": "3478960",
    "end": "3485440"
  },
  {
    "text": "still doing synchronous io we still have to little bit of offload of course we want to avoid this but this will come in the version",
    "start": "3485440",
    "end": "3491359"
  },
  {
    "text": "seven and or eight of the clients and then if you're on.net five you can also start thinking about",
    "start": "3491359",
    "end": "3497520"
  },
  {
    "text": "returning to the valid task instead of task to actually benefit from buffer pulling from the value task pool sorry value",
    "start": "3497520",
    "end": "3504160"
  },
  {
    "text": "task pulling underneath and of course whenever you can the code that you don't write yourself",
    "start": "3504160",
    "end": "3509680"
  },
  {
    "text": "although you're probably much better developers than i am um i encourage you to replace the things",
    "start": "3509680",
    "end": "3515440"
  },
  {
    "text": "that you have in place when you can with reliable battle tested bcl components so like we",
    "start": "3515440",
    "end": "3521200"
  },
  {
    "text": "did with system threading channels and of course as a last message of this talk if you can",
    "start": "3521200",
    "end": "3526960"
  },
  {
    "text": "contribute back to open source that you use in your company and because then you can like me",
    "start": "3526960",
    "end": "3533520"
  },
  {
    "text": "and some of my other community colleagues you can make the libraries that you use better and better and those benefits will come back to you",
    "start": "3533520",
    "end": "3541359"
  },
  {
    "text": "thank you very much for attending this talk and listening to me if you want to see",
    "start": "3541359",
    "end": "3547040"
  },
  {
    "text": "the slides the demos the code and everything it's on github.com in motorbox asyncrabidmq",
    "start": "3547040",
    "end": "3554559"
  },
  {
    "text": "and if you're interested to see how answer response uses rabbitmq go to",
    "start": "3554559",
    "end": "3560520"
  },
  {
    "text": "docs.particular.net transports rabbitmq and yeah if you have any more questions",
    "start": "3560520",
    "end": "3568720"
  },
  {
    "text": "uh i think i can potentially take one one question here but i will also share a webex link",
    "start": "3568720",
    "end": "3575920"
  },
  {
    "text": "in the slack channel track one slack channel where you can hop on a call with me and discuss",
    "start": "3575920",
    "end": "3581280"
  },
  {
    "text": "whatever you have on your mind",
    "start": "3581280",
    "end": "3587839"
  },
  {
    "text": "you",
    "start": "3593440",
    "end": "3595520"
  }
]