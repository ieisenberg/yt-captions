[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "so let's get started we are on day one right after lunch carb load has hit us",
    "start": "9780",
    "end": "15369"
  },
  {
    "text": "and my general rule is no more than half the audience is allowed to be asleep at one point in time so if you feel",
    "start": "15369",
    "end": "21789"
  },
  {
    "text": "yourself nodding off just kind of nudge the person on the right again so we're gonna talk about data cleansing using",
    "start": "21789",
    "end": "27279"
  },
  {
    "text": "both sequel server and our my name is Kevin feasel I'm a manager of a predictive analytics team in Durham",
    "start": "27279",
    "end": "32558"
  },
  {
    "start": "30000",
    "end": "53000"
  },
  {
    "text": "North Carolina also I'm a data platform in VP and I have a blog curated sequel the idea behind curated sequel is that I",
    "start": "32559",
    "end": "39520"
  },
  {
    "text": "want to try to find and link to five to ten interesting blog posts per day so interesting here is database development",
    "start": "39520",
    "end": "45100"
  },
  {
    "text": "data based Administration could be security power bi our Hadoop this broad data platform space curated",
    "start": "45100",
    "end": "51910"
  },
  {
    "text": "SQL comm the general concept that I want to hit today is what is dirty data how",
    "start": "51910",
    "end": "59530"
  },
  {
    "start": "53000",
    "end": "73000"
  },
  {
    "text": "can we go about trying to fix some of it there are a lot of different definitions of what dirty data is different concepts",
    "start": "59530",
    "end": "67030"
  },
  {
    "text": "in this space and I want to take a couple of minutes and talk about them in some procession before I get into that",
    "start": "67030",
    "end": "75219"
  },
  {
    "start": "73000",
    "end": "148000"
  },
  {
    "text": "detail let's talk about my general philosophy clean the data as soon as",
    "start": "75219",
    "end": "80319"
  },
  {
    "text": "possible I mean this sounds kind of anodyne this sounds like well obviously this is the case but we have data come",
    "start": "80319",
    "end": "86920"
  },
  {
    "text": "in and it goes into a transactional system maybe or comes into some source of record and then if it's bad in there",
    "start": "86920",
    "end": "95139"
  },
  {
    "text": "it's probably gonna be bad downstream as well so maybe we can try to fix it in a warehouse it's like the movie equivalent",
    "start": "95139",
    "end": "101979"
  },
  {
    "text": "of we'll fix it in post production sometimes that works sometimes though you don't exactly know what the correct",
    "start": "101979",
    "end": "108159"
  },
  {
    "text": "answer is the correct answer might have been known at the beginning but we lost",
    "start": "108159",
    "end": "113229"
  },
  {
    "text": "that ability when we accepted the bad data occasionally we're able to clean",
    "start": "113229",
    "end": "118359"
  },
  {
    "text": "things up down at the end when we're generating reports when we're doing analytics that's pretty interesting and",
    "start": "118359",
    "end": "125619"
  },
  {
    "text": "I'll show you some tools to help you out in this regard but as a general principle this is not the best place to",
    "start": "125619",
    "end": "131110"
  },
  {
    "text": "do it because my analysis will be different from your analysis because the way that I cleanse the data will be a",
    "start": "131110",
    "end": "137650"
  },
  {
    "text": "little different from the way that you do unless we follow exactly the same rules we may get different results when",
    "start": "137650",
    "end": "143140"
  },
  {
    "text": "to do the same thing so try to fix it as soon as possible and as mentioned we're",
    "start": "143140",
    "end": "150400"
  },
  {
    "start": "148000",
    "end": "172000"
  },
  {
    "text": "going to be looking at different techniques across sequel server and are there are plenty of tools and other",
    "start": "150400",
    "end": "156610"
  },
  {
    "text": "techniques that I wish I could get into but we're not going to have the time for that and that includes things like data",
    "start": "156610",
    "end": "161890"
  },
  {
    "text": "provenance tools data quality services is a mediocre to bad attempt at a",
    "start": "161890",
    "end": "167020"
  },
  {
    "text": "solution but it exists there are much better solutions available as well so",
    "start": "167020",
    "end": "172590"
  },
  {
    "start": "172000",
    "end": "188000"
  },
  {
    "text": "we'll talk first about high-level concepts and then hit some sequel server stuff item number three I'm going to",
    "start": "172590",
    "end": "179739"
  },
  {
    "text": "leave as a bonus for you because bonus means I won't have time to cover it because I want to cover are in some",
    "start": "179739",
    "end": "187030"
  },
  {
    "text": "detail so let's start talking high-level at the basic concept of dirty data think",
    "start": "187030",
    "end": "196209"
  },
  {
    "start": "188000",
    "end": "238000"
  },
  {
    "text": "about you have a title and that could be a mr. or mrs. MS miss dr. Reverend",
    "start": "196209",
    "end": "203170"
  },
  {
    "text": "captain whatever your titles may be now suppose that is a free-form text field",
    "start": "203170",
    "end": "208480"
  },
  {
    "text": "so anybody can type something in so you may have Mr dot you may have Mis ter you",
    "start": "208480",
    "end": "214329"
  },
  {
    "text": "may have mr without the dot you may have mer furrow which is my attempt at trying to type mr on my cell phone and all of",
    "start": "214329",
    "end": "221560"
  },
  {
    "text": "those things are supposed to represent the same concept but when we pull it in and try to group by that title we're",
    "start": "221560",
    "end": "229840"
  },
  {
    "text": "going to get a whole bunch of different answers so that's one example of dirty",
    "start": "229840",
    "end": "235720"
  },
  {
    "text": "data this is a data consistency problem we have other problems like is your data",
    "start": "235720",
    "end": "242650"
  },
  {
    "start": "238000",
    "end": "254000"
  },
  {
    "text": "physically or logically impossible if you have a six-year-old with a driver's",
    "start": "242650",
    "end": "247690"
  },
  {
    "text": "license then that's a very interesting place to live in probably that's just",
    "start": "247690",
    "end": "253000"
  },
  {
    "text": "bad data do you have data that you expect to be complete but isn't maybe",
    "start": "253000",
    "end": "259510"
  },
  {
    "start": "254000",
    "end": "311000"
  },
  {
    "text": "you have a lot of nulls one example of this might be that I have a sales a suit",
    "start": "259510",
    "end": "265000"
  },
  {
    "text": "a group of sales associates they have you fill out a form and well you as a consumer say I just want to consume this",
    "start": "265000",
    "end": "272080"
  },
  {
    "text": "thing I don't want to spend the rest of my life filling out a fifteen page document telling you everything from my blood type twos and the name and",
    "start": "272080",
    "end": "278440"
  },
  {
    "text": "numbers of my dogs so I'm gonna leave a lot of this stuff blank so then the",
    "start": "278440",
    "end": "283960"
  },
  {
    "text": "sales associate dutifully enters nothing into the form and then you try to analyze and you have a bunch of empty",
    "start": "283960",
    "end": "290199"
  },
  {
    "text": "results so the question would be do we have some sort of reasonable coding that",
    "start": "290199",
    "end": "295360"
  },
  {
    "text": "we can do for a blank value or do we have to throw these rows away can we do",
    "start": "295360",
    "end": "300400"
  },
  {
    "text": "something with this and are these fields even vital for analysis if they're not why are you asking me what my blood type",
    "start": "300400",
    "end": "307270"
  },
  {
    "text": "is if you're not gonna do anything with it the answer to sell it to somebody else so data accuracy I wish that I",
    "start": "307270",
    "end": "316120"
  },
  {
    "start": "311000",
    "end": "367000"
  },
  {
    "text": "could make five hundred million dollars an hour spoilers I don't I don't know of anybody who does that if I see it on a",
    "start": "316120",
    "end": "323169"
  },
  {
    "text": "form is an absurd answer it is patently wrong what is the correct answer I don't know maybe it's $500 an hour maybe it's",
    "start": "323169",
    "end": "330550"
  },
  {
    "text": "$50 an hour maybe it's 500 million Zimbabwe dollars that doesn't sound",
    "start": "330550",
    "end": "335889"
  },
  {
    "text": "nearly as nice as 500 million Australian or US dollars but yeah this comes down",
    "start": "335889",
    "end": "342639"
  },
  {
    "text": "to what do we do when something is just completely wrong or a little bit less",
    "start": "342639",
    "end": "350530"
  },
  {
    "text": "absurd of example we have two source systems and those two source systems should agree but sometimes source system",
    "start": "350530",
    "end": "358090"
  },
  {
    "text": "one says the value is 14 source system 2 says that it's 18 how do we determine",
    "start": "358090",
    "end": "363460"
  },
  {
    "text": "which of those two values if either is actually correct how do I figure out if",
    "start": "363460",
    "end": "370330"
  },
  {
    "start": "367000",
    "end": "398000"
  },
  {
    "text": "I have duplicate results let's say we have a form somebody hit the submit button and got really impatient and hit",
    "start": "370330",
    "end": "378130"
  },
  {
    "text": "it five more times so now we have six rows in there or alternatively maybe we",
    "start": "378130",
    "end": "383710"
  },
  {
    "text": "had six people enter the exact same data and hit the submit button and we have no way of differentiating so is this truly",
    "start": "383710",
    "end": "390729"
  },
  {
    "text": "duplicate data is there some absolute uniqueness and if so you know are we",
    "start": "390729",
    "end": "396880"
  },
  {
    "text": "taking advantage of that this leads to some rules of thumb if you've got",
    "start": "396880",
    "end": "402909"
  },
  {
    "start": "398000",
    "end": "489000"
  },
  {
    "text": "impossible measures throw them away this is obviously bad data if you have",
    "start": "402909",
    "end": "407969"
  },
  {
    "text": "data that's missing you know all values I keep it because it's",
    "start": "407969",
    "end": "413830"
  },
  {
    "text": "missing but that doesn't mean it's bad and also may be interesting to see only 2% of people are filling out their blood",
    "start": "413830",
    "end": "420130"
  },
  {
    "text": "type what's up with those 2% who actually are if you have data that you know is fixable like those various forms",
    "start": "420130",
    "end": "428770"
  },
  {
    "text": "of mr. probably should fix those because you know what the answer was you know what the intended answer was",
    "start": "428770",
    "end": "434920"
  },
  {
    "text": "and just because somebody didn't get quite to that intended answer doesn't mean that our analyses down the stream",
    "start": "434920",
    "end": "441370"
  },
  {
    "text": "should fail as a result but that five hundred million dollars an hour that's a",
    "start": "441370",
    "end": "447250"
  },
  {
    "text": "tougher call I don't think you can just say well let's change it to some other value because you don't know what that",
    "start": "447250",
    "end": "453820"
  },
  {
    "text": "other value is if you absolutely have to change it typically a safe thing to do",
    "start": "453820",
    "end": "458830"
  },
  {
    "text": "during a data analysis is I'll set it to the median of all values in the set that way I don't affect the aggregates but if",
    "start": "458830",
    "end": "467680"
  },
  {
    "text": "you have a bunch of values you're doing this to this can lead to distortions of its own so you may end up dropping those",
    "start": "467680",
    "end": "475240"
  },
  {
    "text": "records from the analysis so those are principles I'd like you to keep in mind",
    "start": "475240",
    "end": "480580"
  },
  {
    "text": "through through this talk and we'll look at ways to try to clean up some of these",
    "start": "480580",
    "end": "485860"
  },
  {
    "text": "problems using both sequel server and are the biggest most interesting way to",
    "start": "485860",
    "end": "491920"
  },
  {
    "start": "489000",
    "end": "530000"
  },
  {
    "text": "do it in sequel server is something that probably you're already doing and if so bully for you",
    "start": "491920",
    "end": "497080"
  },
  {
    "text": "keys constraints ways of limiting the data that can get into a system",
    "start": "497080",
    "end": "504720"
  },
  {
    "text": "normalization is the first constraint it's a logical constraints not a physical one and there are several ways",
    "start": "504720",
    "end": "512380"
  },
  {
    "text": "to normalize a set of tables to get to an answer but this is probably the first",
    "start": "512380",
    "end": "519070"
  },
  {
    "text": "gate in order to protect bad data from getting through then we talk about data types and the",
    "start": "519070",
    "end": "526450"
  },
  {
    "text": "actual physical constraints that exist in a relational database system so with",
    "start": "526450",
    "end": "532810"
  },
  {
    "start": "530000",
    "end": "589000"
  },
  {
    "text": "normalization when in doubt go up with Boyce Codd boyce codd is a general form",
    "start": "532810",
    "end": "539050"
  },
  {
    "text": "of third normal form there are higher forms than third and I love me some fourth and normal form but that's not always",
    "start": "539050",
    "end": "548139"
  },
  {
    "text": "important for most applications it pains me I'm having to force myself to say",
    "start": "548139",
    "end": "553569"
  },
  {
    "text": "these words because I am such a normalization aficionado I love the I love the concept so much and yet I could",
    "start": "553569",
    "end": "559689"
  },
  {
    "text": "say it's not always that important to go up to fifth normal form that was",
    "start": "559689",
    "end": "567579"
  },
  {
    "text": "actually painful for me but I have a link in case you're interested in how to find a really cool way of deriving Boyce",
    "start": "567579",
    "end": "574720"
  },
  {
    "text": "Codd normal form from a gigantic table so you have all of your attributes together and then this will get you out",
    "start": "574720",
    "end": "581139"
  },
  {
    "text": "into the set of a set of tables that will work fine for Boyce Codd it's a",
    "start": "581139",
    "end": "586149"
  },
  {
    "text": "really interesting solution so I'll show you the link at the end and also give",
    "start": "586149",
    "end": "592629"
  },
  {
    "start": "589000",
    "end": "658000"
  },
  {
    "text": "you a quick example of it when we get into the demo as far as data types go",
    "start": "592629",
    "end": "597689"
  },
  {
    "text": "smallest data type you possibly can use tightest data type you possibly can use so in other words this is numeric values",
    "start": "597689",
    "end": "604989"
  },
  {
    "text": "must be greater than zero and let's say between 0 and 100 is our domain well I",
    "start": "604989",
    "end": "611619"
  },
  {
    "text": "probably don't want to use of our char max because that's gonna let anybody type in whatever they want and there",
    "start": "611619",
    "end": "618099"
  },
  {
    "text": "will be some troll who will come in and type in the word 70 instead of the number 7-0",
    "start": "618099",
    "end": "623289"
  },
  {
    "text": "I'm usually that troll something to think about",
    "start": "623289",
    "end": "628349"
  },
  {
    "text": "Titus domain you don't necessarily need an 8-byte big int for a 0 to 100 value",
    "start": "628349",
    "end": "634809"
  },
  {
    "text": "you could use a one-bite tinyint that has some benefits downstream for the",
    "start": "634809",
    "end": "640059"
  },
  {
    "text": "database administrator who won't be chewing you out quite as much about why are you using up all of my disk space I",
    "start": "640059",
    "end": "645399"
  },
  {
    "text": "can't just don't grow on trees you know I have to go buy these things thankfully",
    "start": "645399",
    "end": "650679"
  },
  {
    "text": "I'm no longer that crotchety database administrator I'm still crotchety I'm just no longer a database administrator",
    "start": "650679",
    "end": "656699"
  },
  {
    "text": "so smallest data type best data type when it comes to constraints primary key",
    "start": "656699",
    "end": "662529"
  },
  {
    "start": "658000",
    "end": "771000"
  },
  {
    "text": "constraints tell you this is a unique row this is the thing that tells me that",
    "start": "662529",
    "end": "667839"
  },
  {
    "text": "this is a new row a neat key constraint is another way of saying this is a",
    "start": "667839",
    "end": "673509"
  },
  {
    "text": "unique row there are some minor differences in how relational systems implement primary vs.",
    "start": "673509",
    "end": "679600"
  },
  {
    "text": "unique key constraints let's gloss over that stuff basically every one of the",
    "start": "679600",
    "end": "686050"
  },
  {
    "text": "sets of columns that define a unique row should get their own key constraint pick",
    "start": "686050",
    "end": "692079"
  },
  {
    "text": "one for the primary all the rest will get uniques we use foreign keys to tie",
    "start": "692079",
    "end": "697120"
  },
  {
    "text": "together different tables that way if you take over a data model that has 500 tables and has 0 foreign keys you go",
    "start": "697120",
    "end": "704860"
  },
  {
    "text": "insane because there's a very difficult way of trying to reconcile how these",
    "start": "704860",
    "end": "710019"
  },
  {
    "text": "things actually relate foreign key constraints let us get that easy reconciliation but they also provide",
    "start": "710019",
    "end": "716769"
  },
  {
    "text": "data cleansing in the sense that if I have a lookup value mr. mrs. MS miss I",
    "start": "716769",
    "end": "723209"
  },
  {
    "text": "can choose that lookup value and I can ensure that this value exists in my",
    "start": "723209",
    "end": "728920"
  },
  {
    "text": "table so there's no way that I can get an invalid value in my table the foreign",
    "start": "728920",
    "end": "734439"
  },
  {
    "text": "key constraint will actively prohibit it check in default constraints are ways of tightening down our domain even further",
    "start": "734439",
    "end": "741160"
  },
  {
    "text": "so a check constraint might say well you can only go from 0 to 100 a tinyint has",
    "start": "741160",
    "end": "748120"
  },
  {
    "text": "a domain of 0 to 255 but I can put in a check constraint that says that if you're over 100 then this is not valid",
    "start": "748120",
    "end": "755560"
  },
  {
    "text": "this is an invalid element default constraints say I don't need to accept a",
    "start": "755560",
    "end": "762069"
  },
  {
    "text": "null I can accept some value that will default to maybe a true or false maybe it could default to some specific number",
    "start": "762069",
    "end": "769360"
  },
  {
    "text": "or some specific text all right let's talk demo so I have a database in here",
    "start": "769360",
    "end": "775959"
  },
  {
    "start": "771000",
    "end": "1273000"
  },
  {
    "text": "that is called Raleigh crime this is a set of crime data for Raleigh Durham or",
    "start": "775959",
    "end": "782110"
  },
  {
    "text": "excuse me for the City of Raleigh in North Carolina from the Year 2005 through 2014 and I have this database",
    "start": "782110",
    "end": "790720"
  },
  {
    "text": "already created and I've got a table loaded called police so this police",
    "start": "790720",
    "end": "797439"
  },
  {
    "text": "table came from a CSV data set that had",
    "start": "797439",
    "end": "802449"
  },
  {
    "text": "columns that were really helpful II named like LCR and Inc know so we're",
    "start": "802449",
    "end": "808750"
  },
  {
    "text": "gonna work on that a little bit we're going to normalize these tables we're going to create some constraints",
    "start": "808750",
    "end": "816310"
  },
  {
    "text": "we're going to give slightly better names because I like naming things and we're going to try to make this overall",
    "start": "816310",
    "end": "823000"
  },
  {
    "text": "just a better data set so let's top 100 from DB up police and we'll see a quick",
    "start": "823000",
    "end": "834160"
  },
  {
    "text": "idea of what this data set looks like LCR and LCR desk it turns out this is",
    "start": "834160",
    "end": "841810"
  },
  {
    "text": "the incident code and this is a description of that incident code now in",
    "start": "841810",
    "end": "848139"
  },
  {
    "text": "this data set every time you see this particular incident code you are always",
    "start": "848139",
    "end": "853329"
  },
  {
    "text": "going to see this particular incident description this is what is known as a functional dependency so my description",
    "start": "853329",
    "end": "861310"
  },
  {
    "text": "is functionally dependent upon a code these functional dependencies define how",
    "start": "861310",
    "end": "868810"
  },
  {
    "text": "we normalize tables using Boyce Codd normal form because what it says is if I",
    "start": "868810",
    "end": "874420"
  },
  {
    "text": "have a functional dependency like I have on LC are pointing to I'll see our desk or see an other way around then I need",
    "start": "874420",
    "end": "883449"
  },
  {
    "text": "to make sure that LCR is a primary or a unique key on this table well LCR cannot be a primary or unique",
    "start": "883449",
    "end": "891759"
  },
  {
    "text": "key on the table because we have duplicates so I have to break that out into its own table this does not belong",
    "start": "891759",
    "end": "898569"
  },
  {
    "text": "in the big table police so I'm going to give it a new table that will have my",
    "start": "898569",
    "end": "905670"
  },
  {
    "text": "key column the LCR and it will have my functionally dependent columns the",
    "start": "905670",
    "end": "911110"
  },
  {
    "text": "description here I'll break that out it will become a new lookup table and I repeat this for each functional",
    "start": "911110",
    "end": "918009"
  },
  {
    "text": "dependency that's in this data set and that in about 50 words is how you get to",
    "start": "918009",
    "end": "923199"
  },
  {
    "text": "Boyce Codd normal form again the link that I have will show you a video where you walk through it in a little bit more",
    "start": "923199",
    "end": "929829"
  },
  {
    "text": "detail it's kind of an interesting concept aside from that we have a few",
    "start": "929829",
    "end": "936220"
  },
  {
    "text": "other data types incident date/time this is a string that represents a date/time value so probably should make that a",
    "start": "936220",
    "end": "942730"
  },
  {
    "text": "date in a time the beat this is where and a set of officers",
    "start": "942730",
    "end": "948670"
  },
  {
    "text": "patrols in a specific area during a specifics at a time we don't have any",
    "start": "948670",
    "end": "953770"
  },
  {
    "text": "additional information on this beat but I could see this being a lookup table of its own we can pretend that the Raleigh",
    "start": "953770",
    "end": "961780"
  },
  {
    "text": "the Wake County Police Department is going to send us this data in a couple of weeks but we don't have it today",
    "start": "961780",
    "end": "970110"
  },
  {
    "text": "incident number is a unique identifier for a particular incident now this follows the first rule of data",
    "start": "970110",
    "end": "977740"
  },
  {
    "text": "type naming which is that if you have number in the name it is never numeric",
    "start": "977740",
    "end": "982950"
  },
  {
    "text": "so all right we got to deal with its text that's okay I'll live with it",
    "start": "982950",
    "end": "987970"
  },
  {
    "text": "and location is geo located notice that it has a precision of some ridiculous",
    "start": "987970",
    "end": "993610"
  },
  {
    "text": "number after the decimal in reality this event did not happen exactly at this location this would be approximately a",
    "start": "993610",
    "end": "1000750"
  },
  {
    "text": "molecule given how far out we're at the police know exactly which molecule the bullet hit in reality with what they're",
    "start": "1000750",
    "end": "1008730"
  },
  {
    "text": "doing is more of let's say the incident is here they're going to build a an area and they're going to pick some arbitrary",
    "start": "1008730",
    "end": "1015210"
  },
  {
    "text": "point in that area that way your analysis is still accurate on net but it protects the privacy of everybody",
    "start": "1015210",
    "end": "1021840"
  },
  {
    "text": "involved so all right that's a lot of words let's let's start doing this stuff I have a",
    "start": "1021840",
    "end": "1028439"
  },
  {
    "text": "table incident code and this is what I was talking about this was the LCR so",
    "start": "1028440",
    "end": "1034140"
  },
  {
    "text": "these are really incident codes and I've given them type so incident code has a varchar' v and incident description of",
    "start": "1034140",
    "end": "1041280"
  },
  {
    "text": "our char 55 that's the maximum length of an incident description in my data set I",
    "start": "1041280",
    "end": "1046370"
  },
  {
    "text": "want to create a primary key constraint so that I can tie it later to a foreign key and I'll insert all of my",
    "start": "1046370",
    "end": "1054650"
  },
  {
    "text": "descriptions from the police table and so I've already created these two tables",
    "start": "1054650",
    "end": "1060360"
  },
  {
    "text": "so I don't have to run them here save me about a minute I also create an incident",
    "start": "1060360",
    "end": "1066090"
  },
  {
    "text": "table here I've created a table with a surrogate key there's a whole debate about whether surrogate keys are",
    "start": "1066090",
    "end": "1072690"
  },
  {
    "text": "necessary important wise or foolishly terrible ideas I'm not going to get in",
    "start": "1072690",
    "end": "1078240"
  },
  {
    "text": "that discussion today I'm okay with them I'm also okay without them but we've created one so",
    "start": "1078240",
    "end": "1086129"
  },
  {
    "text": "this is my surrogate key and just years and years of this drilling being drilled",
    "start": "1086129",
    "end": "1093059"
  },
  {
    "text": "into my head has me create that as the primary key and ends up being a clustered index I also have my incident",
    "start": "1093059",
    "end": "1099929"
  },
  {
    "text": "code but I took off incident description that lives in its own table now so it no",
    "start": "1099929",
    "end": "1105149"
  },
  {
    "text": "longer exists in my BigTable I renamed incident date beat ID changed incident number to a varchar' and made incident",
    "start": "1105149",
    "end": "1112350"
  },
  {
    "text": "location of geography I have a unique key on this incident number whoa I have",
    "start": "1112350",
    "end": "1118799"
  },
  {
    "text": "any key on this incident number and I've defined that down here I have a foreign key on incident code and then I've put a",
    "start": "1118799",
    "end": "1126450"
  },
  {
    "text": "check constraint on beat ID I know that the beat must be a positive value so",
    "start": "1126450",
    "end": "1132259"
  },
  {
    "text": "beat must be greater than or equal to zero I also know that my incident date",
    "start": "1132259",
    "end": "1137789"
  },
  {
    "text": "must be within a certain range because I pulled this data in the year 2014 I know",
    "start": "1137789",
    "end": "1143129"
  },
  {
    "text": "that there shouldn't be any values that are later than 2014 because if there were then we've got some side of a",
    "start": "1143129",
    "end": "1149730"
  },
  {
    "text": "Minority Report precognition police thing going on and as far as I know Raleigh doesn't hire any of those police",
    "start": "1149730",
    "end": "1155639"
  },
  {
    "text": "officers so I insert data into the incident table from dbo police let's",
    "start": "1155639",
    "end": "1162149"
  },
  {
    "text": "start polling I'm using tryparse which was introduced in sequel Server 2012 this will allow me to specify a",
    "start": "1162149",
    "end": "1171480"
  },
  {
    "text": "particular date format and say you know what you figure this out I'm gonna tell you that the people who put this in",
    "start": "1171480",
    "end": "1177210"
  },
  {
    "text": "we're using us English as their as their locale and I want you to give me a date",
    "start": "1177210",
    "end": "1183419"
  },
  {
    "text": "time that matches up with it so going back to this table might notice that",
    "start": "1183419",
    "end": "1189809"
  },
  {
    "text": "these are day month or excuse me month day year instead of day month year so if",
    "start": "1189809",
    "end": "1195720"
  },
  {
    "text": "I change this to UK or Australian English then I'm going to get some different results but because I know",
    "start": "1195720",
    "end": "1202860"
  },
  {
    "text": "that this was US English then I specify and I get back the appropriate values a date time my geography I'm using a CLR",
    "start": "1202860",
    "end": "1210419"
  },
  {
    "text": "function that's been around since sequel's 2008 to convert a string into a point a",
    "start": "1210419",
    "end": "1219540"
  },
  {
    "text": "geographic point so latitude and longitude pair I run that and I now have",
    "start": "1219540",
    "end": "1225840"
  },
  {
    "text": "a couple of tables so there's this incident table and this incident code a table the incident code table shouldn't",
    "start": "1225840",
    "end": "1233040"
  },
  {
    "text": "shock you is a couple hundred records and just shows you different incident",
    "start": "1233040",
    "end": "1238710"
  },
  {
    "text": "codes ignore this for now we're going to talk about it in a moment so we have the different incident codes if I come back",
    "start": "1238710",
    "end": "1245610"
  },
  {
    "text": "to the incident table I have incident IDs and the Associated code we've got the date the beat the",
    "start": "1245610",
    "end": "1252660"
  },
  {
    "text": "incident number and location notice that location is now binary it's because geography types are saved as binary and",
    "start": "1252660",
    "end": "1259680"
  },
  {
    "text": "then I can use other functions to convert them back into point values this",
    "start": "1259680",
    "end": "1265800"
  },
  {
    "text": "means is I can also say show me all the values within a certain circle or show me all the values that are relatively",
    "start": "1265800",
    "end": "1271980"
  },
  {
    "text": "near to this point so I've opened up the things that I can do with this data so",
    "start": "1271980",
    "end": "1279000"
  },
  {
    "start": "1273000",
    "end": "1787000"
  },
  {
    "text": "we skip this and start going into mapping tables which is what I was hiding just a moment ago see ya the",
    "start": "1279000",
    "end": "1286110"
  },
  {
    "text": "problem sometimes with a data set is I want to analyze data at a higher a",
    "start": "1286110",
    "end": "1292560"
  },
  {
    "text": "coarser grain than may exist a transactional system in the police",
    "start": "1292560",
    "end": "1298140"
  },
  {
    "text": "example I may want to take all of my types of a particular incident and",
    "start": "1298140",
    "end": "1304910"
  },
  {
    "text": "convert them into a more generic form so we'll see that there are cases where",
    "start": "1304910",
    "end": "1311550"
  },
  {
    "text": "there are a bunch of different types of forgery I don't necessarily care about that for my analysis I just want to know",
    "start": "1311550",
    "end": "1317880"
  },
  {
    "text": "is this forged is this forgery or not if it is forgery let's assign it to a",
    "start": "1317880",
    "end": "1323280"
  },
  {
    "text": "forgery type so let's do that now I have",
    "start": "1323280",
    "end": "1328710"
  },
  {
    "text": "a note in my text that you know sometimes I'm not able to modify the",
    "start": "1328710",
    "end": "1334050"
  },
  {
    "text": "table itself sometimes I may have to create another table and create a minute",
    "start": "1334050",
    "end": "1339120"
  },
  {
    "text": "of any relationship with that table where I'll create an incident type table and an incident code table",
    "start": "1339120",
    "end": "1345720"
  },
  {
    "text": "an incident code incident type even though I know that there's only one type",
    "start": "1345720",
    "end": "1351060"
  },
  {
    "text": "her code so sometimes you have to do that because the business side says no",
    "start": "1351060",
    "end": "1357930"
  },
  {
    "text": "we can't make changes to the original data set or you know it's a third party vendor tool where you're not allowed to",
    "start": "1357930",
    "end": "1363990"
  },
  {
    "text": "make any changes to it in this case I talked to the product owner who happened to be me and I explained to the product",
    "start": "1363990",
    "end": "1370440"
  },
  {
    "text": "owner that this is a really important thing and the product owner said okay go ahead and do it so I was able to add an",
    "start": "1370440",
    "end": "1376620"
  },
  {
    "text": "incident type ID so my incident codes are going to have a type and I've got a",
    "start": "1376620",
    "end": "1382830"
  },
  {
    "text": "type table notice here that yeah I did use a surrogate in this case I didn't",
    "start": "1382830",
    "end": "1388710"
  },
  {
    "text": "need to I could have just made an incident type my primary key is on the",
    "start": "1388710",
    "end": "1393840"
  },
  {
    "text": "ID my unique key is on the type so every type is unique and then I'm going to",
    "start": "1393840",
    "end": "1399720"
  },
  {
    "text": "create a foreign key constraint going back to incident code that says every",
    "start": "1399720",
    "end": "1404880"
  },
  {
    "text": "record and incident code that has a value better show up in this incident type table if it doesn't show up in the",
    "start": "1404880",
    "end": "1410520"
  },
  {
    "text": "incident type table then it's invalid now I have a whole bunch of rules for",
    "start": "1410520",
    "end": "1418230"
  },
  {
    "text": "mapping and don't even worry about reading this I'm gonna show you something even better spoilers so let's",
    "start": "1418230",
    "end": "1427110"
  },
  {
    "text": "create a table incident type and I'm going to switch over to use the Raleigh",
    "start": "1427110",
    "end": "1433590"
  },
  {
    "text": "crime data set I'm going to insert into the incident type table this let's let's",
    "start": "1433590",
    "end": "1444630"
  },
  {
    "text": "walk through this because I love a cross apply but it's kind of unfamiliar to a",
    "start": "1444630",
    "end": "1450090"
  },
  {
    "text": "lot of people from the incident code table I'm going to use the cross apply function to chain a bunch of replace",
    "start": "1450090",
    "end": "1458130"
  },
  {
    "text": "statements think of the cross apply operation as for each record go do something so for a developer this makes",
    "start": "1458130",
    "end": "1467010"
  },
  {
    "text": "a lot of sense intuitively and for a database administrator or a database developer this performs well because",
    "start": "1467010",
    "end": "1474780"
  },
  {
    "text": "it's a set based operation it's one of those cases where that procedural step by step and the set-based do everything at once",
    "start": "1474780",
    "end": "1483230"
  },
  {
    "text": "turn out to work quite nicely together so my first case I'm going to replace",
    "start": "1483230",
    "end": "1490500"
  },
  {
    "text": "the incident description and replace every statement that has a space and a W",
    "start": "1490500",
    "end": "1496260"
  },
  {
    "text": "/ with a minus W / let's go take a quick look at what that means as I mentioned",
    "start": "1496260",
    "end": "1505530"
  },
  {
    "text": "there are a bunch of cases where I have",
    "start": "1505530",
    "end": "1510679"
  },
  {
    "text": "forgery or there are a lot of different types of fraud there's flim-flam and then there's",
    "start": "1510679",
    "end": "1517200"
  },
  {
    "text": "identity flecked flim-flam I to this day I still don't know exactly what",
    "start": "1517200",
    "end": "1522360"
  },
  {
    "text": "flim-flam is and I don't want to look it up because that would ruin my vision of what flim-flam is but I don't care that",
    "start": "1522360",
    "end": "1530520"
  },
  {
    "text": "this is flim-flam with identity theft I just it's fraud so I want to try to basically take the first",
    "start": "1530520",
    "end": "1538040"
  },
  {
    "text": "interesting-looking word in the description and slice it out so forgery",
    "start": "1538040",
    "end": "1545160"
  },
  {
    "text": "all of these things are forgery all of these things are fraud and so on to do",
    "start": "1545160",
    "end": "1553230"
  },
  {
    "text": "this let's replace any space W with a - W murder",
    "start": "1553230",
    "end": "1559559"
  },
  {
    "text": "embezzlement assault larceny and rape tend to have are the only cases that have space and then another word so I",
    "start": "1559559",
    "end": "1568500"
  },
  {
    "text": "replace that space with a slash then I now have a an incident description and",
    "start": "1568500",
    "end": "1575610"
  },
  {
    "text": "by the way you'll see that I'm I'm chaining this so that like ID W is what I create here and so I look at ID w",
    "start": "1575610",
    "end": "1582720"
  },
  {
    "text": "sense in description and call that IDM and I look at ID Em's incident description I call that IDE and so on",
    "start": "1582720",
    "end": "1589130"
  },
  {
    "text": "when I'm done I've got a final incident description and I'm going to find the",
    "start": "1589130",
    "end": "1594809"
  },
  {
    "text": "first case of a slash of a - of a and",
    "start": "1594809",
    "end": "1600090"
  },
  {
    "text": "open parens and I'm going to assign those as values first / first - first",
    "start": "1600090",
    "end": "1605669"
  },
  {
    "text": "print then here I have a case where I",
    "start": "1605669",
    "end": "1612090"
  },
  {
    "text": "say all right I know that char index work will return 0 if that value does not",
    "start": "1612090",
    "end": "1619080"
  },
  {
    "text": "exist so if my incident description does not have an open parenthesis it will",
    "start": "1619080",
    "end": "1624690"
  },
  {
    "text": "return 0 I don't want it to return 0 because I want to get the minimum value so I want to find the first of these",
    "start": "1624690",
    "end": "1630929"
  },
  {
    "text": "characters so let's just put that as a sentinel value way out they at the end I know I'm never gonna hit 999 because my",
    "start": "1630929",
    "end": "1638250"
  },
  {
    "text": "incident description length is 55 characters so I'm never gonna get there",
    "start": "1638250",
    "end": "1645409"
  },
  {
    "text": "until eventually it gets 1,024 characters and I get there but those are",
    "start": "1645409",
    "end": "1650639"
  },
  {
    "text": "problems for another day so I've got 999 if it doesn't exist and I've got the",
    "start": "1650639",
    "end": "1657179"
  },
  {
    "text": "actual character value if it does exist so I've got that that point in the array",
    "start": "1657179",
    "end": "1663830"
  },
  {
    "text": "so to speak then quick check what's the first of these deciding characters I'm",
    "start": "1663830",
    "end": "1671190"
  },
  {
    "text": "gonna take that first deciding character and I'm going to get the substring from incident description starting at Point 1",
    "start": "1671190",
    "end": "1679139"
  },
  {
    "text": "up to but not including that deciding character and il trim in our trim or if",
    "start": "1679139",
    "end": "1685139"
  },
  {
    "text": "you're really fancy and you're using sequel server 2017 I trim that right",
    "start": "1685139",
    "end": "1692010"
  },
  {
    "text": "there just save me a bunch of characters 316 rows affected but notice that a",
    "start": "1692010",
    "end": "1698570"
  },
  {
    "text": "intellisense doesn't work for it in the version of management studio that I have which is 2016 so if you want to tell us",
    "start": "1698570",
    "end": "1705450"
  },
  {
    "text": "since to work with trim function you have to download 2017 s management studio ok so now we have incident types",
    "start": "1705450",
    "end": "1714360"
  },
  {
    "text": "and I want to get a dense ranking of them what that does is it will give me a",
    "start": "1714360",
    "end": "1720210"
  },
  {
    "text": "unique number for each one of these incident types and by making it a dense rank we go from 1 to 2 and then the next",
    "start": "1720210",
    "end": "1728039"
  },
  {
    "text": "value will be a 3 and so on otherwise it would have gone 1 to 9 on and on dense",
    "start": "1728039",
    "end": "1736049"
  },
  {
    "text": "rank is not necessary for this it just it makes it look a little bit nicer I can easily see when the next value comes",
    "start": "1736049",
    "end": "1743669"
  },
  {
    "text": "up so 316 rows I can then insert all these values into",
    "start": "1743669",
    "end": "1749129"
  },
  {
    "text": "the incident type table so I've got all of these incident types 1 2 3 and so on",
    "start": "1749129",
    "end": "1756720"
  },
  {
    "text": "and through this I update incident code set the incident type ID based off of",
    "start": "1756720",
    "end": "1763379"
  },
  {
    "text": "that incident type I've been able to take this data set winnow down to the",
    "start": "1763379",
    "end": "1769230"
  },
  {
    "text": "specific rules that I want create a new table based off of that and then update",
    "start": "1769230",
    "end": "1774239"
  },
  {
    "text": "the existing table to point to that higher-level type I'm gonna use that type later on in analysis so that I can",
    "start": "1774239",
    "end": "1780840"
  },
  {
    "text": "see all the cases of fraud all the cases of murder all the cases of embezzlement",
    "start": "1780840",
    "end": "1787070"
  },
  {
    "text": "so let's go into the are section and we've looked at handling some of this",
    "start": "1788659",
    "end": "1795239"
  },
  {
    "text": "work with sequel server now let's go pick out another fun language are this",
    "start": "1795239",
    "end": "1802859"
  },
  {
    "text": "section flows from a vignette that had Lee Wickham put together Wickham is one",
    "start": "1802859",
    "end": "1810090"
  },
  {
    "text": "of the key players in the our space and he his work structuring data sets to",
    "start": "1810090",
    "end": "1815759"
  },
  {
    "text": "facilitate analysis despite having a very academic title is a really good read",
    "start": "1815759",
    "end": "1820789"
  },
  {
    "text": "basically he starts from the premise that look you've got data sets and data data sets are made up of variables and",
    "start": "1820789",
    "end": "1826980"
  },
  {
    "text": "observations a variable in a data set is a column an observation in a data set is",
    "start": "1826980",
    "end": "1835259"
  },
  {
    "text": "a row in relational database terminology variables are attributes and",
    "start": "1835259",
    "end": "1841909"
  },
  {
    "text": "observations are entities all the same thing variables he goes a little bit",
    "start": "1841909",
    "end": "1847859"
  },
  {
    "text": "further and defines down variables a little bit more and says that these actually should measure the same",
    "start": "1847859",
    "end": "1853590"
  },
  {
    "text": "underlying thing so let's say you have a variable called height every observation",
    "start": "1853590",
    "end": "1860879"
  },
  {
    "text": "that thing should be a height it should not be a height for one person and then a favorite color for another person and",
    "start": "1860879",
    "end": "1867629"
  },
  {
    "text": "then a tail length if it's a dog in this data set instead of a person it should",
    "start": "1867629",
    "end": "1874230"
  },
  {
    "text": "all be the same thing observations should contain the same set",
    "start": "1874230",
    "end": "1880710"
  },
  {
    "text": "of variables for each element in the set so in other words let's say we have a",
    "start": "1880710",
    "end": "1887609"
  },
  {
    "text": "set of hospital visits every one of these things is a hospital visit and it",
    "start": "1887609",
    "end": "1893009"
  },
  {
    "text": "has the same set of variables we have the same structure the reason that we do",
    "start": "1893009",
    "end": "1899489"
  },
  {
    "text": "this is because it's a lot easier to describe relationships between variables in other words to build those functional",
    "start": "1899489",
    "end": "1906239"
  },
  {
    "text": "dependencies that I just glanced over age is a function of when you were born",
    "start": "1906239",
    "end": "1912809"
  },
  {
    "text": "and what today's date is we can tease that out a lot more easily if if today's",
    "start": "1912809",
    "end": "1918869"
  },
  {
    "text": "date and date of birth are two variables instead of two observations by contrast",
    "start": "1918869",
    "end": "1925649"
  },
  {
    "text": "it's a lot easier to group data to aggregate data across observations than it is across variables our tooling is",
    "start": "1925649",
    "end": "1932999"
  },
  {
    "text": "built for grouping observations so how many people are using this phone number",
    "start": "1932999",
    "end": "1938369"
  },
  {
    "text": "how many people came in on the 19th with a case of they're presenting for",
    "start": "1938369",
    "end": "1944639"
  },
  {
    "text": "tuberculosis we can get these numbers pretty easily in either a sequel",
    "start": "1944639",
    "end": "1951029"
  },
  {
    "text": "language or in our and what Wickham ends up saying is look tidy data is basically",
    "start": "1951029",
    "end": "1957960"
  },
  {
    "text": "third normal form you take your data you normalize it I prefer Boyce Codd but you",
    "start": "1957960",
    "end": "1964710"
  },
  {
    "text": "normalize your data and then he says as a practical matter you probably are going to denormalize the data back in",
    "start": "1964710",
    "end": "1971099"
  },
  {
    "text": "for that art analysis but at least at one point you know this data was normalized so he has a series of",
    "start": "1971099",
    "end": "1979369"
  },
  {
    "text": "libraries that are designed to solve these problems around data cleansing the",
    "start": "1979369",
    "end": "1984840"
  },
  {
    "text": "first one that I'm going to look at is tied er this is just some simple functions to tidy up data and let's take",
    "start": "1984840",
    "end": "1992309"
  },
  {
    "start": "1991000",
    "end": "2006000"
  },
  {
    "text": "a look I've got some Jupiter notebooks notebooks are also fully available the",
    "start": "1992309",
    "end": "1998070"
  },
  {
    "text": "thing that I like about these notebooks is that you can take them and if you have Jupiter installed you can run them",
    "start": "1998070",
    "end": "2003589"
  },
  {
    "text": "yourself so I'm going to start",
    "start": "2003589",
    "end": "2008780"
  },
  {
    "text": "actually let me quickly talk about what I did up here I am saying if you don't",
    "start": "2008780",
    "end": "2014450"
  },
  {
    "text": "have this package tidy verse then go install it and load it if you do have it already in memory cool I don't I don't",
    "start": "2014450",
    "end": "2021470"
  },
  {
    "text": "need you to do anything and I do the same thing with the ggplot let's bump",
    "start": "2021470",
    "end": "2028010"
  },
  {
    "text": "this up a little bit more so tidy verse is a whole bunch of different libraries",
    "start": "2028010",
    "end": "2033140"
  },
  {
    "text": "that are all designed to work together to cleanse data tidy our is just one of them and we're gonna look at a couple",
    "start": "2033140",
    "end": "2039260"
  },
  {
    "text": "more as well ggplot2 is a way of mapping data it's a",
    "start": "2039260",
    "end": "2044270"
  },
  {
    "text": "way of displaying graphs it's a way of visualizing data I run them and I get a",
    "start": "2044270",
    "end": "2050600"
  },
  {
    "text": "bunch of warning messages that say nothing of importance to us and then we start looking at our first data set",
    "start": "2050600",
    "end": "2056500"
  },
  {
    "text": "alright so this is a data set that was a Pew survey of people and this is what",
    "start": "2056500",
    "end": "2063649"
  },
  {
    "text": "they presented to people we have one variable here it's very clearly a",
    "start": "2063650",
    "end": "2069220"
  },
  {
    "text": "variable and it's a religion we have another variable this spread out a bunch",
    "start": "2069220",
    "end": "2074990"
  },
  {
    "text": "of among a bunch of columns and that is income bracket and the numbers in",
    "start": "2074990",
    "end": "2080510"
  },
  {
    "text": "between these are the numbers of people who decided to answer a particular way",
    "start": "2080510",
    "end": "2086148"
  },
  {
    "text": "on religion and income bracket the problem is that if we want to do any",
    "start": "2086149",
    "end": "2092780"
  },
  {
    "text": "type of analysis as opposed to just looking at the chart if we want to",
    "start": "2092780",
    "end": "2098330"
  },
  {
    "text": "analyze this at all what we really want is for this to be its own variable so",
    "start": "2098330",
    "end": "2104690"
  },
  {
    "text": "that we would have a religion variable and an income bracket variable and then we would have a measure that would tell",
    "start": "2104690",
    "end": "2110180"
  },
  {
    "text": "us how many people answered this religion this income bracket and we can",
    "start": "2110180",
    "end": "2116720"
  },
  {
    "text": "use tidy R to do that in just a few lines of code so I'm going to read the",
    "start": "2116720",
    "end": "2121730"
  },
  {
    "text": "data from this CSV that I have everything eventually comes down to CSV",
    "start": "2121730",
    "end": "2127070"
  },
  {
    "text": "s and I am taking the data and I'm using",
    "start": "2127070",
    "end": "2134090"
  },
  {
    "text": "a pipe operator so this is the pipe operator and our percent angle bracket percent what that is saying is take this",
    "start": "2134090",
    "end": "2141380"
  },
  {
    "text": "data frame this data set and send it to the next function the next function here is",
    "start": "2141380",
    "end": "2147470"
  },
  {
    "text": "called gather gather act as an unpaid operation so what I'm doing is I'm",
    "start": "2147470",
    "end": "2154849"
  },
  {
    "text": "saying I want you to gather and create a new column called income and that income",
    "start": "2154849",
    "end": "2160730"
  },
  {
    "text": "is going to have all of the names of the income brackets I want a new column",
    "start": "2160730",
    "end": "2165950"
  },
  {
    "text": "that's called frequency and that's going to be the the inner guts of the cross",
    "start": "2165950",
    "end": "2172790"
  },
  {
    "text": "product of this religion and an income bracket all of the columns that are",
    "start": "2172790",
    "end": "2178730"
  },
  {
    "text": "needed for income are then defined afterwards and I have a - before",
    "start": "2178730",
    "end": "2184369"
  },
  {
    "text": "religion basically says every column except for religion is actually part of income so I can use - I can also list",
    "start": "2184369",
    "end": "2192920"
  },
  {
    "text": "the columns specifically and I do that and now I have the data that is unfitted",
    "start": "2192920",
    "end": "2199099"
  },
  {
    "text": "so I have frequencies that I have religions and I have incomes and now I",
    "start": "2199099",
    "end": "2204230"
  },
  {
    "text": "can use a tool like ggplot and in a half-dozen lines of code what I can do",
    "start": "2204230",
    "end": "2210589"
  },
  {
    "text": "is create a really ugly looking graph in about four more lines of code I could",
    "start": "2210589",
    "end": "2215780"
  },
  {
    "text": "make it a much less ugly looking graph but all I'm doing here is I'm saying okay take this data set that you have",
    "start": "2215780",
    "end": "2222200"
  },
  {
    "text": "now and then apply it basically apply",
    "start": "2222200",
    "end": "2227810"
  },
  {
    "text": "that data set and say the x-axis is religion the y-axis is the number of",
    "start": "2227810",
    "end": "2232820"
  },
  {
    "text": "participants who accept who said this and the coloration is income bracket I",
    "start": "2232820",
    "end": "2238540"
  },
  {
    "text": "could also lay it out as a heat map I could also lay it out as some other",
    "start": "2238540",
    "end": "2245109"
  },
  {
    "text": "visualization format the rest of this is commentary the rest of this is just",
    "start": "2245109",
    "end": "2251450"
  },
  {
    "text": "adding things like labels and titles so",
    "start": "2251450",
    "end": "2259160"
  },
  {
    "text": "that's unfitting data it's a pretty simple example I want to get to a",
    "start": "2259160",
    "end": "2264230"
  },
  {
    "text": "slightly more complex example and here's a data set this is tuberculosis data I",
    "start": "2264230",
    "end": "2269480"
  },
  {
    "text": "have a couple of variables here and that I have these things this is males between the age of 5 and",
    "start": "2269480",
    "end": "2276529"
  },
  {
    "text": "14 males between the age of 35 and 44 females between the age of 0 and 14 what",
    "start": "2276529",
    "end": "2283819"
  },
  {
    "text": "I have here is either two or three separate variables for today I'm gonna",
    "start": "2283819",
    "end": "2289940"
  },
  {
    "text": "treat it as two variables male or female and age range and then I have some",
    "start": "2289940",
    "end": "2298640"
  },
  {
    "text": "number of cases notice that a lot of these have n/a we don't have any samples",
    "start": "2298640",
    "end": "2305960"
  },
  {
    "text": "for this particular sex and age range whereas here we do have a sample and",
    "start": "2305960",
    "end": "2314150"
  },
  {
    "text": "there were zero values so those are important to consider all of these n A's",
    "start": "2314150",
    "end": "2320299"
  },
  {
    "text": "I don't have those samples we can throw them out there only",
    "start": "2320299",
    "end": "2325489"
  },
  {
    "text": "they were only needed to pad out this grid but we're not going to keep the grid we're going to unfit the data break",
    "start": "2325489",
    "end": "2332690"
  },
  {
    "text": "it out and then get our measures appropriately so that's what we do",
    "start": "2332690",
    "end": "2337910"
  },
  {
    "text": "starting here I take my tuberculosis data and I pipe it together so again we",
    "start": "2337910",
    "end": "2344960"
  },
  {
    "text": "unfit the data we take all of those values we say I want to create a new variable called demo demographic data I",
    "start": "2344960",
    "end": "2351890"
  },
  {
    "text": "want to create a new measure called in number of participants then I take the",
    "start": "2351890",
    "end": "2359779"
  },
  {
    "text": "set of columns that will make up demographic and that is everything except for ISO 2 and year and by the way",
    "start": "2359779",
    "end": "2368150"
  },
  {
    "text": "let's remove all the n/a values let's remove all of those null values because they don't really exist",
    "start": "2368150",
    "end": "2374539"
  },
  {
    "text": "they were just padding after that I used the separate function to crack out the",
    "start": "2374539",
    "end": "2383329"
  },
  {
    "text": "particular demo value into two separate columns so I do that and I end up by the",
    "start": "2383329",
    "end": "2394849"
  },
  {
    "text": "way I should probably note that the one parameter here separate after the first character so I know that there is one",
    "start": "2394849",
    "end": "2401029"
  },
  {
    "text": "character and I can break it out into 0 to 4 and as we keep going down we have",
    "start": "2401029",
    "end": "2407150"
  },
  {
    "text": "females who are undefined and that's at the end of the data set so overall this was a little",
    "start": "2407150",
    "end": "2413900"
  },
  {
    "text": "bit over a hundred and fifteen thousand observations and I can take those observations and I can plug them into a",
    "start": "2413900",
    "end": "2419839"
  },
  {
    "text": "model maybe I say I think that the number of cases of tuberculosis is dependent upon",
    "start": "2419839",
    "end": "2427630"
  },
  {
    "text": "it's a linear function related to year and whether you're male or female and",
    "start": "2427630",
    "end": "2432740"
  },
  {
    "text": "your age range and I can run this and I can scroll down and see all these",
    "start": "2432740",
    "end": "2439160"
  },
  {
    "text": "asterisks which say that yes these this is a really tiny p-value and I'm going",
    "start": "2439160",
    "end": "2444680"
  },
  {
    "text": "out and I'm gonna publish this paper and I'm gonna get rich because that's what Pat that's how you do it in academia I",
    "start": "2444680",
    "end": "2450710"
  },
  {
    "text": "think until somebody goes and looks and says you know that that explains about",
    "start": "2450710",
    "end": "2455900"
  },
  {
    "text": "2% of your variability right and I say yeah but I got p-values prolonged sigh",
    "start": "2455900",
    "end": "2467930"
  },
  {
    "start": "2465000",
    "end": "2690000"
  },
  {
    "text": "and they're on purpose so let's let's move on from my horrible horrible",
    "start": "2467930",
    "end": "2473839"
  },
  {
    "text": "academic career and talk about eav by horrible horrible professional career",
    "start": "2473839",
    "end": "2479450"
  },
  {
    "text": "sometimes datasets come in as entity attribute value combinations so what we",
    "start": "2479450",
    "end": "2484970"
  },
  {
    "text": "have is we have the entity itself we have an attribute and we have a value",
    "start": "2484970",
    "end": "2492530"
  },
  {
    "text": "associated with it in this case we have kind of a real kicker because you know",
    "start": "2492530",
    "end": "2499609"
  },
  {
    "text": "first of all you'll never guess how many number of these there are because this",
    "start": "2499609",
    "end": "2506089"
  },
  {
    "text": "is this is de values this is like day 1 day 2 day 3 we're looking at weather",
    "start": "2506089",
    "end": "2511609"
  },
  {
    "text": "station data and these are the minimum and maximum temperatures for weather",
    "start": "2511609",
    "end": "2516650"
  },
  {
    "text": "stations in some locations in Mexico over a multi-month period so we have 31",
    "start": "2516650",
    "end": "2525200"
  },
  {
    "text": "of these that are all really supposed to be the same thing it's the day of the month we have min and Max and notice",
    "start": "2525200",
    "end": "2533690"
  },
  {
    "text": "that for day two the men and the max are in the same column the problem here is",
    "start": "2533690",
    "end": "2539839"
  },
  {
    "text": "that if I want to do some sort of aggregate let's say I want to get the average max in this data set I cannot simply take",
    "start": "2539839",
    "end": "2548360"
  },
  {
    "text": "the average of day two I have to take the average of day two where element is",
    "start": "2548360",
    "end": "2554720"
  },
  {
    "text": "T Max and that's a lot more effort than it should be so let's fix that up I am",
    "start": "2554720",
    "end": "2560900"
  },
  {
    "text": "going to take my data set and in just a few lines we're gonna crack this thing",
    "start": "2560900",
    "end": "2566140"
  },
  {
    "text": "we start by taking the original data and piping it to gather so we run pivoting",
    "start": "2566140",
    "end": "2573200"
  },
  {
    "text": "now previously I'd showed you unfitting by taking every value except for the",
    "start": "2573200",
    "end": "2579020"
  },
  {
    "text": "ones that we named here I'm naming the values specifically I'm saying every",
    "start": "2579020",
    "end": "2584720"
  },
  {
    "text": "value between D 1 and D 31 in my data set I want you to take all of those",
    "start": "2584720",
    "end": "2590930"
  },
  {
    "text": "things and unfit them turn them into a column that column will be called day",
    "start": "2590930",
    "end": "2595970"
  },
  {
    "text": "and the numeric measure that was there will be called value then I use another",
    "start": "2595970",
    "end": "2604120"
  },
  {
    "text": "element of the tidy verse read R to parse the number out from that day",
    "start": "2604120",
    "end": "2609230"
  },
  {
    "text": "basically get rid of the D so that's gonna be called day and I mutate so I'm",
    "start": "2609230",
    "end": "2614960"
  },
  {
    "text": "modifying the data set I'm modifying that data frame and changing a d1 to 1",
    "start": "2614960",
    "end": "2622310"
  },
  {
    "text": "and making it numeric then I want to select only the relevant fields so I",
    "start": "2622310",
    "end": "2628310"
  },
  {
    "text": "used the Select function then I want to sort them then I want to spread which is",
    "start": "2628310",
    "end": "2636470"
  },
  {
    "text": "pivoting so I have a min value and a max value and I can spread them and turn",
    "start": "2636470",
    "end": "2643010"
  },
  {
    "text": "them into two separate columns so let's do that here I take the head of that",
    "start": "2643010",
    "end": "2649940"
  },
  {
    "text": "weather set and now what I have is four",
    "start": "2649940",
    "end": "2655400"
  },
  {
    "text": "explanatory variables which weather station what year what month what day and then to aggregate able measures max",
    "start": "2655400",
    "end": "2664520"
  },
  {
    "text": "temperature and min temperature and now I can plot the max temperatures and I",
    "start": "2664520",
    "end": "2670010"
  },
  {
    "text": "can see the range and this is by day of month so the maximum temperature the fourth day of the month",
    "start": "2670010",
    "end": "2678280"
  },
  {
    "text": "was just over 24 degrees Celsius and then we go up to the top where I was",
    "start": "2678280",
    "end": "2683770"
  },
  {
    "text": "about 37 somewhere on the 26th or so of the month so that was tidy our let's go",
    "start": "2683770",
    "end": "2693640"
  },
  {
    "text": "talk about the next set of the tidy verse which is deep liar see that tidy R was just one little piece I also showed",
    "start": "2693640",
    "end": "2700810"
  },
  {
    "text": "you a little bit of deep liar which we'll see I also showed you just a little bit of radar which we got to see",
    "start": "2700810",
    "end": "2707500"
  },
  {
    "text": "as parsing numbers we also we won't be able to see some of the other elements",
    "start": "2707500",
    "end": "2712900"
  },
  {
    "text": "of the tidy verse like date which is a date function so this will allow you to convert strings to dates really",
    "start": "2712900",
    "end": "2719590"
  },
  {
    "text": "easily there's also other libraries like purr which works really well with api's",
    "start": "2719590",
    "end": "2726120"
  },
  {
    "text": "but all of these things are in that tidy verse library so let's talk about deep",
    "start": "2726120",
    "end": "2732130"
  },
  {
    "text": "lyre once again I'm loading my tidy verse data and I also have this data set",
    "start": "2732130",
    "end": "2738160"
  },
  {
    "text": "called Gapminder Gapminder is a data set that looks at data from the year 1958",
    "start": "2738160",
    "end": "2746830"
  },
  {
    "text": "I believe 1952 to 2007 and it will include the population of a country and",
    "start": "2746830",
    "end": "2753700"
  },
  {
    "text": "also the GDP of that country in each of the five year periods so we sample every",
    "start": "2753700",
    "end": "2759040"
  },
  {
    "text": "five years once I load the data so I've loaded that package already oh I hope I",
    "start": "2759040",
    "end": "2767260"
  },
  {
    "text": "didn't mess myself up good okay I'm not on the internet right now so it's probably it would have choked if it had",
    "start": "2767260",
    "end": "2773890"
  },
  {
    "text": "to go look anything up anyhow I can use the filter function tact as sort of like",
    "start": "2773890",
    "end": "2779200"
  },
  {
    "text": "a where clause filter Gapminder where the life expectancy is less than 30 and we can see that in two cases Afghanistan",
    "start": "2779200",
    "end": "2787300"
  },
  {
    "text": "in 1952 and Rwanda in 1992 the life expectancy was less than 30 years let's",
    "start": "2787300",
    "end": "2794500"
  },
  {
    "text": "go the opposite way filter where life expectancy was greater than eighty one",
    "start": "2794500",
    "end": "2799630"
  },
  {
    "text": "and a half and here we have a couple of Asian countries and a couple of European",
    "start": "2799630",
    "end": "2804760"
  },
  {
    "text": "countries all in the most recent data sets and Japan was in 2002 as well so filter",
    "start": "2804760",
    "end": "2812830"
  },
  {
    "text": "acts like a where clause now there is no and or or but you can get that same",
    "start": "2812830",
    "end": "2819940"
  },
  {
    "text": "logic by saying comma is an and a pipe is an or so let's ask for countries",
    "start": "2819940",
    "end": "2828100"
  },
  {
    "text": "where the population is at least twenty million and the GDP per capita is at least thirty six thousand USD we at",
    "start": "2828100",
    "end": "2835000"
  },
  {
    "text": "Canada in the United States let's look at where population is 1.2 billion or",
    "start": "2835000",
    "end": "2840790"
  },
  {
    "text": "GDP per capita is over a hundred thousand USD this is all purchasing",
    "start": "2840790",
    "end": "2846430"
  },
  {
    "text": "power parity normalized for I think two thousand seven dollars but that's beyond",
    "start": "2846430",
    "end": "2852310"
  },
  {
    "text": "the scope of this discussion anyhow 1.2 billion we have one country in the list",
    "start": "2852310",
    "end": "2858250"
  },
  {
    "text": "GDP per capita over a hundred thousand we have one country in the list but we're able to pipe that together and get",
    "start": "2858250",
    "end": "2866380"
  },
  {
    "text": "two separate data sets combined together the Select function let me pick certain",
    "start": "2866380",
    "end": "2872950"
  },
  {
    "text": "columns from my data set so from Gapminder give me your life expectancy",
    "start": "2872950",
    "end": "2878020"
  },
  {
    "text": "and it only shows you those two columns so select works that way I showed you",
    "start": "2878020",
    "end": "2885640"
  },
  {
    "text": "the pipe operator already and I can use the pipe operator to say filter out any",
    "start": "2885640",
    "end": "2891190"
  },
  {
    "text": "country that has a population under 1.2 billion so throw those away and then give me the country your life expectancy",
    "start": "2891190",
    "end": "2898150"
  },
  {
    "text": "of China basically and we see the life",
    "start": "2898150",
    "end": "2903160"
  },
  {
    "text": "expectancy during that point in time i I've used mutate already to good effect",
    "start": "2903160",
    "end": "2908830"
  },
  {
    "text": "but as a quick reminder mutate allows you to modify a data frame add to add",
    "start": "2908830",
    "end": "2915040"
  },
  {
    "text": "columns or to change columns here I'm adding a new column called GDP it is equal to population multiplied by GDP",
    "start": "2915040",
    "end": "2922270"
  },
  {
    "text": "per capita and when I do that I now have a GDP of this as a quick note",
    "start": "2922270",
    "end": "2929640"
  },
  {
    "text": "Afghanistan in 1952 that's about six billion dollars that means that Bill",
    "start": "2929640",
    "end": "2935110"
  },
  {
    "text": "Gates and Jeff Bezos could have bought and sold Afghanistan between them about what thirty six times",
    "start": "2935110",
    "end": "2942140"
  },
  {
    "text": "more or less a strange game they have but whatever I can also arrange datasets",
    "start": "2942140",
    "end": "2950809"
  },
  {
    "text": "sorting so I've arranged by GDP the lowest GDP turns out to be sell Thome",
    "start": "2950809",
    "end": "2957049"
  },
  {
    "text": "and grab the top ten using the head function if I want a sort in descending",
    "start": "2957049",
    "end": "2963170"
  },
  {
    "text": "order I use the de SC function descending function and I can see that the top 10",
    "start": "2963170",
    "end": "2968569"
  },
  {
    "text": "per capita China and Japan show up here on this set so continuing along new gap",
    "start": "2968569",
    "end": "2979190"
  },
  {
    "text": "I want to group it by a continent just like in T sequel I have a group by function there I have a group by",
    "start": "2979190",
    "end": "2984950"
  },
  {
    "text": "function here then I have a summarize function that gives me a measure so in",
    "start": "2984950",
    "end": "2991309"
  },
  {
    "text": "is just a count so let give me the count of cases by continent and Africa has 624",
    "start": "2991309",
    "end": "3000220"
  },
  {
    "text": "this is not surprising because there are more African countries than there are in other places of the world and Oceania is",
    "start": "3000220",
    "end": "3009549"
  },
  {
    "text": "down here in 24 that is Australia in New Zealand at least in this data set again",
    "start": "3009549",
    "end": "3016569"
  },
  {
    "text": "I'm not going to get into details on how we label data I want to group by",
    "start": "3016569",
    "end": "3021700"
  },
  {
    "text": "continent and I want to summarize by the average life expectancy across the entire data set so I use the mean",
    "start": "3021700",
    "end": "3030700"
  },
  {
    "text": "function to get that I can also get the median life expectancy or any other aggregation function that I that I could",
    "start": "3030700",
    "end": "3037450"
  },
  {
    "text": "think of Australia New Zealand combined to have the highest average life expectancy in the overall data set I can",
    "start": "3037450",
    "end": "3046599"
  },
  {
    "text": "then say you know I don't think it's that fair to talk about life expectancies in 1952 versus 2007 maybe",
    "start": "3046599",
    "end": "3054430"
  },
  {
    "text": "we want to filter them out and look at slices in time because that's more meaningful",
    "start": "3054430",
    "end": "3059489"
  },
  {
    "text": "during the slice of time we see average life expectancies per continent and then",
    "start": "3059489",
    "end": "3065200"
  },
  {
    "text": "in 2007 you can see that Africa had jumped about 15 years Oceania had jumped",
    "start": "3065200",
    "end": "3071380"
  },
  {
    "text": "about 11 years so there was some catching up in terms of average life expectancy but I'll cut",
    "start": "3071380",
    "end": "3078400"
  },
  {
    "text": "all continents across the board increased here's a little bit more",
    "start": "3078400",
    "end": "3084810"
  },
  {
    "text": "detailed data so I have selecting certain values and I'm grouping it by",
    "start": "3084810",
    "end": "3090160"
  },
  {
    "text": "continent and country then I'm building this new column life expectancy Delta",
    "start": "3090160",
    "end": "3096370"
  },
  {
    "text": "change and life expectancy so it's this life expectancy minus the lag life expectancy in other words the",
    "start": "3096370",
    "end": "3103570"
  },
  {
    "text": "previous in my group and then I want to summarize that by finding the worst life",
    "start": "3103570",
    "end": "3110500"
  },
  {
    "text": "expectancy change and getting the the first value in that in that regard for",
    "start": "3110500",
    "end": "3116440"
  },
  {
    "text": "each continent so Rwanda had a 20-year",
    "start": "3116440",
    "end": "3122260"
  },
  {
    "text": "drop in life expectancy during their genocide Cambodia had a nine-year drop during there's al salvador montenegro",
    "start": "3122260",
    "end": "3128950"
  },
  {
    "text": "strain interestingly I guess it's not strange it's it's a positive Australian",
    "start": "3128950",
    "end": "3134140"
  },
  {
    "text": "New Zealand have never had a five-year drop in life expectancy the worst case was a very slight increase so if you",
    "start": "3134140",
    "end": "3142390"
  },
  {
    "text": "don't want to be depressed you can go look on the opposite side we could say how about we get the largest life",
    "start": "3142390",
    "end": "3150160"
  },
  {
    "text": "expectancy change Cambodia and Rwanda had big jumps after their genocides",
    "start": "3150160",
    "end": "3157660"
  },
  {
    "text": "ended New Zealand was the biggest jump in Oceania with two years so everywhere",
    "start": "3157660",
    "end": "3164020"
  },
  {
    "text": "else there were big increases big decreases but this has been a very consistent data set for Oceania and",
    "start": "3164020",
    "end": "3171780"
  },
  {
    "text": "single biggest gains in GDP by continent so again I take GDP per capita and I lag",
    "start": "3171780",
    "end": "3178390"
  },
  {
    "text": "it and I get the max so this is all the same pattern Asia Kuwait had a huge jump",
    "start": "3178390",
    "end": "3187000"
  },
  {
    "text": "when they found out that they have a lot of oil Libya had a big jump Trinidad and",
    "start": "3187000",
    "end": "3193630"
  },
  {
    "text": "Tobago had a big jump I don't I don't get that one I looked into it and I said I still don't get it",
    "start": "3193630",
    "end": "3199859"
  },
  {
    "text": "not surprisingly there's been a slow but consistent growth in Australia New",
    "start": "3200130",
    "end": "3205540"
  },
  {
    "text": "Zealand never any decreases always consist stantly slow increases or normal",
    "start": "3205540",
    "end": "3211720"
  },
  {
    "text": "increases so in that regard it's doing pretty well for Australia in New Zealand so let's spend the last few minutes and",
    "start": "3211720",
    "end": "3219960"
  },
  {
    "start": "3216000",
    "end": "3600000"
  },
  {
    "text": "look back at the data set the Raleigh crime data set while I do this I am going to start",
    "start": "3219960",
    "end": "3226960"
  },
  {
    "text": "running this query so that it'll be done by the time I'm done talking about this",
    "start": "3226960",
    "end": "3232270"
  },
  {
    "text": "stuff basically we have looked at using ty DRD plier on some academic examples",
    "start": "3232270",
    "end": "3239380"
  },
  {
    "text": "and now I want to tie it back to the crime data set that I had before and perform some fairly simplistic",
    "start": "3239380",
    "end": "3246720"
  },
  {
    "text": "analysis to try to check data cleanliness nope I did not",
    "start": "3246720",
    "end": "3254440"
  },
  {
    "text": "vamp long enough but that's okay we'll talk about this this is my query from incident oh hey I've finished the way",
    "start": "3254440",
    "end": "3261250"
  },
  {
    "text": "that I could tell it finished is if this dot is dark color that means it's doing something it is currently uh white so didn't do",
    "start": "3261250",
    "end": "3268270"
  },
  {
    "text": "anything all right from incident joined incident code joined incident type I have my beat code description type",
    "start": "3268270",
    "end": "3275080"
  },
  {
    "text": "incident date and incident number have all of that data in this set and that",
    "start": "3275080",
    "end": "3280900"
  },
  {
    "text": "gives me four hundred and thirteen thousand observations of those six variables I have some basic data cleanup",
    "start": "3280900",
    "end": "3287560"
  },
  {
    "text": "here where I'm changing data types making certain values character and others I'm making numeric specifically",
    "start": "3287560",
    "end": "3295240"
  },
  {
    "text": "I'm adding some new variables incident year month and day and those are integer values I have a function here called",
    "start": "3295240",
    "end": "3304240"
  },
  {
    "text": "complete cases this is an R and what it does is it will tell you if you have any",
    "start": "3304240",
    "end": "3309910"
  },
  {
    "text": "nulls in your in your record so if I just run complete cases like this rally",
    "start": "3309910",
    "end": "3316900"
  },
  {
    "text": "2014 it will give me back a true or false value for each observation what",
    "start": "3316900",
    "end": "3323710"
  },
  {
    "text": "I'm doing here is I'm saying filter out any of the records where complete cases returns false so in other",
    "start": "3323710",
    "end": "3330190"
  },
  {
    "text": "words if I have any nulls throw out that row turns out I don't have any Naz we",
    "start": "3330190",
    "end": "3337000"
  },
  {
    "text": "took care of all of that in the original data cleansing in sequel server so I",
    "start": "3337000",
    "end": "3342340"
  },
  {
    "text": "Group the data in a couple different ways using the group by and summarize functions I want to get by year and by",
    "start": "3342340",
    "end": "3349569"
  },
  {
    "text": "incident type that way I can plot incidents by year and what I see is",
    "start": "3349569",
    "end": "3355710"
  },
  {
    "text": "fairly consistent year by year a little bit over 40,000 incidents each year",
    "start": "3355710",
    "end": "3360819"
  },
  {
    "text": "until 2014 when there's a big drop that's not because people stop",
    "start": "3360819",
    "end": "3365950"
  },
  {
    "text": "committing crimes in 2014 it's because I took the data in the middle of the year so 2014 is probably not a great year for",
    "start": "3365950",
    "end": "3373180"
  },
  {
    "text": "our analysis I can skip 2014 when it",
    "start": "3373180",
    "end": "3379210"
  },
  {
    "text": "comes to looking at year by year comparisons next up do a histogram of",
    "start": "3379210",
    "end": "3384819"
  },
  {
    "text": "incidents by incident type and what we see is this is how many times an",
    "start": "3384819",
    "end": "3391569"
  },
  {
    "text": "incident occurs in our data set there's a couple that occur very frequently there are a couple that occur pretty",
    "start": "3391569",
    "end": "3398319"
  },
  {
    "text": "frequently and there are a bunch that are less frequent so we can see what are",
    "start": "3398319",
    "end": "3404170"
  },
  {
    "text": "the most frequent incidents larceny and miscellaneous we can see what's between",
    "start": "3404170",
    "end": "3410440"
  },
  {
    "text": "20 and 80 thousand incidents assault drugs burglary and everything else",
    "start": "3410440",
    "end": "3416969"
  },
  {
    "text": "taking these I can focus in on these particular incident types and say I",
    "start": "3416969",
    "end": "3422349"
  },
  {
    "text": "would like to put up a box and whisker plots I want to see how these change on",
    "start": "3422349",
    "end": "3427779"
  },
  {
    "text": "a month-to-month basis so filtering where incident years less than 2014",
    "start": "3427779",
    "end": "3434519"
  },
  {
    "text": "grouping it by type year and incident month giving me the count",
    "start": "3434519",
    "end": "3440880"
  },
  {
    "text": "and also tell me about these popular",
    "start": "3440880",
    "end": "3445960"
  },
  {
    "text": "incidents where I have at least 20,000 of this incident I have these data sets and I want to",
    "start": "3445960",
    "end": "3452289"
  },
  {
    "text": "merge them together this is the equivalent of joining so merge using Raleigh instance by type and month",
    "start": "3452289",
    "end": "3458619"
  },
  {
    "text": "joined two popular incidents on this column incident type I take those two",
    "start": "3458619",
    "end": "3464890"
  },
  {
    "text": "together and I can build them in a box plot and what this shows is a few very",
    "start": "3464890",
    "end": "3471549"
  },
  {
    "text": "interesting measures the median 75th and 25th percentiles and",
    "start": "3471549",
    "end": "3476590"
  },
  {
    "text": "the data point that is at least the first data point that is least one point",
    "start": "3476590",
    "end": "3481630"
  },
  {
    "text": "five times the interquartile range away from the median if I scroll up and down",
    "start": "3481630",
    "end": "3489250"
  },
  {
    "text": "a little bit I also see these little dots here are outliers there aren't many",
    "start": "3489250",
    "end": "3494530"
  },
  {
    "text": "outliers in this data set we're in a fairly consistent band in other words so",
    "start": "3494530",
    "end": "3501520"
  },
  {
    "text": "different incident types happen pretty consistently from month to month but if",
    "start": "3501520",
    "end": "3506710"
  },
  {
    "text": "I group it by beat I get to see a slightly different result here's our cut",
    "start": "3506710",
    "end": "3514660"
  },
  {
    "text": "off everything cuts off at zero we can't have less than zero incidents but notice",
    "start": "3514660",
    "end": "3521500"
  },
  {
    "text": "that we start to see some actual outliers including these two up here",
    "start": "3521500",
    "end": "3526690"
  },
  {
    "text": "that are way higher than everybody else so something is going on in this",
    "start": "3526690",
    "end": "3531970"
  },
  {
    "text": "miscellaneous category in two particular locations and we can dig into that we",
    "start": "3531970",
    "end": "3537730"
  },
  {
    "text": "can figure out what's going on here I'm gonna spoil it for you turns out that",
    "start": "3537730",
    "end": "3543100"
  },
  {
    "text": "those are near a mental health facility and those were all related to mental",
    "start": "3543100",
    "end": "3548350"
  },
  {
    "text": "health incidents where a police officer was involved in taking somebody to or from a mental health facility I can also",
    "start": "3548350",
    "end": "3558760"
  },
  {
    "text": "filter by other categories so in this case we have prostitution and again we",
    "start": "3558760",
    "end": "3565720"
  },
  {
    "text": "have a couple of major outliers and should you be inclined to investigate",
    "start": "3565720",
    "end": "3571510"
  },
  {
    "text": "you could figure out well is it that this is where all the prostitution is or is it that this is where the police are",
    "start": "3571510",
    "end": "3577810"
  },
  {
    "text": "looking for it serious question to which I don't have an answer",
    "start": "3577810",
    "end": "3583510"
  },
  {
    "text": "wrapping up with QQ plots basically a QQ plot in this case I'm asking the",
    "start": "3583510",
    "end": "3589540"
  },
  {
    "text": "question is this data normal in the sense that it follows a normal",
    "start": "3589540",
    "end": "3595360"
  },
  {
    "text": "distribution and I'm building up a normal distribution and applying a QQ",
    "start": "3595360",
    "end": "3601660"
  },
  {
    "text": "norm and Q cube line the idea is that if your data set follows this line",
    "start": "3601660",
    "end": "3608140"
  },
  {
    "text": "then it will be it'll show you that yeah your data is fairly normally distributed our data set falls off of the line so",
    "start": "3608140",
    "end": "3616360"
  },
  {
    "text": "there's this big fall off at the end and that tells us that no it is not normally distributed and in fact I could go and",
    "start": "3616360",
    "end": "3623770"
  },
  {
    "text": "look at the box plot up above and tell you this is not going to be normally distributed because you have a hard stop",
    "start": "3623770",
    "end": "3629620"
  },
  {
    "text": "at zero so if you can't have less than zero you can't go out infinitely long on",
    "start": "3629620",
    "end": "3636340"
  },
  {
    "text": "both sides then your data is not going to be normally distributed and here's a",
    "start": "3636340",
    "end": "3642250"
  },
  {
    "text": "QQ plot somebody who knows a lot more about QQ plots than I do could look at",
    "start": "3642250",
    "end": "3647350"
  },
  {
    "text": "that and explain the same thing that I did oh the distribution here yeah that",
    "start": "3647350",
    "end": "3652780"
  },
  {
    "text": "you're falling off here means that you're going to end up being capped on the left-hand side so over the course of",
    "start": "3652780",
    "end": "3661990"
  },
  {
    "text": "the last hour and we've looked at a variety of data cleansing techniques from here you could look at data quality",
    "start": "3661990",
    "end": "3667990"
  },
  {
    "text": "services or look at something that's better than that you could integrate with external api's",
    "start": "3667990",
    "end": "3673590"
  },
  {
    "text": "so for example if you're looking up a postal address or if you're looking up a UPC or hitting Amazon and pulling back a",
    "start": "3673590",
    "end": "3682470"
  },
  {
    "text": "manufacturer's number and continuing along into other analyses Benford's law",
    "start": "3682470",
    "end": "3688450"
  },
  {
    "text": "is one of my favorites it is around the distribution of the first digit of of",
    "start": "3688450",
    "end": "3694030"
  },
  {
    "text": "data sets I have a link to that in the slides here as well if you want to get",
    "start": "3694030",
    "end": "3700060"
  },
  {
    "text": "more details so slides if you want the code if you'd like additional examples it's all at CS more info slash on slash",
    "start": "3700060",
    "end": "3707380"
  },
  {
    "text": "cleansing if you have any questions at all please feel free to reach out to me email address Twitter handle I'm also here all week so thanks",
    "start": "3707380",
    "end": "3714340"
  },
  {
    "text": "everybody [Applause] you",
    "start": "3714340",
    "end": "3719900"
  }
]