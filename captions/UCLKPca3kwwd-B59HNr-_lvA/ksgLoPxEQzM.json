[
  {
    "text": "[Music]",
    "start": "350",
    "end": "13080"
  },
  {
    "text": "all right well hi everyone thanks so much for joining us today um so today I",
    "start": "13080",
    "end": "18880"
  },
  {
    "text": "get the great opportunity of introducing Sova and some of our capabilities around",
    "start": "18880",
    "end": "24439"
  },
  {
    "text": "reaching over a th000 tokens per second using llama 3 today I'm going to spend a",
    "start": "24439",
    "end": "30160"
  },
  {
    "text": "little bit of time getting you oriented around Sova some of the capabilities",
    "start": "30160",
    "end": "35640"
  },
  {
    "text": "that we provide as an AI platform and also some of the underlying technologies that are providing the means of",
    "start": "35640",
    "end": "41960"
  },
  {
    "text": "achieving some of the accomplishments like a th000 tokens per second before I start jumping into the content I want to",
    "start": "41960",
    "end": "48600"
  },
  {
    "text": "take the opportunity to introduce some of my colleagues really quick so joining me today I have Petro Milan who is a",
    "start": "48600",
    "end": "55879"
  },
  {
    "text": "principal AI engineer he's going to also be leading our Workshop component and be here as we're getting handson with the",
    "start": "55879",
    "end": "62800"
  },
  {
    "text": "technology I also have Veron Krishna who is a senior uh senior principal AI",
    "start": "62800",
    "end": "68280"
  },
  {
    "text": "Solutions engineer uh joining me as well and I'm Michelle matern I'm our director",
    "start": "68280",
    "end": "73680"
  },
  {
    "text": "of solutions engineering I serve our Global customer base at s NOA so before jumping into our content I",
    "start": "73680",
    "end": "81640"
  },
  {
    "text": "just want to cover what are we going to be talking about today so we're going to start off with a little bit of housekeeping uh talk a little bit about",
    "start": "81640",
    "end": "87680"
  },
  {
    "text": "the prerequisites we are going to be getting hands on on today and also introduce our Discord Channel which is",
    "start": "87680",
    "end": "92840"
  },
  {
    "text": "where we're going to be communicating with one another and also sharing some content and information that you're",
    "start": "92840",
    "end": "97960"
  },
  {
    "text": "going to need such as files and and other uh like API and uh keys and things like that um I'm going to talk about",
    "start": "97960",
    "end": "104680"
  },
  {
    "text": "Sova just get you oriented around who are we and how are we achieving a th000",
    "start": "104680",
    "end": "110200"
  },
  {
    "text": "tokens per second um then I'm going to pass it off to Petro he's going to uh talk you through our Workshop today how",
    "start": "110200",
    "end": "117000"
  },
  {
    "text": "we're going to get handson get you oriented around how to get started uh and we're actually",
    "start": "117000",
    "end": "122479"
  },
  {
    "text": "going to go through a live build and we'll support you along the way uh we'll also spend some time around questions uh",
    "start": "122479",
    "end": "128479"
  },
  {
    "text": "just in case that um you have any questions about Sova our technology or anything that we're doing in the Hands-On",
    "start": "128479",
    "end": "134879"
  },
  {
    "text": "component so before we get started I want to talk a little bit about prerequisites so first of all we are going to be using our laptops so",
    "start": "134879",
    "end": "141680"
  },
  {
    "text": "hopefully you have them today uh and we're also going to need internet access to get through our Workshop uh we're",
    "start": "141680",
    "end": "146959"
  },
  {
    "text": "going to be working in a python environment um so hopefully we have python set up and ready um and also",
    "start": "146959",
    "end": "152599"
  },
  {
    "text": "we're going to be needing to install some packages uh through pip um we are going to be working in Discord so if you",
    "start": "152599",
    "end": "159560"
  },
  {
    "text": "don't mind uh I'm going to give everybody just a second to hopefully get on Discord and um and join our channel",
    "start": "159560",
    "end": "167080"
  },
  {
    "text": "so I'll just get everyone a quick moment to to get set",
    "start": "167080",
    "end": "171640"
  },
  {
    "text": "up once you're set up just maybe give me a thumbs up so so I know",
    "start": "173040",
    "end": "179840"
  },
  {
    "text": "question time to build capital t uh capital T capital B the general one",
    "start": "189360",
    "end": "197159"
  },
  {
    "text": "not the one that says speaker okay engineer okay so just to repeat uh it's",
    "start": "197159",
    "end": "202480"
  },
  {
    "text": "AI engineer the password is time to build with a capital t and a capital B",
    "start": "202480",
    "end": "207959"
  },
  {
    "text": "no no capital T capital T capital okay all it it's all capitalize each word but",
    "start": "207959",
    "end": "215000"
  },
  {
    "text": "concatenated",
    "start": "215000",
    "end": "218000"
  },
  {
    "text": "okay good were we able to join okay awesome cool just want to make sure there's no",
    "start": "221680",
    "end": "228120"
  },
  {
    "text": "problems awesome we'll give everyone a couple minutes so we' just got Wi-Fi access",
    "start": "228120",
    "end": "234920"
  },
  {
    "text": "all right um in case you haven't got a chance to set up maybe just take a picture of this really quick we'll also",
    "start": "255439",
    "end": "260919"
  },
  {
    "text": "go back to it um before we kick off the hands on component um but want to spend",
    "start": "260919",
    "end": "266960"
  },
  {
    "text": "a little bit of time just getting you oriented around us as somova so somova",
    "start": "266960",
    "end": "272320"
  },
  {
    "text": "we are a full stack AI platform and we've existed since 2017 uh we were",
    "start": "272320",
    "end": "277720"
  },
  {
    "text": "founded out of Stanford University so two of our uh co-founders are actually",
    "start": "277720",
    "end": "283639"
  },
  {
    "text": "Stanford professors uh kle and also Chris um both of them and including",
    "start": "283639",
    "end": "289720"
  },
  {
    "text": "Rodrigo each bring a unique perspective to our founding team uh including previous uh startups that were uh",
    "start": "289720",
    "end": "296960"
  },
  {
    "text": "Acquired and had uh various exits uh also building other AI startups that are",
    "start": "296960",
    "end": "302360"
  },
  {
    "text": "pretty well known in the industry such as snorkel together AI um and also a really uh depth of experience around",
    "start": "302360",
    "end": "309600"
  },
  {
    "text": "building out hardware and chips um we are uh building the full stack from the",
    "start": "309600",
    "end": "314960"
  },
  {
    "text": "ground up so that means we build our own chip um I'll go into that a little bit later later but also all the way through",
    "start": "314960",
    "end": "320919"
  },
  {
    "text": "the system level and the software layer um we're on our fourth generation stick",
    "start": "320919",
    "end": "326360"
  },
  {
    "text": "uh chip um and we have built a entire stack that allows you to both fine-tune",
    "start": "326360",
    "end": "331600"
  },
  {
    "text": "models pre-train models and also deploy those models with really high performant",
    "start": "331600",
    "end": "337160"
  },
  {
    "text": "inference um we have achieved over a billion dollars in funding from various",
    "start": "337160",
    "end": "342639"
  },
  {
    "text": "well-known names like black rock or Google Ventures Intel",
    "start": "342639",
    "end": "347880"
  },
  {
    "text": "GIC um and so we are really wellestablished to solve the challenge",
    "start": "347880",
    "end": "352960"
  },
  {
    "text": "around building and deploying AI Hardware so what exactly are we",
    "start": "352960",
    "end": "358840"
  },
  {
    "text": "targeting and what exactly are we trying to solve for our customer base comes from a wide variety of Enterprises and",
    "start": "358840",
    "end": "366280"
  },
  {
    "text": "also to government organizations so we're really aiming to deliver capabilities that can help service the",
    "start": "366280",
    "end": "372759"
  },
  {
    "text": "Enterprise grade AI capabilities that companies and governments require to",
    "start": "372759",
    "end": "377840"
  },
  {
    "text": "deliver um unique and differentiated capabilities and also things like Sovereign AI um our our underlying",
    "start": "377840",
    "end": "385479"
  },
  {
    "text": "platform is delivering the means to actually achieve the scale of a trillion",
    "start": "385479",
    "end": "390639"
  },
  {
    "text": "parameters uh plus um and we're doing that through delivering full stack",
    "start": "390639",
    "end": "395919"
  },
  {
    "text": "capabilities when I say full stack um many folks have probably utilized many",
    "start": "395919",
    "end": "401160"
  },
  {
    "text": "of these Technologies on uh this slide we're not necessarily trying to compete with every single layer uh involved here",
    "start": "401160",
    "end": "407720"
  },
  {
    "text": "but what we are trying to do is ease the process of getting started and ease the journey along the way and so instead of",
    "start": "407720",
    "end": "414400"
  },
  {
    "text": "having to make a decision at every single one of these layers we are actually integrating things into a very",
    "start": "414400",
    "end": "420720"
  },
  {
    "text": "seamless experience from deciding on what Chip is going to work with uh what",
    "start": "420720",
    "end": "426280"
  },
  {
    "text": "compute what compute and Chip is going to work with what operation systems what",
    "start": "426280",
    "end": "432000"
  },
  {
    "text": "operation system works with what models you don't have to actually make each of these decisions and know that they have",
    "start": "432000",
    "end": "437800"
  },
  {
    "text": "to integrate with one another we actually create this very seamless experience along the way um where",
    "start": "437800",
    "end": "443680"
  },
  {
    "text": "everything kind of orchestrates and works very nicely and what we're doing at the end",
    "start": "443680",
    "end": "449960"
  },
  {
    "text": "of this is actually delivering the capability to not only F tune but deliver really really fast inference",
    "start": "449960",
    "end": "455840"
  },
  {
    "text": "capabilities so we're going to demo a little bit of this later but uh recently we released a a a demo um that you can",
    "start": "455840",
    "end": "463440"
  },
  {
    "text": "actually go try live and we'll do so later called one turbo this is uh exceeding World Records around uh speed",
    "start": "463440",
    "end": "470319"
  },
  {
    "text": "of inference especially when it comes to llama 3 um and you can see that through",
    "start": "470319",
    "end": "475440"
  },
  {
    "text": "some of the metrics that were recently published um through uh artificial analysis artificial analysis did a um a",
    "start": "475440",
    "end": "483960"
  },
  {
    "text": "benchmarking exercise to understand the different capabilities uh the speed at",
    "start": "483960",
    "end": "489360"
  },
  {
    "text": "which they are able to deliver inference throughput um for a thousand tokens per",
    "start": "489360",
    "end": "494720"
  },
  {
    "text": "second across various Hardware providers and what you can see is that we are far out exceeding as a platform um the the",
    "start": "494720",
    "end": "502599"
  },
  {
    "text": "throughput capabilities compared to some of the other other providers out there and so I want to to talk a little",
    "start": "502599",
    "end": "510080"
  },
  {
    "text": "bit about how are we enabling such speed um so when it comes to the underlying",
    "start": "510080",
    "end": "515120"
  },
  {
    "text": "technology many of us have experienced some of these Trends in the industry many of us got started with what you see",
    "start": "515120",
    "end": "522240"
  },
  {
    "text": "on the right hand side of the slide which is the large monolithic model this is the likes of like an open AI for",
    "start": "522240",
    "end": "528600"
  },
  {
    "text": "example or a gemini or a cloud and many of us started our llm journey or our",
    "start": "528600",
    "end": "533640"
  },
  {
    "text": "generative AI Journey using some of these Technologies but along the way many uh many other capabilities in the",
    "start": "533640",
    "end": "540440"
  },
  {
    "text": "open source Community started to pop up specifically these smaller models and these smaller models allowed us to do",
    "start": "540440",
    "end": "546440"
  },
  {
    "text": "things like fine tuning and actually adapting some of these models to our Enterprise data and our Enterprise",
    "start": "546440",
    "end": "552600"
  },
  {
    "text": "requirements and so that really started to take off in the industry and each of these started to see different pros and",
    "start": "552600",
    "end": "558880"
  },
  {
    "text": "cons associated with them when it came to large monolithic models when we actually started to put these into",
    "start": "558880",
    "end": "565000"
  },
  {
    "text": "practice when it came to Enterprise applications one of the reasons many of us lean into this is because of the",
    "start": "565000",
    "end": "571040"
  },
  {
    "text": "broad capabilities that the likes of openi brought right and also the ease of integration in terms of open AI into the",
    "start": "571040",
    "end": "578360"
  },
  {
    "text": "actual platform itself it's super easy to manage and it also was trained on the internet's data and so it can handle a",
    "start": "578360",
    "end": "584480"
  },
  {
    "text": "lot of different things but when it came to actual Enterprise applications Enterprises have unique capabilities",
    "start": "584480",
    "end": "591120"
  },
  {
    "text": "required to deliver on some of the use cases and challenges they're trying to solve for for example Enterprises have",
    "start": "591120",
    "end": "598760"
  },
  {
    "text": "unique data that they they you know Al often times segregate from the internet",
    "start": "598760",
    "end": "604440"
  },
  {
    "text": "well most of the time segregate from the internet um that's proprietary to them and oftentimes they have spent the last",
    "start": "604440",
    "end": "611600"
  },
  {
    "text": "10 years trying to actually aggregate that data into the likes of data lakes and other um uh kind of centralized",
    "start": "611600",
    "end": "618360"
  },
  {
    "text": "capabilities and so now how do you actually transform that into AI capabilities that you can leverage that",
    "start": "618360",
    "end": "624120"
  },
  {
    "text": "was very difficult when it came to large monolithic models the other challenges that we saw is that that many started to",
    "start": "624120",
    "end": "630680"
  },
  {
    "text": "become very concerned about security when it came to open AI they wanted to preserve uh data privacy they wanted to",
    "start": "630680",
    "end": "636720"
  },
  {
    "text": "also own and use the model as a differentiation for themselves um and then also as you start to see more and",
    "start": "636720",
    "end": "643399"
  },
  {
    "text": "more adoption that cost just started to Skyrocket um open the ey and and a lot",
    "start": "643399",
    "end": "648639"
  },
  {
    "text": "of these Clos Source models charge on a per token rate and so as you start to utilize more and more llms uh and",
    "start": "648639",
    "end": "655519"
  },
  {
    "text": "utilize llms uh more heavily the Costa starts to go up and up and up and up and it's really really hard to",
    "start": "655519",
    "end": "662160"
  },
  {
    "text": "control on the other hand when it came to adopting the smaller expert models or",
    "start": "662160",
    "end": "667279"
  },
  {
    "text": "the smaller open- Source models like the likes of llama 38b that we'll talk about later um we were able to address some of",
    "start": "667279",
    "end": "674399"
  },
  {
    "text": "the Enterprise accuracy concerns by actually pre-training fine-tuning these models to adapt them to the Enterprise",
    "start": "674399",
    "end": "681519"
  },
  {
    "text": "requirements um while we're doing this at a smaller scale to address like different capabilities and tasks that we",
    "start": "681519",
    "end": "688279"
  },
  {
    "text": "needed to solve for Enterprise they weren't trying to solve like a broad set of tasks and also adopt a broad set of",
    "start": "688279",
    "end": "696519"
  },
  {
    "text": "general knowledge um thus manageability became a little bit challenging because now we have all of these kind of like",
    "start": "696519",
    "end": "702639"
  },
  {
    "text": "micro models that we have to orchestrate and have them work together um but we were able to solve for some things like",
    "start": "702639",
    "end": "709160"
  },
  {
    "text": "security model ownership uh data privacy and data ownership um but again because",
    "start": "709160",
    "end": "715120"
  },
  {
    "text": "we had so many of these and we had to fine-tune each one of these the cost also became very challenging so what we",
    "start": "715120",
    "end": "721399"
  },
  {
    "text": "what are we trying to solve for uh through our capability of one",
    "start": "721399",
    "end": "726600"
  },
  {
    "text": "we're trying to bring the best of both of these uh paradigms together um to",
    "start": "726600",
    "end": "732600"
  },
  {
    "text": "deliver the the capabilities of each um in a very simplistic way and the way",
    "start": "732600",
    "end": "738720"
  },
  {
    "text": "that we actually deliver this is through four core capabilities first of all we",
    "start": "738720",
    "end": "744320"
  },
  {
    "text": "take all of those expert models behind the scenes so let's just say we fine-tuned a model for our legal",
    "start": "744320",
    "end": "750519"
  },
  {
    "text": "purposes we fine-tuned a model for our HR purposes we fine-tuned a model for",
    "start": "750519",
    "end": "757000"
  },
  {
    "text": "coding capabilities each of those have different groups and tasks and use cases",
    "start": "757000",
    "end": "762480"
  },
  {
    "text": "that are going to consume those but we want to really ease the experience of having to integrate those into the",
    "start": "762480",
    "end": "768040"
  },
  {
    "text": "application so we put all of those behind a secure single end point and so",
    "start": "768040",
    "end": "773760"
  },
  {
    "text": "you only have to interface with one endpoint to gain access to all these various models",
    "start": "773760",
    "end": "779760"
  },
  {
    "text": "now we need to determine how are we going to actually use those or consume those various models behind the scenes",
    "start": "779760",
    "end": "785560"
  },
  {
    "text": "so now we need uh capabilities around orchestration so one of the other capabilities we're delivering as a part",
    "start": "785560",
    "end": "790600"
  },
  {
    "text": "of this is around um routing so we're delivering the ability to determine based off of an incoming prompt what is",
    "start": "790600",
    "end": "798120"
  },
  {
    "text": "the best suited expert behind the scenes to solve that prompt and we're doing so",
    "start": "798120",
    "end": "803360"
  },
  {
    "text": "through a router we're also bringing the means of dynamically fine-tuning every",
    "start": "803360",
    "end": "808600"
  },
  {
    "text": "expert is is going to have a different Cadence in which fine tuning is going to make sense um so maybe your Finance",
    "start": "808600",
    "end": "815480"
  },
  {
    "text": "model gets adjusted at annually when your policies are updated but maybe your",
    "start": "815480",
    "end": "820639"
  },
  {
    "text": "coding model because you're pushing code so regularly needs to get updated on a quarterly basis and so you want to",
    "start": "820639",
    "end": "827440"
  },
  {
    "text": "actually be able to schedule your fine-tuning jobs and adjust and swap these models at the rate at it which it",
    "start": "827440",
    "end": "833440"
  },
  {
    "text": "makes sense to actually retrain these models and lastly you have a bunch of",
    "start": "833440",
    "end": "838800"
  },
  {
    "text": "model mod under the scenes not every application or group is going to or should be able to access each of those",
    "start": "838800",
    "end": "845480"
  },
  {
    "text": "models so now you need to figure out a way to uh manage the access controls for these so what we're also deliver",
    "start": "845480",
    "end": "852040"
  },
  {
    "text": "delivering as a part of this capability is model level our back so you can actually determine this application or",
    "start": "852040",
    "end": "858839"
  },
  {
    "text": "this person or this group of people should be able to access this set of models and it allows you a ton of",
    "start": "858839",
    "end": "865160"
  },
  {
    "text": "efficiency from a computation and management operation standpoint with the security and fine grain control that you",
    "start": "865160",
    "end": "871120"
  },
  {
    "text": "need to actually manage a access to these different models and data under underlying these",
    "start": "871120",
    "end": "877480"
  },
  {
    "text": "models So within one we have two ways of delivering this we have something called a flexible Coe that",
    "start": "877480",
    "end": "883720"
  },
  {
    "text": "allows you to kind of determine exactly what models Li under the hood and we also have a precomposed version of",
    "start": "883720",
    "end": "890720"
  },
  {
    "text": "one composition of experts so our pre-trained um or our pre-configured I",
    "start": "890720",
    "end": "897240"
  },
  {
    "text": "should say our pre-composed version of the this model has 92 underlying experts and when I say experts I'm really",
    "start": "897240",
    "end": "903320"
  },
  {
    "text": "referring to a a a specific model um that can bring different capabilities uh",
    "start": "903320",
    "end": "909560"
  },
  {
    "text": "associated with it and within those 92 experts we have a broad range of languages that are covered within those",
    "start": "909560",
    "end": "916040"
  },
  {
    "text": "models a broad range of domains and a diverse set of tasks that are very very",
    "start": "916040",
    "end": "921880"
  },
  {
    "text": "relevant to the Enterprise all of these models are supported by seven different Foundation",
    "start": "921880",
    "end": "927160"
  },
  {
    "text": "model architectures including l two llama 3 mistal Falcon Bloom and even",
    "start": "927160",
    "end": "932720"
  },
  {
    "text": "some multimodal capabilities such as like lava clip deep plot um that is kind",
    "start": "932720",
    "end": "937880"
  },
  {
    "text": "of starting to support some of the multimodal trends that are coming and of all these 92 models we are",
    "start": "937880",
    "end": "945279"
  },
  {
    "text": "actually delivering and partnering with organizations to um create and contribute back to the open- source",
    "start": "945279",
    "end": "952040"
  },
  {
    "text": "community so out of the 92 experts 12 of them are actually ones that we've helped",
    "start": "952040",
    "end": "957279"
  },
  {
    "text": "uh either develop ourselves or co-develop with organizations out there including some of the language",
    "start": "957279",
    "end": "963199"
  },
  {
    "text": "capabilities that we've delivered such as models that can support things like Thai or Japanese Hungarian um and",
    "start": "963199",
    "end": "970680"
  },
  {
    "text": "through that we've developed a lot of experience on how to actually adapt models to different languages um we've",
    "start": "970680",
    "end": "976959"
  },
  {
    "text": "also uh created a model for text tosql capabilities and delivered uh really",
    "start": "976959",
    "end": "983279"
  },
  {
    "text": "really good results through that model for text to SQL and lastly we have contributed back to BL Blom chat uh it's",
    "start": "983279",
    "end": "990199"
  },
  {
    "text": "the second largest open source model um and uh it's also bringing a lot of the multilingual",
    "start": "990199",
    "end": "997639"
  },
  {
    "text": "capabilities so organizations essentially as they're constructing these different composition of experts",
    "start": "997720",
    "end": "1004279"
  },
  {
    "text": "can add as many expert models as they need as I mentioned before while we have that preconfigured composition this is",
    "start": "1004279",
    "end": "1011440"
  },
  {
    "text": "really intended for you to be able to construct exactly what you need in terms of models under the hood so you can add",
    "start": "1011440",
    "end": "1017440"
  },
  {
    "text": "as many as you need so our end goal with this is to really",
    "start": "1017440",
    "end": "1023079"
  },
  {
    "text": "be able to bring the capabilities that enterprises need to um handle the diverse set of use cases and",
    "start": "1023079",
    "end": "1029760"
  },
  {
    "text": "capabilities they need to solve their problems and so one of the things that we've created along the way to measure ourselves against this is an Enterprise",
    "start": "1029760",
    "end": "1036720"
  },
  {
    "text": "grade AI benchmarking set and this is really tailored to understand our capabilities against the best and",
    "start": "1036720",
    "end": "1043199"
  },
  {
    "text": "Industry models across various uh Enterprise specific task and domains",
    "start": "1043199",
    "end": "1049080"
  },
  {
    "text": "that are needed things like information extraction that's where many many Enterprises are starting um but also",
    "start": "1049080",
    "end": "1055200"
  },
  {
    "text": "broads set of capabilities like TX tql coding function calling um and we're",
    "start": "1055200",
    "end": "1060480"
  },
  {
    "text": "measuring ourselves against GPT 3.5 turbo and GPT 4 um and what we can see",
    "start": "1060480",
    "end": "1066120"
  },
  {
    "text": "is along the way um we are meeting or exceeding um the capabilities that that",
    "start": "1066120",
    "end": "1071320"
  },
  {
    "text": "open AI is bringing um but alongside the",
    "start": "1071320",
    "end": "1076720"
  },
  {
    "text": "capabilities you also need to orchestrate these models I talked a little bit about this before but one of the deliverables as far as one uh",
    "start": "1076720",
    "end": "1083799"
  },
  {
    "text": "from a product standpoint is we're bringing routing capabilities so that you can actually uh take a prompt",
    "start": "1083799",
    "end": "1089360"
  },
  {
    "text": "determine what is the best best suited expert and then route to that um there's also scenarios where you may need to do",
    "start": "1089360",
    "end": "1096799"
  },
  {
    "text": "something outside of routing you may actually just want to directly call a specific model or in the case that is",
    "start": "1096799",
    "end": "1103200"
  },
  {
    "text": "popping up really really regularly now with agentic AI you may need to do model chaining so that's another capability",
    "start": "1103200",
    "end": "1109320"
  },
  {
    "text": "that we're bringing as a part of the product suite for",
    "start": "1109320",
    "end": "1114679"
  },
  {
    "text": "one and so how are how are we actually uniquely set up to deliver the",
    "start": "1116400",
    "end": "1121559"
  },
  {
    "text": "composition of experts capability um and also the really fast inference speed that we're going to see uh briefly so I",
    "start": "1121559",
    "end": "1128960"
  },
  {
    "text": "mentioned earlier but we are a full stack uh AI platform but we also build our own chip our chip we call that an",
    "start": "1128960",
    "end": "1135840"
  },
  {
    "text": "RDU or a reconfigurable data flow unit so instead of a GPU we'll refer to our",
    "start": "1135840",
    "end": "1141679"
  },
  {
    "text": "chip as an RDU and our current version of that chip is called sn40 L and what",
    "start": "1141679",
    "end": "1147679"
  },
  {
    "text": "is unique about sn40 L is actually the memory structure of this chip so our",
    "start": "1147679",
    "end": "1153159"
  },
  {
    "text": "chip supports a a three tier uh memory architecture so we have our onchip",
    "start": "1153159",
    "end": "1159080"
  },
  {
    "text": "memory and then we have our high bandwidth memory and then we have a huge uh memory capacity in DDR and this",
    "start": "1159080",
    "end": "1166760"
  },
  {
    "text": "really allows us to store a ton T of models um and have those models be",
    "start": "1166760",
    "end": "1172600"
  },
  {
    "text": "swapped in and out of various tiers within the memory um to achieve really",
    "start": "1172600",
    "end": "1177720"
  },
  {
    "text": "strong performance and also really really efficient compute",
    "start": "1177720",
    "end": "1183640"
  },
  {
    "text": "utilization so what does this look like um as I mentioned before we can store up",
    "start": "1184240",
    "end": "1189760"
  },
  {
    "text": "to five trillion parameters on DDR so that's like if we were to store like",
    "start": "1189760",
    "end": "1195200"
  },
  {
    "text": "three to four open AI on a single chip and then as we need to actually execute",
    "start": "1195200",
    "end": "1201919"
  },
  {
    "text": "those models they can move up the memory stack um and this allows us to one take",
    "start": "1201919",
    "end": "1207799"
  },
  {
    "text": "into consideration what models need to be used at what time and then two do so",
    "start": "1207799",
    "end": "1213120"
  },
  {
    "text": "in a really performant way because the the network connectivity between these three layers is really",
    "start": "1213120",
    "end": "1219760"
  },
  {
    "text": "tight so just to kind of go into some of the specs um our on onchip SRAM uh has 4",
    "start": "1222120",
    "end": "1229159"
  },
  {
    "text": "gigs uh our high bandwidth memory has 512 gigs and our DDR has up to 6",
    "start": "1229159",
    "end": "1234520"
  },
  {
    "text": "terabytes so um lots of memory to work with and when we think about how does",
    "start": "1234520",
    "end": "1240480"
  },
  {
    "text": "this compare to what you experience in the GPU world when you think about the number if you want to host a really",
    "start": "1240480",
    "end": "1247000"
  },
  {
    "text": "large amount of models when you have to do so on a GPU you're basically needing",
    "start": "1247000",
    "end": "1253840"
  },
  {
    "text": "to work with the memory that that is that the GPU has on chip but with us",
    "start": "1253840",
    "end": "1259000"
  },
  {
    "text": "because we have the DDR component we're able to store a huge amount of models in a single system and keep it coupled with",
    "start": "1259000",
    "end": "1265840"
  },
  {
    "text": "the other memory uh tiers and so with gpus you're often if you wanted to host 500 different models you're going to",
    "start": "1265840",
    "end": "1271960"
  },
  {
    "text": "have to actually uh align those models to various systems and you're going to have to basically call various system to",
    "start": "1271960",
    "end": "1279480"
  },
  {
    "text": "actually access each of those models with us it's going to be one underlying system um because we're able to store",
    "start": "1279480",
    "end": "1286720"
  },
  {
    "text": "again up to 5 trillion parameters",
    "start": "1286720",
    "end": "1290520"
  },
  {
    "text": "so um I'm going to hand it over to Pedro uh what we're going to do next is actually uh get the chance to get hands",
    "start": "1292000",
    "end": "1299279"
  },
  {
    "text": "yeah thanks uh relle for the presentation um so what we will be doing next is a demo of our L 3 and samb one",
    "start": "1299279",
    "end": "1308799"
  },
  {
    "text": "um turbo and after this um we will move to the uh Hands-On portion of the workshop um so if you want to try out",
    "start": "1308799",
    "end": "1316039"
  },
  {
    "text": "our Lama 3 end point um so we can go to our website uh",
    "start": "1316039",
    "end": "1321600"
  },
  {
    "text": "sambanova Ai and then uh click on uh samb one",
    "start": "1321600",
    "end": "1329279"
  },
  {
    "text": "turbo so this is where we can access um our um chat and front and here you have",
    "start": "1329279",
    "end": "1336640"
  },
  {
    "text": "um options to select um various models you know 3 um 8B 7tv our Coe mro and",
    "start": "1336640",
    "end": "1345480"
  },
  {
    "text": "even some of our um in-house um models we train ourselves like you know Singo",
    "start": "1345480",
    "end": "1351039"
  },
  {
    "text": "and others um so yeah we'll do a demo of Lama 3 um 8B and I'll ask it uh the",
    "start": "1351039",
    "end": "1358159"
  },
  {
    "text": "following question um so you know create a three-day a week workout schedule for",
    "start": "1358159",
    "end": "1364600"
  },
  {
    "text": "intermediate Fitness level let me actually redo it again you can see you",
    "start": "1364600",
    "end": "1370240"
  },
  {
    "text": "know it gets um instant uh response and then for the uh performance metric so",
    "start": "1370240",
    "end": "1376080"
  },
  {
    "text": "you can see the um insights here um so few things uh to note um so of",
    "start": "1376080",
    "end": "1382520"
  },
  {
    "text": "course we have a pretty high uh throughput which is uh a th000 tokens um",
    "start": "1382520",
    "end": "1388080"
  },
  {
    "text": "per second but that's not only um the end of the story you know we also have a pretty small uh time to First token",
    "start": "1388080",
    "end": "1395880"
  },
  {
    "text": "which is basically the um input inference time of 0.09 seconds and we also have a pretty small um end to",
    "start": "1395880",
    "end": "1403240"
  },
  {
    "text": "endend uh total inference time of 65 seconds right so with our a full stack",
    "start": "1403240",
    "end": "1409600"
  },
  {
    "text": "um platform can achieve high throughput and very small um inference time and if",
    "start": "1409600",
    "end": "1416320"
  },
  {
    "text": "just want to do a comparison let's say with um Chad GPT uh for instance um if",
    "start": "1416320",
    "end": "1422279"
  },
  {
    "text": "you ask uh the same question just copy it",
    "start": "1422279",
    "end": "1429840"
  },
  {
    "text": "um yeah you can clearly see the uh difference in",
    "start": "1430600",
    "end": "1436159"
  },
  {
    "text": "speed and the uh other another uh cool thing which uh we built so we have this",
    "start": "1436159",
    "end": "1443320"
  },
  {
    "text": "uh real time uh option where as you write your",
    "start": "1443320",
    "end": "1449400"
  },
  {
    "text": "prompt you can instantly see the model's response and as you change the prompt you can also see the change in the",
    "start": "1449400",
    "end": "1455960"
  },
  {
    "text": "response yeah so let's say I don't know hi I want to write an",
    "start": "1455960",
    "end": "1466640"
  },
  {
    "text": "email about blah blah blah right so but the point here is that you know with",
    "start": "1466640",
    "end": "1472039"
  },
  {
    "text": "this um real time option you know you can do um real time chatting and this",
    "start": "1472039",
    "end": "1477320"
  },
  {
    "text": "can be helpful for instance if you're drafting emails or even if you want to do some like real time um prompt",
    "start": "1477320",
    "end": "1483320"
  },
  {
    "text": "engineering let's say so yeah that's it for the uh samb one uh turbo I can give",
    "start": "1483320",
    "end": "1489520"
  },
  {
    "text": "maybe a few minutes for folks to try it out before we move on to the um handson um so again you go to uh our website",
    "start": "1489520",
    "end": "1497919"
  },
  {
    "text": "some Nova a and then you click on samb one turbo um so you can do it from your laptop or from your cell",
    "start": "1497919",
    "end": "1506200"
  },
  {
    "text": "phone yes question can we tweak the generation parameters like yes yes yes",
    "start": "1507000",
    "end": "1512200"
  },
  {
    "text": "yeah",
    "start": "1512200",
    "end": "1514840"
  },
  {
    "text": "uh let me see how to do it on this uh",
    "start": "1519360",
    "end": "1524919"
  },
  {
    "text": "UI okay I think uh from this UI it seems it is fixed but um in the hands on once",
    "start": "1526320",
    "end": "1532600"
  },
  {
    "text": "uh you will be calling our endpoint um from the API yeah we will be changing some of the um configs as well",
    "start": "1532600",
    "end": "1541320"
  },
  {
    "text": "so yeah any other uh question",
    "start": "1541919",
    "end": "1547600"
  },
  {
    "text": "yeah were you able to uh try it out uh okay great okay so I think um yeah we can",
    "start": "1569960",
    "end": "1576159"
  },
  {
    "text": "move on to the um handson uh portion um so what I'll do",
    "start": "1576159",
    "end": "1581440"
  },
  {
    "text": "first is just um introduce what um I'll be talking about in this part and then",
    "start": "1581440",
    "end": "1586919"
  },
  {
    "text": "uh we'll Dive In um to the uh Hands-On so yeah we prepared two uh exercises um for you",
    "start": "1586919",
    "end": "1594919"
  },
  {
    "text": "today so the first one is a basic um example to get you started um so here um",
    "start": "1594919",
    "end": "1601360"
  },
  {
    "text": "I'll be showing how you can load um environment variables set up the S NOA",
    "start": "1601360",
    "end": "1607720"
  },
  {
    "text": "API um initialize the llm and do a simple um uh inference call in",
    "start": "1607720",
    "end": "1614480"
  },
  {
    "text": "Python and the uh second um example is a more practical one so we will be uh",
    "start": "1614480",
    "end": "1621559"
  },
  {
    "text": "building and deploying a Q&A system um with rag for um Enterprise search uh",
    "start": "1621559",
    "end": "1628279"
  },
  {
    "text": "using our platform and we will also be using um other libraries and packages",
    "start": "1628279",
    "end": "1633880"
  },
  {
    "text": "like you know L chain um various data loaders uh E5 large V2 embedding um",
    "start": "1633880",
    "end": "1641440"
  },
  {
    "text": "chroma DB Vector store and of course the Lama 3 um endpoint which runs at the",
    "start": "1641440",
    "end": "1646919"
  },
  {
    "text": "speed of 1,000 token per second so yeah let's start with the basic um example and um if you want to",
    "start": "1646919",
    "end": "1656520"
  },
  {
    "text": "follow along so you can uh yeah go to Google write AI St kit",
    "start": "1656520",
    "end": "1663559"
  },
  {
    "text": "uh Sova and then uh click on this link or you can just um write this uh",
    "start": "1663559",
    "end": "1671799"
  },
  {
    "text": "URL so this is our uh stait uh repo um",
    "start": "1671799",
    "end": "1677480"
  },
  {
    "text": "we have a collection of Open Source um examples on um gen apps and um yeah once",
    "start": "1677480",
    "end": "1684960"
  },
  {
    "text": "you get there yeah you can go to uh workshops AI engineer",
    "start": "1684960",
    "end": "1692480"
  },
  {
    "text": "2024 um basic examples so I'll go over the read me and then do a live demo um",
    "start": "1692480",
    "end": "1698640"
  },
  {
    "text": "of the work of this um exercise and then I'll give you uh some time to uh try it",
    "start": "1698640",
    "end": "1704360"
  },
  {
    "text": "out so um yeah first um you'll need to clone uh this uh repo here and then uh you'll",
    "start": "1704360",
    "end": "1713399"
  },
  {
    "text": "need to create aend file um in the repo route uh directory so this is where um",
    "start": "1713399",
    "end": "1721519"
  },
  {
    "text": "we will be uh specifying the uh sambba studio um API key um yeah so let me show",
    "start": "1721519",
    "end": "1728559"
  },
  {
    "text": "you how this is done uh it's going to be yeah I guess with the two mic need to",
    "start": "1728559",
    "end": "1735880"
  },
  {
    "text": "find a way",
    "start": "1735880",
    "end": "1739320"
  },
  {
    "text": "yeah yeah so I've already cloned um the rep yeah the repo and then",
    "start": "1752519",
    "end": "1759720"
  },
  {
    "text": "uh at this level this is where you will need to create the uh dot nend file so",
    "start": "1759720",
    "end": "1765840"
  },
  {
    "text": "VI well in case yeah you'll have to do touch. EnV and",
    "start": "1765840",
    "end": "1774039"
  },
  {
    "text": "then yeah you can add uh the these information here so for the first um",
    "start": "1774919",
    "end": "1781880"
  },
  {
    "text": "handson that's the uh um only thing that you'll need um and you can access or",
    "start": "1781880",
    "end": "1788399"
  },
  {
    "text": "copy those from our um Discord uh channel so",
    "start": "1788399",
    "end": "1795398"
  },
  {
    "text": "yeah if you go to",
    "start": "1799159",
    "end": "1801880"
  },
  {
    "text": "Discord events um yeah so you can copy either um",
    "start": "1806279",
    "end": "1812880"
  },
  {
    "text": "of these keys so we have two dedicated you know end points um for this Workshop all",
    "start": "1812880",
    "end": "1819600"
  },
  {
    "text": "right and um once we finish setting up the N uh the third step is basically you",
    "start": "1819600",
    "end": "1826200"
  },
  {
    "text": "know installing the um packages so for this one you can either do it with you knowa or python environment um",
    "start": "1826200",
    "end": "1834799"
  },
  {
    "text": "so first yeah we will go to the basic examples um repo so just CD and then the",
    "start": "1834799",
    "end": "1841480"
  },
  {
    "text": "repo path then you can create um aond uh environment so I would recommend using",
    "start": "1841480",
    "end": "1848200"
  },
  {
    "text": "python um 3.10 and then you activate your uh cond",
    "start": "1848200",
    "end": "1853760"
  },
  {
    "text": "environment um and here we name it basic undor X and then yeah you just um insall the",
    "start": "1853760",
    "end": "1860440"
  },
  {
    "text": "requirements with the uh pip andall minus r um requirements and then you can",
    "start": "1860440",
    "end": "1865600"
  },
  {
    "text": "use this um line to just um link the kernel to your um",
    "start": "1865600",
    "end": "1871279"
  },
  {
    "text": "notebook okay so if I go to my",
    "start": "1871279",
    "end": "1877519"
  },
  {
    "text": "terminal yeah so we'll go to uh Workshop a i",
    "start": "1882000",
    "end": "1889440"
  },
  {
    "text": "engineer basic examples okay so this is where you can",
    "start": "1889440",
    "end": "1895320"
  },
  {
    "text": "create the cond so I've already done it um beforehand so I'm just going to activate the environment",
    "start": "1895320",
    "end": "1902919"
  },
  {
    "text": "so yeah and this is the requirements file so we only have like a few packages",
    "start": "1916279",
    "end": "1921360"
  },
  {
    "text": "that is needed yeah and that's it uh for the",
    "start": "1921360",
    "end": "1928720"
  },
  {
    "text": "installation um so once this is done yeah you should be able to open uh the notebook and again you can do it you",
    "start": "1928720",
    "end": "1935720"
  },
  {
    "text": "know from the terminal which will write you to a browser or um uh through vs",
    "start": "1935720",
    "end": "1941200"
  },
  {
    "text": "code so if you want to do it with a terminal you just write Jupiter notebook and then",
    "start": "1941200",
    "end": "1948799"
  },
  {
    "text": "the name of the notebook so it's going to be uh example uh with uh samb studio.",
    "start": "1950080",
    "end": "1958200"
  },
  {
    "text": "II uh 1B I'm actually going to do the demo via vs code just because I can show",
    "start": "1958200",
    "end": "1963360"
  },
  {
    "text": "you the uh time step so I already have this uh set",
    "start": "1963360",
    "end": "1970559"
  },
  {
    "text": "up okay so you know we are in the basic examples repo and then example with sambba studio and again if you doing it",
    "start": "1971639",
    "end": "1978399"
  },
  {
    "text": "uh with vsod just make sure that you have the uh kernel set up um so this is",
    "start": "1978399",
    "end": "1983440"
  },
  {
    "text": "a pretty basic um script so we will first uh let me just restart it yeah so",
    "start": "1983440",
    "end": "1991480"
  },
  {
    "text": "yeah we will be loading uh the libraries um so we",
    "start": "1991480",
    "end": "1999279"
  },
  {
    "text": "actually have a wrapper with L chain so this is um where uh we will be um",
    "start": "1999279",
    "end": "2005480"
  },
  {
    "text": "initializing and calling our end points the second step is to load the um",
    "start": "2005480",
    "end": "2011440"
  },
  {
    "text": "environment variable so these are actually the information which we added in the uh end",
    "start": "2011440",
    "end": "2017639"
  },
  {
    "text": "file and then uh we will initialize the um llm so yeah we will be using the",
    "start": "2017639",
    "end": "2023760"
  },
  {
    "text": "sambba studio um wrapper and uh we set the um sambba",
    "start": "2023760",
    "end": "2030399"
  },
  {
    "text": "Studio API key and then we specify the model configs I had a question earlier about um the model configs um so this is",
    "start": "2030399",
    "end": "2037279"
  },
  {
    "text": "where we can can uh set this up and play with this um again I think most of you",
    "start": "2037279",
    "end": "2042440"
  },
  {
    "text": "are familiar with these configs so you know do a sample if you set it to false this is basically a um deterministic",
    "start": "2042440",
    "end": "2049358"
  },
  {
    "text": "output if you set it to true then becomes um probabilistic you can change the temperature and also the max tokens",
    "start": "2049359",
    "end": "2057079"
  },
  {
    "text": "um to generate and also this is basically our C end point right so we have one end point through which you can",
    "start": "2057079",
    "end": "2063398"
  },
  {
    "text": "actually call um different models and in this case uh we set the x expert to uh",
    "start": "2063399",
    "end": "2068760"
  },
  {
    "text": "metal Lama 3 um HB instruct okay so I'll be running the uh",
    "start": "2068760",
    "end": "2075079"
  },
  {
    "text": "the step and yeah we have now our model loaded and now we're good to go so um",
    "start": "2075079",
    "end": "2082520"
  },
  {
    "text": "I'll first show you how you can do an inference call using a simple um invoke method in L chain um so just write llm",
    "start": "2082520",
    "end": "2091480"
  },
  {
    "text": "do invoke and then add your prompt and the prompt here is um what is the",
    "start": "2091480",
    "end": "2096520"
  },
  {
    "text": "capital of France and what you'll uh notice is it",
    "start": "2096520",
    "end": "2105720"
  },
  {
    "text": "gives the right answer but also um give you other stuff which you didn't ask for",
    "start": "2105720",
    "end": "2110880"
  },
  {
    "text": "and this is a common thing with open source um models because when you ask a prompt you need to include the the",
    "start": "2110880",
    "end": "2117320"
  },
  {
    "text": "special tags right so in the case of uh Lama 3 you can get it from a meta model",
    "start": "2117320",
    "end": "2124040"
  },
  {
    "text": "card so you'll have to actually in the prompt add these uh special tags or",
    "start": "2124040",
    "end": "2129920"
  },
  {
    "text": "tokens um in particular you have to let the llm know that this is the beginning of text and this is where you will",
    "start": "2129920",
    "end": "2137400"
  },
  {
    "text": "insert the user query right so and then let the LM know where it uh needs to um",
    "start": "2137400",
    "end": "2144160"
  },
  {
    "text": "answer and once you have the special tags um inserted um now you should be able to get the right response so this",
    "start": "2144160",
    "end": "2150800"
  },
  {
    "text": "is the first way to do the um inference call the other way is to do it via a um",
    "start": "2150800",
    "end": "2156520"
  },
  {
    "text": "LCL in nchain chain so basically um you can use the LCL to connect um a prom",
    "start": "2156520",
    "end": "2162000"
  },
  {
    "text": "template with llm and an output parser and nchain has very um templates that",
    "start": "2162000",
    "end": "2168040"
  },
  {
    "text": "you can use so um we asking the same prompt it just that the main difference",
    "start": "2168040",
    "end": "2175000"
  },
  {
    "text": "here we are adding the country as a placeholder and then when you prompt the model you can actually specify the value",
    "start": "2175000",
    "end": "2181000"
  },
  {
    "text": "of this country right yeah so that's it for the basic um",
    "start": "2181000",
    "end": "2187640"
  },
  {
    "text": "example and as I said this is just to get you started so um yeah we can spend",
    "start": "2187640",
    "end": "2192839"
  },
  {
    "text": "10 minutes for you guys to try it out U me Baron and orelle will be here to help",
    "start": "2192839",
    "end": "2199200"
  },
  {
    "text": "and um yeah then you can move on to the second um",
    "start": "2199200",
    "end": "2204839"
  },
  {
    "text": "exercise I use if you're",
    "start": "2206760",
    "end": "2211640"
  },
  {
    "text": "yeah I'll move around if also people have",
    "start": "2224280",
    "end": "2228079"
  },
  {
    "text": "questions okay cool I think uh many of you were able to try out this simple um",
    "start": "2236319",
    "end": "2242760"
  },
  {
    "text": "exercise so yeah we'll move now to the uh second uh one everyone want",
    "start": "2242760",
    "end": "2248630"
  },
  {
    "text": "[Music] to um",
    "start": "2248630",
    "end": "2254280"
  },
  {
    "text": "yeah yeah so and as I said earlier this is going to be a uh Q&A uh system um",
    "start": "2261640",
    "end": "2269280"
  },
  {
    "text": "with Rag and uh if you want to follow along um again from the same repo uh go to",
    "start": "2269280",
    "end": "2276839"
  },
  {
    "text": "Workshop AI engineer 2024 and then eqr Rag and",
    "start": "2276839",
    "end": "2283440"
  },
  {
    "text": "like the previous exercise I'll go through the readme do a live demo of the installation and the Run setup and then",
    "start": "2283440",
    "end": "2290240"
  },
  {
    "text": "uh give you some time to um try it out and the uh app here we have two versions",
    "start": "2290240",
    "end": "2296760"
  },
  {
    "text": "of it one with a Jupiter notebook and the other one um with a streamlet which",
    "start": "2296760",
    "end": "2302359"
  },
  {
    "text": "is a UI based and before I uh jump into the Hands-On just wanted to give a brief",
    "start": "2302359",
    "end": "2310200"
  },
  {
    "text": "um overview of what rag is although I'm sure many of you already know uh this",
    "start": "2310200",
    "end": "2316319"
  },
  {
    "text": "concept but just for um completeness so yeah so rag is a technique that um we can use to",
    "start": "2316319",
    "end": "2322880"
  },
  {
    "text": "supplement um llm with um additional information from um various sources to",
    "start": "2322880",
    "end": "2328839"
  },
  {
    "text": "improve the model's response and rag is very helpful um if you want to use an",
    "start": "2328839",
    "end": "2334599"
  },
  {
    "text": "off-the-shelf llm to ask a question um Beyond its uh training data or if you",
    "start": "2334599",
    "end": "2341040"
  },
  {
    "text": "want to um have the llm access to up toate information without um retraining",
    "start": "2341040",
    "end": "2348079"
  },
  {
    "text": "it also in aaq can help reduce um hallucinations in some uh context and",
    "start": "2348079",
    "end": "2354119"
  },
  {
    "text": "the typical uh rag workflow um consist",
    "start": "2354119",
    "end": "2360200"
  },
  {
    "text": "of yeah the uh following steps so we first have um document",
    "start": "2360200",
    "end": "2368599"
  },
  {
    "text": "loading and parsing so this is where um we can use a data loader to actually",
    "start": "2368599",
    "end": "2374319"
  },
  {
    "text": "load the data into a um digital text that we can edit and format um and you know various data",
    "start": "2374319",
    "end": "2381240"
  },
  {
    "text": "loaders are um available depending on the um extension of the file you're",
    "start": "2381240",
    "end": "2386800"
  },
  {
    "text": "using so in a PDF text um PowerPoint after this we have a splitting step so this is where um we",
    "start": "2386800",
    "end": "2394200"
  },
  {
    "text": "will be splitting the document into um smaller chunks and you know the chunk",
    "start": "2394200",
    "end": "2399440"
  },
  {
    "text": "size and the overlap all of these are um hyper prams and the uh next step is um",
    "start": "2399440",
    "end": "2406800"
  },
  {
    "text": "vectorization so this is where um we will be using a um in Bedding model like",
    "start": "2406800",
    "end": "2412599"
  },
  {
    "text": "E5 large V2 to map each uh chunk to a a numerical",
    "start": "2412599",
    "end": "2418000"
  },
  {
    "text": "vector and um we can store you know the vectors along with the content and the",
    "start": "2418000",
    "end": "2424960"
  },
  {
    "text": "metadata in a vector store like phas and chroma DB and today um we will be using",
    "start": "2424960",
    "end": "2430280"
  },
  {
    "text": "um chroma DB which is um open source and again the whole uh goal of this",
    "start": "2430280",
    "end": "2436520"
  },
  {
    "text": "embedding is that it allows us to do um like semantic similarity and uh semantic",
    "start": "2436520",
    "end": "2443599"
  },
  {
    "text": "search and in the retrieval step um this is where we ask uh the question which is",
    "start": "2443599",
    "end": "2449560"
  },
  {
    "text": "going to also be embedded into the vector um space and then we have a retriever which is going to um retrieve",
    "start": "2449560",
    "end": "2456760"
  },
  {
    "text": "the closest uh chunk vectors to the query Vector According to some",
    "start": "2456760",
    "end": "2462440"
  },
  {
    "text": "similarity metric and we can also add a ranker which can rank the retrieved uh",
    "start": "2462440",
    "end": "2470560"
  },
  {
    "text": "chunks um per uh relevance and also remove some of the um unnecessary chunks",
    "start": "2470560",
    "end": "2476960"
  },
  {
    "text": "and the last step is basically Q&A um generation so this is where we provide the llm um with the query and the final",
    "start": "2476960",
    "end": "2485319"
  },
  {
    "text": "retrieved uh chunk to get the grounded",
    "start": "2485319",
    "end": "2490640"
  },
  {
    "text": "response and um yeah maybe also would like to precise that um in this exercise",
    "start": "2490640",
    "end": "2496720"
  },
  {
    "text": "so the we will be using third party tools for document loading splitting and",
    "start": "2496720",
    "end": "2502119"
  },
  {
    "text": "uh storage for the in bending model um you can either run it on CPU or on our",
    "start": "2502119",
    "end": "2508200"
  },
  {
    "text": "hardware and we'll be doing both to show the differences and for the llm part this is going to be done on our um",
    "start": "2508200",
    "end": "2516240"
  },
  {
    "text": "Hardware so yeah that's it for the uh overview I think we can move on to the",
    "start": "2516240",
    "end": "2522040"
  },
  {
    "text": "uh read me um yeah so we first uh clone uh the repo I think if you've done the",
    "start": "2522040",
    "end": "2528560"
  },
  {
    "text": "other um exercise then you don't have to do um the step um same thing um after",
    "start": "2528560",
    "end": "2534319"
  },
  {
    "text": "this we will set up the um environment um variable um so for this one we will be",
    "start": "2534319",
    "end": "2541160"
  },
  {
    "text": "using uh sambba studio um API for the llm and also",
    "start": "2541160",
    "end": "2547800"
  },
  {
    "text": "uh we will be using a embedding um API as well so both are available on",
    "start": "2547800",
    "end": "2555520"
  },
  {
    "text": "Discord yeah let me show you uh in the",
    "start": "2555520",
    "end": "2560640"
  },
  {
    "text": "terminal yeah and actually yeah I would also recommend to deactivate your previous um",
    "start": "2561480",
    "end": "2568720"
  },
  {
    "text": "environment okay so we go to the ekr rag repo um yeah actually for the dot and",
    "start": "2570599",
    "end": "2577960"
  },
  {
    "text": "file this one you'll have to quit it um at this level the AI s kit okay so for",
    "start": "2577960",
    "end": "2584160"
  },
  {
    "text": "this one we will need the ined endpoint and API key and also the uh Samba studio",
    "start": "2584160",
    "end": "2591040"
  },
  {
    "text": "um endpoint and API key all right then we go back to the",
    "start": "2591040",
    "end": "2599400"
  },
  {
    "text": "uh e r folder and then we are ready to go with the installation so here we will",
    "start": "2599400",
    "end": "2606359"
  },
  {
    "text": "be needing more packages um so first um Tesseract uh this is our OCR um data",
    "start": "2606359",
    "end": "2613440"
  },
  {
    "text": "extractor so let's say you're using a Mac um just run Brew install desct and",
    "start": "2613440",
    "end": "2619599"
  },
  {
    "text": "this one you can do it outside your local um environment it should take you know a few minute to install and you",
    "start": "2619599",
    "end": "2626880"
  },
  {
    "text": "also need um popler if you don't have it already um you can just do brew and",
    "start": "2626880",
    "end": "2632480"
  },
  {
    "text": "stall on popular and then yeah we will need to set our um virtual environment so since",
    "start": "2632480",
    "end": "2640559"
  },
  {
    "text": "this is a more complicated um exercise I would just recommend to use um the you",
    "start": "2640559",
    "end": "2646520"
  },
  {
    "text": "know default option so the uh python environment with a python um 3.10 so if",
    "start": "2646520",
    "end": "2653640"
  },
  {
    "text": "you don't have uh python 3.10 um let's say on a Mac you can install it using",
    "start": "2653640",
    "end": "2658680"
  },
  {
    "text": "this uh command here and then you can add the path to your shell like B RC or",
    "start": "2658680",
    "end": "2665640"
  },
  {
    "text": "uh zshrc see um using this command here",
    "start": "2665640",
    "end": "2670760"
  },
  {
    "text": "and then you can just Source your uh shell file okay and then yeah we will uh go to",
    "start": "2670760",
    "end": "2679839"
  },
  {
    "text": "the uh repo if you haven't done already um create your python environment um so",
    "start": "2679839",
    "end": "2686559"
  },
  {
    "text": "again if you added this step here then your laptop should recognize the python 3.0 then minus m v and then the name of",
    "start": "2686559",
    "end": "2694400"
  },
  {
    "text": "the environment you activate that environment and then you run the",
    "start": "2694400",
    "end": "2700400"
  },
  {
    "text": "installed script so this should take I would say you know 5 minutes if you have um good internet and once this is done",
    "start": "2700400",
    "end": "2708000"
  },
  {
    "text": "you'll also need to install IPI Corel and also um link your kernel to your um",
    "start": "2708000",
    "end": "2716480"
  },
  {
    "text": "notebook so when we tested this on different laptops um you know some folks were having also sometimes nltk and slsl",
    "start": "2716480",
    "end": "2724640"
  },
  {
    "text": "certificate so you might also need to uh run this script",
    "start": "2724640",
    "end": "2732319"
  },
  {
    "text": "here yeah it's in my bag basically yeah so let's activate the uh",
    "start": "2735319",
    "end": "2742599"
  },
  {
    "text": "uh cond the python",
    "start": "2742599",
    "end": "2746079"
  },
  {
    "text": "environment yeah and this is the uh requirements file and as and as you can",
    "start": "2750680",
    "end": "2755839"
  },
  {
    "text": "see right we have a more packages here and then um yeah this is the file",
    "start": "2755839",
    "end": "2761359"
  },
  {
    "text": "which you may need to also um run as",
    "start": "2761359",
    "end": "2765880"
  },
  {
    "text": "well yeah and once this is set up that's all you need to uh run it's fine I can",
    "start": "2766400",
    "end": "2772920"
  },
  {
    "text": "do okay yeah uh the the notebook and as I said earlier right so we uh have the app",
    "start": "2772920",
    "end": "2780680"
  },
  {
    "text": "in a notebook and in a streamlit so for the uh notebook um again right you can",
    "start": "2780680",
    "end": "2787839"
  },
  {
    "text": "open it from the terminal or from vs code let me do it from vs",
    "start": "2787839",
    "end": "2794760"
  },
  {
    "text": "code yeah so you'll go to the eqr rag AO notebooks and then um rag LCL II 1B and",
    "start": "2799079",
    "end": "2808000"
  },
  {
    "text": "this is going to be our main uh script which is using actually you know files",
    "start": "2808000",
    "end": "2813359"
  },
  {
    "text": "and modules from other um files so in particular uh we will be using the",
    "start": "2813359",
    "end": "2821400"
  },
  {
    "text": "um the document retrieval dop and also uh some files uh from the vector DB",
    "start": "2821400",
    "end": "2828800"
  },
  {
    "text": "which I'll explain in in more details all right so um let's go maybe",
    "start": "2828800",
    "end": "2834200"
  },
  {
    "text": "first over the structure of the notebook um so we first um import uh the",
    "start": "2834200",
    "end": "2841680"
  },
  {
    "text": "libraries and set the required path so you don't have to do um anything at this point but yeah just know that the kit",
    "start": "2841680",
    "end": "2849000"
  },
  {
    "text": "directory this is the absolute path for your eqr Rag and then the repo directory",
    "start": "2849000",
    "end": "2854440"
  },
  {
    "text": "this is the absolute path for the um Astic kit so let's run this uh",
    "start": "2854440",
    "end": "2862440"
  },
  {
    "text": "repo and then for the document uh loading and splitting",
    "start": "2862480",
    "end": "2868400"
  },
  {
    "text": "um yeah so we added um you know various um data loaders you know like P PDF and",
    "start": "2868400",
    "end": "2875079"
  },
  {
    "text": "unstructured and which which uh data loader you want uh you can set this up",
    "start": "2875079",
    "end": "2881040"
  },
  {
    "text": "um in the config file which is um here so yeah I'm just going to do a test with",
    "start": "2881040",
    "end": "2887359"
  },
  {
    "text": "P PDF 2 for now but we can switch to other um data loaders um",
    "start": "2887359",
    "end": "2893760"
  },
  {
    "text": "afterwards and um yeah for the experiment I will be using the sn4l um paper so this is an archive",
    "start": "2894240",
    "end": "2901240"
  },
  {
    "text": "paper which we um recently uh submitted so this contains like information about",
    "start": "2901240",
    "end": "2907000"
  },
  {
    "text": "the stack the hardware and our um uh Coe um I can show you uh the paper as well",
    "start": "2907000",
    "end": "2914319"
  },
  {
    "text": "um we also have it in GitHub but you can also um upload your own um",
    "start": "2914319",
    "end": "2919839"
  },
  {
    "text": "PDF and yeah you'll have it to put it under data Temp and",
    "start": "2919839",
    "end": "2926800"
  },
  {
    "text": "then yeah this is the uh paper that I'll be using uh uh for the demo",
    "start": "2929599",
    "end": "2937920"
  },
  {
    "text": "all right and let's go back to our vs code so this is where um you know we",
    "start": "2939920",
    "end": "2946400"
  },
  {
    "text": "will be using p PDF to actually load the content um into a list and then um we",
    "start": "2946400",
    "end": "2953680"
  },
  {
    "text": "are using the uh recursive uh Splitter from um length chain so what I'll do",
    "start": "2953680",
    "end": "2959040"
  },
  {
    "text": "first is going to the notebook and then you can go into the functions in more details if you are um interested yeah so",
    "start": "2959040",
    "end": "2965880"
  },
  {
    "text": "let's run this step",
    "start": "2965880",
    "end": "2968720"
  },
  {
    "text": "yeah and um in the config um so I set the chunk size to 1200 and then the uh",
    "start": "2977480",
    "end": "2984160"
  },
  {
    "text": "chunk overlap to 240 but again you can change those configs so for this 15 page",
    "start": "2984160",
    "end": "2989920"
  },
  {
    "text": "um PDF we end up getting um 89 chunks the next step is the",
    "start": "2989920",
    "end": "2995680"
  },
  {
    "text": "vectorization and storage so this is where um we will be using the edding model to map each uh chunk to a edding",
    "start": "2995680",
    "end": "3003960"
  },
  {
    "text": "vector and as I said earlier right so we can actually run the in bending model either on CPU or on RDU so RDU is",
    "start": "3003960",
    "end": "3012280"
  },
  {
    "text": "basically our AI chip so if you want to do it on RDU then you'll have to go to the configo file and then uh set the",
    "start": "3012280",
    "end": "3021200"
  },
  {
    "text": "type to a sambba studio and then uh bat size so this one",
    "start": "3021200",
    "end": "3026720"
  },
  {
    "text": "we can have it either one or uh 32 so 32 means that we are actually processing 32",
    "start": "3026720",
    "end": "3032920"
  },
  {
    "text": "chunks at the same time and this is a standalone um end point so yeah the Coe",
    "start": "3032920",
    "end": "3038440"
  },
  {
    "text": "is set to um false and I'll show later um if you want to run the endpoint um",
    "start": "3038440",
    "end": "3044319"
  },
  {
    "text": "for the embedding on your laptop uh how you can change the configs um for that",
    "start": "3044319",
    "end": "3049680"
  },
  {
    "text": "all right and then yeah let's run the uh vectorization and after this we're",
    "start": "3049680",
    "end": "3055079"
  },
  {
    "text": "actually storing or indexing the in beding vectors um into the chroma DB Vector",
    "start": "3055079",
    "end": "3061839"
  },
  {
    "text": "store I think this should take around uh 10 seconds let me see what's",
    "start": "3061839",
    "end": "3068520"
  },
  {
    "text": "happening yeah",
    "start": "3072799",
    "end": "3076000"
  },
  {
    "text": "yeah yeah always good to restart the notebooks I'll just go over the steps again",
    "start": "3080359",
    "end": "3087640"
  },
  {
    "text": "okay yeah so it took you know four seconds to embed the whole thing and",
    "start": "3096559",
    "end": "3103119"
  },
  {
    "text": "yeah this is where um we will initialize our um QA chain um so again we have",
    "start": "3103119",
    "end": "3108599"
  },
  {
    "text": "different uh wrappers and classes which I can go over it in details um",
    "start": "3108599",
    "end": "3114480"
  },
  {
    "text": "afterwards but for now let's just execute the cell yeah now you're ready to go um",
    "start": "3114480",
    "end": "3119799"
  },
  {
    "text": "ask a question and then what happens is right through the QA chain the question",
    "start": "3119799",
    "end": "3125319"
  },
  {
    "text": "gets embedded to the vector uh space we retrieve the uh top K chunks so in this",
    "start": "3125319",
    "end": "3132799"
  },
  {
    "text": "experiment yeah we have this set with three since I won't be using a ranker but I can also show you how to use the",
    "start": "3132799",
    "end": "3140079"
  },
  {
    "text": "ranker and then yeah the question and the context are provided as uh uh",
    "start": "3140079",
    "end": "3147640"
  },
  {
    "text": "basically context for the llm to get the answer so yeah what is a monolitic model",
    "start": "3147640",
    "end": "3154240"
  },
  {
    "text": "and you can see right the response is um instantaneous yeah so that's it for the",
    "start": "3154240",
    "end": "3160559"
  },
  {
    "text": "um uh experiment uh let's maybe try ask it a bit more complicated question so if",
    "start": "3160559",
    "end": "3166440"
  },
  {
    "text": "I open the PDF again uh",
    "start": "3166440",
    "end": "3172880"
  },
  {
    "text": "thank yeah there was a table in the",
    "start": "3178720",
    "end": "3182200"
  },
  {
    "text": "PDF yeah like this is a table you know showing um operation intensity versus",
    "start": "3185520",
    "end": "3191520"
  },
  {
    "text": "Fusion level so yeah let's see if P PDF is able to um uh retrieve uh some of the",
    "start": "3191520",
    "end": "3198599"
  },
  {
    "text": "information from this table here so I already have the uh questions",
    "start": "3198599",
    "end": "3203680"
  },
  {
    "text": "prepared let's try to access those yeah so yeah I got the response right so",
    "start": "3203680",
    "end": "3210280"
  },
  {
    "text": "41.4 basically um and again if you end up having like more complicated um tables in PDF um in this case I would",
    "start": "3210280",
    "end": "3217319"
  },
  {
    "text": "recommend to switch to the unstructured um data loader and for this yeah all you have to do is just go to the config file",
    "start": "3217319",
    "end": "3224040"
  },
  {
    "text": "and then um change uh P PDF to um unstructured so yeah that's it for uh",
    "start": "3224040",
    "end": "3231720"
  },
  {
    "text": "the second um exercise um",
    "start": "3231720",
    "end": "3237440"
  },
  {
    "text": "I can go into more details about each uh function if you're interested um or have",
    "start": "3237440",
    "end": "3244400"
  },
  {
    "text": "you try it out first and then you can maybe uh come back and then um go over the the functions um yeah so you want",
    "start": "3244400",
    "end": "3252240"
  },
  {
    "text": "maybe try it out first I guess okay great",
    "start": "3252240",
    "end": "3257520"
  },
  {
    "text": "[Music]",
    "start": "3258050",
    "end": "3275630"
  }
]