[
  {
    "text": "[Music]",
    "start": "350",
    "end": "13080"
  },
  {
    "text": "all right well hi everyone thanks so much for joining us today um so today I",
    "start": "13080",
    "end": "18880"
  },
  {
    "text": "get the great opportunity of introducing Sova and some of our capabilities around",
    "start": "18880",
    "end": "24439"
  },
  {
    "text": "reaching over a th000 tokens per second using llama 3 today I'm going to spend a",
    "start": "24439",
    "end": "30160"
  },
  {
    "text": "little bit of time getting you oriented around Sova some of the capabilities",
    "start": "30160",
    "end": "35640"
  },
  {
    "text": "that we provide as an AI platform and also some of the underlying technologies that are providing the means of",
    "start": "35640",
    "end": "41960"
  },
  {
    "text": "achieving some of the accomplishments like a th000 tokens per second before I start jumping into the content I want to",
    "start": "41960",
    "end": "48600"
  },
  {
    "text": "take the opportunity to introduce some of my colleagues really quick so joining me today I have Petro Milan who is a",
    "start": "48600",
    "end": "55879"
  },
  {
    "text": "principal AI engineer he's going to also be leading our Workshop component and be here as we're getting handson with the",
    "start": "55879",
    "end": "62800"
  },
  {
    "text": "technology I also have Veron Krishna who is a senior uh senior principal AI",
    "start": "62800",
    "end": "68280"
  },
  {
    "text": "Solutions engineer uh joining me as well and I'm Michelle matern I'm our director",
    "start": "68280",
    "end": "73680"
  },
  {
    "text": "of solutions engineering I serve our Global customer base at s NOA so before jumping into our content I",
    "start": "73680",
    "end": "81640"
  },
  {
    "text": "just want to cover what are we going to be talking about today so we're going to start off with a little bit of housekeeping uh talk a little bit about",
    "start": "81640",
    "end": "87680"
  },
  {
    "text": "the prerequisites we are going to be getting hands on on today and also introduce our Discord Channel which is",
    "start": "87680",
    "end": "92840"
  },
  {
    "text": "where we're going to be communicating with one another and also sharing some content and information that you're",
    "start": "92840",
    "end": "97960"
  },
  {
    "text": "going to need such as files and and other uh like API and uh keys and things like that um I'm going to talk about",
    "start": "97960",
    "end": "104680"
  },
  {
    "text": "Sova just get you oriented around who are we and how are we achieving a th000",
    "start": "104680",
    "end": "110200"
  },
  {
    "text": "tokens per second um then I'm going to pass it off to Petro he's going to uh talk you through our Workshop today how",
    "start": "110200",
    "end": "117000"
  },
  {
    "text": "we're going to get handson get you oriented around how to get started uh and we're actually",
    "start": "117000",
    "end": "122479"
  },
  {
    "text": "going to go through a live build and we'll support you along the way uh we'll also spend some time around questions uh",
    "start": "122479",
    "end": "128479"
  },
  {
    "text": "just in case that um you have any questions about Sova our technology or anything that we're doing in the Hands-On",
    "start": "128479",
    "end": "134879"
  },
  {
    "text": "component so before we get started I want to talk a little bit about prerequisites so first of all we are going to be using our laptops so",
    "start": "134879",
    "end": "141680"
  },
  {
    "text": "hopefully you have them today uh and we're also going to need internet access to get through our Workshop uh we're",
    "start": "141680",
    "end": "146959"
  },
  {
    "text": "going to be working in a python environment um so hopefully we have python set up and ready um and also",
    "start": "146959",
    "end": "152599"
  },
  {
    "text": "we're going to be needing to install some packages uh through pip um we are going to be working in Discord so if you",
    "start": "152599",
    "end": "159560"
  },
  {
    "text": "don't mind uh I'm going to give everybody just a second to hopefully get on Discord and um and join our channel",
    "start": "159560",
    "end": "167080"
  },
  {
    "text": "so I'll just get everyone a quick moment to to get set",
    "start": "167080",
    "end": "171640"
  },
  {
    "text": "up once you're set up just maybe give me a thumbs up so so I know",
    "start": "173040",
    "end": "179840"
  },
  {
    "text": "question time to build capital t uh capital T capital B the general one",
    "start": "189360",
    "end": "197159"
  },
  {
    "text": "not the one that says speaker okay engineer okay so just to repeat uh it's",
    "start": "197159",
    "end": "202480"
  },
  {
    "text": "AI engineer the password is time to build with a capital t and a capital B",
    "start": "202480",
    "end": "207959"
  },
  {
    "text": "no no capital T capital T capital okay all it it's all capitalize each word but",
    "start": "207959",
    "end": "215000"
  },
  {
    "text": "concatenated",
    "start": "215000",
    "end": "218000"
  },
  {
    "text": "okay good were we able to join okay awesome cool just want to make sure there's no",
    "start": "221680",
    "end": "228120"
  },
  {
    "text": "problems awesome we'll give everyone a couple minutes so we' just got Wi-Fi access",
    "start": "228120",
    "end": "234920"
  },
  {
    "text": "all right um in case you haven't got a chance to set up maybe just take a picture of this really quick we'll also",
    "start": "255439",
    "end": "260919"
  },
  {
    "text": "go back to it um before we kick off the hands on component um but want to spend",
    "start": "260919",
    "end": "266960"
  },
  {
    "text": "a little bit of time just getting you oriented around us as somova so somova",
    "start": "266960",
    "end": "272320"
  },
  {
    "text": "we are a full stack AI platform and we've existed since 2017 uh we were",
    "start": "272320",
    "end": "277720"
  },
  {
    "text": "founded out of Stanford University so two of our uh co-founders are actually",
    "start": "277720",
    "end": "283639"
  },
  {
    "text": "Stanford professors uh kle and also Chris um both of them and including",
    "start": "283639",
    "end": "289720"
  },
  {
    "text": "Rodrigo each bring a unique perspective to our founding team uh including previous uh startups that were uh",
    "start": "289720",
    "end": "296960"
  },
  {
    "text": "Acquired and had uh various exits uh also building other AI startups that are",
    "start": "296960",
    "end": "302360"
  },
  {
    "text": "pretty well known in the industry such as snorkel together AI um and also a really uh depth of experience around",
    "start": "302360",
    "end": "309600"
  },
  {
    "text": "building out hardware and chips um we are uh building the full stack from the",
    "start": "309600",
    "end": "314960"
  },
  {
    "text": "ground up so that means we build our own chip um I'll go into that a little bit later later but also all the way through",
    "start": "314960",
    "end": "320919"
  },
  {
    "text": "the system level and the software layer um we're on our fourth generation stick",
    "start": "320919",
    "end": "326360"
  },
  {
    "text": "uh chip um and we have built a entire stack that allows you to both fine-tune",
    "start": "326360",
    "end": "331600"
  },
  {
    "text": "models pre-train models and also deploy those models with really high performant",
    "start": "331600",
    "end": "337160"
  },
  {
    "text": "inference um we have achieved over a billion dollars in funding from various",
    "start": "337160",
    "end": "342639"
  },
  {
    "text": "well-known names like black rock or Google Ventures Intel",
    "start": "342639",
    "end": "347880"
  },
  {
    "text": "GIC um and so we are really wellestablished to solve the challenge",
    "start": "347880",
    "end": "352960"
  },
  {
    "text": "around building and deploying AI Hardware so what exactly are we",
    "start": "352960",
    "end": "358840"
  },
  {
    "text": "targeting and what exactly are we trying to solve for our customer base comes from a wide variety of Enterprises and",
    "start": "358840",
    "end": "366280"
  },
  {
    "text": "also to government organizations so we're really aiming to deliver capabilities that can help service the",
    "start": "366280",
    "end": "372759"
  },
  {
    "text": "Enterprise grade AI capabilities that companies and governments require to",
    "start": "372759",
    "end": "377840"
  },
  {
    "text": "deliver um unique and differentiated capabilities and also things like Sovereign AI um our our underlying",
    "start": "377840",
    "end": "385479"
  },
  {
    "text": "platform is delivering the means to actually achieve the scale of a trillion",
    "start": "385479",
    "end": "390639"
  },
  {
    "text": "parameters uh plus um and we're doing that through delivering full stack",
    "start": "390639",
    "end": "395919"
  },
  {
    "text": "capabilities when I say full stack um many folks have probably utilized many",
    "start": "395919",
    "end": "401160"
  },
  {
    "text": "of these Technologies on uh this slide we're not necessarily trying to compete with every single layer uh involved here",
    "start": "401160",
    "end": "407720"
  },
  {
    "text": "but what we are trying to do is ease the process of getting started and ease the journey along the way and so instead of",
    "start": "407720",
    "end": "414400"
  },
  {
    "text": "having to make a decision at every single one of these layers we are actually integrating things into a very",
    "start": "414400",
    "end": "420720"
  },
  {
    "text": "seamless experience from deciding on what Chip is going to work with uh what",
    "start": "420720",
    "end": "426280"
  },
  {
    "text": "compute what compute and Chip is going to work with what operation systems what",
    "start": "426280",
    "end": "432000"
  },
  {
    "text": "operation system works with what models you don't have to actually make each of these decisions and know that they have",
    "start": "432000",
    "end": "437800"
  },
  {
    "text": "to integrate with one another we actually create this very seamless experience along the way um where",
    "start": "437800",
    "end": "443680"
  },
  {
    "text": "everything kind of orchestrates and works very nicely and what we're doing at the end",
    "start": "443680",
    "end": "449960"
  },
  {
    "text": "of this is actually delivering the capability to not only F tune but deliver really really fast inference",
    "start": "449960",
    "end": "455840"
  },
  {
    "text": "capabilities so we're going to demo a little bit of this later but uh recently we released a a a demo um that you can",
    "start": "455840",
    "end": "463440"
  },
  {
    "text": "actually go try live and we'll do so later called one turbo this is uh exceeding World Records around uh speed",
    "start": "463440",
    "end": "470319"
  },
  {
    "text": "of inference especially when it comes to llama 3 um and you can see that through",
    "start": "470319",
    "end": "475440"
  },
  {
    "text": "some of the metrics that were recently published um through uh artificial analysis artificial analysis did a um a",
    "start": "475440",
    "end": "483960"
  },
  {
    "text": "benchmarking exercise to understand the different capabilities uh the speed at",
    "start": "483960",
    "end": "489360"
  },
  {
    "text": "which they are able to deliver inference throughput um for a thousand tokens per",
    "start": "489360",
    "end": "494720"
  },
  {
    "text": "second across various Hardware providers and what you can see is that we are far out exceeding as a platform um the the",
    "start": "494720",
    "end": "502599"
  },
  {
    "text": "throughput capabilities compared to some of the other other providers out there and so I want to to talk a little",
    "start": "502599",
    "end": "510080"
  },
  {
    "text": "bit about how are we enabling such speed um so when it comes to the underlying",
    "start": "510080",
    "end": "515120"
  },
  {
    "text": "technology many of us have experienced some of these Trends in the industry many of us got started with what you see",
    "start": "515120",
    "end": "522240"
  },
  {
    "text": "on the right hand side of the slide which is the large monolithic model this is the likes of like an open AI for",
    "start": "522240",
    "end": "528600"
  },
  {
    "text": "example or a gemini or a cloud and many of us started our llm journey or our",
    "start": "528600",
    "end": "533640"
  },
  {
    "text": "generative AI Journey using some of these Technologies but along the way many uh many other capabilities in the",
    "start": "533640",
    "end": "540440"
  },
  {
    "text": "open source Community started to pop up specifically these smaller models and these smaller models allowed us to do",
    "start": "540440",
    "end": "546440"
  },
  {
    "text": "things like fine tuning and actually adapting some of these models to our Enterprise data and our Enterprise",
    "start": "546440",
    "end": "552600"
  },
  {
    "text": "requirements and so that really started to take off in the industry and each of these started to see different pros and",
    "start": "552600",
    "end": "558880"
  },
  {
    "text": "cons associated with them when it came to large monolithic models when we actually started to put these into",
    "start": "558880",
    "end": "565000"
  },
  {
    "text": "practice when it came to Enterprise applications one of the reasons many of us lean into this is because of the",
    "start": "565000",
    "end": "571040"
  },
  {
    "text": "broad capabilities that the likes of openi brought right and also the ease of integration in terms of open AI into the",
    "start": "571040",
    "end": "578360"
  },
  {
    "text": "actual platform itself it's super easy to manage and it also was trained on the internet's data and so it can handle a",
    "start": "578360",
    "end": "584480"
  },
  {
    "text": "lot of different things but when it came to actual Enterprise applications Enterprises have unique capabilities",
    "start": "584480",
    "end": "591120"
  },
  {
    "text": "required to deliver on some of the use cases and challenges they're trying to solve for for example Enterprises have",
    "start": "591120",
    "end": "598760"
  },
  {
    "text": "unique data that they they you know Al often times segregate from the internet",
    "start": "598760",
    "end": "604440"
  },
  {
    "text": "well most of the time segregate from the internet um that's proprietary to them and oftentimes they have spent the last",
    "start": "604440",
    "end": "611600"
  },
  {
    "text": "10 years trying to actually aggregate that data into the likes of data lakes and other um uh kind of centralized",
    "start": "611600",
    "end": "618360"
  },
  {
    "text": "capabilities and so now how do you actually transform that into AI capabilities that you can leverage that",
    "start": "618360",
    "end": "624120"
  },
  {
    "text": "was very difficult when it came to large monolithic models the other challenges that we saw is that that many started to",
    "start": "624120",
    "end": "630680"
  },
  {
    "text": "become very concerned about security when it came to open AI they wanted to preserve uh data privacy they wanted to",
    "start": "630680",
    "end": "636720"
  },
  {
    "text": "also own and use the model as a differentiation for themselves um and then also as you start to see more and",
    "start": "636720",
    "end": "643399"
  },
  {
    "text": "more adoption that cost just started to Skyrocket um open the ey and and a lot",
    "start": "643399",
    "end": "648639"
  },
  {
    "text": "of these Clos Source models charge on a per token rate and so as you start to utilize more and more llms uh and",
    "start": "648639",
    "end": "655519"
  },
  {
    "text": "utilize llms uh more heavily the Costa starts to go up and up and up and up and it's really really hard to",
    "start": "655519",
    "end": "662160"
  },
  {
    "text": "control on the other hand when it came to adopting the smaller expert models or",
    "start": "662160",
    "end": "667279"
  },
  {
    "text": "the smaller open- Source models like the likes of llama 38b that we'll talk about later um we were able to address some of",
    "start": "667279",
    "end": "674399"
  },
  {
    "text": "the Enterprise accuracy concerns by actually pre-training fine-tuning these models to adapt them to the Enterprise",
    "start": "674399",
    "end": "681519"
  },
  {
    "text": "requirements um while we're doing this at a smaller scale to address like different capabilities and tasks that we",
    "start": "681519",
    "end": "688279"
  },
  {
    "text": "needed to solve for Enterprise they weren't trying to solve like a broad set of tasks and also adopt a broad set of",
    "start": "688279",
    "end": "696519"
  },
  {
    "text": "general knowledge um thus manageability became a little bit challenging because now we have all of these kind of like",
    "start": "696519",
    "end": "702639"
  },
  {
    "text": "micro models that we have to orchestrate and have them work together um but we were able to solve for some things like",
    "start": "702639",
    "end": "709160"
  },
  {
    "text": "security model ownership uh data privacy and data ownership um but again because",
    "start": "709160",
    "end": "715120"
  },
  {
    "text": "we had so many of these and we had to fine-tune each one of these the cost also became very challenging so what we",
    "start": "715120",
    "end": "721399"
  },
  {
    "text": "what are we trying to solve for uh through our capability of one",
    "start": "721399",
    "end": "726600"
  },
  {
    "text": "we're trying to bring the best of both of these uh paradigms together um to",
    "start": "726600",
    "end": "732600"
  },
  {
    "text": "deliver the the capabilities of each um in a very simplistic way and the way",
    "start": "732600",
    "end": "738720"
  },
  {
    "text": "that we actually deliver this is through four core capabilities first of all we",
    "start": "738720",
    "end": "744320"
  },
  {
    "text": "take all of those expert models behind the scenes so let's just say we fine-tuned a model for our legal",
    "start": "744320",
    "end": "750519"
  },
  {
    "text": "purposes we fine-tuned a model for our HR purposes we fine-tuned a model for",
    "start": "750519",
    "end": "757000"
  },
  {
    "text": "coding capabilities each of those have different groups and tasks and use cases",
    "start": "757000",
    "end": "762480"
  },
  {
    "text": "that are going to consume those but we want to really ease the experience of having to integrate those into the",
    "start": "762480",
    "end": "768040"
  },
  {
    "text": "application so we put all of those behind a secure single end point and so",
    "start": "768040",
    "end": "773760"
  },
  {
    "text": "you only have to interface with one endpoint to gain access to all these various models",
    "start": "773760",
    "end": "779760"
  },
  {
    "text": "now we need to determine how are we going to actually use those or consume those various models behind the scenes",
    "start": "779760",
    "end": "785560"
  },
  {
    "text": "so now we need uh capabilities around orchestration so one of the other capabilities we're delivering as a part",
    "start": "785560",
    "end": "790600"
  },
  {
    "text": "of this is around um routing so we're delivering the ability to determine based off of an incoming prompt what is",
    "start": "790600",
    "end": "798120"
  },
  {
    "text": "the best suited expert behind the scenes to solve that prompt and we're doing so",
    "start": "798120",
    "end": "803360"
  },
  {
    "text": "through a router we're also bringing the means of dynamically fine-tuning every",
    "start": "803360",
    "end": "808600"
  },
  {
    "text": "expert is is going to have a different Cadence in which fine tuning is going to make sense um so maybe your Finance",
    "start": "808600",
    "end": "815480"
  },
  {
    "text": "model gets adjusted at annually when your policies are updated but maybe your",
    "start": "815480",
    "end": "820639"
  },
  {
    "text": "coding model because you're pushing code so regularly needs to get updated on a quarterly basis and so you want to",
    "start": "820639",
    "end": "827440"
  },
  {
    "text": "actually be able to schedule your fine-tuning jobs and adjust and swap these models at the rate at it which it",
    "start": "827440",
    "end": "833440"
  },
  {
    "text": "makes sense to actually retrain these models and lastly you have a bunch of",
    "start": "833440",
    "end": "838800"
  },
  {
    "text": "model mod under the scenes not every application or group is going to or should be able to access each of those",
    "start": "838800",
    "end": "845480"
  },
  {
    "text": "models so now you need to figure out a way to uh manage the access controls for these so what we're also deliver",
    "start": "845480",
    "end": "852040"
  },
  {
    "text": "delivering as a part of this capability is model level our back so you can actually determine this application or",
    "start": "852040",
    "end": "858839"
  },
  {
    "text": "this person or this group of people should be able to access this set of models and it allows you a ton of",
    "start": "858839",
    "end": "865160"
  },
  {
    "text": "efficiency from a computation and management operation standpoint with the security and fine grain control that you",
    "start": "865160",
    "end": "871120"
  },
  {
    "text": "need to actually manage a access to these different models and data under underlying these",
    "start": "871120",
    "end": "877480"
  },
  {
    "text": "models So within one we have two ways of delivering this we have something called a flexible Coe that",
    "start": "877480",
    "end": "883720"
  },
  {
    "text": "allows you to kind of determine exactly what models Li under the hood and we also have a precomposed version of",
    "start": "883720",
    "end": "890720"
  },
  {
    "text": "one composition of experts so our pre-trained um or our pre-configured I",
    "start": "890720",
    "end": "897240"
  },
  {
    "text": "should say our pre-composed version of the this model has 92 underlying experts and when I say experts I'm really",
    "start": "897240",
    "end": "903320"
  },
  {
    "text": "referring to a a a specific model um that can bring different capabilities uh",
    "start": "903320",
    "end": "909560"
  },
  {
    "text": "associated with it and within those 92 experts we have a broad range of languages that are covered within those",
    "start": "909560",
    "end": "916040"
  },
  {
    "text": "models a broad range of domains and a diverse set of tasks that are very very",
    "start": "916040",
    "end": "921880"
  },
  {
    "text": "relevant to the Enterprise all of these models are supported by seven different Foundation",
    "start": "921880",
    "end": "927160"
  },
  {
    "text": "model architectures including l two llama 3 mistal Falcon Bloom and even",
    "start": "927160",
    "end": "932720"
  },
  {
    "text": "some multimodal capabilities such as like lava clip deep plot um that is kind",
    "start": "932720",
    "end": "937880"
  },
  {
    "text": "of starting to support some of the multimodal trends that are coming and of all these 92 models we are",
    "start": "937880",
    "end": "945279"
  },
  {
    "text": "actually delivering and partnering with organizations to um create and contribute back to the open- source",
    "start": "945279",
    "end": "952040"
  },
  {
    "text": "community so out of the 92 experts 12 of them are actually ones that we've helped",
    "start": "952040",
    "end": "957279"
  },
  {
    "text": "uh either develop ourselves or co-develop with organizations out there including some of the language",
    "start": "957279",
    "end": "963199"
  },
  {
    "text": "capabilities that we've delivered such as models that can support things like Thai or Japanese Hungarian um and",
    "start": "963199",
    "end": "970680"
  },
  {
    "text": "through that we've developed a lot of experience on how to actually adapt models to different languages um we've",
    "start": "970680",
    "end": "976959"
  },
  {
    "text": "also uh created a model for text tosql capabilities and delivered uh really",
    "start": "976959",
    "end": "983279"
  },
  {
    "text": "really good results through that model for text to SQL and lastly we have contributed back to BL Blom chat uh it's",
    "start": "983279",
    "end": "990199"
  },
  {
    "text": "the second largest open source model um and uh it's also bringing a lot of the multilingual",
    "start": "990199",
    "end": "997639"
  },
  {
    "text": "capabilities so organizations essentially as they're constructing these different composition of experts",
    "start": "997720",
    "end": "1004279"
  },
  {
    "text": "can add as many expert models as they need as I mentioned before while we have that preconfigured composition this is",
    "start": "1004279",
    "end": "1011440"
  },
  {
    "text": "really intended for you to be able to construct exactly what you need in terms of models under the hood so you can add",
    "start": "1011440",
    "end": "1017440"
  },
  {
    "text": "as many as you need so our end goal with this is to really",
    "start": "1017440",
    "end": "1023079"
  },
  {
    "text": "be able to bring the capabilities that enterprises need to um handle the diverse set of use cases and",
    "start": "1023079",
    "end": "1029760"
  },
  {
    "text": "capabilities they need to solve their problems and so one of the things that we've created along the way to measure ourselves against this is an Enterprise",
    "start": "1029760",
    "end": "1036720"
  },
  {
    "text": "grade AI benchmarking set and this is really tailored to understand our capabilities against the best and",
    "start": "1036720",
    "end": "1043199"
  },
  {
    "text": "Industry models across various uh Enterprise specific task and domains",
    "start": "1043199",
    "end": "1049080"
  },
  {
    "text": "that are needed things like information extraction that's where many many Enterprises are starting um but also",
    "start": "1049080",
    "end": "1055200"
  },
  {
    "text": "broads set of capabilities like TX tql coding function calling um and we're",
    "start": "1055200",
    "end": "1060480"
  },
  {
    "text": "measuring ourselves against GPT 3.5 turbo and GPT 4 um and what we can see",
    "start": "1060480",
    "end": "1066120"
  },
  {
    "text": "is along the way um we are meeting or exceeding um the capabilities that that",
    "start": "1066120",
    "end": "1071320"
  },
  {
    "text": "open AI is bringing um but alongside the",
    "start": "1071320",
    "end": "1076720"
  },
  {
    "text": "capabilities you also need to orchestrate these models I talked a little bit about this before but one of the deliverables as far as one uh",
    "start": "1076720",
    "end": "1083799"
  },
  {
    "text": "from a product standpoint is we're bringing routing capabilities so that you can actually uh take a prompt",
    "start": "1083799",
    "end": "1089360"
  },
  {
    "text": "determine what is the best best suited expert and then route to that um there's also scenarios where you may need to do",
    "start": "1089360",
    "end": "1096799"
  },
  {
    "text": "something outside of routing you may actually just want to directly call a specific model or in the case that is",
    "start": "1096799",
    "end": "1103200"
  },
  {
    "text": "popping up really really regularly now with agentic AI you may need to do model chaining so that's another capability",
    "start": "1103200",
    "end": "1109320"
  },
  {
    "text": "that we're bringing as a part of the product suite for",
    "start": "1109320",
    "end": "1114679"
  },
  {
    "text": "one and so how are how are we actually uniquely set up to deliver the",
    "start": "1116400",
    "end": "1121559"
  },
  {
    "text": "composition of experts capability um and also the really fast inference speed that we're going to see uh briefly so I",
    "start": "1121559",
    "end": "1128960"
  },
  {
    "text": "mentioned earlier but we are a full stack uh AI platform but we also build our own chip our chip we call that an",
    "start": "1128960",
    "end": "1135840"
  },
  {
    "text": "RDU or a reconfigurable data flow unit so instead of a GPU we'll refer to our",
    "start": "1135840",
    "end": "1141679"
  },
  {
    "text": "chip as an RDU and our current version of that chip is called sn40 L and what",
    "start": "1141679",
    "end": "1147679"
  },
  {
    "text": "is unique about sn40 L is actually the memory structure of this chip so our",
    "start": "1147679",
    "end": "1153159"
  },
  {
    "text": "chip supports a a three tier uh memory architecture so we have our onchip",
    "start": "1153159",
    "end": "1159080"
  },
  {
    "text": "memory and then we have our high bandwidth memory and then we have a huge uh memory capacity in DDR and this",
    "start": "1159080",
    "end": "1166760"
  },
  {
    "text": "really allows us to store a ton T of models um and have those models be",
    "start": "1166760",
    "end": "1172600"
  },
  {
    "text": "swapped in and out of various tiers within the memory um to achieve really",
    "start": "1172600",
    "end": "1177720"
  },
  {
    "text": "strong performance and also really really efficient compute",
    "start": "1177720",
    "end": "1183640"
  },
  {
    "text": "utilization so what does this look like um as I mentioned before we can store up",
    "start": "1184240",
    "end": "1189760"
  },
  {
    "text": "to five trillion parameters on DDR so that's like if we were to store like",
    "start": "1189760",
    "end": "1195200"
  },
  {
    "text": "three to four open AI on a single chip and then as we need to actually execute",
    "start": "1195200",
    "end": "1201919"
  },
  {
    "text": "those models they can move up the memory stack um and this allows us to one take",
    "start": "1201919",
    "end": "1207799"
  },
  {
    "text": "into consideration what models need to be used at what time and then two do so",
    "start": "1207799",
    "end": "1213120"
  },
  {
    "text": "in a really performant way because the the network connectivity between these three layers is really",
    "start": "1213120",
    "end": "1219760"
  },
  {
    "text": "tight so just to kind of go into some of the specs um our on onchip SRAM uh has 4",
    "start": "1222120",
    "end": "1229159"
  },
  {
    "text": "gigs uh our high bandwidth memory has 512 gigs and our DDR has up to 6",
    "start": "1229159",
    "end": "1234520"
  },
  {
    "text": "terabytes so um lots of memory to work with and when we think about how does",
    "start": "1234520",
    "end": "1240480"
  },
  {
    "text": "this compare to what you experience in the GPU world when you think about the number if you want to host a really",
    "start": "1240480",
    "end": "1247000"
  },
  {
    "text": "large amount of models when you have to do so on a GPU you're basically needing",
    "start": "1247000",
    "end": "1253840"
  },
  {
    "text": "to work with the memory that that is that the GPU has on chip but with us",
    "start": "1253840",
    "end": "1259000"
  },
  {
    "text": "because we have the DDR component we're able to store a huge amount of models in a single system and keep it coupled with",
    "start": "1259000",
    "end": "1265840"
  },
  {
    "text": "the other memory uh tiers and so with gpus you're often if you wanted to host 500 different models you're going to",
    "start": "1265840",
    "end": "1271960"
  },
  {
    "text": "have to actually uh align those models to various systems and you're going to have to basically call various system to",
    "start": "1271960",
    "end": "1279480"
  },
  {
    "text": "actually access each of those models with us it's going to be one underlying system um because we're able to store",
    "start": "1279480",
    "end": "1286720"
  },
  {
    "text": "again up to 5 trillion parameters",
    "start": "1286720",
    "end": "1290520"
  },
  {
    "text": "so um I'm going to hand it over to Pedro uh what we're going to do next is actually uh get the chance to get hands",
    "start": "1292000",
    "end": "1299279"
  },
  {
    "text": "yeah thanks uh relle for the presentation um so what we will be doing next is a demo of our L 3 and samb one",
    "start": "1299279",
    "end": "1308799"
  },
  {
    "text": "um turbo and after this um we will move to the uh Hands-On portion of the workshop um so if you want to try out",
    "start": "1308799",
    "end": "1316039"
  },
  {
    "text": "our Lama 3 end point um so we can go to our website uh",
    "start": "1316039",
    "end": "1321600"
  },
  {
    "text": "sambanova Ai and then uh click on uh samb one",
    "start": "1321600",
    "end": "1329279"
  },
  {
    "text": "turbo so this is where we can access um our um chat and front and here you have",
    "start": "1329279",
    "end": "1336640"
  },
  {
    "text": "um options to select um various models you know 3 um 8B 7tv our Coe mro and",
    "start": "1336640",
    "end": "1345480"
  },
  {
    "text": "even some of our um in-house um models we train ourselves like you know Singo",
    "start": "1345480",
    "end": "1351039"
  },
  {
    "text": "and others um so yeah we'll do a demo of Lama 3 um 8B and I'll ask it uh the",
    "start": "1351039",
    "end": "1358159"
  },
  {
    "text": "following question um so you know create a three-day a week workout schedule for",
    "start": "1358159",
    "end": "1364600"
  },
  {
    "text": "intermediate Fitness level let me actually redo it again you can see you",
    "start": "1364600",
    "end": "1370240"
  },
  {
    "text": "know it gets um instant uh response and then for the uh performance metric so",
    "start": "1370240",
    "end": "1376080"
  },
  {
    "text": "you can see the um insights here um so few things uh to note um so of",
    "start": "1376080",
    "end": "1382520"
  },
  {
    "text": "course we have a pretty high uh throughput which is uh a th000 tokens um",
    "start": "1382520",
    "end": "1388080"
  },
  {
    "text": "per second but that's not only um the end of the story you know we also have a pretty small uh time to First token",
    "start": "1388080",
    "end": "1395880"
  },
  {
    "text": "which is basically the um input inference time of 0.09 seconds and we also have a pretty small um end to",
    "start": "1395880",
    "end": "1403240"
  },
  {
    "text": "endend uh total inference time of 65 seconds right so with our a full stack",
    "start": "1403240",
    "end": "1409600"
  },
  {
    "text": "um platform can achieve high throughput and very small um inference time and if",
    "start": "1409600",
    "end": "1416320"
  },
  {
    "text": "just want to do a comparison let's say with um Chad GPT uh for instance um if",
    "start": "1416320",
    "end": "1422279"
  },
  {
    "text": "you ask uh the same question just copy it",
    "start": "1422279",
    "end": "1429840"
  },
  {
    "text": "um yeah you can clearly see the uh difference in",
    "start": "1430600",
    "end": "1436159"
  },
  {
    "text": "speed and the uh other another uh cool thing which uh we built so we have this",
    "start": "1436159",
    "end": "1443320"
  },
  {
    "text": "uh real time uh option where as you write your",
    "start": "1443320",
    "end": "1449400"
  },
  {
    "text": "prompt you can instantly see the model's response and as you change the prompt you can also see the change in the",
    "start": "1449400",
    "end": "1455960"
  },
  {
    "text": "response yeah so let's say I don't know hi I want to write an",
    "start": "1455960",
    "end": "1466640"
  },
  {
    "text": "email about blah blah blah right so but the point here is that you know with",
    "start": "1466640",
    "end": "1472039"
  },
  {
    "text": "this um real time option you know you can do um real time chatting and this",
    "start": "1472039",
    "end": "1477320"
  },
  {
    "text": "can be helpful for instance if you're drafting emails or even if you want to do some like real time um prompt",
    "start": "1477320",
    "end": "1483320"
  },
  {
    "text": "engineering let's say so yeah that's it for the uh samb one uh turbo I can give",
    "start": "1483320",
    "end": "1489520"
  },
  {
    "text": "maybe a few minutes for folks to try it out before we move on to the um handson um so again you go to uh our website",
    "start": "1489520",
    "end": "1497919"
  },
  {
    "text": "some Nova a and then you click on samb one turbo um so you can do it from your laptop or from your cell",
    "start": "1497919",
    "end": "1506200"
  },
  {
    "text": "phone yes question can we tweak the generation parameters like yes yes yes",
    "start": "1507000",
    "end": "1512200"
  },
  {
    "text": "yeah",
    "start": "1512200",
    "end": "1514840"
  },
  {
    "text": "uh let me see how to do it on this uh",
    "start": "1519360",
    "end": "1524919"
  },
  {
    "text": "UI okay I think uh from this UI it seems it is fixed but um in the hands on once",
    "start": "1526320",
    "end": "1532600"
  },
  {
    "text": "uh you will be calling our endpoint um from the API yeah we will be changing some of the um configs as well",
    "start": "1532600",
    "end": "1541320"
  },
  {
    "text": "so yeah any other uh question",
    "start": "1541919",
    "end": "1547600"
  },
  {
    "text": "yeah were you able to uh try it out uh okay great okay so I think um yeah we can",
    "start": "1569960",
    "end": "1576159"
  },
  {
    "text": "move on to the um handson uh portion um so what I'll do",
    "start": "1576159",
    "end": "1581440"
  },
  {
    "text": "first is just um introduce what um I'll be talking about in this part and then",
    "start": "1581440",
    "end": "1586919"
  },
  {
    "text": "uh we'll Dive In um to the uh Hands-On so yeah we prepared two uh exercises um for you",
    "start": "1586919",
    "end": "1594919"
  },
  {
    "text": "today so the first one is a basic um example to get you started um so here um",
    "start": "1594919",
    "end": "1601360"
  },
  {
    "text": "I'll be showing how you can load um environment variables set up the S NOA",
    "start": "1601360",
    "end": "1607720"
  },
  {
    "text": "API um initialize the llm and do a simple um uh inference call in",
    "start": "1607720",
    "end": "1614480"
  },
  {
    "text": "Python and the uh second um example is a more practical one so we will be uh",
    "start": "1614480",
    "end": "1621559"
  },
  {
    "text": "building and deploying a Q&A system um with rag for um Enterprise search uh",
    "start": "1621559",
    "end": "1628279"
  },
  {
    "text": "using our platform and we will also be using um other libraries and packages",
    "start": "1628279",
    "end": "1633880"
  },
  {
    "text": "like you know L chain um various data loaders uh E5 large V2 embedding um",
    "start": "1633880",
    "end": "1641440"
  },
  {
    "text": "chroma DB Vector store and of course the Lama 3 um endpoint which runs at the",
    "start": "1641440",
    "end": "1646919"
  },
  {
    "text": "speed of 1,000 token per second so yeah let's start with the basic um example and um if you want to",
    "start": "1646919",
    "end": "1656520"
  },
  {
    "text": "follow along so you can uh yeah go to Google write AI St kit",
    "start": "1656520",
    "end": "1663559"
  },
  {
    "text": "uh Sova and then uh click on this link or you can just um write this uh",
    "start": "1663559",
    "end": "1671799"
  },
  {
    "text": "URL so this is our uh stait uh repo um",
    "start": "1671799",
    "end": "1677480"
  },
  {
    "text": "we have a collection of Open Source um examples on um gen apps and um yeah once",
    "start": "1677480",
    "end": "1684960"
  },
  {
    "text": "you get there yeah you can go to uh workshops AI engineer",
    "start": "1684960",
    "end": "1692480"
  },
  {
    "text": "2024 um basic examples so I'll go over the read me and then do a live demo um",
    "start": "1692480",
    "end": "1698640"
  },
  {
    "text": "of the work of this um exercise and then I'll give you uh some time to uh try it",
    "start": "1698640",
    "end": "1704360"
  },
  {
    "text": "out so um yeah first um you'll need to clone uh this uh repo here and then uh you'll",
    "start": "1704360",
    "end": "1713399"
  },
  {
    "text": "need to create aend file um in the repo route uh directory so this is where um",
    "start": "1713399",
    "end": "1721519"
  },
  {
    "text": "we will be uh specifying the uh sambba studio um API key um yeah so let me show",
    "start": "1721519",
    "end": "1728559"
  },
  {
    "text": "you how this is done uh it's going to be yeah I guess with the two mic need to",
    "start": "1728559",
    "end": "1735880"
  },
  {
    "text": "find a way",
    "start": "1735880",
    "end": "1739320"
  },
  {
    "text": "yeah yeah so I've already cloned um the rep yeah the repo and then",
    "start": "1752519",
    "end": "1759720"
  },
  {
    "text": "uh at this level this is where you will need to create the uh dot nend file so",
    "start": "1759720",
    "end": "1765840"
  },
  {
    "text": "VI well in case yeah you'll have to do touch. EnV and",
    "start": "1765840",
    "end": "1774039"
  },
  {
    "text": "then yeah you can add uh the these information here so for the first um",
    "start": "1774919",
    "end": "1781880"
  },
  {
    "text": "handson that's the uh um only thing that you'll need um and you can access or",
    "start": "1781880",
    "end": "1788399"
  },
  {
    "text": "copy those from our um Discord uh channel so",
    "start": "1788399",
    "end": "1795398"
  },
  {
    "text": "yeah if you go to",
    "start": "1799159",
    "end": "1801880"
  },
  {
    "text": "Discord events um yeah so you can copy either um",
    "start": "1806279",
    "end": "1812880"
  },
  {
    "text": "of these keys so we have two dedicated you know end points um for this Workshop all",
    "start": "1812880",
    "end": "1819600"
  },
  {
    "text": "right and um once we finish setting up the N uh the third step is basically you",
    "start": "1819600",
    "end": "1826200"
  },
  {
    "text": "know installing the um packages so for this one you can either do it with you knowa or python environment um",
    "start": "1826200",
    "end": "1834799"
  },
  {
    "text": "so first yeah we will go to the basic examples um repo so just CD and then the",
    "start": "1834799",
    "end": "1841480"
  },
  {
    "text": "repo path then you can create um aond uh environment so I would recommend using",
    "start": "1841480",
    "end": "1848200"
  },
  {
    "text": "python um 3.10 and then you activate your uh cond",
    "start": "1848200",
    "end": "1853760"
  },
  {
    "text": "environment um and here we name it basic undor X and then yeah you just um insall the",
    "start": "1853760",
    "end": "1860440"
  },
  {
    "text": "requirements with the uh pip andall minus r um requirements and then you can",
    "start": "1860440",
    "end": "1865600"
  },
  {
    "text": "use this um line to just um link the kernel to your um",
    "start": "1865600",
    "end": "1871279"
  },
  {
    "text": "notebook okay so if I go to my",
    "start": "1871279",
    "end": "1877519"
  },
  {
    "text": "terminal yeah so we'll go to uh Workshop a i",
    "start": "1882000",
    "end": "1889440"
  },
  {
    "text": "engineer basic examples okay so this is where you can",
    "start": "1889440",
    "end": "1895320"
  },
  {
    "text": "create the cond so I've already done it um beforehand so I'm just going to activate the environment",
    "start": "1895320",
    "end": "1902919"
  },
  {
    "text": "so yeah and this is the requirements file so we only have like a few packages",
    "start": "1916279",
    "end": "1921360"
  },
  {
    "text": "that is needed yeah and that's it uh for the",
    "start": "1921360",
    "end": "1928720"
  },
  {
    "text": "installation um so once this is done yeah you should be able to open uh the notebook and again you can do it you",
    "start": "1928720",
    "end": "1935720"
  },
  {
    "text": "know from the terminal which will write you to a browser or um uh through vs",
    "start": "1935720",
    "end": "1941200"
  },
  {
    "text": "code so if you want to do it with a terminal you just write Jupiter notebook and then",
    "start": "1941200",
    "end": "1948799"
  },
  {
    "text": "the name of the notebook so it's going to be uh example uh with uh samb studio.",
    "start": "1950080",
    "end": "1958200"
  },
  {
    "text": "II uh 1B I'm actually going to do the demo via vs code just because I can show",
    "start": "1958200",
    "end": "1963360"
  },
  {
    "text": "you the uh time step so I already have this uh set",
    "start": "1963360",
    "end": "1970559"
  },
  {
    "text": "up okay so you know we are in the basic examples repo and then example with sambba studio and again if you doing it",
    "start": "1971639",
    "end": "1978399"
  },
  {
    "text": "uh with vsod just make sure that you have the uh kernel set up um so this is",
    "start": "1978399",
    "end": "1983440"
  },
  {
    "text": "a pretty basic um script so we will first uh let me just restart it yeah so",
    "start": "1983440",
    "end": "1991480"
  },
  {
    "text": "yeah we will be loading uh the libraries um so we",
    "start": "1991480",
    "end": "1999279"
  },
  {
    "text": "actually have a wrapper with L chain so this is um where uh we will be um",
    "start": "1999279",
    "end": "2005480"
  },
  {
    "text": "initializing and calling our end points the second step is to load the um",
    "start": "2005480",
    "end": "2011440"
  },
  {
    "text": "environment variable so these are actually the information which we added in the uh end",
    "start": "2011440",
    "end": "2017639"
  },
  {
    "text": "file and then uh we will initialize the um llm so yeah we will be using the",
    "start": "2017639",
    "end": "2023760"
  },
  {
    "text": "sambba studio um wrapper and uh we set the um sambba",
    "start": "2023760",
    "end": "2030399"
  },
  {
    "text": "Studio API key and then we specify the model configs I had a question earlier about um the model configs um so this is",
    "start": "2030399",
    "end": "2037279"
  },
  {
    "text": "where we can can uh set this up and play with this um again I think most of you",
    "start": "2037279",
    "end": "2042440"
  },
  {
    "text": "are familiar with these configs so you know do a sample if you set it to false this is basically a um deterministic",
    "start": "2042440",
    "end": "2049358"
  },
  {
    "text": "output if you set it to true then becomes um probabilistic you can change the temperature and also the max tokens",
    "start": "2049359",
    "end": "2057079"
  },
  {
    "text": "um to generate and also this is basically our C end point right so we have one end point through which you can",
    "start": "2057079",
    "end": "2063398"
  },
  {
    "text": "actually call um different models and in this case uh we set the x expert to uh",
    "start": "2063399",
    "end": "2068760"
  },
  {
    "text": "metal Lama 3 um HB instruct okay so I'll be running the uh",
    "start": "2068760",
    "end": "2075079"
  },
  {
    "text": "the step and yeah we have now our model loaded and now we're good to go so um",
    "start": "2075079",
    "end": "2082520"
  },
  {
    "text": "I'll first show you how you can do an inference call using a simple um invoke method in L chain um so just write llm",
    "start": "2082520",
    "end": "2091480"
  },
  {
    "text": "do invoke and then add your prompt and the prompt here is um what is the",
    "start": "2091480",
    "end": "2096520"
  },
  {
    "text": "capital of France and what you'll uh notice is it",
    "start": "2096520",
    "end": "2105720"
  },
  {
    "text": "gives the right answer but also um give you other stuff which you didn't ask for",
    "start": "2105720",
    "end": "2110880"
  },
  {
    "text": "and this is a common thing with open source um models because when you ask a prompt you need to include the the",
    "start": "2110880",
    "end": "2117320"
  },
  {
    "text": "special tags right so in the case of uh Lama 3 you can get it from a meta model",
    "start": "2117320",
    "end": "2124040"
  },
  {
    "text": "card so you'll have to actually in the prompt add these uh special tags or",
    "start": "2124040",
    "end": "2129920"
  },
  {
    "text": "tokens um in particular you have to let the llm know that this is the beginning of text and this is where you will",
    "start": "2129920",
    "end": "2137400"
  },
  {
    "text": "insert the user query right so and then let the LM know where it uh needs to um",
    "start": "2137400",
    "end": "2144160"
  },
  {
    "text": "answer and once you have the special tags um inserted um now you should be able to get the right response so this",
    "start": "2144160",
    "end": "2150800"
  },
  {
    "text": "is the first way to do the um inference call the other way is to do it via a um",
    "start": "2150800",
    "end": "2156520"
  },
  {
    "text": "LCL in nchain chain so basically um you can use the LCL to connect um a prom",
    "start": "2156520",
    "end": "2162000"
  },
  {
    "text": "template with llm and an output parser and nchain has very um templates that",
    "start": "2162000",
    "end": "2168040"
  },
  {
    "text": "you can use so um we asking the same prompt it just that the main difference",
    "start": "2168040",
    "end": "2175000"
  },
  {
    "text": "here we are adding the country as a placeholder and then when you prompt the model you can actually specify the value",
    "start": "2175000",
    "end": "2181000"
  },
  {
    "text": "of this country right yeah so that's it for the basic um",
    "start": "2181000",
    "end": "2187640"
  },
  {
    "text": "example and as I said this is just to get you started so um yeah we can spend",
    "start": "2187640",
    "end": "2192839"
  },
  {
    "text": "10 minutes for you guys to try it out U me Baron and orelle will be here to help",
    "start": "2192839",
    "end": "2199200"
  },
  {
    "text": "and um yeah then you can move on to the second um",
    "start": "2199200",
    "end": "2204839"
  },
  {
    "text": "exercise I use if you're",
    "start": "2206760",
    "end": "2211640"
  },
  {
    "text": "yeah I'll move around if also people have",
    "start": "2224280",
    "end": "2228079"
  },
  {
    "text": "questions do you have the right end point set basically",
    "start": "2236599",
    "end": "2241920"
  },
  {
    "text": "uh can you open your NV actually working",
    "start": "2243720",
    "end": "2248760"
  },
  {
    "text": "I don't know",
    "start": "2248760",
    "end": "2255000"
  },
  {
    "text": "comp um yeah can you try the E4 the the other",
    "start": "2255520",
    "end": "2262880"
  },
  {
    "text": "one oh yeah actually the one here is outdated basically so if you're using the one the GitHub is outdated so yeah",
    "start": "2263920",
    "end": "2270119"
  },
  {
    "text": "yeah you need to use the one from Discord yeah because yeah this is like public so everyone can actually uh yeah",
    "start": "2270119",
    "end": "2275960"
  },
  {
    "text": "sorry about that yeah so you can use um anyone you have it working right",
    "start": "2275960",
    "end": "2283880"
  },
  {
    "text": "yeah and yeah basically for you know this Workshop the uh Co endpoint we've",
    "start": "2286200",
    "end": "2292200"
  },
  {
    "text": "only uh activated the Lama 3 but eventually um like if you were to",
    "start": "2292200",
    "end": "2297839"
  },
  {
    "text": "activate the whole Coe again it's just the same end point then you can just switch between models basically and also",
    "start": "2297839",
    "end": "2303280"
  },
  {
    "text": "um other benchmarks for the Llama 3 in",
    "start": "2303280",
    "end": "2311240"
  },
  {
    "text": "terms of tokens per second like you mean performance yeah yeah yeah so I mean there's the GPU numbers right which I",
    "start": "2311240",
    "end": "2317720"
  },
  {
    "text": "think uh orell presented in the slide deck um so we have like I think 8 to4x",
    "start": "2317720",
    "end": "2325040"
  },
  {
    "text": "speed up with gpus and there are also like you know other startups who have um also like high inference speeds like",
    "start": "2325040",
    "end": "2331319"
  },
  {
    "text": "like basically Gro but you know one difference we have with the gro is they run with 576 chips we only do do it with",
    "start": "2331319",
    "end": "2337839"
  },
  {
    "text": "16 chips basically and you actually conserve the full Precision they do it with reduced uh Precision you know so",
    "start": "2337839",
    "end": "2345760"
  },
  {
    "text": "um it's like weird to get a response that quickly yeah yeah yeah feels like",
    "start": "2345760",
    "end": "2352200"
  },
  {
    "text": "fake",
    "start": "2352200",
    "end": "2355200"
  },
  {
    "text": "yeah were you able to uh get it right or are you not trying it that's fine but",
    "start": "2383920",
    "end": "2389760"
  },
  {
    "text": "yeah let me know if you have questions okay nice",
    "start": "2389760",
    "end": "2397880"
  },
  {
    "text": "yeah maybe you can have our stuff there as yeah yeah so now you can yeah but this is simple if you want to try it out",
    "start": "2398520",
    "end": "2405040"
  },
  {
    "text": "you know so yeah okay so only for the Mig rate",
    "start": "2405040",
    "end": "2410119"
  },
  {
    "text": "[Music] yeah uh which one are we",
    "start": "2413630",
    "end": "2421079"
  },
  {
    "text": "using we tried the two that are in here E2 and E4 yeah then maybe you're in",
    "start": "2421079",
    "end": "2427319"
  },
  {
    "text": "connection then like it pull up your web page Prett quickly yeah yeah no it's not",
    "start": "2427319",
    "end": "2432960"
  },
  {
    "text": "uh can you restart the uh notebook and then yeah you did",
    "start": "2432960",
    "end": "2437680"
  },
  {
    "text": "that I wonder if there's it's",
    "start": "2439280",
    "end": "2444720"
  },
  {
    "text": "authenticating yeah like this should be faster but anyway let's let's no yeah I mean this one should take yeah 3 seconds",
    "start": "2447119",
    "end": "2452440"
  },
  {
    "text": "yeah that's it yeah but because this is without the special tags right so because it's a long um yeah but this one",
    "start": "2452440",
    "end": "2458680"
  },
  {
    "text": "yeah should be just yeah quick you know so yeah yeah yeah no",
    "start": "2458680",
    "end": "2464800"
  },
  {
    "text": "because no it's basically waiting for the whole thing to complete and then we are outputting the answer you know so",
    "start": "2464800",
    "end": "2474000"
  },
  {
    "text": "yeah you have questions were you able to try it out or just okay no let me know if you have questions sure thanks",
    "start": "2475000",
    "end": "2483960"
  },
  {
    "text": "yeah so the first one yeah is the FR template so the same question has been wrapped into so what is",
    "start": "2497440",
    "end": "2506119"
  },
  {
    "text": "Prov instruction to the that so that's",
    "start": "2517480",
    "end": "2524760"
  },
  {
    "text": "provide should move to the second one or maybe just go",
    "start": "2545760",
    "end": "2551200"
  },
  {
    "text": "more",
    "start": "2551200",
    "end": "2554200"
  },
  {
    "text": "yeah yes so I mean you can get it from the so yeah so if you go to Google and",
    "start": "2569240",
    "end": "2575079"
  },
  {
    "text": "then uh write meta 3 Lama card you you get it",
    "start": "2575079",
    "end": "2581440"
  },
  {
    "text": "there yeah",
    "start": "2581440",
    "end": "2584680"
  },
  {
    "text": "like I think we can move to the second one since everyone try it out yeah",
    "start": "2597800",
    "end": "2606040"
  },
  {
    "text": "yeah like yeah",
    "start": "2607839",
    "end": "2611240"
  },
  {
    "text": "yesterday okay cool I think uh many of you were able to try out this simple um",
    "start": "2619760",
    "end": "2626240"
  },
  {
    "text": "exercise so yeah we'll move now to the uh second uh one everyone want",
    "start": "2626240",
    "end": "2632100"
  },
  {
    "text": "[Music] to um yeah",
    "start": "2632100",
    "end": "2638760"
  },
  {
    "text": "yeah so and as I said earlier this is going to be a Q&A uh system um with",
    "start": "2646119",
    "end": "2653680"
  },
  {
    "text": "Rag and uh if you want to follow along um again from the same repo uh go to",
    "start": "2653680",
    "end": "2661079"
  },
  {
    "text": "Workshop AI engineer 2024 and then e Rag",
    "start": "2661079",
    "end": "2666599"
  },
  {
    "text": "and like the previous exercise I'll go through the read me do a live demo of the installation and the Run setup and",
    "start": "2666599",
    "end": "2673280"
  },
  {
    "text": "then give you some time to um try it out and the uh app here we have two versions",
    "start": "2673280",
    "end": "2680240"
  },
  {
    "text": "of it one with a Jupiter notebook and the other one um with a streamlet which",
    "start": "2680240",
    "end": "2685839"
  },
  {
    "text": "is a UI based and before I uh jump into the Hands-On just wanted to give a brief",
    "start": "2685839",
    "end": "2693640"
  },
  {
    "text": "um overview of what rag is although I'm I'm sure many of you already know uh",
    "start": "2693640",
    "end": "2699559"
  },
  {
    "text": "this concept but just for um completeness so yeah so rag is a technique that um you can use to",
    "start": "2699559",
    "end": "2706359"
  },
  {
    "text": "supplement um llm with um additional information from um various sources to",
    "start": "2706359",
    "end": "2712319"
  },
  {
    "text": "improve the models response and rag is very helpful um if you want to use an",
    "start": "2712319",
    "end": "2718079"
  },
  {
    "text": "off-the-shelf llm to ask a question um Beyond its uh training data or if you",
    "start": "2718079",
    "end": "2724480"
  },
  {
    "text": "want to um have the LM access to up to- dat information without um retraining it",
    "start": "2724480",
    "end": "2731760"
  },
  {
    "text": "also in Ira can help reduce um hallucinations in some uh context and",
    "start": "2731760",
    "end": "2737559"
  },
  {
    "text": "the typical uh rag workflow um consist",
    "start": "2737559",
    "end": "2743680"
  },
  {
    "text": "of yeah the following steps so we first have um document",
    "start": "2743680",
    "end": "2752079"
  },
  {
    "text": "loading and parsing so this is where um we can use a data loader to actually",
    "start": "2752079",
    "end": "2757760"
  },
  {
    "text": "load the data into a um digital text that we can edit and format um and you know various data",
    "start": "2757760",
    "end": "2764680"
  },
  {
    "text": "loaders are um available depending on the um extension of the file you're",
    "start": "2764680",
    "end": "2770240"
  },
  {
    "text": "using so in a PDF text um PowerPoint after this we have a splitting step so this is where um we",
    "start": "2770240",
    "end": "2777680"
  },
  {
    "text": "will be splitting the document into um smaller chunks and you know the chunk",
    "start": "2777680",
    "end": "2782920"
  },
  {
    "text": "size and the overlap all of these are um hyper prams and the uh next step is um vectorization",
    "start": "2782920",
    "end": "2791200"
  },
  {
    "text": "so this is where um we will be using a um in beding model like E5 large V2 to",
    "start": "2791200",
    "end": "2798040"
  },
  {
    "text": "map each uh chunk to a numerical vector and um we can store you know the",
    "start": "2798040",
    "end": "2805960"
  },
  {
    "text": "vectors along with the content and the metadata in a vector store like phase and chroma DB and today um we will be",
    "start": "2805960",
    "end": "2813200"
  },
  {
    "text": "using um chroma DB which is um open source and again the whole uh goal of",
    "start": "2813200",
    "end": "2819720"
  },
  {
    "text": "this embedding is that it allows us to do um like semantic similarity and uh",
    "start": "2819720",
    "end": "2826480"
  },
  {
    "text": "semantic search and in the retrieval step um this is where we ask uh the",
    "start": "2826480",
    "end": "2832119"
  },
  {
    "text": "question which is going to also be embedded into the vector um space and",
    "start": "2832119",
    "end": "2837200"
  },
  {
    "text": "then we have a retriever which is going to um retrieve the closest uh chunk",
    "start": "2837200",
    "end": "2842720"
  },
  {
    "text": "vectors to the query Vector According to some similarity metric",
    "start": "2842720",
    "end": "2847760"
  },
  {
    "text": "and we can also add a ranker which can rank the retrieved uh chunks um per uh",
    "start": "2847760",
    "end": "2856359"
  },
  {
    "text": "relevance and also remove some of the um unnecessary chunks and the last step is basically Q&A um generation so this is",
    "start": "2856359",
    "end": "2863920"
  },
  {
    "text": "where we provide the llm um with the query and the final retrieved uh chunk",
    "start": "2863920",
    "end": "2871079"
  },
  {
    "text": "to get the grounded response and um yeah maybe also would",
    "start": "2871079",
    "end": "2876440"
  },
  {
    "text": "like to precise that um in this exercise so the we will be using third party",
    "start": "2876440",
    "end": "2882680"
  },
  {
    "text": "tools for document loading splitting and uh storage for the in bending model um",
    "start": "2882680",
    "end": "2888720"
  },
  {
    "text": "you can either run it on CPU or on our hardware and we'll be doing both to show",
    "start": "2888720",
    "end": "2893760"
  },
  {
    "text": "the differences and for the llm part this is going to be done on our um",
    "start": "2893760",
    "end": "2899800"
  },
  {
    "text": "Hardware so yeah that's it for the uh overview I think we can move on to the",
    "start": "2899800",
    "end": "2905520"
  },
  {
    "text": "uh read me um yeah so we first uh clone uh the repo",
    "start": "2905520",
    "end": "2911160"
  },
  {
    "text": "I think if you've done the other um exercise then you don't have to do um the step um same thing um after this we",
    "start": "2911160",
    "end": "2918520"
  },
  {
    "text": "will set up the um environment um variable um so for this one we will be",
    "start": "2918520",
    "end": "2924559"
  },
  {
    "text": "using uh sambba studio um API for the llm and also uh we will be using a",
    "start": "2924559",
    "end": "2933359"
  },
  {
    "text": "embedding um API as well so both are available on",
    "start": "2933359",
    "end": "2939000"
  },
  {
    "text": "Discord yeah let me show you uh in the",
    "start": "2939000",
    "end": "2944079"
  },
  {
    "text": "terminal yeah and actually yeah I would also recommend to deactivate your previous um",
    "start": "2945000",
    "end": "2952200"
  },
  {
    "text": "environment okay so we go to the ekr rag repo um yeah actually for the dot and",
    "start": "2954079",
    "end": "2961400"
  },
  {
    "text": "file this one you'll have to put it um at this level the AI s kit okay so for",
    "start": "2961400",
    "end": "2967599"
  },
  {
    "text": "this one we will need the ined endpoint and API key and also the uh sambba",
    "start": "2967599",
    "end": "2973599"
  },
  {
    "text": "studio um endpoint and API key all right then we go back to the",
    "start": "2973599",
    "end": "2982880"
  },
  {
    "text": "uh e r folder and then we are ready to go with the installation so here we will",
    "start": "2982880",
    "end": "2989839"
  },
  {
    "text": "be needing more packages um so first um TCT uh this is our OCR um data ractor so",
    "start": "2989839",
    "end": "2998319"
  },
  {
    "text": "let's say you're using a Mac um just run Brew install desct and this one you can",
    "start": "2998319",
    "end": "3003520"
  },
  {
    "text": "do it outside your local um environment it should take you know a few minute to",
    "start": "3003520",
    "end": "3009079"
  },
  {
    "text": "install and you also need um popular if you don't have it already um you can",
    "start": "3009079",
    "end": "3014640"
  },
  {
    "text": "just do Brew install um popler and then yeah we will need to set",
    "start": "3014640",
    "end": "3020480"
  },
  {
    "text": "our um virtual environment so since this is a more complicated um exercise I",
    "start": "3020480",
    "end": "3026880"
  },
  {
    "text": "would just recommend to use um the you know default option so the uh python",
    "start": "3026880",
    "end": "3033520"
  },
  {
    "text": "environment with a python um 3.10 so if you don't have uh python 3.10 um let's",
    "start": "3033520",
    "end": "3040040"
  },
  {
    "text": "say on a Mac you can install it using this uh command here and then you can",
    "start": "3040040",
    "end": "3045520"
  },
  {
    "text": "add the path to your shell like bashrc or uh zshrc um using this command here",
    "start": "3045520",
    "end": "3054200"
  },
  {
    "text": "and then you can just Source your uh uh shell file okay and then yeah we will uh go to",
    "start": "3054200",
    "end": "3063319"
  },
  {
    "text": "the repo if you haven't done already um create your python environment um so",
    "start": "3063319",
    "end": "3070040"
  },
  {
    "text": "again if you added this step here then your laptop should recognize the python 3.10 then minus M van and then the name",
    "start": "3070040",
    "end": "3077720"
  },
  {
    "text": "of the environment you activate that environment and then you run the",
    "start": "3077720",
    "end": "3083839"
  },
  {
    "text": "installed script so this should take I would say you know 5 minutes if you have um good internet and once this is done",
    "start": "3083839",
    "end": "3091440"
  },
  {
    "text": "you'll also need to install IPI Corel and also um link your kernel to your um",
    "start": "3091440",
    "end": "3099920"
  },
  {
    "text": "notebook so when we tested this on different laptops um you know some folks were having also sometimes nltk and slsl",
    "start": "3099920",
    "end": "3108079"
  },
  {
    "text": "certificate so you might also need to uh run this script here",
    "start": "3108079",
    "end": "3116759"
  },
  {
    "text": "yeah it's in my bag basically yeah so let's activate the uh",
    "start": "3119799",
    "end": "3126079"
  },
  {
    "text": "uh cond the python",
    "start": "3126079",
    "end": "3129559"
  },
  {
    "text": "environment yeah and this is the uh requirements file and as and as you can",
    "start": "3134160",
    "end": "3139319"
  },
  {
    "text": "see right we have more packages here and then um yeah this is the file",
    "start": "3139319",
    "end": "3144799"
  },
  {
    "text": "which you may need to also um um run as",
    "start": "3144799",
    "end": "3149359"
  },
  {
    "text": "well yeah and once this is set up that's all you need to uh run it's fine I can",
    "start": "3149880",
    "end": "3156440"
  },
  {
    "text": "do okay yeah uh the the notebook and as I said earlier right so we uh have the app",
    "start": "3156440",
    "end": "3164160"
  },
  {
    "text": "in a notebook and in a streamlit so for the uh notebook um again right you can",
    "start": "3164160",
    "end": "3171280"
  },
  {
    "text": "open it from the terminal or from vs code let me do it um from vs code",
    "start": "3171280",
    "end": "3178440"
  },
  {
    "text": "yeah so you'll go to the eqr rag repo notebooks and then um rag LCL II 1B and",
    "start": "3183520",
    "end": "3191480"
  },
  {
    "text": "this is going to be our main uh script which is using actually you know files",
    "start": "3191480",
    "end": "3196839"
  },
  {
    "text": "and modules from other um files so in particular uh we will be using the",
    "start": "3196839",
    "end": "3204880"
  },
  {
    "text": "um the doc document retrieval dop and also uh some files uh from the vector DB",
    "start": "3204880",
    "end": "3212280"
  },
  {
    "text": "which I'll explain in in more details all right so um let's go maybe",
    "start": "3212280",
    "end": "3217680"
  },
  {
    "text": "first over the structure of the notebook um so we first um import uh the",
    "start": "3217680",
    "end": "3225119"
  },
  {
    "text": "libraries and set the required path so you don't have to do um anything at this point but yeah just know that the kit",
    "start": "3225119",
    "end": "3232480"
  },
  {
    "text": "directory this is the absolute path for your eqr rag and then the repo directory",
    "start": "3232480",
    "end": "3237920"
  },
  {
    "text": "this is the absolute path for the um as kit so let's run this uh",
    "start": "3237920",
    "end": "3245920"
  },
  {
    "text": "repo and then for the uh document uh loading and splitting um yeah so we added um you",
    "start": "3245960",
    "end": "3254160"
  },
  {
    "text": "know various um data loaders you know like P PDF and unstructured and which uh data loader",
    "start": "3254160",
    "end": "3261799"
  },
  {
    "text": "you want uh you can set this up um in the config file",
    "start": "3261799",
    "end": "3266880"
  },
  {
    "text": "which is um here so yeah I'm just going to do a test with P pdf2 for now but we",
    "start": "3266880",
    "end": "3272440"
  },
  {
    "text": "can switch to other um data loaders um",
    "start": "3272440",
    "end": "3277200"
  },
  {
    "text": "afterwards and um yeah for the experiment I will be using the sn4l um paper so this is an archive",
    "start": "3277680",
    "end": "3284680"
  },
  {
    "text": "paper which we um recently uh submitted so this contains like information about",
    "start": "3284680",
    "end": "3290520"
  },
  {
    "text": "the stack the hardware and our um uh Coe um I can show you uh paper as well um we",
    "start": "3290520",
    "end": "3298240"
  },
  {
    "text": "also have it in GitHub but you can also um upload your own um",
    "start": "3298240",
    "end": "3303280"
  },
  {
    "text": "PDF and yeah you'll have it to put it under data temp and",
    "start": "3303280",
    "end": "3310280"
  },
  {
    "text": "then yeah this is the uh paper that I'll be using uh uh for the",
    "start": "3313119",
    "end": "3320400"
  },
  {
    "text": "demo all right and let's go back to our vs code code so this is where um you",
    "start": "3322400",
    "end": "3329640"
  },
  {
    "text": "know we will be using p PDF to actually load the content um into a list and then",
    "start": "3329640",
    "end": "3336880"
  },
  {
    "text": "um we are using the uh recursive uh Splitter from um length chain so what",
    "start": "3336880",
    "end": "3342200"
  },
  {
    "text": "I'll do first is going to the notebook and then we can go into the functions in more details if you are um interested",
    "start": "3342200",
    "end": "3349079"
  },
  {
    "text": "yes so let's run this step",
    "start": "3349079",
    "end": "3352920"
  },
  {
    "text": "yeah and um in the config um so I set the chunk size to 1200 and then the uh",
    "start": "3360960",
    "end": "3367640"
  },
  {
    "text": "chunk overlap to 240 but again you can change those configs so for this 15 page",
    "start": "3367640",
    "end": "3373400"
  },
  {
    "text": "um PDF we end up getting um 89 chunks the next step is the uh",
    "start": "3373400",
    "end": "3379160"
  },
  {
    "text": "vectorization and storage so this is where um we will be using the in bending model to map each uh chunk to a",
    "start": "3379160",
    "end": "3386640"
  },
  {
    "text": "embedding vector and as I said earlier right so we can actually run the embedding model either on CPU or on RDU",
    "start": "3386640",
    "end": "3395119"
  },
  {
    "text": "so RDU is basically our AI chip so if you want to do it on RDU then you'll have to go to the configo file and then",
    "start": "3395119",
    "end": "3403920"
  },
  {
    "text": "uh set the type to a sambba studio and then uh bat size so this one",
    "start": "3403920",
    "end": "3410079"
  },
  {
    "text": "you can have it either one or uh 32 so 32 means that we are actually processing",
    "start": "3410079",
    "end": "3415880"
  },
  {
    "text": "3 two chunks at the same time and this is a standalone um endpoint so yeah the",
    "start": "3415880",
    "end": "3421520"
  },
  {
    "text": "Coe is set to um false and I'll show later um if you want to run the",
    "start": "3421520",
    "end": "3426640"
  },
  {
    "text": "endpoint um for the embedding on your laptop uh how you can change the configs um for that all right and then yeah",
    "start": "3426640",
    "end": "3435119"
  },
  {
    "text": "let's run the uh vectorization and after this we're actually storing or indexing the in Bedding vectors um into the",
    "start": "3435119",
    "end": "3442640"
  },
  {
    "text": "chroma DB Vector store I think yeah this should take around uh",
    "start": "3442640",
    "end": "3448240"
  },
  {
    "text": "10 seconds let me see what's",
    "start": "3448240",
    "end": "3451960"
  },
  {
    "text": "happening",
    "start": "3456240",
    "end": "3459240"
  },
  {
    "text": "yeah yeah always good to restart the notebooks I'll just go over the steps",
    "start": "3463799",
    "end": "3470079"
  },
  {
    "text": "again okay",
    "start": "3473799",
    "end": "3477799"
  },
  {
    "text": "yeah so it took you know four seconds to embed the whole thing and uh yeah this is where um we",
    "start": "3481039",
    "end": "3488000"
  },
  {
    "text": "will initialize our um QA chain um so again we have different uh wrappers and",
    "start": "3488000",
    "end": "3494000"
  },
  {
    "text": "classes which I can go over it in details um afterwards but for now let's",
    "start": "3494000",
    "end": "3499359"
  },
  {
    "text": "just execute the cell yeah now you're ready to go um ask a question and then",
    "start": "3499359",
    "end": "3505440"
  },
  {
    "text": "what happens is is right through the QA chain the question gets embedded to the vector uh space we retrieve the uh top K",
    "start": "3505440",
    "end": "3514599"
  },
  {
    "text": "chunks so in this experiment yeah we have this set with three since I won't",
    "start": "3514599",
    "end": "3520000"
  },
  {
    "text": "be using a ranker but I can also show you how to use the ranker and then yeah",
    "start": "3520000",
    "end": "3526000"
  },
  {
    "text": "the question and the context are provided as uh basically contacts for",
    "start": "3526000",
    "end": "3532559"
  },
  {
    "text": "the llm to get the answer so yeah what is a monotic model and you can see right",
    "start": "3532559",
    "end": "3538559"
  },
  {
    "text": "the response is um instantaneous yeah so that's it for the",
    "start": "3538559",
    "end": "3543960"
  },
  {
    "text": "um uh experiment uh let's maybe try ask it a bit more complicated question so if",
    "start": "3543960",
    "end": "3549920"
  },
  {
    "text": "I open the PDF again",
    "start": "3549920",
    "end": "3555359"
  },
  {
    "text": "uh thank yeah there was a table in the PDF",
    "start": "3561200",
    "end": "3566640"
  },
  {
    "text": "yeah like this is a table you know showing um operation intensity versus",
    "start": "3570000",
    "end": "3575039"
  },
  {
    "text": "Fusion level so yeah let's see if P PDF is able to um uh retrieve uh some of the",
    "start": "3575039",
    "end": "3582079"
  },
  {
    "text": "information from this table here so I already have the uh questions",
    "start": "3582079",
    "end": "3587160"
  },
  {
    "text": "prepared let's try to access those yeah so yeah we got the response",
    "start": "3587160",
    "end": "3593200"
  },
  {
    "text": "right so 41.4 basically um and again if you end up having like more complicated",
    "start": "3593200",
    "end": "3598440"
  },
  {
    "text": "um tables in PDF um in this case I would recommend to switch to the unstructured um data loader and for this yeah all you",
    "start": "3598440",
    "end": "3605480"
  },
  {
    "text": "have to do is just go to the config file and then um change uh P PDF to um",
    "start": "3605480",
    "end": "3611920"
  },
  {
    "text": "unstructured so yeah that's it for uh this second um exercise",
    "start": "3611920",
    "end": "3619920"
  },
  {
    "text": "um I can go into more details about each uh function if you're interested um or",
    "start": "3619920",
    "end": "3627640"
  },
  {
    "text": "have you try it out first and then we can maybe uh come back and then um go over the the functions um yeah so do you",
    "start": "3627640",
    "end": "3635480"
  },
  {
    "text": "want maybe try it out first I guess okay great",
    "start": "3635480",
    "end": "3640960"
  },
  {
    "text": "[Music]",
    "start": "3641530",
    "end": "3656119"
  },
  {
    "text": "oh",
    "start": "3656119",
    "end": "3659119"
  }
]