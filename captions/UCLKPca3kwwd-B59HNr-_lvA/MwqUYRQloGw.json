[
  {
    "start": "0",
    "end": "41000"
  },
  {
    "text": "[Music]",
    "start": "1040",
    "end": "13859"
  },
  {
    "text": "right good day everyone good to see you",
    "start": "14240",
    "end": "16320"
  },
  {
    "text": "all today I'm here to tell you to how to",
    "start": "16320",
    "end": "19119"
  },
  {
    "text": "harness the power of local LMS using our",
    "start": "19119",
    "end": "21400"
  },
  {
    "text": "rust",
    "start": "21400",
    "end": "22359"
  },
  {
    "text": "Library quick intro I'm aone as you just",
    "start": "22359",
    "end": "25240"
  },
  {
    "text": "heard but I go by Phil packs online I",
    "start": "25240",
    "end": "27760"
  },
  {
    "text": "hail from Australia H the accent but I",
    "start": "27760",
    "end": "30000"
  },
  {
    "text": "live in Sweden I do a lot of things for",
    "start": "30000",
    "end": "31840"
  },
  {
    "text": "computers but my day job is at ambient",
    "start": "31840",
    "end": "33840"
  },
  {
    "text": "where I build a game engine of the",
    "start": "33840",
    "end": "35200"
  },
  {
    "text": "future today though I'm here to talk to",
    "start": "35200",
    "end": "37440"
  },
  {
    "text": "you about lm. RS a rushed library that I",
    "start": "37440",
    "end": "41320"
  },
  {
    "start": "41000",
    "end": "163000"
  },
  {
    "text": "maintain so lm. RS or LM between friends",
    "start": "41320",
    "end": "45960"
  },
  {
    "text": "I realized that after dis ambigu when I",
    "start": "45960",
    "end": "47600"
  },
  {
    "text": "started when I signed the Simon",
    "start": "47600",
    "end": "49039"
  },
  {
    "text": "newsletter it's an all in one solution",
    "start": "49039",
    "end": "51399"
  },
  {
    "text": "for local inference of llms but what",
    "start": "51399",
    "end": "53559"
  },
  {
    "text": "does that actually mean well most of the",
    "start": "53559",
    "end": "56320"
  },
  {
    "text": "models we've discussed at this",
    "start": "56320",
    "end": "57280"
  },
  {
    "text": "conference have been Cloud models you",
    "start": "57280",
    "end": "59359"
  },
  {
    "text": "chat P your claws your BS local models",
    "start": "59359",
    "end": "63000"
  },
  {
    "text": "offer another way where you own the",
    "start": "63000",
    "end": "65439"
  },
  {
    "text": "model and it runs on your computer so",
    "start": "65439",
    "end": "67880"
  },
  {
    "text": "let's quickly go over what that actually",
    "start": "67880",
    "end": "70799"
  },
  {
    "text": "means first up size model size can be",
    "start": "70799",
    "end": "74960"
  },
  {
    "text": "used as a rough proxy for the",
    "start": "74960",
    "end": "76720"
  },
  {
    "text": "intelligence of the model most to models",
    "start": "76720",
    "end": "79240"
  },
  {
    "text": "are really really big you can see that",
    "start": "79240",
    "end": "82040"
  },
  {
    "text": "it's dominating the right hand side of",
    "start": "82040",
    "end": "83360"
  },
  {
    "text": "the chart there you have your gpt3 your",
    "start": "83360",
    "end": "86680"
  },
  {
    "text": "GPD 4 we'll get back to that you go for",
    "start": "86680",
    "end": "88920"
  },
  {
    "text": "your palm 2 these are all insanely big",
    "start": "88920",
    "end": "91479"
  },
  {
    "text": "in comparison to the open source models",
    "start": "91479",
    "end": "92880"
  },
  {
    "text": "we have we we're beginning to see some",
    "start": "92880",
    "end": "95520"
  },
  {
    "text": "uh bigger models thanks to Lon Falcon",
    "start": "95520",
    "end": "98119"
  },
  {
    "text": "but even they pale in comparison to what",
    "start": "98119",
    "end": "99520"
  },
  {
    "text": "the bigger players can do this means the",
    "start": "99520",
    "end": "101759"
  },
  {
    "text": "local models don't have the same",
    "start": "101759",
    "end": "103560"
  },
  {
    "text": "capacity for intelligence however a",
    "start": "103560",
    "end": "106200"
  },
  {
    "text": "smaller more focused model may be able",
    "start": "106200",
    "end": "108680"
  },
  {
    "text": "to solve problems better than a large",
    "start": "108680",
    "end": "111000"
  },
  {
    "text": "General model by the way we don't",
    "start": "111000",
    "end": "113360"
  },
  {
    "text": "actually know what size gb4 is that's",
    "start": "113360",
    "end": "115520"
  },
  {
    "text": "rumors uh only open AI",
    "start": "115520",
    "end": "118840"
  },
  {
    "text": "knows next let's talk about speed and",
    "start": "118840",
    "end": "122280"
  },
  {
    "text": "capacity Cloud models run on Specialized",
    "start": "122280",
    "end": "124600"
  },
  {
    "text": "Hardware with special configuration",
    "start": "124600",
    "end": "126479"
  },
  {
    "text": "local models run on whatever Hardware",
    "start": "126479",
    "end": "128360"
  },
  {
    "text": "you can scrun up including rented",
    "start": "128360",
    "end": "130239"
  },
  {
    "text": "Hardware the further up the access you",
    "start": "130239",
    "end": "132120"
  },
  {
    "text": "go the more speed and or parallel",
    "start": "132120",
    "end": "133959"
  },
  {
    "text": "inference you can do but the more",
    "start": "133959",
    "end": "135800"
  },
  {
    "text": "inaccessible becomes this end a few",
    "start": "135800",
    "end": "138640"
  },
  {
    "text": "hundred that end a few hundred",
    "start": "138640",
    "end": "142840"
  },
  {
    "text": "million next up latency Cloud models",
    "start": "142959",
    "end": "146519"
  },
  {
    "text": "need the full prompt before they can",
    "start": "146519",
    "end": "148319"
  },
  {
    "text": "start inference and you have to wait for",
    "start": "148319",
    "end": "150120"
  },
  {
    "text": "the message back and uh back and forth",
    "start": "150120",
    "end": "152519"
  },
  {
    "text": "local models can give you a response",
    "start": "152519",
    "end": "154040"
  },
  {
    "text": "immediately you can feed the prompt as",
    "start": "154040",
    "end": "155920"
  },
  {
    "text": "you go along this is very important for",
    "start": "155920",
    "end": "157959"
  },
  {
    "text": "conversations where you want the model",
    "start": "157959",
    "end": "159879"
  },
  {
    "text": "to be able to process what you're saying",
    "start": "159879",
    "end": "161720"
  },
  {
    "text": "as you say",
    "start": "161720",
    "end": "163040"
  },
  {
    "start": "163000",
    "end": "237000"
  },
  {
    "text": "it and of course you can't escape",
    "start": "163040",
    "end": "165599"
  },
  {
    "text": "talking about cost the cloud vendors",
    "start": "165599",
    "end": "167879"
  },
  {
    "text": "will charge you a per token price when",
    "start": "167879",
    "end": "170400"
  },
  {
    "text": "running locally it's entirely up to you",
    "start": "170400",
    "end": "172519"
  },
  {
    "text": "how much it costs you to run the machine",
    "start": "172519",
    "end": "175080"
  },
  {
    "text": "if the running cost of your uh model is",
    "start": "175080",
    "end": "177360"
  },
  {
    "text": "less than the cost of running your",
    "start": "177360",
    "end": "178280"
  },
  {
    "text": "workload through the cloud you're going",
    "start": "178280",
    "end": "179959"
  },
  {
    "text": "to make a profit and if you're running",
    "start": "179959",
    "end": "181920"
  },
  {
    "text": "on a machine you already own well that's",
    "start": "181920",
    "end": "184239"
  },
  {
    "text": "basically free",
    "start": "184239",
    "end": "185799"
  },
  {
    "text": "right with the cloud you have to offer",
    "start": "185799",
    "end": "188319"
  },
  {
    "text": "you have to use the models they offer",
    "start": "188319",
    "end": "189440"
  },
  {
    "text": "you some vendors offer fine tuning but",
    "start": "189440",
    "end": "192239"
  },
  {
    "text": "they often charge more than uh just",
    "start": "192239",
    "end": "194959"
  },
  {
    "text": "using the regular model and they often",
    "start": "194959",
    "end": "196239"
  },
  {
    "text": "charge you for the process of actually",
    "start": "196239",
    "end": "197640"
  },
  {
    "text": "fine-tuning this means that it's not",
    "start": "197640",
    "end": "199640"
  },
  {
    "text": "often cost effective to actually do that",
    "start": "199640",
    "end": "202519"
  },
  {
    "text": "with local models the sky the limit",
    "start": "202519",
    "end": "204920"
  },
  {
    "text": "there are hundreds potentially thousands",
    "start": "204920",
    "end": "207159"
  },
  {
    "text": "of custom models that can suit any need",
    "start": "207159",
    "end": "209360"
  },
  {
    "text": "you have",
    "start": "209360",
    "end": "210360"
  },
  {
    "text": "knowledge retrieval storytelling",
    "start": "210360",
    "end": "212360"
  },
  {
    "text": "conversation tool use you name it",
    "start": "212360",
    "end": "214560"
  },
  {
    "text": "someone's probably already done it and",
    "start": "214560",
    "end": "216439"
  },
  {
    "text": "if they haven't find tuning an existing",
    "start": "216439",
    "end": "218519"
  },
  {
    "text": "model for your own use is easy enough",
    "start": "218519",
    "end": "221000"
  },
  {
    "text": "special shout out to axle over there",
    "start": "221000",
    "end": "223000"
  },
  {
    "text": "which makes it easy to find models of",
    "start": "223000",
    "end": "224840"
  },
  {
    "text": "any",
    "start": "224840",
    "end": "226080"
  },
  {
    "text": "architecture",
    "start": "226080",
    "end": "228159"
  },
  {
    "text": "and of course privacy there are some",
    "start": "228159",
    "end": "231799"
  },
  {
    "text": "questions you don't want to ask the",
    "start": "231799",
    "end": "233000"
  },
  {
    "text": "internet local mods let you privately",
    "start": "233000",
    "end": "235200"
  },
  {
    "text": "embarrass",
    "start": "235200",
    "end": "237599"
  },
  {
    "start": "237000",
    "end": "340000"
  },
  {
    "text": "yourself now you might be wondering how",
    "start": "237599",
    "end": "239959"
  },
  {
    "text": "it's actually possible to run these",
    "start": "239959",
    "end": "241319"
  },
  {
    "text": "models locally that my friends is",
    "start": "241319",
    "end": "243959"
  },
  {
    "text": "possible with the power of",
    "start": "243959",
    "end": "245200"
  },
  {
    "text": "quantization if each model is billions",
    "start": "245200",
    "end": "247439"
  },
  {
    "text": "of parameters and those parameters are",
    "start": "247439",
    "end": "249519"
  },
  {
    "text": "are like individual numbers how could",
    "start": "249519",
    "end": "251360"
  },
  {
    "text": "you how could you possibly run them a",
    "start": "251360",
    "end": "252680"
  },
  {
    "text": "consumer Hardware when there's only so",
    "start": "252680",
    "end": "254280"
  },
  {
    "text": "much memory given for available for a",
    "start": "254280",
    "end": "255840"
  },
  {
    "text": "given uh performance level well we can",
    "start": "255840",
    "end": "258400"
  },
  {
    "text": "use quantization quantization lets you",
    "start": "258400",
    "end": "260799"
  },
  {
    "text": "Lely compress a model while maintaining",
    "start": "260799",
    "end": "262520"
  },
  {
    "text": "the majority of its Ms we can take the",
    "start": "262520",
    "end": "264440"
  },
  {
    "text": "original model here in blue and squish",
    "start": "264440",
    "end": "267080"
  },
  {
    "text": "it down to something much smaller using",
    "start": "267080",
    "end": "268639"
  },
  {
    "text": "one of these green formats",
    "start": "268639",
    "end": "270360"
  },
  {
    "text": "this is a secret source that makes it",
    "start": "270360",
    "end": "272400"
  },
  {
    "text": "viable to run models locally small",
    "start": "272400",
    "end": "275039"
  },
  {
    "text": "models aren't easier to store aren't",
    "start": "275039",
    "end": "276440"
  },
  {
    "text": "just easier to store they can also run",
    "start": "276440",
    "end": "278360"
  },
  {
    "text": "faster as your process uh as your",
    "start": "278360",
    "end": "280639"
  },
  {
    "text": "computer can process more of the model",
    "start": "280639",
    "end": "281960"
  },
  {
    "text": "at any given",
    "start": "281960",
    "end": "283600"
  },
  {
    "text": "moment but that's enough about local",
    "start": "283600",
    "end": "286199"
  },
  {
    "text": "models you've probably already heard",
    "start": "286199",
    "end": "287560"
  },
  {
    "text": "much much that already let's talk about",
    "start": "287560",
    "end": "289800"
  },
  {
    "text": "the actual",
    "start": "289800",
    "end": "291039"
  },
  {
    "text": "Library it all started with this man who",
    "start": "291039",
    "end": "293680"
  },
  {
    "text": "buil something you may have heard of of",
    "start": "293680",
    "end": "296039"
  },
  {
    "text": "course I'm referring to L cppp and",
    "start": "296039",
    "end": "298199"
  },
  {
    "text": "that's what it looked like on day one",
    "start": "298199",
    "end": "299560"
  },
  {
    "text": "look at the mere 98 Stars how pedestrian",
    "start": "299560",
    "end": "302080"
  },
  {
    "text": "compared to today we're it's 42,000",
    "start": "302080",
    "end": "304840"
  },
  {
    "text": "Stars uh but let's go back to March when",
    "start": "304840",
    "end": "307400"
  },
  {
    "text": "I first saw it when I saw it I had but",
    "start": "307400",
    "end": "309639"
  },
  {
    "text": "one idea it's time to reroute it and",
    "start": "309639",
    "end": "312800"
  },
  {
    "text": "rust for both the meme and because I",
    "start": "312800",
    "end": "315280"
  },
  {
    "text": "wanted to do use it for other things",
    "start": "315280",
    "end": "317759"
  },
  {
    "text": "well I wanted to say I well I said I",
    "start": "317759",
    "end": "319400"
  },
  {
    "text": "wanted to do it and I did but to the",
    "start": "319400",
    "end": "321440"
  },
  {
    "text": "briide here set of 22 was also working",
    "start": "321440",
    "end": "323919"
  },
  {
    "text": "on the same problem and well there was",
    "start": "323919",
    "end": "326240"
  },
  {
    "text": "just one catch he beat me it he beat me",
    "start": "326240",
    "end": "329240"
  },
  {
    "text": "to it completely beat me to it I'm not",
    "start": "329240",
    "end": "331000"
  },
  {
    "text": "afraid to admit it luckily we came",
    "start": "331000",
    "end": "334000"
  },
  {
    "text": "together managed our projects and I",
    "start": "334000",
    "end": "335759"
  },
  {
    "text": "ended up as a maintainer of the",
    "start": "335759",
    "end": "337280"
  },
  {
    "text": "resulting project and that's how LM was",
    "start": "337280",
    "end": "340360"
  },
  {
    "start": "340000",
    "end": "529000"
  },
  {
    "text": "born so you might be wondering why if",
    "start": "340360",
    "end": "344080"
  },
  {
    "text": "llama CPP exists why use lmrs well with",
    "start": "344080",
    "end": "348360"
  },
  {
    "text": "lm. RS I had six principles in mind it",
    "start": "348360",
    "end": "351360"
  },
  {
    "text": "must be a library when I first started",
    "start": "351360",
    "end": "354080"
  },
  {
    "text": "in March llama CPP was not a library it",
    "start": "354080",
    "end": "356680"
  },
  {
    "text": "was an application and that made it",
    "start": "356680",
    "end": "358479"
  },
  {
    "text": "impossible to reuse",
    "start": "358479",
    "end": "360039"
  },
  {
    "text": "it must not be coupled to an application",
    "start": "360039",
    "end": "362440"
  },
  {
    "text": "you must be able to customize Its",
    "start": "362440",
    "end": "363680"
  },
  {
    "text": "Behavior you must be able to go in and",
    "start": "363680",
    "end": "365680"
  },
  {
    "text": "change every little bit of it to make it",
    "start": "365680",
    "end": "367479"
  },
  {
    "text": "work for your application and we we",
    "start": "367479",
    "end": "369440"
  },
  {
    "text": "shouldn't make any assumptions about how",
    "start": "369440",
    "end": "371120"
  },
  {
    "text": "it's going to be used uh it should",
    "start": "371120",
    "end": "373240"
  },
  {
    "text": "support a multitude of model",
    "start": "373240",
    "end": "374599"
  },
  {
    "text": "architectures of course llama CPP",
    "start": "374599",
    "end": "376800"
  },
  {
    "text": "supports llama and our Falcon but",
    "start": "376800",
    "end": "379160"
  },
  {
    "text": "clearly there are more out there next up",
    "start": "379160",
    "end": "381919"
  },
  {
    "text": "it should be rust native it should feel",
    "start": "381919",
    "end": "383880"
  },
  {
    "text": "like using a rust Library it shouldn't",
    "start": "383880",
    "end": "385639"
  },
  {
    "text": "feel like using a a library with",
    "start": "385639",
    "end": "386919"
  },
  {
    "text": "bindings and it should feel work how you",
    "start": "386919",
    "end": "388639"
  },
  {
    "text": "expect a rust library to work next up",
    "start": "388639",
    "end": "391319"
  },
  {
    "text": "backends it should support all other all",
    "start": "391319",
    "end": "394000"
  },
  {
    "text": "possible kinds of backends you can write",
    "start": "394000",
    "end": "395280"
  },
  {
    "text": "on your CPU your GPU or of course your",
    "start": "395280",
    "end": "397560"
  },
  {
    "text": "ml power toaster I'm sure that's going",
    "start": "397560",
    "end": "399199"
  },
  {
    "text": "to be a thing we we we were going to see",
    "start": "399199",
    "end": "401120"
  },
  {
    "text": "it coming I'm I swear and finally",
    "start": "401120",
    "end": "403639"
  },
  {
    "text": "platforms it should work the same",
    "start": "403639",
    "end": "405520"
  },
  {
    "text": "whether it's on Windows Linux Mac OS or",
    "start": "405520",
    "end": "408120"
  },
  {
    "text": "something else it shouldn't have you",
    "start": "408120",
    "end": "410520"
  },
  {
    "text": "shouldn't have to change it",
    "start": "410520",
    "end": "411160"
  },
  {
    "text": "significantly to make it work because",
    "start": "411160",
    "end": "413120"
  },
  {
    "text": "deployment has always been an",
    "start": "413120",
    "end": "415080"
  },
  {
    "text": "issue today I'm proud to say we support",
    "start": "415080",
    "end": "417720"
  },
  {
    "text": "a myriad of architectures incl including",
    "start": "417720",
    "end": "419680"
  },
  {
    "text": "the uh The Darlings of the movement",
    "start": "419680",
    "end": "421280"
  },
  {
    "text": "llama and Falcon these architectures all",
    "start": "421280",
    "end": "423599"
  },
  {
    "text": "use the same interface so you don't have",
    "start": "423599",
    "end": "425400"
  },
  {
    "text": "to worry about changing your code to use",
    "start": "425400",
    "end": "427000"
  },
  {
    "text": "a different model this is made possible",
    "start": "427000",
    "end": "429000"
  },
  {
    "text": "by the coordinate coordinate concerted",
    "start": "429000",
    "end": "430720"
  },
  {
    "text": "efforts by co-contributors Lucas and Dan",
    "start": "430720",
    "end": "433800"
  },
  {
    "text": "who couldn't have done this without as",
    "start": "433800",
    "end": "435319"
  },
  {
    "text": "well as well as many",
    "start": "435319",
    "end": "436960"
  },
  {
    "text": "others here's some sample code for the",
    "start": "436960",
    "end": "439360"
  },
  {
    "text": "library I won't go too much into it",
    "start": "439360",
    "end": "441319"
  },
  {
    "text": "because it's quite dense but the idea is",
    "start": "441319",
    "end": "444280"
  },
  {
    "text": "that you load a model right there at the",
    "start": "444280",
    "end": "445759"
  },
  {
    "text": "top you can see it's actually quite",
    "start": "445759",
    "end": "446840"
  },
  {
    "text": "small and with that model you create",
    "start": "446840",
    "end": "448639"
  },
  {
    "text": "sessions which an ongoing use to the",
    "start": "448639",
    "end": "450400"
  },
  {
    "text": "model you can have as many of these as",
    "start": "450400",
    "end": "452360"
  },
  {
    "text": "you would like but they do have a memory",
    "start": "452360",
    "end": "454039"
  },
  {
    "text": "cost so you want to be careful once you",
    "start": "454039",
    "end": "456199"
  },
  {
    "text": "have a session you can pass you can pass",
    "start": "456199",
    "end": "458160"
  },
  {
    "text": "a prompt in and infer with the model to",
    "start": "458160",
    "end": "460000"
  },
  {
    "text": "determine what comes next you can keep",
    "start": "460000",
    "end": "462080"
  },
  {
    "text": "reusing the same session which is very",
    "start": "462080",
    "end": "463520"
  },
  {
    "text": "useful for conversation you don't need",
    "start": "463520",
    "end": "465319"
  },
  {
    "text": "to keep ring the context the last",
    "start": "465319",
    "end": "467560"
  },
  {
    "text": "argument of the call of the function is",
    "start": "467560",
    "end": "469960"
  },
  {
    "text": "the Callback that's where you actually",
    "start": "469960",
    "end": "471680"
  },
  {
    "text": "get the tokens out um it's worth noting",
    "start": "471680",
    "end": "474599"
  },
  {
    "text": "that the function itself is actually a",
    "start": "474599",
    "end": "476159"
  },
  {
    "text": "helper all it does is call the model in",
    "start": "476159",
    "end": "479000"
  },
  {
    "text": "a loop loop with some boundary",
    "start": "479000",
    "end": "480440"
  },
  {
    "text": "conditions so if you want to change the",
    "start": "480440",
    "end": "481919"
  },
  {
    "text": "logic in some significant way you can",
    "start": "481919",
    "end": "484240"
  },
  {
    "text": "we're not going to stop you from doing",
    "start": "484240",
    "end": "485560"
  },
  {
    "text": "that one last thing about this though",
    "start": "485560",
    "end": "488120"
  },
  {
    "text": "you see all the Cs to default there",
    "start": "488120",
    "end": "490000"
  },
  {
    "text": "those are all customization points you",
    "start": "490000",
    "end": "491720"
  },
  {
    "text": "can change pretty much anything about",
    "start": "491720",
    "end": "493080"
  },
  {
    "text": "this you can change how the model is",
    "start": "493080",
    "end": "494280"
  },
  {
    "text": "loaded you can change how it'll do the",
    "start": "494280",
    "end": "496199"
  },
  {
    "text": "inference you can change how it sample",
    "start": "496199",
    "end": "498520"
  },
  {
    "text": "the entire point is you have the control",
    "start": "498520",
    "end": "500360"
  },
  {
    "text": "you need to make the thing uh you need",
    "start": "500360",
    "end": "502720"
  },
  {
    "text": "to",
    "start": "502720",
    "end": "503840"
  },
  {
    "text": "work here's a quick demo of uh the",
    "start": "503840",
    "end": "507440"
  },
  {
    "text": "library working with llama 7 uh",
    "start": "507440",
    "end": "509759"
  },
  {
    "text": "on my MacBook CPU it's reasonbly fast",
    "start": "509759",
    "end": "512880"
  },
  {
    "text": "but it could be faster right well thanks",
    "start": "512880",
    "end": "516680"
  },
  {
    "text": "to the power",
    "start": "516680",
    "end": "519039"
  },
  {
    "text": "of GPU acceleration we have something",
    "start": "519039",
    "end": "522360"
  },
  {
    "text": "that's much more usable and believe me",
    "start": "522360",
    "end": "525160"
  },
  {
    "text": "it's even faster than nid gpus AMD and",
    "start": "525160",
    "end": "527440"
  },
  {
    "text": "Intel support uh",
    "start": "527440",
    "end": "529880"
  },
  {
    "start": "529000",
    "end": "680000"
  },
  {
    "text": "pending now let's talk about what you",
    "start": "529880",
    "end": "531720"
  },
  {
    "text": "can actually do with a",
    "start": "531720",
    "end": "533120"
  },
  {
    "text": "library let's start with three Community",
    "start": "533120",
    "end": "535080"
  },
  {
    "text": "projects to begin with first we've got",
    "start": "535080",
    "end": "536920"
  },
  {
    "text": "local AI local AI is a simple app that",
    "start": "536920",
    "end": "540640"
  },
  {
    "text": "you can install to do inference locally",
    "start": "540640",
    "end": "543040"
  },
  {
    "text": "there's nothing magical about it it's",
    "start": "543040",
    "end": "544600"
  },
  {
    "text": "just exactly what it says I think that's",
    "start": "544600",
    "end": "546640"
  },
  {
    "text": "really wonderful because it it means",
    "start": "546640",
    "end": "548440"
  },
  {
    "text": "anyone can download this app and get",
    "start": "548440",
    "end": "550440"
  },
  {
    "text": "ready uh get be able to use local models",
    "start": "550440",
    "end": "552760"
  },
  {
    "text": "without think about it next up LM chain",
    "start": "552760",
    "end": "555760"
  },
  {
    "text": "it's a lang chain but for rust and of",
    "start": "555760",
    "end": "558200"
  },
  {
    "text": "course it's Sports inference with our",
    "start": "558200",
    "end": "559360"
  },
  {
    "text": "library and finally we have flum which",
    "start": "559360",
    "end": "561880"
  },
  {
    "text": "is a flowchart based application where",
    "start": "561880",
    "end": "563720"
  },
  {
    "text": "you can build your own workflo I think",
    "start": "563720",
    "end": "565000"
  },
  {
    "text": "we've seen a few of that few of those at",
    "start": "565000",
    "end": "566480"
  },
  {
    "text": "this this conference and you can combine",
    "start": "566480",
    "end": "568880"
  },
  {
    "text": "and create V nodes to uh build the",
    "start": "568880",
    "end": "571360"
  },
  {
    "text": "workflow you need and of course it",
    "start": "571360",
    "end": "573200"
  },
  {
    "text": "supports the library as an inference",
    "start": "573200",
    "end": "575480"
  },
  {
    "text": "engine now I wouldn't be a very good",
    "start": "575480",
    "end": "578000"
  },
  {
    "text": "Library author if I didn't actually test",
    "start": "578000",
    "end": "579640"
  },
  {
    "text": "my own Library so I'm going to go",
    "start": "579640",
    "end": "581600"
  },
  {
    "text": "through three applications the first two",
    "start": "581600",
    "end": "583920"
  },
  {
    "text": "approves the concept the first is LM",
    "start": "583920",
    "end": "586040"
  },
  {
    "text": "code it's a Discord",
    "start": "586040",
    "end": "588240"
  },
  {
    "text": "bot you can see it's exactly what you'd",
    "start": "588240",
    "end": "590560"
  },
  {
    "text": "expect you send uh give it a prompt",
    "start": "590560",
    "end": "593079"
  },
  {
    "text": "it'll give you a response any hitches",
    "start": "593079",
    "end": "595079"
  },
  {
    "text": "you see come from Discord limits not",
    "start": "595079",
    "end": "596920"
  },
  {
    "text": "from the actual uh inferencing itself",
    "start": "596920",
    "end": "598720"
  },
  {
    "text": "you can see bam all",
    "start": "598720",
    "end": "602760"
  },
  {
    "text": "there when an issue when our us issues a",
    "start": "602760",
    "end": "605880"
  },
  {
    "text": "request for Generation it goes through",
    "start": "605880",
    "end": "607680"
  },
  {
    "text": "this process here where the request goes",
    "start": "607680",
    "end": "609800"
  },
  {
    "text": "through a generation thread uh with a",
    "start": "609800",
    "end": "611839"
  },
  {
    "text": "channel that channel is then used uh to",
    "start": "611839",
    "end": "614560"
  },
  {
    "text": "create a response task and then that",
    "start": "614560",
    "end": "617200"
  },
  {
    "text": "response task is responsible for sending",
    "start": "617200",
    "end": "619560"
  },
  {
    "text": "the responses to the uh user now the",
    "start": "619560",
    "end": "623160"
  },
  {
    "text": "interesting thing is these sessions are",
    "start": "623160",
    "end": "625240"
  },
  {
    "text": "created and thrown away immediately with",
    "start": "625240",
    "end": "626920"
  },
  {
    "text": "each query but you don't need to do that",
    "start": "626920",
    "end": "629279"
  },
  {
    "text": "if you keep them around you can actually",
    "start": "629279",
    "end": "630839"
  },
  {
    "text": "use them for",
    "start": "630839",
    "end": "632519"
  },
  {
    "text": "conversation and just to illustrate this",
    "start": "632519",
    "end": "634680"
  },
  {
    "text": "is just like the request response",
    "start": "634680",
    "end": "636279"
  },
  {
    "text": "workflow you would use for anything if I",
    "start": "636279",
    "end": "638160"
  },
  {
    "text": "just take what I had there drop the",
    "start": "638160",
    "end": "640240"
  },
  {
    "text": "Disco bit and add in HTTP you can see",
    "start": "640240",
    "end": "642959"
  },
  {
    "text": "request generation response",
    "start": "642959",
    "end": "646120"
  },
  {
    "text": "easy next up Alpa I love using GitHub",
    "start": "646120",
    "end": "649519"
  },
  {
    "text": "co-pilot but it's only available in my",
    "start": "649519",
    "end": "651440"
  },
  {
    "text": "code data and it requires internet",
    "start": "651440",
    "end": "653079"
  },
  {
    "text": "connection Alpa is my attempt to solve",
    "start": "653079",
    "end": "655480"
  },
  {
    "text": "this it is order to complete anywhere in",
    "start": "655480",
    "end": "658079"
  },
  {
    "text": "your system just by taking what's left",
    "start": "658079",
    "end": "660200"
  },
  {
    "text": "of your cursor and uh having passing to",
    "start": "660200",
    "end": "663120"
  },
  {
    "text": "a model to type in and of course you can",
    "start": "663120",
    "end": "665440"
  },
  {
    "text": "use any model including a Model F tuned",
    "start": "665440",
    "end": "667440"
  },
  {
    "text": "in own writing ask me how I",
    "start": "667440",
    "end": "670160"
  },
  {
    "text": "know Alp is also quite simple in fact",
    "start": "670160",
    "end": "672839"
  },
  {
    "text": "it's so simple I don't really need to",
    "start": "672839",
    "end": "673959"
  },
  {
    "text": "cover it listen for input copy the input",
    "start": "673959",
    "end": "677200"
  },
  {
    "text": "um into a prompt start generating type",
    "start": "677200",
    "end": "679639"
  },
  {
    "text": "out response",
    "start": "679639",
    "end": "681360"
  },
  {
    "start": "680000",
    "end": "764000"
  },
  {
    "text": "easy now the first two examples were",
    "start": "681360",
    "end": "683920"
  },
  {
    "text": "pretty simple they approves the concept",
    "start": "683920",
    "end": "685880"
  },
  {
    "text": "but now I want to talk about an actual",
    "start": "685880",
    "end": "687320"
  },
  {
    "text": "use case this is a real World data",
    "start": "687320",
    "end": "689519"
  },
  {
    "text": "extraction task over the last few years",
    "start": "689519",
    "end": "692240"
  },
  {
    "text": "I've been working on a project to make a",
    "start": "692240",
    "end": "694000"
  },
  {
    "text": "timeline for the dates of Wikipedia",
    "start": "694000",
    "end": "695600"
  },
  {
    "text": "because there are millions of pages and",
    "start": "695600",
    "end": "696959"
  },
  {
    "text": "they all have dates and you can build a",
    "start": "696959",
    "end": "698600"
  },
  {
    "text": "world history from it however these",
    "start": "698600",
    "end": "700959"
  },
  {
    "text": "dates are often unstructured and more or",
    "start": "700959",
    "end": "703120"
  },
  {
    "text": "less impossible to passes and",
    "start": "703120",
    "end": "704160"
  },
  {
    "text": "traditional means like yes you can try",
    "start": "704160",
    "end": "706200"
  },
  {
    "text": "using Rex to extract the dates but you",
    "start": "706200",
    "end": "708160"
  },
  {
    "text": "can't get the context out in any",
    "start": "708160",
    "end": "709320"
  },
  {
    "text": "meaningful sense and there are some",
    "start": "709320",
    "end": "710720"
  },
  {
    "text": "dates here that just don't make any",
    "start": "710720",
    "end": "712079"
  },
  {
    "text": "sense at all so that's why as is the",
    "start": "712079",
    "end": "715880"
  },
  {
    "text": "theme of this conference I threw a large",
    "start": "715880",
    "end": "717600"
  },
  {
    "text": "language model audit how however GPT 3",
    "start": "717600",
    "end": "719920"
  },
  {
    "text": "and 4 aren't perfect even after rounds",
    "start": "719920",
    "end": "721920"
  },
  {
    "text": "of prompt engineering you can see I",
    "start": "721920",
    "end": "723399"
  },
  {
    "text": "tried here and handling millions of",
    "start": "723399",
    "end": "725480"
  },
  {
    "text": "dates is just too expensive and slow so",
    "start": "725480",
    "end": "728680"
  },
  {
    "text": "I decided I'd find CH my own model I",
    "start": "728680",
    "end": "730720"
  },
  {
    "text": "generated a representive data set using",
    "start": "730720",
    "end": "732279"
  },
  {
    "text": "gg3 built a a tool to go through the",
    "start": "732279",
    "end": "734760"
  },
  {
    "text": "data set so pick out any data point fix",
    "start": "734760",
    "end": "736959"
  },
  {
    "text": "it up and then correct the errors build",
    "start": "736959",
    "end": "739199"
  },
  {
    "text": "a new data set and train a new model so",
    "start": "739199",
    "end": "742800"
  },
  {
    "text": "I did that using axx LEL which I",
    "start": "742800",
    "end": "744120"
  },
  {
    "text": "mentioned earlier again check out Axel",
    "start": "744120",
    "end": "745920"
  },
  {
    "text": "for all your find Ching needs highly",
    "start": "745920",
    "end": "747480"
  },
  {
    "text": "recommended and now have a small fast",
    "start": "747480",
    "end": "749680"
  },
  {
    "text": "and consistent model that can pass any",
    "start": "749680",
    "end": "751279"
  },
  {
    "text": "data to sorry any dat to and get back a",
    "start": "751279",
    "end": "753720"
  },
  {
    "text": "structured representation which I can of",
    "start": "753720",
    "end": "756000"
  },
  {
    "text": "course immediately pass using frust and",
    "start": "756000",
    "end": "758760"
  },
  {
    "text": "I can treat that as a black box so I",
    "start": "758760",
    "end": "760399"
  },
  {
    "text": "have a function there FN pass pass some",
    "start": "760399",
    "end": "762279"
  },
  {
    "text": "dates get some dates back",
    "start": "762279",
    "end": "764600"
  },
  {
    "start": "764000",
    "end": "979000"
  },
  {
    "text": "simple now let's quickly talk about the",
    "start": "764600",
    "end": "766959"
  },
  {
    "text": "benefits of using local models and the",
    "start": "766959",
    "end": "769279"
  },
  {
    "text": "library first off deployments show of",
    "start": "769279",
    "end": "772240"
  },
  {
    "text": "hands who's have to deal with python",
    "start": "772240",
    "end": "773920"
  },
  {
    "text": "deployment hell dependency hell",
    "start": "773920",
    "end": "777480"
  },
  {
    "text": "even yeah yeah I know it's it's awful",
    "start": "777480",
    "end": "780519"
  },
  {
    "text": "you spend hours just trying to sort out",
    "start": "780519",
    "end": "782360"
  },
  {
    "text": "your your cond your pip your pipen it's",
    "start": "782360",
    "end": "785360"
  },
  {
    "text": "awful with the library you inherit Russ",
    "start": "785360",
    "end": "788839"
  },
  {
    "text": "excellent crossplatform support and",
    "start": "788839",
    "end": "790120"
  },
  {
    "text": "build system making it easy to ship",
    "start": "790120",
    "end": "792000"
  },
  {
    "text": "self- enclosed support uh binaries to",
    "start": "792000",
    "end": "793920"
  },
  {
    "text": "your platform knowing more on making",
    "start": "793920",
    "end": "795440"
  },
  {
    "text": "your use install torch as you might",
    "start": "795440",
    "end": "797440"
  },
  {
    "text": "imagine this unlocks use of desktop",
    "start": "797440",
    "end": "799079"
  },
  {
    "text": "applications with",
    "start": "799079",
    "end": "800639"
  },
  {
    "text": "models next up the ecosystem rust has",
    "start": "800639",
    "end": "804000"
  },
  {
    "text": "one of the strongest ecosystems of of",
    "start": "804000",
    "end": "805920"
  },
  {
    "text": "any native language you can combine",
    "start": "805920",
    "end": "808079"
  },
  {
    "text": "these libraries with llms to build all",
    "start": "808079",
    "end": "810600"
  },
  {
    "text": "kinds of things it's will let me build a",
    "start": "810600",
    "end": "812240"
  },
  {
    "text": "Discord bot a system or completion",
    "start": "812240",
    "end": "814040"
  },
  {
    "text": "utility a data inje Pipeline with a data",
    "start": "814040",
    "end": "816560"
  },
  {
    "text": "set a utility Explorer all in the same",
    "start": "816560",
    "end": "819040"
  },
  {
    "text": "language and I think if you use lmrs you",
    "start": "819040",
    "end": "821800"
  },
  {
    "text": "can do the same thing with your uh task",
    "start": "821800",
    "end": "823639"
  },
  {
    "text": "as",
    "start": "823639",
    "end": "824440"
  },
  {
    "text": "well of course you also have control",
    "start": "824440",
    "end": "826720"
  },
  {
    "text": "over how uh the model generates I",
    "start": "826720",
    "end": "828399"
  },
  {
    "text": "alluded to this earlier but you can",
    "start": "828399",
    "end": "829800"
  },
  {
    "text": "choose exactly how it samples tokens",
    "start": "829800",
    "end": "832120"
  },
  {
    "text": "normally when you use a cloud model you",
    "start": "832120",
    "end": "833399"
  },
  {
    "text": "have to get back the uh logits the",
    "start": "833399",
    "end": "835680"
  },
  {
    "text": "probabilities but those probabilities",
    "start": "835680",
    "end": "837680"
  },
  {
    "text": "are limited like you have to keep going",
    "start": "837680",
    "end": "839639"
  },
  {
    "text": "back and forth and that's slow and",
    "start": "839639",
    "end": "841079"
  },
  {
    "text": "expensive with this you can directly",
    "start": "841079",
    "end": "843399"
  },
  {
    "text": "control what you are",
    "start": "843399",
    "end": "846560"
  },
  {
    "text": "sampling finally let's talk about the",
    "start": "846560",
    "end": "848600"
  },
  {
    "text": "innovation in space if you're here you",
    "start": "848600",
    "end": "850560"
  },
  {
    "text": "probably know there's a paper almost",
    "start": "850560",
    "end": "852399"
  },
  {
    "text": "every single day it's impossible to keep",
    "start": "852399",
    "end": "854399"
  },
  {
    "text": "up with trust me I've tried but it mean",
    "start": "854399",
    "end": "856800"
  },
  {
    "text": "but the use of local models means you",
    "start": "856800",
    "end": "858040"
  },
  {
    "text": "can try this out before anyone else can",
    "start": "858040",
    "end": "859880"
  },
  {
    "text": "you can go through you can try out some",
    "start": "859880",
    "end": "861120"
  },
  {
    "text": "of these papers and be like oh wow",
    "start": "861120",
    "end": "862440"
  },
  {
    "text": "that's actually worthwhile Improvement",
    "start": "862440",
    "end": "864079"
  },
  {
    "text": "and eventually the cloud providers will",
    "start": "864079",
    "end": "865399"
  },
  {
    "text": "provide them but in the meantime the",
    "start": "865399",
    "end": "867079"
  },
  {
    "text": "controller remains with you",
    "start": "867079",
    "end": "869759"
  },
  {
    "text": "however it's time to talk about the",
    "start": "869759",
    "end": "871519"
  },
  {
    "text": "problems there ain't no such thing as a",
    "start": "871519",
    "end": "873320"
  },
  {
    "text": "free lunch except if you're a conference",
    "start": "873320",
    "end": "875199"
  },
  {
    "text": "of",
    "start": "875199",
    "end": "876839"
  },
  {
    "text": "course let's talk about Hardware again I",
    "start": "876839",
    "end": "880160"
  },
  {
    "text": "mentioned earlier that you can pretty",
    "start": "880160",
    "end": "881800"
  },
  {
    "text": "much run any uh these things on almost",
    "start": "881800",
    "end": "884079"
  },
  {
    "text": "any hardware but that's kind of a lie",
    "start": "884079",
    "end": "886519"
  },
  {
    "text": "you still need some kind of power you",
    "start": "886519",
    "end": "889040"
  },
  {
    "text": "you can only get so much out of your",
    "start": "889040",
    "end": "890199"
  },
  {
    "text": "10-year-old computer your smartphone or",
    "start": "890199",
    "end": "892399"
  },
  {
    "text": "your rasy Pi we're finding clever ways",
    "start": "892399",
    "end": "894480"
  },
  {
    "text": "to improve this like smaller models and",
    "start": "894480",
    "end": "896720"
  },
  {
    "text": "better influencing but it's still",
    "start": "896720",
    "end": "898160"
  },
  {
    "text": "something to be aware of next as with",
    "start": "898160",
    "end": "900880"
  },
  {
    "text": "all things the fast cheap good Tri",
    "start": "900880",
    "end": "903160"
  },
  {
    "text": "applies you can make all kinds of",
    "start": "903160",
    "end": "905240"
  },
  {
    "text": "trade-offs here and you see I've listed",
    "start": "905240",
    "end": "906720"
  },
  {
    "text": "a couple of them here but fundamentally",
    "start": "906720",
    "end": "908839"
  },
  {
    "text": "you have to choose what are you willing",
    "start": "908839",
    "end": "910600"
  },
  {
    "text": "to sacrifice in in order to Ser your",
    "start": "910600",
    "end": "912680"
  },
  {
    "text": "application are you willing to go for a",
    "start": "912680",
    "end": "915320"
  },
  {
    "text": "bigger model to get better quality",
    "start": "915320",
    "end": "916560"
  },
  {
    "text": "results at the cost of speed these are",
    "start": "916560",
    "end": "918800"
  },
  {
    "text": "all decisions you have to make and",
    "start": "918800",
    "end": "921320"
  },
  {
    "text": "they're not always obvious it's",
    "start": "921320",
    "end": "923320"
  },
  {
    "text": "something you have to think",
    "start": "923320",
    "end": "925279"
  },
  {
    "text": "about next there's no other way of",
    "start": "925279",
    "end": "927360"
  },
  {
    "text": "putting this the ecosystem CHS",
    "start": "927360",
    "end": "929959"
  },
  {
    "text": "Innovation is a double- Ed sword when",
    "start": "929959",
    "end": "931920"
  },
  {
    "text": "those changes come in they can often",
    "start": "931920",
    "end": "933480"
  },
  {
    "text": "break your existing workflows I've",
    "start": "933480",
    "end": "935279"
  },
  {
    "text": "helped alleviate this to some extent",
    "start": "935279",
    "end": "937160"
  },
  {
    "text": "using the ggf file format which help",
    "start": "937160",
    "end": "939000"
  },
  {
    "text": "standardize but it's still a problem",
    "start": "939000",
    "end": "941519"
  },
  {
    "text": "some days you will just wake up try",
    "start": "941519",
    "end": "943639"
  },
  {
    "text": "application with a new model and just",
    "start": "943639",
    "end": "945040"
  },
  {
    "text": "won't work there's nothing you can do",
    "start": "945040",
    "end": "946839"
  },
  {
    "text": "except deal with it finally a lot of the",
    "start": "946839",
    "end": "950519"
  },
  {
    "text": "models in this space are open source",
    "start": "950519",
    "end": "953519"
  },
  {
    "text": "they're free for use personally but they",
    "start": "953519",
    "end": "955319"
  },
  {
    "text": "have very strange Clauses and exceptions",
    "start": "955319",
    "end": "957680"
  },
  {
    "text": "for most of us this this doesn't matter",
    "start": "957680",
    "end": "959440"
  },
  {
    "text": "you can just use the model personally",
    "start": "959440",
    "end": "961279"
  },
  {
    "text": "but it's a reminder that even though",
    "start": "961279",
    "end": "962639"
  },
  {
    "text": "that these models are free they're not",
    "start": "962639",
    "end": "964279"
  },
  {
    "text": "capital F free luckily there's been some",
    "start": "964279",
    "end": "967079"
  },
  {
    "text": "recent change in the space with mistal",
    "start": "967079",
    "end": "968920"
  },
  {
    "text": "and stable LM giving you strong",
    "start": "968920",
    "end": "970399"
  },
  {
    "text": "performance of a small level uh sorry a",
    "start": "970399",
    "end": "972480"
  },
  {
    "text": "small size and being completely",
    "start": "972480",
    "end": "974079"
  },
  {
    "text": "unburdened but it's still a problem and",
    "start": "974079",
    "end": "976160"
  },
  {
    "text": "they're still uh much smaller than the",
    "start": "976160",
    "end": "978079"
  },
  {
    "text": "big ones like Lama and",
    "start": "978079",
    "end": "979680"
  },
  {
    "start": "979000",
    "end": "1028000"
  },
  {
    "text": "Falcon unfortunately I've got to wrap",
    "start": "979680",
    "end": "981959"
  },
  {
    "text": "things up here there's only so much you",
    "start": "981959",
    "end": "983560"
  },
  {
    "text": "can talk about in 18 minutes I'm afraid",
    "start": "983560",
    "end": "986040"
  },
  {
    "text": "local models are great and I'd like to",
    "start": "986040",
    "end": "988600"
  },
  {
    "text": "think our library is too they're getting",
    "start": "988600",
    "end": "990440"
  },
  {
    "text": "easier to run day by day with smaller",
    "start": "990440",
    "end": "992160"
  },
  {
    "text": "more powerful models however the",
    "start": "992160",
    "end": "994040"
  },
  {
    "text": "situation isn't perfect and there isn't",
    "start": "994040",
    "end": "995560"
  },
  {
    "text": "always one obvious solution for your",
    "start": "995560",
    "end": "997600"
  },
  {
    "text": "problem thanks for listening you can",
    "start": "997600",
    "end": "1000120"
  },
  {
    "text": "contact me by email or by masteron the",
    "start": "1000120",
    "end": "1002600"
  },
  {
    "text": "library can be found at you guest at lm.",
    "start": "1002600",
    "end": "1004959"
  },
  {
    "text": "RS or by scanning the QR code finally",
    "start": "1004959",
    "end": "1007480"
  },
  {
    "text": "we're always looking for contributors if",
    "start": "1007480",
    "end": "1009000"
  },
  {
    "text": "you're interested in LMS or rust feel",
    "start": "1009000",
    "end": "1011079"
  },
  {
    "text": "free to reach out sponsorships are also",
    "start": "1011079",
    "end": "1013079"
  },
  {
    "text": "very welcome because they help me try",
    "start": "1013079",
    "end": "1014600"
  },
  {
    "text": "out new hardware which is always",
    "start": "1014600",
    "end": "1015880"
  },
  {
    "text": "necessary and if you want to chat in",
    "start": "1015880",
    "end": "1017600"
  },
  {
    "text": "person I'll be hanging around the",
    "start": "1017600",
    "end": "1018639"
  },
  {
    "text": "conference see you",
    "start": "1018639",
    "end": "1021050"
  },
  {
    "text": "[Music]",
    "start": "1021050",
    "end": "1021080"
  },
  {
    "text": "[Applause]",
    "start": "1021080",
    "end": "1024639"
  },
  {
    "text": "later",
    "start": "1027000",
    "end": "1030000"
  }
]