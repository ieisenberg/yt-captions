[
  {
    "text": "[Music]",
    "start": "350",
    "end": "14050"
  },
  {
    "text": "hey everybody yes we're going to get started here uh thanks everybody for coming along um and thanks to AI Engineers for",
    "start": "14639",
    "end": "22000"
  },
  {
    "text": "for having us uh my name is Daman Murphy I'm a senior applied engineer at Deep gr",
    "start": "22000",
    "end": "27480"
  },
  {
    "text": "a lot of people ask me what is an applied engineer it's a customer facing engineer right so we work directly with",
    "start": "27480",
    "end": "32879"
  },
  {
    "text": "customers uh to basically help them achieve their business cases um yeah",
    "start": "32879",
    "end": "38760"
  },
  {
    "text": "about 20 years of experience full stack developer uh been working in pre-sales post sales uh really just customer",
    "start": "38760",
    "end": "44520"
  },
  {
    "text": "facing roles for the last 10 years or so uh I've helped hundreds of companies uh working with deep gram to actually build",
    "start": "44520",
    "end": "51399"
  },
  {
    "text": "uh and scale low latency real-time voice spots uh a lot has obviously changed in the past few few years around how we",
    "start": "51399",
    "end": "57559"
  },
  {
    "text": "actually build these voice spots uh and I'm going to kind of walk you through that Evolution as well if you're not familiar with deep gr",
    "start": "57559",
    "end": "64640"
  },
  {
    "text": "uh we're a foundational AI company that means we build our own models we label",
    "start": "64640",
    "end": "70360"
  },
  {
    "text": "our own data and basically train and deploy at scale um we have a lot of",
    "start": "70360",
    "end": "77040"
  },
  {
    "text": "models we've over a thousand models running in production so we run a lot of custom models um for different use cases",
    "start": "77040",
    "end": "82799"
  },
  {
    "text": "right so you can think of meeting use cases drive-thru use cases phone call use cases uh the vast majority of audio",
    "start": "82799",
    "end": "89479"
  },
  {
    "text": "today is generated in a call center and call center data is probably the worst",
    "start": "89479",
    "end": "94759"
  },
  {
    "text": "audio you've ever heard right it's you know 8K Mula or linear 16 and uh it's",
    "start": "94759",
    "end": "100280"
  },
  {
    "text": "just very hard to actually understand for an AI model so by us building and training models specifically targeted to",
    "start": "100280",
    "end": "107240"
  },
  {
    "text": "that lowquality audio we're able to have a uh a much better performance um we're research Le so you know about 70% of the",
    "start": "107240",
    "end": "114560"
  },
  {
    "text": "company is research and Engineering uh and that means that we we really focus on you know building that foundational",
    "start": "114560",
    "end": "121119"
  },
  {
    "text": "scalable and cost effective AI Solutions um you know building a model is easy",
    "start": "121119",
    "end": "126439"
  },
  {
    "text": "rolling it out into production at a at a low price point is hard yeah so what are we going to learn",
    "start": "126439",
    "end": "133319"
  },
  {
    "text": "today uh we're going to build a voice too AI agent that means you're literally",
    "start": "133319",
    "end": "138480"
  },
  {
    "text": "just going to send audio data in and you're going to get audio data back right so you don't you don't need to",
    "start": "138480",
    "end": "143519"
  },
  {
    "text": "hook in you know the llm the speech to text and the text to speech uh you're going to be able to basically do audio",
    "start": "143519",
    "end": "149680"
  },
  {
    "text": "in audio out uh we're going to build a simple backend API for the AI agent and",
    "start": "149680",
    "end": "154920"
  },
  {
    "text": "that's going to enable us to you know build a front end that shows what's happening uh and also uh allow the llm",
    "start": "154920",
    "end": "161360"
  },
  {
    "text": "to do function calling um and then we're going to talk a little bit about how you could scale you know these AI agent",
    "start": "161360",
    "end": "169440"
  },
  {
    "text": "swarms yeah a couple of prerequisites I'm going to get into those first just so that people have time to kind of uh",
    "start": "169519",
    "end": "175519"
  },
  {
    "text": "install any tooling that they need um I'll help you understand you know the kind of evolution of AI voice Bots right",
    "start": "175519",
    "end": "182599"
  },
  {
    "text": "so how they were previously how they're moving today uh I'll help you get set up uh we'll go over a little bit of",
    "start": "182599",
    "end": "188879"
  },
  {
    "text": "application architecture how it works and then we'll we'll touch on uh scaling it as",
    "start": "188879",
    "end": "194519"
  },
  {
    "text": "well yeah so you're going to want to go to deep.com uh sign up for an account uh",
    "start": "194519",
    "end": "200200"
  },
  {
    "text": "it's a free account you get $200 in credit uh that's about $ 750 hours of of",
    "start": "200200",
    "end": "205280"
  },
  {
    "text": "transcription for free um you'll need no JS installed on your machine um that",
    "start": "205280",
    "end": "211239"
  },
  {
    "text": "will allow you to run a little HTTP server and if you want to modify the back end and run your own back end um",
    "start": "211239",
    "end": "217519"
  },
  {
    "text": "you can also do that uh we recommend Chrome browser uh haven't tested on other browsers so if you have Chrome",
    "start": "217519",
    "end": "223720"
  },
  {
    "text": "great uh it should work in other browsers but you just never know what browsers these days um you're going to",
    "start": "223720",
    "end": "230280"
  },
  {
    "text": "need a microphone and a speaker or headphones uh you're going to be talking to this AI agent so um should work fine",
    "start": "230280",
    "end": "237799"
  },
  {
    "text": "on a laptop with a with a speaker on but yeah just keep the volume down a little bit so it's not it's not communicating",
    "start": "237799",
    "end": "243680"
  },
  {
    "text": "with other people's agents we need the",
    "start": "243680",
    "end": "248760"
  },
  {
    "text": "API uh I I've set it up today that you're not going to need an llm API key",
    "start": "248760",
    "end": "254519"
  },
  {
    "text": "or a deep gr API key just to keep it simple um but uh after this you'll have the opportunity to uh sign up to the",
    "start": "254519",
    "end": "261840"
  },
  {
    "text": "wait list where you will need that API key awesome yeah so the current approach",
    "start": "261840",
    "end": "268759"
  },
  {
    "text": "and and I've been building these sorts of uh voice Bots at scale for quite some",
    "start": "268759",
    "end": "273840"
  },
  {
    "text": "time um and typically they revolve around Tre key pieces right so you've",
    "start": "273840",
    "end": "278919"
  },
  {
    "text": "got your speech to text that's going to take your audio give you back uh a transcript and then you've got an llm",
    "start": "278919",
    "end": "286440"
  },
  {
    "text": "that's going to take that text uh that you've detected uh process it and then generate a text reply uh and then you're",
    "start": "286440",
    "end": "293680"
  },
  {
    "text": "going to use text to speech to actually speak that back um this has been around for a while right it's better and better",
    "start": "293680",
    "end": "300680"
  },
  {
    "text": "and lower and lower latency uh we can really bring down the latency on on both the speech to text and the text to",
    "start": "300680",
    "end": "307160"
  },
  {
    "text": "speech uh especially if you run it self-hosted um so one of the things I help a lot of customers with is you know",
    "start": "307160",
    "end": "313560"
  },
  {
    "text": "collocating all of these pieces together to bring the latency down uh the challenge with that is it becomes a",
    "start": "313560",
    "end": "319520"
  },
  {
    "text": "infrastructure challenge rather than a software challenge um so what we've tried to do uh with our new voice agent",
    "start": "319520",
    "end": "326240"
  },
  {
    "text": "API is actually offer all of that as a single API right so you can send us",
    "start": "326240",
    "end": "331440"
  },
  {
    "text": "audio we'll collocate all of those Services together um and then we'll be able to send you back uh the response uh",
    "start": "331440",
    "end": "339120"
  },
  {
    "text": "we also handle a lot of the complexity of you know endp pointing when is the user finished speaking um and those can",
    "start": "339120",
    "end": "346479"
  },
  {
    "text": "be challenging um so being able to have all of that in a single API really just makes it easy for the developer to to",
    "start": "346479",
    "end": "353280"
  },
  {
    "text": "build the application that they want to actually achieve um one of the things you'll notice here as well is function",
    "start": "353280",
    "end": "359039"
  },
  {
    "text": "calling so so depending on the llm you use right um You may want to shift your",
    "start": "359039",
    "end": "364560"
  },
  {
    "text": "entire infrastructure to a different provider right so if you're using Cloud you'll probably go with something like",
    "start": "364560",
    "end": "369599"
  },
  {
    "text": "AWS if you're using open AI you might go with a Zur right you want the llm and the other pieces of the puzzle uh to be",
    "start": "369599",
    "end": "376639"
  },
  {
    "text": "collocated um if you're running your own local llm right so Lama tree uh fi",
    "start": "376639",
    "end": "381919"
  },
  {
    "text": "things like that um you can just put them really anywhere you want uh and you can see here as well the",
    "start": "381919",
    "end": "388599"
  },
  {
    "text": "the time difference right so going from 500 milliseconds for the speech to text to 700 milliseconds for the llm and and",
    "start": "388599",
    "end": "397039"
  },
  {
    "text": "TTS um and you can really bring that all down right so as you bring those latencies down you actually start to",
    "start": "397039",
    "end": "405199"
  },
  {
    "text": "respond too fast right and that's when we start adding in delays and things like that to make sure that you're not",
    "start": "405199",
    "end": "411039"
  },
  {
    "text": "you know being rude um excuse me yeah I see a lot of people",
    "start": "411039",
    "end": "417680"
  },
  {
    "text": "taking upload uh I don't have them uploaded but after the event I I can talk to the",
    "start": "417680",
    "end": "424639"
  },
  {
    "text": "organizers and get them shared all righty so yeah let's see if the demo gods are with me today and the",
    "start": "424639",
    "end": "430639"
  },
  {
    "text": "audio",
    "start": "430639",
    "end": "433000"
  },
  {
    "text": "Works",
    "start": "441680",
    "end": "444680"
  },
  {
    "text": "hello NOP allow my mic let's try that one more",
    "start": "446759",
    "end": "453800"
  },
  {
    "text": "time hello hi there welcome to the Crusty Crab drive-thru what can I get started",
    "start": "453960",
    "end": "460960"
  },
  {
    "text": "for you today yeah could I get a Krabby Patty please actual thing",
    "start": "460960",
    "end": "468440"
  },
  {
    "text": "would I think it might be picking itself up would you like to would you like to",
    "start": "468440",
    "end": "473479"
  },
  {
    "text": "go ahead and add anything else to your order yeah can I get a Kelp Shake as",
    "start": "473479",
    "end": "479000"
  },
  {
    "text": "well please",
    "start": "479000",
    "end": "481440"
  },
  {
    "text": "I've added a crabby patty and the CP shake to your order anything else yeah could I get a crust crusty combo as",
    "start": "484840",
    "end": "492800"
  },
  {
    "text": "well we don't have a crust Krispy combo on the menu would you like a crusty combo instead",
    "start": "492800",
    "end": "501240"
  },
  {
    "text": "yeah crust let me just refresh this so it's not picking me up um so you can see",
    "start": "504599",
    "end": "510120"
  },
  {
    "text": "right we we've basically just sent audio to the service it's gone off and figured",
    "start": "510120",
    "end": "515560"
  },
  {
    "text": "out what to do function calling wise so the llm is actually making the order um",
    "start": "515560",
    "end": "521680"
  },
  {
    "text": "I'm not doing any sort of order making within the app I'm not parsing the llm response everything's just happened uh",
    "start": "521680",
    "end": "529720"
  },
  {
    "text": "automatically um cool so let's jump back into the",
    "start": "529720",
    "end": "535519"
  },
  {
    "text": "slides right so if you have noj installed um I have two repositories for",
    "start": "535519",
    "end": "541600"
  },
  {
    "text": "you um which is located here so github.com SL Damian deepgram SL",
    "start": "541600",
    "end": "550480"
  },
  {
    "text": "deepgram Workshop client and deepgram workshop server um the server itself um I'm",
    "start": "550480",
    "end": "557839"
  },
  {
    "text": "actually running it on glitch.me um if you want to make changes to the server you're going to need it",
    "start": "557839",
    "end": "564440"
  },
  {
    "text": "publicly accessible right so in order for the llm to be able to reach out to it and you you'll need that publicly",
    "start": "564440",
    "end": "570800"
  },
  {
    "text": "accessible um that's really a stretch goal you don't need to modify the back end uh the back end I have running",
    "start": "570800",
    "end": "576800"
  },
  {
    "text": "should should be able to handle everything uh if you do want to make modifications and we'll go through that a little bit later um you can basically",
    "start": "576800",
    "end": "583320"
  },
  {
    "text": "spin it up yourself uh you know point the llm to that new API um and it'll be",
    "start": "583320",
    "end": "589000"
  },
  {
    "text": "able to then call those functions um hands up anybody already got node.js",
    "start": "589000",
    "end": "596440"
  },
  {
    "text": "installed okay we got good few and anybody having",
    "start": "596440",
    "end": "602240"
  },
  {
    "text": "trouble all good cool yeah so so once you have that set up um you can simply",
    "start": "602240",
    "end": "608839"
  },
  {
    "text": "uh run the workshop client um so the workshop client is vanilla JS right I",
    "start": "608839",
    "end": "614360"
  },
  {
    "text": "try to keep it as simple as possible uh simple HTML um uh page and then the",
    "start": "614360",
    "end": "622160"
  },
  {
    "text": "main.js is um where we open that web socket connection uh we capture the",
    "start": "622160",
    "end": "627880"
  },
  {
    "text": "audio and then we send it so you know this this can send audio at a pretty fast rate um if you were running this",
    "start": "627880",
    "end": "635440"
  },
  {
    "text": "with a telephone system uh you could probably send audio at like 20 millisecond chunks uh that's going to",
    "start": "635440",
    "end": "641360"
  },
  {
    "text": "give you the lowest latency uh the browser tends to send larger chunks um so yeah you can you can definitely bring",
    "start": "641360",
    "end": "647360"
  },
  {
    "text": "the latency down when you increase that chunk rate uh we we can't process audio",
    "start": "647360",
    "end": "652760"
  },
  {
    "text": "faster then you you send it to us unfortunately um within the config and",
    "start": "652760",
    "end": "658120"
  },
  {
    "text": "and this is really where your telling you know the the API what it's actually going to do who it is how it's",
    "start": "658120",
    "end": "664440"
  },
  {
    "text": "going to work uh you can see we have a base URL um so that base URL is just pointing",
    "start": "664440",
    "end": "669959"
  },
  {
    "text": "to my websocket server or sorry my API server and um we have some input",
    "start": "669959",
    "end": "675760"
  },
  {
    "text": "parameters right so we're sending linear 16 audio and uh down here at the",
    "start": "675760",
    "end": "682040"
  },
  {
    "text": "drive-thru uh Speech to speech config um this is telling you know the system okay",
    "start": "682040",
    "end": "687560"
  },
  {
    "text": "I want to use open AI want to use GPD 40 these are the instructions right so you",
    "start": "687560",
    "end": "693360"
  },
  {
    "text": "know simple uh system prompt and and we have a function call here as well let me just bring this down a bit Yeah so this",
    "start": "693360",
    "end": "700360"
  },
  {
    "text": "function call is telling the system hey if you want to add an item to the order",
    "start": "700360",
    "end": "706240"
  },
  {
    "text": "this is the API you're going to call right and there's going to be a call ID so you know when the system starts up it",
    "start": "706240",
    "end": "712800"
  },
  {
    "text": "you know generates unique call ID so that all your order items can go into your particular order um and and yeah",
    "start": "712800",
    "end": "719680"
  },
  {
    "text": "it's pretty straightforward um later on we'll actually look at how we can add more of these um and I'll just go back",
    "start": "719680",
    "end": "726480"
  },
  {
    "text": "to the slides here oh yeah so just kind of talk at a high level uh I probably should have",
    "start": "726480",
    "end": "732000"
  },
  {
    "text": "done this before jumping into the code but um we have a user we have a browser the browser is generating microphone",
    "start": "732000",
    "end": "739000"
  },
  {
    "text": "data and and we're sending that microphone data to the service uh so the",
    "start": "739000",
    "end": "744440"
  },
  {
    "text": "voice agent API wraps all three pieces together uh the llm can do the function call and so can the the client browser",
    "start": "744440",
    "end": "751639"
  },
  {
    "text": "so uh the llm adds the items automatically and then the browser is just displaying what items are actually",
    "start": "751639",
    "end": "758680"
  },
  {
    "text": "in that um in that order uh any questions on that before I",
    "start": "758680",
    "end": "764880"
  },
  {
    "text": "move on oh yes you need that again there you",
    "start": "764880",
    "end": "769959"
  },
  {
    "text": "go",
    "start": "769959",
    "end": "772959"
  },
  {
    "text": "was everybody able to find uh the G repos okay",
    "start": "781720",
    "end": "789160"
  },
  {
    "text": "nice you missed it it's on the SL thank you",
    "start": "789160",
    "end": "797000"
  },
  {
    "text": "awesome yeah so so this is the agent configuration I was showing you earlier um right now the app is pretty basic it",
    "start": "797399",
    "end": "805399"
  },
  {
    "text": "only has an add item right so you can't take items away can't modify items um",
    "start": "805399",
    "end": "811199"
  },
  {
    "text": "and the reason we've limited it to that is you know that's a challenge for you can you add order modification um we've",
    "start": "811199",
    "end": "819320"
  },
  {
    "text": "already got the API there so we have a remove item API and uh a few others let",
    "start": "819320",
    "end": "824920"
  },
  {
    "text": "me grab yeah so there's yeah these are all the apis that",
    "start": "824920",
    "end": "830440"
  },
  {
    "text": "are in the server code um so if you want to add that order modification um you're",
    "start": "830440",
    "end": "836120"
  },
  {
    "text": "going to need to look at you know uh basically removing an item from uh the",
    "start": "836120",
    "end": "842399"
  },
  {
    "text": "order I believe this one here uh Delete call ID order items uh so we've already",
    "start": "842399",
    "end": "849160"
  },
  {
    "text": "hooked up the ad item um but in order to remove items your llm is need to go to",
    "start": "849160",
    "end": "854440"
  },
  {
    "text": "understand what's already in the order and you'll need to use the get order uh call but we'll get into that a little",
    "start": "854440",
    "end": "860360"
  },
  {
    "text": "bit later okay so we support multiple llms",
    "start": "860360",
    "end": "866040"
  },
  {
    "text": "um So within the API today you can call you know CLA uh lamry mixl uh supported",
    "start": "866040",
    "end": "873079"
  },
  {
    "text": "with open Ai and Tropic and Gro uh we will be adding to that as well over time but those are the kind of initial ones",
    "start": "873079",
    "end": "880600"
  },
  {
    "text": "um this API is is pre-release as well so you're basically getting a sneak peek to",
    "start": "880600",
    "end": "887240"
  },
  {
    "text": "it yeah so we have the the menu here so the menu is coming from the menu items",
    "start": "887240",
    "end": "892920"
  },
  {
    "text": "API um when I want to create a new call so on on loading of the web page we",
    "start": "892920",
    "end": "899120"
  },
  {
    "text": "basically grab a unique call ID and and we get the menu um the menu itself right",
    "start": "899120",
    "end": "904600"
  },
  {
    "text": "now is is baked into to the um llm system prompt um stretch goal would be",
    "start": "904600",
    "end": "911920"
  },
  {
    "text": "let's turn that into a function call as well right let's allow items to go out of stock right and the llm will need to",
    "start": "911920",
    "end": "918320"
  },
  {
    "text": "know when you're out of chicken wings and things like that um and then we have the the get order for the call Api as",
    "start": "918320",
    "end": "925639"
  },
  {
    "text": "well yes so the web socer client itself um if you are familiar with the Chrome",
    "start": "925639",
    "end": "931079"
  },
  {
    "text": "Dev tools let me see if I can grab it",
    "start": "931079",
    "end": "935680"
  },
  {
    "text": "here yeah so in the network tab of the Chrome Dev tools there's a cool little uh web socket",
    "start": "937440",
    "end": "944120"
  },
  {
    "text": "inspector so if I start a conversation um you'll be able to see the messages as",
    "start": "944120",
    "end": "949279"
  },
  {
    "text": "they happen right so you can see here I'm streaming audio at a pretty rapid rate um and you know when the API",
    "start": "949279",
    "end": "955800"
  },
  {
    "text": "responds it's it's going to send us back um messages as well so there's there's quite a few life cycle messages that",
    "start": "955800",
    "end": "962480"
  },
  {
    "text": "we'll send you too um and I can I can bring those up now as",
    "start": "962480",
    "end": "968279"
  },
  {
    "text": "well yeah so the settings configuration um that's one that we send so we tell it",
    "start": "970440",
    "end": "975880"
  },
  {
    "text": "uh and this is being truncated as well there's there's a lot of like system stuff and function calling as well in it",
    "start": "975880",
    "end": "982319"
  },
  {
    "text": "and but we're basically telling the system what we want to actually send it what we want it to be how we want it to",
    "start": "982319",
    "end": "987680"
  },
  {
    "text": "work uh what function we wanted to call and it's going to send us back this session ID um and then the conversation",
    "start": "987680",
    "end": "994480"
  },
  {
    "text": "text right so that's going to give us a transcript of what the user is saying um",
    "start": "994480",
    "end": "999519"
  },
  {
    "text": "that can be useful if you want to display you know text on screen while the person is speaking uh sometimes that",
    "start": "999519",
    "end": "1005440"
  },
  {
    "text": "feedback is is really useful so people know it's it's hearing you as you talk um and then yeah the function calling so",
    "start": "1005440",
    "end": "1012120"
  },
  {
    "text": "right now um this is only doing ad item and you can see the argument here is",
    "start": "1012120",
    "end": "1017480"
  },
  {
    "text": "basically the the Krabby pad um yeah so the tool will respond uh",
    "start": "1017480",
    "end": "1023399"
  },
  {
    "text": "whether it was a success or failure so you know if it can't add the item you'll know about it um and then you'll also",
    "start": "1023399",
    "end": "1030400"
  },
  {
    "text": "get the assistance uh response back in text uh can be useful to know what the",
    "start": "1030400",
    "end": "1036959"
  },
  {
    "text": "AI is saying if you want to do content moderation um so you don't want it offering free Krabby Patties for",
    "start": "1036959",
    "end": "1042480"
  },
  {
    "text": "instance you might have a a mechanism to detect you know um various things and",
    "start": "1042480",
    "end": "1047520"
  },
  {
    "text": "you can apply that to a lot of different use cases as well yeah so the backend API um this is",
    "start": "1047520",
    "end": "1054840"
  },
  {
    "text": "the main one that we're going to be be using today U but yeah I do recommend trying to add a few more um and yeah on",
    "start": "1054840",
    "end": "1063960"
  },
  {
    "text": "the server logs so you know in the server where it's running this API you're going to see things like this",
    "start": "1063960",
    "end": "1070000"
  },
  {
    "text": "right so there's a new call coming in you've got that order ID uh you're adding an item to that order uh and then",
    "start": "1070000",
    "end": "1077320"
  },
  {
    "text": "you're getting uh the updated order here as well um and this is essentially what",
    "start": "1077320",
    "end": "1083120"
  },
  {
    "text": "you know we're consuming in the front end to display the items as are ordered yeah so walking through the",
    "start": "1083120",
    "end": "1089520"
  },
  {
    "text": "client code uh there's five files I tried to split them out logically as best I could um so let's go through uh",
    "start": "1089520",
    "end": "1096960"
  },
  {
    "text": "each of those and just kind of explain what each of them does um the main.js this is going to be you know all the",
    "start": "1096960",
    "end": "1102880"
  },
  {
    "text": "kind of front-end hookup code uh config.js that's going to be how we tell the llm and uh the the agent API what to",
    "start": "1102880",
    "end": "1110919"
  },
  {
    "text": "do uh the",
    "start": "1110919",
    "end": "1114000"
  },
  {
    "text": "services.jpg doeses some kind of interesting stuff around you know audio manipulation and down sampling uh the",
    "start": "1119360",
    "end": "1127080"
  },
  {
    "text": "browser itself actually sends higher um higher sample rate audio and but we",
    "start": "1127080",
    "end": "1132679"
  },
  {
    "text": "don't we don't need 48 kher audio at that rate that's going to be a huge bandwidth uh usage um so we drop that",
    "start": "1132679",
    "end": "1139840"
  },
  {
    "text": "down to 16 khz which is you know essentially what the the API can handle um pretty fast anyways uh and then",
    "start": "1139840",
    "end": "1146640"
  },
  {
    "text": "animations.js is really just animating that little bubble that you know kind of responds to",
    "start": "1146640",
    "end": "1152280"
  },
  {
    "text": "speech um and then the server code super simple expressjs API um if you're",
    "start": "1152280",
    "end": "1157799"
  },
  {
    "text": "familiar with expressjs you know it doesn't really get much more simple than that okay so let's take a look at the",
    "start": "1157799",
    "end": "1164520"
  },
  {
    "text": "code yeah so this is the uh the index.js",
    "start": "1164520",
    "end": "1172320"
  },
  {
    "text": "within the server code um it's just got a few very simple uh function calls here",
    "start": "1172320",
    "end": "1179640"
  },
  {
    "text": "so um I'm not not sure how much I should go into this but yeah it's it's pretty",
    "start": "1179640",
    "end": "1185280"
  },
  {
    "text": "pretty straightforward it's just updating uh the App State um and handling some of the crud",
    "start": "1185280",
    "end": "1191559"
  },
  {
    "text": "operations um yeah so let's go through these top to bottom uh the animation stuff it's just a simple canvas it's got",
    "start": "1191559",
    "end": "1198720"
  },
  {
    "text": "going to uh modify the the bubble uh and I'll show you the bubble again if you forget what it looks like um yeah so",
    "start": "1198720",
    "end": "1206760"
  },
  {
    "text": "this bubble here is going to respond to speech so you can see it kind of getting bigger smaller bigger smaller and that",
    "start": "1206760",
    "end": "1214120"
  },
  {
    "text": "that's pretty much what that does um within audio JS so we've got a",
    "start": "1214120",
    "end": "1219880"
  },
  {
    "text": "couple of different uh functions in here we've got uh receive audio and capture audio and then clear scheduled audio and",
    "start": "1219880",
    "end": "1227600"
  },
  {
    "text": "down sample um and this little function is just conversion function that the down sample",
    "start": "1227600",
    "end": "1233120"
  },
  {
    "text": "uses um the reason we need a clear scheduled audio is because you can",
    "start": "1233120",
    "end": "1238159"
  },
  {
    "text": "interrupt the llm while you're talking right or or the agent um and we may not",
    "start": "1238159",
    "end": "1244000"
  },
  {
    "text": "know that on the server side because you're handling it on the client side so if you've got audio already playing",
    "start": "1244000",
    "end": "1250799"
  },
  {
    "text": "you're going to want to pause that audio as soon as you can uh you could even add a client side uh voice activity detector",
    "start": "1250799",
    "end": "1257720"
  },
  {
    "text": "uh solero bad is a is a really good one that I've used before um and that just allows you to do that bargin so when you",
    "start": "1257720",
    "end": "1263799"
  },
  {
    "text": "do start speaking you know it knows to stop um you know and more advanced",
    "start": "1263799",
    "end": "1268960"
  },
  {
    "text": "systems will help the llm actually understand whereabouts in its speech did",
    "start": "1268960",
    "end": "1274279"
  },
  {
    "text": "it get interrupted right because it then it it may not know you didn't hear part you know at the end of its prior",
    "start": "1274279",
    "end": "1280840"
  },
  {
    "text": "response um and then yeah receiving audio so this is basically grabbing the audio from the websocket um and just",
    "start": "1280840",
    "end": "1288600"
  },
  {
    "text": "sticking it into a buffer and then the capture audio this is just grabbing it from the the media devices on the",
    "start": "1288600",
    "end": "1294799"
  },
  {
    "text": "browser um and then once we get that we we call the call back and that call back",
    "start": "1294799",
    "end": "1300080"
  },
  {
    "text": "is is what we saw here uh which is the websocket send data um yeah so walking",
    "start": "1300080",
    "end": "1306440"
  },
  {
    "text": "down through this um unload we're going to prepare the agents config and and",
    "start": "1306440",
    "end": "1312760"
  },
  {
    "text": "send that over uh that'll give us the order ID um and then when I click Start conversation on the U",
    "start": "1312760",
    "end": "1319279"
  },
  {
    "text": "um that's going to call this uh code here opens the websocket and begins sending audio data uh the errors any",
    "start": "1319279",
    "end": "1327000"
  },
  {
    "text": "websocket errors will be handed handled there um and then this is essentially where we're getting back um text based",
    "start": "1327000",
    "end": "1335640"
  },
  {
    "text": "uh status messages so user started speaking um you know like what the AI",
    "start": "1335640",
    "end": "1341400"
  },
  {
    "text": "said um and then here we're actually receiving the audio um so receiving the audio is is is um what we looked at in",
    "start": "1341400",
    "end": "1349240"
  },
  {
    "text": "the audio JS file um and then we also have the ability to update voices um so",
    "start": "1349240",
    "end": "1355039"
  },
  {
    "text": "I don't think I showed that actually in the prior example uh have a version",
    "start": "1355039",
    "end": "1360080"
  },
  {
    "text": "running here let me just kick it",
    "start": "1360080",
    "end": "1364158"
  },
  {
    "text": "off hello I there welcome to the Crusty Crab",
    "start": "1373919",
    "end": "1381360"
  },
  {
    "text": "drive-thru what can I get for you today yeah can I get a uh crabby patty",
    "start": "1381360",
    "end": "1389200"
  },
  {
    "text": "please God is a crabby patty anything else for you today yeah can I get a",
    "start": "1389520",
    "end": "1395000"
  },
  {
    "text": "crabby meal as well sure a crabby",
    "start": "1395000",
    "end": "1400400"
  },
  {
    "text": "meal actually can I change that to two crabby",
    "start": "1400400",
    "end": "1405440"
  },
  {
    "text": "putties I've added an addition crabby patty to your order anything else you'd like yeah so we we have 12 different",
    "start": "1406679",
    "end": "1414000"
  },
  {
    "text": "voices um that voice that second voice I used there is actually my own voice um",
    "start": "1414000",
    "end": "1420279"
  },
  {
    "text": "which which is pretty handy yeah I had to I had to tell my parents when I train the voice is like if you ever get a phone call from me",
    "start": "1420279",
    "end": "1426679"
  },
  {
    "text": "looking for",
    "start": "1426679",
    "end": "1429080"
  },
  {
    "text": "money uh excuse me I'm still talking to it am I",
    "start": "1432919",
    "end": "1439200"
  },
  {
    "text": "I I have a question oh yes sorry going ahead um where in the stack do you categorize the type of function call so",
    "start": "1439200",
    "end": "1445799"
  },
  {
    "text": "whether that's add item or uh remove item or mhm yeah yeah",
    "start": "1445799",
    "end": "1452440"
  },
  {
    "text": "that's in the config JS so you can see here we we tell it",
    "start": "1452440",
    "end": "1458400"
  },
  {
    "text": "what to call and we give it a a base URL um so it's not Dynamic based on the",
    "start": "1458400",
    "end": "1467399"
  },
  {
    "text": "prompt uh the llm will dynamically decide what to use okay MH okay yeah so there",
    "start": "1467399",
    "end": "1475559"
  },
  {
    "text": "there's no like direct calling of of it the llm is kind of kind of like you know",
    "start": "1475559",
    "end": "1481039"
  },
  {
    "text": "chaty be plugins right you don't really know if it's going to use it or not um but yeah they've gotten pretty good",
    "start": "1481039",
    "end": "1486679"
  },
  {
    "text": "especially gbd 40 and um I think uh mistol as well is pretty good at calling",
    "start": "1486679",
    "end": "1492520"
  },
  {
    "text": "it uh you'll probably have problems with gbd 3.5 and function calling um it's",
    "start": "1492520",
    "end": "1497720"
  },
  {
    "text": "just not really up to the level um to do it um but yeah llms are getting better",
    "start": "1497720",
    "end": "1503039"
  },
  {
    "text": "so uh they're able to do it now",
    "start": "1503039",
    "end": "1509320"
  },
  {
    "text": "thanks I'm sorry putting on top of what you just said I don't know if anybody hear sorry sorry I've realiz you have a",
    "start": "1513799",
    "end": "1523760"
  },
  {
    "text": "good system promp along with a good function definition that really works with most newer models",
    "start": "1523760",
    "end": "1531880"
  },
  {
    "text": "to consistently get it to use the tool or request the actual tool instead of just coming up with yeah yeah definitely",
    "start": "1531880",
    "end": "1538960"
  },
  {
    "text": "and like it really is only those newer models right so the the latest CLA um I",
    "start": "1538960",
    "end": "1545360"
  },
  {
    "text": "don't know if hiq is going to work super great with function calling but definitely like Sonet and Opus uh work a",
    "start": "1545360",
    "end": "1551520"
  },
  {
    "text": "lot better um and then on grock um they host Lama um Lama tree 70 billion I",
    "start": "1551520",
    "end": "1559039"
  },
  {
    "text": "believe and M Mixel um you kind of your mileage may vary with those open source",
    "start": "1559039",
    "end": "1564600"
  },
  {
    "text": "llms I don't think they've caught up to the function calling level just yet um but yeah like you know shoot for where",
    "start": "1564600",
    "end": "1570720"
  },
  {
    "text": "the puck is going to be and I think uh a lot of those will catch up pretty soon",
    "start": "1570720",
    "end": "1576919"
  },
  {
    "text": "yeah so I'm curious about your take on the ux of the voice uh in",
    "start": "1576919",
    "end": "1585279"
  },
  {
    "text": "particular what do you have in terms of recommendation specifically for",
    "start": "1585279",
    "end": "1591039"
  },
  {
    "text": "interrup for these model for these kind of interaction think one place where people",
    "start": "1591039",
    "end": "1597120"
  },
  {
    "text": "get up is well this is cute but often times people think while they are saying",
    "start": "1597120",
    "end": "1603640"
  },
  {
    "text": "something so are awward silences where",
    "start": "1603640",
    "end": "1610440"
  },
  {
    "text": "not changes and that and how does that affect the use of the likewise",
    "start": "1610440",
    "end": "1619639"
  },
  {
    "text": "question yeah yeah so for anybody who didn't hear the question is about interruptibility and and how to handle",
    "start": "1628440",
    "end": "1634320"
  },
  {
    "text": "things like long pauses um and that really comes down to endp pointing and contextual kind of um uh semantic endp",
    "start": "1634320",
    "end": "1642039"
  },
  {
    "text": "pointing is what we call it uh so that's something we're going to build into uh this voice agent API so you know you can",
    "start": "1642039",
    "end": "1648240"
  },
  {
    "text": "imagine a scenario where the user says hang on a minute let me get that for you right you know say they're going to get their account number or whatever it is",
    "start": "1648240",
    "end": "1655159"
  },
  {
    "text": "and that doesn't necessarily require the llm to go off on another kind of monologue and the llm might say sure you",
    "start": "1655159",
    "end": "1662360"
  },
  {
    "text": "know let me wait for you to get that right uh and that's kind of uh semantic endp pointing the other type of endp",
    "start": "1662360",
    "end": "1668240"
  },
  {
    "text": "pointing which is kind of you know traditionally what people used was you know a span of Silence um is used to",
    "start": "1668240",
    "end": "1674360"
  },
  {
    "text": "determine when somebody's finished speaking um but yeah like if people are calling out credit card numbers um it's",
    "start": "1674360",
    "end": "1679600"
  },
  {
    "text": "pretty common for them to do back channeling um so when you do back channeling you're essentially waiting",
    "start": "1679600",
    "end": "1684919"
  },
  {
    "text": "for like a a noise from the other person like a mhm you know so if I do like you know 1 2 3 4 mhm 5 6 7 8 mhm and and",
    "start": "1684919",
    "end": "1694360"
  },
  {
    "text": "that just kind of gives you that like I've I've captured what you said um so that you don't go too fast and then the",
    "start": "1694360",
    "end": "1700600"
  },
  {
    "text": "person like falls behind um and a lot of the time with these voice agents what I",
    "start": "1700600",
    "end": "1705720"
  },
  {
    "text": "recommend to customers is you know what would a human do right um and there seems to be this really high expectation",
    "start": "1705720",
    "end": "1712600"
  },
  {
    "text": "that the AI should be able to understand pretty much anything right um but the",
    "start": "1712600",
    "end": "1718399"
  },
  {
    "text": "the reality is is that like nobody can understand my email address over the phone um and I have to call it out like",
    "start": "1718399",
    "end": "1724320"
  },
  {
    "text": "you know D for Damian uh you know a for apple and and and this is with a human",
    "start": "1724320",
    "end": "1729399"
  },
  {
    "text": "but you know with an AI you need to build in that kind of understanding logic and and I'll go into it a little",
    "start": "1729399",
    "end": "1735240"
  },
  {
    "text": "bit later about you know how you can make that composability with these agents and because you don't want to create an agent that does everything you",
    "start": "1735240",
    "end": "1742720"
  },
  {
    "text": "know for your entire business right uh you want to create an agent that's capable at a particular task and then",
    "start": "1742720",
    "end": "1749279"
  },
  {
    "text": "build them together right so having that kind of multi-agent uh system where you can offload parts of the conversation to",
    "start": "1749279",
    "end": "1757120"
  },
  {
    "text": "you know a slightly different AI agent that's able to collect credit card numbers very accurately and handle all",
    "start": "1757120",
    "end": "1763279"
  },
  {
    "text": "of the edge cases or you know verify account information you know versus you know take an order um they're they're",
    "start": "1763279",
    "end": "1769760"
  },
  {
    "text": "all very different use cases but um yeah from what I've seen in in the market people tend to want to make it do all",
    "start": "1769760",
    "end": "1776279"
  },
  {
    "text": "the things in one system prompt um and it's just not there yet you know even with these large context windows I don't",
    "start": "1776279",
    "end": "1782279"
  },
  {
    "text": "think it's it's really good to try to get it to do everything and have you know um a massive system prompt um like",
    "start": "1782279",
    "end": "1790000"
  },
  {
    "text": "as you increase the system prompt lent you also increase your time to First token uh time to First token is is",
    "start": "1790000",
    "end": "1796559"
  },
  {
    "text": "really the key metric trick for an llm uh to respond um so you can start responding as soon as you get you know",
    "start": "1796559",
    "end": "1803279"
  },
  {
    "text": "let's say five tokens or 10 tokens and you can start the TTS playback at that point um if you wait until like you know",
    "start": "1803279",
    "end": "1810840"
  },
  {
    "text": "the 250th token uh the latency is going to be much higher right maybe if you're using grock you could wait uh that long",
    "start": "1810840",
    "end": "1817480"
  },
  {
    "text": "because it's so fast but you most llms are outputting you know maybe 30 30 tokens per second uh and it's highly",
    "start": "1817480",
    "end": "1824519"
  },
  {
    "text": "variable right like uh even GPD 40 can give you like 900 millisecond uh latency",
    "start": "1824519",
    "end": "1830159"
  },
  {
    "text": "on first token um and you know that that's something that's going to improve over time but yeah it's definitely",
    "start": "1830159",
    "end": "1835960"
  },
  {
    "text": "something you have to be aware of when you're building these voice spots Okay",
    "start": "1835960",
    "end": "1843679"
  },
  {
    "text": "so casil",
    "start": "1843679",
    "end": "1847679"
  },
  {
    "text": "soltion your experience",
    "start": "1858840",
    "end": "1863120"
  },
  {
    "text": "are and just to clarify so um is the question about using this approach",
    "start": "1876639",
    "end": "1882639"
  },
  {
    "text": "versus which other approach sorry yeah like a",
    "start": "1882639",
    "end": "1888000"
  },
  {
    "text": "yeah so not sure if fully understand the question but um let me kind of paraphrase it so using a chaty BT um",
    "start": "1894720",
    "end": "1902960"
  },
  {
    "text": "today doesn't doesn't have ears or a mount right so so you you just have an llm text in text out so you still have",
    "start": "1902960",
    "end": "1910200"
  },
  {
    "text": "to add the ears and the mount and and what we're doing here is is real time low latency streaming so the audio is",
    "start": "1910200",
    "end": "1916720"
  },
  {
    "text": "being streamed like you know uh straight into the system and then audio is being streamed straight out of the system um",
    "start": "1916720",
    "end": "1924080"
  },
  {
    "text": "you know obviously GPD 40 had that big Fanfare announcement the day before Google's announcement but neither of",
    "start": "1924080",
    "end": "1929960"
  },
  {
    "text": "them have released anything yet um and the reason that they haven't released anything yet is that it's hard right um",
    "start": "1929960",
    "end": "1936720"
  },
  {
    "text": "we've had realtime voice agents you know for for years and uh they've just gotten",
    "start": "1936720",
    "end": "1941880"
  },
  {
    "text": "better and better and better and and one of the key things there is the latency from you know end of speech to",
    "start": "1941880",
    "end": "1948480"
  },
  {
    "text": "transcript um once you go self-hosted with deepgram you get that down to like 50 milliseconds um in our hosted API",
    "start": "1948480",
    "end": "1955559"
  },
  {
    "text": "you're going to get closer to half a second uh and that's just because we we don't like crank up the compute um so as",
    "start": "1955559",
    "end": "1961720"
  },
  {
    "text": "you increase the compute like say to 5x you can get down to that 50 millisecond",
    "start": "1961720",
    "end": "1967159"
  },
  {
    "text": "so a lot of these you know companies that you see showing real-time voice spots and they're using deep gr under",
    "start": "1967159",
    "end": "1973159"
  },
  {
    "text": "the hood um you know today I think we're the only option for low latency real",
    "start": "1973159",
    "end": "1978639"
  },
  {
    "text": "time uh speech recognition um that may change in the future but yeah today this is kind of state-ofthe-art I",
    "start": "1978639",
    "end": "1985440"
  },
  {
    "text": "think that answer your question",
    "start": "1985440",
    "end": "1990600"
  },
  {
    "text": "Yeahs theflow that yes so we're llm agnostic so you",
    "start": "2001200",
    "end": "2007159"
  },
  {
    "text": "you can use function calling with a lot of llms um like building a GPT assistant",
    "start": "2007159",
    "end": "2013519"
  },
  {
    "text": "um kind of follows a very similar API format it's a pretty standard open AI",
    "start": "2013519",
    "end": "2018720"
  },
  {
    "text": "kind of uh interface and and most llms have actually adopted that same interface so you know you can use that",
    "start": "2018720",
    "end": "2024960"
  },
  {
    "text": "same function calling and system prompt with another llm um so um yeah I I think",
    "start": "2024960",
    "end": "2031399"
  },
  {
    "text": "that's definitely interchangeable but there there's no real difference between a GPD agent and what we're doing here",
    "start": "2031399",
    "end": "2038919"
  },
  {
    "text": "yeah um suppose the function call that you're making is like is going to be long running um is that blocking to the",
    "start": "2038919",
    "end": "2046240"
  },
  {
    "text": "voice agent and like I guess what are some ways around it if the function call",
    "start": "2046240",
    "end": "2051638"
  },
  {
    "text": "that you want to make is something that's long MH yeah so the question was about uh long running function calling",
    "start": "2051639",
    "end": "2057878"
  },
  {
    "text": "uh and that's definitely a concern um I don't know if you want to do long running function calling with a a",
    "start": "2057879",
    "end": "2063720"
  },
  {
    "text": "real-time voice bot uh you might hand that off to you know a a secondary system um so you say hey okay you know",
    "start": "2063720",
    "end": "2071000"
  },
  {
    "text": "I'm checking on that for you is anything else can help you with and then when when it comes back your agent can then",
    "start": "2071000",
    "end": "2076440"
  },
  {
    "text": "offer up the the information is is this voice agent functions like only so like",
    "start": "2076440",
    "end": "2083358"
  },
  {
    "text": "the Contex of this uh demo there's like a get order function so like it first",
    "start": "2083359",
    "end": "2089919"
  },
  {
    "text": "adds things to the order and then like looks at the order but is there a way to proactively push things to the",
    "start": "2089919",
    "end": "2096679"
  },
  {
    "text": "conversation window M um as part as part of this yeah yeah so we're pushing the",
    "start": "2096679",
    "end": "2102520"
  },
  {
    "text": "order items from the llm so the llm is actually making the call to the add",
    "start": "2102520",
    "end": "2107839"
  },
  {
    "text": "order or add item to order API um so it's doing the pushing right I'm not pushing anything client side I'm only",
    "start": "2107839",
    "end": "2114200"
  },
  {
    "text": "reading client side so client side I'm just polling the order as like you know give me the order give me the order give",
    "start": "2114200",
    "end": "2119640"
  },
  {
    "text": "me the order um and that's able to allow me to display the order um but the actual pushing it's happening all from",
    "start": "2119640",
    "end": "2126160"
  },
  {
    "text": "the llm so with respect to like um like if we need to add information about the order",
    "start": "2126160",
    "end": "2132560"
  },
  {
    "text": "to the llm that is in and of itself a function lull the API yeah so what you would want",
    "start": "2132560",
    "end": "2140720"
  },
  {
    "text": "to do is you would want to give it a new function um and we've already created the functions uh to give it and that's",
    "start": "2140720",
    "end": "2146720"
  },
  {
    "text": "kind of uh the next step in this so you would want to add uh a new function here",
    "start": "2146720",
    "end": "2152040"
  },
  {
    "text": "uh for get order right so this would be like your get order function and then you can point",
    "start": "2152040",
    "end": "2158680"
  },
  {
    "text": "that to uh the API to get the order another one you'll probably want to do",
    "start": "2158680",
    "end": "2163800"
  },
  {
    "text": "is is get menu right so you know is is there an item that's no longer available",
    "start": "2163800",
    "end": "2169040"
  },
  {
    "text": "right because we're pulling from the you know the menu ordering system to see if something's out of stock and because we",
    "start": "2169040",
    "end": "2174119"
  },
  {
    "text": "know all the orders that have gone through uh and then another one you'll want is actually a remove item so with",
    "start": "2174119",
    "end": "2180560"
  },
  {
    "text": "remove item you have the ability to modify the existing order and I didn't Implement these in the function calls",
    "start": "2180560",
    "end": "2186880"
  },
  {
    "text": "cuz I I thought be a good kind of learning exercise for people here um but yeah you could definitely add those and",
    "start": "2186880",
    "end": "2192680"
  },
  {
    "text": "and uh understand a little bit more how to work got it and um if if we did want",
    "start": "2192680",
    "end": "2198920"
  },
  {
    "text": "to have a long running function that ran um it would be a nonblocking function",
    "start": "2198920",
    "end": "2204000"
  },
  {
    "text": "called it triggers the job and then at some later point the assistant attempts",
    "start": "2204000",
    "end": "2209520"
  },
  {
    "text": "to fetch that information is there a way for client to actually push data into the conversation blog like in with this",
    "start": "2209520",
    "end": "2216200"
  },
  {
    "text": "API is that yeah yeah absolutely so um as a part of",
    "start": "2216200",
    "end": "2221800"
  },
  {
    "text": "the you know the the information that the llm has access to um I don't know if",
    "start": "2221800",
    "end": "2227440"
  },
  {
    "text": "you can see it here it might be off screen uh you can see here the menu right so the menu there is a part of it",
    "start": "2227440",
    "end": "2234480"
  },
  {
    "text": "system prompt and but you could remove that menu from there and add it as a function call so now anything that modif",
    "start": "2234480",
    "end": "2241880"
  },
  {
    "text": "like you could have a separate service modifying the menu it could be a separate llm right and when that menu is",
    "start": "2241880",
    "end": "2248119"
  },
  {
    "text": "modified and it pulls the menu it's now updated its system",
    "start": "2248119",
    "end": "2254280"
  },
  {
    "text": "context cool yeah yeah",
    "start": "2255000",
    "end": "2261359"
  },
  {
    "text": "sure um I haven't tried it myself but I I'd probably imagine you'd want some",
    "start": "2275760",
    "end": "2281240"
  },
  {
    "text": "sort of web hook call back um so when the function call is complete it would",
    "start": "2281240",
    "end": "2286720"
  },
  {
    "text": "it would instantly return but then have a separate Handler that would would know that it's uh completed um and then you",
    "start": "2286720",
    "end": "2293880"
  },
  {
    "text": "can prompt the llm from that web hook Handler to say Hey you know this thing",
    "start": "2293880",
    "end": "2299040"
  },
  {
    "text": "you asked for earlier and it would kind of act like a user um input as well",
    "start": "2299040",
    "end": "2306200"
  },
  {
    "text": "uh the the webbook Handler would probably run in the back end not in the llm itself so the the llm would just say",
    "start": "2309800",
    "end": "2315960"
  },
  {
    "text": "hey go do this long running task instantly return and say okay I've kicked off that long running task and",
    "start": "2315960",
    "end": "2322119"
  },
  {
    "text": "then when the webbook Handler gets fired by the long running task it would then tell the llm hey you know this long",
    "start": "2322119",
    "end": "2328599"
  },
  {
    "text": "running task has completed yeah I think you had a question as well on the in handling side",
    "start": "2328599",
    "end": "2336680"
  },
  {
    "text": "how do you differentiate between noise verus action",
    "start": "2336680",
    "end": "2341760"
  },
  {
    "text": "speech yeah so we have voice activity detectors uh and the voice activity detector will only trigger on um audio",
    "start": "2341760",
    "end": "2349280"
  },
  {
    "text": "that's generated by the vocal cords uh it will trigger on coughs and huming and things like that um but you may want",
    "start": "2349280",
    "end": "2356480"
  },
  {
    "text": "that to happen as well um so you can put in things in place to detect okay did I actually transcribe a word um you know",
    "start": "2356480",
    "end": "2363599"
  },
  {
    "text": "should I respond to this um so those are things you can Implement as well so on that will it added latency when I'm",
    "start": "2363599",
    "end": "2370960"
  },
  {
    "text": "speaking on the and if I while you have audio going on you'll",
    "start": "2370960",
    "end": "2378359"
  },
  {
    "text": "have to wait for the response yes we we'll send you a very quick uh user started speaking event using a serers",
    "start": "2378359",
    "end": "2386000"
  },
  {
    "text": "side uh vad and so the voice activity detector is going to tell you as soon as you get it and we actually have code in",
    "start": "2386000",
    "end": "2392560"
  },
  {
    "text": "there as well to uh to clear I believe let me see where is it",
    "start": "2392560",
    "end": "2398640"
  },
  {
    "text": "uh in here yeah so you see if user started speaking happens we basically stop the",
    "start": "2398640",
    "end": "2405920"
  },
  {
    "text": "audio playback what's the laty on that uh it",
    "start": "2405920",
    "end": "2411760"
  },
  {
    "text": "can vary I think it should be in the order of like less than 100 milliseconds mhm yeah is memory being handled at all",
    "start": "2411760",
    "end": "2418680"
  },
  {
    "text": "or is that just something separate uh memory would be kind of like separate uh",
    "start": "2418680",
    "end": "2424359"
  },
  {
    "text": "challenge you can obviously build up the system context um you know depending on your use case like if you want to handle",
    "start": "2424359",
    "end": "2430319"
  },
  {
    "text": "hourong calls you probably don't want to keep building up the system context you want to use some sort of uh memory",
    "start": "2430319",
    "end": "2437599"
  },
  {
    "text": "system autogen have a pretty good teachable Agent um if you're if you're familiar with it uh it has the ability",
    "start": "2437599",
    "end": "2444160"
  },
  {
    "text": "to run a secondary llm to ask is there anything new or updated you know in this",
    "start": "2444160",
    "end": "2449319"
  },
  {
    "text": "new content update you know the existing memory um a good example of that might",
    "start": "2449319",
    "end": "2454520"
  },
  {
    "text": "be like oh I live at 123 street and then it comes down later on it's like oh actually I live at 456 street right and",
    "start": "2454520",
    "end": "2461359"
  },
  {
    "text": "you don't want both conflicting in your system prompt you want to update uh the prior memory of",
    "start": "2461359",
    "end": "2467599"
  },
  {
    "text": "it yeah how do you protect against",
    "start": "2467599",
    "end": "2474000"
  },
  {
    "text": "injection content into the system yeah and and that's really the uh the reason",
    "start": "2474000",
    "end": "2479640"
  },
  {
    "text": "that we have um let me see this one here um this is why we have the conversation",
    "start": "2479640",
    "end": "2485280"
  },
  {
    "text": "text so you know you you'll hear stories of people getting you know Chevrolet",
    "start": "2485280",
    "end": "2490680"
  },
  {
    "text": "cars for for Z by you know modifying uh the system and but what you can do is",
    "start": "2490680",
    "end": "2496280"
  },
  {
    "text": "you can actually have a process on the text that you know tries to block certain things right um and that content",
    "start": "2496280",
    "end": "2503720"
  },
  {
    "text": "moderation is very important to prevent things like that I don't think you're ever going to be able to prevent like",
    "start": "2503720",
    "end": "2509520"
  },
  {
    "text": "prompt modification right cuz you know if if you ask an AI bot five times or",
    "start": "2509520",
    "end": "2515040"
  },
  {
    "text": "even three times to like break its rules it probably will right like the first time is like no no I can't do that and",
    "start": "2515040",
    "end": "2521119"
  },
  {
    "text": "then the second time is like no definitely can't do that third time's like sure I'll do that for you um and and that's just an inherent problem with",
    "start": "2521119",
    "end": "2526800"
  },
  {
    "text": "llm so you know you're not going to be able to stop it on the way in but on the way out you could be like hey you know",
    "start": "2526800",
    "end": "2533880"
  },
  {
    "text": "you've offered something that we've detected is is invalid um but yeah it's it's it's a hard problem I don't think",
    "start": "2533880",
    "end": "2540119"
  },
  {
    "text": "anybody's really solve that I don't think we understand llms enough to solve",
    "start": "2540119",
    "end": "2545319"
  },
  {
    "text": "it yeah I'm the TDS",
    "start": "2545319",
    "end": "2551520"
  },
  {
    "text": "andage yeah so the question was about the language support on um uh text to",
    "start": "2568880",
    "end": "2575000"
  },
  {
    "text": "speech and speech to text um let me just log in here",
    "start": "2575000",
    "end": "2580760"
  },
  {
    "text": "quickly yeah so if I jump over to the text of speech um these are the different uh voices we have so we have",
    "start": "2584440",
    "end": "2591319"
  },
  {
    "text": "12 voices uh today we've got all of our English voices uh publicly available um",
    "start": "2591319",
    "end": "2598040"
  },
  {
    "text": "they're uh super low latency uh and very low cost right about 20x cheaper than 11",
    "start": "2598040",
    "end": "2604359"
  },
  {
    "text": "Labs um so we're we're kind of competing at the you know the Google AWS iser um",
    "start": "2604359",
    "end": "2611000"
  },
  {
    "text": "voice pricing um but with the quality that's pretty close to 11 laps um we're",
    "start": "2611000",
    "end": "2616240"
  },
  {
    "text": "working today on prompt TTS um so that's going to give you the ability to say Hey you know say this in an empathic Voice",
    "start": "2616240",
    "end": "2623480"
  },
  {
    "text": "or say it in a pirates voice right like the ability to prompt it um and once we've completed that research we're then",
    "start": "2623480",
    "end": "2630480"
  },
  {
    "text": "going to roll out other languages um the challenge with building them now would be that we'd have to go back and retrain",
    "start": "2630480",
    "end": "2636839"
  },
  {
    "text": "all of the models once the prompt TTS is out um I believe our TTS launched about",
    "start": "2636839",
    "end": "2642720"
  },
  {
    "text": "4 months ago um so yeah it's pretty pretty new to the market but uh you can play all of them here as well which is",
    "start": "2642720",
    "end": "2648960"
  },
  {
    "text": "Prett is great for Real Time conversations and also you can build apps for",
    "start": "2648960",
    "end": "2654800"
  },
  {
    "text": "things deep gram is great for Real Time convers deep gram is great for real time",
    "start": "2654800",
    "end": "2662000"
  },
  {
    "text": "yeah and what I found with customers is uh the vast majority of customers want female voices uh",
    "start": "2662000",
    "end": "2667720"
  },
  {
    "text": "um I don't know what it is but I guess nobody wants to be mansplained um yeah and then on the uh",
    "start": "2667720",
    "end": "2676200"
  },
  {
    "text": "on the language side uh we have 36 languages supported uh today on our Nova 2 model uh we have a few more supported",
    "start": "2676200",
    "end": "2683720"
  },
  {
    "text": "as well on our older models and but we're adding languages you know every month there as well um we've actually",
    "start": "2683720",
    "end": "2690440"
  },
  {
    "text": "got an auto training pipeline set up probably the first in the world I think where we have the ability to detect like",
    "start": "2690440",
    "end": "2697040"
  },
  {
    "text": "like low confidence words and then like retrain uh based on uh low performance",
    "start": "2697040",
    "end": "2703440"
  },
  {
    "text": "um we've also got a ton of other intelligence API so if you want to do summarization topic detection intent",
    "start": "2703440",
    "end": "2709760"
  },
  {
    "text": "recognition sentiment analysis uh you can send all of those off as well and I think of like a customer service one",
    "start": "2709760",
    "end": "2716680"
  },
  {
    "text": "here um and and those can be pretty useful because like detecting Topics in",
    "start": "2716680",
    "end": "2722000"
  },
  {
    "text": "an actual um audio file and where they happen um is super useful right so if",
    "start": "2722000",
    "end": "2729079"
  },
  {
    "text": "you if you're in a call center and you want to understand you know at a high level how many calls you know of my",
    "start": "2729079",
    "end": "2734359"
  },
  {
    "text": "millions of calls touched on these different things and and which ones to automate uh and we usually say to people",
    "start": "2734359",
    "end": "2740559"
  },
  {
    "text": "that are you know automating the call center is like you know Step One is analyze all your all your existing calls",
    "start": "2740559",
    "end": "2746640"
  },
  {
    "text": "right figure out what you've got and if you've got 40% you know uh phone issue",
    "start": "2746640",
    "end": "2752000"
  },
  {
    "text": "look at automating you know the phone issue first right and um a lot of them do agent assist which is like bubbling",
    "start": "2752000",
    "end": "2758839"
  },
  {
    "text": "up knowledge based articles to real people uh and then once they have that built you know it's very easy to then",
    "start": "2758839",
    "end": "2764400"
  },
  {
    "text": "just you know uh use an AI um we don't necessarily want to replace people in call centers we just want to take away",
    "start": "2764400",
    "end": "2770680"
  },
  {
    "text": "the work that uh can be automated um so like what we're seeing now in call",
    "start": "2770680",
    "end": "2775720"
  },
  {
    "text": "centers is that like one call center agent using a cloned AI voice can hand",
    "start": "2775720",
    "end": "2781119"
  },
  {
    "text": "off a call um to the agent so let's say it's collecting credit card information they can just you know press a button",
    "start": "2781119",
    "end": "2787839"
  },
  {
    "text": "let the AI collect credit card information and they can do five calls simultaneously and then when a call",
    "start": "2787839",
    "end": "2793839"
  },
  {
    "text": "needs their help they can jump over to the call that needs their help um so you're kind of fxing productivity uh",
    "start": "2793839",
    "end": "2799640"
  },
  {
    "text": "with that approach yeah C at the back there how do you go about monitoring like if you want",
    "start": "2799640",
    "end": "2807000"
  },
  {
    "text": "a monitor where wrong ja all those things mhm yeah",
    "start": "2807000",
    "end": "2815559"
  },
  {
    "text": "so there's multiple places you can do that um you're probably going to have the agent actually running on a server",
    "start": "2815559",
    "end": "2821119"
  },
  {
    "text": "uh you know for this Workshop we just put it in a browser um but you know the vast majority of people will have this",
    "start": "2821119",
    "end": "2827319"
  },
  {
    "text": "agent running directly like on a twilio uh you know phone line um so when when",
    "start": "2827319",
    "end": "2832680"
  },
  {
    "text": "that agent is doing the work um you would handle that in your server code right you might have some Moder",
    "start": "2832680",
    "end": "2837920"
  },
  {
    "text": "moderation code that um you know detects something out of out of allow and then",
    "start": "2837920",
    "end": "2843599"
  },
  {
    "text": "you know uh blocks it um you can definitely add it to the system prompt but uh it's only going to get you so",
    "start": "2843599",
    "end": "2850400"
  },
  {
    "text": "far Y what kind",
    "start": "2850400",
    "end": "2855559"
  },
  {
    "text": "ofu yeah so the question is about like how to monitor and and track metrics is",
    "start": "2867240",
    "end": "2874280"
  },
  {
    "text": "it MH yeah yeah yeah so there's a lot of Open Source projects out there at the",
    "start": "2874280",
    "end": "2879640"
  },
  {
    "text": "moment for kind of doing agent Ops uh one of them is called Agent Ops uh which is pretty good that can give you you",
    "start": "2879640",
    "end": "2885960"
  },
  {
    "text": "know uh temporal debugging so you can actually debug what was happening throughout the llm flow um so there's a",
    "start": "2885960",
    "end": "2892559"
  },
  {
    "text": "lot of stuff in that space that's happening right now but yeah a lot of your typical uh monitoring tools will",
    "start": "2892559",
    "end": "2897960"
  },
  {
    "text": "will work there as well y um well I know of course that's",
    "start": "2897960",
    "end": "2904160"
  },
  {
    "text": "basically a big part of your company secret so to speak I would be",
    "start": "2904160",
    "end": "2910520"
  },
  {
    "text": "interested in general techniques you use to reach for speed up compared to just",
    "start": "2910520",
    "end": "2918359"
  },
  {
    "text": "the traditional building your own just as you show in the beginning to",
    "start": "2918359",
    "end": "2925680"
  },
  {
    "text": "text",
    "start": "2925680",
    "end": "2928640"
  },
  {
    "text": "the yes so a lot of our customers do that today right they host all the different pieces um and it becomes like",
    "start": "2933200",
    "end": "2939839"
  },
  {
    "text": "a large infrastructure challenge right you and we'll we'll touch on that a little bit later as well uh as a part of",
    "start": "2939839",
    "end": "2945799"
  },
  {
    "text": "the the agent swarm stuff but it's like how do you make sure that you have low latency in different regions right so",
    "start": "2945799",
    "end": "2953040"
  },
  {
    "text": "people in the EU don't want to be hitting a server in the US right um not just for gdpr reasons but the latency is",
    "start": "2953040",
    "end": "2959280"
  },
  {
    "text": "going to be higher same with APAC so now you have to build and scale each of your",
    "start": "2959280",
    "end": "2965079"
  },
  {
    "text": "clusters uh in multiple regions and they have to be able to Auto scale as well right like um one of the major use cases",
    "start": "2965079",
    "end": "2972480"
  },
  {
    "text": "for AI agents is Peak traffic right so like you might only need 10 customer",
    "start": "2972480",
    "end": "2978160"
  },
  {
    "text": "service agents you know five days a week but if there's like an outage in pg&",
    "start": "2978160",
    "end": "2983559"
  },
  {
    "text": "suddenly they need a million agents right for one hour and so the ability to actually scale up uh same with um you",
    "start": "2983559",
    "end": "2990040"
  },
  {
    "text": "know 911 services um they can't take all the calls when there's a large disaster",
    "start": "2990040",
    "end": "2995480"
  },
  {
    "text": "or something happens um so a lot of people actually just you know get a a busy tone and so the ability to to do",
    "start": "2995480",
    "end": "3002119"
  },
  {
    "text": "that Spike up and scale in multiple regions and that that's a huge challenge for a lot of startups so having somebody",
    "start": "3002119",
    "end": "3008720"
  },
  {
    "text": "that offers that as a service I think is is pretty useful sorry what I actually",
    "start": "3008720",
    "end": "3014400"
  },
  {
    "text": "meant question was more like if you thetically have it all running locally",
    "start": "3014400",
    "end": "3019880"
  },
  {
    "text": "so we completely forget about actual Hardware or scaling Etc what kind of te is there",
    "start": "3019880",
    "end": "3029160"
  },
  {
    "text": "to yeah so so when you run in or hosted API you're running at a 1 second",
    "start": "3032680",
    "end": "3037920"
  },
  {
    "text": "interval right so every second you're getting what was spoken right kind of like a metronome um once you run it",
    "start": "3037920",
    "end": "3044480"
  },
  {
    "text": "yourself you can crank that up and you can say you know what I'm going to run it five times a second um so you're",
    "start": "3044480",
    "end": "3050200"
  },
  {
    "text": "inferencing like an Ever growing context window um you know at a much faster Pace",
    "start": "3050200",
    "end": "3056240"
  },
  {
    "text": "uh most words are about half a second long so you know you're chop you're basically inferencing words you know",
    "start": "3056240",
    "end": "3062599"
  },
  {
    "text": "partially as well so the word something might be so some something right so",
    "start": "3062599",
    "end": "3068160"
  },
  {
    "text": "you're getting this increasing um context window uh we run with like a 3",
    "start": "3068160",
    "end": "3073359"
  },
  {
    "text": "to 5c context window in a real time streaming uh which allows us to solidify",
    "start": "3073359",
    "end": "3078640"
  },
  {
    "text": "you know every 3 to 5 Seconds what was spoken um and then as soon as we detect that end of speech we'll basically you",
    "start": "3078640",
    "end": "3084920"
  },
  {
    "text": "know say we're not going to get any more words let's you know finalize what we have so far uh and that's really how you",
    "start": "3084920",
    "end": "3091280"
  },
  {
    "text": "can achieve those low latencies um but it is a lot of compute um yeah but with our system it's very fast very uh light",
    "start": "3091280",
    "end": "3098480"
  },
  {
    "text": "on comput so you can actually run a lot of streams like on a Tesla",
    "start": "3098480",
    "end": "3104200"
  },
  {
    "text": "T4 awesome oh one more question using our LM how can",
    "start": "3104200",
    "end": "3111760"
  },
  {
    "text": "we yeah so we we offer uh self hosting um so you can basically basically you",
    "start": "3111760",
    "end": "3117000"
  },
  {
    "text": "know grab our Docker images models uh run it on a GPU um like if if you can",
    "start": "3117000",
    "end": "3122319"
  },
  {
    "text": "get like three gpus on a single motorboard um that's going to give you you know Lightning Fast uh uh end to",
    "start": "3122319",
    "end": "3130400"
  },
  {
    "text": "end all right one more",
    "start": "3130400",
    "end": "3134039"
  },
  {
    "text": "question um so speed quality and price so yeah all",
    "start": "3144240",
    "end": "3152559"
  },
  {
    "text": "Tre all yeah it's it's it's easy to sell deep ground uh okay I'm going to jump",
    "start": "3153119",
    "end": "3159559"
  },
  {
    "text": "back into slides because I think some of the other stuff might be of interest as well",
    "start": "3159559",
    "end": "3165319"
  },
  {
    "text": "um so yeah so some more advanced features that you could play around with um make a whole new back end right maybe",
    "start": "3165319",
    "end": "3173160"
  },
  {
    "text": "it's you know a table booking API um you know a lot of businesses have to answer",
    "start": "3173160",
    "end": "3179280"
  },
  {
    "text": "the phone um I don't think they want to right they're already busy um handle multi-agent flows so have a rooting",
    "start": "3179280",
    "end": "3186240"
  },
  {
    "text": "agent with two sub agents um so you could have a booking agent and a cancellation agent um and that can be",
    "start": "3186240",
    "end": "3191799"
  },
  {
    "text": "the same voice on the same phone line um so you know if I say hey I want to make a booking root it to the booking agent",
    "start": "3191799",
    "end": "3198720"
  },
  {
    "text": "or I want to cancel a booking root it to canellation agent um th those are uh essentially how you would build out",
    "start": "3198720",
    "end": "3204799"
  },
  {
    "text": "these uh these more complex uh systems uh use cases call center AI",
    "start": "3204799",
    "end": "3210720"
  },
  {
    "text": "agents I think this is already here uh we we're seeing this right now like you",
    "start": "3210720",
    "end": "3216000"
  },
  {
    "text": "know you you have companies that have replaced you know a large proportion of their call volume with AI um and you",
    "start": "3216000",
    "end": "3224559"
  },
  {
    "text": "know they they still employ call center agents because you know they they always need to hand off difficult calls that",
    "start": "3224559",
    "end": "3230799"
  },
  {
    "text": "they haven't covered yet with the AI agent you know to somebody that wants to handle it um iot AI devices right so",
    "start": "3230799",
    "end": "3237799"
  },
  {
    "text": "wearables uh Toys things like that um there's a lot of them out there um and",
    "start": "3237799",
    "end": "3244240"
  },
  {
    "text": "then yeah AI worker agents so you know working in a Drive-Thru taking those orders and you know the workers that are",
    "start": "3244240",
    "end": "3250880"
  },
  {
    "text": "actually doing that are also you know busy preparing food and doing other things um yeah so multi-agent swarms um",
    "start": "3250880",
    "end": "3260079"
  },
  {
    "text": "so reduced complexity keep it simple right get something that works really well really robust and kind of like box",
    "start": "3260079",
    "end": "3267400"
  },
  {
    "text": "it off right like single responsibility um reduce the costs use the smallest cheapest model you can to achieve the",
    "start": "3267400",
    "end": "3274200"
  },
  {
    "text": "use case um like right now it's probably going to be those bigger models but I think in time you know the price point",
    "start": "3274200",
    "end": "3280400"
  },
  {
    "text": "of those is going to come down and the new generation will will kind of take its place so you know every 6 months",
    "start": "3280400",
    "end": "3285839"
  },
  {
    "text": "we're seeing like a 10x droing cost um and then composability so you can reuse",
    "start": "3285839",
    "end": "3291920"
  },
  {
    "text": "you know a sub agent in multiple different flows um so this is a kind of a pretty",
    "start": "3291920",
    "end": "3298720"
  },
  {
    "text": "basic kind of layout so you have your Rootin agent that's able to figure out you know which agent to use um you have",
    "start": "3298720",
    "end": "3306119"
  },
  {
    "text": "a support agent and a booking agent in this example and maybe you have a technical support agent and an account",
    "start": "3306119",
    "end": "3311480"
  },
  {
    "text": "support agent right two different uh types of agent but they they can each kind of help depending on the need um",
    "start": "3311480",
    "end": "3317880"
  },
  {
    "text": "your tech support agent is probably going to be hooked up to some sort of rag system um Account Support an",
    "start": "3317880",
    "end": "3323200"
  },
  {
    "text": "existing booking agent is probably need to are going to need to verify uh that",
    "start": "3323200",
    "end": "3328359"
  },
  {
    "text": "this person you know owns the account that they're calling about new booking agent might uh leverage like a credit",
    "start": "3328359",
    "end": "3334640"
  },
  {
    "text": "card payment agent um yeah so scaling it right we we talked a little bit about the latency um",
    "start": "3334640",
    "end": "3341799"
  },
  {
    "text": "so distance uh kills latency right like if you call a server in the US from Asia",
    "start": "3341799",
    "end": "3348200"
  },
  {
    "text": "um you're going to see like you know a second extra uh at least latency um if",
    "start": "3348200",
    "end": "3353720"
  },
  {
    "text": "you want to do Regional scaling right you're going to need to have uh the ability to horizontally scale um you",
    "start": "3353720",
    "end": "3360760"
  },
  {
    "text": "know within us East within us West within Amia right you you're going to want to have redundancy as well and as",
    "start": "3360760",
    "end": "3367440"
  },
  {
    "text": "you add redundancy um you increase costs as well um but you know if you want High",
    "start": "3367440",
    "end": "3373440"
  },
  {
    "text": "availability and your agent to always be on you're going to need that uh redundancy to do it um and then",
    "start": "3373440",
    "end": "3380640"
  },
  {
    "text": "horizontal scaling within your you know your Regional clusters um we support kubernetes and we'll give you all the",
    "start": "3380640",
    "end": "3387000"
  },
  {
    "text": "auto scaling Helm charts and everything um so that can be pretty pretty powerful and but you can imagine like if you",
    "start": "3387000",
    "end": "3392920"
  },
  {
    "text": "wanted to you know build an agent do you really want to worry about all that",
    "start": "3392920",
    "end": "3398960"
  },
  {
    "text": "infrastructure right or do you want to just build the agent achieve the you know the value the business value and",
    "start": "3398960",
    "end": "3405039"
  },
  {
    "text": "and roll it out on a large scale yeah sorry going",
    "start": "3405039",
    "end": "3410480"
  },
  {
    "text": "on Bas this I was wondering what out on the",
    "start": "3410480",
    "end": "3417960"
  },
  {
    "text": "yeah I I think it's possible uh you still have to distribute the models right the models tend to be quite large",
    "start": "3432039",
    "end": "3437960"
  },
  {
    "text": "um I think Gemma is like you know two gigs you can put it in the browser but it's it's going to take you a while to",
    "start": "3437960",
    "end": "3443520"
  },
  {
    "text": "download you know as you move to mobile devices you you know it's going to be pretty hard to do it as well um like I I",
    "start": "3443520",
    "end": "3449799"
  },
  {
    "text": "do believe that there's a lot of use cases where on device makes sense and but those are like single single stream",
    "start": "3449799",
    "end": "3457319"
  },
  {
    "text": "right so you have a single stream you're sending a single request to an llm you're getting you know single response",
    "start": "3457319",
    "end": "3463400"
  },
  {
    "text": "um what we're building here is like you know a million simultaneous calls right can come in um that's never really going",
    "start": "3463400",
    "end": "3470799"
  },
  {
    "text": "to work on device um just from a distributions perspective I guess um but there there's use cases for it so uh",
    "start": "3470799",
    "end": "3478319"
  },
  {
    "text": "like for the wearable use case uh there's an open source project called friend uh help them integrate uh deep",
    "start": "3478319",
    "end": "3484119"
  },
  {
    "text": "grams real time uh speech recognition into that um so you know running it on",
    "start": "3484119",
    "end": "3489200"
  },
  {
    "text": "device doesn't necessarily mean the model has to be on device",
    "start": "3489200",
    "end": "3494838"
  },
  {
    "text": "um yeah",
    "start": "3503839",
    "end": "3507839"
  },
  {
    "text": "yeah something we're looking at uh you can run our current model on a Raspberry Pi um it's not going to be super fast",
    "start": "3513839",
    "end": "3521319"
  },
  {
    "text": "and it's not going to handle multiple concurrent uh requests but uh it will run uh so it can run on CPU not just",
    "start": "3521319",
    "end": "3529760"
  },
  {
    "text": "GPU question what service do you recommend for the tel",
    "start": "3531200",
    "end": "3537079"
  },
  {
    "text": "use for things other options yeah there's a few telephony",
    "start": "3537079",
    "end": "3542559"
  },
  {
    "text": "providers out there uh twio vaj um we're kind of you know telepon agnostic as",
    "start": "3542559",
    "end": "3548680"
  },
  {
    "text": "long as you can get us the the audio stream uh usually that's achieved either through like you know hooking into their",
    "start": "3548680",
    "end": "3554559"
  },
  {
    "text": "API or just doing a zip trunk um so zip trunk basically just hands off the processing of the call to a different",
    "start": "3554559",
    "end": "3562880"
  },
  {
    "text": "server yeah um G4 eventually releases",
    "start": "3562960",
    "end": "3568960"
  },
  {
    "text": "their audio to Audio model is deep kind of be likeing around that appliations",
    "start": "3568960",
    "end": "3575319"
  },
  {
    "text": "around uh I I don't think we would necessarily use it in in that regard um",
    "start": "3575319",
    "end": "3581400"
  },
  {
    "text": "so uh I think it's great for the space that you know they're they're releasing this um I think maybe in the future this",
    "start": "3581400",
    "end": "3588680"
  },
  {
    "text": "type of uh multimodal model will make sense and but it's yet to be seen what the price point is going to be and what",
    "start": "3588680",
    "end": "3594880"
  },
  {
    "text": "the latency will be um like even their check completion API for for 40 is taking like up to a second um so if you",
    "start": "3594880",
    "end": "3602000"
  },
  {
    "text": "add audio into that as well that's additional processing um like it was a really cool demo and and I loved like",
    "start": "3602000",
    "end": "3608720"
  },
  {
    "text": "how the like a single model could have you know infinite voices um and I I",
    "start": "3608720",
    "end": "3613920"
  },
  {
    "text": "think that's where we'll see a lot of uh changes in the future um at Deep gram our main focus is um like scalable uh",
    "start": "3613920",
    "end": "3621839"
  },
  {
    "text": "low cost efficient uh inferencing so you know the ability to run at these price",
    "start": "3621839",
    "end": "3627319"
  },
  {
    "text": "points um you know is probably going to be a barrier to entry um but yeah I'm looking forward to see what they what",
    "start": "3627319",
    "end": "3633760"
  },
  {
    "text": "they release and and when yeah when you implement solution",
    "start": "3633760",
    "end": "3642119"
  },
  {
    "text": "for yeah so for model fine tuning uh the question was how long does it take so we",
    "start": "3646200",
    "end": "3651400"
  },
  {
    "text": "require between like 20 and 50 hours of audio uh to find tun speech to text um",
    "start": "3651400",
    "end": "3657760"
  },
  {
    "text": "and then we actually human label that so we have our our own team of human labelers and what they'll do is they'll",
    "start": "3657760",
    "end": "3663440"
  },
  {
    "text": "actually do three passes so the first pass um usually works out at about like 12% word error rate so if if you give",
    "start": "3663440",
    "end": "3670280"
  },
  {
    "text": "somebody a piece of audio and you ask them to write down exactly what was said and they'll have errors in it as well um",
    "start": "3670280",
    "end": "3675880"
  },
  {
    "text": "so then we run it through a second pass they fix the prior errors and then the third pass is and they're all different",
    "start": "3675880",
    "end": "3681240"
  },
  {
    "text": "people as well so the third pass goes in and basically gets us to like 99% uh label accuracy uh and then we train the",
    "start": "3681240",
    "end": "3688160"
  },
  {
    "text": "model training the model is pretty quick right we could probably do it in under a day um but yeah getting all that audio",
    "start": "3688160",
    "end": "3693880"
  },
  {
    "text": "and then uh labeled and then we kick off the training cycle um for the text to speech side we don't offer cloning uh",
    "start": "3693880",
    "end": "3700200"
  },
  {
    "text": "voice cloning today I think there's a lot of concerns around you know what happens when you clone people's voices",
    "start": "3700200",
    "end": "3706000"
  },
  {
    "text": "and um we do do cloning for certain customers um so if a customer comes to",
    "start": "3706000",
    "end": "3711039"
  },
  {
    "text": "us and says hey you know we want to use you but we need our own voice for our brand um we can we can do that training",
    "start": "3711039",
    "end": "3717680"
  },
  {
    "text": "but that would be a a business engagement",
    "start": "3717680",
    "end": "3723559"
  },
  {
    "text": "Yeah question over",
    "start": "3723559",
    "end": "3727039"
  },
  {
    "text": "there yeah so right now uh everything is just on this sandbox API U this API will",
    "start": "3729559",
    "end": "3736240"
  },
  {
    "text": "probably go away after the workshop um but we'll have a a way to actually sign up uh for the API weight list so if you",
    "start": "3736240",
    "end": "3744160"
  },
  {
    "text": "do want to get access to this uh and then it will require an API key um and all of the services will be wrapped",
    "start": "3744160",
    "end": "3751920"
  },
  {
    "text": "under a single um kind of usage fee so your speech to text your llm and your",
    "start": "3751920",
    "end": "3757520"
  },
  {
    "text": "text to speech will all be under a single uh",
    "start": "3757520",
    "end": "3762640"
  },
  {
    "text": "cost awesome any more",
    "start": "3763000",
    "end": "3768200"
  },
  {
    "text": "questions yeah so you go yeah I was just wondering are there any plans to allow kind of multiple speaker input within",
    "start": "3768200",
    "end": "3775079"
  },
  {
    "text": "this M being able to recognize speaker one speaker two speaker yeah yes so we",
    "start": "3775079",
    "end": "3781599"
  },
  {
    "text": "we have diarization so if you send multiple speakers on the same uh channel",
    "start": "3781599",
    "end": "3788160"
  },
  {
    "text": "uh we'll be able to determine you know speaker a speaker B um if you're sending us multi- Channel audio and that will",
    "start": "3788160",
    "end": "3795359"
  },
  {
    "text": "allow us to influence them separately um yeah so specifically me more the single",
    "start": "3795359",
    "end": "3800839"
  },
  {
    "text": "Channel and I played around a little bit it's not perfect today I'm just wondering are to kind of enhance that",
    "start": "3800839",
    "end": "3808319"
  },
  {
    "text": "yeah we're always improving uh diarization uh it's definitely a challenge because um to to understand",
    "start": "3808319",
    "end": "3814160"
  },
  {
    "text": "how diarization works is you're building up embeddings of what people say um so",
    "start": "3814160",
    "end": "3819440"
  },
  {
    "text": "like our conversation so far you you've had maybe three or four sentences you know maybe 20 seconds of audio and that",
    "start": "3819440",
    "end": "3826359"
  },
  {
    "text": "may not be enough for the model to say okay this is a unique speaker right um and it's it's building out these",
    "start": "3826359",
    "end": "3832079"
  },
  {
    "text": "embeddings in like 512 dimensional space so you know as more data comes in and we",
    "start": "3832079",
    "end": "3837680"
  },
  {
    "text": "we typically recommend 30 seconds per speaker um to actually generate a solid",
    "start": "3837680",
    "end": "3842799"
  },
  {
    "text": "embedding um if if we were to lower that requirement we might start like mislabeling people from the same person",
    "start": "3842799",
    "end": "3850319"
  },
  {
    "text": "um but it it is a challenge and it's I don't think it's ever going to be perfect um you know one of the hardest",
    "start": "3850319",
    "end": "3856440"
  },
  {
    "text": "parts of diarization is actually when um is actually when uh people actually say",
    "start": "3856440",
    "end": "3863799"
  },
  {
    "text": "like yeah or mm so like like if if you're on a call and somebody while you're speaking says yeah it's very hard",
    "start": "3863799",
    "end": "3870520"
  },
  {
    "text": "for the AI with that tiny little you know segment of audio to know that it's",
    "start": "3870520",
    "end": "3875599"
  },
  {
    "text": "somebody else speaking um but yeah we we we've seen a lot of cases where you know",
    "start": "3875599",
    "end": "3880839"
  },
  {
    "text": "if it's a longer call um it works very well but those first 60 seconds it's",
    "start": "3880839",
    "end": "3886440"
  },
  {
    "text": "probably not going to determine who's who sure sense and I imagine behind the",
    "start": "3886440",
    "end": "3891599"
  },
  {
    "text": "scenes there's maybe some accuracy percentage right simil or like is that",
    "start": "3891599",
    "end": "3897119"
  },
  {
    "text": "something that might ever get exposed or is that exposed today so we can kind of make a decision our right where it's we",
    "start": "3897119",
    "end": "3903160"
  },
  {
    "text": "get something back it's probably a 5% chance that it's this guy speaking that would be really helpful",
    "start": "3903160",
    "end": "3910640"
  },
  {
    "text": "yeah and one of the things a lot of people ask for is the ability to you know get speaker identification right so",
    "start": "3910640",
    "end": "3917039"
  },
  {
    "text": "like a unique identifier for a speaker so like if you have a call center agent",
    "start": "3917039",
    "end": "3922279"
  },
  {
    "text": "and you know who they are on every call you know could you pass that to us and we'll tell you you know where they the",
    "start": "3922279",
    "end": "3928480"
  },
  {
    "text": "first Speaker the second speaker and it's not something we expose today um obviously there's you know legal",
    "start": "3928480",
    "end": "3934520"
  },
  {
    "text": "challenges around fingerprinting voices and stuff um but yeah it's something we're thinking about the ability to at",
    "start": "3934520",
    "end": "3941440"
  },
  {
    "text": "least like identify a speaker um and just say like you know this speaker is",
    "start": "3941440",
    "end": "3946680"
  },
  {
    "text": "this person you're welcome awesome um so I think that's",
    "start": "3946680",
    "end": "3954880"
  },
  {
    "text": "everything I have uh I'm interested though had anybody achieved order update with the remove",
    "start": "3954880",
    "end": "3963039"
  },
  {
    "text": "item any anybody get any uh additional apis up and",
    "start": "3963720",
    "end": "3970319"
  },
  {
    "text": "running you got the demo little B too loud yeah right right yeah it's fast yeah I",
    "start": "3973119",
    "end": "3981920"
  },
  {
    "text": "told it to lots of items back to back did nice yeah and and this is again running on our hosted API so we we",
    "start": "3981920",
    "end": "3988480"
  },
  {
    "text": "haven't even optimized this for low latency yet um but you can see how quick it is even with those um hosted apis",
    "start": "3988480",
    "end": "3995920"
  },
  {
    "text": "that it can respond that time um like we we did run a um kind of a Sandbox",
    "start": "3995920",
    "end": "4002279"
  },
  {
    "text": "environment where you know we cranked up that compute and and it was just so fast that like it it was just like kind of",
    "start": "4002279",
    "end": "4008960"
  },
  {
    "text": "like interrupting you like the moment you stop speaking um uh which is which",
    "start": "4008960",
    "end": "4014160"
  },
  {
    "text": "is a pretty funny uh challenge but yeah uh thanks everybody for coming",
    "start": "4014160",
    "end": "4020039"
  },
  {
    "text": "um feel free to hit me up uh or chat with me after uh the workshop but yeah",
    "start": "4020039",
    "end": "4025240"
  },
  {
    "text": "hope you all enjoyed it [Music]",
    "start": "4025240",
    "end": "4040800"
  }
]