[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "[Music]",
    "start": "350",
    "end": "14050"
  },
  {
    "text": "hi everyone uh in this session we're",
    "start": "14639",
    "end": "16560"
  },
  {
    "text": "going to talk about uh everything you",
    "start": "16560",
    "end": "18119"
  },
  {
    "text": "need to know about fine-tuning llms and",
    "start": "18119",
    "end": "20600"
  },
  {
    "text": "model merging uh quick intro my name is",
    "start": "20600",
    "end": "23720"
  },
  {
    "text": "Maxim laon uh I'm a Staff machine",
    "start": "23720",
    "end": "26320"
  },
  {
    "text": "learning scientist at liquid AI I'm also",
    "start": "26320",
    "end": "28599"
  },
  {
    "text": "a go developer expert I write blog post",
    "start": "28599",
    "end": "31359"
  },
  {
    "text": "on these topics I created the LM course",
    "start": "31359",
    "end": "34200"
  },
  {
    "text": "which is uh super popular in GitHub um I",
    "start": "34200",
    "end": "37000"
  },
  {
    "text": "also contributed to the open source",
    "start": "37000",
    "end": "38399"
  },
  {
    "text": "community Through models uh through",
    "start": "38399",
    "end": "41079"
  },
  {
    "text": "tools and I'm the author of Hands-On",
    "start": "41079",
    "end": "43640"
  },
  {
    "text": "graph Neal networks using python uh with",
    "start": "43640",
    "end": "46960"
  },
  {
    "text": "packed um so first of all let's talk",
    "start": "46960",
    "end": "49600"
  },
  {
    "text": "about fine truning uh we saw a bit of",
    "start": "49600",
    "end": "51760"
  },
  {
    "text": "fine training in the previous session so",
    "start": "51760",
    "end": "53640"
  },
  {
    "text": "I'll try to not repeat too much uh but",
    "start": "53640",
    "end": "56920"
  },
  {
    "start": "54000",
    "end": "114000"
  },
  {
    "text": "basically here's the llm training life",
    "start": "56920",
    "end": "59280"
  },
  {
    "text": "cycle you see three stages uh first of",
    "start": "59280",
    "end": "61920"
  },
  {
    "text": "all you have the pre-training stage uh",
    "start": "61920",
    "end": "64198"
  },
  {
    "text": "where you give a lot of Road text to the",
    "start": "64199",
    "end": "66720"
  },
  {
    "text": "model and the idea is that the model",
    "start": "66720",
    "end": "69360"
  },
  {
    "text": "learns to do a next token",
    "start": "69360",
    "end": "71560"
  },
  {
    "text": "prediction the result of that is called",
    "start": "71560",
    "end": "73640"
  },
  {
    "text": "a base model this base model uh is",
    "start": "73640",
    "end": "76360"
  },
  {
    "text": "really nice uh but if you ask it",
    "start": "76360",
    "end": "79640"
  },
  {
    "text": "questions or instructions it's going to",
    "start": "79640",
    "end": "81720"
  },
  {
    "text": "autocomplete your question instead of",
    "start": "81720",
    "end": "83560"
  },
  {
    "text": "answering it which is why we have the",
    "start": "83560",
    "end": "85799"
  },
  {
    "text": "supervis fine tuning stage where this",
    "start": "85799",
    "end": "88200"
  },
  {
    "text": "time we give pairs of questions and",
    "start": "88200",
    "end": "90720"
  },
  {
    "text": "answers to the model and we have a",
    "start": "90720",
    "end": "93399"
  },
  {
    "text": "similar uh training objective but the",
    "start": "93399",
    "end": "95600"
  },
  {
    "text": "idea is that uh at the end of it it's",
    "start": "95600",
    "end": "98079"
  },
  {
    "text": "going to actually answer your questions",
    "start": "98079",
    "end": "100079"
  },
  {
    "text": "and follow your instructions then we",
    "start": "100079",
    "end": "102079"
  },
  {
    "text": "have a third and final stage the",
    "start": "102079",
    "end": "104000"
  },
  {
    "text": "preference alignment stage where we give",
    "start": "104000",
    "end": "106759"
  },
  {
    "text": "uh human preferences to align the model",
    "start": "106759",
    "end": "109000"
  },
  {
    "text": "to how we want it to behave and the",
    "start": "109000",
    "end": "111600"
  },
  {
    "text": "result is commonly referred to as chat",
    "start": "111600",
    "end": "113560"
  },
  {
    "text": "model so when to use fine tuning uh here",
    "start": "113560",
    "end": "116680"
  },
  {
    "start": "114000",
    "end": "182000"
  },
  {
    "text": "you can see a little flat chart that",
    "start": "116680",
    "end": "118920"
  },
  {
    "text": "I've made it's very level uh but",
    "start": "118920",
    "end": "121600"
  },
  {
    "text": "basically uh there's a conversation",
    "start": "121600",
    "end": "124119"
  },
  {
    "text": "about when to use promp engineering when",
    "start": "124119",
    "end": "126200"
  },
  {
    "text": "to use functioning I think it's good in",
    "start": "126200",
    "end": "128399"
  },
  {
    "text": "general to start with promp engineering",
    "start": "128399",
    "end": "130080"
  },
  {
    "text": "if you can and uh the idea is to have a",
    "start": "130080",
    "end": "133000"
  },
  {
    "text": "really robust evaluation uh structure",
    "start": "133000",
    "end": "136480"
  },
  {
    "text": "where you have a lot of different",
    "start": "136480",
    "end": "138239"
  },
  {
    "text": "metrics that you're interested in it can",
    "start": "138239",
    "end": "140040"
  },
  {
    "text": "be uh the accuracy of the model uh does",
    "start": "140040",
    "end": "142879"
  },
  {
    "text": "it answer my question well you can",
    "start": "142879",
    "end": "144920"
  },
  {
    "text": "create a custom Benchmark uh if you have",
    "start": "144920",
    "end": "147160"
  },
  {
    "text": "a very Niche use case or you can reuse",
    "start": "147160",
    "end": "150160"
  },
  {
    "text": "um open source benchmarks also in terms",
    "start": "150160",
    "end": "152239"
  },
  {
    "text": "of cost latency because the question is",
    "start": "152239",
    "end": "155280"
  },
  {
    "text": "is it good enough if it's good enough",
    "start": "155280",
    "end": "157000"
  },
  {
    "text": "with just prompt engineering then",
    "start": "157000",
    "end": "158319"
  },
  {
    "text": "probably you don't need fine tring the",
    "start": "158319",
    "end": "160080"
  },
  {
    "text": "problem is solved uh congrats otherwise",
    "start": "160080",
    "end": "162519"
  },
  {
    "text": "the question is can you make an",
    "start": "162519",
    "end": "163800"
  },
  {
    "text": "instruction data set so can you create",
    "start": "163800",
    "end": "166080"
  },
  {
    "text": "pairs of questions and answers uh to",
    "start": "166080",
    "end": "168920"
  },
  {
    "text": "fine tune the model if it's not the case",
    "start": "168920",
    "end": "171400"
  },
  {
    "text": "can be for multiple reasons but it's",
    "start": "171400",
    "end": "173040"
  },
  {
    "text": "probably a good sign that you need to",
    "start": "173040",
    "end": "174840"
  },
  {
    "text": "recope the project uh otherwise fine",
    "start": "174840",
    "end": "177080"
  },
  {
    "text": "truning is an option and you can reuse",
    "start": "177080",
    "end": "178879"
  },
  {
    "text": "the evaluation framework workk that you",
    "start": "178879",
    "end": "180400"
  },
  {
    "text": "created uh to evaluate the",
    "start": "180400",
    "end": "182599"
  },
  {
    "start": "182000",
    "end": "211000"
  },
  {
    "text": "model uh so that was the technical",
    "start": "182599",
    "end": "185239"
  },
  {
    "text": "answer but you also have a non-technical",
    "start": "185239",
    "end": "187080"
  },
  {
    "text": "answer to that uh here is a report from",
    "start": "187080",
    "end": "190040"
  },
  {
    "text": "a16z and the question is why do",
    "start": "190040",
    "end": "192959"
  },
  {
    "text": "Enterprises care about open source you",
    "start": "192959",
    "end": "194680"
  },
  {
    "text": "can see that the the two main items are",
    "start": "194680",
    "end": "196640"
  },
  {
    "text": "actually control and customer ability",
    "start": "196640",
    "end": "199319"
  },
  {
    "text": "and custom ability is mostly about fine",
    "start": "199319",
    "end": "201519"
  },
  {
    "text": "training models so even if there's like",
    "start": "201519",
    "end": "204920"
  },
  {
    "text": "arguments about the technical side and",
    "start": "204920",
    "end": "206640"
  },
  {
    "text": "cost and lency there's also like a",
    "start": "206640",
    "end": "208159"
  },
  {
    "text": "strong argument for customizability",
    "start": "208159",
    "end": "209879"
  },
  {
    "text": "control over these models so in terms of",
    "start": "209879",
    "end": "212599"
  },
  {
    "start": "211000",
    "end": "238000"
  },
  {
    "text": "fining libraries I think that you know",
    "start": "212599",
    "end": "214319"
  },
  {
    "text": "about ANS slof now uh but I'm going to",
    "start": "214319",
    "end": "216720"
  },
  {
    "text": "talk about the other ones uh so TRL from",
    "start": "216720",
    "end": "219879"
  },
  {
    "text": "hugging face uh Great Library buil on",
    "start": "219879",
    "end": "221920"
  },
  {
    "text": "top of Transformers very easy to use you",
    "start": "221920",
    "end": "224080"
  },
  {
    "text": "have Axel all excellent Library uh very",
    "start": "224080",
    "end": "226720"
  },
  {
    "text": "versatile uh you you have a lot of um",
    "start": "226720",
    "end": "229599"
  },
  {
    "text": "yaml config files and then you have lamb",
    "start": "229599",
    "end": "232400"
  },
  {
    "text": "Factory uh where you have a really good",
    "start": "232400",
    "end": "235159"
  },
  {
    "text": "graphical usual interface uh that is",
    "start": "235159",
    "end": "237120"
  },
  {
    "text": "built in so to talk a bit more about",
    "start": "237120",
    "end": "239879"
  },
  {
    "text": "supervis fine training um here you see",
    "start": "239879",
    "end": "242079"
  },
  {
    "text": "an example of a sample that we we give",
    "start": "242079",
    "end": "244920"
  },
  {
    "text": "to the model so we have the instruction",
    "start": "244920",
    "end": "247840"
  },
  {
    "text": "which is both the system prompt and the",
    "start": "247840",
    "end": "250000"
  },
  {
    "text": "user prompt and the answer which is the",
    "start": "250000",
    "end": "252599"
  },
  {
    "text": "output so in this uh in this case the",
    "start": "252599",
    "end": "254959"
  },
  {
    "text": "system prompt is used to steer the",
    "start": "254959",
    "end": "257440"
  },
  {
    "text": "behavior of the model uh think like",
    "start": "257440",
    "end": "259440"
  },
  {
    "text": "you're answering to a 5-year-old and the",
    "start": "259440",
    "end": "262040"
  },
  {
    "text": "user actually gives the task remove the",
    "start": "262040",
    "end": "264120"
  },
  {
    "text": "spaces from the following sentence we",
    "start": "264120",
    "end": "266560"
  },
  {
    "text": "train the model usually like generally",
    "start": "266560",
    "end": "269720"
  },
  {
    "text": "on the outputs only so we mask the rest",
    "start": "269720",
    "end": "272280"
  },
  {
    "text": "it's used as context and what we want to",
    "start": "272280",
    "end": "275000"
  },
  {
    "text": "to do is train the model to Output the",
    "start": "275000",
    "end": "277080"
  },
  {
    "text": "correct answer uh most sft data set I",
    "start": "277080",
    "end": "279919"
  },
  {
    "text": "want you to say that use synthetic data",
    "start": "279919",
    "end": "281720"
  },
  {
    "text": "and that's perfectly fine usually it's",
    "start": "281720",
    "end": "283199"
  },
  {
    "text": "generated with Frontier models and",
    "start": "283199",
    "end": "285360"
  },
  {
    "text": "that's a great way of uh building higher",
    "start": "285360",
    "end": "288039"
  },
  {
    "text": "quality data sets then you have the",
    "start": "288039",
    "end": "290560"
  },
  {
    "text": "preference alignments I'm just going to",
    "start": "290560",
    "end": "291960"
  },
  {
    "text": "mention it here uh there lot of",
    "start": "291960",
    "end": "293919"
  },
  {
    "text": "different methods pop DPO kto IPO uh in",
    "start": "293919",
    "end": "297440"
  },
  {
    "text": "practice direct preference optimization",
    "start": "297440",
    "end": "299199"
  },
  {
    "text": "is probably the most popular one um so",
    "start": "299199",
    "end": "302520"
  },
  {
    "text": "here you see that you have a different",
    "start": "302520",
    "end": "303759"
  },
  {
    "text": "format uh with an instruction and you",
    "start": "303759",
    "end": "305840"
  },
  {
    "text": "have a chosen answer and a rejected",
    "start": "305840",
    "end": "307360"
  },
  {
    "text": "answer so the idea here is that you're",
    "start": "307360",
    "end": "309919"
  },
  {
    "text": "going to show like a positive example",
    "start": "309919",
    "end": "312479"
  },
  {
    "text": "negative example to the model and with",
    "start": "312479",
    "end": "314960"
  },
  {
    "text": "DPO the goal is to make sure that the",
    "start": "314960",
    "end": "317199"
  },
  {
    "text": "model that you're currently training",
    "start": "317199",
    "end": "318680"
  },
  {
    "text": "outputs higher probabilities uh for the",
    "start": "318680",
    "end": "321039"
  },
  {
    "text": "chosen answers than the untrained",
    "start": "321039",
    "end": "322880"
  },
  {
    "text": "version of the same model not going to",
    "start": "322880",
    "end": "324840"
  },
  {
    "text": "delve uh too much into the details here",
    "start": "324840",
    "end": "326800"
  },
  {
    "text": "but this is the general ID and can be",
    "start": "326800",
    "end": "328720"
  },
  {
    "text": "used to either answer the model how to",
    "start": "328720",
    "end": "330960"
  },
  {
    "text": "make a bomb uh The Chosen answer would",
    "start": "330960",
    "end": "333000"
  },
  {
    "text": "be as an aist I cannot tell you that or",
    "start": "333000",
    "end": "335639"
  },
  {
    "text": "it can also be used to um boost the",
    "start": "335639",
    "end": "338000"
  },
  {
    "text": "performance of the model in",
    "start": "338000",
    "end": "339560"
  },
  {
    "text": "general uh how to create sft data sets",
    "start": "339560",
    "end": "342600"
  },
  {
    "start": "340000",
    "end": "495000"
  },
  {
    "text": "so this is a very fundamental question",
    "start": "342600",
    "end": "345560"
  },
  {
    "text": "uh in the post trining world uh and the",
    "start": "345560",
    "end": "348919"
  },
  {
    "text": "the main question is okay what's a good",
    "start": "348919",
    "end": "350600"
  },
  {
    "text": "sample human evaluation is quite bad at",
    "start": "350600",
    "end": "353600"
  },
  {
    "text": "um actually reviewing the samples but",
    "start": "353600",
    "end": "356120"
  },
  {
    "text": "what I like to Define is like three main",
    "start": "356120",
    "end": "358479"
  },
  {
    "text": "features the first first one is the",
    "start": "358479",
    "end": "360400"
  },
  {
    "text": "accuracy we want the samples the outputs",
    "start": "360400",
    "end": "363160"
  },
  {
    "text": "to be uh factually correct um maybe no",
    "start": "363160",
    "end": "366800"
  },
  {
    "text": "typos would be good to uh we don't want",
    "start": "366800",
    "end": "369199"
  },
  {
    "text": "to compromise uh the knowledge of the",
    "start": "369199",
    "end": "372160"
  },
  {
    "text": "model by giving it fake",
    "start": "372160",
    "end": "374319"
  },
  {
    "text": "information uh then you have diversity",
    "start": "374319",
    "end": "376639"
  },
  {
    "text": "and diversity you want to cover um as",
    "start": "376639",
    "end": "378840"
  },
  {
    "text": "many topics as you can of course it",
    "start": "378840",
    "end": "380800"
  },
  {
    "text": "depends on your use case because if you",
    "start": "380800",
    "end": "382680"
  },
  {
    "text": "do summarization you won't be as general",
    "start": "382680",
    "end": "385639"
  },
  {
    "text": "as if you do a general purpose fine",
    "start": "385639",
    "end": "387880"
  },
  {
    "text": "tuning um but it's is a good idea to",
    "start": "387880",
    "end": "390840"
  },
  {
    "text": "include a lot of different uh topics",
    "start": "390840",
    "end": "393479"
  },
  {
    "text": "different writing styles um in this uh",
    "start": "393479",
    "end": "396199"
  },
  {
    "text": "data set and finally you have complexity",
    "start": "396199",
    "end": "398919"
  },
  {
    "text": "I think this one is is a bit less",
    "start": "398919",
    "end": "401039"
  },
  {
    "text": "trivial uh and it's about giving complex",
    "start": "401039",
    "end": "404080"
  },
  {
    "text": "task to the model forcing reasoning so",
    "start": "404080",
    "end": "406639"
  },
  {
    "text": "for example the output will have Chain",
    "start": "406639",
    "end": "409199"
  },
  {
    "text": "of Thought reasoning because you want to",
    "start": "409199",
    "end": "411639"
  },
  {
    "text": "train the model to have this kind of of",
    "start": "411639",
    "end": "413880"
  },
  {
    "text": "reasoning or it can be task like",
    "start": "413880",
    "end": "415800"
  },
  {
    "text": "summarization explain me like I'm",
    "start": "415800",
    "end": "418319"
  },
  {
    "text": "5-year-old uh this kind of task really",
    "start": "418319",
    "end": "421280"
  },
  {
    "text": "um forc the model not to only answer the",
    "start": "421280",
    "end": "424520"
  },
  {
    "text": "question like a QA with um answers you",
    "start": "424520",
    "end": "427240"
  },
  {
    "text": "could find on Wikipedia it also forces",
    "start": "427240",
    "end": "429560"
  },
  {
    "text": "it to reasen over the The Prompt and uh",
    "start": "429560",
    "end": "432479"
  },
  {
    "text": "give a more complex answer so as a",
    "start": "432479",
    "end": "435639"
  },
  {
    "text": "little recipe you can see here um I",
    "start": "435639",
    "end": "438039"
  },
  {
    "text": "would recommend in general uh starting",
    "start": "438039",
    "end": "440360"
  },
  {
    "text": "with open source data sets if you can uh",
    "start": "440360",
    "end": "442720"
  },
  {
    "text": "combine some of them then you can apply",
    "start": "442720",
    "end": "444840"
  },
  {
    "text": "different filters the first one is data",
    "start": "444840",
    "end": "447199"
  },
  {
    "text": "dat duplication it can be either exact",
    "start": "447199",
    "end": "449759"
  },
  {
    "text": "because you want to remove uh duplicates",
    "start": "449759",
    "end": "452080"
  },
  {
    "text": "it can be fuzzy uh so same um same idea",
    "start": "452080",
    "end": "456319"
  },
  {
    "text": "and then you have data quality filters",
    "start": "456319",
    "end": "458560"
  },
  {
    "text": "here you have different techniques can",
    "start": "458560",
    "end": "460039"
  },
  {
    "text": "be rule based filtering uh for example",
    "start": "460039",
    "end": "462199"
  },
  {
    "text": "you want to remove every single row",
    "start": "462199",
    "end": "464599"
  },
  {
    "text": "where you have as an a assistant I",
    "start": "464599",
    "end": "466479"
  },
  {
    "text": "cannot because people hate it uh but you",
    "start": "466479",
    "end": "469199"
  },
  {
    "text": "can also use more clever techniques like",
    "start": "469199",
    "end": "471599"
  },
  {
    "text": "reward models or LM as a judge to",
    "start": "471599",
    "end": "474319"
  },
  {
    "text": "evaluate the quality of each sample and",
    "start": "474319",
    "end": "476919"
  },
  {
    "text": "filter out the bad samples and then you",
    "start": "476919",
    "end": "479560"
  },
  {
    "text": "can use data Exploration with different",
    "start": "479560",
    "end": "481479"
  },
  {
    "text": "tools like L ecomic Atlas text",
    "start": "481479",
    "end": "483319"
  },
  {
    "text": "clustering to have um topic clustering",
    "start": "483319",
    "end": "485720"
  },
  {
    "text": "to visualize your data set uh to get",
    "start": "485720",
    "end": "488159"
  },
  {
    "text": "ideas on how to improve it and with",
    "start": "488159",
    "end": "490479"
  },
  {
    "text": "these ideas you can uh go back to data",
    "start": "490479",
    "end": "492720"
  },
  {
    "text": "generation and start the process all",
    "start": "492720",
    "end": "494800"
  },
  {
    "text": "over again in terms of sft techniques we",
    "start": "494800",
    "end": "498120"
  },
  {
    "start": "495000",
    "end": "562000"
  },
  {
    "text": "have three main techniques uh full fine",
    "start": "498120",
    "end": "500039"
  },
  {
    "text": "training this is like the most basic one",
    "start": "500039",
    "end": "501840"
  },
  {
    "text": "you take the base model and you just uh",
    "start": "501840",
    "end": "505039"
  },
  {
    "text": "train it on the instruction uh data set",
    "start": "505039",
    "end": "507599"
  },
  {
    "text": "it has the best performance but it's",
    "start": "507599",
    "end": "509039"
  },
  {
    "text": "also like efficient in general a more",
    "start": "509039",
    "end": "511360"
  },
  {
    "text": "efficient way of seeing it is lower uh",
    "start": "511360",
    "end": "513959"
  },
  {
    "text": "with lower you are going to freeze all",
    "start": "513959",
    "end": "516479"
  },
  {
    "text": "the pre-rain weights and you add",
    "start": "516479",
    "end": "518880"
  },
  {
    "text": "adapters to each targeted layer uh these",
    "start": "518880",
    "end": "522120"
  },
  {
    "text": "matrices A and B are these adapters so",
    "start": "522120",
    "end": "525560"
  },
  {
    "text": "you have um you don't train on all the",
    "start": "525560",
    "end": "528040"
  },
  {
    "text": "parameters of the base model you only",
    "start": "528040",
    "end": "530920"
  },
  {
    "text": "ret Trin a subset of them uh so this is",
    "start": "530920",
    "end": "534160"
  },
  {
    "text": "a lot faster uh but it can still be",
    "start": "534160",
    "end": "536760"
  },
  {
    "text": "costly because you're still loading the",
    "start": "536760",
    "end": "538720"
  },
  {
    "text": "entire model in 6bit 16bit Precision",
    "start": "538720",
    "end": "542440"
  },
  {
    "text": "here so a more efficient way is to",
    "start": "542440",
    "end": "545560"
  },
  {
    "text": "quantize uh the pre-train model here in",
    "start": "545560",
    "end": "548480"
  },
  {
    "text": "4bit Precision this is cow and you apply",
    "start": "548480",
    "end": "552200"
  },
  {
    "text": "the same idea that you had with low but",
    "start": "552200",
    "end": "554200"
  },
  {
    "text": "this time uh the weights are heavily",
    "start": "554200",
    "end": "556399"
  },
  {
    "text": "quantized so you have a lower VR usage",
    "start": "556399",
    "end": "559079"
  },
  {
    "text": "the problem is that it also degrades",
    "start": "559079",
    "end": "560680"
  },
  {
    "text": "performance so there's a trade-off here",
    "start": "560680",
    "end": "563279"
  },
  {
    "start": "562000",
    "end": "597000"
  },
  {
    "text": "um I want to briefly mention some",
    "start": "563279",
    "end": "565120"
  },
  {
    "text": "hyperparameters but Daniel already",
    "start": "565120",
    "end": "567839"
  },
  {
    "text": "talked about a lot of them so I'm going",
    "start": "567839",
    "end": "569680"
  },
  {
    "text": "to be brief I think the most important",
    "start": "569680",
    "end": "571440"
  },
  {
    "text": "one is the learning rate the learning",
    "start": "571440",
    "end": "573680"
  },
  {
    "text": "rate is modal dependent uh it requires a",
    "start": "573680",
    "end": "576279"
  },
  {
    "text": "few experiments to be able to really",
    "start": "576279",
    "end": "578519"
  },
  {
    "text": "tweak it and find the the best one",
    "start": "578519",
    "end": "580760"
  },
  {
    "text": "generally I would recommend to go as",
    "start": "580760",
    "end": "582560"
  },
  {
    "text": "high as you can uh until your loss",
    "start": "582560",
    "end": "584800"
  },
  {
    "text": "explodes like in this graph uh then you",
    "start": "584800",
    "end": "587399"
  },
  {
    "text": "can um reduce the size of the learning",
    "start": "587399",
    "end": "590120"
  },
  {
    "text": "rate uh other super important hyper",
    "start": "590120",
    "end": "592800"
  },
  {
    "text": "parameters number of epoch um I would",
    "start": "592800",
    "end": "595360"
  },
  {
    "text": "say that depending on the size of your",
    "start": "595360",
    "end": "596760"
  },
  {
    "text": "data set you can have like more or less",
    "start": "596760",
    "end": "598640"
  },
  {
    "start": "597000",
    "end": "635000"
  },
  {
    "text": "epoch sequence length is also good uh",
    "start": "598640",
    "end": "601560"
  },
  {
    "text": "because um it's a trade-off with the",
    "start": "601560",
    "end": "603959"
  },
  {
    "text": "batch size because the longer sequence L",
    "start": "603959",
    "end": "607480"
  },
  {
    "text": "you have so the the bigger the context",
    "start": "607480",
    "end": "609120"
  },
  {
    "text": "window the more vram you're going to use",
    "start": "609120",
    "end": "612040"
  },
  {
    "text": "uh but you don't need to use uh a",
    "start": "612040",
    "end": "614040"
  },
  {
    "text": "sequence L that's as big as the",
    "start": "614040",
    "end": "615839"
  },
  {
    "text": "pre-train model then you have the batch",
    "start": "615839",
    "end": "618200"
  },
  {
    "text": "size you want to maximize it to maximize",
    "start": "618200",
    "end": "620519"
  },
  {
    "text": "the utilization of your gpus um and then",
    "start": "620519",
    "end": "623760"
  },
  {
    "text": "you have the lower with the rank um this",
    "start": "623760",
    "end": "626800"
  },
  {
    "text": "is like quite easy to to fine tune so",
    "start": "626800",
    "end": "629440"
  },
  {
    "text": "don't want to go into the details here",
    "start": "629440",
    "end": "632040"
  },
  {
    "text": "um let's talk about model merging now so",
    "start": "632040",
    "end": "635120"
  },
  {
    "start": "635000",
    "end": "720000"
  },
  {
    "text": "model merging is the idea that you can",
    "start": "635120",
    "end": "637399"
  },
  {
    "text": "take the weights of different fine",
    "start": "637399",
    "end": "639440"
  },
  {
    "text": "models and you can combine them together",
    "start": "639440",
    "end": "642560"
  },
  {
    "text": "so you you just can leverage uh what the",
    "start": "642560",
    "end": "646959"
  },
  {
    "text": "open source Community uh has uh produced",
    "start": "646959",
    "end": "649760"
  },
  {
    "text": "on the hagging face hub for example uh",
    "start": "649760",
    "end": "652959"
  },
  {
    "text": "it doesn't required any GPU so it's",
    "start": "652959",
    "end": "654720"
  },
  {
    "text": "super efficient and it provides",
    "start": "654720",
    "end": "656320"
  },
  {
    "text": "excellent results uh so the open l",
    "start": "656320",
    "end": "659320"
  },
  {
    "text": "leaderboard was updated this morning uh",
    "start": "659320",
    "end": "661880"
  },
  {
    "text": "so we have a version two now but this is",
    "start": "661880",
    "end": "664360"
  },
  {
    "text": "the version one uh I haven't had time to",
    "start": "664360",
    "end": "666959"
  },
  {
    "text": "update it uh but you can see that for 7B",
    "start": "666959",
    "end": "670800"
  },
  {
    "text": "uh parameter models uh the entire top",
    "start": "670800",
    "end": "673959"
  },
  {
    "text": "eight or top 10 is just merge models uh",
    "start": "673959",
    "end": "676720"
  },
  {
    "text": "so it really shows that uh this approach",
    "start": "676720",
    "end": "678920"
  },
  {
    "text": "is extremely effective at producing high",
    "start": "678920",
    "end": "680959"
  },
  {
    "text": "quality models uh and you can find",
    "start": "680959",
    "end": "683519"
  },
  {
    "text": "similar results on like a really a lot",
    "start": "683519",
    "end": "685519"
  },
  {
    "text": "of different data sets I would recommend",
    "start": "685519",
    "end": "687600"
  },
  {
    "text": "using merch kit uh this is like the",
    "start": "687600",
    "end": "689440"
  },
  {
    "text": "leading library in this in this space uh",
    "start": "689440",
    "end": "692079"
  },
  {
    "text": "with a lot of different techniques that",
    "start": "692079",
    "end": "693440"
  },
  {
    "text": "are implemented there um so here you can",
    "start": "693440",
    "end": "696560"
  },
  {
    "text": "see the family tree of uh merge models",
    "start": "696560",
    "end": "700240"
  },
  {
    "text": "uh so in you don't really need to see",
    "start": "700240",
    "end": "703200"
  },
  {
    "text": "the the name of the model but you see",
    "start": "703200",
    "end": "705040"
  },
  {
    "text": "that every node is actually a model and",
    "start": "705040",
    "end": "708320"
  },
  {
    "text": "we actually merge different merges",
    "start": "708320",
    "end": "710399"
  },
  {
    "text": "together until it becomes like a giant",
    "start": "710399",
    "end": "712240"
  },
  {
    "text": "family tree this one is actually quite",
    "start": "712240",
    "end": "714519"
  },
  {
    "text": "small like it can get a lot crazier than",
    "start": "714519",
    "end": "716760"
  },
  {
    "text": "that but it didn't fit on one slide so I",
    "start": "716760",
    "end": "719600"
  },
  {
    "text": "choose this one instead um about the",
    "start": "719600",
    "end": "722279"
  },
  {
    "start": "720000",
    "end": "799000"
  },
  {
    "text": "merch techniques themselves I want to",
    "start": "722279",
    "end": "725279"
  },
  {
    "text": "mention like a few of them the first one",
    "start": "725279",
    "end": "728040"
  },
  {
    "text": "is called slurp it stands for spherical",
    "start": "728040",
    "end": "730320"
  },
  {
    "text": "linear interpolation so the idea is",
    "start": "730320",
    "end": "732360"
  },
  {
    "text": "really to apply spherical but linear",
    "start": "732360",
    "end": "735839"
  },
  {
    "text": "interpolation uh with the weights of",
    "start": "735839",
    "end": "737880"
  },
  {
    "text": "different models you can only merge two",
    "start": "737880",
    "end": "740120"
  },
  {
    "text": "models at the same time with this",
    "start": "740120",
    "end": "741760"
  },
  {
    "text": "technique uh but you can really tweak it",
    "start": "741760",
    "end": "744560"
  },
  {
    "text": "uh with different uh interpetation",
    "start": "744560",
    "end": "746440"
  },
  {
    "text": "factors for different layers uh here's",
    "start": "746440",
    "end": "749240"
  },
  {
    "text": "model that I've made your ble 147b uh",
    "start": "749240",
    "end": "752480"
  },
  {
    "text": "which was a really um efficient way of",
    "start": "752480",
    "end": "756880"
  },
  {
    "text": "uh leveraging the different models that",
    "start": "756880",
    "end": "759199"
  },
  {
    "text": "were created by the open source",
    "start": "759199",
    "end": "760560"
  },
  {
    "text": "community and then you have there uh so",
    "start": "760560",
    "end": "763680"
  },
  {
    "text": "in there you want to reduce the",
    "start": "763680",
    "end": "765480"
  },
  {
    "text": "redundancy of the model parameters uh to",
    "start": "765480",
    "end": "767839"
  },
  {
    "text": "do that you're going to use pruning",
    "start": "767839",
    "end": "769240"
  },
  {
    "text": "you're going to select the most",
    "start": "769240",
    "end": "770680"
  },
  {
    "text": "significant parameters in your model",
    "start": "770680",
    "end": "772600"
  },
  {
    "text": "weights and you're going to rescale um",
    "start": "772600",
    "end": "775600"
  },
  {
    "text": "the weights of these uh Source models uh",
    "start": "775600",
    "end": "778440"
  },
  {
    "text": "the AG that it has that you can merge",
    "start": "778440",
    "end": "781320"
  },
  {
    "text": "different models not just two but even",
    "start": "781320",
    "end": "783399"
  },
  {
    "text": "more together and I would advise uh I",
    "start": "783399",
    "end": "786639"
  },
  {
    "text": "would recommend this technique and not",
    "start": "786639",
    "end": "788519"
  },
  {
    "text": "with just two model not with three but",
    "start": "788519",
    "end": "790600"
  },
  {
    "text": "like with seven or eight models it works",
    "start": "790600",
    "end": "792600"
  },
  {
    "text": "really really well uh so I strongly",
    "start": "792600",
    "end": "794800"
  },
  {
    "text": "recommend",
    "start": "794800",
    "end": "796160"
  },
  {
    "text": "that uh then you have a very funny um uh",
    "start": "796160",
    "end": "799560"
  },
  {
    "start": "799000",
    "end": "866000"
  },
  {
    "text": "technique called pass through and in",
    "start": "799560",
    "end": "801560"
  },
  {
    "text": "pass through you can concate concate",
    "start": "801560",
    "end": "804440"
  },
  {
    "text": "layers from different llms um it can",
    "start": "804440",
    "end": "806880"
  },
  {
    "text": "also be the same one we call it self",
    "start": "806880",
    "end": "809399"
  },
  {
    "text": "merge um and so here you have an example",
    "start": "809399",
    "end": "812199"
  },
  {
    "text": "that I've made uh recently it's called",
    "start": "812199",
    "end": "814040"
  },
  {
    "text": "meta Lama 320b instruct because I took",
    "start": "814040",
    "end": "817880"
  },
  {
    "text": "Lama 370d instruct and I just uh",
    "start": "817880",
    "end": "820880"
  },
  {
    "text": "repeated 10 layers six times uh so you",
    "start": "820880",
    "end": "824360"
  },
  {
    "text": "could say like this shouldn't work at",
    "start": "824360",
    "end": "826120"
  },
  {
    "text": "all like come on you you haven't even",
    "start": "826120",
    "end": "828279"
  },
  {
    "text": "trained the model this is ridiculous uh",
    "start": "828279",
    "end": "830839"
  },
  {
    "text": "actually yeah this is ridiculous uh",
    "start": "830839",
    "end": "832360"
  },
  {
    "text": "people loved it on Twitter and Reddit",
    "start": "832360",
    "end": "834720"
  },
  {
    "text": "and online in general uh so it it shows",
    "start": "834720",
    "end": "837880"
  },
  {
    "text": "that uh there's a lot of things that we",
    "start": "837880",
    "end": "840759"
  },
  {
    "text": "can still discover uh with these merge",
    "start": "840759",
    "end": "843839"
  },
  {
    "text": "techniques with these models uh they do",
    "start": "843839",
    "end": "846639"
  },
  {
    "text": "not um they can be counterintuitive",
    "start": "846639",
    "end": "849160"
  },
  {
    "text": "sometimes and you can see that this",
    "start": "849160",
    "end": "850680"
  },
  {
    "text": "model in particular was particularly",
    "start": "850680",
    "end": "852120"
  },
  {
    "text": "good at uh creative writing uh it was",
    "start": "852120",
    "end": "855440"
  },
  {
    "text": "also quite unhinged in general but",
    "start": "855440",
    "end": "857800"
  },
  {
    "text": "really good at creative writing and uh",
    "start": "857800",
    "end": "859959"
  },
  {
    "text": "now it's being used by a lot of people",
    "start": "859959",
    "end": "862120"
  },
  {
    "text": "uh even though it's it's super big but",
    "start": "862120",
    "end": "864560"
  },
  {
    "text": "no kind ofine tuning at all no no fine",
    "start": "864560",
    "end": "866959"
  },
  {
    "start": "866000",
    "end": "912000"
  },
  {
    "text": "tuning nothing",
    "start": "866959",
    "end": "869680"
  },
  {
    "text": "um and then I want to mention the last",
    "start": "869680",
    "end": "872160"
  },
  {
    "text": "technique uh which is called mixture of",
    "start": "872160",
    "end": "873920"
  },
  {
    "text": "experts uh so in traditional mixture of",
    "start": "873920",
    "end": "876440"
  },
  {
    "text": "experts you are going to pre-train a",
    "start": "876440",
    "end": "879680"
  },
  {
    "text": "model uh with a router you can see on",
    "start": "879680",
    "end": "883240"
  },
  {
    "text": "the bottom here and different fit for w",
    "start": "883240",
    "end": "885720"
  },
  {
    "text": "netri layers and you pre pre-rain it",
    "start": "885720",
    "end": "888519"
  },
  {
    "text": "from scratch but you can do something",
    "start": "888519",
    "end": "891320"
  },
  {
    "text": "quite smart uh with merging where you",
    "start": "891320",
    "end": "894000"
  },
  {
    "text": "extract the fit forward Network layers",
    "start": "894000",
    "end": "896040"
  },
  {
    "text": "from different fine tune models and you",
    "start": "896040",
    "end": "897839"
  },
  {
    "text": "combine them together like this so we",
    "start": "897839",
    "end": "900680"
  },
  {
    "text": "call this a Franken M you add a router",
    "start": "900680",
    "end": "903720"
  },
  {
    "text": "you combine the FFN layers from",
    "start": "903720",
    "end": "905480"
  },
  {
    "text": "different uh models and this is how you",
    "start": "905480",
    "end": "908600"
  },
  {
    "text": "create like your kind of a mixture of",
    "start": "908600",
    "end": "911320"
  },
  {
    "text": "experts it's actually uh pretty cool it",
    "start": "911320",
    "end": "914560"
  },
  {
    "text": "works pretty well in practice uh you can",
    "start": "914560",
    "end": "916880"
  },
  {
    "text": "see on the left a merch kit config uh",
    "start": "916880",
    "end": "920320"
  },
  {
    "text": "for the Beyond uh model uh so for this",
    "start": "920320",
    "end": "923079"
  },
  {
    "text": "model I selected four different uh fing",
    "start": "923079",
    "end": "926279"
  },
  {
    "text": "models one uh as a chat model one as a",
    "start": "926279",
    "end": "929240"
  },
  {
    "text": "code model one as a roleplay model and",
    "start": "929240",
    "end": "931480"
  },
  {
    "text": "one as a math model you can see that I'm",
    "start": "931480",
    "end": "933600"
  },
  {
    "text": "using positive prompts here so actually",
    "start": "933600",
    "end": "936759"
  },
  {
    "text": "it's it's a way to initialize the router",
    "start": "936759",
    "end": "940000"
  },
  {
    "text": "because if you go back to the previous",
    "start": "940000",
    "end": "941920"
  },
  {
    "text": "uh slide we can see that the router is",
    "start": "941920",
    "end": "943959"
  },
  {
    "text": "supposed to uh select for each token and",
    "start": "943959",
    "end": "946839"
  },
  {
    "text": "each layer where like which fit forward",
    "start": "946839",
    "end": "949519"
  },
  {
    "text": "network uh layer is going to be used we",
    "start": "949519",
    "end": "952279"
  },
  {
    "text": "used to uh in general um and so how do",
    "start": "952279",
    "end": "955759"
  },
  {
    "text": "we initialize it if we do not find unit",
    "start": "955759",
    "end": "958079"
  },
  {
    "text": "once again we don't want to function it",
    "start": "958079",
    "end": "960240"
  },
  {
    "text": "we can but we we don't necessarily want",
    "start": "960240",
    "end": "961920"
  },
  {
    "text": "to in this case we just going to use",
    "start": "961920",
    "end": "964199"
  },
  {
    "text": "these positive prompts uh calculate the",
    "start": "964199",
    "end": "966199"
  },
  {
    "text": "embeddings and use these embeddings to",
    "start": "966199",
    "end": "968240"
  },
  {
    "text": "initialize uh the routers and that works",
    "start": "968240",
    "end": "970680"
  },
  {
    "text": "really really well so those are two",
    "start": "970680",
    "end": "972440"
  },
  {
    "text": "models that I've used for fix trol I had",
    "start": "972440",
    "end": "974199"
  },
  {
    "text": "to modify it to make it compatible with",
    "start": "974199",
    "end": "976360"
  },
  {
    "text": "F2 and uh that outperform the base model",
    "start": "976360",
    "end": "980399"
  },
  {
    "text": "uh on a lot of task um so it's really a",
    "start": "980399",
    "end": "984319"
  },
  {
    "text": "good technique to to use in general but",
    "start": "984319",
    "end": "986720"
  },
  {
    "text": "I would say that um if you comp compare",
    "start": "986720",
    "end": "989160"
  },
  {
    "text": "it to merging uh as we saw with slurp",
    "start": "989160",
    "end": "992959"
  },
  {
    "text": "and with dare I would say that um if you",
    "start": "992959",
    "end": "997160"
  },
  {
    "text": "want to increase the performance it's",
    "start": "997160",
    "end": "999000"
  },
  {
    "text": "better to use uh slap and dare instead",
    "start": "999000",
    "end": "1001759"
  },
  {
    "text": "of mixture of experts because this is a",
    "start": "1001759",
    "end": "1003720"
  },
  {
    "text": "bit more experimental uh this doesn't",
    "start": "1003720",
    "end": "1006959"
  },
  {
    "text": "this will not bring you the same level",
    "start": "1006959",
    "end": "1008519"
  },
  {
    "text": "of",
    "start": "1008519",
    "end": "1009440"
  },
  {
    "text": "performance um and here you can see the",
    "start": "1009440",
    "end": "1012040"
  },
  {
    "text": "results of the Beyond uh model uh you",
    "start": "1012040",
    "end": "1015440"
  },
  {
    "text": "can see that the other models I'm",
    "start": "1015440",
    "end": "1017560"
  },
  {
    "text": "comparing to are the um Source models",
    "start": "1017560",
    "end": "1020000"
  },
  {
    "text": "that have used in this in this merge uh",
    "start": "1020000",
    "end": "1022759"
  },
  {
    "text": "so it's quite remarkable to see that",
    "start": "1022759",
    "end": "1024798"
  },
  {
    "text": "it's actually performing better than the",
    "start": "1024799",
    "end": "1027120"
  },
  {
    "text": "source models um on the a lot of",
    "start": "1027120",
    "end": "1030438"
  },
  {
    "text": "different benchmarks um so yeah that's",
    "start": "1030439",
    "end": "1034360"
  },
  {
    "text": "it for me uh thank you for your",
    "start": "1034360",
    "end": "1035959"
  },
  {
    "text": "attention uh if you are interested in",
    "start": "1035959",
    "end": "1039360"
  },
  {
    "text": "knowing more if you want notebooks uh to",
    "start": "1039360",
    "end": "1042079"
  },
  {
    "text": "to run some code I created the large",
    "start": "1042079",
    "end": "1044319"
  },
  {
    "text": "language model course all these",
    "start": "1044319",
    "end": "1045839"
  },
  {
    "text": "notebooks are available on GitHub uh llm",
    "start": "1045839",
    "end": "1048520"
  },
  {
    "text": "course",
    "start": "1048520",
    "end": "1049520"
  },
  {
    "text": "and yeah thank you",
    "start": "1049520",
    "end": "1053720"
  },
  {
    "text": "[Music]",
    "start": "1056600",
    "end": "1073520"
  }
]