[
  {
    "text": "[Music]",
    "start": "350",
    "end": "13240"
  },
  {
    "text": "good to be here good to see a fair number of people it's early so I wasn't sure if anyone would come but thank you",
    "start": "13240",
    "end": "18920"
  },
  {
    "text": "for coming um one quick note I just put my slides on Twitter I wasn't sure the",
    "start": "18920",
    "end": "24680"
  },
  {
    "text": "best way to access everyone I'm at Rance Martin um if there's another way I can",
    "start": "24680",
    "end": "31279"
  },
  {
    "text": "get everyone's slides um then yeah I see some people opening um so the slides",
    "start": "31279",
    "end": "37480"
  },
  {
    "text": "will link to a few different uh it'll link to a collab and other notebooks I'm providing so all the code will be",
    "start": "37480",
    "end": "42719"
  },
  {
    "text": "available for you um okay good as I see people finding it that's fantastic uh if",
    "start": "42719",
    "end": "48480"
  },
  {
    "text": "there's a better way let me know but uh figure this is somewhat easy um well",
    "start": "48480",
    "end": "53960"
  },
  {
    "text": "it's great to be here we have a bit of time so I think the format is I'll like lay out some slides to kind of set the",
    "start": "53960",
    "end": "59680"
  },
  {
    "text": "pr Aries and like kind of give the big picture and then there'll be a bunch of time where I can just walk around talk",
    "start": "59680",
    "end": "65158"
  },
  {
    "text": "to people and because we have I guess three hours so I think the idea this will be a Hands-On Workshop I provide a",
    "start": "65159",
    "end": "70799"
  },
  {
    "text": "bunch of starter code and also one of my slides shows it'll be kind of a Choose Your Own Adventure",
    "start": "70799",
    "end": "76240"
  },
  {
    "text": "format um so why don't I kick it off I think maybe one or two people still coming in but so the the theme Here is",
    "start": "76240",
    "end": "83200"
  },
  {
    "text": "building uh and testing reliable agents and let me go to slideshow mode",
    "start": "83200",
    "end": "88479"
  },
  {
    "text": "here um and maybe I'll just kind of start with like the very Basics you know llm",
    "start": "88479",
    "end": "94479"
  },
  {
    "text": "applications follow a general control flow of some sort you start usually it's to user input there's some set of steps",
    "start": "94479",
    "end": "101439"
  },
  {
    "text": "and then you end um and you've heard a lot about chains you know when you build",
    "start": "101439",
    "end": "107240"
  },
  {
    "text": "applications often times we talk about this idea of chains and chain is just basically you know it is some control",
    "start": "107240",
    "end": "112520"
  },
  {
    "text": "flow set by the developer uh again you start proceed through some steps and you",
    "start": "112520",
    "end": "118000"
  },
  {
    "text": "end so retrieval to generation is a super popular application many some of",
    "start": "118000",
    "end": "123079"
  },
  {
    "text": "you may be familiar with um basically refers to retrieving documents from an index and passing them to an llm this is",
    "start": "123079",
    "end": "129319"
  },
  {
    "text": "a good example of a chain it's a control flow set by a user the questions you know provided um Vector store retrieves",
    "start": "129319",
    "end": "136920"
  },
  {
    "text": "them pass to an llm llm produces an answer so this is kind of a classic",
    "start": "136920",
    "end": "142599"
  },
  {
    "text": "chain now when you get into agents there's a lot of different confusing interpretations what is an agent here's",
    "start": "142599",
    "end": "147800"
  },
  {
    "text": "the way I might think about it which is just a really simple kind of framing is agent is just one of",
    "start": "147800",
    "end": "152959"
  },
  {
    "text": "the control flows set by an llm and so you can imagine we talked about this process of you start your app step one",
    "start": "152959",
    "end": "159959"
  },
  {
    "text": "step two in this case I have an LM in there LM looks at the output of step one makes the decision do I go back and do I",
    "start": "159959",
    "end": "166159"
  },
  {
    "text": "proceed so that's like the way to simple way to think about an agent so again chains developer defined",
    "start": "166159",
    "end": "172519"
  },
  {
    "text": "control flow I said it ahead of time I follow some set of steps every time an agent an LM kind of determines the",
    "start": "172519",
    "end": "179159"
  },
  {
    "text": "control flow LM makes a decision about where to go inside my application that's one simple way to think about it now you",
    "start": "179159",
    "end": "186280"
  },
  {
    "text": "hear about function calling a lot and this is kind of a confusing topic so I want to talk through it kind of",
    "start": "186280",
    "end": "191560"
  },
  {
    "text": "carefully um agents typically use function calling to kind of determine",
    "start": "191560",
    "end": "196959"
  },
  {
    "text": "what step to go to so usually the way this works is what you do is you basically give the llm awareness of some",
    "start": "196959",
    "end": "204080"
  },
  {
    "text": "number of tools or steps that it can take so in my little example here I Define this tool in this little",
    "start": "204080",
    "end": "209879"
  },
  {
    "text": "decorator is a line chain thing but the point is I have some step it's some function I'm defining it as a tool and",
    "start": "209879",
    "end": "217200"
  },
  {
    "text": "I'm would I'm binding it to the llm so then the llm has awareness of this tool and here's the key point when it sees an",
    "start": "217200",
    "end": "223519"
  },
  {
    "text": "input like what is the output of step two it actually produces the payload needed to run that tool now this is",
    "start": "223519",
    "end": "229879"
  },
  {
    "text": "often confusing remember llms are just string to string they don't have the ability to magically call some function",
    "start": "229879",
    "end": "235480"
  },
  {
    "text": "what they can do is produce the payload or arguments needed to run that function and the function name so really think",
    "start": "235480",
    "end": "240959"
  },
  {
    "text": "about tool calling or function calling is just an LM producing a structured output still you know obviously a string",
    "start": "240959",
    "end": "247480"
  },
  {
    "text": "but it's a structured output that then can be used to call a tool so that's all that function calling",
    "start": "247480",
    "end": "252879"
  },
  {
    "text": "is and you might have heard of react agents so the way to think about this is It's just basically binding some set of",
    "start": "252879",
    "end": "259560"
  },
  {
    "text": "tools to my llm and again we talked about tool calling so LM makes decisions about what",
    "start": "259560",
    "end": "265320"
  },
  {
    "text": "step or what tool to use and you have some node that will call that tool so LM",
    "start": "265320",
    "end": "270360"
  },
  {
    "text": "says okay run step one I have some node that runs step one and passes the output of step one back to my agent react",
    "start": "270360",
    "end": "277360"
  },
  {
    "text": "typically stands for like basically action so the LM chooses the action a tool is a tool is run it observes the",
    "start": "277360",
    "end": "285440"
  },
  {
    "text": "output that's what goes back to the agent observe that tool response thinks about what to do next maybe runs another",
    "start": "285440",
    "end": "290479"
  },
  {
    "text": "tool and this runs in a loop until you end and usually the end condition is the LM just outputs a a string response not",
    "start": "290479",
    "end": "296759"
  },
  {
    "text": "a tool call so this is the way to think about a classic react agent um and it's really flexible that's",
    "start": "296759",
    "end": "304320"
  },
  {
    "text": "the nice thing about a react agent so basically it can Implement many different control flows it can do step",
    "start": "304320",
    "end": "309800"
  },
  {
    "text": "one only step two one two 2 one that's the beauty of of these open-ended style react",
    "start": "309800",
    "end": "315840"
  },
  {
    "text": "agents and these have a lot of Promise these kind of flexible tool calling agents were really hyped last year",
    "start": "315840",
    "end": "322120"
  },
  {
    "text": "they're still really hyped it's really exciting because they're flexible they're open-ended you can give them a task give them some tools and can just",
    "start": "322120",
    "end": "328319"
  },
  {
    "text": "execute arbitrary control flows given those tools to solve open-ended problems the catches and this is kind of",
    "start": "328319",
    "end": "335280"
  },
  {
    "text": "the Crux of what we're getting to with this Workshop is they do have poor reliability or they can so you can get",
    "start": "335280",
    "end": "341080"
  },
  {
    "text": "caught if you've played with agents some as you can see they kind of get caught on one step and they keep calling the same",
    "start": "341080",
    "end": "346360"
  },
  {
    "text": "tool and you know really this is often caused by llm non-determinism LMS are not deterministic and also errors in",
    "start": "346360",
    "end": "353800"
  },
  {
    "text": "tool calling so tool calling is kind of tricky if you think about it LM has to",
    "start": "353800",
    "end": "359039"
  },
  {
    "text": "basically pick the right tool given the input and it has to pick the right payload so it has to produce the right inputs need to run the tool and these",
    "start": "359039",
    "end": "365800"
  },
  {
    "text": "both can break so here's a good example the tool I'm passing is step two and the",
    "start": "365800",
    "end": "371319"
  },
  {
    "text": "llm is saying the tool name to run is step three so that's obviously wrong or I'm passing what is step two of the",
    "start": "371319",
    "end": "378160"
  },
  {
    "text": "input three and the LM says okay pass four so these both these errors can happen tool calling is a tricky thing",
    "start": "378160",
    "end": "385440"
  },
  {
    "text": "and it's exacerbated if you pass an llm five Tools 10 tools it actually gets worse if you have very long dialogues it",
    "start": "385440",
    "end": "391280"
  },
  {
    "text": "gets worse and so this idea of open-ended tool calling agents is really promising it's really exciting but it's",
    "start": "391280",
    "end": "397240"
  },
  {
    "text": "really challenging because of these issues that we were mentioning here so this is kind of the buildup here like so",
    "start": "397240",
    "end": "403599"
  },
  {
    "text": "can we envision something in the middle so again we talked about chains they are not flexible but they're very reliable",
    "start": "403599",
    "end": "409199"
  },
  {
    "text": "this chain will always run step one two in order we talked about like react agents On The Other Extreme they're",
    "start": "409199",
    "end": "414960"
  },
  {
    "text": "extremely flexible they can run any sequence of tool calls you know it can run step one two step one only two only",
    "start": "414960",
    "end": "420759"
  },
  {
    "text": "two one but they do have reliability issues so can we imagine something in the middle that's both flexible and",
    "start": "420759",
    "end": "426919"
  },
  {
    "text": "reliable so here's kind of the setup and this like kind of the intuition like a lot of times in many applications you",
    "start": "426919",
    "end": "432240"
  },
  {
    "text": "have some idea of what you want the thing to do um every time so some parts of the application may be fixed like the",
    "start": "432240",
    "end": "438919"
  },
  {
    "text": "developer can set okay I always want to run step one and I want to end with step two and you can inject an llm in certain",
    "start": "438919",
    "end": "445840"
  },
  {
    "text": "places that you want there to be some kind of branching or kind of option it and the control flow okay so this is the",
    "start": "445840",
    "end": "453759"
  },
  {
    "text": "motivation for what we call Lang graph so Lang graph is basically a library from the Lang chain team that can be",
    "start": "453759",
    "end": "459720"
  },
  {
    "text": "used to express control flows as graphs and it is a very general tool and I put",
    "start": "459720",
    "end": "465639"
  },
  {
    "text": "out a bunch of videos on it and we're going to use it today and by the end of this uh you will all have an agent that",
    "start": "465639",
    "end": "471440"
  },
  {
    "text": "runs reliably using L graph hopefully and and we'll we'll we'll see so you could you should test me on that if it",
    "start": "471440",
    "end": "477560"
  },
  {
    "text": "if if things don't work for you then then we'll work it out but the idea is kind of this this graph has some set of",
    "start": "477560",
    "end": "485599"
  },
  {
    "text": "nodes and edges so nodes you can think about are basically well maybe I should start with this this graph has something",
    "start": "485599",
    "end": "492159"
  },
  {
    "text": "called state so it's like short-term memory that lives across the lifetime of this graph that contains things you want",
    "start": "492159",
    "end": "497199"
  },
  {
    "text": "to operate on nodes modify the state in some way so basically each node can like",
    "start": "497199",
    "end": "502560"
  },
  {
    "text": "call a tool and can modify the state edges just make decisions about what node to go to next okay so you basically",
    "start": "502560",
    "end": "508800"
  },
  {
    "text": "have this idea of memory and this is the same as common agents right me agents are characterized by having tool calling",
    "start": "508800",
    "end": "514760"
  },
  {
    "text": "um and and short-term memory as well as planning those same things are present in L graph memory is the state that",
    "start": "514760",
    "end": "520560"
  },
  {
    "text": "lives across your graph tools exist within your nodes and planning uh",
    "start": "520560",
    "end": "525680"
  },
  {
    "text": "basically you can incorporate llm dictated decisionmaking in the edges of your graph so like why is this interesting",
    "start": "525680",
    "end": "533360"
  },
  {
    "text": "and where's this been cropping up we've actually been seeing this theme crop up a lot of places um so there's a really interesting paper there's actually a few",
    "start": "533360",
    "end": "539600"
  },
  {
    "text": "I really like this one's called corrective Rag and the idea is pretty simple like with a naive rag pipeline",
    "start": "539600",
    "end": "545560"
  },
  {
    "text": "you're doing a retrieval you're taking retrieve docks and you're generating your answer corrective rag like is doing",
    "start": "545560",
    "end": "550880"
  },
  {
    "text": "one step more where it's saying well why don't we reflect on the docs we retrieved and ask are they actually relevant you can have lots of issues",
    "start": "550880",
    "end": "556200"
  },
  {
    "text": "with retrieval you can reflect on the documents see if they're relevant if they're not relevant you can do different things you can kick out and do",
    "start": "556200",
    "end": "561880"
  },
  {
    "text": "web search so make sure application a lot more Dynamic to poor quality retrieval so this is one of the First",
    "start": "561880",
    "end": "567480"
  },
  {
    "text": "videos I put out on Lang graph back in February was was very popular and I basically showed you can build corrective rag inside L graph and it's",
    "start": "567480",
    "end": "573920"
  },
  {
    "text": "super simple this is what the graph looks like I do retrieval I grade my documents and we're going to actually",
    "start": "573920",
    "end": "579480"
  },
  {
    "text": "we're going to do this today and I have a bunch of code for you that does exactly this so we're going to go way in detail on this one um but this is kind",
    "start": "579480",
    "end": "587120"
  },
  {
    "text": "of the setup and I showed this working I showed it works locally with AMA using in that at that time it was um mrol 7B",
    "start": "587120",
    "end": "595079"
  },
  {
    "text": "um and it works really well so this is like one simple illustration of how you can use Lang graph to build kind of a self-reflective or corrective rag",
    "start": "595079",
    "end": "602200"
  },
  {
    "text": "application now another cool paper was called selfrag which actually looked at the an the generation so basically we're",
    "start": "602200",
    "end": "608760"
  },
  {
    "text": "all familiar with the idea of hallucinations it's a real problem um instead of just allowing hallucinations to propagate to the user you can",
    "start": "608760",
    "end": "615000"
  },
  {
    "text": "actually reflect on the answer relative to the documents and catch hallucinations if there's hallucinations",
    "start": "615000",
    "end": "620680"
  },
  {
    "text": "you can like basically do different things and they they propose a few ideas here um I implemented this and this is a",
    "start": "620680",
    "end": "626800"
  },
  {
    "text": "this is actually our most popular video of all time so this was showing Lang graph and llama 3 um implementing three",
    "start": "626800",
    "end": "632760"
  },
  {
    "text": "different things corrective rag which we just talked about the self- rag thing of hallucination checking and this adaptive",
    "start": "632760",
    "end": "638040"
  },
  {
    "text": "rag thing so I can kind of walk through it this all runs in Lang graph locally uh and I have the notebook here if you",
    "start": "638040",
    "end": "644200"
  },
  {
    "text": "want to test that today you definitely could um so that's the point that's reliable enough to run this whole thing locally so what's happening here is I",
    "start": "644200",
    "end": "650920"
  },
  {
    "text": "take a question I route it either to my index or to web search um I then retrieve documents I grade them for",
    "start": "650920",
    "end": "657480"
  },
  {
    "text": "relevance if any are not relevant I kick out and do a web search to supplement my retrieval if they're relevant I generate",
    "start": "657480",
    "end": "663720"
  },
  {
    "text": "my answer I check it for hallucinations and then I finally check it for answer relevance so basically does it have",
    "start": "663720",
    "end": "669920"
  },
  {
    "text": "hallucinations relative to my documents and does it answer my question if all that passes I go I finish and return",
    "start": "669920",
    "end": "675560"
  },
  {
    "text": "that to the user so this is kind of like a complex rag flow but with Lang graph",
    "start": "675560",
    "end": "680959"
  },
  {
    "text": "you can actually run this on your laptop um it is reliable enough to run a laptop with Lang graph and the intuition again",
    "start": "680959",
    "end": "686519"
  },
  {
    "text": "is that you're constraining the control flow you're LM to make certain decisions but at very discret points if you",
    "start": "686519",
    "end": "692560"
  },
  {
    "text": "implement this as a rag agent this could be very open-ended and a lot of opportunities for breakage and so that's",
    "start": "692560",
    "end": "697839"
  },
  {
    "text": "the real intuition here um now a final theme is karpathy kind of mentioned this",
    "start": "697839",
    "end": "703399"
  },
  {
    "text": "idea of flow engineering related to this Alpha codium paper a really nice paper on code generation and the intuition",
    "start": "703399",
    "end": "709240"
  },
  {
    "text": "here is um produce a code solution they tested this on a bunch of coding challenge produce a code solution and",
    "start": "709240",
    "end": "715000"
  },
  {
    "text": "check it against the number of unit tests autogenerated or pre-existing and basically if it fails unit test feedback",
    "start": "715000",
    "end": "721720"
  },
  {
    "text": "those errors to Elm and try again really simple idea uh I implemented this in L graph again the code is here um and this",
    "start": "721720",
    "end": "728680"
  },
  {
    "text": "works really well so I basically I share a blog post as well I ran this on our internal uh coding we have an internal",
    "start": "728680",
    "end": "736600"
  },
  {
    "text": "application for rag at Lang chain and we're actually working on implementing this right now in production uh because",
    "start": "736600",
    "end": "742720"
  },
  {
    "text": "the performance is way better and a common thing this can fix with code generation and code Solutions is",
    "start": "742720",
    "end": "748440"
  },
  {
    "text": "hallucinations import so we see that a lot with our rag app um so what I did was I very simply implemented a unit",
    "start": "748440",
    "end": "755480"
  },
  {
    "text": "test for import checks just run that it significantly improves performance um",
    "start": "755480",
    "end": "761160"
  },
  {
    "text": "relative to without doing it and so we're actually working on implementing this in our internal rag system so super",
    "start": "761160",
    "end": "766320"
  },
  {
    "text": "simple idea that can really improve code generation so you know if if I kind of",
    "start": "766320",
    "end": "771680"
  },
  {
    "text": "like back up what what did we talk about I we talked about chains they are not flexible which is fine in some cases but",
    "start": "771680",
    "end": "778880"
  },
  {
    "text": "a lot of interesting newer papers with rag for example this idea of self-reflection is really beneficial uh",
    "start": "778880",
    "end": "784440"
  },
  {
    "text": "the ability to kind of self-correct applications can be really beneficial not Beyond rag as well for",
    "start": "784440",
    "end": "789880"
  },
  {
    "text": "coding um so chains are very reliable but they're not flexible now if you go to the other end like a classic react",
    "start": "789880",
    "end": "795880"
  },
  {
    "text": "agent is very flexible it can Implement any sequence control flows through your different tools uh but it does have",
    "start": "795880",
    "end": "801760"
  },
  {
    "text": "reliability problems due to things we talked about non determinism tool calling errors and Lang graph kind of",
    "start": "801760",
    "end": "806800"
  },
  {
    "text": "sits in the Middle where you can actually Implement these userdefined SL llm gated control flows um and they",
    "start": "806800",
    "end": "815440"
  },
  {
    "text": "can actually be extremely reliable because of that constraint um they are less flexible than a classic react agent",
    "start": "815440",
    "end": "822040"
  },
  {
    "text": "so that is true so for very open-ended task I do agree maybe you do need you know a very open-ended more like",
    "start": "822040",
    "end": "828000"
  },
  {
    "text": "autonomous style react agent but for a lot of applications that our customers are are like are working on and seeing",
    "start": "828000",
    "end": "834279"
  },
  {
    "text": "um these kinds of like hybrid flows are sufficient and you gain a lot of",
    "start": "834279",
    "end": "839399"
  },
  {
    "text": "reliability and so we've talked to a lot of companies that have impl land graph successfully um for agentic flows for",
    "start": "839399",
    "end": "844959"
  },
  {
    "text": "this reason because reliability is just incredibly important in a production setting um so this gets into if you look",
    "start": "844959",
    "end": "852320"
  },
  {
    "text": "at the slides I have a few different notebooks um and what I show is we",
    "start": "852320",
    "end": "858360"
  },
  {
    "text": "talked about corrective rag these notebooks show how to build corrective rag yourself and I thought that's a fun starting application it's a really",
    "start": "858360",
    "end": "865040"
  },
  {
    "text": "popular one um it's super simple there are not many dependencies um you can use your own whatever tool you want to use",
    "start": "865040",
    "end": "871399"
  },
  {
    "text": "your web search you can use other things as well you have a look at the notebooks and I'll kind of walk around um and",
    "start": "871399",
    "end": "876959"
  },
  {
    "text": "we're going to we're going to keep going I'm just want to like this is just like a placeholder here um but so if you want",
    "start": "876959",
    "end": "883320"
  },
  {
    "text": "to test this locally if you have a laptop capable of running things locally then we have a notebook to support that I use a Lama I can talk a lot about that",
    "start": "883320",
    "end": "890800"
  },
  {
    "text": "that's a really cool thing um if you don't then I have two options for you so",
    "start": "890800",
    "end": "896680"
  },
  {
    "text": "one is a collab so that's probably the easiest um if there's issues let me know",
    "start": "896680",
    "end": "902160"
  },
  {
    "text": "I've tested it um so if you have a Google account you're can to spin up a collab all you need is a few API Keys",
    "start": "902160",
    "end": "907800"
  },
  {
    "text": "depending on what models you want to use it's all kind of there you can set those accordingly and I also have a notebook",
    "start": "907800",
    "end": "913880"
  },
  {
    "text": "so this just kind of is like a kind of a gives you a road map of the different things you can try today since this is a",
    "start": "913880",
    "end": "920320"
  },
  {
    "text": "workshop format and I'll I'll just be walking around and we we'll do questions for a while but I want to talk about the",
    "start": "920320",
    "end": "925920"
  },
  {
    "text": "second half of this of this you know story um so one of the things we're seeing a",
    "start": "925920",
    "end": "932720"
  },
  {
    "text": "lot I think you're going to hear a lot at this conference is the challenge of testing and evaluation and this is a real pain Point like for example how do",
    "start": "932720",
    "end": "940199"
  },
  {
    "text": "I actually know that my Lang graph agent is more reliable than the react agent um",
    "start": "940199",
    "end": "946720"
  },
  {
    "text": "how do I know what llm to use how do I know what prompt to use right so testing agents is is testing in general is",
    "start": "946720",
    "end": "953000"
  },
  {
    "text": "really hard and agents in particular is challenging so there's kind of three types of testing Loops I like to think",
    "start": "953000",
    "end": "959759"
  },
  {
    "text": "about uh one is this inapp error correction and that's actually what we just talked about so Lang graph and Lang",
    "start": "959759",
    "end": "965759"
  },
  {
    "text": "graph agents are really good for that so basically inapp error handling uh where you can catch and fix errors is really",
    "start": "965759",
    "end": "972040"
  },
  {
    "text": "useful for code generation for rag we just talked about that so that's like placeholder one now we get into this idea of pre-production testing um and",
    "start": "972040",
    "end": "979279"
  },
  {
    "text": "then finally production monitoring and I want to introduce a few ideas on the latter too um so we just talk through",
    "start": "979279",
    "end": "985360"
  },
  {
    "text": "this here we're going to build corrective rag a few different ways and I just showed the choose your in adventure stuff and so this is just kind",
    "start": "985360",
    "end": "991959"
  },
  {
    "text": "of reiterating that um but I want to show you some other things so Lang Smith",
    "start": "991959",
    "end": "998839"
  },
  {
    "text": "is a tool from the Lang chain team that supports testing and evaluation as well as monitoring and tracing and so we've",
    "start": "998839",
    "end": "1004440"
  },
  {
    "text": "seen a lot of interest in this and it's it's quite popular um it is really useful for doing these types of testings",
    "start": "1004440",
    "end": "1010440"
  },
  {
    "text": "and evaluations so the key idea behind langing swith and the notebooks actually have this so this is totally optional if",
    "start": "1010440",
    "end": "1017759"
  },
  {
    "text": "you just want to build an agent that's totally fine if you want to also test it you can you can use Langs Smith you",
    "start": "1017759",
    "end": "1023240"
  },
  {
    "text": "don't have to of course I have it all set up to use Langs Smith if you want it's free to use of course um and so the",
    "start": "1023240",
    "end": "1029240"
  },
  {
    "text": "idea is there's kind of four components that I like to think about when it comes to testing evaluation you have some data",
    "start": "1029240",
    "end": "1035199"
  },
  {
    "text": "set that's some set of examples you want to test on so say you a rag app that's like a set of ground truth question answer pairs you've built like you're",
    "start": "1035199",
    "end": "1041360"
  },
  {
    "text": "testing your system you have question answer pairs that you know are correct can your system produce those answers",
    "start": "1041360",
    "end": "1046438"
  },
  {
    "text": "you know how many will actually get right you have application that's your agent that's your rag app that's your code app whatever that is that's your",
    "start": "1046439",
    "end": "1053760"
  },
  {
    "text": "application now the thing that's often the trickest you have this evaluator thing and I'm I and the notebook show",
    "start": "1053760",
    "end": "1058919"
  },
  {
    "text": "you in detail but this evaluator is something as simple as a userdefined function that can Implement a few",
    "start": "1058919",
    "end": "1064080"
  },
  {
    "text": "different things you can think about using an llm to actually judge your output so in that case let's take rag as",
    "start": "1064080",
    "end": "1069480"
  },
  {
    "text": "an example my application produces an answer I have a ground truth answer you can actually have an llm look at those",
    "start": "1069480",
    "end": "1075039"
  },
  {
    "text": "two answers jointly and reason is it correct and this is often very effective it requires some prompt engineering I",
    "start": "1075039",
    "end": "1080559"
  },
  {
    "text": "have some nice templates in the notebooks to show you but this something that's very pop this idea of llm as judge evaluators is very interesting a",
    "start": "1080559",
    "end": "1087280"
  },
  {
    "text": "lot of people actually you'll probably hear about this week uh it's a really good theme it's still kind of in",
    "start": "1087280",
    "end": "1092520"
  },
  {
    "text": "development but that's like one placeholder to keep in mind so for one option for time testing is this idea of",
    "start": "1092520",
    "end": "1098000"
  },
  {
    "text": "using llms themselves the other is building your own heris evaluator so a custom",
    "start": "1098000",
    "end": "1103840"
  },
  {
    "text": "evaluator of some sort and actually the notebooks that I share have both uh",
    "start": "1103840",
    "end": "1109440"
  },
  {
    "text": "um and so we're actually going to the notebooks actually show how to evaluate an agent specifically and there's a few",
    "start": "1109440",
    "end": "1116120"
  },
  {
    "text": "different things you can look at with an agent so one is you go to that far right in blue the endend performance so our",
    "start": "1116120",
    "end": "1121960"
  },
  {
    "text": "notebooks are basically going to be a rag agent the eval set has five questions um and I basically have a set",
    "start": "1121960",
    "end": "1127840"
  },
  {
    "text": "of question answer pairs so basically I'm going to compare my agent answers to reference answers and we'll walk through",
    "start": "1127840",
    "end": "1134280"
  },
  {
    "text": "that in the notebook but that's kind of one thing I just want to introduce the ideas so one big idea is you can you can",
    "start": "1134280",
    "end": "1139880"
  },
  {
    "text": "evaluate the endend performance of your agent right you don't care anything about's happening inside the endtoend performance the other which I actually",
    "start": "1139880",
    "end": "1145960"
  },
  {
    "text": "like to look at a lot is this thing on top what are actually the tool calls that the agent executed this is how you",
    "start": "1145960",
    "end": "1151240"
  },
  {
    "text": "can actually test the agent's reasoning so what you see often with agents is they can make some weird trajectory of",
    "start": "1151240",
    "end": "1156600"
  },
  {
    "text": "tool calls it's highly inefficient but still get to the right answer you don't get that if you only look at the answer say okay it's got the right answer but",
    "start": "1156600",
    "end": "1162320"
  },
  {
    "text": "if you look at the trajectory it's some crazy path and so you want to actually look at both like how efficient how correct is the trajectory and does it",
    "start": "1162320",
    "end": "1168240"
  },
  {
    "text": "get the right answer answer right and so the notebooks I share actually do both um now this is uh actually an",
    "start": "1168240",
    "end": "1176000"
  },
  {
    "text": "evaluation that I ran and this data set is public this is on the agents that we actually just talked about so this is",
    "start": "1176000",
    "end": "1182320"
  },
  {
    "text": "kind of what you see when you open Langs Smith so these are your different experiment names um this is just saying",
    "start": "1182320",
    "end": "1187440"
  },
  {
    "text": "like I've run three rep replicates of each experiment and these are my aggregate scores so this first score is",
    "start": "1187440",
    "end": "1193559"
  },
  {
    "text": "basically the answer correctness thing and the second score is like the tool use trajectory like does it do use the",
    "start": "1193559",
    "end": "1200080"
  },
  {
    "text": "right reasoning trace and I can go through my experiments so this top one is actually this is kind of cool this is",
    "start": "1200080",
    "end": "1206320"
  },
  {
    "text": "actually my local agent running on my laptop with Lang graph okay is a five question EV set small eval set just a",
    "start": "1206320",
    "end": "1213080"
  },
  {
    "text": "bunch of it's some very small test examples but basically my local agent does not does fine it does 60% in terms",
    "start": "1213080",
    "end": "1219919"
  },
  {
    "text": "of the ultimate answer so that's not amazing uh but it does do very well in terms of the tool calling trajectory so",
    "start": "1219919",
    "end": "1225840"
  },
  {
    "text": "it's very reliable in terms of reasoning um it's an 8 billion parameter model so I basically I think the quality of its",
    "start": "1225840",
    "end": "1232080"
  },
  {
    "text": "outputs are a little bit lower than you might see with larger models um now fire",
    "start": "1232080",
    "end": "1237280"
  },
  {
    "text": "function V2 is another option it's basically a fine-tuned llama 70b from fireworks this one with Lang graph so",
    "start": "1237280",
    "end": "1245000"
  },
  {
    "text": "this is actually showing this top actually gets up to 80% so very strong performance in terms of answers and 100%",
    "start": "1245000",
    "end": "1250840"
  },
  {
    "text": "again in terms of tool calling so the key observation here is the tool calling or reasoning is consistent whether",
    "start": "1250840",
    "end": "1256400"
  },
  {
    "text": "you're using a local model or 70 billian parameter model with Lang graph so you get very high consistency in your tool",
    "start": "1256400",
    "end": "1262080"
  },
  {
    "text": "calling the answer quality degrades that's more an llm capacity problem but the reasoning of the agent is consistent",
    "start": "1262080",
    "end": "1269559"
  },
  {
    "text": "so that's the key point now here's where it gets interesting fire function V2 again that's law 70b this is with uh a",
    "start": "1269559",
    "end": "1277559"
  },
  {
    "text": "react agent what you can see here is the answer quality is degraded but here's the interesting thing the tool calling",
    "start": "1277559",
    "end": "1283600"
  },
  {
    "text": "trajectories are really bad and this again gets back to that problem with react agents they're open-ended they can",
    "start": "1283600",
    "end": "1288799"
  },
  {
    "text": "choose arbitrary sequences of tool calls and you can deviate really quickly from your expected trajectory so that's the",
    "start": "1288799",
    "end": "1294120"
  },
  {
    "text": "key intuition here now the final two are GPD 40 That's obviously a very Flagship",
    "start": "1294120",
    "end": "1300120"
  },
  {
    "text": "model it's maybe number two now Rel against soned on the chapot arena at least you know again answers ultimately",
    "start": "1300120",
    "end": "1306279"
  },
  {
    "text": "are are strong the tool calling though even here is degraded so basically it follows some weird trajectories to get",
    "start": "1306279",
    "end": "1312400"
  },
  {
    "text": "to its answers that are unexpected so what's the high level Point here the high level point is Lang graph allows",
    "start": "1312400",
    "end": "1318919"
  },
  {
    "text": "you to significantly constrain the control flow of your app and get higher liability if you look at these tool calling scores it's very kind of",
    "start": "1318919",
    "end": "1325120"
  },
  {
    "text": "consistent going all the way down to local models it follows the same sequence every time react agents kind of",
    "start": "1325120",
    "end": "1331760"
  },
  {
    "text": "go off the rails much more easily the answer performance is really a function of the model capacity so using an 8",
    "start": "1331760",
    "end": "1337640"
  },
  {
    "text": "billion parameter model locally the answer quality is lower than a 70 billion that's to be expected but the",
    "start": "1337640",
    "end": "1343480"
  },
  {
    "text": "reasoning of my app is consistent and strong so that's the key thing that you kind of get with Lang graph and this is",
    "start": "1343480",
    "end": "1349039"
  },
  {
    "text": "all public and hopefully some of you will actually you know implement this or or reproduce this today um and this is",
    "start": "1349039",
    "end": "1355559"
  },
  {
    "text": "just walking through those same insights I just mentioned um and then deployment we're going to be talking later this",
    "start": "1355559",
    "end": "1361679"
  },
  {
    "text": "week we have an announcement related to deployment of Lang graph so this is actually a very good setup um if you're",
    "start": "1361679",
    "end": "1366720"
  },
  {
    "text": "playing with Lang graph you enjoy working with it we're going to have some really nice options for deploying later this week and so Harrison will be here",
    "start": "1366720",
    "end": "1373000"
  },
  {
    "text": "on Thursday to give a keynote on that one um and if you've deployed we also have some really nice Tools in but to",
    "start": "1373000",
    "end": "1379000"
  },
  {
    "text": "actually monitor your deployment and this is not as relevant for this Workshop it's something just be aware of",
    "start": "1379000",
    "end": "1384120"
  },
  {
    "text": "um I can talk about if you're interested um so maybe to to close out so this is a",
    "start": "1384120",
    "end": "1389720"
  },
  {
    "text": "really nice write up um these guys are actually going to give a keynote later this week it's uh Jason and Company um",
    "start": "1389720",
    "end": "1395679"
  },
  {
    "text": "haml and others and they kind of made a really nice point that the model is not the mode like LMS are always changing",
    "start": "1395679",
    "end": "1403159"
  },
  {
    "text": "the moat is really the systems you build around your application that's what we talked about today like do you have an orchestration framework for example like",
    "start": "1403159",
    "end": "1409440"
  },
  {
    "text": "Lang graph do you have an evaluation chassis like Langs Smith and again you don't have to use Lang graph you don't have to use Langs Smith for these things",
    "start": "1409440",
    "end": "1415799"
  },
  {
    "text": "but this Workshop will introduce these ideas to you and frankly I think it's important just understand the ideas",
    "start": "1415799",
    "end": "1421360"
  },
  {
    "text": "rather than the implementation whether or not you use Lang graph whether or not you use lsmith I think understanding these these principles is still helpful",
    "start": "1421360",
    "end": "1428799"
  },
  {
    "text": "um but you know an evaluation chassis guard rails data fly well these are like the components that that give you the",
    "start": "1428799",
    "end": "1434679"
  },
  {
    "text": "ability to improve your app over time that's the that's really the big idea that's the the goal um and I think",
    "start": "1434679",
    "end": "1440120"
  },
  {
    "text": "you'll hear more on that later this week the the goal here is how are you measuring Improvement of your app and ensuring it always gets better that's",
    "start": "1440120",
    "end": "1445720"
  },
  {
    "text": "what we're actually trying to achieve here um and that's kind of what evaluation is giving you and yeah this",
    "start": "1445720",
    "end": "1451640"
  },
  {
    "text": "this is kind of my last slide then maybe we can just move into maybe some Q&A I",
    "start": "1451640",
    "end": "1456840"
  },
  {
    "text": "can actually show the notebooks themselves if you want to walk through them together um I mean I'll just do",
    "start": "1456840",
    "end": "1463320"
  },
  {
    "text": "that I'll let you guys kind of hack on them in parallel as I walk through them um",
    "start": "1463320",
    "end": "1468919"
  },
  {
    "text": "um and then I can just go walk around and talk to people something like that so you know the three types of feedback",
    "start": "1468919",
    "end": "1474840"
  },
  {
    "text": "loops you have this design phase feedback something like L graph inapp error handling that's kind of step one",
    "start": "1474840",
    "end": "1480240"
  },
  {
    "text": "we talked about cool examples there for coding for rag a lot of nice papers",
    "start": "1480240",
    "end": "1485840"
  },
  {
    "text": "really promising I'm very excited about anything you can do here with terms of kind of agentic self-correction",
    "start": "1485840",
    "end": "1491200"
  },
  {
    "text": "self-reflection in your app itself pre-production testing we just talked through that building evaluation sets",
    "start": "1491200",
    "end": "1497279"
  },
  {
    "text": "running evaluations testing for an agent like your tool use trajectory your answer quality all",
    "start": "1497279",
    "end": "1502880"
  },
  {
    "text": "really interesting and important and then in production phase production monitoring um this gets into we didn't",
    "start": "1502880",
    "end": "1509080"
  },
  {
    "text": "talk about it too much but basically this stuff so basically you can have evaluators running on your app in production looking at inputs looking at",
    "start": "1509080",
    "end": "1515480"
  },
  {
    "text": "outputs tagging them accordingly and they can go back and look um later so",
    "start": "1515480",
    "end": "1520919"
  },
  {
    "text": "that's kind of the the setup here I know that's probably a lot and we went about half an hour so if there's any questions",
    "start": "1520919",
    "end": "1526679"
  },
  {
    "text": "I can just open it up um um and we can kind of talk through stuff I can also start ripping through some of the",
    "start": "1526679",
    "end": "1532360"
  },
  {
    "text": "notebooks just to kind of give you an overview of the code itself uh but if there's any questions here maybe um you",
    "start": "1532360",
    "end": "1539520"
  },
  {
    "text": "know happy to take a few give give you a minute to digest all that",
    "start": "1539520",
    "end": "1546000"
  },
  {
    "text": "yeah uh that's a good point let's see um uh is there a non- Twitter link to",
    "start": "1546799",
    "end": "1554279"
  },
  {
    "text": "the slides uh let's see if the conference organized just give me some I",
    "start": "1554279",
    "end": "1560039"
  },
  {
    "text": "don't know if I have an email list for everyone in [Music] here is there a okay is there a slack",
    "start": "1560039",
    "end": "1566440"
  },
  {
    "text": "yeah if someone can I actually didn't know there's a slack so that's very helpful yeah if there's a slack or an app for this conference then please",
    "start": "1566440",
    "end": "1572679"
  },
  {
    "text": "someone post I appreciate that I actually didn't know that thank you for that",
    "start": "1572679",
    "end": "1578480"
  },
  {
    "text": "question yes sir so about testing and",
    "start": "1578480",
    "end": "1583840"
  },
  {
    "text": "evaluation does it really scale to uh the exact sequence of the agent it's",
    "start": "1583840",
    "end": "1592679"
  },
  {
    "text": "smart enough dealing with a complex problem",
    "start": "1592679",
    "end": "1598960"
  },
  {
    "text": "hard okay this is a very good point so I'm going to repeat the question now um",
    "start": "1600240",
    "end": "1606279"
  },
  {
    "text": "how do you evaluate an agent's like reasoning trajectory if it's a large and",
    "start": "1606279",
    "end": "1611399"
  },
  {
    "text": "open-ended problem that can be solved in many different ways in that particular case you are right it is hard to to",
    "start": "1611399",
    "end": "1618320"
  },
  {
    "text": "enumerate a specific trajectory uh that is actually reasonable for really",
    "start": "1618320",
    "end": "1623799"
  },
  {
    "text": "open-ended long uh running type problems trajectory evaluation may not be appropriate so one one thing I would",
    "start": "1623799",
    "end": "1631159"
  },
  {
    "text": "think about is can you define a few canonical trajectories of tool use through your application",
    "start": "1631159",
    "end": "1637279"
  },
  {
    "text": "so it depends on the St if it's a very long running agent I think it's probably infeasible if it's a shorter run agent",
    "start": "1637279",
    "end": "1643399"
  },
  {
    "text": "where it's like you expect something in the order of maybe 5 to 10 steps you can probably enumerate some set of",
    "start": "1643399",
    "end": "1649000"
  },
  {
    "text": "reasonable trajectories and basically check does it follow any of these you can also do things so you can do things",
    "start": "1649000",
    "end": "1655520"
  },
  {
    "text": "like this you can do things like check for the re repeat of certain tool calls",
    "start": "1655520",
    "end": "1662200"
  },
  {
    "text": "you can be very flexible at this Ian it's kind of open fuse you can look for like is it repeating certain tool calls",
    "start": "1662200",
    "end": "1667799"
  },
  {
    "text": "um you can look at recall like is it for sure calling this tool or not um so you",
    "start": "1667799",
    "end": "1673240"
  },
  {
    "text": "can actually be very flexible actually the way we set up Lang Lang Smith evaluator for this it's just a simple function that you can Define yourself so",
    "start": "1673240",
    "end": "1680480"
  },
  {
    "text": "it's a very good point you can be incred you can be arbitrarily creative about how you evaluate that but I would say for very long running you're right you",
    "start": "1680480",
    "end": "1687159"
  },
  {
    "text": "can't really articulate step one two and three um but I would then think about more like evaluating is it repeating",
    "start": "1687159",
    "end": "1694159"
  },
  {
    "text": "steps can you evaluate for clearly aberant behaviors excessive number of tool use repeats excessive number of",
    "start": "1694159",
    "end": "1701399"
  },
  {
    "text": "overall tool call so kind of like guard rails related to like kind of clear aberant Behavior it's very shortterm you",
    "start": "1701399",
    "end": "1706799"
  },
  {
    "text": "can actually enumerate the trajectory specifically but a good question so in the code you can see we actually lay out",
    "start": "1706799",
    "end": "1712919"
  },
  {
    "text": "a custom function you can Define that yourself so that's a that's a very good point though yeah",
    "start": "1712919",
    "end": "1720679"
  },
  {
    "text": "yep yeah human the loop is a good one the workshop uh notebooks I share do not",
    "start": "1724559",
    "end": "1730360"
  },
  {
    "text": "have that but langra does have some good support for human in the loop and I can share with you some notebooks that",
    "start": "1730360",
    "end": "1737120"
  },
  {
    "text": "showcase that also what we have uh shipping on Thursday has very good support for human",
    "start": "1737120",
    "end": "1743080"
  },
  {
    "text": "in Loop so um I will share some notebooks with you for that and wait for",
    "start": "1743080",
    "end": "1748480"
  },
  {
    "text": "Thursday for even more there building a",
    "start": "1748480",
    "end": "1756799"
  },
  {
    "text": "fig is Right",
    "start": "1768399",
    "end": "1771519"
  },
  {
    "text": "Way yeah so the question was this is actually a really good one um for rag",
    "start": "1777080",
    "end": "1783960"
  },
  {
    "text": "I'm just going to go to our docs because I actually wrote a doc on this recently so um you can still see my slides for",
    "start": "1783960",
    "end": "1791960"
  },
  {
    "text": "rag in a pre-production setting it's easy to define or not even easy but you can define a set of question answer",
    "start": "1791960",
    "end": "1798200"
  },
  {
    "text": "Pairs and evaluate them when you're in production though how do you evaluate your app because you don't have a ground truth answer so what are other things",
    "start": "1798200",
    "end": "1804159"
  },
  {
    "text": "you can actually evaluate for rag app that don't require a reference yeah so",
    "start": "1804159",
    "end": "1809360"
  },
  {
    "text": "there is a a conceptual guy that I will share so this is actually our rag section I have kind of a nice overview",
    "start": "1809360",
    "end": "1815440"
  },
  {
    "text": "of this um there's actually a few different things you can evaluate for rag that don't require reference um that",
    "start": "1815440",
    "end": "1822960"
  },
  {
    "text": "are very useful yeah so it's this right here",
    "start": "1822960",
    "end": "1829960"
  },
  {
    "text": "so this is like a typical rag flow so I have a question I retrieve",
    "start": "1829960",
    "end": "1835840"
  },
  {
    "text": "documents I pass them to an llm I get an answer right what we just talked about and we showed is comparing your answer",
    "start": "1835840",
    "end": "1842480"
  },
  {
    "text": "to some reference answer now this is to be honest pretty hard to do you have to build an eval set of question answer",
    "start": "1842480",
    "end": "1848480"
  },
  {
    "text": "pairs very important to do but it's not easy so what else can you do so some we've seen that are really easy and",
    "start": "1848480",
    "end": "1855200"
  },
  {
    "text": "actually pretty popular there's three different types ofp grading you can do that don't require a reference that are like internal checks you can run I mean",
    "start": "1855200",
    "end": "1862480"
  },
  {
    "text": "you can run them online so one is retrieval grading so basically looking",
    "start": "1862480",
    "end": "1867639"
  },
  {
    "text": "at the docu your retriev documents relative to your question so like an internal self consistency check so this",
    "start": "1867639",
    "end": "1873159"
  },
  {
    "text": "is actually a great check to run and actually the corrective rag paper that or the corrective rag thing that is in",
    "start": "1873159",
    "end": "1878279"
  },
  {
    "text": "the cookbooks that I share here does this so you can play with the prompt and all that um but basically this is just",
    "start": "1878279",
    "end": "1885639"
  },
  {
    "text": "checking the consistency your retriev do relative to your question you can do that and we have some really good",
    "start": "1885639",
    "end": "1891200"
  },
  {
    "text": "prompts for that another one I like is just comparing your answer to your question have an llm look at here's my",
    "start": "1891200",
    "end": "1897200"
  },
  {
    "text": "answer here's the question is this sayane are they related and this is a really nice check uh just for like you",
    "start": "1897200",
    "end": "1902799"
  },
  {
    "text": "know of course you don't have a reference answer but like you can still sanity check and say does this deviate significantly from what the questioner",
    "start": "1902799",
    "end": "1908600"
  },
  {
    "text": "is asking the other this is a great one is hallucination and this is this is super intuitive compare your answer to",
    "start": "1908600",
    "end": "1914799"
  },
  {
    "text": "the retrieved documents so if the llm went off the rails and didn't ground the answer properly and you hallucinated you",
    "start": "1914799",
    "end": "1920159"
  },
  {
    "text": "can catch that really easily um and so um I need to get on this slack because I",
    "start": "1920159",
    "end": "1927440"
  },
  {
    "text": "want to share this link with you I'll figure that out I I'll find you but uh",
    "start": "1927440",
    "end": "1932919"
  },
  {
    "text": "this is in our Langs Smith docs if you search lsmith valuation concept so we have I actually have a bunch of videos that showcase how to do this and um I",
    "start": "1932919",
    "end": "1940399"
  },
  {
    "text": "have a bunch of code as well so but those are three things you can do that don't require a reference and we do run",
    "start": "1940399",
    "end": "1946399"
  },
  {
    "text": "those in as on evaluation with our application so",
    "start": "1946399",
    "end": "1951880"
  },
  {
    "text": "yeah yep yeah unit testing so",
    "start": "1951880",
    "end": "1960480"
  },
  {
    "text": "um yeah okay so do we have any thoughts on unit testing so lsmith supports pie tests as",
    "start": "1960480",
    "end": "1968240"
  },
  {
    "text": "unit test but basically it depends what you mean by unit test typically like conventional",
    "start": "1968240",
    "end": "1973519"
  },
  {
    "text": "software engineering unit tests um are are very effectively done in things like",
    "start": "1973519",
    "end": "1979320"
  },
  {
    "text": "P test there's a lot of Frameworks for that what I like to think about in unit testing with respect to LM apps is um",
    "start": "1979320",
    "end": "1986559"
  },
  {
    "text": "kind of like what we show um in this uh code generation example um here we use",
    "start": "1986559",
    "end": "1994840"
  },
  {
    "text": "some really simple unit tests just for like Imports and code execution um simple unit test like this can actually",
    "start": "1994840",
    "end": "2000000"
  },
  {
    "text": "run in your app itself so basically one place you can think about putting unit tests are actually in your app itself within Lang graph for self for kind of",
    "start": "2000000",
    "end": "2006919"
  },
  {
    "text": "inapp error handling or self correction so that's like one place for unit test that's kind of interesting and new with",
    "start": "2006919",
    "end": "2012159"
  },
  {
    "text": "LM applications they can live inside your app itself um another good one for unit tests within app is uh if you're",
    "start": "2012159",
    "end": "2019639"
  },
  {
    "text": "doing structured output anywhere in your application which is like a really common thing people like confirm the schema is correct that's another good",
    "start": "2019639",
    "end": "2026320"
  },
  {
    "text": "use case for unit test within your application th those also both of those things could also be done independently",
    "start": "2026320",
    "end": "2033159"
  },
  {
    "text": "like in CI outside of your application so we are going to have more ation support for CI with Lang Smith soon uh I",
    "start": "2033159",
    "end": "2041039"
  },
  {
    "text": "will check with the team on that um but I think the interesting idea for unit",
    "start": "2041039",
    "end": "2046480"
  },
  {
    "text": "tests with llm applications is this idea of inline within your app itself because LMS are so good at self-correcting if",
    "start": "2046480",
    "end": "2052878"
  },
  {
    "text": "you run unit test in your application they can often catch the error and then correct thems and unit tests are fast",
    "start": "2052879",
    "end": "2058240"
  },
  {
    "text": "and cheap to run so it's actually a really nice kind of like piece of alpha that in fact that's exactly what kpoy",
    "start": "2058240",
    "end": "2063679"
  },
  {
    "text": "was mentioning here that you know running unit tests in line with your application is is is actually really",
    "start": "2063679",
    "end": "2069560"
  },
  {
    "text": "quite nice um and produce significant Improvement in performance in Alpha",
    "start": "2069560",
    "end": "2076440"
  },
  {
    "text": "codium cool",
    "start": "2076440",
    "end": "2079919"
  },
  {
    "text": "yep okay yeah yeah so what the question was",
    "start": "2084320",
    "end": "2091878"
  },
  {
    "text": "if I want to do some of this inapp error correction stuff so let's take this example the corrective rag thing",
    "start": "2091879",
    "end": "2098280"
  },
  {
    "text": "if I actually want this to run in my application it obviously needs to be super fast so that's actually what we've",
    "start": "2098280",
    "end": "2103760"
  },
  {
    "text": "seen the tricks we've seen here are basically use very fast smaller LM so you mentioned for example even the",
    "start": "2103760",
    "end": "2109800"
  },
  {
    "text": "ability to find tune that's actually a good idea if you have a judging task that is very consistent it's a very good",
    "start": "2109800",
    "end": "2116880"
  },
  {
    "text": "use case for fine tuning actually fine tune is small low capacity extremely fast and effectively very cheap to",
    "start": "2116880",
    "end": "2123320"
  },
  {
    "text": "deploy model that's a very good idea we've seen people do that um also use",
    "start": "2123320",
    "end": "2128640"
  },
  {
    "text": "very simple grading criteria don't have some kind of arbitrary scale from 0 to five with high cognitive load the LM has",
    "start": "2128640",
    "end": "2134640"
  },
  {
    "text": "to think about yes no very simple binary grade you can even for some of the stuff you can even be old school and fine tune",
    "start": "2134640",
    "end": "2141000"
  },
  {
    "text": "a classifier but basically really simple lightweight fast llm as judge style",
    "start": "2141000",
    "end": "2148079"
  },
  {
    "text": "classifier wouldn't be an llm necessarily but but basically very simple fast test for in app anything",
    "start": "2148079",
    "end": "2154720"
  },
  {
    "text": "like kind of in runtime you will need or want another cool use case for this or or",
    "start": "2154720",
    "end": "2160240"
  },
  {
    "text": "another like interesting option for this is grock is very very fast kind of with their lpu stuff um and they actually",
    "start": "2160240",
    "end": "2166880"
  },
  {
    "text": "would be a very interesting option uh we've done some work on that with grock basically for any of these kind of inapp",
    "start": "2166880",
    "end": "2172800"
  },
  {
    "text": "LMS judge error correction things using something like grock which is extremely fast but it's a very good Insight",
    "start": "2172800",
    "end": "2179160"
  },
  {
    "text": "fine-tuning your own model is actually a really good we've seen people do that for these types of any anything with Lang graph in in app error correction",
    "start": "2179160",
    "end": "2188160"
  },
  {
    "text": "cool",
    "start": "2188520",
    "end": "2191040"
  },
  {
    "text": "yep yeah we have some good uh cookbooks talking about multi-agent um which I",
    "start": "2194119",
    "end": "2200960"
  },
  {
    "text": "again will need to find a way to share with you",
    "start": "2200960",
    "end": "2206000"
  },
  {
    "text": "um yep so we have this if you go to Lang graph langra GitHub examples multi-agent",
    "start": "2207560",
    "end": "2214400"
  },
  {
    "text": "there's a few different notebooks here uh that are worth checking",
    "start": "2214400",
    "end": "2219920"
  },
  {
    "text": "out",
    "start": "2221760",
    "end": "2224760"
  },
  {
    "text": "yep yeah yeah so actually Lang graph is specifically designed for Cycles um so",
    "start": "2231079",
    "end": "2237160"
  },
  {
    "text": "some of the examp like what we're showed today is only a branch so it's a simpler graph um but for example the uh react",
    "start": "2237160",
    "end": "2244480"
  },
  {
    "text": "agent we'll show today is a cycle so it's basically going going to continue in a loop just like this and what you do",
    "start": "2244480",
    "end": "2251119"
  },
  {
    "text": "is you set a recursion limit in your Lang graph config so you basically tell it to only proceed for some number of",
    "start": "2251119",
    "end": "2257720"
  },
  {
    "text": "cycles and this is default set for you I believe it's like 20 or something like that uh but that's what you're going to",
    "start": "2257720",
    "end": "2263920"
  },
  {
    "text": "that's what you're going to want to",
    "start": "2263920",
    "end": "2266960"
  },
  {
    "text": "do",
    "start": "2269319",
    "end": "2272319"
  },
  {
    "text": "yep um so the question was about timing of the responses so do you mean like if you're implementing some kind of",
    "start": "2275599",
    "end": "2282280"
  },
  {
    "text": "self-correction how long that takes well that's that's kind of a that gets back to the question uh that that this",
    "start": "2282280",
    "end": "2289720"
  },
  {
    "text": "gentleman asked um it depends a lot on the LM you choose to use for your judging and it's latency so that's kind",
    "start": "2289720",
    "end": "2297640"
  },
  {
    "text": "of where if you're so maybe there's two sides of this one side of it is choosing",
    "start": "2297640",
    "end": "2303760"
  },
  {
    "text": "an llm that's very very fast and that's very important to do could be something like grock could be a",
    "start": "2303760",
    "end": "2309680"
  },
  {
    "text": "fine-tuned like deployment that you do yourself could be a GPD 35 so that's like one side of it the other side of it",
    "start": "2309680",
    "end": "2316440"
  },
  {
    "text": "is um how do you actually kind of Monitor and measure that and so again",
    "start": "2316440",
    "end": "2321680"
  },
  {
    "text": "lsmith actually does have very good support for tracing and observability and we do have timings all in fact I can",
    "start": "2321680",
    "end": "2328560"
  },
  {
    "text": "go ahead and show you very quickly if you want to see um so this is my Langs Smith dashboard I'll zoom in a little",
    "start": "2328560",
    "end": "2334839"
  },
  {
    "text": "bit these are my experiments um now if I if I zoom in here I can open up one um",
    "start": "2334839",
    "end": "2340839"
  },
  {
    "text": "the Wi-Fi is a little bit slow these are my replicates I can open up one of my traces and what I can see here is over",
    "start": "2340839",
    "end": "2347440"
  },
  {
    "text": "here I get the timing so this is the timing of the entire graph and I can go through my steps so this is like the",
    "start": "2347440",
    "end": "2353440"
  },
  {
    "text": "retrieval is really fast that's good you know less than a second okay now here's what's interesting my grading in this",
    "start": "2353440",
    "end": "2359119"
  },
  {
    "text": "particular case is like 4 seconds that's you know not acceptable in a production setting most likely um but again this is",
    "start": "2359119",
    "end": "2365599"
  },
  {
    "text": "just like a this is a test case in fact I'm using what am I using to grade here uh gbd 40 so it is um you know there's",
    "start": "2365599",
    "end": "2374359"
  },
  {
    "text": "ways you could speed this up by using different models or different prompts or grading everything in bulk there's a lot of ideas I actually grade each document",
    "start": "2374359",
    "end": "2381000"
  },
  {
    "text": "independently oh actually you know what this is using chat Fire Works uh so anyway but you can look at your timings",
    "start": "2381000",
    "end": "2386520"
  },
  {
    "text": "in lengths but that's a really nice way I'd like to do it to kind of see um to",
    "start": "2386520",
    "end": "2391680"
  },
  {
    "text": "kind of monitor the timing of my applications",
    "start": "2391680",
    "end": "2396160"
  },
  {
    "text": "yep don't have already",
    "start": "2421720",
    "end": "2428400"
  },
  {
    "text": "um I'll make sure I got the question so the question was with agents you",
    "start": "2430079",
    "end": "2435520"
  },
  {
    "text": "typically pass them a message history that is absolutely true um and",
    "start": "2435520",
    "end": "2441880"
  },
  {
    "text": "in fact the react agent that we Implement here like I can even open up one of the traces we can look at it",
    "start": "2441880",
    "end": "2447240"
  },
  {
    "text": "together so um so here's a react agent with uh GPD",
    "start": "2447240",
    "end": "2454960"
  },
  {
    "text": "40 here's one of my traces let's open it up and actually see what's going on here",
    "start": "2454960",
    "end": "2460400"
  },
  {
    "text": "so what happens is um first my assistant is right here this is open AI will you",
    "start": "2460400",
    "end": "2467960"
  },
  {
    "text": "know here's the system prompt right so your helpful assistant you're answering questions um here's the human question",
    "start": "2467960",
    "end": "2473880"
  },
  {
    "text": "right so again this is the start of our message history okay so what the what the and also these are the tools that",
    "start": "2473880",
    "end": "2481280"
  },
  {
    "text": "the LM has and this is pretty cool we can see this is the one it called so",
    "start": "2481280",
    "end": "2486599"
  },
  {
    "text": "what happened is rlm it looked at here's our assistance prom here's the human instruction and it",
    "start": "2486599",
    "end": "2491760"
  },
  {
    "text": "says okay I'm going to retrieve documents great so then it goes and this is the",
    "start": "2491760",
    "end": "2497319"
  },
  {
    "text": "tool call it goes and retrieves the documents now that goes so you look here we can actually open up the retriever what do we get here's our documents cool",
    "start": "2497319",
    "end": "2505560"
  },
  {
    "text": "now I go it goes back to the assistant so back this is a looping thing it started with our assistant it made the tool call the tool ran we got documents",
    "start": "2505560",
    "end": "2512319"
  },
  {
    "text": "now we get them back now let's go back to our llm so now our llm this is pretty cool right right here's the message",
    "start": "2512319",
    "end": "2518560"
  },
  {
    "text": "history like you're were saying instructions question document retrieval",
    "start": "2518560",
    "end": "2524839"
  },
  {
    "text": "the documents that are retrieved are right here and then now the LM says Okay",
    "start": "2524839",
    "end": "2530040"
  },
  {
    "text": "I want to grade them it calls a grader tool and this is its reasoning and this is its grade so anyway you are right",
    "start": "2530040",
    "end": "2537280"
  },
  {
    "text": "that as this goes through you basically accumulate a message history and the llm",
    "start": "2537280",
    "end": "2542720"
  },
  {
    "text": "will use the message history and the most recent instruction to reason about what tool to call next that's exactly",
    "start": "2542720",
    "end": "2547839"
  },
  {
    "text": "how it works um I think I answered the question is is there anything that isn't",
    "start": "2547839",
    "end": "2554280"
  },
  {
    "text": "clear about that so it is true like let's look at an example right so in this particular case right the llm sees",
    "start": "2554280",
    "end": "2562240"
  },
  {
    "text": "the retriev documents from the tool and then it makes the decision say okay I have this tool response it's retrieve",
    "start": "2562240",
    "end": "2568040"
  },
  {
    "text": "documents what should I do next and it says okay well why don't I go ahead and grade them and it calls a grade tool so",
    "start": "2568040",
    "end": "2573680"
  },
  {
    "text": "it looks at the message history and reasons about what tool it'll call and that's exactly how these style agents work the and the whole issue is that's",
    "start": "2573680",
    "end": "2580839"
  },
  {
    "text": "kind of a noisy process like it can look at that whole trajectory it can get confused it can call the wrong tool then it's all on the wrong track and that's",
    "start": "2580839",
    "end": "2587160"
  },
  {
    "text": "exactly why these more open-ended tool calling agents fail",
    "start": "2587160",
    "end": "2594200"
  },
  {
    "text": "um y",
    "start": "2595920",
    "end": "2599920"
  },
  {
    "text": "okay right so I think that the question is well let's say this is a multi-turn conversation where I can the user can go",
    "start": "2627760",
    "end": "2634920"
  },
  {
    "text": "ahead and ask a second question of course and that hold kind of message history will be propagated um yes that is that is a",
    "start": "2634920",
    "end": "2644800"
  },
  {
    "text": "common pattern um and that let's see I mean what's the",
    "start": "2644800",
    "end": "2651559"
  },
  {
    "text": "question on that though like it it could use context from its initial trajectory",
    "start": "2651559",
    "end": "2658839"
  },
  {
    "text": "to answer the second question for sure um it'll probably look at that jointly when it's deciding what tool to call so",
    "start": "2658839",
    "end": "2665440"
  },
  {
    "text": "for example if it receives a question and in its message history it sees the context need to answer the question the",
    "start": "2665440",
    "end": "2671240"
  },
  {
    "text": "agent could probably decide okay I don't need to retrieve documents again I have the documents need to answer the question I'll answer that question",
    "start": "2671240",
    "end": "2676839"
  },
  {
    "text": "directly so it is true that a multi-term conversation the agent can look at its message history to inform what to do",
    "start": "2676839",
    "end": "2683400"
  },
  {
    "text": "next that's definitely true here I don't consider evaluation of multi-turn",
    "start": "2683400",
    "end": "2689160"
  },
  {
    "text": "conversations um but that is it's a good topic actually um I don't quite have a",
    "start": "2689160",
    "end": "2696119"
  },
  {
    "text": "tutorial for that yet yet but I could think about putting that together yeah that's a good point uh",
    "start": "2696119",
    "end": "2703359"
  },
  {
    "text": "I'll make a note of that actually yeah so multi-turn is a good",
    "start": "2703359",
    "end": "2712319"
  },
  {
    "text": "one cool um okay yep yep",
    "start": "2712319",
    "end": "2719720"
  },
  {
    "text": "um so the question is I believe",
    "start": "2733800",
    "end": "2738960"
  },
  {
    "text": "um have I tested like the ability to kind of online autogenerate unit",
    "start": "2739800",
    "end": "2748000"
  },
  {
    "text": "tests yeah okay so that's a big topic so basically the alpha codium paper",
    "start": "2748000",
    "end": "2755119"
  },
  {
    "text": "um that kpoy references here does",
    "start": "2755119",
    "end": "2760920"
  },
  {
    "text": "that I have not tested that because it does ramp up the complexity because then",
    "start": "2760920",
    "end": "2766359"
  },
  {
    "text": "you're relying on and and I don't think I mean that would be aggressive for a",
    "start": "2766359",
    "end": "2771839"
  },
  {
    "text": "production setting because basically be relying on an llm to autogenerate you unit tests testing against things that",
    "start": "2771839",
    "end": "2777480"
  },
  {
    "text": "are autogenerated there's a lot of opportunity for error there I think it's interesting particularly in terms of",
    "start": "2777480",
    "end": "2783280"
  },
  {
    "text": "like offline challenges like this but in terms of like a production application that feels pretty pretty hard and risky",
    "start": "2783280",
    "end": "2788480"
  },
  {
    "text": "but it's an interesting theme the thing I've tested more on I found to be very effective is super simple crisp",
    "start": "2788480",
    "end": "2794160"
  },
  {
    "text": "lightweight free effectively unit tests like again the good use case I found was",
    "start": "2794160",
    "end": "2801559"
  },
  {
    "text": "our um so Lang chain we have an internal rag application called chat Lang chain",
    "start": "2801559",
    "end": "2807960"
  },
  {
    "text": "and it indexes our documents and provides QA it occasionally hallucinates",
    "start": "2807960",
    "end": "2813760"
  },
  {
    "text": "the Imports and that's a really bad experience right if you take a code block copy it from this app and then it",
    "start": "2813760",
    "end": "2819240"
  },
  {
    "text": "this import doesn't exist it's like what the hell you know that's really annoying so I incorporate a really simple check",
    "start": "2819240",
    "end": "2824880"
  },
  {
    "text": "where I have a unit test that just it does a function call where it extracts the imports from the code block from the",
    "start": "2824880",
    "end": "2831119"
  },
  {
    "text": "answer and it tests the Imports in isolation if they don't exist there's an error I feed it back to LM say look this",
    "start": "2831119",
    "end": "2836680"
  },
  {
    "text": "isn't a real import try again and I can you can do other tricks like then context stuff relevant docum documents",
    "start": "2836680",
    "end": "2843800"
  },
  {
    "text": "or something like that anyway you can handle that differently but that just that little Alpha significantly improved",
    "start": "2843800",
    "end": "2849720"
  },
  {
    "text": "our performance so I kind of like simple lightweight free unit tests um the idea",
    "start": "2849720",
    "end": "2856559"
  },
  {
    "text": "of online gener unit test is interesting but like opens up a lot more surface area for errors",
    "start": "2856559",
    "end": "2863800"
  },
  {
    "text": "yeah um so the follow-up there was um try let's see trying to write the test",
    "start": "2887400",
    "end": "2893880"
  },
  {
    "text": "before the app is implemented um I see yes so they also do",
    "start": "2893880",
    "end": "2900280"
  },
  {
    "text": "that so basically for each question they do have they so the alpha codium work",
    "start": "2900280",
    "end": "2906079"
  },
  {
    "text": "references existing unit tests for a given question as well as autog generates so you are",
    "start": "2906079",
    "end": "2913319"
  },
  {
    "text": "right that would be interesting to have a bunch of pregenerated unit tests that you know are good for certain questions",
    "start": "2913319",
    "end": "2919720"
  },
  {
    "text": "and to run them absolutely hard to do in a production setting with an open-ended input but potentially very useful in",
    "start": "2919720",
    "end": "2927920"
  },
  {
    "text": "well okay even in a production set you could maybe have some battery of unit test and based upon the question type",
    "start": "2927920",
    "end": "2934559"
  },
  {
    "text": "pull related unit tests that you know going to be relevant it's a good idea yeah it's a good idea",
    "start": "2934559",
    "end": "2939880"
  },
  {
    "text": "for sure so basically using some battery of dynamically chosen pre-existing unit",
    "start": "2939880",
    "end": "2945599"
  },
  {
    "text": "tests based on the question type or the documentation whatever documentation they're asking about that's a good",
    "start": "2945599",
    "end": "2953640"
  },
  {
    "text": "idea yeah he was saying so for larger projects you can also use that to test for regressions yes that is that's",
    "start": "2958880",
    "end": "2964880"
  },
  {
    "text": "definitely a good idea and this paper does incorporate that idea as well as this autogenerated unit test thing which",
    "start": "2964880",
    "end": "2970520"
  },
  {
    "text": "is a little bit more aggressive cool yep",
    "start": "2970520",
    "end": "2978280"
  },
  {
    "text": "yeah yep so the question was in some of these self-reflective applications like",
    "start": "3021799",
    "end": "3027599"
  },
  {
    "text": "let's say this one the self-reflective rag right we're doing a few different checks here we're checking documents we're checking hallucinations we're",
    "start": "3027599",
    "end": "3033599"
  },
  {
    "text": "checking answer quality instead of having some hardcoded single prompt uh to do that can you have",
    "start": "3033599",
    "end": "3040920"
  },
  {
    "text": "another agent like kind of a a checker a grader agent that's a bit more sophisticated um I have and he mentioned",
    "start": "3040920",
    "end": "3048240"
  },
  {
    "text": "a few Frameworks I have not played with them I think it's interesting I think it's it's one of these things where it's",
    "start": "3048240",
    "end": "3054040"
  },
  {
    "text": "really good for kind of academic papers it's very interesting it's really good it could be good for offline testing in",
    "start": "3054040",
    "end": "3059760"
  },
  {
    "text": "an online or production setting or it's the latency and complexity is probably a",
    "start": "3059760",
    "end": "3065359"
  },
  {
    "text": "bit much I think in in a production SC it goes back to what I think this guy",
    "start": "3065359",
    "end": "3070720"
  },
  {
    "text": "was referencing you probably would want something that's extremely fast lightweight um and I would not think",
    "start": "3070720",
    "end": "3078119"
  },
  {
    "text": "about like a multi-agent system in a production setting doing any kind of grading this whole idea of llm as llm",
    "start": "3078119",
    "end": "3084640"
  },
  {
    "text": "graders is kind of a new idea so I think this idea of like more",
    "start": "3084640",
    "end": "3089720"
  },
  {
    "text": "complex a agent graders is interesting but we're kind of in the we're taking baby steps at this point especially in a",
    "start": "3089720",
    "end": "3096280"
  },
  {
    "text": "production setting so I'd probably shy away from that for now particularly if you're think about production but for",
    "start": "3096280",
    "end": "3101920"
  },
  {
    "text": "something offline or experimentation it's probably interesting yeah what's the best",
    "start": "3101920",
    "end": "3108480"
  },
  {
    "text": "practice that you've seen so far for performing tests or evaluations on those",
    "start": "3108480",
    "end": "3113680"
  },
  {
    "text": "Cycles because obviously end up with many many different of that",
    "start": "3113680",
    "end": "3122960"
  },
  {
    "text": "obviously provid tring what abouty what kind of",
    "start": "3122960",
    "end": "3129839"
  },
  {
    "text": "PR per yeah exactly so actually The Notebook shared here go into it a little",
    "start": "3129839",
    "end": "3136200"
  },
  {
    "text": "bit so the way I like to do it is um and I can even why don't I just show you the",
    "start": "3136200",
    "end": "3141559"
  },
  {
    "text": "one of the notebooks so basically um so at the bottom I have",
    "start": "3141559",
    "end": "3147720"
  },
  {
    "text": "kind of all the different evaluations so this goes back to a question that someone mentioned down here as well which so this is uh so",
    "start": "3147720",
    "end": "3156400"
  },
  {
    "text": "there's a collab and then there's a notebook and they are both the same so",
    "start": "3156400",
    "end": "3161799"
  },
  {
    "text": "they have the same uh evaluation section uh this is this is rag agent testing.",
    "start": "3161799",
    "end": "3169040"
  },
  {
    "text": "ipnb uh and there's also collab which is uh it's the same notebook basically it's",
    "start": "3169040",
    "end": "3175160"
  },
  {
    "text": "it's in the slides just make sure you got those links but but to your question so the way I did it and you could be",
    "start": "3175160",
    "end": "3181520"
  },
  {
    "text": "very flexible with this is basically I Define the expected trajectory through the nodes of My Graph and in this",
    "start": "3181520",
    "end": "3188680"
  },
  {
    "text": "particular simple case the trajectories that I expect are basically retrieval grading web search generate or retrieval",
    "start": "3188680",
    "end": "3196720"
  },
  {
    "text": "grade generate those are the two expected trajectories that I want to take now in this case I don't do Cycles",
    "start": "3196720",
    "end": "3204000"
  },
  {
    "text": "if you did Cycles you could just incorporate more steps here to say okay here's kind of the expected number of steps I want or expect to see through",
    "start": "3204000",
    "end": "3210559"
  },
  {
    "text": "that cycle and I think someone mentioned here if you have a really open-ended challenging task then it may be hard to enumerate exactly but for a lot of like",
    "start": "3210559",
    "end": "3217960"
  },
  {
    "text": "more production setting applications where these are not extremely long running you can enumerate here's the",
    "start": "3217960",
    "end": "3223000"
  },
  {
    "text": "steps I expect to take through the cycle and in the way I do the evaluator it's",
    "start": "3223000",
    "end": "3228200"
  },
  {
    "text": "as simple as this there's not much code it's all in the notebooks but basically I compare the tool calls that the thing",
    "start": "3228200",
    "end": "3233280"
  },
  {
    "text": "did to these trajectories that's it super simple so that's how I would think",
    "start": "3233280",
    "end": "3239000"
  },
  {
    "text": "about it I keep it really simple I would basically try to enumerate here is the steps through the difference the cycle I",
    "start": "3239000",
    "end": "3244119"
  },
  {
    "text": "wanted to take and yeah go ahead",
    "start": "3244119",
    "end": "3251599"
  },
  {
    "text": "right Ys that's",
    "start": "3252839",
    "end": "3260200"
  },
  {
    "text": "it that's it that's it so it's actually really simple in in this is actually a",
    "start": "3260200",
    "end": "3266200"
  },
  {
    "text": "good point in the Lang graph case for this custom agent you'll see all I do is",
    "start": "3266200",
    "end": "3273240"
  },
  {
    "text": "for every node and we could get into the code if we want maybe people have already explored",
    "start": "3273240",
    "end": "3279079"
  },
  {
    "text": "so well I'll just answer the question directly then we can back up and go through all the code if we want but",
    "start": "3279079",
    "end": "3285520"
  },
  {
    "text": "basically each node in My Graph I just append this step name right so like",
    "start": "3285520",
    "end": "3291559"
  },
  {
    "text": "retrieve I just say retrieve documents this is my generate node I just you know aend this thing generate answer and I",
    "start": "3291559",
    "end": "3298359"
  },
  {
    "text": "return that in my state as steps so then I have this record of the steps I took",
    "start": "3298359",
    "end": "3303520"
  },
  {
    "text": "in My Graph that's it and I can go ahead and fetch that at eval time and compare",
    "start": "3303520",
    "end": "3308960"
  },
  {
    "text": "it to what I expect that's all you're doing so that's how it work with like the custom Lang graph thing and now with the react agent actually it's a little",
    "start": "3308960",
    "end": "3316680"
  },
  {
    "text": "easier because the react agent uses a message history so I can just go to my message history and that's exactly what",
    "start": "3316680",
    "end": "3323319"
  },
  {
    "text": "I show here you can strip out um I I guess I do it up above but basically I",
    "start": "3323319",
    "end": "3328520"
  },
  {
    "text": "have this function in the notebook um let's see where is it yeah is find tool calls react so it's this little function",
    "start": "3328520",
    "end": "3337039"
  },
  {
    "text": "that will um basically look at my message history and strip out all the tool calls so um yeah it's a nice",
    "start": "3337039",
    "end": "3346079"
  },
  {
    "text": "little nice little thing with a react agent is really easy to get the tool calls out with Lang graph I just log",
    "start": "3346079",
    "end": "3351839"
  },
  {
    "text": "them at every node and then at at eval time I just can extract them",
    "start": "3351839",
    "end": "3358240"
  },
  {
    "text": "yep yeah so the question was with an agent in a multi-turn conversation",
    "start": "3384240",
    "end": "3391440"
  },
  {
    "text": "setting how does it know whether or not it has the answer to to a given",
    "start": "3391440",
    "end": "3398000"
  },
  {
    "text": "question where to go exactly yeah that's right so there's a",
    "start": "3398000",
    "end": "3405760"
  },
  {
    "text": "couple different ways to kind of break this down so with these",
    "start": "3405760",
    "end": "3410880"
  },
  {
    "text": "agents there's a few levels of instruction you give it first you give it an overall agent prompt so if I look",
    "start": "3410880",
    "end": "3418839"
  },
  {
    "text": "at the notebook here uh we can go look at the react agent as an example of this so the react agent is defined uh right",
    "start": "3418839",
    "end": "3426839"
  },
  {
    "text": "here this is kind of like the planning step here's my like naive prompt okay so",
    "start": "3426839",
    "end": "3432520"
  },
  {
    "text": "you're helpful assistant task with answering tasks use the provided Vector stored retrieve documents grade them and",
    "start": "3432520",
    "end": "3439440"
  },
  {
    "text": "go on now let's take the case that's more complicated let's say I had two Vector stores so one thing I can do is I",
    "start": "3439440",
    "end": "3445880"
  },
  {
    "text": "can explicitly put in the agent prompt you have two Vector stores a b a has",
    "start": "3445880",
    "end": "3450960"
  },
  {
    "text": "this B has this and then you're implicitly having the llm uh giving it",
    "start": "3450960",
    "end": "3457960"
  },
  {
    "text": "the ability to kind of Reason about which one to use now this is where the second piece comes in you have to Al you",
    "start": "3457960",
    "end": "3463039"
  },
  {
    "text": "also bind it to a set of tools this is really where the decision- making comes in when you create this tool so here's",
    "start": "3463039",
    "end": "3470359"
  },
  {
    "text": "retrieve documents right this tool description is captured by the agent so the agent knows what's in this tool",
    "start": "3470359",
    "end": "3476839"
  },
  {
    "text": "and this is really where that decision to use this retriever tool versus",
    "start": "3476839",
    "end": "3481960"
  },
  {
    "text": "another one would be made it' be a combination of the prompt you give to the agent and or the tool description so",
    "start": "3481960",
    "end": "3488599"
  },
  {
    "text": "if you had two Vector stores you could basically say like retrieve documents one this Vector store contains information about X another one contains",
    "start": "3488599",
    "end": "3495440"
  },
  {
    "text": "information about y then the agent is deciding what tool to call based on that description and maybe based on its",
    "start": "3495440",
    "end": "3502280"
  },
  {
    "text": "overall prompt but to your point it's not easy so actually with custom agent",
    "start": "3502280",
    "end": "3509520"
  },
  {
    "text": "this is with a react style agent with the Lang graph custom agent you can do it a little bit differently where I",
    "start": "3509520",
    "end": "3515119"
  },
  {
    "text": "actually don't have it in this notebook but I have other cases where you actually can build a router",
    "start": "3515119",
    "end": "3520240"
  },
  {
    "text": "node and I mean I'll show you actually so this particular notebook um so this",
    "start": "3520240",
    "end": "3527640"
  },
  {
    "text": "one this selfrag it's in the slides if you open this up we did this with uh llama folks so this is actually a trick",
    "start": "3527640",
    "end": "3534760"
  },
  {
    "text": "I really like if you go to um if you go to Lang graph rag agent local here",
    "start": "3534760",
    "end": "3542160"
  },
  {
    "text": "um see it's Wii is a little slow what I Define here is a very specific router at",
    "start": "3542160",
    "end": "3549880"
  },
  {
    "text": "the start of my agent that decides where to send the query and this is something I really like to do because like we saw",
    "start": "3549880",
    "end": "3556680"
  },
  {
    "text": "with the react thing it has to kind of decide the right tool to use which can be kind of noisy",
    "start": "3556680",
    "end": "3562599"
  },
  {
    "text": "versus right here so here's my router right this is reliable",
    "start": "3562599",
    "end": "3569880"
  },
  {
    "text": "enough to run locally and what I do here is I run this at the start of My Graph",
    "start": "3569880",
    "end": "3574920"
  },
  {
    "text": "and I have the agent or yeah the agent explicitly take the question and decide what to use and based on what decision",
    "start": "3574920",
    "end": "3582720"
  },
  {
    "text": "it makes I can send it to either web search or in this case a vector store so to answer your question if I pull all the way back I personally like to do",
    "start": "3582720",
    "end": "3591039"
  },
  {
    "text": "explicit routing as a Noe at the start of My Graph because it's it's pretty clean",
    "start": "3591039",
    "end": "3596799"
  },
  {
    "text": "and you can see in the flow of this overall graph this router runs first and it looks in my question and sends it to",
    "start": "3596799",
    "end": "3602520"
  },
  {
    "text": "one of two places and this can be more complex you can send it to one of end places but I see one of two here this is",
    "start": "3602520",
    "end": "3607920"
  },
  {
    "text": "with like a custom Lang graph agent this is what I like to do if you're using a react agent it is then using uh a",
    "start": "3607920",
    "end": "3615119"
  },
  {
    "text": "combination of the the tool definitions and the overall agent prompt to decide",
    "start": "3615119",
    "end": "3620240"
  },
  {
    "text": "what tool to call but you can see it's more squishy because it has to call the",
    "start": "3620240",
    "end": "3625599"
  },
  {
    "text": "right tool tool and as opposed to giving it a router and saying always run this router first it has to kind of make the",
    "start": "3625599",
    "end": "3631359"
  },
  {
    "text": "right decision as to what tool to call based on the context question which is harder and that gets back to get that",
    "start": "3631359",
    "end": "3636599"
  },
  {
    "text": "gets back to our overall story about these kind of Lang graph explicit defined agents are more reliable because",
    "start": "3636599",
    "end": "3642720"
  },
  {
    "text": "you can like lay out this routing step right up front and have it always execute that step before going forward",
    "start": "3642720",
    "end": "3650280"
  },
  {
    "text": "yep okay got it so the question was how do I incorporate the of routing with",
    "start": "3683960",
    "end": "3690720"
  },
  {
    "text": "history so here's what you would do it's actually you know kind of it should be pretty",
    "start": "3690720",
    "end": "3696640"
  },
  {
    "text": "straightforward you can Define this router node in your graph and that router node I'll actually",
    "start": "3696640",
    "end": "3702799"
  },
  {
    "text": "go down to it so basically here's my graph and here's the the uh route",
    "start": "3702799",
    "end": "3708520"
  },
  {
    "text": "question node right",
    "start": "3708520",
    "end": "3712680"
  },
  {
    "text": "um yeah actually in this particular case it's it's an edge don't worry about those details basically what you could",
    "start": "3713599",
    "end": "3719599"
  },
  {
    "text": "do is you could have a node that takes in the state now that state could",
    "start": "3719599",
    "end": "3725480"
  },
  {
    "text": "include your history so what you could do is in that router prompt you could",
    "start": "3725480",
    "end": "3730799"
  },
  {
    "text": "really easily um here include another placeholder",
    "start": "3730799",
    "end": "3737079"
  },
  {
    "text": "variable for like your your message history or something and you what you could say then is make a decision about",
    "start": "3737079",
    "end": "3743920"
  },
  {
    "text": "where to go next based upon the question and based upon something in a history and so you actually would plum in your",
    "start": "3743920",
    "end": "3750400"
  },
  {
    "text": "message history here and use it to jointly decide what to do next so that's actually really easily handled in Lang",
    "start": "3750400",
    "end": "3756839"
  },
  {
    "text": "graph using a node and you can reference the history that can be passed you can",
    "start": "3756839",
    "end": "3762039"
  },
  {
    "text": "pass into that no as State uh cool",
    "start": "3762039",
    "end": "3770240"
  },
  {
    "text": "yep the notion of State yeah let's let's talk about state in a little bit more detail so let's actually go to the",
    "start": "3772079",
    "end": "3780319"
  },
  {
    "text": "notebooks that we're working with here that I shared um",
    "start": "3780319",
    "end": "3785400"
  },
  {
    "text": "so here's this raggage and testing notebook so if you go down to the custom",
    "start": "3785400",
    "end": "3792000"
  },
  {
    "text": "Lang graph agent the way you do it is um find the state yeah so here is",
    "start": "3792000",
    "end": "3799920"
  },
  {
    "text": "what I call graph state so the graph state is basically something that lives across the Lifetime",
    "start": "3799920",
    "end": "3806279"
  },
  {
    "text": "on my graph I typically like to do something as simple as just a dictionary so basically this is a rag",
    "start": "3806279",
    "end": "3814279"
  },
  {
    "text": "graph and here I'm basically going to define a number of attributes in my state that are relevant to what I want",
    "start": "3814279",
    "end": "3820640"
  },
  {
    "text": "to do with rag a question my answer generated whether or not to run search",
    "start": "3820640",
    "end": "3826279"
  },
  {
    "text": "some documents My Step list and basically the idea here is that I Define",
    "start": "3826279",
    "end": "3831720"
  },
  {
    "text": "my state up front and then at every node I bring I",
    "start": "3831720",
    "end": "3837000"
  },
  {
    "text": "ex accept state as the input and then I operate on it in some way and write back out to state so",
    "start": "3837000",
    "end": "3843960"
  },
  {
    "text": "basically what's happening is I Define State generally up front as like a dictionary or something like that it's a",
    "start": "3843960",
    "end": "3849760"
  },
  {
    "text": "placeholder for things I want to modify throughout my graph throughout my agent and every node",
    "start": "3849760",
    "end": "3856760"
  },
  {
    "text": "just takes in state does something and writes back out to State that's really it so basically it's a way you can think",
    "start": "3856760",
    "end": "3864440"
  },
  {
    "text": "of it as a really simple mechanism to persist information across the lifetime of my agent and for this rag agent it's",
    "start": "3864440",
    "end": "3871599"
  },
  {
    "text": "things that are really intuitive for rag it's like question it's documents and so let's take an example like um okay",
    "start": "3871599",
    "end": "3879119"
  },
  {
    "text": "here's a fun one so my gr documents node right what I'm doing here is I'm taking in state and from my state it's just a",
    "start": "3879119",
    "end": "3885760"
  },
  {
    "text": "dictionary so I'm extracting my question I'm extracting my documents um and I'm appending my I'm",
    "start": "3885760",
    "end": "3892200"
  },
  {
    "text": "appending a new Step because I'm notifying hey here's my new step and basically I'm doing some operation I'm",
    "start": "3892200",
    "end": "3897880"
  },
  {
    "text": "iterating through my documents I'm grading each one uh the grades yes I I I",
    "start": "3897880",
    "end": "3903480"
  },
  {
    "text": "keep it if um if the grade is no so yes",
    "start": "3903480",
    "end": "3908680"
  },
  {
    "text": "no means like is it relevant or not basically so if yes it's relevant I keep it I put in this filter docs list if",
    "start": "3908680",
    "end": "3914680"
  },
  {
    "text": "it's not I set the search flag to yes which means I'm going to run web search because I want to supplement I have some",
    "start": "3914680",
    "end": "3920119"
  },
  {
    "text": "docs that are irrelevant and I write back to State at the end my filter docs my question search and steps that's it",
    "start": "3920119",
    "end": "3928079"
  },
  {
    "text": "so st's a really convenient way to just pass information across my",
    "start": "3928079",
    "end": "3933200"
  },
  {
    "text": "agent and I like using a dictionary it's just like nice and clean to manage as",
    "start": "3933200",
    "end": "3938599"
  },
  {
    "text": "opposed to a message history which is like a little more confusing like in any note if you use a message history it's like a stack of",
    "start": "3938599",
    "end": "3944720"
  },
  {
    "text": "messages so if you want the question you have to like it's like usually the first message you have to just index it just",
    "start": "3944720",
    "end": "3950440"
  },
  {
    "text": "kind of ugly I like using a dict which just like I just get the question out as a as a key in my dictionary",
    "start": "3950440",
    "end": "3957640"
  },
  {
    "text": "um cool let's see we're about an hour in I",
    "start": "3959559",
    "end": "3965400"
  },
  {
    "text": "can also um you know let people just hack and walk around talk to",
    "start": "3965400",
    "end": "3971240"
  },
  {
    "text": "people stay up here whatever is best uh you keep asking questions if you want to",
    "start": "3971240",
    "end": "3976559"
  },
  {
    "text": "I think people are just working doing their own thing now anyway um",
    "start": "3976559",
    "end": "3982880"
  },
  {
    "text": "so I might ask one question just for fun is anyone interested in local agents we didn't talk about that too much uh it's",
    "start": "3982880",
    "end": "3989760"
  },
  {
    "text": "a big theme yeah so I shared a notebook for that um and by default I am using llama",
    "start": "3989760",
    "end": "3998680"
  },
  {
    "text": "3 uh you can absolutely test other models um so this is set up to test just",
    "start": "3998680",
    "end": "4006440"
  },
  {
    "text": "llama 3 with a llama um try other things I have a M2 32",
    "start": "4006440",
    "end": "4013599"
  },
  {
    "text": "gig so 8B runs fine for me if you something bigger um you could actually",
    "start": "4013599",
    "end": "4019000"
  },
  {
    "text": "bump that up a little bit so that can be a nice idea",
    "start": "4019000",
    "end": "4026920"
  },
  {
    "text": "uh yeah exactly 70b is uh it's unfortunate that yeah I've um I actually want a",
    "start": "4026920",
    "end": "4034400"
  },
  {
    "text": "bigger machine so I can run that because I found for Tool calling 8B is really at the edge of of",
    "start": "4034400",
    "end": "4043599"
  },
  {
    "text": "reliability um so that's actually why you really can't",
    "start": "4043599",
    "end": "4048880"
  },
  {
    "text": "run the react agent locally with an 8 billion model reliably you can run the Lang graph agent very reliably because",
    "start": "4048880",
    "end": "4055000"
  },
  {
    "text": "it doesn't actually need tool calling it only needs structured outputs you'll see in the notebook so that's actually",
    "start": "4055000",
    "end": "4060880"
  },
  {
    "text": "really nice thing um but react agent won't won't run",
    "start": "4060880",
    "end": "4066440"
  },
  {
    "text": "locally reliably at least yep",
    "start": "4066440",
    "end": "4073520"
  },
  {
    "text": "you say chunk documents okay yeah so this is this question always comes up with rag what",
    "start": "4077880",
    "end": "4083240"
  },
  {
    "text": "is a typical chunk size yeah if you ask 10 people you'll get 10 answers it's uh",
    "start": "4083240",
    "end": "4091039"
  },
  {
    "text": "notoriously ad hoc you know what to be honest I did um I did kind of a talk on",
    "start": "4091039",
    "end": "4098120"
  },
  {
    "text": "the future of rag with long context models I'm kind of a fan of um trying to",
    "start": "4098120",
    "end": "4103880"
  },
  {
    "text": "keep chunk size as large as possible actually I think this is a whole tangent but I actually think one of the nicest",
    "start": "4103880",
    "end": "4110400"
  },
  {
    "text": "tricks uh let me see if I have a good visual for it",
    "start": "4110400",
    "end": "4115600"
  },
  {
    "text": "um basically uh let me try to find something here um this a whole separate talk but",
    "start": "4115600",
    "end": "4124040"
  },
  {
    "text": "basically um I think uh yeah this one",
    "start": "4124040",
    "end": "4129278"
  },
  {
    "text": "ragon long context so this is a whole different thing but um",
    "start": "4129279",
    "end": "4136560"
  },
  {
    "text": "yeah this idea so I think for rag one of the ideas I like the most",
    "start": "4136560",
    "end": "4143758"
  },
  {
    "text": "is decoupling I'll explain this I'll just say it and I'll explain it decoupling what you actually index for",
    "start": "4143759",
    "end": "4149600"
  },
  {
    "text": "retrieval from what you pass the llm because you have this weird tension right smaller semantic related chunks",
    "start": "4149600",
    "end": "4156480"
  },
  {
    "text": "are good for retrieval relative to a question right but llms can process huge",
    "start": "4156480",
    "end": "4162520"
  },
  {
    "text": "amounts of context at this point you know up to say million tokens so historically what we would do is you",
    "start": "4162520",
    "end": "4168600"
  },
  {
    "text": "would chunk really small like very very try to get it as tight as possible all these tricks semantic chunking a lot of",
    "start": "4168600",
    "end": "4175278"
  },
  {
    "text": "things to really compress down to just compress and group semantic related chunks of contexts right but the problem",
    "start": "4175279",
    "end": "4182719"
  },
  {
    "text": "is then you're passing to the llm very narrow chunks of information which has problems in recall so basically it's",
    "start": "4182719",
    "end": "4189600"
  },
  {
    "text": "more likely that you'll miss context necessary so it's good for retrievable but bad for",
    "start": "4189600",
    "end": "4195880"
  },
  {
    "text": "answer generation so a nice trick is for retrieval use some chunking strategy",
    "start": "4195880",
    "end": "4201920"
  },
  {
    "text": "whatever you want like make it you know small but you can actually use like a",
    "start": "4201920",
    "end": "4207239"
  },
  {
    "text": "doc store to store the full document and what you can do is retrieve based on small chunks but then reference the full",
    "start": "4207239",
    "end": "4214560"
  },
  {
    "text": "document and pass the full document to the llm for actual generation time so you decouple the problem of like are you",
    "start": "4214560",
    "end": "4220920"
  },
  {
    "text": "sure you're passing sufficient context to LM itself now I also understand if you've massive Dot docents it can be",
    "start": "4220920",
    "end": "4226000"
  },
  {
    "text": "wasteful in terms of tokens to pass full documents through but um there's some",
    "start": "4226000",
    "end": "4231080"
  },
  {
    "text": "predo Optimum here where I think IND being too strict with your indexing approach doesn't make sense",
    "start": "4231080",
    "end": "4237960"
  },
  {
    "text": "anymore given that you can process very large context in your llm so you want to",
    "start": "4237960",
    "end": "4243640"
  },
  {
    "text": "avoid being overly restrictive with your chunk size so this is a nice way to get around it basically to to summarize you",
    "start": "4243640",
    "end": "4250199"
  },
  {
    "text": "can choose a chunking strategy whatever one you want but I like this idea of referencing full and basically passing",
    "start": "4250199",
    "end": "4256600"
  },
  {
    "text": "full documents to the LM for the answer itself it gets around that problem of an overly aggressive chunking strategy that",
    "start": "4256600",
    "end": "4263400"
  },
  {
    "text": "misses context needed to actually answer your question and I think with long context LMS getting cheaper and cheaper",
    "start": "4263400",
    "end": "4268800"
  },
  {
    "text": "this is starting to make more sense um I'm trying to think I had a whole kind of like slide on uh yeah th this one is",
    "start": "4268800",
    "end": "4275679"
  },
  {
    "text": "kind of like balancing system complexity and latency so it's kind of like on the",
    "start": "4275679",
    "end": "4280800"
  },
  {
    "text": "left is like maybe the historical view like you need the exact relevant chunk you can get really complex chunking",
    "start": "4280800",
    "end": "4286719"
  },
  {
    "text": "schemes a lot of over engineering lower recall like if you're passing 100 token chunks to your LM for the final answer",
    "start": "4286719",
    "end": "4293480"
  },
  {
    "text": "you might miss the exact you know part of the document that's necessary um very sensitive to chunk size K all these",
    "start": "4293480",
    "end": "4299840"
  },
  {
    "text": "weird parameters right on the Other Extreme just throw in everything throw everything into context Google actually",
    "start": "4299840",
    "end": "4306960"
  },
  {
    "text": "I think this week will probably announce some interesting stuff with context caching seems really cool maybe that",
    "start": "4306960",
    "end": "4312280"
  },
  {
    "text": "actually could be a really good option for this but h higher latency higher token usage can't audit retrieval",
    "start": "4312280",
    "end": "4318560"
  },
  {
    "text": "security and authentication like if you're passing 10 million tokens of context in for your answer generation so",
    "start": "4318560",
    "end": "4325600"
  },
  {
    "text": "something in the middle is what I'm advocating for and I think uh this kind of document level uh",
    "start": "4325600",
    "end": "4333520"
  },
  {
    "text": "decoupling where basically you index and reference full documents and pass full",
    "start": "4333520",
    "end": "4338600"
  },
  {
    "text": "documents to your llm is like a nice trick um we've seen a lot of people use",
    "start": "4338600",
    "end": "4343920"
  },
  {
    "text": "this pretty effectively so so",
    "start": "4343920",
    "end": "4348440"
  },
  {
    "text": "yeah okay that's interesting so the question was how can you evaluate the",
    "start": "4364719",
    "end": "4371719"
  },
  {
    "text": "amount of context you're using",
    "start": "4371719",
    "end": "4376840"
  },
  {
    "text": "okay uh okay but I guess so in a rag context you have a question you have an",
    "start": "4380239",
    "end": "4386120"
  },
  {
    "text": "answer so you have a question you have an answer and you have sum retrieve documents so you can",
    "start": "4386120",
    "end": "4393000"
  },
  {
    "text": "evaluate the question relative to your document that's one way to get at this",
    "start": "4393000",
    "end": "4398960"
  },
  {
    "text": "like how much of the document is relevant to your question so that's maybe one approach and actually the",
    "start": "4398960",
    "end": "4405080"
  },
  {
    "text": "notebook show a few prompts to kind of get at that um and actually a good way to think",
    "start": "4405080",
    "end": "4411920"
  },
  {
    "text": "about that is you can you can think about document precision and document recall and this is a little confusing",
    "start": "4411920",
    "end": "4418000"
  },
  {
    "text": "maybe so I should explain it basically document recall is does a document",
    "start": "4418000",
    "end": "4423199"
  },
  {
    "text": "contain the answer to my question anywhere so let's say you a 100 page document on page 55 is my answer recalls",
    "start": "4423199",
    "end": "4429760"
  },
  {
    "text": "one it's in there Precision is the other side of that coin which is doesn't contain information not relev to my",
    "start": "4429760",
    "end": "4434840"
  },
  {
    "text": "question question in that particular case huge amount of irrelevant information so recall be one Precision will be very low so that's one thing you",
    "start": "4434840",
    "end": "4442600"
  },
  {
    "text": "can do you can actually look at your retrieve docs measure precision and recall that's like one thing I think I would probably like to do there and",
    "start": "4442600",
    "end": "4449040"
  },
  {
    "text": "that's probably the best way to get at this question of like how much of my doc am I actually using now with this",
    "start": "4449040",
    "end": "4455920"
  },
  {
    "text": "approach your recall will be high your Precision will be kind of low and you would say I don't care it's fine if I",
    "start": "4455920",
    "end": "4461040"
  },
  {
    "text": "have a model it's super cheap to use large number of tokens okay maybe I'll frame it another way I",
    "start": "4461040",
    "end": "4468840"
  },
  {
    "text": "care more about recall than Precision I want to make sure I always I answer the question if I pass a little bit more",
    "start": "4468840",
    "end": "4473880"
  },
  {
    "text": "context than necessary I'm okay with that versus if you're a Precision gated system then you would say okay I'm going",
    "start": "4473880",
    "end": "4480239"
  },
  {
    "text": "to miss the answer sometimes so I'm okay with that because I never want to pass more context as necessary I think a lot",
    "start": "4480239",
    "end": "4486159"
  },
  {
    "text": "of people are moving towards higher recall because these LMS are getting cheaper and they can process more",
    "start": "4486159",
    "end": "4491679"
  },
  {
    "text": "context so that's kind of how I might think about it and I think this approach is a nice idea of you know indexing",
    "start": "4491679",
    "end": "4498880"
  },
  {
    "text": "indexing full documents or or indexing chunks but then referencing full documents passing full documents to your LM to actually generate",
    "start": "4498880",
    "end": "4506520"
  },
  {
    "text": "answers so yep",
    "start": "4506520",
    "end": "4511080"
  },
  {
    "text": "and then user",
    "start": "4525320",
    "end": "4528840"
  },
  {
    "text": "yep yeah okay so this was a great question the question was related to kind of like a gentic rag",
    "start": "4574440",
    "end": "4582440"
  },
  {
    "text": "and where to actually put documents so let's walk through the cases so case one",
    "start": "4582440",
    "end": "4590000"
  },
  {
    "text": "is I have a fixed context for every question you're just going to ask let's say I have a like a",
    "start": "4590000",
    "end": "4596400"
  },
  {
    "text": "rag bot against like one particular document and that document is always going to be referenced so that you make",
    "start": "4596400",
    "end": "4603320"
  },
  {
    "text": "a very good point in that case what I would do is let's go back to our like",
    "start": "4603320",
    "end": "4608400"
  },
  {
    "text": "agent example you can put that in the system prompt itself like you said so",
    "start": "4608400",
    "end": "4613600"
  },
  {
    "text": "and actually I'll I'll mention something else about this it' be like uh right here so here's your system prompt for",
    "start": "4613600",
    "end": "4619080"
  },
  {
    "text": "your agent you just Plum that whole document in you say every question you're going to reference this document",
    "start": "4619080",
    "end": "4625360"
  },
  {
    "text": "you don't need retriever system you're done that's a very nice case no to retrieval complexity you just Conta",
    "start": "4625360",
    "end": "4631040"
  },
  {
    "text": "stuff your whole document the thing that Google is going to announce this week I believe this",
    "start": "4631040",
    "end": "4637440"
  },
  {
    "text": "context caching seems really interesting for this because basically what they're saying is if I get it right I think",
    "start": "4637440",
    "end": "4643560"
  },
  {
    "text": "Logan will speak to this but and I think they have a context when a million to 10 million tokens is huge",
    "start": "4643560",
    "end": "4649719"
  },
  {
    "text": "right so basically you can take a large set of documents and stuff them into this model effectively and they house",
    "start": "4649719",
    "end": "4655639"
  },
  {
    "text": "them for you somehow I think you've some minor data storage fee but then for",
    "start": "4655639",
    "end": "4661120"
  },
  {
    "text": "every inference call they don't charge you for all those tokens that are cash",
    "start": "4661120",
    "end": "4666400"
  },
  {
    "text": "which is pretty nice so basically here's the use case to your point exactly I have some set of documentation it's 10",
    "start": "4666400",
    "end": "4673080"
  },
  {
    "text": "million tokens that's a lot of pages hundreds of pages I have it cashed with the model and every time that user ask a",
    "start": "4673080",
    "end": "4678880"
  },
  {
    "text": "question I don't get charged you know 10 millions of tokens to process the answer they just cashed for me really nice idea",
    "start": "4678880",
    "end": "4685120"
  },
  {
    "text": "so that's your point of like the first thing that's like not quite in the system prompt it's in like the cash but that's the same idea so you have cached",
    "start": "4685120",
    "end": "4692120"
  },
  {
    "text": "or system prompt fixed context that's like case one so case two is you have",
    "start": "4692120",
    "end": "4698159"
  },
  {
    "text": "you want a dyamic retrieve so you don't you can't stuff your context maybe you have a few different Vector stores like we were talking about here with routing",
    "start": "4698159",
    "end": "4705120"
  },
  {
    "text": "in that case yeah you have to use a you have to use an index of some sort maybe a router to choose which index to",
    "start": "4705120",
    "end": "4711040"
  },
  {
    "text": "retrieve from so that's kind of case too um I'm trying to remember um the oh okay",
    "start": "4711040",
    "end": "4719080"
  },
  {
    "text": "yeah so like in that particular case for follow-up questions how do I kind of control",
    "start": "4719080",
    "end": "4725600"
  },
  {
    "text": "whether I r- retrieve or not so that's a nice thing about either one of these",
    "start": "4725600",
    "end": "4730840"
  },
  {
    "text": "agents it has some state so the state lives across life to an agent so basically the agent and this actually",
    "start": "4730840",
    "end": "4737880"
  },
  {
    "text": "gets exactly to what this the the other question was on let's say I built my agent with uh",
    "start": "4737880",
    "end": "4743760"
  },
  {
    "text": "I'll show you right here so let's say I have a router node at the start of my agent okay and that that",
    "start": "4743760",
    "end": "4751920"
  },
  {
    "text": "router has access to State what I can do is then given a question this could be let's say it's a multi-turn thing this",
    "start": "4751920",
    "end": "4757600"
  },
  {
    "text": "is the second question in my in my conversation I have an appended state from the rest of my discussion here um",
    "start": "4757600",
    "end": "4763679"
  },
  {
    "text": "the agent knows it return an answer so um basically when a new question comes",
    "start": "4763679",
    "end": "4769199"
  },
  {
    "text": "in you could pass like the entire State back to that router and the router could know okay here's the docs I've already",
    "start": "4769199",
    "end": "4775600"
  },
  {
    "text": "retrieved and it can basically then decide to answer directly because I already have the answer to the question",
    "start": "4775600",
    "end": "4781840"
  },
  {
    "text": "so that's a long way of saying you can use State either message history or",
    "start": "4781840",
    "end": "4787280"
  },
  {
    "text": "explicitly defined in in your Lang graph agent to preserve docs you've retrieved",
    "start": "4787280",
    "end": "4792639"
  },
  {
    "text": "already and then to just use them to answer the question without re- retrieving so that's kind of what these",
    "start": "4792639",
    "end": "4798080"
  },
  {
    "text": "graph these these rag agents can be really good at that was kind of like storing that in short-term memory and",
    "start": "4798080",
    "end": "4804159"
  },
  {
    "text": "reasoning about hey do I need to r- retrieve or not so that's exactly the intuition behind why these rag agents",
    "start": "4804159",
    "end": "4809639"
  },
  {
    "text": "can be pretty nice yeah",
    "start": "4809639",
    "end": "4814560"
  },
  {
    "text": "yeah yeah yeah exactly okay this is a really good this is a really good discussion",
    "start": "4874120",
    "end": "4880000"
  },
  {
    "text": "and whole debate so the highest level framing of this is how do you want to um",
    "start": "4880000",
    "end": "4887080"
  },
  {
    "text": "how do you want your model to learn so one option is you can modify the weights of the model itself with something like",
    "start": "4887080",
    "end": "4892199"
  },
  {
    "text": "fine tuning another is you can use what we call like in context learning through your prompt so rag is like a form of in",
    "start": "4892199",
    "end": "4899400"
  },
  {
    "text": "context learning I'm basically giving it some documents it's reason about those documents producing answers not touching the weights of my model fine tunu be",
    "start": "4899400",
    "end": "4906400"
  },
  {
    "text": "taking the knowledge I want to run rag on fine-tuning your model and updating the weight so it has that knowledge",
    "start": "4906400",
    "end": "4912320"
  },
  {
    "text": "there's a lot of debates on this and the and actually I think haml has a whole",
    "start": "4912320",
    "end": "4917679"
  },
  {
    "text": "course on finetuning oh yeah do you have",
    "start": "4917679",
    "end": "4921719"
  },
  {
    "text": "aun okay",
    "start": "4937760",
    "end": "4941760"
  },
  {
    "text": "yeah yep I got it okay I'll I'll repeat that so the clarification was thinking",
    "start": "4970440",
    "end": "4977400"
  },
  {
    "text": "about using fine-tuning more to kind of to uh govern the behavior of the agent",
    "start": "4977400",
    "end": "4984320"
  },
  {
    "text": "rather than to kind of encode facts which is a good it's a very good clarification because I was going to say",
    "start": "4984320",
    "end": "4991280"
  },
  {
    "text": "using fine tun to encode facts I think a lot of kind of literature has pointed to that being a bad idea for a lot of reasons it's costly you have to",
    "start": "4991280",
    "end": "4998000"
  },
  {
    "text": "continually find tunit facts change so let's dispatch that I think that's a kind of not a great idea but you make a",
    "start": "4998000",
    "end": "5003960"
  },
  {
    "text": "very interesting point about fine-tuning to govern Behavior now there's a paper",
    "start": "5003960",
    "end": "5009639"
  },
  {
    "text": "called raft that came out kind of recently and actually as far as my understanding as I haven't played with",
    "start": "5009639",
    "end": "5014719"
  },
  {
    "text": "it at life it's fine-tuning to kind of do what our notebooks show today of this kind of",
    "start": "5014719",
    "end": "5020120"
  },
  {
    "text": "like look at the documents they retrieved reason if they're relevant and",
    "start": "5020120",
    "end": "5025360"
  },
  {
    "text": "then don't use them if they're not automatically filter it's doing exactly what we're doing in this langra thing",
    "start": "5025360",
    "end": "5030480"
  },
  {
    "text": "but it's kind of achieving that same outcome through process fine tuning so that's a very good insight you're right it seems promising to have these like",
    "start": "5030480",
    "end": "5037440"
  },
  {
    "text": "kind of fine-tuned rag agents so to speak or or it wouldn't be an agent be an LM fine tuned for rag that",
    "start": "5037440",
    "end": "5044679"
  },
  {
    "text": "incorporates this kind of logical reasoning or what you're saying like maybe some kind of reasoning about if",
    "start": "5044679",
    "end": "5051400"
  },
  {
    "text": "you have a multi-term conversation like avoid recency bias whatever it is that",
    "start": "5051400",
    "end": "5056600"
  },
  {
    "text": "seems like a very good Trend and interesting um the challenge is a little",
    "start": "5056600",
    "end": "5061920"
  },
  {
    "text": "bit if you're fine tuning yourself fine tuning is hard and somewhat Advanced and all that alternatively if it's like a",
    "start": "5061920",
    "end": "5069719"
  },
  {
    "text": "very Niche use case like like the raft system basically if fine tunes to kind of do this as models get changed or",
    "start": "5069719",
    "end": "5077719"
  },
  {
    "text": "update all the time you kind of need to keep your fine tune model up to date if you see what I'm saying so I think I'm",
    "start": "5077719",
    "end": "5083440"
  },
  {
    "text": "still a little bit queasy about using fine tuning even in that context because of the challenge of keeping it kind of",
    "start": "5083440",
    "end": "5089840"
  },
  {
    "text": "up to date with the state of thee art but it's interesting I think the raft paper is a good ref in this direction",
    "start": "5089840",
    "end": "5095719"
  },
  {
    "text": "and it does exactly what we do in this Workshop but I believe it fine-tunes this into the model or attempts to which",
    "start": "5095719",
    "end": "5101639"
  },
  {
    "text": "is a very intuitive thing to think about basically let the model reflect automatically on retriev documents and",
    "start": "5101639",
    "end": "5107280"
  },
  {
    "text": "like automatically filter them for you seems like it should be able to do that um seems like a good idea so but my",
    "start": "5107280",
    "end": "5114560"
  },
  {
    "text": "hesitation would still be like well if I want to switch my models I need to like refine tune I want to use llama 3 I to",
    "start": "5114560",
    "end": "5120360"
  },
  {
    "text": "fine tune llama 3 on this task I can't use propri maybe I can finetune you know gbd4 might I find tuning now I'm not",
    "start": "5120360",
    "end": "5126360"
  },
  {
    "text": "even sure I could find tun GB you know so again if I find tuning myself that's hard it still feels a little bit like",
    "start": "5126360",
    "end": "5133119"
  },
  {
    "text": "I'd rather just sit up a simple like orchestrated agent that does it rather than rely on find tuning is my",
    "start": "5133119",
    "end": "5138840"
  },
  {
    "text": "sense",
    "start": "5138840",
    "end": "5141840"
  },
  {
    "text": "yeah yeah the provider",
    "start": "5147560",
    "end": "5152000"
  },
  {
    "text": "right right",
    "start": "5158480",
    "end": "5165280"
  },
  {
    "text": "right",
    "start": "5167159",
    "end": "5170159"
  },
  {
    "text": "yep right right",
    "start": "5177320",
    "end": "5185040"
  },
  {
    "text": "right yeah okay so that's a very good point I think this is this also a very big debate so openi just did an",
    "start": "5198639",
    "end": "5205600"
  },
  {
    "text": "acquisition this week on a retrieval company I forget the name Rock rocket I believe so I think they are moving more",
    "start": "5205600",
    "end": "5212239"
  },
  {
    "text": "in the direction of retrieval I could AB see them offering you know an API that potentially does reval for you and",
    "start": "5212239",
    "end": "5219239"
  },
  {
    "text": "incorporate some of these ideas for you so how much do this does get pushed behind apis and they take care of",
    "start": "5219239",
    "end": "5224960"
  },
  {
    "text": "whatever is necessary behind the scenes for you that could absolutely be the case I would not be surprised if at all",
    "start": "5224960",
    "end": "5230440"
  },
  {
    "text": "they move in that direction and I think there's always you know it's an interesting trade-off like how much are",
    "start": "5230440",
    "end": "5235480"
  },
  {
    "text": "you willing to kind of um you know abstract behind an API versus not I",
    "start": "5235480",
    "end": "5241320"
  },
  {
    "text": "think there's always a lot of companies developers that want to kind of control everything themselves and build it themselves have full transparency and",
    "start": "5241320",
    "end": "5246960"
  },
  {
    "text": "others that don't and so you know it's an interesting question but of course for certain functionalities multimodality very few",
    "start": "5246960",
    "end": "5253840"
  },
  {
    "text": "people are going to stand that up themselves you you kind of let that live behind an API so what do you allowed to live behind an API or not my only",
    "start": "5253840",
    "end": "5260560"
  },
  {
    "text": "concern is I think for some of these kind of things they could be very domain specific like what you consider relevant",
    "start": "5260560",
    "end": "5267239"
  },
  {
    "text": "or not could be very relevant to you in your application you kind of want to be able to control that that's the only thing I can",
    "start": "5267239",
    "end": "5272840"
  },
  {
    "text": "imagine it could be kind of hard to abstract that all behind an API which I think is maybe why open I hasn't done",
    "start": "5272840",
    "end": "5279760"
  },
  {
    "text": "too much in retrieval yet it's just a hard Beast I know they've been trying for a while um I don't know it's a great",
    "start": "5279760",
    "end": "5286679"
  },
  {
    "text": "debate though yeah we can discuss more if uh yeah it's a good topic for",
    "start": "5286679",
    "end": "5292679"
  },
  {
    "text": "sure yep window oh yeah so there's a problem",
    "start": "5292679",
    "end": "5299280"
  },
  {
    "text": "of like lost in the middle for example the Precision like in the context say 95% isant",
    "start": "5299280",
    "end": "5307159"
  },
  {
    "text": "information 5% somewhere the",
    "start": "5307159",
    "end": "5310639"
  },
  {
    "text": "midle yeah yes so questions on Lost in the middle of long context I actually did a whole study on this with Greg",
    "start": "5313639",
    "end": "5320880"
  },
  {
    "text": "camand yeah it's a really interesting topic um so the Insight was basically",
    "start": "5320880",
    "end": "5327400"
  },
  {
    "text": "that long context LMS tend to have lower recall or like you know factual recall",
    "start": "5327400",
    "end": "5334080"
  },
  {
    "text": "for things in the middle of the context okay so that was one observation at least that's what they're",
    "start": "5334080",
    "end": "5339639"
  },
  {
    "text": "paper reported so I actually looked at this with Greg and we did something a",
    "start": "5339639",
    "end": "5344800"
  },
  {
    "text": "little bit even harder we actually tested for multiple fact retrieval so we tested can you retrieve one three or 10",
    "start": "5344800",
    "end": "5351639"
  },
  {
    "text": "different facts from the context and this was using gbd4 uh gbd4 turbo single turn um and on",
    "start": "5351639",
    "end": "5361520"
  },
  {
    "text": "the x-axis you can see the fraction of of the needles that it basically can get",
    "start": "5361520",
    "end": "5366560"
  },
  {
    "text": "and then on the Y is the number of needles so basically one needle um three",
    "start": "5366560",
    "end": "5371880"
  },
  {
    "text": "needles 10 needles green versus red is basically just retrieving versus retrieving and reasoning so it's like",
    "start": "5371880",
    "end": "5378280"
  },
  {
    "text": "reasoning is a little bit harder than than than just retrieving um these needles were",
    "start": "5378280",
    "end": "5383679"
  },
  {
    "text": "actually Pizza ingredients so basically the background was you know this was",
    "start": "5383679",
    "end": "5388760"
  },
  {
    "text": "120,000 tokens of Paul Graham essays and three secret piece ingredients are however many one three or 10 but I",
    "start": "5388760",
    "end": "5395920"
  },
  {
    "text": "injected in that context and I basically as the LM what's the ingredients need to build the secret pizza so it have to",
    "start": "5395920",
    "end": "5401440"
  },
  {
    "text": "find them in there and basically as you ramp up the number of needles go from 1 to 10 um it gets worse so with 10 it's",
    "start": "5401440",
    "end": "5411080"
  },
  {
    "text": "actually retrieval itself is only like 60% so then I looked at okay well where",
    "start": "5411080",
    "end": "5416440"
  },
  {
    "text": "is it failing and that's what I look at here in this heat map so basically this is telling you like how long the context",
    "start": "5416440",
    "end": "5421840"
  },
  {
    "text": "is so a th tokens all the way up to 12 120,000 and then here's like the needle",
    "start": "5421840",
    "end": "5427440"
  },
  {
    "text": "placement so 1 to 10 so this red means you couldn't retrieve it and what I found is actually it doesn't get them",
    "start": "5427440",
    "end": "5434360"
  },
  {
    "text": "towards the start of the document so the retrieval gets worse if the needle at the front so it's like this I read a",
    "start": "5434360",
    "end": "5440040"
  },
  {
    "text": "book I asked you a question about the first chapter you forgot because I read that a month ago or something same idea",
    "start": "5440040",
    "end": "5446080"
  },
  {
    "text": "and actually I put this on Twitter and then someone said oh yeah it's probably recency bias and that's a good point that basically the most informative",
    "start": "5446080",
    "end": "5452840"
  },
  {
    "text": "tokens in next token prediction are often the more recent on so you know basically LMS learn a bias to attend to",
    "start": "5452840",
    "end": "5458880"
  },
  {
    "text": "recent tokens and that's not good for rag so that is all to say I'm a little",
    "start": "5458880",
    "end": "5466199"
  },
  {
    "text": "wary about long context retrieval I wouldn't quite Trust basically high",
    "start": "5466199",
    "end": "5471320"
  },
  {
    "text": "quality rag across a million tokens of context you can see look if it's a thousand tokens no problem if it's",
    "start": "5471320",
    "end": "5477520"
  },
  {
    "text": "12,000 tokens of context you know it depends a lot on where those facts are if they're towards",
    "start": "5477520",
    "end": "5482719"
  },
  {
    "text": "the start you actually can have much lower recall and so that's a real risk which is kind of why goes back to this",
    "start": "5482719",
    "end": "5488560"
  },
  {
    "text": "whole thing of uh like I don't really buy just stuffing everything into context that far right",
    "start": "5488560",
    "end": "5494639"
  },
  {
    "text": "side I think there's too many issues with bad recall recency bias like you said uh and so I think until we have",
    "start": "5494639",
    "end": "5503000"
  },
  {
    "text": "very and by the way I also don't really trust you know when they show those Needle on the Hast stack charts it's like perfect I don't trust any of that I",
    "start": "5503000",
    "end": "5510159"
  },
  {
    "text": "I did my own study I found there's like a lot of errors and I think it depends a lot on um a couple different things one",
    "start": "5510159",
    "end": "5517159"
  },
  {
    "text": "how many needles so in this case you see with one it's okay with 10 it's really bad right so how many needles and then I",
    "start": "5517159",
    "end": "5523600"
  },
  {
    "text": "saw an interesting study saying that like the the difference in the needles relative to your context makes it easier",
    "start": "5523600",
    "end": "5529679"
  },
  {
    "text": "so like in these studies it's like pizza ingredients in Paul gr m is really different but if it's just like related",
    "start": "5529679",
    "end": "5535840"
  },
  {
    "text": "slightly it's actually harder still so that is to say I don't really trust an NE and HX studies I don't particularly",
    "start": "5535840",
    "end": "5542840"
  },
  {
    "text": "Trust stuffing you know passing a million tokens on context and counting that and",
    "start": "5542840",
    "end": "5549080"
  },
  {
    "text": "counting on that to just work effectively I'd be very wary about that that's kind of my thing but you know in",
    "start": "5549080",
    "end": "5554639"
  },
  {
    "text": "these studies look a thousand tokens and cons already you know that's that is uh if you're stuffing a thousand tokens h i",
    "start": "5554639",
    "end": "5560719"
  },
  {
    "text": "mean that's actually still pretty small so yeah I I just be wary about very retrieval from very large contexts Yeahs",
    "start": "5560719",
    "end": "5570800"
  },
  {
    "text": "yeah okay yeah that's a great question with agents the number of tools this is a really big issue I hear",
    "start": "5575320",
    "end": "5582080"
  },
  {
    "text": "mentioned a lot um so if you recall if you go back to the agent stuff",
    "start": "5582080",
    "end": "5589400"
  },
  {
    "text": "um so you're basically binding some set of tools to your llm right and that's",
    "start": "5589719",
    "end": "5595480"
  },
  {
    "text": "what we show here right uh I've seen a lot of issues with",
    "start": "5595480",
    "end": "5601360"
  },
  {
    "text": "a large number of tools so I don't exactly know what the the exact cut off is but this is one of the big problems",
    "start": "5601360",
    "end": "5609280"
  },
  {
    "text": "with open-ended tool calling agents is if I am basically selecting from 20",
    "start": "5609280",
    "end": "5615159"
  },
  {
    "text": "different tools I actually I maybe the Berkeley leaderboard has data on this if",
    "start": "5615159",
    "end": "5620920"
  },
  {
    "text": "someone knows feel free to mention it but uh reliability of tool calling even with",
    "start": "5620920",
    "end": "5626600"
  },
  {
    "text": "a small number of tools like on the order of five is already challenging if you're talking about hundreds of tools",
    "start": "5626600",
    "end": "5631719"
  },
  {
    "text": "or dozens of tools I don't think there's really yeah I think it's quite challenging and",
    "start": "5631719",
    "end": "5637840"
  },
  {
    "text": "which is why I've seen more success in a not using these openend St tool calling",
    "start": "5637840",
    "end": "5645440"
  },
  {
    "text": "agents laying it out more explicitly as a lang graph where uh the tool calls",
    "start": "5645440",
    "end": "5650760"
  },
  {
    "text": "live inside nodes and you're not relying on your agent to pick from like 20 different tools so you can lay out more",
    "start": "5650760",
    "end": "5657119"
  },
  {
    "text": "of like a control flow where um you route to different tool nodes based upon the logic so so that's kind of one thing",
    "start": "5657119",
    "end": "5663800"
  },
  {
    "text": "I've seen another thing I've seen is maybe multi-agent type things where you have different agents with subtask with each having like maybe a small number of",
    "start": "5663800",
    "end": "5670280"
  },
  {
    "text": "tools but basically what I've seen is it seems to be",
    "start": "5670280",
    "end": "5675639"
  },
  {
    "text": "that okay maybe there two things selection from large tools is definitely challenging one of the most interesting",
    "start": "5675639",
    "end": "5681880"
  },
  {
    "text": "things I saw is something like um you can use something like rag where basically you can take a description of",
    "start": "5681880",
    "end": "5687000"
  },
  {
    "text": "your tools create a natural language description embed it and then use basic like semantic similarity search query",
    "start": "5687000",
    "end": "5694080"
  },
  {
    "text": "versus the embedded summaries to select using semantics that's actually not a bad idea I would actually use that more",
    "start": "5694080",
    "end": "5700159"
  },
  {
    "text": "than I would trust in LM to just like do the tool selection from a list of 100 that's not going to work so actually I think that like rag for Tool selection",
    "start": "5700159",
    "end": "5707400"
  },
  {
    "text": "is a cool idea I was going to do a little like test that out and do a little tutorial so actually maybe I'll",
    "start": "5707400",
    "end": "5712960"
  },
  {
    "text": "just make a note of that uh that's a that's a great a great",
    "start": "5712960",
    "end": "5718000"
  },
  {
    "text": "question uh to do rag for many tools",
    "start": "5718000",
    "end": "5725560"
  },
  {
    "text": "right yeah well I think I think using semantic similarity",
    "start": "5740320",
    "end": "5746679"
  },
  {
    "text": "for Tool selection is is a good idea definitely about data qu what do you",
    "start": "5746679",
    "end": "5752320"
  },
  {
    "text": "mean by data quering though um",
    "start": "5752320",
    "end": "5760199"
  },
  {
    "text": "PLS inform right um let make sure I understand I",
    "start": "5763560",
    "end": "5769280"
  },
  {
    "text": "think the way I would think about it is so you know how in your in the notebook like in the code for all of our tools",
    "start": "5769280",
    "end": "5776760"
  },
  {
    "text": "right you have this little tool description right like retrieve",
    "start": "5776760",
    "end": "5782440"
  },
  {
    "text": "documents grade them run web search I would actually write verbose",
    "start": "5782440",
    "end": "5788560"
  },
  {
    "text": "descriptions for all my tools and index those descriptions right or embed them and then I would do and I would probably",
    "start": "5788560",
    "end": "5795239"
  },
  {
    "text": "create like very very verbose high quality summaries of what the tool actually does and then do semantic similarity search against those",
    "start": "5795239",
    "end": "5802239"
  },
  {
    "text": "summaries I think that could actually I haven't done that yet but I think that could work really well um because it's a",
    "start": "5802239",
    "end": "5808920"
  },
  {
    "text": "very tall task asking LM to differentiate between like 20 different tools whereas",
    "start": "5808920",
    "end": "5814159"
  },
  {
    "text": "you could do something like sematic similarity that would actually probably be very effective",
    "start": "5814159",
    "end": "5821920"
  },
  {
    "text": "yep right",
    "start": "5835119",
    "end": "5839119"
  },
  {
    "text": "yeah yeah yeah so the question was about um multi-agent context when you want to",
    "start": "5850480",
    "end": "5857880"
  },
  {
    "text": "orchestrate a large number of tools having sub agents with specializations that manage some small",
    "start": "5857880",
    "end": "5863599"
  },
  {
    "text": "set of tools each and how do you kind of move between them so if you look at our Lang graph repo we do have a",
    "start": "5863599",
    "end": "5871159"
  },
  {
    "text": "subdirectory it's under um it's under uh Lang graph examples multi-agent we have a few different",
    "start": "5871159",
    "end": "5877239"
  },
  {
    "text": "notebooks that have multi-agent style kind of layouts which I would encourage you to look at I haven't personally done",
    "start": "5877239",
    "end": "5882639"
  },
  {
    "text": "too much work on it um it seems promising but I haven't played with it",
    "start": "5882639",
    "end": "5889199"
  },
  {
    "text": "um multi-agent in general and these all reference papers so you can also look at",
    "start": "5889199",
    "end": "5894280"
  },
  {
    "text": "the papers but multi-agent in general for production setting feels quite aggressive uh although that said as far",
    "start": "5894280",
    "end": "5900599"
  },
  {
    "text": "as I understand I remember looking at the code a while ago Devon and some of the software agents do use like multi-agent style",
    "start": "5900599",
    "end": "5906239"
  },
  {
    "text": "setups um so maybe have a look at the Devon repo",
    "start": "5906239",
    "end": "5911880"
  },
  {
    "text": "or there's open Devon uh have a look at these notebooks those could all be useful if you want to learn more about",
    "start": "5911880",
    "end": "5918920"
  },
  {
    "text": "multi-agent yep I'm",
    "start": "5918920",
    "end": "5924080"
  },
  {
    "text": "wondering ipped for yeah sure Wonder like we've been talking a",
    "start": "5924080",
    "end": "5930560"
  },
  {
    "text": "lot about kind of theity",
    "start": "5930560",
    "end": "5934920"
  },
  {
    "text": "right an agent That's a classic question yeah so",
    "start": "5944760",
    "end": "5952920"
  },
  {
    "text": "the question was when do I use a chain versus an agent so that's very good so",
    "start": "5952920",
    "end": "5958280"
  },
  {
    "text": "we kind of touched on a little bit uh kind of here so I think the the intuition behind why",
    "start": "5958280",
    "end": "5964920"
  },
  {
    "text": "where and why an agent can make sense is simply that sometimes you want your",
    "start": "5964920",
    "end": "5970480"
  },
  {
    "text": "application control flow to be variable and if you want some flexibility within your application an",
    "start": "5970480",
    "end": "5977199"
  },
  {
    "text": "agent is a nice idea and so all this self-corrective type stuff we're talking about the corrective rag thing those are",
    "start": "5977199",
    "end": "5983719"
  },
  {
    "text": "all kind of agentic flows where the control flow depends upon the grading of",
    "start": "5983719",
    "end": "5989040"
  },
  {
    "text": "the documents and so um you",
    "start": "5989040",
    "end": "5994760"
  },
  {
    "text": "know historically people have largely been building chains and chains are very reliable and they're easy to ship and",
    "start": "5994760",
    "end": "6000760"
  },
  {
    "text": "all that I think with things like langra and of course I work at Lang chain so I'll speak my book about langra but I've",
    "start": "6000760",
    "end": "6006560"
  },
  {
    "text": "really used it quite a bit and I found it to be very reliable and I we are seeing a lot of people starting to deploy with it because you can actually",
    "start": "6006560",
    "end": "6012679"
  },
  {
    "text": "ship and deploy a reliable Asian with L graph and so I think a blocker to the ability to kind of ship agents has been",
    "start": "6012679",
    "end": "6019440"
  },
  {
    "text": "reliability I think we're I would actually encourage you to play with the notebooks and look at L graph because because um it does allow you to have",
    "start": "6019440",
    "end": "6025840"
  },
  {
    "text": "that kind of reliability whether it be necessary to to ship something in production and we do have customers that have langra in production whereas a",
    "start": "6025840",
    "end": "6032000"
  },
  {
    "text": "react agent production is not",
    "start": "6032000",
    "end": "6036960"
  },
  {
    "text": "recommended",
    "start": "6037520",
    "end": "6040520"
  },
  {
    "text": "yeah 100%",
    "start": "6042840",
    "end": "6046840"
  },
  {
    "text": "so okay so I I think yeah so the different why would you ever want kind of like an agent be it langra reactor",
    "start": "6053639",
    "end": "6060040"
  },
  {
    "text": "otherwise versus not and I again I think it goes back to do you want your application to have any kind of",
    "start": "6060040",
    "end": "6065679"
  },
  {
    "text": "adaptability so okay here's one we can talk about routing I have three different Vector stores I want to BU",
    "start": "6065679",
    "end": "6071280"
  },
  {
    "text": "able route between them that is kind of a quote unquote atic use case because the control flow depends on the question",
    "start": "6071280",
    "end": "6076800"
  },
  {
    "text": "so that's one you might want routing you may want self-correction so that's kind of what we talked about here A whole bunch with the corrective rag stuff so",
    "start": "6076800",
    "end": "6082880"
  },
  {
    "text": "you want any want self correction um I mean those are two obvious ones in the context of rag",
    "start": "6082880",
    "end": "6089760"
  },
  {
    "text": "itself um I mean that's one thing I've often found problem with rag systems is is the routing thing is a real issue",
    "start": "6089760",
    "end": "6097040"
  },
  {
    "text": "like you want your to be flexible enough to deal with questions or out of domain for your vector store and uh you need",
    "start": "6097040",
    "end": "6102520"
  },
  {
    "text": "some you need some kind of dynamism in your application to handle that so",
    "start": "6102520",
    "end": "6107599"
  },
  {
    "text": "looking at the question saying okay just answer this directly don't don't use a vector store so those are like the most popular",
    "start": "6107599",
    "end": "6113800"
  },
  {
    "text": "one self correction or routing yeah yep I'm wondering if",
    "start": "6113800",
    "end": "6118880"
  },
  {
    "text": "there's anything to be said about building evaluation data sets like answer pairs are so domain specific I'm",
    "start": "6118880",
    "end": "6124719"
  },
  {
    "text": "wondering if there are like General best practices mental models like things to think about sitting down to build",
    "start": "6124719",
    "end": "6130520"
  },
  {
    "text": "evaluation dat yeah yeah okay so the question was about kind of building eval data sets Okay that's a great question",
    "start": "6130520",
    "end": "6137000"
  },
  {
    "text": "it's often a very very challenging part of app development um so if if you have a rag application that's domains",
    "start": "6137000",
    "end": "6143199"
  },
  {
    "text": "specific then often times you have some set of canonical question answer pairs you care about uh you know it's hard to sign to",
    "start": "6143199",
    "end": "6150520"
  },
  {
    "text": "find like you know very very general rules for that I think it depends on your application um I think there's kind of",
    "start": "6150520",
    "end": "6157560"
  },
  {
    "text": "this this hurdle any evaluation is better than no evaluation so small scale eval sets that",
    "start": "6157560",
    "end": "6164400"
  },
  {
    "text": "you can use and just work are already better than not doing any evaluations so",
    "start": "6164400",
    "end": "6170040"
  },
  {
    "text": "I mean for this particular case I just looked at the document now maybe maybe I'll back up and and",
    "start": "6170040",
    "end": "6176840"
  },
  {
    "text": "answer some so one thing I've seen and I've done this a little bit is you can use LM assisted QA generation so here's",
    "start": "6176840",
    "end": "6184800"
  },
  {
    "text": "one thing you can do and I've done this a little bit with Lang chain docs I can build a prompt that says",
    "start": "6184800",
    "end": "6191040"
  },
  {
    "text": "given this document produce three high quality question answer pairs from it right and I can just basically load my",
    "start": "6191040",
    "end": "6197840"
  },
  {
    "text": "documents and pass them into that LM I use a high capacity model like Sonet or 40 and have it generate QA pairs for me",
    "start": "6197840",
    "end": "6204040"
  },
  {
    "text": "and then audit them that's a nice trick I've used that actually kind of works now you have to be careful with it you only would pass it like usually one",
    "start": "6204040",
    "end": "6210440"
  },
  {
    "text": "document at a time to keep it really like you know restricted um and you audit them but actually that's a nice",
    "start": "6210440",
    "end": "6215679"
  },
  {
    "text": "way to bootstrap your evet that's like idea one and this that gets in the whole idea of synthetic data sets but if",
    "start": "6215679",
    "end": "6221119"
  },
  {
    "text": "you're building you know domain specific synthetic QA pair data sets that's a nice trick so basically use an LM to",
    "start": "6221119",
    "end": "6226560"
  },
  {
    "text": "help boost strip I think that's one idea that can help a lot um yeah and otherwise I think that",
    "start": "6226560",
    "end": "6234599"
  },
  {
    "text": "basically trying to stand up a small evaluation set for example for rag is",
    "start": "6234599",
    "end": "6240000"
  },
  {
    "text": "even this case five questions but you can already see I can get some nice insights it's very simple to set these up I have my little experiments all over",
    "start": "6240000",
    "end": "6247679"
  },
  {
    "text": "here and again it's only five questions but it gives me some immediate insights about the reliability of react versus",
    "start": "6247679",
    "end": "6253159"
  },
  {
    "text": "versus langra Agent so keep it small potentially use synthetic data start",
    "start": "6253159",
    "end": "6259040"
  },
  {
    "text": "with something and then like kind of build it out over time now a whole other thing here is we didn't talk about this",
    "start": "6259040",
    "end": "6265080"
  },
  {
    "text": "tooo much but if you have an app in production then the way this whole thing kind of comes together is you can",
    "start": "6265080",
    "end": "6271280"
  },
  {
    "text": "actually have different types of evaluators that run on your app online we call those online evaluators okay so",
    "start": "6271280",
    "end": "6278119"
  },
  {
    "text": "this is with our internal app and this gets back to the question I think he mentioned of you can have a bunch of evaluators for rag that don't require a",
    "start": "6278119",
    "end": "6284480"
  },
  {
    "text": "reference so like um I don't show it here but but basically I can look at like document retrieval quality I can",
    "start": "6284480",
    "end": "6290480"
  },
  {
    "text": "look at my answer relevance or hallucinations I can run that online I can flag cases where things did not work",
    "start": "6290480",
    "end": "6297360"
  },
  {
    "text": "well and I can actually roll those back into my eval set so if I do that uh then",
    "start": "6297360",
    "end": "6305199"
  },
  {
    "text": "I actually have this self-perpetuating Loop like kpoy talked about like the data flag wheel where actually I'm running my app in production I'm",
    "start": "6305199",
    "end": "6311880"
  },
  {
    "text": "collecting cases of bad behavior that I'm tagging with like online evaluation and I'm rolling those back into my",
    "start": "6311880",
    "end": "6317280"
  },
  {
    "text": "offline eval set so what you would do there is look at the case that the app is doing poorly in production audit them",
    "start": "6317280",
    "end": "6324520"
  },
  {
    "text": "correct them so build like a canonical question answer pair from that and put that back in your test set and that's a good way to build bootstrap and build it",
    "start": "6324520",
    "end": "6330679"
  },
  {
    "text": "up so I'd start cold start synthetic data small scale examples online",
    "start": "6330679",
    "end": "6336080"
  },
  {
    "text": "evaluation or some system to check online where it's failing Loop those back in and build it up that way that's",
    "start": "6336080",
    "end": "6341800"
  },
  {
    "text": "like your data flywheel yeah actually I even had a slide on this in one of my older talks I",
    "start": "6341800",
    "end": "6347639"
  },
  {
    "text": "used to work in self-driving for many years and I actually was a big fan of caroy stuff at Tesla and actually I've had his his thing here of this is like",
    "start": "6347639",
    "end": "6355159"
  },
  {
    "text": "the the data engine thing of like you know you ship your model you do some kind of online evaluation where's it",
    "start": "6355159",
    "end": "6361520"
  },
  {
    "text": "failing capture those failures curate them put them back in your test set run that as a loop that's like he called it",
    "start": "6361520",
    "end": "6368599"
  },
  {
    "text": "operation vacation because you can go on vacation and the model keeps getting better and this was more in the context of like training models because",
    "start": "6368599",
    "end": "6374599"
  },
  {
    "text": "basically all those those failed examples once they're labeled they become part of your your training set but the same thing applies here with llm",
    "start": "6374599",
    "end": "6381199"
  },
  {
    "text": "apps cool yeah so justed to ask you just",
    "start": "6381199",
    "end": "6391639"
  },
  {
    "text": "mentioned T to SQL and everything so I just had a",
    "start": "6391639",
    "end": "6398599"
  },
  {
    "text": "question where like for example all that all that",
    "start": "6398599",
    "end": "6404480"
  },
  {
    "text": "happens and and then we it on the database and",
    "start": "6404480",
    "end": "6410599"
  },
  {
    "text": "the result is to something so uh we cannot send it for",
    "start": "6410599",
    "end": "6416119"
  },
  {
    "text": "another to user friendly user how do we",
    "start": "6416119",
    "end": "6424320"
  },
  {
    "text": "that the is we Cann",
    "start": "6424320",
    "end": "6430159"
  },
  {
    "text": "put us yeah yeah the question was on text to",
    "start": "6431239",
    "end": "6437679"
  },
  {
    "text": "SQL um we actually have a pretty nice text to SQL agent",
    "start": "6437679",
    "end": "6443560"
  },
  {
    "text": "uh example here um so it's in langra examples um I think it's in is it SQL",
    "start": "6443560",
    "end": "6451360"
  },
  {
    "text": "where is it um I'll find it",
    "start": "6451360",
    "end": "6458840"
  },
  {
    "text": "here where is it",
    "start": "6460199",
    "end": "6465679"
  },
  {
    "text": "um tutorials oh yeah it's in tutorials um so SQL agent here um",
    "start": "6465679",
    "end": "6473440"
  },
  {
    "text": "I think A lot's in the prompting so basically in this particular case I believe anush from our team set this up",
    "start": "6473440",
    "end": "6481159"
  },
  {
    "text": "um you prompt your you can do a couple things so you can prompt your SQL agent",
    "start": "6481159",
    "end": "6488239"
  },
  {
    "text": "to um where is it it's somewhere where he tells it to yeah it's basically on all these",
    "start": "6488239",
    "end": "6495639"
  },
  {
    "text": "instructions um be very careful about uh let me just",
    "start": "6495639",
    "end": "6501760"
  },
  {
    "text": "find the but B in short you basically instruct a SQL agent when it's writing",
    "start": "6501760",
    "end": "6507000"
  },
  {
    "text": "its query to um ensure not to extract an excessive amount of context and I can't",
    "start": "6507000",
    "end": "6513520"
  },
  {
    "text": "remember exactly where it does that uh okay yeah it's right here um limit",
    "start": "6513520",
    "end": "6522440"
  },
  {
    "text": "your always use a limit statement on your query to restrict it to like whatever it is Five results and that's the hard-coded thing here and then also",
    "start": "6522440",
    "end": "6528960"
  },
  {
    "text": "in your SQL agent you can incorporate a query check node um to actually look at the query before",
    "start": "6528960",
    "end": "6536599"
  },
  {
    "text": "you execute it to sanity check for things like this so basically I would have a look at the U Lang graph examples",
    "start": "6536599",
    "end": "6543679"
  },
  {
    "text": "tutorial SQL agent notebook to have I've I've actually I ran this and evaluated",
    "start": "6543679",
    "end": "6549920"
  },
  {
    "text": "and I found it did work pretty well so that's one thing that I would look at",
    "start": "6549920",
    "end": "6556760"
  },
  {
    "text": "h yeah so the question was related to like how do you handle High cardinality",
    "start": "6591639",
    "end": "6597119"
  },
  {
    "text": "columns and I guess that is related to restricting the output size um I mean I'm actually not really a",
    "start": "6597119",
    "end": "6605560"
  },
  {
    "text": "SQL expert so I'm probably not the right person to ask about very gory details of text to SQL um",
    "start": "6605560",
    "end": "6613560"
  },
  {
    "text": "but in general I would kind of consider can you prompt the llm effectively like",
    "start": "6613560",
    "end": "6619719"
  },
  {
    "text": "okay two different things one the general ideas here where basically upfront prompting of your",
    "start": "6619719",
    "end": "6624880"
  },
  {
    "text": "llm uh to kind of follow some general kind of query formulation criteria and",
    "start": "6624880",
    "end": "6630360"
  },
  {
    "text": "then two an actual query check explicitly to review and confirm it",
    "start": "6630360",
    "end": "6636520"
  },
  {
    "text": "doesn't have some of the issues like extracting excessive context but for anything more detailed than that um I'm",
    "start": "6636520",
    "end": "6644079"
  },
  {
    "text": "probably not the person for those insights but there might be a specific Texas SQL Deep dive at some other point",
    "start": "6644079",
    "end": "6650880"
  },
  {
    "text": "in this conference and you should definitely seek that out but I haven't done that much of Texas SQL",
    "start": "6650880",
    "end": "6658079"
  },
  {
    "text": "yeah a lot",
    "start": "6658079",
    "end": "6661320"
  },
  {
    "text": "of",
    "start": "6666440",
    "end": "6669440"
  },
  {
    "text": "yeah yeah okay so the question is related to kind of document generation uh that is a really good",
    "start": "6676040",
    "end": "6683360"
  },
  {
    "text": "theme so we actually have kind of there was an interesting paper I did this a while ago I need to find it um where is",
    "start": "6683360",
    "end": "6690679"
  },
  {
    "text": "it so we have a notebook if you look in Lang graph examples um storm so this was",
    "start": "6690679",
    "end": "6697880"
  },
  {
    "text": "actually for Wiki article Generation Um here's kind of the diagram",
    "start": "6697880",
    "end": "6703079"
  },
  {
    "text": "for it we actually have a video on this too um I'm actually trying to refresh myself I did this like three or four",
    "start": "6703079",
    "end": "6708639"
  },
  {
    "text": "months ago um but it was basically a multi-agent style setup in Lang graph uh",
    "start": "6708639",
    "end": "6714280"
  },
  {
    "text": "where if I recall correctly I'm just looking at the flow here myself but basically what it did was you give it a",
    "start": "6714280",
    "end": "6720280"
  },
  {
    "text": "topic and it'll kind of do initially this kind of like generation of related",
    "start": "6720280",
    "end": "6726639"
  },
  {
    "text": "topics um and actually uses the multi-agent thing of like editors and per and um experts the experts go and do",
    "start": "6726639",
    "end": "6735199"
  },
  {
    "text": "like web research don't worry too much about the details so the point it was it was an interesting paper in flow for",
    "start": "6735199",
    "end": "6741320"
  },
  {
    "text": "Wiki article generation using Lang graph in a multi-step process and actually the wik are like pretty good I think at the",
    "start": "6741320",
    "end": "6747480"
  },
  {
    "text": "bottom of the notebook I have an example Wiki so you can see all the code here uh that's the",
    "start": "6747480",
    "end": "6753760"
  },
  {
    "text": "graph yeah and then here's the final Wiki that you get out from this this type thing so it's pretty good um have a",
    "start": "6753760",
    "end": "6761079"
  },
  {
    "text": "look at that notebook um I think yeah Jason Lou also had a post on this recently this idea of",
    "start": "6761079",
    "end": "6767280"
  },
  {
    "text": "like report generation a theme that we're going to see more and more of this was a this is one idea that was pretty",
    "start": "6767280",
    "end": "6772760"
  },
  {
    "text": "sophisticated though you can probably simplify it a lot I've done a lot of work just on like a simple kind of",
    "start": "6772760",
    "end": "6778760"
  },
  {
    "text": "distillation prompt like perform Rag and then have some generation prompt give a",
    "start": "6778760",
    "end": "6783880"
  },
  {
    "text": "bunch of instructions for how I want the output to be formatted that also is really effective yeah",
    "start": "6783880",
    "end": "6792880"
  },
  {
    "text": "yeah so the idea the question was related to user feedback uh that's a really good one so",
    "start": "6822880",
    "end": "6828320"
  },
  {
    "text": "we do have I think I mentioned previously if you look at Lang graph um",
    "start": "6828320",
    "end": "6833360"
  },
  {
    "text": "where is it we I believe we have some user feedback examples um Thursday we're",
    "start": "6833360",
    "end": "6841679"
  },
  {
    "text": "definitely going to be announcing something that has a lot of support for user feedback and I would encourage you to keep an eye out for that so terison",
    "start": "6841679",
    "end": "6847800"
  },
  {
    "text": "is going to launch that here on Thursday um I will look for I know we have some",
    "start": "6847800",
    "end": "6854800"
  },
  {
    "text": "user feedback examples in langra but I",
    "start": "6854800",
    "end": "6860000"
  },
  {
    "text": "will need to find them let me see see we",
    "start": "6860000",
    "end": "6865880"
  },
  {
    "text": "probably check length we probably tweeted about it at some point I haven't actually done anything with user",
    "start": "6865880",
    "end": "6871560"
  },
  {
    "text": "feedback though let's see",
    "start": "6871560",
    "end": "6878800"
  },
  {
    "text": "H",
    "start": "6879440",
    "end": "6882440"
  },
  {
    "text": "[Music] yeah yeah maybe I might have to get back to you on that I thought we had some",
    "start": "6884860",
    "end": "6891599"
  },
  {
    "text": "nice examples s with",
    "start": "6891599",
    "end": "6896198"
  },
  {
    "text": "[Music] langra yeah I'll have to get back to on",
    "start": "6903000",
    "end": "6908280"
  },
  {
    "text": "that one",
    "start": "6908280",
    "end": "6915920"
  },
  {
    "text": "feedback customer support might be in here",
    "start": "6918520",
    "end": "6924320"
  },
  {
    "text": "let's try something else look at Lang graph docks",
    "start": "6933040",
    "end": "6938598"
  },
  {
    "text": "I'd poke around the langra docs we have a bunch of tutorials in the docs here um",
    "start": "6966760",
    "end": "6971840"
  },
  {
    "text": "just Google Lang graph documentation um I'm just poking around",
    "start": "6971840",
    "end": "6976880"
  },
  {
    "text": "here for user feedback uh ah here we go look at this",
    "start": "6976880",
    "end": "6984840"
  },
  {
    "text": "so Lang graph howto human in the loop I would have a look at that I've have not played with that myself but yeah",
    "start": "6984840",
    "end": "6993719"
  },
  {
    "text": "yeah yeah yeah okay right so for like um Mid to longer term problem solving tasks",
    "start": "7017400",
    "end": "7024199"
  },
  {
    "text": "how do you incorporate user feedback to ask for more information so um I would",
    "start": "7024199",
    "end": "7029560"
  },
  {
    "text": "have a look at this documentation because I would imagine it will uh cover examples along those lines I haven't",
    "start": "7029560",
    "end": "7036639"
  },
  {
    "text": "personally done that um I also believe",
    "start": "7036639",
    "end": "7041920"
  },
  {
    "text": "that I would have a look at the customer support bot um because that's an example",
    "start": "7041920",
    "end": "7049000"
  },
  {
    "text": "of of a kind of multi-turn interaction between a user and a support agent um so",
    "start": "7049000",
    "end": "7054400"
  },
  {
    "text": "I would look at the customer support bot which will and my team did as well as the documentation on human in the loop",
    "start": "7054400",
    "end": "7062280"
  },
  {
    "text": "so those are two things I would check out there um",
    "start": "7062280",
    "end": "7068280"
  },
  {
    "text": "nice yep model AR tring",
    "start": "7076599",
    "end": "7084280"
  },
  {
    "text": "models for agentic reasoning",
    "start": "7085400",
    "end": "7091360"
  },
  {
    "text": "um yeah so that's that's kind of an interesting question so the the question is related to training models",
    "start": "7094320",
    "end": "7099920"
  },
  {
    "text": "specifically for gentic reasoning um if anyone so I mean there's a lot",
    "start": "7099920",
    "end": "7106760"
  },
  {
    "text": "of there's a lot of work on prompting approaches for different types of reasoning for",
    "start": "7106760",
    "end": "7112199"
  },
  {
    "text": "sure um I'm a little bit less familiar with like efforts to find tuna model",
    "start": "7112199",
    "end": "7117800"
  },
  {
    "text": "specifically for particular like agenic architecture or use case but you could",
    "start": "7117800",
    "end": "7123199"
  },
  {
    "text": "imagine it um most of the work that I've encountered though is just using",
    "start": "7123199",
    "end": "7128639"
  },
  {
    "text": "generalist like high capacity generalist models with tool calling and specific prompting techniques so like react is",
    "start": "7128639",
    "end": "7136719"
  },
  {
    "text": "kind of a particular orchestration flow and prompting technique rather than you",
    "start": "7136719",
    "end": "7143520"
  },
  {
    "text": "know and you can interchange the llm accordingly I think the main thing typically or historically for for agents",
    "start": "7143520",
    "end": "7151599"
  },
  {
    "text": "has been the ability to perform high quality and accurate tool calling because agents that's one of the central",
    "start": "7151599",
    "end": "7157320"
  },
  {
    "text": "components of agents and so um that's kind of been the gating thing and I think model providers been focused a lot",
    "start": "7157320",
    "end": "7163920"
  },
  {
    "text": "on just high quality tool calling which helps kind of like all agent architecture so I haven't seen as much on like f tuning for one one particular",
    "start": "7163920",
    "end": "7170599"
  },
  {
    "text": "architecture I think it's like high capacity generalist models with tool calling and then prompting so it's more like in context learning that's kind of",
    "start": "7170599",
    "end": "7177880"
  },
  {
    "text": "the trend I've seen at least",
    "start": "7177880",
    "end": "7184719"
  },
  {
    "text": "yeah check I",
    "start": "7186320",
    "end": "7191760"
  },
  {
    "text": "yeah yeah so the checkpointing stuff actually this Thursday that's going to",
    "start": "7204159",
    "end": "7209199"
  },
  {
    "text": "be a lot more relevant because we're launching some stuff to support uh deployments for Lang graph in which case",
    "start": "7209199",
    "end": "7216480"
  },
  {
    "text": "you you can do a bunch of different things but you can have a single state that persists across many different",
    "start": "7216480",
    "end": "7222199"
  },
  {
    "text": "sessions you can also have checkpoints or you can return to State and revisit an agent from a particular Point um",
    "start": "7222199",
    "end": "7229320"
  },
  {
    "text": "don't worry about that too much for now I think there'll be a lot more documentation and and kind of context for that on",
    "start": "7229320",
    "end": "7237280"
  },
  {
    "text": "Thursday when this stuff to deployment comes in but it's good to be somewhat aware of and I would poke around the",
    "start": "7237280",
    "end": "7242639"
  },
  {
    "text": "documentation uh for a little bit more on checkpoint thing but it really becomes relevant on the stuff we're announcing on",
    "start": "7242639",
    "end": "7248760"
  },
  {
    "text": "Thursday um so I would have a look then",
    "start": "7248760",
    "end": "7253920"
  },
  {
    "text": "uh see if there's if we updated our docs yeah yeah so there are there is",
    "start": "7253920",
    "end": "7261679"
  },
  {
    "text": "some documentation on it now but it'll become a lot more interesting and relevant come Thursday we have a lot more support for",
    "start": "7261679",
    "end": "7268480"
  },
  {
    "text": "deployment yep",
    "start": "7268480",
    "end": "7272480"
  },
  {
    "text": "yeah yeah okay yeah so that that's a that's a",
    "start": "7287440",
    "end": "7293480"
  },
  {
    "text": "really good question and so the way it works with the",
    "start": "7293480",
    "end": "7302119"
  },
  {
    "text": "existing uh so it depends on the architecture so using the react architecture let's see if I can find an",
    "start": "7302119",
    "end": "7307560"
  },
  {
    "text": "example of it um so here's with react agent let's look at one of the traces",
    "start": "7307560",
    "end": "7313360"
  },
  {
    "text": "let's see if I have an example so basically the tool call itself will return like an",
    "start": "7313360",
    "end": "7318480"
  },
  {
    "text": "error and the llm then is expected to self-correct from that error it has to",
    "start": "7318480",
    "end": "7324639"
  },
  {
    "text": "kind of self-correct so that that's kind of one approach that at least we do with the",
    "start": "7324639",
    "end": "7331960"
  },
  {
    "text": "react agent um so actually you can see it in the notebook um if you go to",
    "start": "7331960",
    "end": "7342040"
  },
  {
    "text": "and if I can find some traces that have that example I will pull them up um",
    "start": "7342199",
    "end": "7349239"
  },
  {
    "text": "but I think it's in utilities somewhere yeah so basically this tool node with",
    "start": "7349239",
    "end": "7354840"
  },
  {
    "text": "fallbacks basically what happens is in this tool error so this if there's an",
    "start": "7354840",
    "end": "7361840"
  },
  {
    "text": "error in the tool call itself it'll return that error and usually the agent will then for the llm assistant will",
    "start": "7361840",
    "end": "7368320"
  },
  {
    "text": "look at that and like self-correct its tool call so that's that's typically how it's done and this",
    "start": "7368320",
    "end": "7374800"
  },
  {
    "text": "actually is reasonably effective but again you know the nice thing about the",
    "start": "7374800",
    "end": "7380320"
  },
  {
    "text": "the other implementation the custom agent I called in the notebook is you don't rely on tool calling in this way",
    "start": "7380320",
    "end": "7385880"
  },
  {
    "text": "and so you can get around this type of issue um but basically catching the",
    "start": "7385880",
    "end": "7392000"
  },
  {
    "text": "errors in the tool call itself with this code is is what's currently done let's see if I can actually find an",
    "start": "7392000",
    "end": "7397639"
  },
  {
    "text": "example um yeah I made to I can look for one where it gets the answer wrong yeah",
    "start": "7397639",
    "end": "7404800"
  },
  {
    "text": "let's see this one let's see if we can find a tool call failure um so here's the",
    "start": "7404800",
    "end": "7412320"
  },
  {
    "text": "trace um let's",
    "start": "7412320",
    "end": "7417239"
  },
  {
    "text": "see okay it didn't have a tool call error Yeah so basically you what you'll",
    "start": "7417639",
    "end": "7423280"
  },
  {
    "text": "see in the message history is that like the tool itself will return this error message and then the llm will say oh okay I need to re retry and then it'll",
    "start": "7423280",
    "end": "7429559"
  },
  {
    "text": "retry and hopefully get it right yeah",
    "start": "7429559",
    "end": "7435679"
  },
  {
    "text": "yep oh",
    "start": "7438559",
    "end": "7441719"
  },
  {
    "text": "yeah",
    "start": "7446400",
    "end": "7449400"
  },
  {
    "text": "yes okay so this is a very good point so yeah I'm a big fan of instructor um I haven't used it as much",
    "start": "7455960",
    "end": "7463639"
  },
  {
    "text": "but what you're saying is is one particular type of tool call so basically that pertains I believe more",
    "start": "7463639",
    "end": "7470119"
  },
  {
    "text": "to structured outputs which is indeed a kind of tool call and with when you're using something like a pantic schema",
    "start": "7470119",
    "end": "7475520"
  },
  {
    "text": "you're right it's very easy to check and like correct errors so I found catching errors like with schema validation like",
    "start": "7475520",
    "end": "7482360"
  },
  {
    "text": "using a structor is is really good and we have some other things you can use",
    "start": "7482360",
    "end": "7487599"
  },
  {
    "text": "within Lang chain to do the same thing so so that's one type of that's actually",
    "start": "7487599",
    "end": "7492639"
  },
  {
    "text": "particularly easy to kind of detect and correct what we show in this notebook",
    "start": "7492639",
    "end": "7498679"
  },
  {
    "text": "here and the code I showed is more for any general tool so um so this code here",
    "start": "7498679",
    "end": "7506679"
  },
  {
    "text": "will operate on any tool you call regardless so it doesn't have to do with structured outputs or anything and so it's just a more General uh check for",
    "start": "7506679",
    "end": "7514719"
  },
  {
    "text": "Tool call errors now in terms of instructor with Lang chain now maybe",
    "start": "7514719",
    "end": "7520880"
  },
  {
    "text": "I'll just back up a a little bit so Lang graph does not require Lang chain at all",
    "start": "7520880",
    "end": "7526079"
  },
  {
    "text": "so that's kind of point one and neither does Lang Smith so actually everything we're doing here does not need to use",
    "start": "7526079",
    "end": "7531239"
  },
  {
    "text": "Lang chain so actually that could be a pretty",
    "start": "7531239",
    "end": "7536880"
  },
  {
    "text": "interesting thing to try for like kind of the Choose Your Own Adventure thing but basically in the custom agent part um I",
    "start": "7536880",
    "end": "7545320"
  },
  {
    "text": "use with structured outputs to do the grading so if you go to um",
    "start": "7545320",
    "end": "7552800"
  },
  {
    "text": "yeah if you look at the um the retrieval grader here so this is",
    "start": "7552800",
    "end": "7559040"
  },
  {
    "text": "using LM with structured output and here's my grade schema try that instructor that should work great you",
    "start": "7559040",
    "end": "7564239"
  },
  {
    "text": "don't need Lang chain at all for this uh and that'll fit right into Lang graph so actually I think it'd be great to use",
    "start": "7564239",
    "end": "7571000"
  },
  {
    "text": "instructor with Lang graph for this particular use case and I do agree that Lang that instructor is really nice for",
    "start": "7571000",
    "end": "7577480"
  },
  {
    "text": "those kind of like scheme of validation error correction I plug and play",
    "start": "7577480",
    "end": "7583040"
  },
  {
    "text": "that that I'm going to make a note of that's a really good kind of Choose Your Own Adventure case um where should I put",
    "start": "7583040",
    "end": "7592440"
  },
  {
    "text": "that uh try instructor with Lang graph for grading yeah I like that a",
    "start": "7592440",
    "end": "7600800"
  },
  {
    "text": "lot yep",
    "start": "7602679",
    "end": "7608000"
  },
  {
    "text": "yep kind more a question just",
    "start": "7608000",
    "end": "7613239"
  },
  {
    "text": "yeah so question was related to just rag in general and where is rag rag agents yeah",
    "start": "7630800",
    "end": "7638800"
  },
  {
    "text": "sure well to be honest a lot of the problems with",
    "start": "7638800",
    "end": "7644679"
  },
  {
    "text": "rag I think about our own internal application chat Ling chain a lot of problems with rag",
    "start": "7644679",
    "end": "7649719"
  },
  {
    "text": "actually retrieval problems retrieval is just hard I'll give a good example like Lang chain we",
    "start": "7649719",
    "end": "7655960"
  },
  {
    "text": "have um I'm trying to remember five million tokens of context across all our",
    "start": "7655960",
    "end": "7661559"
  },
  {
    "text": "docs something like that we have all sorts of different we have a very long T integration docks you want very high",
    "start": "7661559",
    "end": "7668000"
  },
  {
    "text": "quality coverage and questions across all of that there's a lot in how you index all that stuff to ensure that",
    "start": "7668000",
    "end": "7674800"
  },
  {
    "text": "you boost retrievals from more canonical how two guys that are much better",
    "start": "7674800",
    "end": "7681480"
  },
  {
    "text": "documented but still having coverage over longtail content for like you know longtail questions for example if you're",
    "start": "7681480",
    "end": "7687719"
  },
  {
    "text": "using raw semantic similarity search you could have relevance to you know say your how-to guy which is really",
    "start": "7687719",
    "end": "7693520"
  },
  {
    "text": "welldeveloped and three random longtail documents that are not welldeveloped and they'll all get returned and so how do",
    "start": "7693520",
    "end": "7699639"
  },
  {
    "text": "you overlay different systems it could be reranking uh to basically promote",
    "start": "7699639",
    "end": "7708000"
  },
  {
    "text": "content uh that you believe to be more accurate or better based on some criteria so that is all to say I think",
    "start": "7708000",
    "end": "7715159"
  },
  {
    "text": "with rag the challenge is actually just domain specific retrieval for your application that's just a hard problem",
    "start": "7715159",
    "end": "7720400"
  },
  {
    "text": "and there's been a lot of work on this it's been around for a long time I think that's really the limiter and actually",
    "start": "7720400",
    "end": "7725920"
  },
  {
    "text": "there's kind of no no Silver Bullet like in our case we're having to look at the",
    "start": "7725920",
    "end": "7731119"
  },
  {
    "text": "structure of our documents very carefully design our retrieval strategy based on that doc structure like in",
    "start": "7731119",
    "end": "7737079"
  },
  {
    "text": "particular we're thinking about applying certain re um post retrieval ranking to",
    "start": "7737079",
    "end": "7742920"
  },
  {
    "text": "docs of certain types based upon their importance we're thinking about retrieving a large like a large initial number of docs and then boiling them",
    "start": "7742920",
    "end": "7749880"
  },
  {
    "text": "down with with kind of reranking based upon importance so I still think retrieval is very hard it's very domain",
    "start": "7749880",
    "end": "7756880"
  },
  {
    "text": "specific it depends on the structure of your documentation and there's kind of no free lunch I think the things that",
    "start": "7756880",
    "end": "7761920"
  },
  {
    "text": "are good about rag is context Windows getting much larger for llms and so back to that point I was making before I",
    "start": "7761920",
    "end": "7768360"
  },
  {
    "text": "think we're seeing and we're considering this ourselves less worry about the exact",
    "start": "7768360",
    "end": "7774719"
  },
  {
    "text": "right chunk size you can think more about chunking in you know different ways um and then passing full documents",
    "start": "7774719",
    "end": "7783360"
  },
  {
    "text": "to your final model so I think that part of it's really good um but still even",
    "start": "7783360",
    "end": "7788719"
  },
  {
    "text": "like even in this particular case you probably still need some uh reranking to",
    "start": "7788719",
    "end": "7795119"
  },
  {
    "text": "promote the most important documents um so I think retrieval is",
    "start": "7795119",
    "end": "7801119"
  },
  {
    "text": "still quite hard in particular um like looking at the line chain docs in",
    "start": "7801119",
    "end": "7807880"
  },
  {
    "text": "particular the overlay of document importance on top of raw semantic",
    "start": "7807880",
    "end": "7814159"
  },
  {
    "text": "similarity search right take a case of like I have a question semantically it's",
    "start": "7814159",
    "end": "7820559"
  },
  {
    "text": "similar to Tender different documents those documents though vary widely in their quality and their relevant",
    "start": "7820559",
    "end": "7826480"
  },
  {
    "text": "like more like higher level relevance like maybe that passage is related but like it might be a general question",
    "start": "7826480",
    "end": "7832840"
  },
  {
    "text": "about how do you build an agent and then some random integration doc talks about building an agent for integration X and",
    "start": "7832840",
    "end": "7838159"
  },
  {
    "text": "I want to make sure that the more canonical well-developed agent you know overview doc gets promoted and passed",
    "start": "7838159",
    "end": "7845360"
  },
  {
    "text": "back in the answer stuff like that Sor it's a long answer but basically rag is",
    "start": "7845360",
    "end": "7850480"
  },
  {
    "text": "it's hard I mean I think retrieval is really the hard part the generation part is getting better and better as long",
    "start": "7850480",
    "end": "7856040"
  },
  {
    "text": "contexts grow",
    "start": "7856040",
    "end": "7861040"
  },
  {
    "text": "yep",
    "start": "7863159",
    "end": "7866159"
  },
  {
    "text": "yeah yeah okay that's a great question so the question was when we talking about this",
    "start": "7870280",
    "end": "7875360"
  },
  {
    "text": "reranking how do you assign this relevance to your documents like what is that so I'll just give you what we've",
    "start": "7875360",
    "end": "7882079"
  },
  {
    "text": "been thinking about uh I actually think it is for us going to be a hand tuned",
    "start": "7882079",
    "end": "7890040"
  },
  {
    "text": "kind of relevant score based upon our doc structure so if you look at the Lang chain docs like um go to Lang",
    "start": "7890040",
    "end": "7899760"
  },
  {
    "text": "chain uh documents um yeah so Lang chain",
    "start": "7899760",
    "end": "7905040"
  },
  {
    "text": "documentation we have these sections up here tutorials how two guides conceptual guides which are like really well",
    "start": "7905040",
    "end": "7911320"
  },
  {
    "text": "developed more recent well curated these you could imagine have some kind of",
    "start": "7911320",
    "end": "7916559"
  },
  {
    "text": "relevance or importance ranking of one or highest so these are documents that contain very high quality well curated",
    "start": "7916559",
    "end": "7922239"
  },
  {
    "text": "answers that we want to promote and serve to users in the generation phase however what say someone asked a",
    "start": "7922239",
    "end": "7927960"
  },
  {
    "text": "question about one particular integration right if you go to Integrations we have all these pages right components go to Retrievers look",
    "start": "7927960",
    "end": "7935239"
  },
  {
    "text": "at the you know Zep Cloud retriever this is some stuff related to Zep Cloud",
    "start": "7935239",
    "end": "7940400"
  },
  {
    "text": "specifically if someone ask about Zep Cloud you do want to be able to retrieve that doc right and so",
    "start": "7940400",
    "end": "7946760"
  },
  {
    "text": "um some ability to differentiate between questions that need you know general",
    "start": "7946760",
    "end": "7952079"
  },
  {
    "text": "answers in which case you would promote your more canonical howto guides conceptual docs versus questions that",
    "start": "7952079",
    "end": "7958840"
  },
  {
    "text": "require retrieval from very specific integration docs in which case you would still promote this information that's",
    "start": "7958840",
    "end": "7964199"
  },
  {
    "text": "kind of the Crux of it and I think we'll probably use kind of manual or heris scoring to up upweight or up",
    "start": "7964199",
    "end": "7972480"
  },
  {
    "text": "um are core like how two guides and conceptual guides um over longer tail",
    "start": "7972480",
    "end": "7979199"
  },
  {
    "text": "integration docs and we might have a router that will indicate whether the question is general or specific so those",
    "start": "7979199",
    "end": "7985840"
  },
  {
    "text": "are the two things I probably do so routing on the question side and then some kind of heris uh relevance or",
    "start": "7985840",
    "end": "7993159"
  },
  {
    "text": "importance grading or quality grading on the document side and that can be packed in the metadata that you pack along with",
    "start": "7993159",
    "end": "7999599"
  },
  {
    "text": "your your index chunk yep maybe",
    "start": "7999599",
    "end": "8007480"
  },
  {
    "text": "not yeah oh yeah so so let's say a typical rag application where there's a",
    "start": "8007480",
    "end": "8014159"
  },
  {
    "text": "question answer fair but we kind of maintain the multi like for example we maintain the conversation history of the user to kind of create a convers the",
    "start": "8014159",
    "end": "8021800"
  },
  {
    "text": "problem is like let's say for example a question is asked and then the the retri CHS are like let's say five right and",
    "start": "8021800",
    "end": "8029639"
  },
  {
    "text": "then a subsequent question is so let's say uh no but is related to the first question but still somehow in the first",
    "start": "8029639",
    "end": "8036880"
  },
  {
    "text": "note you transform the query and then the retri chunks are still the same so like I would just get an answer which is",
    "start": "8036880",
    "end": "8043599"
  },
  {
    "text": "like more of this first answer so like how do you uh my question is like how do",
    "start": "8043599",
    "end": "8050119"
  },
  {
    "text": "you make sure that let's say uh he wanted to Deep dive into the document",
    "start": "8050119",
    "end": "8055360"
  },
  {
    "text": "like uh into the more context how do you make that happen in like yeah so the question I guess was",
    "start": "8055360",
    "end": "8061880"
  },
  {
    "text": "like in a multi-turn rag context let's say a case where um a user",
    "start": "8061880",
    "end": "8067159"
  },
  {
    "text": "ask an initial question and you retrieve some documents you produce an answer and they ask a",
    "start": "8067159",
    "end": "8073599"
  },
  {
    "text": "follow-up that says give me more information about this now do you want to r- retrieve or do you want to R",
    "start": "8073599",
    "end": "8082199"
  },
  {
    "text": "reference those same docs no so",
    "start": "8082199",
    "end": "8087960"
  },
  {
    "text": "what reite question okay you do a rewriting so you rewrite the question",
    "start": "8088000",
    "end": "8095158"
  },
  {
    "text": "okay the same as before",
    "start": "8097760",
    "end": "8102519"
  },
  {
    "text": "okay okay okay interesting so the problem",
    "start": "8102800",
    "end": "8108000"
  },
  {
    "text": "there is more of a retrieval problem you're doing a rewrite you're still retrieving the same set of documents though now what do you want to have",
    "start": "8108000",
    "end": "8114679"
  },
  {
    "text": "happen you want to do you actually want to retrieve different documents or do",
    "start": "8114679",
    "end": "8121079"
  },
  {
    "text": "you want to Deep dive yeah but that's a question",
    "start": "8121079",
    "end": "8127119"
  },
  {
    "text": "what do you mean by Deep dive like you're retrieving let's say it's a",
    "start": "8127119",
    "end": "8132280"
  },
  {
    "text": "chapter of a book you're retrieving only the first page you want to retrieve the whole chapter yeah",
    "start": "8132280",
    "end": "8139280"
  },
  {
    "text": "okay okay then I think actually a question rewrite would probably not sufficient what I would think about more",
    "start": "8139320",
    "end": "8145000"
  },
  {
    "text": "is for that second pass um you could actually do something like",
    "start": "8145000",
    "end": "8152119"
  },
  {
    "text": "metadata filtering on your chunks if you have your data or your documents partitioned by like chapters or or some",
    "start": "8152119",
    "end": "8159599"
  },
  {
    "text": "sections I would just do a bulk retrieval the whole section or something like that so it's more like a trick on",
    "start": "8159599",
    "end": "8164840"
  },
  {
    "text": "the on the retrieval side rather than a rewrite of the query because I hear I see what you're saying you rewrite the",
    "start": "8164840",
    "end": "8169880"
  },
  {
    "text": "query you might get the same docs back if you want to guarantee that you actually get like a deeper dive in your docs then maybe it's something in your",
    "start": "8169880",
    "end": "8176320"
  },
  {
    "text": "retriever itself you could increase K so retrieve more docs you could use metad filtering to like ensure you get all the",
    "start": "8176320",
    "end": "8183440"
  },
  {
    "text": "docs in a given chapter so I think it's more retrieval thing but that's kind of an interesting",
    "start": "8183440",
    "end": "8190400"
  },
  {
    "text": "point though",
    "start": "8190400",
    "end": "8193079"
  },
  {
    "text": "yeah cool well I know it's been two and a half hours almost so there we go it's",
    "start": "8195639",
    "end": "8202518"
  },
  {
    "text": "good um yeah yeah yeah I'm I'm I'm hanging out for another till noon so",
    "start": "8202519",
    "end": "8211200"
  },
  {
    "text": "oh",
    "start": "8212599",
    "end": "8214840"
  },
  {
    "text": "cool yeah yeah okay okay this is good so um see the question was he's doing the",
    "start": "8218679",
    "end": "8225359"
  },
  {
    "text": "the local the local um agent tutorial and the questions on the eval set so",
    "start": "8225360",
    "end": "8232319"
  },
  {
    "text": "actually that's a fun one modify them any way you want the key point was I wanted some questions that are definitely outside the vector store so I",
    "start": "8232320",
    "end": "8239399"
  },
  {
    "text": "asked something about like two things about sports because I know it's not in my VOR store about agents so I think I",
    "start": "8239400",
    "end": "8244678"
  },
  {
    "text": "index three blog posts about like agents and prompting and adversarial examples I just wanted some orthogonal questions",
    "start": "8244679",
    "end": "8251598"
  },
  {
    "text": "that'll Force web search so that's the only thing there but you actually play with those and you can modify them and",
    "start": "8251599",
    "end": "8257000"
  },
  {
    "text": "all that um yeah but that's cool it's working or are you using llama 3 yeah",
    "start": "8257000",
    "end": "8265598"
  },
  {
    "text": "cool oh you you you have a laptop big enough for 70b that's okay okay you're",
    "start": "8265599",
    "end": "8270678"
  },
  {
    "text": "at at the edge of okay okay yeah AP APS yeah",
    "start": "8270679",
    "end": "8279439"
  },
  {
    "text": "exactly I mean it's actually kind of nice you can even run the 70b to be honest I'm not sure I can even run it but yeah that's",
    "start": "8279440",
    "end": "8287679"
  },
  {
    "text": "cool uh nice",
    "start": "8287880",
    "end": "8295518"
  },
  {
    "text": "see yeah well I can just hang out and um oh yeah you sure what would be the best",
    "start": "8295559",
    "end": "8303598"
  },
  {
    "text": "way to interp questions is",
    "start": "8303599",
    "end": "8309598"
  },
  {
    "text": "or yeah well if you have a chat",
    "start": "8309599",
    "end": "8316800"
  },
  {
    "text": "application so the question was related to how do you incorporate multi- turn",
    "start": "8316800",
    "end": "8322800"
  },
  {
    "text": "so if you look at the react agent it",
    "start": "8322800",
    "end": "8328080"
  },
  {
    "text": "uses a chat history as it State and that case followup questions will be captured just in the message history as part of",
    "start": "8328080",
    "end": "8334678"
  },
  {
    "text": "chat the I think the current layout of the custom L graph agent though is a",
    "start": "8334679",
    "end": "8339840"
  },
  {
    "text": "little bit more single single turn so be kind of like question answer",
    "start": "8339840",
    "end": "8345598"
  },
  {
    "text": "um that's oh",
    "start": "8345599",
    "end": "8349638"
  },
  {
    "text": "yeah oh okay okay got it so the okay okay I got",
    "start": "8353679",
    "end": "8361080"
  },
  {
    "text": "it so the question was how do you modify the agent so that it it will actually return like if it needs more",
    "start": "8361080",
    "end": "8367719"
  },
  {
    "text": "clarification from the user um yeah these particular Asian examples",
    "start": "8367719",
    "end": "8374960"
  },
  {
    "text": "uh don't do that um but again I think that's a maybe a good takeaway for",
    "start": "8374960",
    "end": "8382319"
  },
  {
    "text": "me I should add that to these tutorials um incorporate a simple example of",
    "start": "8382320",
    "end": "8389319"
  },
  {
    "text": "multi-turn um I will I will do that and get my contact",
    "start": "8389319",
    "end": "8397520"
  },
  {
    "text": "and I will send that to you",
    "start": "8397520",
    "end": "8402080"
  },
  {
    "text": "um yes yes",
    "start": "8405600",
    "end": "8411160"
  },
  {
    "text": "exactly so you want so I mentioned previously if you look at Lang graph",
    "start": "8411160",
    "end": "8418359"
  },
  {
    "text": "um let me find it it's it's it's one of our",
    "start": "8418359",
    "end": "8423479"
  },
  {
    "text": "notebooks um the customer support",
    "start": "8423479",
    "end": "8429000"
  },
  {
    "text": "agent uh this so Lang graph examples customer support is an example of an agent that has like multi-term dialogue",
    "start": "8429000",
    "end": "8436800"
  },
  {
    "text": "but it's complicated so I'd like to maybe augment these tutorials with a simpler",
    "start": "8436800",
    "end": "8444200"
  },
  {
    "text": "example um I will yeah I will I'll follow follow up",
    "start": "8444200",
    "end": "8450960"
  },
  {
    "text": "on that if you give me give me contact info and I'll I'll send you something",
    "start": "8450960",
    "end": "8457399"
  },
  {
    "text": "cool well I'll just I'll sit up here anyone can just come and grab me uh thanks for everything hopefully the",
    "start": "8482160",
    "end": "8488560"
  },
  {
    "text": "cookbooks are working um yeah it was good made it two and a half hours",
    "start": "8488560",
    "end": "8493960"
  },
  {
    "text": "so good thanks [Music]",
    "start": "8493960",
    "end": "8510591"
  },
  {
    "text": "a",
    "start": "8510760",
    "end": "8513760"
  }
]