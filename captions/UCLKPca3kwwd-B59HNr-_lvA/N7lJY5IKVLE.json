[
  {
    "start": "0",
    "end": "232000"
  },
  {
    "text": "[Music]",
    "start": "250",
    "end": "15750"
  },
  {
    "text": "uh in the workshop today was everything that comes after once you've you know",
    "start": "17520",
    "end": "23279"
  },
  {
    "text": "written your thin wrapper around the open AI API to make your chat GPT",
    "start": "23279",
    "end": "28640"
  },
  {
    "text": "powered app you've acquired hundred million in Venture funding at a 4 billion valuation and now",
    "start": "28640",
    "end": "35520"
  },
  {
    "text": "you're like what what am I supposed to do next um so we're g to I'm split this",
    "start": "35520",
    "end": "43200"
  },
  {
    "text": "into two parts so we can take kind of a break it's like a three-hour long Workshop it's a long time uh so in the",
    "start": "43200",
    "end": "49320"
  },
  {
    "text": "first half we're going to talk about uh inference so about the what exactly this",
    "start": "49320",
    "end": "55440"
  },
  {
    "text": "workload is that we have given over to the uh the inference as a service provider what you know what's the shape",
    "start": "55440",
    "end": "62280"
  },
  {
    "text": "of it um why do we need these expensive accelerators what are the other options",
    "start": "62280",
    "end": "68600"
  },
  {
    "text": "available um and we're going to spend half of our time on that because that's a place where we can actually talk you",
    "start": "68600",
    "end": "74479"
  },
  {
    "text": "know in engineering terms about constraints uh about uh service level",
    "start": "74479",
    "end": "80240"
  },
  {
    "text": "objectives and service level agreements uh the kinds of things that that lead to robust systems um and then we'll spend",
    "start": "80240",
    "end": "86799"
  },
  {
    "text": "90 minutes on the rest of the owl uh the rest of what it takes to make a successful AI powered app just because",
    "start": "86799",
    "end": "93000"
  },
  {
    "text": "there's so uh very little to say just yet about how to engineer these robustly but we'll talk about what the what the",
    "start": "93000",
    "end": "99479"
  },
  {
    "text": "emerging consensus on what to do is uh so far and what tools are out there and",
    "start": "99479",
    "end": "104640"
  },
  {
    "text": "available to start accelerating that process um so yeah so for inference what",
    "start": "104640",
    "end": "112920"
  },
  {
    "text": "uh what are we talking about when we're doing inference workloads how do we decide between using open and proprietary models uh to do that INF",
    "start": "112920",
    "end": "120759"
  },
  {
    "text": "um where do those models live do they live on a device do they live in a Cloud",
    "start": "120759",
    "end": "125880"
  },
  {
    "text": "Server um uh and then we'll uh spend some time talking about what takes to",
    "start": "125880",
    "end": "131280"
  },
  {
    "text": "self-serve inference um for the rest of the owl we'll talk about architectures and",
    "start": "131280",
    "end": "137640"
  },
  {
    "text": "patterns so what are the emerging kind of uh patterns for usage of of large",
    "start": "137640",
    "end": "143120"
  },
  {
    "text": "language models in AI applications and we'll talk about monitoring evaluation",
    "start": "143120",
    "end": "148640"
  },
  {
    "text": "and observability which is how we try and actually improve applications that fit those patterns over",
    "start": "148640",
    "end": "156239"
  },
  {
    "text": "time um yeah any High Lev questions um",
    "start": "156360",
    "end": "161959"
  },
  {
    "text": "my mic's off the zoom is not hooked up no questions probably not still at a very",
    "start": "161959",
    "end": "168280"
  },
  {
    "text": "high level um no just stretch it great yeah maybe",
    "start": "168280",
    "end": "173800"
  },
  {
    "text": "we should all stretch um yeah so uh who am I and before we dive in like who am I",
    "start": "173800",
    "end": "179879"
  },
  {
    "text": "why you to listen to me tell you about any of these things so uh my name is Charles I like to teach people about AI",
    "start": "179879",
    "end": "186080"
  },
  {
    "text": "I've been doing it for a while now uh I went to Berkeley studied neural networks back in the 2010s um taught people about",
    "start": "186080",
    "end": "193440"
  },
  {
    "text": "uh how to use them and beian networks rip um for data science um then I worked",
    "start": "193440",
    "end": "200680"
  },
  {
    "text": "uh in developer relations and education for weights and biases um a previous generation mlops tool um generation",
    "start": "200680",
    "end": "207560"
  },
  {
    "text": "times being shorter than more doubling time I guess at this point um and then",
    "start": "207560",
    "end": "212959"
  },
  {
    "text": "for the last two years I've been working with full stack deep learning uh teaching not just things like the math",
    "start": "212959",
    "end": "218720"
  },
  {
    "text": "of machine learning or how to do monitoring of an ml application but how to build an application that uses ml",
    "start": "218720",
    "end": "225840"
  },
  {
    "text": "from Soup To Nuts uh from the gpus up to the user",
    "start": "225840",
    "end": "231239"
  },
  {
    "text": "experience all right so now let's dive into the first half here on inference",
    "start": "231439",
    "end": "236840"
  },
  {
    "start": "232000",
    "end": "616000"
  },
  {
    "text": "and specifically what is actually going on why does it cost um so much to uh to",
    "start": "236840",
    "end": "243000"
  },
  {
    "text": "pay open AI um and so generative model inference",
    "start": "243000",
    "end": "248480"
  },
  {
    "text": "the kind of inference that's done uh by these the generative AIS is uh you see",
    "start": "248480",
    "end": "255959"
  },
  {
    "text": "when you use it as a via an API you see a kind of data to dat function where both sides of that data are are human",
    "start": "255959",
    "end": "263040"
  },
  {
    "text": "interpretable data so things like text images sounds and outcome back new generated text IM sounds so for example",
    "start": "263040",
    "end": "271440"
  },
  {
    "text": "this is from the Palm eaper from Google you might show a picture of a restaurant and the question if a robot wanted to be",
    "start": "271440",
    "end": "277360"
  },
  {
    "text": "useful here what steps should it take and the output of this uh uh language",
    "start": "277360",
    "end": "282840"
  },
  {
    "text": "modeling generative API would be you know clean the table pick up trash pick up chairs wipe chairs put chairs down um",
    "start": "282840",
    "end": "289080"
  },
  {
    "text": "which could then be sent to a cleaning robot and now you've got something you know it's a it's a pretty useful system",
    "start": "289080",
    "end": "295639"
  },
  {
    "text": "there so this is what you see from the outside um but as you start like digging in a",
    "start": "295639",
    "end": "301520"
  },
  {
    "text": "little bit you'll start hearing about things like tokens um and and log probs and",
    "start": "301520",
    "end": "306840"
  },
  {
    "text": "temperatures and um to understand what's going on and and weights and networks",
    "start": "306840",
    "end": "313160"
  },
  {
    "text": "and to understand what's going on you have to realize that this has been broken down into kind of three pieces",
    "start": "313160",
    "end": "318800"
  },
  {
    "text": "one part that goes from what humans understand like text and images and sound to what neural networks understand",
    "start": "318800",
    "end": "325800"
  },
  {
    "text": "uh and then one part that goes back uh and in between the operations of a neural network that operates on arrays",
    "start": "325800",
    "end": "332000"
  },
  {
    "text": "and returns arrays so tokenizers take in text that a human can read and turn it",
    "start": "332000",
    "end": "338000"
  },
  {
    "text": "into an array of numbers a tensor um just due to the kind of",
    "start": "338000",
    "end": "345199"
  },
  {
    "text": "but really an array of numbers um those tensors get turned back to things the humans care about um and",
    "start": "359840",
    "end": "368080"
  },
  {
    "text": "in between neural networks map tensors to tensors this step is the bottleneck this",
    "start": "368080",
    "end": "374919"
  },
  {
    "text": "step is the hard part there's interesting stuff going on in the sampling process there's interesting stuff going on in tokenization there's",
    "start": "374919",
    "end": "381360"
  },
  {
    "text": "cursed stuff going on in tokenization um but this step is the bottleneck this is where the vast majority of the",
    "start": "381360",
    "end": "387720"
  },
  {
    "text": "engineering time is spent this is where the vast majority of the compute time the memory are spent and this is the",
    "start": "387720",
    "end": "392960"
  },
  {
    "text": "interesting part and this is the part where the engineering Focus uh needs to be um or this is the part that you farm",
    "start": "392960",
    "end": "400639"
  },
  {
    "text": "out to somebody else um so diving in double clicking on that uh tensor to",
    "start": "400639",
    "end": "407680"
  },
  {
    "text": "tensor Arrow at the bottom that uh a neural network is kind of a fancy term",
    "start": "407680",
    "end": "414000"
  },
  {
    "text": "for a composition of a bunch of tensor to tensor functions if you have a function that takes in an A and returns",
    "start": "414000",
    "end": "419720"
  },
  {
    "text": "an a you can just stack those one after another um and that's what gives neural networks the kind of legoy flavor you",
    "start": "419720",
    "end": "426000"
  },
  {
    "text": "can grab uh bricks from one set and bricks from another set and attach them to each",
    "start": "426000",
    "end": "431199"
  },
  {
    "text": "other um so this neural network this is an ancient neural network uh the Inception V3 model from Google that was",
    "start": "431199",
    "end": "438400"
  },
  {
    "text": "state-of-the-art and computer vision for a few months in the 2010s um and each of",
    "start": "438400",
    "end": "443639"
  },
  {
    "text": "those little blocks there takes in a tensor and returns a tensor uh and so",
    "start": "443639",
    "end": "449520"
  },
  {
    "text": "starts with a tensor that looks like an image so it's got a red green and blue Channel and comes out something uh that",
    "start": "449520",
    "end": "455800"
  },
  {
    "text": "8X 8X 2048 example there that's probably somewhere in the middle of the network it's not the final classification output",
    "start": "455800",
    "end": "462240"
  },
  {
    "text": "anyway big block of numbers they get passed into each other uh and then each one of those is",
    "start": "462240",
    "end": "468840"
  },
  {
    "text": "Itself parameterized by a tensor so it's not just like uh a map that you could kind of like write down by hand that's",
    "start": "468840",
    "end": "474879"
  },
  {
    "text": "like add one to every entry or something like that um it's uh it's defined by",
    "start": "474879",
    "end": "481080"
  },
  {
    "text": "like another big pile of uh of numbers the weights and biases of the neural network so this is uh the weights from",
    "start": "481080",
    "end": "488400"
  },
  {
    "text": "the first layer of a computer vision Network the Alex net that kicked off this uh whole deep learning Revolution",
    "start": "488400",
    "end": "495879"
  },
  {
    "text": "um and it's these These are these are little this is a visualization of a little block of red green blue uh uh",
    "start": "495879",
    "end": "504080"
  },
  {
    "text": "like tensor with three color channels in it so humans can actually look at it and interpret it unlike the rest of them and",
    "start": "504080",
    "end": "510520"
  },
  {
    "text": "see what's going on that it's got little things for detecting edges and textures and color differences and",
    "start": "510520",
    "end": "517959"
  },
  {
    "text": "things and so uh we want to run a big",
    "start": "518800",
    "end": "524039"
  },
  {
    "text": "tensor to tensor map and we are going to parameterize that with a big pile of tensors and before it's time to actually",
    "start": "524039",
    "end": "531440"
  },
  {
    "text": "serve users those tensors need to be generated by the training process and",
    "start": "531440",
    "end": "536800"
  },
  {
    "text": "back uh a couple of years ago um or even as recently as 18 months ago this would",
    "start": "536800",
    "end": "542160"
  },
  {
    "text": "be the part where we would stop talk about gradient descent optimization statistical learning theory and uh you",
    "start": "542160",
    "end": "548640"
  },
  {
    "text": "know GPU acceleration and all the things that are needed to turn something into to like get those numbers to get things",
    "start": "548640",
    "end": "556839"
  },
  {
    "text": "like that involving like grabbing you know hoovering up a bunch of information from the internet uh without consent um",
    "start": "556839",
    "end": "563959"
  },
  {
    "text": "and then crystallizing it into those piles of numbers but nowadays you don't",
    "start": "563959",
    "end": "569440"
  },
  {
    "text": "have to do that anymore specialized Foundation modeling teams uh generate these weights and then they either put",
    "start": "569440",
    "end": "575519"
  },
  {
    "text": "them behind proprietary service or they share them with everybody else to use um and so we can skip past all of that",
    "start": "575519",
    "end": "582000"
  },
  {
    "text": "stuff and jump into the actual application um uh yeah so any questions",
    "start": "582000",
    "end": "589320"
  },
  {
    "text": "before we start talking about you know where those you know where those weights come from uh or or rather what the",
    "start": "589320",
    "end": "595720"
  },
  {
    "text": "various ways to get a hold of such a set of Weights are to be able to use such a set of Weights are um any questions at",
    "start": "595720",
    "end": "601440"
  },
  {
    "text": "the level of what we're trying to do with inference probably pretty clear that's",
    "start": "601440",
    "end": "608320"
  },
  {
    "text": "uh maybe a reminder from the 101 stuff all right so now let's start diving a little bit",
    "start": "608320",
    "end": "615120"
  },
  {
    "text": "deeper so you want to run an AI uh your f one of the first choices that you need",
    "start": "615440",
    "end": "621279"
  },
  {
    "start": "616000",
    "end": "1282000"
  },
  {
    "text": "to make as is you know pretty common with um with software is a build versus",
    "start": "621279",
    "end": "627240"
  },
  {
    "text": "buy question um you going to like are you going to make this uh you know out",
    "start": "627240",
    "end": "633120"
  },
  {
    "text": "of existing open components or are you going to uh are you going to kick it off to a service so um there's in the",
    "start": "633120",
    "end": "641560"
  },
  {
    "text": "proprietary corner there are a couple of players in the open corner there are a couple of Players let's walk through",
    "start": "641560",
    "end": "647279"
  },
  {
    "text": "what those are and um what the sort of dividing lines are and why to choose one",
    "start": "647279",
    "end": "652399"
  },
  {
    "text": "or the other so there's uh a number of proprietary modeling Services uh like",
    "start": "652399",
    "end": "659200"
  },
  {
    "text": "anthropics that uh from whom we just heard uh and the good thing about these",
    "start": "659200",
    "end": "664880"
  },
  {
    "text": "proprietary models is that they are the most capable models out there uh so this",
    "start": "664880",
    "end": "670120"
  },
  {
    "text": "is from the LM CIS uh leaderboard maybe the the hugging face hosted version of",
    "start": "670120",
    "end": "677040"
  },
  {
    "text": "that leaderboard if you look at the the top five models on that are all proprietary models and they're all from",
    "start": "677040",
    "end": "683720"
  },
  {
    "text": "open AI or anthropic um so there are a couple of other players out there and in the",
    "start": "683720",
    "end": "689959"
  },
  {
    "text": "future that you know they could release really high quality models um but for",
    "start": "689959",
    "end": "696680"
  },
  {
    "text": "now this is the the the state of things so if you need the absolute highest level of intelligence in your",
    "start": "696680",
    "end": "703040"
  },
  {
    "text": "application uh then you want to roll with one of these um it's also often",
    "start": "703040",
    "end": "708680"
  },
  {
    "text": "common to start with some of the most highly capable models and then kind of uh prove out your application there and",
    "start": "708680",
    "end": "715120"
  },
  {
    "text": "then move to doing it uh like with a less capable model that it's cheaper easier to run uh the sort of like",
    "start": "715120",
    "end": "721839"
  },
  {
    "text": "rewriting it in rust or something um so the usual concern with",
    "start": "721839",
    "end": "728279"
  },
  {
    "text": "using a proprietary service there's a number of them including things like vendor lockin um but one of the one of",
    "start": "728279",
    "end": "734000"
  },
  {
    "text": "the ones that comes up immediately is like how much is it going to cost me to use a proprietary service and can't I",
    "start": "734000",
    "end": "740480"
  },
  {
    "text": "save money by doing it myself so the fact that the capabilities are higher with the proprietary models is one",
    "start": "740480",
    "end": "747000"
  },
  {
    "text": "reason to say well you're not going to get exactly the same thing right now um using an open model um but then the",
    "start": "747000",
    "end": "754480"
  },
  {
    "text": "other kind of kicker here is that the proprietary models are priced very affordably um so this is something",
    "start": "754480",
    "end": "760920"
  },
  {
    "text": "that's that has been the case uh this right quote here is from a blog post I wrote back in like January um when the",
    "start": "760920",
    "end": "768680"
  },
  {
    "text": "only open large model was glm 130b from singua um and at that time just like",
    "start": "768680",
    "end": "775560"
  },
  {
    "text": "trying to get the thing running uh in a day I got to within an order of magnitude of the cost of open AI but you",
    "start": "775560",
    "end": "782079"
  },
  {
    "text": "know on the up on the upper end um and then more for a more recent uh and more",
    "start": "782079",
    "end": "787760"
  },
  {
    "text": "serious example the folks at honeycomb uh made a natural language SQL uh kind",
    "start": "787760",
    "end": "793199"
  },
  {
    "text": "of transformation maybe not SQL but query language uh transform transformation uh AI product and like",
    "start": "793199",
    "end": "800480"
  },
  {
    "text": "their opinion was that open AI was very inexpensive to run for their task uh and",
    "start": "800480",
    "end": "805839"
  },
  {
    "text": "so you'll find it seems that like nobody's attempting to uh like extract",
    "start": "805839",
    "end": "811240"
  },
  {
    "text": "rents from Monopoly pricing the way that you can get with some proprietary Services uh where they know that you",
    "start": "811240",
    "end": "817480"
  },
  {
    "text": "have no choice but to pay them $10 million for a mat lab license every year or whatever",
    "start": "817480",
    "end": "823880"
  },
  {
    "text": "um not to pick on it sorry for a license for an array programming language",
    "start": "823880",
    "end": "830880"
  },
  {
    "text": "um uh yeah so and the costs here you know a dollar for a million tokens for",
    "start": "831279",
    "end": "837440"
  },
  {
    "text": "Claude instance 10 for a million tokens for Claude 2 U relative to how much we're used to paying like humans for tax",
    "start": "837440",
    "end": "844839"
  },
  {
    "text": "you know that's that's like a pretty decent deal anthropic has been slightly more expensive than open AI without like",
    "start": "844839",
    "end": "850880"
  },
  {
    "text": "a clear win on um capabilities uh but that's the current state of things they",
    "start": "850880",
    "end": "857399"
  },
  {
    "text": "do have longer context Windows which is kind of nice but um uh but yeah most",
    "start": "857399",
    "end": "863079"
  },
  {
    "text": "people for the pricing reason and the capability reason uh choose open AI at this time",
    "start": "863079",
    "end": "870040"
  },
  {
    "text": "um so the bad thing about proprietary models besides like you know uh hearing",
    "start": "870040",
    "end": "876000"
  },
  {
    "text": "Richard stalman screaming in the back of your mind all the time uh is that proprietary model models cannot offer",
    "start": "876000",
    "end": "882360"
  },
  {
    "text": "you full control by Dent of their like very nature proprietary models um so",
    "start": "882360",
    "end": "887759"
  },
  {
    "text": "Alex graveley created a co-pilot part of the team that created co-pilot at GitHub um was celebrating that GPT 3.5 turbo",
    "start": "887759",
    "end": "895759"
  },
  {
    "text": "instruct a recent release from open AI had brought back log probabilities so",
    "start": "895759",
    "end": "900880"
  },
  {
    "text": "you can see what not just like what text did the model generate but what probabilities the model gave each token",
    "start": "900880",
    "end": "907720"
  },
  {
    "text": "along the way and back when the back when it was just the gpt3 API and the",
    "start": "907720",
    "end": "913199"
  },
  {
    "text": "playground you could see that information and that's where a lot of the like early work on sort of like intuition about prompt engineering came",
    "start": "913199",
    "end": "919800"
  },
  {
    "text": "from being able to see those numbers and there's all kinds of cool techniques that you can get up to if you have those log probs um and if you can manipulate",
    "start": "919800",
    "end": "926759"
  },
  {
    "text": "those log probs uh so yeah you can read out confidence information you can use it like during your development process",
    "start": "926759",
    "end": "933600"
  },
  {
    "text": "to sort of like more gather more Rich information about the system um and",
    "start": "933600",
    "end": "939199"
  },
  {
    "text": "there's really you know you are you're interacting with a probabilistic model if you can't see the model's probabilities you're like fundamentally",
    "start": "939199",
    "end": "946160"
  },
  {
    "text": "hamstrung um and so that was mid-september and then like a couple of days ago they turned that off um and the",
    "start": "946160",
    "end": "954000"
  },
  {
    "text": "reason why is because if you give somebody that amount of information they can start to reverse engineer your model",
    "start": "954000",
    "end": "959040"
  },
  {
    "text": "pretty quickly um and then you are stuck in the situation of IBM creating the personal computer and then a bunch of",
    "start": "959040",
    "end": "964880"
  },
  {
    "text": "people with soldering irons and oscilloscopes turn around and make clones of your machine like within a year uh so proprietary models are like",
    "start": "964880",
    "end": "974160"
  },
  {
    "text": "fundamentally disincentivized from giving you that level of control despite the fact that it's very critical for um",
    "start": "974160",
    "end": "981399"
  },
  {
    "text": "for like actually effectively operating the system so there needs to be that capabilities Edge um that like raw",
    "start": "981399",
    "end": "988120"
  },
  {
    "text": "capability Edge in order to make up for this fact um then lastly maybe some people",
    "start": "988120",
    "end": "996519"
  },
  {
    "text": "work in an Enterprise uh in this room don't you don't have to out yourself but",
    "start": "996519",
    "end": "1001800"
  },
  {
    "text": "maybe some um and if you're operating in this sort of situation you can't just",
    "start": "1001800",
    "end": "1007680"
  },
  {
    "text": "ship a ping to an external API out there uh into your business you people want to",
    "start": "1007680",
    "end": "1013199"
  },
  {
    "text": "know about governance people want to know about gdpr compliance um and the",
    "start": "1013199",
    "end": "1018839"
  },
  {
    "text": "one of the nice things about open a eyes offering probably true about anthropic by this point and definitely true about",
    "start": "1018839",
    "end": "1025160"
  },
  {
    "text": "Google a soon is there's a nice white glove Enterprise tier uh around this",
    "start": "1025160",
    "end": "1031240"
  },
  {
    "text": "that gives you like right underneath like you know launch an artificial",
    "start": "1031240",
    "end": "1036400"
  },
  {
    "text": "intelligence application and achieve your childhood sci-fi dreams is built-in security and compliance um we will spend",
    "start": "1036400",
    "end": "1043400"
  },
  {
    "text": "$20 billion on cyber security so that you don't have to um so this like if you",
    "start": "1043400",
    "end": "1048720"
  },
  {
    "text": "are in a situation where you need to like you know assuage concerns about data privacy um this sort of Enterprise",
    "start": "1048720",
    "end": "1057360"
  },
  {
    "text": "tier um you know sock to compliance Etc is uh can be really critical for making",
    "start": "1057360",
    "end": "1063039"
  },
  {
    "text": "your life easier uh any questions about proprietary models and such how much",
    "start": "1063039",
    "end": "1068320"
  },
  {
    "text": "more expensive is it to Runing the API cloud with uh with Azure yeah I want to",
    "start": "1068320",
    "end": "1076480"
  },
  {
    "text": "say that Azure was cheaper at the start um but maybe uh I haven't had any reason",
    "start": "1076480",
    "end": "1082919"
  },
  {
    "text": "to use it um so I'm not sure just a follow question like are there like any",
    "start": "1082919",
    "end": "1089320"
  },
  {
    "text": "um tradeoffs in general regards to Performance I it's per minute consuming",
    "start": "1089320",
    "end": "1094400"
  },
  {
    "text": "VI ah so for you mean for the Enterprise tier versus yeah so the the Enterprise",
    "start": "1094400",
    "end": "1101120"
  },
  {
    "text": "tier also offers like an actual SLA which the open AI API like doesn't um",
    "start": "1101120",
    "end": "1106360"
  },
  {
    "text": "and is like geared that's that's you know that's maybe another very critical feature besides security and compliance",
    "start": "1106360",
    "end": "1112320"
  },
  {
    "text": "like they will promise that you will get a response and not like a 500 um and",
    "start": "1112320",
    "end": "1118679"
  },
  {
    "text": "they have much more generous uh rate limits and things like that and I think",
    "start": "1118679",
    "end": "1123720"
  },
  {
    "text": "they also offer like a little bit more control over stuff so you might be able to do some fine tuning that you can't do",
    "start": "1123720",
    "end": "1129760"
  },
  {
    "text": "via the generic open a API oh fine tuning for Azure limited just like three",
    "start": "1129760",
    "end": "1135880"
  },
  {
    "text": "models okay yeah but there's 's also limits on the public API for fine tuning",
    "start": "1135880",
    "end": "1141440"
  },
  {
    "text": "right yeah um I personally did not find it particularly useful to like fine tuning API both like hard to use and not",
    "start": "1141440",
    "end": "1148760"
  },
  {
    "text": "clear benefits um I think the example from gradient did show that like if you want to achieve a style and you don't",
    "start": "1148760",
    "end": "1154720"
  },
  {
    "text": "want to spend money on like context to set up that style uh then maybe you can",
    "start": "1154720",
    "end": "1160159"
  },
  {
    "text": "win with fine tunes but it's like it's not really a successful way to inject new information so you aren't saving on",
    "start": "1160159",
    "end": "1166840"
  },
  {
    "text": "the tokens that you would retrieve um and you have to pay more to inference a fine-tuned model and that just goes",
    "start": "1166840",
    "end": "1173240"
  },
  {
    "text": "down to the fundamental like you're asking them to do more work for you um and they can amortize that cost over",
    "start": "1173240",
    "end": "1178799"
  },
  {
    "text": "fewer users and so it's just always going to be more expensive um and so uh",
    "start": "1178799",
    "end": "1185159"
  },
  {
    "text": "yeah so that limits the utility of those fine tuning apis yeah please definitely",
    "start": "1185159",
    "end": "1190520"
  },
  {
    "text": "like ask questions um uh customize to what people are interested in and also",
    "start": "1190520",
    "end": "1195880"
  },
  {
    "text": "if I don't know about something like please do uh I",
    "start": "1195880",
    "end": "1200880"
  },
  {
    "text": "interrupt great Google has ver yeah yeah so uh well so vertex is a little bit",
    "start": "1201520",
    "end": "1209440"
  },
  {
    "text": "different from this I think um I think of vertex which I was going to talk about later as sort of like a something",
    "start": "1209440",
    "end": "1216720"
  },
  {
    "text": "that I can launch my own Services into as opposed to like uh oh here's a",
    "start": "1216720",
    "end": "1221919"
  },
  {
    "text": "private version of the Palm bison API but maybe maybe is that part of vertex",
    "start": "1221919",
    "end": "1227720"
  },
  {
    "text": "okay great yeah um so yeah that's um so",
    "start": "1227720",
    "end": "1233960"
  },
  {
    "text": "they're already available for Google AI does anybody know if anthropics AWS like uh P like Enterprise offering is up",
    "start": "1233960",
    "end": "1241799"
  },
  {
    "text": "yet yes yes it is great yeah I like",
    "start": "1241799",
    "end": "1248880"
  },
  {
    "text": "refuse to make slides about this stuff more than like 48 hours in advance and I still find myself getting cut like",
    "start": "1248880",
    "end": "1256120"
  },
  {
    "text": "yeah",
    "start": "1257120",
    "end": "1260120"
  },
  {
    "text": "model so the question was uh between proprietary and open models which ones are you betting on um gambling is",
    "start": "1264679",
    "end": "1271000"
  },
  {
    "text": "illegal in the state of California so yeah um uh we'll get we'll get there so um let's talk about the open models and",
    "start": "1271000",
    "end": "1278440"
  },
  {
    "text": "then we can answer or uh open up that discussion um so open models are less",
    "start": "1278440",
    "end": "1283960"
  },
  {
    "start": "1282000",
    "end": "1841000"
  },
  {
    "text": "capable but catching up um and their hack ability is very powerful um so going back to that leader board",
    "start": "1283960",
    "end": "1291400"
  },
  {
    "text": "that I showed if you look at the next five out of 10 um four of them are llama",
    "start": "1291400",
    "end": "1297720"
  },
  {
    "text": "2 models actually so this column here to be clear is the license uh so the top",
    "start": "1297720",
    "end": "1303039"
  },
  {
    "text": "five are a proprietary like there's no uh there is no license for those weights um for the bottom five they have uh",
    "start": "1303039",
    "end": "1310799"
  },
  {
    "text": "special licenses um so these are fine tunes of meta's llama model series and",
    "start": "1310799",
    "end": "1319320"
  },
  {
    "text": "this model Series has like kind of captured mind share in the free and open source software world so a lot of the",
    "start": "1319320",
    "end": "1325799"
  },
  {
    "text": "people who are like hacking independently um and uh you know making",
    "start": "1325799",
    "end": "1331520"
  },
  {
    "text": "public git commits um and you know funded by the Linux foundation and",
    "start": "1331520",
    "end": "1336760"
  },
  {
    "text": "things like that these people are working in the main on uh adjustments to",
    "start": "1336760",
    "end": "1342039"
  },
  {
    "text": "or improvements to the Llama model series um and this is really critical",
    "start": "1342039",
    "end": "1348320"
  },
  {
    "text": "because the secret to like the success of Open Source software in general is",
    "start": "1348320",
    "end": "1353360"
  },
  {
    "text": "the ability to do this kind of like highly parallelized development where lots and lots of people are adding tiny little features and like you know going",
    "start": "1353360",
    "end": "1359919"
  },
  {
    "text": "out into the Last Mile and and adding those tiny little things that they need um and sort of like making use of all",
    "start": "1359919",
    "end": "1366960"
  },
  {
    "text": "that uh work by others so in so far as you're able to do that you are able to",
    "start": "1366960",
    "end": "1372919"
  },
  {
    "text": "provide useful open source software that can compete with software that's made by you know High remunerated teams um you",
    "start": "1372919",
    "end": "1380720"
  },
  {
    "text": "know in in Northern California um so the important question uh is this actually",
    "start": "1380720",
    "end": "1388120"
  },
  {
    "text": "uh open source so you'll notice these licenses here do not have friendly beloved names like lgpl or MIT or Apache",
    "start": "1388120",
    "end": "1396760"
  },
  {
    "text": "they they have a special unique name um and that's because meta's license for",
    "start": "1396760",
    "end": "1402200"
  },
  {
    "text": "the Llama two weights the Llama one weights were released under a research only license and were only sent to certain people and then were immediately",
    "start": "1402200",
    "end": "1408840"
  },
  {
    "text": "like torrented and the license was violated so they gave up on that uh on like fully controlling it but they did",
    "start": "1408840",
    "end": "1415480"
  },
  {
    "text": "say you cannot use the uh data or output to improve other large language models",
    "start": "1415480",
    "end": "1421159"
  },
  {
    "text": "you can only use it to improve llama models um and also release under the same license which is pretty typical",
    "start": "1421159",
    "end": "1426760"
  },
  {
    "text": "with open source um which uh is partly an attempt to sort of like capture this",
    "start": "1426760",
    "end": "1432720"
  },
  {
    "text": "like as people are doing parallel development they should only be contributing to the development of this",
    "start": "1432720",
    "end": "1438840"
  },
  {
    "text": "um this Branch um and then also if your products monthly active users in June of",
    "start": "1438840",
    "end": "1444720"
  },
  {
    "text": "2023 was 700 million users or above um you're not allowed to use it or sorry",
    "start": "1444720",
    "end": "1450279"
  },
  {
    "text": "you have to pay for a special license um and that uh so apologies to anybody you",
    "start": "1450279",
    "end": "1456360"
  },
  {
    "text": "know who's um you know if you're running an app with more than 700 million users um uh you'll have to go elsewhere I",
    "start": "1456360",
    "end": "1462880"
  },
  {
    "text": "guess but the um the key thing is that these are",
    "start": "1462880",
    "end": "1468159"
  },
  {
    "text": "violations of the like sort of agreed terms of what makes something an open source license according to the open",
    "start": "1468159",
    "end": "1473440"
  },
  {
    "text": "source initiative who you know has some uh claim to controlling how that",
    "start": "1473440",
    "end": "1480480"
  },
  {
    "text": "term is used I didn't they I don't think they ever ended up getting a trademark um but they uh you know they are got the",
    "start": "1480480",
    "end": "1487919"
  },
  {
    "text": "community aligned around a small number of licenses that and around a key set of principles that include like you can't",
    "start": "1487919",
    "end": "1493760"
  },
  {
    "text": "tell you can't say who's allowed to use this software for example which is included in the Llama license so there's",
    "start": "1493760",
    "end": "1500279"
  },
  {
    "text": "like uh a they they have opened up a multi-stakeholder process to Define open",
    "start": "1500279",
    "end": "1505360"
  },
  {
    "text": "source AI this is occurring in a time in which like the meaning of Open Source is also being contested in sort of like",
    "start": "1505360",
    "end": "1511679"
  },
  {
    "text": "software as a service so things are a little things are a little tense there",
    "start": "1511679",
    "end": "1516880"
  },
  {
    "text": "um but hopefully we'll come to an agreement as a community on what that means um so this is a fast moving space",
    "start": "1516880",
    "end": "1525200"
  },
  {
    "text": "still so just because you get mind share early on if things change rapidly that doesn't mean like llama's Locked In",
    "start": "1525200",
    "end": "1531080"
  },
  {
    "text": "Forever um so mistol for example dropped a model like two weeks ago um that at",
    "start": "1531080",
    "end": "1537600"
  },
  {
    "text": "only 7 billion parameters was outperforming um like larger models in the 13 to 30 billion parameter range and",
    "start": "1537600",
    "end": "1544039"
  },
  {
    "text": "those models were outperforming the previous models um at their size so there's like except in so far as those",
    "start": "1544039",
    "end": "1551640"
  },
  {
    "text": "things like continue to get updated um you know they uh yeah they can be out",
    "start": "1551640",
    "end": "1559039"
  },
  {
    "text": "competed um the other thing to watch out for is that there are a lot of people who are very excited about like taking",
    "start": "1559039",
    "end": "1564840"
  },
  {
    "text": "on the Death Star of open AI or whatever and um get very excited about these open",
    "start": "1564840",
    "end": "1570440"
  },
  {
    "text": "models there's also some political things about the politics of how chat gbt likes to respond to questions versus",
    "start": "1570440",
    "end": "1576440"
  },
  {
    "text": "the politics of how people who meet other people in discords like to respond to questions um and that can that sort",
    "start": "1576440",
    "end": "1584440"
  },
  {
    "text": "of like enthusiasm can lead to like pretty big errors so for example there's",
    "start": "1584440",
    "end": "1589640"
  },
  {
    "text": "a lot of excitement about these models that were take take an open Llama model",
    "start": "1589640",
    "end": "1595600"
  },
  {
    "text": "uh the the weights from that and then grab like 10,000 requests from the openai API or scrape like r/ chat gbt or",
    "start": "1595600",
    "end": "1604720"
  },
  {
    "text": "whatever and then just fine-tune like now there's a data set use it to fine-tune the model and those are the",
    "start": "1604720",
    "end": "1610480"
  },
  {
    "text": "ones that were like up there on the arena uh on that uh like ELO ranking um",
    "start": "1610480",
    "end": "1617039"
  },
  {
    "text": "from the leader board that I showed before um and the there was a claim that",
    "start": "1617039",
    "end": "1622440"
  },
  {
    "text": "these uh had like 90% of chat gbt's quality there are only 7 billion parameters like you know people were",
    "start": "1622440",
    "end": "1629520"
  },
  {
    "text": "like very enthusiastic about this back in April when I was talking about this a lot of people are like why are you even talking about open AI anymore we like",
    "start": "1629520",
    "end": "1636399"
  },
  {
    "text": "you know vun has done it and like yeah uh so um this is a fake",
    "start": "1636399",
    "end": "1641760"
  },
  {
    "text": "screenshot uh that's uh from a paper or that I modified from from a paper about",
    "start": "1641760",
    "end": "1647919"
  },
  {
    "text": "uh about this this topic of how well these models work um so this is one",
    "start": "1647919",
    "end": "1653279"
  },
  {
    "text": "output from a language model anonymized um on answer how does actor",
    "start": "1653279",
    "end": "1658600"
  },
  {
    "text": "critic improve over reinforce um so this is an algorithm from reinforcement learning so one language model says uh",
    "start": "1658600",
    "end": "1666279"
  },
  {
    "text": "actor critic algorithms are an extension of reinforc that combine both policy based and value based methods um it's",
    "start": "1666279",
    "end": "1672840"
  },
  {
    "text": "got a Critic Network it's got Advantage estimation it's got function approximation so it's one answer that's",
    "start": "1672840",
    "end": "1678600"
  },
  {
    "text": "a answer B uh actor critic algorithms are a type of reinforcement learning algorithm that improves the reinforcment",
    "start": "1678600",
    "end": "1685640"
  },
  {
    "text": "algorithm by combining actor policy and critic value components actor critic algorithms use a single critic",
    "start": "1685640",
    "end": "1691480"
  },
  {
    "text": "reinforced has a separate critic for each action actor critic algorithms learn the policy and actor simultaneously but reinforced learns",
    "start": "1691480",
    "end": "1698399"
  },
  {
    "text": "them separately um so you know those might seem fairly similar so does",
    "start": "1698399",
    "end": "1703440"
  },
  {
    "text": "anybody have a strong preference for uh answer a here",
    "start": "1703440",
    "end": "1709440"
  },
  {
    "text": "anybody have a weak preference for answer a over answer B",
    "start": "1711000",
    "end": "1716039"
  },
  {
    "text": "here some people raising like a like a a soft hand maybe yeah um anybody have a",
    "start": "1716039",
    "end": "1723960"
  },
  {
    "text": "strong preference for answer B maybe one maybe two and a weak",
    "start": "1723960",
    "end": "1730360"
  },
  {
    "text": "preference for answer B this one's got like a they they both got these nice numbered lists you know which looks very",
    "start": "1730360",
    "end": "1737080"
  },
  {
    "text": "author reminds me of a medium article which is likely to be true of course um uh so the",
    "start": "1737080",
    "end": "1744480"
  },
  {
    "text": "uh so uh answer B comes from uh I want to say this was",
    "start": "1744480",
    "end": "1750360"
  },
  {
    "text": "GPT 4 yeah um answer B comes from gp4",
    "start": "1750360",
    "end": "1755600"
  },
  {
    "text": "and oh wait wait or a sorry answer a comes from gp4 and has the advantage of",
    "start": "1755600",
    "end": "1762000"
  },
  {
    "text": "being correct um answer B comes from one of the uh fine-tuned models",
    "start": "1762000",
    "end": "1767960"
  },
  {
    "text": "um and is like gibberish basically um and so this just you know like just",
    "start": "1767960",
    "end": "1775519"
  },
  {
    "text": "having humans rate the outputs of of language models in the way that a lot of those uh leaderboards were constructed",
    "start": "1775519",
    "end": "1781519"
  },
  {
    "text": "um did not like it didn't have any grounding in the actual utility of the answers it was just a lot of people",
    "start": "1781519",
    "end": "1786919"
  },
  {
    "text": "going like looks good to me like nice yes merge um",
    "start": "1786919",
    "end": "1792640"
  },
  {
    "text": "and um without like knowing whether it was actually right or not um and so uh",
    "start": "1792640",
    "end": "1798720"
  },
  {
    "text": "there's a nice paper from some folks at uh Berkeley about um sort of walking through like what's going on basically",
    "start": "1798720",
    "end": "1805120"
  },
  {
    "text": "the models are picking up style from a fine tune which is things like that delightful little split into bullet",
    "start": "1805120",
    "end": "1811519"
  },
  {
    "text": "points and like you know like a very authoritative and friendly educational style um but not like actual knowledge",
    "start": "1811519",
    "end": "1818720"
  },
  {
    "text": "not like reasoning capabilities and like a lot of people in the open modeling communities like sort of missed this or",
    "start": "1818720",
    "end": "1826120"
  },
  {
    "text": "like willfully ignored it um some of the sharpest people were definitely up on this like the guano paper for example",
    "start": "1826120",
    "end": "1832200"
  },
  {
    "text": "mentions that there's uh some issues with evaluation came out before this paper um but definitely a lot of people",
    "start": "1832200",
    "end": "1838000"
  },
  {
    "text": "missed it um so the the immediate question that",
    "start": "1838000",
    "end": "1843320"
  },
  {
    "start": "1841000",
    "end": "2179000"
  },
  {
    "text": "comes up is like between these open models and these uh proprietary models who's going to win long term like who",
    "start": "1843320",
    "end": "1850039"
  },
  {
    "text": "should I bet on um and in some ways I think that's a bit of a misguided",
    "start": "1850039",
    "end": "1855240"
  },
  {
    "text": "question um so consider operating systems uh like the first operating",
    "start": "1855240",
    "end": "1861399"
  },
  {
    "text": "system roughly was system 360 from IBM on mainframes extremely closed um in the",
    "start": "1861399",
    "end": "1867880"
  },
  {
    "text": "80s there was a rash of operating systems most of them closed the original Xerox pilot on the Xerox star uh was",
    "start": "1867880",
    "end": "1875799"
  },
  {
    "text": "extremely closed Doss and wind Doss was closed uh Mac OS at the time was like completely closed um and there were Unix",
    "start": "1875799",
    "end": "1883399"
  },
  {
    "text": "operating systems that were kind of like mixed um then over time the like closed",
    "start": "1883399",
    "end": "1888880"
  },
  {
    "text": "versions of Unix lost out to more like friendly licensed ones and in particular",
    "start": "1888880",
    "end": "1894240"
  },
  {
    "text": "to gnu Linux um and there's been a bit of a trend towards open uh open operating systems kind of taking more",
    "start": "1894240",
    "end": "1900919"
  },
  {
    "text": "mind and market share over time like data centers have more Linux in them now than they did in 2005 and then than they",
    "start": "1900919",
    "end": "1907519"
  },
  {
    "text": "did in 1995 but uh from what I can tell it's still like 70% plus Windows um for",
    "start": "1907519",
    "end": "1914000"
  },
  {
    "text": "uh for operating web servers in Mobile phones we also have have an open operating system and a closed operating",
    "start": "1914000",
    "end": "1919679"
  },
  {
    "text": "system and these things have been able to coexist and serve different needs for different organizations throughout like",
    "start": "1919679",
    "end": "1926240"
  },
  {
    "text": "the history of operating systems um and the same is true of",
    "start": "1926240",
    "end": "1932080"
  },
  {
    "text": "databases uh so back in the 70s and 80s it was Oracle and IBM's dbt2 um which is",
    "start": "1932080",
    "end": "1940159"
  },
  {
    "text": "still around I found out um like you live too long in in the in San Francisco",
    "start": "1940159",
    "end": "1945639"
  },
  {
    "text": "and you forget that there are people who IBM db2 um in the 9s there was some like",
    "start": "1945639",
    "end": "1951360"
  },
  {
    "text": "consolidation around more open implementations of of SQL uh and in the",
    "start": "1951360",
    "end": "1958039"
  },
  {
    "text": "2000s to 2010s there was the no SQL movement but that was still like mostly",
    "start": "1958039",
    "end": "1963639"
  },
  {
    "text": "open source databases uh so there's been a lot of like movement in the direction of open",
    "start": "1963639",
    "end": "1968679"
  },
  {
    "text": "databases um with streaming databases we have both Pro proprietary and open options if you look at the top 10 uh",
    "start": "1968679",
    "end": "1975600"
  },
  {
    "text": "databases as ranked by DB engines.com which is you could quibble with the",
    "start": "1975600",
    "end": "1980760"
  },
  {
    "text": "ranking thing but the the key point is that there are like it's like half and half split between um between",
    "start": "1980760",
    "end": "1987600"
  },
  {
    "text": "proprietary databases and um and open source databases uh and that has been like",
    "start": "1987600",
    "end": "1994880"
  },
  {
    "text": "relatively stable over time with like a soft maybe a soft Trend in the direction of open uh",
    "start": "1994880",
    "end": "2000279"
  },
  {
    "text": "databases uh so with these like very like language models Foundation models are this very like lowlevel the",
    "start": "2000279",
    "end": "2007799"
  },
  {
    "text": "component of a software stack more like an like an operating system or a database I think than like um you know",
    "start": "2007799",
    "end": "2014639"
  },
  {
    "text": "than a SAS app or a or a UI uh and because of that they're likely to be",
    "start": "2014639",
    "end": "2019679"
  },
  {
    "text": "subject to some of these same forces that say there's some people who want to works one way there's some people who want it to work in another and for some",
    "start": "2019679",
    "end": "2026440"
  },
  {
    "text": "of them that openness that hack ability is going to be critical for others the like reliability the existence of a",
    "start": "2026440",
    "end": "2032480"
  },
  {
    "text": "white glove Enterprise version is going to be really critical um and those will allow these two things things to",
    "start": "2032480",
    "end": "2037960"
  },
  {
    "text": "coexist um and uh yeah and the CEO of hugging",
    "start": "2037960",
    "end": "2044360"
  },
  {
    "text": "face liked my tweet when I said that so it's probably true um but I think a lot of people are",
    "start": "2044360",
    "end": "2052000"
  },
  {
    "text": "like well no but like who's like who's going to win like who should I bet on um",
    "start": "2052000",
    "end": "2057240"
  },
  {
    "text": "and I think the closest thing to an answer that I have is that if capabilities requirements saturate if",
    "start": "2057240",
    "end": "2063040"
  },
  {
    "text": "people no longer want the absolute smartest model out there they just want a model smart enough for XYZ then open",
    "start": "2063040",
    "end": "2069118"
  },
  {
    "text": "models will probably catch up and then like start to dominate um the thing that keeps open models behind prary models is",
    "start": "2069119",
    "end": "2075638"
  },
  {
    "text": "the extreme expense of maintaining a large resource team and like you know continually constructing new data",
    "start": "2075639",
    "end": "2082320"
  },
  {
    "text": "centers at an increased scale um to the tune of like $500 million in order to",
    "start": "2082320",
    "end": "2087398"
  },
  {
    "text": "hit that next capability level before everybody else but you know uh at a",
    "start": "2087399",
    "end": "2092679"
  },
  {
    "text": "certain point processors got fast enough that people were not like clamoring for the next upgrade as soon as possible um",
    "start": "2092679",
    "end": "2099440"
  },
  {
    "text": "and at that point we're starting to see like a little bit more opening up in the sort of like in the chip space with like brisk v u and so like at like with in",
    "start": "2099440",
    "end": "2106880"
  },
  {
    "text": "any number of other technological domains you've seen that when requirements start to saturate um then",
    "start": "2106880",
    "end": "2113079"
  },
  {
    "text": "like open uh like open versions can catch up um if they are unbounded and",
    "start": "2113079",
    "end": "2119680"
  },
  {
    "text": "it's like you know uh what a good example like Ram like nobody has enough",
    "start": "2119680",
    "end": "2125040"
  },
  {
    "text": "RAM everybody wants more RAM I don't think there are any like open like attempts to make like an open Ram",
    "start": "2125040",
    "end": "2130599"
  },
  {
    "text": "architecture or something um and that's because and one reason why I think is capabilities requirements there are",
    "start": "2130599",
    "end": "2137599"
  },
  {
    "text": "remain unbounded um and so um if that's the case for uh cognition and AI models",
    "start": "2137599",
    "end": "2144960"
  },
  {
    "text": "then proprietary models should be able to maintain that edge in in capabilities which would sort of tilt the balance in",
    "start": "2144960",
    "end": "2151319"
  },
  {
    "text": "favor more people would say oh no I need this this proprietary thing um so the",
    "start": "2151319",
    "end": "2156960"
  },
  {
    "text": "closest to an answer that I have um yeah any questions on that front before we",
    "start": "2156960",
    "end": "2162040"
  },
  {
    "text": "dive into uh um where we actually run these",
    "start": "2162040",
    "end": "2166960"
  },
  {
    "text": "models",
    "start": "2170880",
    "end": "2173880"
  },
  {
    "text": "yeah yeah um so the question was what kind of language support do these models",
    "start": "2176400",
    "end": "2181960"
  },
  {
    "start": "2179000",
    "end": "2652000"
  },
  {
    "text": "have um and because it's only an API call away you can of course use python",
    "start": "2181960",
    "end": "2187480"
  },
  {
    "text": "or node or whatever you want uh no so the question was about like these are language modeling like machines what",
    "start": "2187480",
    "end": "2194040"
  },
  {
    "text": "languages do they model um and the basic answer is that the more text in that",
    "start": "2194040",
    "end": "2200240"
  },
  {
    "text": "language that is available on the open internet the better the language models will be on on that language so they are",
    "start": "2200240",
    "end": "2208119"
  },
  {
    "text": "like I want to say gbd4 is smarter in maybe smarter in malayum than it is",
    "start": "2208119",
    "end": "2216319"
  },
  {
    "text": "in Mandarin um I forget there's like some interesting inversions of like number of people who speak the language",
    "start": "2216319",
    "end": "2222880"
  },
  {
    "text": "versus how how intelligent the language models are um so I think a lot of them release benchmarks that say like how",
    "start": "2222880",
    "end": "2229000"
  },
  {
    "text": "multilingual is this language model and for Which languages",
    "start": "2229000",
    "end": "2234200"
  },
  {
    "text": "um there is you run into the fundamental token constraint of like you need uh you",
    "start": "2234200",
    "end": "2241440"
  },
  {
    "text": "need existing like you need examples of that language that you can get a hold of in order to train the model in them um",
    "start": "2241440",
    "end": "2248720"
  },
  {
    "text": "and there just are more English tokens",
    "start": "2248720",
    "end": "2253800"
  },
  {
    "text": "um but for a given capacity you can probably achieve like higher quality in",
    "start": "2253800",
    "end": "2258960"
  },
  {
    "text": "a specific model by looking for uh by looking for a model trained in that",
    "start": "2258960",
    "end": "2264160"
  },
  {
    "text": "language so there's definitely some like good old nationalist European Endeavors to make like a French language model",
    "start": "2264160",
    "end": "2270440"
  },
  {
    "text": "that insults you if you ask it for stuff in English um which it of course picks up just from Reading French",
    "start": "2270440",
    "end": "2278520"
  },
  {
    "text": "um um but yeah but the but the core models like they support English really well the instruction fine-tuning and the",
    "start": "2278520",
    "end": "2284880"
  },
  {
    "text": "RF is actually mostly applied to them in English since the annotators who uh",
    "start": "2284880",
    "end": "2290319"
  },
  {
    "text": "enforce that policy uh through their examples are mostly writing in English",
    "start": "2290319",
    "end": "2295400"
  },
  {
    "text": "um so fun fact you can get Chad gbt to and Claude probably to tell you how to build a bomb if you ask in the right low",
    "start": "2295400",
    "end": "2301480"
  },
  {
    "text": "resource language um uh just fun facts about language model",
    "start": "2301480",
    "end": "2307280"
  },
  {
    "text": "um yeah so that that does that that is a problem and it does sort of like in uh",
    "start": "2307280",
    "end": "2313319"
  },
  {
    "text": "it has a multiplying effect on the English languages kind of like cultural hegemony um which is a bit unfortunate",
    "start": "2313319",
    "end": "2319880"
  },
  {
    "text": "yeah so for the languages",
    "start": "2319880",
    "end": "2325160"
  },
  {
    "text": "representability lower oringer also are there Arbitrage",
    "start": "2326440",
    "end": "2332240"
  },
  {
    "text": "opportunities in translating first transl first and",
    "start": "2332240",
    "end": "2340280"
  },
  {
    "text": "then yeah I'm unaware of any AC but you know any benchmarking work on this my",
    "start": "2340280",
    "end": "2346599"
  },
  {
    "text": "gut tells me that translating to English first doing Chain of Thought and then translating back to the original",
    "start": "2346599",
    "end": "2352079"
  },
  {
    "text": "language would work better um you you're kind of like wondering whether the Lost in Translation effect is bigger than the",
    "start": "2352079",
    "end": "2360319"
  },
  {
    "text": "like boost of Chain of Thought in English a lot of the like circuits and language models are very token specific",
    "start": "2360319",
    "end": "2367800"
  },
  {
    "text": "um and then yeah so it's like the like just one example if you ask it who Tom",
    "start": "2367800",
    "end": "2373280"
  },
  {
    "text": "Cruz's mother is then it answers better than if you ask it that woman's name's son I don't know her her name at all um",
    "start": "2373280",
    "end": "2381079"
  },
  {
    "text": "so I can't really do this example effectively um jbt wins again um but the",
    "start": "2381079",
    "end": "2386920"
  },
  {
    "text": "uh there so that's like an example of a very of a token specific circuit it's like related to Tom Cruz uh and so you",
    "start": "2386920",
    "end": "2393200"
  },
  {
    "text": "can see like it's not reasoning the way that a person would or that or that you",
    "start": "2393200",
    "end": "2398359"
  },
  {
    "text": "would guess from like you know how how you would think about a knowledge graph or something like that um and so that's",
    "start": "2398359",
    "end": "2404880"
  },
  {
    "text": "where you get these unintuitive things um but yeah so I saw",
    "start": "2404880",
    "end": "2410680"
  },
  {
    "text": "you used to may still deep learning oh",
    "start": "2410680",
    "end": "2415960"
  },
  {
    "text": "yeah and I was wondering one of the pieces of the whole event is okay there is these reasoning service AP now right",
    "start": "2415960",
    "end": "2425359"
  },
  {
    "text": "can",
    "start": "2425359",
    "end": "2428359"
  },
  {
    "text": "without yeah that's a great question um for individuals I think I think it's a",
    "start": "2450400",
    "end": "2456720"
  },
  {
    "text": "matter of your personal interest in understanding the modeling",
    "start": "2456720",
    "end": "2465400"
  },
  {
    "text": "like I guess the analogy I would immediately jump to is as an individual developer you can get away with not",
    "start": "2465400",
    "end": "2471599"
  },
  {
    "text": "knowing anything about databases like I have done that I couldn't write a b tree right now I don't want to ever learn how",
    "start": "2471599",
    "end": "2478079"
  },
  {
    "text": "to do that like and to think about page sizes and yeah it makes me ill to think",
    "start": "2478079",
    "end": "2483160"
  },
  {
    "text": "about that and whereas I get excited if I wake up in the morning I can think about beian inference in language models",
    "start": "2483160",
    "end": "2490040"
  },
  {
    "text": "and so as an individual I think you can like kind of be guided by your like what you find most exciting as a team and as",
    "start": "2490040",
    "end": "2497480"
  },
  {
    "text": "an organization though if you have nobody who understands databases in your organization you're probably going to be in trouble um just like it ends up like",
    "start": "2497480",
    "end": "2506480"
  },
  {
    "text": "most applications require like pretty decent knowledge of databases and when they go down or when they need to be",
    "start": "2506480",
    "end": "2512359"
  },
  {
    "text": "configured even if you are using red shift or or you know you're using some managed service being able to like",
    "start": "2512359",
    "end": "2520599"
  },
  {
    "text": "understand some stuff about them is actually critical for debugging and being able to know when you need to switch manage services or like yeah or",
    "start": "2520599",
    "end": "2527680"
  },
  {
    "text": "how to reconfigure them so I think like the direction that we're going to go is to evolve there it's a question of",
    "start": "2527680",
    "end": "2533079"
  },
  {
    "text": "whether you want to be uh uh a site reliability engineer focused",
    "start": "2533079",
    "end": "2539240"
  },
  {
    "text": "on llm reliability or um you know a a a a modeling engineer or whether you want",
    "start": "2539240",
    "end": "2545119"
  },
  {
    "text": "to be like more at the like application layer um yeah so just an interesting one what's",
    "start": "2545119",
    "end": "2552200"
  },
  {
    "text": "your personal opinion on this there's a handful of companies that have gotten recent funding to build like vertical",
    "start": "2552200",
    "end": "2557680"
  },
  {
    "text": "oriented commercial models Finance healthare Etc yeah yeah so the question was what about",
    "start": "2557680",
    "end": "2564839"
  },
  {
    "text": "these models that are foundational but like less broad so it's like a",
    "start": "2564839",
    "end": "2571079"
  },
  {
    "text": "foundational model for law a foundational model for um for healthcare",
    "start": "2571079",
    "end": "2576760"
  },
  {
    "text": "yeah I um my experience has been that if you",
    "start": "2576760",
    "end": "2582520"
  },
  {
    "text": "bet that some capability is not going to be available in a language model um or",
    "start": "2582520",
    "end": "2588079"
  },
  {
    "text": "in a foundation model like you will get you will lose that bet um uh so just as",
    "start": "2588079",
    "end": "2593640"
  },
  {
    "text": "an example in the Deep learning boot camp uh we spent a long time trying to",
    "start": "2593640",
    "end": "2599280"
  },
  {
    "text": "make an optical character recognition system uh and it's like you know it's the like Pinnacle of the class when you",
    "start": "2599280",
    "end": "2605319"
  },
  {
    "text": "can finally like deploy a web service that does optical character recognition and that's like an accidental side",
    "start": "2605319",
    "end": "2610960"
  },
  {
    "text": "feature of GPT 4V and it's like better at it than the thing that we built um and a lot of ml teams have experienced",
    "start": "2610960",
    "end": "2618319"
  },
  {
    "text": "something like that um so I worry I would worry if if somebody were like",
    "start": "2618319",
    "end": "2623520"
  },
  {
    "text": "offering me that as a job opportunity for example I would worry that it's going to get like scooped on either side",
    "start": "2623520",
    "end": "2629280"
  },
  {
    "text": "by a hyper specific model that's like 10 times more efficient and isn't like a generic Healthcare model but is like a",
    "start": "2629280",
    "end": "2635960"
  },
  {
    "text": "um uh ultrasound for the heart model one I worked on before",
    "start": "2635960",
    "end": "2642680"
  },
  {
    "text": "um yeah or just send it to the chat GPT API or to the GPT",
    "start": "2642680",
    "end": "2650280"
  },
  {
    "text": "API great questions um so uh maybe another reason to think that there might",
    "start": "2650280",
    "end": "2656680"
  },
  {
    "start": "2652000",
    "end": "3872000"
  },
  {
    "text": "be like a little more Alpha in in actually learning more about the models",
    "start": "2656680",
    "end": "2662240"
  },
  {
    "text": "is um inference doesn't have to be executed Network it doesn't have to be",
    "start": "2662240",
    "end": "2667359"
  },
  {
    "text": "executed like in some Central server there are lots of reasons why you might want to execute your uh uh your",
    "start": "2667359",
    "end": "2676680"
  },
  {
    "text": "inference on an enduser device um so we'll talk about the different types of end user devices and the different",
    "start": "2676680",
    "end": "2682720"
  },
  {
    "text": "constraints that they put on inference and the implications um like engineering and",
    "start": "2682720",
    "end": "2687760"
  },
  {
    "text": "strategic and then also um talk about what the options are for doing things over a",
    "start": "2687760",
    "end": "2693680"
  },
  {
    "text": "network um so running stuff for end users is like uh like where the sorry",
    "start": "2693680",
    "end": "2699040"
  },
  {
    "text": "the end user actually executes it themselves is not quite there yet but it's like uh it's getting there and",
    "start": "2699040",
    "end": "2705040"
  },
  {
    "text": "maybe a little bit faster than I personally expected um like I've run llama",
    "start": "2705040",
    "end": "2710599"
  },
  {
    "text": "2 13B on this very laptop um without it",
    "start": "2710599",
    "end": "2715680"
  },
  {
    "text": "catching on fire um so there's uh there's some hope uh that there will",
    "start": "2715680",
    "end": "2721200"
  },
  {
    "text": "that that will continue to get better um and so this uh this is critical for",
    "start": "2721200",
    "end": "2727839"
  },
  {
    "text": "latency sensitive applications so like being able to actually execute the inference at the same place that the",
    "start": "2727839",
    "end": "2734520"
  },
  {
    "text": "user where at the same place where the user is um and the reason why it goes back to this like this famous set of",
    "start": "2734520",
    "end": "2740960"
  },
  {
    "text": "numbers every engineer should know from uh Peter norvig and Jeff Dean at Google",
    "start": "2740960",
    "end": "2746359"
  },
  {
    "text": "um which is that the time it takes to send a packet just one packet so",
    "start": "2746359",
    "end": "2752839"
  },
  {
    "text": "probably this probably isn't even a whole HTP request I'd have to check again but let's just say you send",
    "start": "2752839",
    "end": "2757880"
  },
  {
    "text": "information back and forth from like here in California to Europe and back it's 150 milliseconds um and there's",
    "start": "2757880",
    "end": "2766960"
  },
  {
    "text": "like a number of kind of madeup numbers in the ux world about like how fast you",
    "start": "2766960",
    "end": "2772160"
  },
  {
    "text": "need to be for something to feel interactive um so one of them going back to like the 70s or 80s it's the Doty",
    "start": "2772160",
    "end": "2779240"
  },
  {
    "text": "threshold which says the user and the computer can interact with each other in under 400 milliseconds then the like",
    "start": "2779240",
    "end": "2785960"
  },
  {
    "text": "human won't feel like they're waiting on the computer and as you're programming things you won't like end up blocked on",
    "start": "2785960",
    "end": "2792200"
  },
  {
    "text": "human input um but you'll and you'll still have plenty of time for doing stuff in inside threads and things like",
    "start": "2792200",
    "end": "2797680"
  },
  {
    "text": "that uh so if you like if you have to like do a network call every single time",
    "start": "2797680",
    "end": "2803200"
  },
  {
    "text": "you're using up like a third of your budget just on like waiting for information to come back and now you're going to spend a ton of engineering",
    "start": "2803200",
    "end": "2809359"
  },
  {
    "text": "effort on like trying to find ways things that you can do asynchronously during that like P like that Network",
    "start": "2809359",
    "end": "2815319"
  },
  {
    "text": "call and like you can you can work around it but it is punishing um and",
    "start": "2815319",
    "end": "2820520"
  },
  {
    "text": "that's like there are even tighter like reaction time things like if you have a self-driving car um you can't wait 150",
    "start": "2820520",
    "end": "2827920"
  },
  {
    "text": "milliseconds uh to find out that you need to break",
    "start": "2827920",
    "end": "2833359"
  },
  {
    "text": "um so uh yeah so and the nice thing about this uh the other benefit to it",
    "start": "2833359",
    "end": "2839680"
  },
  {
    "text": "beside it being necessary in some places is that if end users run the computation",
    "start": "2839680",
    "end": "2844839"
  },
  {
    "text": "then you don't need to pay for it um so your inferencing costs can be",
    "start": "2844839",
    "end": "2851200"
  },
  {
    "text": "0 which would be which would be great um so the cost that you pay um is control",
    "start": "2851200",
    "end": "2857920"
  },
  {
    "text": "so you have less control over the execution environment um your ability to do Telemetry and see what is going on is",
    "start": "2857920",
    "end": "2864800"
  },
  {
    "text": "limited uh people don't like it when you like carefully observe their activity",
    "start": "2864800",
    "end": "2870480"
  },
  {
    "text": "using software on their machine but if they you put the same software at a UR",
    "start": "2870480",
    "end": "2875680"
  },
  {
    "text": "URL you can spy on them as much as you want and they don't get mad um so you lose out on Telemetry you uh have to",
    "start": "2875680",
    "end": "2882920"
  },
  {
    "text": "worry about compatibility with different execution environments and you have to actually support past versions unlike uh",
    "start": "2882920",
    "end": "2888760"
  },
  {
    "text": "if you're running it as a service um or rather if you like you know control the execution",
    "start": "2888760",
    "end": "2894880"
  },
  {
    "text": "environment um so the things that are unlocked by this are some of the best applications here uh like some of the",
    "start": "2894880",
    "end": "2901480"
  },
  {
    "text": "most exciting ones especially to me so uh use on Smart use in smart phones using robots using wearables um so",
    "start": "2901480",
    "end": "2908920"
  },
  {
    "text": "Google just in the past couple days announced that the pixel Pro 8 um is",
    "start": "2908920",
    "end": "2914480"
  },
  {
    "text": "going to have uh large language models directly on device um they mostly showed off stuff that looked like kind of like",
    "start": "2914480",
    "end": "2919839"
  },
  {
    "text": "summarization and some like light image editing so not like full on like you know like Hey Siri um why did the",
    "start": "2919839",
    "end": "2927280"
  },
  {
    "text": "Ottoman Empire fall like I don't know what you talk about with chat gbt um but",
    "start": "2927280",
    "end": "2932520"
  },
  {
    "text": "uh like it's not quite that level but it's a move in that direction and a trend we can expect to kind of continue",
    "start": "2932520",
    "end": "2938400"
  },
  {
    "text": "getting that inference onto the device um and uh there was also a recent",
    "start": "2938400",
    "end": "2944680"
  },
  {
    "text": "Hardware hack um on like using you know getting this inference on mobile",
    "start": "2944680",
    "end": "2950280"
  },
  {
    "text": "robotics platforms um and so there's there was a ton of cool applications there like um yeah some stuff with like",
    "start": "2950280",
    "end": "2956559"
  },
  {
    "text": "3D like Point Cloud rendering for for your inside your house like a Roomba you",
    "start": "2956559",
    "end": "2961880"
  },
  {
    "text": "can control with your voice very cool stuff um the the constraints that appear here uh that",
    "start": "2961880",
    "end": "2969799"
  },
  {
    "text": "you'll have to engineer around are like very tight Hardware constraints so um",
    "start": "2969799",
    "end": "2975680"
  },
  {
    "text": "there there's memory limits both disk and like vram and RAM are like are",
    "start": "2975680",
    "end": "2981559"
  },
  {
    "text": "extremely tight and like current language models you can always trade more up to points where you're spending",
    "start": "2981559",
    "end": "2989559"
  },
  {
    "text": "like $100,000 on a machine you can trade more money for smarter models um and and",
    "start": "2989559",
    "end": "2996119"
  },
  {
    "text": "phones are down at like gigabytes uh low gigabytes of RAM uh like yeah um was",
    "start": "2996119",
    "end": "3003000"
  },
  {
    "text": "running a language model on a single board computer and that had like two gigabytes of shared Ram between the CPU and GPU not a lot of space um and the",
    "start": "3003000",
    "end": "3012079"
  },
  {
    "text": "like real uh deep limit is power um it's",
    "start": "3012079",
    "end": "3018160"
  },
  {
    "text": "uh uh or the sorry there's a limit on power which is like an a100 uh which you",
    "start": "3018160",
    "end": "3023799"
  },
  {
    "text": "might use for inference draws 300 watts of power and something like the single board computer was using the Jetson Nano",
    "start": "3023799",
    "end": "3029559"
  },
  {
    "text": "that's 10 watts of power so a factor of 30 not going to make that up anytime soon um and underneath both of these is",
    "start": "3029559",
    "end": "3036960"
  },
  {
    "text": "the problem of heat dissipation um there that's like a really like tough thing to deal with when you are in these like",
    "start": "3036960",
    "end": "3043559"
  },
  {
    "text": "small environments um and like prevents them from just being like Oh I'll just",
    "start": "3043559",
    "end": "3048720"
  },
  {
    "text": "like make a chip where you can actually move like nine pedabytes a second um",
    "start": "3048720",
    "end": "3054040"
  },
  {
    "text": "like across an inch and it's like uh like you just do some like back of the envelope math and it's like that's going",
    "start": "3054040",
    "end": "3059880"
  },
  {
    "text": "to like egress so much heat the thing's going to catch on fire um so um yeah",
    "start": "3059880",
    "end": "3068079"
  },
  {
    "text": "this is we're talking about some hardcore engineering stuff here um all right so the like mobile",
    "start": "3068079",
    "end": "3075280"
  },
  {
    "text": "environments uh maybe like further out in the future to get uh large capabilities onto them but um not",
    "start": "3075280",
    "end": "3081880"
  },
  {
    "text": "impossible what about um what about other consumer Hardware desktops um",
    "start": "3081880",
    "end": "3086960"
  },
  {
    "text": "which are a place where you could have video games with actual artificial intelligence in them uh operating system",
    "start": "3086960",
    "end": "3092480"
  },
  {
    "text": "level assistance n native apps with these like kinds of features that we're starting to see in um in browser apps uh",
    "start": "3092480",
    "end": "3100720"
  },
  {
    "text": "so you still run like you run into even more heterogeneous hardware and that's going to give you different constraints",
    "start": "3100720",
    "end": "3106520"
  },
  {
    "text": "depending on the system that you're on and that is going to require like really heterogeneous software to meet those",
    "start": "3106520",
    "end": "3112680"
  },
  {
    "text": "constraints like you probably can't assume that that everybody has an Nvidia 30 series or later GPU even though it",
    "start": "3112680",
    "end": "3119160"
  },
  {
    "text": "would make your life a lot easier and you probably can't assume that you can use up all the RAM on that uh uh uh you",
    "start": "3119160",
    "end": "3126200"
  },
  {
    "text": "know on that chip even if it would make your life easier um I think the long",
    "start": "3126200",
    "end": "3131960"
  },
  {
    "text": "term we might be able to expect ecosystems to adjust around the requirements of these workloads a bit so like kind of uh like make it uh like",
    "start": "3131960",
    "end": "3140160"
  },
  {
    "text": "make cleaner interfaces for using these things so you don't have to write 15 different versions um or write a make",
    "start": "3140160",
    "end": "3145799"
  },
  {
    "text": "file that looks like llama cpps don't look at it um uh very scary uh there's",
    "start": "3145799",
    "end": "3151799"
  },
  {
    "text": "kind of a sweet spot actually in what little like next generation of video game consoles because you have you have",
    "start": "3151799",
    "end": "3157680"
  },
  {
    "text": "total authority to just use up as much of the system as you want people pay lots of money for them they often build",
    "start": "3157680",
    "end": "3163920"
  },
  {
    "text": "custom silicon based on what developers want so that could be if you're thinking about what you want to be doing in like",
    "start": "3163920",
    "end": "3169760"
  },
  {
    "text": "5 years seven years in this field consider that as a possibility lots of people would love to to have um a real",
    "start": "3169760",
    "end": "3177680"
  },
  {
    "text": "like human-like intelligence in the um uh in the things they're shooting in",
    "start": "3177680",
    "end": "3184200"
  },
  {
    "text": "their first person shooter you know like that that I think that would make a lot of money yeah when it comes to building",
    "start": "3184200",
    "end": "3190119"
  },
  {
    "text": "ESP for mobile",
    "start": "3190119",
    "end": "3194799"
  },
  {
    "text": "yeah yeah so the question was for mobile Hardware like what are solutions and",
    "start": "3196799",
    "end": "3202440"
  },
  {
    "text": "specifically quantization so um when one of the key constraints is memory like",
    "start": "3202440",
    "end": "3209359"
  },
  {
    "text": "just trying to make the size of the model smaller and the size of the computation smaller is helpful so the",
    "start": "3209359",
    "end": "3215920"
  },
  {
    "text": "like people are pushing to try and take the parameters of language models down from being two bytes to one bite to half",
    "start": "3215920",
    "end": "3223839"
  },
  {
    "text": "a bite to like a single bit um and I think people are kind of stalling out at",
    "start": "3223839",
    "end": "3229040"
  },
  {
    "text": "the like half bite level um and often to actually recognize those gains you need",
    "start": "3229040",
    "end": "3236920"
  },
  {
    "text": "to like write assembler and stuff it's like it can get pretty gnarly um so",
    "start": "3236920",
    "end": "3244280"
  },
  {
    "text": "that's often only like highly resourced teams working for a long time that they can actually see those benefits um the",
    "start": "3244280",
    "end": "3249960"
  },
  {
    "text": "other thing that people talk about a lot uh for like making models work on smaller devices is",
    "start": "3249960",
    "end": "3255000"
  },
  {
    "text": "sparity um and so sparsity means like oh there's this giant weight Matrix maybe",
    "start": "3255000",
    "end": "3260160"
  },
  {
    "text": "most of them are close to zero and maybe we can just like get rid of those like if we were going to go to one bit",
    "start": "3260160",
    "end": "3265599"
  },
  {
    "text": "there's zero or one like why not there's zeros and then zero is like a very easy number to work with like you the number",
    "start": "3265599",
    "end": "3272160"
  },
  {
    "text": "that comes out is multiply it's zero add you just keep the number so it's you don't need like a full logic circuit to",
    "start": "3272160",
    "end": "3278440"
  },
  {
    "text": "handle it um so there are some things that make use of sparsity the problem is",
    "start": "3278440",
    "end": "3283960"
  },
  {
    "text": "that the type of sparsity that neural networks need is called unstructured sparsity you have just have zeros kind",
    "start": "3283960",
    "end": "3289280"
  },
  {
    "text": "of like scattered around your Matrix multiply and the all the like existing",
    "start": "3289280",
    "end": "3295119"
  },
  {
    "text": "easy to use has like you know python API uh stuff is um uh is in structured",
    "start": "3295119",
    "end": "3301920"
  },
  {
    "text": "sparcity and so you get gains there and it's like you might need yeah a lot like",
    "start": "3301920",
    "end": "3307319"
  },
  {
    "text": "yeah hand tuned Cuda kernels or yeah to like actually take use make use of unstructured sparcity so that's",
    "start": "3307319",
    "end": "3313599"
  },
  {
    "text": "something if there's a ton of pressure we could see those developments in five years or so but um we haven't se people",
    "start": "3313599",
    "end": "3319520"
  },
  {
    "text": "have been thinking about that for almost for like seven years and it's like not made a ton of progress but yeah helps",
    "start": "3319520",
    "end": "3326559"
  },
  {
    "text": "definitely has made it easier and like Google has been able to fit decent amount of language modeling capabilities",
    "start": "3326559",
    "end": "3333400"
  },
  {
    "text": "on a uh on a mobile device um using distillation and quantization and",
    "start": "3333400",
    "end": "3339880"
  },
  {
    "text": "probably more secrets that won't share",
    "start": "3339880",
    "end": "3344759"
  },
  {
    "text": "I yeah yeah um so the if somebody tells",
    "start": "3347720",
    "end": "3356760"
  },
  {
    "text": "you a number like 50b you know like llama llama anthropic 52b llama 70b",
    "start": "3356760",
    "end": "3365359"
  },
  {
    "text": "that's billions of parameters and then the question is like how what what how big is a parameter like how many bytes",
    "start": "3365359",
    "end": "3372400"
  },
  {
    "text": "and the like they're trained or where they the way the way they come out of the factory is two bytes per parameter",
    "start": "3372400",
    "end": "3378440"
  },
  {
    "text": "so take the number that somebody gives you multiply it by two and then the B is giga so like a small llama model is like",
    "start": "3378440",
    "end": "3387559"
  },
  {
    "text": "14 GB 7 B * 2 14 GB so not going to fit",
    "start": "3387559",
    "end": "3393280"
  },
  {
    "text": "that in phone Ram uh and that does make yeah doing this a lot harder um you can",
    "start": "3393280",
    "end": "3399760"
  },
  {
    "text": "do thing you can like try and do stuff with paging like put stuff on the disc bring it bring it into RAM then like",
    "start": "3399760",
    "end": "3405240"
  },
  {
    "text": "execute with it um but that like slows things down a ton um so in general",
    "start": "3405240",
    "end": "3412559"
  },
  {
    "text": "yeah hm so people used to train in float 32 and",
    "start": "3412559",
    "end": "3418760"
  },
  {
    "text": "release models in float 32 maybe I thought the Llama models were released in float 16 it's float 16 usually yeah a",
    "start": "3418760",
    "end": "3425799"
  },
  {
    "text": "lot of like a lot of people train in this new like Google brain float uh thing and then they they're doing that",
    "start": "3425799",
    "end": "3432480"
  },
  {
    "text": "because they want to be able to use float 16 and so have two bytes per parameter but like definitely the so the",
    "start": "3432480",
    "end": "3438599"
  },
  {
    "text": "like the default before that was four bytes per per parameter and before that when people were doing scientific Compu",
    "start": "3438599",
    "end": "3445559"
  },
  {
    "text": "with graphics with graphics cards like um people at the National Labs the default was like four byes for parameter",
    "start": "3445559",
    "end": "3452760"
  },
  {
    "text": "like um or sorry 8 bytes 64 bits um because they really needed that like high precision and high range um and",
    "start": "3452760",
    "end": "3460839"
  },
  {
    "text": "yeah but now the trend has been to push them lower and many model releases are now like already two bytes 16 bits",
    "start": "3460839",
    "end": "3470880"
  },
  {
    "text": "yeah um and now gei geov is like immediately converting them down to four bits uh and three bits um which is like",
    "start": "3470880",
    "end": "3479000"
  },
  {
    "text": "wild like what does that even mean um like a non- power of tube it's scary",
    "start": "3479000",
    "end": "3485920"
  },
  {
    "text": "unsettling um okay so uh with there's another execution",
    "start": "3485920",
    "end": "3492559"
  },
  {
    "text": "Target that gets you a lot of the benefits of desktops which is like you",
    "start": "3492559",
    "end": "3498200"
  },
  {
    "text": "have a b machine to run on and it's not yours so you don't have to pay for it um",
    "start": "3498200",
    "end": "3503559"
  },
  {
    "text": "but then you get a more genius execution environment which is the browser um so this is not like a web app where they",
    "start": "3503559",
    "end": "3510000"
  },
  {
    "text": "like talk to a model running in a service but like there is a model inside of the browser that runs inside the",
    "start": "3510000",
    "end": "3515160"
  },
  {
    "text": "browser like runtime uh and that like the homog homogenization environments",
    "start": "3515160",
    "end": "3521680"
  },
  {
    "text": "would be very huge um right now this is kind of like awaiting some technical improvements in the world of browser so",
    "start": "3521680",
    "end": "3528400"
  },
  {
    "text": "there is a Target in web assembly that you could compile your programs down to",
    "start": "3528400",
    "end": "3533920"
  },
  {
    "text": "um and in principle run them uh the support for uh gpus is very gross um",
    "start": "3533920",
    "end": "3540440"
  },
  {
    "text": "there's uh a working draft from the www Consortium for web GPU which would make",
    "start": "3540440",
    "end": "3546640"
  },
  {
    "text": "it cleaner and easier to use um so that would help the like ecosyst like stack",
    "start": "3546640",
    "end": "3551760"
  },
  {
    "text": "and ecosystem around this for other kinds of web applications is developing will like maybe lead developments and",
    "start": "3551760",
    "end": "3558799"
  },
  {
    "text": "using this for uh delivering inference um you have a new constraint distinct",
    "start": "3558799",
    "end": "3564359"
  },
  {
    "text": "from the other ones which is you now at least as it stands right now you would need to deliver weights over the network",
    "start": "3564359",
    "end": "3571640"
  },
  {
    "text": "and so now it's like you're you're you have kind of the model size constraints that you might associate with mobile",
    "start": "3571640",
    "end": "3577039"
  },
  {
    "text": "Hardware um but only during the like first load um so um there are probably",
    "start": "3577039",
    "end": "3584039"
  },
  {
    "text": "clever ways to get around that like progressively delivering them um or uh",
    "start": "3584039",
    "end": "3589440"
  },
  {
    "text": "like browser uh companies sort of agreeing to incorporate some Foundation",
    "start": "3589440",
    "end": "3594640"
  },
  {
    "text": "models into the actual browser runtime itself um so like inside of uh like a uh",
    "start": "3594640",
    "end": "3602680"
  },
  {
    "text": "like v9 uh an update to V8 with a foundation model already built into the runtime that would make your life a lot",
    "start": "3602680",
    "end": "3609440"
  },
  {
    "text": "easier um so this like would uh yeah",
    "start": "3609440",
    "end": "3615240"
  },
  {
    "text": "browser assistance maybe sort of like General uh like executing apps inside of a browser that feel more like native",
    "start": "3615240",
    "end": "3621240"
  },
  {
    "text": "apps um that's the potential applications here but um still a little",
    "start": "3621240",
    "end": "3627319"
  },
  {
    "text": "um at the edge that was a pun I guess at the",
    "start": "3627319",
    "end": "3632640"
  },
  {
    "text": "edge um okay so because oh yeah question",
    "start": "3632640",
    "end": "3638280"
  },
  {
    "text": "how many gabes is a small and large Model A small and a large model so when",
    "start": "3638280",
    "end": "3644520"
  },
  {
    "text": "I hear small large language model first I cringe internally and then I accept GPS",
    "start": "3644520",
    "end": "3653160"
  },
  {
    "text": "system ATM machine whatever um so a small large language model in my mind is something that has",
    "start": "3653160",
    "end": "3661119"
  },
  {
    "text": "like kind of limited ability to like speak and interact with and that's what you see at",
    "start": "3661119",
    "end": "3668200"
  },
  {
    "text": "like the like 13 billion to 30 billion parameter range like the medium size is",
    "start": "3668200",
    "end": "3674559"
  },
  {
    "text": "like the 70 billion parameter range which is like the largest open models and then like a true large language",
    "start": "3674559",
    "end": "3680799"
  },
  {
    "text": "model the ones that like make people scared about losing their jobs are generally like mixtures of 70 to 100",
    "start": "3680799",
    "end": "3687319"
  },
  {
    "text": "billion parameter models or maybe they are themselves 200 billion 280 billion parameter models yeah so then for all of",
    "start": "3687319",
    "end": "3694559"
  },
  {
    "text": "those take that and multiply it by we'll call it two um to get the number of gigabytes so like half a terabyte for",
    "start": "3694559",
    "end": "3700640"
  },
  {
    "text": "the um for like a you know Palm well a whole terabyte for Palm",
    "start": "3700640",
    "end": "3706920"
  },
  {
    "text": "540b um which is one of the larger ones ever trained I guess the cont like they",
    "start": "3706920",
    "end": "3712440"
  },
  {
    "text": "areed inows mhm",
    "start": "3712440",
    "end": "3716440"
  },
  {
    "text": "have um yeah it's been a long while since I downloaded a browser to my computer but I want to say that the",
    "start": "3718920",
    "end": "3725839"
  },
  {
    "text": "package that you download to install browser is in the like couple of gigabytes range right no I think it's like hundreds of Megs",
    "start": "3725839",
    "end": "3734720"
  },
  {
    "text": "really uh yeah oh uh yeah wait so you",
    "start": "3736039",
    "end": "3741160"
  },
  {
    "text": "download an installer and then you have to download anyway yeah so if if people want in to install stuff that's only a",
    "start": "3741160",
    "end": "3746720"
  },
  {
    "text": "few hundred megabytes then that's a nonstarter um I guess I expect a Linux",
    "start": "3746720",
    "end": "3752559"
  },
  {
    "text": "Dro image to be in the like couple of gigabytes like if I'm playing around with containers so that's um uh that's",
    "start": "3752559",
    "end": "3759720"
  },
  {
    "text": "like another Anchor Point um and also I for those things we're probably talking like 2 four eight years before that kind",
    "start": "3759720",
    "end": "3766440"
  },
  {
    "text": "of like standardization effort agreement like happens and we can hope that",
    "start": "3766440",
    "end": "3771720"
  },
  {
    "text": "internet speeds will increase in that time to match the increasing needs uh of",
    "start": "3771720",
    "end": "3778359"
  },
  {
    "text": "the internet um but yeah that's it's a pretty tight constraint and like probably looks a lot more like mobile",
    "start": "3778359",
    "end": "3784359"
  },
  {
    "text": "stuff for a very long time um",
    "start": "3784359",
    "end": "3790480"
  },
  {
    "text": "yeah no I think like right now I've been kind of assuming that you're doing stuff",
    "start": "3794160",
    "end": "3799279"
  },
  {
    "text": "relatively efficiently and to be honest like pytorch is like pretty good at this already like",
    "start": "3799279",
    "end": "3804880"
  },
  {
    "text": "um the fact that the application layer is written in Python isn't the problem um but yeah good",
    "start": "3804880",
    "end": "3813520"
  },
  {
    "text": "question",
    "start": "3816039",
    "end": "3819039"
  },
  {
    "text": "yeah yeah um I think we'll come to that in uh once like wanted to talk about um",
    "start": "3822799",
    "end": "3829279"
  },
  {
    "text": "after we talk about running AI over Network talk about the actual inference workloads um so we definitely get to",
    "start": "3829279",
    "end": "3835240"
  },
  {
    "text": "talking about that I don't think I'm not going to have a price number to give to you um but you have to take whatever",
    "start": "3835240",
    "end": "3840839"
  },
  {
    "text": "tokens per second you can get and then like however much you're spending on gpus um and then convert that into a",
    "start": "3840839",
    "end": "3848000"
  },
  {
    "text": "dollars per token um and that's going to give you something you can compare to",
    "start": "3848000",
    "end": "3853640"
  },
  {
    "text": "the like model providers um and until you put some decent optimization into it you aren't going to match",
    "start": "3853640",
    "end": "3859880"
  },
  {
    "text": "them um you did ask about CPU inference that that is rapidly evolving I think",
    "start": "3859880",
    "end": "3866359"
  },
  {
    "text": "there are cases where you can kind of like compete in price there but um yeah we'll cover that more later it seems",
    "start": "3866359",
    "end": "3873160"
  },
  {
    "start": "3872000",
    "end": "4200000"
  },
  {
    "text": "like let's put a pin in those two things and we'll come back to them after we talk about the like really the thing",
    "start": "3873160",
    "end": "3878559"
  },
  {
    "text": "that almost everybody's going to do like immediately after they leave is going to be run AI somewhere in a data center um",
    "start": "3878559",
    "end": "3885480"
  },
  {
    "text": "but those are important questions long term okay so uh like uh the running",
    "start": "3885480",
    "end": "3892839"
  },
  {
    "text": "stuff on end user devices has a lot of reasons why it's not so great right now so what what what do you get when you",
    "start": "3892839",
    "end": "3899160"
  },
  {
    "text": "run AI in in a AI workloads in a data center um the biggest win is Simplicity",
    "start": "3899160",
    "end": "3904520"
  },
  {
    "text": "uh the biggest pain point is latency as we've already discussed um so Simplicity like you just you control the whole",
    "start": "3904520",
    "end": "3910279"
  },
  {
    "text": "environment it makes your life a lot easier yeah he said latency the biggest",
    "start": "3910279",
    "end": "3915359"
  },
  {
    "text": "is that really a thing for llm compared to like Vision model because like any way the tokens that you can infer per",
    "start": "3915359",
    "end": "3921720"
  },
  {
    "text": "second are quite a bit slower than of",
    "start": "3921720",
    "end": "3926400"
  },
  {
    "text": "theat yeah so I would say that you can let's see so the question is whether",
    "start": "3927680",
    "end": "3933079"
  },
  {
    "text": "latency is actually a problem so um if you need to do like back and forth like",
    "start": "3933079",
    "end": "3939520"
  },
  {
    "text": "you need to get something back from the open API then possibly like call it again with some added context or like",
    "start": "3939520",
    "end": "3946640"
  },
  {
    "text": "run some if statements and then send it back now you're looking at like multiple Network calls and you could avoid all of",
    "start": "3946640",
    "end": "3953799"
  },
  {
    "text": "that overhead if you were running things locally so that's an example of the case where people would run into a latency",
    "start": "3953799",
    "end": "3959440"
  },
  {
    "text": "problem locally you're get way lower tokens per second anyway so you're",
    "start": "3959440",
    "end": "3966319"
  },
  {
    "text": "completely dominated by the time it takes to generate so tokens per second is a throughput number not a latency",
    "start": "3966319",
    "end": "3972920"
  },
  {
    "text": "number right so it doesn't matter if your tokens per second is half if your tokens per second is half that of what",
    "start": "3972920",
    "end": "3979799"
  },
  {
    "text": "open AI is getting but you only need to generate 30 tokens right that then it",
    "start": "3979799",
    "end": "3984920"
  },
  {
    "text": "like the the latency number is going to be the larger one even though your like",
    "start": "3984920",
    "end": "3990279"
  },
  {
    "text": "throughput is lower like this is definitely something that people have run like run into when you have like",
    "start": "3990279",
    "end": "3996079"
  },
  {
    "text": "highly interactive things like they're definitely so to be clear there are tons of applications in which you don't feel",
    "start": "3996079",
    "end": "4003160"
  },
  {
    "text": "this pain um and like chat gbt for example is at this point like the latency of the response from the machine",
    "start": "4003160",
    "end": "4010000"
  },
  {
    "text": "is not really the problem um so yeah it's not guarant guaranteed to be a pain point I would say as well um",
    "start": "4010000",
    "end": "4020558"
  },
  {
    "text": "yeah um so yeah and I I guess I'm also kind of",
    "start": "4022319",
    "end": "4028640"
  },
  {
    "text": "maybe imagining situations that are closer to the computer vision case in which you need cognition well in the computer vision case you need rapid",
    "start": "4028640",
    "end": "4035160"
  },
  {
    "text": "responses because it's like in the motor Loop of a system for example and if we want to use language models as the",
    "start": "4035160",
    "end": "4041079"
  },
  {
    "text": "cognitive component of a of a moving system then they would need latencies like that um like in the tens of",
    "start": "4041079",
    "end": "4046599"
  },
  {
    "text": "milliseconds or something yeah um and uh you are never going to be able to",
    "start": "4046599",
    "end": "4053000"
  },
  {
    "text": "achieve that over a network but yeah great question um all",
    "start": "4053000",
    "end": "4058279"
  },
  {
    "text": "right so inference as a service providers this it makes it super easy to get started it's what you know when you're using open AI they are inferenced",
    "start": "4058279",
    "end": "4065160"
  },
  {
    "text": "as a service provider um so all the proprietary models basically live here there's not some like way that they",
    "start": "4065160",
    "end": "4071200"
  },
  {
    "text": "would ship you the model and you could run it and it's proprietary license um that doesn't exist yet um open models",
    "start": "4071200",
    "end": "4078079"
  },
  {
    "text": "are also available so like if you want if you want to bet on the the like open ecosystem uh you can use a service like",
    "start": "4078079",
    "end": "4085359"
  },
  {
    "text": "replicate um that will like they'll run uh open models for you it's uh generally",
    "start": "4085359",
    "end": "4092039"
  },
  {
    "text": "like easy get started it's not that much more expensive than running it yourself in a lot of cases um but you have",
    "start": "4092039",
    "end": "4098159"
  },
  {
    "text": "limited control of the model um for proprietary models we already talked about how you would have less control",
    "start": "4098159",
    "end": "4104159"
  },
  {
    "text": "kind of inherently um for even for people who are providing open models and",
    "start": "4104159",
    "end": "4109560"
  },
  {
    "text": "so don't have IP they want to protect in order for them to like serve it cheaply to you they need to have like and to",
    "start": "4109560",
    "end": "4115679"
  },
  {
    "text": "have like an economic win that they can like pass on to you um and like keep a little bit for themselves they need to",
    "start": "4115679",
    "end": "4121600"
  },
  {
    "text": "do something like amortise costs across many users of models many more than than you have and that requires some amount",
    "start": "4121600",
    "end": "4128040"
  },
  {
    "text": "of homogeneity of usage um and it's like right now it's proven to be like pretty hard to give people control while also",
    "start": "4128040",
    "end": "4134400"
  },
  {
    "text": "giving them homogeneity usage um for something like AWS they came up with",
    "start": "4134400",
    "end": "4139679"
  },
  {
    "text": "really smart ways to cach pieces of containers so that the fact that everybody's using kind of the same",
    "start": "4139679",
    "end": "4145560"
  },
  {
    "text": "software allows them to amortize while also giving customization um but people",
    "start": "4145560",
    "end": "4151199"
  },
  {
    "text": "have not figured out a similar trick for um for language models or image",
    "start": "4151199",
    "end": "4156719"
  },
  {
    "text": "generation models yet um so you don't have as much control as you would have",
    "start": "4156719",
    "end": "4162199"
  },
  {
    "text": "yourself um so so new constraints arise so there are things like API rate limits",
    "start": "4162199",
    "end": "4168318"
  },
  {
    "text": "um and now this is sort of like a cost management game you look at this as like um rather than like paying up front uh",
    "start": "4168319",
    "end": "4175880"
  },
  {
    "text": "for compute that you have and then you think about maximizing the use of that",
    "start": "4175880",
    "end": "4181080"
  },
  {
    "text": "compute you think the other direction you try to minimize your use of compute while fitting the rest of your constraints so it's a very different",
    "start": "4181080",
    "end": "4187440"
  },
  {
    "text": "feeling if you know you ever switch between having your own compute and switching to Cloud it's the same idea",
    "start": "4187440",
    "end": "4195719"
  },
  {
    "text": "um yeah uh so right I so you could do that",
    "start": "4197199",
    "end": "4203640"
  },
  {
    "start": "4200000",
    "end": "4666000"
  },
  {
    "text": "inference yourself so rather than having somebody else do it for you um and this",
    "start": "4203640",
    "end": "4209040"
  },
  {
    "text": "works pretty well and is getting easier every day so cloud like this is like running stuff on a public Cloud um is",
    "start": "4209040",
    "end": "4217719"
  },
  {
    "text": "like one of the most popular choices for how to run ml workloads um and for like SAS in general um there's some",
    "start": "4217719",
    "end": "4224400"
  },
  {
    "text": "specialist Cloud providers in this space um like Lambda Labs that can be like",
    "start": "4224400",
    "end": "4229800"
  },
  {
    "text": "very competitive they're like often cheaper than the um than the like Big Three um and it's a nice balance of",
    "start": "4229800",
    "end": "4237320"
  },
  {
    "text": "control with like complexity and have like which things you actually care about having to deal with versus not um",
    "start": "4237320",
    "end": "4243480"
  },
  {
    "text": "it can get expensive over time it's definitely like you know more expensive than um uh like over a long period of",
    "start": "4243480",
    "end": "4250880"
  },
  {
    "text": "time than if you bought the stuff yourself uh gbus times feel like second class citizens and especially a lot of",
    "start": "4250880",
    "end": "4256120"
  },
  {
    "text": "the like big public clouds um uh Google Cloud's a bit of a distinction there and",
    "start": "4256120",
    "end": "4262480"
  },
  {
    "text": "that you can just add gpus to any instance it's kind of nice but in other other uh public clouds that's not really",
    "start": "4262480",
    "end": "4269480"
  },
  {
    "text": "the case uh and again this is like uh a cost management problem and um the one",
    "start": "4269480",
    "end": "4277040"
  },
  {
    "text": "of the popular ways to solve the cloud cost is to just agree to a large deal upfront and now you're starting to get",
    "start": "4277040",
    "end": "4283880"
  },
  {
    "text": "some of the IL liquidity associated with actually build buying Hardware um and you start to get some of the like vendor",
    "start": "4283880",
    "end": "4289400"
  },
  {
    "text": "lock in um that you would also associate with that um so the you have the",
    "start": "4289400",
    "end": "4295000"
  },
  {
    "text": "opportunity to kind of like trade those things off but um they are your constraints to work with um did want to",
    "start": "4295000",
    "end": "4301639"
  },
  {
    "text": "call out that there are some serverless approaches which gives you some of the like usage ba like really tightly usage",
    "start": "4301639",
    "end": "4308400"
  },
  {
    "text": "based pricing associated with inference as a service providers um but also",
    "start": "4308400",
    "end": "4314239"
  },
  {
    "text": "the like control associated with like you know have renting servers in the",
    "start": "4314239",
    "end": "4319440"
  },
  {
    "text": "cloud and by this by serverless I mean anything with like scale to zero semantics and pricing um uh that doesn't",
    "start": "4319440",
    "end": "4327199"
  },
  {
    "text": "involve you having to like literally manage servers so like thinking about the operating system uh for example um",
    "start": "4327199",
    "end": "4333239"
  },
  {
    "text": "and that offers High availability um this is a like a relatively new category in software in general and especially in",
    "start": "4333239",
    "end": "4339800"
  },
  {
    "text": "machine learning there's a couple of players here um modal uh Labs is one that I like quite a bit because it",
    "start": "4339800",
    "end": "4346159"
  },
  {
    "text": "doesn't just do the ml stuff though it is very good at it um replicate which also does inference as a service will do",
    "start": "4346159",
    "end": "4352320"
  },
  {
    "text": "this uh hugging face spaces recently Chang their end points to scale to zero um and there's also yeah banana dodev",
    "start": "4352320",
    "end": "4358880"
  },
  {
    "text": "and and others um the good thing is that it's like easier to get started",
    "start": "4358880",
    "end": "4365679"
  },
  {
    "text": "especially if you're not like a cloud Ops person um and very inexpensive at low traffic like you only have to pay",
    "start": "4365679",
    "end": "4371719"
  },
  {
    "text": "when you have traffic and if you're like like running a small if you're running a demo that only needs to be up when",
    "start": "4371719",
    "end": "4378080"
  },
  {
    "text": "you're showing it to um investors or if you are working on a tiny feature at a",
    "start": "4378080",
    "end": "4383480"
  },
  {
    "text": "large organization then you might have very low traffic patterns oh my it's uh it's Fleet week I",
    "start": "4383480",
    "end": "4390440"
  },
  {
    "text": "think so that might be the Blue Angels um yeah",
    "start": "4390440",
    "end": "4395960"
  },
  {
    "text": "um scale to zero means that you um when",
    "start": "4395960",
    "end": "4402639"
  },
  {
    "text": "there are no requests you are not when your requests go to zero the amount of",
    "start": "4402639",
    "end": "4409360"
  },
  {
    "text": "resources that you are using and being charged for also goes to zero yes um so yeah for a while hugging face",
    "start": "4409360",
    "end": "4419080"
  },
  {
    "text": "uh and Spaces end points they changed there's inference end points and yeah",
    "start": "4419080",
    "end": "4424400"
  },
  {
    "text": "they for a while it was like it could scale down to one and it would Auto scale so there's like having a Cloud",
    "start": "4424400",
    "end": "4429639"
  },
  {
    "text": "Server with auto scaling built in and then there's that thing but then it also scales to zero and you don't have to",
    "start": "4429639",
    "end": "4435159"
  },
  {
    "text": "think about server management and that that like is the combination that it's the original like aw definition of",
    "start": "4435159",
    "end": "4441480"
  },
  {
    "text": "serverless that has kind of Fallen not everyone goes by the old ways um yeah so",
    "start": "4441480",
    "end": "4448880"
  },
  {
    "text": "inexpensive at low traffic when nobody's calling your API um you don't pay for anything if you like come up with a",
    "start": "4448880",
    "end": "4454719"
  },
  {
    "text": "feature it doesn't work then doesn't matter um bad news is that you kind of",
    "start": "4454719",
    "end": "4460320"
  },
  {
    "text": "generally lose like tight control over aut scaling behavior that you could have if you were like you know if you have a",
    "start": "4460320",
    "end": "4467880"
  },
  {
    "text": "you know kubernetes team to work with they can very tightly set it up so that",
    "start": "4467880",
    "end": "4473360"
  },
  {
    "text": "the auto scaling is delivers exactly the throughput and latencies p99s that you",
    "start": "4473360",
    "end": "4480000"
  },
  {
    "text": "promised um and you kind of give all over some of that control to these serverless fighters who are themselves",
    "start": "4480000",
    "end": "4486199"
  },
  {
    "text": "probably running kubernetes but for a lot of people um and then the real the",
    "start": "4486199",
    "end": "4491560"
  },
  {
    "text": "thing that has kind of prevented this from being as successful as maybe Serv architectures in many other places is",
    "start": "4491560",
    "end": "4499440"
  },
  {
    "text": "latency um so when your it shows up as",
    "start": "4499440",
    "end": "4504639"
  },
  {
    "text": "uh kind of P99 latency so the 99th percentile of requests that hit a point",
    "start": "4504639",
    "end": "4510440"
  },
  {
    "text": "when you need to do auto scaling um the you need to get the weights of the model",
    "start": "4510440",
    "end": "4516840"
  },
  {
    "text": "you're using into uh not just off of dis and into RAM but then from there into",
    "start": "4516840",
    "end": "4523800"
  },
  {
    "text": "the ram of the accelerator and that takes uh like that can take a very long",
    "start": "4523800",
    "end": "4530080"
  },
  {
    "text": "amount of time and so you're looking at like 30C 1 minute 3 minute cold boots in",
    "start": "4530080",
    "end": "4536120"
  },
  {
    "text": "some cases cuz you are moving a half a terabyte of data around um and so that's",
    "start": "4536120",
    "end": "4542920"
  },
  {
    "text": "a place where people could maybe come up with these like clever ways to to cash and share um but yeah it's the memory",
    "start": "4542920",
    "end": "4550199"
  },
  {
    "text": "constraint that you hit in other domains showing up like in Disguise as latency um and uh yeah so the like you",
    "start": "4550199",
    "end": "4560360"
  },
  {
    "text": "still are probably going to be thinking of this in terms of like cost management and and cost reduction as opposed to like resource maximization um",
    "start": "4560360",
    "end": "4569320"
  },
  {
    "text": "yeah",
    "start": "4572600",
    "end": "4575600"
  },
  {
    "text": "yeah yeah so uh the point was about cloudflare workers so I did see that",
    "start": "4582120",
    "end": "4588400"
  },
  {
    "text": "cloud I didn't include them on the slide but cloudfare actually recently released these like GPU workers which is their",
    "start": "4588400",
    "end": "4594080"
  },
  {
    "text": "entry into this and I haven't had time to play with it so I don't know that much about it um I think if I need",
    "start": "4594080",
    "end": "4602840"
  },
  {
    "text": "to go from not consuming any of your resources to having a terabyte of my own",
    "start": "4602840",
    "end": "4609800"
  },
  {
    "text": "personal bytes like in the vram of a GPU you I find it hard to believe that they",
    "start": "4609800",
    "end": "4614880"
  },
  {
    "text": "don't have a latency problem there like so that but so I'd be Cur I'm curious what the what you know about the",
    "start": "4614880",
    "end": "4620199"
  },
  {
    "text": "solution yeah yeah yeah yeah yeah that's interesting i' yeah I'd",
    "start": "4620199",
    "end": "4626400"
  },
  {
    "text": "love to hear about that that's been my experience with the other serverless GPU providers so I'd love to hear more about",
    "start": "4626400",
    "end": "4632560"
  },
  {
    "text": "the cloudflare workers and um yeah if that goes away then serverless becomes a much more competitive way of delivering",
    "start": "4632560",
    "end": "4639480"
  },
  {
    "text": "inference so yeah um so uh I maintain a page for full stack deep",
    "start": "4639480",
    "end": "4645920"
  },
  {
    "text": "learning that has information about uh like cloud gpus and server providers pricing and um uh and what compute they",
    "start": "4645920",
    "end": "4653400"
  },
  {
    "text": "provide um so you can check that out from the slides later if you're interested um all right and then last uh",
    "start": "4653400",
    "end": "4662280"
  },
  {
    "text": "let's talk um like actually what if you",
    "start": "4662280",
    "end": "4667360"
  },
  {
    "start": "4666000",
    "end": "4812000"
  },
  {
    "text": "actually physically owned the computers that the inference ran on like uh you",
    "start": "4667360",
    "end": "4673040"
  },
  {
    "text": "can do that uh and rather than having to like actually you know construct a",
    "start": "4673040",
    "end": "4678199"
  },
  {
    "text": "building which which maybe the largest uh Enterprises could go uh go about uh",
    "start": "4678199",
    "end": "4684280"
  },
  {
    "text": "using a collocation facility isn't so bad um and there's more reason to do",
    "start": "4684280",
    "end": "4689360"
  },
  {
    "text": "this than for other kinds of workloads and in particular there's actually room",
    "start": "4689360",
    "end": "4694760"
  },
  {
    "text": "to beat a lot of the major public clouds which is why there's competitive clouds like Alternatives in this space uh a lot",
    "start": "4694760",
    "end": "4701480"
  },
  {
    "text": "of data centers that have been like around or which that were designed before 2021 or so are configured for",
    "start": "4701480",
    "end": "4708880"
  },
  {
    "text": "like dis and network heavy workloads rather than power heavy workloads so even if you can uh get a hold of like uh",
    "start": "4708880",
    "end": "4717360"
  },
  {
    "text": "30,000 a100 you can't just necessarily put them in the same Us East data center",
    "start": "4717360",
    "end": "4722880"
  },
  {
    "text": "that used to run uh that was designed for like uh running databases um it's so",
    "start": "4722880",
    "end": "4730199"
  },
  {
    "text": "it's Capital intensive but ends up being cheaper in the long run you have total control if you if you need it which is",
    "start": "4730199",
    "end": "4736159"
  },
  {
    "text": "awesome um but it's very hard very rare skill set um because it like kind of crosses this like the ml stuff and the",
    "start": "4736159",
    "end": "4744560"
  },
  {
    "text": "the the hardware stuff um and all these people can go and work for open AI for like a million and a half a year so uh",
    "start": "4744560",
    "end": "4752080"
  },
  {
    "text": "uh good luck holding on to them um uh and the biggest constraint that shows up",
    "start": "4752080",
    "end": "4757199"
  },
  {
    "text": "is il liquidity so you're going to make a big bet on what this looks like for example that inference is not going to",
    "start": "4757199",
    "end": "4762520"
  },
  {
    "text": "move on to CPU or not going to move onto custom silicon that behaves very differently from graphics cards um",
    "start": "4762520",
    "end": "4768480"
  },
  {
    "text": "there's a great talk from mates AAL of Lambda Labs about this that goes into kind of detail I think it's it's only",
    "start": "4768480",
    "end": "4775120"
  },
  {
    "text": "like a year old if I if I remember this talk right um and of course he makes it",
    "start": "4775120",
    "end": "4780360"
  },
  {
    "text": "sound very hard because he wants you to use their Cloud um or to pay them to",
    "start": "4780360",
    "end": "4785880"
  },
  {
    "text": "like help you build uh your collocation uh um uh you help you actually build it",
    "start": "4785880",
    "end": "4793040"
  },
  {
    "text": "um but it is a detailed explanation of everything involved and uh you know there's not very many of those out",
    "start": "4793040",
    "end": "4800440"
  },
  {
    "text": "there um all right we're at half time uh so I plan to take a break when I",
    "start": "4800440",
    "end": "4806400"
  },
  {
    "text": "finished part one which goes the rest of Self Serve inference which we'll I'll says another 15 minutes so let's do that",
    "start": "4806400",
    "end": "4813320"
  },
  {
    "start": "4812000",
    "end": "5227000"
  },
  {
    "text": "and we'll leave uh an hour for part two after a little",
    "start": "4813320",
    "end": "4818679"
  },
  {
    "text": "break um all right so let's uh we actually haven't talked in great",
    "start": "4818679",
    "end": "4826239"
  },
  {
    "text": "detail about uh like you know why why are we using gpus in the first place like what what is actually going on here",
    "start": "4826239",
    "end": "4833120"
  },
  {
    "text": "when we run this uh this like tensor to tensor map with neural networks um like",
    "start": "4833120",
    "end": "4840040"
  },
  {
    "text": "what what what actually does that workload turn into we have two tasks we need to load numbers from memory and",
    "start": "4840040",
    "end": "4846560"
  },
  {
    "text": "then we need to do math on those numbers those are our our like two basic tasks",
    "start": "4846560",
    "end": "4852159"
  },
  {
    "text": "um and that is the reason why we have why we end up using Graphics processing",
    "start": "4852159",
    "end": "4858040"
  },
  {
    "text": "units um because memory is slow and math is fast and in most in the in the",
    "start": "4858040",
    "end": "4866239"
  },
  {
    "text": "Transformer architecture in particular but in many like sort of most neural",
    "start": "4866239",
    "end": "4871280"
  },
  {
    "text": "network architectures you might write down um you only need a given number",
    "start": "4871280",
    "end": "4877480"
  },
  {
    "text": "from the weights like one time per input so that means you need to do a memory",
    "start": "4877480",
    "end": "4883800"
  },
  {
    "text": "read like uh of this of like a couple of byes for a particular parameter to use",
    "start": "4883800",
    "end": "4889639"
  },
  {
    "text": "it in a single floating Point operation and the memory read is going to be very slow and the floating Point operation is",
    "start": "4889639",
    "end": "4895600"
  },
  {
    "text": "going to be basically instant um so in order to do this economically you need to do a lot of math for each read for",
    "start": "4895600",
    "end": "4902280"
  },
  {
    "text": "memory uh you need to like load uh you know load the weight um out and in",
    "start": "4902280",
    "end": "4909320"
  },
  {
    "text": "particular we're talking here about like getting out of the vram and into the place where the you know into the like",
    "start": "4909320",
    "end": "4916120"
  },
  {
    "text": "um the what is it called well yeah it's basically like an L1 cache like closer",
    "start": "4916120",
    "end": "4921360"
  },
  {
    "text": "to the actual comp uh uh computation um and so you want to do that and then like",
    "start": "4921360",
    "end": "4927639"
  },
  {
    "text": "use it multiple times you know and uh that means you want to run on multiple",
    "start": "4927639",
    "end": "4933400"
  },
  {
    "text": "inputs at once and that is memory intensive single uh instruction multiple",
    "start": "4933400",
    "end": "4940120"
  },
  {
    "text": "data parallel linear algebra the same thing you need for graphics workloads so",
    "start": "4940120",
    "end": "4945400"
  },
  {
    "text": "the uh like graphics cards have turned out to be like pretty good at solving this",
    "start": "4945400",
    "end": "4950960"
  },
  {
    "text": "problem um and the the numbers there in",
    "start": "4950960",
    "end": "4956400"
  },
  {
    "text": "the corner are um uh show like demonstrate this uh like general fact of",
    "start": "4956400",
    "end": "4963440"
  },
  {
    "text": "like memory is slow logic or math is fast the you can do 312 Tera flops per",
    "start": "4963440",
    "end": "4970520"
  },
  {
    "text": "second uh in for two byte numbers uh two by floating Point numbers in a tensor",
    "start": "4970520",
    "end": "4975840"
  },
  {
    "text": "core in an a00 um and you only get one and a half terabytes a second of memory",
    "start": "4975840",
    "end": "4983920"
  },
  {
    "text": "bandwidth um and when you're using like you know optimized existing Cuda kernels",
    "start": "4983920",
    "end": "4989400"
  },
  {
    "text": "these two things are like Multiplex so like you load a weight you start doing math on it and then the next weight gets",
    "start": "4989400",
    "end": "4995600"
  },
  {
    "text": "loaded like you know uh concurrently um but you do still have this like mismatch",
    "start": "4995600",
    "end": "5001600"
  },
  {
    "text": "in the band wids that means that you want to be able to like amortise a",
    "start": "5001600",
    "end": "5006760"
  },
  {
    "text": "memory load across as many computations as possible and like in principle this could be flipped around and we would",
    "start": "5006760",
    "end": "5012120"
  },
  {
    "text": "have like very you know things would look very different um uh so you can get",
    "start": "5012120",
    "end": "5018080"
  },
  {
    "text": "like very very large throughput gains by advertising memory reads where basically",
    "start": "5018080",
    "end": "5023280"
  },
  {
    "text": "if you are operating on a very small number of tokens uh then you'll see that",
    "start": "5023280",
    "end": "5028719"
  },
  {
    "text": "as you add like if you're running this this workload yourself as you add more tokens you would think like you should",
    "start": "5028719",
    "end": "5034400"
  },
  {
    "text": "expect like a a slight increase in the amount of time that it takes and you'll see like basically a flat curve for a",
    "start": "5034400",
    "end": "5040159"
  },
  {
    "text": "very long time um and then once you hit the point where the um yeah so if you",
    "start": "5040159",
    "end": "5051158"
  },
  {
    "text": "look batch size at which you'll see that flip so for an a100 it's about 200 uh uh",
    "start": "5053600",
    "end": "5060199"
  },
  {
    "text": "elements in the batch um and and you'll see for an h100 you'll see that that ratio these numbers both go up but the",
    "start": "5060199",
    "end": "5067000"
  },
  {
    "text": "ratio becomes more extreme um so you uh",
    "start": "5067000",
    "end": "5072120"
  },
  {
    "text": "there's a great blog post from Carol Chen on like inference arithmetic that both goes through in Greater detail and",
    "start": "5072120",
    "end": "5078639"
  },
  {
    "text": "then like matches that onto some like actual experimental results and it's able to like track where did each you",
    "start": "5078639",
    "end": "5084760"
  },
  {
    "text": "know micr basically of uh of inference time come from um so the like he take",
    "start": "5084760",
    "end": "5091880"
  },
  {
    "text": "away from this is that if you want to get large throughput in like an",
    "start": "5091880",
    "end": "5097800"
  },
  {
    "text": "inference system that you're running yourself you're going to need batching you're going to need to like collect up multiple inputs uh from multiple users",
    "start": "5097800",
    "end": "5104800"
  },
  {
    "text": "and operate on them at the same time uh so this is uh it's challenging to",
    "start": "5104800",
    "end": "5110239"
  },
  {
    "text": "achieve the same thing in an end user device um you are like if you're only",
    "start": "5110239",
    "end": "5115679"
  },
  {
    "text": "working for one person then they might not make 10 requests uh you know quickly enough for you to fill up a batch um and",
    "start": "5115679",
    "end": "5122480"
  },
  {
    "text": "that actually might kind of tilt things in the direction of uh of CPUs or of uh",
    "start": "5122480",
    "end": "5129600"
  },
  {
    "text": "you know uh different uh different architecture with different um you know memory bandwidth versus tensor core uh",
    "start": "5129600",
    "end": "5136800"
  },
  {
    "text": "versus uh math bandwidth um uh",
    "start": "5136800",
    "end": "5142638"
  },
  {
    "text": "tradeoff um the one of the useful things that came out of this I guess this is a restatement of kind of what I just said",
    "start": "5142719",
    "end": "5149400"
  },
  {
    "text": "when you have a like low load API um you will end up with smaller batch sizes",
    "start": "5149400",
    "end": "5155520"
  },
  {
    "text": "because maybe you have you have to deliver with a certain latency so you can't just wait forever um and so you",
    "start": "5155520",
    "end": "5160960"
  },
  {
    "text": "will make different decisions about compute memory trade-offs for example like caching past computations of uh",
    "start": "5160960",
    "end": "5168719"
  },
  {
    "text": "keys and values is very common when you're doing batch inference but it's actually not necessarily the right",
    "start": "5168719",
    "end": "5173760"
  },
  {
    "text": "choice when you are already memory bound trading off memory for compute um isn't",
    "start": "5173760",
    "end": "5179040"
  },
  {
    "text": "a good idea um if you're like at for larger batch sizes um you if you're",
    "start": "5179040",
    "end": "5185440"
  },
  {
    "text": "doing something that's an API where like serving requests directly then you would want to like roughly balance being uh",
    "start": "5185440",
    "end": "5193760"
  },
  {
    "text": "flop bound and memory bound so that you can get quick latency of responses even though your throughput is less than it",
    "start": "5193760",
    "end": "5199440"
  },
  {
    "text": "would be otherwise um whereas if you're doing like a batch job like an overnight type job um or that is like typical of",
    "start": "5199440",
    "end": "5207320"
  },
  {
    "text": "data science or or Big Data workloads then you would just go for the largest B",
    "start": "5207320",
    "end": "5212360"
  },
  {
    "text": "that you can and that's what people do during training they go for the absolute largest batch that they can because there's no latency requirement uh",
    "start": "5212360",
    "end": "5219159"
  },
  {
    "text": "there's only",
    "start": "5219159",
    "end": "5221520"
  },
  {
    "text": "throughput uh yeah yeah so tpus end up I I like looked around and they I I've",
    "start": "5226239",
    "end": "5231679"
  },
  {
    "start": "5227000",
    "end": "5771000"
  },
  {
    "text": "never personally used them for anything serious so I wasn't able to like directly map on like pull out the",
    "start": "5231679",
    "end": "5237239"
  },
  {
    "text": "equivalent of a tensor core um like flop bandwidth and the and and Ram to L1",
    "start": "5237239",
    "end": "5244080"
  },
  {
    "text": "latency um but the results and benchmarks that I've seen is that they're like 30% better um like they",
    "start": "5244080",
    "end": "5252080"
  },
  {
    "text": "they have a they do have a slightly different choice of tradeoff um and they end and it's better for neural network",
    "start": "5252080",
    "end": "5259239"
  },
  {
    "text": "workloads than graphics workloads and that gets what from what I have seen like 30% Improvement but not like a 10",
    "start": "5259239",
    "end": "5266719"
  },
  {
    "text": "to 50x Improvement which is what you would really like to see if you're making as drastic of a decision as like",
    "start": "5266719",
    "end": "5273000"
  },
  {
    "text": "going over to a completely different accelerator with a completely different software stack um yeah I guess yeah I",
    "start": "5273000",
    "end": "5281159"
  },
  {
    "text": "had I got into a discussion with uh one of the people on the TPU team about like",
    "start": "5281159",
    "end": "5286360"
  },
  {
    "text": "the hardware Lottery and he his he was like the GPU is already like is just an excellent machine like it does this",
    "start": "5286360",
    "end": "5293040"
  },
  {
    "text": "workload really really well um and I think that is worn out in the numbers yeah from what I heard the main",
    "start": "5293040",
    "end": "5299520"
  },
  {
    "text": "advantage of dpus is uh operational efficiency and power",
    "start": "5299520",
    "end": "5304760"
  },
  {
    "text": "efficiency not like of performance yeah so um for the folks listening online",
    "start": "5304760",
    "end": "5311080"
  },
  {
    "text": "repeating the point uh the one of the benefits of tpus is power I totally buy that um yeah uh I think yeah the numbers",
    "start": "5311080",
    "end": "5319719"
  },
  {
    "text": "that tend to get reported in things like Google's Pathways paper and palm papers or open and like what is known about",
    "start": "5319719",
    "end": "5325679"
  },
  {
    "text": "open AI it's like it's all about like flop utilization and and total flops and stuff like that and not things like",
    "start": "5325679",
    "end": "5332199"
  },
  {
    "text": "power that do matter um I'd say like people have not gone rushing to try and get a hold of them which feels uh uh",
    "start": "5332199",
    "end": "5340480"
  },
  {
    "text": "like a strong signal um similar with like other types of custom silicon like cis's um uh chip uh but you know who",
    "start": "5340480",
    "end": "5350239"
  },
  {
    "text": "knows I think this is getting back to a question that got asked earlier um I believe the like having custom chips",
    "start": "5350239",
    "end": "5357159"
  },
  {
    "text": "works really well when workloads stay very fixed in like kind of precise detail not just this like oh we need to",
    "start": "5357159",
    "end": "5363880"
  },
  {
    "text": "load for memory and then we need to do but like no we need to do this shaped thing like with this many like that",
    "start": "5363880",
    "end": "5369840"
  },
  {
    "text": "works super well and you can see that in blockchain mining where there's like for",
    "start": "5369840",
    "end": "5375560"
  },
  {
    "text": "a for many chains for a long time as6 were the like only profitable way to mine um or like the most profitable way",
    "start": "5375560",
    "end": "5382719"
  },
  {
    "text": "way to mine and that's the workload is like unchangeable except by like distributed consensus um and that allows",
    "start": "5382719",
    "end": "5390119"
  },
  {
    "text": "you to like very tightly Target a specific workload whereas like neural network architectures actually kind of",
    "start": "5390119",
    "end": "5396320"
  },
  {
    "text": "change reasonably often and they like you know even the difference the difference between a 7 billion parameter",
    "start": "5396320",
    "end": "5402119"
  },
  {
    "text": "model a 13 billion parameter model 170 billion parameter model that's like a bigger difference than you would see",
    "start": "5402119",
    "end": "5407679"
  },
  {
    "text": "between workloads for um like the same blockchain over time um and so that's",
    "start": "5407679",
    "end": "5414360"
  },
  {
    "text": "that like difference in Tech like that technical difference is I think a big reason why we haven't seen uh much",
    "start": "5414360",
    "end": "5421080"
  },
  {
    "text": "uptake of Custom",
    "start": "5421080",
    "end": "5425000"
  },
  {
    "text": "Custom yeah I think that like they are one of the um they're one of the providers for um like they're one of the",
    "start": "5427560",
    "end": "5434760"
  },
  {
    "text": "startups working on this they come up most frequently um uh I don't it's still",
    "start": "5434760",
    "end": "5441600"
  },
  {
    "text": "like uh fairly experimental I think like it's not it's not the um it's not like generally available",
    "start": "5441600",
    "end": "5449800"
  },
  {
    "text": "in a public cloud or something so um yeah I don't have much to say about",
    "start": "5449800",
    "end": "5455400"
  },
  {
    "text": "it I guess yeah I think main thing you can see there is the memory that they have is 40 GB",
    "start": "5455400",
    "end": "5464000"
  },
  {
    "text": "compar that with the neon which is 80 and for most",
    "start": "5464000",
    "end": "5469800"
  },
  {
    "text": "of yeah yeah so cabus has done some large model training was definitely like possible um they um the the they have",
    "start": "5478119",
    "end": "5486920"
  },
  {
    "text": "this very fast memory bandwidth I think is like maybe the the big one 20 pedabytes memory bandwidth now like the",
    "start": "5486920",
    "end": "5494400"
  },
  {
    "text": "the point that was raised about the bottleneck being the memory capacity is very is uh is well taken um you can",
    "start": "5494400",
    "end": "5501280"
  },
  {
    "text": "Network multiple multiple chips together and then you have more storage but um",
    "start": "5501280",
    "end": "5507320"
  },
  {
    "text": "yeah and so like Bal better balancing of of memory bandwidth and math math bandwidth would maybe get you more e",
    "start": "5507320",
    "end": "5514800"
  },
  {
    "text": "like more efficacy at lower batch sizes um but you can't exactly put that chip",
    "start": "5514800",
    "end": "5519840"
  },
  {
    "text": "in a phone um so that yeah um but definitely uh like people are really",
    "start": "5519840",
    "end": "5527239"
  },
  {
    "text": "converging on Transformer architectures so that could mean that this um custom silicon works better um in 5 years um",
    "start": "5527239",
    "end": "5536440"
  },
  {
    "text": "yeah I think I yeah the way that you like the way that you solve this problem is if you have multiple G gpus these the",
    "start": "5536440",
    "end": "5543360"
  },
  {
    "text": "like memory per second and flops per second like scale at the same amount right you have now you have two gpus",
    "start": "5543360",
    "end": "5549000"
  },
  {
    "text": "loading you have two gpus doing operations they both scale linearly if you're really clever you can like actually get that linear scaling in",
    "start": "5549000",
    "end": "5555520"
  },
  {
    "text": "practice um the GPU memory goes up as well um and so you can serve larger",
    "start": "5555520",
    "end": "5561679"
  },
  {
    "text": "batches and eventually hit this ratio is staying the same so you can eventually hit that point um and that's what and",
    "start": "5561679",
    "end": "5567360"
  },
  {
    "text": "then like once you hit the point where you're actually uh you know bound um you",
    "start": "5567360",
    "end": "5573040"
  },
  {
    "text": "can just like crank it you know just crank it up uh just get really big uh uh",
    "start": "5573040",
    "end": "5579400"
  },
  {
    "text": "really big batches speed up uh speed of the computation and that is what people do during are are like used to do used",
    "start": "5579400",
    "end": "5585639"
  },
  {
    "text": "to doing during training um and so you'll very commonly see people talking about this way of making inference",
    "start": "5585639",
    "end": "5591600"
  },
  {
    "text": "efficient um so if you are running a um like a web service this is going to give",
    "start": "5591600",
    "end": "5596880"
  },
  {
    "text": "you your sort of like uh pod size roughly like um like a pod in kubernetes",
    "start": "5596880",
    "end": "5602320"
  },
  {
    "text": "it's like a bunch of services that end up on a single physical machine so this is your like unit uh um uh is determined",
    "start": "5602320",
    "end": "5611320"
  },
  {
    "text": "by like how many gpus do you need to hit the like backat size um for efficiency",
    "start": "5611320",
    "end": "5617080"
  },
  {
    "text": "implied by the like uh this flops memory per second and you're checking to make sure this is also true for your",
    "start": "5617080",
    "end": "5623280"
  },
  {
    "text": "architecture um yeah um all that's like kind of theoretical stuff or or sort of like",
    "start": "5623280",
    "end": "5629960"
  },
  {
    "text": "trying to be relatively first principal uh when you when it comes time to actually check whether you're doing",
    "start": "5629960",
    "end": "5635280"
  },
  {
    "text": "things correctly make sure to use like a profile or a tracer and actually like look at these things um even like people",
    "start": "5635280",
    "end": "5642199"
  },
  {
    "text": "who are pretty good at this will like Miss stuff that shows up on a on a tracer um like the VM uh implementation",
    "start": "5642199",
    "end": "5650600"
  },
  {
    "text": "was launching a Cuda kernel for like every individual element of a batch and",
    "start": "5650600",
    "end": "5655920"
  },
  {
    "text": "like a a tiny change made it switch to like one Cuda kernel launch and that save them like uh",
    "start": "5655920",
    "end": "5663840"
  },
  {
    "text": "uh increase their throughput by 30 33% um and it's something that shows up as like a like way more Cuda kernel",
    "start": "5663840",
    "end": "5671520"
  },
  {
    "text": "launches which would show up as like I guess yeah if I had the interactive version of this it would show up as this",
    "start": "5671520",
    "end": "5678000"
  },
  {
    "text": "like huge thing of red lines um and also just the like General utilization shows up as these big uh like patches of gray",
    "start": "5678000",
    "end": "5685679"
  },
  {
    "text": "here which shows you where you're like not actually using your GPU at all um and uh I found like looking directly at",
    "start": "5685679",
    "end": "5692679"
  },
  {
    "text": "these um at these traces not just at statistical profiles helps like catch",
    "start": "5692679",
    "end": "5698239"
  },
  {
    "text": "those uh those kinds of um easy to fix 8020 kind of bugs um you can also",
    "start": "5698239",
    "end": "5704840"
  },
  {
    "text": "profile and Trace memory uh and that's going to be like given that memory is this key constraint that's likely to be",
    "start": "5704840",
    "end": "5711320"
  },
  {
    "text": "at least as useful um and these two are examples from P torch um these uh this",
    "start": "5711320",
    "end": "5719520"
  },
  {
    "text": "the compute trace and the um the memory Trace both from pytorch you might not be",
    "start": "5719520",
    "end": "5725320"
  },
  {
    "text": "using pytorch um so you might have to fall back on generic like GPU profiling",
    "start": "5725320",
    "end": "5730560"
  },
  {
    "text": "or CPU profiling or memory profiling and tracing tools um if you're using like a",
    "start": "5730560",
    "end": "5736639"
  },
  {
    "text": "custom inference solution um yeah speaking of which there's a bunch of specialized llm",
    "start": "5736639",
    "end": "5743400"
  },
  {
    "text": "inference libraries out there there's a nice blog post from Ham Hussein about using these and this is specifically for",
    "start": "5743400",
    "end": "5749360"
  },
  {
    "text": "like batch size one um and so the the VM is people are like",
    "start": "5749360",
    "end": "5756800"
  },
  {
    "text": "pretty that seems to be getting like the most Community excitement and Community contribution um but hum will suggest",
    "start": "5756800",
    "end": "5763040"
  },
  {
    "text": "some reasons why you might be into mlc and C translate 2 instead um for doing",
    "start": "5763040",
    "end": "5768719"
  },
  {
    "text": "your uh llama inference um so actually so all that was",
    "start": "5768719",
    "end": "5774719"
  },
  {
    "start": "5771000",
    "end": "6196000"
  },
  {
    "text": "about like just thinking in terms of like an individual workload um setting up an entire inference service",
    "start": "5774719",
    "end": "5782080"
  },
  {
    "text": "um which is now something that maybe like somebody in the company sets up an inference service or a inference",
    "start": "5782080",
    "end": "5788360"
  },
  {
    "text": "platform and then uh people can you know submit workloads to it that can be",
    "start": "5788360",
    "end": "5793560"
  },
  {
    "text": "fairly challenging um containerization for GPU accelerated workloads is less",
    "start": "5793560",
    "end": "5798920"
  },
  {
    "text": "painful now than it used to be um like Nvidia Docker actually works in a way",
    "start": "5798920",
    "end": "5804440"
  },
  {
    "text": "that it did not um like early on um but",
    "start": "5804440",
    "end": "5809480"
  },
  {
    "text": "uh containerization is like fun fundamentally uh you know more dubious for these uh these workloads than a lot",
    "start": "5809480",
    "end": "5817280"
  },
  {
    "text": "of other ones so one is that the like application layer of the container is",
    "start": "5817280",
    "end": "5823199"
  },
  {
    "text": "probably where the weights are going to live like it doesn't live in the operating system yet um it's not built",
    "start": "5823199",
    "end": "5828840"
  },
  {
    "text": "into the docker engine so it's like that's up there and that's like maybe half a terabyte in in in bad situations",
    "start": "5828840",
    "end": "5835760"
  },
  {
    "text": "um so now your container images are really large and that's um that can be pretty UNP",
    "start": "5835760",
    "end": "5842119"
  },
  {
    "text": "um you could try and do the things that people do with databases and like move it into remote storage but um yeah then",
    "start": "5842119",
    "end": "5848199"
  },
  {
    "text": "the container is not as like unitary um and then the like the worst problem",
    "start": "5848199",
    "end": "5854600"
  },
  {
    "text": "maybe is that the Cuda driver changes the choices that you need to make kind",
    "start": "5854600",
    "end": "5860480"
  },
  {
    "text": "of at the like application Level of like we were just talking about actually going one level down but like which GPU",
    "start": "5860480",
    "end": "5867000"
  },
  {
    "text": "is going to run on changes the point at which you switch from being memory bound to flops bound because that has a",
    "start": "5867000",
    "end": "5872400"
  },
  {
    "text": "different uh like memory bandwidth to math bandwidth ratio um so things at the",
    "start": "5872400",
    "end": "5878480"
  },
  {
    "text": "level of the Nvidia of the GPU are entangled with things at the application layer um like choices of like batching",
    "start": "5878480",
    "end": "5886040"
  },
  {
    "text": "strategy um so that's and a similar thing can be said for the Cuda driver I was like naive about containers and I",
    "start": "5886040",
    "end": "5893560"
  },
  {
    "text": "was like why is the like Cuda driver showing up as 12.0 which is what I have",
    "start": "5893560",
    "end": "5899599"
  },
  {
    "text": "installed in my host operating system when I specifically downloaded the Cuda 11.2 container and it's like containers",
    "start": "5899599",
    "end": "5907000"
  },
  {
    "text": "can only only virtualize so much um and so that also changes like which kernels",
    "start": "5907000",
    "end": "5913800"
  },
  {
    "text": "are available by default um uh I guess well kernels would be a layer above so",
    "start": "5913800",
    "end": "5920679"
  },
  {
    "text": "there are features of Cuda of Cuda drivers that change the like you know how hugging face Transformers or pytorch",
    "start": "5920679",
    "end": "5926239"
  },
  {
    "text": "will work um and so that will and that will also change decisions they're made at the application later so when you're",
    "start": "5926239",
    "end": "5931880"
  },
  {
    "text": "in these like super like this is clearly a performance limited regime that we're talking about here like the inference is",
    "start": "5931880",
    "end": "5938040"
  },
  {
    "text": "like extremely expensive um and we're like trying to maximize performance really aggressively and that like starts",
    "start": "5938040",
    "end": "5945480"
  },
  {
    "text": "to reveal the limitations of virtualization and containerization um so that doesn't mean it's impossible it",
    "start": "5945480",
    "end": "5951520"
  },
  {
    "text": "just means it's hard um and yeah I don't know if you've ever worked with like you",
    "start": "5951520",
    "end": "5956560"
  },
  {
    "text": "know trying to set up a heterogeneous compute kubernetes cluster where there's like some have gpus and some have arm",
    "start": "5956560",
    "end": "5964000"
  },
  {
    "text": "and some have uh like x86 you like this dealing with this requires a a better um",
    "start": "5964000",
    "end": "5972960"
  },
  {
    "text": "Ops engineer than if you can ignore that um speaking of which this like",
    "start": "5972960",
    "end": "5978000"
  },
  {
    "text": "application surveying can be and is in fact done with the like industry standard for container orchestration in",
    "start": "5978000",
    "end": "5983840"
  },
  {
    "text": "kubernetes um I would note that now the like Nvidia stuff is showing up here",
    "start": "5983840",
    "end": "5989040"
  },
  {
    "text": "again and like all the problem like the entangling thing and now kubernetes is in between them so now there's like a",
    "start": "5989040",
    "end": "5995440"
  },
  {
    "text": "lot of opportunity for cross talk a lot of opportunity for Tears um and uh so",
    "start": "5995440",
    "end": "6002480"
  },
  {
    "text": "this is a hard mode version of that problem which is not notorious for being",
    "start": "6002480",
    "end": "6007520"
  },
  {
    "text": "easy um so you might choose and need to do it uh by hand there's a comment on",
    "start": "6007520",
    "end": "6013400"
  },
  {
    "text": "r/m Ops that was like oh yeah we look you know I was looking around to see what the opinions are on these tools and",
    "start": "6013400",
    "end": "6021840"
  },
  {
    "text": "uh opinion I saw in a couple places was like if you care about like you've",
    "start": "6021840",
    "end": "6027080"
  },
  {
    "text": "chosen to do serving of inference yourself and not do like just API calls",
    "start": "6027080",
    "end": "6033360"
  },
  {
    "text": "then um maybe you should make mlops a core competency of your company and you're going to want to do something",
    "start": "6033360",
    "end": "6039599"
  },
  {
    "text": "more like building it out of the like kubernetes Affiliated econ system of",
    "start": "6039599",
    "end": "6045280"
  },
  {
    "text": "Open Source tools or other open- source things um like Cube Ray to do um to run",
    "start": "6045280",
    "end": "6052080"
  },
  {
    "text": "Ray on kubernetes for serving or Selden core to run that on again on kubernetes",
    "start": "6052080",
    "end": "6059199"
  },
  {
    "text": "as your container orchestration layer um but given how like painful that is",
    "start": "6059199",
    "end": "6066520"
  },
  {
    "text": "and the fact that there are so many people who are trying to run a relatively similar workload um you might",
    "start": "6066520",
    "end": "6072280"
  },
  {
    "text": "consider um like either of two kind of tiers of managed Services either the",
    "start": "6072280",
    "end": "6077320"
  },
  {
    "text": "like white glove endtoend kind of cloud provider approach the old one Amazon stag maker um more recently vertex AI",
    "start": "6077320",
    "end": "6084800"
  },
  {
    "text": "from Google I'm sure there's an Azure version that I'm forgetting um there are also from some startups a sort of more",
    "start": "6084800",
    "end": "6091840"
  },
  {
    "text": "toolbox approach a less end to end approach in part because it's not like integrated all the way at the layer of",
    "start": "6091840",
    "end": "6097119"
  },
  {
    "text": "the like actual Hardware like the cloud providers are um so uh Bento Cloud uh",
    "start": "6097119",
    "end": "6102920"
  },
  {
    "text": "Seldin and any scale any scale um being a managed version of Ray Seldon offers a",
    "start": "6102920",
    "end": "6109520"
  },
  {
    "text": "managed version of Seldon core Bento Cloud managed version of Bento ml um so",
    "start": "6109520",
    "end": "6115960"
  },
  {
    "text": "uh depending on where you want to maybe you have a really great sweet deal with gcp and so vertex is the right choice um",
    "start": "6115960",
    "end": "6124400"
  },
  {
    "text": "but yeah I think from I have had limited experience with these things I've tried to keep my life uh simple and happy um",
    "start": "6124400",
    "end": "6131880"
  },
  {
    "text": "but if you're uh if you end up in this space I've I've seen some nice things and played around with race serve um and",
    "start": "6131880",
    "end": "6139920"
  },
  {
    "text": "any scale um Rays the tool that a lot of people use for cluster management for",
    "start": "6139920",
    "end": "6146040"
  },
  {
    "text": "training so a lot of the teams that train the foundation models use Ray um and so there's kind of like a natural",
    "start": "6146040",
    "end": "6151440"
  },
  {
    "text": "competency for them uh both in the open source library and any scale as a as a",
    "start": "6151440",
    "end": "6157520"
  },
  {
    "text": "um layer around that so maybe it'd be a good choice for um for inference as",
    "start": "6157520",
    "end": "6164080"
  },
  {
    "text": "well all right um so rather than take",
    "start": "6164080",
    "end": "6169119"
  },
  {
    "text": "questions I think I'm gonna do a five minute break in which you can ask me questions up here while we all get a",
    "start": "6169119",
    "end": "6174880"
  },
  {
    "text": "little stretch maybe a little air um and we'll come back for the rest of the owl in 5",
    "start": "6174880",
    "end": "6181800"
  },
  {
    "text": "[Music]",
    "start": "6183070",
    "end": "6194069"
  },
  {
    "text": "minutes",
    "start": "6194159",
    "end": "6197159"
  }
]