[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "[Music]",
    "start": "350",
    "end": "13839"
  },
  {
    "text": "I'm Phoebe I'm a machine learning",
    "start": "13839",
    "end": "15120"
  },
  {
    "text": "engineer at normal Computing and I'm",
    "start": "15120",
    "end": "16800"
  },
  {
    "text": "really excited to tell you guys about",
    "start": "16800",
    "end": "18240"
  },
  {
    "text": "some of our recent research uh and in",
    "start": "18240",
    "end": "19880"
  },
  {
    "text": "particular extended mind",
    "start": "19880",
    "end": "22560"
  },
  {
    "text": "Transformers all right so just to",
    "start": "22560",
    "end": "24199"
  },
  {
    "text": "briefly cover what we're going to go",
    "start": "24199",
    "end": "25279"
  },
  {
    "text": "over in today's talk uh we'll introduce",
    "start": "25279",
    "end": "27199"
  },
  {
    "text": "the problem which I think will be quite",
    "start": "27199",
    "end": "28439"
  },
  {
    "text": "familiar given the amazing talk which",
    "start": "28439",
    "end": "30199"
  },
  {
    "text": "came before mine uh and then dive right",
    "start": "30199",
    "end": "32320"
  },
  {
    "text": "into the method so uh what is the",
    "start": "32320",
    "end": "34399"
  },
  {
    "text": "retrieval mechanism that extended mind",
    "start": "34399",
    "end": "36120"
  },
  {
    "text": "Transformers Implement uh and then we'll",
    "start": "36120",
    "end": "38200"
  },
  {
    "text": "dive into some experiments which give us",
    "start": "38200",
    "end": "39879"
  },
  {
    "text": "confidence that these methods are",
    "start": "39879",
    "end": "41039"
  },
  {
    "text": "actually performant after that we'll get",
    "start": "41039",
    "end": "43039"
  },
  {
    "text": "into two of my favorite and I think most",
    "start": "43039",
    "end": "44840"
  },
  {
    "text": "compelling features that extended my",
    "start": "44840",
    "end": "46520"
  },
  {
    "text": "Transformers enable this is a new kind",
    "start": "46520",
    "end": "48399"
  },
  {
    "text": "of citation uh as well as a new kind of",
    "start": "48399",
    "end": "50800"
  },
  {
    "text": "generation Paradigm which is active",
    "start": "50800",
    "end": "52800"
  },
  {
    "text": "learning inspired uh and then we'll go",
    "start": "52800",
    "end": "55039"
  },
  {
    "text": "over the most important parameters to",
    "start": "55039",
    "end": "56480"
  },
  {
    "text": "tune when implementing uh EMTs in your",
    "start": "56480",
    "end": "59000"
  },
  {
    "text": "applications and generally how to use",
    "start": "59000",
    "end": "63039"
  },
  {
    "start": "63000",
    "end": "212000"
  },
  {
    "text": "them all right so we pre-train language",
    "start": "63559",
    "end": "66520"
  },
  {
    "text": "models uh so that they have general",
    "start": "66520",
    "end": "68479"
  },
  {
    "text": "knowledge but as we've been discussing",
    "start": "68479",
    "end": "70680"
  },
  {
    "text": "all this conference that's not enough we",
    "start": "70680",
    "end": "73080"
  },
  {
    "text": "need a lot of application specific",
    "start": "73080",
    "end": "74920"
  },
  {
    "text": "information and a topical uh description",
    "start": "74920",
    "end": "77400"
  },
  {
    "text": "of the world in order to make these",
    "start": "77400",
    "end": "78680"
  },
  {
    "text": "things useful um I'm not going to",
    "start": "78680",
    "end": "81439"
  },
  {
    "text": "belabor the two most popular methods um",
    "start": "81439",
    "end": "84000"
  },
  {
    "text": "which try to load this description into",
    "start": "84000",
    "end": "86119"
  },
  {
    "text": "the language model those being long",
    "start": "86119",
    "end": "88119"
  },
  {
    "text": "context and rag as a I think yeah we've",
    "start": "88119",
    "end": "90680"
  },
  {
    "text": "heard a lot about those um great methods",
    "start": "90680",
    "end": "93119"
  },
  {
    "text": "already but I'd like to point out that",
    "start": "93119",
    "end": "94920"
  },
  {
    "text": "they solve the problem in different ways",
    "start": "94920",
    "end": "96920"
  },
  {
    "text": "and th suffer from different downsides",
    "start": "96920",
    "end": "99560"
  },
  {
    "text": "so long context seeks to extend the",
    "start": "99560",
    "end": "102200"
  },
  {
    "text": "context window of the Transformer model",
    "start": "102200",
    "end": "104600"
  },
  {
    "text": "so we train language models we train",
    "start": "104600",
    "end": "106399"
  },
  {
    "text": "them on sequences of a fixed length and",
    "start": "106399",
    "end": "108799"
  },
  {
    "text": "then we're trying to say well can we can",
    "start": "108799",
    "end": "111520"
  },
  {
    "text": "we extend that so we can include more in",
    "start": "111520",
    "end": "113240"
  },
  {
    "text": "the context more in the prompt during",
    "start": "113240",
    "end": "115200"
  },
  {
    "text": "inference time uh fine tuning is usually",
    "start": "115200",
    "end": "117759"
  },
  {
    "text": "how this is done and that's awfully",
    "start": "117759",
    "end": "119360"
  },
  {
    "text": "expensive",
    "start": "119360",
    "end": "120600"
  },
  {
    "text": "uh and more so than that including all",
    "start": "120600",
    "end": "122640"
  },
  {
    "text": "of that context in your prompt can",
    "start": "122640",
    "end": "125079"
  },
  {
    "text": "confuse the model with a lot of",
    "start": "125079",
    "end": "126200"
  },
  {
    "text": "irrelevant information um and kind of",
    "start": "126200",
    "end": "129200"
  },
  {
    "text": "beyond that just conceptually speaking",
    "start": "129200",
    "end": "131080"
  },
  {
    "text": "it seems a little like wasteful right",
    "start": "131080",
    "end": "132760"
  },
  {
    "text": "like if we're trying to do question",
    "start": "132760",
    "end": "133920"
  },
  {
    "text": "answering over a big code base uh our",
    "start": "133920",
    "end": "136720"
  },
  {
    "text": "query is most usually does not need to",
    "start": "136720",
    "end": "139319"
  },
  {
    "text": "reference like all of those different",
    "start": "139319",
    "end": "140560"
  },
  {
    "text": "function definitions but just need some",
    "start": "140560",
    "end": "142160"
  },
  {
    "text": "subset of them to answer the query",
    "start": "142160",
    "end": "144200"
  },
  {
    "text": "correctly um okay so this is what rag",
    "start": "144200",
    "end": "146640"
  },
  {
    "text": "tries to do right let's try to subset",
    "start": "146640",
    "end": "148440"
  },
  {
    "text": "that information down and just include",
    "start": "148440",
    "end": "150519"
  },
  {
    "text": "the most relevant context in our prompt",
    "start": "150519",
    "end": "154400"
  },
  {
    "text": "um so what are the issues here well",
    "start": "154400",
    "end": "157720"
  },
  {
    "text": "these these mechanisms which are",
    "start": "157720",
    "end": "159159"
  },
  {
    "text": "external to the Transformer are kind of",
    "start": "159159",
    "end": "160920"
  },
  {
    "text": "like necessarily limited by being",
    "start": "160920",
    "end": "163000"
  },
  {
    "text": "external to the model so we make this",
    "start": "163000",
    "end": "165239"
  },
  {
    "text": "choice of what's relevant once and",
    "start": "165239",
    "end": "167120"
  },
  {
    "text": "upfront before the generation starts and",
    "start": "167120",
    "end": "169840"
  },
  {
    "text": "we're also making this choice about",
    "start": "169840",
    "end": "171239"
  },
  {
    "text": "what's relevant using kind of the least",
    "start": "171239",
    "end": "173519"
  },
  {
    "text": "granular representation of that data and",
    "start": "173519",
    "end": "176159"
  },
  {
    "text": "often ones that are disjoint from the",
    "start": "176159",
    "end": "178120"
  },
  {
    "text": "way that the model will reason about",
    "start": "178120",
    "end": "179920"
  },
  {
    "text": "that data um kind of also just",
    "start": "179920",
    "end": "183280"
  },
  {
    "text": "conceptually neither of these methods",
    "start": "183280",
    "end": "185720"
  },
  {
    "text": "make a difference uh or make a",
    "start": "185720",
    "end": "187560"
  },
  {
    "text": "distinction between things that should",
    "start": "187560",
    "end": "189319"
  },
  {
    "text": "go in memory and things that should be",
    "start": "189319",
    "end": "191000"
  },
  {
    "text": "included along with your inference query",
    "start": "191000",
    "end": "193159"
  },
  {
    "text": "and this is more than just Aesthetics",
    "start": "193159",
    "end": "194480"
  },
  {
    "text": "it's actually going to enable us to",
    "start": "194480",
    "end": "198080"
  },
  {
    "text": "oh it's going to enable us to have these",
    "start": "198080",
    "end": "201440"
  },
  {
    "text": "like more granular causal citations uh",
    "start": "201440",
    "end": "204000"
  },
  {
    "text": "and allow the model to retrieve more",
    "start": "204000",
    "end": "205959"
  },
  {
    "text": "information when we can tell it's",
    "start": "205959",
    "end": "207080"
  },
  {
    "text": "uncertain kind of actively within the",
    "start": "207080",
    "end": "209000"
  },
  {
    "text": "generation",
    "start": "209000",
    "end": "211760"
  },
  {
    "start": "212000",
    "end": "376000"
  },
  {
    "text": "all right so how do we do this extended",
    "start": "212239",
    "end": "214360"
  },
  {
    "text": "mind attention is a very simple edit to",
    "start": "214360",
    "end": "216799"
  },
  {
    "text": "the attention mechanism of the",
    "start": "216799",
    "end": "218080"
  },
  {
    "text": "Transformer I'm not going to get too",
    "start": "218080",
    "end": "219840"
  },
  {
    "text": "much into the math because we don't have",
    "start": "219840",
    "end": "221000"
  },
  {
    "text": "a ton of time today but would love for",
    "start": "221000",
    "end": "222799"
  },
  {
    "text": "anyone to check out the paper and let me",
    "start": "222799",
    "end": "224280"
  },
  {
    "text": "know what you think um so but I'll just",
    "start": "224280",
    "end": "227080"
  },
  {
    "text": "go over kind of yeah from a qualitative",
    "start": "227080",
    "end": "229319"
  },
  {
    "text": "perspective how this works so the model",
    "start": "229319",
    "end": "232439"
  },
  {
    "text": "represents data within each decoder",
    "start": "232439",
    "end": "234799"
  },
  {
    "text": "layer most of the Transformers that",
    "start": "234799",
    "end": "236319"
  },
  {
    "text": "we're using today are decoder only",
    "start": "236319",
    "end": "238079"
  },
  {
    "text": "Transformers and within each of those",
    "start": "238079",
    "end": "239959"
  },
  {
    "text": "decoder layers the model will represent",
    "start": "239959",
    "end": "242120"
  },
  {
    "text": "that data as a key value pair so it",
    "start": "242120",
    "end": "244480"
  },
  {
    "text": "actually already has this retrieval",
    "start": "244480",
    "end": "246400"
  },
  {
    "text": "mechanism built into the Transformer all",
    "start": "246400",
    "end": "248560"
  },
  {
    "text": "we have to do is kind of hack around it",
    "start": "248560",
    "end": "251560"
  },
  {
    "text": "and so we pass all of the memory tokens",
    "start": "251560",
    "end": "254000"
  },
  {
    "text": "through the model and save off those key",
    "start": "254000",
    "end": "256239"
  },
  {
    "text": "value representations and then during",
    "start": "256239",
    "end": "258440"
  },
  {
    "text": "generation time we allow each query",
    "start": "258440",
    "end": "261079"
  },
  {
    "text": "token just like rag using cosine",
    "start": "261079",
    "end": "263400"
  },
  {
    "text": "similarity to go retrieve a particular",
    "start": "263400",
    "end": "266160"
  },
  {
    "text": "number of those memory tokens and attend",
    "start": "266160",
    "end": "268639"
  },
  {
    "text": "to them so this in this picture these",
    "start": "268639",
    "end": "271160"
  },
  {
    "text": "kind of red tokens red highlighted",
    "start": "271160",
    "end": "273360"
  },
  {
    "text": "tokens are meant to uh represent those",
    "start": "273360",
    "end": "275160"
  },
  {
    "text": "retrieved",
    "start": "275160",
    "end": "277000"
  },
  {
    "text": "tokens uh again this actually ends up",
    "start": "277000",
    "end": "279600"
  },
  {
    "text": "being a very simple change to the",
    "start": "279600",
    "end": "281120"
  },
  {
    "text": "Transformer model what's difficult uh is",
    "start": "281120",
    "end": "284479"
  },
  {
    "text": "figuring out how to assign position",
    "start": "284479",
    "end": "286199"
  },
  {
    "text": "information to those tokens so this uh",
    "start": "286199",
    "end": "289240"
  },
  {
    "text": "work is based on Research from a couple",
    "start": "289240",
    "end": "291120"
  },
  {
    "text": "years ago but they needed to fine-tune",
    "start": "291120",
    "end": "293280"
  },
  {
    "text": "their model in order to kind of teach",
    "start": "293280",
    "end": "295600"
  },
  {
    "text": "the model how to leverage these",
    "start": "295600",
    "end": "296840"
  },
  {
    "text": "retrieved tokens and that's in large",
    "start": "296840",
    "end": "299120"
  },
  {
    "text": "part due to absolute position embeddings",
    "start": "299120",
    "end": "301160"
  },
  {
    "text": "that were popular during that time so",
    "start": "301160",
    "end": "303800"
  },
  {
    "text": "because Transformer models are position",
    "start": "303800",
    "end": "305720"
  },
  {
    "text": "agnostic we have to figure out how to",
    "start": "305720",
    "end": "307840"
  },
  {
    "text": "kind of tell them like okay this token",
    "start": "307840",
    "end": "309720"
  },
  {
    "text": "is position zero this one is to position",
    "start": "309720",
    "end": "311800"
  },
  {
    "text": "one etc etc um but due to today's more",
    "start": "311800",
    "end": "316160"
  },
  {
    "text": "kind of like their softer position",
    "start": "316160",
    "end": "318639"
  },
  {
    "text": "embeddings this allows us to really",
    "start": "318639",
    "end": "320600"
  },
  {
    "text": "leverage this method without any further",
    "start": "320600",
    "end": "322919"
  },
  {
    "text": "fine-tuning so in particular these",
    "start": "322919",
    "end": "325080"
  },
  {
    "text": "relative position medings that have",
    "start": "325080",
    "end": "326560"
  },
  {
    "text": "become popular and I'll talk about two",
    "start": "326560",
    "end": "328600"
  },
  {
    "text": "different methods that we've tested and",
    "start": "328600",
    "end": "330240"
  },
  {
    "text": "implemented this on um really enable the",
    "start": "330240",
    "end": "333120"
  },
  {
    "text": "model to kind of generalize um to these",
    "start": "333120",
    "end": "336440"
  },
  {
    "text": "retrieved tokens the first one uh that",
    "start": "336440",
    "end": "339000"
  },
  {
    "text": "we tested on is present in all of the",
    "start": "339000",
    "end": "340919"
  },
  {
    "text": "Llama models these are the rotary",
    "start": "340919",
    "end": "342560"
  },
  {
    "text": "position embeddings and this generalizes",
    "start": "342560",
    "end": "345039"
  },
  {
    "text": "the principle of using kind of like an",
    "start": "345039",
    "end": "347600"
  },
  {
    "text": "angle between two vectors as a distance",
    "start": "347600",
    "end": "349680"
  },
  {
    "text": "metric so we kind of take the whole",
    "start": "349680",
    "end": "351199"
  },
  {
    "text": "embedding and we rotate kind of two",
    "start": "351199",
    "end": "352759"
  },
  {
    "text": "positions at a time the other one that",
    "start": "352759",
    "end": "355039"
  },
  {
    "text": "we implemented um this method into is",
    "start": "355039",
    "end": "358600"
  },
  {
    "text": "The Alibi uh linear biases so these",
    "start": "358600",
    "end": "361840"
  },
  {
    "text": "actually aren't positioning embeddings",
    "start": "361840",
    "end": "363520"
  },
  {
    "text": "at all it's just kind of linearly damps",
    "start": "363520",
    "end": "366319"
  },
  {
    "text": "down uh information which is further",
    "start": "366319",
    "end": "369080"
  },
  {
    "text": "away and these are uh the way that all",
    "start": "369080",
    "end": "371199"
  },
  {
    "text": "of the mosaics MPT models are",
    "start": "371199",
    "end": "375199"
  },
  {
    "start": "376000",
    "end": "447000"
  },
  {
    "text": "trained okay so let's talk about some",
    "start": "376000",
    "end": "378400"
  },
  {
    "text": "evaluations um we also just open sourced",
    "start": "378400",
    "end": "380759"
  },
  {
    "text": "a new counterfactual retrieval Benchmark",
    "start": "380759",
    "end": "383039"
  },
  {
    "text": "and I'm just going to briefly describe",
    "start": "383039",
    "end": "384440"
  },
  {
    "text": "what that Benchmark looks like so this",
    "start": "384440",
    "end": "386599"
  },
  {
    "text": "is a long context Benchmark so our input",
    "start": "386599",
    "end": "389039"
  },
  {
    "text": "context is are query answer pairs uh and",
    "start": "389039",
    "end": "392440"
  },
  {
    "text": "the context to answer those questions",
    "start": "392440",
    "end": "394240"
  },
  {
    "text": "range from about 2,000 tokens to all the",
    "start": "394240",
    "end": "396720"
  },
  {
    "text": "way up to 16,000 tokens and the again",
    "start": "396720",
    "end": "399840"
  },
  {
    "text": "these are like query so like the",
    "start": "399840",
    "end": "401479"
  },
  {
    "text": "question might be who wrote the song",
    "start": "401479",
    "end": "402960"
  },
  {
    "text": "these shoes were made for walking and",
    "start": "402960",
    "end": "404680"
  },
  {
    "text": "then the corresponding Wikipedia snippet",
    "start": "404680",
    "end": "407680"
  },
  {
    "text": "um we wanted to control for facts",
    "start": "407680",
    "end": "409720"
  },
  {
    "text": "memorized during pre-training though and",
    "start": "409720",
    "end": "411639"
  },
  {
    "text": "actually any fine tuning also so what we",
    "start": "411639",
    "end": "414520"
  },
  {
    "text": "did was we looked up for instance in",
    "start": "414520",
    "end": "416520"
  },
  {
    "text": "this case the answer is Lee Hazelwood we",
    "start": "416520",
    "end": "418560"
  },
  {
    "text": "did a little bit of research we figured",
    "start": "418560",
    "end": "419919"
  },
  {
    "text": "out okay well Terry Allen is a similar",
    "start": "419919",
    "end": "422039"
  },
  {
    "text": "songwriter this is a plausible answer",
    "start": "422039",
    "end": "424639"
  },
  {
    "text": "but it's wrong we went in and we",
    "start": "424639",
    "end": "426560"
  },
  {
    "text": "replaced all the instances of Lee",
    "start": "426560",
    "end": "428440"
  },
  {
    "text": "Hazelwood with Terry Allen and now we",
    "start": "428440",
    "end": "430360"
  },
  {
    "text": "asked the model to retrieve this new you",
    "start": "430360",
    "end": "432720"
  },
  {
    "text": "know not factually correct but in the",
    "start": "432720",
    "end": "435599"
  },
  {
    "text": "sense it we're trying to test whether",
    "start": "435599",
    "end": "437280"
  },
  {
    "text": "it's prioritizing what's being provided",
    "start": "437280",
    "end": "439319"
  },
  {
    "text": "at inference time um so now we're asking",
    "start": "439319",
    "end": "442120"
  },
  {
    "text": "it to retrieve this Terry Allen",
    "start": "442120",
    "end": "446319"
  },
  {
    "text": "answer all right so how to extend M",
    "start": "446759",
    "end": "449759"
  },
  {
    "start": "447000",
    "end": "538000"
  },
  {
    "text": "Transformers stack up here we're",
    "start": "449759",
    "end": "451639"
  },
  {
    "text": "comparing it with fine-tuned models as",
    "start": "451639",
    "end": "454160"
  },
  {
    "text": "well as the base llama model with uh",
    "start": "454160",
    "end": "456840"
  },
  {
    "text": "interpolated position embeddings so we",
    "start": "456840",
    "end": "459120"
  },
  {
    "text": "can see here in the green that the base",
    "start": "459120",
    "end": "461560"
  },
  {
    "text": "model does a pretty good job",
    "start": "461560",
    "end": "462960"
  },
  {
    "text": "extrapolating even like many times more",
    "start": "462960",
    "end": "466360"
  },
  {
    "text": "this is a model trained up to like 20 48",
    "start": "466360",
    "end": "469280"
  },
  {
    "text": "tokens um during pre-training and you",
    "start": "469280",
    "end": "471560"
  },
  {
    "text": "can see even up to 8K it's like doing",
    "start": "471560",
    "end": "473560"
  },
  {
    "text": "okay 16k it really falls off the",
    "start": "473560",
    "end": "476360"
  },
  {
    "text": "position embeddings can't extrapolate",
    "start": "476360",
    "end": "478319"
  },
  {
    "text": "that far the tune models you can see",
    "start": "478319",
    "end": "480680"
  },
  {
    "text": "actually perform worse than the extended",
    "start": "480680",
    "end": "482400"
  },
  {
    "text": "mind model on these shorter inputs and",
    "start": "482400",
    "end": "485080"
  },
  {
    "text": "this is another data point that suggests",
    "start": "485080",
    "end": "486720"
  },
  {
    "text": "that fine tuning on super long contexts",
    "start": "486720",
    "end": "489400"
  },
  {
    "text": "actually degrades the quality of",
    "start": "489400",
    "end": "490759"
  },
  {
    "text": "attention that you get on shorter inputs",
    "start": "490759",
    "end": "493240"
  },
  {
    "text": "and extended mind Transformers continue",
    "start": "493240",
    "end": "494960"
  },
  {
    "text": "to be competitive with those fine-tuned",
    "start": "494960",
    "end": "496599"
  },
  {
    "text": "models all the way up to 16k again our",
    "start": "496599",
    "end": "499159"
  },
  {
    "text": "models are not fine-tuned at",
    "start": "499159",
    "end": "501639"
  },
  {
    "text": "all and in this particular experiment so",
    "start": "501639",
    "end": "504879"
  },
  {
    "text": "what the extended mind model sees in",
    "start": "504879",
    "end": "507080"
  },
  {
    "text": "context is the query only so it only",
    "start": "507080",
    "end": "509639"
  },
  {
    "text": "sees the like who wrote the song these",
    "start": "509639",
    "end": "511599"
  },
  {
    "text": "uses made for walking and relies heavily",
    "start": "511599",
    "end": "513880"
  },
  {
    "text": "on that internal retrieval mechanism to",
    "start": "513880",
    "end": "516800"
  },
  {
    "text": "go look up that new information in this",
    "start": "516800",
    "end": "519839"
  },
  {
    "text": "second experiment we seed it with a",
    "start": "519839",
    "end": "521599"
  },
  {
    "text": "little bit more information in context",
    "start": "521599",
    "end": "523959"
  },
  {
    "text": "uh using rag but again mostly relying on",
    "start": "523959",
    "end": "527000"
  },
  {
    "text": "that uh internal mechanism still uh and",
    "start": "527000",
    "end": "529560"
  },
  {
    "text": "you can see we're outperforming dpt4",
    "start": "529560",
    "end": "531320"
  },
  {
    "text": "here now when we combine it with that",
    "start": "531320",
    "end": "532920"
  },
  {
    "text": "more information and context as",
    "start": "532920",
    "end": "536600"
  },
  {
    "text": "well okay now we're going to talk about",
    "start": "537600",
    "end": "539760"
  },
  {
    "start": "538000",
    "end": "627000"
  },
  {
    "text": "citations so I think uh this would be a",
    "start": "539760",
    "end": "541880"
  },
  {
    "text": "topic that lots of you here can",
    "start": "541880",
    "end": "543440"
  },
  {
    "text": "empathize with uh as AI Engineers I",
    "start": "543440",
    "end": "546480"
  },
  {
    "text": "think this is one of the most important",
    "start": "546480",
    "end": "548320"
  },
  {
    "text": "things to provide in an application such",
    "start": "548320",
    "end": "550480"
  },
  {
    "text": "that people can learn to trust the model",
    "start": "550480",
    "end": "552320"
  },
  {
    "text": "outputs in fact you might actually use",
    "start": "552320",
    "end": "554399"
  },
  {
    "text": "rag just to get citations um so with rag",
    "start": "554399",
    "end": "558560"
  },
  {
    "text": "though the citations that you get are a",
    "start": "558560",
    "end": "559920"
  },
  {
    "text": "little bit kind of like post talk",
    "start": "559920",
    "end": "562000"
  },
  {
    "text": "rationalization so maybe if like the",
    "start": "562000",
    "end": "564040"
  },
  {
    "text": "date appears in the output and we knew",
    "start": "564040",
    "end": "565959"
  },
  {
    "text": "it was also in the input to the language",
    "start": "565959",
    "end": "567839"
  },
  {
    "text": "model we feel pretty confid that that",
    "start": "567839",
    "end": "569920"
  },
  {
    "text": "date is not hallucinated um but again",
    "start": "569920",
    "end": "572200"
  },
  {
    "text": "this is not really like a causally",
    "start": "572200",
    "end": "573880"
  },
  {
    "text": "related to what information the model",
    "start": "573880",
    "end": "575640"
  },
  {
    "text": "used during the generation now with",
    "start": "575640",
    "end": "578240"
  },
  {
    "text": "extended mind Transformers We can look",
    "start": "578240",
    "end": "579720"
  },
  {
    "text": "up exactly which tokens were retrieved",
    "start": "579720",
    "end": "582920"
  },
  {
    "text": "from the from those memories and used",
    "start": "582920",
    "end": "585040"
  },
  {
    "text": "during generation so in this example on",
    "start": "585040",
    "end": "587480"
  },
  {
    "text": "the top left here we have the memories",
    "start": "587480",
    "end": "589959"
  },
  {
    "text": "this is a sampit from Wikipedia about",
    "start": "589959",
    "end": "591880"
  },
  {
    "text": "one of my favorite mathematicians",
    "start": "591880",
    "end": "593240"
  },
  {
    "text": "Alexander grend and the query is when",
    "start": "593240",
    "end": "595839"
  },
  {
    "text": "did he get his friend citizenship and",
    "start": "595839",
    "end": "598240"
  },
  {
    "text": "then in the bottom you can see the",
    "start": "598240",
    "end": "599480"
  },
  {
    "text": "completion with a correct date I think",
    "start": "599480",
    "end": "601320"
  },
  {
    "text": "he got it in",
    "start": "601320",
    "end": "602519"
  },
  {
    "text": "1971 so the blue highlighted tokens here",
    "start": "602519",
    "end": "606720"
  },
  {
    "text": "uh importantly the 1971 as well as some",
    "start": "606720",
    "end": "609240"
  },
  {
    "text": "of the Alexander growth and de tokens um",
    "start": "609240",
    "end": "611839"
  },
  {
    "text": "those are the ones that the model",
    "start": "611839",
    "end": "613120"
  },
  {
    "text": "retrieved and attended to when",
    "start": "613120",
    "end": "614760"
  },
  {
    "text": "generating that 1971 correct token and",
    "start": "614760",
    "end": "617839"
  },
  {
    "text": "so being able to report that uh gives a",
    "start": "617839",
    "end": "619880"
  },
  {
    "text": "lot of confidence and also just insight",
    "start": "619880",
    "end": "621560"
  },
  {
    "text": "into how the model is using those",
    "start": "621560",
    "end": "623160"
  },
  {
    "text": "retrieved",
    "start": "623160",
    "end": "625759"
  },
  {
    "text": "tokens okay we can also use extended",
    "start": "626839",
    "end": "629880"
  },
  {
    "start": "627000",
    "end": "712000"
  },
  {
    "text": "mind Transformers to reduce",
    "start": "629880",
    "end": "631200"
  },
  {
    "text": "hallucinations so how do we do this so",
    "start": "631200",
    "end": "633959"
  },
  {
    "text": "right now we have access to in the like",
    "start": "633959",
    "end": "636519"
  },
  {
    "text": "simplest case just kind of token level",
    "start": "636519",
    "end": "638320"
  },
  {
    "text": "entropy over that output distribution",
    "start": "638320",
    "end": "641440"
  },
  {
    "text": "and if you wanted to get fancier we're",
    "start": "641440",
    "end": "642959"
  },
  {
    "text": "also doing some basy and fine-tuning of",
    "start": "642959",
    "end": "644560"
  },
  {
    "text": "language models at normal but you can",
    "start": "644560",
    "end": "646480"
  },
  {
    "text": "use any uncertainty metric to determine",
    "start": "646480",
    "end": "649000"
  },
  {
    "text": "kind of how certain the model is about a",
    "start": "649000",
    "end": "651040"
  },
  {
    "text": "generated token and if we kind of can",
    "start": "651040",
    "end": "654079"
  },
  {
    "text": "detect that the model is uncertain about",
    "start": "654079",
    "end": "655760"
  },
  {
    "text": "that token we can regenerate that step",
    "start": "655760",
    "end": "658600"
  },
  {
    "text": "using more information from these",
    "start": "658600",
    "end": "661079"
  },
  {
    "text": "memories uh okay so in the top right",
    "start": "661079",
    "end": "663279"
  },
  {
    "text": "here we can see this is we just set like",
    "start": "663279",
    "end": "664959"
  },
  {
    "text": "a baseline default number of memories",
    "start": "664959",
    "end": "667639"
  },
  {
    "text": "that each query token is allowed to",
    "start": "667639",
    "end": "669040"
  },
  {
    "text": "retrieve and attend to and you can see",
    "start": "669040",
    "end": "671120"
  },
  {
    "text": "it wasn't quite enough information uh to",
    "start": "671120",
    "end": "673480"
  },
  {
    "text": "get this query right so if you remember",
    "start": "673480",
    "end": "675440"
  },
  {
    "text": "from the previous Slide the correct",
    "start": "675440",
    "end": "676600"
  },
  {
    "text": "answer here is",
    "start": "676600",
    "end": "677600"
  },
  {
    "text": "1971 and you can see we've got 1993 here",
    "start": "677600",
    "end": "680760"
  },
  {
    "text": "so wasn't enough we didn't attend to",
    "start": "680760",
    "end": "683440"
  },
  {
    "text": "that memory quite enough to get this",
    "start": "683440",
    "end": "684880"
  },
  {
    "text": "question right and in the bottom example",
    "start": "684880",
    "end": "688200"
  },
  {
    "text": "we allow it to read generate some subset",
    "start": "688200",
    "end": "690399"
  },
  {
    "text": "of those tokens using more information",
    "start": "690399",
    "end": "692800"
  },
  {
    "text": "from the cache when we can tell the",
    "start": "692800",
    "end": "694920"
  },
  {
    "text": "model was",
    "start": "694920",
    "end": "696600"
  },
  {
    "text": "uncertain and again got this right so",
    "start": "696600",
    "end": "699600"
  },
  {
    "text": "it's kind of like kind of a nice",
    "start": "699600",
    "end": "701360"
  },
  {
    "text": "intuition for uh when the model's",
    "start": "701360",
    "end": "703800"
  },
  {
    "text": "uncertain and then okay if it's really",
    "start": "703800",
    "end": "705399"
  },
  {
    "text": "uncertain let's go use more information",
    "start": "705399",
    "end": "707360"
  },
  {
    "text": "and also can be more efficient kind of",
    "start": "707360",
    "end": "709000"
  },
  {
    "text": "depending on how the math works",
    "start": "709000",
    "end": "712079"
  },
  {
    "start": "712000",
    "end": "881000"
  },
  {
    "text": "out all right so now I'm going to tell",
    "start": "712320",
    "end": "714440"
  },
  {
    "text": "you guys about the most important uh",
    "start": "714440",
    "end": "716079"
  },
  {
    "text": "parameters to set when using extended",
    "start": "716079",
    "end": "718240"
  },
  {
    "text": "mind Transformers so you may have heard",
    "start": "718240",
    "end": "720120"
  },
  {
    "text": "of something called stride length before",
    "start": "720120",
    "end": "722720"
  },
  {
    "text": "uh and this is um a parameter that comes",
    "start": "722720",
    "end": "725160"
  },
  {
    "text": "up a lot even just kind of in regular",
    "start": "725160",
    "end": "726880"
  },
  {
    "text": "perplexity computations so when we",
    "start": "726880",
    "end": "729440"
  },
  {
    "text": "compute the memories that we're going to",
    "start": "729440",
    "end": "731120"
  },
  {
    "text": "attend to we pass them through the model",
    "start": "731120",
    "end": "733480"
  },
  {
    "text": "and then again save off these key value",
    "start": "733480",
    "end": "735440"
  },
  {
    "text": "representations that the model saves",
    "start": "735440",
    "end": "737680"
  },
  {
    "text": "internally um but again the models that",
    "start": "737680",
    "end": "740480"
  },
  {
    "text": "we're using are trained on this fixed",
    "start": "740480",
    "end": "742240"
  },
  {
    "text": "context length so we need to kind of",
    "start": "742240",
    "end": "745240"
  },
  {
    "text": "pass over them with some stride such",
    "start": "745240",
    "end": "747440"
  },
  {
    "text": "that each of those tokens has an",
    "start": "747440",
    "end": "749680"
  },
  {
    "text": "appropriate amount of context um to",
    "start": "749680",
    "end": "752240"
  },
  {
    "text": "generate the representation so if the",
    "start": "752240",
    "end": "754560"
  },
  {
    "text": "stride is smaller uh you're going to get",
    "start": "754560",
    "end": "757320"
  },
  {
    "text": "more uh high quality representations but",
    "start": "757320",
    "end": "760279"
  },
  {
    "text": "also will require more computations um",
    "start": "760279",
    "end": "762839"
  },
  {
    "text": "so you can kind of tune this and there",
    "start": "762839",
    "end": "764560"
  },
  {
    "text": "are some graphs in the paper as well",
    "start": "764560",
    "end": "766360"
  },
  {
    "text": "that kind of represent this tradeoff um",
    "start": "766360",
    "end": "768880"
  },
  {
    "text": "but this is an important parameter to",
    "start": "768880",
    "end": "770120"
  },
  {
    "text": "set when yeah generating the memories",
    "start": "770120",
    "end": "772440"
  },
  {
    "text": "themselves top K is uh probably the most",
    "start": "772440",
    "end": "775160"
  },
  {
    "text": "important parameter to think about so",
    "start": "775160",
    "end": "776920"
  },
  {
    "text": "this is the number of key Valu pairs or",
    "start": "776920",
    "end": "779920"
  },
  {
    "text": "memories that each query token is",
    "start": "779920",
    "end": "781519"
  },
  {
    "text": "allowed to retrieve and attend to um",
    "start": "781519",
    "end": "784480"
  },
  {
    "text": "when your memory is quite long kind of",
    "start": "784480",
    "end": "786480"
  },
  {
    "text": "the more the better um but again uh yeah",
    "start": "786480",
    "end": "790360"
  },
  {
    "text": "this it's kind of should be dynamically",
    "start": "790360",
    "end": "791959"
  },
  {
    "text": "set based on how long your memory",
    "start": "791959",
    "end": "794720"
  },
  {
    "text": "is um okay yeah so lastly uh we want to",
    "start": "794720",
    "end": "799079"
  },
  {
    "text": "retrieve as much information as we can",
    "start": "799079",
    "end": "800920"
  },
  {
    "text": "from the memory without confusing the",
    "start": "800920",
    "end": "802920"
  },
  {
    "text": "model it's making analogy back to kind",
    "start": "802920",
    "end": "805199"
  },
  {
    "text": "of putting everything into context we",
    "start": "805199",
    "end": "807160"
  },
  {
    "text": "don't want to just throw everything in",
    "start": "807160",
    "end": "808639"
  },
  {
    "text": "there because that will be confusing to",
    "start": "808639",
    "end": "810199"
  },
  {
    "text": "the model um so we have two different",
    "start": "810199",
    "end": "812360"
  },
  {
    "text": "regularization techniques that we",
    "start": "812360",
    "end": "813760"
  },
  {
    "text": "Implement that we have found to be",
    "start": "813760",
    "end": "815560"
  },
  {
    "text": "especially effective um the first one is",
    "start": "815560",
    "end": "818560"
  },
  {
    "text": "called similarity masking so again we we",
    "start": "818560",
    "end": "822000"
  },
  {
    "text": "retrieve these tokens uh based on",
    "start": "822000",
    "end": "824600"
  },
  {
    "text": "similarity with our query token and the",
    "start": "824600",
    "end": "826760"
  },
  {
    "text": "key that we are retrieving from and so",
    "start": "826760",
    "end": "829279"
  },
  {
    "text": "we might say like well if we don't hit",
    "start": "829279",
    "end": "830759"
  },
  {
    "text": "some similarity threshold like we'll",
    "start": "830759",
    "end": "833079"
  },
  {
    "text": "retrieve a lot of them but then if they",
    "start": "833079",
    "end": "834839"
  },
  {
    "text": "you know if they're not at least like",
    "start": "834839",
    "end": "836040"
  },
  {
    "text": "0.25 similar then we'll just throw them",
    "start": "836040",
    "end": "838240"
  },
  {
    "text": "out so we can retrieve and then just",
    "start": "838240",
    "end": "840160"
  },
  {
    "text": "mask the ones that end up being less",
    "start": "840160",
    "end": "841959"
  },
  {
    "text": "important uh",
    "start": "841959",
    "end": "843839"
  },
  {
    "text": "another another important regularization",
    "start": "843839",
    "end": "846040"
  },
  {
    "text": "technique in particular for models that",
    "start": "846040",
    "end": "848160"
  },
  {
    "text": "are trained using rope uh is to",
    "start": "848160",
    "end": "850279"
  },
  {
    "text": "eliminate tokens from the memory that",
    "start": "850279",
    "end": "852519"
  },
  {
    "text": "correspond to unknown tokens so",
    "start": "852519",
    "end": "854839"
  },
  {
    "text": "especially if your data is super messy a",
    "start": "854839",
    "end": "856639"
  },
  {
    "text": "lot of the Wikipedia based benchmarks",
    "start": "856639",
    "end": "858440"
  },
  {
    "text": "are like really way more messy than I",
    "start": "858440",
    "end": "860440"
  },
  {
    "text": "even knew before I started working on",
    "start": "860440",
    "end": "861800"
  },
  {
    "text": "this stuff uh they have a lot of like",
    "start": "861800",
    "end": "863920"
  },
  {
    "text": "just unknown tokens and so they're kind",
    "start": "863920",
    "end": "866079"
  },
  {
    "text": "of like poorly represented by the models",
    "start": "866079",
    "end": "867920"
  },
  {
    "text": "often because they're unknown",
    "start": "867920",
    "end": "869560"
  },
  {
    "text": "they end up having a lot of matches with",
    "start": "869560",
    "end": "871040"
  },
  {
    "text": "your query tokens but then they're not",
    "start": "871040",
    "end": "873000"
  },
  {
    "text": "actually containing a lot of useful",
    "start": "873000",
    "end": "874480"
  },
  {
    "text": "information um so we just eliminate",
    "start": "874480",
    "end": "876560"
  },
  {
    "text": "those from the memory before we allow it",
    "start": "876560",
    "end": "878360"
  },
  {
    "text": "to start",
    "start": "878360",
    "end": "880880"
  },
  {
    "text": "retrieving all right so we have a whole",
    "start": "881120",
    "end": "883880"
  },
  {
    "text": "collection of these models on hugging",
    "start": "883880",
    "end": "885240"
  },
  {
    "text": "face all of the uh code is on GitHub as",
    "start": "885240",
    "end": "887720"
  },
  {
    "text": "well as that data set um and encourage",
    "start": "887720",
    "end": "890440"
  },
  {
    "text": "you all to read the paper if you're",
    "start": "890440",
    "end": "891600"
  },
  {
    "text": "curious about more of the technical",
    "start": "891600",
    "end": "892639"
  },
  {
    "text": "details uh as I hope you can see here",
    "start": "892639",
    "end": "895120"
  },
  {
    "text": "it's actually pretty easy to use these",
    "start": "895120",
    "end": "896639"
  },
  {
    "text": "things so it's as simple as passing",
    "start": "896639",
    "end": "898320"
  },
  {
    "text": "those memories in in as inputs uh as",
    "start": "898320",
    "end": "901279"
  },
  {
    "text": "tokens into the model during",
    "start": "901279",
    "end": "903279"
  },
  {
    "text": "instantiation um you can dynamically",
    "start": "903279",
    "end": "905279"
  },
  {
    "text": "change them after that as well but it's",
    "start": "905279",
    "end": "906920"
  },
  {
    "text": "the easiest way to do it uh and then",
    "start": "906920",
    "end": "908759"
  },
  {
    "text": "making sure your config is set up",
    "start": "908759",
    "end": "912040"
  },
  {
    "text": "correctly all right so just to conclude",
    "start": "912639",
    "end": "914800"
  },
  {
    "text": "here uh I hope you all will take away",
    "start": "914800",
    "end": "916800"
  },
  {
    "text": "that these new kinds of models um",
    "start": "916800",
    "end": "919240"
  },
  {
    "text": "improve achieve impressive performance",
    "start": "919240",
    "end": "921120"
  },
  {
    "text": "on retrieval tasks they enable these",
    "start": "921120",
    "end": "923440"
  },
  {
    "text": "great new kind of citations um they also",
    "start": "923440",
    "end": "926360"
  },
  {
    "text": "enable this new kind of hallucination",
    "start": "926360",
    "end": "928440"
  },
  {
    "text": "reduction Tech technique which is",
    "start": "928440",
    "end": "929759"
  },
  {
    "text": "inspired by Active Learning they do not",
    "start": "929759",
    "end": "932319"
  },
  {
    "text": "require fine-tuning unlike kind of long",
    "start": "932319",
    "end": "934959"
  },
  {
    "text": "context methods uh and they can easily",
    "start": "934959",
    "end": "937519"
  },
  {
    "text": "run using our open source models and",
    "start": "937519",
    "end": "940600"
  },
  {
    "text": "code thanks so much and uh find me after",
    "start": "940600",
    "end": "943120"
  },
  {
    "text": "four questions",
    "start": "943120",
    "end": "946360"
  },
  {
    "text": "[Music]",
    "start": "948110",
    "end": "965378"
  }
]