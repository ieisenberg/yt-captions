[
  {
    "text": "[Music]",
    "start": "350",
    "end": "13519"
  },
  {
    "text": "my name is Kathleen canel I'm a research",
    "start": "13519",
    "end": "16480"
  },
  {
    "text": "engineer at Google Deep Mind and as was",
    "start": "16480",
    "end": "19160"
  },
  {
    "text": "just mentioned I'm the technical lead of",
    "start": "19160",
    "end": "21279"
  },
  {
    "text": "the Gemma team before I get started I",
    "start": "21279",
    "end": "24760"
  },
  {
    "text": "just wanted to say uh how awesome it is",
    "start": "24760",
    "end": "27800"
  },
  {
    "text": "to get to be here with you all today",
    "start": "27800",
    "end": "30439"
  },
  {
    "text": "when we were building Gemma our North",
    "start": "30439",
    "end": "33239"
  },
  {
    "text": "Star the the thing we were most excited",
    "start": "33239",
    "end": "35399"
  },
  {
    "text": "about was building something to empower",
    "start": "35399",
    "end": "38440"
  },
  {
    "text": "and accelerate the amazing work being",
    "start": "38440",
    "end": "40960"
  },
  {
    "text": "done by the open source community and",
    "start": "40960",
    "end": "44079"
  },
  {
    "text": "since we launched our first models in",
    "start": "44079",
    "end": "46399"
  },
  {
    "text": "February I have been absolutely Blown",
    "start": "46399",
    "end": "49520"
  },
  {
    "text": "Away by the incredible projects and",
    "start": "49520",
    "end": "52399"
  },
  {
    "text": "research and and innovations that have",
    "start": "52399",
    "end": "55640"
  },
  {
    "text": "been already been built on top of Gemma",
    "start": "55640",
    "end": "58120"
  },
  {
    "text": "um so I'm particularly excited to be",
    "start": "58120",
    "end": "60680"
  },
  {
    "text": "here with so many developers today and",
    "start": "60680",
    "end": "64000"
  },
  {
    "text": "especially delighted to unveil the",
    "start": "64000",
    "end": "66799"
  },
  {
    "text": "latest advancements and additions to the",
    "start": "66799",
    "end": "69360"
  },
  {
    "text": "Gemma model family so without further",
    "start": "69360",
    "end": "72080"
  },
  {
    "text": "Ado we'll get",
    "start": "72080",
    "end": "74680"
  },
  {
    "text": "started as many of you probably know",
    "start": "74680",
    "end": "77880"
  },
  {
    "text": "Google has been a Pioneer in",
    "start": "77880",
    "end": "80520"
  },
  {
    "text": "publications of AI and ml research for",
    "start": "80520",
    "end": "83240"
  },
  {
    "text": "the past decade including publishing",
    "start": "83240",
    "end": "86520"
  },
  {
    "text": "some of the key research that has",
    "start": "86520",
    "end": "88799"
  },
  {
    "text": "sparked recent innov ations we've seen",
    "start": "88799",
    "end": "90880"
  },
  {
    "text": "in AI research like the transformer",
    "start": "90880",
    "end": "94399"
  },
  {
    "text": "senten piece Bert to name a few Google",
    "start": "94399",
    "end": "98399"
  },
  {
    "text": "Deep Mind has really continued this",
    "start": "98399",
    "end": "100680"
  },
  {
    "text": "tradition and is actively working to",
    "start": "100680",
    "end": "103280"
  },
  {
    "text": "share our research for the world to",
    "start": "103280",
    "end": "106040"
  },
  {
    "text": "validate and examine and build upon but",
    "start": "106040",
    "end": "109479"
  },
  {
    "text": "Google's support of the open Community",
    "start": "109479",
    "end": "111479"
  },
  {
    "text": "for AI and ml is not just limited to",
    "start": "111479",
    "end": "114520"
  },
  {
    "text": "publishing research we've also been",
    "start": "114520",
    "end": "116880"
  },
  {
    "text": "doing work to support ml across the",
    "start": "116880",
    "end": "119399"
  },
  {
    "text": "entire technical stack for a long time",
    "start": "119399",
    "end": "122039"
  },
  {
    "text": "from Hardware breakthroughs of tpus um",
    "start": "122039",
    "end": "125200"
  },
  {
    "text": "which I imagine is especially relevant",
    "start": "125200",
    "end": "127159"
  },
  {
    "text": "for this crowd in this track um all the",
    "start": "127159",
    "end": "130360"
  },
  {
    "text": "way to an evolution in ml Frameworks",
    "start": "130360",
    "end": "133000"
  },
  {
    "text": "from tensorflow to Jax through out all",
    "start": "133000",
    "end": "136480"
  },
  {
    "text": "of this open development has been",
    "start": "136480",
    "end": "139440"
  },
  {
    "text": "especially critical for Google our",
    "start": "139440",
    "end": "141800"
  },
  {
    "text": "ability to collaborate with the open",
    "start": "141800",
    "end": "143760"
  },
  {
    "text": "source Community has helped us all",
    "start": "143760",
    "end": "147400"
  },
  {
    "text": "discover more innovate faster",
    "start": "147400",
    "end": "150599"
  },
  {
    "text": "and really push the limits of what AI is",
    "start": "150599",
    "end": "153000"
  },
  {
    "text": "capable of so this long history of",
    "start": "153000",
    "end": "156319"
  },
  {
    "text": "support of the open source Community",
    "start": "156319",
    "end": "158400"
  },
  {
    "text": "leads us to today and to Google's latest",
    "start": "158400",
    "end": "161440"
  },
  {
    "text": "investment in open models Gemma Gemma is",
    "start": "161440",
    "end": "166159"
  },
  {
    "text": "Google deep Minds family of open-source",
    "start": "166159",
    "end": "169560"
  },
  {
    "text": "lightweight state-of-the-art models",
    "start": "169560",
    "end": "172720"
  },
  {
    "text": "which we build from the same research",
    "start": "172720",
    "end": "174599"
  },
  {
    "text": "and technology used to create the Gemini",
    "start": "174599",
    "end": "176680"
  },
  {
    "text": "models I'm so sorry I think that's my",
    "start": "176680",
    "end": "179080"
  },
  {
    "text": "phone off during this talk please feel",
    "start": "179080",
    "end": "182640"
  },
  {
    "text": "free to rumage through that bag wow",
    "start": "182640",
    "end": "185760"
  },
  {
    "text": "lesson learned that even the speaker",
    "start": "185760",
    "end": "187360"
  },
  {
    "text": "needs to remember to silence her cell",
    "start": "187360",
    "end": "189799"
  },
  {
    "text": "phone all right back to Gemma there are",
    "start": "189799",
    "end": "193400"
  },
  {
    "text": "a couple of key advantages of the Gemma",
    "start": "193400",
    "end": "195640"
  },
  {
    "text": "models that I want to highlight today",
    "start": "195640",
    "end": "198080"
  },
  {
    "text": "the first is that Gemma models were",
    "start": "198080",
    "end": "200200"
  },
  {
    "text": "built to be responsible from by Design I",
    "start": "200200",
    "end": "203920"
  },
  {
    "text": "can tell you from personal experience",
    "start": "203920",
    "end": "206400"
  },
  {
    "text": "that from day Zero of developing a Gemma",
    "start": "206400",
    "end": "209280"
  },
  {
    "text": "model safety is a top priority that",
    "start": "209280",
    "end": "212360"
  },
  {
    "text": "means we are manually inspecting data",
    "start": "212360",
    "end": "215560"
  },
  {
    "text": "sets to make sure that we are not only",
    "start": "215560",
    "end": "217360"
  },
  {
    "text": "training on the highest quality data but",
    "start": "217360",
    "end": "220000"
  },
  {
    "text": "also the safest data we can this means",
    "start": "220000",
    "end": "223280"
  },
  {
    "text": "that we are evaluating our models for",
    "start": "223280",
    "end": "225680"
  },
  {
    "text": "safety starting with our earliest",
    "start": "225680",
    "end": "228400"
  },
  {
    "text": "experimentation and ablations so that we",
    "start": "228400",
    "end": "230720"
  },
  {
    "text": "are selecting training methodologies",
    "start": "230720",
    "end": "233000"
  },
  {
    "text": "methodologies that we know will result",
    "start": "233000",
    "end": "235560"
  },
  {
    "text": "in a safer model and at the end of our",
    "start": "235560",
    "end": "238159"
  },
  {
    "text": "development our final model models are",
    "start": "238159",
    "end": "240560"
  },
  {
    "text": "evaluated against the same rigorous",
    "start": "240560",
    "end": "243079"
  },
  {
    "text": "state-of-the-art safety evaluations that",
    "start": "243079",
    "end": "245200"
  },
  {
    "text": "we evaluate Gemini models against and we",
    "start": "245200",
    "end": "248400"
  },
  {
    "text": "really do this to make sure that no",
    "start": "248400",
    "end": "252000"
  },
  {
    "text": "matter where or how you deploy a Gemma",
    "start": "252000",
    "end": "254599"
  },
  {
    "text": "model you can count on the fact that you",
    "start": "254599",
    "end": "257280"
  },
  {
    "text": "will have a trustworthy and responsible",
    "start": "257280",
    "end": "259799"
  },
  {
    "text": "AI application no matter how you've",
    "start": "259799",
    "end": "261919"
  },
  {
    "text": "customized Gemma models you can trust",
    "start": "261919",
    "end": "264400"
  },
  {
    "text": "that it will be a responsible",
    "start": "264400",
    "end": "266199"
  },
  {
    "text": "model Gemma models also achieve un",
    "start": "266199",
    "end": "269720"
  },
  {
    "text": "unparalleled breakthrough performance",
    "start": "269720",
    "end": "272160"
  },
  {
    "text": "for models of their scale including",
    "start": "272160",
    "end": "275560"
  },
  {
    "text": "outperforming significantly larger",
    "start": "275560",
    "end": "278240"
  },
  {
    "text": "models but we'll get some more on that",
    "start": "278240",
    "end": "280960"
  },
  {
    "text": "very",
    "start": "280960",
    "end": "282080"
  },
  {
    "text": "shortly we also designed the Gemma",
    "start": "282080",
    "end": "284520"
  },
  {
    "text": "models to be highly extensible so that",
    "start": "284520",
    "end": "287440"
  },
  {
    "text": "you can use a demo model wherever and",
    "start": "287440",
    "end": "290240"
  },
  {
    "text": "however you want this means they're",
    "start": "290240",
    "end": "292160"
  },
  {
    "text": "optimized for tpus and gpus as well as",
    "start": "292160",
    "end": "295320"
  },
  {
    "text": "for use on your local device they're",
    "start": "295320",
    "end": "297320"
  },
  {
    "text": "supported across many Frameworks cancer",
    "start": "297320",
    "end": "299960"
  },
  {
    "text": "flow Jack Caris pytorch olama",
    "start": "299960",
    "end": "303560"
  },
  {
    "text": "Transformers you name it Gemma is",
    "start": "303560",
    "end": "305759"
  },
  {
    "text": "probably there and finally the real",
    "start": "305759",
    "end": "308840"
  },
  {
    "text": "power of the Gemma models comes from",
    "start": "308840",
    "end": "311360"
  },
  {
    "text": "their Open Access and open",
    "start": "311360",
    "end": "313639"
  },
  {
    "text": "license that period that's what's",
    "start": "313639",
    "end": "316440"
  },
  {
    "text": "powerful about Gemma we put",
    "start": "316440",
    "end": "318479"
  },
  {
    "text": "state-of-the-art technology into your",
    "start": "318479",
    "end": "321000"
  },
  {
    "text": "hands so you can decide what the next",
    "start": "321000",
    "end": "323000"
  },
  {
    "text": "wave of innovation looks",
    "start": "323000",
    "end": "325240"
  },
  {
    "text": "like when we decided to launch the Gemma",
    "start": "325240",
    "end": "328160"
  },
  {
    "text": "models we wanted to make sure that we",
    "start": "328160",
    "end": "330240"
  },
  {
    "text": "could meet developers exactly where they",
    "start": "330240",
    "end": "332680"
  },
  {
    "text": "are which is why gemo models are",
    "start": "332680",
    "end": "335280"
  },
  {
    "text": "available anywhere and everywhere you",
    "start": "335280",
    "end": "338039"
  },
  {
    "text": "can find an open model I will not list",
    "start": "338039",
    "end": "341560"
  },
  {
    "text": "all of the Frameworks on this slide but",
    "start": "341560",
    "end": "343680"
  },
  {
    "text": "this is only a fraction of the places",
    "start": "343680",
    "end": "345840"
  },
  {
    "text": "where you can find Gemma models today",
    "start": "345840",
    "end": "348400"
  },
  {
    "text": "this means you can use Gemma how you",
    "start": "348400",
    "end": "350479"
  },
  {
    "text": "need it when you need it with the tools",
    "start": "350479",
    "end": "353160"
  },
  {
    "text": "that you prefer for",
    "start": "353160",
    "end": "356000"
  },
  {
    "text": "development since our initial launch",
    "start": "356000",
    "end": "358720"
  },
  {
    "text": "back in February we've added a couple of",
    "start": "358720",
    "end": "361199"
  },
  {
    "text": "different variants to the Gemma model",
    "start": "361199",
    "end": "362919"
  },
  {
    "text": "family we of course have our initial",
    "start": "362919",
    "end": "365400"
  },
  {
    "text": "models Gemma 1.0 which are our",
    "start": "365400",
    "end": "367919"
  },
  {
    "text": "foundational llms we also released",
    "start": "367919",
    "end": "371039"
  },
  {
    "text": "shortly after that code Gemma which are",
    "start": "371039",
    "end": "373520"
  },
  {
    "text": "the Gemma 1.0 models fine tuned for",
    "start": "373520",
    "end": "376199"
  },
  {
    "text": "improved performance on code generation",
    "start": "376199",
    "end": "378800"
  },
  {
    "text": "and code",
    "start": "378800",
    "end": "379720"
  },
  {
    "text": "evaluation and one variant that I am",
    "start": "379720",
    "end": "382280"
  },
  {
    "text": "particularly excited about is recurrent",
    "start": "382280",
    "end": "384440"
  },
  {
    "text": "Gemma which is a novel architecture a",
    "start": "384440",
    "end": "387240"
  },
  {
    "text": "state space model that's designed for",
    "start": "387240",
    "end": "390039"
  },
  {
    "text": "faster and more efficient inference",
    "start": "390039",
    "end": "392759"
  },
  {
    "text": "especially at long",
    "start": "392759",
    "end": "394759"
  },
  {
    "text": "contexts we've also updated all of these",
    "start": "394759",
    "end": "398280"
  },
  {
    "text": "models since their initial release we",
    "start": "398280",
    "end": "400639"
  },
  {
    "text": "now have Gemma 1.1 which is better at",
    "start": "400639",
    "end": "403759"
  },
  {
    "text": "instruction following and chat we've",
    "start": "403759",
    "end": "405800"
  },
  {
    "text": "updated code Gemma to have even more",
    "start": "405800",
    "end": "407880"
  },
  {
    "text": "improved code performance and we now",
    "start": "407880",
    "end": "410120"
  },
  {
    "text": "have recurrent Gemma not not only the",
    "start": "410120",
    "end": "412160"
  },
  {
    "text": "original 2B size but also at a 9 billion",
    "start": "412160",
    "end": "415400"
  },
  {
    "text": "parameter",
    "start": "415400",
    "end": "417680"
  },
  {
    "text": "size so there's a lot going on in the",
    "start": "417680",
    "end": "421120"
  },
  {
    "text": "Gemma model family and I'm especially",
    "start": "421120",
    "end": "423919"
  },
  {
    "text": "excited to tell you about our two most",
    "start": "423919",
    "end": "426560"
  },
  {
    "text": "recent",
    "start": "426560",
    "end": "427919"
  },
  {
    "text": "launches um the first one is actually",
    "start": "427919",
    "end": "430919"
  },
  {
    "text": "our most highly requested feature since",
    "start": "430919",
    "end": "433840"
  },
  {
    "text": "day Zero of launch and that was",
    "start": "433840",
    "end": "438720"
  },
  {
    "text": "multimodality so we launched P Gemma P",
    "start": "438720",
    "end": "442440"
  },
  {
    "text": "Gemma oh thank you I appreciate",
    "start": "442440",
    "end": "445840"
  },
  {
    "text": "it this is why I love the open source",
    "start": "445840",
    "end": "448520"
  },
  {
    "text": "Community truly the most passionate",
    "start": "448520",
    "end": "450199"
  },
  {
    "text": "developers that there are P Gemma is a",
    "start": "450199",
    "end": "453680"
  },
  {
    "text": "combination of the Sig lip Vision",
    "start": "453680",
    "end": "455879"
  },
  {
    "text": "encoder combined with the Gemma 1.0 text",
    "start": "455879",
    "end": "459759"
  },
  {
    "text": "decoder this combination allows us to do",
    "start": "459759",
    "end": "462599"
  },
  {
    "text": "a variety of image text sort of tasks",
    "start": "462599",
    "end": "466240"
  },
  {
    "text": "and capabilities including question",
    "start": "466240",
    "end": "468960"
  },
  {
    "text": "answering image and video captioning",
    "start": "468960",
    "end": "471560"
  },
  {
    "text": "object detection and object",
    "start": "471560",
    "end": "474159"
  },
  {
    "text": "segmentation the model comes in a couple",
    "start": "474159",
    "end": "476479"
  },
  {
    "text": "of different variants it's currently",
    "start": "476479",
    "end": "478080"
  },
  {
    "text": "only available at the two V size but we",
    "start": "478080",
    "end": "480879"
  },
  {
    "text": "have pre-trained weights that are",
    "start": "480879",
    "end": "482400"
  },
  {
    "text": "available that can be fine-tuned for",
    "start": "482400",
    "end": "484599"
  },
  {
    "text": "specific tasks we have a couple of",
    "start": "484599",
    "end": "486720"
  },
  {
    "text": "different fine-tuned variants as well",
    "start": "486720",
    "end": "488759"
  },
  {
    "text": "that are already targeted towards things",
    "start": "488759",
    "end": "490560"
  },
  {
    "text": "like object detection and object",
    "start": "490560",
    "end": "492599"
  },
  {
    "text": "segmentation and we also have transfer",
    "start": "492599",
    "end": "495479"
  },
  {
    "text": "checkpoints that are models that are",
    "start": "495479",
    "end": "497400"
  },
  {
    "text": "specialized to Target a couple of",
    "start": "497400",
    "end": "499840"
  },
  {
    "text": "academic",
    "start": "499840",
    "end": "502400"
  },
  {
    "text": "benchmarks up until this morning that",
    "start": "502720",
    "end": "505560"
  },
  {
    "text": "was our latest release but I'm very",
    "start": "505560",
    "end": "508840"
  },
  {
    "text": "excited to be here today with you guys",
    "start": "508840",
    "end": "511159"
  },
  {
    "text": "because it is Gemma V2 launch day",
    "start": "511159",
    "end": "516320"
  },
  {
    "text": "woohoo wow",
    "start": "516680",
    "end": "518919"
  },
  {
    "text": "thanks we have been working very hard on",
    "start": "518919",
    "end": "522200"
  },
  {
    "text": "these models since Gemma 1.0 launch date",
    "start": "522200",
    "end": "526279"
  },
  {
    "text": "we tried to do as much as we could to",
    "start": "526279",
    "end": "528279"
  },
  {
    "text": "gather feedback from the community to",
    "start": "528279",
    "end": "530560"
  },
  {
    "text": "learn where the 1.0 and 1.1 models fell",
    "start": "530560",
    "end": "534080"
  },
  {
    "text": "short and what we could do to make them",
    "start": "534080",
    "end": "536000"
  },
  {
    "text": "better and so we created Gemma 2 Gemma 2",
    "start": "536000",
    "end": "540440"
  },
  {
    "text": "comes in both a 9 billion parameter size",
    "start": "540440",
    "end": "543279"
  },
  {
    "text": "and a 27 billion parameter size both",
    "start": "543279",
    "end": "546800"
  },
  {
    "text": "models are without a doubt the most",
    "start": "546800",
    "end": "550040"
  },
  {
    "text": "performant of their size and both models",
    "start": "550040",
    "end": "553680"
  },
  {
    "text": "also",
    "start": "553680",
    "end": "554680"
  },
  {
    "text": "outperform models that are even two to",
    "start": "554680",
    "end": "557120"
  },
  {
    "text": "three times larger than these base",
    "start": "557120",
    "end": "560040"
  },
  {
    "text": "models but Gemma 2 isn't just powerful",
    "start": "560040",
    "end": "563920"
  },
  {
    "text": "it's designed to easily integrate into",
    "start": "563920",
    "end": "565920"
  },
  {
    "text": "the workflows that you already have",
    "start": "565920",
    "end": "567959"
  },
  {
    "text": "existing so Gemma 2 uses all of the same",
    "start": "567959",
    "end": "571440"
  },
  {
    "text": "tools all of the same Frameworks as",
    "start": "571440",
    "end": "573720"
  },
  {
    "text": "Gemma 1 which means if you've already",
    "start": "573720",
    "end": "575839"
  },
  {
    "text": "started developing with Gemma one you",
    "start": "575839",
    "end": "578279"
  },
  {
    "text": "can with only a couple of lines of code",
    "start": "578279",
    "end": "581079"
  },
  {
    "text": "automatically switch to using the Gemma",
    "start": "581079",
    "end": "583200"
  },
  {
    "text": "2 models and have increased performance",
    "start": "583200",
    "end": "586120"
  },
  {
    "text": "and um more power behind your",
    "start": "586120",
    "end": "589160"
  },
  {
    "text": "applications we also have the same broad",
    "start": "589160",
    "end": "592640"
  },
  {
    "text": "framework compatibility again tensorflow",
    "start": "592640",
    "end": "595480"
  },
  {
    "text": "Jacks Transformers AMA all of the ones I",
    "start": "595480",
    "end": "598680"
  },
  {
    "text": "previously named we have them for Gemma",
    "start": "598680",
    "end": "600519"
  },
  {
    "text": "2 as well we also have significantly",
    "start": "600519",
    "end": "604200"
  },
  {
    "text": "improved documentation we have more",
    "start": "604200",
    "end": "606480"
  },
  {
    "text": "guides more tutorials so that we can",
    "start": "606480",
    "end": "609079"
  },
  {
    "text": "coach you through how to get started not",
    "start": "609079",
    "end": "610800"
  },
  {
    "text": "only with inference but with Advanced",
    "start": "610800",
    "end": "613160"
  },
  {
    "text": "and efficient fine-tuning from day Zero",
    "start": "613160",
    "end": "617040"
  },
  {
    "text": "and finally we really wanted to Target",
    "start": "617040",
    "end": "619760"
  },
  {
    "text": "fine-tuning as one of the key",
    "start": "619760",
    "end": "621920"
  },
  {
    "text": "capabilities of these models we did",
    "start": "621920",
    "end": "624560"
  },
  {
    "text": "extensive research into how our core",
    "start": "624560",
    "end": "628360"
  },
  {
    "text": "modeling decision",
    "start": "628360",
    "end": "630120"
  },
  {
    "text": "impact users ability to do Downstream",
    "start": "630120",
    "end": "632519"
  },
  {
    "text": "fine-tuning so we believe these models",
    "start": "632519",
    "end": "634720"
  },
  {
    "text": "are going to be incredibly easy to find",
    "start": "634720",
    "end": "637360"
  },
  {
    "text": "tune so you can customize them to",
    "start": "637360",
    "end": "639279"
  },
  {
    "text": "whatever your use case may",
    "start": "639279",
    "end": "642240"
  },
  {
    "text": "be in addition to make it especially",
    "start": "642240",
    "end": "644920"
  },
  {
    "text": "easy to get started using Gemma 2 models",
    "start": "644920",
    "end": "648000"
  },
  {
    "text": "we have made the 27b model available in",
    "start": "648000",
    "end": "651160"
  },
  {
    "text": "Google AI Studios this means you can go",
    "start": "651160",
    "end": "653959"
  },
  {
    "text": "to the AI Studio homepage and select",
    "start": "653959",
    "end": "656120"
  },
  {
    "text": "Gemma 2 now if you wanted to and start",
    "start": "656120",
    "end": "660079"
  },
  {
    "text": "playing around with prompts right away",
    "start": "660079",
    "end": "662959"
  },
  {
    "text": "you shouldn't have to do anything except",
    "start": "662959",
    "end": "665240"
  },
  {
    "text": "come up with an idea for how you want to",
    "start": "665240",
    "end": "666880"
  },
  {
    "text": "push the limits of our model I am I am",
    "start": "666880",
    "end": "670240"
  },
  {
    "text": "especially excited to see what you all",
    "start": "670240",
    "end": "672560"
  },
  {
    "text": "end up doing with AI Studios and Gemma",
    "start": "672560",
    "end": "676120"
  },
  {
    "text": "um and we have a couple of different",
    "start": "676120",
    "end": "678040"
  },
  {
    "text": "ways for you to let us know what you're",
    "start": "678040",
    "end": "679800"
  },
  {
    "text": "building which I'll get to down the road",
    "start": "679800",
    "end": "682120"
  },
  {
    "text": "um but if you have ideas I'll be here",
    "start": "682120",
    "end": "684800"
  },
  {
    "text": "all day and want to hear what you're",
    "start": "684800",
    "end": "686079"
  },
  {
    "text": "doing with the Gemma",
    "start": "686079",
    "end": "688240"
  },
  {
    "text": "models but let's dive a little bit more",
    "start": "688240",
    "end": "690839"
  },
  {
    "text": "into performance we are incredibly proud",
    "start": "690839",
    "end": "695000"
  },
  {
    "text": "of the models that we've made as I",
    "start": "695000",
    "end": "697040"
  },
  {
    "text": "mentioned they are without a doubt the",
    "start": "697040",
    "end": "699639"
  },
  {
    "text": "best most performant models of their",
    "start": "699639",
    "end": "701959"
  },
  {
    "text": "size and are also competitive with",
    "start": "701959",
    "end": "704200"
  },
  {
    "text": "models two to three times larger so our",
    "start": "704200",
    "end": "707120"
  },
  {
    "text": "27b model is has performance in the same",
    "start": "707120",
    "end": "711120"
  },
  {
    "text": "ballpark as llama",
    "start": "711120",
    "end": "713160"
  },
  {
    "text": "370b and outperforms grock models on",
    "start": "713160",
    "end": "717200"
  },
  {
    "text": "many benchmarks by a fairly sign",
    "start": "717200",
    "end": "719240"
  },
  {
    "text": "significant margin in some cases um but",
    "start": "719240",
    "end": "723160"
  },
  {
    "text": "I think academic benchmarks are only",
    "start": "723160",
    "end": "726079"
  },
  {
    "text": "part of the way that we evaluate Gemma",
    "start": "726079",
    "end": "728120"
  },
  {
    "text": "models sometimes these benchmarks are",
    "start": "728120",
    "end": "730120"
  },
  {
    "text": "not always indicative of how a model",
    "start": "730120",
    "end": "733240"
  },
  {
    "text": "will perform once it's in your hands so",
    "start": "733240",
    "end": "735880"
  },
  {
    "text": "we've done extensive human evaluations",
    "start": "735880",
    "end": "738480"
  },
  {
    "text": "as well where we find that the Gemma",
    "start": "738480",
    "end": "740519"
  },
  {
    "text": "models are consistently heavily",
    "start": "740519",
    "end": "744199"
  },
  {
    "text": "preferred to other open models including",
    "start": "744199",
    "end": "747079"
  },
  {
    "text": "larger open models um and I'm also proud",
    "start": "747079",
    "end": "750680"
  },
  {
    "text": "to say that the Gemma 27b model is",
    "start": "750680",
    "end": "754360"
  },
  {
    "text": "currently the number one open model of",
    "start": "754360",
    "end": "756920"
  },
  {
    "text": "its size and it currently outranks llama",
    "start": "756920",
    "end": "760760"
  },
  {
    "text": "370b neotron 340b Gro claw three many",
    "start": "760760",
    "end": "766959"
  },
  {
    "text": "many other models as well um thank you",
    "start": "766959",
    "end": "772079"
  },
  {
    "text": "wow you guys are very supportive I",
    "start": "772079",
    "end": "774040"
  },
  {
    "text": "appreciate",
    "start": "774040",
    "end": "775279"
  },
  {
    "text": "it um the only other open model of any e",
    "start": "775279",
    "end": "779440"
  },
  {
    "text": "size that outperforms the Gemma 27b",
    "start": "779440",
    "end": "782320"
  },
  {
    "text": "model is the E large model on on LMS um",
    "start": "782320",
    "end": "787160"
  },
  {
    "text": "so we expect that you should have some",
    "start": "787160",
    "end": "789240"
  },
  {
    "text": "fun playing around with it especially",
    "start": "789240",
    "end": "791120"
  },
  {
    "text": "for chat applications we found in our",
    "start": "791120",
    "end": "793680"
  },
  {
    "text": "evaluations that the Gemma 2 models are",
    "start": "793680",
    "end": "796800"
  },
  {
    "text": "even better at instruction following",
    "start": "796800",
    "end": "798600"
  },
  {
    "text": "they're even more creative they're",
    "start": "798600",
    "end": "800560"
  },
  {
    "text": "better at factuality better all around",
    "start": "800560",
    "end": "803399"
  },
  {
    "text": "than the geml 1.0 and 1.1",
    "start": "803399",
    "end": "806920"
  },
  {
    "text": "models the other important thing that I",
    "start": "806920",
    "end": "809480"
  },
  {
    "text": "want to make sure to highlight from our",
    "start": "809480",
    "end": "812079"
  },
  {
    "text": "most recent launch is the Gemma cookbook",
    "start": "812079",
    "end": "815079"
  },
  {
    "text": "current the Gemma cookbook is available",
    "start": "815079",
    "end": "816800"
  },
  {
    "text": "on GitHub now and contains 20 different",
    "start": "816800",
    "end": "819320"
  },
  {
    "text": "recipes of ranging from easy to very",
    "start": "819320",
    "end": "822720"
  },
  {
    "text": "Advanced applications of how to use the",
    "start": "822720",
    "end": "824839"
  },
  {
    "text": "Gemma models and the thing that I am",
    "start": "824839",
    "end": "827279"
  },
  {
    "text": "most excited about is the Gemma cookbook",
    "start": "827279",
    "end": "829480"
  },
  {
    "text": "is currently accepting poll requests so",
    "start": "829480",
    "end": "832199"
  },
  {
    "text": "this is a great opportunity to share",
    "start": "832199",
    "end": "835160"
  },
  {
    "text": "with us what you're building with the",
    "start": "835160",
    "end": "837320"
  },
  {
    "text": "Gemma models and so we can help share it",
    "start": "837320",
    "end": "840440"
  },
  {
    "text": "with the rest of the",
    "start": "840440",
    "end": "841920"
  },
  {
    "text": "world and of course I have to say we",
    "start": "841920",
    "end": "845120"
  },
  {
    "text": "also wouldn't mind if you start the",
    "start": "845120",
    "end": "846639"
  },
  {
    "text": "repository come go take a look and tell",
    "start": "846639",
    "end": "848880"
  },
  {
    "text": "us what you're building with",
    "start": "848880",
    "end": "850800"
  },
  {
    "text": "Gemma so there are a couple of different",
    "start": "850800",
    "end": "853160"
  },
  {
    "text": "ways you can get started with the Gemma",
    "start": "853160",
    "end": "855000"
  },
  {
    "text": "to models of course I just mentioned the",
    "start": "855000",
    "end": "857759"
  },
  {
    "text": "cookbook you can also apply to get uh",
    "start": "857759",
    "end": "860959"
  },
  {
    "text": "gcp credits to accelerate your research",
    "start": "860959",
    "end": "864199"
  },
  {
    "text": "using Gemma 2 we have a lot of funding",
    "start": "864199",
    "end": "867920"
  },
  {
    "text": "available to support research I would",
    "start": "867920",
    "end": "870279"
  },
  {
    "text": "really encourage you to fill out an",
    "start": "870279",
    "end": "872839"
  },
  {
    "text": "application regardless of how small or",
    "start": "872839",
    "end": "875320"
  },
  {
    "text": "big your project is we also as I",
    "start": "875320",
    "end": "878680"
  },
  {
    "text": "mentioned have significantly improved",
    "start": "878680",
    "end": "880839"
  },
  {
    "text": "documentation we have many guides",
    "start": "880839",
    "end": "883079"
  },
  {
    "text": "tutorials collabs across every framework",
    "start": "883079",
    "end": "886360"
  },
  {
    "text": "so you can get started doing inference",
    "start": "886360",
    "end": "887959"
  },
  {
    "text": "fine-tuning and evaluation with Gemma 2",
    "start": "887959",
    "end": "890120"
  },
  {
    "text": "models you can download them anywhere",
    "start": "890120",
    "end": "892720"
  },
  {
    "text": "open models are available and please",
    "start": "892720",
    "end": "895880"
  },
  {
    "text": "chat with us on Discord or other social",
    "start": "895880",
    "end": "898120"
  },
  {
    "text": "media channels so we can learn more",
    "start": "898120",
    "end": "900079"
  },
  {
    "text": "about what you're",
    "start": "900079",
    "end": "902440"
  },
  {
    "text": "building and that's about all from me",
    "start": "902440",
    "end": "905519"
  },
  {
    "text": "today I am so excited to see what you",
    "start": "905519",
    "end": "909399"
  },
  {
    "text": "all build with Gemma I have been working",
    "start": "909399",
    "end": "912519"
  },
  {
    "text": "on this project for almost two years now",
    "start": "912519",
    "end": "917079"
  },
  {
    "text": "and started working on this project",
    "start": "917079",
    "end": "919360"
  },
  {
    "text": "because I as a researcher in Academia",
    "start": "919360",
    "end": "922680"
  },
  {
    "text": "was disappointed to see how far behind",
    "start": "922680",
    "end": "926399"
  },
  {
    "text": "open foundational llms were compared to",
    "start": "926399",
    "end": "929600"
  },
  {
    "text": "the rapid improvements we were seeing in",
    "start": "929600",
    "end": "931800"
  },
  {
    "text": "proprietary models so this is something",
    "start": "931800",
    "end": "934839"
  },
  {
    "text": "that's very near and dear to my heart",
    "start": "934839",
    "end": "936759"
  },
  {
    "text": "and that I wish I had had when I was",
    "start": "936759",
    "end": "940040"
  },
  {
    "text": "actively part of the open source",
    "start": "940040",
    "end": "941480"
  },
  {
    "text": "community so I'm very excited to see the",
    "start": "941480",
    "end": "944440"
  },
  {
    "text": "projects and the research that you all",
    "start": "944440",
    "end": "946279"
  },
  {
    "text": "do with these models please engage with",
    "start": "946279",
    "end": "948720"
  },
  {
    "text": "us on social media on GitHub on hugging",
    "start": "948720",
    "end": "951600"
  },
  {
    "text": "face here at the event and let us know",
    "start": "951600",
    "end": "955160"
  },
  {
    "text": "what you think of the models let us know",
    "start": "955160",
    "end": "957079"
  },
  {
    "text": "what you think we can do better for next",
    "start": "957079",
    "end": "959920"
  },
  {
    "text": "time and thank you all very much really",
    "start": "959920",
    "end": "963319"
  },
  {
    "text": "appreciate your time",
    "start": "963319",
    "end": "967040"
  },
  {
    "text": "[Applause]",
    "start": "967600",
    "end": "969390"
  },
  {
    "text": "[Music]",
    "start": "969390",
    "end": "986330"
  }
]