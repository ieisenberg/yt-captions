[
  {
    "text": "[Music]",
    "start": "3520",
    "end": "7860"
  },
  {
    "text": "so the talk is actually going to be",
    "start": "13559",
    "end": "15080"
  },
  {
    "text": "about um uh how you run things extremely",
    "start": "15080",
    "end": "18880"
  },
  {
    "text": "easy directly from Python and the",
    "start": "18880",
    "end": "21320"
  },
  {
    "text": "example that I'm going to show you here",
    "start": "21320",
    "end": "22800"
  },
  {
    "text": "is obviously I just have five minutes",
    "start": "22800",
    "end": "24279"
  },
  {
    "text": "and on my end but I'm going to try my",
    "start": "24279",
    "end": "26240"
  },
  {
    "text": "best to Showcase how you can fine tune",
    "start": "26240",
    "end": "28199"
  },
  {
    "text": "pretty much 20 is an arbitrary number",
    "start": "28199",
    "end": "29800"
  },
  {
    "text": "here but hundreds of models that you can",
    "start": "29800",
    "end": "31759"
  },
  {
    "text": "do right from python without needing",
    "start": "31759",
    "end": "33960"
  },
  {
    "text": "anything like kubernetes joer or",
    "start": "33960",
    "end": "35520"
  },
  {
    "text": "anything on your side and uh so before",
    "start": "35520",
    "end": "37559"
  },
  {
    "text": "that you can find the talk and the",
    "start": "37559",
    "end": "39120"
  },
  {
    "text": "actual code for what I'm going to do in",
    "start": "39120",
    "end": "40559"
  },
  {
    "text": "this QR code and you'll find lot more",
    "start": "40559",
    "end": "43600"
  },
  {
    "text": "interesting examples over there to try",
    "start": "43600",
    "end": "45200"
  },
  {
    "text": "out on run as well um okay so what do we",
    "start": "45200",
    "end": "48879"
  },
  {
    "text": "do um so calent is an open source slop",
    "start": "48879",
    "end": "52680"
  },
  {
    "text": "core uh product on its end and what we",
    "start": "52680",
    "end": "55760"
  },
  {
    "text": "do is we help people write python",
    "start": "55760",
    "end": "58320"
  },
  {
    "text": "locally and ship the code to any kind of",
    "start": "58320",
    "end": "60719"
  },
  {
    "text": "compute backend that you need to send it",
    "start": "60719",
    "end": "62359"
  },
  {
    "text": "to so what that means is hey you have a",
    "start": "62359",
    "end": "65198"
  },
  {
    "text": "python function that you want to run on",
    "start": "65199",
    "end": "66560"
  },
  {
    "text": "a GPU um in your local laptop open up a",
    "start": "66560",
    "end": "69200"
  },
  {
    "text": "jupyter notebook add a single decorator",
    "start": "69200",
    "end": "71200"
  },
  {
    "text": "on top to say hey I want to run this on",
    "start": "71200",
    "end": "73320"
  },
  {
    "text": "h00 with 36 gigs of memory for 2 days",
    "start": "73320",
    "end": "76400"
  },
  {
    "text": "maximum time limit and press shift enter",
    "start": "76400",
    "end": "78880"
  },
  {
    "text": "in your Jupiter notebook and that's it",
    "start": "78880",
    "end": "81000"
  },
  {
    "text": "the code gets shipped to a back end in a",
    "start": "81000",
    "end": "83479"
  },
  {
    "text": "GPU and you get back the result on your",
    "start": "83479",
    "end": "85119"
  },
  {
    "text": "side in in the open source case it sends",
    "start": "85119",
    "end": "87680"
  },
  {
    "text": "it to your own compute you can attach",
    "start": "87680",
    "end": "89159"
  },
  {
    "text": "your own compute class cluster and it",
    "start": "89159",
    "end": "90360"
  },
  {
    "text": "runs over there in the cloud case it",
    "start": "90360",
    "end": "92759"
  },
  {
    "text": "runs in our GPU cluster and you just pay",
    "start": "92759",
    "end": "94799"
  },
  {
    "text": "for the GPU time that it runs in so it",
    "start": "94799",
    "end": "96320"
  },
  {
    "text": "runs for 5 minutes you pay for 5 minutes",
    "start": "96320",
    "end": "98240"
  },
  {
    "text": "of h00 it runs for 10 seconds you pay",
    "start": "98240",
    "end": "100399"
  },
  {
    "text": "for 10 seconds of hunds on your side and",
    "start": "100399",
    "end": "102680"
  },
  {
    "text": "you can also bring your own Compu and",
    "start": "102680",
    "end": "104119"
  },
  {
    "text": "attached to us and we'll help you",
    "start": "104119",
    "end": "105399"
  },
  {
    "text": "orchestrate the entire Compu that you're",
    "start": "105399",
    "end": "106960"
  },
  {
    "text": "handling on your side be it your own",
    "start": "106960",
    "end": "108479"
  },
  {
    "text": "cloud or on-prem systems or whatever it",
    "start": "108479",
    "end": "111000"
  },
  {
    "text": "is on your end",
    "start": "111000",
    "end": "112320"
  },
  {
    "text": "then okay so covalent basically has a",
    "start": "112320",
    "end": "115560"
  },
  {
    "text": "bunch of perimeters that you define in",
    "start": "115560",
    "end": "117399"
  },
  {
    "text": "you can submit in jobs which are called",
    "start": "117399",
    "end": "119479"
  },
  {
    "text": "single functions so essentially all you",
    "start": "119479",
    "end": "121640"
  },
  {
    "text": "need to do is as I said add a single",
    "start": "121640",
    "end": "123640"
  },
  {
    "text": "decorator on top and say what is the",
    "start": "123640",
    "end": "125399"
  },
  {
    "text": "computer that you need to ship it to it",
    "start": "125399",
    "end": "127280"
  },
  {
    "text": "goes there it runs and you get back the",
    "start": "127280",
    "end": "129360"
  },
  {
    "text": "python object back and you just pay for",
    "start": "129360",
    "end": "131200"
  },
  {
    "text": "the function that you are running",
    "start": "131200",
    "end": "132680"
  },
  {
    "text": "in we also let you run inferences and",
    "start": "132680",
    "end": "135800"
  },
  {
    "text": "again it's completely pythonic you don't",
    "start": "135800",
    "end": "137680"
  },
  {
    "text": "dockerize you don't run kubernetes",
    "start": "137680",
    "end": "139400"
  },
  {
    "text": "cluster you don't do anything you just",
    "start": "139400",
    "end": "141560"
  },
  {
    "text": "say hey I have an initializer function",
    "start": "141560",
    "end": "144360"
  },
  {
    "text": "and I have a I need an endpoint called",
    "start": "144360",
    "end": "145959"
  },
  {
    "text": "SL generate and you define your python",
    "start": "145959",
    "end": "148280"
  },
  {
    "text": "functions you click a single cc. deploy",
    "start": "148280",
    "end": "151200"
  },
  {
    "text": "command uh in your jupyter notebook the",
    "start": "151200",
    "end": "153440"
  },
  {
    "text": "entire service gets shipped to us and we",
    "start": "153440",
    "end": "155480"
  },
  {
    "text": "scale you get back an API endpoint that",
    "start": "155480",
    "end": "157519"
  },
  {
    "text": "scales to zero or scales in its request",
    "start": "157519",
    "end": "159720"
  },
  {
    "text": "as and when your new request comes in",
    "start": "159720",
    "end": "161720"
  },
  {
    "text": "you can Define your Custom Auto scaling",
    "start": "161720",
    "end": "163319"
  },
  {
    "text": "mechanism like hey I want to Auto scale",
    "start": "163319",
    "end": "165800"
  },
  {
    "text": "it to 10 gpus exactly at 9:00 every day",
    "start": "165800",
    "end": "168680"
  },
  {
    "text": "or I want to Auto scale whenever my GPU",
    "start": "168680",
    "end": "170319"
  },
  {
    "text": "utilization hits in 80% or I want to",
    "start": "170319",
    "end": "172680"
  },
  {
    "text": "Auto scale whenever the number of",
    "start": "172680",
    "end": "173959"
  },
  {
    "text": "request I get in is thousand U so you",
    "start": "173959",
    "end": "176400"
  },
  {
    "text": "can Define whatever Auto scaling you",
    "start": "176400",
    "end": "177640"
  },
  {
    "text": "want you can Define authentication and",
    "start": "177640",
    "end": "179360"
  },
  {
    "text": "everything and everything happens in the",
    "start": "179360",
    "end": "180840"
  },
  {
    "text": "background for you you don't even touch",
    "start": "180840",
    "end": "182040"
  },
  {
    "text": "a single code of kubernetes or Docker or",
    "start": "182040",
    "end": "184400"
  },
  {
    "text": "anything on your",
    "start": "184400",
    "end": "186080"
  },
  {
    "text": "side and the talk I'm going to give in",
    "start": "186080",
    "end": "189040"
  },
  {
    "text": "is a very tiny example um of what we do",
    "start": "189040",
    "end": "192680"
  },
  {
    "text": "from our side but if you go to this",
    "start": "192680",
    "end": "194599"
  },
  {
    "text": "Linkin there's a whole host of examples",
    "start": "194599",
    "end": "197360"
  },
  {
    "text": "uh that you can run in right from Real",
    "start": "197360",
    "end": "199760"
  },
  {
    "text": "Time Time series analysis to uh you know",
    "start": "199760",
    "end": "203000"
  },
  {
    "text": "using inverter Transformers for time",
    "start": "203000",
    "end": "204599"
  },
  {
    "text": "series which is like a state-ofthe-art u",
    "start": "204599",
    "end": "206959"
  },
  {
    "text": "time series Transformer on its end uh",
    "start": "206959",
    "end": "209080"
  },
  {
    "text": "running inar systems um large language",
    "start": "209080",
    "end": "212120"
  },
  {
    "text": "models on your serving uh systems and",
    "start": "212120",
    "end": "214560"
  },
  {
    "text": "even building an entire AI model Foundry",
    "start": "214560",
    "end": "216360"
  },
  {
    "text": "out of our just pure pythonic code on",
    "start": "216360",
    "end": "218720"
  },
  {
    "text": "your side and so without further Ado",
    "start": "218720",
    "end": "221959"
  },
  {
    "text": "I'll quickly run through the code",
    "start": "221959",
    "end": "224040"
  },
  {
    "text": "example of how you do essentially",
    "start": "224040",
    "end": "226319"
  },
  {
    "text": "fine-tune a bunch of huge set of models",
    "start": "226319",
    "end": "229080"
  },
  {
    "text": "uh directly just from python on your end",
    "start": "229080",
    "end": "231400"
  },
  {
    "text": "and I'll also show you how it looks like",
    "start": "231400",
    "end": "233239"
  },
  {
    "text": "uh from the front side as well so um",
    "start": "233239",
    "end": "236799"
  },
  {
    "text": "it's rather simple all you do is I have",
    "start": "236799",
    "end": "239239"
  },
  {
    "text": "written a un of U normal pythonic",
    "start": "239239",
    "end": "242000"
  },
  {
    "text": "training functions in my local package",
    "start": "242000",
    "end": "243760"
  },
  {
    "text": "called fine tune and evaluate on our end",
    "start": "243760",
    "end": "246120"
  },
  {
    "text": "and what we going to do is hey I'm going",
    "start": "246120",
    "end": "248360"
  },
  {
    "text": "to define a python task which",
    "start": "248360",
    "end": "250159"
  },
  {
    "text": "essentially calls in my fine tune",
    "start": "250159",
    "end": "251640"
  },
  {
    "text": "function which is going to accept a",
    "start": "251640",
    "end": "252799"
  },
  {
    "text": "model and data and return back a fine",
    "start": "252799",
    "end": "254640"
  },
  {
    "text": "tune model so this is a simple python",
    "start": "254640",
    "end": "256560"
  },
  {
    "text": "function and I'm going to say I want to",
    "start": "256560",
    "end": "258160"
  },
  {
    "text": "run it on a 24 core CPU with one GPU in",
    "start": "258160",
    "end": "261359"
  },
  {
    "text": "it of type h 100 with 48 gigs of memory",
    "start": "261359",
    "end": "264120"
  },
  {
    "text": "and going to give a Max limit of 18",
    "start": "264120",
    "end": "266000"
  },
  {
    "text": "hours on it and then I'm going to say",
    "start": "266000",
    "end": "268400"
  },
  {
    "text": "I'm going to once the model is done I'm",
    "start": "268400",
    "end": "269680"
  },
  {
    "text": "I'm going to accept the model and then",
    "start": "269680",
    "end": "270840"
  },
  {
    "text": "evaluate its accuracy on its hand and",
    "start": "270840",
    "end": "273919"
  },
  {
    "text": "finally I'm going to just sort the model",
    "start": "273919",
    "end": "276080"
  },
  {
    "text": "among all the best models and then pick",
    "start": "276080",
    "end": "278120"
  },
  {
    "text": "the best model in it and I want this to",
    "start": "278120",
    "end": "279759"
  },
  {
    "text": "run on a CPU based machion I don't want",
    "start": "279759",
    "end": "281199"
  },
  {
    "text": "to waste GPU for my sorting or whatever",
    "start": "281199",
    "end": "283400"
  },
  {
    "text": "I'm going to do in on my end and finally",
    "start": "283400",
    "end": "286280"
  },
  {
    "text": "I'm going to deploy the best model that",
    "start": "286280",
    "end": "288039"
  },
  {
    "text": "I figured um and it's that has performed",
    "start": "288039",
    "end": "290759"
  },
  {
    "text": "well on my end and this is again a",
    "start": "290759",
    "end": "292560"
  },
  {
    "text": "simple decorative say hey this is my",
    "start": "292560",
    "end": "294400"
  },
  {
    "text": "initialization service and I'm going to",
    "start": "294400",
    "end": "296479"
  },
  {
    "text": "create a endpoint called SL generate um",
    "start": "296479",
    "end": "299479"
  },
  {
    "text": "and and I'm going to generate the text",
    "start": "299479",
    "end": "301039"
  },
  {
    "text": "and give back the",
    "start": "301039",
    "end": "303039"
  },
  {
    "text": "prediction and finally this is where the",
    "start": "303039",
    "end": "305160"
  },
  {
    "text": "magic happens to tie together all of",
    "start": "305160",
    "end": "307240"
  },
  {
    "text": "these things what I do is I'm going to",
    "start": "307240",
    "end": "308880"
  },
  {
    "text": "create a workflow where I'm pretty much",
    "start": "308880",
    "end": "310680"
  },
  {
    "text": "just going to Simply Loop over a bunch",
    "start": "310680",
    "end": "312520"
  },
  {
    "text": "of models to finetune call the fine tune",
    "start": "312520",
    "end": "315120"
  },
  {
    "text": "function evaluate the task and get the",
    "start": "315120",
    "end": "317360"
  },
  {
    "text": "accuracy make a list of all the models",
    "start": "317360",
    "end": "319440"
  },
  {
    "text": "and accuracy sort the best models and",
    "start": "319440",
    "end": "322080"
  },
  {
    "text": "then deploy the model from my end and",
    "start": "322080",
    "end": "324479"
  },
  {
    "text": "this is completely pythonic and once you",
    "start": "324479",
    "end": "326800"
  },
  {
    "text": "dispatch this to our server which is",
    "start": "326800",
    "end": "328880"
  },
  {
    "text": "essentially calling single line over",
    "start": "328880",
    "end": "330520"
  },
  {
    "text": "here what you will go back and see is a",
    "start": "330520",
    "end": "334080"
  },
  {
    "text": "new job that creates in our application",
    "start": "334080",
    "end": "337440"
  },
  {
    "text": "and all of the functions that you called",
    "start": "337440",
    "end": "339400"
  },
  {
    "text": "will run in the respective devices that",
    "start": "339400",
    "end": "341560"
  },
  {
    "text": "you just defined so for instance here is",
    "start": "341560",
    "end": "344560"
  },
  {
    "text": "uh one of the evaluation step that ran",
    "start": "344560",
    "end": "346800"
  },
  {
    "text": "in and it has its own machine that we",
    "start": "346800",
    "end": "348759"
  },
  {
    "text": "ran in it ran in L4 it ran for 6 minutes",
    "start": "348759",
    "end": "353319"
  },
  {
    "text": "and you get back just 87 cents to",
    "start": "353319",
    "end": "355639"
  },
  {
    "text": "evaluate your model in another model ran",
    "start": "355639",
    "end": "357880"
  },
  {
    "text": "in v00 on its end and it ran for 6",
    "start": "357880",
    "end": "361400"
  },
  {
    "text": "minutes again it cost 11 cents to do it",
    "start": "361400",
    "end": "363800"
  },
  {
    "text": "in and in total you finally have",
    "start": "363800",
    "end": "366199"
  },
  {
    "text": "deployed try fine tuned untrained",
    "start": "366199",
    "end": "368440"
  },
  {
    "text": "completely in Python without needing",
    "start": "368440",
    "end": "370400"
  },
  {
    "text": "anything like Docker or kubernetes on",
    "start": "370400",
    "end": "372120"
  },
  {
    "text": "your end and we have a booth over there",
    "start": "372120",
    "end": "374319"
  },
  {
    "text": "do visit us and we can have more chat",
    "start": "374319",
    "end": "376199"
  },
  {
    "text": "over there thank you guys",
    "start": "376199",
    "end": "379310"
  },
  {
    "text": "[Music]",
    "start": "379310",
    "end": "386760"
  }
]