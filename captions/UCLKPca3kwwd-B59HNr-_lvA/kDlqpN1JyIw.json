[
  {
    "text": "hey folks my name is Nico i work on the AI SDK at Verscell and in this session",
    "start": "80",
    "end": "6080"
  },
  {
    "text": "today we're going to be looking at building agents with the AI SDK now this",
    "start": "6080",
    "end": "11759"
  },
  {
    "text": "session is roughly divided into two sections we're going to start with a fundamental section this is going to",
    "start": "11759",
    "end": "17039"
  },
  {
    "text": "introduce you to the building blocks that you need to understand about using the AI SDK before we jump into building",
    "start": "17039",
    "end": "23760"
  },
  {
    "text": "agents and then we're going to be building a deep research clone uh in",
    "start": "23760",
    "end": "28880"
  },
  {
    "text": "Node.js js so without further ado let's get right into it to follow along the",
    "start": "28880",
    "end": "35120"
  },
  {
    "text": "first thing that you're going to have to do is clone the repository here install the dependencies copy over uh any",
    "start": "35120",
    "end": "41680"
  },
  {
    "text": "environment variables um and then you'll be ready to go in this project we have",
    "start": "41680",
    "end": "47280"
  },
  {
    "text": "just one file index.ts and you can run this file by just running pmpp",
    "start": "47280",
    "end": "53640"
  },
  {
    "text": "rundev i have this alias to just pd so if you see me typing that that's just",
    "start": "53640",
    "end": "59039"
  },
  {
    "text": "running the script great so let's start with the first primitive that we're",
    "start": "59039",
    "end": "64559"
  },
  {
    "text": "going to be looking at the generate text function now this is as it sounds a way",
    "start": "64559",
    "end": "70159"
  },
  {
    "text": "for you to call a larger language model and generate some text so let's take a look in this session rather than typing",
    "start": "70159",
    "end": "77200"
  },
  {
    "text": "everything out line by line I'm going to be copying over snippets so we can get through things a little bit faster and",
    "start": "77200",
    "end": "82799"
  },
  {
    "text": "focus on the core concepts rather than necessarily remembering to type everything out properly so let's start",
    "start": "82799",
    "end": "88320"
  },
  {
    "text": "with the first snippet what we've got here is a single",
    "start": "88320",
    "end": "95520"
  },
  {
    "text": "function called main it's asynchronous and inside this function we call generate text which we import from the",
    "start": "95520",
    "end": "101600"
  },
  {
    "text": "AI SDK we specify the model we want to use in this case OpenAI's GPT40 mini um",
    "start": "101600",
    "end": "108240"
  },
  {
    "text": "and then we pass in a prompt hello world finally we log out the resulting text",
    "start": "108240",
    "end": "114399"
  },
  {
    "text": "that is generated and call the function so we can head into the terminal run",
    "start": "114399",
    "end": "120320"
  },
  {
    "text": "pmppm rundev and we should see a message back from GPT4 mini hello how can I",
    "start": "120320",
    "end": "125759"
  },
  {
    "text": "assist you today now each of these generate text functions and uh stream text and",
    "start": "125759",
    "end": "132160"
  },
  {
    "text": "generate object and stream object as we'll see later uh can take in either a prompt as input or messages and in this",
    "start": "132160",
    "end": "139680"
  },
  {
    "text": "case messages is just an array of messages where a message has a role and",
    "start": "139680",
    "end": "144800"
  },
  {
    "text": "then some content so this would be the same as we had before if we change this",
    "start": "144800",
    "end": "151120"
  },
  {
    "text": "to user for the rest of this session we'll be using mostly the prompt key so one of",
    "start": "151120",
    "end": "157599"
  },
  {
    "text": "the core features of the AI SDK is its unified interface and what that means is",
    "start": "157599",
    "end": "163760"
  },
  {
    "text": "that we're able to switch between language models by just changing one single line of code now there are many",
    "start": "163760",
    "end": "170080"
  },
  {
    "text": "reasons why you might want to do this it might be because a model is cheaper faster um better at your specific use",
    "start": "170080",
    "end": "178560"
  },
  {
    "text": "case um and speaking of better one thing that we can try asking this model in",
    "start": "178560",
    "end": "185120"
  },
  {
    "text": "particular is uh something that we know it might struggle with like when was the",
    "start": "185120",
    "end": "192400"
  },
  {
    "text": "AI engineer summit in 2025 now I know for a fact that GPT40",
    "start": "192400",
    "end": "200640"
  },
  {
    "text": "Mini is not going to be able to do this because it doesn't have access to the web and its training data cutoff is",
    "start": "200640",
    "end": "207280"
  },
  {
    "text": "somewhere in 2024 so we can see I'm sorry but I don't have information about",
    "start": "207280",
    "end": "212959"
  },
  {
    "text": "events scheduled for 2025 including the AI engineer summit so how could we solve",
    "start": "212959",
    "end": "218000"
  },
  {
    "text": "this well we could and we'll look into later add a tool and that tool could",
    "start": "218000",
    "end": "223840"
  },
  {
    "text": "call the web and return those results and pipe those into the context of the conversation and then the language model",
    "start": "223840",
    "end": "229920"
  },
  {
    "text": "can deduce from there but we could also just pick a model that has web search",
    "start": "229920",
    "end": "235760"
  },
  {
    "text": "built in something like perplexity so how do we change to a different model",
    "start": "235760",
    "end": "241040"
  },
  {
    "text": "well all we have to do is change the model that we specify here",
    "start": "241040",
    "end": "247280"
  },
  {
    "text": "to perplexity invoke that model provider instance and then select the model that",
    "start": "247280",
    "end": "253360"
  },
  {
    "text": "we want to use so in this case we've imported a new uh provider this time",
    "start": "253360",
    "end": "258639"
  },
  {
    "text": "from AISDK/perplexity uh and then we specify we want to use sonar pro so if we run",
    "start": "258639",
    "end": "265199"
  },
  {
    "text": "this again now the model will come back the AI",
    "start": "265199",
    "end": "270479"
  },
  {
    "text": "engineer summit in 2025 took place from February 19th to February 22nd 2025 in",
    "start": "270479",
    "end": "276479"
  },
  {
    "text": "New York City this is right one thing that's interesting here is that unlike the OpenAI response",
    "start": "276479",
    "end": "283040"
  },
  {
    "text": "because Perplexity is using sources it's actually named them or referenced them",
    "start": "283040",
    "end": "289120"
  },
  {
    "text": "in its response how do we access them well the AI SDK makes this accessible",
    "start": "289120",
    "end": "295440"
  },
  {
    "text": "with the sources property so we can save run",
    "start": "295440",
    "end": "301120"
  },
  {
    "text": "the script again and we'll see the sources in line",
    "start": "301120",
    "end": "307919"
  },
  {
    "text": "very very very cool and this isn't just limited to perplexity we support a ton of providers and you can check all of",
    "start": "307919",
    "end": "314479"
  },
  {
    "text": "them out at our documentation if you head to a uh SDK.forell.ai AI and head",
    "start": "314479",
    "end": "320080"
  },
  {
    "text": "to our providers page you can see we have a whole host of providers here um",
    "start": "320080",
    "end": "325520"
  },
  {
    "text": "many of which support web search as well so if for example you wanted to use",
    "start": "325520",
    "end": "332080"
  },
  {
    "text": "let's say Google's Gemini so we can import the Google provider here and use Gemini",
    "start": "332080",
    "end": "338639"
  },
  {
    "text": "flash uh 1.5 and then specify that we want to use search",
    "start": "338639",
    "end": "343880"
  },
  {
    "text": "grounding save run the script again and we'll see this same prompt with the same",
    "start": "343880",
    "end": "350360"
  },
  {
    "text": "configuration going off to a different model at a different provider and getting what we hope is a similar",
    "start": "350360",
    "end": "357280"
  },
  {
    "text": "accurate answer ai engineer summit took place from February 19th to 2025 and a",
    "start": "357280",
    "end": "362560"
  },
  {
    "text": "bunch of sources in line to confirm this really really cool and really powerful",
    "start": "362560",
    "end": "367600"
  },
  {
    "text": "stuff so that was the first primitive that we looked at something as basic and simple as just generating text from a",
    "start": "367600",
    "end": "373680"
  },
  {
    "text": "language model but what if we want to go beyond generating text and use language models to interact with the outside",
    "start": "373680",
    "end": "380240"
  },
  {
    "text": "world and to perform actions this is where tools or function calling comes in",
    "start": "380240",
    "end": "385840"
  },
  {
    "text": "while tools may seem complicated at the core it's a very simple idea we give the",
    "start": "385840",
    "end": "390880"
  },
  {
    "text": "model a prompt and then we also pass as part of the conversation context a list",
    "start": "390880",
    "end": "395919"
  },
  {
    "text": "of tools that it has available to it each of these tools will be provided with the name of the tool as well as a",
    "start": "395919",
    "end": "402720"
  },
  {
    "text": "description of what the tool does so the model knows when to use it and finally any data that it requires in order to",
    "start": "402720",
    "end": "410400"
  },
  {
    "text": "use those tools let's say the model decides it needs to use one of the tools to solve the user's query rather than",
    "start": "410400",
    "end": "418240"
  },
  {
    "text": "generating some text as a response it would generate a tool call meaning it would generate the the name of the tool",
    "start": "418240",
    "end": "425199"
  },
  {
    "text": "it wants to use and any arguments or data that it can parse from the context",
    "start": "425199",
    "end": "431120"
  },
  {
    "text": "of the conversation necessary to run those tools it's then on you the",
    "start": "431120",
    "end": "436319"
  },
  {
    "text": "developer to parse that tool call run that code and then do with that as you",
    "start": "436319",
    "end": "443199"
  },
  {
    "text": "please so how do we do this with the AI SDK well let's check out a very simple example and when I say simple I mean",
    "start": "443199",
    "end": "449840"
  },
  {
    "text": "simple we're going to ask a language model to add two numbers together so",
    "start": "449840",
    "end": "455120"
  },
  {
    "text": "copy in some code here we've got our same main function as before we're calling generate text again we're",
    "start": "455120",
    "end": "460960"
  },
  {
    "text": "specifying our our model as GPT40 our prompt this time is what's 10 + 5 very",
    "start": "460960",
    "end": "467120"
  },
  {
    "text": "difficult question but this time we are passing a tool now to pass tools to the",
    "start": "467120",
    "end": "472400"
  },
  {
    "text": "generate text or stream text function you pass a tools object within that",
    "start": "472400",
    "end": "477680"
  },
  {
    "text": "object you specify a key or the name of the tool in this case add numbers then",
    "start": "477680",
    "end": "484160"
  },
  {
    "text": "you can as the value use this tool utility function which I'll explain in a second to then uh define what the tool",
    "start": "484160",
    "end": "491919"
  },
  {
    "text": "should be so the tool is going to have a description this is what the tool does and this is really important because this is what the language model uses to",
    "start": "491919",
    "end": "498560"
  },
  {
    "text": "decide whether it should invoke that tool uh parameters this is the data",
    "start": "498560",
    "end": "503759"
  },
  {
    "text": "necessary for the language model to run this tool and this is what the language model is going to have to parse from the",
    "start": "503759",
    "end": "510720"
  },
  {
    "text": "conversation context in order to use the tool and then you have the execute function this can be any arbitrary",
    "start": "510720",
    "end": "516919"
  },
  {
    "text": "asynchronous JavaScript code that will be run when the language model generates",
    "start": "516919",
    "end": "522320"
  },
  {
    "text": "a tool call and what this tool function does here is this is completely",
    "start": "522320",
    "end": "527360"
  },
  {
    "text": "unnecessary but is a really nice DX um improvement here and what it does is",
    "start": "527360",
    "end": "533839"
  },
  {
    "text": "that it provides tape type safety between the parameters that you define here and the arguments that uh come into",
    "start": "533839",
    "end": "541440"
  },
  {
    "text": "your execute function so to showcase this if we change this to string if I can spell you can see I can't spell um",
    "start": "541440",
    "end": "549600"
  },
  {
    "text": "but we can see that num one is still a number where num two is a string uh so again this is not necessary at all um",
    "start": "549600",
    "end": "558000"
  },
  {
    "text": "but it does make building and uh working with these tools a lot easier now what's",
    "start": "558000",
    "end": "563440"
  },
  {
    "text": "going on behind the scenes is that the AI SDK is going to parse any tool calls and then it's going to automatically",
    "start": "563440",
    "end": "570240"
  },
  {
    "text": "invoke the execute function and return that in a tool results array as we can",
    "start": "570240",
    "end": "577120"
  },
  {
    "text": "see right here and that's what we're logging out to the console so if we head to the console clear it out and run the",
    "start": "577120",
    "end": "584160"
  },
  {
    "text": "script we should see a tool result logged out to the console and that indeed we do we see tool result uh add",
    "start": "584160",
    "end": "591959"
  },
  {
    "text": "numbers we see the arguments that are parsed from the conversation context and then we see the result",
    "start": "591959",
    "end": "599560"
  },
  {
    "text": "itself but notice that we have logged out the tool results this time and",
    "start": "599560",
    "end": "605440"
  },
  {
    "text": "instead of instead of the the text generated and and I had said before it's",
    "start": "605440",
    "end": "611519"
  },
  {
    "text": "kind of a nuanced thing here but the language model isn't generating text anymore it's generating a tool call and",
    "start": "611519",
    "end": "617440"
  },
  {
    "text": "we can see that if we said console.log actually we just delete this al together and we log out the resulting",
    "start": "617440",
    "end": "624839"
  },
  {
    "text": "text we won't see anything in the console it's completely empty so how can we get the model to actually incorporate",
    "start": "624839",
    "end": "631920"
  },
  {
    "text": "the tool results into a generated text answer right we want it to synthesize",
    "start": "631920",
    "end": "638399"
  },
  {
    "text": "this action whatever it's performed and communicate that back to the user",
    "start": "638399",
    "end": "644640"
  },
  {
    "text": "we could look at the last tool result so",
    "start": "644640",
    "end": "649680"
  },
  {
    "text": "con's last tool result equals result.tool results and we pop we take",
    "start": "649680",
    "end": "656320"
  },
  {
    "text": "that last one and then we say if last tool",
    "start": "656320",
    "end": "661399"
  },
  {
    "text": "result equal uh dot dot name dot tool",
    "start": "661399",
    "end": "667160"
  },
  {
    "text": "name uh equals and it's cool we've got full type safety here um and then we can",
    "start": "667160",
    "end": "672320"
  },
  {
    "text": "say oh maybe we want to await and and generate text further and return that to",
    "start": "672320",
    "end": "678240"
  },
  {
    "text": "the user but as you can see this is errorprone i've made like three mistakes already but also it doesn't scale right",
    "start": "678240",
    "end": "685200"
  },
  {
    "text": "if we add 10 15 20 100 tools in here we don't want to write out 100 of these",
    "start": "685200",
    "end": "691279"
  },
  {
    "text": "conditional statements so what we can do instead is use a property called max",
    "start": "691279",
    "end": "698040"
  },
  {
    "text": "steps and we're going to set this to a number now max steps can seem again a",
    "start": "698040",
    "end": "703920"
  },
  {
    "text": "little bit confusing but what it is at its core is that if the language model",
    "start": "703920",
    "end": "710079"
  },
  {
    "text": "decides to generate a tool call and therefore there is a tool result we are",
    "start": "710079",
    "end": "715519"
  },
  {
    "text": "going to send that tool result alongside the previous conversation context back",
    "start": "715519",
    "end": "721040"
  },
  {
    "text": "to the model and trigger another generation and the model will continue doing this until it either generates",
    "start": "721040",
    "end": "728560"
  },
  {
    "text": "just plain text i.e there's no tool result or we reach the threshold for the",
    "start": "728560",
    "end": "734320"
  },
  {
    "text": "maximum number of steps so this is quite a um a a simple but powerful way to",
    "start": "734320",
    "end": "741440"
  },
  {
    "text": "allow the model to keep running autonomously picking the next step in",
    "start": "741440",
    "end": "746639"
  },
  {
    "text": "the process um without necessarily having to add any logic or rerouting",
    "start": "746639",
    "end": "754200"
  },
  {
    "text": "repiping output here so if we run this code now and and what we're going to do",
    "start": "754200",
    "end": "760000"
  },
  {
    "text": "is we're actually going to log out the result.steps.length and actually we",
    "start": "760000",
    "end": "766320"
  },
  {
    "text": "could even log out the whole steps so we can see what's really",
    "start": "766320",
    "end": "771880"
  },
  {
    "text": "happening and we're going to stringify this so it looks nice uh we should see that the language model is going to",
    "start": "771880",
    "end": "778680"
  },
  {
    "text": "respond to our if we go up all of the the crux uh",
    "start": "778680",
    "end": "784399"
  },
  {
    "text": "we can see 10 + 5 equals 15 so it responded with plain text and now we can",
    "start": "784399",
    "end": "789600"
  },
  {
    "text": "see the different steps we've got the initial step which generated a tool call add numbers 10 and five and then we can",
    "start": "789600",
    "end": "796720"
  },
  {
    "text": "see the the the tool result that we had logged out to the console a bunch of uh",
    "start": "796720",
    "end": "802240"
  },
  {
    "text": "stuff here uh cool note that you can tap into literally the the raw request and",
    "start": "802240",
    "end": "807839"
  },
  {
    "text": "response body with the AIS SDK so you can see exactly what's being sent to language models if you ever need to",
    "start": "807839",
    "end": "813600"
  },
  {
    "text": "debug but that's aside um and then we should see the second step uh and the second step was step",
    "start": "813600",
    "end": "820639"
  },
  {
    "text": "type tool result um and we can see that there is text actually in this uh",
    "start": "820639",
    "end": "828200"
  },
  {
    "text": "generation so cool we've now seen how we can build what we're going to call a multi-step agent because that's what",
    "start": "828200",
    "end": "834880"
  },
  {
    "text": "this is this this language model has been given the ability to choose the number of steps that it wants to use and",
    "start": "834880",
    "end": "840399"
  },
  {
    "text": "choose the the tools or the direction it wants to go down but obviously having just one tool here doesn't show doesn't",
    "start": "840399",
    "end": "847920"
  },
  {
    "text": "showcase this very well so why don't we add another tool here to see this agentic behavior in action what we're",
    "start": "847920",
    "end": "855839"
  },
  {
    "text": "going to do is again I'm going to just copy over some code we are going to add",
    "start": "855839",
    "end": "861120"
  },
  {
    "text": "a tool that has the ability to get weather description here kind of obvious",
    "start": "861120",
    "end": "868000"
  },
  {
    "text": "get the current weather at a location the parameters here we need the latitude longitude and city um and then finally",
    "start": "868000",
    "end": "875519"
  },
  {
    "text": "in the execute function we're going to make a fetch request passing in the latitude and longitude and then we are",
    "start": "875519",
    "end": "881440"
  },
  {
    "text": "going to unwrap the weather data and return it in the in the tool result now",
    "start": "881440",
    "end": "886800"
  },
  {
    "text": "a lot of people might be uh thinking in their head we're not providing the latitude",
    "start": "886800",
    "end": "893120"
  },
  {
    "text": "and we're not providing the longitude like is this going to fail and this taps into a really cool uh inference",
    "start": "893120",
    "end": "900160"
  },
  {
    "text": "capability that we can use uh in these parameters in that we can let the",
    "start": "900160",
    "end": "905839"
  },
  {
    "text": "language model infer these parameters from the context of the conversation so",
    "start": "905839",
    "end": "911279"
  },
  {
    "text": "we're fairly certain that the user is going to give city as as part of their",
    "start": "911279",
    "end": "916880"
  },
  {
    "text": "prompt and in this case we're going to be asking for the weather in two cities and then we're going to want to add them",
    "start": "916880",
    "end": "922720"
  },
  {
    "text": "together so we'll know that we'll have the city and we'll let the the language model use its training data to",
    "start": "922720",
    "end": "929600"
  },
  {
    "text": "effectively infer what these might be now I wouldn't suggest doing this particular example in production but",
    "start": "929600",
    "end": "936320"
  },
  {
    "text": "there are a lot of really cool use cases that you can use this pattern with so I",
    "start": "936320",
    "end": "943040"
  },
  {
    "text": "foreshadowed a little bit our prompt here is going to be get the weather in San Francisco and New York and then add them together so we have two tools",
    "start": "943040",
    "end": "950160"
  },
  {
    "text": "available here get weather and add numbers and we have a maximum number of steps of three so we'd expect probably",
    "start": "950160",
    "end": "957199"
  },
  {
    "text": "well actually let's not even guess let's see we can",
    "start": "957199",
    "end": "962279"
  },
  {
    "text": "um print out the steps the length of the steps uh the steps themselves and then",
    "start": "962279",
    "end": "968959"
  },
  {
    "text": "uh the resulting text generation so I save run the",
    "start": "968959",
    "end": "974240"
  },
  {
    "text": "script and we'll see a response the current temperature in San Francisco is 12.3Â° C in New York it's 15.2 when you",
    "start": "975240",
    "end": "983519"
  },
  {
    "text": "add these temperatures together you get 27.5 degrees CC so let's see how many",
    "start": "983519",
    "end": "988880"
  },
  {
    "text": "steps we had three steps we had an initial step which was a tool call step",
    "start": "988880",
    "end": "994560"
  },
  {
    "text": "uh we didn't unwrap this in the in the in the console log but I know exactly",
    "start": "994560",
    "end": "999680"
  },
  {
    "text": "what these are because I have done this demo before uh and this is cool we're using parallel tool calls here um so the",
    "start": "999680",
    "end": "1006720"
  },
  {
    "text": "actually we can see in the body we can see in the body right here um uh or it's",
    "start": "1006720",
    "end": "1012079"
  },
  {
    "text": "probably going to be in the in the response actually which is also wrapped in in here not unwrapped uh but what's",
    "start": "1012079",
    "end": "1019440"
  },
  {
    "text": "happening here is that we're doing our get weather our our get weather call twice we're doing for New York and then",
    "start": "1019440",
    "end": "1026000"
  },
  {
    "text": "San Francisco and then in our second step we're going to be doing our add two numbers together and then for our final",
    "start": "1026000",
    "end": "1033280"
  },
  {
    "text": "step we actually uh have the the text generation itself um and I can even if",
    "start": "1033280",
    "end": "1040640"
  },
  {
    "text": "we do it again here just to show you uh let's stringify this out um just in case",
    "start": "1040640",
    "end": "1048720"
  },
  {
    "text": "people don't believe me we'll do it one more time and this time we'll see exactly what is what is coming out",
    "start": "1048720",
    "end": "1057640"
  },
  {
    "text": "so if we jump it's good good sign that we're getting exactly the same response it means that the data is correct um and",
    "start": "1058480",
    "end": "1066080"
  },
  {
    "text": "if we scroll up to the where are we the first step so three steps here and yeah you",
    "start": "1066080",
    "end": "1073440"
  },
  {
    "text": "can see so we've got two tool tool calls we've got get weather for uh the latitude and longitude and the city of",
    "start": "1073440",
    "end": "1080000"
  },
  {
    "text": "San Francisco the same for oops scrolled too fast for New York and then we have",
    "start": "1080000",
    "end": "1087600"
  },
  {
    "text": "the add to numbers step if we get",
    "start": "1087600",
    "end": "1094600"
  },
  {
    "text": "there let's scroll down there we go add numbers number one",
    "start": "1094600",
    "end": "1101039"
  },
  {
    "text": "number two we get our result and then finally we have",
    "start": "1101039",
    "end": "1106200"
  },
  {
    "text": "our actual text generation which comes at the very end so awesome that's two",
    "start": "1106200",
    "end": "1112000"
  },
  {
    "text": "major fundamental building blocks out of the way generating text and using tool calls the final thing that we're going",
    "start": "1112000",
    "end": "1118320"
  },
  {
    "text": "to look at is generating structured data also known as structured outputs there",
    "start": "1118320",
    "end": "1123360"
  },
  {
    "text": "are two ways to generate structured outputs with the AI SDK we can one use generate text with its experimental",
    "start": "1123360",
    "end": "1129679"
  },
  {
    "text": "output option which we'll look at first and two which we'll be using more later in the session using the generate object",
    "start": "1129679",
    "end": "1136240"
  },
  {
    "text": "function which is a function dedicated to structured outputs the generate object function I will I will tell you",
    "start": "1136240",
    "end": "1142960"
  },
  {
    "text": "is my favorite function in the entire AI SDK absolute workhorse of a function and",
    "start": "1142960",
    "end": "1148160"
  },
  {
    "text": "we'll see it later so let's take a look at our existing tool calling example where we're getting the weather in two locations and then finally getting the",
    "start": "1148160",
    "end": "1154559"
  },
  {
    "text": "sum of those two numbers added together and see how we could add structured",
    "start": "1154559",
    "end": "1160000"
  },
  {
    "text": "outputs to make the eventual output easier to use maybe later on in our",
    "start": "1160000",
    "end": "1166080"
  },
  {
    "text": "program so the way that we're going to introduce structured outputs here is using the experimental I think we have",
    "start": "1166080",
    "end": "1172799"
  },
  {
    "text": "to head up here the experimental outputs um output key we can pull in",
    "start": "1172799",
    "end": "1180360"
  },
  {
    "text": "output.object from AI um and then we define in here our",
    "start": "1180360",
    "end": "1186440"
  },
  {
    "text": "schema we're going to be using ZOD to define our schema and if you've never used Zod before it's a TypeScript",
    "start": "1186440",
    "end": "1192880"
  },
  {
    "text": "validation library that is super powerful and particularly paired with the AI SDK seems like a match made in",
    "start": "1192880",
    "end": "1199840"
  },
  {
    "text": "heaven we'll be looking at it a lot in this session and it makes working with",
    "start": "1199840",
    "end": "1205200"
  },
  {
    "text": "structured outputs an absolute breeze so let's define what we want our output to actually be in this case we know we want",
    "start": "1205200",
    "end": "1211600"
  },
  {
    "text": "the sum which is going to be a number so let's define a key of sum and make it a",
    "start": "1211600",
    "end": "1217600"
  },
  {
    "text": "zumber finally we add our comma and now instead of logging out all",
    "start": "1217600",
    "end": "1225280"
  },
  {
    "text": "of this all of the steps and instead of logging out the text I'm just going to log out the experimental output and",
    "start": "1225280",
    "end": "1231919"
  },
  {
    "text": "let's run the script and see what we get so this time instead of getting all",
    "start": "1231919",
    "end": "1238480"
  },
  {
    "text": "of that extra text which we maybe could have prompted away we just have a simple type-S safe object that we can access in",
    "start": "1238480",
    "end": "1246240"
  },
  {
    "text": "this case we've got experimental output sum which we know is going to be a",
    "start": "1246240",
    "end": "1251280"
  },
  {
    "text": "number otherwise it will throw an error so you can combine tool calling with structured output to build out some",
    "start": "1251280",
    "end": "1256720"
  },
  {
    "text": "really really awesome uh use cases now let's very quickly look at the generate object function so I'm going to delete",
    "start": "1256720",
    "end": "1264320"
  },
  {
    "text": "everything in our main function uh and copy over a generate object example so",
    "start": "1264320",
    "end": "1270400"
  },
  {
    "text": "we'll save we'll clear everything away and in this case uh we're going to be looking and seeing",
    "start": "1270400",
    "end": "1277679"
  },
  {
    "text": "if AI can help us with a very invogue problem and that is defining what an AI",
    "start": "1277679",
    "end": "1284320"
  },
  {
    "text": "agent is i know Simon um had had posted",
    "start": "1284320",
    "end": "1289360"
  },
  {
    "text": "a a survey on Twitter asking for definitions and had crowdsourced",
    "start": "1289360",
    "end": "1295679"
  },
  {
    "text": "something like 250 different definitions which ended up being six categories of",
    "start": "1295679",
    "end": "1301200"
  },
  {
    "text": "different definitions anyway nobody can can agree on it and so let's see if AI can help us uh spoiler alert I don't",
    "start": "1301200",
    "end": "1308559"
  },
  {
    "text": "think it will but so what we've got here is our main function as per usual we're importing generate object from the AI",
    "start": "1308559",
    "end": "1315280"
  },
  {
    "text": "SDK uh we specify our model as GPT4 mini this should start to look quite uh",
    "start": "1315280",
    "end": "1322159"
  },
  {
    "text": "similar to how we've been doing it previously pass in a prompt please come up with 10 definitions for AI agents and",
    "start": "1322159",
    "end": "1328640"
  },
  {
    "text": "now with generate object we can define a schema in this case our schema is going to have one key it's going to be an",
    "start": "1328640",
    "end": "1334640"
  },
  {
    "text": "object that key is definitions every um and this is defined as an array of",
    "start": "1334640",
    "end": "1340000"
  },
  {
    "text": "strings and then that will give us a type- safe resulting object that has our",
    "start": "1340000",
    "end": "1345200"
  },
  {
    "text": "definitions key on it which is an array of strings so if we run the script wait",
    "start": "1345200",
    "end": "1350960"
  },
  {
    "text": "a second we should see 10 probably pretty bad definitions of what an AI",
    "start": "1350960",
    "end": "1356880"
  },
  {
    "text": "agent is an AI agent is a software entity that uses artificial intelligence",
    "start": "1356880",
    "end": "1362080"
  },
  {
    "text": "techniques to perform tasks autonomously or",
    "start": "1362080",
    "end": "1367520"
  },
  {
    "text": "semiautonomously not as bad as I thought but why don't we see if we can alter",
    "start": "1367640",
    "end": "1373520"
  },
  {
    "text": "this slightly and and the way I want to do this is provide some more details to",
    "start": "1373520",
    "end": "1379760"
  },
  {
    "text": "the language model of exactly what I want it to do for each of the strings in this array now we could jump into the",
    "start": "1379760",
    "end": "1387280"
  },
  {
    "text": "prompt and say each of these definitions should be X but one of the really really",
    "start": "1387280",
    "end": "1393520"
  },
  {
    "text": "cool features we can tap into with ZOD is the dotescribe function and with",
    "start": "1393520",
    "end": "1398600"
  },
  {
    "text": "dotescribe we can as it sounds go to any key or any value sorry and we can chain",
    "start": "1398600",
    "end": "1405280"
  },
  {
    "text": "on this.escribe function and in here we can describe exactly what we want for",
    "start": "1405280",
    "end": "1410960"
  },
  {
    "text": "that exact value so I'm going to say something uh kind of cheeky and I'm",
    "start": "1410960",
    "end": "1417280"
  },
  {
    "text": "going to say um I'm just going to paste this in i want the language model to use as much",
    "start": "1417280",
    "end": "1424000"
  },
  {
    "text": "jargon as possible it should be completely incoherent so let's see what",
    "start": "1424000",
    "end": "1429280"
  },
  {
    "text": "this can do spoiler alert this is how I spend a lot of my time hacking on on",
    "start": "1429280",
    "end": "1434400"
  },
  {
    "text": "language model stuff just like asking really ridiculous stuff and so let's see what we got here autonomous entities",
    "start": "1434400",
    "end": "1440400"
  },
  {
    "text": "that leverage algorithmic huristics to optimize decision-making processes in",
    "start": "1440400",
    "end": "1445520"
  },
  {
    "text": "dynamic environments that is complete BS um and we've got 10 of them so um so this is a",
    "start": "1445520",
    "end": "1453679"
  },
  {
    "text": "really cool thing about working with uh structured outputs with the AI SDK and",
    "start": "1453679",
    "end": "1459760"
  },
  {
    "text": "particularly with that SOD integration it makes defining these schemas and providing context super simple and",
    "start": "1459760",
    "end": "1467600"
  },
  {
    "text": "really easy to maintain as well great and with that we've gone through the fundamentals of building agents with the",
    "start": "1467600",
    "end": "1475039"
  },
  {
    "text": "AI SDK now we're going to move on to a more practical project we're going to be",
    "start": "1475039",
    "end": "1480640"
  },
  {
    "text": "trying to build a deep research clone now obviously this isn't going to be a",
    "start": "1480640",
    "end": "1485840"
  },
  {
    "text": "full crazy application implementation we're just going to be doing it in node so we're going to have a terminal script",
    "start": "1485840",
    "end": "1491760"
  },
  {
    "text": "that we're going to build we're going to pass a query and then we're going to conduct a bunch of deep research and",
    "start": "1491760",
    "end": "1496880"
  },
  {
    "text": "write a a markdown report into our file system but in doing this this should",
    "start": "1496880",
    "end": "1503200"
  },
  {
    "text": "introduce us to a few things it should introduce us to one how we would break down this idea of of deep research into",
    "start": "1503200",
    "end": "1510080"
  },
  {
    "text": "like a structured workflow um and within this workflow uh we're going to have some autonomous agentic elements so",
    "start": "1510080",
    "end": "1517679"
  },
  {
    "text": "we'll see what a production AI system might look like um and we'll also look at how we can combine different AI SDK",
    "start": "1517679",
    "end": "1525840"
  },
  {
    "text": "functions together to build these more complex AI systems that can really cater",
    "start": "1525840",
    "end": "1531480"
  },
  {
    "text": "to interesting and honestly cool use",
    "start": "1531480",
    "end": "1537039"
  },
  {
    "text": "cases that you can use in production to help build cool stuff i don't know how",
    "start": "1537039",
    "end": "1542640"
  },
  {
    "text": "how better to to to say it than that so without further ado I'm going to head to",
    "start": "1542640",
    "end": "1547919"
  },
  {
    "text": "the deep research section of the companion site because I've provided a nice explanation of how the workflow is",
    "start": "1547919",
    "end": "1554640"
  },
  {
    "text": "going to work in in natural language and also a nice diagram to explain exactly",
    "start": "1554640",
    "end": "1560640"
  },
  {
    "text": "what's going to be happening now also if you haven't used deep research before one I highly suggest you check it out um",
    "start": "1560640",
    "end": "1567279"
  },
  {
    "text": "OpenAI has a version Deep Research uh Gemini has a version as well but roughly speaking what's happening is you give",
    "start": "1567279",
    "end": "1574159"
  },
  {
    "text": "these products a topic to think about and it will go off for an extended period of time searching the web",
    "start": "1574159",
    "end": "1580480"
  },
  {
    "text": "aggregating resources going down like webs of thought and then it will aggregate all of that information",
    "start": "1580480",
    "end": "1587120"
  },
  {
    "text": "together and finally return it into a report to hopefully solve your query and",
    "start": "1587120",
    "end": "1592720"
  },
  {
    "text": "the cool thing is this really taps into a fundamental a",
    "start": "1592720",
    "end": "1599320"
  },
  {
    "text": "fundamentally strong part of of language models of this like synthesizing troves and troves of of",
    "start": "1599320",
    "end": "1606919"
  },
  {
    "text": "information um so yeah let's look at how this workflow is going to look like so",
    "start": "1606919",
    "end": "1612720"
  },
  {
    "text": "the rough steps are going to be we're going to take an input a rough query a prompt we're then going to for that",
    "start": "1612720",
    "end": "1618880"
  },
  {
    "text": "prompt generate a bunch of subqueries so let's think about like we want to do research on electric cars uh the queries",
    "start": "1618880",
    "end": "1626559"
  },
  {
    "text": "or the search queries that we might generate are like what is an electric car who are the biggest electric car",
    "start": "1626559",
    "end": "1632640"
  },
  {
    "text": "producers and so on then for each of those queries we're going to search the",
    "start": "1632640",
    "end": "1637840"
  },
  {
    "text": "web for relevant result and then we're going to analyze that result for learnings and follow-up questions and",
    "start": "1637840",
    "end": "1644480"
  },
  {
    "text": "then if we want to go into more depth which we'll look at in a second how that",
    "start": "1644480",
    "end": "1649919"
  },
  {
    "text": "works uh we will take those follow-up questions as well as like the existing research generate a new query and like",
    "start": "1649919",
    "end": "1657720"
  },
  {
    "text": "completely recursively complete that process meaning like we basically start again while keeping all of the",
    "start": "1657720",
    "end": "1664600"
  },
  {
    "text": "accumulated research and in this way we can go down these like these webs of of",
    "start": "1664600",
    "end": "1670320"
  },
  {
    "text": "thought and of questions and and and build a comprehensive",
    "start": "1670320",
    "end": "1676520"
  },
  {
    "text": "aggregated a comprehensive set of information about a a given topic so",
    "start": "1676520",
    "end": "1682080"
  },
  {
    "text": "what this might look like in theory I like this little explanation uh so let's say we have electric cars we're calling",
    "start": "1682080",
    "end": "1688399"
  },
  {
    "text": "this level zero just like this is the initial query and at level one uh we're going to have a breadth now breadth is",
    "start": "1688399",
    "end": "1695360"
  },
  {
    "text": "just like how many different uh queries do we want at each step so let's say",
    "start": "1695360",
    "end": "1700799"
  },
  {
    "text": "we're at we're at level one um and we're doing electric cars and the three search queries that we generate are Tesla Model",
    "start": "1700799",
    "end": "1708799"
  },
  {
    "text": "3 specification electric car charging infrastructure and electric vehicle battery technology and then for each of",
    "start": "1708799",
    "end": "1715840"
  },
  {
    "text": "those different queries we'd complete the research and maybe for Tesla Model 3",
    "start": "1715840",
    "end": "1721440"
  },
  {
    "text": "we would then start generating uh go down the web have two breadth uh like we",
    "start": "1721440",
    "end": "1728240"
  },
  {
    "text": "want to generate two different u lines of inquiry to go down and then we go",
    "start": "1728240",
    "end": "1733840"
  },
  {
    "text": "down okay we want to know Model 3 range capacity and model 3 pricing and then for electric char car charging",
    "start": "1733840",
    "end": "1740559"
  },
  {
    "text": "infrastructure we want to know fast charging stations in the US and home charging installation and so on and so",
    "start": "1740559",
    "end": "1746640"
  },
  {
    "text": "depending on what depth and breadth settings we set um we're able",
    "start": "1746640",
    "end": "1753720"
  },
  {
    "text": "to control the level of information um",
    "start": "1753720",
    "end": "1758799"
  },
  {
    "text": "and the depth of information that we gather so that was a lot let's uh let's",
    "start": "1758799",
    "end": "1765840"
  },
  {
    "text": "jump into actually trying to build this and the first thing that we're going to look at is building a function that can",
    "start": "1765840",
    "end": "1772320"
  },
  {
    "text": "generate some search queries so I'm going to dive back into",
    "start": "1772320",
    "end": "1777880"
  },
  {
    "text": "the back into the code editor we're going to clear out this file and we are",
    "start": "1777880",
    "end": "1783440"
  },
  {
    "text": "going to copy in our first function so the first thing like we said we want to generate a bunch of search queries this",
    "start": "1783440",
    "end": "1790000"
  },
  {
    "text": "is going to be a single function called generate search queries i'm very creative uh it's going to take in a",
    "start": "1790000",
    "end": "1796159"
  },
  {
    "text": "query uh which is of type string and the number of search queries that we actually want to generate we're going to",
    "start": "1796159",
    "end": "1801360"
  },
  {
    "text": "set this to be three just as default this is for you to play around with later if you'd like um then we use the",
    "start": "1801360",
    "end": "1808960"
  },
  {
    "text": "generate object function we're specifying a main model so that we can just uh set it once reference it",
    "start": "1808960",
    "end": "1816159"
  },
  {
    "text": "everywhere and if we want to update it later we can in just one place so that's why we're doing that we have a prompt",
    "start": "1816159",
    "end": "1821679"
  },
  {
    "text": "here uh we we use template strings all over the place in this and you'll probably end up using them a ton super",
    "start": "1821679",
    "end": "1828159"
  },
  {
    "text": "helpful uh for this use case so we say generate n number of search queries for",
    "start": "1828159",
    "end": "1833279"
  },
  {
    "text": "the following query passing in the the search query and then we ask for a",
    "start": "1833279",
    "end": "1838640"
  },
  {
    "text": "structured output which should have a uh an array of strings minimum of one",
    "start": "1838640",
    "end": "1844640"
  },
  {
    "text": "maximum of five although we're setting it kind of loosely here at uh three and",
    "start": "1844640",
    "end": "1849919"
  },
  {
    "text": "then we return those queries so that's our generate search queries function if",
    "start": "1849919",
    "end": "1855200"
  },
  {
    "text": "we go back to our companion site we see the next thing that we have to do is map through each of those queries and search",
    "start": "1855200",
    "end": "1862159"
  },
  {
    "text": "the web for relevant results analyze the result for learning and follow-up",
    "start": "1862159",
    "end": "1867679"
  },
  {
    "text": "questions and then follow up with new queries so let's go one at a time the first thing actually we we'll want to do",
    "start": "1867679",
    "end": "1873760"
  },
  {
    "text": "is have a main function so that we can actually uh specify a prompt and then",
    "start": "1873760",
    "end": "1879279"
  },
  {
    "text": "call this generate search queries so our prompt is going to be if you were at the AI engineer summit there's an awesome",
    "start": "1879279",
    "end": "1884880"
  },
  {
    "text": "talk and actually it's I think it's available online from the Gemini team where they walked through uh how they",
    "start": "1884880",
    "end": "1890640"
  },
  {
    "text": "went about building deep research it was a really really good talk and one of my favorite things from it was hearing the",
    "start": "1890640",
    "end": "1896960"
  },
  {
    "text": "the prompt that they used as like the the kind of the gold standard to",
    "start": "1896960",
    "end": "1902000"
  },
  {
    "text": "evaluate the progress of the the product and that was what do you need to be a D1",
    "start": "1902000",
    "end": "1909039"
  },
  {
    "text": "shot put athlete so in that similar vein we're going to be using that today so we take that prompt and then we",
    "start": "1909039",
    "end": "1916159"
  },
  {
    "text": "pass it into our generate search queries function uh because we specified a default number of search queries as",
    "start": "1916159",
    "end": "1922799"
  },
  {
    "text": "three we we don't need to specify one here uh and then that's going to return our queries so we can do just a little",
    "start": "1922799",
    "end": "1929840"
  },
  {
    "text": "checkin to see what's happening here and log out our queries",
    "start": "1929840",
    "end": "1935440"
  },
  {
    "text": "run the script and we should see fairly quickly we've got three potential search",
    "start": "1935440",
    "end": "1940960"
  },
  {
    "text": "queries for our prompt of what do you need to be a D1 shotput athlete these are obviously geared for a search engine",
    "start": "1940960",
    "end": "1947679"
  },
  {
    "text": "so like requirements to become a D1 shotput athlete training regiment for D1 shotput athlete qualifications for NCAA",
    "start": "1947679",
    "end": "1955279"
  },
  {
    "text": "division one shot put so three pretty good queries that I think would make sense to to look for if you wanted to",
    "start": "1955279",
    "end": "1961679"
  },
  {
    "text": "learn more about this all right back to our companion site we'll see the next",
    "start": "1961679",
    "end": "1967519"
  },
  {
    "text": "thing that we need to do map through these queries and search the web for a relevant result first thing that we're going to implement now is a function to",
    "start": "1967519",
    "end": "1974640"
  },
  {
    "text": "actually search the web for relevant results and the service we're actually going to use today to do this is called",
    "start": "1974640",
    "end": "1981360"
  },
  {
    "text": "Exa uh if you haven't used it before highly rate it um",
    "start": "1981360",
    "end": "1986640"
  },
  {
    "text": "uh really enjoy using it it's fast cheap um but judge for yourself like we'll use",
    "start": "1986640",
    "end": "1993039"
  },
  {
    "text": "it now and uh I think they've got a great API and so yeah let's build a",
    "start": "1993039",
    "end": "1998159"
  },
  {
    "text": "function to search the web with Excel so I'm going to copy over some",
    "start": "1998159",
    "end": "2004720"
  },
  {
    "text": "code we'll head back to uh our codebase and we're going to make a few new lines here uh so what we're going to do is",
    "start": "2004720",
    "end": "2011120"
  },
  {
    "text": "we're going to import XA from the XAJS package we're going to instantiate XA",
    "start": "2011120",
    "end": "2016320"
  },
  {
    "text": "extra XA passing in our XA API key uh we're going to define a type which we're",
    "start": "2016320",
    "end": "2022320"
  },
  {
    "text": "going to be using heavily later and then finally we are going to actually search",
    "start": "2022320",
    "end": "2027519"
  },
  {
    "text": "the web so our search web function takes in a query which is a type string and then it's going to use the excess search",
    "start": "2027519",
    "end": "2034640"
  },
  {
    "text": "and contents function uh passing in our query and then specifying some optional",
    "start": "2034640",
    "end": "2041000"
  },
  {
    "text": "config two optional config we have here is one the number of results we want i've left this as one just as this is a",
    "start": "2041000",
    "end": "2048158"
  },
  {
    "text": "simple demo but you'd probably want to set this as configurable and allow the language model to infer what is",
    "start": "2048159",
    "end": "2053839"
  },
  {
    "text": "necessary based on maybe like how complicated something might be i don't",
    "start": "2053839",
    "end": "2058960"
  },
  {
    "text": "know that's something for you to experiment with um and then the second option is live crawl um and live crawl",
    "start": "2058960",
    "end": "2065760"
  },
  {
    "text": "is kind of as it sound allows you to or ensures that the results that you're getting are live uh rather than",
    "start": "2065760",
    "end": "2072240"
  },
  {
    "text": "something that's in their cache so you obviously take a little bit of a hit on performance or time for this uh result",
    "start": "2072240",
    "end": "2080480"
  },
  {
    "text": "to be executed but you're sure that everything that is executed is is is live is up to",
    "start": "2080480",
    "end": "2086760"
  },
  {
    "text": "date now the other thing that I'm doing here is that I'm actually mapping through the results and only returning",
    "start": "2086760",
    "end": "2094320"
  },
  {
    "text": "the information that I feel is relevant or necessary for completing this process",
    "start": "2094320",
    "end": "2100880"
  },
  {
    "text": "and there are a few reasons for doing this but the main reason is to reduce the number of tokens that I'm sending to",
    "start": "2100880",
    "end": "2106760"
  },
  {
    "text": "OpenAI um two reasons for that one is that it's cheaper fewer tokens just",
    "start": "2106760",
    "end": "2114640"
  },
  {
    "text": "going to be cheaper um but second and more important I found that the language model is so much more effective when you",
    "start": "2114640",
    "end": "2121280"
  },
  {
    "text": "trim away all of the irrelevant information so um things like a favicon",
    "start": "2121280",
    "end": "2126560"
  },
  {
    "text": "favicon link is going to take up a decent amount of space not necessary at",
    "start": "2126560",
    "end": "2131680"
  },
  {
    "text": "all for the language model to actually use these resources so I tend to do this a lot whenever I have tool calls or or",
    "start": "2131680",
    "end": "2138400"
  },
  {
    "text": "building out tools for working with language models ensuring that the information that I'm returning or",
    "start": "2138400",
    "end": "2143440"
  },
  {
    "text": "providing as part of the context is entirely relevant to the generation or",
    "start": "2143440",
    "end": "2150240"
  },
  {
    "text": "to to the task at hand we should say cool so that's our search web",
    "start": "2150240",
    "end": "2156280"
  },
  {
    "text": "function and now if we go back to again our we can think of this a bit like a",
    "start": "2156280",
    "end": "2162320"
  },
  {
    "text": "checklist the next thing that we're going to have to do is analyze the results um the search results for",
    "start": "2162320",
    "end": "2168079"
  },
  {
    "text": "learnings and follow-up questions now this is going to be the most complicated part of the entire workflow and this is",
    "start": "2168079",
    "end": "2174160"
  },
  {
    "text": "also going to be the agentic part of the workflow and so what we're going to do is we're going to use generate text as",
    "start": "2174160",
    "end": "2180560"
  },
  {
    "text": "we did before giving it two tools we're going to have a tool for searching the web and then we're going to have a tool",
    "start": "2180560",
    "end": "2186000"
  },
  {
    "text": "for evaluating the relevance of that tool call and this is kind of I I say",
    "start": "2186000",
    "end": "2192400"
  },
  {
    "text": "this is the most interesting and equally also the the agentic part um because",
    "start": "2192400",
    "end": "2198880"
  },
  {
    "text": "it's going to continue doing that flow for as long as it takes to get a a relevant search result so let's see how",
    "start": "2198880",
    "end": "2206960"
  },
  {
    "text": "we can implement this i'm going to copy over this code and bear with me because there's a lot there's a a decent amount",
    "start": "2206960",
    "end": "2214079"
  },
  {
    "text": "of code here uh but we'll walk through all of it and see how it works so I'm",
    "start": "2214079",
    "end": "2219599"
  },
  {
    "text": "going to also uh make sure that we've got our imports all good i'm going to",
    "start": "2219599",
    "end": "2224760"
  },
  {
    "text": "close off uh some of our tools and now we're ready to go so this function is",
    "start": "2224760",
    "end": "2230560"
  },
  {
    "text": "called search and process it is going to search and process it's going to take in our query again remember search the web",
    "start": "2230560",
    "end": "2237920"
  },
  {
    "text": "for a relevant result and analyze uh search the web for map through each query and search the web for a relevant",
    "start": "2237920",
    "end": "2244640"
  },
  {
    "text": "result so we're taking in that query uh we're going to take we're going to create two local variables for this for",
    "start": "2244640",
    "end": "2251119"
  },
  {
    "text": "this function uh pending search results and if you can think about it like this process of searching the web and trying",
    "start": "2251119",
    "end": "2258240"
  },
  {
    "text": "to figure out if it's relevant when you search the web you're going to add that result to the pending search results",
    "start": "2258240",
    "end": "2263599"
  },
  {
    "text": "array and then for evaluate you're going to pull out whatever the most recent uh",
    "start": "2263599",
    "end": "2268800"
  },
  {
    "text": "pending result is check if it's relevant if it is pop it into the final search",
    "start": "2268800",
    "end": "2274000"
  },
  {
    "text": "results array if it's not just discard it um so that's the rough flow that we're going to be building here we use",
    "start": "2274000",
    "end": "2280400"
  },
  {
    "text": "our main model as the model here our prompt is very complicated search the",
    "start": "2280400",
    "end": "2285680"
  },
  {
    "text": "web for information about our query uh we give it a system prompt you are a researcher for each query search the web",
    "start": "2285680",
    "end": "2292880"
  },
  {
    "text": "and then evaluate if the results are relevant and will help answer the",
    "start": "2292880",
    "end": "2297920"
  },
  {
    "text": "following query we then pass in the query which is right here i was looking",
    "start": "2297920",
    "end": "2305440"
  },
  {
    "text": "for it below it's right above um we set our max steps so we want this agentic loop to run up to five times you could",
    "start": "2305440",
    "end": "2313359"
  },
  {
    "text": "set this higher and we can set this higher for now um and that will just allow that process of finding a relevant",
    "start": "2313359",
    "end": "2320160"
  },
  {
    "text": "link to continue onwards and onwards i like to keep these relatively low just",
    "start": "2320160",
    "end": "2325280"
  },
  {
    "text": "to ensure that it doesn't go off the deep end but experiment experiment experiment is my like main advice for",
    "start": "2325280",
    "end": "2332400"
  },
  {
    "text": "building with with these tools our first tool that we have here I'm saying tool a",
    "start": "2332400",
    "end": "2338079"
  },
  {
    "text": "lot uh is the search web this searches the web for information about a given query we take in a query which is a",
    "start": "2338079",
    "end": "2344079"
  },
  {
    "text": "string uh our string uh we then pass into the search web function that we just created earlier uh we get back some",
    "start": "2344079",
    "end": "2351599"
  },
  {
    "text": "search results that type we declared uh at the beginning which I can show you here this type right there um and then",
    "start": "2351599",
    "end": "2361280"
  },
  {
    "text": "finally as I was saying we we take this local variable this pending search results and we push whatever these new",
    "start": "2361280",
    "end": "2369920"
  },
  {
    "text": "uh searched web results that we get and we also add those to the uh conversation",
    "start": "2369920",
    "end": "2375599"
  },
  {
    "text": "context by returning them from the tool and then we have the evaluate tool and",
    "start": "2375599",
    "end": "2381040"
  },
  {
    "text": "this is to evaluate the search results it doesn't take in any parameters um and",
    "start": "2381040",
    "end": "2386079"
  },
  {
    "text": "in the execute function we are going to pull out the latest pending result uh",
    "start": "2386079",
    "end": "2391119"
  },
  {
    "text": "then run it through generate object asking it to evaluate whether the search results are relevant and will help",
    "start": "2391119",
    "end": "2396560"
  },
  {
    "text": "answer the following query uh and then we pass in with this XML syntax our",
    "start": "2396560",
    "end": "2402119"
  },
  {
    "text": "stringified pending result now uh the final thing here with",
    "start": "2402119",
    "end": "2408160"
  },
  {
    "text": "this generate object rather than specifying the schema with zod because we just want to know whether it's",
    "start": "2408160",
    "end": "2413280"
  },
  {
    "text": "relevant or irrelevant we can actually use generate objects enum mode to specify just the two values that we need",
    "start": "2413280",
    "end": "2420480"
  },
  {
    "text": "something's either going to be relevant or irrelevant uh this is super ergonomic",
    "start": "2420480",
    "end": "2425920"
  },
  {
    "text": "you could use zod here i just find this very easy particularly if you have just two values um and it's also easier for",
    "start": "2425920",
    "end": "2431920"
  },
  {
    "text": "the language model when it's literally restricted to two model to two values rather than generating a full structured",
    "start": "2431920",
    "end": "2439240"
  },
  {
    "text": "object uh then if the evaluation is relevant uh we will push the pending",
    "start": "2439240",
    "end": "2445599"
  },
  {
    "text": "result to the final result because we know that it's now relevant so that's going to be a good sorts that we want to",
    "start": "2445599",
    "end": "2451760"
  },
  {
    "text": "keep uh we have some logs here and then this is interesting here we say if the",
    "start": "2451760",
    "end": "2457359"
  },
  {
    "text": "evaluation is irrelevant we return the following string from this tool call search results are irrelevant please",
    "start": "2457359",
    "end": "2464079"
  },
  {
    "text": "search again with a more specific query so when max steps triggers the next",
    "start": "2464079",
    "end": "2469359"
  },
  {
    "text": "generation the language model is going to see the most recent step that took and the last part of that most recent",
    "start": "2469359",
    "end": "2475599"
  },
  {
    "text": "step i.e the tool result was please search again with a more specific query",
    "start": "2475599",
    "end": "2480640"
  },
  {
    "text": "so we have this system that is repeating um but with feedback based on what's",
    "start": "2480640",
    "end": "2486400"
  },
  {
    "text": "going on the last thing to mention here is the fact that I did not use the parameters here to parse out the search",
    "start": "2486400",
    "end": "2494160"
  },
  {
    "text": "result and uh there's a very uh specific reason for doing that search result can",
    "start": "2494160",
    "end": "2499920"
  },
  {
    "text": "be super long right this could be a whole entire crawled web page that could be I don't know 10,000 tokens and we",
    "start": "2499920",
    "end": "2506560"
  },
  {
    "text": "don't want the model to have to actually parse that out it's literally generating",
    "start": "2506560",
    "end": "2511839"
  },
  {
    "text": "the text that already exists in the context above one costs money two it",
    "start": "2511839",
    "end": "2518960"
  },
  {
    "text": "takes a long time three it's probably errorprone because it's literally just writing out stuff that's above it's not",
    "start": "2518960",
    "end": "2525040"
  },
  {
    "text": "referencing it and that's why I like to do this um these local variables within",
    "start": "2525040",
    "end": "2531040"
  },
  {
    "text": "this function faster cheaper more accurate and so on so this is the search",
    "start": "2531040",
    "end": "2536960"
  },
  {
    "text": "and process function and we can add it to our main function and see how it",
    "start": "2536960",
    "end": "2544599"
  },
  {
    "text": "works so we're going to console.log our search results and see",
    "start": "2544599",
    "end": "2550720"
  },
  {
    "text": "see what we got so we'll clear this out run the function again searching the web for requirements",
    "start": "2550720",
    "end": "2557839"
  },
  {
    "text": "to become a D1 shotput athlete found throws university benchmark lifts",
    "start": "2557839",
    "end": "2564640"
  },
  {
    "text": "to throw 50 ft from school shotput evaluation completed it's",
    "start": "2564640",
    "end": "2572078"
  },
  {
    "text": "irrelevant so it found a new one considered it irrelevant",
    "start": "2572839",
    "end": "2579880"
  },
  {
    "text": "found another one men's track recruiting standards and considered that irrelevant or considered that relevant sorry and",
    "start": "2586640",
    "end": "2592720"
  },
  {
    "text": "then it returned a bunch of oh boy we returned the whole result so this is like the whole scraped uh web page in",
    "start": "2592720",
    "end": "2600079"
  },
  {
    "text": "here so we can see very cool it is working great so let's move on to the",
    "start": "2600079",
    "end": "2607119"
  },
  {
    "text": "next step and what is the next step uh the next step is to analyze the results",
    "start": "2607119",
    "end": "2612240"
  },
  {
    "text": "for learnings and follow-up questions so to do this we're going to again",
    "start": "2612240",
    "end": "2619119"
  },
  {
    "text": "create a new function this new function is going to be called generate learnings it's going to take in a query which is a",
    "start": "2619119",
    "end": "2626000"
  },
  {
    "text": "string and then uh the search result itself so this is like the the web page in the scripted content we're then going",
    "start": "2626000",
    "end": "2632960"
  },
  {
    "text": "to use uh surprise surprise my favorite function generate object using our main",
    "start": "2632960",
    "end": "2638480"
  },
  {
    "text": "model again and providing a prompt here this time the user is researching query the following search results were deemed",
    "start": "2638480",
    "end": "2644880"
  },
  {
    "text": "relevant generate a learning and a follow-up question from the following search result we use those XML tags",
    "start": "2644880",
    "end": "2651200"
  },
  {
    "text": "again to nest in our search result that is stringified and then finally we",
    "start": "2651200",
    "end": "2656319"
  },
  {
    "text": "specify as our schema we want uh a a string which is the learning itself the",
    "start": "2656319",
    "end": "2661520"
  },
  {
    "text": "insight we could probably call it and then uh any follow-up questions which are an array of strings and we return",
    "start": "2661520",
    "end": "2669119"
  },
  {
    "text": "that object which is this type safe uh structured output cool so we can incorporate that",
    "start": "2669119",
    "end": "2676960"
  },
  {
    "text": "into our main function again let's replace all of that and we can see now",
    "start": "2676960",
    "end": "2682160"
  },
  {
    "text": "that we are we have our prompt we have our queries we we for each query we're",
    "start": "2682160",
    "end": "2688640"
  },
  {
    "text": "going to search the web search and process so we're searching and making sure that the the result is relevant and",
    "start": "2688640",
    "end": "2694560"
  },
  {
    "text": "then for each of the results that are returned we're then going to pass it to our generate learnings function passing",
    "start": "2694560",
    "end": "2699839"
  },
  {
    "text": "in that original query and then the um the search result so we could again test",
    "start": "2699839",
    "end": "2707119"
  },
  {
    "text": "to see how things are looking run the",
    "start": "2707119",
    "end": "2712599"
  },
  {
    "text": "script searching the web for requirements to become a D1 shop athlete let's see how many times uh I have said",
    "start": "2712599",
    "end": "2719200"
  },
  {
    "text": "that in um in this",
    "start": "2719200",
    "end": "2723520"
  },
  {
    "text": "video so it's deemed that this is irrelevant but the next link that it finds is relevant so it's now processing",
    "start": "2724280",
    "end": "2731760"
  },
  {
    "text": "this search result and then we're going to get a learning so what do we have here to become a D1 shop at athlete high",
    "start": "2731760",
    "end": "2737680"
  },
  {
    "text": "school athletes typically need to have four years of varsity experience achieve highstate finishes or be state champions",
    "start": "2737680",
    "end": "2743520"
  },
  {
    "text": "and participate in national events like USATF National Junior Olympic Outdoor",
    "start": "2743520",
    "end": "2749599"
  },
  {
    "text": "Track and Field Championships they should also aim for shot put distance of around I think that's 60 feet um for",
    "start": "2749599",
    "end": "2756960"
  },
  {
    "text": "tier one recruits um and then we've got some follow-up questions what are the specific training regimens for shot put",
    "start": "2756960",
    "end": "2762720"
  },
  {
    "text": "athletes how do division one recruiting standards differ from division two and three some like really interesting",
    "start": "2762720",
    "end": "2769359"
  },
  {
    "text": "threads to to go down so what we're going to have to do naturally now is",
    "start": "2769359",
    "end": "2774800"
  },
  {
    "text": "introduce recursion to this whole process meaning now that we have our learnings we effectively want to take",
    "start": "2774800",
    "end": "2780880"
  },
  {
    "text": "the follow-up questions create a new query and call the entire process again",
    "start": "2780880",
    "end": "2786720"
  },
  {
    "text": "until uh effectively we we are happy with the the depth of information that",
    "start": "2786720",
    "end": "2793760"
  },
  {
    "text": "we've gathered so there are going to be a few things that we need to do one we need to probably create a new function",
    "start": "2793760",
    "end": "2799040"
  },
  {
    "text": "that can handle this recursion rather than doing it all in the main function",
    "start": "2799040",
    "end": "2804319"
  },
  {
    "text": "two we're going to have to we're not going to be able to track all of our uh",
    "start": "2804319",
    "end": "2810240"
  },
  {
    "text": "accumulated research in in the local function anymore we're going to have to create some state or a variable outside",
    "start": "2810240",
    "end": "2817440"
  },
  {
    "text": "in in the global state to be able to track this as we go through and then we're probably going to need some types",
    "start": "2817440",
    "end": "2822880"
  },
  {
    "text": "as well just to help make this all a bit easier so let's add those now first",
    "start": "2822880",
    "end": "2827920"
  },
  {
    "text": "things first we're going to create a new function uh called uh deep research to",
    "start": "2827920",
    "end": "2835280"
  },
  {
    "text": "um to actually that will be able to handle our recursion so all we've done here is taken our previous logic that",
    "start": "2835280",
    "end": "2841359"
  },
  {
    "text": "was within the main function and put it into a new function called deep research um so as you can see we we're generating",
    "start": "2841359",
    "end": "2848480"
  },
  {
    "text": "search queries we are then searching and process then we're generating learning",
    "start": "2848480",
    "end": "2854000"
  },
  {
    "text": "so same as before and then in our main function this time we define our prompt and then we um call that deep research",
    "start": "2854000",
    "end": "2861880"
  },
  {
    "text": "function so what we're going to have to do now is actually call this function recursively but first things first we're",
    "start": "2861880",
    "end": "2868640"
  },
  {
    "text": "going to need to create uh a place to store our accumulated",
    "start": "2868640",
    "end": "2874720"
  },
  {
    "text": "research state so I'm going to copy in some code",
    "start": "2874720",
    "end": "2879920"
  },
  {
    "text": "here uh we're defining two types uh a learning type which we've seen before this was the the the learning the",
    "start": "2879920",
    "end": "2886560"
  },
  {
    "text": "insight from the research and then follow-up questions um and then research this is the core accumulated research",
    "start": "2886560",
    "end": "2893920"
  },
  {
    "text": "object um or store um and we store in like our original query the queries that",
    "start": "2893920",
    "end": "2899839"
  },
  {
    "text": "have been performed um or the queries the active queries right now the search",
    "start": "2899839",
    "end": "2905200"
  },
  {
    "text": "results and accumulation of all of them uh learnings all the insights and then completed queries and at the very end as",
    "start": "2905200",
    "end": "2912240"
  },
  {
    "text": "this develops we're going to pass all of that state that store of information to",
    "start": "2912240",
    "end": "2918319"
  },
  {
    "text": "one large model that can synthesize all of that and generate our report but first we need to update our deep",
    "start": "2918319",
    "end": "2924640"
  },
  {
    "text": "research function to actually update the store as it's iterating through these",
    "start": "2924640",
    "end": "2930240"
  },
  {
    "text": "levels of recursion so let's head to our deep research function i'm going to copy",
    "start": "2930240",
    "end": "2936480"
  },
  {
    "text": "all of this and we'll look at what we've updated here so the first thing that we've done um is we we've changed the",
    "start": "2936480",
    "end": "2943200"
  },
  {
    "text": "name of one uh parameter from query to prompt just so you know uh we've also",
    "start": "2943200",
    "end": "2949359"
  },
  {
    "text": "updated our depth and breadth uh numbers so depth remember are the levels deep that we go into the the the the strands",
    "start": "2949359",
    "end": "2957280"
  },
  {
    "text": "that will or the levels of the strands that will go down and then breadth are those like individual forks how many of",
    "start": "2957280",
    "end": "2963760"
  },
  {
    "text": "them will go down at each level so we first check to see if a query is undefined that will be on the first time",
    "start": "2963760",
    "end": "2970800"
  },
  {
    "text": "we're ever running this um and if it is we'll set it to the the prompt we then",
    "start": "2970800",
    "end": "2976480"
  },
  {
    "text": "like last time we generate the search queries this time we update our queries to be that that uh whatever is returned",
    "start": "2976480",
    "end": "2983839"
  },
  {
    "text": "from the search queries um and that's a theme as we go through here we're going to update our accumulated research to",
    "start": "2983839",
    "end": "2989520"
  },
  {
    "text": "take in our our search results our learnings and our completed queries and now that we have this all out of the way",
    "start": "2989520",
    "end": "2996079"
  },
  {
    "text": "we can actually call deep research recursively um and at the same time we're going to decrement the depth and",
    "start": "2996079",
    "end": "3003040"
  },
  {
    "text": "breadth so that we can eventually get to a resolution uh so this doesn't run forever and we don't run out of all of",
    "start": "3003040",
    "end": "3009839"
  },
  {
    "text": "our API credits and leave the user waiting for for way too long so let's",
    "start": "3009839",
    "end": "3015359"
  },
  {
    "text": "add that final recursion i'm going to replace the entire function again um",
    "start": "3015359",
    "end": "3022319"
  },
  {
    "text": "just so I don't make any mistakes here we've added two things here first we've said uh if the depth is is zero uh just",
    "start": "3022319",
    "end": "3029040"
  },
  {
    "text": "return effectively like recursion is done at this point uh return the accumulated research um and now for the",
    "start": "3029040",
    "end": "3037119"
  },
  {
    "text": "new stuff where before we had the comments saying perform the deep research now instead we create a new",
    "start": "3037119",
    "end": "3043839"
  },
  {
    "text": "query saying the overall research goal is this these are the previous search queries that have been performed here",
    "start": "3043839",
    "end": "3050400"
  },
  {
    "text": "are some follow-up questions um and then we pass that back and call the entire",
    "start": "3050400",
    "end": "3056480"
  },
  {
    "text": "function again with that new prompt uh again decrementing the depth and the",
    "start": "3056480",
    "end": "3062800"
  },
  {
    "text": "breadth at a more exponential rate so I think that's a good as good a",
    "start": "3062800",
    "end": "3071839"
  },
  {
    "text": "time as any to to run the function again and and kind of see what's happening we should see it now going into various",
    "start": "3071839",
    "end": "3078640"
  },
  {
    "text": "levels of depth again I'm saying requirements to become a D1 shotput",
    "start": "3078640",
    "end": "3083880"
  },
  {
    "text": "athlete so this uh source is deemed irrelevant it keeps liking to use that",
    "start": "3083880",
    "end": "3091880"
  },
  {
    "text": "one instead the track recruiting standards is deemed relevant we're now processing the search result we're going",
    "start": "3091880",
    "end": "3098000"
  },
  {
    "text": "to be generating some learnings we're now searching the web for what physical and technical skills",
    "start": "3098000",
    "end": "3104160"
  },
  {
    "text": "are essential for a D1 shot athlete so we're going a level of depth here we found uh some some relevant",
    "start": "3104160",
    "end": "3114520"
  },
  {
    "text": "Now we're searching again training and skills needed for D1 shotput athletes and so on and so forth so we",
    "start": "3115520",
    "end": "3122559"
  },
  {
    "text": "can see that it is indeed working",
    "start": "3122559",
    "end": "3127720"
  },
  {
    "text": "one thing though that we haven't done yet is that we haven't incorporated into",
    "start": "3129040",
    "end": "3134720"
  },
  {
    "text": "and funny enough we didn't see this in this example but the way that our logic",
    "start": "3134720",
    "end": "3139920"
  },
  {
    "text": "works right now when the model is trying to decide whether a link is relevant it doesn't have context as to the links",
    "start": "3139920",
    "end": "3146640"
  },
  {
    "text": "that have already been used in other steps and naturally we've been talking about context length price speed we",
    "start": "3146640",
    "end": "3154319"
  },
  {
    "text": "don't we we surely do not want to provide the same source course twice particularly if it's taking up 10 20,000",
    "start": "3154319",
    "end": "3162079"
  },
  {
    "text": "tokens in length so what we're going to do is we're going to go to our search",
    "start": "3162079",
    "end": "3167520"
  },
  {
    "text": "and process function we're going to head into our evaluate tool and in this",
    "start": "3167520",
    "end": "3172720"
  },
  {
    "text": "evaluate tool we're also going to pass in previously used search results and if",
    "start": "3172720",
    "end": "3178240"
  },
  {
    "text": "the search result exists in that previously used search results we're going to say that's an irrelevant source",
    "start": "3178240",
    "end": "3184400"
  },
  {
    "text": "and and try again so I'm going to copy the entire function",
    "start": "3184400",
    "end": "3191359"
  },
  {
    "text": "replace the entire function here um this time we're taking in a new parameter",
    "start": "3191359",
    "end": "3196559"
  },
  {
    "text": "here accumulated sources um and if we scroll down we should see our updated",
    "start": "3196559",
    "end": "3201680"
  },
  {
    "text": "prompt we say if the page already exists in the existing results mark it as irrelevant and we can see we're passing",
    "start": "3201680",
    "end": "3208319"
  },
  {
    "text": "it in in XML tags existing results and stringifying through passing just the",
    "start": "3208319",
    "end": "3213599"
  },
  {
    "text": "URL because also we don't need all the content from the page here we literally just need the URL we have one error here",
    "start": "3213599",
    "end": "3220960"
  },
  {
    "text": "and that's because we added a new argument but we haven't passed it in here so I'm going to just pass that in",
    "start": "3220960",
    "end": "3227040"
  },
  {
    "text": "here uh this will be the accumulated research dot um I think this is the",
    "start": "3227040",
    "end": "3235599"
  },
  {
    "text": "search results yeah search results perfect uh so if we were to to run this",
    "start": "3235599",
    "end": "3243680"
  },
  {
    "text": "again what we would see we didn't see it in the previous example before but if it reused a source or if Exa returned a",
    "start": "3243680",
    "end": "3252079"
  },
  {
    "text": "source that it had already used in a previous step that would now be marked as irrelevant and that agentic process",
    "start": "3252079",
    "end": "3258880"
  },
  {
    "text": "would continue on in a loop the last thing that we're going to want to do which I actually don't have in in this",
    "start": "3258880",
    "end": "3265440"
  },
  {
    "text": "process is that now that we have all this accumulated research we want to give it to a big model I I prefer a",
    "start": "3265440",
    "end": "3272640"
  },
  {
    "text": "reasoning model in this case to synthesize all of this information and put it into a report that we can consume",
    "start": "3272640",
    "end": "3279119"
  },
  {
    "text": "to hopefully solve our query so let's create a new function this function is",
    "start": "3279119",
    "end": "3285040"
  },
  {
    "text": "going to be called a very creative name as per usual uh generate report it's",
    "start": "3285040",
    "end": "3291280"
  },
  {
    "text": "going to take in that accumulated research which is typed as such um and then we're going to call generate texts",
    "start": "3291280",
    "end": "3297920"
  },
  {
    "text": "this time we're using 03 mini rather than our main model uh again play around with this clone this and and see which",
    "start": "3297920",
    "end": "3304720"
  },
  {
    "text": "models uh you like best for this kind of thing i found 03 mini very good at this and then our prompt here we're saying",
    "start": "3304720",
    "end": "3310880"
  },
  {
    "text": "generate a report based on the following research data we've got two new lines and then we stringify the report",
    "start": "3310880",
    "end": "3317520"
  },
  {
    "text": "returning the final generated text so let's actually refactor our main",
    "start": "3317520",
    "end": "3324319"
  },
  {
    "text": "function very quickly to use this new",
    "start": "3324319",
    "end": "3329599"
  },
  {
    "text": "uh this new function so what we're doing here uh we've just inlined our prompt so we say uh what do you need to to be a D1",
    "start": "3329599",
    "end": "3337920"
  },
  {
    "text": "shop athlete we log out some status updates and then we pass that research into the generate report function await",
    "start": "3337920",
    "end": "3346000"
  },
  {
    "text": "it and get out a report which will finally write to the file system to a markdown file so I'm going to import fs",
    "start": "3346000",
    "end": "3353200"
  },
  {
    "text": "from fs and then we're ready to go so let's see let's see what this does",
    "start": "3353200",
    "end": "3362440"
  },
  {
    "text": "great so after about a minute we've got our report below is an integrated report",
    "start": "3372319",
    "end": "3378319"
  },
  {
    "text": "summarizing the research findings on what it takes to become a division one shotput athlete along with a with",
    "start": "3378319",
    "end": "3384480"
  },
  {
    "text": "related insights on technique training and common beginner pitfalls so okay we've got our our entire report here um",
    "start": "3384480",
    "end": "3393520"
  },
  {
    "text": "and to be honest given all of the information that we gave it it's it's pretty it's pretty great but one thing",
    "start": "3393520",
    "end": "3401440"
  },
  {
    "text": "that we'll note here is that we didn't give the model any guidance on what exactly we wanted this report to look",
    "start": "3401440",
    "end": "3407359"
  },
  {
    "text": "like and so the model had to infer um and when when you're working with",
    "start": "3407359",
    "end": "3412799"
  },
  {
    "text": "language models in order to get the best response you want to leave as little up to the model to infer as possible so one",
    "start": "3412799",
    "end": "3422000"
  },
  {
    "text": "thing I want this to be in a markdown format two I'd like it to to form a bit more of a structure um that I want to",
    "start": "3422000",
    "end": "3429280"
  },
  {
    "text": "specify and so what we're going to do here is we're going to head back to our uh generate research function and we're",
    "start": "3429280",
    "end": "3436160"
  },
  {
    "text": "going to do two things we're going to actually just one thing sorry we're going to create a system prompt we're",
    "start": "3436160",
    "end": "3443119"
  },
  {
    "text": "going to tell it give it a persona you are an expert researcher we're going to give it today's date we're going to tell it to follow these instructions exactly",
    "start": "3443119",
    "end": "3450480"
  },
  {
    "text": "um and a few key things in here we're going to say use markdown formatting you may use high levels of",
    "start": "3450480",
    "end": "3457280"
  },
  {
    "text": "speculation or prediction just flag it um and and in general we just give it",
    "start": "3457280",
    "end": "3463200"
  },
  {
    "text": "these these uh guidelines that are very research uh analyst oriented so I'm",
    "start": "3463200",
    "end": "3470640"
  },
  {
    "text": "going to run this entire thing again and we're going to see the difference in the output",
    "start": "3470640",
    "end": "3477480"
  },
  {
    "text": "and here it is we have our new report so let's jump into it and what can we notice right off the bat we're using",
    "start": "3492240",
    "end": "3498559"
  },
  {
    "text": "markdown which is awesome much more structured uh we can see that we've got",
    "start": "3498559",
    "end": "3504480"
  },
  {
    "text": "date and line kind of helpful to have um but like the the level of quality of",
    "start": "3504480",
    "end": "3511760"
  },
  {
    "text": "this kind of report even just with this basic workflow really astounds me so you can see to be considered like right at",
    "start": "3511760",
    "end": "3518559"
  },
  {
    "text": "the top level to be considered a division one shot pro prospect athletes are expected to demonstrate a high level",
    "start": "3518559",
    "end": "3524240"
  },
  {
    "text": "of competitive success varsity experience um typically four years of varsity participation competitive",
    "start": "3524240",
    "end": "3531119"
  },
  {
    "text": "exposure uh we have performance benchmarks a benchmark throw of 55 ft",
    "start": "3531119",
    "end": "3536160"
  },
  {
    "text": "elite or top tier recruits tend to have distances around 60 feet and 18 inches like this is uh and the difference",
    "start": "3536160",
    "end": "3543119"
  },
  {
    "text": "between men's and women it it is so so so cool how uh we were able to build",
    "start": "3543119",
    "end": "3548799"
  },
  {
    "text": "this in just 218 lines of code um so",
    "start": "3548799",
    "end": "3554640"
  },
  {
    "text": "yeah this is this is it this is a session i hope you enjoyed this if you have any questions you can reach me on",
    "start": "3554640",
    "end": "3562720"
  },
  {
    "text": "um X the everything platform at niko albanese10 uh feel free to send me a DM",
    "start": "3562720",
    "end": "3569839"
  },
  {
    "text": "uh if you have any questions on building with the SDK head to",
    "start": "3569839",
    "end": "3575079"
  },
  {
    "text": "SDK.purcell.ai check out our docs we got some awesome uh guides in the cookbook as well um and yeah I hope you enjoyed",
    "start": "3575079",
    "end": "3582160"
  },
  {
    "text": "this i want to thank Swix for asking me to to do this uh session and I really",
    "start": "3582160",
    "end": "3587839"
  },
  {
    "text": "hope you enjoyed it and hope to see you at the next one take care",
    "start": "3587839",
    "end": "3593200"
  }
]