[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "[Music]",
    "start": "350",
    "end": "13679"
  },
  {
    "text": "hello everyone um so today I'm going to",
    "start": "13679",
    "end": "15639"
  },
  {
    "text": "talk about how to fix bugs and open",
    "start": "15639",
    "end": "17840"
  },
  {
    "text": "source models um thanks for coming again",
    "start": "17840",
    "end": "20359"
  },
  {
    "text": "um we had a talk yesterday the 3-hour",
    "start": "20359",
    "end": "22519"
  },
  {
    "text": "workshop um and thanks for coming again",
    "start": "22519",
    "end": "25720"
  },
  {
    "text": "um so we have slides it's at tinyurl.com",
    "start": "25720",
    "end": "30199"
  },
  {
    "text": "to for the workshop slides which we did",
    "start": "30199",
    "end": "32238"
  },
  {
    "start": "32000",
    "end": "135000"
  },
  {
    "text": "yesterday um you can also access that",
    "start": "32239",
    "end": "34239"
  },
  {
    "text": "now tinyurl.com unof as",
    "start": "34239",
    "end": "37520"
  },
  {
    "text": "well so you might know me from like the",
    "start": "37520",
    "end": "41039"
  },
  {
    "text": "Gemma bug fixes that we did so Gemma was",
    "start": "41039",
    "end": "42960"
  },
  {
    "text": "an open source model by Google and there",
    "start": "42960",
    "end": "44680"
  },
  {
    "text": "like a few bugs in there and we fixed a",
    "start": "44680",
    "end": "46879"
  },
  {
    "text": "few of them um and we just did some",
    "start": "46879",
    "end": "49520"
  },
  {
    "text": "tweets about this and you know like",
    "start": "49520",
    "end": "51760"
  },
  {
    "text": "there's many bugs in there like you know",
    "start": "51760",
    "end": "53480"
  },
  {
    "text": "the activation function had to be ex um",
    "start": "53480",
    "end": "56079"
  },
  {
    "text": "approximate jel not exact jelo and there",
    "start": "56079",
    "end": "58840"
  },
  {
    "text": "are some other issues that we talked",
    "start": "58840",
    "end": "60359"
  },
  {
    "text": "about for",
    "start": "60359",
    "end": "61399"
  },
  {
    "text": "Gemma we also have like some a few",
    "start": "61399",
    "end": "63440"
  },
  {
    "text": "stickers which you can you know get them",
    "start": "63440",
    "end": "65518"
  },
  {
    "text": "from like when we're outside um but this",
    "start": "65519",
    "end": "68159"
  },
  {
    "text": "is yeah we won't be handing them during",
    "start": "68159",
    "end": "70040"
  },
  {
    "text": "the talk um but yeah they're very cool",
    "start": "70040",
    "end": "72280"
  },
  {
    "text": "and cute um and also there's like",
    "start": "72280",
    "end": "74520"
  },
  {
    "text": "tokenization problems as well in",
    "start": "74520",
    "end": "76080"
  },
  {
    "text": "language models which we also help to",
    "start": "76080",
    "end": "78320"
  },
  {
    "text": "fix today I'm just going to be talking",
    "start": "78320",
    "end": "80799"
  },
  {
    "text": "about llama 3 bogs so yesterday I talked",
    "start": "80799",
    "end": "82960"
  },
  {
    "text": "about Gemma and V3 and today we're just",
    "start": "82960",
    "end": "85479"
  },
  {
    "text": "sharing all the stuff that we found with",
    "start": "85479",
    "end": "86880"
  },
  {
    "text": "llama",
    "start": "86880",
    "end": "88000"
  },
  {
    "text": "3 for JMA you can access like all the",
    "start": "88000",
    "end": "90640"
  },
  {
    "text": "bug fixes that we did um in our blog",
    "start": "90640",
    "end": "92920"
  },
  {
    "text": "post and we have a collab notebook for",
    "start": "92920",
    "end": "95320"
  },
  {
    "text": "all of the J bug fixes as",
    "start": "95320",
    "end": "97240"
  },
  {
    "text": "well for fe3 for example um we talked",
    "start": "97240",
    "end": "100520"
  },
  {
    "text": "about this yesterday um and I just",
    "start": "100520",
    "end": "103399"
  },
  {
    "text": "pasted some slides again if you want to",
    "start": "103399",
    "end": "104880"
  },
  {
    "text": "like review this in your own time um for",
    "start": "104880",
    "end": "107399"
  },
  {
    "text": "example like the sliding window should",
    "start": "107399",
    "end": "108759"
  },
  {
    "text": "be 2048 not",
    "start": "108759",
    "end": "111240"
  },
  {
    "text": "2047 um you can also like you should",
    "start": "111240",
    "end": "114119"
  },
  {
    "text": "unfuse all of the qkv matrices otherwise",
    "start": "114119",
    "end": "116960"
  },
  {
    "text": "lurine tuning will not work that well",
    "start": "116960",
    "end": "120520"
  },
  {
    "text": "but we'll be talking mainly about llama",
    "start": "120520",
    "end": "122079"
  },
  {
    "text": "3 um so there's actually eight bugs in",
    "start": "122079",
    "end": "124479"
  },
  {
    "text": "llama 3 we some of them are not",
    "start": "124479",
    "end": "126560"
  },
  {
    "text": "announced yet um we will be announcing",
    "start": "126560",
    "end": "128280"
  },
  {
    "text": "these later um so this is like a",
    "start": "128280",
    "end": "130239"
  },
  {
    "text": "pre-release um and we'll be going",
    "start": "130239",
    "end": "132560"
  },
  {
    "text": "through each of them",
    "start": "132560",
    "end": "134160"
  },
  {
    "text": "separately the first one is you must not",
    "start": "134160",
    "end": "136519"
  },
  {
    "start": "135000",
    "end": "180000"
  },
  {
    "text": "use double BOS tokens um so this is",
    "start": "136519",
    "end": "139640"
  },
  {
    "text": "actually a very common theme in fine",
    "start": "139640",
    "end": "140920"
  },
  {
    "text": "shoting Lama 3 some people don't",
    "start": "140920",
    "end": "143239"
  },
  {
    "text": "actually know that you're adding two",
    "start": "143239",
    "end": "144560"
  },
  {
    "text": "beginning of sentence tokens to the fine",
    "start": "144560",
    "end": "146239"
  },
  {
    "text": "tune and this will actually ruin your",
    "start": "146239",
    "end": "147879"
  },
  {
    "text": "fine tune um by making the accuracy of",
    "start": "147879",
    "end": "150760"
  },
  {
    "text": "your inference time lower um so please",
    "start": "150760",
    "end": "152920"
  },
  {
    "text": "like check before you find tune if",
    "start": "152920",
    "end": "154200"
  },
  {
    "text": "you're using double BOS tokens um in",
    "start": "154200",
    "end": "156480"
  },
  {
    "text": "unsoft we do this we check this",
    "start": "156480",
    "end": "158080"
  },
  {
    "text": "automatically and we'll remove the extra",
    "start": "158080",
    "end": "160159"
  },
  {
    "text": "BOS token automatically for you so this",
    "start": "160159",
    "end": "162640"
  },
  {
    "text": "will actually cause your model to lose",
    "start": "162640",
    "end": "164360"
  },
  {
    "text": "accuracy because if you trained on two",
    "start": "164360",
    "end": "166760"
  },
  {
    "text": "BOS tokens and you do inference on one",
    "start": "166760",
    "end": "169080"
  },
  {
    "text": "then your model template will be",
    "start": "169080",
    "end": "170360"
  },
  {
    "text": "incorrect um so please check this it's",
    "start": "170360",
    "end": "173040"
  },
  {
    "text": "not just a llama 3 problem other models",
    "start": "173040",
    "end": "175480"
  },
  {
    "text": "like mistro and GMA also have problems",
    "start": "175480",
    "end": "177239"
  },
  {
    "text": "like this so just be careful of this",
    "start": "177239",
    "end": "179080"
  },
  {
    "text": "issue",
    "start": "179080",
    "end": "181159"
  },
  {
    "start": "180000",
    "end": "410000"
  },
  {
    "text": "so a very easy way to check if you have",
    "start": "181159",
    "end": "183560"
  },
  {
    "text": "double BOS tokens if you use the apply",
    "start": "183560",
    "end": "185560"
  },
  {
    "text": "chat template um from hugging face if",
    "start": "185560",
    "end": "188799"
  },
  {
    "text": "you do the first one your chat template",
    "start": "188799",
    "end": "191000"
  },
  {
    "text": "must have a BOS token otherwise it won't",
    "start": "191000",
    "end": "192760"
  },
  {
    "text": "add it um llama 3 does require a BOS",
    "start": "192760",
    "end": "195200"
  },
  {
    "text": "token if you do the second one you're",
    "start": "195200",
    "end": "197760"
  },
  {
    "text": "actually having two BOS tokens if you do",
    "start": "197760",
    "end": "200000"
  },
  {
    "text": "this so please do not add a BOS token to",
    "start": "200000",
    "end": "202120"
  },
  {
    "text": "the chat",
    "start": "202120",
    "end": "203560"
  },
  {
    "text": "template the second issue we found was",
    "start": "203560",
    "end": "206040"
  },
  {
    "text": "you must not use the Llama 3 base model",
    "start": "206040",
    "end": "209439"
  },
  {
    "text": "if you're using the Llama 3 template um",
    "start": "209439",
    "end": "211720"
  },
  {
    "text": "there are some untrained tokens in llama",
    "start": "211720",
    "end": "213560"
  },
  {
    "text": "3 instru um in llama 3 base the instruct",
    "start": "213560",
    "end": "216120"
  },
  {
    "text": "version actually has these tokens",
    "start": "216120",
    "end": "217560"
  },
  {
    "text": "trained so please be careful when you",
    "start": "217560",
    "end": "220000"
  },
  {
    "text": "want to use the Llama 3 base model when",
    "start": "220000",
    "end": "222080"
  },
  {
    "text": "you want to do your fine tuning because",
    "start": "222080",
    "end": "223640"
  },
  {
    "text": "some of these tokens will cause nans for",
    "start": "223640",
    "end": "225200"
  },
  {
    "text": "your gradients these tokens include the",
    "start": "225200",
    "end": "227239"
  },
  {
    "text": "reserve special tokens from 0 to 250 the",
    "start": "227239",
    "end": "230640"
  },
  {
    "text": "um end of the end of turn token the",
    "start": "230640",
    "end": "232959"
  },
  {
    "text": "start header and the end of header um",
    "start": "232959",
    "end": "235079"
  },
  {
    "text": "and the graph I showed shows you the um",
    "start": "235079",
    "end": "237560"
  },
  {
    "text": "mean of the embeddings versus the other",
    "start": "237560",
    "end": "240159"
  },
  {
    "text": "tokens and some of them actually are",
    "start": "240159",
    "end": "241840"
  },
  {
    "text": "zero so the Llama 3 team made some of",
    "start": "241840",
    "end": "244280"
  },
  {
    "text": "these tokens go to zero purposefully",
    "start": "244280",
    "end": "246879"
  },
  {
    "text": "because these tokens are not actually",
    "start": "246879",
    "end": "248239"
  },
  {
    "text": "used for the model um so just please",
    "start": "248239",
    "end": "250439"
  },
  {
    "text": "don't use these some of these tokens",
    "start": "250439",
    "end": "252439"
  },
  {
    "text": "when you do fine tuning as well um if",
    "start": "252439",
    "end": "254519"
  },
  {
    "text": "you want to fix them set them to the",
    "start": "254519",
    "end": "256160"
  },
  {
    "text": "mean of the entire",
    "start": "256160",
    "end": "258560"
  },
  {
    "text": "tokens and in unof we do this",
    "start": "258560",
    "end": "260720"
  },
  {
    "text": "automatically as well for you um so we",
    "start": "260720",
    "end": "262680"
  },
  {
    "text": "showed some code where you can take the",
    "start": "262680",
    "end": "264080"
  },
  {
    "text": "mean of the um train tokens and set them",
    "start": "264080",
    "end": "267440"
  },
  {
    "text": "for the untrained tokens just be careful",
    "start": "267440",
    "end": "269919"
  },
  {
    "text": "don't do this like incorrectly as well",
    "start": "269919",
    "end": "272680"
  },
  {
    "text": "if you want to take the average of all",
    "start": "272680",
    "end": "273960"
  },
  {
    "text": "the tokens don't just take the average",
    "start": "273960",
    "end": "275360"
  },
  {
    "text": "you must remove the untrained tokens",
    "start": "275360",
    "end": "277400"
  },
  {
    "text": "from the average if you do not do that",
    "start": "277400",
    "end": "279400"
  },
  {
    "text": "you might actually have an incorrect",
    "start": "279400",
    "end": "280800"
  },
  {
    "text": "average right if there's like 10,000",
    "start": "280800",
    "end": "282720"
  },
  {
    "text": "tokens which are untrained if you divide",
    "start": "282720",
    "end": "284560"
  },
  {
    "text": "it by 10,000 plus the number of trained",
    "start": "284560",
    "end": "287199"
  },
  {
    "text": "tokens your average will be incorrect so",
    "start": "287199",
    "end": "290000"
  },
  {
    "text": "um you have to do this more complicated",
    "start": "290000",
    "end": "291600"
  },
  {
    "text": "method of masking out the untrain tokens",
    "start": "291600",
    "end": "294400"
  },
  {
    "text": "and then take the",
    "start": "294400",
    "end": "296199"
  },
  {
    "text": "average also reminder because of this",
    "start": "296199",
    "end": "298960"
  },
  {
    "text": "issue the Lama 3 chat template will not",
    "start": "298960",
    "end": "301039"
  },
  {
    "text": "work for the base model I've known like",
    "start": "301039",
    "end": "303440"
  },
  {
    "text": "many fine tuning people have used llama",
    "start": "303440",
    "end": "305320"
  },
  {
    "text": "3 instruct um chat template for the base",
    "start": "305320",
    "end": "308160"
  },
  {
    "text": "model and your fine tune will actually",
    "start": "308160",
    "end": "309720"
  },
  {
    "text": "be incorrect um you will get nans in",
    "start": "309720",
    "end": "311960"
  },
  {
    "text": "your gradients and your whole fine tune",
    "start": "311960",
    "end": "313520"
  },
  {
    "text": "will be broken so please do not use the",
    "start": "313520",
    "end": "315680"
  },
  {
    "text": "Llama 3 instruct chat template for the",
    "start": "315680",
    "end": "317960"
  },
  {
    "text": "Llama 3 base model only use this for the",
    "start": "317960",
    "end": "320120"
  },
  {
    "text": "instruct model itself another way to fix",
    "start": "320120",
    "end": "322440"
  },
  {
    "text": "this is to actually train the LM head",
    "start": "322440",
    "end": "324160"
  },
  {
    "text": "and the embed tokens which will actually",
    "start": "324160",
    "end": "326039"
  },
  {
    "text": "learn and remove the nans in your models",
    "start": "326039",
    "end": "331199"
  },
  {
    "text": "another interesting fact and not just a",
    "start": "331199",
    "end": "332800"
  },
  {
    "text": "llama 3 problem but for other models is",
    "start": "332800",
    "end": "335720"
  },
  {
    "text": "the pad token and the US token um must",
    "start": "335720",
    "end": "338319"
  },
  {
    "text": "not be the same if you do this the same",
    "start": "338319",
    "end": "341080"
  },
  {
    "text": "your model will have infinite",
    "start": "341080",
    "end": "343000"
  },
  {
    "text": "Generations um the reason is because the",
    "start": "343000",
    "end": "345039"
  },
  {
    "text": "pad token gets masked out during the",
    "start": "345039",
    "end": "346880"
  },
  {
    "text": "error um during the cross enty loss and",
    "start": "346880",
    "end": "349360"
  },
  {
    "text": "if you use the same pad token as the E",
    "start": "349360",
    "end": "352120"
  },
  {
    "text": "token then your e token the end of",
    "start": "352120",
    "end": "353960"
  },
  {
    "text": "sentence token will be masked out so",
    "start": "353960",
    "end": "356039"
  },
  {
    "text": "just be very very careful when you doine",
    "start": "356039",
    "end": "357960"
  },
  {
    "text": "tuning to check what is the pad token",
    "start": "357960",
    "end": "359639"
  },
  {
    "text": "token ID and the US token ID for example",
    "start": "359639",
    "end": "362639"
  },
  {
    "text": "if you look at feed3 they're the same so",
    "start": "362639",
    "end": "365520"
  },
  {
    "text": "technically feed3 when you do fine",
    "start": "365520",
    "end": "367759"
  },
  {
    "text": "tuning it will be infinite Generations",
    "start": "367759",
    "end": "370120"
  },
  {
    "text": "so just be careful and look you know",
    "start": "370120",
    "end": "372280"
  },
  {
    "text": "before you do the fine tune check what",
    "start": "372280",
    "end": "374039"
  },
  {
    "text": "is the EOS token and what is a pad token",
    "start": "374039",
    "end": "375919"
  },
  {
    "text": "they must be",
    "start": "375919",
    "end": "378120"
  },
  {
    "text": "different for unof we also do this",
    "start": "378120",
    "end": "380560"
  },
  {
    "text": "automatically we fix this for you um and",
    "start": "380560",
    "end": "383240"
  },
  {
    "text": "we essentially check if there is any",
    "start": "383240",
    "end": "385160"
  },
  {
    "text": "unreserved if there is any unreserved",
    "start": "385160",
    "end": "387280"
  },
  {
    "text": "tokens and we just select one which is",
    "start": "387280",
    "end": "389000"
  },
  {
    "text": "untrained if there is no untrained",
    "start": "389000",
    "end": "391039"
  },
  {
    "text": "tokens then we will add an extra pad",
    "start": "391039",
    "end": "392919"
  },
  {
    "text": "token ourselves um be careful do not add",
    "start": "392919",
    "end": "395599"
  },
  {
    "text": "a pad token which has the same like",
    "start": "395599",
    "end": "397960"
  },
  {
    "text": "vocabulary as your current vocabulary so",
    "start": "397960",
    "end": "400800"
  },
  {
    "text": "what we do is we actually check the",
    "start": "400800",
    "end": "402000"
  },
  {
    "text": "tokens inside the vocabulary and add",
    "start": "402000",
    "end": "404240"
  },
  {
    "text": "like extra hashes to see you know to",
    "start": "404240",
    "end": "406720"
  },
  {
    "text": "make a new pad",
    "start": "406720",
    "end": "409360"
  },
  {
    "text": "token another issue we found for fine",
    "start": "409840",
    "end": "412080"
  },
  {
    "text": "shooting people is like when you finish",
    "start": "412080",
    "end": "413440"
  },
  {
    "text": "a fine tune you don't actually know how",
    "start": "413440",
    "end": "415720"
  },
  {
    "text": "to export it to olama and that is",
    "start": "415720",
    "end": "417720"
  },
  {
    "text": "because the chat template for olama must",
    "start": "417720",
    "end": "419319"
  },
  {
    "text": "be exactly the same as your fine tune um",
    "start": "419319",
    "end": "421840"
  },
  {
    "text": "and this was actually very complicated",
    "start": "421840",
    "end": "423280"
  },
  {
    "text": "to do before and now we can actually",
    "start": "423280",
    "end": "425479"
  },
  {
    "text": "automatically generate the model file",
    "start": "425479",
    "end": "427000"
  },
  {
    "text": "for you during the fine tune so we have",
    "start": "427000",
    "end": "429680"
  },
  {
    "text": "like two collab notebooks for you to use",
    "start": "429680",
    "end": "431280"
  },
  {
    "text": "for AMA um one of them is the alpaca",
    "start": "431280",
    "end": "433919"
  },
  {
    "text": "data set and one of them's a you can",
    "start": "433919",
    "end": "435879"
  },
  {
    "text": "upload a CSV file to make um a Lama work",
    "start": "435879",
    "end": "439319"
  },
  {
    "text": "after you finish fine",
    "start": "439319",
    "end": "441479"
  },
  {
    "text": "tuning now there are some Community",
    "start": "441479",
    "end": "443599"
  },
  {
    "text": "contributions for llama 3 U bugs um",
    "start": "443599",
    "end": "446879"
  },
  {
    "text": "there is like three of them the first",
    "start": "446879",
    "end": "448879"
  },
  {
    "text": "one is someone notice that you can only",
    "start": "448879",
    "end": "450520"
  },
  {
    "text": "use CPU conversion and not GPU",
    "start": "450520",
    "end": "452360"
  },
  {
    "text": "conversion when you convert to ggw or",
    "start": "452360",
    "end": "454680"
  },
  {
    "text": "llama CPP um so be you know be careful",
    "start": "454680",
    "end": "457199"
  },
  {
    "text": "when you convert to llama CPP that you",
    "start": "457199",
    "end": "459720"
  },
  {
    "text": "must use the CPU version um I think the",
    "start": "459720",
    "end": "461720"
  },
  {
    "text": "main reason is because the Precision is",
    "start": "461720",
    "end": "463919"
  },
  {
    "text": "different in a GPU than a CPU the CPU",
    "start": "463919",
    "end": "466680"
  },
  {
    "text": "when you do float 16 it's different from",
    "start": "466680",
    "end": "468879"
  },
  {
    "text": "when the GPU does float 16 conversion so",
    "start": "468879",
    "end": "470879"
  },
  {
    "text": "just be careful on that as",
    "start": "470879",
    "end": "473199"
  },
  {
    "text": "well another issue is remember we talked",
    "start": "473199",
    "end": "475599"
  },
  {
    "text": "about the double BOS tokens um through",
    "start": "475599",
    "end": "477639"
  },
  {
    "text": "our community contribution um llama CPP",
    "start": "477639",
    "end": "480840"
  },
  {
    "text": "now has a warning for you to tell you",
    "start": "480840",
    "end": "483039"
  },
  {
    "text": "that you're using wbos tokens so please",
    "start": "483039",
    "end": "485560"
  },
  {
    "text": "you know take heed of the warning and do",
    "start": "485560",
    "end": "487800"
  },
  {
    "text": "not add double BOS tokens to your chat",
    "start": "487800",
    "end": "490000"
  },
  {
    "text": "template and when you do",
    "start": "490000",
    "end": "492879"
  },
  {
    "text": "inference another point someone found",
    "start": "492879",
    "end": "495199"
  },
  {
    "text": "was adding a system prompt could make",
    "start": "495199",
    "end": "497280"
  },
  {
    "text": "fine tuning much better um and so like",
    "start": "497280",
    "end": "500120"
  },
  {
    "text": "sometimes when you do inference on Lama",
    "start": "500120",
    "end": "502319"
  },
  {
    "text": "3 instruct if you add a actual system",
    "start": "502319",
    "end": "504120"
  },
  {
    "text": "prompt this could make your whole fine",
    "start": "504120",
    "end": "505840"
  },
  {
    "text": "tuning better um I think for some people",
    "start": "505840",
    "end": "508479"
  },
  {
    "text": "when they add the system prompt you",
    "start": "508479",
    "end": "509840"
  },
  {
    "text": "actually miss the system prompt like you",
    "start": "509840",
    "end": "511319"
  },
  {
    "text": "don't actually add one and so maybe try",
    "start": "511319",
    "end": "513039"
  },
  {
    "text": "your fine tune with the system prompt",
    "start": "513039",
    "end": "515000"
  },
  {
    "text": "and you never know this could",
    "start": "515000",
    "end": "517880"
  },
  {
    "text": "work so we have like a GitHub package um",
    "start": "517880",
    "end": "520640"
  },
  {
    "start": "518000",
    "end": "640000"
  },
  {
    "text": "which is open source um and you can",
    "start": "520640",
    "end": "523159"
  },
  {
    "text": "click the button start free fine tune to",
    "start": "523159",
    "end": "524680"
  },
  {
    "text": "start your first free fin tune using",
    "start": "524680",
    "end": "526040"
  },
  {
    "text": "unso we already pushed all the LMA 3 bug",
    "start": "526040",
    "end": "528560"
  },
  {
    "text": "fixes to our GitHub repo and so the",
    "start": "528560",
    "end": "530600"
  },
  {
    "text": "start free fine tune button will",
    "start": "530600",
    "end": "532320"
  },
  {
    "text": "redirect you to a fixed collab notebook",
    "start": "532320",
    "end": "534480"
  },
  {
    "text": "for all of these issues um feel free to",
    "start": "534480",
    "end": "537000"
  },
  {
    "text": "Star us as well we also like have a",
    "start": "537000",
    "end": "538920"
  },
  {
    "text": "Discord Channel so if you have any",
    "start": "538920",
    "end": "540040"
  },
  {
    "text": "questions um you can ask you know any",
    "start": "540040",
    "end": "542240"
  },
  {
    "text": "question that you like about our you",
    "start": "542240",
    "end": "544360"
  },
  {
    "text": "know how to define tuning talk about Ai",
    "start": "544360",
    "end": "547320"
  },
  {
    "text": "and talk about our bugs as",
    "start": "547320",
    "end": "548959"
  },
  {
    "text": "well we also have like a blog post so",
    "start": "548959",
    "end": "551800"
  },
  {
    "text": "blog posts about all our fixes about",
    "start": "551800",
    "end": "554240"
  },
  {
    "text": "Gemma um llama 3 V3 and more um for",
    "start": "554240",
    "end": "558040"
  },
  {
    "text": "example we talked about continued",
    "start": "558040",
    "end": "559320"
  },
  {
    "text": "pre-training you can do continued",
    "start": "559320",
    "end": "560600"
  },
  {
    "text": "pre-training using unso now we you can",
    "start": "560600",
    "end": "563200"
  },
  {
    "text": "train on the LM head and the embed",
    "start": "563200",
    "end": "564800"
  },
  {
    "text": "tokens and we show that instead of just",
    "start": "564800",
    "end": "567200"
  },
  {
    "text": "training like that you need to reduce",
    "start": "567200",
    "end": "568880"
  },
  {
    "text": "the learning rate of the LM head and the",
    "start": "568880",
    "end": "570480"
  },
  {
    "text": "embed tokens by 10 or you know maybe 5",
    "start": "570480",
    "end": "573160"
  },
  {
    "text": "to 10 and this will make your training",
    "start": "573160",
    "end": "574760"
  },
  {
    "text": "much better we also support four times",
    "start": "574760",
    "end": "576880"
  },
  {
    "text": "longer context using onso um and this",
    "start": "576880",
    "end": "579839"
  },
  {
    "text": "also does not increase the time uh the",
    "start": "579839",
    "end": "582399"
  },
  {
    "text": "time of completion so we make it uh one",
    "start": "582399",
    "end": "585079"
  },
  {
    "text": "to 2% slower but you get four times",
    "start": "585079",
    "end": "587640"
  },
  {
    "text": "longer contacts using unso um and this",
    "start": "587640",
    "end": "590519"
  },
  {
    "text": "this was because like we used something",
    "start": "590519",
    "end": "591920"
  },
  {
    "text": "called um offloading gradient",
    "start": "591920",
    "end": "593560"
  },
  {
    "text": "checkpointing where we offload the",
    "start": "593560",
    "end": "595240"
  },
  {
    "text": "gradients to system Ram there are some",
    "start": "595240",
    "end": "597440"
  },
  {
    "text": "other systems which offload the",
    "start": "597440",
    "end": "598640"
  },
  {
    "text": "gradients to the dis please do not do",
    "start": "598640",
    "end": "601040"
  },
  {
    "text": "that if you offload to dis then your",
    "start": "601040",
    "end": "603040"
  },
  {
    "text": "your time of completion of your fine",
    "start": "603040",
    "end": "604360"
  },
  {
    "text": "tune will be extremely slow um so try to",
    "start": "604360",
    "end": "606920"
  },
  {
    "text": "offload to a system Ram first and then",
    "start": "606920",
    "end": "608760"
  },
  {
    "text": "offload to disk although if you don't if",
    "start": "608760",
    "end": "611360"
  },
  {
    "text": "you offload incorrectly you might",
    "start": "611360",
    "end": "613160"
  },
  {
    "text": "actually make this slower as well so",
    "start": "613160",
    "end": "614680"
  },
  {
    "text": "your offloading must be non-blocking",
    "start": "614680",
    "end": "616360"
  },
  {
    "text": "calls and do not do blocking calls to",
    "start": "616360",
    "end": "618360"
  },
  {
    "text": "the system",
    "start": "618360",
    "end": "620560"
  },
  {
    "text": "Ram yeah so I will show you um okay",
    "start": "620560",
    "end": "625240"
  },
  {
    "text": "let's see if I can open up um let me go",
    "start": "625240",
    "end": "631320"
  },
  {
    "text": "to this",
    "start": "633920",
    "end": "637200"
  },
  {
    "text": "work okay I'm going to open up a collab",
    "start": "638720",
    "end": "640959"
  },
  {
    "start": "640000",
    "end": "1061000"
  },
  {
    "text": "notbook um for the oama one um",
    "start": "640959",
    "end": "647000"
  },
  {
    "text": "yes okay so for the olama collab",
    "start": "647959",
    "end": "651600"
  },
  {
    "text": "notebook you can simply just install",
    "start": "651600",
    "end": "653880"
  },
  {
    "text": "unso over here um this is already for",
    "start": "653880",
    "end": "655920"
  },
  {
    "text": "free for everyone to use um and",
    "start": "655920",
    "end": "658480"
  },
  {
    "text": "essentially don't for get when you do",
    "start": "658480",
    "end": "659760"
  },
  {
    "text": "the Cod notebook you have to select a",
    "start": "659760",
    "end": "661360"
  },
  {
    "text": "Max sequence length um this this",
    "start": "661360",
    "end": "664000"
  },
  {
    "text": "determines how long your model wants to",
    "start": "664000",
    "end": "665600"
  },
  {
    "text": "do long context fine tuning um you can",
    "start": "665600",
    "end": "667800"
  },
  {
    "text": "set this to any number that you like but",
    "start": "667800",
    "end": "669720"
  },
  {
    "text": "remember your data set must match the",
    "start": "669720",
    "end": "671680"
  },
  {
    "text": "max sequence length so for example if",
    "start": "671680",
    "end": "673440"
  },
  {
    "text": "you have if you want to set the max",
    "start": "673440",
    "end": "674639"
  },
  {
    "text": "sequence length to like 10 million or 1",
    "start": "674639",
    "end": "676480"
  },
  {
    "text": "million um but your data set is only",
    "start": "676480",
    "end": "678399"
  },
  {
    "text": "like 1 million tokens or like less try",
    "start": "678399",
    "end": "680720"
  },
  {
    "text": "to like not set that Max sequence left",
    "start": "680720",
    "end": "682560"
  },
  {
    "text": "to be that large otherwise your model",
    "start": "682560",
    "end": "684040"
  },
  {
    "text": "cannot do fine tuning on Long sequence",
    "start": "684040",
    "end": "686279"
  },
  {
    "text": "um load in 4bit does four-bit training",
    "start": "686279",
    "end": "689320"
  },
  {
    "text": "so this actually reduces memory usage by",
    "start": "689320",
    "end": "690839"
  },
  {
    "text": "four times um if you do if you do it to",
    "start": "690839",
    "end": "693279"
  },
  {
    "text": "false your memory usage will explode so",
    "start": "693279",
    "end": "695800"
  },
  {
    "text": "please do not try false um especially on",
    "start": "695800",
    "end": "698519"
  },
  {
    "text": "a free collab Tesla T4 um if you do",
    "start": "698519",
    "end": "701480"
  },
  {
    "text": "false your memory usage might Skyrocket",
    "start": "701480",
    "end": "703560"
  },
  {
    "text": "to 16 GB so do not do that um you only",
    "start": "703560",
    "end": "707279"
  },
  {
    "text": "should do this if you use more stronger",
    "start": "707279",
    "end": "709600"
  },
  {
    "text": "gpus we support like unsoft supports",
    "start": "709600",
    "end": "712279"
  },
  {
    "text": "fine tuning or models including like",
    "start": "712279",
    "end": "714320"
  },
  {
    "text": "llama mistro Gemma V3 and more um so",
    "start": "714320",
    "end": "717920"
  },
  {
    "text": "this area like the model name name over",
    "start": "717920",
    "end": "719680"
  },
  {
    "text": "here you can actually try to select any",
    "start": "719680",
    "end": "721600"
  },
  {
    "text": "model name that you like um I don't",
    "start": "721600",
    "end": "723320"
  },
  {
    "text": "think so people know that unso can",
    "start": "723320",
    "end": "724880"
  },
  {
    "text": "support other models other than the ones",
    "start": "724880",
    "end": "726560"
  },
  {
    "text": "we listed so please try to put any model",
    "start": "726560",
    "end": "729399"
  },
  {
    "text": "like a hugging face model name in there",
    "start": "729399",
    "end": "731240"
  },
  {
    "text": "um and it should",
    "start": "731240",
    "end": "733760"
  },
  {
    "text": "work so for the get PFT model this is",
    "start": "734199",
    "end": "737040"
  },
  {
    "text": "where you add the PFT low",
    "start": "737040",
    "end": "738959"
  },
  {
    "text": "adapters the r is the rank so we said it",
    "start": "738959",
    "end": "741839"
  },
  {
    "text": "to be 16 but you can select any number",
    "start": "741839",
    "end": "743680"
  },
  {
    "text": "that you like for fine tuning um so we",
    "start": "743680",
    "end": "745880"
  },
  {
    "text": "suggest you normally to use powers of",
    "start": "745880",
    "end": "747279"
  },
  {
    "text": "two but you can use any number like one",
    "start": "747279",
    "end": "749199"
  },
  {
    "text": "2 three like any number that you like",
    "start": "749199",
    "end": "751480"
  },
  {
    "text": "the larger the rank you select you can",
    "start": "751480",
    "end": "753360"
  },
  {
    "text": "make the model learn more about your",
    "start": "753360",
    "end": "755079"
  },
  {
    "text": "data set so but if you add two large of",
    "start": "755079",
    "end": "757399"
  },
  {
    "text": "a rank you might actually overfit your",
    "start": "757399",
    "end": "758800"
  },
  {
    "text": "data set and also your memory usage",
    "start": "758800",
    "end": "760360"
  },
  {
    "text": "might Skyrocket again so we normally",
    "start": "760360",
    "end": "762040"
  },
  {
    "text": "suggest people to select 16 32 64 or 128",
    "start": "762040",
    "end": "765800"
  },
  {
    "text": "try not to select two large ranks um the",
    "start": "765800",
    "end": "768279"
  },
  {
    "text": "maximum rank you should select is the",
    "start": "768279",
    "end": "769880"
  },
  {
    "text": "size of the dimension of the uh model",
    "start": "769880",
    "end": "772040"
  },
  {
    "text": "itself so if it's 496 set this to be",
    "start": "772040",
    "end": "776079"
  },
  {
    "text": "496 for the Target modules be careful um",
    "start": "776079",
    "end": "779600"
  },
  {
    "text": "you must do fine tuning on all linear",
    "start": "779600",
    "end": "782040"
  },
  {
    "text": "layers right so qkv down up and a gate",
    "start": "782040",
    "end": "785920"
  },
  {
    "text": "some people have done fine tuning",
    "start": "785920",
    "end": "787399"
  },
  {
    "text": "without doing some of these layers",
    "start": "787399",
    "end": "789399"
  },
  {
    "text": "please do not do that because this will",
    "start": "789399",
    "end": "791120"
  },
  {
    "text": "cause your fine tune to be not optimal",
    "start": "791120",
    "end": "793360"
  },
  {
    "text": "and the Lowa Alpha there is actually a",
    "start": "793360",
    "end": "794839"
  },
  {
    "text": "trick for this um normally speaking",
    "start": "794839",
    "end": "797240"
  },
  {
    "text": "select the alpha to be the same as a",
    "start": "797240",
    "end": "799199"
  },
  {
    "text": "rank or larger we found that if you do",
    "start": "799199",
    "end": "801639"
  },
  {
    "text": "16 * 2 so the rank times two this this",
    "start": "801639",
    "end": "804600"
  },
  {
    "text": "can make your fine treating much better",
    "start": "804600",
    "end": "806399"
  },
  {
    "text": "you can also use use RS lower to be true",
    "start": "806399",
    "end": "808800"
  },
  {
    "text": "to to set the rank automatically for you",
    "start": "808800",
    "end": "811639"
  },
  {
    "text": "to set the alpha automatically for you",
    "start": "811639",
    "end": "813519"
  },
  {
    "text": "for the gradient checkpointing UNS slof",
    "start": "813519",
    "end": "815160"
  },
  {
    "text": "is the method which we showed that you",
    "start": "815160",
    "end": "816639"
  },
  {
    "text": "can do long context fine tuning you can",
    "start": "816639",
    "end": "818560"
  },
  {
    "text": "also set this to be true um but your",
    "start": "818560",
    "end": "820720"
  },
  {
    "text": "memory usage will increase",
    "start": "820720",
    "end": "823199"
  },
  {
    "text": "again we also show you how to do data",
    "start": "823199",
    "end": "825560"
  },
  {
    "text": "preparation in aama collab notebook so",
    "start": "825560",
    "end": "828000"
  },
  {
    "text": "this one is we upload a Titanic CSV so",
    "start": "828000",
    "end": "830440"
  },
  {
    "text": "the Titanic data set the goal was can",
    "start": "830440",
    "end": "832399"
  },
  {
    "text": "you predict if someone died or survived",
    "start": "832399",
    "end": "834480"
  },
  {
    "text": "um if you're in the",
    "start": "834480",
    "end": "835800"
  },
  {
    "text": "Titanic and you get details about the",
    "start": "835800",
    "end": "838079"
  },
  {
    "text": "person for example the age their like",
    "start": "838079",
    "end": "840959"
  },
  {
    "text": "Fair where did they Embark from and so",
    "start": "840959",
    "end": "844040"
  },
  {
    "text": "on with our new cab notebook you have to",
    "start": "844040",
    "end": "846560"
  },
  {
    "text": "be very careful when you do Umama chat",
    "start": "846560",
    "end": "848759"
  },
  {
    "text": "templates um because when you do fine",
    "start": "848759",
    "end": "851079"
  },
  {
    "text": "tuning you can only have two columns the",
    "start": "851079",
    "end": "852639"
  },
  {
    "text": "instruction and the output but what",
    "start": "852639",
    "end": "854399"
  },
  {
    "text": "happens if your CSV has more than one",
    "start": "854399",
    "end": "856240"
  },
  {
    "text": "like more than two columns the",
    "start": "856240",
    "end": "857399"
  },
  {
    "text": "instruction and output what we can do is",
    "start": "857399",
    "end": "859199"
  },
  {
    "text": "you can merge The Columns into one",
    "start": "859199",
    "end": "860800"
  },
  {
    "text": "column and with unof now you can",
    "start": "860800",
    "end": "863000"
  },
  {
    "text": "actually do that you can merge The",
    "start": "863000",
    "end": "864279"
  },
  {
    "text": "Columns into one",
    "start": "864279",
    "end": "867839"
  },
  {
    "text": "and also we show you that you can do",
    "start": "869399",
    "end": "870720"
  },
  {
    "text": "customizable chat templates now so",
    "start": "870720",
    "end": "872759"
  },
  {
    "text": "previously if you want to do an alaca",
    "start": "872759",
    "end": "874399"
  },
  {
    "text": "style fine tune you have to use",
    "start": "874399",
    "end": "876160"
  },
  {
    "text": "instruction input and response for the",
    "start": "876160",
    "end": "877720"
  },
  {
    "text": "apaca style fine tuning but remember the",
    "start": "877720",
    "end": "880279"
  },
  {
    "text": "problem is if you want to Output to Ama",
    "start": "880279",
    "end": "882759"
  },
  {
    "text": "or ggw you can only have two columns the",
    "start": "882759",
    "end": "884759"
  },
  {
    "text": "instruction and output right if you do",
    "start": "884759",
    "end": "886560"
  },
  {
    "text": "chat TPT you you have to type something",
    "start": "886560",
    "end": "888800"
  },
  {
    "text": "and then the output comes along you",
    "start": "888800",
    "end": "890199"
  },
  {
    "text": "can't have like three inputs right so so",
    "start": "890199",
    "end": "893480"
  },
  {
    "text": "what we do is you can actually customize",
    "start": "893480",
    "end": "895839"
  },
  {
    "text": "your T chat template and you must",
    "start": "895839",
    "end": "897560"
  },
  {
    "text": "include the input and the output and you",
    "start": "897560",
    "end": "899759"
  },
  {
    "text": "must do this repetition twice um some",
    "start": "899759",
    "end": "902600"
  },
  {
    "text": "people have asked me like why do you",
    "start": "902600",
    "end": "903720"
  },
  {
    "text": "have to do two repetitions of this chat",
    "start": "903720",
    "end": "905519"
  },
  {
    "text": "template it's because there is dangling",
    "start": "905519",
    "end": "907320"
  },
  {
    "text": "new lines um and we found this we found",
    "start": "907320",
    "end": "910079"
  },
  {
    "text": "a solution to this is you have to",
    "start": "910079",
    "end": "911720"
  },
  {
    "text": "specify two iterations of your chat",
    "start": "911720",
    "end": "915000"
  },
  {
    "text": "template we also show examples of how to",
    "start": "915000",
    "end": "917320"
  },
  {
    "text": "do the Llama 3 chat template using our",
    "start": "917320",
    "end": "919000"
  },
  {
    "text": "methodology um so you can see there is",
    "start": "919000",
    "end": "921040"
  },
  {
    "text": "two iterations of the chat",
    "start": "921040",
    "end": "923480"
  },
  {
    "text": "template reminder if you don't use two",
    "start": "923480",
    "end": "925639"
  },
  {
    "text": "iterations you actually it will error",
    "start": "925639",
    "end": "927240"
  },
  {
    "text": "out",
    "start": "927240",
    "end": "929480"
  },
  {
    "text": "and this is the training methodologies",
    "start": "929480",
    "end": "931120"
  },
  {
    "text": "we normally suggest people to use a",
    "start": "931120",
    "end": "932480"
  },
  {
    "text": "batch size of two gradient accumulation",
    "start": "932480",
    "end": "934480"
  },
  {
    "text": "of four remember the memory usage is",
    "start": "934480",
    "end": "937079"
  },
  {
    "text": "only relevant to the batch size so try",
    "start": "937079",
    "end": "939160"
  },
  {
    "text": "not to set the batch size to be very",
    "start": "939160",
    "end": "940480"
  },
  {
    "text": "large otherwise your memory usage will",
    "start": "940480",
    "end": "941720"
  },
  {
    "text": "explode instead set your gradient",
    "start": "941720",
    "end": "943800"
  },
  {
    "text": "accumulation steps to be larger um so",
    "start": "943800",
    "end": "946399"
  },
  {
    "text": "the formula for the effective batch size",
    "start": "946399",
    "end": "948319"
  },
  {
    "text": "is batch size times the gradient",
    "start": "948319",
    "end": "949759"
  },
  {
    "text": "accumulation so in this case it's 2 * 4",
    "start": "949759",
    "end": "951759"
  },
  {
    "text": "which is 8 set your learning rate to be",
    "start": "951759",
    "end": "954160"
  },
  {
    "text": "2 e minus 4 or smaller maybe 2 eus 5",
    "start": "954160",
    "end": "959600"
  },
  {
    "text": "and after that you can also do inference",
    "start": "960480",
    "end": "962000"
  },
  {
    "text": "on the model um so now you have to use",
    "start": "962000",
    "end": "964120"
  },
  {
    "text": "the apply chat template remember be",
    "start": "964120",
    "end": "966000"
  },
  {
    "text": "careful of Double B tokens but we in",
    "start": "966000",
    "end": "967839"
  },
  {
    "text": "unsw fix",
    "start": "967839",
    "end": "970199"
  },
  {
    "text": "this and finally you have to save this",
    "start": "970199",
    "end": "972720"
  },
  {
    "text": "tolama um and you know you have to",
    "start": "972720",
    "end": "975040"
  },
  {
    "text": "install Lama first um saving now we now",
    "start": "975040",
    "end": "978240"
  },
  {
    "text": "support saving multiple ggw files so you",
    "start": "978240",
    "end": "980480"
  },
  {
    "text": "don't actually have to save it to one",
    "start": "980480",
    "end": "981639"
  },
  {
    "text": "ggw file you can save it to multiple and",
    "start": "981639",
    "end": "983519"
  },
  {
    "text": "we actually allow you to do this now um",
    "start": "983519",
    "end": "985560"
  },
  {
    "text": "before if you want to save to multiple",
    "start": "985560",
    "end": "987040"
  },
  {
    "text": "GG files you have to wait 10 minutes",
    "start": "987040",
    "end": "988920"
  },
  {
    "text": "minutes extra you can now do this",
    "start": "988920",
    "end": "990440"
  },
  {
    "text": "automatically by you know specifying",
    "start": "990440",
    "end": "992079"
  },
  {
    "text": "more than one",
    "start": "992079",
    "end": "994880"
  },
  {
    "text": "format we also can show you the model",
    "start": "994880",
    "end": "997000"
  },
  {
    "text": "file which we created so you can",
    "start": "997000",
    "end": "998240"
  },
  {
    "text": "actually copy paste the model file and",
    "start": "998240",
    "end": "999720"
  },
  {
    "text": "put this to custom like a custom oama as",
    "start": "999720",
    "end": "1002680"
  },
  {
    "text": "well um so the model file was the",
    "start": "1002680",
    "end": "1004440"
  },
  {
    "text": "complicated part when we had to",
    "start": "1004440",
    "end": "1005639"
  },
  {
    "text": "automatically generate this so we have",
    "start": "1005639",
    "end": "1007399"
  },
  {
    "text": "like internal code to generate the",
    "start": "1007399",
    "end": "1008680"
  },
  {
    "text": "modifier",
    "start": "1008680",
    "end": "1011160"
  },
  {
    "text": "automatically and finally when you want",
    "start": "1011160",
    "end": "1012920"
  },
  {
    "text": "to do inference you can do low Lama to",
    "start": "1012920",
    "end": "1014920"
  },
  {
    "text": "do inference um and you know it works um",
    "start": "1014920",
    "end": "1018120"
  },
  {
    "text": "in general so try that out uh the AMA",
    "start": "1018120",
    "end": "1020759"
  },
  {
    "text": "chat template notebook is in the slide",
    "start": "1020759",
    "end": "1022959"
  },
  {
    "text": "so tinyurl.com unsoft 2 um and remember",
    "start": "1022959",
    "end": "1026280"
  },
  {
    "text": "the workshop slides which we did",
    "start": "1026280",
    "end": "1027438"
  },
  {
    "text": "yesterday um is tinyurl.com unoff um and",
    "start": "1027439",
    "end": "1032240"
  },
  {
    "text": "don't forget to join our Discord Channel",
    "start": "1032240",
    "end": "1033880"
  },
  {
    "text": "um if you have any questions um outside",
    "start": "1033880",
    "end": "1035640"
  },
  {
    "text": "you can ask questions and stuff like",
    "start": "1035640",
    "end": "1036918"
  },
  {
    "text": "that um and yes like thanks for uh",
    "start": "1036919",
    "end": "1039798"
  },
  {
    "text": "thanks for coming I'm much appreciate it",
    "start": "1039799",
    "end": "1041160"
  },
  {
    "text": "thanks a lot",
    "start": "1041160",
    "end": "1044439"
  },
  {
    "text": "[Music]",
    "start": "1046000",
    "end": "1062940"
  }
]