[
  {
    "text": "[Music]",
    "start": "350",
    "end": "14050"
  },
  {
    "text": "I go to a lot of AI conferences I go to",
    "start": "14160",
    "end": "16240"
  },
  {
    "text": "AI research conferences I go to you know",
    "start": "16240",
    "end": "18920"
  },
  {
    "text": "more General Tech conferences and what I",
    "start": "18920",
    "end": "21199"
  },
  {
    "text": "absolutely love about this conference is",
    "start": "21199",
    "end": "23039"
  },
  {
    "text": "that it's really about the builders and",
    "start": "23039",
    "end": "24519"
  },
  {
    "text": "it's really about the techniques that we",
    "start": "24519",
    "end": "26279"
  },
  {
    "text": "need to get AI into the hands of our",
    "start": "26279",
    "end": "28800"
  },
  {
    "text": "customers and so we're all here in the",
    "start": "28800",
    "end": "31240"
  },
  {
    "text": "AI space we're all AI practitioners here",
    "start": "31240",
    "end": "33440"
  },
  {
    "text": "and we know that AI is developing at an",
    "start": "33440",
    "end": "37040"
  },
  {
    "text": "unprecedented Pace it's pretty hard to",
    "start": "37040",
    "end": "39040"
  },
  {
    "text": "keep up with it right every week There's",
    "start": "39040",
    "end": "41360"
  },
  {
    "text": "a new model a new capability a new",
    "start": "41360",
    "end": "43360"
  },
  {
    "text": "feature so much to keep up with and when",
    "start": "43360",
    "end": "47760"
  },
  {
    "text": "we see these new models capabilities and",
    "start": "47760",
    "end": "50440"
  },
  {
    "text": "features they're often shown to us as a",
    "start": "50440",
    "end": "52879"
  },
  {
    "text": "demo or",
    "start": "52879",
    "end": "54600"
  },
  {
    "text": "prototype and as Builders and Engineers",
    "start": "54600",
    "end": "58000"
  },
  {
    "text": "here today we all know there is a big",
    "start": "58000",
    "end": "60640"
  },
  {
    "text": "difference between a demo and a",
    "start": "60640",
    "end": "62239"
  },
  {
    "text": "prototype and scaling and",
    "start": "62239",
    "end": "64119"
  },
  {
    "text": "productionizing AI so one of the biggest",
    "start": "64119",
    "end": "67320"
  },
  {
    "text": "ways that we can bridge that Gap we can",
    "start": "67320",
    "end": "69360"
  },
  {
    "text": "go from having cool awesome demos to",
    "start": "69360",
    "end": "72200"
  },
  {
    "text": "actually bringing that to customers is",
    "start": "72200",
    "end": "74000"
  },
  {
    "text": "with efficiency so if we have techniques",
    "start": "74000",
    "end": "76600"
  },
  {
    "text": "for making our AI efficient we can get",
    "start": "76600",
    "end": "79200"
  },
  {
    "text": "closer to productization and so that's",
    "start": "79200",
    "end": "81560"
  },
  {
    "text": "what I want to tell you about today I'm",
    "start": "81560",
    "end": "82920"
  },
  {
    "text": "going to tell you about some practical",
    "start": "82920",
    "end": "84079"
  },
  {
    "text": "ways you can take away today to start",
    "start": "84079",
    "end": "87079"
  },
  {
    "text": "making your AI models more efficient",
    "start": "87079",
    "end": "91360"
  },
  {
    "text": "so let me introduce myself so I'm Shelby",
    "start": "91360",
    "end": "94079"
  },
  {
    "text": "and I lead an AI research team at",
    "start": "94079",
    "end": "96439"
  },
  {
    "text": "Salesforce my team ships AI today so we",
    "start": "96439",
    "end": "100920"
  },
  {
    "text": "deliver for example llm solutions to our",
    "start": "100920",
    "end": "104000"
  },
  {
    "text": "data platform at Salesforce the data",
    "start": "104000",
    "end": "105600"
  },
  {
    "text": "platform is the foundation of all of",
    "start": "105600",
    "end": "107240"
  },
  {
    "text": "Salesforce that's scale now while we're",
    "start": "107240",
    "end": "111000"
  },
  {
    "text": "delivering AI today we're also",
    "start": "111000",
    "end": "113000"
  },
  {
    "text": "envisioning what we'll need for tomorrow",
    "start": "113000",
    "end": "116159"
  },
  {
    "text": "and to do that we've released over 15",
    "start": "116159",
    "end": "118840"
  },
  {
    "text": "Cutting Edge research papers in agents",
    "start": "118840",
    "end": "121680"
  },
  {
    "text": "llms on device Ai and more and we've",
    "start": "121680",
    "end": "125719"
  },
  {
    "text": "also released over six open source repos",
    "start": "125719",
    "end": "128840"
  },
  {
    "text": "so my team has released these repos I'm",
    "start": "128840",
    "end": "130360"
  },
  {
    "text": "going to talk about one of them uh today",
    "start": "130360",
    "end": "132200"
  },
  {
    "text": "so you'll get to see and um this is all",
    "start": "132200",
    "end": "135200"
  },
  {
    "text": "in vain of pushing AI forward and",
    "start": "135200",
    "end": "139360"
  },
  {
    "text": "getting the AI that we'll need for",
    "start": "139360",
    "end": "140560"
  },
  {
    "text": "tomorrow now a little bit about my",
    "start": "140560",
    "end": "142319"
  },
  {
    "text": "personal background I have a PhD in",
    "start": "142319",
    "end": "145120"
  },
  {
    "text": "machine learning so I focused on",
    "start": "145120",
    "end": "146800"
  },
  {
    "text": "developing learning algorithms uh that",
    "start": "146800",
    "end": "149200"
  },
  {
    "text": "are sample and communication efficient",
    "start": "149200",
    "end": "151680"
  },
  {
    "text": "and I have a bachelor's and MERS in math",
    "start": "151680",
    "end": "153800"
  },
  {
    "text": "as well so if you're interested in",
    "start": "153800",
    "end": "155519"
  },
  {
    "text": "learning more about my team my",
    "start": "155519",
    "end": "157360"
  },
  {
    "text": "background want to connect on LinkedIn",
    "start": "157360",
    "end": "158840"
  },
  {
    "text": "feel free to scan the QR code I'm always",
    "start": "158840",
    "end": "160800"
  },
  {
    "text": "happy to chat with you",
    "start": "160800",
    "end": "163159"
  },
  {
    "text": "all now what about Salesforce this is",
    "start": "163159",
    "end": "166239"
  },
  {
    "text": "the this is AI and the Fortune 500",
    "start": "166239",
    "end": "168120"
  },
  {
    "text": "section let's talk about Salesforce and",
    "start": "168120",
    "end": "169519"
  },
  {
    "text": "what we're doing Salesforce has been",
    "start": "169519",
    "end": "171800"
  },
  {
    "text": "deploying AI for 10 years everyone 10",
    "start": "171800",
    "end": "175200"
  },
  {
    "text": "years so the AI research team was",
    "start": "175200",
    "end": "178159"
  },
  {
    "text": "founded in 2014 and since then",
    "start": "178159",
    "end": "181519"
  },
  {
    "text": "Salesforce has accumulated over 300 AI",
    "start": "181519",
    "end": "184640"
  },
  {
    "text": "patents over 227 AI research papers all",
    "start": "184640",
    "end": "188400"
  },
  {
    "text": "in the all in the last 10 years and you",
    "start": "188400",
    "end": "190680"
  },
  {
    "text": "can see here the map of all the",
    "start": "190680",
    "end": "192280"
  },
  {
    "text": "deployments that have taken place okay",
    "start": "192280",
    "end": "195560"
  },
  {
    "text": "and at Salesforce trust is our number",
    "start": "195560",
    "end": "197760"
  },
  {
    "text": "one value so we don't just deliver we",
    "start": "197760",
    "end": "199480"
  },
  {
    "text": "don't just build and deliver AI in",
    "start": "199480",
    "end": "201200"
  },
  {
    "text": "isolation we build and deliver trusted",
    "start": "201200",
    "end": "204159"
  },
  {
    "text": "AI that is a key so to do that we're",
    "start": "204159",
    "end": "206799"
  },
  {
    "text": "part of six ethical Ai councils and and",
    "start": "206799",
    "end": "209959"
  },
  {
    "text": "we're also involved in the White House",
    "start": "209959",
    "end": "211400"
  },
  {
    "text": "commitment for trusted",
    "start": "211400",
    "end": "213439"
  },
  {
    "text": "AI so I want to zoom in here past two",
    "start": "213439",
    "end": "217319"
  },
  {
    "text": "years that's where all the AI action has",
    "start": "217319",
    "end": "218840"
  },
  {
    "text": "been happening right the past two years",
    "start": "218840",
    "end": "220319"
  },
  {
    "text": "let's look at 2022 and 2023 what's",
    "start": "220319",
    "end": "223239"
  },
  {
    "text": "Salesforce been up to well we've been",
    "start": "223239",
    "end": "225519"
  },
  {
    "text": "deploying a lot of llm products right if",
    "start": "225519",
    "end": "227760"
  },
  {
    "text": "you look here you'll see um you'll see",
    "start": "227760",
    "end": "230599"
  },
  {
    "text": "Coen based products you'll see service",
    "start": "230599",
    "end": "233000"
  },
  {
    "text": "GPT Einstein GP Tavo GPT that's very",
    "start": "233000",
    "end": "236760"
  },
  {
    "text": "similar to the rest of the tech industry",
    "start": "236760",
    "end": "238280"
  },
  {
    "text": "right like if we zoom out the rest of",
    "start": "238280",
    "end": "239799"
  },
  {
    "text": "tech industry we're we're deploying llm",
    "start": "239799",
    "end": "242239"
  },
  {
    "text": "products and now for us to do it at",
    "start": "242239",
    "end": "244680"
  },
  {
    "text": "Salesforce efficiency is key think about",
    "start": "244680",
    "end": "247079"
  },
  {
    "text": "Salesforce scale think about Fortune 500",
    "start": "247079",
    "end": "249000"
  },
  {
    "text": "scale that we're talking here efficiency",
    "start": "249000",
    "end": "251040"
  },
  {
    "text": "is",
    "start": "251040",
    "end": "252319"
  },
  {
    "text": "key and we're all on the same boat here",
    "start": "252319",
    "end": "255159"
  },
  {
    "text": "we're all working on the same deployment",
    "start": "255159",
    "end": "257040"
  },
  {
    "text": "environment so let's review that a",
    "start": "257040",
    "end": "258320"
  },
  {
    "text": "little bit when we've got an AI model",
    "start": "258320",
    "end": "260440"
  },
  {
    "text": "we're mostly deploying you know a lot of",
    "start": "260440",
    "end": "262160"
  },
  {
    "text": "times we're deploying on a cloud right",
    "start": "262160",
    "end": "263759"
  },
  {
    "text": "private or public Cloud we're paying for",
    "start": "263759",
    "end": "266320"
  },
  {
    "text": "resource consumption we're paying for uh",
    "start": "266320",
    "end": "268880"
  },
  {
    "text": "you know we're paying for G U CPU dis",
    "start": "268880",
    "end": "270840"
  },
  {
    "text": "space we're paying for all of that so",
    "start": "270840",
    "end": "272160"
  },
  {
    "text": "we've got to keep that in mind right",
    "start": "272160",
    "end": "273720"
  },
  {
    "text": "when we deploy we're paying for that",
    "start": "273720",
    "end": "275000"
  },
  {
    "text": "cost to serve or now we're seeing even",
    "start": "275000",
    "end": "277680"
  },
  {
    "text": "on Prim Solutions maybe we have an on PR",
    "start": "277680",
    "end": "279560"
  },
  {
    "text": "maybe you have an on-prem cluster so not",
    "start": "279560",
    "end": "281960"
  },
  {
    "text": "only are you paying for that you've got",
    "start": "281960",
    "end": "283400"
  },
  {
    "text": "restricted gpus to work",
    "start": "283400",
    "end": "285600"
  },
  {
    "text": "within and more recently and this is",
    "start": "285600",
    "end": "288479"
  },
  {
    "text": "pretty exciting small devices we're",
    "start": "288479",
    "end": "290320"
  },
  {
    "text": "seeing llms being feasible on small",
    "start": "290320",
    "end": "293880"
  },
  {
    "text": "devices so if you guys were paying",
    "start": "293880",
    "end": "295800"
  },
  {
    "text": "attention to the news in the past couple",
    "start": "295800",
    "end": "297199"
  },
  {
    "text": "of weeks you'll see you remember that",
    "start": "297199",
    "end": "299360"
  },
  {
    "text": "app has announced their llm on their",
    "start": "299360",
    "end": "301639"
  },
  {
    "text": "newer devices so this is so exciting um",
    "start": "301639",
    "end": "304639"
  },
  {
    "text": "and so if we're seeing it on on iPhones",
    "start": "304639",
    "end": "307440"
  },
  {
    "text": "and small devices like that we can think",
    "start": "307440",
    "end": "309639"
  },
  {
    "text": "maybe llms and lmm multimodal models on",
    "start": "309639",
    "end": "312520"
  },
  {
    "text": "tablets on laptops on edge devices now",
    "start": "312520",
    "end": "316240"
  },
  {
    "text": "that's an even more challenging",
    "start": "316240",
    "end": "317560"
  },
  {
    "text": "situation right small devices have even",
    "start": "317560",
    "end": "319759"
  },
  {
    "text": "worse Hardware have even more resource",
    "start": "319759",
    "end": "321880"
  },
  {
    "text": "constraints the point here is that when",
    "start": "321880",
    "end": "324199"
  },
  {
    "text": "we're de when you're deploying AI models",
    "start": "324199",
    "end": "326639"
  },
  {
    "text": "you're deploying in these constrained",
    "start": "326639",
    "end": "328080"
  },
  {
    "text": "environments it's never we're never in a",
    "start": "328080",
    "end": "329960"
  },
  {
    "text": "situation where we have infinite",
    "start": "329960",
    "end": "331080"
  },
  {
    "text": "resources so efficiency is going to be",
    "start": "331080",
    "end": "333720"
  },
  {
    "text": "key so how do we make AI more",
    "start": "333720",
    "end": "337160"
  },
  {
    "text": "efficient so that's what I want to talk",
    "start": "337160",
    "end": "339479"
  },
  {
    "text": "to you today about I've summarized it",
    "start": "339479",
    "end": "341479"
  },
  {
    "text": "into five Dimensions five orthogonal",
    "start": "341479",
    "end": "343919"
  },
  {
    "text": "directions that I would love for you to",
    "start": "343919",
    "end": "345840"
  },
  {
    "text": "consider as you're as you're thinking",
    "start": "345840",
    "end": "347479"
  },
  {
    "text": "about building your AI for customers and",
    "start": "347479",
    "end": "349680"
  },
  {
    "text": "deploying the first and this is just",
    "start": "349680",
    "end": "351759"
  },
  {
    "text": "scratching the surface this is just",
    "start": "351759",
    "end": "353280"
  },
  {
    "text": "scratching the surface but I'm hoping",
    "start": "353280",
    "end": "355000"
  },
  {
    "text": "these five Dimensions will be easy for",
    "start": "355000",
    "end": "356360"
  },
  {
    "text": "you to remember the first is picking",
    "start": "356360",
    "end": "359880"
  },
  {
    "text": "efficient architectures from the very",
    "start": "359880",
    "end": "361520"
  },
  {
    "text": "beginning for the very beginning so this",
    "start": "361520",
    "end": "363680"
  },
  {
    "text": "includes picking small models I'm going",
    "start": "363680",
    "end": "365479"
  },
  {
    "text": "to talk about that today um this",
    "start": "365479",
    "end": "368039"
  },
  {
    "text": "includes using sophisticated",
    "start": "368039",
    "end": "369880"
  },
  {
    "text": "architecture such as mixture of experts",
    "start": "369880",
    "end": "372080"
  },
  {
    "text": "for example and if you're building your",
    "start": "372080",
    "end": "375080"
  },
  {
    "text": "um architecture from scratch includes uh",
    "start": "375080",
    "end": "377759"
  },
  {
    "text": "choosing efficient attention mechanisms",
    "start": "377759",
    "end": "379599"
  },
  {
    "text": "and so on so there's a lot we can say",
    "start": "379599",
    "end": "381240"
  },
  {
    "text": "there today I'm just going to touch on a",
    "start": "381240",
    "end": "382680"
  },
  {
    "text": "little bit moving on to the second one",
    "start": "382680",
    "end": "384759"
  },
  {
    "text": "efficient pre-training now not a lot of",
    "start": "384759",
    "end": "386800"
  },
  {
    "text": "us are doing pre-training it's a really",
    "start": "386800",
    "end": "388319"
  },
  {
    "text": "expensive thing to do but if you're",
    "start": "388319",
    "end": "389880"
  },
  {
    "text": "doing it you know the GPU costs you want",
    "start": "389880",
    "end": "392560"
  },
  {
    "text": "to use mixed Precision training scaling",
    "start": "392560",
    "end": "394639"
  },
  {
    "text": "methods among other methods here so",
    "start": "394639",
    "end": "396199"
  },
  {
    "text": "definitely make your pre-training",
    "start": "396199",
    "end": "397800"
  },
  {
    "text": "efficient now efficient fine tuning this",
    "start": "397800",
    "end": "400160"
  },
  {
    "text": "is the this is the case Mo a lot of us",
    "start": "400160",
    "end": "401960"
  },
  {
    "text": "are in today efficient fine tuning it's",
    "start": "401960",
    "end": "404720"
  },
  {
    "text": "you want to pick methods that are not",
    "start": "404720",
    "end": "406759"
  },
  {
    "text": "optimizing all of the weights every",
    "start": "406759",
    "end": "408560"
  },
  {
    "text": "single weight full fine tuning you want",
    "start": "408560",
    "end": "410120"
  },
  {
    "text": "to pick um methods that are that are",
    "start": "410120",
    "end": "412720"
  },
  {
    "text": "only optimizing you know a subset of",
    "start": "412720",
    "end": "414919"
  },
  {
    "text": "those weights so think about Laura Q",
    "start": "414919",
    "end": "416759"
  },
  {
    "text": "Laura and so",
    "start": "416759",
    "end": "417879"
  },
  {
    "text": "on and fourth the fourth dimension",
    "start": "417879",
    "end": "420960"
  },
  {
    "text": "efficient inference so you've got your",
    "start": "420960",
    "end": "422360"
  },
  {
    "text": "model it's pre-trained it's fine-tuned",
    "start": "422360",
    "end": "424639"
  },
  {
    "text": "you're ready to you're almost ready to",
    "start": "424639",
    "end": "426039"
  },
  {
    "text": "serve it how can we do that efficiently",
    "start": "426039",
    "end": "428080"
  },
  {
    "text": "we're paying for cost to serve right so",
    "start": "428080",
    "end": "430400"
  },
  {
    "text": "with that you want to consider",
    "start": "430400",
    "end": "431440"
  },
  {
    "text": "posttraining quantization which I'll get",
    "start": "431440",
    "end": "433120"
  },
  {
    "text": "into today and speculative decoding and",
    "start": "433120",
    "end": "435639"
  },
  {
    "text": "there's many others to cover as well and",
    "start": "435639",
    "end": "437680"
  },
  {
    "text": "finally prompting prompting we got to",
    "start": "437680",
    "end": "440240"
  },
  {
    "text": "think about that prompts uh you know",
    "start": "440240",
    "end": "441840"
  },
  {
    "text": "consume memory they also directly affect",
    "start": "441840",
    "end": "445120"
  },
  {
    "text": "latency as well so you want your prompts",
    "start": "445120",
    "end": "447120"
  },
  {
    "text": "to be as concise as possible concise as",
    "start": "447120",
    "end": "449240"
  },
  {
    "text": "possible so think about template",
    "start": "449240",
    "end": "451000"
  },
  {
    "text": "formatting and prompt compression now",
    "start": "451000",
    "end": "454160"
  },
  {
    "text": "with our limited time here today",
    "start": "454160",
    "end": "456520"
  },
  {
    "text": "together I'm going to dive into two",
    "start": "456520",
    "end": "458680"
  },
  {
    "text": "crucial directions that you can take",
    "start": "458680",
    "end": "460280"
  },
  {
    "text": "away with you and apply right away the",
    "start": "460280",
    "end": "463039"
  },
  {
    "text": "first direction is around efficient",
    "start": "463039",
    "end": "464960"
  },
  {
    "text": "architecture selection I want to tell",
    "start": "464960",
    "end": "466680"
  },
  {
    "text": "you about the power of small models",
    "start": "466680",
    "end": "468680"
  },
  {
    "text": "small models are coming back guys we",
    "start": "468680",
    "end": "470440"
  },
  {
    "text": "went we went big models small models are",
    "start": "470440",
    "end": "472479"
  },
  {
    "text": "are super efficient we'll talk about it",
    "start": "472479",
    "end": "475000"
  },
  {
    "text": "second I want to go into efficient",
    "start": "475000",
    "end": "476520"
  },
  {
    "text": "inference I want to tell you about",
    "start": "476520",
    "end": "477840"
  },
  {
    "text": "posttraining quantization this is",
    "start": "477840",
    "end": "479199"
  },
  {
    "text": "something that you could you could",
    "start": "479199",
    "end": "480960"
  },
  {
    "text": "actually apply at the end of the day on",
    "start": "480960",
    "end": "482199"
  },
  {
    "text": "your model so efficient and so",
    "start": "482199",
    "end": "484840"
  },
  {
    "text": "quick so let's get started with small",
    "start": "484840",
    "end": "487199"
  },
  {
    "text": "llms the power of these small",
    "start": "487199",
    "end": "489759"
  },
  {
    "text": "llms so let's uh think about the past",
    "start": "489759",
    "end": "493199"
  },
  {
    "text": "two years as I mentioned every week new",
    "start": "493199",
    "end": "496000"
  },
  {
    "text": "model new model new feature when we look",
    "start": "496000",
    "end": "499319"
  },
  {
    "text": "at these these llms that have been",
    "start": "499319",
    "end": "500639"
  },
  {
    "text": "released they're mostly pretty big",
    "start": "500639",
    "end": "502720"
  },
  {
    "text": "they're mostly pretty big so here are",
    "start": "502720",
    "end": "504759"
  },
  {
    "text": "just a few these are older models but I",
    "start": "504759",
    "end": "506400"
  },
  {
    "text": "just wanted to prove a point here if we",
    "start": "506400",
    "end": "508319"
  },
  {
    "text": "look at the Palm model for example 540",
    "start": "508319",
    "end": "510960"
  },
  {
    "text": "billion parameters right so parameters",
    "start": "510960",
    "end": "513000"
  },
  {
    "text": "again everyone is the number of Weights",
    "start": "513000",
    "end": "514719"
  },
  {
    "text": "in that deep neural network 540 billion",
    "start": "514719",
    "end": "518360"
  },
  {
    "text": "parameters these other models Bloom yum",
    "start": "518360",
    "end": "521080"
  },
  {
    "text": "176 billion parameters 100 billion",
    "start": "521080",
    "end": "523959"
  },
  {
    "text": "parameters so those parameters have got",
    "start": "523959",
    "end": "526279"
  },
  {
    "text": "to be stored in memory they're all going",
    "start": "526279",
    "end": "528320"
  },
  {
    "text": "to be used in computation GPU",
    "start": "528320",
    "end": "530120"
  },
  {
    "text": "computation CPU computation they're",
    "start": "530120",
    "end": "531720"
  },
  {
    "text": "going to take up space long story short",
    "start": "531720",
    "end": "534360"
  },
  {
    "text": "these huge models are resource hungry",
    "start": "534360",
    "end": "536839"
  },
  {
    "text": "they're going to take a lot of resources",
    "start": "536839",
    "end": "538120"
  },
  {
    "text": "to train certain certainly to pre-train",
    "start": "538120",
    "end": "540440"
  },
  {
    "text": "to fine-tune and to serve now in",
    "start": "540440",
    "end": "543800"
  },
  {
    "text": "parallel let's think over the past",
    "start": "543800",
    "end": "545959"
  },
  {
    "text": "several months we're seeing these",
    "start": "545959",
    "end": "548120"
  },
  {
    "text": "smaller models emerge and and when I",
    "start": "548120",
    "end": "551079"
  },
  {
    "text": "think about small llms I'm thinking",
    "start": "551079",
    "end": "553399"
  },
  {
    "text": "models that are 13 billion parameters or",
    "start": "553399",
    "end": "556560"
  },
  {
    "text": "less we're seeing these emerge and for",
    "start": "556560",
    "end": "558519"
  },
  {
    "text": "very good reason they're emerging for",
    "start": "558519",
    "end": "559920"
  },
  {
    "text": "good",
    "start": "559920",
    "end": "561120"
  },
  {
    "text": "reason there's so many benefits to these",
    "start": "561120",
    "end": "563399"
  },
  {
    "text": "smaller models so as you can imagine",
    "start": "563399",
    "end": "565880"
  },
  {
    "text": "with less parameters with less weights",
    "start": "565880",
    "end": "568760"
  },
  {
    "text": "they consume consum less Ram they",
    "start": "568760",
    "end": "570399"
  },
  {
    "text": "consume less GPU less CPU less dis dis",
    "start": "570399",
    "end": "573240"
  },
  {
    "text": "space and they're just faster to",
    "start": "573240",
    "end": "574480"
  },
  {
    "text": "fine-tune they're super they're super",
    "start": "574480",
    "end": "576399"
  },
  {
    "text": "resource efficient this is exactly what",
    "start": "576399",
    "end": "577920"
  },
  {
    "text": "we're looking for today they also low",
    "start": "577920",
    "end": "580440"
  },
  {
    "text": "latency fewer weights means the forward",
    "start": "580440",
    "end": "582640"
  },
  {
    "text": "pass is faster there's just fewer",
    "start": "582640",
    "end": "584200"
  },
  {
    "text": "weights to go through right and both of",
    "start": "584200",
    "end": "586800"
  },
  {
    "text": "those together the resource efficiency",
    "start": "586800",
    "end": "589000"
  },
  {
    "text": "the low latency makes them perfect for",
    "start": "589000",
    "end": "591880"
  },
  {
    "text": "additional deployment options so not",
    "start": "591880",
    "end": "593959"
  },
  {
    "text": "only can you take these small llms and",
    "start": "593959",
    "end": "595680"
  },
  {
    "text": "deploy them on the cloud on on Prim they",
    "start": "595680",
    "end": "598880"
  },
  {
    "text": "can also be deployed on mobile if",
    "start": "598880",
    "end": "600839"
  },
  {
    "text": "they're small enough they can be",
    "start": "600839",
    "end": "601800"
  },
  {
    "text": "deployed on laptops for personal models",
    "start": "601800",
    "end": "603720"
  },
  {
    "text": "they can be deployed on edge devices",
    "start": "603720",
    "end": "605760"
  },
  {
    "text": "they're super Nimble and super useful so",
    "start": "605760",
    "end": "607600"
  },
  {
    "text": "let me tell you about how so what I want",
    "start": "607600",
    "end": "609680"
  },
  {
    "text": "to do today is tell you about a few",
    "start": "609680",
    "end": "611920"
  },
  {
    "text": "small state-of-the-art llms to keep in",
    "start": "611920",
    "end": "614240"
  },
  {
    "text": "mind as you're building your solutions",
    "start": "614240",
    "end": "616040"
  },
  {
    "text": "for your",
    "start": "616040",
    "end": "617079"
  },
  {
    "text": "customers so the first one I'm going to",
    "start": "617079",
    "end": "618920"
  },
  {
    "text": "tell you about is a five3 you guys may",
    "start": "618920",
    "end": "621519"
  },
  {
    "text": "have been um may have heard of this one",
    "start": "621519",
    "end": "623880"
  },
  {
    "text": "this is a 3.8 billion model super super",
    "start": "623880",
    "end": "626279"
  },
  {
    "text": "small and as I'm talking about small",
    "start": "626279",
    "end": "628920"
  },
  {
    "text": "models I showed you these 540 billion",
    "start": "628920",
    "end": "631320"
  },
  {
    "text": "parameter model now we're talking about",
    "start": "631320",
    "end": "632920"
  },
  {
    "text": "a 3.8 billion parameter model your first",
    "start": "632920",
    "end": "635279"
  },
  {
    "text": "question might be what is the",
    "start": "635279",
    "end": "636839"
  },
  {
    "text": "performance is the performance good so",
    "start": "636839",
    "end": "639399"
  },
  {
    "text": "interesting so 53 is actually pretty",
    "start": "639399",
    "end": "641920"
  },
  {
    "text": "it's a very strong performing model so",
    "start": "641920",
    "end": "644440"
  },
  {
    "text": "as you can see here I took this clip",
    "start": "644440",
    "end": "645839"
  },
  {
    "text": "right from their technical report feel",
    "start": "645839",
    "end": "647160"
  },
  {
    "text": "free to check it out as you can see here",
    "start": "647160",
    "end": "649160"
  },
  {
    "text": "53 is outperforming a very very",
    "start": "649160",
    "end": "651279"
  },
  {
    "text": "well-known 7B model a model that's",
    "start": "651279",
    "end": "654040"
  },
  {
    "text": "almost twice its size so this 3 3B model",
    "start": "654040",
    "end": "657279"
  },
  {
    "text": "is pretty powerful for being so small",
    "start": "657279",
    "end": "660560"
  },
  {
    "text": "and now with that model being powerful",
    "start": "660560",
    "end": "663880"
  },
  {
    "text": "we're seeing even smaller models emerge",
    "start": "663880",
    "end": "666040"
  },
  {
    "text": "even smaller models because even smaller",
    "start": "666040",
    "end": "667680"
  },
  {
    "text": "models will fit on edge devices on",
    "start": "667680",
    "end": "669480"
  },
  {
    "text": "mobile and so on so one I want to point",
    "start": "669480",
    "end": "671920"
  },
  {
    "text": "out to you is mobile llm it has less",
    "start": "671920",
    "end": "674639"
  },
  {
    "text": "than one B so we're this has 350 million",
    "start": "674639",
    "end": "678560"
  },
  {
    "text": "parameters 350 million not even a",
    "start": "678560",
    "end": "681040"
  },
  {
    "text": "billion parameters so super super tiny",
    "start": "681040",
    "end": "683760"
  },
  {
    "text": "and here's the key after fine-tuning",
    "start": "683760",
    "end": "686079"
  },
  {
    "text": "after fine tuning it's on par with the",
    "start": "686079",
    "end": "688560"
  },
  {
    "text": "7B mod mod on tasks so this is one of",
    "start": "688560",
    "end": "691120"
  },
  {
    "text": "the takeaways I want to share with you",
    "start": "691120",
    "end": "692440"
  },
  {
    "text": "is that the power of these small models",
    "start": "692440",
    "end": "695360"
  },
  {
    "text": "you the way you use them is important",
    "start": "695360",
    "end": "698399"
  },
  {
    "text": "they're they're great after fine-tuning",
    "start": "698399",
    "end": "700440"
  },
  {
    "text": "they are very competitive that is what",
    "start": "700440",
    "end": "702200"
  },
  {
    "text": "this is showing and finally I want to",
    "start": "702200",
    "end": "704839"
  },
  {
    "text": "bring up a model that's really",
    "start": "704839",
    "end": "705959"
  },
  {
    "text": "interesting for function calling so um",
    "start": "705959",
    "end": "708639"
  },
  {
    "text": "this this octopus model is a fine-tune",
    "start": "708639",
    "end": "711200"
  },
  {
    "text": "model it's fine-tune Gemma 2B it's",
    "start": "711200",
    "end": "713399"
  },
  {
    "text": "fine-tune on Android tasks and again a",
    "start": "713399",
    "end": "716519"
  },
  {
    "text": "2B model they are showing after",
    "start": "716519",
    "end": "718680"
  },
  {
    "text": "fine-tuning",
    "start": "718680",
    "end": "719959"
  },
  {
    "text": "it's outperforming GPT 4 llama 7B on",
    "start": "719959",
    "end": "723240"
  },
  {
    "text": "these Android tasks so super super",
    "start": "723240",
    "end": "725440"
  },
  {
    "text": "promising so definitely check out these",
    "start": "725440",
    "end": "727240"
  },
  {
    "text": "small llms they have a ton of",
    "start": "727240",
    "end": "730320"
  },
  {
    "text": "potential and finally I will go to our",
    "start": "730320",
    "end": "732680"
  },
  {
    "text": "next topic which is quantization this is",
    "start": "732680",
    "end": "734560"
  },
  {
    "text": "about",
    "start": "734560",
    "end": "736360"
  },
  {
    "text": "inference so what is quantization",
    "start": "736360",
    "end": "739440"
  },
  {
    "text": "quantization is actually not a new topic",
    "start": "739440",
    "end": "741240"
  },
  {
    "text": "it's not a new topic what's new is",
    "start": "741240",
    "end": "743079"
  },
  {
    "text": "applying it to llms and lmm so the idea",
    "start": "743079",
    "end": "746639"
  },
  {
    "text": "of quantization is to take a big number",
    "start": "746639",
    "end": "748920"
  },
  {
    "text": "and to map it to a smaller number so",
    "start": "748920",
    "end": "752519"
  },
  {
    "text": "what we want to do for quantization for",
    "start": "752519",
    "end": "753959"
  },
  {
    "text": "llms is we want to reduce the Precision",
    "start": "753959",
    "end": "756880"
  },
  {
    "text": "of the weights so typically weights in",
    "start": "756880",
    "end": "758600"
  },
  {
    "text": "llms depending on the model is um could",
    "start": "758600",
    "end": "761320"
  },
  {
    "text": "be 32 bit or 16 bit",
    "start": "761320",
    "end": "763760"
  },
  {
    "text": "floats what we want to do with what",
    "start": "763760",
    "end": "765880"
  },
  {
    "text": "quantization does is reduces that 32 or",
    "start": "765880",
    "end": "767959"
  },
  {
    "text": "16 down to eight down to eight four bits",
    "start": "767959",
    "end": "770360"
  },
  {
    "text": "down to actually you can specify just a",
    "start": "770360",
    "end": "772240"
  },
  {
    "text": "smaller number of bits reducing the",
    "start": "772240",
    "end": "774279"
  },
  {
    "text": "Precision of all those weights so as you",
    "start": "774279",
    "end": "777120"
  },
  {
    "text": "can imagine that's hugely hugely",
    "start": "777120",
    "end": "778720"
  },
  {
    "text": "beneficial massive efficiency gains so",
    "start": "778720",
    "end": "781199"
  },
  {
    "text": "as you can see here if it's as you can",
    "start": "781199",
    "end": "782920"
  },
  {
    "text": "imagine if each weight was originally",
    "start": "782920",
    "end": "784560"
  },
  {
    "text": "32bit taking up 32bit space now we",
    "start": "784560",
    "end": "787399"
  },
  {
    "text": "reduce it to four bit it's going to take",
    "start": "787399",
    "end": "788720"
  },
  {
    "text": "up a lot less space it's going to",
    "start": "788720",
    "end": "790160"
  },
  {
    "text": "consume a lot less memory and it's going",
    "start": "790160",
    "end": "791480"
  },
  {
    "text": "to be consume a lot less CPU and GPU so",
    "start": "791480",
    "end": "793839"
  },
  {
    "text": "as you can see here really quickly just",
    "start": "793839",
    "end": "795760"
  },
  {
    "text": "some models looking at these llama",
    "start": "795760",
    "end": "797120"
  },
  {
    "text": "models 7B 13B 70b the original you can",
    "start": "797120",
    "end": "800639"
  },
  {
    "text": "see the dis space it was taking up after",
    "start": "800639",
    "end": "802720"
  },
  {
    "text": "4bit quantization it's taking up a",
    "start": "802720",
    "end": "804279"
  },
  {
    "text": "fraction of the dis this space and now",
    "start": "804279",
    "end": "808079"
  },
  {
    "text": "but what about latency so so as the",
    "start": "808079",
    "end": "809959"
  },
  {
    "text": "resource consumption comes down the",
    "start": "809959",
    "end": "811320"
  },
  {
    "text": "latency improves so as we can see here",
    "start": "811320",
    "end": "813519"
  },
  {
    "text": "in this in this study on large",
    "start": "813519",
    "end": "816040"
  },
  {
    "text": "multimodal models so these are large",
    "start": "816040",
    "end": "818680"
  },
  {
    "text": "lmms 16 bit was their original um",
    "start": "818680",
    "end": "821480"
  },
  {
    "text": "encoding originally 16 bit now if you",
    "start": "821480",
    "end": "824399"
  },
  {
    "text": "look at the 4bit four bit quantization",
    "start": "824399",
    "end": "826360"
  },
  {
    "text": "you can see that the latency measured",
    "start": "826360",
    "end": "828160"
  },
  {
    "text": "here as time to First token has",
    "start": "828160",
    "end": "830519"
  },
  {
    "text": "decreased so lots of benefits so reduced",
    "start": "830519",
    "end": "834160"
  },
  {
    "text": "uh so reduced resource consumption",
    "start": "834160",
    "end": "836560"
  },
  {
    "text": "faster again though the most important",
    "start": "836560",
    "end": "838560"
  },
  {
    "text": "question is is the performance still",
    "start": "838560",
    "end": "840360"
  },
  {
    "text": "there are we making this are we making a",
    "start": "840360",
    "end": "841880"
  },
  {
    "text": "huge tradeoff by with this and the good",
    "start": "841880",
    "end": "844519"
  },
  {
    "text": "news is no this is this is pretty",
    "start": "844519",
    "end": "847320"
  },
  {
    "text": "amazing quantization generally has",
    "start": "847320",
    "end": "849320"
  },
  {
    "text": "negligible effects on performance so I",
    "start": "849320",
    "end": "852240"
  },
  {
    "text": "want to show you that so look here at",
    "start": "852240",
    "end": "853880"
  },
  {
    "text": "this chart at this graph and again we're",
    "start": "853880",
    "end": "857440"
  },
  {
    "text": "looking at lmm and you can see here",
    "start": "857440",
    "end": "860240"
  },
  {
    "text": "we've on this particular task this",
    "start": "860240",
    "end": "862440"
  },
  {
    "text": "well-known Vision language task we took",
    "start": "862440",
    "end": "864880"
  },
  {
    "text": "the lmm and we measured performance on",
    "start": "864880",
    "end": "867639"
  },
  {
    "text": "16 bit and then 18- bit quantization and",
    "start": "867639",
    "end": "870079"
  },
  {
    "text": "4bit quantization and as you can see",
    "start": "870079",
    "end": "872079"
  },
  {
    "text": "essentially no movement 4bit",
    "start": "872079",
    "end": "874199"
  },
  {
    "text": "quantization was essentially free like",
    "start": "874199",
    "end": "876160"
  },
  {
    "text": "we could just quantize with four bits",
    "start": "876160",
    "end": "878519"
  },
  {
    "text": "and just enjoy reduced latency enjoy",
    "start": "878519",
    "end": "881759"
  },
  {
    "text": "increased uh improved late uh sorry",
    "start": "881759",
    "end": "884360"
  },
  {
    "text": "enjoy reduced resource consumption enjoy",
    "start": "884360",
    "end": "886639"
  },
  {
    "text": "improved latency and no effect to",
    "start": "886639",
    "end": "889320"
  },
  {
    "text": "Performance we retain performance",
    "start": "889320",
    "end": "891240"
  },
  {
    "text": "however you can take this too far there",
    "start": "891240",
    "end": "892920"
  },
  {
    "text": "is there is a way to take this too far",
    "start": "892920",
    "end": "894399"
  },
  {
    "text": "so as you can see when we quantize down",
    "start": "894399",
    "end": "896320"
  },
  {
    "text": "to three bits performance did drop so",
    "start": "896320",
    "end": "899440"
  },
  {
    "text": "evaluating your quantize model is super",
    "start": "899440",
    "end": "901440"
  },
  {
    "text": "important so don't just assume for bit",
    "start": "901440",
    "end": "903519"
  },
  {
    "text": "is the answer definitely measure uh you",
    "start": "903519",
    "end": "905920"
  },
  {
    "text": "definitely want to measure quantize",
    "start": "905920",
    "end": "907560"
  },
  {
    "text": "performance so really quickly so you can",
    "start": "907560",
    "end": "910160"
  },
  {
    "text": "get started on this today you can add",
    "start": "910160",
    "end": "912000"
  },
  {
    "text": "you can quantize any of your models",
    "start": "912000",
    "end": "913600"
  },
  {
    "text": "whether it's ml models llms lmm and so",
    "start": "913600",
    "end": "915959"
  },
  {
    "text": "on I want to just highlight a couple of",
    "start": "915959",
    "end": "918279"
  },
  {
    "text": "of Frameworks that are really um awesome",
    "start": "918279",
    "end": "920079"
  },
  {
    "text": "for that so llama CBP is one of the most",
    "start": "920079",
    "end": "922320"
  },
  {
    "text": "well-known Frameworks right now it's",
    "start": "922320",
    "end": "923759"
  },
  {
    "text": "gaining a lot of traction it has",
    "start": "923759",
    "end": "925800"
  },
  {
    "text": "quantization from 16bit all the way down",
    "start": "925800",
    "end": "928360"
  },
  {
    "text": "to 1 and a half bit so pretty crazy um",
    "start": "928360",
    "end": "930920"
  },
  {
    "text": "wide adoption so actually you may not",
    "start": "930920",
    "end": "932920"
  },
  {
    "text": "even need to quantize models that you're",
    "start": "932920",
    "end": "934480"
  },
  {
    "text": "using just check hugging face a lot of",
    "start": "934480",
    "end": "935959"
  },
  {
    "text": "people are as they're releasing their",
    "start": "935959",
    "end": "937240"
  },
  {
    "text": "models they're going they're releasing",
    "start": "937240",
    "end": "938600"
  },
  {
    "text": "the the LA CPP compatible quantization",
    "start": "938600",
    "end": "941079"
  },
  {
    "text": "models too so pretty awesome and there's",
    "start": "941079",
    "end": "943639"
  },
  {
    "text": "uh Python and Java wrappers second thing",
    "start": "943639",
    "end": "946360"
  },
  {
    "text": "I just want to quickly mention Onyx",
    "start": "946360",
    "end": "948040"
  },
  {
    "text": "runtime this is uh Onyx has been around",
    "start": "948040",
    "end": "950319"
  },
  {
    "text": "for some time if if you've been around",
    "start": "950319",
    "end": "952120"
  },
  {
    "text": "since the ml days Onyx was around in the",
    "start": "952120",
    "end": "954319"
  },
  {
    "text": "ml days and so um they have some 8 Bit",
    "start": "954319",
    "end": "957240"
  },
  {
    "text": "quantization And you know the beauty of",
    "start": "957240",
    "end": "959040"
  },
  {
    "text": "Onyx is that it's compatible across so",
    "start": "959040",
    "end": "961319"
  },
  {
    "text": "many programming languages so definitely",
    "start": "961319",
    "end": "963880"
  },
  {
    "text": "take uh take a look at these and there's",
    "start": "963880",
    "end": "965839"
  },
  {
    "text": "a bunch of others ones you can consider",
    "start": "965839",
    "end": "968000"
  },
  {
    "text": "too now final Point here as we mentioned",
    "start": "968000",
    "end": "971519"
  },
  {
    "text": "with quantization now um you have your",
    "start": "971519",
    "end": "973639"
  },
  {
    "text": "quantize model I mentioned before it is",
    "start": "973639",
    "end": "975440"
  },
  {
    "text": "still important to evaluate your",
    "start": "975440",
    "end": "976680"
  },
  {
    "text": "quantitized model before you deploy it",
    "start": "976680",
    "end": "978600"
  },
  {
    "text": "so I want to introduce you one of the",
    "start": "978600",
    "end": "980519"
  },
  {
    "text": "open source repos that my team just",
    "start": "980519",
    "end": "982360"
  },
  {
    "text": "developed we just released this maybe",
    "start": "982360",
    "end": "983720"
  },
  {
    "text": "like a week ago it's called mobile AI",
    "start": "983720",
    "end": "986440"
  },
  {
    "text": "bench and the point of this is an open",
    "start": "986440",
    "end": "989160"
  },
  {
    "text": "source framework for you to evaluate",
    "start": "989160",
    "end": "991600"
  },
  {
    "text": "your quantized models okay so this is",
    "start": "991600",
    "end": "993880"
  },
  {
    "text": "going to give you some rigor before you",
    "start": "993880",
    "end": "996199"
  },
  {
    "text": "actually deploy that quantized model",
    "start": "996199",
    "end": "997959"
  },
  {
    "text": "just to make sure that it is performing",
    "start": "997959",
    "end": "999600"
  },
  {
    "text": "as expected so it's going to streamline",
    "start": "999600",
    "end": "1002279"
  },
  {
    "text": "evaluation uh your evaluation across",
    "start": "1002279",
    "end": "1004600"
  },
  {
    "text": "text task trust and safety that's really",
    "start": "1004600",
    "end": "1006680"
  },
  {
    "text": "important make sure trust and safety",
    "start": "1006680",
    "end": "1007959"
  },
  {
    "text": "doesn't degrade with quantization vision",
    "start": "1007959",
    "end": "1010199"
  },
  {
    "text": "language now if you're interested in",
    "start": "1010199",
    "end": "1012000"
  },
  {
    "text": "deploying your quantize models to device",
    "start": "1012000",
    "end": "1014720"
  },
  {
    "text": "we even have an IOS app right now an IOS",
    "start": "1014720",
    "end": "1017360"
  },
  {
    "text": "app that you can use that will measure",
    "start": "1017360",
    "end": "1018720"
  },
  {
    "text": "the latency of the quantized model and",
    "start": "1018720",
    "end": "1021240"
  },
  {
    "text": "even measure the hardware usage so you",
    "start": "1021240",
    "end": "1022880"
  },
  {
    "text": "can even check like battery drainage for",
    "start": "1022880",
    "end": "1025438"
  },
  {
    "text": "um for deploying these models so feel",
    "start": "1025439",
    "end": "1027360"
  },
  {
    "text": "free to check out our open source",
    "start": "1027360",
    "end": "1030000"
  },
  {
    "text": "repo and with that that wraps up the",
    "start": "1030000",
    "end": "1032280"
  },
  {
    "text": "content for today it was absolutely",
    "start": "1032280",
    "end": "1033798"
  },
  {
    "text": "great being here so again remember these",
    "start": "1033799",
    "end": "1036038"
  },
  {
    "text": "five dimensions of AI efficiency as",
    "start": "1036039",
    "end": "1038678"
  },
  {
    "text": "you're building and deploying your",
    "start": "1038679",
    "end": "1041079"
  },
  {
    "text": "models thank you so much and if you're",
    "start": "1041079",
    "end": "1043038"
  },
  {
    "text": "interested feel free to check out these",
    "start": "1043039",
    "end": "1044360"
  },
  {
    "text": "QR codes thank you",
    "start": "1044360",
    "end": "1048438"
  },
  {
    "text": "[Music]",
    "start": "1049760",
    "end": "1066690"
  }
]