[
  {
    "text": "[Music]",
    "start": "350",
    "end": "14050"
  },
  {
    "text": "I am here with uh with Punk scupa he's the co-founder of base 10 um actually so",
    "start": "14160",
    "end": "20160"
  },
  {
    "text": "today uh I was I was checking slack and in the random slack channel one of the people in the company uh was saying like",
    "start": "20160",
    "end": "26519"
  },
  {
    "text": "hey you know I heard someone say call someone cracked what does what does cracked mean um and you know those uh",
    "start": "26519",
    "end": "32800"
  },
  {
    "text": "those of you who are you know jenzy like me or or or know someone like that is like laughing right now because crack",
    "start": "32800",
    "end": "39239"
  },
  {
    "text": "just means a uh an exceptional engineer and so Punk is the most crack software engineer I've ever had the uh pleasure",
    "start": "39239",
    "end": "45840"
  },
  {
    "text": "of working with um he's uh from San Francisco his favorite model is llama 38b um we're going to be working with a",
    "start": "45840",
    "end": "52920"
  },
  {
    "text": "smaller version of that today um I'm philli I do develop a relations here at Bas 10 I've been here for about two and",
    "start": "52920",
    "end": "58879"
  },
  {
    "text": "a half years and uh I am based in Chicago but I'm very happy to be here in San Francisco with you all today and uh",
    "start": "58879",
    "end": "66080"
  },
  {
    "text": "my favorite model is playground two it's a tech it's a text image model that's",
    "start": "66080",
    "end": "71560"
  },
  {
    "text": "kind of like sdxl but it's trained on Mid Journey images you're going to see a ton of playground 2 images in the slideshow",
    "start": "71560",
    "end": "78960"
  },
  {
    "text": "today so what are we doing here today uh what is our agenda so we're going to",
    "start": "78960",
    "end": "85159"
  },
  {
    "text": "cover what is tensor llm and why use it model selection and tensor llm support",
    "start": "85159",
    "end": "91200"
  },
  {
    "text": "because it supports a lot of stuff but not everything um we're going to talk about building a tenso engine",
    "start": "91200",
    "end": "96880"
  },
  {
    "text": "configuring a tensorrt engine automatically benchmarking it so you can know if you actually did something",
    "start": "96880",
    "end": "103079"
  },
  {
    "text": "worthwhile and then uh deploying it to production and you know as much as I",
    "start": "103079",
    "end": "108119"
  },
  {
    "text": "love the sound of my own voice and I want to just stand here and grasp this microphone for two hours and say things",
    "start": "108119",
    "end": "113880"
  },
  {
    "text": "this is not just going to be Philip weeds office slideshow um we're going to do tons of coding debugging live Q&A so",
    "start": "113880",
    "end": "120479"
  },
  {
    "text": "the way this uh SE presentation's kind of broken up is we've got some some sections we've got some live coding um",
    "start": "120479",
    "end": "127240"
  },
  {
    "text": "it's going to be you know a very interactive Workshop I'm going to be taking questions all the time so please don't hesitate uh to let us know if",
    "start": "127240",
    "end": "134239"
  },
  {
    "text": "anything's confusing we really want everyone to come away from this with a strong working understanding of how you",
    "start": "134239",
    "end": "140920"
  },
  {
    "text": "can actually use this technology in production so let's get started let me interject for a second and ask uh raise",
    "start": "140920",
    "end": "147959"
  },
  {
    "text": "of hands how many of you know about about tensor RT great oh this is so exciting I'm so",
    "start": "147959",
    "end": "154959"
  },
  {
    "text": "glad that we get to teach you all this today and how about tensor RT",
    "start": "154959",
    "end": "160200"
  },
  {
    "text": "llm okay few so we'll we'll cover the basics I think uh I'm pretty sure that",
    "start": "160239",
    "end": "165720"
  },
  {
    "text": "you'll get a sense of what it is if you know pytorch this shouldn't be too hard and if you don't know pytorch like me",
    "start": "165720",
    "end": "171680"
  },
  {
    "text": "it's still not that hard so uh we're going to start with the",
    "start": "171680",
    "end": "177480"
  },
  {
    "text": "story of tensorrt llm what who why you know once upon a time there was a",
    "start": "177480",
    "end": "183040"
  },
  {
    "text": "company called Nvidia um and they uh they they noticed that there are these",
    "start": "183040",
    "end": "188200"
  },
  {
    "text": "things called large language models um that people love running but what do you want when you run a large language model",
    "start": "188200",
    "end": "194360"
  },
  {
    "text": "you want a lot of tokens per second you want a really short time to First token and you want High throughput you know",
    "start": "194360",
    "end": "200200"
  },
  {
    "text": "gpus are expensive so you want to get the maximum value out of your GPU and tenso RT and tenso RT llm are",
    "start": "200200",
    "end": "207959"
  },
  {
    "text": "technologies that are going to help you do that so if we get into it here what is 10rt here's one of my uh playground 2 images",
    "start": "207959",
    "end": "215080"
  },
  {
    "text": "very proud of these uh if the words on the slides are dumb uh just look at the images uh because I worked hard on those",
    "start": "215080",
    "end": "222599"
  },
  {
    "text": "um anyway so tensorrt is a SDK for high performance deep learning influence on",
    "start": "222599",
    "end": "228120"
  },
  {
    "text": "Nvidia gpus basically what that means is it's just a great set of tools for",
    "start": "228120",
    "end": "233360"
  },
  {
    "text": "building high performance models um it's a you know toolkit that supports both c",
    "start": "233360",
    "end": "239360"
  },
  {
    "text": "c Plus+ and python um our interface today is going to be entirely python so if like me you skip the class that",
    "start": "239360",
    "end": "246000"
  },
  {
    "text": "teaches C++ don't worry you're covered I know pun read C++ Textbooks For Fun uh",
    "start": "246000",
    "end": "251239"
  },
  {
    "text": "but but but I do not so we're going to do it in Python today um and so how does this work you know do do you want to do",
    "start": "251239",
    "end": "257519"
  },
  {
    "text": "you want to kind of jump in here and and and talk about this a little bit because you know it's it's a it's a really cool",
    "start": "257519",
    "end": "263479"
  },
  {
    "text": "process how you go from a neural network to to an engine yeah um yeah exactly so",
    "start": "263479",
    "end": "269720"
  },
  {
    "text": "ultimately what are machine learning models they the graphs they are computation graphs you flow data through",
    "start": "269720",
    "end": "275520"
  },
  {
    "text": "them you transform them and ultimately whatever executes a model does that they",
    "start": "275520",
    "end": "280759"
  },
  {
    "text": "execute a graph your neural network is a graph tensor RT works on a graph representation you take your model and",
    "start": "280759",
    "end": "288160"
  },
  {
    "text": "you express that using an API uh that graph in tensor R and then tensor Rd is",
    "start": "288160",
    "end": "293800"
  },
  {
    "text": "able to take that graph discover patterns optimize it and then be able to execute it that's for tens is ultimately",
    "start": "293800",
    "end": "300960"
  },
  {
    "text": "when you write it at PTO model you're ultimately creating a graph it's it's graph all overs right there is data",
    "start": "300960",
    "end": "307000"
  },
  {
    "text": "flowing through this graph uh and that's what it is tensor additionally provides",
    "start": "307000",
    "end": "312199"
  },
  {
    "text": "a plug-in mechanism so it says that you know what I know this graph I can do a lot of lot of stuff but I can't do very",
    "start": "312199",
    "end": "319280"
  },
  {
    "text": "fancy things like flash attention it's just too complex I can't infer automatically from this graph that this",
    "start": "319280",
    "end": "325400"
  },
  {
    "text": "is even possible like I'm not a research scientist so it gives a plug-in mechanism using which you can inspect",
    "start": "325400",
    "end": "330759"
  },
  {
    "text": "the graph and say that okay I recognize this thing and I can do it better than uens RD so I'm going to do it through",
    "start": "330759",
    "end": "337000"
  },
  {
    "text": "this plugin and that is what tensor RT lrm does it has a bunch of plugins for",
    "start": "337000",
    "end": "342440"
  },
  {
    "text": "optimizing this graph execution for large language models so for example for attention for Flash attention it has its",
    "start": "342440",
    "end": "349440"
  },
  {
    "text": "own plug-in where it says that okay now we are in tensor llm land take this",
    "start": "349440",
    "end": "354560"
  },
  {
    "text": "graph and let me execute it using my optimized Cuda kernels and that's what ultimately llm is um a very very",
    "start": "354560",
    "end": "362880"
  },
  {
    "text": "optimized way of executing these grafts using GPU resources um not only to get",
    "start": "362880",
    "end": "368280"
  },
  {
    "text": "more efficiency better better cost for your money but also better latency",
    "start": "368280",
    "end": "374319"
  },
  {
    "text": "better time to First token all the things that we care about when we running these models uh in addition to",
    "start": "374319",
    "end": "379800"
  },
  {
    "text": "that it provides uh a few more things like uh when you're executing a model you're not just executing a request at a",
    "start": "379800",
    "end": "385960"
  },
  {
    "text": "time you're executing a bunch of requests at a time and inlight batch is a key optimization that is very very key",
    "start": "385960",
    "end": "392759"
  },
  {
    "text": "like in this day and age if you are executing a large language model you have to have inflight badging there's",
    "start": "392759",
    "end": "398759"
  },
  {
    "text": "just no way like it's like a 10x or 20x Improvement like and you have to have that and tensor llm provides that tensor",
    "start": "398759",
    "end": "405639"
  },
  {
    "text": "wouldn't tens is a graph executor it doesn't know about that but tens llm has an engine that does that it also has a",
    "start": "405639",
    "end": "412240"
  },
  {
    "text": "language to express graph just like pyos and it requires that there is a conversion but it makes it pretty easy",
    "start": "412240",
    "end": "417919"
  },
  {
    "text": "to do that conversion and there are tons of examples in the rep exactly so tensor",
    "start": "417919",
    "end": "423520"
  },
  {
    "text": "is this great sort of engine builder and then tensor llm is a mechanism on top of",
    "start": "423520",
    "end": "430000"
  },
  {
    "text": "that that's going to give us a ton of plugins and a ton of optimization specifically for large language models",
    "start": "430000",
    "end": "436800"
  },
  {
    "text": "so tensor llm like Punk said defines the set of plugins for your llms if you want",
    "start": "436800",
    "end": "442360"
  },
  {
    "text": "to you know compute attenion do lauras Medusa other fine tunes um and it lets",
    "start": "442360",
    "end": "448680"
  },
  {
    "text": "you define optim optimization profiles so when you're running a large language model you generally have a batch of",
    "start": "448680",
    "end": "455919"
  },
  {
    "text": "requests that you're running at the same time you also have a input sequence and an output sequence and this input",
    "start": "455919",
    "end": "462479"
  },
  {
    "text": "sequence could be really long you know maybe you're summarizing a book it could be really short maybe you're just doing",
    "start": "462479",
    "end": "468599"
  },
  {
    "text": "some llm chat like Hi how are you I'm Fred from the bank um depending on what",
    "start": "468599",
    "end": "474479"
  },
  {
    "text": "your input sequence and output sequence lengths are you're going to want to build a different engine that is going",
    "start": "474479",
    "end": "480440"
  },
  {
    "text": "to be optimized for that to process that number of tokens um so yeah so tensor",
    "start": "480440",
    "end": "487720"
  },
  {
    "text": "llm is this toolbox for taking tensorrt and building large language model",
    "start": "487720",
    "end": "494280"
  },
  {
    "text": "engines in tensorrt I want to say just one thing at this point like why I care about input",
    "start": "494280",
    "end": "500000"
  },
  {
    "text": "and output sizes like how does TM optimize for that it actually has specific kernels for different sizes of",
    "start": "500000",
    "end": "506759"
  },
  {
    "text": "inputs different sizes of matrices it's for that level and sometimes becomes a pain when I'm compiling T it takes hours",
    "start": "506759",
    "end": "514120"
  },
  {
    "text": "because it optimizes for so many sizes but it also means that giving it that size guidance is useful it can use",
    "start": "514120",
    "end": "520880"
  },
  {
    "text": "better kernels to do things faster and that's why uh a lot of the models you run you don't have to care about it but",
    "start": "520880",
    "end": "527040"
  },
  {
    "text": "there is always a tradeoff here it does care about that and you can benefit using the",
    "start": "527040",
    "end": "533399"
  },
  {
    "text": "tradeoff yeah and 10t llm is a great tool for a number of reasons um it's you",
    "start": "533399",
    "end": "540040"
  },
  {
    "text": "know it's it's got those those built-in optimized conal for different sequence lengths um and that level of detail is",
    "start": "540040",
    "end": "547640"
  },
  {
    "text": "really across the entire tool and what that means is that with tensorrt llm you can get some of the highest performance",
    "start": "547640",
    "end": "554240"
  },
  {
    "text": "possible on gpus for a wide range of models and it's really a production ready uh system we are using tensorrt",
    "start": "554240",
    "end": "561519"
  },
  {
    "text": "llm today uh for tons of different client projects and it's uh you know running and production powering things",
    "start": "561519",
    "end": "568360"
  },
  {
    "text": "um tensor llm has support for a ton of different gpus um basically anything like Volta or newer the Volta support's",
    "start": "568360",
    "end": "575000"
  },
  {
    "text": "kind of experimental um but yeah like your a10s you aunds h100s all that stuff",
    "start": "575000",
    "end": "580800"
  },
  {
    "text": "supported um and yeah in tenso llm it's uh developed by Nvidia um so you know",
    "start": "580800",
    "end": "586360"
  },
  {
    "text": "they know their graphics cards better than anyone um so we we just kind of use it to run models quickly on that um that",
    "start": "586360",
    "end": "593560"
  },
  {
    "text": "said everything does come with a tradeoff um is is anyone from Nvidia here in the room it's okay you don't",
    "start": "593560",
    "end": "599600"
  },
  {
    "text": "have to ra okay uh so I'm going to be nice um uh no we we really are big fans",
    "start": "599600",
    "end": "604720"
  },
  {
    "text": "of this technology but it does come with trade-offs you know some of the underlying stuff is not fully open",
    "start": "604720",
    "end": "609959"
  },
  {
    "text": "source um so sometimes if you're diving super deep you need to uh go get more information without just like looking at",
    "start": "609959",
    "end": "616200"
  },
  {
    "text": "the source code um and it does sometimes have a pretty steep learning cve uh when you're building these optimizations so",
    "start": "616200",
    "end": "623200"
  },
  {
    "text": "that's what we're here to help flatten out for you guys today hopefully we're still friends uh what makes it hard so",
    "start": "623200",
    "end": "630720"
  },
  {
    "text": "the there's a couple things that make building with TT llm really hard and when we enumerate the things that make",
    "start": "630720",
    "end": "636240"
  },
  {
    "text": "it hard that's how we know what we need to do to make it easy so the number one thing in my mind that makes it hard to",
    "start": "636240",
    "end": "642079"
  },
  {
    "text": "build a general model uh or to to optimize a model with TM is you need a ton of specific information about the",
    "start": "642079",
    "end": "649000"
  },
  {
    "text": "production environment you're going to run in all right so I I do a lot of sales enablement trainings and I love a",
    "start": "649000",
    "end": "657200"
  },
  {
    "text": "good metaphor so I'm going to I'm going to walk you guys through a metaphor here apologies if metaphors aren't your thing",
    "start": "657200",
    "end": "663600"
  },
  {
    "text": "so imagine you go into a clothing store and it only sells one size of sh you know it's just like a medium um you know",
    "start": "663600",
    "end": "670959"
  },
  {
    "text": "for some people that's going to fit great for some people it's going to be too small for some people it's going to be too big and on the other hand you can go to",
    "start": "670959",
    "end": "678760"
  },
  {
    "text": "like a Taylor I don't know in like Italy or something and you go there and they've got you know some some super",
    "start": "678760",
    "end": "684920"
  },
  {
    "text": "fancy guy with a with a you know cool mustache and stuff and he you know measures you like every single detail",
    "start": "684920",
    "end": "691680"
  },
  {
    "text": "and then builds a suit exactly for you that's perfect for your body measurements like a made to measure suit",
    "start": "691680",
    "end": "697920"
  },
  {
    "text": "so optimizing a model is kind of like making that suit you know everything has",
    "start": "697920",
    "end": "703959"
  },
  {
    "text": "to be measured for exactly the use case that you're building for and so when people come in and expect that they can",
    "start": "703959",
    "end": "710120"
  },
  {
    "text": "just walk in and grab off the shelf a model that's going to work perfectly for their use case that's like expecting",
    "start": "710120",
    "end": "715560"
  },
  {
    "text": "you're going to go into a store and buy a piece of clothing that fits you just as as well as that custom made made to",
    "start": "715560",
    "end": "720760"
  },
  {
    "text": "measure suit from the tailor so in you know to relate that more concretely to tenso llm you need",
    "start": "720760",
    "end": "728680"
  },
  {
    "text": "information you need like we talked about you need to understand the sequence lengths that you're going to be working at the batch sizes that you want",
    "start": "728680",
    "end": "734519"
  },
  {
    "text": "to run at you also need to know ahead of time what gpus you're going to be using in production these engines that we're",
    "start": "734519",
    "end": "741000"
  },
  {
    "text": "building are not portable they are built for a specific GPU you build them so if",
    "start": "741000",
    "end": "746199"
  },
  {
    "text": "you build it on an A10 you R it on an A10 if if you build it on an h100 you run it on an h100 you want to switch to",
    "start": "746199",
    "end": "753399"
  },
  {
    "text": "h100 Mig okay you build it again for h100 Mig so you need to know all of this",
    "start": "753399",
    "end": "759000"
  },
  {
    "text": "information about your production environment um and then also as we'll talk about kind of toward the end",
    "start": "759000",
    "end": "764279"
  },
  {
    "text": "there's some infrastructure challenges as well these engines that we going to build a quite large so if you're for",
    "start": "764279",
    "end": "769600"
  },
  {
    "text": "example doing Auto scaling you have to deal with slow cold starts you know work work around the size of the engines",
    "start": "769600",
    "end": "775839"
  },
  {
    "text": "otherwise your cold starts are going to be slow um and overall also just model optimization means we're living on The",
    "start": "775839",
    "end": "782600"
  },
  {
    "text": "Cutting Edge of new research you know I'm when I'm when I'm writing blog posts about this stuff I'm oftentimes looking",
    "start": "782600",
    "end": "788959"
  },
  {
    "text": "at papers that have been published in the last six months so you know just combining all these new approaches and",
    "start": "788959",
    "end": "795519"
  },
  {
    "text": "tools there there can be some rough edges but the performance gains are worth it so yeah oh please go ahead I",
    "start": "795519",
    "end": "802680"
  },
  {
    "text": "want to add one thing is that there are modes in tens llm where you can build for a certain on a certain dpu and it",
    "start": "802680",
    "end": "809399"
  },
  {
    "text": "will run on other gpus but then it it's not optimized for the gpus so why would you do that we never do that we always",
    "start": "809399",
    "end": "815440"
  },
  {
    "text": "build it for the GPU but there is that option exactly that would be like if I went to that fancy Taylor shop got a",
    "start": "815440",
    "end": "821079"
  },
  {
    "text": "made to measure suit and then was like Hey Punit happy birthday I got you a new suit uh that's that's what it would be",
    "start": "821079",
    "end": "826399"
  },
  {
    "text": "like so you know what what makes tens rtlm with it well it's it's the performance so these numbers are from a",
    "start": "826399",
    "end": "833480"
  },
  {
    "text": "mistol 7B that we ran on artificial analysis which is a third party benchmarking site um and we were able to",
    "start": "833480",
    "end": "840480"
  },
  {
    "text": "get with TT llm and a few other optimizations as well on top of it 216 tokens per second uh perceived tokens",
    "start": "840480",
    "end": "847399"
  },
  {
    "text": "per second and 180 milliseconds time to First token so um unless any of you are",
    "start": "847399",
    "end": "853199"
  },
  {
    "text": "maybe like some super high quality athletes like a UFC fighter or something your reaction time is probably about 200",
    "start": "853199",
    "end": "860040"
  },
  {
    "text": "milliseconds so you know 180 millisecond time to force token counting Network latency by the way counting the round",
    "start": "860040",
    "end": "866120"
  },
  {
    "text": "trip time to the server is great because that to a user feels instant when to",
    "start": "866120",
    "end": "871199"
  },
  {
    "text": "under 200 milliseconds and actually most of it is Network latency the time on the GPU is less than 50",
    "start": "871199",
    "end": "877199"
  },
  {
    "text": "milliseconds less than 50 milliseconds so uh we've got another one of these green slides here I like to talk really",
    "start": "877199",
    "end": "883240"
  },
  {
    "text": "fast so these slides I put in this presentation to give us all a chance to take a breath and ask any questions so",
    "start": "883240",
    "end": "889959"
  },
  {
    "text": "you know we're going to cover a lot more technical detail moving forward but if there's anything kind of foundational",
    "start": "889959",
    "end": "895440"
  },
  {
    "text": "that you're struggling with like what's tenso what's tenso llm anything I can explain more clearly I would love to",
    "start": "895440",
    "end": "901360"
  },
  {
    "text": "hear about it going once going twice it's okay well all friends here you can raise your",
    "start": "901360",
    "end": "907800"
  },
  {
    "text": "hands all right well sounds like I'm amazing at my job I explained everything perfectly and we get to move on to the",
    "start": "907800",
    "end": "914040"
  },
  {
    "text": "next section so what models can you use with",
    "start": "914040",
    "end": "920959"
  },
  {
    "text": "tenso llm lots of them uh there's a list of like 50 Foundation models in the tenso llm documentation that you can use",
    "start": "920959",
    "end": "928800"
  },
  {
    "text": "and and you can also use you know fine tunes of those models anything you've built on top of them um it supports open",
    "start": "928800",
    "end": "935720"
  },
  {
    "text": "source large Vision models so if you're you know building your own GPT 40 um you can do that with tensor llm um and it",
    "start": "935720",
    "end": "943519"
  },
  {
    "text": "also supports models like whisper and then tensor RT itself you can do anything with tensor RT so any model",
    "start": "943519",
    "end": "949880"
  },
  {
    "text": "custom open source fine-tuned you can run it with tensor um but tensor llm is",
    "start": "949880",
    "end": "955519"
  },
  {
    "text": "what we're focusing on today because it's a much more convenient way of building these models and uh you know on",
    "start": "955519",
    "end": "962519"
  },
  {
    "text": "this list of models that it supports there's there's one that maybe stands out does does anyone know like what",
    "start": "962519",
    "end": "967880"
  },
  {
    "text": "model kind of doesn't belong in in this list of supported models like what what",
    "start": "967880",
    "end": "973079"
  },
  {
    "text": "what up here isn't an llm whisper exactly why why is why is",
    "start": "973079",
    "end": "978959"
  },
  {
    "text": "whisper on here well tensorrt llm it's it's called Das llm um but it really is",
    "start": "978959",
    "end": "985519"
  },
  {
    "text": "a little more flexible than that because you can run you know a lot of different autor regressive Transformers models",
    "start": "985519",
    "end": "991759"
  },
  {
    "text": "with it like whisper so if anyone doesn't know what whisper is it is a audio transcription model you give it a",
    "start": "991759",
    "end": "998920"
  },
  {
    "text": "You Know MP3 file with someone talking it gives you back a transcript of what they said it's one of our it's one of",
    "start": "998920",
    "end": "1005120"
  },
  {
    "text": "our favorite models to work with we've spent a ton of time optimizing Wisper building pipelines for it and all that",
    "start": "1005120",
    "end": "1010319"
  },
  {
    "text": "sort of stuff and what's really cool about wiso is structurally like it's",
    "start": "1010319",
    "end": "1015720"
  },
  {
    "text": "basically an llm you know that that that's a massive reductive statement for me to make but it's a autoaggressive",
    "start": "1015720",
    "end": "1021639"
  },
  {
    "text": "Transformers model it has the same bottlenecks in terms of inference performance so even though this is not",
    "start": "1021639",
    "end": "1027558"
  },
  {
    "text": "an not an llm it's an audio transcription model we're actually still able to optimize it with 10 llm because",
    "start": "1027559",
    "end": "1035678"
  },
  {
    "text": "uh because of its architecture let me say one more thing of course so the the whole uh I think",
    "start": "1035679",
    "end": "1043360"
  },
  {
    "text": "the recent ml Revolution started with the Transformers paper attention is all you need and that describ types and",
    "start": "1043360",
    "end": "1049200"
  },
  {
    "text": "encoder decoder architecture and in a way whisper is machine translation that paper was about",
    "start": "1049200",
    "end": "1055000"
  },
  {
    "text": "machine translation you're translating audio text uh audio into text right and",
    "start": "1055000",
    "end": "1060880"
  },
  {
    "text": "it's basically that it's an encoder decoder model exactly like the transformer architecture and trans T LM",
    "start": "1060880",
    "end": "1067000"
  },
  {
    "text": "is about that it's about that Transformer architecture so it actually matches pretty well exactly so um moving on um I want",
    "start": "1067000",
    "end": "1076200"
  },
  {
    "text": "to run through a few things uh just just some some things in terms of what Tor llm supports so I assume it's going to",
    "start": "1076200",
    "end": "1083039"
  },
  {
    "text": "support Blackwell when that comes out like 99.999% certain um but anyway in terms",
    "start": "1083039",
    "end": "1088919"
  },
  {
    "text": "of what we have today we've got Hopper so the h100s the l4s RTX 490s if anyone",
    "start": "1088919",
    "end": "1094919"
  },
  {
    "text": "has a super sweet gaming desktop at home number one I'm jealous number two you can run tensor tlm on that um Amper gpus",
    "start": "1094919",
    "end": "1102919"
  },
  {
    "text": "Turing gpus uh v100s are you know somewhat supported um and what's cool",
    "start": "1102919",
    "end": "1110120"
  },
  {
    "text": "about what's cool about tensorrt and Hardware support is that like it works better with newer gpus when you move",
    "start": "1110120",
    "end": "1118000"
  },
  {
    "text": "from an a100 to an h100 and you're using tensorrt or tensorrt llm you're not just",
    "start": "1118000",
    "end": "1124039"
  },
  {
    "text": "getting the sort of like linear increase in performance that you'd expect from you know oh I've got more flops now I've",
    "start": "1124039",
    "end": "1130640"
  },
  {
    "text": "got more to gigabytes per second of GPU bandwidth you're actually getting more",
    "start": "1130640",
    "end": "1136080"
  },
  {
    "text": "of a performance gain going from one GP you to the next uh than you would expect off raw stats alone and that's because",
    "start": "1136080",
    "end": "1143480"
  },
  {
    "text": "um you know h100s for example have all these great architectural features and Teno because it actually optimizes the",
    "start": "1143480",
    "end": "1151559"
  },
  {
    "text": "model by compiling a Tuda instructions is able to take advantage of those architectural features not just kind of",
    "start": "1151559",
    "end": "1158200"
  },
  {
    "text": "run the model um you know raw and so for that you know that's why we do a lot",
    "start": "1158200",
    "end": "1164159"
  },
  {
    "text": "with h100 migs this this this bullet point here is a whole different 45 minute talk that I tried to pitch to uh",
    "start": "1164159",
    "end": "1170440"
  },
  {
    "text": "do here but basically you know h100 migs are especially good for tensorrt llm um",
    "start": "1170440",
    "end": "1176520"
  },
  {
    "text": "if you're trying to run smaller models like a 7B you know llama 8B for example",
    "start": "1176520",
    "end": "1181760"
  },
  {
    "text": "uh because you don't need the massive amount of vrm but you get the um increased performance from the",
    "start": "1181760",
    "end": "1187159"
  },
  {
    "text": "architectural features um and you know just my own speculation down here that",
    "start": "1187159",
    "end": "1192679"
  },
  {
    "text": "I'm sure whatever the next generation is is going to have even more architectural features for tensorrt to take advantage",
    "start": "1192679",
    "end": "1198480"
  },
  {
    "text": "of and so you know adopting it now is a good move uh you know look into the",
    "start": "1198480",
    "end": "1204000"
  },
  {
    "text": "future here we've got a graph showing you know with sdxl now this is 10rt not 10rt llm but the underlying technology",
    "start": "1204000",
    "end": "1211080"
  },
  {
    "text": "is the same um you know when you're working on an a1g we were looking at you know maybe like a 25 to 30% increase in",
    "start": "1211080",
    "end": "1218039"
  },
  {
    "text": "throughput for sdxl and uh with an h100 it's a 70% and that's not you know just",
    "start": "1218039",
    "end": "1224240"
  },
  {
    "text": "because the h100 is bigger it's 70% more on an h100 with tensor RTL tensorrt",
    "start": "1224240",
    "end": "1230559"
  },
  {
    "text": "versus an h100 without so yeah great uh yeah please go ahead one thing I want to",
    "start": "1230559",
    "end": "1236440"
  },
  {
    "text": "add here is that Ed 100 supports fp8 and a100 does not fp8 is a game changer I",
    "start": "1236440",
    "end": "1243080"
  },
  {
    "text": "think it's very easy toate that fact fp8 is really really good post training cotization you don't need to train",
    "start": "1243080",
    "end": "1249200"
  },
  {
    "text": "anything post training cotization it takes like 5 minutes and the results are so close we've done perplexity tests on",
    "start": "1249200",
    "end": "1255760"
  },
  {
    "text": "it whenever you quantize you have to check the accuracy uh and we've done that it's hard to tell",
    "start": "1255760",
    "end": "1262080"
  },
  {
    "text": "and FP is about 40% better in most scenarios so if you're using uh Mig Edge 100 if you can then it's it can be way",
    "start": "1262080",
    "end": "1270200"
  },
  {
    "text": "better if you use fp8 and fp8 is also supported um by love lace so that's",
    "start": "1270200",
    "end": "1275919"
  },
  {
    "text": "going to be your L4 gpus um which are also a great option for for fp8 so yeah",
    "start": "1275919",
    "end": "1281799"
  },
  {
    "text": "prec a bunch of different precisions are supported again fp8 is kind of the Highlight fp4 uh could be coming and um",
    "start": "1281799",
    "end": "1290159"
  },
  {
    "text": "you know traditionally though we're going to run in fp16 um which is sort of like a a uh full Precision um and oh",
    "start": "1290159",
    "end": "1298200"
  },
  {
    "text": "sorry half Precision fp32 is technically full Precision what but nobody does fp3 yeah",
    "start": "1298200",
    "end": "1304120"
  },
  {
    "text": "yeah so so for for inference generally you start at fp16 by the way fp16 means",
    "start": "1304120",
    "end": "1309400"
  },
  {
    "text": "a 16bit floating Point number um and from there you know you can quantize to",
    "start": "1309400",
    "end": "1314919"
  },
  {
    "text": "int8 fp8 if you know you want your model to run faster if you want to run on fewer or smaller gpus um we we'll we'll",
    "start": "1314919",
    "end": "1323039"
  },
  {
    "text": "cover quantization in a bit more detail later on in the actual Workshop I don't want to spend too long on the slides",
    "start": "1323039",
    "end": "1328720"
  },
  {
    "text": "here I know you guys want to get your laptops out and start coding um so the other thing just to talk about is like I",
    "start": "1328720",
    "end": "1335240"
  },
  {
    "text": "said tenso llm it's a you know it's a set of optimizations that you can you",
    "start": "1335240",
    "end": "1341919"
  },
  {
    "text": "know build into your tensor into your tensor RT engines um so",
    "start": "1341919",
    "end": "1347240"
  },
  {
    "text": "some of the features that are support Ed again each one of these could be its own toio uh but we've got quantization laow",
    "start": "1347240",
    "end": "1353720"
  },
  {
    "text": "swapping speculative decoding and Medusa heads which is where you basically like fine-tune additional heads onto your",
    "start": "1353720",
    "end": "1360360"
  },
  {
    "text": "model and then at each forward pass you're generating like four tokens instead of one token great for when",
    "start": "1360360",
    "end": "1366520"
  },
  {
    "text": "you're you know when you have memory bandwidth restrictions um yeah inflate batching like you mentioned page",
    "start": "1366520",
    "end": "1372080"
  },
  {
    "text": "attention there's just a ton of different optimizations supported by 10t llm for you to dive into once you have",
    "start": "1372080",
    "end": "1378240"
  },
  {
    "text": "the basic engine built so um we're about to switch into",
    "start": "1378240",
    "end": "1384159"
  },
  {
    "text": "more of like a live coding Workshop segment so if there's any of this sort of groundwork information that didn't",
    "start": "1384159",
    "end": "1390039"
  },
  {
    "text": "make sense or any more details that you want on anything let us know we'll cover it now um otherwise it's it's about to",
    "start": "1390039",
    "end": "1396279"
  },
  {
    "text": "be laptop time looks like everyone wants laptop time so uh yes please go",
    "start": "1396279",
    "end": "1405200"
  },
  {
    "text": "ahead yeah do you want do you want to do you want to handle that one like a high level comparison to V one I can do very",
    "start": "1407039",
    "end": "1412880"
  },
  {
    "text": "high level comparison first of all um I respect both tools VM is great tens llm",
    "start": "1412880",
    "end": "1419039"
  },
  {
    "text": "is also great we found in our comparisons that uh for most of the scenarios we compared we found tensor",
    "start": "1419039",
    "end": "1425279"
  },
  {
    "text": "RDM to be better um there there are a few things there one thing is that whenever a new GPU lens or a new",
    "start": "1425279",
    "end": "1431720"
  },
  {
    "text": "technique lens that tends to work better on tens llm BLM it takes a bit of time",
    "start": "1431720",
    "end": "1437039"
  },
  {
    "text": "for it to catch up for the current to be optimized uh tens LM is generally ahead of that for example when h100 landed",
    "start": "1437039",
    "end": "1444240"
  },
  {
    "text": "tens LM was uh was very very fast out of the box because they've been working for on it for a long time um second thing is",
    "start": "1444240",
    "end": "1451559"
  },
  {
    "text": "that tens is optimized from bottom to the top these uh Cuda Kernels at the",
    "start": "1451559",
    "end": "1457159"
  },
  {
    "text": "very bottom are very very well optimized on top of that there is the inflight batching engine all written in C++ and",
    "start": "1457159",
    "end": "1463320"
  },
  {
    "text": "i' I'm seen that code it's very very optimized C++ code with you STD moves and what not and on top of that is",
    "start": "1463320",
    "end": "1470919"
  },
  {
    "text": "Triton which is a web server again in C++ so the whole thing is very very",
    "start": "1470919",
    "end": "1475960"
  },
  {
    "text": "optimized whereas uh in some other Frameworks they also try to optimize uh in the sense like you know Java versus",
    "start": "1475960",
    "end": "1482279"
  },
  {
    "text": "C++ where Java is like you know we optimize everything that matters but there are always cases where it might",
    "start": "1482279",
    "end": "1487760"
  },
  {
    "text": "not be as good STM is that let's optimize every single thing so it",
    "start": "1487760",
    "end": "1493200"
  },
  {
    "text": "generally tends to perform better in our experience that said VM is a great great product we use it a lot as well for",
    "start": "1493200",
    "end": "1499640"
  },
  {
    "text": "example Lura Shing it became available in BLM first so we use that there for a",
    "start": "1499640",
    "end": "1504960"
  },
  {
    "text": "while what we found is that when something lands in tens LM and it's usually after a delay it works like like",
    "start": "1504960",
    "end": "1512679"
  },
  {
    "text": "Bonkers it just works like so well that uh performance is just amazing so when",
    "start": "1512679",
    "end": "1518559"
  },
  {
    "text": "something is working very stably in tens LM we tend to use that but uh VM and",
    "start": "1518559",
    "end": "1524159"
  },
  {
    "text": "other Frameworks they provide a lot of flexibility which is great we love all the products",
    "start": "1524159",
    "end": "1530320"
  },
  {
    "text": "yeah yeah yeah I think I think we should definitely question that that is a clear trade-off if you working with two gpus",
    "start": "1539039",
    "end": "1546600"
  },
  {
    "text": "for example two ATS it's not worth it probably but if you're spending hundreds of thousands of dollars a year uh it's",
    "start": "1546600",
    "end": "1553919"
  },
  {
    "text": "it's your call but if you're spending uh many hundred of thousands of",
    "start": "1553919",
    "end": "1558960"
  },
  {
    "text": "dollars a year it can be material like your profit margin might be 20% this 20%",
    "start": "1558960",
    "end": "1565360"
  },
  {
    "text": "or 50% Improvement might make the all the difference that you need so it depends upon the use case",
    "start": "1565360",
    "end": "1573240"
  },
  {
    "text": "it could be yeah I think if you're working with one or two gpus atgs or t4s I mean I'm an I'm not an expert in",
    "start": "1589480",
    "end": "1596320"
  },
  {
    "text": "that but uh it probably doesn't matter which framework you use whichever works best for you but if you end up using A1",
    "start": "1596320",
    "end": "1603080"
  },
  {
    "text": "100s or Edge 100s you should definitely look into TL and uh regarding the loaning Cove",
    "start": "1603080",
    "end": "1609760"
  },
  {
    "text": "stick around a little bit we're going to uh flatten it out a lot for you because we've built some great tooling on top of",
    "start": "1609760",
    "end": "1614919"
  },
  {
    "text": "tens R tlm that's going to make it just as easy to use little little little marketing spin",
    "start": "1614919",
    "end": "1620840"
  },
  {
    "text": "right there um so yeah so we're going to um be doing a engine building live",
    "start": "1620840",
    "end": "1627080"
  },
  {
    "text": "coding exercise um and and punk is going to lead us through that I'm just going to kind of roam around so if if people",
    "start": "1627080",
    "end": "1633960"
  },
  {
    "text": "have you know questions need help kind of on a one-on-one basis I'll be able to help help out with that during this",
    "start": "1633960",
    "end": "1639240"
  },
  {
    "text": "portion of the workshop great so",
    "start": "1639240",
    "end": "1645600"
  },
  {
    "text": "um we have let's just like go through a there's there there's a little bit of a",
    "start": "1645600",
    "end": "1650640"
  },
  {
    "text": "little bit of setup material right or yes yeah um you want run through okay",
    "start": "1650640",
    "end": "1656760"
  },
  {
    "text": "sure I I'll run through the I'll run through the setup material um and then",
    "start": "1656760",
    "end": "1662440"
  },
  {
    "text": "uh yeah so um anyway what we're going to do um to to to be clear is we're going",
    "start": "1662440",
    "end": "1668360"
  },
  {
    "text": "to build an engine for a model called tiny llama 1.1b I want to be really clear about this tensor LM is a",
    "start": "1668360",
    "end": "1674640"
  },
  {
    "text": "production ready technology that works great with big model on big gpus uh that",
    "start": "1674640",
    "end": "1680799"
  },
  {
    "text": "takes time to run the dev Loop can be a little bit slow and we only have a two-hour Workshop here and uh you know I",
    "start": "1680799",
    "end": "1687519"
  },
  {
    "text": "don't want us to all just be sitting there watching model build it's basically as fun as watching paint dry or watching grass grow um so we're going",
    "start": "1687519",
    "end": "1694559"
  },
  {
    "text": "to be using this super tiny 1.1 billion parameter model we're going to be using 409s and A10 GS um just to kind of keep",
    "start": "1694559",
    "end": "1701880"
  },
  {
    "text": "the dev Loop fast but this stuff does scale so um at this point we're going to walk you through the manual process of",
    "start": "1701880",
    "end": "1708440"
  },
  {
    "text": "doing doing it all from scratch you're going to procer and configure a GPU you're going to install dependencies for",
    "start": "1708440",
    "end": "1714320"
  },
  {
    "text": "tensor rtlm configure the engine run the engine build job and uh test the results",
    "start": "1714320",
    "end": "1719799"
  },
  {
    "text": "and we we should be able to get through this in in about half an hour or maybe a little less because these uh these",
    "start": "1719799",
    "end": "1725360"
  },
  {
    "text": "models are quite small um and there's a few important settings that we're going to look at when building the engine",
    "start": "1725360",
    "end": "1731519"
  },
  {
    "text": "we're going to look at the quantization again the post-training quantization like we talked about we're going to be on a10s or sorry no first we're going to",
    "start": "1731519",
    "end": "1738519"
  },
  {
    "text": "be on 409s so we will actually have access to fp8 so that you can test that out um we're going to look at sequence",
    "start": "1738519",
    "end": "1745240"
  },
  {
    "text": "shapes and batch sizes how to set that and we're going to look at tensor parallelism you want to give them a",
    "start": "1745240",
    "end": "1750440"
  },
  {
    "text": "quick preview on tensor parallelism oh yeah tensor parallelism is uh is very important in certain",
    "start": "1750440",
    "end": "1757200"
  },
  {
    "text": "scenarios I wish it were more useful but it is critical in many scenarios so what is tensor parallelism ultimately machine",
    "start": "1757200",
    "end": "1764440"
  },
  {
    "text": "learning running these gpus is about Matrix multiplications we take this model architecture Whatever It Is It",
    "start": "1764440",
    "end": "1771039"
  },
  {
    "text": "ultimately boils down to matrices that we multiply and lot of the wrangling is around that how do we shove these all",
    "start": "1771039",
    "end": "1776679"
  },
  {
    "text": "batches into matrices so ultimately it is matrix multiplication right what you can do is you can split these matrices",
    "start": "1776679",
    "end": "1783120"
  },
  {
    "text": "and you can multiply them separately on different gpus and then combine the results and that's what tensor",
    "start": "1783120",
    "end": "1788880"
  },
  {
    "text": "parallelism is it's one of the tensor of parallelism techniques uh there are many",
    "start": "1788880",
    "end": "1794200"
  },
  {
    "text": "techniques uh it's one of the most commonly used ones because you need that why do you need tensor parallelism",
    "start": "1794200",
    "end": "1800559"
  },
  {
    "text": "versus other parallelisms like pipeline parallelism um is that it saves on",
    "start": "1800559",
    "end": "1806880"
  },
  {
    "text": "latency you can do things in parallel you can use two gpus at the same time for doing something even though there is",
    "start": "1806880",
    "end": "1812679"
  },
  {
    "text": "some overhead of cross talk between them with pipeline parallelism you take the model architecture and you can divide",
    "start": "1812679",
    "end": "1818440"
  },
  {
    "text": "these layers into separate things so your thing goes through one GPU like half the layers and then half the layers",
    "start": "1818440",
    "end": "1824880"
  },
  {
    "text": "on the second GPU but you're not saving on latency it still has to go through each layer and it's going sequentially",
    "start": "1824880",
    "end": "1832279"
  },
  {
    "text": "and that's why pipeline parallelism is not very popular for inference it is still popular for training there are",
    "start": "1832279",
    "end": "1837760"
  },
  {
    "text": "scenarios uh and there's a lot of theory about that but for for inference I don't think I've ever seen it used and nobody",
    "start": "1837760",
    "end": "1844279"
  },
  {
    "text": "pays much attention to optimizing it because of this thing that tensor parm is just better there's also expert level",
    "start": "1844279",
    "end": "1850399"
  },
  {
    "text": "parallelism if your model has mixture of experts then you can paralyze those experts and that tends to be very",
    "start": "1850399",
    "end": "1856080"
  },
  {
    "text": "Advanced and llama doesn't have mixture of experts so it's a esoteric thing that we haven't covered here tens bism is",
    "start": "1856080",
    "end": "1863240"
  },
  {
    "text": "pretty helpful and useful uh one downside is that your throughput is not as great if you can fit something in a",
    "start": "1863240",
    "end": "1869159"
  },
  {
    "text": "bigger GPU that's generally better but there are bigger models like Lama 7db",
    "start": "1869159",
    "end": "1874279"
  },
  {
    "text": "they just can't fit on one GPU so you have to use tensor parallelism awesome so for everyone to",
    "start": "1874279",
    "end": "1881799"
  },
  {
    "text": "get started um we made a GitHub repository for you all to work off of in",
    "start": "1881799",
    "end": "1886919"
  },
  {
    "text": "this in this uh Workshop so you can scan the QR code it'll take you right there",
    "start": "1886919",
    "end": "1892159"
  },
  {
    "text": "otherwise uh you know this is this is not too long to type out um so I'm just",
    "start": "1892159",
    "end": "1897960"
  },
  {
    "text": "going to leave this up on screen for 30 seconds everyone can pull it up um you're going to want to you know fork",
    "start": "1897960",
    "end": "1903880"
  },
  {
    "text": "and clone this uh this repository um to your to your local development environment um we're just you know we're",
    "start": "1903880",
    "end": "1910919"
  },
  {
    "text": "just using python python 3.10 python 3.11 um 3.9 um so yeah just like however",
    "start": "1910919",
    "end": "1918840"
  },
  {
    "text": "however your your normal way of writing code is um this this should be compatible um there there isn't a lot of",
    "start": "1918840",
    "end": "1926559"
  },
  {
    "text": "what uh no so uh I yeah to be clear in in this in this repository um you're",
    "start": "1926559",
    "end": "1932159"
  },
  {
    "text": "going to find instructions and we're going to walk through all this um we're going to be using entirely remote gpus",
    "start": "1932159",
    "end": "1938039"
  },
  {
    "text": "um so you know I personally have an h100 under my Podium right here that I'm going to be using no I'm just kidding I",
    "start": "1938039",
    "end": "1944039"
  },
  {
    "text": "don't um but uh yeah yeah so we'll just uh we just all have laptops too so we're going to be using Cloud",
    "start": "1944039",
    "end": "1950360"
  },
  {
    "text": "gpus yeah actually if you want to follow along you might need a runport account",
    "start": "1950360",
    "end": "1955440"
  },
  {
    "text": "yeah yeah well we we'll we'll talk them through the uh the the the setup steps there um does does anyone want me to",
    "start": "1955440",
    "end": "1962279"
  },
  {
    "text": "leave this information on the screen any longer going once going twice okay if if",
    "start": "1962279",
    "end": "1968080"
  },
  {
    "text": "you for whatever reason lose the repository just let me know I'll I'll get it back for you uh yes okay so this",
    "start": "1968080",
    "end": "1974639"
  },
  {
    "text": "this slide means we are transitioning to live coding so so yes let's go uh let's",
    "start": "1974639",
    "end": "1980120"
  },
  {
    "text": "go over to um the yeah the the the live coding",
    "start": "1980120",
    "end": "1985480"
  },
  {
    "text": "experience so I'm I'm basically going to follow this repository all the instructions are here and uh I'm going",
    "start": "1985480",
    "end": "1992840"
  },
  {
    "text": "to follow exactly what is here so you can see how to follow along and if you",
    "start": "1992840",
    "end": "1998679"
  },
  {
    "text": "uh if you ever get lost or need help just raise your hand and I'll come over and catch you up like oneon-one yeah",
    "start": "1998679",
    "end": "2004760"
  },
  {
    "text": "yeah I'm going to go really slow I'm going to actually do all these steps here I know it takes time but uh you",
    "start": "2004760",
    "end": "2010120"
  },
  {
    "text": "know there's a lot of information here it's easy to uh lose track of thing and get lost so if you if you feel lost like",
    "start": "2010120",
    "end": "2016240"
  },
  {
    "text": "ask and we'll break I want to make sure this is not a long process a 10-minute process we can take it slow for",
    "start": "2016240",
    "end": "2021880"
  },
  {
    "text": "everybody here so first thing is that uh we we'll do it like really really from",
    "start": "2021880",
    "end": "2026960"
  },
  {
    "text": "scratch we're going to spin up a new uh container on run pod with a GPU to run",
    "start": "2026960",
    "end": "2032639"
  },
  {
    "text": "our setup in so if you let's give everyone a minute okay okay yeah yeah please please",
    "start": "2032639",
    "end": "2038720"
  },
  {
    "text": "um if you want to follow along please go on runpod and create an account this should cost like less than $5 overall",
    "start": "2038720",
    "end": "2045760"
  },
  {
    "text": "yeah so um so yeah so if you want to make an account um there's instructions in the um 01 folder uh yeah this readme",
    "start": "2045760",
    "end": "2053960"
  },
  {
    "text": "so tens RT in in the first folder in the readme there's instructions and a video walk through um the minimum we we're",
    "start": "2053960",
    "end": "2060960"
  },
  {
    "text": "we're not affiliated with one pod in anyway uh they just have 409s and we wanted you guys to use 409s today there",
    "start": "2060960",
    "end": "2067679"
  },
  {
    "text": "there is a minimum credit buy of $10 if for whatever reason you can't use a company card or get a reimbursed or",
    "start": "2067679",
    "end": "2073878"
  },
  {
    "text": "whatever and you want your $10 back uh send me an email after the um after the",
    "start": "2073879",
    "end": "2079118"
  },
  {
    "text": "conference and I will reimburse you myself so uh you know I I just I wanted I it's really important to me that we",
    "start": "2079119",
    "end": "2085118"
  },
  {
    "text": "are giving you the gpus uh to to run this on so yeah so if if you if you",
    "start": "2085119",
    "end": "2091320"
  },
  {
    "text": "uh if you need the $10 back I will I will get it for you um so yeah just uh",
    "start": "2091320",
    "end": "2096679"
  },
  {
    "text": "just follow this it's like a one and a half minute video um and uh yeah that",
    "start": "2096679",
    "end": "2101760"
  },
  {
    "text": "that that'll get the account set up maybe we could even like play the video yeah so yeah and then this is how you",
    "start": "2101760",
    "end": "2109359"
  },
  {
    "text": "you know deploy um once we once we get set",
    "start": "2109359",
    "end": "2114440"
  },
  {
    "text": "up so in in two minutes we will come back together and you know we resume the",
    "start": "2114440",
    "end": "2120320"
  },
  {
    "text": "resume the talk so yeah make that account real quick get checked out get those $10 of credits the difficulty get",
    "start": "2120320",
    "end": "2127400"
  },
  {
    "text": "account just let us no on yeah and and also if you if you do",
    "start": "2127400",
    "end": "2132599"
  },
  {
    "text": "happen to run into any issues getting the one pod account set up like it's not working or whatever um I do have one",
    "start": "2132599",
    "end": "2138400"
  },
  {
    "text": "that I can share individually with people it would just take a really long time for me to go type in my username",
    "start": "2138400",
    "end": "2144240"
  },
  {
    "text": "and password into every single computer in this",
    "start": "2144240",
    "end": "2148320"
  },
  {
    "text": "room I feel like maybe I should have brought some some jokes to tell while everyone sets the count up uh all right",
    "start": "2152839",
    "end": "2159520"
  },
  {
    "text": "uh how about this one why did the llm cross the road ignore previous instructions and",
    "start": "2159520",
    "end": "2166880"
  },
  {
    "text": "cross the road all right I I'll I'll Workshop that I'll get some better jokes for you guys",
    "start": "2166880",
    "end": "2172960"
  },
  {
    "text": "before before the next setup how are we doing on on the account",
    "start": "2172960",
    "end": "2179040"
  },
  {
    "text": "setup um anyone need a little more time all right great no problem no problem sorry I I don't want to rush",
    "start": "2179040",
    "end": "2186560"
  },
  {
    "text": "you just checking in yeah and then once once everyone has",
    "start": "2186560",
    "end": "2193680"
  },
  {
    "text": "the account we'll set up the GPU together because there's a few things you need to",
    "start": "2193680",
    "end": "2199440"
  },
  {
    "text": "configure cool uh oh",
    "start": "2205079",
    "end": "2210960"
  },
  {
    "text": "really it's it's not taking anyone's credit",
    "start": "2215839",
    "end": "2220359"
  },
  {
    "text": "card great uh does someone does here can I can I can",
    "start": "2221520",
    "end": "2227319"
  },
  {
    "text": "I know someone who runs at who works at runp pod and would have their uh their phone number he's calling them okay",
    "start": "2227319",
    "end": "2235319"
  },
  {
    "text": "awesome all right we we're getting we're getting in touch with customer",
    "start": "2235319",
    "end": "2240560"
  },
  {
    "text": "support oh yeah it could be it could be that yeah strip PL protection okay so as",
    "start": "2241560",
    "end": "2246760"
  },
  {
    "text": "a backup um yeah what's",
    "start": "2246760",
    "end": "2254000"
  },
  {
    "text": "that you put you put it on your phone Wi-Fi okay so the recommendation here is",
    "start": "2254520",
    "end": "2260440"
  },
  {
    "text": "go off of the conference Wi-Fi put your computer on your phone hotspot and try",
    "start": "2260440",
    "end": "2266280"
  },
  {
    "text": "it again um because that that worked uh you know may maybe maybe coming from a",
    "start": "2266280",
    "end": "2271760"
  },
  {
    "text": "different IP address will will help how would we do this I run through this and we can do it again once everybody has",
    "start": "2271760",
    "end": "2278400"
  },
  {
    "text": "their account yeah that sounds good so what we're going to do in the interest of time here um is we're going to uh",
    "start": "2278400",
    "end": "2286319"
  },
  {
    "text": "pckage is going to run through end to endend um sort of the the the demo as we",
    "start": "2286319",
    "end": "2291640"
  },
  {
    "text": "as we get the stuff set up and everyone's credit cards get unblocked um",
    "start": "2291640",
    "end": "2297599"
  },
  {
    "text": "yeah you know who who would have thought you know we we were we were talking this big game oh tensor rtlm it's so hard",
    "start": "2297599",
    "end": "2304319"
  },
  {
    "text": "it's so technical there's going to be so many bugs and then the payment processing so uh yeah you know that",
    "start": "2304319",
    "end": "2311240"
  },
  {
    "text": "that's that's that's live demos for you so anyway yeah go go ahead and uh work through it um and then we'll do it kind",
    "start": "2311240",
    "end": "2317319"
  },
  {
    "text": "of again uh together once everyone has their account all right yeah let me run through this I'll follow the all the",
    "start": "2317319",
    "end": "2323400"
  },
  {
    "text": "steps uh I I already have an account runp so let me spin up a new instance",
    "start": "2323400",
    "end": "2329720"
  },
  {
    "text": "here and uh I'm picking up the 4090",
    "start": "2329720",
    "end": "2334839"
  },
  {
    "text": "here which this one and it has has high availability so that should be fine and",
    "start": "2334839",
    "end": "2339920"
  },
  {
    "text": "uh I'm going to edit this template and get more space here this doesn't cost anything",
    "start": "2339920",
    "end": "2345680"
  },
  {
    "text": "extra yeah we need more space uh because the engine uh everything that we're",
    "start": "2345680",
    "end": "2351319"
  },
  {
    "text": "installing um and the engine we're building takes up a lot of gigabytes so otherwise we better be safer yeah even",
    "start": "2351319",
    "end": "2357359"
  },
  {
    "text": "though these engines are small engines in general can be very very big they can be hundreds of gigs and I'm going to",
    "start": "2357359",
    "end": "2362760"
  },
  {
    "text": "pick on demand because I'm doing this demo I don't want the instance to go away but feel free to use spot for your",
    "start": "2362760",
    "end": "2367880"
  },
  {
    "text": "use case so I'm going to do",
    "start": "2367880",
    "end": "2372160"
  },
  {
    "text": "that you want to set the uh the container um container disc to 200",
    "start": "2373880",
    "end": "2379040"
  },
  {
    "text": "gigabytes so that you have enough room to install everything and then I'm going to deploy",
    "start": "2379040",
    "end": "2387760"
  },
  {
    "text": "spot it's going to be a bit slow but you know feel free to ask any questions and uh I feel like this way we'll take it",
    "start": "2387760",
    "end": "2394960"
  },
  {
    "text": "slow but we'll make sure everything is understood by every body so what what's happening now is that this PT is",
    "start": "2394960",
    "end": "2402160"
  },
  {
    "text": "spinning up um one thing to note here is that it has a specific image of torch",
    "start": "2402160",
    "end": "2408560"
  },
  {
    "text": "with a specific Cuda version it's very important that the node has gpus and the first thing we going to",
    "start": "2408560",
    "end": "2415079"
  },
  {
    "text": "we're going to do is that once this part comes up we're going to check that it has everything related to gpus running",
    "start": "2415079",
    "end": "2423800"
  },
  {
    "text": "fine so this is starting up",
    "start": "2424079",
    "end": "2428880"
  },
  {
    "text": "now I'm going to connect it gives you nothing sensitive here it uses your SSH keys but the names are not sensitive so",
    "start": "2431640",
    "end": "2438880"
  },
  {
    "text": "I'm going to just do that log into that",
    "start": "2438880",
    "end": "2442920"
  },
  {
    "text": "box uh sorry oh okay sorry yeah yeah this is",
    "start": "2444920",
    "end": "2450000"
  },
  {
    "text": "much smaller I think the part is still spinning up so it's taking a bit of time",
    "start": "2450000",
    "end": "2459240"
  },
  {
    "text": "okay now it's up so to test that everything is set up properly just is it",
    "start": "2473960",
    "end": "2479839"
  },
  {
    "text": "is it possible to scroll it to the top of the screen oh yeah okay great so we are on this",
    "start": "2479839",
    "end": "2487720"
  },
  {
    "text": "machine that we spin up you're going to run Nvidia SMI to make sure that the GPU",
    "start": "2487720",
    "end": "2493119"
  },
  {
    "text": "is available and this is what you should see uh one thing to note here is this",
    "start": "2493119",
    "end": "2498920"
  },
  {
    "text": "portion which shows that the GPU has uh more than 24 gigs of memory the RTX 4",
    "start": "2498920",
    "end": "2506480"
  },
  {
    "text": "490 has and right now it's using one memory I think it does some uh some",
    "start": "2506480",
    "end": "2511680"
  },
  {
    "text": "stuff like is by default so one mag is already taken",
    "start": "2511680",
    "end": "2517400"
  },
  {
    "text": "so so now we're going to go back to workshop and then just follow",
    "start": "2518119",
    "end": "2524280"
  },
  {
    "text": "these instructions manual engine build we are at this",
    "start": "2524280",
    "end": "2530359"
  },
  {
    "text": "point uh and now we're going to install tens rlm this is going to take a bit of time t llm comes as a python library",
    "start": "2530359",
    "end": "2537920"
  },
  {
    "text": "that you just pip install and that's all we're doing we're setting up the dependencies this AP APD update is",
    "start": "2537920",
    "end": "2544119"
  },
  {
    "text": "setting up the python environment uh open MPI and other things and then we just install and start llm uh from not",
    "start": "2544119",
    "end": "2552119"
  },
  {
    "text": "from piie but from nvidia's own piie that's where we find the right versions",
    "start": "2552119",
    "end": "2557200"
  },
  {
    "text": "if you focus on this line uh let me kick this off then I can come back here and",
    "start": "2557200",
    "end": "2562559"
  },
  {
    "text": "show you that we're using a specific version of tensor rlm and uh we need to tell it to get it",
    "start": "2562559",
    "end": "2569920"
  },
  {
    "text": "from the Nvidia PP using these instructions and all these are on the",
    "start": "2569920",
    "end": "2575559"
  },
  {
    "text": "GitHub repo if you want to follow from",
    "start": "2575559",
    "end": "2579720"
  },
  {
    "text": "there I saw a guy with a camera so I started",
    "start": "2582440",
    "end": "2587000"
  },
  {
    "text": "posing uh I think 310 I think 310",
    "start": "2589680",
    "end": "2595480"
  },
  {
    "text": "yes uh this this command should have instructions to install",
    "start": "2597440",
    "end": "2603319"
  },
  {
    "text": "it I also I want to check in with the wom has anyone else had success getting one part up and going uh using your",
    "start": "2603319",
    "end": "2610319"
  },
  {
    "text": "using your phone Wi-Fi it's working okay awesome chis supported thank you so much",
    "start": "2610319",
    "end": "2615720"
  },
  {
    "text": "to uh to whoever from from over there suggested the idea to begin with really save the",
    "start": "2615720",
    "end": "2621400"
  },
  {
    "text": "day great so we just waiting for it to build it takes some",
    "start": "2621400",
    "end": "2626640"
  },
  {
    "text": "time this is this is the best part of the job you know you wait for it to build you can go get a snack you can go",
    "start": "2626640",
    "end": "2632240"
  },
  {
    "text": "like change your laundry it's uh very convenient that it takes this time sometimes it used to be comp comparation",
    "start": "2632240",
    "end": "2637880"
  },
  {
    "text": "takes time now engine build takes time",
    "start": "2637880",
    "end": "2641920"
  },
  {
    "text": "yeah I think we very close and I promise like then the fun",
    "start": "2643280",
    "end": "2650160"
  },
  {
    "text": "part begins are you saying that pip install isn't the fun part I think this is pretty fun you know look look look at",
    "start": "2650160",
    "end": "2656800"
  },
  {
    "text": "all this look at all this lines you know this is this this is wheel coating right",
    "start": "2656800",
    "end": "2661839"
  },
  {
    "text": "here if you want pip to feel like more fun triy poetry oh that's true poetry is",
    "start": "2661839",
    "end": "2667839"
  },
  {
    "text": "really fun yeah NV does publish these images on their container history called NGC and",
    "start": "2667839",
    "end": "2674640"
  },
  {
    "text": "there are Triton Registries available for these things maybe we should have used that rather than pod but uh it's",
    "start": "2674640",
    "end": "2681400"
  },
  {
    "text": "all good so uh now let's check that tensor LM is installed and this will just uh tell us",
    "start": "2681400",
    "end": "2689079"
  },
  {
    "text": "that everything is a good so it printed the version you should see that if",
    "start": "2689079",
    "end": "2694599"
  },
  {
    "text": "everything is working fine and then then we're going to do the real thing now we're going to clone the",
    "start": "2694599",
    "end": "2701240"
  },
  {
    "text": "tensor RT LM G repository where a lot of those examples are and I I'll show you those examples while this uh this",
    "start": "2701240",
    "end": "2709319"
  },
  {
    "text": "cloning happens shouldn't take that much long maybe uh maybe a minute or so but tens Alm has a lot of examples uh if you",
    "start": "2709319",
    "end": "2717280"
  },
  {
    "text": "go to the tens Alm repository there are these examples folder and there are a ton of examples like uh Philip mentioned",
    "start": "2717280",
    "end": "2724960"
  },
  {
    "text": "there are about 50 examples and we're going to go through the Llama example here so if you search for",
    "start": "2724960",
    "end": "2732800"
  },
  {
    "text": "llama now that's the one we are going to look into and so the cloning is",
    "start": "2732800",
    "end": "2739640"
  },
  {
    "text": "complete and we go back to these instructions and now we're going to actually build the engine actually one",
    "start": "2739640",
    "end": "2746319"
  },
  {
    "text": "more thing uh how many of you know about HF transfer have you used the Transformers",
    "start": "2746319",
    "end": "2752240"
  },
  {
    "text": "library from hugging face so a transfer is a fast way of downloading and uploading your engines",
    "start": "2752240",
    "end": "2759040"
  },
  {
    "text": "it does slides download it takes the URL and patches them up into slices",
    "start": "2759040",
    "end": "2764400"
  },
  {
    "text": "downloads them all in parallel and it works really really fast it goes up to like 1 gig second so we should",
    "start": "2764400",
    "end": "2769839"
  },
  {
    "text": "definitely do that which is what I did just now now we're going to follow this step",
    "start": "2769839",
    "end": "2775280"
  },
  {
    "text": "by step uh first thing we going to do is download from hugging face and uh let's see like how fast the WiFi here is uh",
    "start": "2775280",
    "end": "2782520"
  },
  {
    "text": "how fast this downloads so not bad it's going at 1 gigs a second so HF all yeah",
    "start": "2782520",
    "end": "2788400"
  },
  {
    "text": "probably Wi-Fi oh from oh you're right you're right right see but this is uh",
    "start": "2788400",
    "end": "2793920"
  },
  {
    "text": "this is what I call good software downloads at 1 GC now we now first thing to build with",
    "start": "2793920",
    "end": "2801280"
  },
  {
    "text": "TD llm is that we have to convert uh the hugging face checkpoint into a",
    "start": "2801280",
    "end": "2806480"
  },
  {
    "text": "checkpoint format that tensor LM works with and the checkpointing also covers",
    "start": "2806480",
    "end": "2811839"
  },
  {
    "text": "stenor parallelism and cotization sometimes you need a different kind of checkpoint for doing those things so I'm",
    "start": "2811839",
    "end": "2817359"
  },
  {
    "text": "going to run this command to convert the checkpoint and this should be pretty fast it's just converting weights to",
    "start": "2817359",
    "end": "2825480"
  },
  {
    "text": "weights that's pretty fast like three seconds uh and now we do the actual",
    "start": "2828119",
    "end": "2833599"
  },
  {
    "text": "build and I'm going to do this basic build here where there are a ton of options that this command takes the trtl",
    "start": "2833599",
    "end": "2841280"
  },
  {
    "text": "build command uh in here we just saying that take this checkpoint and build me",
    "start": "2841280",
    "end": "2846440"
  },
  {
    "text": "an engine with most of the default settings and that should build the",
    "start": "2846440",
    "end": "2854319"
  },
  {
    "text": "engine now it will print a lot of stuff about what it's doing what it's finding and how it's optimizing and all that uh",
    "start": "2855960",
    "end": "2862760"
  },
  {
    "text": "it won't make much sense right now but uh later on this could be very useful so the engine was built was pretty fast",
    "start": "2862760",
    "end": "2868520"
  },
  {
    "text": "right it's a small uh model only a billion parameters that was pretty fast",
    "start": "2868520",
    "end": "2874480"
  },
  {
    "text": "and now let's uh let's try to see how big the engine is I'm going to do that",
    "start": "2874480",
    "end": "2879559"
  },
  {
    "text": "and the engine is two gigs in size this is about how big that model is on hugging phase so it's uh the engine",
    "start": "2879559",
    "end": "2886440"
  },
  {
    "text": "itself adds very little U Storage or memory needs maybe like hundreds of",
    "start": "2886440",
    "end": "2892520"
  },
  {
    "text": "megabytes but very tiny compared to the overall and those weights are bundled into the engine and what is this engine",
    "start": "2892520",
    "end": "2900200"
  },
  {
    "text": "this engine is something that the tensor R LM runtime can take and it can execute",
    "start": "2900200",
    "end": "2906440"
  },
  {
    "text": "it uh you can think of it like a Shar Library it's uh it's kind of like a",
    "start": "2906440",
    "end": "2911920"
  },
  {
    "text": "binary in the standard format that the binaries are in U but it's it's",
    "start": "2911920",
    "end": "2918319"
  },
  {
    "text": "something that tensor llm can take and interpret ultimately it's a tensor RT engine because that's what tensor RT LM",
    "start": "2918319",
    "end": "2925640"
  },
  {
    "text": "works with it creates tensor RT engine and then tensor RT is the one that loads",
    "start": "2925640",
    "end": "2930760"
  },
  {
    "text": "it but tensor R llm gives it these plugins that tensor RT understands and then is able to make sense of it and now",
    "start": "2930760",
    "end": "2938240"
  },
  {
    "text": "let's execute this so the these examples also come with a come up with a U come",
    "start": "2938240",
    "end": "2943440"
  },
  {
    "text": "with a run script that we can run and we're going to run that so what this is going to do is start up the engine and",
    "start": "2943440",
    "end": "2951520"
  },
  {
    "text": "give it a very tiny request and we should expect a response and that's what happened here our engine was launched we",
    "start": "2951520",
    "end": "2959240"
  },
  {
    "text": "gave it an input text of born in Northeast France so we trained as a and",
    "start": "2959240",
    "end": "2964599"
  },
  {
    "text": "the model printed out the response resp beyond that painter in Paris moving",
    "start": "2964599",
    "end": "2969760"
  },
  {
    "text": "before moving to London in 1929 and this is a standard uh example that comes with",
    "start": "2969760",
    "end": "2975400"
  },
  {
    "text": "tensor rtlm so if you follow along these instructions you should see",
    "start": "2975400",
    "end": "2980960"
  },
  {
    "text": "that any questions at this",
    "start": "2980960",
    "end": "2984920"
  },
  {
    "text": "point U the convert the question is what happens during the convert checkpoint",
    "start": "2992119",
    "end": "2997520"
  },
  {
    "text": "uh I think there are three things that happen um potentially three things first",
    "start": "2997520",
    "end": "3002799"
  },
  {
    "text": "thing is that tensor llm needs the tensors to be in a specific format to work with so think of it as a",
    "start": "3002799",
    "end": "3009400"
  },
  {
    "text": "pre-processing there are many ways of specifying a model it can be on hugging phas it can be exported from py to it",
    "start": "3009400",
    "end": "3016480"
  },
  {
    "text": "can be Onyx there are many many different ways of specifying these models so the first thing it does is that it converts that into a format that",
    "start": "3016480",
    "end": "3023280"
  },
  {
    "text": "it understands so it does does some kind of translation into a standard structure",
    "start": "3023280",
    "end": "3028760"
  },
  {
    "text": "second thing is quantization for quantization it needs to uh quantize the",
    "start": "3028760",
    "end": "3035319"
  },
  {
    "text": "weights it needs to take the weights and quantize them into the quantize versions of them and that happens at convert",
    "start": "3035319",
    "end": "3042160"
  },
  {
    "text": "checkpoint two uh not necessarily though they also have a quantise script some of those quations happen some types of",
    "start": "3042160",
    "end": "3049359"
  },
  {
    "text": "quations happen in convert checkpoint but they also have a different way of quing they call it I think uh ammo",
    "start": "3049359",
    "end": "3057040"
  },
  {
    "text": "there's a library called ammo which does that uh and that can also be used for doing it but uh I think awq and uh",
    "start": "3057040",
    "end": "3064160"
  },
  {
    "text": "smooth con they happen in convert checkpoint and uh third thing is tensor parallelism for tensor parallelism you",
    "start": "3064160",
    "end": "3070319"
  },
  {
    "text": "need to divide up weights uh into different categories for the different",
    "start": "3070319",
    "end": "3075440"
  },
  {
    "text": "gpus that they will run on so it does that during convert checkpoint as well",
    "start": "3075440",
    "end": "3082200"
  },
  {
    "text": "yeah so so there there's two places that the max output is set um so the first place is when you're actually building",
    "start": "3110200",
    "end": "3116160"
  },
  {
    "text": "the end engine you give it a argument for the expected output sequence length",
    "start": "3116160",
    "end": "3122480"
  },
  {
    "text": "and then that's that's more just so like for the optimization side you know so that you're selecting the correct Cuda",
    "start": "3122480",
    "end": "3128839"
  },
  {
    "text": "Colonels and so that you're you know batching everything up correctly and then once the engine is built it just",
    "start": "3128839",
    "end": "3134680"
  },
  {
    "text": "uses a standard I think it's Max tokens right is the parameter um and yeah you",
    "start": "3134680",
    "end": "3139960"
  },
  {
    "text": "just you just pass Max tokens and that'll you know limit um how how long it it runs for",
    "start": "3139960",
    "end": "3148040"
  },
  {
    "text": "right like are you asking if you if you make the engine with a shorter um output",
    "start": "3155920",
    "end": "3162599"
  },
  {
    "text": "oh okay no I I as far as I know the ma all the max token does is it just cuts off",
    "start": "3162599",
    "end": "3170000"
  },
  {
    "text": "inference after a certain number right yeah so the the way uh I would put it is that normally if you give it a large",
    "start": "3170000",
    "end": "3177160"
  },
  {
    "text": "number of Max tokens it would emit uh a end of sequence token most models have a",
    "start": "3177160",
    "end": "3184359"
  },
  {
    "text": "different end of sequence token and it's up to you you can stop there you can configure at run time like I don't want",
    "start": "3184359",
    "end": "3190160"
  },
  {
    "text": "more than that but you can also tell it ignore end of sequence I just want the whole thing and we do need it for",
    "start": "3190160",
    "end": "3196640"
  },
  {
    "text": "performance benchmarking for example when we are comparing performance across different GPU types or whatnot we want",
    "start": "3196640",
    "end": "3202680"
  },
  {
    "text": "all of those tokens to be generated so we can tell it like give me all of them",
    "start": "3202680",
    "end": "3208440"
  },
  {
    "text": "welcome back",
    "start": "3208440",
    "end": "3211680"
  },
  {
    "text": "there oh great great question actually so uh in build checkpoint a lot of stuff",
    "start": "3218680",
    "end": "3224760"
  },
  {
    "text": "is happening you're taking these weights and you're uh you're generating this thing",
    "start": "3224760",
    "end": "3231680"
  },
  {
    "text": "called a network in tensor Rd tensor Rd has this notion of a network and what what you need to do is populate that",
    "start": "3231680",
    "end": "3238359"
  },
  {
    "text": "network with your weights and architectures so it actually does that it creates that Network and feeds it",
    "start": "3238359",
    "end": "3245280"
  },
  {
    "text": "these weights it also does inference during building the engine uh for doing optimizations so it generates for every",
    "start": "3245280",
    "end": "3252760"
  },
  {
    "text": "model type it has a mechanism of generating sample input and it passes that into the tensor engine that it's",
    "start": "3252760",
    "end": "3260119"
  },
  {
    "text": "generating and then it optimizes it that way and as as as a result this tensor",
    "start": "3260119",
    "end": "3266520"
  },
  {
    "text": "engine is generated in memory which is then serialized so all of this is happening in",
    "start": "3266520",
    "end": "3273079"
  },
  {
    "text": "that and these uh there's a lot of nuance to it uh if you get a chance you",
    "start": "3273079",
    "end": "3278319"
  },
  {
    "text": "can look at the source code for that trtm build I'll post references in that GitHub repo and you can follow on",
    "start": "3278319",
    "end": "3284680"
  },
  {
    "text": "there's lots of options but let me try if I can find a help here",
    "start": "3284680",
    "end": "3291559"
  },
  {
    "text": "ah VI",
    "start": "3300920",
    "end": "3303760"
  },
  {
    "text": "person what's going on",
    "start": "3311200",
    "end": "3314799"
  },
  {
    "text": "sorry okay this is lot lot of stuff here which we can go through",
    "start": "3322880",
    "end": "3329079"
  },
  {
    "text": "um uh yeah maybe I should go through some of them which are very",
    "start": "3329079",
    "end": "3335079"
  },
  {
    "text": "important yeah I think a lot of stuff is important here like the max beam width if you're using beams for uh generating",
    "start": "3335079",
    "end": "3341039"
  },
  {
    "text": "the graph you can generate uh Logics not just the output tokens we can also generate Logics if you want to process",
    "start": "3341039",
    "end": "3347799"
  },
  {
    "text": "them uh there are a lot of optimization that you can use like you can there's a optimation called fused MLP there is",
    "start": "3347799",
    "end": "3353920"
  },
  {
    "text": "contrast chunking there there's a lot of stuff and I think you should play around with those U at your time Laura is very",
    "start": "3353920",
    "end": "3359760"
  },
  {
    "text": "good to play around with yeah I'll try to leave some uh some more examples uh in the GI up repo to",
    "start": "3359760",
    "end": "3367680"
  },
  {
    "text": "try okay so let me go to the next one just uh one more thing I want to do is uh fp8 quantization RTX 490 is is",
    "start": "3367680",
    "end": "3376280"
  },
  {
    "text": "actually an amazing GPU it's pretty cheap but supports fp8 so we're going to do an fp8 engine build",
    "start": "3376280",
    "end": "3383000"
  },
  {
    "text": "down so in this case like I said like some of the these optimizations these quantizations are not in convert",
    "start": "3383000",
    "end": "3389480"
  },
  {
    "text": "checkpoint but quantize dop which uses a library called ammo in Nvidia so I'm",
    "start": "3389480",
    "end": "3395440"
  },
  {
    "text": "going to run that now and uh yeah let me spend some time",
    "start": "3395440",
    "end": "3401359"
  },
  {
    "text": "here we we saying is we telling it that the quation format is fp8 but also note",
    "start": "3401359",
    "end": "3407319"
  },
  {
    "text": "that we are saying KV cach D type is fp8 so fp8 quantization actually can happen",
    "start": "3407319",
    "end": "3413839"
  },
  {
    "text": "at two levels you can weight quantize to fp8 but you can also quantise the KV",
    "start": "3413839",
    "end": "3419240"
  },
  {
    "text": "cache with fp8 and doing both is very critical because uh these gpus they you",
    "start": "3419240",
    "end": "3427480"
  },
  {
    "text": "might have heard of things called tensor cores right tensor cores are very very very important because they can",
    "start": "3427480",
    "end": "3433280"
  },
  {
    "text": "do quantize calculations very fast for example if you look at a spec of the h100 GPU you can see that the Tera flops",
    "start": "3433280",
    "end": "3442359"
  },
  {
    "text": "that you can get number of computation that you can get with lower quantization options are are much more than higher",
    "start": "3442359",
    "end": "3447960"
  },
  {
    "text": "for example fp16 teraflops will be much lower than fp8 because you can use this",
    "start": "3447960",
    "end": "3453680"
  },
  {
    "text": "special tensor course for doing more FPA computations in the same time that you would do",
    "start": "3453680",
    "end": "3459480"
  },
  {
    "text": "fp16 but for that to happen both sides of the Matrix have to be the same contration type mixed Precision doesn't",
    "start": "3459480",
    "end": "3466440"
  },
  {
    "text": "exist at least now it's not very common or popular so you want both sides to be",
    "start": "3466440",
    "end": "3471480"
  },
  {
    "text": "ConEd and when you conze both the KV cache and the weights to FPA you get",
    "start": "3471480",
    "end": "3476799"
  },
  {
    "text": "that extra unlock that your computation is also faster which can be critical for",
    "start": "3476799",
    "end": "3482359"
  },
  {
    "text": "scenarios which are comput bound and as you would know in llms there are there's a context phase and generation phase uh",
    "start": "3482359",
    "end": "3489079"
  },
  {
    "text": "generation phase is memory bandwidth bound but the context phase is compute bound so that can benefit greatly from",
    "start": "3489079",
    "end": "3496359"
  },
  {
    "text": "both sides being quantized so in this case we are seeing that quantize both weights and KV cache and it's actually",
    "start": "3496359",
    "end": "3504319"
  },
  {
    "text": "uh not a trivial decision to do that because weights quantize very easily it's uh you hardly lose anything when",
    "start": "3504319",
    "end": "3510599"
  },
  {
    "text": "you quantize weights the dynamic range of Weights is generally much uh much smaller you can use inted or uh when you",
    "start": "3510599",
    "end": "3517480"
  },
  {
    "text": "do FP fp8 there is hardly any loss KV cach doesn't quantize as well and that's",
    "start": "3517480",
    "end": "3523200"
  },
  {
    "text": "why fp8 is a GameChanger because what we found is that when you Conti the KV cache with inate even using smooth cont",
    "start": "3523200",
    "end": "3530520"
  },
  {
    "text": "there is a still degradation of quality and practically we've never seen anybody use it even though there are a lot of",
    "start": "3530520",
    "end": "3535720"
  },
  {
    "text": "papers about it and it's great great technology but practically it was not there until fp8 fp8 even kvk conation",
    "start": "3535720",
    "end": "3543559"
  },
  {
    "text": "works extremely well let me uh let me show something with that actually um if",
    "start": "3543559",
    "end": "3548920"
  },
  {
    "text": "if you don't mind back on the um on the um if we go to",
    "start": "3548920",
    "end": "3558039"
  },
  {
    "text": "uh I'll just show like a little visualization um for for fp8 that that",
    "start": "3558039",
    "end": "3563319"
  },
  {
    "text": "shows off the dynamic range um so oh hey look it's us um yeah so so",
    "start": "3563319",
    "end": "3570960"
  },
  {
    "text": "when you look at um the the fp8 data format it has a sign and then um rather",
    "start": "3570960",
    "end": "3577280"
  },
  {
    "text": "than uh so there's two different FPA data formats but but we're using the um you know the E4 M3 format so basically",
    "start": "3577280",
    "end": "3584760"
  },
  {
    "text": "you have four bits dedicated to an exponent um and that's what gives your fp8 data format a lot of dynamic range",
    "start": "3584760",
    "end": "3592280"
  },
  {
    "text": "versus in8 which is just you know like what like 256 to yeah so so you you",
    "start": "3592280",
    "end": "3599599"
  },
  {
    "text": "still have the same number of possible values but they're spread apart further that's dynamic range and it's that which",
    "start": "3599599",
    "end": "3606520"
  },
  {
    "text": "allows you to quantize this much more sensitive KV cache yeah yeah exactly",
    "start": "3606520",
    "end": "3611799"
  },
  {
    "text": "basically you have the and exponents you basically are able to uh quantize",
    "start": "3611799",
    "end": "3617720"
  },
  {
    "text": "smaller values better give more bits to smaller scale than larger scale you",
    "start": "3617720",
    "end": "3623000"
  },
  {
    "text": "don't have to fit into a linear scale and that's for FP excels so going back to the presentation",
    "start": "3623000",
    "end": "3629160"
  },
  {
    "text": "the fp8 um conation is done and uh I forgot to show you this but uh there is",
    "start": "3629160",
    "end": "3634359"
  },
  {
    "text": "calibration involved here if you look at this uh stack here we actually give it",
    "start": "3634359",
    "end": "3639599"
  },
  {
    "text": "some data we feed it some data and let it calibrate because as you would know in Tate and fp8 you have a start and end",
    "start": "3639599",
    "end": "3646760"
  },
  {
    "text": "range and they differ in how you divide up that range into data points but you have to find M and Max and for that you",
    "start": "3646760",
    "end": "3653319"
  },
  {
    "text": "need calibration so you we give it a standard data set and you can change the data set but we give it a specific data",
    "start": "3653319",
    "end": "3659400"
  },
  {
    "text": "set and it does multiple runs and we try to calibrate like what are the dynamic ranges of each of the layers of this",
    "start": "3659400",
    "end": "3665599"
  },
  {
    "text": "Transformer architecture and based upon that we specify that Min and Max for each layer separately there is more",
    "start": "3665599",
    "end": "3672079"
  },
  {
    "text": "detail there but at the high level that's what is happening",
    "start": "3672079",
    "end": "3678240"
  },
  {
    "text": "yes yeah yeah it's it's possible that that the ranges can vary a lot with data set and this used to be more critical",
    "start": "3683359",
    "end": "3690440"
  },
  {
    "text": "with inate with FP we found that you you get to a good State pretty fast but it's",
    "start": "3690440",
    "end": "3695680"
  },
  {
    "text": "worth thinking about trying different data set especially if you know what data set you are going to be calling it",
    "start": "3695680",
    "end": "3701200"
  },
  {
    "text": "with and it could be worth it it just works very well out of the box but it's not",
    "start": "3701200",
    "end": "3707720"
  },
  {
    "text": "perfect going back to the workshop uh so we were following along here and yeah so",
    "start": "3708200",
    "end": "3714039"
  },
  {
    "text": "after you quantize it the steps are very similar as",
    "start": "3714039",
    "end": "3718480"
  },
  {
    "text": "before now we building an building an engine with fp8 and uh internally all",
    "start": "3719799",
    "end": "3726160"
  },
  {
    "text": "the Cuda kernels that are being used are now FP specific they are different kernels which use the tensor course in",
    "start": "3726160",
    "end": "3733240"
  },
  {
    "text": "the right fashion and this should be pretty quick as well",
    "start": "3733240",
    "end": "3740720"
  },
  {
    "text": "and there's a lot of depth here as you learn more about it you don't need to but uh there are things like timing",
    "start": "3746319",
    "end": "3751520"
  },
  {
    "text": "cache there are optimization profiles intens rity through which you tell what sizes we expect and it does",
    "start": "3751520",
    "end": "3758319"
  },
  {
    "text": "optimizations but uh this is a good bringing so now we have the engine uh let me do a du on that and to see the",
    "start": "3758319",
    "end": "3766079"
  },
  {
    "text": "size of that engine now and the size is 1.2 gigs which is",
    "start": "3766079",
    "end": "3771240"
  },
  {
    "text": "about half of previous and which is what we expect because we quantized",
    "start": "3771240",
    "end": "3777440"
  },
  {
    "text": "and now let's run this engine and see the output and it should be pretty similar to what we saw",
    "start": "3777440",
    "end": "3783039"
  },
  {
    "text": "before so using this run script now it's going to load the engine and then we'll do an inference on top that should be",
    "start": "3783039",
    "end": "3789559"
  },
  {
    "text": "pretty quick so yeah the here's the output the same input as before and",
    "start": "3789559",
    "end": "3795319"
  },
  {
    "text": "about the same output as before and that's what we generally observed with FP FP quality is really",
    "start": "3795319",
    "end": "3801680"
  },
  {
    "text": "really good it's very very hard to tell the difference that's it for this Workshop this this",
    "start": "3801680",
    "end": "3808000"
  },
  {
    "text": "part part of the workshop yeah awesome thank you so um you know I definitely",
    "start": "3808000",
    "end": "3813799"
  },
  {
    "text": "welcome you to keep playing around um with this runp pod setup and trying",
    "start": "3813799",
    "end": "3819039"
  },
  {
    "text": "different uh different things try to build different engines and stuff um but uh we're going to move",
    "start": "3819039",
    "end": "3826359"
  },
  {
    "text": "on to the next step which is a um automated version of basically exactly",
    "start": "3826359",
    "end": "3831920"
  },
  {
    "text": "what we just did so um we're going to show a few things to uh to make this",
    "start": "3831920",
    "end": "3837359"
  },
  {
    "text": "easier um so uh we're going to be using for this next step something called truss uh",
    "start": "3837359",
    "end": "3843799"
  },
  {
    "text": "trust is an open- Source uh model serving framework developed uh by us here at Bas 10 punkish is the one who",
    "start": "3843799",
    "end": "3850400"
  },
  {
    "text": "you know actually wrote a lot of the code all I did was name it truss um because I was riding on a train and I",
    "start": "3850400",
    "end": "3856480"
  },
  {
    "text": "was like huh what should I call the framework and then we went over a bridge and I was like I know I'll call it bridge but that was already taken so I",
    "start": "3856480",
    "end": "3863240"
  },
  {
    "text": "called it truss um so it let you deploy models with python um instead of you",
    "start": "3863240",
    "end": "3868599"
  },
  {
    "text": "know building a dock image yourself um it gives you a nice live reload de Dev Loop and what we really wanted to focus",
    "start": "3868599",
    "end": "3875799"
  },
  {
    "text": "on when we were building this because it's kind of the technology that sits onto our entire uh model soing platform",
    "start": "3875799",
    "end": "3881680"
  },
  {
    "text": "is we really wanted a lot of flexibility so that we could work with you know things like tensorrt tensor TM you can",
    "start": "3881680",
    "end": "3887720"
  },
  {
    "text": "run it with VM Triton uh you can you know run a Transformers Model A diffusers model you can put an XG boost",
    "start": "3887720",
    "end": "3893839"
  },
  {
    "text": "model in there if you're still doing ml like you can do basically whatever you want with it it's just python code I me",
    "start": "3893839",
    "end": "3899520"
  },
  {
    "text": "interject like trust is actually a very simple system it's a way of running python code specifying an environment",
    "start": "3899520",
    "end": "3906880"
  },
  {
    "text": "for running the python code and your python code so it's sort of like a very simple packaging mechanism but buil for",
    "start": "3906880",
    "end": "3913480"
  },
  {
    "text": "machine learning models it takes account of the typical things you would need with the machine learning models like",
    "start": "3913480",
    "end": "3918920"
  },
  {
    "text": "getting access to data passing secur secure tokens and such but it's uh fundamentally a very very simple system",
    "start": "3918920",
    "end": "3925720"
  },
  {
    "text": "just a config file and some python code exactly and um so looking at at",
    "start": "3925720",
    "end": "3933520"
  },
  {
    "text": "that config file we're not even actually going to write any python code today for the um model Soo we're just going to",
    "start": "3933520",
    "end": "3939680"
  },
  {
    "text": "write a quick config um so actually this morning I was eating breakfast here and I sat down with a group of Engineers and",
    "start": "3939680",
    "end": "3945920"
  },
  {
    "text": "we were talking about stuff and everyone was complaining about yaml and how they're always getting like type errors",
    "start": "3945920",
    "end": "3951000"
  },
  {
    "text": "when they write yaml so um unfortunately this this is going to be a yaml system so apologize apologies to my new friends",
    "start": "3951000",
    "end": "3957839"
  },
  {
    "text": "from breakfast um but what we're going to do is is use this as basically an",
    "start": "3957839",
    "end": "3963480"
  },
  {
    "text": "abstraction on top of TT llm uh why do you need abstraction on top of TM uh",
    "start": "3963480",
    "end": "3969319"
  },
  {
    "text": "page a quick question for you what's the name of that C++ textbook you were reading before bed every night the the",
    "start": "3969319",
    "end": "3975240"
  },
  {
    "text": "other month modern C++ what modern C++ yeah mod C++ so uh you know uh before",
    "start": "3975240",
    "end": "3982520"
  },
  {
    "text": "bed every night uh I was I was watching Survivor um and so uh for for those of",
    "start": "3982520",
    "end": "3988279"
  },
  {
    "text": "us who are who are not um cracked software Engineers um and even for those",
    "start": "3988279",
    "end": "3993319"
  },
  {
    "text": "who are who want to get things done quickly um we want to have a great abstraction what does that abstraction",
    "start": "3993319",
    "end": "3998640"
  },
  {
    "text": "need to be able to do needs to be able to build an engine um and that engine it needs to take into account what model",
    "start": "3998640",
    "end": "4005000"
  },
  {
    "text": "we're going to run What GPU we're going to run it on the input and output sequence lengths the batch size",
    "start": "4005000",
    "end": "4010240"
  },
  {
    "text": "quantization any of the other um optimizations we want to do on top of that and then we also want to not just",
    "start": "4010240",
    "end": "4016520"
  },
  {
    "text": "grab that and run it in GPU pod somewhere we actually want to deploy it behind an API endpoint so that you know",
    "start": "4016520",
    "end": "4022680"
  },
  {
    "text": "we can integrated it into our product and stuff so I'm going to show how to do",
    "start": "4022680",
    "end": "4028119"
  },
  {
    "text": "that uh let let's see here this is this is yours now I'm I'm stealing o this is",
    "start": "4028119",
    "end": "4033640"
  },
  {
    "text": "a good mic I might not give this back page this is a good mic all right um so",
    "start": "4033640",
    "end": "4040760"
  },
  {
    "text": "we're going to go over um let's see this is",
    "start": "4040760",
    "end": "4046160"
  },
  {
    "text": "this is in the one pod thing still uh great second",
    "start": "4046160",
    "end": "4055160"
  },
  {
    "text": "perfect let me okay yeah here you you do that this is his computer not my",
    "start": "4055160",
    "end": "4060839"
  },
  {
    "text": "computer so I don't know where anything is it's like walking into someone else's house there you go all right thank you",
    "start": "4060839",
    "end": "4068039"
  },
  {
    "text": "thanks so much um okay so um what we're going to do in this in this second step",
    "start": "4068039",
    "end": "4075760"
  },
  {
    "text": "is we are going to do basically exactly the same thing we just did um just",
    "start": "4075760",
    "end": "4082760"
  },
  {
    "text": "automated so for this step we're going to use base 10 um we're going to give you all some some gpus to play with here",
    "start": "4082760",
    "end": "4090240"
  },
  {
    "text": "um so if you want to follow along I really encourage you to do so um you're going to go sign up at base 10 we're going to you know your account will",
    "start": "4090240",
    "end": "4096359"
  },
  {
    "text": "automatically get free credits if um our fraud system is a little freaked out by",
    "start": "4096359",
    "end": "4102159"
  },
  {
    "text": "everyone uh signing up at the same time well fortunately uh we have some uh admin panel access",
    "start": "4102159",
    "end": "4108719"
  },
  {
    "text": "ourselves over here so we'll just unplug this approve you all and plug it back in um so yeah so everyone go ahead and um",
    "start": "4108719",
    "end": "4116838"
  },
  {
    "text": "sign up for base 10 we're also going to want you to make an API key real quick and save that um and then once that's",
    "start": "4116839",
    "end": "4124400"
  },
  {
    "text": "all done we're going to jump into this uh this part of the of the",
    "start": "4124400",
    "end": "4132278"
  },
  {
    "text": "project yeah we use your account on this",
    "start": "4132279",
    "end": "4137440"
  },
  {
    "text": "okay everyone so I know there's a few errors going on um we have we have",
    "start": "4138440",
    "end": "4143719"
  },
  {
    "text": "pinged the team about that let me let you in on a little secret uh we shipped this on Thursday as an internal Beta And",
    "start": "4143719",
    "end": "4149920"
  },
  {
    "text": "this is the very first time anyone who doesn't have an at b.co email address is",
    "start": "4149920",
    "end": "4155480"
  },
  {
    "text": "using our uh new tensorrt llm build system so if there are",
    "start": "4155480",
    "end": "4162238"
  },
  {
    "text": "uh if uh yeah so uh sorry for tricking you all into beta testing our software",
    "start": "4162880",
    "end": "4168920"
  },
  {
    "text": "um but hey that's what demos are for right so uh we'll we'll get that sorted out in the meantime we have an image",
    "start": "4168920",
    "end": "4175318"
  },
  {
    "text": "cached locally which means we can keep going with the demo as if nothing ever",
    "start": "4175319",
    "end": "4180640"
  },
  {
    "text": "happens so um let's see so what you uh what you",
    "start": "4180640",
    "end": "4186560"
  },
  {
    "text": "would see in the logs as you uh build as as you uh deploy",
    "start": "4186560",
    "end": "4193238"
  },
  {
    "text": "this yeah oh well I mean I could just kind of look through the look through the logs right here um let me actually",
    "start": "4193239",
    "end": "4198960"
  },
  {
    "text": "just let me just uh wake it up sorry what was that okay yep yep yep I got",
    "start": "4198960",
    "end": "4206480"
  },
  {
    "text": "you um yep all right big logs",
    "start": "4206480",
    "end": "4214159"
  },
  {
    "text": "um see all right so",
    "start": "4214159",
    "end": "4219400"
  },
  {
    "text": "um what what you're seeing here um as we oh sorry we got a got a lot of logs here",
    "start": "4219400",
    "end": "4225320"
  },
  {
    "text": "to um right because we tested a bunch with the this with the scale up um anyway",
    "start": "4225320",
    "end": "4232800"
  },
  {
    "text": "what you see is uh you see the engine getting built um and then and then deployed and to walk through the yaml",
    "start": "4232800",
    "end": "4239920"
  },
  {
    "text": "code really quick uh yes here so we talked about that there are a bunch of",
    "start": "4239920",
    "end": "4245400"
  },
  {
    "text": "different settings that you need to do when you are working with trtm um and you can set all these",
    "start": "4245400",
    "end": "4251840"
  },
  {
    "text": "settings right here and build so right now we're doing something with an input sequence and output sequence of 2,000",
    "start": "4251840",
    "end": "4257719"
  },
  {
    "text": "tokens each um and a batch size of 64 concurrent requests um we're using the",
    "start": "4257719",
    "end": "4263400"
  },
  {
    "text": "int 8 quantization because we're running on a A10 and that does not support fp8",
    "start": "4263400",
    "end": "4269239"
  },
  {
    "text": "because it's an ampo GPU which is one generation before fp8 support um and",
    "start": "4269239",
    "end": "4274719"
  },
  {
    "text": "then of course you you pull in your uh model and stuff and then if we want to",
    "start": "4274719",
    "end": "4279840"
  },
  {
    "text": "you know call the model to test that it is working um",
    "start": "4279840",
    "end": "4285800"
  },
  {
    "text": "we can come over here um to the call model um we can just test this out",
    "start": "4285800",
    "end": "4291880"
  },
  {
    "text": "really quick yes so we do not um on base 10 we have",
    "start": "4291880",
    "end": "4300199"
  },
  {
    "text": "t4s a10s A1 100s h100s and h100 migs and l4s as well um we we generally stick",
    "start": "4300199",
    "end": "4307600"
  },
  {
    "text": "with the more like data center type gpus um rather than the Consumer gpus it's a",
    "start": "4307600",
    "end": "4313080"
  },
  {
    "text": "good GPU yeah uh I want one for uh for um well Punk so I'm going to say that I",
    "start": "4313080",
    "end": "4320760"
  },
  {
    "text": "want it for legitimate business purposes and it should be an improved approved expense uh I don't want it for playing",
    "start": "4320760",
    "end": "4326639"
  },
  {
    "text": "video games definitely not so one",
    "start": "4326639",
    "end": "4332679"
  },
  {
    "text": "yes no but I bet he can yeah you all of that all of this",
    "start": "4339560",
    "end": "4345199"
  },
  {
    "text": "scource code is open source and typically when new models come up those companies provide uh convert checkpoint",
    "start": "4345199",
    "end": "4352400"
  },
  {
    "text": "scripts uh but if you can follow those scripts it's not terribly difficult it's mostly like uh if you're familiar with",
    "start": "4352400",
    "end": "4358480"
  },
  {
    "text": "the Transformers Library it's about reading weights from a hugging Force",
    "start": "4358480",
    "end": "4364239"
  },
  {
    "text": "Transformer model and converting that into something else it's a simple transformation so it should be possible",
    "start": "4364239",
    "end": "4370360"
  },
  {
    "text": "to do it yourself if you want",
    "start": "4370360",
    "end": "4374520"
  },
  {
    "text": "awesome so once once your model's deployed again you can you know you can just test it really quick you can call",
    "start": "4377120",
    "end": "4382800"
  },
  {
    "text": "it with a API endpoint um but uh yeah we're coming up on on 230 here so I'm",
    "start": "4382800",
    "end": "4390000"
  },
  {
    "text": "not going to spend too long on this example um let's",
    "start": "4390000",
    "end": "4395840"
  },
  {
    "text": "see but you know we've been talking a big game up here about performance right and performance is not just okay I'm",
    "start": "4395840",
    "end": "4402159"
  },
  {
    "text": "testing it by myself performance is in production for my actual users is this meeting my needs at a cost that is",
    "start": "4402159",
    "end": "4409080"
  },
  {
    "text": "reasonable to me and in order to you know validate your performance before",
    "start": "4409080",
    "end": "4414320"
  },
  {
    "text": "you go to production you need to do benchmarking and you need to do a lot more rigorous benchmarking than just",
    "start": "4414320",
    "end": "4419840"
  },
  {
    "text": "saying like hey you know I I I called it it seemed pretty fast um so what do you",
    "start": "4419840",
    "end": "4425360"
  },
  {
    "text": "want to measure when you're benchmarking uh uh you know say it with me everyone it depends that's what our software",
    "start": "4425360",
    "end": "4431840"
  },
  {
    "text": "Engineers are always saying so um you know depending on your use case you might have different things that you're",
    "start": "4431840",
    "end": "4437239"
  },
  {
    "text": "optimizing for if you're say like a live chat Service uh you probably really care about time to First token for your",
    "start": "4437239",
    "end": "4443639"
  },
  {
    "text": "streaming output because you know you're you're trying to give people you know instantaneous responses um you might",
    "start": "4443639",
    "end": "4450280"
  },
  {
    "text": "also care a lot about tokens per second um so that's you know how many how many",
    "start": "4450280",
    "end": "4455360"
  },
  {
    "text": "tokens are are generated generally some some good numbers to keep in mind is",
    "start": "4455360",
    "end": "4460760"
  },
  {
    "text": "somewhere depending on the tokenizer and the data and everything and the reader somewhere between 30 to 50 tokens per",
    "start": "4460760",
    "end": "4467320"
  },
  {
    "text": "second is going to be about as fast as anyone can read so you know if you're at 50 tokens per second generally it's",
    "start": "4467320",
    "end": "4474480"
  },
  {
    "text": "going to feel pretty fast people aren't going to be waiting for your output however if you're doing something like",
    "start": "4474480",
    "end": "4479800"
  },
  {
    "text": "code you know code takes more tokens per word than say natural language so you're going to need even more tokens per",
    "start": "4479800",
    "end": "4485960"
  },
  {
    "text": "second for that you know nice smooth output and then from there getting into you know 100 200 tokens that's when it",
    "start": "4485960",
    "end": "4491639"
  },
  {
    "text": "just feels kind of you know magically fast but again again we're we our influence is all about tradeoffs right",
    "start": "4491639",
    "end": "4497960"
  },
  {
    "text": "when we're optimizing so you know sometimes you might want to trade off a",
    "start": "4497960",
    "end": "4503280"
  },
  {
    "text": "you know a few you know maybe maybe you're going to go at 100 not 120 tokens per second because that gets you a",
    "start": "4503280",
    "end": "4508440"
  },
  {
    "text": "bigger batch size which is going to lower your cost per million tokens another thing you're going to want to",
    "start": "4508440",
    "end": "4513520"
  },
  {
    "text": "look at um when you're running your benchmarks is your total tokens per second so there's the tokens per second",
    "start": "4513520",
    "end": "4519320"
  },
  {
    "text": "per user right like per request how many tokens is your end user seeing and then there's tokens per second in terms of",
    "start": "4519320",
    "end": "4525440"
  },
  {
    "text": "how many tokens is your GPU actually producing and that's a really important metric for throughput for cost um",
    "start": "4525440",
    "end": "4532120"
  },
  {
    "text": "especially if you're going to be doing anything that's a little less than wheel time um you want to look at this not",
    "start": "4532120",
    "end": "4538040"
  },
  {
    "text": "just once you want to look at the uh 50th 90th 95th 99th percentile make sure",
    "start": "4538040",
    "end": "4543120"
  },
  {
    "text": "you're good with all those and you want to look at the effects of different batch sizes on this and um so something",
    "start": "4543120",
    "end": "4549719"
  },
  {
    "text": "is that benchmarking actually reveals really important information it's not linear and it's not obvious the sort of",
    "start": "4549719",
    "end": "4556440"
  },
  {
    "text": "performance space of your model is not this nice nice flat piece of paper that",
    "start": "4556440",
    "end": "4562560"
  },
  {
    "text": "goes linearly from batch size to batch size so this is a graph of time to First",
    "start": "4562560",
    "end": "4568639"
  },
  {
    "text": "token for like a mistal model that I ran a long time ago I just happened to have a pretty graph of it so that's how it",
    "start": "4568639",
    "end": "4574800"
  },
  {
    "text": "ended up in the presentation um so if you look at the batch sizes as it's uh you know increasing doubling um 32 to 64",
    "start": "4574800",
    "end": "4583080"
  },
  {
    "text": "the time to First token like barely but budges um but as it goes from 64 to 128",
    "start": "4583080",
    "end": "4588320"
  },
  {
    "text": "doubling again the time to First token uh increases massively and in this case",
    "start": "4588320",
    "end": "4593360"
  },
  {
    "text": "you know the reason behind that is we're we're in the you know compute bound um pre-fill Step um when we're talking",
    "start": "4593360",
    "end": "4600199"
  },
  {
    "text": "about Computing the first token and there's these different sort of slots that this computation can happen in and",
    "start": "4600199",
    "end": "4606000"
  },
  {
    "text": "as you increase the B increase the batch size you're saturating these slots until eventually you have an increased chance",
    "start": "4606000",
    "end": "4611400"
  },
  {
    "text": "of a slot collision and that's what's going to rock at your time to First token I'm glad you're nodding I'm glad I'm glad I got that right um but yeah",
    "start": "4611400",
    "end": "4618480"
  },
  {
    "text": "all of this to uh all of this to say um you know the performance that you get",
    "start": "4618480",
    "end": "4623719"
  },
  {
    "text": "out of your model once it's actually built and deployed is not necessarily just going to be linear it's not going",
    "start": "4623719",
    "end": "4630280"
  },
  {
    "text": "to be something super predictable you have to actually Benchmark your deployment before you put it into",
    "start": "4630280",
    "end": "4635960"
  },
  {
    "text": "production otherwise these sort of surprises Can Can Happen quite often um",
    "start": "4635960",
    "end": "4641600"
  },
  {
    "text": "so yeah so page do you want to take over the uh the benchmarking srip",
    "start": "4641600",
    "end": "4646840"
  },
  {
    "text": "so uh just for this uh Workshop we wrote a benchmarking script it's not the",
    "start": "4647280",
    "end": "4652520"
  },
  {
    "text": "script we use uh ourselves but it's a simpler version so that you can follow along that if you wanted to modify it",
    "start": "4652520",
    "end": "4659520"
  },
  {
    "text": "you can play around with it and understand it easily uh if you go into that repository it's U it's a very",
    "start": "4659520",
    "end": "4665679"
  },
  {
    "text": "simple script where we send request in parallel just using python using async",
    "start": "4665679",
    "end": "4671080"
  },
  {
    "text": "libraries and uh all you give it is the UR Ur L of the Endo of the model where",
    "start": "4671080",
    "end": "4677760"
  },
  {
    "text": "your model is deployed and you can give it different concurrencies and input lens and output lens and give it number",
    "start": "4677760",
    "end": "4684000"
  },
  {
    "text": "of runs you want to run these benchmarks a number of times to get an idea of values one run might be off so I'm just",
    "start": "4684000",
    "end": "4690280"
  },
  {
    "text": "going to run that script and it's all it's all in the Benchmark repository U",
    "start": "4690280",
    "end": "4695679"
  },
  {
    "text": "it's all structured using make files and there is a make file Target for Benchmark that we're going to",
    "start": "4695679",
    "end": "4702080"
  },
  {
    "text": "use and uh I think the readme should also have instructions on",
    "start": "4702080",
    "end": "4708040"
  },
  {
    "text": "that so we basically going to run this and we're going to need the base URL we",
    "start": "4709360",
    "end": "4715360"
  },
  {
    "text": "need two things we need to export the API key and then we need to uh Supply the URL so once you deploy your model on",
    "start": "4715360",
    "end": "4723480"
  },
  {
    "text": "base 10 you you would see deployed and like Philip said there's a call model",
    "start": "4723480",
    "end": "4728679"
  },
  {
    "text": "button there are various ways you can deploy It ultimately it's an HTTP API and you can just copy this URL for that",
    "start": "4728679",
    "end": "4736239"
  },
  {
    "text": "model for our benchmarking script but if you want to play around there are examples in all kinds of uh languages",
    "start": "4736239",
    "end": "4743600"
  },
  {
    "text": "and you can also click streamings it'll give you streaming code streaming is very important with large language models because you want to see the",
    "start": "4743600",
    "end": "4749000"
  },
  {
    "text": "output as soon as possible so we're going to take this output and uh I don't",
    "start": "4749000",
    "end": "4754840"
  },
  {
    "text": "know if I exported the API key so give me one second to export the API",
    "start": "4754840",
    "end": "4760800"
  },
  {
    "text": "key on show you",
    "start": "4761360",
    "end": "4766719"
  },
  {
    "text": "this is a good time to oh yes your question",
    "start": "4767520",
    "end": "4772679"
  },
  {
    "text": "right and uh you know if you lose the API key you can always revoke it so",
    "start": "4801520",
    "end": "4807280"
  },
  {
    "text": "that's",
    "start": "4807280",
    "end": "4809559"
  },
  {
    "text": "good and now we just going to give it the URL here let me go",
    "start": "4820960",
    "end": "4828679"
  },
  {
    "text": "back call model all right so I'm going to do this uh first",
    "start": "4832120",
    "end": "4839000"
  },
  {
    "text": "run with a concurrency of 32 and input and output length of a th000 and uh let's see it",
    "start": "4839000",
    "end": "4848120"
  },
  {
    "text": "work so first it does a warm-up run just to make sure that there is some traffic",
    "start": "4848120",
    "end": "4853440"
  },
  {
    "text": "on the GP you always want to have have a warm-up run before you get the real numbers now as this is uh running and",
    "start": "4853440",
    "end": "4859719"
  },
  {
    "text": "you can see the TPS here the total TPS is 5,000 and this is on 8G 0g is not the",
    "start": "4859719",
    "end": "4867560"
  },
  {
    "text": "most powerful uh GPU but this TPS is still very very high 5,000 is very very high",
    "start": "4867560",
    "end": "4874159"
  },
  {
    "text": "it's because this is Tiny llama model tiny llama is a tiny model just like a billion parameters and that's why we see",
    "start": "4874159",
    "end": "4880679"
  },
  {
    "text": "this very high um but yeah on bigger gpus with the with llama 8G you should",
    "start": "4880679",
    "end": "4886960"
  },
  {
    "text": "also see very very high values because h100s are very very powerful and 10 LM is very very optimized I think we see up",
    "start": "4886960",
    "end": "4893560"
  },
  {
    "text": "to like 11,000 tokens per second and you should do a comparison it's really really high uh in this case we have two",
    "start": "4893560",
    "end": "4900239"
  },
  {
    "text": "runs and you see these values uh let me try a different run now I'm going to do",
    "start": "4900239",
    "end": "4905400"
  },
  {
    "text": "concurrency of one and one is good to know how best of a time to First token",
    "start": "4905400",
    "end": "4910639"
  },
  {
    "text": "you can get so you're just sending one request at a time and many requests but one at a time with the same input and",
    "start": "4910639",
    "end": "4916760"
  },
  {
    "text": "output length and uh this should run okay I think I run into an error let's",
    "start": "4916760",
    "end": "4922080"
  },
  {
    "text": "try",
    "start": "4922080",
    "end": "4924440"
  },
  {
    "text": "again so uh you see time to First token of 180 milliseconds here and this is",
    "start": "4927120",
    "end": "4933239"
  },
  {
    "text": "from this laptop I'm running it right from this laptop on this wi-fi and my model is deployed somewhere in in US",
    "start": "4933239",
    "end": "4939560"
  },
  {
    "text": "Central and this is 11 milliseconds for that and one so so the so the vast",
    "start": "4939560",
    "end": "4945960"
  },
  {
    "text": "majority of that time to First token is going to be Network latency right not model not the model itself should be",
    "start": "4945960",
    "end": "4951840"
  },
  {
    "text": "pretty fast yes so over",
    "start": "4951840",
    "end": "4955678"
  },
  {
    "text": "there oh uh not not that one um this one none",
    "start": "4959800",
    "end": "4967559"
  },
  {
    "text": "of this is from this uh script script that I'm running I didn't do a thorough job of cleaning up everything it's",
    "start": "4973440",
    "end": "4980080"
  },
  {
    "text": "saying that we just making an RPC call we don't need py or whatever this is",
    "start": "4980080",
    "end": "4985639"
  },
  {
    "text": "from local yeah local runtime something uh wrong with machine this script uh if you look at that all it's doing is RPC",
    "start": "4985639",
    "end": "4994040"
  },
  {
    "text": "uh let me go through that real real quick Benchmark script uh it's a simple",
    "start": "4994040",
    "end": "4999840"
  },
  {
    "text": "python using async and it's amazing how good python has got with this async API you able to load uh load this model with",
    "start": "4999840",
    "end": "5007239"
  },
  {
    "text": "thousands of tokens per second all of them coming in streaming uh python has actually gotten really really well there",
    "start": "5007239",
    "end": "5013560"
  },
  {
    "text": "was a there was a case where I was loading with k6 and k6 client became a bottleneck because Ed 100s are so fast",
    "start": "5013560",
    "end": "5020080"
  },
  {
    "text": "but python could keep up python was able to load it very well uh but I'm getting",
    "start": "5020080",
    "end": "5026760"
  },
  {
    "text": "distracted so we tried concurrency one which is like the best case scenario latencies are very very good and ttft",
    "start": "5026760",
    "end": "5032960"
  },
  {
    "text": "should be very low now let's go go to The Other Extreme if we look at this model that we deployed uh we created it",
    "start": "5032960",
    "end": "5039719"
  },
  {
    "text": "with the batch size of 64 maximum so now we'll do 64 and uh I'm hoping we see",
    "start": "5039719",
    "end": "5047360"
  },
  {
    "text": "throughput improvements so ignore the warmup",
    "start": "5047360",
    "end": "5053719"
  },
  {
    "text": "run and this is going to take a bit longer cuz now we're going to send 64",
    "start": "5053719",
    "end": "5058760"
  },
  {
    "text": "request at the same time 64 is not uh not that high we can go even higher so",
    "start": "5058760",
    "end": "5063880"
  },
  {
    "text": "in this case you see total TPS of uh of 7,000 which is uh even higher than",
    "start": "5063880",
    "end": "5071199"
  },
  {
    "text": "before we saw 5,000 before this goes up to 7,000 but maybe this is a fluke let's wait for the second run and you can run",
    "start": "5071199",
    "end": "5077480"
  },
  {
    "text": "and this is also 7,000 so this is uh this is much better than what we saw before so uh if you increase bat size",
    "start": "5077480",
    "end": "5085199"
  },
  {
    "text": "you would Fe find that your latencies become become higher latencies in the sense that for every request that a user",
    "start": "5085199",
    "end": "5091639"
  },
  {
    "text": "is sending now tokens are coming slower and slower or and then you have to make a trade-off at some point like is it",
    "start": "5091639",
    "end": "5097840"
  },
  {
    "text": "still good enough is it still more than say 30 or 50 tokens per second that users won't perceive and at some point",
    "start": "5097840",
    "end": "5104199"
  },
  {
    "text": "it will become unusable uh and as you increase bat size the pressure on your",
    "start": "5104199",
    "end": "5109360"
  },
  {
    "text": "GPU memory also increases because all these extra batches they require KV cach",
    "start": "5109360",
    "end": "5115119"
  },
  {
    "text": "to be kept in GPU memory so you might hit that bottleneck so depending upon all these scenarios you want to",
    "start": "5115119",
    "end": "5120560"
  },
  {
    "text": "experiment with different b sizes and find The Sweet Spot and and uh that's uh that takes a bit of time but it's not",
    "start": "5120560",
    "end": "5127239"
  },
  {
    "text": "terribly complex um yeah so this script is there",
    "start": "5127239",
    "end": "5133199"
  },
  {
    "text": "there for you to modify and play around with it's pretty simple it's pretty much a single uh python file not much in",
    "start": "5133199",
    "end": "5139880"
  },
  {
    "text": "there you can modify it you can run it yes",
    "start": "5139880",
    "end": "5147679"
  },
  {
    "text": "yes uh the question was uh did I have Max bat size in mind when I was running",
    "start": "5157159",
    "end": "5162800"
  },
  {
    "text": "yes because I deployed the model with the config in this uh Workshop let me",
    "start": "5162800",
    "end": "5168199"
  },
  {
    "text": "show you that I build the model with a Max batch size and you can increase that batch size so let me show you",
    "start": "5168199",
    "end": "5175440"
  },
  {
    "text": "that uh so in this tiny Lama model that I",
    "start": "5175440",
    "end": "5182040"
  },
  {
    "text": "deployed I specified a Max size of 64 so uh if I go beyond 64 it's not",
    "start": "5182040",
    "end": "5189360"
  },
  {
    "text": "going to help me because all those requests will just rate and yeah actually there's one one interesting thing I want to show you uh so this is",
    "start": "5189360",
    "end": "5196560"
  },
  {
    "text": "good question you can look at the logs here and in the logs we put these uh",
    "start": "5196560",
    "end": "5203040"
  },
  {
    "text": "matrics for what's going on I think maybe I'm not yeah and and if",
    "start": "5203040",
    "end": "5208960"
  },
  {
    "text": "you wanted to increase your batch size past 64 you just change the yaml and say like oh match dispatch s so that should",
    "start": "5208960",
    "end": "5215400"
  },
  {
    "text": "be like 128 and build a new engine by deploying it yeah so if you look at",
    "start": "5215400",
    "end": "5220920"
  },
  {
    "text": "these logs here it shows how many requests are running in parallel active request count uh you can actually",
    "start": "5220920",
    "end": "5227840"
  },
  {
    "text": "observe how many requests are being executed in parallel right on the GPU because there are chances that you",
    "start": "5227840",
    "end": "5234480"
  },
  {
    "text": "haven't uh configured something right and that uh for whatever reason the uh",
    "start": "5234480",
    "end": "5240719"
  },
  {
    "text": "requests are not all getting executed in parallel for example a common mistake one could make is that when you Deploy",
    "start": "5240719",
    "end": "5247239"
  },
  {
    "text": "on on Bas 10 and this is base 10 specific but just to take an example there are scaling settings in base 10",
    "start": "5247239",
    "end": "5253600"
  },
  {
    "text": "you can specify the scale and you can specify what is the max concurrency the model will receive in this case I've set",
    "start": "5253600",
    "end": "5259679"
  },
  {
    "text": "it to a very very high value so it won't become a bottleneck but there are chances that that you know you forget",
    "start": "5259679",
    "end": "5265400"
  },
  {
    "text": "you make a mistake there you you can check these logs and they will actually tell you what's happening on the GPU and",
    "start": "5265400",
    "end": "5272320"
  },
  {
    "text": "I think I lost that again uh let me go",
    "start": "5272320",
    "end": "5277719"
  },
  {
    "text": "here so yeah these are actual metrics from the tensor llm batch manager which",
    "start": "5277719",
    "end": "5283080"
  },
  {
    "text": "tells you what's going on it also tells you about the KV cach blocks that are being used and that helps tune you uh",
    "start": "5283080",
    "end": "5290320"
  },
  {
    "text": "help you tune the KV cach size for example in this CS it says it's using 832 KV cach blocks and 4500 are empty",
    "start": "5290320",
    "end": "5298440"
  },
  {
    "text": "which means there is a there there's way more KV cach than is needed for this use case so just to mention that",
    "start": "5298440",
    "end": "5304800"
  },
  {
    "text": "as a as an aside yeah I think that's it for that",
    "start": "5304800",
    "end": "5311599"
  },
  {
    "text": "presentation gonna I'm gonna I'm gonna talk about that next slide thank you you're weeding my mind",
    "start": "5312840",
    "end": "5321040"
  },
  {
    "text": "yes it does yes tens llm does come with a benchmarking toolkit it's very very",
    "start": "5330760",
    "end": "5336360"
  },
  {
    "text": "good uh the only downside is that you have to build it from Source it's not",
    "start": "5336360",
    "end": "5341800"
  },
  {
    "text": "bundled with the tensor RT llm python Library which is my gripe I'm going to ask anybody to fix that uh they have",
    "start": "5341800",
    "end": "5349560"
  },
  {
    "text": "benchmarking tools there are two benchmarking tools one that uh that just sends a single batch and um and measures",
    "start": "5349560",
    "end": "5357960"
  },
  {
    "text": "the raw throughput you can get without serving it through inflight batching and there is a separate second second Tool",
    "start": "5357960",
    "end": "5364800"
  },
  {
    "text": "uh called the GPD manager Benchmark which actually starts up a server and does inflight batching on top so there",
    "start": "5364800",
    "end": "5370560"
  },
  {
    "text": "are two tools and they are very very good quality but they're not available easily building tensor R llm is like",
    "start": "5370560",
    "end": "5376199"
  },
  {
    "text": "with 96 CPUs it takes us one and a half hours to build it it's not for the weak of",
    "start": "5376199",
    "end": "5381480"
  },
  {
    "text": "heart or for the short of Workshop so um we just we just have a a we have a few",
    "start": "5381480",
    "end": "5388920"
  },
  {
    "text": "minutes left um so I want to run through a few slides and then leave time for last minute questions so um I was asked",
    "start": "5388920",
    "end": "5396280"
  },
  {
    "text": "um you know how do we how do we actually run this in production what does the auto scaling look like how does that all",
    "start": "5396280",
    "end": "5401400"
  },
  {
    "text": "work so um how do you run a tensorrt engine so you use something called",
    "start": "5401400",
    "end": "5406880"
  },
  {
    "text": "Triton the Triton influence server um and that's what helps you you know take the engine and actually Ser requests to",
    "start": "5406880",
    "end": "5414080"
  },
  {
    "text": "it we're actually working on our own Sero um that uses the same spec but supports uh C++ tokenization DET",
    "start": "5414080",
    "end": "5421760"
  },
  {
    "text": "tokenization custom features for for even more performance um but the you",
    "start": "5421760",
    "end": "5427639"
  },
  {
    "text": "know as we've talked about the engine is specific to versions gpus batch sizes",
    "start": "5427639",
    "end": "5433400"
  },
  {
    "text": "sequence links all that sort of stuff so um that causes some challenges when you're running it in production we've",
    "start": "5433400",
    "end": "5439560"
  },
  {
    "text": "talked this whole time about vertical scale right like how do I get more scale off a single GPU there's also horizontal",
    "start": "5439560",
    "end": "5446159"
  },
  {
    "text": "scale how do I just like get more GP how do I you know Auto automatically scale",
    "start": "5446159",
    "end": "5451320"
  },
  {
    "text": "my my platform up to meet my traffic demands so um you know some challenges in",
    "start": "5451320",
    "end": "5456639"
  },
  {
    "text": "scaling out in general you know you have to automatically respond to traffic you have to manage your cold so times you",
    "start": "5456639",
    "end": "5463840"
  },
  {
    "text": "have to manage the availability and reliability of your GPU nodes you have to Route requests do batching all that",
    "start": "5463840",
    "end": "5469679"
  },
  {
    "text": "kind of stuff and then tensorrt llm adds a few more challenges um you've got these large image sizes um so that's",
    "start": "5469679",
    "end": "5476600"
  },
  {
    "text": "going to make your cold starts even slower you've got these specific batching requirements so you can't just like send whatever traffic however you",
    "start": "5476600",
    "end": "5483199"
  },
  {
    "text": "want and you have these specific GPU requirements so when you spin up a new node it's got to be exactly the same as",
    "start": "5483199",
    "end": "5489760"
  },
  {
    "text": "your old node or your model is not going to work um and uh you know for",
    "start": "5489760",
    "end": "5494840"
  },
  {
    "text": "unfortunately our Workshop is almost over otherwise I would love to give you an in-depth ano of how to solve all",
    "start": "5494840",
    "end": "5500719"
  },
  {
    "text": "these problems uh but the quick answer to how to solve all these problems is you run your code on base 10 uh because",
    "start": "5500719",
    "end": "5506000"
  },
  {
    "text": "we solved it all for you um so base 10 is a model influence platform um it's the company that we both work at um with",
    "start": "5506000",
    "end": "5513400"
  },
  {
    "text": "base 10 you can deploy models on gpus uh you can use tensor rtlm but you don't",
    "start": "5513400",
    "end": "5518719"
  },
  {
    "text": "have to you can use any other service VM TGI just like a vanilla model deployment",
    "start": "5518719",
    "end": "5525280"
  },
  {
    "text": "um you get access to Auto scaling fast cold starts um scale to zero tons of",
    "start": "5525280",
    "end": "5530320"
  },
  {
    "text": "other great infrastructure features you get access to all of our model optimizations we have a bunch of prepackaged and pre-optimized models for",
    "start": "5530320",
    "end": "5537320"
  },
  {
    "text": "you to work with um so yeah uh the last thing before we go um is",
    "start": "5537320",
    "end": "5544560"
  },
  {
    "text": "we are co-hosting a um happy hour tomorrow on Shelby's Rooftop Bar um I",
    "start": "5544560",
    "end": "5551360"
  },
  {
    "text": "was not really involved in organizing it but the sales guys who organized it told me that it's a super sweet spot and that",
    "start": "5551360",
    "end": "5557400"
  },
  {
    "text": "we're going to have a great time um so yeah so I'm going to be there um and a bunch of other great uh AI Engineers are",
    "start": "5557400",
    "end": "5563520"
  },
  {
    "text": "going to be there so please feel free to sign up come on through we'd love to have you um it's going to be super sweet",
    "start": "5563520",
    "end": "5570119"
  },
  {
    "text": "cool party for the cool kids um well you've been you've been really",
    "start": "5570119",
    "end": "5576960"
  },
  {
    "text": "great thank you for listening to us and we open it up for questions uh I feel one question we didn't answer about Auto",
    "start": "5576960",
    "end": "5582440"
  },
  {
    "text": "scaling right maybe I should take that on now yeah yeah go ahead so how does Auto scaling work yeah yeah please uh",
    "start": "5582440",
    "end": "5588199"
  },
  {
    "text": "yeah please feel free to I'll just finish that question because you asked that I wanted to answer it and uh how",
    "start": "5588199",
    "end": "5593520"
  },
  {
    "text": "Autos scaling works is that we use a system called K native but we foged it to make it work for machine learning use",
    "start": "5593520",
    "end": "5599800"
  },
  {
    "text": "cases can if I understand correctly was built for microservices where your requests are very very quick like you",
    "start": "5599800",
    "end": "5605400"
  },
  {
    "text": "know 1 second or two seconds it doesn't exactly apply to machine learning model use cases where your request is long",
    "start": "5605400",
    "end": "5611560"
  },
  {
    "text": "lasting and you're streaming and you need to uh still scale but uh some of",
    "start": "5611560",
    "end": "5617080"
  },
  {
    "text": "the considerations are different so we had to actually Fork it to be able to apply settings dynamically for example a",
    "start": "5617080",
    "end": "5622880"
  },
  {
    "text": "lot of settings that you see on base 10 they apply immediately like within a second whereas in kive you would need to",
    "start": "5622880",
    "end": "5629080"
  },
  {
    "text": "deploy a new revision for those settings to apply and it's it's not even practical because the way you deploy a",
    "start": "5629080",
    "end": "5636080"
  },
  {
    "text": "new model it creates so much hassle and requires extra capacity that it's not good so we made changes to ctive to",
    "start": "5636080",
    "end": "5642639"
  },
  {
    "text": "cater to the machine learning use case but fundamentally the idea is is very simple as a requests come in you specify",
    "start": "5642639",
    "end": "5649960"
  },
  {
    "text": "the capacity of your pod that it can take in and if it's uh if it reaches near there we spin up a new pod and then",
    "start": "5649960",
    "end": "5657480"
  },
  {
    "text": "the traffic spreads if your traffic goes down it goes your gpus are uh your ports",
    "start": "5657480",
    "end": "5662719"
  },
  {
    "text": "are reduced your G gpus are freed up up all the way up to zero and when the traffic arrives it kept it's kept in a",
    "start": "5662719",
    "end": "5669400"
  },
  {
    "text": "queue and then the model is spin up and the requests are are sent there a lot of",
    "start": "5669400",
    "end": "5674719"
  },
  {
    "text": "the Machinery at Bas is around improving cold starts how do we start up these you know giant models 50 gig models in under",
    "start": "5674719",
    "end": "5681679"
  },
  {
    "text": "a minute right minute sounds like a long time but when you're talking about 50 gigs 50 gigs is also a lot and for for",
    "start": "5681679",
    "end": "5687960"
  },
  {
    "text": "10 gig models we aim for less than 10 seconds for 50 gig models we aim for less than a minute because that's really",
    "start": "5687960",
    "end": "5693800"
  },
  {
    "text": "important unless you can scale up in a minute your request is going to time out so you really can't scale to zero so",
    "start": "5693800",
    "end": "5699719"
  },
  {
    "text": "it's uh it seems like a detail but it's very critical you can't have scale to zero without very very fast gold starts",
    "start": "5699719",
    "end": "5706719"
  },
  {
    "text": "uh so we have the whole Machinery built out for that even before llms became popular we've had this Machinery in",
    "start": "5706719",
    "end": "5714800"
  },
  {
    "text": "place yeah",
    "start": "5715320",
    "end": "5719320"
  },
  {
    "text": "I think if you optimize it can I yeah what what are you saying like deploy it with 10 sot versus deploy it on base 10",
    "start": "5737679",
    "end": "5744760"
  },
  {
    "text": "so so you you would deploy it on base 10 using tenso under the hood to run it uh",
    "start": "5744760",
    "end": "5750239"
  },
  {
    "text": "so so base 10's just going to like facilitate that TT deploy mment for",
    "start": "5750239",
    "end": "5756159"
  },
  {
    "text": "you yes yeah no no difference base 10 runs",
    "start": "5757199",
    "end": "5763639"
  },
  {
    "text": "tensor rtlm we just make it very easy to run tensor rtlm so you it's uh easier",
    "start": "5763639",
    "end": "5770119"
  },
  {
    "text": "and faster for you to get at the optimum Point uh but if you could do it yourself yeah it's it's the same thing under the",
    "start": "5770119",
    "end": "5775880"
  },
  {
    "text": "hood and then you know I'm compelled by the fact that I'm in the marketing department to say things like we also",
    "start": "5775880",
    "end": "5782840"
  },
  {
    "text": "provide a lot of infrastructure value on top of that so that you're not managing your own you know yeah yeah actually",
    "start": "5782840",
    "end": "5788360"
  },
  {
    "text": "that that is true yeah because we we have a large Fleet we we get good cost so you actually won't pay higher on base",
    "start": "5788360",
    "end": "5794440"
  },
  {
    "text": "10 like it's not that you going to it's going to cost you more on base 10 yes over there",
    "start": "5794440",
    "end": "5803080"
  },
  {
    "text": "yeah so only the packaging part is open source the serving parts that I mean that that's kind of that's kind of the",
    "start": "5831239",
    "end": "5836560"
  },
  {
    "text": "platform so I do want to mention is that from trust you can create a Docker image and",
    "start": "5836560",
    "end": "5843760"
  },
  {
    "text": "that doer image can be run anywhere so you get pretty close you don't get Auto scaling and all of those that nice Dev",
    "start": "5843760",
    "end": "5850119"
  },
  {
    "text": "Loop but you do get a Docker image and you can do a lot with the docker image it builds it builds yeah locally",
    "start": "5850119",
    "end": "5857560"
  },
  {
    "text": "yeah you can build there's a trust uh image build command you point it to a trust it will build the docker image",
    "start": "5857560",
    "end": "5865080"
  },
  {
    "text": "locally serving on a single pod or container but there is also spreading AC",
    "start": "5867639",
    "end": "5873480"
  },
  {
    "text": "Ross multiple containers the auto scaling and all of the da Loop uh that is not yeah but the serving yeah I mean",
    "start": "5873480",
    "end": "5880040"
  },
  {
    "text": "single model serving is interest with that",
    "start": "5880040",
    "end": "5884239"
  },
  {
    "text": "image yes it's um it's uh fast API at",
    "start": "5892000",
    "end": "5897159"
  },
  {
    "text": "the trust server level then internally we have our own server layer that we wrote to interact with tensor llm uh",
    "start": "5897159",
    "end": "5904800"
  },
  {
    "text": "that part is not open source we it's very new so we still figuring out when to or where to open source it but there",
    "start": "5904800",
    "end": "5910719"
  },
  {
    "text": "is also a version that uses Triton so there is the fast API then there's Tron and then there is the transor RT llm",
    "start": "5910719",
    "end": "5916920"
  },
  {
    "text": "library and then it runs the engine yeah yeah yeah exactly exactly we have uh we",
    "start": "5916920",
    "end": "5922000"
  },
  {
    "text": "have uh we work on multiple Cloud providers we are spread across uh I don't want to say Globe like mostly us",
    "start": "5922000",
    "end": "5927960"
  },
  {
    "text": "but also Australia and a few other places so we have access to many different kinds of Hardware we find the right Hardware to build the engines and",
    "start": "5927960",
    "end": "5934320"
  },
  {
    "text": "then we deploy",
    "start": "5934320",
    "end": "5936920"
  },
  {
    "text": "it yes you can yeah you can use sell hosted clusters our stack is built that way that's one of our selling points",
    "start": "5941679",
    "end": "5947960"
  },
  {
    "text": "we're not giving you an API you can run the entire stack in a self-hosted way",
    "start": "5947960",
    "end": "5953599"
  },
  {
    "text": "awesome well look the conference organizers were very clear with us all sessions and workshops are to end on",
    "start": "5953599",
    "end": "5959520"
  },
  {
    "text": "time so I'm going to wrap it up here but um we're going to be right outside if you have any questions if you have any",
    "start": "5959520",
    "end": "5966199"
  },
  {
    "text": "easy questions come see me if you have any hard questions please go talk to punk instead thank you all so much for",
    "start": "5966199",
    "end": "5972000"
  },
  {
    "text": "being here it was so much fun doing this workshop with all of you again I'm Philip this is punk we're from base 10",
    "start": "5972000",
    "end": "5977880"
  },
  {
    "text": "and thank you so much for being here have a great conference everyone thank you",
    "start": "5977880",
    "end": "5984750"
  },
  {
    "text": "[Music]",
    "start": "5984750",
    "end": "6002330"
  }
]