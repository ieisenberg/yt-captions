[
  {
    "text": "[Music]",
    "start": "1040",
    "end": "13859"
  },
  {
    "text": "hi everyone welcome to my",
    "start": "14719",
    "end": "18439"
  },
  {
    "text": "presentation so I don't know what's the",
    "start": "18439",
    "end": "21680"
  },
  {
    "text": "best way to start about it I would",
    "start": "21680",
    "end": "23720"
  },
  {
    "text": "probably say something along the lines",
    "start": "23720",
    "end": "25599"
  },
  {
    "text": "of well you've heard a lot of really",
    "start": "25599",
    "end": "27960"
  },
  {
    "text": "good presentation that are focused on",
    "start": "27960",
    "end": "30080"
  },
  {
    "text": "one very specific thing in this session",
    "start": "30080",
    "end": "34079"
  },
  {
    "text": "itself we'll more so focus on an",
    "start": "34079",
    "end": "36360"
  },
  {
    "text": "overview of domain adaptation and fine",
    "start": "36360",
    "end": "39280"
  },
  {
    "text": "tuning for large language models because",
    "start": "39280",
    "end": "41960"
  },
  {
    "text": "there's so much information out there",
    "start": "41960",
    "end": "44120"
  },
  {
    "text": "which is like oh take this use that so",
    "start": "44120",
    "end": "47399"
  },
  {
    "text": "my goal is to sum up all the literature",
    "start": "47399",
    "end": "49879"
  },
  {
    "text": "for you to be able to make an informed",
    "start": "49879",
    "end": "52199"
  },
  {
    "text": "decision on how to be able to do domain",
    "start": "52199",
    "end": "54879"
  },
  {
    "text": "adaptation for your particular",
    "start": "54879",
    "end": "56559"
  },
  {
    "text": "Enterprise use case or for your hobby",
    "start": "56559",
    "end": "58440"
  },
  {
    "text": "use case however you use using it",
    "start": "58440",
    "end": "61039"
  },
  {
    "text": "for so about me uh this has already been",
    "start": "61039",
    "end": "66240"
  },
  {
    "text": "spoken so let's let's skip this um why",
    "start": "66240",
    "end": "70360"
  },
  {
    "text": "do we care I think I think the answer to",
    "start": "70360",
    "end": "73320"
  },
  {
    "text": "this is pretty obvious which is I mean",
    "start": "73320",
    "end": "75280"
  },
  {
    "text": "there are chat gbt as a model or even if",
    "start": "75280",
    "end": "78840"
  },
  {
    "text": "you're looking at open source large",
    "start": "78840",
    "end": "80320"
  },
  {
    "text": "language models they're not trained for",
    "start": "80320",
    "end": "82360"
  },
  {
    "text": "every single use case out there there",
    "start": "82360",
    "end": "84200"
  },
  {
    "text": "are some domains that are under",
    "start": "84200",
    "end": "85759"
  },
  {
    "text": "represented there are some domains for",
    "start": "85759",
    "end": "87479"
  },
  {
    "text": "which there is not enough data because",
    "start": "87479",
    "end": "89520"
  },
  {
    "text": "of compliance or for whatever reasons",
    "start": "89520",
    "end": "92040"
  },
  {
    "text": "and for that we need to be able to have",
    "start": "92040",
    "end": "95040"
  },
  {
    "text": "some sort of method to be able to find",
    "start": "95040",
    "end": "98000"
  },
  {
    "text": "tune the models or use some other",
    "start": "98000",
    "end": "100159"
  },
  {
    "text": "strategy and alternative of fine tuning",
    "start": "100159",
    "end": "102680"
  },
  {
    "text": "whether that's knowledge bases whether",
    "start": "102680",
    "end": "104280"
  },
  {
    "text": "that's Rags or whether that's promp the",
    "start": "104280",
    "end": "106880"
  },
  {
    "text": "second is",
    "start": "106880",
    "end": "108000"
  },
  {
    "text": "basically you don't want to collect new",
    "start": "108000",
    "end": "111399"
  },
  {
    "text": "data for every single domain one of the",
    "start": "111399",
    "end": "114040"
  },
  {
    "text": "best things that has happened with large",
    "start": "114040",
    "end": "115960"
  },
  {
    "text": "language models I would say is the",
    "start": "115960",
    "end": "118280"
  },
  {
    "text": "ability of these models to be able to",
    "start": "118280",
    "end": "120640"
  },
  {
    "text": "transition to a new domain so there's",
    "start": "120640",
    "end": "123240"
  },
  {
    "text": "one paper that I would reference um so",
    "start": "123240",
    "end": "127960"
  },
  {
    "text": "one quick example I would say is before",
    "start": "127960",
    "end": "131360"
  },
  {
    "text": "let's say Transformer models uh or even",
    "start": "131360",
    "end": "134319"
  },
  {
    "text": "while we were having Transformer models",
    "start": "134319",
    "end": "136720"
  },
  {
    "text": "um to be able to train a model to learn",
    "start": "136720",
    "end": "139280"
  },
  {
    "text": "a new language we needed to collect the",
    "start": "139280",
    "end": "141120"
  },
  {
    "text": "data for that particular language and",
    "start": "141120",
    "end": "143920"
  },
  {
    "text": "then to be able to um do whatever task",
    "start": "143920",
    "end": "147760"
  },
  {
    "text": "that we want to do in that language one",
    "start": "147760",
    "end": "150000"
  },
  {
    "text": "of the best things that has happened is",
    "start": "150000",
    "end": "153400"
  },
  {
    "text": "now because the models are learning by",
    "start": "153400",
    "end": "155959"
  },
  {
    "text": "embeddings they're able to learn on a",
    "start": "155959",
    "end": "158360"
  },
  {
    "text": "new language that they have previously",
    "start": "158360",
    "end": "160760"
  },
  {
    "text": "not seen as well because they're",
    "start": "160760",
    "end": "162480"
  },
  {
    "text": "essentially learning the structure of",
    "start": "162480",
    "end": "164920"
  },
  {
    "text": "the languages instead of like what is",
    "start": "164920",
    "end": "167120"
  },
  {
    "text": "the taxonomy of the language which means",
    "start": "167120",
    "end": "170159"
  },
  {
    "text": "there are some languages which are",
    "start": "170159",
    "end": "172040"
  },
  {
    "text": "semantically similar so for example",
    "start": "172040",
    "end": "174319"
  },
  {
    "text": "English is very semantically similar to",
    "start": "174319",
    "end": "176440"
  },
  {
    "text": "Latin I'm not entirely sure there are a",
    "start": "176440",
    "end": "178560"
  },
  {
    "text": "couple of languages that do do fall into",
    "start": "178560",
    "end": "181400"
  },
  {
    "text": "like that one domain which is oh these",
    "start": "181400",
    "end": "184360"
  },
  {
    "text": "languages are similar they have semantic",
    "start": "184360",
    "end": "186560"
  },
  {
    "text": "similarities there are other set of",
    "start": "186560",
    "end": "188319"
  },
  {
    "text": "languages that have semantic",
    "start": "188319",
    "end": "189920"
  },
  {
    "text": "similarities so it's very easy to be",
    "start": "189920",
    "end": "191959"
  },
  {
    "text": "able to transition between those",
    "start": "191959",
    "end": "193599"
  },
  {
    "text": "languages without ever having seen any",
    "start": "193599",
    "end": "195640"
  },
  {
    "text": "data or any examples in those languages",
    "start": "195640",
    "end": "198519"
  },
  {
    "text": "the third is basically you want the",
    "start": "198519",
    "end": "200200"
  },
  {
    "text": "models to be able to be accessible to a",
    "start": "200200",
    "end": "203280"
  },
  {
    "text": "wide range of users and what I mean by",
    "start": "203280",
    "end": "206159"
  },
  {
    "text": "that is more so like all the work that",
    "start": "206159",
    "end": "208000"
  },
  {
    "text": "was happening along in person",
    "start": "208000",
    "end": "211200"
  },
  {
    "text": "ization so simple reasons this is",
    "start": "211200",
    "end": "214159"
  },
  {
    "text": "something almost everybody is aware",
    "start": "214159",
    "end": "217599"
  },
  {
    "text": "of what is fine-tuning fine tuning is",
    "start": "217599",
    "end": "220439"
  },
  {
    "text": "almost a way of",
    "start": "220439",
    "end": "222799"
  },
  {
    "text": "us um teaching the model to be able to",
    "start": "222799",
    "end": "228439"
  },
  {
    "text": "learn something for which it hasn't",
    "start": "228439",
    "end": "231680"
  },
  {
    "text": "already been trained before so improving",
    "start": "231680",
    "end": "234400"
  },
  {
    "text": "the performance of a pre-trained model",
    "start": "234400",
    "end": "237280"
  },
  {
    "text": "um one of the ways we're doing that is",
    "start": "237280",
    "end": "240599"
  },
  {
    "text": "by updating the parameters right you",
    "start": "240599",
    "end": "242840"
  },
  {
    "text": "take some inputs you have a hidden layer",
    "start": "242840",
    "end": "245840"
  },
  {
    "text": "um in which you're calculating the",
    "start": "245840",
    "end": "247360"
  },
  {
    "text": "weights you're calculating the biases",
    "start": "247360",
    "end": "249200"
  },
  {
    "text": "and then you have an output layer that",
    "start": "249200",
    "end": "251640"
  },
  {
    "text": "all stuff I think is obvious to almost",
    "start": "251640",
    "end": "253680"
  },
  {
    "text": "everybody you've seen what a Transformer",
    "start": "253680",
    "end": "255640"
  },
  {
    "text": "model for people who don't know what the",
    "start": "255640",
    "end": "258160"
  },
  {
    "text": "structure of a transformer model is",
    "start": "258160",
    "end": "260160"
  },
  {
    "text": "there's an encoder there's a decoder the",
    "start": "260160",
    "end": "262560"
  },
  {
    "text": "reason I'm referencing this is we'll go",
    "start": "262560",
    "end": "265000"
  },
  {
    "text": "a little bit more into details of these",
    "start": "265000",
    "end": "267479"
  },
  {
    "text": "while we are talking about different",
    "start": "267479",
    "end": "268680"
  },
  {
    "text": "fine-tuning methods itself so there's an",
    "start": "268680",
    "end": "271440"
  },
  {
    "text": "encoder there's a decoder it has uh a",
    "start": "271440",
    "end": "274960"
  },
  {
    "text": "feed forward Network it has an attention",
    "start": "274960",
    "end": "277080"
  },
  {
    "text": "Network same for theed one now this is",
    "start": "277080",
    "end": "281639"
  },
  {
    "text": "this is how we were looking at",
    "start": "281639",
    "end": "284080"
  },
  {
    "text": "Transformer models they the way they are",
    "start": "284080",
    "end": "287080"
  },
  {
    "text": "and this is storing the weights and the",
    "start": "287080",
    "end": "289000"
  },
  {
    "text": "biases right",
    "start": "289000",
    "end": "290840"
  },
  {
    "text": "now",
    "start": "290840",
    "end": "292400"
  },
  {
    "text": "but now let's talk about making these",
    "start": "292400",
    "end": "295960"
  },
  {
    "text": "models better so there are a couple of",
    "start": "295960",
    "end": "298280"
  },
  {
    "text": "ways that we can find methods we can",
    "start": "298280",
    "end": "301160"
  },
  {
    "text": "update all the model weights or we can",
    "start": "301160",
    "end": "303440"
  },
  {
    "text": "update some of the weights if we update",
    "start": "303440",
    "end": "306280"
  },
  {
    "text": "all the model weights that falls into",
    "start": "306280",
    "end": "308000"
  },
  {
    "text": "the category of some of the models that",
    "start": "308000",
    "end": "310039"
  },
  {
    "text": "you've seen earlier which is the all the",
    "start": "310039",
    "end": "312039"
  },
  {
    "text": "research work that was between 2018 2016",
    "start": "312039",
    "end": "316440"
  },
  {
    "text": "all those all those years which is more",
    "start": "316440",
    "end": "318840"
  },
  {
    "text": "around transfer learning cross",
    "start": "318840",
    "end": "320240"
  },
  {
    "text": "distillation models in which you have a",
    "start": "320240",
    "end": "322039"
  },
  {
    "text": "teacher model and a student model the",
    "start": "322039",
    "end": "324000"
  },
  {
    "text": "student model is learning from the",
    "start": "324000",
    "end": "326240"
  },
  {
    "text": "teacher model and that's that's the way",
    "start": "326240",
    "end": "328400"
  },
  {
    "text": "you're sort of updating",
    "start": "328400",
    "end": "330720"
  },
  {
    "text": "all the weights but it is very expensive",
    "start": "330720",
    "end": "333280"
  },
  {
    "text": "to do that and it is",
    "start": "333280",
    "end": "335880"
  },
  {
    "text": "computationally it takes more storage as",
    "start": "335880",
    "end": "339080"
  },
  {
    "text": "well the second option that we've we're",
    "start": "339080",
    "end": "342160"
  },
  {
    "text": "now looking at the reason we are having",
    "start": "342160",
    "end": "344360"
  },
  {
    "text": "this discussion today is how can we",
    "start": "344360",
    "end": "347639"
  },
  {
    "text": "update our models because the parameters",
    "start": "347639",
    "end": "349759"
  },
  {
    "text": "have gone so big we cannot keep updating",
    "start": "349759",
    "end": "353680"
  },
  {
    "text": "all the weight so how about we update",
    "start": "353680",
    "end": "356000"
  },
  {
    "text": "just some of the weight without making",
    "start": "356000",
    "end": "358319"
  },
  {
    "text": "sure uh uh while making sure that we are",
    "start": "358319",
    "end": "361960"
  },
  {
    "text": "able to get equivalent performance and I",
    "start": "361960",
    "end": "365240"
  },
  {
    "text": "would I would put an srisk on you know",
    "start": "365240",
    "end": "368080"
  },
  {
    "text": "like equivalent performance because we",
    "start": "368080",
    "end": "371319"
  },
  {
    "text": "may not be able to get the chat GPT",
    "start": "371319",
    "end": "373680"
  },
  {
    "text": "performance and that is something we'll",
    "start": "373680",
    "end": "375080"
  },
  {
    "text": "talk about eventually so in terms of if",
    "start": "375080",
    "end": "377960"
  },
  {
    "text": "we update some of the weights you can",
    "start": "377960",
    "end": "379680"
  },
  {
    "text": "break it down into three categories to",
    "start": "379680",
    "end": "382319"
  },
  {
    "text": "be honest more like five categories but",
    "start": "382319",
    "end": "384039"
  },
  {
    "text": "there are three main ones which is adap",
    "start": "384039",
    "end": "385759"
  },
  {
    "text": "to tuning uh there's prefixed tuning and",
    "start": "385759",
    "end": "388400"
  },
  {
    "text": "there's parameter effic tuning there's",
    "start": "388400",
    "end": "390759"
  },
  {
    "text": "instruction tuning which is basically",
    "start": "390759",
    "end": "392479"
  },
  {
    "text": "giving a couple of examples um this is",
    "start": "392479",
    "end": "395520"
  },
  {
    "text": "something you've seen a lot at a couple",
    "start": "395520",
    "end": "397759"
  },
  {
    "text": "of so many examples throughout this",
    "start": "397759",
    "end": "399919"
  },
  {
    "text": "conference and the one that was prior to",
    "start": "399919",
    "end": "401800"
  },
  {
    "text": "my talk as well where we were doing",
    "start": "401800",
    "end": "403880"
  },
  {
    "text": "instruction tuning are El obviously not",
    "start": "403880",
    "end": "407520"
  },
  {
    "text": "super relevant to most of us which is we",
    "start": "407520",
    "end": "410120"
  },
  {
    "text": "would it's too expensive to have real",
    "start": "410120",
    "end": "412680"
  },
  {
    "text": "human beings to be able to F tune your",
    "start": "412680",
    "end": "415720"
  },
  {
    "text": "parameters for you um or to be able to",
    "start": "415720",
    "end": "419160"
  },
  {
    "text": "prove provide your examples and say this",
    "start": "419160",
    "end": "421120"
  },
  {
    "text": "is wrong this is right so we are only",
    "start": "421120",
    "end": "423800"
  },
  {
    "text": "left with three techniques which is",
    "start": "423800",
    "end": "425240"
  },
  {
    "text": "adapted tuning prefix tuning and F uh",
    "start": "425240",
    "end": "428440"
  },
  {
    "text": "parameter efficient fine tuning we'll go",
    "start": "428440",
    "end": "430560"
  },
  {
    "text": "a little bit more into detail of what",
    "start": "430560",
    "end": "433560"
  },
  {
    "text": "these are why are we using these",
    "start": "433560",
    "end": "436360"
  },
  {
    "text": "ones and when do they do well so the",
    "start": "436360",
    "end": "439400"
  },
  {
    "text": "first one this is basically adapter",
    "start": "439400",
    "end": "441639"
  },
  {
    "text": "based tuning the the thing that really",
    "start": "441639",
    "end": "444520"
  },
  {
    "text": "happens in adaptive based tuning is it's",
    "start": "444520",
    "end": "447000"
  },
  {
    "text": "really good uh what it does it it adds a",
    "start": "447000",
    "end": "450240"
  },
  {
    "text": "small number of parameters to the",
    "start": "450240",
    "end": "452680"
  },
  {
    "text": "existing model uh those parameters are",
    "start": "452680",
    "end": "456199"
  },
  {
    "text": "basically stored in the adapter",
    "start": "456199",
    "end": "458479"
  },
  {
    "text": "components that you're seeing over there",
    "start": "458479",
    "end": "460319"
  },
  {
    "text": "this is the entire model of the",
    "start": "460319",
    "end": "463479"
  },
  {
    "text": "Transformer Remains the Same but we",
    "start": "463479",
    "end": "465159"
  },
  {
    "text": "adding two new components to it that",
    "start": "465159",
    "end": "467840"
  },
  {
    "text": "contains the extra weights so what this",
    "start": "467840",
    "end": "470039"
  },
  {
    "text": "does is this exposes the model to the",
    "start": "470039",
    "end": "472199"
  },
  {
    "text": "new information and according to",
    "start": "472199",
    "end": "475520"
  },
  {
    "text": "according to the original paper that",
    "start": "475520",
    "end": "477479"
  },
  {
    "text": "came out you know it is able to improve",
    "start": "477479",
    "end": "479960"
  },
  {
    "text": "the performance of the",
    "start": "479960",
    "end": "481680"
  },
  {
    "text": "model or you could say it matches the",
    "start": "481680",
    "end": "484680"
  },
  {
    "text": "performance of the model with only",
    "start": "484680",
    "end": "486720"
  },
  {
    "text": "0.15% of the parameters where is it good",
    "start": "486720",
    "end": "490479"
  },
  {
    "text": "where in which cases would we use",
    "start": "490479",
    "end": "493000"
  },
  {
    "text": "something like this so addictor fine",
    "start": "493000",
    "end": "495479"
  },
  {
    "text": "tuning or adaptive fine-tuning both are",
    "start": "495479",
    "end": "498240"
  },
  {
    "text": "the same things um ideally you use it",
    "start": "498240",
    "end": "501599"
  },
  {
    "text": "when you're trying to learn a new domain",
    "start": "501599",
    "end": "504840"
  },
  {
    "text": "itself which is if you're trying to fine",
    "start": "504840",
    "end": "507800"
  },
  {
    "text": "tune your model for like a very",
    "start": "507800",
    "end": "510479"
  },
  {
    "text": "different domain let's say biochemical",
    "start": "510479",
    "end": "512159"
  },
  {
    "text": "engineering that's that's more so where",
    "start": "512159",
    "end": "513919"
  },
  {
    "text": "you would use uh adaptive based fine",
    "start": "513919",
    "end": "516360"
  },
  {
    "text": "tuning the second is prefix based fine",
    "start": "516360",
    "end": "518560"
  },
  {
    "text": "tuning so prefix based fine tuning what",
    "start": "518560",
    "end": "521039"
  },
  {
    "text": "it does is it introduces some prefixes",
    "start": "521039",
    "end": "525279"
  },
  {
    "text": "where we are storing the model weights",
    "start": "525279",
    "end": "527880"
  },
  {
    "text": "and what they are able to do is they are",
    "start": "527880",
    "end": "530160"
  },
  {
    "text": "able to mimic the behavior of the prefix",
    "start": "530160",
    "end": "533399"
  },
  {
    "text": "that we are giving it which is the",
    "start": "533399",
    "end": "534720"
  },
  {
    "text": "couple of Weights that we are adding in",
    "start": "534720",
    "end": "536560"
  },
  {
    "text": "front of the tension model um so in a",
    "start": "536560",
    "end": "539760"
  },
  {
    "text": "very simple in very simple words what it",
    "start": "539760",
    "end": "542120"
  },
  {
    "text": "does it it um it adds an embedding layer",
    "start": "542120",
    "end": "546120"
  },
  {
    "text": "at the front of the tension layer uh to",
    "start": "546120",
    "end": "549360"
  },
  {
    "text": "mimic that behavior one very simple",
    "start": "549360",
    "end": "552240"
  },
  {
    "text": "example to understand this a little bit",
    "start": "552240",
    "end": "554640"
  },
  {
    "text": "better is you know all of the water that",
    "start": "554640",
    "end": "557279"
  },
  {
    "text": "we get comes out of a tank right but the",
    "start": "557279",
    "end": "560240"
  },
  {
    "text": "way we are able to access it is using a",
    "start": "560240",
    "end": "563640"
  },
  {
    "text": "tap and water takes the form of a tap",
    "start": "563640",
    "end": "567200"
  },
  {
    "text": "which is it comes out in in this",
    "start": "567200",
    "end": "569279"
  },
  {
    "text": "quantity so that's that's very much like",
    "start": "569279",
    "end": "571519"
  },
  {
    "text": "how prefix tuning Works which is it's",
    "start": "571519",
    "end": "573800"
  },
  {
    "text": "not changing the behavior of the model",
    "start": "573800",
    "end": "576240"
  },
  {
    "text": "but it's just mimicking or adding a",
    "start": "576240",
    "end": "578839"
  },
  {
    "text": "masking layer on top of uh the existing",
    "start": "578839",
    "end": "582440"
  },
  {
    "text": "weights or on top of the existing model",
    "start": "582440",
    "end": "584480"
  },
  {
    "text": "that there is the third and the final",
    "start": "584480",
    "end": "587640"
  },
  {
    "text": "one which is the prefix B uh which is",
    "start": "587640",
    "end": "590120"
  },
  {
    "text": "the parameter efficient fine-tuning",
    "start": "590120",
    "end": "592560"
  },
  {
    "text": "method so this one the one example that",
    "start": "592560",
    "end": "595480"
  },
  {
    "text": "you're seeing is basically the Lowa one",
    "start": "595480",
    "end": "597800"
  },
  {
    "text": "there're two commonly known parameter",
    "start": "597800",
    "end": "600800"
  },
  {
    "text": "efficient fine tuning methods that are",
    "start": "600800",
    "end": "602399"
  },
  {
    "text": "out there Laura and Kora the way Laura",
    "start": "602399",
    "end": "606160"
  },
  {
    "text": "really works uh what it really is is",
    "start": "606160",
    "end": "608959"
  },
  {
    "text": "basically low rank adaptation method",
    "start": "608959",
    "end": "611360"
  },
  {
    "text": "where you use it any sort of parameter",
    "start": "611360",
    "end": "614160"
  },
  {
    "text": "efficient fine tuning method is sort of",
    "start": "614160",
    "end": "615920"
  },
  {
    "text": "used where you want to compress the",
    "start": "615920",
    "end": "617680"
  },
  {
    "text": "model sizes or you want to run it on low",
    "start": "617680",
    "end": "620880"
  },
  {
    "text": "resource devices so very ideal for large",
    "start": "620880",
    "end": "623880"
  },
  {
    "text": "langage model's biggest reason is",
    "start": "623880",
    "end": "626600"
  },
  {
    "text": "because we have massive parameters that",
    "start": "626600",
    "end": "628720"
  },
  {
    "text": "we are trying to run on very small",
    "start": "628720",
    "end": "630519"
  },
  {
    "text": "devices which could be our laptops and",
    "start": "630519",
    "end": "633160"
  },
  {
    "text": "even smaller devices which is basically",
    "start": "633160",
    "end": "635240"
  },
  {
    "text": "the HML devices the arros and all of",
    "start": "635240",
    "end": "638120"
  },
  {
    "text": "that stuff so that's one reason the",
    "start": "638120",
    "end": "640639"
  },
  {
    "text": "entire Community has been talking more",
    "start": "640639",
    "end": "642920"
  },
  {
    "text": "so about Laura and Cur because again we",
    "start": "642920",
    "end": "645399"
  },
  {
    "text": "are looking for efficiency the way it",
    "start": "645399",
    "end": "648240"
  },
  {
    "text": "works under the hood is um all of the",
    "start": "648240",
    "end": "651920"
  },
  {
    "text": "weights are usually stored as what is",
    "start": "651920",
    "end": "654360"
  },
  {
    "text": "basically a matrix right",
    "start": "654360",
    "end": "657360"
  },
  {
    "text": "so most of these weights um there there",
    "start": "657360",
    "end": "660800"
  },
  {
    "text": "are a lot of layers in these weights",
    "start": "660800",
    "end": "663240"
  },
  {
    "text": "that aren't U unique and whatever what",
    "start": "663240",
    "end": "667800"
  },
  {
    "text": "Lura usually does is it identifies the",
    "start": "667800",
    "end": "671079"
  },
  {
    "text": "linearly",
    "start": "671079",
    "end": "672360"
  },
  {
    "text": "independent layers um in in terms of the",
    "start": "672360",
    "end": "676079"
  },
  {
    "text": "weight uh Matrix itself so in The Matrix",
    "start": "676079",
    "end": "679320"
  },
  {
    "text": "you're looking at all the linearly",
    "start": "679320",
    "end": "681160"
  },
  {
    "text": "independent lines or the columns and",
    "start": "681160",
    "end": "684480"
  },
  {
    "text": "you're picking and choosing only those",
    "start": "684480",
    "end": "686440"
  },
  {
    "text": "ones so what it does is if two things",
    "start": "686440",
    "end": "689480"
  },
  {
    "text": "are very similar or two things are",
    "start": "689480",
    "end": "691519"
  },
  {
    "text": "almost like you could transform one",
    "start": "691519",
    "end": "693920"
  },
  {
    "text": "easily through a mathematical function",
    "start": "693920",
    "end": "696360"
  },
  {
    "text": "as like uh multiplier of the other one",
    "start": "696360",
    "end": "699240"
  },
  {
    "text": "then storing that one extra layer which",
    "start": "699240",
    "end": "701279"
  },
  {
    "text": "is a copy of the original one doesn't",
    "start": "701279",
    "end": "703120"
  },
  {
    "text": "really make sense right so that's that's",
    "start": "703120",
    "end": "705839"
  },
  {
    "text": "how Laura works under the hood which is",
    "start": "705839",
    "end": "708160"
  },
  {
    "text": "we are reducing the size of the Matrix",
    "start": "708160",
    "end": "710240"
  },
  {
    "text": "which is the size of the weight Matrix",
    "start": "710240",
    "end": "713880"
  },
  {
    "text": "essentially practical benefit obviously",
    "start": "713880",
    "end": "717040"
  },
  {
    "text": "um you know you're you're able to to",
    "start": "717040",
    "end": "719600"
  },
  {
    "text": "decrease the size of the model and",
    "start": "719600",
    "end": "722560"
  },
  {
    "text": "you're also you can also um you can also",
    "start": "722560",
    "end": "728360"
  },
  {
    "text": "you you're also using less memory right",
    "start": "728360",
    "end": "730839"
  },
  {
    "text": "now um the second method that we are",
    "start": "730839",
    "end": "733440"
  },
  {
    "text": "looking at is basically called Cura",
    "start": "733440",
    "end": "736360"
  },
  {
    "text": "which is quantized uh Laura method the",
    "start": "736360",
    "end": "740440"
  },
  {
    "text": "way it works is it changes the model",
    "start": "740440",
    "end": "742720"
  },
  {
    "text": "weights to 4bit Precision um the way it",
    "start": "742720",
    "end": "747279"
  },
  {
    "text": "usually works is you start the",
    "start": "747279",
    "end": "749120"
  },
  {
    "text": "pre-trained models uh you collect a data",
    "start": "749120",
    "end": "752120"
  },
  {
    "text": "set with labeled uh with labeled data",
    "start": "752120",
    "end": "755720"
  },
  {
    "text": "and you train adaptation Matrix uh and",
    "start": "755720",
    "end": "759440"
  },
  {
    "text": "multiply it with the main weight Matrix",
    "start": "759440",
    "end": "762800"
  },
  {
    "text": "and what you're essentially trying to do",
    "start": "762800",
    "end": "764440"
  },
  {
    "text": "is you're trying to decrease the",
    "start": "764440",
    "end": "767079"
  },
  {
    "text": "distance between the predicted outputs",
    "start": "767079",
    "end": "769800"
  },
  {
    "text": "of the source domain and the target",
    "start": "769800",
    "end": "771680"
  },
  {
    "text": "domain that's that's what's essentially",
    "start": "771680",
    "end": "774079"
  },
  {
    "text": "going on in kulur one quick comparison",
    "start": "774079",
    "end": "777959"
  },
  {
    "text": "uh obviously I",
    "start": "777959",
    "end": "779920"
  },
  {
    "text": "mean in in terms of like the people who",
    "start": "779920",
    "end": "782480"
  },
  {
    "text": "are saying okay Kora is great should we",
    "start": "782480",
    "end": "784600"
  },
  {
    "text": "use Laura Cur",
    "start": "784600",
    "end": "786920"
  },
  {
    "text": "um one quick thing I'll say on that one",
    "start": "786920",
    "end": "789880"
  },
  {
    "text": "is while Kora works really good on the",
    "start": "789880",
    "end": "792800"
  },
  {
    "text": "original data set that I was trained on",
    "start": "792800",
    "end": "795399"
  },
  {
    "text": "but to be able to get to perform really",
    "start": "795399",
    "end": "798360"
  },
  {
    "text": "well requires a library bits and whites",
    "start": "798360",
    "end": "800560"
  },
  {
    "text": "libraries and some other things which",
    "start": "800560",
    "end": "803680"
  },
  {
    "text": "are not available on all the devices not",
    "start": "803680",
    "end": "806680"
  },
  {
    "text": "a lot of testing has really happened for",
    "start": "806680",
    "end": "809000"
  },
  {
    "text": "ura's efficiency on all of the models so",
    "start": "809000",
    "end": "811279"
  },
  {
    "text": "I would probably say maybe still",
    "start": "811279",
    "end": "813519"
  },
  {
    "text": "sticking with Laura and being able to",
    "start": "813519",
    "end": "815199"
  },
  {
    "text": "optimize the performance with the Lowa",
    "start": "815199",
    "end": "817279"
  },
  {
    "text": "model is ideally like the better way to",
    "start": "817279",
    "end": "819240"
  },
  {
    "text": "go at least at this current point in",
    "start": "819240",
    "end": "822279"
  },
  {
    "text": "time so to to very quickly summarize um",
    "start": "822279",
    "end": "827680"
  },
  {
    "text": "which is again we have three different",
    "start": "827680",
    "end": "830440"
  },
  {
    "text": "methods to be able to do domain",
    "start": "830440",
    "end": "832440"
  },
  {
    "text": "adaptation we have prompting we have",
    "start": "832440",
    "end": "834560"
  },
  {
    "text": "Rags we have fine",
    "start": "834560",
    "end": "836040"
  },
  {
    "text": "tuning for prompting you can sort of",
    "start": "836040",
    "end": "838720"
  },
  {
    "text": "promer models again with no examples",
    "start": "838720",
    "end": "842399"
  },
  {
    "text": "with one example with a couple of",
    "start": "842399",
    "end": "844040"
  },
  {
    "text": "examples when it comes to a couple of",
    "start": "844040",
    "end": "845600"
  },
  {
    "text": "examples I think a good answer would be",
    "start": "845600",
    "end": "847959"
  },
  {
    "text": "about 10 which is what chart gbt says",
    "start": "847959",
    "end": "850880"
  },
  {
    "text": "where obviously the performance is",
    "start": "850880",
    "end": "852199"
  },
  {
    "text": "better the more examples you're able to",
    "start": "852199",
    "end": "853880"
  },
  {
    "text": "give it where it works is um in in the",
    "start": "853880",
    "end": "857920"
  },
  {
    "text": "domains that you're looking for more",
    "start": "857920",
    "end": "859360"
  },
  {
    "text": "generalizable models but usually that's",
    "start": "859360",
    "end": "861759"
  },
  {
    "text": "just demos not real world examples",
    "start": "861759",
    "end": "864399"
  },
  {
    "text": "requires less training data it's cheaper",
    "start": "864399",
    "end": "867000"
  },
  {
    "text": "obviously but is not as performant as",
    "start": "867000",
    "end": "869759"
  },
  {
    "text": "fine-tuning on fine tuning you're",
    "start": "869759",
    "end": "872759"
  },
  {
    "text": "looking at three different methods which",
    "start": "872759",
    "end": "874560"
  },
  {
    "text": "is like adaptive fine tuning you're",
    "start": "874560",
    "end": "876440"
  },
  {
    "text": "looking a behavioral fine tuning and",
    "start": "876440",
    "end": "878120"
  },
  {
    "text": "parameter efficient fine tuning um on",
    "start": "878120",
    "end": "881240"
  },
  {
    "text": "each of these ones um you don't need to",
    "start": "881240",
    "end": "884360"
  },
  {
    "text": "pick one of these three techniques you",
    "start": "884360",
    "end": "886240"
  },
  {
    "text": "can also combine them with prompt",
    "start": "886240",
    "end": "888279"
  },
  {
    "text": "engineering you can combine them with",
    "start": "888279",
    "end": "889959"
  },
  {
    "text": "rxs as well or you can do both of those",
    "start": "889959",
    "end": "892759"
  },
  {
    "text": "things which you can do adaptive as well",
    "start": "892759",
    "end": "894759"
  },
  {
    "text": "as behavioral fine tuning the key",
    "start": "894759",
    "end": "897120"
  },
  {
    "text": "difference between all those three",
    "start": "897120",
    "end": "898399"
  },
  {
    "text": "method methods is adaptive fine tuning",
    "start": "898399",
    "end": "900920"
  },
  {
    "text": "really works well on when you have a",
    "start": "900920",
    "end": "903920"
  },
  {
    "text": "Target domain that you're trying to",
    "start": "903920",
    "end": "905360"
  },
  {
    "text": "optimize for so for example like if you",
    "start": "905360",
    "end": "907240"
  },
  {
    "text": "have multiple tasks within a single",
    "start": "907240",
    "end": "909040"
  },
  {
    "text": "domain let's say you are um you have",
    "start": "909040",
    "end": "911600"
  },
  {
    "text": "legal company and you're trying to build",
    "start": "911600",
    "end": "915000"
  },
  {
    "text": "a model that works really well on five",
    "start": "915000",
    "end": "917040"
  },
  {
    "text": "different or 10 different tasks within",
    "start": "917040",
    "end": "919000"
  },
  {
    "text": "just the legal domain itself adaptive",
    "start": "919000",
    "end": "921000"
  },
  {
    "text": "fine tuning works great behavioral fine",
    "start": "921000",
    "end": "923199"
  },
  {
    "text": "tuning is basically where you're trying",
    "start": "923199",
    "end": "925040"
  },
  {
    "text": "to optimize the model performance on a",
    "start": "925040",
    "end": "927199"
  },
  {
    "text": "Target task only so you're not really",
    "start": "927199",
    "end": "930680"
  },
  {
    "text": "optimizing for the entire domain you're",
    "start": "930680",
    "end": "932720"
  },
  {
    "text": "optimizing for just one particular task",
    "start": "932720",
    "end": "935920"
  },
  {
    "text": "the weight really works is you're",
    "start": "935920",
    "end": "938040"
  },
  {
    "text": "optimizing for the label space and the",
    "start": "938040",
    "end": "940560"
  },
  {
    "text": "prior probability distribution so very",
    "start": "940560",
    "end": "943519"
  },
  {
    "text": "helpful when you're trying to um get",
    "start": "943519",
    "end": "947040"
  },
  {
    "text": "to uh show some sort of like inference",
    "start": "947040",
    "end": "950000"
  },
  {
    "text": "and reasoning",
    "start": "950000",
    "end": "951440"
  },
  {
    "text": "capabilities you could also a good",
    "start": "951440",
    "end": "954240"
  },
  {
    "text": "analogy on behavioral fine tuning is",
    "start": "954240",
    "end": "956319"
  },
  {
    "text": "it's very similar to Lang chain function",
    "start": "956319",
    "end": "958880"
  },
  {
    "text": "Fons um if if you use Lang chain",
    "start": "958880",
    "end": "961839"
  },
  {
    "text": "functions and parameter efficient fine",
    "start": "961839",
    "end": "964000"
  },
  {
    "text": "tuning is like the standard fine tuning",
    "start": "964000",
    "end": "965680"
  },
  {
    "text": "where we are freezing some of the",
    "start": "965680",
    "end": "967639"
  },
  {
    "text": "parameters and we're only updating a",
    "start": "967639",
    "end": "970800"
  },
  {
    "text": "very small amount of parameters using",
    "start": "970800",
    "end": "973319"
  },
  {
    "text": "the techniques Lowa C and so",
    "start": "973319",
    "end": "976600"
  },
  {
    "text": "on but coming to you know are these",
    "start": "976600",
    "end": "980800"
  },
  {
    "text": "techniques going to really work sure we",
    "start": "980800",
    "end": "983440"
  },
  {
    "text": "have all of this available it would only",
    "start": "983440",
    "end": "986000"
  },
  {
    "text": "work depending on how good your data is",
    "start": "986000",
    "end": "989360"
  },
  {
    "text": "which is how it depends on how you're",
    "start": "989360",
    "end": "991519"
  },
  {
    "text": "collecting your data how you're",
    "start": "991519",
    "end": "993079"
  },
  {
    "text": "tokenizing your data how you're cleaning",
    "start": "993079",
    "end": "995600"
  },
  {
    "text": "and normalizing your data are you",
    "start": "995600",
    "end": "997560"
  },
  {
    "text": "removing the noise and sort of",
    "start": "997560",
    "end": "999000"
  },
  {
    "text": "sanitizing your models are you doing",
    "start": "999000",
    "end": "1001120"
  },
  {
    "text": "data duplication as well to be able to",
    "start": "1001120",
    "end": "1004199"
  },
  {
    "text": "um remove the duplicate entry so there",
    "start": "1004199",
    "end": "1006360"
  },
  {
    "text": "was another research that was published",
    "start": "1006360",
    "end": "1008480"
  },
  {
    "text": "which was basically like the",
    "start": "1008480",
    "end": "1010160"
  },
  {
    "text": "memorization which happens in models is",
    "start": "1010160",
    "end": "1013360"
  },
  {
    "text": "mainly because of dat duplication um if",
    "start": "1013360",
    "end": "1016720"
  },
  {
    "text": "if we're removing the D duplicate",
    "start": "1016720",
    "end": "1018880"
  },
  {
    "text": "entries that that reduces the",
    "start": "1018880",
    "end": "1021120"
  },
  {
    "text": "probability of a model to be able to",
    "start": "1021120",
    "end": "1023040"
  },
  {
    "text": "memorize certain task because again it's",
    "start": "1023040",
    "end": "1025038"
  },
  {
    "text": "seeing those uh tear sets over and over",
    "start": "1025039",
    "end": "1028038"
  },
  {
    "text": "again in some form or the other so it's",
    "start": "1028039",
    "end": "1030319"
  },
  {
    "text": "it's naturally collecting creating sort",
    "start": "1030319",
    "end": "1032760"
  },
  {
    "text": "of a bias towards those things and it's",
    "start": "1032760",
    "end": "1034798"
  },
  {
    "text": "naturally outputting those very quickly",
    "start": "1034799",
    "end": "1037160"
  },
  {
    "text": "and the last one being data",
    "start": "1037160",
    "end": "1040360"
  },
  {
    "text": "augmentation now let's say you've done",
    "start": "1040360",
    "end": "1043600"
  },
  {
    "text": "all of this let's say you've picked the",
    "start": "1043600",
    "end": "1045319"
  },
  {
    "text": "right model let's say you've done your",
    "start": "1045319",
    "end": "1047438"
  },
  {
    "text": "data collection thing perfectly you've",
    "start": "1047439",
    "end": "1049039"
  },
  {
    "text": "got the best data out there what are",
    "start": "1049039",
    "end": "1051039"
  },
  {
    "text": "still the things that you can think of",
    "start": "1051039",
    "end": "1052720"
  },
  {
    "text": "while optimizing the performance of your",
    "start": "1052720",
    "end": "1054679"
  },
  {
    "text": "model so the first thing is do not try",
    "start": "1054679",
    "end": "1058200"
  },
  {
    "text": "to compare whether gp4 or GPT 5 it's not",
    "start": "1058200",
    "end": "1061320"
  },
  {
    "text": "going to work comparatively especially",
    "start": "1061320",
    "end": "1063760"
  },
  {
    "text": "for more complex tasks it's not a",
    "start": "1063760",
    "end": "1066360"
  },
  {
    "text": "generalized model um while it may be",
    "start": "1066360",
    "end": "1069880"
  },
  {
    "text": "able to capture the nuances of your",
    "start": "1069880",
    "end": "1072840"
  },
  {
    "text": "actual data but it may not be able to",
    "start": "1072840",
    "end": "1074960"
  },
  {
    "text": "capture the nuances of the new data that",
    "start": "1074960",
    "end": "1077480"
  },
  {
    "text": "it hasn't seen or in newer domains that",
    "start": "1077480",
    "end": "1079799"
  },
  {
    "text": "it hasn't seen before um so that's",
    "start": "1079799",
    "end": "1082919"
  },
  {
    "text": "that's one thing which I've seen a lot",
    "start": "1082919",
    "end": "1084400"
  },
  {
    "text": "of companies are trying to sort of in a",
    "start": "1084400",
    "end": "1087679"
  },
  {
    "text": "dilemma with uh which is oh we fine",
    "start": "1087679",
    "end": "1090480"
  },
  {
    "text": "tuned our model but it's not working as",
    "start": "1090480",
    "end": "1092400"
  },
  {
    "text": "good as gbd4 the second one is basically",
    "start": "1092400",
    "end": "1096280"
  },
  {
    "text": "using in context learning with Dynamic",
    "start": "1096280",
    "end": "1099919"
  },
  {
    "text": "examples and one of the big reasons for",
    "start": "1099919",
    "end": "1102760"
  },
  {
    "text": "that is um the the big problem that we",
    "start": "1102760",
    "end": "1105760"
  },
  {
    "text": "see with the drift in the model with the",
    "start": "1105760",
    "end": "1108480"
  },
  {
    "text": "data drift and the models so using in",
    "start": "1108480",
    "end": "1112280"
  },
  {
    "text": "context learning with Dynamic example",
    "start": "1112280",
    "end": "1114559"
  },
  {
    "text": "loading allows you to be able to deal",
    "start": "1114559",
    "end": "1116679"
  },
  {
    "text": "with that particular problem while also",
    "start": "1116679",
    "end": "1119840"
  },
  {
    "text": "making sure that you are able to do uh",
    "start": "1119840",
    "end": "1123600"
  },
  {
    "text": "cost management as",
    "start": "1123600",
    "end": "1125320"
  },
  {
    "text": "well um the third thing that also one",
    "start": "1125320",
    "end": "1128520"
  },
  {
    "text": "needs to think of is breaking down this",
    "start": "1128520",
    "end": "1130880"
  },
  {
    "text": "task into smaller task so for example",
    "start": "1130880",
    "end": "1132919"
  },
  {
    "text": "like if we are working with any sort of",
    "start": "1132919",
    "end": "1134919"
  },
  {
    "text": "language then instead of trying to train",
    "start": "1134919",
    "end": "1137280"
  },
  {
    "text": "the model for like the ENT High language",
    "start": "1137280",
    "end": "1138880"
  },
  {
    "text": "can we break it down into like very",
    "start": "1138880",
    "end": "1140760"
  },
  {
    "text": "specific task um so that's that's",
    "start": "1140760",
    "end": "1143520"
  },
  {
    "text": "another thing which people need to think",
    "start": "1143520",
    "end": "1146559"
  },
  {
    "text": "of um the final thing I would say is uh",
    "start": "1146559",
    "end": "1150200"
  },
  {
    "text": "implementing some sort of gradient",
    "start": "1150200",
    "end": "1152640"
  },
  {
    "text": "checkpointing so what gradient",
    "start": "1152640",
    "end": "1154640"
  },
  {
    "text": "checkpointing essentially does it um it",
    "start": "1154640",
    "end": "1157760"
  },
  {
    "text": "reduces the memory usage um what what",
    "start": "1157760",
    "end": "1162280"
  },
  {
    "text": "essentially does is it retrains the",
    "start": "1162280",
    "end": "1165480"
  },
  {
    "text": "model uh and recomputes the during the",
    "start": "1165480",
    "end": "1169039"
  },
  {
    "text": "backward pass while it may look like you",
    "start": "1169039",
    "end": "1173159"
  },
  {
    "text": "know it's it's not the it's not the",
    "start": "1173159",
    "end": "1176000"
  },
  {
    "text": "smartest choice to make um but you know",
    "start": "1176000",
    "end": "1180039"
  },
  {
    "text": "while the computation is higher which is",
    "start": "1180039",
    "end": "1182400"
  },
  {
    "text": "yes the the weights will need to be",
    "start": "1182400",
    "end": "1184640"
  },
  {
    "text": "recomputed uh but the downsides are",
    "start": "1184640",
    "end": "1187120"
  },
  {
    "text": "easily weighed by the memory consumption",
    "start": "1187120",
    "end": "1190039"
  },
  {
    "text": "so the memory consumption is very very",
    "start": "1190039",
    "end": "1192720"
  },
  {
    "text": "less if we are implementing some sort of",
    "start": "1192720",
    "end": "1194520"
  },
  {
    "text": "gradient checkpointing so another cost",
    "start": "1194520",
    "end": "1196760"
  },
  {
    "text": "effect uh cost management",
    "start": "1196760",
    "end": "1200120"
  },
  {
    "text": "thing um now few more considerations and",
    "start": "1200120",
    "end": "1204240"
  },
  {
    "text": "limitations which is let's talk about",
    "start": "1204240",
    "end": "1206440"
  },
  {
    "text": "the hyperparameters now choosing a bat",
    "start": "1206440",
    "end": "1208840"
  },
  {
    "text": "size ideally we go with a bat size of 32",
    "start": "1208840",
    "end": "1211480"
  },
  {
    "text": "or 64 uh choosing the number of training",
    "start": "1211480",
    "end": "1214559"
  },
  {
    "text": "EPO again one of the questions I often",
    "start": "1214559",
    "end": "1217400"
  },
  {
    "text": "get is what's the right number of epoch",
    "start": "1217400",
    "end": "1220360"
  },
  {
    "text": "that we should be training with um if",
    "start": "1220360",
    "end": "1222559"
  },
  {
    "text": "you're if you're doing a simple test",
    "start": "1222559",
    "end": "1224440"
  },
  {
    "text": "which is if you're running something in",
    "start": "1224440",
    "end": "1226080"
  },
  {
    "text": "a Google Cod lab uh for fun thing maybe",
    "start": "1226080",
    "end": "1229679"
  },
  {
    "text": "having Epoch one is nice uh but if",
    "start": "1229679",
    "end": "1233799"
  },
  {
    "text": "you're if you are working with a good",
    "start": "1233799",
    "end": "1236520"
  },
  {
    "text": "model and if you're trying to optimize",
    "start": "1236520",
    "end": "1239200"
  },
  {
    "text": "for like a particular domain then",
    "start": "1239200",
    "end": "1241400"
  },
  {
    "text": "choosing to go with 100 epoxes like the",
    "start": "1241400",
    "end": "1244159"
  },
  {
    "text": "starting point is is probably like the",
    "start": "1244159",
    "end": "1246600"
  },
  {
    "text": "ideal Choice choosing an Optimizer there",
    "start": "1246600",
    "end": "1248720"
  },
  {
    "text": "are different optimizers that are out",
    "start": "1248720",
    "end": "1250559"
  },
  {
    "text": "there uh Adam Optimizer is the standard",
    "start": "1250559",
    "end": "1253400"
  },
  {
    "text": "choice because it's general purpose and",
    "start": "1253400",
    "end": "1256159"
  },
  {
    "text": "it works really well with different",
    "start": "1256159",
    "end": "1257840"
  },
  {
    "text": "domains as well um implementing some",
    "start": "1257840",
    "end": "1260880"
  },
  {
    "text": "sort of regularization early stopping",
    "start": "1260880",
    "end": "1263960"
  },
  {
    "text": "again one of the things is basically",
    "start": "1263960",
    "end": "1266760"
  },
  {
    "text": "like in in terms of if if you're looking",
    "start": "1266760",
    "end": "1269600"
  },
  {
    "text": "at the models that have been trained",
    "start": "1269600",
    "end": "1271720"
  },
  {
    "text": "till now they're they're not a lot of",
    "start": "1271720",
    "end": "1274120"
  },
  {
    "text": "there's not a lot of implementation on",
    "start": "1274120",
    "end": "1276240"
  },
  {
    "text": "optimizing those performances while",
    "start": "1276240",
    "end": "1278600"
  },
  {
    "text": "there are bigger models that we are",
    "start": "1278600",
    "end": "1280440"
  },
  {
    "text": "seeing every single day with more and",
    "start": "1280440",
    "end": "1282080"
  },
  {
    "text": "more",
    "start": "1282080",
    "end": "1283200"
  },
  {
    "text": "parameters they're not essentially",
    "start": "1283200",
    "end": "1285600"
  },
  {
    "text": "squeezing all the performance out of",
    "start": "1285600",
    "end": "1287480"
  },
  {
    "text": "those models so one of the easy ways to",
    "start": "1287480",
    "end": "1290799"
  },
  {
    "text": "be able to do that is using some sort of",
    "start": "1290799",
    "end": "1293120"
  },
  {
    "text": "early stopping which is making sure that",
    "start": "1293120",
    "end": "1294799"
  },
  {
    "text": "you're only working with the data that",
    "start": "1294799",
    "end": "1297039"
  },
  {
    "text": "is most efficient if the model",
    "start": "1297039",
    "end": "1300000"
  },
  {
    "text": "performance is declining then you need",
    "start": "1300000",
    "end": "1301919"
  },
  {
    "text": "to reconsider your batch and look into",
    "start": "1301919",
    "end": "1303919"
  },
  {
    "text": "that batch consider your",
    "start": "1303919",
    "end": "1307080"
  },
  {
    "text": "embeddings now let's say if you fine",
    "start": "1307080",
    "end": "1309799"
  },
  {
    "text": "train fine- tuned the model the next",
    "start": "1309799",
    "end": "1311960"
  },
  {
    "text": "part which is the hardest part of the",
    "start": "1311960",
    "end": "1313760"
  },
  {
    "text": "process is um you know how do we",
    "start": "1313760",
    "end": "1316159"
  },
  {
    "text": "evaluate our models there are so many",
    "start": "1316159",
    "end": "1318080"
  },
  {
    "text": "any um benchmarks out there there are so",
    "start": "1318080",
    "end": "1321640"
  },
  {
    "text": "many libraries out there uh so there's",
    "start": "1321640",
    "end": "1324360"
  },
  {
    "text": "every by Ray there's um libraries by",
    "start": "1324360",
    "end": "1328279"
  },
  {
    "text": "Nvidia um but what you're essentially",
    "start": "1328279",
    "end": "1331080"
  },
  {
    "text": "looking for mostly is the law accuracy",
    "start": "1331080",
    "end": "1334240"
  },
  {
    "text": "and perplexity but that doesn't really",
    "start": "1334240",
    "end": "1336320"
  },
  {
    "text": "paint the full picture so while I say",
    "start": "1336320",
    "end": "1339559"
  },
  {
    "text": "you know is the hardest part which is",
    "start": "1339559",
    "end": "1341760"
  },
  {
    "text": "they needs to be some sort of adaptation",
    "start": "1341760",
    "end": "1344000"
  },
  {
    "text": "for every single business and every",
    "start": "1344000",
    "end": "1345799"
  },
  {
    "text": "single use case which is we need to be",
    "start": "1345799",
    "end": "1348120"
  },
  {
    "text": "looking at evaluation from four",
    "start": "1348120",
    "end": "1350000"
  },
  {
    "text": "different perspectives or four different",
    "start": "1350000",
    "end": "1352039"
  },
  {
    "text": "components the first is doing some sort",
    "start": "1352039",
    "end": "1354520"
  },
  {
    "text": "of metric based evaluation which is",
    "start": "1354520",
    "end": "1356360"
  },
  {
    "text": "something like blue score Rogue score",
    "start": "1356360",
    "end": "1359039"
  },
  {
    "text": "that we were considering before doing",
    "start": "1359039",
    "end": "1361480"
  },
  {
    "text": "some sort of tool-based evaluation so I",
    "start": "1361480",
    "end": "1363320"
  },
  {
    "text": "think weights and bies does have a",
    "start": "1363320",
    "end": "1365039"
  },
  {
    "text": "library for doing that particularly",
    "start": "1365039",
    "end": "1367159"
  },
  {
    "text": "which is their Auto evaluate the",
    "start": "1367159",
    "end": "1368840"
  },
  {
    "text": "debugger one and then there's another",
    "start": "1368840",
    "end": "1371080"
  },
  {
    "text": "one Auto evaluator um so that is able to",
    "start": "1371080",
    "end": "1374159"
  },
  {
    "text": "catch the compilation errors very",
    "start": "1374159",
    "end": "1376480"
  },
  {
    "text": "quickly the Third One is using some sort",
    "start": "1376480",
    "end": "1379320"
  },
  {
    "text": "of model based evaluation which is using",
    "start": "1379320",
    "end": "1381279"
  },
  {
    "text": "a smaller model to be a able to evaluate",
    "start": "1381279",
    "end": "1384440"
  },
  {
    "text": "the other model so while this is",
    "start": "1384440",
    "end": "1387679"
  },
  {
    "text": "something which is um i' I've not seen a",
    "start": "1387679",
    "end": "1391480"
  },
  {
    "text": "lot of performance with this one because",
    "start": "1391480",
    "end": "1394279"
  },
  {
    "text": "again it's hard to do but it has a lot",
    "start": "1394279",
    "end": "1396640"
  },
  {
    "text": "of potential which is it does",
    "start": "1396640",
    "end": "1398159"
  },
  {
    "text": "standardize the process eventually and",
    "start": "1398159",
    "end": "1400320"
  },
  {
    "text": "it automates the process and the final",
    "start": "1400320",
    "end": "1402520"
  },
  {
    "text": "one is basically human in the loop which",
    "start": "1402520",
    "end": "1404440"
  },
  {
    "text": "is something I feel like you know this",
    "start": "1404440",
    "end": "1407279"
  },
  {
    "text": "is something that",
    "start": "1407279",
    "end": "1408679"
  },
  {
    "text": "everybody is doing um but not the most",
    "start": "1408679",
    "end": "1411480"
  },
  {
    "text": "efficient so let's let's just ignore",
    "start": "1411480",
    "end": "1413200"
  },
  {
    "text": "human in the loop maybe let's let open",
    "start": "1413200",
    "end": "1415400"
  },
  {
    "text": "the eye talk about this",
    "start": "1415400",
    "end": "1419679"
  },
  {
    "text": "um the final thing that I wanted to say",
    "start": "1419679",
    "end": "1422400"
  },
  {
    "text": "on this one or for this particular",
    "start": "1422400",
    "end": "1424440"
  },
  {
    "text": "presentation is um while fine tuning is",
    "start": "1424440",
    "end": "1427240"
  },
  {
    "text": "great yes you but you also need to think",
    "start": "1427240",
    "end": "1430640"
  },
  {
    "text": "about the entire pipeline which is how",
    "start": "1430640",
    "end": "1433880"
  },
  {
    "text": "you're thinking about the data",
    "start": "1433880",
    "end": "1435080"
  },
  {
    "text": "collection how you're thinking about the",
    "start": "1435080",
    "end": "1436520"
  },
  {
    "text": "storage management how you choosing a",
    "start": "1436520",
    "end": "1438520"
  },
  {
    "text": "base model so optimizing the performance",
    "start": "1438520",
    "end": "1440679"
  },
  {
    "text": "of your model doesn't really depend on",
    "start": "1440679",
    "end": "1442440"
  },
  {
    "text": "just one feature while it may work",
    "start": "1442440",
    "end": "1445360"
  },
  {
    "text": "perfectly for like a single oneoff demo",
    "start": "1445360",
    "end": "1448279"
  },
  {
    "text": "but to be able to put a robust",
    "start": "1448279",
    "end": "1450360"
  },
  {
    "text": "application that does sustain the test",
    "start": "1450360",
    "end": "1453960"
  },
  {
    "text": "of time and obviously I I'm I'm not",
    "start": "1453960",
    "end": "1456760"
  },
  {
    "text": "saying you know what would be an ideal",
    "start": "1456760",
    "end": "1459120"
  },
  {
    "text": "time that you should be testing on um",
    "start": "1459120",
    "end": "1461440"
  },
  {
    "text": "but in in the case the goal is to be",
    "start": "1461440",
    "end": "1465279"
  },
  {
    "text": "able to get the Optimal Performance of",
    "start": "1465279",
    "end": "1466960"
  },
  {
    "text": "the model to be able ble to deal with",
    "start": "1466960",
    "end": "1468559"
  },
  {
    "text": "all the data drift and the prompt drift",
    "start": "1468559",
    "end": "1470360"
  },
  {
    "text": "and all of those things while also",
    "start": "1470360",
    "end": "1472520"
  },
  {
    "text": "making sure that we're catching a few",
    "start": "1472520",
    "end": "1474360"
  },
  {
    "text": "things early and we're not exposing the",
    "start": "1474360",
    "end": "1476159"
  },
  {
    "text": "Enterprise to like reputational Risk",
    "start": "1476159",
    "end": "1478120"
  },
  {
    "text": "compliance risk and all of those things",
    "start": "1478120",
    "end": "1480120"
  },
  {
    "text": "the entire thing has to be thought of um",
    "start": "1480120",
    "end": "1482880"
  },
  {
    "text": "so is a big picture decision that I",
    "start": "1482880",
    "end": "1485120"
  },
  {
    "text": "would say um that needs to be taken so",
    "start": "1485120",
    "end": "1488240"
  },
  {
    "text": "that's that's all my presentation for",
    "start": "1488240",
    "end": "1490039"
  },
  {
    "text": "today um I I hope everybody learned",
    "start": "1490039",
    "end": "1492960"
  },
  {
    "text": "something new if there is uh something",
    "start": "1492960",
    "end": "1495559"
  },
  {
    "text": "you would like to go with me in detail",
    "start": "1495559",
    "end": "1497840"
  },
  {
    "text": "well then we can do that after the",
    "start": "1497840",
    "end": "1499240"
  },
  {
    "text": "presentation but thank you so",
    "start": "1499240",
    "end": "1501500"
  },
  {
    "text": "[Applause]",
    "start": "1501500",
    "end": "1503020"
  },
  {
    "text": "[Music]",
    "start": "1503020",
    "end": "1506150"
  },
  {
    "text": "much",
    "start": "1507640",
    "end": "1510640"
  }
]