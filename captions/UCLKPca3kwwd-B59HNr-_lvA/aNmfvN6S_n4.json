[
  {
    "text": "[Music]",
    "start": "350",
    "end": "13519"
  },
  {
    "text": "so yeah my name is Remy I'm the",
    "start": "13519",
    "end": "16240"
  },
  {
    "text": "co-author and co-maintainer of the open",
    "start": "16240",
    "end": "18160"
  },
  {
    "text": "source Library outlines which some of",
    "start": "18160",
    "end": "20000"
  },
  {
    "text": "you might know and I'm also the CEO and",
    "start": "20000",
    "end": "22720"
  },
  {
    "text": "co-founder of doxt or do text uh",
    "start": "22720",
    "end": "25480"
  },
  {
    "text": "whichever you prefer uh we're more",
    "start": "25480",
    "end": "27840"
  },
  {
    "text": "traditional machine learning people and",
    "start": "27840",
    "end": "30640"
  },
  {
    "text": "the motivation for work is the very",
    "start": "30640",
    "end": "33120"
  },
  {
    "text": "simple observation that large language",
    "start": "33120",
    "end": "35520"
  },
  {
    "text": "models are fundamentally flawed I'll",
    "start": "35520",
    "end": "37760"
  },
  {
    "text": "give you a very simple example um you're",
    "start": "37760",
    "end": "40280"
  },
  {
    "text": "trying to extract flight information",
    "start": "40280",
    "end": "42600"
  },
  {
    "text": "from a bunch of emails uh of course you",
    "start": "42600",
    "end": "45680"
  },
  {
    "text": "want them to be you know a j Json object",
    "start": "45680",
    "end": "49160"
  },
  {
    "text": "um you know with origin destination Etc",
    "start": "49160",
    "end": "51760"
  },
  {
    "text": "so you go to open AI you prompt the",
    "start": "51760",
    "end": "53800"
  },
  {
    "text": "model to death you threaten it uh you",
    "start": "53800",
    "end": "56160"
  },
  {
    "text": "use function calling and what you get",
    "start": "56160",
    "end": "58519"
  },
  {
    "text": "sometimes as an answer is Jason decode",
    "start": "58519",
    "end": "60920"
  },
  {
    "text": "eror um I give you very simple examples",
    "start": "60920",
    "end": "63800"
  },
  {
    "text": "but this has like very fundamental",
    "start": "63800",
    "end": "65760"
  },
  {
    "text": "implications because Computing rest on",
    "start": "65760",
    "end": "68680"
  },
  {
    "text": "interfaces we're able to build module",
    "start": "68680",
    "end": "70960"
  },
  {
    "text": "infrastructures and very complex",
    "start": "70960",
    "end": "72880"
  },
  {
    "text": "infrastructure because we can trust the",
    "start": "72880",
    "end": "75600"
  },
  {
    "text": "API of the pieces of code and here what",
    "start": "75600",
    "end": "78360"
  },
  {
    "text": "telling you and what you've probably",
    "start": "78360",
    "end": "79960"
  },
  {
    "text": "witnessed uh you can't actually trust",
    "start": "79960",
    "end": "82200"
  },
  {
    "text": "large language model to return",
    "start": "82200",
    "end": "83799"
  },
  {
    "text": "consistant outputs and you know in short",
    "start": "83799",
    "end": "86880"
  },
  {
    "text": "is that the technology for agents is",
    "start": "86880",
    "end": "89119"
  },
  {
    "text": "currently not there here",
    "start": "89119",
    "end": "91520"
  },
  {
    "text": "um so the good news is that structure",
    "start": "91520",
    "end": "94600"
  },
  {
    "text": "generation which is the ability of",
    "start": "94600",
    "end": "97360"
  },
  {
    "text": "guiding the model to return to the",
    "start": "97360",
    "end": "100799"
  },
  {
    "text": "specific structure actually Sol we see",
    "start": "100799",
    "end": "103720"
  },
  {
    "text": "is that it allows you to be GPT for a",
    "start": "103720",
    "end": "105520"
  },
  {
    "text": "sort of a by",
    "start": "105520",
    "end": "106880"
  },
  {
    "text": "product the goals for today are first to",
    "start": "106880",
    "end": "109280"
  },
  {
    "text": "introduce the open source Library",
    "start": "109280",
    "end": "110560"
  },
  {
    "text": "outlines for those of you who don't know",
    "start": "110560",
    "end": "111960"
  },
  {
    "text": "about it then very briefly explain how",
    "start": "111960",
    "end": "114560"
  },
  {
    "text": "it works I won't get into the technical",
    "start": "114560",
    "end": "116039"
  },
  {
    "text": "details and then try to convince you",
    "start": "116039",
    "end": "118920"
  },
  {
    "text": "that you should use it today uh for you",
    "start": "118920",
    "end": "121960"
  },
  {
    "text": "know most of the workflows uh that you",
    "start": "121960",
    "end": "124000"
  },
  {
    "text": "have to deal with and sort of a very",
    "start": "124000",
    "end": "126000"
  },
  {
    "text": "short glimpse into the near future so",
    "start": "126000",
    "end": "128679"
  },
  {
    "text": "outlines a python Library emphasis on",
    "start": "128679",
    "end": "132160"
  },
  {
    "text": "Library uh you can actually include",
    "start": "132160",
    "end": "134959"
  },
  {
    "text": "outlines in your workflow and it's not",
    "start": "134959",
    "end": "136720"
  },
  {
    "text": "like Frameworks where you have to make",
    "start": "136720",
    "end": "138519"
  },
  {
    "text": "your workflow you know fit inside a",
    "start": "138519",
    "end": "140599"
  },
  {
    "text": "framework um I think as a result uh it's",
    "start": "140599",
    "end": "143560"
  },
  {
    "text": "been adopted by VM and TGI uh in the",
    "start": "143560",
    "end": "147080"
  },
  {
    "text": "serving Frameworks and if you use",
    "start": "147080",
    "end": "149080"
  },
  {
    "text": "function calling in either of these",
    "start": "149080",
    "end": "150760"
  },
  {
    "text": "libraries you're actually using outlines",
    "start": "150760",
    "end": "152519"
  },
  {
    "text": "under the hood uh outlines under the",
    "start": "152519",
    "end": "154959"
  },
  {
    "text": "hood so I'm co-author uh but outlines",
    "start": "154959",
    "end": "158360"
  },
  {
    "text": "would be nothing with uh its",
    "start": "158360",
    "end": "159680"
  },
  {
    "text": "contributors today it's 87 uh I think it",
    "start": "159680",
    "end": "162400"
  },
  {
    "text": "might be 88 I think I merged a PR this",
    "start": "162400",
    "end": "164440"
  },
  {
    "text": "morning I don't remember and so outlines",
    "start": "164440",
    "end": "166519"
  },
  {
    "text": "would be nothing with all without all",
    "start": "166519",
    "end": "168319"
  },
  {
    "text": "these people and I thank them uh thank",
    "start": "168319",
    "end": "170159"
  },
  {
    "text": "them a lot um people thought were crazy",
    "start": "170159",
    "end": "174080"
  },
  {
    "text": "about a year ago when we're talking",
    "start": "174080",
    "end": "175440"
  },
  {
    "text": "about structure generation uh but since",
    "start": "175440",
    "end": "177840"
  },
  {
    "text": "then I pretty happy because it looks",
    "start": "177840",
    "end": "180400"
  },
  {
    "text": "like people are sort of caught up with",
    "start": "180400",
    "end": "182200"
  },
  {
    "text": "the topic and realize that you can",
    "start": "182200",
    "end": "184319"
  },
  {
    "text": "actually you know you can actually uh do",
    "start": "184319",
    "end": "187159"
  },
  {
    "text": "structur out well um so just now just to",
    "start": "187159",
    "end": "190560"
  },
  {
    "text": "run through quick run through outline um",
    "start": "190560",
    "end": "192959"
  },
  {
    "text": "so usually generating text happens in",
    "start": "192959",
    "end": "195000"
  },
  {
    "text": "three stages uh the first stage is that",
    "start": "195000",
    "end": "197200"
  },
  {
    "text": "you need to choose the model and",
    "start": "197200",
    "end": "198640"
  },
  {
    "text": "instantiate it so outlands is purely",
    "start": "198640",
    "end": "200840"
  },
  {
    "text": "focused on open source models uh we have",
    "start": "200840",
    "end": "203519"
  },
  {
    "text": "integration with six different model",
    "start": "203519",
    "end": "205040"
  },
  {
    "text": "providers uh Transformers Lama CPP and",
    "start": "205040",
    "end": "208120"
  },
  {
    "text": "also uh recently we added mlx ml xlm um",
    "start": "208120",
    "end": "212400"
  },
  {
    "text": "we have an integration with open AI but",
    "start": "212400",
    "end": "214280"
  },
  {
    "text": "that's mostly for us to compare the",
    "start": "214280",
    "end": "216280"
  },
  {
    "text": "results that we get with open models",
    "start": "216280",
    "end": "218400"
  },
  {
    "text": "with the results that are given by open",
    "start": "218400",
    "end": "220080"
  },
  {
    "text": "AI the second step is to I mean generate",
    "start": "220080",
    "end": "223879"
  },
  {
    "text": "text what you do is that you instantiate",
    "start": "223879",
    "end": "226000"
  },
  {
    "text": "a generator using generate. text here we",
    "start": "226000",
    "end": "228840"
  },
  {
    "text": "just want to you know return a single",
    "start": "228840",
    "end": "231000"
  },
  {
    "text": "sentence so we're telling the generator",
    "start": "231000",
    "end": "233159"
  },
  {
    "text": "stop whenever you encounter a period and",
    "start": "233159",
    "end": "236239"
  },
  {
    "text": "question is described then you call the",
    "start": "236239",
    "end": "237959"
  },
  {
    "text": "generator uh with your prompt here is",
    "start": "237959",
    "end": "240280"
  },
  {
    "text": "describ the benefits of structure",
    "start": "240280",
    "end": "241680"
  },
  {
    "text": "generation in one sentence and you'll",
    "start": "241680",
    "end": "245360"
  },
  {
    "text": "have to wait for 10 more minutes uh",
    "start": "245360",
    "end": "247280"
  },
  {
    "text": "hopefully less okay now we get into",
    "start": "247280",
    "end": "249799"
  },
  {
    "text": "structure generation so with outlines",
    "start": "249799",
    "end": "253560"
  },
  {
    "text": "without outlines if you ask what is the",
    "start": "253560",
    "end": "255599"
  },
  {
    "text": "IP address of the public Google DNS",
    "start": "255599",
    "end": "257440"
  },
  {
    "text": "servers and you just generate text you",
    "start": "257440",
    "end": "259840"
  },
  {
    "text": "just let the llm do its thing then",
    "start": "259840",
    "end": "262199"
  },
  {
    "text": "generally it will Yap for a long time uh",
    "start": "262199",
    "end": "264560"
  },
  {
    "text": "you know 100 tokens 500 tokens and the",
    "start": "264560",
    "end": "267280"
  },
  {
    "text": "answer will be somewhere in there and",
    "start": "267280",
    "end": "269360"
  },
  {
    "text": "the way you extract the answer is using",
    "start": "269360",
    "end": "271000"
  },
  {
    "text": "regular Expressions generally here what",
    "start": "271000",
    "end": "274080"
  },
  {
    "text": "you can do with outlines is actually",
    "start": "274080",
    "end": "275759"
  },
  {
    "text": "taking that regular expression that you",
    "start": "275759",
    "end": "277400"
  },
  {
    "text": "use you would use to extract the answer",
    "start": "277400",
    "end": "279840"
  },
  {
    "text": "and use it to guide the model to tell",
    "start": "279840",
    "end": "281960"
  },
  {
    "text": "the model this is the structure that the",
    "start": "281960",
    "end": "283759"
  },
  {
    "text": "output should follow and as you see you",
    "start": "283759",
    "end": "285919"
  },
  {
    "text": "can of remove the yapping you print the",
    "start": "285919",
    "end": "288000"
  },
  {
    "text": "look you just call generator rejects",
    "start": "288000",
    "end": "290720"
  },
  {
    "text": "call the generator and what you get is",
    "start": "290720",
    "end": "292680"
  },
  {
    "text": "just the result and it's actually the",
    "start": "292680",
    "end": "294320"
  },
  {
    "text": "correct answer uh that was with mol 7",
    "start": "294320",
    "end": "298639"
  },
  {
    "text": "bb01 regular expressions are not the",
    "start": "298639",
    "end": "300759"
  },
  {
    "text": "only way to Define structure uh",
    "start": "300759",
    "end": "302479"
  },
  {
    "text": "something that people need a lot in",
    "start": "302479",
    "end": "303960"
  },
  {
    "text": "practice is like Json and outlines allow",
    "start": "303960",
    "end": "307039"
  },
  {
    "text": "you to generate um to generate text that",
    "start": "307039",
    "end": "312199"
  },
  {
    "text": "you know is adjacent object with a given",
    "start": "312199",
    "end": "313960"
  },
  {
    "text": "structure the way you specify the",
    "start": "313960",
    "end": "315800"
  },
  {
    "text": "structure is using Json schema or you",
    "start": "315800",
    "end": "318759"
  },
  {
    "text": "can pass pantic models as well um now",
    "start": "318759",
    "end": "321800"
  },
  {
    "text": "you might notice on the flight",
    "start": "321800",
    "end": "323280"
  },
  {
    "text": "information so here we you know it's",
    "start": "323280",
    "end": "325600"
  },
  {
    "text": "example that I use at the beginning",
    "start": "325600",
    "end": "326919"
  },
  {
    "text": "you're extracting flight information",
    "start": "326919",
    "end": "328199"
  },
  {
    "text": "from an email I could have I've used",
    "start": "328199",
    "end": "330120"
  },
  {
    "text": "string as a type for origin and",
    "start": "330120",
    "end": "331800"
  },
  {
    "text": "destination but I did not I use actually",
    "start": "331800",
    "end": "334199"
  },
  {
    "text": "a custom type that we implemented in",
    "start": "334199",
    "end": "336039"
  },
  {
    "text": "outlines and the reason is that origin",
    "start": "336039",
    "end": "338319"
  },
  {
    "text": "and destination have way more structure",
    "start": "338319",
    "end": "340360"
  },
  {
    "text": "than just text it's actually you know",
    "start": "340360",
    "end": "342639"
  },
  {
    "text": "it's it's an airport code that has three",
    "start": "342639",
    "end": "344759"
  },
  {
    "text": "letters that's capitalized and you can",
    "start": "344759",
    "end": "346400"
  },
  {
    "text": "actually specify more and more structure",
    "start": "346400",
    "end": "349120"
  },
  {
    "text": "all the structure that you have in your",
    "start": "349120",
    "end": "350280"
  },
  {
    "text": "problem",
    "start": "350280",
    "end": "351280"
  },
  {
    "text": "basically uh you can use this with",
    "start": "351280",
    "end": "353680"
  },
  {
    "text": "vision models uh that's something that",
    "start": "353680",
    "end": "355440"
  },
  {
    "text": "we merged recently so here we took um I",
    "start": "355440",
    "end": "358360"
  },
  {
    "text": "think it's a picture from Wikipedia Med",
    "start": "358360",
    "end": "360520"
  },
  {
    "text": "of a dish uh we tell the model what is",
    "start": "360520",
    "end": "365240"
  },
  {
    "text": "the Json that we expect as a as a as an",
    "start": "365240",
    "end": "369039"
  },
  {
    "text": "output and then we instantiate the",
    "start": "369039",
    "end": "371199"
  },
  {
    "text": "generator and then pass the image on the",
    "start": "371199",
    "end": "372960"
  },
  {
    "text": "prompt to the generator and we get valid",
    "start": "372960",
    "end": "375160"
  },
  {
    "text": "Jas um if you want to install outlines",
    "start": "375160",
    "end": "379000"
  },
  {
    "text": "uh and you think you could benefit from",
    "start": "379000",
    "end": "380199"
  },
  {
    "text": "structure generation then it's very",
    "start": "380199",
    "end": "381560"
  },
  {
    "text": "simple just Pi install outlines now I'm",
    "start": "381560",
    "end": "384840"
  },
  {
    "text": "going to try to very quickly explain how",
    "start": "384840",
    "end": "387000"
  },
  {
    "text": "it works um so models themselves uh what",
    "start": "387000",
    "end": "391080"
  },
  {
    "text": "mol and ker this W are doing uh is",
    "start": "391080",
    "end": "394000"
  },
  {
    "text": "actually training model weights uh what",
    "start": "394000",
    "end": "396000"
  },
  {
    "text": "a model does is uh you input a prompt",
    "start": "396000",
    "end": "399479"
  },
  {
    "text": "you send a prompt it's like token IDs",
    "start": "399479",
    "end": "401360"
  },
  {
    "text": "and what you get as an output is not",
    "start": "401360",
    "end": "402759"
  },
  {
    "text": "text it's logic it's a probability",
    "start": "402759",
    "end": "404759"
  },
  {
    "text": "distribution over the next token now",
    "start": "404759",
    "end": "407039"
  },
  {
    "text": "what happens after that when you want to",
    "start": "407039",
    "end": "408520"
  },
  {
    "text": "generate text the first step is that you",
    "start": "408520",
    "end": "410199"
  },
  {
    "text": "have a logic processor that biases the",
    "start": "410199",
    "end": "412120"
  },
  {
    "text": "Logics you probably use this every day",
    "start": "412120",
    "end": "415039"
  },
  {
    "text": "actually without noticing it when you",
    "start": "415039",
    "end": "416479"
  },
  {
    "text": "use temperature or when you stop K top P",
    "start": "416479",
    "end": "418639"
  },
  {
    "text": "sampling you actually buy in the logits",
    "start": "418639",
    "end": "420879"
  },
  {
    "text": "and once you have your biased logits use",
    "start": "420879",
    "end": "422680"
  },
  {
    "text": "a sampling algorithm then you get a",
    "start": "422680",
    "end": "424199"
  },
  {
    "text": "token and once you have your token you",
    "start": "424199",
    "end": "426120"
  },
  {
    "text": "add it to the prompt and then feed it",
    "start": "426120",
    "end": "427680"
  },
  {
    "text": "back to the",
    "start": "427680",
    "end": "428800"
  },
  {
    "text": "L and where we fit is here we actually",
    "start": "428800",
    "end": "433360"
  },
  {
    "text": "why the",
    "start": "433360",
    "end": "434800"
  },
  {
    "text": "model whenever the model generates",
    "start": "434800",
    "end": "437479"
  },
  {
    "text": "logits we look at every token and we say",
    "start": "437479",
    "end": "441280"
  },
  {
    "text": "if I add this token to the current",
    "start": "441280",
    "end": "443280"
  },
  {
    "text": "generation is it going to violate the",
    "start": "443280",
    "end": "445000"
  },
  {
    "text": "structure if the answer is yes we we",
    "start": "445000",
    "end": "447879"
  },
  {
    "text": "like we mask it so that it doesn't get",
    "start": "447879",
    "end": "449280"
  },
  {
    "text": "gener generated now that story is very",
    "start": "449280",
    "end": "451440"
  },
  {
    "text": "simple what is really hard is doing that",
    "start": "451440",
    "end": "454000"
  },
  {
    "text": "efficiently and that's what we figured",
    "start": "454000",
    "end": "455680"
  },
  {
    "text": "out at Doc text and that's what makes us",
    "start": "455680",
    "end": "457639"
  },
  {
    "text": "different from the other libraries like",
    "start": "457639",
    "end": "459120"
  },
  {
    "text": "guidance or lmq that um do structure",
    "start": "459120",
    "end": "462919"
  },
  {
    "text": "generation and now I'm going to convince",
    "start": "462919",
    "end": "467159"
  },
  {
    "text": "you uh that there's absolutely no reason",
    "start": "467159",
    "end": "470319"
  },
  {
    "text": "to not use sorry for the double negation",
    "start": "470319",
    "end": "472960"
  },
  {
    "text": "here to not use structure",
    "start": "472960",
    "end": "474840"
  },
  {
    "text": "generation uh the first reason is that",
    "start": "474840",
    "end": "476879"
  },
  {
    "text": "most Tex is structured um I talked to",
    "start": "476879",
    "end": "479280"
  },
  {
    "text": "you about Json earlier we talked about",
    "start": "479280",
    "end": "481400"
  },
  {
    "text": "regular Expressions but here I just took",
    "start": "481400",
    "end": "483440"
  },
  {
    "text": "the GSM data set um if you look at it if",
    "start": "483440",
    "end": "486639"
  },
  {
    "text": "you're not me and don't everywhere um",
    "start": "486639",
    "end": "490400"
  },
  {
    "text": "say immediately if you at the right you",
    "start": "490400",
    "end": "493000"
  },
  {
    "text": "can actually see that it's highly",
    "start": "493000",
    "end": "494680"
  },
  {
    "text": "structured it's always Q period text",
    "start": "494680",
    "end": "497639"
  },
  {
    "text": "until a question mark then Etc so on and",
    "start": "497639",
    "end": "500440"
  },
  {
    "text": "so forth arithmetic operation which is",
    "start": "500440",
    "end": "502319"
  },
  {
    "text": "defined by a context free grammar and",
    "start": "502319",
    "end": "504280"
  },
  {
    "text": "you could actually Express this in",
    "start": "504280",
    "end": "505520"
  },
  {
    "text": "outlines and just get the answer at the",
    "start": "505520",
    "end": "507479"
  },
  {
    "text": "end which is you know very sick",
    "start": "507479",
    "end": "509960"
  },
  {
    "text": "so there's a lot of structured text out",
    "start": "509960",
    "end": "512039"
  },
  {
    "text": "there not just",
    "start": "512039",
    "end": "514080"
  },
  {
    "text": "uh thank",
    "start": "514080",
    "end": "516719"
  },
  {
    "text": "you I'll I'll be I'll be quick um of",
    "start": "516719",
    "end": "520479"
  },
  {
    "text": "course the second benefit is that uh you",
    "start": "520479",
    "end": "523399"
  },
  {
    "text": "get valid structure I mean that's an",
    "start": "523399",
    "end": "524800"
  },
  {
    "text": "obvious one that's what we're doing it",
    "start": "524800",
    "end": "526560"
  },
  {
    "text": "uh I like this meme uh at the bottom",
    "start": "526560",
    "end": "528560"
  },
  {
    "text": "this is what people are currently doing",
    "start": "528560",
    "end": "530320"
  },
  {
    "text": "uh it's just crazy stuff to get valid",
    "start": "530320",
    "end": "532080"
  },
  {
    "text": "Chason as an output and it's not even",
    "start": "532080",
    "end": "533959"
  },
  {
    "text": "guaranteed and here with lines you just",
    "start": "533959",
    "end": "536120"
  },
  {
    "text": "sample what you want it's as simple as",
    "start": "536120",
    "end": "538079"
  },
  {
    "text": "this and as a an experiment It's",
    "start": "538079",
    "end": "540560"
  },
  {
    "text": "actually an experiment that pretty based",
    "start": "540560",
    "end": "542000"
  },
  {
    "text": "did they took mol 7 bv01 they used a",
    "start": "542000",
    "end": "544880"
  },
  {
    "text": "version of CNL that they modified so",
    "start": "544880",
    "end": "546720"
  },
  {
    "text": "that give structural output Json what",
    "start": "546720",
    "end": "548920"
  },
  {
    "text": "they found is mrol 7bv 01 only gets",
    "start": "548920",
    "end": "551399"
  },
  {
    "text": "valid Json uh 177% of the time when you",
    "start": "551399",
    "end": "554959"
  },
  {
    "text": "had structure generation on top of it",
    "start": "554959",
    "end": "556839"
  },
  {
    "text": "you get 99.9% and that's without",
    "start": "556839",
    "end": "558959"
  },
  {
    "text": "optimizing the pr so you can actually",
    "start": "558959",
    "end": "561160"
  },
  {
    "text": "get you know you can actually get better",
    "start": "561160",
    "end": "563079"
  },
  {
    "text": "than",
    "start": "563079",
    "end": "565040"
  },
  {
    "text": "this the nice thing is that it also adds",
    "start": "565040",
    "end": "568240"
  },
  {
    "text": "negligible overhead so you actually have",
    "start": "568240",
    "end": "570440"
  },
  {
    "text": "you know you don't have to fear for that",
    "start": "570440",
    "end": "572000"
  },
  {
    "text": "affecting inference time uh which is the",
    "start": "572000",
    "end": "574240"
  },
  {
    "text": "highly you know highly non-trivial thing",
    "start": "574240",
    "end": "576800"
  },
  {
    "text": "uh here we compared uh the overhead",
    "start": "576800",
    "end": "579839"
  },
  {
    "text": "introduced by Guidance when they do",
    "start": "579839",
    "end": "581800"
  },
  {
    "text": "structure generation uh you know as a",
    "start": "581800",
    "end": "583880"
  },
  {
    "text": "function of the number of generated",
    "start": "583880",
    "end": "585200"
  },
  {
    "text": "token and at the bottom it's outlines uh",
    "start": "585200",
    "end": "587560"
  },
  {
    "text": "outline says approximately zero Until",
    "start": "587560",
    "end": "589560"
  },
  {
    "text": "the End uh as a trade-off there's a",
    "start": "589560",
    "end": "591519"
  },
  {
    "text": "compilation time but during inference it",
    "start": "591519",
    "end": "593640"
  },
  {
    "text": "doesn't slow down inference now we're at",
    "start": "593640",
    "end": "595000"
  },
  {
    "text": "a point where we could integrate this in",
    "start": "595000",
    "end": "596480"
  },
  {
    "text": "grock and you wouldn't see the",
    "start": "596480",
    "end": "597920"
  },
  {
    "text": "difference between structur and instru R",
    "start": "597920",
    "end": "600440"
  },
  {
    "text": "um so no overhead but even more than no",
    "start": "600440",
    "end": "604120"
  },
  {
    "text": "overhead it is faster to generate text",
    "start": "604120",
    "end": "607040"
  },
  {
    "text": "with structure Generation Um the first",
    "start": "607040",
    "end": "609959"
  },
  {
    "text": "is that when you take Json you don't",
    "start": "609959",
    "end": "611920"
  },
  {
    "text": "need to generate the tokens that",
    "start": "611920",
    "end": "613399"
  },
  {
    "text": "correspond to the bracket and to the",
    "start": "613399",
    "end": "614920"
  },
  {
    "text": "field names I know that in advance I",
    "start": "614920",
    "end": "616279"
  },
  {
    "text": "don't need to ask the model to return uh",
    "start": "616279",
    "end": "619079"
  },
  {
    "text": "the tokens so here on this very simple",
    "start": "619079",
    "end": "621399"
  },
  {
    "text": "example only five out of 10 tokens need",
    "start": "621399",
    "end": "623959"
  },
  {
    "text": "to be generated so only one half but as",
    "start": "623959",
    "end": "626760"
  },
  {
    "text": "an even more subtle um way in which it",
    "start": "626760",
    "end": "630440"
  },
  {
    "text": "accelerates inference and this is the",
    "start": "630440",
    "end": "633839"
  },
  {
    "text": "example that we took at the beginning so",
    "start": "633839",
    "end": "635560"
  },
  {
    "text": "here I I asked CHP like a good model",
    "start": "635560",
    "end": "637600"
  },
  {
    "text": "that check GPT the same question what is",
    "start": "637600",
    "end": "640720"
  },
  {
    "text": "the uh public like the the of Google's",
    "start": "640720",
    "end": "644560"
  },
  {
    "text": "public DNS servers and CH GPT took 50",
    "start": "644560",
    "end": "648160"
  },
  {
    "text": "tokens you know it yed yed Yap and give",
    "start": "648160",
    "end": "651000"
  },
  {
    "text": "it up 50 tokens it's not as bad it could",
    "start": "651000",
    "end": "653440"
  },
  {
    "text": "get a lot worse uh with lesser models uh",
    "start": "653440",
    "end": "656639"
  },
  {
    "text": "but when you use structure generation",
    "start": "656639",
    "end": "658000"
  },
  {
    "text": "you're just Genera eight token so that's",
    "start": "658000",
    "end": "660040"
  },
  {
    "text": "a subtle way in which it accelerates",
    "start": "660040",
    "end": "661639"
  },
  {
    "text": "inference by your law",
    "start": "661639",
    "end": "664600"
  },
  {
    "text": "um then it improves efficiency and",
    "start": "664600",
    "end": "666959"
  },
  {
    "text": "that's probably the most uh actually",
    "start": "666959",
    "end": "668959"
  },
  {
    "text": "mind-blowing result uh that we've had so",
    "start": "668959",
    "end": "672000"
  },
  {
    "text": "here what you're looking at is the",
    "start": "672000",
    "end": "674360"
  },
  {
    "text": "accuracy on gsm 8K uh with again mral 7",
    "start": "674360",
    "end": "679120"
  },
  {
    "text": "bb01 structured and structured and here",
    "start": "679120",
    "end": "682320"
  },
  {
    "text": "we look at the accuracy as a function of",
    "start": "682320",
    "end": "684079"
  },
  {
    "text": "the number of shots so the number of",
    "start": "684079",
    "end": "685600"
  },
  {
    "text": "examples that you give to the model uh",
    "start": "685600",
    "end": "688120"
  },
  {
    "text": "before asking the question",
    "start": "688120",
    "end": "690240"
  },
  {
    "text": "and what we found is that yeah foreign",
    "start": "690240",
    "end": "692279"
  },
  {
    "text": "structure normal one shot is worse than",
    "start": "692279",
    "end": "694360"
  },
  {
    "text": "eight shots that's completely expected",
    "start": "694360",
    "end": "697079"
  },
  {
    "text": "uh but what we find with structured is",
    "start": "697079",
    "end": "698519"
  },
  {
    "text": "that you actually and that's really",
    "start": "698519",
    "end": "699720"
  },
  {
    "text": "surprised us is that you actually get in",
    "start": "699720",
    "end": "702040"
  },
  {
    "text": "the same ballpack in terms of accuracy",
    "start": "702040",
    "end": "703839"
  },
  {
    "text": "with one shot as you do with eight shots",
    "start": "703839",
    "end": "706360"
  },
  {
    "text": "which is surprising for a machine",
    "start": "706360",
    "end": "707720"
  },
  {
    "text": "learning like you would think that",
    "start": "707720",
    "end": "709800"
  },
  {
    "text": "examples are there to teach the model",
    "start": "709800",
    "end": "711480"
  },
  {
    "text": "about the task but it looks like it's",
    "start": "711480",
    "end": "713279"
  },
  {
    "text": "actually there to teach the model about",
    "start": "713279",
    "end": "714800"
  },
  {
    "text": "the structure of the",
    "start": "714800",
    "end": "716839"
  },
  {
    "text": "problem the more investigations do in",
    "start": "716839",
    "end": "719480"
  },
  {
    "text": "this line but that was very mindblowing",
    "start": "719480",
    "end": "721639"
  },
  {
    "text": "and the last one which probably you know",
    "start": "721639",
    "end": "723680"
  },
  {
    "text": "after faster a lot of people care about",
    "start": "723680",
    "end": "725839"
  },
  {
    "text": "here is that it does improve the",
    "start": "725839",
    "end": "727959"
  },
  {
    "text": "performance of Open Source models um",
    "start": "727959",
    "end": "731839"
  },
  {
    "text": "here um what you're looking at is the",
    "start": "731839",
    "end": "734920"
  },
  {
    "text": "Burly function calling leaderboard a",
    "start": "734920",
    "end": "737120"
  },
  {
    "text": "simple function Benchmark and we'll look",
    "start": "737120",
    "end": "739639"
  },
  {
    "text": "at the accuracy so first thing we did is",
    "start": "739639",
    "end": "742199"
  },
  {
    "text": "that we took Microsoft 3 medium model",
    "start": "742199",
    "end": "745320"
  },
  {
    "text": "which is a small model but we looked at",
    "start": "745320",
    "end": "748240"
  },
  {
    "text": "its accuracy with structure generation",
    "start": "748240",
    "end": "750079"
  },
  {
    "text": "it's 86% which is pretty good for an",
    "start": "750079",
    "end": "751920"
  },
  {
    "text": "open Model 5 is actually pretty good",
    "start": "751920",
    "end": "754519"
  },
  {
    "text": "model when you add structure",
    "start": "754519",
    "end": "756959"
  },
  {
    "text": "generation you get",
    "start": "756959",
    "end": "760000"
  },
  {
    "text": "96.5% and as a comparison GPT 4 the best",
    "start": "760000",
    "end": "763440"
  },
  {
    "text": "G version of GPT 4 on this Tas get",
    "start": "763440",
    "end": "767000"
  },
  {
    "text": "93.5% uh on this Benchmark and now there",
    "start": "767000",
    "end": "770199"
  },
  {
    "text": "are two things to know is that 96.5%",
    "start": "770199",
    "end": "773160"
  },
  {
    "text": "gets dangerously useful and the second",
    "start": "773160",
    "end": "776680"
  },
  {
    "text": "thing is that we have open models that",
    "start": "776680",
    "end": "780639"
  },
  {
    "text": "are available today that can beat you",
    "start": "780639",
    "end": "783240"
  },
  {
    "text": "know larger models um without",
    "start": "783240",
    "end": "786880"
  },
  {
    "text": "fine-tuning so it's pretty huge room for",
    "start": "786880",
    "end": "789639"
  },
  {
    "text": "uh open models and that's why I'm really",
    "start": "789639",
    "end": "791199"
  },
  {
    "text": "bullish on open models I think you know",
    "start": "791199",
    "end": "793320"
  },
  {
    "text": "as a community we can actually extract a",
    "start": "793320",
    "end": "795160"
  },
  {
    "text": "lot more out of these",
    "start": "795160",
    "end": "796920"
  },
  {
    "text": "models um and this is just a glimpse um",
    "start": "796920",
    "end": "801519"
  },
  {
    "text": "the work that I just showed you is what",
    "start": "801519",
    "end": "803680"
  },
  {
    "text": "we did at dotex about a year ago since",
    "start": "803680",
    "end": "806639"
  },
  {
    "text": "then we've generalized from regular",
    "start": "806639",
    "end": "808680"
  },
  {
    "text": "expression to what you call context free",
    "start": "808680",
    "end": "811240"
  },
  {
    "text": "grammars context free grammars are used",
    "start": "811240",
    "end": "813600"
  },
  {
    "text": "to define code they used to define",
    "start": "813600",
    "end": "815079"
  },
  {
    "text": "protein structure I mean and to Define",
    "start": "815079",
    "end": "817360"
  },
  {
    "text": "as well what I showed you earlier on the",
    "start": "817360",
    "end": "819560"
  },
  {
    "text": "gsmk example so we can do the same thing",
    "start": "819560",
    "end": "822199"
  },
  {
    "text": "gener structure generation with no",
    "start": "822199",
    "end": "823720"
  },
  {
    "text": "overhead with um with context fre",
    "start": "823720",
    "end": "826680"
  },
  {
    "text": "grammar we also started working on um",
    "start": "826680",
    "end": "830399"
  },
  {
    "text": "semantics like adding some semantic",
    "start": "830399",
    "end": "832240"
  },
  {
    "text": "constraints to the generation and one",
    "start": "832240",
    "end": "834560"
  },
  {
    "text": "very popular example this is to SQL uh",
    "start": "834560",
    "end": "837839"
  },
  {
    "text": "text to SQL most models",
    "start": "837839",
    "end": "839600"
  },
  {
    "text": "that SQL syntax usually what they get",
    "start": "839600",
    "end": "841880"
  },
  {
    "text": "wrong is they hallucinate table or",
    "start": "841880",
    "end": "843519"
  },
  {
    "text": "column names and they internally were're",
    "start": "843519",
    "end": "847519"
  },
  {
    "text": "able to get perfect test to SQL so I",
    "start": "847519",
    "end": "849720"
  },
  {
    "text": "can't guarantee you that the query will",
    "start": "849720",
    "end": "851399"
  },
  {
    "text": "be correct and give you the answer that",
    "start": "851399",
    "end": "852880"
  },
  {
    "text": "you expect but I can guarantee you that",
    "start": "852880",
    "end": "855079"
  },
  {
    "text": "it will run that's a pretty huge advance",
    "start": "855079",
    "end": "857920"
  },
  {
    "text": "in text to SQL and what else oh yeah and",
    "start": "857920",
    "end": "861720"
  },
  {
    "text": "we're also starting to uh to Bubble Up",
    "start": "861720",
    "end": "863959"
  },
  {
    "text": "computation into the structure",
    "start": "863959",
    "end": "866000"
  },
  {
    "text": "generation into the model architecture",
    "start": "866000",
    "end": "868160"
  },
  {
    "text": "because when you think about we're",
    "start": "868160",
    "end": "869519"
  },
  {
    "text": "biasing logits when you're biasing",
    "start": "869519",
    "end": "871560"
  },
  {
    "text": "logits the model is actually doing",
    "start": "871560",
    "end": "873120"
  },
  {
    "text": "computation for nothing and so you can",
    "start": "873120",
    "end": "875199"
  },
  {
    "text": "gain even more in efficiency by",
    "start": "875199",
    "end": "877320"
  },
  {
    "text": "preventing the model from doing this",
    "start": "877320",
    "end": "878680"
  },
  {
    "text": "computations in the first place and",
    "start": "878680",
    "end": "880399"
  },
  {
    "text": "that's all work that we'll actually",
    "start": "880399",
    "end": "881639"
  },
  {
    "text": "publish in the blog post I think in the",
    "start": "881639",
    "end": "883199"
  },
  {
    "text": "next couple of",
    "start": "883199",
    "end": "884880"
  },
  {
    "text": "weeks so all that to say that if you're",
    "start": "884880",
    "end": "889000"
  },
  {
    "text": "doing if you're not doing a chat bot",
    "start": "889000",
    "end": "891000"
  },
  {
    "text": "there's a really good chance that you",
    "start": "891000",
    "end": "892560"
  },
  {
    "text": "will be using structure generation you",
    "start": "892560",
    "end": "894880"
  },
  {
    "text": "know it's just a matter of time until",
    "start": "894880",
    "end": "896480"
  },
  {
    "text": "you adopt it I think uh or users are",
    "start": "896480",
    "end": "899079"
  },
  {
    "text": "pretty pretty pretty happy",
    "start": "899079",
    "end": "901079"
  },
  {
    "text": "so yeah thank you for your attention and",
    "start": "901079",
    "end": "905240"
  },
  {
    "text": "uh all the all the crazy claims that I",
    "start": "905240",
    "end": "908279"
  },
  {
    "text": "made you can go the QR code there's a",
    "start": "908279",
    "end": "911040"
  },
  {
    "text": "link to all the blog post",
    "start": "911040",
    "end": "915000"
  },
  {
    "text": "[Music]",
    "start": "916240",
    "end": "928990"
  },
  {
    "text": "a",
    "start": "929120",
    "end": "932120"
  }
]