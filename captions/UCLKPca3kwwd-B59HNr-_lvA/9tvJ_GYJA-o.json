[
  {
    "text": "[Music]",
    "start": "1720",
    "end": "8639"
  },
  {
    "text": "it's very difficult to teach um extremely technical material in about 20 20 minutes initially I had planned for",
    "start": "12679",
    "end": "17680"
  },
  {
    "text": "at least a 45 minute session so I left some reading material for you at the end and um all of the resources you could",
    "start": "17680",
    "end": "24000"
  },
  {
    "text": "download slides and everything so feel free to take screenshots or not and so I work at Nvidia I'm a Architects I work",
    "start": "24000",
    "end": "31080"
  },
  {
    "text": "primarily with retail clients and it's my job to essentially work with those clients understand sort of what their",
    "start": "31080",
    "end": "37040"
  },
  {
    "text": "main challenges are this is data processing computer vision um across all of the different use cases and then now",
    "start": "37040",
    "end": "43480"
  },
  {
    "text": "I'm focused on llm inference so my hope today is that um you get a better",
    "start": "43480",
    "end": "48879"
  },
  {
    "text": "intuition of exactly what's happening with this particular workload and how you go about uh to some degree sizing",
    "start": "48879",
    "end": "54760"
  },
  {
    "text": "things choosing different gpus etc etc and more importantly controlling the cost of a deployment cuz that's often",
    "start": "54760",
    "end": "60559"
  },
  {
    "text": "times the thing that's going to really prevent you from taking this taking this",
    "start": "60559",
    "end": "66000"
  },
  {
    "text": "to any meaningful scale is that overall cost of a deployment most folks that I've seen are doing some kind of hybrid",
    "start": "66000",
    "end": "72159"
  },
  {
    "text": "so you choose a big box API you have some set of queries that go there in addition to you have some set of queries",
    "start": "72159",
    "end": "78040"
  },
  {
    "text": "that go to some open- Source you know hosted model or some fine tune model that you have internally so just",
    "start": "78040",
    "end": "85479"
  },
  {
    "text": "reference um if you go to build. nvidia.com or ai. nvidia.com everyone can get a thousand inference",
    "start": "85479",
    "end": "92000"
  },
  {
    "text": "requests for free so I typically recommend this to folks who are benchmarking um different types of op",
    "start": "92000",
    "end": "97880"
  },
  {
    "text": "Source models we have all of those models hosted it's optimized if you're teaching a course um and you are trying",
    "start": "97880",
    "end": "103759"
  },
  {
    "text": "to evaluate all of the different LMS that are out there for your business there are also multimodal llms um speech",
    "start": "103759",
    "end": "110000"
  },
  {
    "text": "llms every model that Nidia accelerates will be available there for you and",
    "start": "110000",
    "end": "115799"
  },
  {
    "text": "that's sort of a path to you um to either go optimize them yourselves or to work with us um you'll see things about",
    "start": "115799",
    "end": "121799"
  },
  {
    "text": "uh Nvidia inference microservice and all of those things that you can take to Enterprise so we have sometimes the uh",
    "start": "121799",
    "end": "129200"
  },
  {
    "text": "I'll call it the rocky road and then there's smooth roads whatever path you want to take we're here to support you",
    "start": "129200",
    "end": "135360"
  },
  {
    "text": "uh in terms of agenda very simple I I want you to understand the llm inference workload and then we'll move to how you",
    "start": "135360",
    "end": "142080"
  },
  {
    "text": "go about measuring a production imployment and some of the things you need to be watching it's a little more",
    "start": "142080",
    "end": "147120"
  },
  {
    "text": "than um let's say you know the total time to generate and really understanding what's happening on the",
    "start": "147120",
    "end": "152519"
  },
  {
    "text": "gpus as you sort of scale out even if you have a single GPU I think it's very important for you to just have the",
    "start": "152519",
    "end": "157920"
  },
  {
    "text": "intuition and then lastly I'll show you some software that you can use um some open source packages that you can use",
    "start": "157920",
    "end": "163440"
  },
  {
    "text": "and then point to uh some paid offerings okay we're going to get into the llm",
    "start": "163440",
    "end": "168519"
  },
  {
    "text": "inference workload itself so the first part is is really understanding what happens when you send a prompt onto the",
    "start": "168519",
    "end": "175400"
  },
  {
    "text": "GPU so I have this example here I'm saying okay write me a presentation so I sound smart I come to the a engineer",
    "start": "175400",
    "end": "180959"
  },
  {
    "text": "conference and you guys are maybe going to like to talk and essentially what I'm going to do is I'm going to put that on",
    "start": "180959",
    "end": "186080"
  },
  {
    "text": "the GPU so the moment that I send that prompt on the GPU it stays on the GPU so think about that and then from there I'm",
    "start": "186080",
    "end": "192560"
  },
  {
    "text": "going to generate one token at a time so I'm generating the tokens llm inference is hard and I put the time stamps T1",
    "start": "192560",
    "end": "200120"
  },
  {
    "text": "through T4 so in every single deployment no matter how fast anyone claims they they're doing things um it's typically",
    "start": "200120",
    "end": "207080"
  },
  {
    "text": "one token that's generated at a time that's very important to understand the next thing is that in order for an llm",
    "start": "207080",
    "end": "214000"
  },
  {
    "text": "to give you a coherent answer just like how you speak you have to remember every single thing that you said before and",
    "start": "214000",
    "end": "220439"
  },
  {
    "text": "that you'll understand the mechanism of um how that how llms are able to do that so that's why I'm putting llm inference",
    "start": "220439",
    "end": "227560"
  },
  {
    "text": "is in red and putting that back onto the GPU so every token that I generate gets locked onto the GPU and then you'll",
    "start": "227560",
    "end": "234239"
  },
  {
    "text": "actually see what that looks like um in terms of vectors how many of you have heard of KV cach before",
    "start": "234239",
    "end": "241239"
  },
  {
    "text": "okay some of you um typically I don't see maybe many leaders here about this thing called KV cach KV cach is this",
    "start": "241239",
    "end": "248239"
  },
  {
    "text": "thing that really drives to some degree the cost uh so whether or not you use some big box API or um you're using a",
    "start": "248239",
    "end": "256320"
  },
  {
    "text": "single GPU it's all the same sort of mechanisms the same algorithm that everyone is trying to solve so in terms",
    "start": "256320",
    "end": "262680"
  },
  {
    "text": "of steps here the the I I like to as I said sharpen your intuition so the first",
    "start": "262680",
    "end": "268440"
  },
  {
    "text": "thing if we move from the left my first job is to convert these text whatever text that you send we're going to focus",
    "start": "268440",
    "end": "274120"
  },
  {
    "text": "on llm inference into some words that the model understands so the model will have its own vocabulary and it's my job",
    "start": "274120",
    "end": "280479"
  },
  {
    "text": "to translate that I'll give you the technical uh terms coming up after that and the first thing that happens is I do",
    "start": "280479",
    "end": "286800"
  },
  {
    "text": "some initial prompt processing so I have to compute the attention mechanism on the entire prompt I repeat that I have",
    "start": "286800",
    "end": "293960"
  },
  {
    "text": "to compute the attention mechanism on the entire prompt per user so if I have a million people hit my service and a",
    "start": "293960",
    "end": "300759"
  },
  {
    "text": "million people send 10,000 tokens that's a million time 10,000 attention",
    "start": "300759",
    "end": "305880"
  },
  {
    "text": "mechanisms that I need to compute also while generating tokens for other people so that's it's good for you to",
    "start": "305880",
    "end": "311479"
  },
  {
    "text": "appreciate sort of that complexity that's happening and once I finish processing that prompt then I'm going to",
    "start": "311479",
    "end": "317400"
  },
  {
    "text": "start generating one token at a time and that typically happens very fast and then from there every token that gets",
    "start": "317400",
    "end": "323479"
  },
  {
    "text": "generated that's in the lm's vocabulary I need to now DET toonize that back into",
    "start": "323479",
    "end": "329160"
  },
  {
    "text": "your language so here's the technical terms that you'll see when you read the literature or you read uh super technical documents",
    "start": "329160",
    "end": "336759"
  },
  {
    "text": "first is tokenization each model will have its own tokenizer and um the thing",
    "start": "336759",
    "end": "342120"
  },
  {
    "text": "to think about when you think of tokenizers when they did pre-training they had they downloaded the internet",
    "start": "342120",
    "end": "348639"
  },
  {
    "text": "and some right and they cleaned it up etc etc so tokenizer and as you start",
    "start": "348639",
    "end": "353680"
  },
  {
    "text": "thinking of the complexity across languages coding languages uh regions etc etc they Tred to get what is the",
    "start": "353680",
    "end": "360960"
  },
  {
    "text": "minimal set of character groups that can represent this entire training data set",
    "start": "360960",
    "end": "366759"
  },
  {
    "text": "efficiently CU it's really all about efficiency so for instance the Llama tokenizer has 128,000 tokens all right",
    "start": "366759",
    "end": "373960"
  },
  {
    "text": "and I I'll talk a bit more about that so here's what it actually kind of looks like on a GPU so I tokenize the llm",
    "start": "373960",
    "end": "380120"
  },
  {
    "text": "understands it I go into this thing called prefill prefill is a stage where you compute the attention mechanism and",
    "start": "380120",
    "end": "385680"
  },
  {
    "text": "many people are doing advancements with attention mechanisms um I'll talk bit more about that so there tons of",
    "start": "385680",
    "end": "391560"
  },
  {
    "text": "different schemes people leverage all of the different types of memory hierarchies in gpus to really accelerate",
    "start": "391560",
    "end": "398720"
  },
  {
    "text": "this type of workload and then I start generating tokens one at a time the red and the green just signify hey I'm",
    "start": "398720",
    "end": "405199"
  },
  {
    "text": "storing those tokens on the GPU the green is the latest one that I sent out so hopefully that makes sort of",
    "start": "405199",
    "end": "410599"
  },
  {
    "text": "intuitive sense all right so the other thing I want you to visualize um I I",
    "start": "410599",
    "end": "416240"
  },
  {
    "text": "think it's nice to visualize what what is the actual data that sits on the GPU",
    "start": "416240",
    "end": "422280"
  },
  {
    "text": "so first a token is approximately four characters that's a nice way for you to think about it um so from here I have",
    "start": "422280",
    "end": "429919"
  },
  {
    "text": "two vectors so the first Vector is just showing token one through token v v is the total number of tokens that I have",
    "start": "429919",
    "end": "436120"
  },
  {
    "text": "in my tokenizer and the second Vector below is just I have some numeric index",
    "start": "436120",
    "end": "441720"
  },
  {
    "text": "I don't want to keep using the token to reference itself I just use use the number as a lookup so my job when a",
    "start": "441720",
    "end": "448840"
  },
  {
    "text": "prompt comes in is to convert that text into those token what I'm going to call token IDs okay so I I have make me sound",
    "start": "448840",
    "end": "457080"
  },
  {
    "text": "smart I make me sound smarter you see two vectors sets of tokens and the key thing I want you to walk away with from",
    "start": "457080",
    "end": "463080"
  },
  {
    "text": "from that distinction is that an l&m's token is not a human word sometimes it",
    "start": "463080",
    "end": "468680"
  },
  {
    "text": "is sometimes it's not it's typically some subpar of words you'll see we symbols when you look at tokenizers from",
    "start": "468680",
    "end": "474440"
  },
  {
    "text": "different models um but you want that first framing so now we have text we hit to a vector right so from there each one",
    "start": "474440",
    "end": "482440"
  },
  {
    "text": "of those llm tokens had a corresponding embedding Vector so embedding Vector is",
    "start": "482440",
    "end": "488000"
  },
  {
    "text": "everything we embed uh videos we embed images we embed text tokens think of it",
    "start": "488000",
    "end": "493680"
  },
  {
    "text": "as a representation that uh an llm can use to compare things and do math on so",
    "start": "493680",
    "end": "499440"
  },
  {
    "text": "that's why we always want to com excuse me convert into some Vector representation right because some Vector",
    "start": "499440",
    "end": "505120"
  },
  {
    "text": "representation is just some high dimensional coordinate space and we're just rearranging objects that's to some degree what",
    "start": "505120",
    "end": "511400"
  },
  {
    "text": "you're doing okay so from those token IDs I went to the actual um embedding",
    "start": "511400",
    "end": "518039"
  },
  {
    "text": "vectors themselves so if you look make me sound smart now becomes a matrix all right make me sound smarter becomes a",
    "start": "518039",
    "end": "524320"
  },
  {
    "text": "matrix with an extra column so in reality what you're doing every time you submit a prompt I don't care what llm",
    "start": "524320",
    "end": "531120"
  },
  {
    "text": "you submitted to who you submitted to this is what you're doing right you are converting your text now images as well",
    "start": "531120",
    "end": "538560"
  },
  {
    "text": "they get converted to some Ida tokens or something like that that'll be another interesting talk to do diffusion models",
    "start": "538560",
    "end": "544720"
  },
  {
    "text": "etc etc but you're really putting this large Matrix on the GPU so why that the",
    "start": "544720",
    "end": "551160"
  },
  {
    "text": "next question you should ask is okay um why are gpus is good for this workload because they process matrices really",
    "start": "551160",
    "end": "556440"
  },
  {
    "text": "really fast so that's sort of the the advantage and the thing hopefully that that makes a little more sense to you",
    "start": "556440",
    "end": "562560"
  },
  {
    "text": "now the next thing I want to talk about is how the llm is going to process these tokens and I'll keep in mind if any well",
    "start": "562560",
    "end": "569839"
  },
  {
    "text": "I'm not even going to ask you to raise your hand I'm 100% sure each of you is used it an LM if you have not I'm not",
    "start": "569839",
    "end": "575120"
  },
  {
    "text": "sure what's happening uh uh the other one is the attention mechanism I I truly think um it's one of",
    "start": "575120",
    "end": "583440"
  },
  {
    "text": "the things that you should understand if we ever drift away from it that's fine but the fundamentals of that mechanism",
    "start": "583440",
    "end": "590240"
  },
  {
    "text": "and and seeing sort of the Innovations around that I think can help anyone any business leader etc etc just because you",
    "start": "590240",
    "end": "596720"
  },
  {
    "text": "are able to speak a different kind of language um in this generative future so as you think of the attention mechanism",
    "start": "596720",
    "end": "603000"
  },
  {
    "text": "the intuition that you should have is just that mechanism of relating tokens how do I distinguish in a sentence what",
    "start": "603000",
    "end": "609720"
  },
  {
    "text": "is important all right and then for the next token that's going to be generated hey what tokens that I said before were",
    "start": "609720",
    "end": "616440"
  },
  {
    "text": "really important for me to make a next good decision for that next token so that's the intuition and now we're going",
    "start": "616440",
    "end": "622560"
  },
  {
    "text": "to won't necessar touch too much of the math but I want you to see sort of what's happening on the GPU so once",
    "start": "622560",
    "end": "628279"
  },
  {
    "text": "again The Prompt comes in I'm just going to do a short one make me sound smart I'm going to generate this token called",
    "start": "628279",
    "end": "633800"
  },
  {
    "text": "llm all right uh we saw the same matrices that I said before so remember my text now turns into a matrix hitting",
    "start": "633800",
    "end": "640800"
  },
  {
    "text": "onto the GPU and and the main thing I want you to understand or visualize here is actually",
    "start": "640800",
    "end": "647560"
  },
  {
    "text": "how an llm memory works so now when you're speaking you've recorded",
    "start": "647560",
    "end": "653279"
  },
  {
    "text": "everything that I've said for the last uh 10 minutes in your brain somewhere it's it's stored so now you're going to",
    "start": "653279",
    "end": "658680"
  },
  {
    "text": "see how the L is storing what it is that you just said so from there um a lot of",
    "start": "658680",
    "end": "663920"
  },
  {
    "text": "folks will hear about these query key and value matrices this is what the actual model weights look like so when",
    "start": "663920",
    "end": "669440"
  },
  {
    "text": "you look at a model weights file if you go on hugging face there's typically a Json file that will show you all of the",
    "start": "669440",
    "end": "674959"
  },
  {
    "text": "different pieces of model files and you'll see this thing called qk andv so I have these model weights so now I've",
    "start": "674959",
    "end": "681800"
  },
  {
    "text": "went from text to a matrix I'm going to Matrix multiply against the weights of the models so now I get these three",
    "start": "681800",
    "end": "688560"
  },
  {
    "text": "output models so think of these weight matrices that I showed here think as um when you're",
    "start": "688560",
    "end": "694320"
  },
  {
    "text": "doing a projection what you're doing is you're taking um some coordinates and you're putting it into a different space",
    "start": "694320",
    "end": "701079"
  },
  {
    "text": "that's really what you're doing when you do Vector Matrix math so now when I do this Matrix multipication this query key",
    "start": "701079",
    "end": "707079"
  },
  {
    "text": "and value Matrix so if you look at different tutorials on attention you'll see these things pop up a lot so",
    "start": "707079",
    "end": "712639"
  },
  {
    "text": "hopefully that'll help you to read it a lot more this is now the lm's interpretation of each of those tokens",
    "start": "712639",
    "end": "719360"
  },
  {
    "text": "that you sent in right and now the job is how do I now take uh these query key",
    "start": "719360",
    "end": "724639"
  },
  {
    "text": "and value matrices and sort of interpret it to try to generate the next best token and this is just happening",
    "start": "724639",
    "end": "731200"
  },
  {
    "text": "constantly over and over every single token that's happening but the key thing I want you to walk away on the slide is",
    "start": "731200",
    "end": "736600"
  },
  {
    "text": "where I drew the key and the value right when people talk about KV cach optimization every llm performance",
    "start": "736600",
    "end": "743760"
  },
  {
    "text": "engineer is just literally trying to make that thing as fast and small as possible and I'll I'll that it'll make a",
    "start": "743760",
    "end": "749920"
  },
  {
    "text": "little more sense as to what that does to your cost But ultimately these key and value matrices this is like your",
    "start": "749920",
    "end": "755240"
  },
  {
    "text": "llms memory so it'll make a little more sense coming up I know I didn't show a ton of the math I show some tutorials",
    "start": "755240",
    "end": "760680"
  },
  {
    "text": "afterwards so you can go read more about that um my intention here is for you to visualize key and value so every time",
    "start": "760680",
    "end": "766600"
  },
  {
    "text": "you see a prompt I just want you thinking crap key and value is on my on my GPU okay the next so here's the real",
    "start": "766600",
    "end": "774560"
  },
  {
    "text": "value of the KV cache so remember we said said that whenever I generate a",
    "start": "774560",
    "end": "780639"
  },
  {
    "text": "token I'm going to push it back into the GPU right so every token I generate it goes back into the GPU and then I have",
    "start": "780639",
    "end": "786839"
  },
  {
    "text": "to compute an attention mechanism so this is what's happening this new token I generated llm I get its Vector",
    "start": "786839",
    "end": "793120"
  },
  {
    "text": "representation as you see in blue but now I I do that Vector Matrix math now",
    "start": "793120",
    "end": "799320"
  },
  {
    "text": "so before I did Matrix Matrix math that's my first prompt first comes in I",
    "start": "799320",
    "end": "804839"
  },
  {
    "text": "generated my first token now I'm doing Vector Matrix math you know people will batch this across all requests but I'm",
    "start": "804839",
    "end": "811480"
  },
  {
    "text": "just showing you a single request so you can see it now the value of the KV cache",
    "start": "811480",
    "end": "817279"
  },
  {
    "text": "is if I were to if I didn't have the KV cache I would have to reprocess all of",
    "start": "817279",
    "end": "823360"
  },
  {
    "text": "that work I did on the prompt that I did before so this is the benefit of your KV caches to now I'm just going to compute",
    "start": "823360",
    "end": "830079"
  },
  {
    "text": "attention on this newest token how does this new token relate to everything that I said before that's the thing that's",
    "start": "830079",
    "end": "836440"
  },
  {
    "text": "really happening intuitively so so if I have this KV cache my Generations can be",
    "start": "836440",
    "end": "841720"
  },
  {
    "text": "fast okay and it's really up to the the What's called the batch manager on the",
    "start": "841720",
    "end": "846920"
  },
  {
    "text": "GPU to make sure that I'm just pushing out as many tokens as possible okay so",
    "start": "846920",
    "end": "852120"
  },
  {
    "text": "if you look at uh an llm these groups of three matrices are calling attention",
    "start": "852120",
    "end": "857440"
  },
  {
    "text": "head there more matrices than not but these are the main ones um llama has 32 attention heads so I just kind of want",
    "start": "857440",
    "end": "863199"
  },
  {
    "text": "you to appreciate what an llm really looks like right so I have 32 sets of these matrices I'll have 32 of those KV",
    "start": "863199",
    "end": "870279"
  },
  {
    "text": "caches happening at the same time and now I have to combine all of that to then generate the next tokens so there's",
    "start": "870279",
    "end": "875920"
  },
  {
    "text": "an incredible amount of work that happens in a very short space of time to give you a coherent token okay a good",
    "start": "875920",
    "end": "884160"
  },
  {
    "text": "mental model for you to keep in your head I'm going to speed up a little bit is to um if you see the number of parameters multiply that by two and that",
    "start": "884160",
    "end": "891600"
  },
  {
    "text": "is your fp16 gigabyte memory on the GPU so if you have let's say an L4 I think",
    "start": "891600",
    "end": "897720"
  },
  {
    "text": "is 20 gigs um and I have a llama 8B that's automatically 16 gigs fp16 so I",
    "start": "897720",
    "end": "904000"
  },
  {
    "text": "only have 4 gigs left for my KB cache so on the GPU it's either the model weights",
    "start": "904000",
    "end": "909519"
  },
  {
    "text": "or tokens that's it there's nothing else on the GPU and I I have a thing to read on that this is a really good blog it",
    "start": "909519",
    "end": "916079"
  },
  {
    "text": "shows you all of the different um optimizations that you can do and okay",
    "start": "916079",
    "end": "921120"
  },
  {
    "text": "now let's talk about measuring so if you ever see this thing called ISL or I have",
    "start": "921120",
    "end": "926959"
  },
  {
    "text": "ITA oh sorry ISL or SL that's input sequence link output sequence link so",
    "start": "926959",
    "end": "932279"
  },
  {
    "text": "now I want you to see what some Advanced monitoring might look like if any of you are devops folks these are things that",
    "start": "932279",
    "end": "937880"
  },
  {
    "text": "you want to record the first thing that we measure is time to First token so how long does it take me to generate um the",
    "start": "937880",
    "end": "944959"
  },
  {
    "text": "process the prompt and then generate my first token and that's typically a measurement of how good your attention",
    "start": "944959",
    "end": "950720"
  },
  {
    "text": "mechanism processing is that's really what you're trying to sus out so that's time to First token into token latencies",
    "start": "950720",
    "end": "957839"
  },
  {
    "text": "so after I've generated my first token every single token after that I'm looking at those individual spaces so",
    "start": "957839",
    "end": "964000"
  },
  {
    "text": "everything that's going to happen there U think about when the system is on the load I have you know a thousand requests",
    "start": "964000",
    "end": "969639"
  },
  {
    "text": "coming into my system I'm generating a thousand sets of different tokens and the more memory I occupy typically that",
    "start": "969639",
    "end": "975480"
  },
  {
    "text": "slows down processing so if you start to see drift in this metric then so I'll show you some plots that you can look at",
    "start": "975480",
    "end": "981319"
  },
  {
    "text": "and then time to Total generation how long did it take me to initially get the prompt fully finish the answer right",
    "start": "981319",
    "end": "987839"
  },
  {
    "text": "super intuitive like I I said ISL OSL that's all that means when you see them on the plots coming up okay uh this is a",
    "start": "987839",
    "end": "996040"
  },
  {
    "text": "very important Paradigm for you to understand in your mind uh so I I worked with a lot of folks on you know maybe uh",
    "start": "996040",
    "end": "1002480"
  },
  {
    "text": "rexus deployments or deployments of other types of models so on the GPU if you're only deploying one model on a GPU",
    "start": "1002480",
    "end": "1008600"
  },
  {
    "text": "outside of llm in France uh in my opinion I think you're wasting the GPU you can put multiple models on the GPU",
    "start": "1008600",
    "end": "1014639"
  },
  {
    "text": "to actually increase your throughput that's why it was really created um so this is a slide is excuse me this figure",
    "start": "1014639",
    "end": "1021240"
  },
  {
    "text": "is just showing I can have multiple models I have some space for data and that's how I increase my throughput per",
    "start": "1021240",
    "end": "1026520"
  },
  {
    "text": "unit Hardware however on the llm INF front side it's very different I have one model you know folks can fit",
    "start": "1026520",
    "end": "1033160"
  },
  {
    "text": "multiple models on a GPU that's cool but that's not a real production use case you'll typically have a single model the",
    "start": "1033160",
    "end": "1038880"
  },
  {
    "text": "remaining space that you have is all for KV cash and generating all those tokens so I just put four different requests",
    "start": "1038880",
    "end": "1044720"
  },
  {
    "text": "and I I just kind of want you to see the boxes that are happening okay uh I would say this is the most important slide in",
    "start": "1044720",
    "end": "1050480"
  },
  {
    "text": "the entire presentation because this is the thing that will determine both your cost and performance so there are four",
    "start": "1050480",
    "end": "1056919"
  },
  {
    "text": "different querying patterns that happen and this is something that you must measure in your deployment because often",
    "start": "1056919",
    "end": "1063640"
  },
  {
    "text": "times you might read benchmarks and they'll just say all right they'll cherry pick one or two of these but in",
    "start": "1063640",
    "end": "1068960"
  },
  {
    "text": "reality in your production system you might have several of these different patterns that are occurring so let's",
    "start": "1068960",
    "end": "1074200"
  },
  {
    "text": "take a look at the first one long input short output so a long input means",
    "start": "1074200",
    "end": "1079360"
  },
  {
    "text": "it's going to take me technically longer to compute the attention mechanism so my pre-fill stage will be longer I occupies",
    "start": "1079360",
    "end": "1085880"
  },
  {
    "text": "more memory from my prompt does that make sense intuitively hopefully it it's grabbing you but then on the generation",
    "start": "1085880",
    "end": "1092280"
  },
  {
    "text": "side I don't generate much tokens so there's not much those tokens are not picking up a lot of memory and they will",
    "start": "1092280",
    "end": "1098120"
  },
  {
    "text": "tend to finish Fast so the second the second one or the maybe the most costly use cases um so I have clients that will",
    "start": "1098120",
    "end": "1105600"
  },
  {
    "text": "message me and say hey my data scientists are putting two bigger prompts on my GPU so now they're killing my deployment cuz",
    "start": "1105600",
    "end": "1111320"
  },
  {
    "text": "if everyone went and put the maximum context length I can only fit so many requests on the GPU so that's something",
    "start": "1111320",
    "end": "1117400"
  },
  {
    "text": "for you to think about you'll have to manage that internally with your deployments so that's why I'm putting",
    "start": "1117400",
    "end": "1122679"
  },
  {
    "text": "you know okay the GP is really full because a long text uh excuse me long input uh long output the next one short",
    "start": "1122679",
    "end": "1128840"
  },
  {
    "text": "long you know your time to first to them be really fast I don't have much to compute the attention mechanism on but",
    "start": "1128840",
    "end": "1134679"
  },
  {
    "text": "hey I'm generating a ton of tokens that's really really fast so hopefully as you start measuring these types of",
    "start": "1134679",
    "end": "1140120"
  },
  {
    "text": "different query patterns um you'll see different results I just put you know what a random sampling set might",
    "start": "1140120",
    "end": "1146760"
  },
  {
    "text": "actually look like on the GPU because not everyone will send the same length of input and output um so that will",
    "start": "1146760",
    "end": "1153400"
  },
  {
    "text": "it'll be good for you to just sort of visualize and track these statistics more importantly why we're doing that uh",
    "start": "1153400",
    "end": "1159120"
  },
  {
    "text": "internally I'm I'm going to steal the time here Peter uh more importantly why we're doing that or why we're tracking",
    "start": "1159120",
    "end": "1165080"
  },
  {
    "text": "these things is that the whole goal is to build I have a big model my goal is to shrink it as much as I can but to",
    "start": "1165080",
    "end": "1172159"
  },
  {
    "text": "keep it as accurate as possible so the more that I shrink the faster it runs the more GPU memory I have for what",
    "start": "1172159",
    "end": "1179679"
  },
  {
    "text": "tokens all right so that's how you really try to improve your cost this is why I'm I'm sort of proposing to you to",
    "start": "1179679",
    "end": "1186159"
  },
  {
    "text": "build inference engines so what all I'm showing here is a 2d histogram of input sequence length versus output sequence",
    "start": "1186159",
    "end": "1192360"
  },
  {
    "text": "length cuz the question that you'll have to answer is hey how long are my actual prompts someone might say okay here's",
    "start": "1192360",
    "end": "1198919"
  },
  {
    "text": "the max prompt length that you can ingest and the max prompt you can out you know excuse me get on the output and",
    "start": "1198919",
    "end": "1205880"
  },
  {
    "text": "all of The Big Box model providers have to estimate this when they go into costing or providing a service to you",
    "start": "1205880",
    "end": "1212080"
  },
  {
    "text": "right because they have to host all of that Machinery under the hood now that you understand what's happening so we",
    "start": "1212080",
    "end": "1218000"
  },
  {
    "text": "use this to statistically determine what is the max input sequence length and the",
    "start": "1218000",
    "end": "1223720"
  },
  {
    "text": "max output sequence length across all of my users and this would give you a really good indication of um how you can",
    "start": "1223720",
    "end": "1229919"
  },
  {
    "text": "size your engines we use that to actually build more optimized engines in addition um it'll just give you good",
    "start": "1229919",
    "end": "1236520"
  },
  {
    "text": "viewers to maybe uh what you call it scaling out and things like that the next one is time to First token analysis",
    "start": "1236520",
    "end": "1243280"
  },
  {
    "text": "remember time to First token is measuring my performance of the attention mechanism under load so",
    "start": "1243280",
    "end": "1249400"
  },
  {
    "text": "someone might show attention mechanism at one query woohoo show me attention mechanism underload when this thing is",
    "start": "1249400",
    "end": "1255799"
  },
  {
    "text": "fully maxed out 24/7 that's when you really need to start um measuring these types of things so this is something you",
    "start": "1255799",
    "end": "1261600"
  },
  {
    "text": "can look at these are sort of experimental plots um there's a package called gen perf that will be released",
    "start": "1261600",
    "end": "1267159"
  },
  {
    "text": "open source it's out already I have a link to it there this is where you can it'll generate these plots for you but I",
    "start": "1267159",
    "end": "1273400"
  },
  {
    "text": "I'm just showing you what the engineers uh looking at internally to measure the performance of the compute platform next",
    "start": "1273400",
    "end": "1279600"
  },
  {
    "text": "time to completion analysis how long did it take me to go from start to finish across every single request naturally",
    "start": "1279600",
    "end": "1286159"
  },
  {
    "text": "The Wider that box plot you have have to intuitively ask what's happening why did",
    "start": "1286159",
    "end": "1291600"
  },
  {
    "text": "this person's prompt take longer than another so you can investigate either batching issues scheduling issues",
    "start": "1291600",
    "end": "1297919"
  },
  {
    "text": "different things like that I I'll take questions in here why I have to move really fast sorry there okay I'm going to speed up here I to to token latency",
    "start": "1297919",
    "end": "1305600"
  },
  {
    "text": "Peter how much time I got oh you're fine definely have time for questions okay cool I'm going to Ste I'm definitely so",
    "start": "1305600",
    "end": "1311720"
  },
  {
    "text": "sorry I just want to I realize I may have gone a little too fast so forgive me for that got five minutes cool all",
    "start": "1311720",
    "end": "1317520"
  },
  {
    "text": "right time uh token to token latency so that is I'm generating token so I'm looking at that spacing versus token",
    "start": "1317520",
    "end": "1324440"
  },
  {
    "text": "position so the longer a sequence gets remember my memory grows so typically",
    "start": "1324440",
    "end": "1330400"
  },
  {
    "text": "that means that system is under more load it has more throttling that might happen under high load of requests so if",
    "start": "1330400",
    "end": "1337480"
  },
  {
    "text": "I see a large variation in token to token latency as the sequence gets",
    "start": "1337480",
    "end": "1343000"
  },
  {
    "text": "longer when I'm generating that means I'm not very performant right so we look at that to see I try to make make sure",
    "start": "1343000",
    "end": "1348760"
  },
  {
    "text": "that that's constant no matter how much tokens I'm generating that means I'm I'm really proficient okay uh last one would",
    "start": "1348760",
    "end": "1356240"
  },
  {
    "text": "be time to First token versus number of input tokens so time to First token",
    "start": "1356240",
    "end": "1361520"
  },
  {
    "text": "remember is Computing the attention mechanism okay versus number of input tokens so if I have a bigger prompt my",
    "start": "1361520",
    "end": "1368400"
  },
  {
    "text": "attentions will take longer but I if that plot goes up like from your perspective it goes up like this in",
    "start": "1368400",
    "end": "1374440"
  },
  {
    "text": "terms of sequence length that's not really good performance we really look at that slope and we try to get that",
    "start": "1374440",
    "end": "1379960"
  },
  {
    "text": "slope almost you know as as low as possible so if you send me this long sequence I can get that thing done",
    "start": "1379960",
    "end": "1386159"
  },
  {
    "text": "really fast okay okay uh in terms of software uh you'll see this thing called trt llm uh Triton is an open source",
    "start": "1386159",
    "end": "1393679"
  },
  {
    "text": "inference server so you can deploy models on CPU on GPU computer vision",
    "start": "1393679",
    "end": "1398760"
  },
  {
    "text": "rexus python pytorch tensorflow it's uh it'll host all of the different types of models so there's one way that your",
    "start": "1398760",
    "end": "1405360"
  },
  {
    "text": "deployment team deploys all your data scientists are happy because they don't have to do conversion you're happy as a",
    "start": "1405360",
    "end": "1410600"
  },
  {
    "text": "deployment person because you don't have to manage a torch serve versus TF serve and uh flask and all of it is done",
    "start": "1410600",
    "end": "1416760"
  },
  {
    "text": "through one it's written in C++ blazingly fast and then the other thing you'll see Nvidia you'll see a lot more",
    "start": "1416760",
    "end": "1422360"
  },
  {
    "text": "coming out of Nvidia is NVIDIA inference micros service cuz building these engines getting them deployed optimiz",
    "start": "1422360",
    "end": "1427720"
  },
  {
    "text": "the scale it's not easy so we've sort of made that easy for you as an Enterprise offering but you guys can try it out uh",
    "start": "1427720",
    "end": "1433240"
  },
  {
    "text": "for free okay so trtm let me just give you high L lots of stuff on the slide",
    "start": "1433240",
    "end": "1438600"
  },
  {
    "text": "but the main thing I want you to walk away with um uh is this is the model compilation package for llms on Nvidia",
    "start": "1438600",
    "end": "1445279"
  },
  {
    "text": "gpus this if you want to get best performance from Nvidia gpus please make sure you use trt llm um and naturally uh",
    "start": "1445279",
    "end": "1453120"
  },
  {
    "text": "once we're investing more in name you'll see some more things come out so you'll see performances on a100 and h100 really",
    "start": "1453120",
    "end": "1459120"
  },
  {
    "text": "focus on fp8 gpus so fp8 will be Hopper and AD love lace okay so fp8 I'll I'll",
    "start": "1459120",
    "end": "1466080"
  },
  {
    "text": "talk a bit more about that what the advantage there is but mainly is if I go from fp16 fp8 is this half my memory",
    "start": "1466080",
    "end": "1475279"
  },
  {
    "text": "almost the same accuracy and so we measure the accuracy and we publish the accuracy so now I have this much more space for tokens but more importantly",
    "start": "1475279",
    "end": "1482559"
  },
  {
    "text": "this model is that much faster okay so I want you to understand where the sort of industry is going this is why Hopper the",
    "start": "1482559",
    "end": "1488799"
  },
  {
    "text": "will ate hopper for breakfast and lunch and dinner because of fp8 it gave folks that cost um benefit to do this thing a",
    "start": "1488799",
    "end": "1497279"
  },
  {
    "text": "lot faster okay um in Flight batching it just means I don't have to wait for all the requests to finish to start a new",
    "start": "1497279",
    "end": "1503760"
  },
  {
    "text": "request the moment your request finishes I can inject a new request While others are going okay tons of features here I",
    "start": "1503760",
    "end": "1511120"
  },
  {
    "text": "put the features um so some wants to to focus on are quantize KV cach so I can",
    "start": "1511120",
    "end": "1517399"
  },
  {
    "text": "actually represent my KV cach in different uh excuse me Precision so that means I'm actively shrinking that memory",
    "start": "1517399",
    "end": "1523600"
  },
  {
    "text": "making it more performant um you have phkv cach that's just you managing uh your gpus is a lot better in terms of",
    "start": "1523600",
    "end": "1529760"
  },
  {
    "text": "all of that memory so there are tons of things you can do tenser parallelism the thing to remember about tenser",
    "start": "1529760",
    "end": "1535120"
  },
  {
    "text": "parallelism if you want to increase latency you use tza parallelism split the model up across multiple gpus that's",
    "start": "1535120",
    "end": "1541640"
  },
  {
    "text": "typically done within a node I repeat that that's typically done within a node",
    "start": "1541640",
    "end": "1547240"
  },
  {
    "text": "you don't like to do tza parallelism across a node you'll see pipeline parallelism go across a pipeline",
    "start": "1547240",
    "end": "1553320"
  },
  {
    "text": "parallelism is more sequential so I process this chunk so in a multi- node",
    "start": "1553320",
    "end": "1558440"
  },
  {
    "text": "model like huge models this box will finish and pass off to the next box but most folks will typically just work most",
    "start": "1558440",
    "end": "1565159"
  },
  {
    "text": "models will work within a single node like um nvidia's 340b model that they",
    "start": "1565159",
    "end": "1570679"
  },
  {
    "text": "just released it was designed to do inference on a single h100 node but that's an fb8 okay so those are some of",
    "start": "1570679",
    "end": "1577760"
  },
  {
    "text": "the things in terms of models that you have access to um we optimize those models and we give you a lot of scripts",
    "start": "1577760",
    "end": "1584039"
  },
  {
    "text": "for you can go do that on your own or you can sort of take our software um and take a easy path you either way we",
    "start": "1584039",
    "end": "1589760"
  },
  {
    "text": "support you so here are some of the models that are there all of the llamas mistol mixs we work with all those teams",
    "start": "1589760",
    "end": "1595320"
  },
  {
    "text": "behind the scenes so typically before any foundation model comes out we we work with those teams to to get them",
    "start": "1595320",
    "end": "1601799"
  },
  {
    "text": "deployed okay what does it mean for tenz RT so you might have seen tenz RT before",
    "start": "1601799",
    "end": "1607080"
  },
  {
    "text": "which was a deep learning compilation package for NVIDIA gpus lots of folks in computer vision etc etc I've used that",
    "start": "1607080",
    "end": "1613880"
  },
  {
    "text": "we took the best practices from there and added all of the extra things that need to happen in the llm inference Loop",
    "start": "1613880",
    "end": "1620559"
  },
  {
    "text": "so that's what trt llm is really about um so mainly focus on llm inference uh",
    "start": "1620559",
    "end": "1626880"
  },
  {
    "text": "here's a good visual an engine that's built to a specific GPU cannot be moved to another GPU so you always have to",
    "start": "1626880",
    "end": "1633480"
  },
  {
    "text": "compile to that GPU that's why it's that performant because we really leverage um all of the the actual Hardware on that",
    "start": "1633480",
    "end": "1641159"
  },
  {
    "text": "system to rewrite the algorithm rewrite that model to that specific piece of Hardware okay um trt LM and Triton so",
    "start": "1641159",
    "end": "1649000"
  },
  {
    "text": "trtm will give me an inference engine I need something to host that inference engine and accept requests batching etc",
    "start": "1649000",
    "end": "1655480"
  },
  {
    "text": "etc so we have Triton Triton works very simply it's literally a folder where you",
    "start": "1655480",
    "end": "1660720"
  },
  {
    "text": "specify it it works on tenser in and tens are out so it will tell you what are my inputs coming in and out and then",
    "start": "1660720",
    "end": "1667440"
  },
  {
    "text": "it'll basically understand how to interpret that file or you can host any other different models that's one that's",
    "start": "1667440",
    "end": "1672919"
  },
  {
    "text": "a thing I do a lot with folks just two more slides this is where the future of inference is going so a lot of folks do",
    "start": "1672919",
    "end": "1679159"
  },
  {
    "text": "fp16 in France today um a lot of folks are moving towards fp8 just because hey",
    "start": "1679159",
    "end": "1685919"
  },
  {
    "text": "I know half the model size almost twice the speed more space for tokens it just",
    "start": "1685919",
    "end": "1691360"
  },
  {
    "text": "makes more sense from a cost perspective that's why folks like that and then uh you saw Blackwell was announced that's",
    "start": "1691360",
    "end": "1697120"
  },
  {
    "text": "the major Innovation I get fp4 so that's where things are really going to get interesting I'll end with um Nvidia",
    "start": "1697120",
    "end": "1704000"
  },
  {
    "text": "inference microservice so we've made this thing really easy we've gone and actually found the best configurations",
    "start": "1704000",
    "end": "1709399"
  },
  {
    "text": "for all of these models on each piece of GPU and we're slowly rolling out all of the models cuz it you know it'll just",
    "start": "1709399",
    "end": "1715399"
  },
  {
    "text": "take some time to optimize the world essentially and yeah you can use this to download all the slides I put papers um",
    "start": "1715399",
    "end": "1722799"
  },
  {
    "text": "tons of other things you for you to read so yeah hopefully your intuition has",
    "start": "1722799",
    "end": "1729398"
  },
  {
    "text": "sharpened shall we just conclude with the because there was someone had a question sure yeah I think where was the",
    "start": "1732440",
    "end": "1737679"
  },
  {
    "text": "question yeah oh hang on I'm going to come over and point my mic at",
    "start": "1737679",
    "end": "1743559"
  },
  {
    "text": "you thank you ome hi um sorry my question is actually on the heat map",
    "start": "1745200",
    "end": "1751399"
  },
  {
    "text": "that you shared you mind um walking through the heat map and how to interpret it it was a little small yeah",
    "start": "1751399",
    "end": "1757399"
  },
  {
    "text": "sorry about that yeah so the heat map all I'm looking at is um so when you go to build an engine you build an engine",
    "start": "1757399",
    "end": "1763960"
  },
  {
    "text": "to the max input sequence length and the max output sequence length so we actually change how that Matrix math is",
    "start": "1763960",
    "end": "1770519"
  },
  {
    "text": "happening under the hood based on those settings so you might say all right uh my users are only going to send 4,000",
    "start": "1770519",
    "end": "1777440"
  },
  {
    "text": "tokens but in reality they might have been sending 1,300 over the past week that you measured so now you can stay",
    "start": "1777440",
    "end": "1783519"
  },
  {
    "text": "with statistical certainty that hey the majority of people that were serving um",
    "start": "1783519",
    "end": "1789799"
  },
  {
    "text": "during this time these were their querying pattern so I can rebuild an engine for that period of time what gets",
    "start": "1789799",
    "end": "1795760"
  },
  {
    "text": "super interesting this is a topic I'm very interested in is seasonal engines so during the day you have different",
    "start": "1795760",
    "end": "1801320"
  },
  {
    "text": "querying patterns so you'll scale down you'll scale up and so you might have different engines built for different",
    "start": "1801320",
    "end": "1807320"
  },
  {
    "text": "types of querying patterns based on traffic and stuff like that so hopefully that that may have answered your",
    "start": "1807320",
    "end": "1813240"
  },
  {
    "text": "question yeah but it's just saying you know looking at the bounds of what's the minimum number of tokens that came in",
    "start": "1813240",
    "end": "1819000"
  },
  {
    "text": "the Max uh Min Min out and max out and just looking at that over the entire",
    "start": "1819000",
    "end": "1824559"
  },
  {
    "text": "distribution yes it oh yeah right when it comes to those uh",
    "start": "1824559",
    "end": "1831240"
  },
  {
    "text": "infant strategies you talk about like Lio and lyso um how do you what kind of strategies do you have to manage like",
    "start": "1831240",
    "end": "1836799"
  },
  {
    "text": "which ones are used at like because obviously each session is going to be pretty generic you don't know which one",
    "start": "1836799",
    "end": "1842240"
  },
  {
    "text": "to use at first correct um do you split those between gpus or do you stick with one and do it switch between so",
    "start": "1842240",
    "end": "1847960"
  },
  {
    "text": "typically we'll we'll go to you try to find what's one configuration that will manage the the plora of types of",
    "start": "1847960",
    "end": "1855080"
  },
  {
    "text": "requests that you have coming in so we we're typically at a a one engine per",
    "start": "1855080",
    "end": "1860120"
  },
  {
    "text": "all the different querying types and I think you'll start seeing I'm giving you a little bit of future ways to think",
    "start": "1860120",
    "end": "1865200"
  },
  {
    "text": "about it on the devop side because that's something you'll have to test right if I look at this quering pattern",
    "start": "1865200",
    "end": "1871399"
  },
  {
    "text": "that came into my system with this engine if I switch the engine does it still satisfy the querying pattern and",
    "start": "1871399",
    "end": "1877320"
  },
  {
    "text": "how much cost does it save how much faster is it so that's more of a an engineering exercise that you'll have to",
    "start": "1877320",
    "end": "1882720"
  },
  {
    "text": "deploy sorry I I didn't have a yeah yeah so I I just I'm very interested in the seasonal side just",
    "start": "1882720",
    "end": "1888840"
  },
  {
    "text": "because okay querying patterns will change um especially when agents come it'll just be that's going to get super",
    "start": "1888840",
    "end": "1895159"
  },
  {
    "text": "interesting when agents are just throwing stuff yes",
    "start": "1895159",
    "end": "1900398"
  },
  {
    "text": "yeah so so what people do in order to scale the attention mechanism is here's",
    "start": "1917960",
    "end": "1923000"
  },
  {
    "text": "another interesting fact that uh why folks don't train huge context models because it's actually now you've seen",
    "start": "1923000",
    "end": "1929600"
  },
  {
    "text": "the bigger my uh prompt the more memory I need so imagine what that does to a huge I don't know 10,000 100,000 GPU",
    "start": "1929600",
    "end": "1938240"
  },
  {
    "text": "deployment it might make it a million gpus just to do that context thing so people will train to a small context",
    "start": "1938240",
    "end": "1943960"
  },
  {
    "text": "length and then interpolate in that value to to give you that length of context length and then you're sort of",
    "start": "1943960",
    "end": "1950120"
  },
  {
    "text": "bound to what attention mechanism you were using designed there there's things like flash attention that will just do",
    "start": "1950120",
    "end": "1956320"
  },
  {
    "text": "everything in the L1 cache really really fast so it depends on the speed of some of the different it also depends on the",
    "start": "1956320",
    "end": "1962679"
  },
  {
    "text": "GPU as well so that's why um if you look at Blackwell that was announced by by Jensen they literally connected I think",
    "start": "1962679",
    "end": "1969960"
  },
  {
    "text": "72 different gpus on one EnV link so NV link connects gpus together that's how",
    "start": "1969960",
    "end": "1975600"
  },
  {
    "text": "we can move data insanely fast so now we' connected like 72 gpus on one that's",
    "start": "1975600",
    "end": "1981000"
  },
  {
    "text": "that's just to show you um like mixture of experts trying to computer attenion across all of these different things but",
    "start": "1981000",
    "end": "1986720"
  },
  {
    "text": "that's a actually a really good question no I I don't necessarily think",
    "start": "1986720",
    "end": "1992960"
  },
  {
    "text": "so like the entire industry is you know going after that problem that's why everybody wants to maybe see something",
    "start": "1992960",
    "end": "1998639"
  },
  {
    "text": "other than attention and ah you know there's so much excitement there yeah unfortunately have to call time on but",
    "start": "1998639",
    "end": "2004200"
  },
  {
    "text": "that's been fantastic thank you Mo um",
    "start": "2004200",
    "end": "2009580"
  },
  {
    "text": "[Applause] [Music]",
    "start": "2009580",
    "end": "2018309"
  }
]