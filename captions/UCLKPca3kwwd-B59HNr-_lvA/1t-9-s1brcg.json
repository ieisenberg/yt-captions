[
  {
    "text": "[Music]",
    "start": "520",
    "end": "14230"
  },
  {
    "text": "thank you so much for coming to the workshop my name is Gabriela dearo and",
    "start": "14599",
    "end": "20519"
  },
  {
    "text": "I'm director of AI at Microsoft I have Pamela here I'm Pamela and I'm a python Cloud",
    "start": "20519",
    "end": "27760"
  },
  {
    "text": "advocate so well done on those you who said python um but I Al I worked in",
    "start": "27760",
    "end": "33120"
  },
  {
    "text": "JavaScript for then for quite a long time and I generally like lots of",
    "start": "33120",
    "end": "38680"
  },
  {
    "text": "languages hi I'm Harold I'm a PM on vs code and GitHub co-pilot chat",
    "start": "39800",
    "end": "47760"
  },
  {
    "text": "so awesome um so today we are going to be talking or showing you how to run a",
    "start": "47760",
    "end": "55719"
  },
  {
    "text": "AI application in minutes so we are going to have a lot of like Hands-On so be ready to do like",
    "start": "55719",
    "end": "62960"
  },
  {
    "text": "some coding not coding but like going through some coding uh using different",
    "start": "62960",
    "end": "69320"
  },
  {
    "text": "tools uh GitHub code spaces Azure and and other tools that we are going to be",
    "start": "69320",
    "end": "74960"
  },
  {
    "text": "talking about but now just to give a a overview of like the agenda I'm going to",
    "start": "74960",
    "end": "80439"
  },
  {
    "text": "be talking about Microsoft for startups a little bit some of the Partnerships some of the pain points and then we'll",
    "start": "80439",
    "end": "87360"
  },
  {
    "text": "go through the AI templates and handson uh so Microsoft has a program for",
    "start": "87360",
    "end": "94159"
  },
  {
    "text": "startups so if you have an idea if you have a startup uh you can apply to this program",
    "start": "94159",
    "end": "102360"
  },
  {
    "text": "and what I always tell people is you don't have to have a startup per se but if you have an idea that's enough to",
    "start": "102360",
    "end": "109200"
  },
  {
    "text": "apply for this program and you get a lot of benefits and benefits that can be um",
    "start": "109200",
    "end": "115240"
  },
  {
    "text": "I'll just skip it can be like credits so you get up to $150,000 in Azure credits you also have third",
    "start": "115240",
    "end": "123200"
  },
  {
    "text": "party benefits like a lot of like different tools that you can use and then of course GitHub Microsoft 365",
    "start": "123200",
    "end": "129479"
  },
  {
    "text": "LinkedIn preman and more uh you can use all the different models from open AI",
    "start": "129479",
    "end": "135160"
  },
  {
    "text": "but also like Lama uh models from kohary mistra and so on and the the",
    "start": "135160",
    "end": "143080"
  },
  {
    "text": "piece that I like the most is about the sessions that you can get oneon-one sessions with people like me or Pamela",
    "start": "143080",
    "end": "149680"
  },
  {
    "text": "uh that uh we volunteer our time to share our knowledge with Founders uh we",
    "start": "149680",
    "end": "155560"
  },
  {
    "text": "can talk about maybe like I don't know you are hiring and then I'm an expert in hiring so you come and talk to me and I",
    "start": "155560",
    "end": "161599"
  },
  {
    "text": "say hey these are some of the best practice for you when you are building your team or you can go to technical",
    "start": "161599",
    "end": "168519"
  },
  {
    "text": "sessions and ask more like technical pieces um as well and inside this",
    "start": "168519",
    "end": "173959"
  },
  {
    "text": "platform we have like several things other than the benefits that I mentioned and the guidance that I just mentioned",
    "start": "173959",
    "end": "180280"
  },
  {
    "text": "is what we call build with AI and inside we have some AI templates that the idea",
    "start": "180280",
    "end": "187000"
  },
  {
    "text": "is like you you we can help you accelerate um the the AI application",
    "start": "187000",
    "end": "193920"
  },
  {
    "text": "piece with some kind of like skeleton in a way um so so you have something up and",
    "start": "193920",
    "end": "199920"
  },
  {
    "text": "running in like few minutes um so again you get Cloud",
    "start": "199920",
    "end": "207280"
  },
  {
    "text": "credits you have access to Dev tools you have the AI templates you have the one one one1",
    "start": "207280",
    "end": "213959"
  },
  {
    "text": "guidance um and no matter where you are in your journey if you have an idea if",
    "start": "213959",
    "end": "219959"
  },
  {
    "text": "you are already building or if we're scaling this programs for",
    "start": "219959",
    "end": "226439"
  },
  {
    "text": "you um you have access to All The Cutting Edge AI tools so you can",
    "start": "226519",
    "end": "231640"
  },
  {
    "text": "innovate and streamline your AI development and on top of like the founders of this program that we have",
    "start": "231640",
    "end": "238640"
  },
  {
    "text": "you also have like programs uh that it kind of like it's like the next step like let's say you",
    "start": "238640",
    "end": "245079"
  },
  {
    "text": "are now scaling growing and then you use all the credits what is next there is a",
    "start": "245079",
    "end": "250599"
  },
  {
    "text": "next like you know we try to guide you through the whole process so there is something called the Pegasus program um",
    "start": "250599",
    "end": "258759"
  },
  {
    "text": "where we help you to coell go to market and so on and then there are some like strategic VC partners and like",
    "start": "258759",
    "end": "265440"
  },
  {
    "text": "accelerators that we partner with so we have partnership with why commun Ator Neo The Alchemist uh",
    "start": "265440",
    "end": "273840"
  },
  {
    "text": "Etc um pain points for startups there are a bunch of them uh one of them is",
    "start": "273840",
    "end": "279919"
  },
  {
    "text": "like you don't have time you cannot wait to go to market you have to go like as",
    "start": "279919",
    "end": "285720"
  },
  {
    "text": "fast as you can you have a lot of like resource constraints we have some issues with scalability you have you don't have",
    "start": "285720",
    "end": "291800"
  },
  {
    "text": "the support and guidance and that's where we are trying to uh help you with",
    "start": "291800",
    "end": "297600"
  },
  {
    "text": "so now we are going to go to the fun Parts it's like the AI template so",
    "start": "297600",
    "end": "302759"
  },
  {
    "text": "that's where Pamela is going to show you all the amazing things that you can do",
    "start": "302759",
    "end": "309039"
  },
  {
    "text": "with all the different tools all right so our goal today is",
    "start": "309039",
    "end": "315360"
  },
  {
    "text": "potentially having you deploy maybe even three different",
    "start": "315360",
    "end": "321039"
  },
  {
    "text": "templates okay um H so we have three different ones all like just um you know show in",
    "start": "321039",
    "end": "328880"
  },
  {
    "text": "the in the brow which ones we're going to be deploying right so we have starting we're going to start simple with this chat application",
    "start": "328880",
    "end": "335240"
  },
  {
    "text": "here just to make sure everything's up and working and then we've got two different rag applications one of them",
    "start": "335240",
    "end": "340840"
  },
  {
    "text": "is a rag on a postgress database like rag on a postgress table that does a SQL",
    "start": "340840",
    "end": "346479"
  },
  {
    "text": "filter building and then we have rag on a unstructured document so here I've got",
    "start": "346479",
    "end": "351560"
  },
  {
    "text": "a rag on my personal blog or like a rag on you know internal company documents whatever it is that you're going to",
    "start": "351560",
    "end": "357759"
  },
  {
    "text": "whatever kind of documents you're going to rag on so those are the three templates we're going to be looking at",
    "start": "357759",
    "end": "362800"
  },
  {
    "text": "today and we have it all set up so that you should be able to deploy those templates without spending any of your",
    "start": "362800",
    "end": "368840"
  },
  {
    "text": "own money and doing it all through our credits which is yay all right so um the",
    "start": "368840",
    "end": "375520"
  },
  {
    "text": "first thing you need to do is get this URL so everybody open this URL on your",
    "start": "375520",
    "end": "381000"
  },
  {
    "text": "computer so it's aka.ms e-workshop it should open up a a Word",
    "start": "381000",
    "end": "388800"
  },
  {
    "text": "document in the browser that looks like the screenshot you see here so you can either type in the URL or scan that QR",
    "start": "388800",
    "end": "395880"
  },
  {
    "text": "code and get that open on your",
    "start": "395880",
    "end": "401240"
  },
  {
    "text": "machine so let's make sure everyone's got it",
    "start": "401240",
    "end": "406319"
  },
  {
    "text": "open welcome welcome so go ahead once you've got your computer ready put this",
    "start": "408039",
    "end": "414639"
  },
  {
    "text": "uh put this URL in your browser Harold maybe you can just memorize it and then help anyone who",
    "start": "414639",
    "end": "421440"
  },
  {
    "text": "doesn't have it yeah ae- Workshop uh okay so then let me go to",
    "start": "421440",
    "end": "429360"
  },
  {
    "text": "that actual doc here so the first thing you need is a GitHub account does anybody here not",
    "start": "429360",
    "end": "436879"
  },
  {
    "text": "have a GitHub account okay so everyone here has a GitHub account great if you don't have a",
    "start": "436879",
    "end": "442199"
  },
  {
    "text": "GitHub account you can sign up for one for free right now and um and that should be fine um the next next thing",
    "start": "442199",
    "end": "449800"
  },
  {
    "text": "you need is an aure pass so this is something that we've got for this workshop for this conference and this is",
    "start": "449800",
    "end": "455680"
  },
  {
    "text": "going to let you deploy stuff on aure without spending any of your own money so we got a passes for 50 bucks and",
    "start": "455680",
    "end": "463120"
  },
  {
    "text": "they're valid for 7 days so if you do want to keep hacking after the workshop you can keep using your pass and uh",
    "start": "463120",
    "end": "470159"
  },
  {
    "text": "after 7 days it'll disappear just like Cinderella and the Pumpkin uh so in",
    "start": "470159",
    "end": "475520"
  },
  {
    "text": "order to get that as your pass you do need to have some sort of Microsoft account so you can use your like uh you",
    "start": "475520",
    "end": "483919"
  },
  {
    "text": "could use a personal Microsoft account if you have one uh so if you're if you like how do you tell which one you're",
    "start": "483919",
    "end": "489599"
  },
  {
    "text": "logged into right now I guess if you just go to outlook. office.com maybe you know what Microsoft account you're currently logged into um and then you",
    "start": "489599",
    "end": "497479"
  },
  {
    "text": "can see some people in the last Workshop were like logged into their kids Minecraft account so just uh just you",
    "start": "497479",
    "end": "503479"
  },
  {
    "text": "you need a Microsoft account and you might want to double check to see which one you're currently signed into if you are signed into a Microsoft account if",
    "start": "503479",
    "end": "509680"
  },
  {
    "text": "you don't have a Microsoft account no big deal you can make one on the spot I made one this morning so uh if you do",
    "start": "509680",
    "end": "516440"
  },
  {
    "text": "need to make one you can just make up a new outlook address and set it up that way um so you can also make it as part",
    "start": "516440",
    "end": "521919"
  },
  {
    "text": "of this progress so we're going to go to this a checkin URL and that's linked from this doc here so if you don't have",
    "start": "521919",
    "end": "527800"
  },
  {
    "text": "this doc if you just came in we can help you get this doc open so we can get this URL and uh we're going to spend 10",
    "start": "527800",
    "end": "534440"
  },
  {
    "text": "minutes making sure we get through this step since it can be a little a little tricky so when you go to this checkin",
    "start": "534440",
    "end": "541200"
  },
  {
    "text": "URL right we put this in the browser it loads this is what you're going to see",
    "start": "541200",
    "end": "546480"
  },
  {
    "text": "and it says I can either create a GitHub account or log in with GitHub so I'm going to log in with GitHub because I already have a GitHub account and I'm",
    "start": "546480",
    "end": "552160"
  },
  {
    "text": "logged into this browser with it already so I'm just going to click on",
    "start": "552160",
    "end": "557760"
  },
  {
    "text": "that and so what that's going to do is create a pass for my GitHub account and",
    "start": "558240",
    "end": "564480"
  },
  {
    "text": "so we get a pass so each of us will get a different code based off our GitHub account so this is my you know basically",
    "start": "564480",
    "end": "570440"
  },
  {
    "text": "my Azure pass promo code so I can copy that and then there's this button here that says get on board with Azure this",
    "start": "570440",
    "end": "577160"
  },
  {
    "text": "is the next step is to click",
    "start": "577160",
    "end": "581800"
  },
  {
    "text": "this and then we get this screen which says okay this is you can",
    "start": "582240",
    "end": "588240"
  },
  {
    "text": "start and when I click this here it says what my currently logged in account is",
    "start": "588240",
    "end": "593800"
  },
  {
    "text": "so this is where you should check to make sure you're happy with what account you're logged in with and you don't want to switch um I don't recommend using a",
    "start": "593800",
    "end": "601600"
  },
  {
    "text": "corporate account if you do have a corporate account like don't just don't use it it's going to be problematic for",
    "start": "601600",
    "end": "607040"
  },
  {
    "text": "various reasons because corporate accounts may have restrictions that won't let you deploy things so we do",
    "start": "607040",
    "end": "612320"
  },
  {
    "text": "recommend using some sort of personal account or making up a new account so that's why you see I'm using my Gmail instead of my",
    "start": "612320",
    "end": "618160"
  },
  {
    "text": "Microsoft uh so I'll confirm my account and then I can enter the promo",
    "start": "618160",
    "end": "624000"
  },
  {
    "text": "code and that was from this screen so I still have this screen open so I just go there I paste it",
    "start": "624000",
    "end": "630959"
  },
  {
    "text": "in and then we go s uh 6 x y y k I think",
    "start": "630959",
    "end": "638680"
  },
  {
    "text": "it's case insensitive submit and then it's going to actually fail for me because I've",
    "start": "638680",
    "end": "644279"
  },
  {
    "text": "already set this up on on this thing here um and this if you see this it's because you've already actually gone",
    "start": "644279",
    "end": "650800"
  },
  {
    "text": "through this stage uh so for you it should work the first time and then uh it'll create the Azure account for you",
    "start": "650800",
    "end": "658639"
  },
  {
    "text": "and if it works then what we can do is go to portal. azure.com so portal.",
    "start": "658639",
    "end": "663760"
  },
  {
    "text": "azure.com and we'll see what it how it loads",
    "start": "663760",
    "end": "671680"
  },
  {
    "text": "in does a bunch of redirects and then we can click on subscriptions and what we",
    "start": "671680",
    "end": "677440"
  },
  {
    "text": "should see is there should be at least one subscription that says as your pass",
    "start": "677440",
    "end": "682600"
  },
  {
    "text": "Dash sponsorship so that's our key that we have done this correctly and as long",
    "start": "682600",
    "end": "687760"
  },
  {
    "text": "as we use this subscription when we're doing our deploys we will not get charged any money well Microsoft will",
    "start": "687760",
    "end": "693680"
  },
  {
    "text": "but you won't that's the important part okay so we're going to spend 10 minutes to make sure that we can get everyone",
    "start": "693680",
    "end": "699240"
  },
  {
    "text": "through this this stage so that we're all on the same page going forward so if you already got it that's awesome you",
    "start": "699240",
    "end": "706160"
  },
  {
    "text": "can um you know like look at Harold's like uh Facebook profile or something",
    "start": "706160",
    "end": "715040"
  },
  {
    "text": "so once you have that set up the next step is the",
    "start": "721880",
    "end": "727519"
  },
  {
    "text": "proxy um so I'll just show that uh so you can start playing with that uh so",
    "start": "727519",
    "end": "733519"
  },
  {
    "text": "here's it's the next Link in here so the reason we have a proxy is because normally when you're using as your",
    "start": "733519",
    "end": "740199"
  },
  {
    "text": "opening eye you actually have to fill out a form and say how you're going to use as your open Ai and then somebody says oh okay yeah that's a good use of",
    "start": "740199",
    "end": "747160"
  },
  {
    "text": "open AI because Microsoft doesn't want able to use AI willy-nilly so we you know check to make sure that something",
    "start": "747160",
    "end": "753440"
  },
  {
    "text": "adheres to our responsible AI principles uh we don't have enough time for you to go through that process while",
    "start": "753440",
    "end": "759240"
  },
  {
    "text": "we're in a workshop so we've set up a an Azure opena proxy that you can use during the workshop with the repos and",
    "start": "759240",
    "end": "766519"
  },
  {
    "text": "we have special instructions for how you can use this proxy with the repos since you can't use the actual as your open AI",
    "start": "766519",
    "end": "773760"
  },
  {
    "text": "uh so this you can follow the link from the doc and log in with your GitHub",
    "start": "773760",
    "end": "778959"
  },
  {
    "text": "account out uh I'll log out so I can show that log in with",
    "start": "778959",
    "end": "786440"
  },
  {
    "text": "GitHub okay it says I'm logged in and then we have an API key and a",
    "start": "787399",
    "end": "795320"
  },
  {
    "text": "proxy endpoint and that's all we need to be able to uh to use an as your open AI",
    "start": "795320",
    "end": "802519"
  },
  {
    "text": "instance now normally I don't like to use keys and I tell everybody to avoid them but uh in this situation we are",
    "start": "802519",
    "end": "809920"
  },
  {
    "text": "going to be using keys and uh yeah and these keys will expire at a certain point so we don't have to worry about",
    "start": "809920",
    "end": "815959"
  },
  {
    "text": "them being exposed uh typically with keys we'd have to protect them very fiercely so that nobody was using them",
    "start": "815959",
    "end": "823120"
  },
  {
    "text": "so you can go ahead and log into this and see your registration details and then you can even play around with the",
    "start": "823120",
    "end": "828880"
  },
  {
    "text": "playground this is really similar to the aure open AI playground or the open.com",
    "start": "828880",
    "end": "834199"
  },
  {
    "text": "playground if any of you played around with this uh you can see here you can play with the system message that's how",
    "start": "834199",
    "end": "839880"
  },
  {
    "text": "you like say like oh you're an AI assistant that constantly makes pirate",
    "start": "839880",
    "end": "848480"
  },
  {
    "text": "jokes y uh and then we update the system",
    "start": "849000",
    "end": "854320"
  },
  {
    "text": "message oh private I wonder what it'll",
    "start": "854320",
    "end": "859440"
  },
  {
    "text": "do there we go and then um let's see oh",
    "start": "859440",
    "end": "864759"
  },
  {
    "text": "enter my API key okay so we need to enter the key I actually never used this before uh so we're going to enter the",
    "start": "864759",
    "end": "870440"
  },
  {
    "text": "key not save it select a model okay so we select a model over",
    "start": "870440",
    "end": "876480"
  },
  {
    "text": "here uh so we've got 35 turbo I didn't know we had four too you",
    "start": "876480",
    "end": "882680"
  },
  {
    "text": "set up four as well cool we can use four four is better all right and four is",
    "start": "882680",
    "end": "889600"
  },
  {
    "text": "slower but better uh okay and then uh please uh tell",
    "start": "889600",
    "end": "897079"
  },
  {
    "text": "audience about open AI",
    "start": "897079",
    "end": "903240"
  },
  {
    "text": "okay all right and you can see different parameters that we send and these are all getting sent to the open AI SDK so",
    "start": "904360",
    "end": "909920"
  },
  {
    "text": "we say the model right here we've set up two models gbd3 5 turbo gbd4 those are",
    "start": "909920",
    "end": "915199"
  },
  {
    "text": "often the ones you're picking between with open AI although now you've got GB 40 that's a good choice if you're doing",
    "start": "915199",
    "end": "921079"
  },
  {
    "text": "something with vision something multimodal I wouldn't use it otherwise just based off of some experience we've",
    "start": "921079",
    "end": "926759"
  },
  {
    "text": "had with it um but is a great one gb40 is good for vision uh so here you can",
    "start": "926759",
    "end": "933040"
  },
  {
    "text": "see you with the combination of the system message and the user message so",
    "start": "933040",
    "end": "938279"
  },
  {
    "text": "this is what we call user message this is what we call system message those combined together we get back a response",
    "start": "938279",
    "end": "943880"
  },
  {
    "text": "like this where it describes open AI with lots of RS and mes and stuff uh we",
    "start": "943880",
    "end": "950959"
  },
  {
    "text": "can you know change different parameters here like how many tokens it should send back the temperature is roughly the",
    "start": "950959",
    "end": "957720"
  },
  {
    "text": "creativity uh top p is also roughly about creativity and there's some more",
    "start": "957720",
    "end": "963600"
  },
  {
    "text": "advanced stuff there and you can see how many tokens you used on the way out and how many tokens you got on the response",
    "start": "963600",
    "end": "970399"
  },
  {
    "text": "so you can play around with this playground to uh you know to try stuff",
    "start": "970399",
    "end": "975639"
  },
  {
    "text": "out and make sure that uh that you're able to to use the key so this is just",
    "start": "975639",
    "end": "981160"
  },
  {
    "text": "linked off of um off of this Workshop right so if you go to the workshop proxy you log in you'll get your key and your",
    "start": "981160",
    "end": "988519"
  },
  {
    "text": "endpoint point you can go to that playground and you can play around with the playground to check that that's",
    "start": "988519",
    "end": "994680"
  },
  {
    "text": "working but we just want to make sure everybody now has an Azure pass and is logged in to the proxy so that you have",
    "start": "994680",
    "end": "1002160"
  },
  {
    "text": "a key and an endpoint so we'll just check to see if anyone had any issues of that this step is hopefully okay all",
    "start": "1002160",
    "end": "1007880"
  },
  {
    "text": "right so here's the like these are if you're looking for the models this is generally the the page to check um so",
    "start": "1007880",
    "end": "1014639"
  },
  {
    "text": "you know gd4 gbd4 and going down those are the gb4 models",
    "start": "1014639",
    "end": "1023000"
  },
  {
    "text": "gb5 you're saying there's a gbd3 5 that supports Vision no no oh four tuber with",
    "start": "1023000",
    "end": "1030600"
  },
  {
    "text": "vision this one yeah so we were using that one but it's a lot slower yeah so",
    "start": "1030600",
    "end": "1037120"
  },
  {
    "text": "that's why we I've started using 40 this one this is the G",
    "start": "1037120",
    "end": "1043280"
  },
  {
    "text": "oh oh okay all right yeah so you just want to compare",
    "start": "1043280",
    "end": "1050039"
  },
  {
    "text": "those so we'll just be using the basic GPD 35 and GPD just GPD 35 today actually and then also the embedding",
    "start": "1050039",
    "end": "1055880"
  },
  {
    "text": "models okay so is everybody set up with the proxy okay all right so now we're going",
    "start": "1055880",
    "end": "1062799"
  },
  {
    "text": "to actually get something working so we have this repo here so you can follow",
    "start": "1062799",
    "end": "1068840"
  },
  {
    "text": "the link from the doc and it has readms for the three different projects that we can deploy",
    "start": "1068840",
    "end": "1075880"
  },
  {
    "text": "and these readmes are specific to using them with the aure openai proxy uh so",
    "start": "1075880",
    "end": "1081640"
  },
  {
    "text": "normally you can just use the the readmes that are on the repos itself but because we are using this AZ your open a",
    "start": "1081640",
    "end": "1086960"
  },
  {
    "text": "proxy we do have to use a slightly different setup so we've made readme specific uh for this for this Workshop",
    "start": "1086960",
    "end": "1094960"
  },
  {
    "text": "uh so we can start off on this uh open AI chat app Quick Start and make sure",
    "start": "1094960",
    "end": "1101640"
  },
  {
    "text": "that that's all working so the first step is to open in GitHub code spaces so you can do that by clicking this button",
    "start": "1101640",
    "end": "1108240"
  },
  {
    "text": "here have any of you used codes spaces before okay a couple people so code",
    "start": "1108240",
    "end": "1114440"
  },
  {
    "text": "spaces will open a VSS code in your browser with a developer environment for",
    "start": "1114440",
    "end": "1121320"
  },
  {
    "text": "that repo so you can actually use code spaces on any GitHub repo so you go to go any giab repo you click on code and",
    "start": "1121320",
    "end": "1127240"
  },
  {
    "text": "you can make a code space for it so it's a way that you can start hacking on any repo uh very quickly so you can open",
    "start": "1127240",
    "end": "1134600"
  },
  {
    "text": "this button here to open in code spaces and and I'll just go ahead and",
    "start": "1134600",
    "end": "1140400"
  },
  {
    "text": "make a new one and I'll say create",
    "start": "1140400",
    "end": "1146200"
  },
  {
    "text": "codespace so this is going to take a few minutes to load because what it's doing is that it's creating the environment",
    "start": "1151080",
    "end": "1156880"
  },
  {
    "text": "for this repository it's opening VSS code in the browser and it's also just",
    "start": "1156880",
    "end": "1162520"
  },
  {
    "text": "setting up vs code so if you actually have like if you use VSS code locally and you've got like extensions that you",
    "start": "1162520",
    "end": "1168000"
  },
  {
    "text": "use locally it's it's actually potentially syncing those extensions and uh enabling them them here I should",
    "start": "1168000",
    "end": "1174360"
  },
  {
    "text": "probably just not do that cuz then it would load faster for me um but yeah you can see in the bottom here as it's",
    "start": "1174360",
    "end": "1180880"
  },
  {
    "text": "setting up and we'll just wait for it so this is",
    "start": "1180880",
    "end": "1186919"
  },
  {
    "text": "you know the slowest part of using Code spaces is just the loading you a",
    "start": "1186919",
    "end": "1193480"
  },
  {
    "text": "tip if you want faster code spaces there's pre-builts available as well yeah and I do have them on the third",
    "start": "1193480",
    "end": "1200480"
  },
  {
    "text": "repo but I think I don't have it on this one so I I should have remembered to do prebuilds for all the repos right and",
    "start": "1200480",
    "end": "1206799"
  },
  {
    "text": "the slowest part is probably installing all the dependencies in the build it's basically it's doing all the things you would do when you install it locally",
    "start": "1206799",
    "end": "1214400"
  },
  {
    "text": "just automated and with a progress bar and at some point it will just light up",
    "start": "1214400",
    "end": "1221480"
  },
  {
    "text": "yeah let's see what the you can even watch can we watch the logs for this one building codes Bas code",
    "start": "1222000",
    "end": "1230240"
  },
  {
    "text": "there we go so if you like this sort of thing like if you like watching Docker containers build because that's what",
    "start": "1230240",
    "end": "1236240"
  },
  {
    "text": "it's actually doing everything's a Docker container so you can actually watch it as it um builds everything here",
    "start": "1236240",
    "end": "1244799"
  },
  {
    "text": "and now it's downloading all the requirements so these are all the python requirements so all the examples that we're going through today have a python",
    "start": "1244799",
    "end": "1251200"
  },
  {
    "text": "back end and then some sort of JavaScript front end uh this one has what we call like a vanilla JavaScript",
    "start": "1251200",
    "end": "1256960"
  },
  {
    "text": "front end as in I just wrote some JavaScript in a script tag H but then the other ones are much fancier so",
    "start": "1256960",
    "end": "1262760"
  },
  {
    "text": "they've got a full typescript and a build system and react components uh using the Microsoft fluent UI uh you",
    "start": "1262760",
    "end": "1269440"
  },
  {
    "text": "know web framework so you can kind of see the range of front ends there okay",
    "start": "1269440",
    "end": "1274919"
  },
  {
    "text": "so you can see it's you know it's still going through the process but at least now uh we can see the file explorer has",
    "start": "1274919",
    "end": "1281440"
  },
  {
    "text": "loaded so we can uh explore the files here and uh and I'll show I'll go ahead",
    "start": "1281440",
    "end": "1287600"
  },
  {
    "text": "and show the the code if you're interested in the code uh it is in the",
    "start": "1287600",
    "end": "1292880"
  },
  {
    "text": "source folder uh we are using a court application and I think nobody has heard of qu but uh has anyone here heard of",
    "start": "1292880",
    "end": "1299520"
  },
  {
    "text": "flask or used flask great so quart is just the async version of flask so it's",
    "start": "1299520",
    "end": "1305880"
  },
  {
    "text": "literally built on top of flask and one day it might be brought back into flask and it just you just take your flask",
    "start": "1305880",
    "end": "1312400"
  },
  {
    "text": "code and you put ayns in it and then you've got you've got court uh that's really uh how it goes so uh if you",
    "start": "1312400",
    "end": "1319080"
  },
  {
    "text": "haven't done async before in Python async is a way that uh if you use async with your functions they become",
    "start": "1319080",
    "end": "1325039"
  },
  {
    "text": "co-routines and then they can be paused and waited on and it's important to use async when we're building applications",
    "start": "1325039",
    "end": "1331960"
  },
  {
    "text": "with AI because we have these really long blocking calls to an AI API rate so",
    "start": "1331960",
    "end": "1337559"
  },
  {
    "text": "we make a call to an llm and we send off our request and these llms they can take like two seconds 5 Seconds 10 seconds",
    "start": "1337559",
    "end": "1343720"
  },
  {
    "text": "right depending on what we're doing and while that's happening we ideally want to to be able to handle other user",
    "start": "1343720",
    "end": "1350240"
  },
  {
    "text": "requests coming in uh so that's why we use async framework so if we use an async",
    "start": "1350240",
    "end": "1355679"
  },
  {
    "text": "framework then while we're making IO calls we can handle other user requests",
    "start": "1355679",
    "end": "1360960"
  },
  {
    "text": "that are coming in so all of the ones that we see today have an async backend either court or fast API anyone heard of",
    "start": "1360960",
    "end": "1366960"
  },
  {
    "text": "fast API it's very very popular these days yeah so fast API is the one most people know of as the acing framework um",
    "start": "1366960",
    "end": "1373880"
  },
  {
    "text": "so I you know I I like both of them fairly equally uh so I I use a mix of both um but I",
    "start": "1373880",
    "end": "1381520"
  },
  {
    "text": "just want to make sure people know about the value of async Frameworks okay so that's all in the coure app folder uh if",
    "start": "1381520",
    "end": "1387760"
  },
  {
    "text": "you want to look at the code there so it is now finished okay anybody else get their code space loaded get a couple",
    "start": "1387760",
    "end": "1395080"
  },
  {
    "text": "okay great finish configuring so I can I",
    "start": "1395080",
    "end": "1400520"
  },
  {
    "text": "in the terminal yeah we are going to be using the terminal and if for some reason your terminal like goes away",
    "start": "1400520",
    "end": "1405640"
  },
  {
    "text": "sometimes this happens in the codes space just click that plus right here sometimes my terminal kind of blinks out so I just click the plus and that'll",
    "start": "1405640",
    "end": "1411919"
  },
  {
    "text": "give me a new terminal right Boop new",
    "start": "1411919",
    "end": "1417360"
  },
  {
    "text": "terminal okay so here we are in the terminal U but actually the first thing we're going to do is that there's aemv",
    "start": "1417360",
    "end": "1425360"
  },
  {
    "text": "do sample we're going to make a EMV file based off of that so I'm going to make a",
    "start": "1425360",
    "end": "1431520"
  },
  {
    "text": "new file and I can do that using this little new file button up here so I'll",
    "start": "1431520",
    "end": "1437240"
  },
  {
    "text": "just click that say new file and I'll type EMV uh you could also like copy and",
    "start": "1437240",
    "end": "1442720"
  },
  {
    "text": "paste um and then I'm just going to paste the EMV in there you could even rename em. sample to. EMV I think that's",
    "start": "1442720",
    "end": "1450320"
  },
  {
    "text": "another way um and then we need to fill in these values to match the values of the proxy so we'll go to the",
    "start": "1450320",
    "end": "1459440"
  },
  {
    "text": "proxy and let's see where's my proxy open here so here's my proxy so I'm going to go ahead and fill in this one",
    "start": "1459440",
    "end": "1467039"
  },
  {
    "text": "that's the end point so the end point should start with HTTP and end with slv1",
    "start": "1467039",
    "end": "1472480"
  },
  {
    "text": "and look like that in the middle uh so that's the end point that's where we'll be sending our open a",
    "start": "1472480",
    "end": "1478279"
  },
  {
    "text": "requests then we need the key so we'll copy that and it'll look like that or",
    "start": "1478279",
    "end": "1485120"
  },
  {
    "text": "slightly different for you and then the deployment is going to be the name of",
    "start": "1485120",
    "end": "1490960"
  },
  {
    "text": "the deployment is gbd 35 turbo uh and that's also the name of the",
    "start": "1490960",
    "end": "1497159"
  },
  {
    "text": "model in this casee so if you has anybody used open.com a few people okay so on",
    "start": "1497159",
    "end": "1503320"
  },
  {
    "text": "open.com you just pick what model you're going to use and that's all you need with as your openi you have to make",
    "start": "1503320",
    "end": "1509559"
  },
  {
    "text": "deployments based off of the model so you actually have a bunch of deployments and you could actually have multiple",
    "start": "1509559",
    "end": "1514840"
  },
  {
    "text": "deployments of a gbd 35 turbo model that have different names so when you're working with aure open AI you have to",
    "start": "1514840",
    "end": "1520200"
  },
  {
    "text": "know the deployment name not just the model name so that's one of the complexities of of using Azure open AI",
    "start": "1520200",
    "end": "1526559"
  },
  {
    "text": "but it does give you more flexibility cuz you can can say oh this deployment is going to have 20 tokens per minute and this one's going to have 30 tokens",
    "start": "1526559",
    "end": "1532640"
  },
  {
    "text": "per minute right and then you can like say which of your colleagues can use what like if they're all trying to like use up your deployment or whatever uh so",
    "start": "1532640",
    "end": "1540159"
  },
  {
    "text": "it's more flexibility but you do have to specify it okay so now myv is set up so",
    "start": "1540159",
    "end": "1545919"
  },
  {
    "text": "this is just so that I can run a a local server and I'm putting local server in quotes because I'm going to run a local",
    "start": "1545919",
    "end": "1552799"
  },
  {
    "text": "server inside GitHub code spaces so it's actually running a local server not on my actual machine but inside the GitHub",
    "start": "1552799",
    "end": "1560360"
  },
  {
    "text": "codes spaces development environment uh so to do that I grab I'll grab the command here that's going to",
    "start": "1560360",
    "end": "1566840"
  },
  {
    "text": "run the court app and just give it and when with code",
    "start": "1566840",
    "end": "1572840"
  },
  {
    "text": "spaces you do have to allow so you'll see this little thing that pops up so if you ever want to copy paste you have to",
    "start": "1572840",
    "end": "1579240"
  },
  {
    "text": "allow for the terminal uh and then I have okay then I paste it and then you can see that it",
    "start": "1579240",
    "end": "1586279"
  },
  {
    "text": "says it's running on this URL now you can't just paste this URL in the browser I'll show what will happen so if I paste",
    "start": "1586279",
    "end": "1593200"
  },
  {
    "text": "in the browser I'm going to get an error because this is not running on my local machine this is running inside GitHub",
    "start": "1593200",
    "end": "1598440"
  },
  {
    "text": "codes spaces so you have two ways to get to it one way is that if you just click on it uh like uh option click at least",
    "start": "1598440",
    "end": "1604320"
  },
  {
    "text": "on my Mac so I Mouse over it'll tell me what to do mouse over option click so",
    "start": "1604320",
    "end": "1609960"
  },
  {
    "text": "code spaces will actually detect that you're clicking on a local URL and it'll turn it into a codespace port URL and",
    "start": "1609960",
    "end": "1617919"
  },
  {
    "text": "it's say this funky URL up here improved disco for me um and uh and that's",
    "start": "1617919",
    "end": "1623520"
  },
  {
    "text": "actually you know like local for that GitHub machine and uh that's one way of doing it another way that you might like",
    "start": "1623520",
    "end": "1629960"
  },
  {
    "text": "more is you go to your ports Tab and you're going to find it listed here and uh we'll see the you know the",
    "start": "1629960",
    "end": "1638559"
  },
  {
    "text": "forwarded address and we can click on that or we can even click the GL Globe icon and we",
    "start": "1638559",
    "end": "1644760"
  },
  {
    "text": "get to the same URL so there's many ways you can get to this locally running URL uh and uh and get to the special code",
    "start": "1644760",
    "end": "1651720"
  },
  {
    "text": "space URL it and you can even change your Port visibility if you want to like share it with a colleague if you just or",
    "start": "1651720",
    "end": "1657000"
  },
  {
    "text": "in a class you can change it to public and then you could actually ping this URL to someone else now this is not a",
    "start": "1657000",
    "end": "1662679"
  },
  {
    "text": "deployed URL like you're not going to use this for like you know your deployed dril but it's fun it's good for",
    "start": "1662679",
    "end": "1668399"
  },
  {
    "text": "development so now I've got this running locally and now we can type stuff be",
    "start": "1668399",
    "end": "1673880"
  },
  {
    "text": "like what's the weather in San Francisco",
    "start": "1673880",
    "end": "1679240"
  },
  {
    "text": "see if it's going to lie uh can",
    "start": "1679240",
    "end": "1686799"
  },
  {
    "text": "see oh good that was a good answer I think this has been trained it refuse to answer it's always",
    "start": "1686799",
    "end": "1694080"
  },
  {
    "text": "good when it refuses to answer something it shouldn't know um so we could go ahead and like you know I could change",
    "start": "1694080",
    "end": "1700200"
  },
  {
    "text": "this now and change the system message and let's see where's our system",
    "start": "1700200",
    "end": "1706840"
  },
  {
    "text": "message in in here right so right now my assistant message is just you are a helpful assistant like you are a",
    "start": "1706840",
    "end": "1716279"
  },
  {
    "text": "assistant that cannot resist a good",
    "start": "1716279",
    "end": "1721720"
  },
  {
    "text": "pasta joke stra pasta jokes I don't know I love llms okay",
    "start": "1721720",
    "end": "1731480"
  },
  {
    "text": "so what's the weather today is it going to make a pasta joke",
    "start": "1731559",
    "end": "1738050"
  },
  {
    "text": "[Laughter] [Music] all right it looks like might getting",
    "start": "1738050",
    "end": "1743159"
  },
  {
    "text": "quite softy today don't forget your umbrella you might end up feeling like a so noodle so good uh",
    "start": "1743159",
    "end": "1750480"
  },
  {
    "text": "so uh so that works but so here we go so now this is running locally um and so",
    "start": "1750480",
    "end": "1758120"
  },
  {
    "text": "this is a good one like when we're developing we can just test things test things locally here the next thing we're going to do once we're happy with it",
    "start": "1758120",
    "end": "1764360"
  },
  {
    "text": "we're like this is the best app it makes pasta jokes we're going to deploy it so then we move on to the deployment",
    "start": "1764360",
    "end": "1771320"
  },
  {
    "text": "instructions so the first step of that is ASD off login so this is going to log",
    "start": "1771320",
    "end": "1777320"
  },
  {
    "text": "in to our aure account that we made earlier so I'll do ASD off",
    "start": "1777320",
    "end": "1784240"
  },
  {
    "text": "login and uh this is going to give us a device code that we're going to paste",
    "start": "1784240",
    "end": "1789480"
  },
  {
    "text": "into this oaf uh browser flow so let me go and open it up maybe over here I",
    "start": "1789480",
    "end": "1797000"
  },
  {
    "text": "think that's my Azure account that I'm using for this and then I go and I take this and I paste it",
    "start": "1797000",
    "end": "1803919"
  },
  {
    "text": "in and I'm going to pick my account I'm going to use this one",
    "start": "1806760",
    "end": "1813240"
  },
  {
    "text": "continue and uh okay and then we're logged in",
    "start": "1813240",
    "end": "1818640"
  },
  {
    "text": "okay all right so that was the device code flow so you just want to make sure that you log into the account that you",
    "start": "1818640",
    "end": "1825440"
  },
  {
    "text": "got with the pass right whatever account you use for the past that's what you want to log",
    "start": "1825440",
    "end": "1830480"
  },
  {
    "text": "into the next step is to create or Gabriel should I pause like should we",
    "start": "1830480",
    "end": "1836919"
  },
  {
    "text": "get through the local step first or should we keep going with ACD",
    "start": "1836919",
    "end": "1842399"
  },
  {
    "text": "deploy yeah we can pause and see if everyone's got the local local one running actually that I think that might",
    "start": "1845840",
    "end": "1851600"
  },
  {
    "text": "be good to do okay so let's just pause and see if there's any questions with getting the local one ready so yes",
    "start": "1851600",
    "end": "1858000"
  },
  {
    "text": "someone asked like can we just run this locally you can totally run these locally as well we like to use get Cub code spaces in workshops because that",
    "start": "1858000",
    "end": "1864159"
  },
  {
    "text": "reduces the number of potential developer environment issues if you want to run it locally uh you can either run",
    "start": "1864159",
    "end": "1870159"
  },
  {
    "text": "it you know just with a python virtual environment and you just have to install all the requirements uh or you can run",
    "start": "1870159",
    "end": "1876519"
  },
  {
    "text": "it with VSS code using the dev containers extension and that will do the dock Riz environment for you if you",
    "start": "1876519",
    "end": "1882760"
  },
  {
    "text": "want kind of the benefit of the dock rise environment without um you know being in the browser and having to pay",
    "start": "1882760",
    "end": "1889240"
  },
  {
    "text": "potentially pay for codes spaces so we should know also that GitHub codes spaces you have a limit of some number",
    "start": "1889240",
    "end": "1894399"
  },
  {
    "text": "of hours a month either 60 or 120 it's 60 okay I must have paid more so uh so",
    "start": "1894399",
    "end": "1900480"
  },
  {
    "text": "it's 60 um so you're not going to go over that today but eventually you could go over that if you use codes spaces a",
    "start": "1900480",
    "end": "1906840"
  },
  {
    "text": "lot so if you're local right I think I have mine open locally as well and I'm",
    "start": "1906840",
    "end": "1912159"
  },
  {
    "text": "just yeah locally I'm just using a a python virtual M so you're also welcome to try these things out locally if you",
    "start": "1912159",
    "end": "1917559"
  },
  {
    "text": "like local environments uh just you know be a good person and make a python",
    "start": "1917559",
    "end": "1922639"
  },
  {
    "text": "virtual M to manage your python dependencies right",
    "start": "1922639",
    "end": "1927799"
  },
  {
    "text": "um yeah okay so I saw a lot of local things so I think we can move on to the",
    "start": "1927799",
    "end": "1933919"
  },
  {
    "text": "addd um so yeah I did the login so you saw me do you saw me do the login here",
    "start": "1933919",
    "end": "1940159"
  },
  {
    "text": "right and that's using the device code flow uh so you should see something like this happen from inside code spaces and",
    "start": "1940159",
    "end": "1948039"
  },
  {
    "text": "next step is to make a new a environment so a is this tool we're using for",
    "start": "1948039",
    "end": "1954080"
  },
  {
    "text": "deployment uh so we make a new environment name you can just call it like chat app whatever you want to call it and then what that does is it",
    "start": "1954080",
    "end": "1961000"
  },
  {
    "text": "actually makes this do aure folder and it makes this chat app folder inside and",
    "start": "1961000",
    "end": "1966480"
  },
  {
    "text": "that's where it's going to store all of our deployment environment variables so we need to set all configure anything we",
    "start": "1966480",
    "end": "1973080"
  },
  {
    "text": "want to customize about our deployment we're going to configure that now and it's going to get update it's going to",
    "start": "1973080",
    "end": "1978639"
  },
  {
    "text": "update this file here uh so the next thing we're going to do is set all these ASD environment",
    "start": "1978639",
    "end": "1985120"
  },
  {
    "text": "variables so the ASD environment variables are different from the ones we just saw in the m the m is just for the",
    "start": "1985120",
    "end": "1990679"
  },
  {
    "text": "local server ASD environment variables are for deployment uh sometimes we use the same but a lot of times we want our",
    "start": "1990679",
    "end": "1996720"
  },
  {
    "text": "local environment to be slightly different from our deployed environment uh so we have two different ways of",
    "start": "1996720",
    "end": "2002960"
  },
  {
    "text": "setting those variables all right so I first set these commands so this is just going to tell it to not create an Azure",
    "start": "2002960",
    "end": "2009600"
  },
  {
    "text": "open AI because we're using the proxy and then we're going to set the name of the deployment to gbd 35",
    "start": "2009600",
    "end": "2016519"
  },
  {
    "text": "turbo then we need to set the key so I'm going to paste this and then I'm going to delete delete",
    "start": "2016519",
    "end": "2023610"
  },
  {
    "text": "[Music] delete gosh that's what happens when you have Wi-Fi issues actually is you see it",
    "start": "2023610",
    "end": "2029440"
  },
  {
    "text": "with the typing then I got to find my key again uh there we go so that sets the",
    "start": "2029440",
    "end": "2036559"
  },
  {
    "text": "key and then I'm going to set the point I'm going to delete how we're going to do this here",
    "start": "2036559",
    "end": "2043399"
  },
  {
    "text": "we go and get the end",
    "start": "2043399",
    "end": "2050040"
  },
  {
    "text": "point all right so now I've set all these things now if I've done it correctly if I look at my DOT as your",
    "start": "2055399",
    "end": "2062079"
  },
  {
    "text": "folder for that environment I I created I should see a EMV that looks like this",
    "start": "2062079",
    "end": "2068158"
  },
  {
    "text": "so this is a EMV that's inside the dot as your folder so this is what is going to be used for the",
    "start": "2068159",
    "end": "2074638"
  },
  {
    "text": "deployment and it's going to tell it you know this is how it's going to set up the AZ your open AI connection okay and",
    "start": "2074639",
    "end": "2081960"
  },
  {
    "text": "now I'm just going to do I'm just going to type type thank you okay all right and then I'm going to do azd",
    "start": "2081960",
    "end": "2090079"
  },
  {
    "text": "up here we go so what ASD up is doing is",
    "start": "2090079",
    "end": "2095118"
  },
  {
    "text": "that it's actually deciding it's doing several stages okay so I I have to select an as your subscription in this",
    "start": "2095119",
    "end": "2101960"
  },
  {
    "text": "case I only have one subscription so you just press enter uh if you had two subscriptions",
    "start": "2101960",
    "end": "2107680"
  },
  {
    "text": "you would want to pick the sponsorship one uh then I select an Azure location",
    "start": "2107680",
    "end": "2112920"
  },
  {
    "text": "to use typically you just choose one that's close to you so Central us is",
    "start": "2112920",
    "end": "2118200"
  },
  {
    "text": "pretty good uh now what ACD is doing the first step is that it's actually packaging up",
    "start": "2118200",
    "end": "2124359"
  },
  {
    "text": "the code that it's going to deploy later uh in this case you're deploying to as your container apps so it's packaging up",
    "start": "2124359",
    "end": "2130480"
  },
  {
    "text": "a Docker container file so it's actually literally building a Docker container right now so if you do like working with",
    "start": "2130480",
    "end": "2136160"
  },
  {
    "text": "Docker as your container apps is a great fit and a lot of people like Docker so",
    "start": "2136160",
    "end": "2141680"
  },
  {
    "text": "we deploy a lot of stuff there but we also are going to be using Azure app service for one of the later templates",
    "start": "2141680",
    "end": "2147040"
  },
  {
    "text": "uh so we've got lots of ways to deploy on Azure so you can see it building up that Docker the step after this is where",
    "start": "2147040",
    "end": "2153480"
  },
  {
    "text": "it's actually going to create a your resources so it's going to create the container apps can create a container",
    "start": "2153480",
    "end": "2158640"
  },
  {
    "text": "registry create a container apps environment and create a log analytics workspace so these are all the",
    "start": "2158640",
    "end": "2164400"
  },
  {
    "text": "components of a containerized app on Azure and uh you know it's multiple",
    "start": "2164400",
    "end": "2170400"
  },
  {
    "text": "components and we have to stitch them together the way we stitch them together is using infrastructure as code uh Has",
    "start": "2170400",
    "end": "2176240"
  },
  {
    "text": "anyone used terraform here before okay so we have our own version of terraform it's called bicep and it is a",
    "start": "2176240",
    "end": "2183760"
  },
  {
    "text": "infrastructures code which means we're declaring what resources we want to make",
    "start": "2183760",
    "end": "2189440"
  },
  {
    "text": "right so we say oh we want to make log analytics we want to make container apps we want to make you know the actual",
    "start": "2189440",
    "end": "2196599"
  },
  {
    "text": "container apps image and then we're going to sign some roles right so all of that is declared in this bicep file so",
    "start": "2196599",
    "end": "2203760"
  },
  {
    "text": "that way you have repeatable repeatable processes for provisioning and this is really helpful when you're making",
    "start": "2203760",
    "end": "2209960"
  },
  {
    "text": "complex applications on Azure because you might have like 10 different things you're using right uh you might have a",
    "start": "2209960",
    "end": "2215400"
  },
  {
    "text": "postgress and a key Vault and redis cache and uh log analytics and app",
    "start": "2215400",
    "end": "2221040"
  },
  {
    "text": "service and you want them all to tie together so you can declare what that you know what that infrastructure looks",
    "start": "2221040",
    "end": "2227240"
  },
  {
    "text": "like and then uh and then put that in a bicep file and then deploy it you can also use terraform so if you really into",
    "start": "2227240",
    "end": "2233839"
  },
  {
    "text": "terraform and very comfortable with it you could totally use terraform tier as well I don't know terraform I haven't",
    "start": "2233839",
    "end": "2239000"
  },
  {
    "text": "used it personally so all of my examples do use bicep but if you want to send a PR with her form I'll I'll review it and",
    "start": "2239000",
    "end": "2247319"
  },
  {
    "text": "just stamp it cuz I don't know how to reason about it so what you can see here is that it is actually creating uh the",
    "start": "2247319",
    "end": "2253839"
  },
  {
    "text": "resources right now so you can watch it here you can also watch it in the portal it's not really super exciting to watch",
    "start": "2253839",
    "end": "2259920"
  },
  {
    "text": "so this is the point where I usually fold my laundry um uh but because it can take some amount of time H or you can",
    "start": "2259920",
    "end": "2267400"
  },
  {
    "text": "even get an error oh I I used okay so I already made one in central us for the",
    "start": "2267400",
    "end": "2274160"
  },
  {
    "text": "earlier demo so I should have picked a different region so for this as your pass there is a constraint of one",
    "start": "2274160",
    "end": "2279440"
  },
  {
    "text": "container app per region which is why we said in the readme that you should pick a region that you haven't picked before",
    "start": "2279440",
    "end": "2286119"
  },
  {
    "text": "and I didn't pay attention to I read me uh so that one won't deploy so uh what I",
    "start": "2286119",
    "end": "2291280"
  },
  {
    "text": "can do is I'm just going to make a I'll just make a new environment I'll just uh I'll just copy everything",
    "start": "2291280",
    "end": "2297880"
  },
  {
    "text": "over um now what you shouldn't run into this because this would be your first uh your first environment right uh so chat",
    "start": "2297880",
    "end": "2304920"
  },
  {
    "text": "app 2 and I'll just copy and paste we'll",
    "start": "2304920",
    "end": "2311319"
  },
  {
    "text": "change chat to and then West us seems like a good region okay and then I'll",
    "start": "2311319",
    "end": "2317760"
  },
  {
    "text": "asdm select chat app",
    "start": "2317760",
    "end": "2322119"
  },
  {
    "text": "to there we go and then ACD up yeah",
    "start": "2323319",
    "end": "2329720"
  },
  {
    "text": "okay so then it'll up do the up again but I have one of these already already",
    "start": "2329720",
    "end": "2334800"
  },
  {
    "text": "deployed so I'll just open up the deployed one so you can can see deployed deployed is going to look pretty darn",
    "start": "2334800",
    "end": "2339920"
  },
  {
    "text": "similar to what it looks like locally where's the one deployed okay so this one's deployed it looks pretty much the",
    "start": "2339920",
    "end": "2345680"
  },
  {
    "text": "same as what it looks like running locally right the difference is that the URL is a container apps URL and you'll",
    "start": "2345680",
    "end": "2351520"
  },
  {
    "text": "see this URL displayed in the terminal once it finishes successfully deploying",
    "start": "2351520",
    "end": "2356560"
  },
  {
    "text": "you'll see this uh displayed let me see if I have that in my history anywhere",
    "start": "2356560",
    "end": "2362240"
  },
  {
    "text": "from earlier today uh no you never know okay so let's see",
    "start": "2362240",
    "end": "2370480"
  },
  {
    "text": "how it's going here",
    "start": "2370480",
    "end": "2375960"
  },
  {
    "text": "yeah I lovingly handcrafted them yeah so um I yeah I write the bicep files some",
    "start": "2377400",
    "end": "2384680"
  },
  {
    "text": "of them all the ones in core are actually from a shared repo that we just copy and paste from we're trying to move",
    "start": "2384680",
    "end": "2390000"
  },
  {
    "text": "towards something called a AVM as your verified modules which are bicop files that are maintained and have security",
    "start": "2390000",
    "end": "2396800"
  },
  {
    "text": "best practice in them so we'll we'll gradually be moving over but basically with bicep files like you can use ones",
    "start": "2396800",
    "end": "2402440"
  },
  {
    "text": "from a central registry you can use ones from your own private registry if you're doing a lot of them uh or you can just",
    "start": "2402440",
    "end": "2408319"
  },
  {
    "text": "use you know ones inside the folder um so there's a lot of techniques you can use depending on how much bicep you're",
    "start": "2408319",
    "end": "2416440"
  },
  {
    "text": "using okay so now it's starting over and deploying again all right so let's walk around and or any questions on what I",
    "start": "2417240",
    "end": "2424760"
  },
  {
    "text": "showed here all right so I saw saw a lot of AG deployments are going I saw some",
    "start": "2424760",
    "end": "2431560"
  },
  {
    "text": "issues with like naming which I run into all the time as your has very obscure naming rules the safest thing is to do",
    "start": "2431560",
    "end": "2438880"
  },
  {
    "text": "short names with no symbols in them and nothing fancy uh if you do have a naming",
    "start": "2438880",
    "end": "2444800"
  },
  {
    "text": "rule you can just always do asdm new and make a new environment and you know and",
    "start": "2444800",
    "end": "2449839"
  },
  {
    "text": "start over uh and that should be okay uh but generally the issues you run into with deployment are usually related to",
    "start": "2449839",
    "end": "2455920"
  },
  {
    "text": "naming region constraints account constraints and that's probably yeah the",
    "start": "2455920",
    "end": "2463680"
  },
  {
    "text": "ones you might run into all right so um we giv me a 45 minutes left I'm going to",
    "start": "2463680",
    "end": "2468880"
  },
  {
    "text": "show you the other ones and uh and these are ones that you can uh that you can",
    "start": "2468880",
    "end": "2474319"
  },
  {
    "text": "also start trying to deploy now and following very similar readmes right so the first one um actually the the these",
    "start": "2474319",
    "end": "2481599"
  },
  {
    "text": "two are both about rag so first I'll talk briefly about rag right so let me first motivate it right so uh let's see",
    "start": "2481599",
    "end": "2488280"
  },
  {
    "text": "like uh tell me what Pamela Fox uh likes",
    "start": "2488280",
    "end": "2494319"
  },
  {
    "text": "to code on I don't know let's try this I'm trying to get it to lie",
    "start": "2494319",
    "end": "2501240"
  },
  {
    "text": "um this is the pasta one okay so this one clearly lied spaghetti Fon great all",
    "start": "2501240",
    "end": "2508200"
  },
  {
    "text": "right so then but then if I go to um this one right here tell me what Pamela",
    "start": "2508200",
    "end": "2514640"
  },
  {
    "text": "Fox likes to code on",
    "start": "2514640",
    "end": "2519039"
  },
  {
    "text": "this one will hopefully be more accurate at least have less pasta jokes here um and this is so basically what",
    "start": "2519680",
    "end": "2526319"
  },
  {
    "text": "we're trying to show is that if we just ask an llm to answer a question it is",
    "start": "2526319",
    "end": "2531359"
  },
  {
    "text": "it's very possible that it's just going to make something up um if that one it seems like there oh good I mean in this",
    "start": "2531359",
    "end": "2537480"
  },
  {
    "text": "case it says it doesn't know what I like to code in I think I should have said like code in um you know like here like",
    "start": "2537480",
    "end": "2543640"
  },
  {
    "text": "what python Frameworks does pamelo use let's try this one um so you know if it doesn't know the answer it'll say in",
    "start": "2543640",
    "end": "2549680"
  },
  {
    "text": "this case yeah in this case it does know the answer because this is actually using the rag technique in order to answer questions based off a knowledge",
    "start": "2549680",
    "end": "2557400"
  },
  {
    "text": "Source right um so those are our last two samples are about",
    "start": "2557400",
    "end": "2563839"
  },
  {
    "text": "rag uh so the general approach of rag is that we get a user question we use that",
    "start": "2563839",
    "end": "2569359"
  },
  {
    "text": "user question to search some sort of database or search engine we get back matching search results for that user",
    "start": "2569359",
    "end": "2576119"
  },
  {
    "text": "question and then we send those uh to the large language model and say Here's the user question here are the sources",
    "start": "2576119",
    "end": "2583520"
  },
  {
    "text": "now answer the question according to the sources and so now we can make customize applications that can actually",
    "start": "2583520",
    "end": "2589760"
  },
  {
    "text": "synthesize and answer questions for any domain so we've got two rag samples here",
    "start": "2589760",
    "end": "2595680"
  },
  {
    "text": "so one of them is rag on postgress so this is for the use case if you've got an existing database and you want to be",
    "start": "2595680",
    "end": "2603079"
  },
  {
    "text": "able to ask questions about that database and have the llm answer accurately based on that so for the",
    "start": "2603079",
    "end": "2609079"
  },
  {
    "text": "example uh you know database that I'm using I have product right so these this",
    "start": "2609079",
    "end": "2615800"
  },
  {
    "text": "is a chat on products so uh you know our table is storing all the products for this website so I can say okay what is",
    "start": "2615800",
    "end": "2623200"
  },
  {
    "text": "the best shoe for hiking so then it's going to go and",
    "start": "2623200",
    "end": "2629160"
  },
  {
    "text": "search the database rows and get back matching rows and then come back and say okay this blah blah blah blah blah blah",
    "start": "2629160",
    "end": "2635960"
  },
  {
    "text": "blah blah and it's going to includes citations so one of the key points of rag is to have citations so that users",
    "start": "2635960",
    "end": "2641800"
  },
  {
    "text": "can verify where the information come from and see that it's actually legit information and we can also look at the",
    "start": "2641800",
    "end": "2650319"
  },
  {
    "text": "uh the process for this rag flow here when we look on the the thought process",
    "start": "2650319",
    "end": "2655760"
  },
  {
    "text": "here and as as we actually you see is that this rag flow is a multi-step",
    "start": "2655760",
    "end": "2661640"
  },
  {
    "text": "process so the first process is actually what we call like the query rewriting phrase or the query clean phrase so",
    "start": "2661640",
    "end": "2668119"
  },
  {
    "text": "that's where we take the user's question and we ask the llm like hey here's a user question turn this into a good",
    "start": "2668119",
    "end": "2674440"
  },
  {
    "text": "search query because a lot a user question may not be that well formulated right like uh please tell me about the",
    "start": "2674440",
    "end": "2681160"
  },
  {
    "text": "best shoes for hiking now okay so you know there like a",
    "start": "2681160",
    "end": "2687160"
  },
  {
    "text": "user query and uh you know that's probably not the optimal search query",
    "start": "2687160",
    "end": "2692720"
  },
  {
    "text": "for uh for a search so if we look now at the thought process we can see that the",
    "start": "2692720",
    "end": "2697960"
  },
  {
    "text": "llm actually turned that whole long thing into best shoes for hiking so that's a better query uh so that's our",
    "start": "2697960",
    "end": "2705800"
  },
  {
    "text": "query rewrite phase so that's an llm call then we get back the resulting rows from the database and then this is our",
    "start": "2705800",
    "end": "2713160"
  },
  {
    "text": "call to the model that says hey you need to answer questions according to the sources here's how you should site your",
    "start": "2713160",
    "end": "2719280"
  },
  {
    "text": "sources here's the user question and then here is all the sources so this is",
    "start": "2719280",
    "end": "2725599"
  },
  {
    "text": "basically rag and then you know we're able to use it with different sorts of uh data data sources so that's rag on",
    "start": "2725599",
    "end": "2734480"
  },
  {
    "text": "postgress so you can get that set up following really similar steps to the to",
    "start": "2734480",
    "end": "2741440"
  },
  {
    "text": "the other one and you can even run that one locally first as well just on a local postgress database uh so here you can run the app",
    "start": "2741440",
    "end": "2749160"
  },
  {
    "text": "locally uh this one is a little more fancy because you've got a react front in there then you can deploy to azer",
    "start": "2749160",
    "end": "2755119"
  },
  {
    "text": "you're going to set similar VAR Ables and uh run it up so if you're interested in that you can start uh going through",
    "start": "2755119",
    "end": "2761839"
  },
  {
    "text": "those steps and then you can customize it the other kind of rag that we",
    "start": "2761839",
    "end": "2766880"
  },
  {
    "text": "have is rag on documents so if you're trying to ask questions about",
    "start": "2766880",
    "end": "2771960"
  },
  {
    "text": "unstructured documents like you've got a bunch of PDFs or word docs Excel files",
    "start": "2771960",
    "end": "2777040"
  },
  {
    "text": "uh anything like that you can actually put those into a search index and then",
    "start": "2777040",
    "end": "2782760"
  },
  {
    "text": "search that so the example we have for that is rag with a your AI search and uh it's a really really full",
    "start": "2782760",
    "end": "2789680"
  },
  {
    "text": "featured sample we've had it for the last like more than a year now and we've had thousands of developers deploy with",
    "start": "2789680",
    "end": "2795200"
  },
  {
    "text": "it and put it into production and so it's been used for a ton of use cases and it's got a lot of features uh Speech",
    "start": "2795200",
    "end": "2801440"
  },
  {
    "text": "voice Vision user access control lots of lots of cool things in it uh so let me",
    "start": "2801440",
    "end": "2806559"
  },
  {
    "text": "show that was the one I was actually showing earlier with my blog right so here's you know I made a version of it",
    "start": "2806559",
    "end": "2813079"
  },
  {
    "text": "that's just based off my blog posts and uh you know it can site my blog post I've also got this one here",
    "start": "2813079",
    "end": "2820960"
  },
  {
    "text": "which is for an internal company handbook which is a very popular way of using it as well and so you can see for",
    "start": "2820960",
    "end": "2826160"
  },
  {
    "text": "each of them we can you know click on the citations and uh and yeah so now this is",
    "start": "2826160",
    "end": "2832960"
  },
  {
    "text": "a bit more complicated because here we have a multi-page document so we've got a 31 page PDF we can't just send an",
    "start": "2832960",
    "end": "2839599"
  },
  {
    "text": "entire 31 page PDF to the llm because for a lot of our llms it's going to go beyond the context window right a lot of",
    "start": "2839599",
    "end": "2846520"
  },
  {
    "text": "our l m have a context window limit so typically that's around 8K 8,000 tokens",
    "start": "2846520",
    "end": "2852319"
  },
  {
    "text": "uh it can go up to 32k even 120k we're seeing um but typically they",
    "start": "2852319",
    "end": "2857800"
  },
  {
    "text": "do have some sort of context window and even if they don't have some sort of context window llms can get lost if you",
    "start": "2857800",
    "end": "2864240"
  },
  {
    "text": "give them too information too much information there's a research paper called Lost in the Middle where they did a study to see if they throw too much",
    "start": "2864240",
    "end": "2870680"
  },
  {
    "text": "information at an llm like at what point it stops paying attention so we generally want to send the llm the most",
    "start": "2870680",
    "end": "2876359"
  },
  {
    "text": "relevant chunks so what we do is that we first have this data ingestion phase",
    "start": "2876359",
    "end": "2881800"
  },
  {
    "text": "that will take a PDF or whatever document takes a document it extracts",
    "start": "2881800",
    "end": "2886839"
  },
  {
    "text": "all the text from it and we do that with Azure document intelligence which is very good at extracting text from all",
    "start": "2886839",
    "end": "2891920"
  },
  {
    "text": "sorts of documents so we extract the text from it we Chunk Up the text into",
    "start": "2891920",
    "end": "2897000"
  },
  {
    "text": "like good siiz chunks usually around 500 tokens each then we store each of those",
    "start": "2897000",
    "end": "2902200"
  },
  {
    "text": "chunks in the search index along with their embeddings and that's what we actually search on and send and then we",
    "start": "2902200",
    "end": "2908800"
  },
  {
    "text": "send right so if we look at the search results here we can actually see that the search results are just chunks from",
    "start": "2908800",
    "end": "2914480"
  },
  {
    "text": "the PDF where we say here's the chunk here's the embedding this is the page it",
    "start": "2914480",
    "end": "2919520"
  },
  {
    "text": "came from and this is the file it came from and we just send back those",
    "start": "2919520",
    "end": "2924800"
  },
  {
    "text": "chunks uh so this is the most complicated of our architectures because we do have to have that data ingestion",
    "start": "2924800",
    "end": "2930680"
  },
  {
    "text": "phase and that means we have to have a you know a script or a process that does that ingestion stage and you know here",
    "start": "2930680",
    "end": "2938160"
  },
  {
    "text": "we can do it locally or or in the [Music] cloud so those are the two rag samples",
    "start": "2938160",
    "end": "2944119"
  },
  {
    "text": "so we have um you know we have another 40 minutes so and we have like a good",
    "start": "2944119",
    "end": "2949599"
  },
  {
    "text": "ratio here of of helpers to y'all so if either of those sound compelling to you",
    "start": "2949599",
    "end": "2955960"
  },
  {
    "text": "like sound like a use case that you're interested in then uh you can try to deploy them now and see and see how they",
    "start": "2955960",
    "end": "2963440"
  },
  {
    "text": "work um so once again you just go to the app templates Workshop shop repo and you can either pick rag on postgress or rag",
    "start": "2963440",
    "end": "2970799"
  },
  {
    "text": "with AI search and then start going through through the steps to try it out",
    "start": "2970799",
    "end": "2976760"
  },
  {
    "text": "uh these will take longer to deploy so it's good to start the deploy now um because they take they've",
    "start": "2976760",
    "end": "2983599"
  },
  {
    "text": "got a lot more infrastructure to set up and then for the AI search it's got to do the whole ingestion step and that",
    "start": "2983599",
    "end": "2988880"
  },
  {
    "text": "ingestion step takes a certain amount of time as well so yeah any questions",
    "start": "2988880",
    "end": "2995640"
  },
  {
    "text": "before Su are you using any libraries for the chunking and all that stuff yeah",
    "start": "2995640",
    "end": "3001240"
  },
  {
    "text": "that's a great question are we using libraries so when this sample was first created it was like last April it was",
    "start": "3001240",
    "end": "3006319"
  },
  {
    "text": "before there was like really good established libraries we kind of used link chain but not heavily uh so all of",
    "start": "3006319",
    "end": "3012160"
  },
  {
    "text": "ours is it's actually custom coded um now if you're going to use a library the",
    "start": "3012160",
    "end": "3017599"
  },
  {
    "text": "big thing I would make sure you're doing is um using a token based chunker a lot",
    "start": "3017599",
    "end": "3023280"
  },
  {
    "text": "of the Splitters out there are doing character-based splitting which is probably fine if you're doing English",
    "start": "3023280",
    "end": "3028400"
  },
  {
    "text": "only documents but we do have lots of international customers and as soon as you start doing non-english documents",
    "start": "3028400",
    "end": "3034440"
  },
  {
    "text": "then you really want to do stuff based off of tokens and not characters because imagine you take like a Chinese document",
    "start": "3034440",
    "end": "3039960"
  },
  {
    "text": "and you you'd say like oh my chunks are a thousand characters long like that's a lot of tokens you you can like go over",
    "start": "3039960",
    "end": "3045760"
  },
  {
    "text": "the context window really fast so we have token based chunking that we've implanted here uh there are there is",
    "start": "3045760",
    "end": "3052079"
  },
  {
    "text": "token based chunking available in Lang chain so if you're going to use Lang chain um the thing to do is is find my",
    "start": "3052079",
    "end": "3058000"
  },
  {
    "text": "colleague's blog post where he talked about it okay yeah working with cjk",
    "start": "3058000",
    "end": "3064599"
  },
  {
    "text": "especially if you're doing anything non-english um he basically analyzed all the Splitters from linkchain um to",
    "start": "3064599",
    "end": "3072599"
  },
  {
    "text": "figure out which of them properly worked with token based splitting and with cjk",
    "start": "3072599",
    "end": "3079119"
  },
  {
    "text": "uh languages in particular um so we've implemented this oursel he actually to",
    "start": "3079119",
    "end": "3084319"
  },
  {
    "text": "my my manager Anthony he he worked on it um but uh Lan chain and llama index both",
    "start": "3084319",
    "end": "3091160"
  },
  {
    "text": "do a lot of this stuff uh they just you know they they take care behind the scenes so what you need is this you need",
    "start": "3091160",
    "end": "3097359"
  },
  {
    "text": "the splitting and you can get that from basically from link chain because llama index uses link chain so I would just",
    "start": "3097359",
    "end": "3103920"
  },
  {
    "text": "say use use l chain probably with this one so you can specify the chunk size",
    "start": "3103920",
    "end": "3110000"
  },
  {
    "text": "and then uh then you just have to vectorize so that that's easy you just use the open a SDK and we do the batch",
    "start": "3110000",
    "end": "3115920"
  },
  {
    "text": "embeddings with that um so that we can do a bunch at a time and then you just store it in AI search so the hard part",
    "start": "3115920",
    "end": "3122680"
  },
  {
    "text": "is really the uh extracting the text so there we either use as your document intelligence in the cloud uh or we do",
    "start": "3122680",
    "end": "3130520"
  },
  {
    "text": "have some local parsers too if somebody doesn't want to use document intelligence we use like Pi PDF uh we",
    "start": "3130520",
    "end": "3136400"
  },
  {
    "text": "use our own csb parser because that's straightforward uh HTML for my blog I",
    "start": "3136400",
    "end": "3142680"
  },
  {
    "text": "just use beautiful soup which is the python package that does html parsing right because I thought I could do a better job at it so this one I just used",
    "start": "3142680",
    "end": "3150000"
  },
  {
    "text": "beautiful soup to extract the text so uh so and that's so you can do that as well and we've got beautiful soup in there so",
    "start": "3150000",
    "end": "3157119"
  },
  {
    "text": "yeah there is actually a surprising amount of things that we've written ourselves for the AI search repo um if",
    "start": "3157119",
    "end": "3162160"
  },
  {
    "text": "we were going to do it today we'd probably use the Lang chain splitter at least yeah good question sry long answer",
    "start": "3162160",
    "end": "3169920"
  },
  {
    "text": "other questions",
    "start": "3169920",
    "end": "3173160"
  },
  {
    "text": "ABM I set",
    "start": "3184359",
    "end": "3190319"
  },
  {
    "text": "andice something else is it toile differences so generally with bicep what",
    "start": "3191839",
    "end": "3198400"
  },
  {
    "text": "it does is that it tries to figure out and bip is really compiled down to arm and arm is just Json so what what you're",
    "start": "3198400",
    "end": "3205880"
  },
  {
    "text": "actually doing is called an arm-based deployment so with arm-based deployments what they try to do is figure out what",
    "start": "3205880",
    "end": "3211520"
  },
  {
    "text": "does your resource currently look like what are you saying you want it to look like and what changes does it need to",
    "start": "3211520",
    "end": "3217319"
  },
  {
    "text": "make happen um so yeah we're like we'll probably switch over to AVM in a lot of",
    "start": "3217319",
    "end": "3222920"
  },
  {
    "text": "our samples and we're probably just going to make sure like it we're trying to make it not have a change but if you",
    "start": "3222920",
    "end": "3228559"
  },
  {
    "text": "want it to change then that's that's fine so you should totally be able to switch between AVM not AVM um as as you",
    "start": "3228559",
    "end": "3236119"
  },
  {
    "text": "decide as you see fit um and the important thing is just it'll figure out",
    "start": "3236119",
    "end": "3241319"
  },
  {
    "text": "the difference and just make sure you are on board with any changes that come up there is like so if you're doing um",
    "start": "3241319",
    "end": "3249280"
  },
  {
    "text": "there's this a deployment command that does what if and that tells you like actually tells you uh what resources",
    "start": "3249280",
    "end": "3255480"
  },
  {
    "text": "will change I want to figure out how we can do that with ACD I think ACD maybe has a dry run command so that might be",
    "start": "3255480",
    "end": "3262079"
  },
  {
    "text": "what we try when we consider switching to AVM because we want to switch to AVM",
    "start": "3262079",
    "end": "3267400"
  },
  {
    "text": "uh so that we don't have to maintain our own modules but we just want to make sure that we aware of any configuration changes that could",
    "start": "3267400",
    "end": "3275558"
  },
  {
    "text": "happen uh there just different things yeah addd is a command line tool that um",
    "start": "3281079",
    "end": "3287079"
  },
  {
    "text": "you know does the does the arm-based deployment and also does code deployment",
    "start": "3287079",
    "end": "3292640"
  },
  {
    "text": "code upload right so I have this a your. here I didn't show this so a your. says",
    "start": "3292640",
    "end": "3298400"
  },
  {
    "text": "this is the code that you're going to deploy to this host so addd does",
    "start": "3298400",
    "end": "3303960"
  },
  {
    "text": "multiple things it does um provisioning which is basically doing an arm-based deployment which is equivalent to if",
    "start": "3303960",
    "end": "3310240"
  },
  {
    "text": "you're doing [Music] a AZ deployment if if you know the Azure",
    "start": "3310240",
    "end": "3315720"
  },
  {
    "text": "CI it's this a deployment command um so it's doing that and then it's also doing",
    "start": "3315720",
    "end": "3321319"
  },
  {
    "text": "packaging and code deployment so um if you ever done like I don't know if you've ever done like web app up that's",
    "start": "3321319",
    "end": "3327359"
  },
  {
    "text": "where you deploy code up to app service ASD will also do that for us so ASD is",
    "start": "3327359",
    "end": "3332559"
  },
  {
    "text": "trying to do the whole workflow of you need to provision your resources and you need to deploy your code and we're",
    "start": "3332559",
    "end": "3338720"
  },
  {
    "text": "trying to make this Central way of doing it across all of our offerings because right now with Azure if you know Azure",
    "start": "3338720",
    "end": "3344960"
  },
  {
    "text": "but we've got like a billion different ways of doing things across all the different things and AD is trying to make a more common wave doing it so if",
    "start": "3344960",
    "end": "3351160"
  },
  {
    "text": "you look at my um my GitHub repo I'm kind of a huge fan girl so you can see",
    "start": "3351160",
    "end": "3357760"
  },
  {
    "text": "all of these repos are all ACD ified almost all uh that's what this ACD",
    "start": "3357760",
    "end": "3363119"
  },
  {
    "text": "column is because for me it's the best way to deploy because it's repeatable right um so if you are looking for",
    "start": "3363119",
    "end": "3369559"
  },
  {
    "text": "examples that I have quite a few here um but uh yeah so we're you know we should",
    "start": "3369559",
    "end": "3376480"
  },
  {
    "text": "be able to do it on different host container apps functions app service kubernetes Etc and uh you know with all",
    "start": "3376480",
    "end": "3383760"
  },
  {
    "text": "all this all the different possible bicep",
    "start": "3383760",
    "end": "3388240"
  },
  {
    "text": "uh other questions so what happens after go to",
    "start": "3389440",
    "end": "3394920"
  },
  {
    "text": "production like observability and all that good stu oh yeah great question um so we do have a like generally there's",
    "start": "3394920",
    "end": "3400960"
  },
  {
    "text": "lots of docks under AZ your search open AI demo so we do actually have a productionizing guide um you also ask",
    "start": "3400960",
    "end": "3408000"
  },
  {
    "text": "specifically about observability we do integrate with application insights with open Telemetry uh so that's what we use",
    "start": "3408000",
    "end": "3413720"
  },
  {
    "text": "by default if you want you could use Lang fuse I like to use l fuse I don't know if you've seen it but it's an observ platform so you could use Lang",
    "start": "3413720",
    "end": "3421039"
  },
  {
    "text": "fuse but by default we're using as your application insights with the open Telemetry packages to to bring",
    "start": "3421039",
    "end": "3426799"
  },
  {
    "text": "everything in there um but we do have a whole productionizing guide that talks about you know how are you going to scale things uh you know loow if you",
    "start": "3426799",
    "end": "3434240"
  },
  {
    "text": "need to load balance your open AI capacity um if you want to do v-net deployment if you want to do user off",
    "start": "3434240",
    "end": "3441160"
  },
  {
    "text": "how to do load testing so I've run quite a few load tests for this one and then how to do EV valuation so I you know I",
    "start": "3441160",
    "end": "3447799"
  },
  {
    "text": "like to do evaluation it's everyone should do it it's basically like the new form of text text uh testing for this",
    "start": "3447799",
    "end": "3455200"
  },
  {
    "text": "world uh let me see I think I closed my evaluation repo I can open it um but",
    "start": "3455200",
    "end": "3460680"
  },
  {
    "text": "basically like you want to be running evaluations to see if you are getting quality results from your llm H because",
    "start": "3460680",
    "end": "3469000"
  },
  {
    "text": "a lot of times you might run here's the thing you know I show those sample questions all the time and they perform great but you cannot trust your sample",
    "start": "3469000",
    "end": "3475839"
  },
  {
    "text": "questions and you can't even trust them like you might make a prompt tweak and be like oh this prompt tweak was so good I'm getting such good results you cannot",
    "start": "3475839",
    "end": "3482559"
  },
  {
    "text": "trust it you have to run an evaluation across a huge number of samples to make",
    "start": "3482559",
    "end": "3488400"
  },
  {
    "text": "sure that it's actually running like I run it across like 200 samples is probably like the minimum what you should do um but you have to run",
    "start": "3488400",
    "end": "3494640"
  },
  {
    "text": "evaluations in order to see do you I'm assuming you run evaluations for co-pilot chat right yeah how many",
    "start": "3494640",
    "end": "3500880"
  },
  {
    "text": "samples do you run off it's generated of public",
    "start": "3500880",
    "end": "3508119"
  },
  {
    "text": "oh it's cool you want to come up and like talk about evaluation because you're like you're I mean Harold is like",
    "start": "3510000",
    "end": "3516079"
  },
  {
    "text": "running an actual because basically you're making co-pilot chat here we go co-pilot chat which you can see I use it",
    "start": "3516079",
    "end": "3523119"
  },
  {
    "text": "a lot um so you're like you're like running a real",
    "start": "3523119",
    "end": "3528400"
  },
  {
    "text": "rag yeah so it's different Rags so so there's not just one rag so if you do at",
    "start": "3528400",
    "end": "3534440"
  },
  {
    "text": "workspace if you ever tried that in in Copa chat we actually run a local sparse",
    "start": "3534440",
    "end": "3540599"
  },
  {
    "text": "index so that's basically your classic how Google works just looking up words on the internet and documents and it's",
    "start": "3540599",
    "end": "3547280"
  },
  {
    "text": "faster we can do it locally so that will always work we also do a um schematic",
    "start": "3547280",
    "end": "3553280"
  },
  {
    "text": "index against index that github.com maintains and ranking those in using",
    "start": "3553280",
    "end": "3559920"
  },
  {
    "text": "another LM Co so R basically becomes a series of indexes like you do postest",
    "start": "3559920",
    "end": "3565079"
  },
  {
    "text": "you showed that you created some keywords up front based on the search so that's what we also do locally um but",
    "start": "3565079",
    "end": "3572319"
  },
  {
    "text": "yeah anytime we have changes we have a one test set that can run on each PR and",
    "start": "3572319",
    "end": "3578119"
  },
  {
    "text": "then a larger test set that we run daily that has a lot more repositories from",
    "start": "3578119",
    "end": "3583319"
  },
  {
    "text": "from across different languages so so you run the avows in the pr on every",
    "start": "3583319",
    "end": "3588839"
  },
  {
    "text": "yeah so we have a subset that's more unit test driven where it's like can it answer questions for this does it hit",
    "start": "3588839",
    "end": "3595440"
  },
  {
    "text": "any issues we've seen in the past so it's more more unit test style where it's like it doesn't behave as it did",
    "start": "3595440",
    "end": "3602160"
  },
  {
    "text": "before so yeah it's important though I mean that's that's the first the big biggest thing we invested on early on",
    "start": "3602160",
    "end": "3609000"
  },
  {
    "text": "because we found it so easy to get lost in prom crafting and assume how rag works and assume how it works in the",
    "start": "3609000",
    "end": "3615559"
  },
  {
    "text": "wild so in the local spars index is it sequ like uh it's tfidf uh it's yeah",
    "start": "3615559",
    "end": "3623520"
  },
  {
    "text": "yeah yeah so okay so let me show I don't know what your I don't know if you can like show",
    "start": "3623520",
    "end": "3629559"
  },
  {
    "text": "your vows but here like I can show a vows for um on the as your AI search um so let's see um all summary",
    "start": "3629559",
    "end": "3640200"
  },
  {
    "text": "summary okay all right so we'll look at them for like my blog uh let's see so",
    "start": "3640200",
    "end": "3646520"
  },
  {
    "text": "these are ones I've run before uh oh probably Pamela's blog",
    "start": "3646520",
    "end": "3654160"
  },
  {
    "text": "Pamela's blog yeah famous blog",
    "start": "3654160",
    "end": "3659720"
  },
  {
    "text": "results okay all right so these are a bunch of valuations that I've run fairly",
    "start": "3660039",
    "end": "3665559"
  },
  {
    "text": "recently right so um with these evaluations I do GPT metrics and then I",
    "start": "3665559",
    "end": "3670920"
  },
  {
    "text": "also do basically like regular expression metrics how are your metrics usually GPT metrics",
    "start": "3670920",
    "end": "3678160"
  },
  {
    "text": "or Code test Code test okay okay so with these GPT metrics what they're actually",
    "start": "3679160",
    "end": "3684799"
  },
  {
    "text": "doing is um sending the original answer",
    "start": "3684799",
    "end": "3689960"
  },
  {
    "text": "uh sending sending the ground truth answer uh which is generated syn synthetically and then also sending the",
    "start": "3689960",
    "end": "3696079"
  },
  {
    "text": "new answer to a llm and saying hey rate this from one to five and then we can",
    "start": "3696079",
    "end": "3701240"
  },
  {
    "text": "see the you know results and this is this is the actual prompt that gets sent is like okay you know rate this you know",
    "start": "3701240",
    "end": "3707680"
  },
  {
    "text": "from 1 to five here's some examples so we do that for groundedness we do that for relevance and then I also check",
    "start": "3707680",
    "end": "3712920"
  },
  {
    "text": "whether citations match across ground truth and not ground truth and that's actually my favorite metric and that's",
    "start": "3712920",
    "end": "3718400"
  },
  {
    "text": "just a Rex so my favorite is just this one this uh citation match here so I'm",
    "start": "3718400",
    "end": "3724079"
  },
  {
    "text": "just making sure that the answer contains the at least the citations that",
    "start": "3724079",
    "end": "3729480"
  },
  {
    "text": "were in the ground truth so I run metrics like this a lot of times I'm looking at retrieval parameters because",
    "start": "3729480",
    "end": "3734839"
  },
  {
    "text": "for rag the retrieval is makes a big difference so here I was comparing stuff like what if I use text only what if I",
    "start": "3734839",
    "end": "3741559"
  },
  {
    "text": "do Vector only what if I do hybrid what if I do hybrid with ranker uh and that's super interesting I was trying with",
    "start": "3741559",
    "end": "3747200"
  },
  {
    "text": "different retrieval amounts like if I retrieve five versus 10 versus three what do I get out uh what if I change a",
    "start": "3747200",
    "end": "3753160"
  },
  {
    "text": "prompt I to say like I've tried so many like tweaks on our prompt and I've never managed to actually get Improvement in",
    "start": "3753160",
    "end": "3759359"
  },
  {
    "text": "the overall stats uh so we still haven't ever changed the prompt because I I",
    "start": "3759359",
    "end": "3764640"
  },
  {
    "text": "haven't really proved that anything is sufficiently better or I'm just a really bad prompt engineer I don't know I've",
    "start": "3764640",
    "end": "3770680"
  },
  {
    "text": "none of my prompt engineering ever moves a needle for me the only thing that moves a needle is retrieval parameters",
    "start": "3770680",
    "end": "3776880"
  },
  {
    "text": "like how you're working with your search engine or changing the model entirely changing to gbd4 has a big difference",
    "start": "3776880",
    "end": "3784160"
  },
  {
    "text": "than gbd 35 uh so that should be part of your uh",
    "start": "3784160",
    "end": "3789720"
  },
  {
    "text": "your putting in production for sure is to make sure that you've got some sort of valuation set",
    "start": "3789720",
    "end": "3795759"
  },
  {
    "text": "up any other questions is anyone trying to get one of",
    "start": "3797799",
    "end": "3803240"
  },
  {
    "text": "the rags up any getting Rags up",
    "start": "3803240",
    "end": "3807760"
  },
  {
    "text": "I try to like take a lot of things to production but they were all like throw away toy projects fast API and like had",
    "start": "3809160",
    "end": "3816440"
  },
  {
    "text": "a hard time evaluating like which weor store to use and how much",
    "start": "3816440",
    "end": "3823599"
  },
  {
    "text": "to you should be testing this yeah so yeah I mean it's hard that's part of why",
    "start": "3828960",
    "end": "3834000"
  },
  {
    "text": "I I do it as well I've also run those on our sample data too but I think it what I've discovered is really helps to run",
    "start": "3834000",
    "end": "3840400"
  },
  {
    "text": "the evaluations on stuff that you know because then like because this is the summary like you can kind of look at the summary and be like okay I guess like",
    "start": "3840400",
    "end": "3847839"
  },
  {
    "text": "things better but then what I usually look at is like I actually look at the changes between um between two runs and",
    "start": "3847839",
    "end": "3853839"
  },
  {
    "text": "be like okay well what was the difference between uh the Baseline and then uh you know the maybe what was it",
    "start": "3853839",
    "end": "3862160"
  },
  {
    "text": "Vector only Vector no ranker okay and then I'll just look at things that",
    "start": "3862160",
    "end": "3867760"
  },
  {
    "text": "changed on citation match okay so this is what I usually do is I look at the",
    "start": "3867760",
    "end": "3873079"
  },
  {
    "text": "overall stuff and then I look and I compare the answers across my ground truth and the and the new one with the",
    "start": "3873079",
    "end": "3880160"
  },
  {
    "text": "parameters and so then I can better reason about it but you really have to know your domain in order to be able to",
    "start": "3880160",
    "end": "3887480"
  },
  {
    "text": "evaluate evaluate your evaluations um but it also helps if",
    "start": "3887480",
    "end": "3893119"
  },
  {
    "text": "other people have run it for you so this is a really good blog post from the AI search team that I always reference",
    "start": "3893119",
    "end": "3898480"
  },
  {
    "text": "where they ran massive queries looking at hybrid search versus Vector search versus Tech search and um you know and",
    "start": "3898480",
    "end": "3905640"
  },
  {
    "text": "they found that hybrid retrieval with semantic ranking outperforms spor only search so I ran my own versions of that",
    "start": "3905640",
    "end": "3913000"
  },
  {
    "text": "and um and recently uh blogged about it but it's basically the stats that I was just showing where uh what I found",
    "start": "3913000",
    "end": "3920000"
  },
  {
    "text": "actually for my use case Vector on its own did horribly like really really",
    "start": "3920000",
    "end": "3925760"
  },
  {
    "text": "really really badly uh where is it um so Vector only got a groundedness of 2.79",
    "start": "3925760",
    "end": "3932440"
  },
  {
    "text": "which is really really low text only got 4.87 so part of that is because a your AI search is really good at full text",
    "start": "3932440",
    "end": "3939200"
  },
  {
    "text": "search like incredibly good at it it does all the spell checks stemming everything you could imagine um hybrid",
    "start": "3939200",
    "end": "3945279"
  },
  {
    "text": "which is where you take vector and text and then you merge them using this algorithm called recipal rank Fusion",
    "start": "3945279",
    "end": "3951400"
  },
  {
    "text": "which you can actually just see the algorithm is just this it's just uh you're just doing a little math here to",
    "start": "3951400",
    "end": "3956599"
  },
  {
    "text": "combine uh rank scores um so just a basic hybrid like that the groundedness",
    "start": "3956599",
    "end": "3961760"
  },
  {
    "text": "is only 3.26 so you can see hybrid on its own is worse than text only and",
    "start": "3961760",
    "end": "3967200"
  },
  {
    "text": "that's because Vector results can add so much noise you accidentally grab the wrong like distracting things uh what I",
    "start": "3967200",
    "end": "3974440"
  },
  {
    "text": "found actually is like if I ever accidentally vectorize like an empty string or something close to an empty",
    "start": "3974440",
    "end": "3979880"
  },
  {
    "text": "string is similar to everything I don't know what this is about the opening eye embedding space but if you actually ly",
    "start": "3979880",
    "end": "3985799"
  },
  {
    "text": "vectorized an empty string or even like uh we have Vision as a feature in the",
    "start": "3985799",
    "end": "3990880"
  },
  {
    "text": "aure open Ice search demo if you I was helping a customer this this week and",
    "start": "3990880",
    "end": "3996480"
  },
  {
    "text": "they were finding that so many of the results were getting this blank blue page because apparently this blank blue",
    "start": "3996480",
    "end": "4002839"
  },
  {
    "text": "page the vector for it and this is a vector via a different model the AZ your computer vision model the vector for it",
    "start": "4002839",
    "end": "4008319"
  },
  {
    "text": "was just matching everything so you gotta like you got to be really careful with Vector spaces um it's so easy to",
    "start": "4008319",
    "end": "4015760"
  },
  {
    "text": "accidentally add noise to them and for there to be distractions so hybrid on its own only you know got like 3.26 once",
    "start": "4015760",
    "end": "4022760"
  },
  {
    "text": "I use hybrid with semantic ranker then I got the best results but only by a couple percentage points now hybrid is",
    "start": "4022760",
    "end": "4029200"
  },
  {
    "text": "semantic ranker semantic ranker that's a feature of azure AI search which is actually another machine learning model",
    "start": "4029200",
    "end": "4034799"
  },
  {
    "text": "it's a it's called a cross- encoder model but basically they actually had humans rank results according to queries",
    "start": "4034799",
    "end": "4040160"
  },
  {
    "text": "they use it for B so they said hey humans here's 10 search results for a query rank these from one to 10 and tell",
    "start": "4040160",
    "end": "4046119"
  },
  {
    "text": "us what's the best so they train a whole model based off a bunch of human data and then they get back like this this uh",
    "start": "4046119",
    "end": "4051960"
  },
  {
    "text": "this model that they can then use for any arbitrary uh ranking of user query along with results so basically hybrid",
    "start": "4051960",
    "end": "4059440"
  },
  {
    "text": "with being ranker gets you the best but if I was going to have to like if I was on a desert island and like I I could pick between vector and text uh I would",
    "start": "4059440",
    "end": "4067400"
  },
  {
    "text": "use text at least for as your AI search it's going to depend how good your full Tech search right if you're doing fulltech search with like SQL light",
    "start": "4067400",
    "end": "4073920"
  },
  {
    "text": "which I don't even know if supports full search it's not going to do well very well yeah so what's actually that full",
    "start": "4073920",
    "end": "4080160"
  },
  {
    "text": "Tex search TF IDE um you so you're using tiffi for your co-pilot chat you said",
    "start": "4080160",
    "end": "4089160"
  },
  {
    "text": "right for this one for this for the at workspace right yeah um for for as your",
    "start": "4089160",
    "end": "4095799"
  },
  {
    "text": "AI search they're using uh they're using several things but they're using one of the things they uses Lucine which is um",
    "start": "4095799",
    "end": "4103318"
  },
  {
    "text": "a search Library and it's got stuff like spell cheing and tokenization and stuff",
    "start": "4103319",
    "end": "4109679"
  },
  {
    "text": "like that um so they're doing a lot but I guess and they're also using bm25",
    "start": "4109679",
    "end": "4115798"
  },
  {
    "text": "which I think is basically Tiff a right okay yeah so bm25 that's what you want to look for um is uh oh we got a search",
    "start": "4115799",
    "end": "4125000"
  },
  {
    "text": "result here yeah so if it's if something is using bm25 I think that's basically",
    "start": "4125000",
    "end": "4130278"
  },
  {
    "text": "the best for full text right now so um that's what you want to look for is you",
    "start": "4130279",
    "end": "4135560"
  },
  {
    "text": "just want to look for a good full text option yeah yeah is overwhelming that's",
    "start": "4135560",
    "end": "4142080"
  },
  {
    "text": "why I love when you know people put out research so we be like okay great because this also has the optimal um",
    "start": "4142080",
    "end": "4147480"
  },
  {
    "text": "chunk size that's why I was saying we do 500 tokens because they did them they did the the work here and said okay the",
    "start": "4147480",
    "end": "4153960"
  },
  {
    "text": "optimal is 512 tokens great that's what we're going to use now obviously for your particular use case it can be",
    "start": "4153960",
    "end": "4160159"
  },
  {
    "text": "different but we can't all run like 2000 different tests to see what the optimal",
    "start": "4160159",
    "end": "4165838"
  },
  {
    "text": "you know thing is so it's really nice when people you know document what worked well for",
    "start": "4165839",
    "end": "4171798"
  },
  {
    "text": "them cool anything",
    "start": "4171799",
    "end": "4177400"
  },
  {
    "text": "else how often do you like update the vectors like if you have a lot",
    "start": "4177759",
    "end": "4183679"
  },
  {
    "text": "of like data that gets updated a lot how often do you choose to update the",
    "start": "4183679",
    "end": "4189960"
  },
  {
    "text": "vectors or well we only need to update the vectors if the data changes or if",
    "start": "4189960",
    "end": "4195880"
  },
  {
    "text": "we're changing our embedding model so if we change our embedding model we have to update everything to use a new embedding",
    "start": "4195880",
    "end": "4201199"
  },
  {
    "text": "model right because now opening eye has these new embedding models I need to do some tests with them to see if I can get",
    "start": "4201199",
    "end": "4207280"
  },
  {
    "text": "like better results for them um so in that case I would I would rerun everything so probably what I want to do",
    "start": "4207280",
    "end": "4213440"
  },
  {
    "text": "is set up like an a a separate AI search index like for this one which has uses",
    "start": "4213440",
    "end": "4219679"
  },
  {
    "text": "one of the new embedding models uh embedding 3 and I have to decide how many dimensions to use um and then",
    "start": "4219679",
    "end": "4227120"
  },
  {
    "text": "compare it to see how much better results are I'm told that generally the results are better but I'm try have you",
    "start": "4227120",
    "end": "4233000"
  },
  {
    "text": "tried any of them switching oh you're switching to the new one what dimension are you going to",
    "start": "4233000",
    "end": "4238120"
  },
  {
    "text": "use small prob so you're going to use small 256 wow 512 does that yeah you can do",
    "start": "4238120",
    "end": "4246000"
  },
  {
    "text": "512 too yeah yeah so you can that's the thing is it it's so it's so many options now a test yeah you get a",
    "start": "4246000",
    "end": "4252760"
  },
  {
    "text": "test yeah oh and you can run you have customers yeah um so yeah but you're",
    "start": "4252760",
    "end": "4259080"
  },
  {
    "text": "going to have to reindex everything um so that's that's when you",
    "start": "4259080",
    "end": "4265760"
  },
  {
    "text": "would uh have to update stuff is it the content changes or if the model changes",
    "start": "4265760",
    "end": "4272800"
  },
  {
    "text": "um and then and then test that yeah I do want to try out the new ones they should redo this one",
    "start": "4272800",
    "end": "4280679"
  },
  {
    "text": "too there's too many decisions cool any other questions",
    "start": "4280679",
    "end": "4287040"
  },
  {
    "text": "Harold do you want to show stuff in",
    "start": "4287040",
    "end": "4290560"
  },
  {
    "text": "workspace close out of",
    "start": "4292600",
    "end": "4296199"
  },
  {
    "text": "this so see Rag and action um so if you ask a question in in co-pilot chat so",
    "start": "4298880",
    "end": "4306719"
  },
  {
    "text": "that's the co-pilot chat panel version there's also another one that's in line",
    "start": "4306719",
    "end": "4312040"
  },
  {
    "text": "so if you open up this is a natural and put we call inline chat in in the in your code",
    "start": "4312040",
    "end": "4319600"
  },
  {
    "text": "basically let you apply code directly or natural language directly to a code which is always niiz you don't have to",
    "start": "4319600",
    "end": "4325440"
  },
  {
    "text": "think about the response you have to think about just you know what you want and you want a AI to do it for you but",
    "start": "4325440",
    "end": "4331400"
  },
  {
    "text": "in in a side panel most of the time what you will run into is this you're going",
    "start": "4331400",
    "end": "4336440"
  },
  {
    "text": "to run things let's pick a function these are",
    "start": "4336440",
    "end": "4342159"
  },
  {
    "text": "tests well these are test so comp the tests now I've have code selected on",
    "start": "4342159",
    "end": "4348920"
  },
  {
    "text": "the right and on the left I can ask things about the code I have select and that's the Surefire way to get good",
    "start": "4348920",
    "end": "4356239"
  },
  {
    "text": "results have code selected and talk about it and you already see that that we do some magic in our responses so",
    "start": "4356239",
    "end": "4363159"
  },
  {
    "text": "everything is code highlighted so actually you jump to the different aspects um that that are being used and",
    "start": "4363159",
    "end": "4371159"
  },
  {
    "text": "even to dependencies so it found that there's a dependency so you can also jump to that so now going back to here let's",
    "start": "4371159",
    "end": "4378920"
  },
  {
    "text": "let's see which tests actually are defined in a repository and because there I want to talk basically about the",
    "start": "4378920",
    "end": "4384480"
  },
  {
    "text": "whole workspace and that's why I can just say which tests are defined or how are bench",
    "start": "4384480",
    "end": "4394159"
  },
  {
    "text": "marks being run so a general question that you would go otherwise to a colleague who hopefully knows this and",
    "start": "4394159",
    "end": "4400960"
  },
  {
    "text": "hopefully they're in the same time zone and they know this but now I can actually send this to add workspace and",
    "start": "4400960",
    "end": "4406639"
  },
  {
    "text": "that's where we kicking in this this whole rag agents scheme so this R is",
    "start": "4406639",
    "end": "4412000"
  },
  {
    "text": "probably not index on the github.com site so if you're in copile Enterprise you will get a semantic index that",
    "start": "4412000",
    "end": "4417880"
  },
  {
    "text": "GitHub keeps updating for you they also have a few open source repost indexed",
    "start": "4417880",
    "end": "4423440"
  },
  {
    "text": "but in this case this is all happening now in vs code itself so this is mostly sparse indexing and actually we we see",
    "start": "4423440",
    "end": "4430080"
  },
  {
    "text": "that sparse indexing is usually on par similar to what you see with the text uh based",
    "start": "4430080",
    "end": "4435920"
  },
  {
    "text": "that this works um really well that of the yeah yeah so just basically fig out",
    "start": "4435920",
    "end": "4446440"
  },
  {
    "text": "all yeah so first we do same as you have an um Azure search where it does uh find",
    "start": "4446440",
    "end": "4454040"
  },
  {
    "text": "more words for what you're potentially looking for that are fitting with the repository so we also do stemming and uh",
    "start": "4454040",
    "end": "4461239"
  },
  {
    "text": "but that's one that's the first LM call then the t5f will find all the results",
    "start": "4461239",
    "end": "4467320"
  },
  {
    "text": "and then we do the reranking on top so and that actually gets us actually mostly better than doing a full Vector",
    "start": "4467320",
    "end": "4474440"
  },
  {
    "text": "search on the same topic how do you do the uh the different ways so we that's",
    "start": "4474440",
    "end": "4481159"
  },
  {
    "text": "where we experiment a lot um but it's another GPT 3.5 call I think oh okay so",
    "start": "4481159",
    "end": "4487120"
  },
  {
    "text": "use the llm as a reer yeah and you see what what's being",
    "start": "4487120",
    "end": "4492639"
  },
  {
    "text": "pulled in so these are all the things that found and the chunks it founded in so what we do what you'll see is we",
    "start": "4492639",
    "end": "4498719"
  },
  {
    "text": "actually do schematic chunking so for most languages we look",
    "start": "4498719",
    "end": "4504159"
  },
  {
    "text": "at function segments we look at specific blocks of code and that's where we found the most impact as well so people",
    "start": "4504159",
    "end": "4510560"
  },
  {
    "text": "brought up chunking is a big big area of uh improvements and that's what we also",
    "start": "4510560",
    "end": "4516320"
  },
  {
    "text": "have in our code that it's the chunking is the biggest impact i f",
    "start": "4516320",
    "end": "4522199"
  },
  {
    "text": "from yeah um which helps that we have all the languages the knowledge around the team",
    "start": "4522199",
    "end": "4528800"
  },
  {
    "text": "like python right so uh but yeah so that's that's the basics and you'll see that it works",
    "start": "4528800",
    "end": "4536280"
  },
  {
    "text": "everywhere that's the nice part it works locally and it works slightly faster if you already have an online index where",
    "start": "4536280",
    "end": "4542600"
  },
  {
    "text": "we can retrieve the schematic index",
    "start": "4542600",
    "end": "4548040"
  },
  {
    "text": "from in action questions",
    "start": "4548159",
    "end": "4555360"
  },
  {
    "text": "what is the prompty oh so prompty is a it's a new um",
    "start": "4555360",
    "end": "4564080"
  },
  {
    "text": "prompt uh format you can show the promp file",
    "start": "4564080",
    "end": "4570120"
  },
  {
    "text": "one yeah all those dot ones those ones yeah uh so this was announced",
    "start": "4570480",
    "end": "4576679"
  },
  {
    "text": "at build build build yeah y build scroll up to the top of it yeah there you go",
    "start": "4576679",
    "end": "4586760"
  },
  {
    "text": "so it's a way of it's like an artifact for prompts because right now like you",
    "start": "4588440",
    "end": "4593719"
  },
  {
    "text": "might store your prompt as a multi-line string variable and we",
    "start": "4593719",
    "end": "4600080"
  },
  {
    "text": "store them in all kinds of formats across the repo so this is like a standard way so it's actually a ginger template plus this yaml at the top so",
    "start": "4600080",
    "end": "4608560"
  },
  {
    "text": "the yaml describes the metadata of the prompt and then the ginger template you know it's template that you can pass",
    "start": "4608560",
    "end": "4613840"
  },
  {
    "text": "things into uh so uh this is used by prompt flow but it's also used by a your AI studio and",
    "start": "4613840",
    "end": "4622159"
  },
  {
    "text": "the goal is and I think maybe link chain might have some for it now or soon but",
    "start": "4622159",
    "end": "4627560"
  },
  {
    "text": "the goal is just to have a common way of representing promps so we'll probably try to use this in more of our",
    "start": "4627560",
    "end": "4634120"
  },
  {
    "text": "stuff there you go just ask",
    "start": "4634400",
    "end": "4639600"
  },
  {
    "text": "co-pilot yeah so I'm using this is using the prom flow",
    "start": "4641159",
    "end": "4646440"
  },
  {
    "text": "package um which has a bunch more things as well other kinds of evaluation uh they actually I wrote my",
    "start": "4646440",
    "end": "4653639"
  },
  {
    "text": "own C UI on top of this but they have they have on to that you can use",
    "start": "4653639",
    "end": "4661040"
  },
  {
    "text": "yeah",
    "start": "4661600",
    "end": "4664600"
  },
  {
    "text": "CR your CI pipeline somewhere yeah if you look at as your",
    "start": "4667920",
    "end": "4673280"
  },
  {
    "text": "Des this one it does um so I run them right now I'm just running them as a smoke test for this REO um but uh you",
    "start": "4673280",
    "end": "4680280"
  },
  {
    "text": "can see what I've done is that I have a Target Ur So that's generally what you want to do is you you need to run the EV",
    "start": "4680280",
    "end": "4687199"
  },
  {
    "text": "valve against your your your live or like for you you're doing PR build so",
    "start": "4687199",
    "end": "4692560"
  },
  {
    "text": "there you want to run it against your y PR build so the tricky thing is just making sure you have a way of contacting",
    "start": "4692560",
    "end": "4699360"
  },
  {
    "text": "your thing with everything all the you know production set up right all the aure stuff in it",
    "start": "4699360",
    "end": "4705719"
  },
  {
    "text": "um so uh yeah I want to I would ideally have it as a CI step for every one of",
    "start": "4705719",
    "end": "4711239"
  },
  {
    "text": "our repos and I'm just figuring out the right way of setting up like The Target",
    "start": "4711239",
    "end": "4716800"
  },
  {
    "text": "URL and all that stuff especially if you because most people aren't making public facing app most people are either",
    "start": "4716800",
    "end": "4722679"
  },
  {
    "text": "putting it behind user op or putting it in a V so we need evaluation flows that",
    "start": "4722679",
    "end": "4729520"
  },
  {
    "text": "both can use your production resources because that's how you know it's working uh but also work with however your app",
    "start": "4729520",
    "end": "4736560"
  },
  {
    "text": "is deployed so I think you can certainly figure out how to set it up for your situation uh I'm still figuring out how",
    "start": "4736560",
    "end": "4743320"
  },
  {
    "text": "to set it up in the general case but the thing to keep in mind is",
    "start": "4743320",
    "end": "4748679"
  },
  {
    "text": "that evaluations are slow if you're doing gbd metrics right or I mean generally they're slow because all of",
    "start": "4748679",
    "end": "4754520"
  },
  {
    "text": "these calls are slow you saw how much time it took to get back a response right so generally they're slow they're much slower than traditional unit tests",
    "start": "4754520",
    "end": "4760560"
  },
  {
    "text": "so you do not want to casually run an evaluation they're also expensive especially if you're doing well first",
    "start": "4760560",
    "end": "4766360"
  },
  {
    "text": "because the llm calls happening behind the scenes and if you're using GD metrics because I'm doing all these GD metrics like relevance and groundedness",
    "start": "4766360",
    "end": "4773000"
  },
  {
    "text": "that's another llm call so you want to have like a higher barrier to running",
    "start": "4773000",
    "end": "4778159"
  },
  {
    "text": "than with normal unit test right and caching caching oh you cach how do you",
    "start": "4778159",
    "end": "4784159"
  },
  {
    "text": "cach how do you know that something hasn't changed based on a prompt and the test yeah yeah I guess yeah if you're if",
    "start": "4784159",
    "end": "4791080"
  },
  {
    "text": "it's all within one repo this one is like a repo that works with other rep you don't know if the app is changed behind the scenes uh but yeah if you",
    "start": "4791080",
    "end": "4799600"
  },
  {
    "text": "yeah so caching you do caching that's good what exactly so we look at each",
    "start": "4799600",
    "end": "4805480"
  },
  {
    "text": "test and we only rerun them when any of the prompts when the input basically change so if you imagine like an open a",
    "start": "4805480",
    "end": "4811280"
  },
  {
    "text": "proxy that you could set up if if if it's the same similar to what they do I think open AI has like the seed variable",
    "start": "4811280",
    "end": "4819159"
  },
  {
    "text": "which is basically caching but it don't tell you um and it's basically if nothing changes in a prom it just sends",
    "start": "4819159",
    "end": "4825199"
  },
  {
    "text": "back the old response oh so you implementing cashing in co-pilot chat",
    "start": "4825199",
    "end": "4830639"
  },
  {
    "text": "you mean or not in Co in in our testing infrastructure some people also Implement caching in the rag application",
    "start": "4830639",
    "end": "4837159"
  },
  {
    "text": "itself or in the maybe or something I don't know how often you're going to get",
    "start": "4837159",
    "end": "4843320"
  },
  {
    "text": "the same question for like tests it helps for for test always s yeah yeah",
    "start": "4843320",
    "end": "4849199"
  },
  {
    "text": "yeah yeah I haven't I haven't can this also like is this just",
    "start": "4849199",
    "end": "4856880"
  },
  {
    "text": "for open AI can this work with mol and all the other ones me from the beginning",
    "start": "4856880",
    "end": "4861960"
  },
  {
    "text": "uh yeah I mean mine just this one I just hit up the URL and get back the answer",
    "start": "4861960",
    "end": "4867199"
  },
  {
    "text": "so the URL is just of your deployed app oh sorry I me like the starter",
    "start": "4867199",
    "end": "4872480"
  },
  {
    "text": "templates oh yeah good question so with the starter templates uh right now they're all configured with open Ai and",
    "start": "4872480",
    "end": "4878719"
  },
  {
    "text": "so you can swap out like different open AI models to G4 but uh they don't work",
    "start": "4878719",
    "end": "4884239"
  },
  {
    "text": "with new um non open models because we can't necessarily use the open AI SDK",
    "start": "4884239",
    "end": "4890880"
  },
  {
    "text": "with them I think there is actually way to use opening with them but we're supposed to pretend we can't um so there",
    "start": "4890880",
    "end": "4897520"
  },
  {
    "text": "is this new SDK and I haven't I haven't messed with it yeah I don't know if you have but as your AI inference have you",
    "start": "4897520",
    "end": "4903880"
  },
  {
    "text": "seen it um I think this is the new unified SDK and um yeah so this is this is what",
    "start": "4903880",
    "end": "4912679"
  },
  {
    "text": "to use for everything that's not open AI oh it says it can even do open",
    "start": "4912679",
    "end": "4918120"
  },
  {
    "text": "AI so we might have to port to this the thing I don't love about this is that this is as your specific because right",
    "start": "4918120",
    "end": "4923480"
  },
  {
    "text": "now we use the opening istd which is like not as your specific exactly um and",
    "start": "4923480",
    "end": "4929120"
  },
  {
    "text": "so it works with like a Lama and stuff I don't know so but we might end up porting for this so if we ported to this",
    "start": "4929120",
    "end": "4934360"
  },
  {
    "text": "then probably we would just it would just work um with everything so this is",
    "start": "4934360",
    "end": "4939560"
  },
  {
    "text": "really new like this came out at build so we just have to decide whether to Port everything up to this out to this",
    "start": "4939560",
    "end": "4946000"
  },
  {
    "text": "so that we can um use all the all the modules all the",
    "start": "4946000",
    "end": "4952239"
  },
  {
    "text": "models yeah everything changes all the",
    "start": "4952239",
    "end": "4957559"
  },
  {
    "text": "time yeah but we would also need to make the bicep for it that's the other thing I haven't done is I haven't because I",
    "start": "4958360",
    "end": "4964760"
  },
  {
    "text": "try to set up bicep for everything so typically bicep creates Your Eyes Are Open AI instance if you were using",
    "start": "4964760",
    "end": "4970400"
  },
  {
    "text": "mistol or llama you would want bicep to You' probably want Bice to create that as well um and so that would be",
    "start": "4970400",
    "end": "4977639"
  },
  {
    "text": "different bicep as an addition so we'll probably end up adding it cuz basically",
    "start": "4977639",
    "end": "4982960"
  },
  {
    "text": "what you do is you go to the issue tracker you file a request and then if enough people ask for it we're like okay",
    "start": "4982960",
    "end": "4988639"
  },
  {
    "text": "guess we're going to do it then we can yeah yeah um but that's how we figure",
    "start": "4988639",
    "end": "4994800"
  },
  {
    "text": "out what you know what it is that people are looking for because it is really nice to be able to swap out models because right now all of the samples do",
    "start": "4994800",
    "end": "5001080"
  },
  {
    "text": "work with AMA so if you have like uh Ama running locally here's my little llama up there um you know you can run like 53",
    "start": "5001080",
    "end": "5008760"
  },
  {
    "text": "and stuff like that uh you just you you go to your terminal you're like Lama is this it I think I don't know if I type",
    "start": "5008760",
    "end": "5015880"
  },
  {
    "text": "by3 correct but um and so they do all run with uh AMA things but none of the",
    "start": "5015880",
    "end": "5022960"
  },
  {
    "text": "olama models have really been sufficient for rag in my experience like I I run",
    "start": "5022960",
    "end": "5028159"
  },
  {
    "text": "them just to check but they they all fail to get um follow directions in my",
    "start": "5028159",
    "end": "5033719"
  },
  {
    "text": "experience uh because I just think they're not they don't have enough parameters like these are like 3B 7B Etc",
    "start": "5033719",
    "end": "5039199"
  },
  {
    "text": "so um they don't provide citations correctly I don't know have you had more success with like 53 mini you see every",
    "start": "5039199",
    "end": "5046760"
  },
  {
    "text": "model requires prompt changes might be yeah and I'm bad at prompt engineering yeah so so out the gate like I haven't",
    "start": "5046760",
    "end": "5054440"
  },
  {
    "text": "had success using any of the small language models for rag I'm sure the big",
    "start": "5054440",
    "end": "5059480"
  },
  {
    "text": "versions of them would work much better so I do want to try out like the 70b 7",
    "start": "5059480",
    "end": "5065120"
  },
  {
    "text": "yeah I've done I've done up to 7B CU that's like how far I can go up locally I can't go much more than that just for",
    "start": "5065120",
    "end": "5071080"
  },
  {
    "text": "space reasons so for them what happens is that like they'll answer the questions fine the issue is that we need",
    "start": "5071080",
    "end": "5077320"
  },
  {
    "text": "citations to be in good format because these are actually comeback as bracketed square brackets and they just don't",
    "start": "5077320",
    "end": "5083199"
  },
  {
    "text": "reliably come back with square bracketed citations which doesn't sound like a big deal but like we're trying to make",
    "start": "5083199",
    "end": "5088560"
  },
  {
    "text": "clickable citations here uh so that's the issue I've had is that I I think they're fine at synthesizing the",
    "start": "5088560",
    "end": "5093960"
  },
  {
    "text": "information but they don't follow the syntax directions in terms of the citations um and they're kind of more",
    "start": "5093960",
    "end": "5102280"
  },
  {
    "text": "maybe more likely to uh make stuff up if I ask an off-topic question that's been",
    "start": "5102280",
    "end": "5107600"
  },
  {
    "text": "my experience there um good for reranking I think finding spot reing or",
    "start": "5107600",
    "end": "5113920"
  },
  {
    "text": "like the yeah tpd judging I think that that's where like finding this one thing",
    "start": "5113920",
    "end": "5119280"
  },
  {
    "text": "maybe not like the expert full answers that that follows to format but like",
    "start": "5119280",
    "end": "5125000"
  },
  {
    "text": "one of the small tests but you can't so most of them don't support function calling off the bat so you're doing",
    "start": "5125000",
    "end": "5130880"
  },
  {
    "text": "would you do reranking with just a simple you'd have to figure out what syntax they come back with but they're",
    "start": "5130880",
    "end": "5136480"
  },
  {
    "text": "all good at coding so they can yeah that's right that's true everyone's good at coding if you can turn something into a coding task yeah you're",
    "start": "5136480",
    "end": "5144920"
  },
  {
    "text": "good yeah that's another form of rag is like uh I was telling someone last time like these are these are all like doing",
    "start": "5145199",
    "end": "5152159"
  },
  {
    "text": "like kind of rag on just a few documents at a time if you're trying to like analyze a whole database or like a huge number of",
    "start": "5152159",
    "end": "5158600"
  },
  {
    "text": "documents then you really want to like actually uh use like a SQL query like with like aggregate functions or do like",
    "start": "5158600",
    "end": "5165239"
  },
  {
    "text": "a pandis query right so at Pyon we did a demo where you like upload a CSV and then you you say like oh I want to count",
    "start": "5165239",
    "end": "5172000"
  },
  {
    "text": "the top restaurants in it and then it just comes up with the pandis code and then it runs the pandis code in a",
    "start": "5172000",
    "end": "5177920"
  },
  {
    "text": "sandbox environment so that's that's another like mean increasingly common U",
    "start": "5177920",
    "end": "5183520"
  },
  {
    "text": "form of rag where if you want to like come up with insights and Analysis and and that sort",
    "start": "5183520",
    "end": "5188920"
  },
  {
    "text": "of thing then you want to consider a different architecture where you're actually going to have the llm generate pandis code or SQL code it's very good",
    "start": "5188920",
    "end": "5195760"
  },
  {
    "text": "at both of those and then run those in a safe",
    "start": "5195760",
    "end": "5200360"
  },
  {
    "text": "way don't",
    "start": "5211280",
    "end": "5215280"
  },
  {
    "text": "yeah do are you actually using type chat because that's basically what type chat does okay okay so yeah but what you're",
    "start": "5220440",
    "end": "5228080"
  },
  {
    "text": "describing that's the same the same approach I've seen a",
    "start": "5228080",
    "end": "5233679"
  },
  {
    "text": "l yeah so that would be yeah we we're trying um I know Daniel's actually experimenting with type Daniel's the",
    "start": "5234040",
    "end": "5240280"
  },
  {
    "text": "creator of typ chat he is experimenting with typ chat with the local models um um so trying because we tried we did",
    "start": "5240280",
    "end": "5247360"
  },
  {
    "text": "also try type chat with 53 locally to see if we could use it in instead of",
    "start": "5247360",
    "end": "5253280"
  },
  {
    "text": "function calling with open Ai and we were having a hard time with it um but I",
    "start": "5253280",
    "end": "5259080"
  },
  {
    "text": "think Daniel like maybe has to like tweet the prompts and maybe they'll end up working",
    "start": "5259080",
    "end": "5264639"
  },
  {
    "text": "better maybe the bigger one but not not the small not the smaller one",
    "start": "5268960",
    "end": "5275800"
  },
  {
    "text": "that so you just hold a dvj yeah yeah and that would work yeah",
    "start": "5283119",
    "end": "5289719"
  },
  {
    "text": "yeah that makes sense yeah yeah with like prom like",
    "start": "5289719",
    "end": "5295400"
  },
  {
    "text": "that nice cool all right everyone well you have those classes for seven days so",
    "start": "5295400",
    "end": "5303520"
  },
  {
    "text": "feel keep deploying uh if you have any feedback for the workshop tell us or we have a",
    "start": "5303520",
    "end": "5311280"
  },
  {
    "text": "survey there which I assume is anonymous yeah it's Anonymous and so you can fill out that one as well you can",
    "start": "5311280",
    "end": "5317280"
  },
  {
    "text": "take picture fill out later [Applause]",
    "start": "5317280",
    "end": "5326770"
  },
  {
    "text": "[Music]",
    "start": "5326770",
    "end": "5343580"
  }
]