[
  {
    "text": "hello and welcome today we're going to talk about the lessons I've learned from building gen application over the last",
    "start": "359",
    "end": "7600"
  },
  {
    "text": "year and a half my name is Juan pero and I'll be",
    "start": "7600",
    "end": "12679"
  },
  {
    "text": "your guide in this journey I'm a Founder architect consultant developer and everything in between uh and I have over",
    "start": "12679",
    "end": "19920"
  },
  {
    "text": "15 years experience uh in the IT industry and one of the first things uh",
    "start": "19920",
    "end": "27840"
  },
  {
    "text": "everyone hears when they're talking about AI is about how much faster",
    "start": "27840",
    "end": "34239"
  },
  {
    "text": "developers can code thanks to AI you can hear this and you've heard this before I can guarantee that nowadays you can just",
    "start": "34239",
    "end": "41920"
  },
  {
    "text": "take a prompt put it into an llm and you will and the llm will code your website",
    "start": "41920",
    "end": "49800"
  },
  {
    "text": "for you in about a minute or so and we all know that that's not true right um",
    "start": "49800",
    "end": "56160"
  },
  {
    "text": "it'll give you some code and it'll get you going but it won't build everything for you",
    "start": "56160",
    "end": "61920"
  },
  {
    "text": "um but there's a lot of tools that have been created to help you with this like some of the ones that I have listed here",
    "start": "62039",
    "end": "67799"
  },
  {
    "text": "like codium and cursor that interact with your ID or an ID by all by",
    "start": "67799",
    "end": "73400"
  },
  {
    "text": "themselves um that help you uh build your applications you don't even have to use",
    "start": "73400",
    "end": "80079"
  },
  {
    "text": "those those tools right you could just take your code paste it into an llm uh and ask a",
    "start": "80079",
    "end": "85880"
  },
  {
    "text": "question right and there's some really good ones out there um but it all depends on what",
    "start": "85880",
    "end": "92079"
  },
  {
    "text": "application you're trying to build at if all you're doing is create a boiler plate um",
    "start": "92079",
    "end": "97759"
  },
  {
    "text": "for yet another uh website then any cod in LM will work just fine um if you're",
    "start": "97759",
    "end": "105240"
  },
  {
    "text": "doing some more something more complex or if it's something that's new or just came out like uh I don't know SV 5 for",
    "start": "105240",
    "end": "112600"
  },
  {
    "text": "example um then you want a be here LM something like uh dipic uh V3",
    "start": "112600",
    "end": "120360"
  },
  {
    "text": "or Cloud right and and these tools will",
    "start": "120360",
    "end": "125719"
  },
  {
    "text": "definitely help you um build faster uh however they",
    "start": "125719",
    "end": "131480"
  },
  {
    "text": "also uh add complexity to your applications once you're trying to integrate AI to your application to",
    "start": "131480",
    "end": "137879"
  },
  {
    "text": "actually interact with your user like here in the left hand side we",
    "start": "137879",
    "end": "143879"
  },
  {
    "text": "still see that in your typical uh application that we've been building till a year ago or two years ago",
    "start": "143879",
    "end": "150440"
  },
  {
    "text": "we were building front ends backends um infrastructure SC all the stuff that we",
    "start": "150440",
    "end": "155959"
  },
  {
    "text": "normally build but the moment you add AI to your application you all you have to",
    "start": "155959",
    "end": "161680"
  },
  {
    "text": "start worrying about all the stuff that's in Gray in the diagram on the right so you have to start wondering",
    "start": "161680",
    "end": "168599"
  },
  {
    "text": "about what model you're going to use are you going to fine tune it are you going to",
    "start": "168599",
    "end": "174720"
  },
  {
    "text": "um um you do some Pro engineering or you try going to try to use rag to uh get",
    "start": "174720",
    "end": "181879"
  },
  {
    "text": "the correct answer to your qu to to your questions and to the user questions how you going to prevent it from",
    "start": "181879",
    "end": "187720"
  },
  {
    "text": "hallucinating because we all know that uh llms are so bent on answering your",
    "start": "187720",
    "end": "193799"
  },
  {
    "text": "questions that sometimes they'll just make up answers uh and you need to a wait to figure out um whether the answer",
    "start": "193799",
    "end": "201519"
  },
  {
    "text": "that you're getting is right or not and so how do you prevent that that's something you don't have to worry about a year and a half ago um also you have",
    "start": "201519",
    "end": "210319"
  },
  {
    "text": "to figure out um how often you're going to have to um replace that model because",
    "start": "210319",
    "end": "217280"
  },
  {
    "text": "we all know that there's models that are coming out every other week and they're all more powerful beefier but they they",
    "start": "217280",
    "end": "225080"
  },
  {
    "text": "will not all work with your use case it's a one thing that they really great at certain benchmarks but that doesn't",
    "start": "225080",
    "end": "231480"
  },
  {
    "text": "mean they're going to work uh with the rest of the application that you built so you have to constantly be evaluating",
    "start": "231480",
    "end": "237480"
  },
  {
    "text": "not only the model that you have already running but also the molds are coming out that may uh help you build a better",
    "start": "237480",
    "end": "246760"
  },
  {
    "text": "application and you also have to start figuring out how you're going to host these things now yeah because that's",
    "start": "246760",
    "end": "252400"
  },
  {
    "text": "beyond hosting your store application right all of a sudden you don't have to worry just about CPU and memory all you",
    "start": "252400",
    "end": "258359"
  },
  {
    "text": "have to worry about gpus so well right where are you going to host this this is going to be in the cloud this you be um",
    "start": "258359",
    "end": "265680"
  },
  {
    "text": "on Prem all this information and all this stuff in Gray is new new to uh and",
    "start": "265680",
    "end": "272320"
  },
  {
    "text": "is specific to AI based",
    "start": "272320",
    "end": "277039"
  },
  {
    "text": "applications speaking of Hosting where to host this model is important right",
    "start": "278199",
    "end": "284600"
  },
  {
    "text": "how you going to host it are you going to host it in on Prem or are you going to host it on your machine while you're",
    "start": "284600",
    "end": "290280"
  },
  {
    "text": "um doing some exploration you have to make these decisions right so if you're going to host it locally you can host it",
    "start": "290280",
    "end": "296560"
  },
  {
    "text": "in something like ol Ola will allow you to um download a bunch of uh models into",
    "start": "296560",
    "end": "303160"
  },
  {
    "text": "your machine where you can play with those models for free interact with them and see if they're going to work for you",
    "start": "303160",
    "end": "309440"
  },
  {
    "text": "and there's a host of other Solutions as well is the one I've been using uh it's pretty good just you just have to make",
    "start": "309440",
    "end": "316800"
  },
  {
    "text": "sure you have a machine that's beefy enough uh for the mod that you're trying to run or you can even use this and all",
    "start": "316800",
    "end": "324560"
  },
  {
    "text": "these other tools uh to host things um on Prime",
    "start": "324560",
    "end": "331039"
  },
  {
    "text": "right but if you're going to host it in the cloud now you have to start thinking not only about what cloud but also about",
    "start": "331039",
    "end": "338880"
  },
  {
    "text": "cost of your gpus and is there enough gpus to go around uh and in general like everything",
    "start": "338880",
    "end": "345600"
  },
  {
    "text": "else you still also have to worry about cost and you have the the typical",
    "start": "345600",
    "end": "351000"
  },
  {
    "text": "players right you have your Google Cloud your AWS your Azure etc etc there's also",
    "start": "351000",
    "end": "357720"
  },
  {
    "text": "a new set of players that have come up uh that are meant to simplify the deployment of",
    "start": "357720",
    "end": "366199"
  },
  {
    "text": "your application in the cloud and that's where tools like model and skyp pilot",
    "start": "366199",
    "end": "372639"
  },
  {
    "text": "um have come up um and if you're using python which a",
    "start": "372639",
    "end": "378360"
  },
  {
    "text": "lot of us are using python for uh interacting with your with our models U model is a great choice mod allows you",
    "start": "378360",
    "end": "386000"
  },
  {
    "text": "to take your python code add some um",
    "start": "386000",
    "end": "391560"
  },
  {
    "text": "some um decorators to your code and use their CLI to deploy uh to",
    "start": "391720",
    "end": "399360"
  },
  {
    "text": "model and model will take care of building your your containers and pushing them to the",
    "start": "399360",
    "end": "405400"
  },
  {
    "text": "cloud um it's really simple I've used it to to host a lot on my",
    "start": "405400",
    "end": "411759"
  },
  {
    "text": "models um and there's also Sky pilot Sky pilot is great to minimize your cost as",
    "start": "411759",
    "end": "418240"
  },
  {
    "text": "they will spread the your clusters and your containers into multiple uh clouds",
    "start": "418240",
    "end": "427080"
  },
  {
    "text": "in order to minimize your cost right and you can configure it",
    "start": "427080",
    "end": "432400"
  },
  {
    "text": "which which class you Cloud you want Sky pilot to to use for hosting your",
    "start": "432400",
    "end": "439120"
  },
  {
    "text": "application um and at the end of the day right if you just want to play with with s in your machine you could also use a a",
    "start": "439720",
    "end": "447120"
  },
  {
    "text": "simple Library like the bugy face transport per that you just can run um locally on your machine without any any",
    "start": "447120",
    "end": "453400"
  },
  {
    "text": "any other tools um but at the end of the day once again",
    "start": "453400",
    "end": "458919"
  },
  {
    "text": "how you host these models is very important um now let's think about",
    "start": "458919",
    "end": "469120"
  },
  {
    "text": "um the fact that we're not in 20 2023 anymore right building a chat B power di",
    "start": "469120",
    "end": "475879"
  },
  {
    "text": "is very simple you can build it in an hour or two if that much all right um",
    "start": "475879",
    "end": "482159"
  },
  {
    "text": "there's a lot of Frameworks there that will help you build this for you",
    "start": "482159",
    "end": "487680"
  },
  {
    "text": "um however even though building the chatbot is easy making sure that that chatbot",
    "start": "487680",
    "end": "493440"
  },
  {
    "text": "gives you the correct answers is really hard like for example this that I have",
    "start": "493440",
    "end": "499039"
  },
  {
    "text": "on the screen um on the right is actually something that happened to me I put in one llm uh a few months ago now",
    "start": "499039",
    "end": "507919"
  },
  {
    "text": "what is three * three and the answer that they gave me was 33 that's obviously not right um so while making",
    "start": "507919",
    "end": "516760"
  },
  {
    "text": "the chat easy validating and moderating the chatbot content and making sure that it's giving you the correct answer to",
    "start": "516760",
    "end": "522760"
  },
  {
    "text": "your user is really really hard so what can we do to ensure that we",
    "start": "522760",
    "end": "531200"
  },
  {
    "text": "are providing the right answer to your to our users right well there's a number of techniques right you can do prompt",
    "start": "531200",
    "end": "536600"
  },
  {
    "text": "engineering which is basically you take the question of of the user you put additional uh details into the CR that",
    "start": "536600",
    "end": "544640"
  },
  {
    "text": "you pass into the um LM however you need to be very precise",
    "start": "544640",
    "end": "551760"
  },
  {
    "text": "in those additional in those additional instructions so that the LM can follow",
    "start": "551760",
    "end": "558000"
  },
  {
    "text": "them properly and even then the LM may choose to ignore them and at that point",
    "start": "558000",
    "end": "563959"
  },
  {
    "text": "what you do right you get you get the wrong you can also put",
    "start": "563959",
    "end": "569800"
  },
  {
    "text": "and guardrail is basically you take a secondary llm and put it to run in front or before your main",
    "start": "569800",
    "end": "576440"
  },
  {
    "text": "llm and this secondary llm is basically a classifier that will",
    "start": "576440",
    "end": "582279"
  },
  {
    "text": "classify the question that they ask that the person is answering um as a safe question or as a",
    "start": "582279",
    "end": "590760"
  },
  {
    "text": "bad question and when I say bad question it could be and something that is trying to go around uh uh or asking for",
    "start": "590760",
    "end": "600040"
  },
  {
    "text": "criminal activities or may be asking about um things that are not not what",
    "start": "600040",
    "end": "606800"
  },
  {
    "text": "you'd expect somebody to to answer the um in general right and at that point",
    "start": "606800",
    "end": "613560"
  },
  {
    "text": "the the the secondary LM is classifying that your questions and it also could be",
    "start": "613560",
    "end": "618839"
  },
  {
    "text": "the the answers of your own LM as either good or bad however you know adding this",
    "start": "618839",
    "end": "624959"
  },
  {
    "text": "secondary LM adds latency because you're making a secondary call right and it also has",
    "start": "624959",
    "end": "630880"
  },
  {
    "text": "additional cost and like anything else with llms uh it may or may not be",
    "start": "630880",
    "end": "639519"
  },
  {
    "text": "right the next thing you could do is you could have retrieval augmented generation or rag which is basically you",
    "start": "639639",
    "end": "645760"
  },
  {
    "text": "take a lot of information that you have in your company you break it into small chunks",
    "start": "645760",
    "end": "651200"
  },
  {
    "text": "of information so you can think of it of as let's say you take a document and you break it into paragraphs although it",
    "start": "651200",
    "end": "657600"
  },
  {
    "text": "could be anything just think of it chunk as a paragraph for now you break it into chunks and you start it in the vector",
    "start": "657600",
    "end": "664279"
  },
  {
    "text": "database and then when somebody ask a question you take um you do a similar similarity",
    "start": "664279",
    "end": "671320"
  },
  {
    "text": "search on your uh Vector database pull information and provide that information",
    "start": "671320",
    "end": "677480"
  },
  {
    "text": "to your U llm and tell the llm here's the information you can use to answer",
    "start": "677480",
    "end": "682959"
  },
  {
    "text": "that question and what that sounds really nice and good is really dependent",
    "start": "682959",
    "end": "688120"
  },
  {
    "text": "on the type of information you've provided in your vector database so if you put all information that's no longer",
    "start": "688120",
    "end": "696079"
  },
  {
    "text": "relevant well guess what what LM is going to reply to you is with information that's no longer",
    "start": "696079",
    "end": "701639"
  },
  {
    "text": "relevant so data is extremely important in this",
    "start": "701639",
    "end": "707839"
  },
  {
    "text": "case um other thing you can do is you can do file tuning right and F tuning is basically",
    "start": "707880",
    "end": "713839"
  },
  {
    "text": "you do additional training uh to an llm this is more expensive than the",
    "start": "713839",
    "end": "719760"
  },
  {
    "text": "other the techniques that we're talking about and it takes long longer period of time and doesn't really uh guarantee",
    "start": "719760",
    "end": "728880"
  },
  {
    "text": "that you're going to get good results in fact depending on the type of information you're putting uh into your",
    "start": "728880",
    "end": "734839"
  },
  {
    "text": "F tuning we could actually degrade the llm quality responses uh because you may actually",
    "start": "734839",
    "end": "742040"
  },
  {
    "text": "forget some of the information it already had or it may get confused between the new information you're passing and the information that it",
    "start": "742040",
    "end": "748399"
  },
  {
    "text": "already had so at the end of the day there's no no no technique that will guarantee that",
    "start": "748399",
    "end": "755480"
  },
  {
    "text": "the answer of your chat B is going to be good but uh using many of these techniques",
    "start": "755480",
    "end": "764800"
  },
  {
    "text": "will help you get there or get you a long",
    "start": "764800",
    "end": "769519"
  },
  {
    "text": "way another thing that you should do is you should evaluate and uh your models",
    "start": "770079",
    "end": "776079"
  },
  {
    "text": "and your application at each step of the development life",
    "start": "776079",
    "end": "782240"
  },
  {
    "text": "cycle for example when you are starting to explore which LMS to use in your in",
    "start": "782240",
    "end": "788680"
  },
  {
    "text": "your application or your idea of an application you could use something like AMA and we talked a little bit about",
    "start": "788680",
    "end": "796199"
  },
  {
    "text": "before it will let you run hundreds of llms locally on your",
    "start": "796199",
    "end": "801240"
  },
  {
    "text": "machine um once you are interested on one or another you can",
    "start": "801240",
    "end": "808760"
  },
  {
    "text": "start using in your application or if you want to play with thousands of uh",
    "start": "808760",
    "end": "814880"
  },
  {
    "text": "open source models you can go to huging face and there you can build some",
    "start": "814880",
    "end": "820000"
  },
  {
    "text": "hugging face space spaces um you can see the spaces that other people have built",
    "start": "820000",
    "end": "825639"
  },
  {
    "text": "and this is basically a lot of code that interacts with llms um and that you can",
    "start": "825639",
    "end": "831279"
  },
  {
    "text": "freely use and review um to see whether you're interested to build something similar or use the the LMS that they're",
    "start": "831279",
    "end": "837680"
  },
  {
    "text": "using now if you want to interact with other",
    "start": "837680",
    "end": "843639"
  },
  {
    "text": "models including um some of the close models you can go to open router open",
    "start": "843639",
    "end": "849240"
  },
  {
    "text": "router will basically take your prompt and it will run it against a number of models or a model of you're choosing and",
    "start": "849240",
    "end": "857199"
  },
  {
    "text": "one of the things that makes open rer really cool is that it will um take your",
    "start": "857199",
    "end": "863399"
  },
  {
    "text": "prompt and let's say you want to run that prompt against Lama three it will",
    "start": "863399",
    "end": "868759"
  },
  {
    "text": "look at the providers that are actually offering",
    "start": "868759",
    "end": "874320"
  },
  {
    "text": "Lama 3 as as an API and they will run it until the one that's against the one that's cheaper which is really cool and",
    "start": "874320",
    "end": "881480"
  },
  {
    "text": "it's a great way to save money um I use it every day and I put",
    "start": "881480",
    "end": "887240"
  },
  {
    "text": "like $50 about six months ago and I still have about 40 bucks left and I use as I say I use it every day so it's a",
    "start": "887240",
    "end": "894320"
  },
  {
    "text": "great way um to interact with models and uh see which one I want to",
    "start": "894320",
    "end": "901240"
  },
  {
    "text": "use uh obviously if you're going to do so open router production there are",
    "start": "901240",
    "end": "906720"
  },
  {
    "text": "there are a number of um safeguards that you you may want to look into you know",
    "start": "906720",
    "end": "912600"
  },
  {
    "text": "you don't want just open R just running into a random llm provider without you",
    "start": "912600",
    "end": "918399"
  },
  {
    "text": "knowing which one it is um but for testing it's a great thing and finally uh the last tool I",
    "start": "918399",
    "end": "926199"
  },
  {
    "text": "have here listed is lsmith which is created by the creators of uh L chain which is a very popular open-source",
    "start": "926199",
    "end": "935240"
  },
  {
    "text": "um framework to interact with llms but LMI one of the great things",
    "start": "935240",
    "end": "941319"
  },
  {
    "text": "about it is that it lets you constantly evaluate your models so it will",
    "start": "941319",
    "end": "948360"
  },
  {
    "text": "uh it can help you record uh a number of Bruns that you have with your llms you",
    "start": "948360",
    "end": "954360"
  },
  {
    "text": "can compare over time um how your LMS have been performed in um is a really",
    "start": "954360",
    "end": "960959"
  },
  {
    "text": "really cool platform um just to keep track of what your application and your models are",
    "start": "960959",
    "end": "967240"
  },
  {
    "text": "doing [Music] um so yeah evaluation you you really",
    "start": "967240",
    "end": "972800"
  },
  {
    "text": "should be doing this at each step of the way another thing that it's very",
    "start": "972800",
    "end": "978480"
  },
  {
    "text": "important is externalizing your prompts and I know that for all of us",
    "start": "978480",
    "end": "984160"
  },
  {
    "text": "it's really easy to just take a prompt put it in the code our code it in there",
    "start": "984160",
    "end": "989800"
  },
  {
    "text": "and move on um but if you do that you're short changing yourself externalizing your",
    "start": "989800",
    "end": "995959"
  },
  {
    "text": "prompt uh in tools like uh repositories like like Lang chain Hub which I have",
    "start": "995959",
    "end": "1003199"
  },
  {
    "text": "here a screenshot on the left uh will allow you to uh share those fronts with experts so",
    "start": "1003199",
    "end": "1012040"
  },
  {
    "text": "if you are build an application that has to do with education you could you could reach out to to an education expert that",
    "start": "1012040",
    "end": "1018040"
  },
  {
    "text": "has never coded in their lives and you can have them this log to to the hub",
    "start": "1018040",
    "end": "1025798"
  },
  {
    "text": "play with with the proms run the llms see the output that they're getting from the",
    "start": "1025799",
    "end": "1031360"
  },
  {
    "text": "llms um and then when they're ready and they're happy with that they're getting the correct uh prompts all they have to",
    "start": "1031360",
    "end": "1037160"
  },
  {
    "text": "do is just uh commit those prompts and that will flow directly into your",
    "start": "1037160",
    "end": "1043160"
  },
  {
    "text": "application it's really cool another not thing that will that uh this will help",
    "start": "1043160",
    "end": "1049080"
  },
  {
    "text": "with is future proofing we all know that models are coming out every other week there a new model and that's better than",
    "start": "1049080",
    "end": "1055760"
  },
  {
    "text": "the previous one um and unfortunately right even",
    "start": "1055760",
    "end": "1061440"
  },
  {
    "text": "let's say if you're using a Lama model and the next Lama model comes out uh your prodct may not work prop properly",
    "start": "1061440",
    "end": "1068600"
  },
  {
    "text": "with a new version you may not get the the same results so by allowing you to",
    "start": "1068600",
    "end": "1073720"
  },
  {
    "text": "easily access the prompts and modify them and test them against something new",
    "start": "1073720",
    "end": "1079440"
  },
  {
    "text": "then you you are basically future proofing and making your life easier in the future and that in turn leads to",
    "start": "1079440",
    "end": "1085520"
  },
  {
    "text": "faster development so bottom line promp should never be",
    "start": "1085520",
    "end": "1091720"
  },
  {
    "text": "encoded in your code list I'll let's talk a little bit about",
    "start": "1091720",
    "end": "1098159"
  },
  {
    "text": "agents agent agents are and I expect them to be in this year",
    "start": "1098159",
    "end": "1105480"
  },
  {
    "text": "one of the best uh additions to the world of other because agents are basically allowing",
    "start": "1105480",
    "end": "1112640"
  },
  {
    "text": "and the LMS to break out of the chatbot and actually interact with the real",
    "start": "1112640",
    "end": "1118080"
  },
  {
    "text": "world so you can have an agent for example um that uh allows you",
    "start": "1118080",
    "end": "1126080"
  },
  {
    "text": "to for example you have a book you want to uh create the the audio for it and",
    "start": "1126080",
    "end": "1133640"
  },
  {
    "text": "you want to translate it into multiple languages that's something that you can",
    "start": "1133640",
    "end": "1138840"
  },
  {
    "text": "do you can have an output out of that those things all right so agents will allow us",
    "start": "1138840",
    "end": "1144840"
  },
  {
    "text": "to interact with the real world all these llms that have been confined to just chatting all of sudden will have a",
    "start": "1144840",
    "end": "1150200"
  },
  {
    "text": "real impact in the world um and we're still not there we're",
    "start": "1150200",
    "end": "1155880"
  },
  {
    "text": "still working on it but it's a a really exciting",
    "start": "1155880",
    "end": "1161280"
  },
  {
    "text": "uh exciting uh set of applications that will be coming out",
    "start": "1161280",
    "end": "1167159"
  },
  {
    "text": "soon and here I have a couple of U drawings",
    "start": "1167159",
    "end": "1172280"
  },
  {
    "text": "and of what an agent looks like and the one on the",
    "start": "1172280",
    "end": "1177480"
  },
  {
    "text": "left that's that has four nodes so it has start nodes assistant tools and it's",
    "start": "1177480",
    "end": "1182919"
  },
  {
    "text": "really really powerful right because basically what you're doing is",
    "start": "1182919",
    "end": "1188360"
  },
  {
    "text": "you're when somebody gives you a question you pass it to the assistant and the system basically has an llm",
    "start": "1188360",
    "end": "1195799"
  },
  {
    "text": "embedded and that llm will an the question and then we start calling",
    "start": "1195799",
    "end": "1202240"
  },
  {
    "text": "tools and tools are this fancy name for functions it will start calling functions um until it gets to the answer",
    "start": "1202240",
    "end": "1209039"
  },
  {
    "text": "that it that it needs uh to answer the question of the end user so for example",
    "start": "1209039",
    "end": "1215880"
  },
  {
    "text": "it could be that the end user sends a the text of a book to the assistant and",
    "start": "1215880",
    "end": "1221320"
  },
  {
    "text": "it tells it to translate it and uh create the audio and then the assistant",
    "start": "1221320",
    "end": "1227159"
  },
  {
    "text": "will call the tool to create audio then it will call the Tool uh to the translation and then it will",
    "start": "1227159",
    "end": "1233200"
  },
  {
    "text": "end right and the assistant is making those decisions all by itself which is a",
    "start": "1233200",
    "end": "1238720"
  },
  {
    "text": "a really really powerful um",
    "start": "1238720",
    "end": "1243120"
  },
  {
    "text": "concept um and while it's really cool that those agents can make those decisions without",
    "start": "1244159",
    "end": "1250960"
  },
  {
    "text": "you actually having to hard code all those paths um they do have his disadvantages",
    "start": "1250960",
    "end": "1257200"
  },
  {
    "text": "right and one of the big ones is is that llms have a lot of latency you know",
    "start": "1257200",
    "end": "1262760"
  },
  {
    "text": "calling one of those those llms takes several seconds right so all of a sudden if you",
    "start": "1262760",
    "end": "1270400"
  },
  {
    "text": "have an agent that is integrating with four or five llms and each one of them",
    "start": "1270400",
    "end": "1277480"
  },
  {
    "text": "takes let's say three seconds to run then all of a sudden you",
    "start": "1277480",
    "end": "1283520"
  },
  {
    "text": "have uh a process that takes 12 15 seconds right and we know that our users",
    "start": "1283520",
    "end": "1290760"
  },
  {
    "text": "nowadays are used to running things on the web and take those things taking",
    "start": "1290760",
    "end": "1297200"
  },
  {
    "text": "only milliseconds right and if they have to wait for a second or two people are leaving our sites and going to our",
    "start": "1297200",
    "end": "1304000"
  },
  {
    "text": "competition which is not the best uh result",
    "start": "1304000",
    "end": "1309480"
  },
  {
    "text": "um so we should take advantage of of uh this platforms that have been built like",
    "start": "1309480",
    "end": "1315080"
  },
  {
    "text": "Lan chain Lama index L flow there there's many of them right that allows to do things like",
    "start": "1315080",
    "end": "1321960"
  },
  {
    "text": "concr calls and branching uh in these agents right so like for",
    "start": "1321960",
    "end": "1328919"
  },
  {
    "text": "example I have here the on the left diagram of an agent that's actually calling uh a couple of things in in",
    "start": "1328919",
    "end": "1336559"
  },
  {
    "text": "parallel right so it could be once again this could be um the user passing uh a",
    "start": "1336559",
    "end": "1343840"
  },
  {
    "text": "book to node a no a calling uh the audio transl the audio creation and the",
    "start": "1343840",
    "end": "1350720"
  },
  {
    "text": "translation in parallel and then sending the answer back to the user now if each",
    "start": "1350720",
    "end": "1357520"
  },
  {
    "text": "one of these things took um two seconds instead of these things taking eight seconds all of a sudden it takes",
    "start": "1357520",
    "end": "1364480"
  },
  {
    "text": "six right so that's much better right so there's a lot of things and and",
    "start": "1364480",
    "end": "1372640"
  },
  {
    "text": "a lot of things that that need to come to mind when you're building these agents so that you can make them as efficient as possible so that we can",
    "start": "1372640",
    "end": "1380120"
  },
  {
    "text": "still get to the to fulfill the the expectations of our",
    "start": "1380120",
    "end": "1385960"
  },
  {
    "text": "users while taking advantage of the lm's",
    "start": "1386120",
    "end": "1391600"
  },
  {
    "text": "powers um and while I do believe that agents are the future for LMS we really",
    "start": "1394960",
    "end": "1400720"
  },
  {
    "text": "have to be aware of the cost of running this agents um",
    "start": "1400720",
    "end": "1408600"
  },
  {
    "text": "because they can the cost can add up really quickly let's take for example the one I have here with once again is a",
    "start": "1408600",
    "end": "1414279"
  },
  {
    "text": "simple um agent um and we're we're going to assume that",
    "start": "1414279",
    "end": "1420799"
  },
  {
    "text": "this is a call center right this call center takes 3,000 calls a day and each",
    "start": "1420799",
    "end": "1426400"
  },
  {
    "text": "one of those calls requires 15 uh function calls or tool",
    "start": "1426400",
    "end": "1432559"
  },
  {
    "text": "calls and we're going to assume that we're using op ai1 it's a gr model right",
    "start": "1432559",
    "end": "1438640"
  },
  {
    "text": "so we're going to take advantage because it's the one of the big biggest and greatest um and the prices for for open",
    "start": "1438640",
    "end": "1446799"
  },
  {
    "text": "AI or1 is 15 million $15 per million tokens or $60 per million tokens",
    "start": "1446799",
    "end": "1454679"
  },
  {
    "text": "out and if we assume that we're inputting 1500 input tokens and 3,000",
    "start": "1454679",
    "end": "1460559"
  },
  {
    "text": "output tokens meaning uh we're sending a bunch of information into the LM the llm",
    "start": "1460559",
    "end": "1465840"
  },
  {
    "text": "is import is is uh up putting a bunch of information",
    "start": "1465840",
    "end": "1471360"
  },
  {
    "text": "out um let's figure out how much that will cost us right uh because it sounds",
    "start": "1471360",
    "end": "1477679"
  },
  {
    "text": "like very little $15 per one million token right it sounds like it shouldn't",
    "start": "1477679",
    "end": "1483240"
  },
  {
    "text": "be much but once you do the math and the math is at the bottom if you want to take a look at it you won't discuss it",
    "start": "1483240",
    "end": "1490200"
  },
  {
    "text": "here uh but it comes out to $24 uh per call which turns into 9,270",
    "start": "1490200",
    "end": "1497760"
  },
  {
    "text": "per day and which turn is almost $300,000 per",
    "start": "1497760",
    "end": "1504240"
  },
  {
    "text": "month um so if you were expecting to get a very small price tag um well you're",
    "start": "1504240",
    "end": "1512600"
  },
  {
    "text": "going to be to have a little bit of a surprise uh at the end of the day right",
    "start": "1512600",
    "end": "1518880"
  },
  {
    "text": "especially if you're using um provider endpoints for your apis to",
    "start": "1518880",
    "end": "1525399"
  },
  {
    "text": "interact with the llms you have to keep in mind that the cost of running this",
    "start": "1525399",
    "end": "1531440"
  },
  {
    "text": "thing is linear right so each additional user will incur you an additional cost",
    "start": "1531440",
    "end": "1539559"
  },
  {
    "text": "now obviously you have to make sure that you're you're you're pricing your um",
    "start": "1539559",
    "end": "1544919"
  },
  {
    "text": "products accordingly but if you expecting to get very low prices uh very",
    "start": "1544919",
    "end": "1550240"
  },
  {
    "text": "low cost uh you may want to uh do the",
    "start": "1550240",
    "end": "1556240"
  },
  {
    "text": "math um but even if you are renting the gpus right so you don't have",
    "start": "1556520",
    "end": "1563559"
  },
  {
    "text": "to make the API calls gpus are at a premium nowaday so",
    "start": "1563559",
    "end": "1570320"
  },
  {
    "text": "um if we compare the price of a GPU versus the price of a CPU on the cloud",
    "start": "1570320",
    "end": "1576520"
  },
  {
    "text": "we're talking cents versus dollars all right so having",
    "start": "1576520",
    "end": "1583120"
  },
  {
    "text": "an a100 GPU for an hour will cost you like",
    "start": "1583120",
    "end": "1588760"
  },
  {
    "text": "potentially four or five dollars an hour versus you know a traditional ac2",
    "start": "1588760",
    "end": "1594760"
  },
  {
    "text": "instance that could cost you a fraction of a scent or a couple of cents an hour",
    "start": "1594760",
    "end": "1600399"
  },
  {
    "text": "so keep that in mind and the choice of the mobile and",
    "start": "1600399",
    "end": "1607240"
  },
  {
    "text": "and the provider that you're going to use in your in your application is really really important and we can see",
    "start": "1607240",
    "end": "1614679"
  },
  {
    "text": "in the price chart on the left that there's a significant if an Val value",
    "start": "1614679",
    "end": "1619760"
  },
  {
    "text": "difference between the uh left so the first llm in the list and the last one",
    "start": "1619760",
    "end": "1626399"
  },
  {
    "text": "right it's it's over 20 times difference in times of price right",
    "start": "1626399",
    "end": "1633320"
  },
  {
    "text": "um so you have to valuate do you really need the power",
    "start": "1633320",
    "end": "1639000"
  },
  {
    "text": "of a very pricey llm or can you do the same thing with a",
    "start": "1639000",
    "end": "1645720"
  },
  {
    "text": "cheaper llm or two that may not be quite as powerful but they can do the task",
    "start": "1645720",
    "end": "1652240"
  },
  {
    "text": "right it might be from a press perspective it might be much much much",
    "start": "1652240",
    "end": "1658279"
  },
  {
    "text": "more efficient right the other thing that's important like we said is uh",
    "start": "1658279",
    "end": "1663919"
  },
  {
    "text": "outto speed right how fast these these models um can uh respond to",
    "start": "1663919",
    "end": "1671000"
  },
  {
    "text": "questions and that's what we see in the chart on the right the output speed and",
    "start": "1671000",
    "end": "1677159"
  },
  {
    "text": "we can see that cerebras in Gro can run Lama 3.1 70b at over a th uh tokens per",
    "start": "1677159",
    "end": "1688519"
  },
  {
    "text": "second in fact cus is over 2,000 tokens per second while all the way on the",
    "start": "1688519",
    "end": "1694600"
  },
  {
    "text": "right the the providers are running 31 in 29",
    "start": "1694600",
    "end": "1700600"
  },
  {
    "text": "tokens per second right all that translates to latency for your",
    "start": "1700600",
    "end": "1706200"
  },
  {
    "text": "users and usually the SI size of the of the model that you're picking also has a big a big impact right so bigger models",
    "start": "1706200",
    "end": "1713600"
  },
  {
    "text": "will take longer to run smaller models will go faster which one you're using and which",
    "start": "1713600",
    "end": "1721240"
  },
  {
    "text": "one you need to use will have a big impact on your application so at the end of the day",
    "start": "1721240",
    "end": "1727320"
  },
  {
    "text": "right small is beautiful and better for the environment and better for your",
    "start": "1727320",
    "end": "1734080"
  },
  {
    "text": "wet so let's go back to our example right now what happens if we replace",
    "start": "1736519",
    "end": "1743279"
  },
  {
    "text": "open a 01 with Lama 3.37 TV just from a price perspective right you may need to",
    "start": "1743279",
    "end": "1749399"
  },
  {
    "text": "use open a for your use case but what if you didn't right you can just use um",
    "start": "1749399",
    "end": "1754440"
  },
  {
    "text": "Lama 3.3 um well we go from three over $3 per",
    "start": "1754440",
    "end": "1760640"
  },
  {
    "text": "Co to 52 cents per go and we go all from",
    "start": "1760640",
    "end": "1766919"
  },
  {
    "text": "almost $300,000 per month to almost $50,000 per",
    "start": "1766919",
    "end": "1774120"
  },
  {
    "text": "month so once again pick the right model for the use",
    "start": "1774120",
    "end": "1781760"
  },
  {
    "text": "case at the end of the day your wallet and the environment will thank",
    "start": "1781760",
    "end": "1788840"
  },
  {
    "text": "you and the last thing I want to touch on is observability for your",
    "start": "1790000",
    "end": "1795200"
  },
  {
    "text": "agents I can't tell you how hard it is uh to try to figure out what an agent is",
    "start": "1795200",
    "end": "1801240"
  },
  {
    "text": "doing once you get multiple nodes that are interacting with each",
    "start": "1801240",
    "end": "1806440"
  },
  {
    "text": "other uh this is a lot harder in my experience and trying to figure out what",
    "start": "1806440",
    "end": "1811519"
  },
  {
    "text": "a regular application think because a regular application is deterministic it will do a b c",
    "start": "1811519",
    "end": "1818640"
  },
  {
    "text": "d while um a probabilistic",
    "start": "1818640",
    "end": "1825799"
  },
  {
    "text": "model add lots of unknowns to your application so all of a sudden things",
    "start": "1825799",
    "end": "1831840"
  },
  {
    "text": "that were running a 100 times with no issue will start failing out 100",
    "start": "1831840",
    "end": "1837600"
  },
  {
    "text": "1001st item right and I'll give you an example right",
    "start": "1837600",
    "end": "1843679"
  },
  {
    "text": "I was running an llm and all he had to do was get information about certain a",
    "start": "1843679",
    "end": "1849640"
  },
  {
    "text": "user from the database right so let's say for example get information for user",
    "start": "1849640",
    "end": "1855519"
  },
  {
    "text": "John and this run for the longest time and all started",
    "start": "1855519",
    "end": "1862840"
  },
  {
    "text": "failing and we realized that eventually because because we had",
    "start": "1862840",
    "end": "1868120"
  },
  {
    "text": "the traces um that the user was entering lower cap John instead of just Capal J",
    "start": "1868120",
    "end": "1874919"
  },
  {
    "text": "and the rest of the word right uh and all it took was going back to the prompt",
    "start": "1874919",
    "end": "1880760"
  },
  {
    "text": "which we had externalized luckily and changed the pro to add the sentence uh",
    "start": "1880760",
    "end": "1886799"
  },
  {
    "text": "ignore case and everything started working again but because this one was",
    "start": "1886799",
    "end": "1892000"
  },
  {
    "text": "an agentic application and it was calling multiple",
    "start": "1892000",
    "end": "1897320"
  },
  {
    "text": "mels it would have taken us a very long time to figure out what was going on I mean we could put we could we could have",
    "start": "1897320",
    "end": "1905159"
  },
  {
    "text": "put the debugger and print statements all that and we still have be very very hard so how have observability for",
    "start": "1905159",
    "end": "1913960"
  },
  {
    "text": "there's a lot of tools out there like lsmith that will make this very simple",
    "start": "1913960",
    "end": "1921000"
  },
  {
    "text": "uh and if you don't want to use one of the tools that's available there you can build one or you can build all this but",
    "start": "1921000",
    "end": "1927480"
  },
  {
    "text": "build it because you are going to need it um and in here um we have our agent",
    "start": "1927480",
    "end": "1933480"
  },
  {
    "text": "traces on the left um of the screen we have an agent",
    "start": "1933480",
    "end": "1938720"
  },
  {
    "text": "that's first calling Lama guard then it's calling Lama 3.1 then it's called mistal and that that that gives us an",
    "start": "1938720",
    "end": "1947399"
  },
  {
    "text": "order of everything that that had run in that in that particular call to the agent it has metadata it has the inputs",
    "start": "1947399",
    "end": "1955919"
  },
  {
    "text": "and the outputs all that is crucial right and when he when it throws an error it keeps track of the errors right",
    "start": "1955919",
    "end": "1963519"
  },
  {
    "text": "all the information that was in and out of that llm so you can see exactly what happened and you can see the error uh",
    "start": "1963519",
    "end": "1970639"
  },
  {
    "text": "the trace like we see on the trace on the picture on the right yeah so",
    "start": "1970639",
    "end": "1978360"
  },
  {
    "text": "and like I said building observability for your agents really",
    "start": "1978360",
    "end": "1985039"
  },
  {
    "text": "important and with that any questions",
    "start": "1985840",
    "end": "1991320"
  }
]