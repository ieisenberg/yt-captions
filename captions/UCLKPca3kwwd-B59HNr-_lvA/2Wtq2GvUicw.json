[
  {
    "text": "[Music]",
    "start": "350",
    "end": "13839"
  },
  {
    "text": "so welcome everyone thanks for making it to this lunch and learn uh my goal today is to make sure that I get to uh share",
    "start": "13839",
    "end": "21359"
  },
  {
    "text": "my knowledge and experience on llm fine-tuning uh just to get a quick sort",
    "start": "21359",
    "end": "28439"
  },
  {
    "text": "of pull from the audience here how many of you are have heard of the concept of fine-tuning here okay so",
    "start": "28439",
    "end": "36200"
  },
  {
    "text": "quite a few people how many of you have actually had hands-on experience in fine-tuning",
    "start": "36200",
    "end": "41399"
  },
  {
    "text": "llms okay all right that's pretty good uh that's more than I'm usually used to I mean this is quite a fantastic that in",
    "start": "41399",
    "end": "48879"
  },
  {
    "text": "this conference the makeup of AI engineer is close to 100% that's not a something I'm generally used to when",
    "start": "48879",
    "end": "55359"
  },
  {
    "text": "presenting at other you know hackathons and conferences so I feel like I'm I'm",
    "start": "55359",
    "end": "60440"
  },
  {
    "text": "speaking to the right crowd so just to kind of contextualize this talk really",
    "start": "60440",
    "end": "66960"
  },
  {
    "text": "I'm trying to address two pains that a lot of gen Engineers face and to get a",
    "start": "66960",
    "end": "74159"
  },
  {
    "text": "sense of where you are in your journey how many really identify and can relate to the first one which is my gen spin",
    "start": "74159",
    "end": "81520"
  },
  {
    "text": "has gone through the roof okay yeah all right and how many of",
    "start": "81520",
    "end": "86880"
  },
  {
    "text": "you are in this other segment of this journey which is is you know you've built pcc's it's showing promise but you",
    "start": "86880",
    "end": "94000"
  },
  {
    "text": "haven't yet quite met this quality bar to go to production can I get a sense of",
    "start": "94000",
    "end": "100960"
  },
  {
    "text": "all right so so I think you know we have a good amount of a good fraction of the audience that can relate to one of these",
    "start": "100960",
    "end": "106840"
  },
  {
    "text": "two problems uh myself I'm a co-founder at octo aai and I'm going to talk a",
    "start": "106840",
    "end": "112240"
  },
  {
    "text": "little bit more about what we do but the customers I've been working with they feel those pains in very real way or",
    "start": "112240",
    "end": "119039"
  },
  {
    "text": "talking about 10 of thousands if not hundreds of thousands of dollars in monthly",
    "start": "119039",
    "end": "124200"
  },
  {
    "text": "bills and perhaps even having issues trying to go to production because the",
    "start": "124200",
    "end": "129319"
  },
  {
    "text": "quality bar hasn't yet been met so the overview of this uh 50-minute",
    "start": "129319",
    "end": "134640"
  },
  {
    "text": "talk uh is going to be spent on understanding the why of fine tuning really try to understand when to use",
    "start": "134640",
    "end": "141040"
  },
  {
    "text": "fine-tuning it's not really a silver bullet for all the problems you're going to face but one used right in the right",
    "start": "141040",
    "end": "147640"
  },
  {
    "text": "context for the right problem it can really deliver results I'm also going to try to contextualize this notion of",
    "start": "147640",
    "end": "153760"
  },
  {
    "text": "fine-tuning within the crawl walk and run of llm quality optimization because",
    "start": "153760",
    "end": "159879"
  },
  {
    "text": "there's different techniques that you should attempt before trying to do fine tuning but finally when you're convinced",
    "start": "159879",
    "end": "165040"
  },
  {
    "text": "that this is the right thing for you I'm going to talk about this continuous deployment cycle of fine-tuned llms so",
    "start": "165040",
    "end": "172400"
  },
  {
    "text": "we're going to go through today over a whole crank of that wheel of this deployment cycle composed of you know",
    "start": "172400",
    "end": "179280"
  },
  {
    "text": "model uh you know data set collection model fine-tuning deployment and",
    "start": "179280",
    "end": "184720"
  },
  {
    "text": "evaluation and really I'm trying to demystify this whole journey to you all because in the next 15 minutes we're",
    "start": "184720",
    "end": "190280"
  },
  {
    "text": "actually going to go through this whole process and hopefully that's something that you're going to feel comfortable going through and you know applying to",
    "start": "190280",
    "end": "196080"
  },
  {
    "text": "your own data set to your own problems and so for illustrating today's use case we're going to use this uh personally",
    "start": "196080",
    "end": "203799"
  },
  {
    "text": "identifiable information redaction use case now that's a pretty traditional sort of data scrubbing type of uh",
    "start": "203799",
    "end": "209319"
  },
  {
    "text": "application but we're going to use llms and we're going to see that we can essentially achieve stateof VR accuracy",
    "start": "209319",
    "end": "215799"
  },
  {
    "text": "while keeping efficiency at at the highest uh using essentially very",
    "start": "215799",
    "end": "221640"
  },
  {
    "text": "compact very lightweight models that have been fine-tuned for that very task so again trying to motivate this talk",
    "start": "221640",
    "end": "229200"
  },
  {
    "text": "what limits gen adoption in most businesses today based on the conversations that I've had in the field",
    "start": "229200",
    "end": "234360"
  },
  {
    "text": "discussion I've had with customers and developers the first one is there's a limited availability",
    "start": "234360",
    "end": "240599"
  },
  {
    "text": "of gpus I think we're all familiar with this problem it's one of the reasons why Nvidia is so successful lately I mean",
    "start": "240599",
    "end": "247120"
  },
  {
    "text": "everyone wants to have access to those precious resources that allow us to run",
    "start": "247120",
    "end": "252480"
  },
  {
    "text": "gen at scale and that can also Drive costs up right so we have to be smart about how to use those GPU",
    "start": "252480",
    "end": "259440"
  },
  {
    "text": "resources and also uh when people build poc's it displays and shows promise but",
    "start": "259440",
    "end": "266199"
  },
  {
    "text": "sometimes you don't reach the expected quality bar to go to production and so on this XY axis you know on this",
    "start": "266199",
    "end": "273400"
  },
  {
    "text": "chart where y AIS is cost and the x-axis symbolizes quality maybe many people",
    "start": "273400",
    "end": "281160"
  },
  {
    "text": "start uh on that Green Cross here right on this upper quadrant of very high cost",
    "start": "281160",
    "end": "287000"
  },
  {
    "text": "maybe not having met the quality bar that's your first po but really to go to production you need to end on the",
    "start": "287000",
    "end": "293000"
  },
  {
    "text": "opposite quadrant right lower cost higher quality where you met the bar you're able to run this in a way that",
    "start": "293000",
    "end": "300680"
  },
  {
    "text": "essentially is margin positive and many of us are on this journey to reach that point of uh you know",
    "start": "300680",
    "end": "307880"
  },
  {
    "text": "profitability so we're going to learn today how to use and how to fine-tune an",
    "start": "307880",
    "end": "313199"
  },
  {
    "text": "llm now fine-tuning is a method that we're going to use to improve the llm quality but as a bonus we're going to be",
    "start": "313199",
    "end": "320639"
  },
  {
    "text": "also showing how to improve quality significantly and I use quality as the",
    "start": "320639",
    "end": "325680"
  },
  {
    "text": "title of this talk because really I think many of us AI Engineers really care about reaching the high quality bar",
    "start": "325680",
    "end": "332080"
  },
  {
    "text": "when we're using llms and hopefully I'm you know the goal of today's talk is to instill you some knowledge on how to",
    "start": "332080",
    "end": "338639"
  },
  {
    "text": "tackle this journey and so in terms of tools that we're going to use today we're going to use open pipe which is a",
    "start": "338639",
    "end": "345360"
  },
  {
    "text": "SAS solution for fine-tuning that really lowers the bar of entry for people to run their own fine tunes uh you don't",
    "start": "345360",
    "end": "352440"
  },
  {
    "text": "need Hardware or Cloud instances to get started and we're going to use this to deliver quality improvements over stere",
    "start": "352440",
    "end": "359280"
  },
  {
    "text": "ofthe art llms and of course since I work at octo aai I'm going to also be using octo AI here for the llm",
    "start": "359280",
    "end": "365840"
  },
  {
    "text": "deployment and that's going to be the solution that we're going to use to achieve cost efficiency at scale and",
    "start": "365840",
    "end": "373240"
  },
  {
    "text": "really the the key here is to be able to build on a solution that is designed to serve models at production scale",
    "start": "373240",
    "end": "382639"
  },
  {
    "text": "volumes and just to give you a little bit of a sneak peek in terms of the results that we're going to Showcase",
    "start": "382639",
    "end": "387800"
  },
  {
    "text": "today after you go through this whole tutorial and this is something that you're going to be able to reproduce independently so you know all the code",
    "start": "387800",
    "end": "394120"
  },
  {
    "text": "is there for you to go through we're going to be able to show that we can achieve 47% better accuracy at the task",
    "start": "394120",
    "end": "401199"
  },
  {
    "text": "that I'm going to Showcase today using this open pipe fine-tuning and by deploying the model on octo AI we're",
    "start": "401199",
    "end": "408560"
  },
  {
    "text": "going to achieve this I mean it seems kind of ridiculous 99.5% reduction in cost this is really a",
    "start": "408560",
    "end": "415800"
  },
  {
    "text": "200x reduction in cost here from a gp4 turbo to llama three and mostly because",
    "start": "415800",
    "end": "421000"
  },
  {
    "text": "this is a much smaller model it's open source and we've optimized the hell of this model to serve it cheaply on ooto",
    "start": "421000",
    "end": "427280"
  },
  {
    "text": "AI so I'm going to explain how this is achieved but I hope your interest at least has been peaked on those results",
    "start": "427280",
    "end": "433080"
  },
  {
    "text": "that you yourself can reproduce so when to use fine-tuning uh",
    "start": "433080",
    "end": "439120"
  },
  {
    "text": "again it's not really a silver bullet for all your quality problems it has its right place and time so I like to",
    "start": "439120",
    "end": "445960"
  },
  {
    "text": "contextualize it within the crawl walk run of quality optim ization right and as gen Engineers uh many of us have",
    "start": "445960",
    "end": "453840"
  },
  {
    "text": "embarked on this journey we're at different stages of this journey and really it should always start with",
    "start": "453840",
    "end": "459120"
  },
  {
    "text": "prompt engineering right and many of you are familiar with this concept you start with a model you're trying to have it",
    "start": "459120",
    "end": "465000"
  },
  {
    "text": "accomplish a task and sometimes you don't really manage to see the result you expect to see so you're going to try",
    "start": "465000",
    "end": "471080"
  },
  {
    "text": "prompt engineering and there's different techniques of varying levels of sophistication uh this talk is not about",
    "start": "471080",
    "end": "476520"
  },
  {
    "text": "prompt engineering so you know you can improve prompt specificity there's few shots prompting where you can provide",
    "start": "476520",
    "end": "482599"
  },
  {
    "text": "examples uh to improve essentially the quality of your output there's also Chain of Thought prompting I mean some",
    "start": "482599",
    "end": "488520"
  },
  {
    "text": "of you probably have heard these Concepts but this is where you should get started right make sure that given the model given those weights you just",
    "start": "488520",
    "end": "494800"
  },
  {
    "text": "try to improve the prompt to get the right results sometimes that's not enough and uh there's a second class of",
    "start": "494800",
    "end": "501520"
  },
  {
    "text": "solutions which I like to map to the walk stage um retrieval augmented generation right we've probably seen a",
    "start": "501520",
    "end": "507720"
  },
  {
    "text": "lot of talks on rag today and throughout this conference so you know there's hallucinated results sometimes the",
    "start": "507720",
    "end": "514039"
  },
  {
    "text": "answer is not truthful well why is that it's because the um the weights of the",
    "start": "514039",
    "end": "519599"
  },
  {
    "text": "model that is really the parametric memory of your model is limited to you",
    "start": "519599",
    "end": "525200"
  },
  {
    "text": "know the point and time at which the model was trained so when you try to ask questions on data it hasn't seen or",
    "start": "525200",
    "end": "532320"
  },
  {
    "text": "information that's more recent than when the model was trained it's not going to know how to respond right so the key here is to provide the right amount of",
    "start": "532320",
    "end": "538839"
  },
  {
    "text": "context and so this is achieved through uh similarity search for instance in a vector database through function calling",
    "start": "538839",
    "end": "545880"
  },
  {
    "text": "to bring the right context by invoking an API through search uh through creating a database and so this is",
    "start": "545880",
    "end": "552160"
  },
  {
    "text": "something that I think many of us a Engineers have been dabbing in in order to provide the right context to generate",
    "start": "552160",
    "end": "558000"
  },
  {
    "text": "truthful answer right complement the parametric memory of your model with non-parametric information and that's",
    "start": "558000",
    "end": "564279"
  },
  {
    "text": "rag in a nutshell right so you've tried prompt engineering you've tried rag you've eliminated quality problems and",
    "start": "564279",
    "end": "569920"
  },
  {
    "text": "hallucinations but that's still not enough right so what do you try next well fine tuning I think is uh the next",
    "start": "569920",
    "end": "577240"
  },
  {
    "text": "stage and again I'm generalizing a very complicated and complex Journey but in spite of your best efforts you've tried",
    "start": "577240",
    "end": "583640"
  },
  {
    "text": "these techniques for maybe days weeks or even months and you still don't get to where you need to be to hit production",
    "start": "583640",
    "end": "590200"
  },
  {
    "text": "and we're going to talk about this journey today right fine tuning so when should you f tun a model uh again after",
    "start": "590200",
    "end": "598040"
  },
  {
    "text": "you spend a lot of time in the first two phases of this journey so spending time on prompt engineering spending time on",
    "start": "598040",
    "end": "604720"
  },
  {
    "text": "retrieval augmented generation and you don't see the results improve and",
    "start": "604720",
    "end": "610040"
  },
  {
    "text": "generally what helps is whenever you use an llm for a very specific task something that's very focused for",
    "start": "610040",
    "end": "616360"
  },
  {
    "text": "instance classification information extraction uh trying to format a prompt using it for function calling if you can",
    "start": "616360",
    "end": "623600"
  },
  {
    "text": "narrow the use case uh to something that is highly specific then you have an interesting use case for for um for",
    "start": "623600",
    "end": "630839"
  },
  {
    "text": "applying fine-tuning here and another requirement is to have a lot of your own high quality data to work with because",
    "start": "630839",
    "end": "637360"
  },
  {
    "text": "that's going to be our fine-tuning data set that goes without saying but a model is only as good as the data that it that",
    "start": "637360",
    "end": "643440"
  },
  {
    "text": "the model was trained on and we're going to apply this principle here in this tutorial and finally I think as an added",
    "start": "643440",
    "end": "649440"
  },
  {
    "text": "incentive oftentimes we're all driven by economic incentive in the work we do for those of you who are feeling the Pains",
    "start": "649440",
    "end": "656560"
  },
  {
    "text": "of uh High gen bill whether it is with open AI or with a cloud vendor or a",
    "start": "656560",
    "end": "663079"
  },
  {
    "text": "third party well this is generally a good reason to explore fine tuning so",
    "start": "663079",
    "end": "668560"
  },
  {
    "text": "we're going to go over all the steps now that we've kind of contextualized why fine tuning and when to consider fine",
    "start": "668560",
    "end": "674360"
  },
  {
    "text": "tuning we're going to consider all the steps here in this continuous deployment cycle it starts with building your data",
    "start": "674360",
    "end": "681680"
  },
  {
    "text": "set then running the fine-tuning of the model deploying that fine-tuned llm into",
    "start": "681680",
    "end": "687560"
  },
  {
    "text": "production so you can achieve scale and serve your your you know your customer",
    "start": "687560",
    "end": "692600"
  },
  {
    "text": "needs or internal needs add High volumes and also evaluate quality and this is an",
    "start": "692600",
    "end": "698000"
  },
  {
    "text": "iterative process there's not a single crank of the wheel this is not a fire and forget situation because data that",
    "start": "698000",
    "end": "705000"
  },
  {
    "text": "your model sees in production is going to drift and evolve and so this is something that you're going to have to monitor you're going to have to update",
    "start": "705000",
    "end": "710920"
  },
  {
    "text": "your data set you're going to have to fine-tune your model and I don't want to scare you away from doing this because it sounds fairly daunting and so by the",
    "start": "710920",
    "end": "718880"
  },
  {
    "text": "end of this talk will have gone through a full crank of that wheel and hopefully",
    "start": "718880",
    "end": "724480"
  },
  {
    "text": "you know it through these SAS toolings I'm going to introduce you to is going to feel a lot more approachable and",
    "start": "724480",
    "end": "730680"
  },
  {
    "text": "hopefully I'll demystify the whole process of fine-tuning models so let's start with step one which is to build a",
    "start": "730680",
    "end": "737480"
  },
  {
    "text": "fine-tuning data set now the data of the model has to be trained on ideally real",
    "start": "737480",
    "end": "743279"
  },
  {
    "text": "world uh data right it has to be as close as possible to what you're going to see in production so kind of a",
    "start": "743279",
    "end": "749680"
  },
  {
    "text": "spectrum of ways to build and generate a data set ideally you build a data set",
    "start": "749680",
    "end": "755120"
  },
  {
    "text": "out of real world prompts and real world generated uh real world human responses",
    "start": "755120",
    "end": "760320"
  },
  {
    "text": "so for instance you have customer service you've loged calls with a customer agent you have an interaction",
    "start": "760320",
    "end": "765480"
  },
  {
    "text": "between two humans that's a very good data set to work with right because it's human generated on both ends this is",
    "start": "765480",
    "end": "771680"
  },
  {
    "text": "very high quality but not everyone has the ability to acquire this data set sometimes you're starting from scratch",
    "start": "771680",
    "end": "777959"
  },
  {
    "text": "so not everyone has a luxury to start there there's also kind of an intermediary between real world and",
    "start": "777959",
    "end": "783639"
  },
  {
    "text": "synthetic where you have real world prompts but AI generated responses and so this is kind of a good middle ground",
    "start": "783639",
    "end": "789680"
  },
  {
    "text": "between cost and quality because you're starting from actual uh ground troof information that is derived from real",
    "start": "789680",
    "end": "797199"
  },
  {
    "text": "data but the responses are generated by a highquality llm say gp4 or clawed and",
    "start": "797199",
    "end": "804399"
  },
  {
    "text": "actually open pipe is a solution that allows you to log the inputs and outputs of an El M like gp4 to build your data",
    "start": "804399",
    "end": "811360"
  },
  {
    "text": "set for fine-tuning an llm so this is something that you know a lot of practitioners use and finally there's a",
    "start": "811360",
    "end": "818920"
  },
  {
    "text": "fully synthetic data set using fully AI generated labels and often times when",
    "start": "818920",
    "end": "825160"
  },
  {
    "text": "you go on hugging face or kaggle you'll encounter data sets that have been built entirely synthetically and that's a",
    "start": "825160",
    "end": "831320"
  },
  {
    "text": "great way to kind of get started on this journey and actually one of the data sets we're going to use today is uh from",
    "start": "831320",
    "end": "837279"
  },
  {
    "text": "that latter category and of course I mean it probably goes without saying but in case people are",
    "start": "837279",
    "end": "843240"
  },
  {
    "text": "not fully uh familiar with this notion you want to split your data set into a training and validation um set because",
    "start": "843240",
    "end": "850480"
  },
  {
    "text": "you don't want to evaluate your model on data that your um fine-tune has seen",
    "start": "850480",
    "end": "856720"
  },
  {
    "text": "right and so many of you who are ML and AI Engineers are already familiar with this but I just want to reiterate that",
    "start": "856720",
    "end": "862519"
  },
  {
    "text": "this is important and finally you know this is used for hyperparameter tuning and when you're deploying it and actually testing it on real examples you",
    "start": "862519",
    "end": "869920"
  },
  {
    "text": "want to have a third set outside of training and validation which is your test set now you've built your data set",
    "start": "869920",
    "end": "877680"
  },
  {
    "text": "you're ready to fine-tune your model and there's a lot of decisions that we need to make at this point and the first one",
    "start": "877680",
    "end": "883000"
  },
  {
    "text": "is going to be open source versus closed Source right and so who here just like raise of hands is using proprietary llms",
    "start": "883000",
    "end": "890279"
  },
  {
    "text": "or gen models today from open AI anthropic mstr AI okay good amount of",
    "start": "890279",
    "end": "897440"
  },
  {
    "text": "crowd who here has been using open-source llms like llama some of the",
    "start": "897440",
    "end": "902759"
  },
  {
    "text": "free mraw AI models okay so maybe a smaller crowd right and maybe that's",
    "start": "902759",
    "end": "908680"
  },
  {
    "text": "because these models are not as capable and sophisticated and but I'm going to walk",
    "start": "908680",
    "end": "914519"
  },
  {
    "text": "you through how you can achieve better results if you do fine tuning right so",
    "start": "914519",
    "end": "920440"
  },
  {
    "text": "of course the benefit of Open Source and this is why you know I'm I'm obviously biased but I'm an open source Advocate",
    "start": "920440",
    "end": "926079"
  },
  {
    "text": "is that you have to you get to have ownership over model weights so when once you've done the fine tuning you are",
    "start": "926079",
    "end": "933279"
  },
  {
    "text": "the proprietor of the weights that are the result of this fine tuning process which means that you can choose how you",
    "start": "933279",
    "end": "939279"
  },
  {
    "text": "deploy it how you serve it this is part of your IP and I find that this is a great thing for anyone who wants to",
    "start": "939279",
    "end": "945160"
  },
  {
    "text": "Embark in this fine-tuning journey with proprietary Solutions you're not quite the owner or you don't have the",
    "start": "945160",
    "end": "951160"
  },
  {
    "text": "flexibility to decide to go with another vendor to host the the the models yourself and so you're kind of locked",
    "start": "951160",
    "end": "957120"
  },
  {
    "text": "into an ecosystem some people are comfortable with that others are less comfortable with it and many of the",
    "start": "957120",
    "end": "962639"
  },
  {
    "text": "customers that we talk to they're very eager to jump on the open source train but they don't really know how to get",
    "start": "962639",
    "end": "968440"
  },
  {
    "text": "started or uh you know where to start on this journey so hopefully this can this can help inform you how to take your",
    "start": "968440",
    "end": "975120"
  },
  {
    "text": "first steps here into the world of Open Source then there's a question of like do I use a small model or a large model",
    "start": "975120",
    "end": "982360"
  },
  {
    "text": "because for instance even in the world of Open Source you have models that are in the order of8 billion parameters like llama",
    "start": "982360",
    "end": "987920"
  },
  {
    "text": "38b and then you have the large models with a mix draw 8X 22b so this is a",
    "start": "987920",
    "end": "993319"
  },
  {
    "text": "mixture of expert model with over 100 billion parameters uh very different beasts and we're going to see even",
    "start": "993319",
    "end": "999360"
  },
  {
    "text": "larger models from meta and generally my recommendations here is well look the",
    "start": "999360",
    "end": "1004680"
  },
  {
    "text": "large models are amazing they have broader Contex Windows they have higher capabilities at reasoning but they're",
    "start": "1004680",
    "end": "1012279"
  },
  {
    "text": "also more expensive to fine tune and more expensive to serve and typically when you have to do a deployment you're",
    "start": "1012279",
    "end": "1017560"
  },
  {
    "text": "going to have to acquire resources like h100s to run these models so generally start with a smaller model like a llama",
    "start": "1017560",
    "end": "1023959"
  },
  {
    "text": "3B and sometimes you'll be surprised by its ability to learn specific problems",
    "start": "1023959",
    "end": "1029880"
  },
  {
    "text": "so that's my recommendation start with a smaller Llama 38b Or mistol 7B and if",
    "start": "1029880",
    "end": "1037120"
  },
  {
    "text": "that doesn't work out for you then move towards uh larger and larger models and today we're going to be using this llama",
    "start": "1037120",
    "end": "1042880"
  },
  {
    "text": "38 billon parameter model there's also different techniques for fine-tuning and I'm going to go over",
    "start": "1042880",
    "end": "1048600"
  },
  {
    "text": "this one fairly quickly but there's two classes of of fine-tuning techniques one",
    "start": "1048600",
    "end": "1054360"
  },
  {
    "text": "which is parameter efficient fine-tuning it produces a Laura and the other one is",
    "start": "1054360",
    "end": "1059679"
  },
  {
    "text": "a full parameter fine tuning which produces a checkpoint a Laura is much more much smaller and efficient in terms",
    "start": "1059679",
    "end": "1066559"
  },
  {
    "text": "of memory footprint we're talking about 50 megabytes versus a checkpoint that is 15 gigabytes and so you can guess that",
    "start": "1066559",
    "end": "1075000"
  },
  {
    "text": "because of its more compact representation you're able to serve it on a",
    "start": "1075000",
    "end": "1081520"
  },
  {
    "text": "GPU that doesn't require as much onboard memory and you can even serve multiple",
    "start": "1081520",
    "end": "1086880"
  },
  {
    "text": "lauras at the same time so multiple fine tunes on a GPU for inference as opposed",
    "start": "1086880",
    "end": "1093000"
  },
  {
    "text": "to the checkpoints which require dedicated GPU for every single fine tune so there's more flexibility in",
    "start": "1093000",
    "end": "1099320"
  },
  {
    "text": "deployment and we're going to use that today we're actually going to serve these luras which are the result of parameter efficient fine tuning on a",
    "start": "1099320",
    "end": "1106559"
  },
  {
    "text": "shirt tendency endpoint with other users who have their own luras all running on",
    "start": "1106559",
    "end": "1111880"
  },
  {
    "text": "the same server and that allows us to really reduce the cost of inference and",
    "start": "1111880",
    "end": "1118400"
  },
  {
    "text": "there is a benefit to checkpoints though and full parameter fine tuning which is that there are more parameters to tune",
    "start": "1118400",
    "end": "1124480"
  },
  {
    "text": "so it's a more flexible uh fine-tuning technique it allows the model to have",
    "start": "1124480",
    "end": "1130320"
  },
  {
    "text": "essentially achieve uh better results at more expensive tasks like logical",
    "start": "1130320",
    "end": "1136120"
  },
  {
    "text": "reasoning but for very specialized task which is we going to look at today like classification or labeling or function",
    "start": "1136120",
    "end": "1142120"
  },
  {
    "text": "calling a Laura is just fine so we're going to use parameter efficient fine tuning and also when you're doing fine",
    "start": "1142120",
    "end": "1149919"
  },
  {
    "text": "tuning you have to decide am I going to DIY it or am I going to use SAS so I'm sure some of you only like to diy things",
    "start": "1149919",
    "end": "1156640"
  },
  {
    "text": "others like the convenience of SAS and here I'm not going to take a side I think there's some great tools right now",
    "start": "1156640",
    "end": "1161679"
  },
  {
    "text": "to DIY your own fine-tuning uh for instance the open source uh project Axel",
    "start": "1161679",
    "end": "1167840"
  },
  {
    "text": "and actually at the conference there's the the Creator behind Axel AEL who you might be able to catch um and you know",
    "start": "1167840",
    "end": "1175520"
  },
  {
    "text": "the challenge here is that you have to find your own GPU resources you have to understand how to use these libraries",
    "start": "1175520",
    "end": "1181159"
  },
  {
    "text": "even though they're they're they're easier than ever uh to to adopt and you have to tune and Tinker uh you know",
    "start": "1181159",
    "end": "1188080"
  },
  {
    "text": "settings and hyperparameters then there's SAS which really aim to make it easy to embark on this journey companies",
    "start": "1188080",
    "end": "1194799"
  },
  {
    "text": "like open pipe and there's U many folks from the open pipe at this conference today so if you can catch them please do",
    "start": "1194799",
    "end": "1201120"
  },
  {
    "text": "talk to them and they're trying to lower the be of Entry to F tuning right to make it easy and they bring all this",
    "start": "1201120",
    "end": "1207000"
  },
  {
    "text": "tooling all these libraries to make it as seamless as possible to for instance move from a gbd4 model to a fine-tune",
    "start": "1207000",
    "end": "1213440"
  },
  {
    "text": "with as the least amount of steps in in collecting your data fine-tuning Etc and so we're going to use SAS today but if",
    "start": "1213440",
    "end": "1219320"
  },
  {
    "text": "you feel more comfortable in this journey uh you might want to start with SAS and then evolve into diying",
    "start": "1219320",
    "end": "1226039"
  },
  {
    "text": "it when it comes to deployment you have to navigate the same options right once you have the fine tune model now you",
    "start": "1226039",
    "end": "1231440"
  },
  {
    "text": "need to decide well how am I going to serve it right because I need to generate maybe thousands millions or",
    "start": "1231440",
    "end": "1237000"
  },
  {
    "text": "billions of tokens a day and so you need infrastructure you need gpus you need inference libraries some people like to",
    "start": "1237000",
    "end": "1243559"
  },
  {
    "text": "DIY it using libraries like VM MLM tensor",
    "start": "1243559",
    "end": "1248919"
  },
  {
    "text": "rtlm hogging face TGI if these are all things that you might have heard of uh",
    "start": "1248919",
    "end": "1254280"
  },
  {
    "text": "these are all solutions to run models on your own on your own",
    "start": "1254280",
    "end": "1260600"
  },
  {
    "text": "infrastructure but you need to provision the resources you need to build the infrastructure to scale with demand and",
    "start": "1260600",
    "end": "1266960"
  },
  {
    "text": "that can get tricky especially achieving High reliability under load that's a challenge that many people face as they",
    "start": "1266960",
    "end": "1273440"
  },
  {
    "text": "scale their business up with SAS you can essentially work with a third party like octo aai and obviously I'm a bit biased",
    "start": "1273440",
    "end": "1280919"
  },
  {
    "text": "again I work there so I'm going to insert a Shameless plug for octo AI which allows users to get these fine",
    "start": "1280919",
    "end": "1288880"
  },
  {
    "text": "deployed on SAS based endpoints so endpoints very similar to the ones from open AI for instance if you're familiar",
    "start": "1288880",
    "end": "1295360"
  },
  {
    "text": "with that or Claude and uh it offers the ability to",
    "start": "1295360",
    "end": "1300799"
  },
  {
    "text": "serve different kinds of customizations as well and so very quickly I want to go over the advantages of octo AI here",
    "start": "1300799",
    "end": "1308120"
  },
  {
    "text": "first of all you get speed so llama 38 billion peret model you get achieve",
    "start": "1308120",
    "end": "1313440"
  },
  {
    "text": "around 150 tokens per second and we keep on improving that number because we've been applying our own in-house",
    "start": "1313440",
    "end": "1318919"
  },
  {
    "text": "optimizations to the model serving layer it also has a significant cost Advantage",
    "start": "1318919",
    "end": "1324039"
  },
  {
    "text": "because it costs about 15 cents per million tokens compared to say gp4 which",
    "start": "1324039",
    "end": "1329400"
  },
  {
    "text": "cost $30 per million tokens so that's where the 200x comes from and we don't charge a tax for customization so",
    "start": "1329400",
    "end": "1335440"
  },
  {
    "text": "whether you're serving the base model or a fine tune it's the same cost there's customization as I mentioned you can",
    "start": "1335440",
    "end": "1342320"
  },
  {
    "text": "load your own Laura and serve it and finally scale our customer some of our customers generate up to to uh billions",
    "start": "1342320",
    "end": "1349799"
  },
  {
    "text": "of tokens per day on our endpoints I think we're serving around over 20 billion tokens per day and so we've",
    "start": "1349799",
    "end": "1356880"
  },
  {
    "text": "focused and spent a lot of time on improving robustness and also worth mentioning if SAS doesn't cut it for you",
    "start": "1356880",
    "end": "1365000"
  },
  {
    "text": "you are working for a Fortune 500 company or you know a Healthcare Company",
    "start": "1365000",
    "end": "1371039"
  },
  {
    "text": "a banking sector government you need to deploy your llms inside of your environment either on Prem or in VPC we",
    "start": "1371039",
    "end": "1378120"
  },
  {
    "text": "also have a solution called octo stack come talk to us at the boof so that's it",
    "start": "1378120",
    "end": "1383320"
  },
  {
    "text": "for the Shameless uh plug section let's go over to section four which is evaluating quality right we've talked",
    "start": "1383320",
    "end": "1389080"
  },
  {
    "text": "about data set collection fine-tuning deployment now quality evaluation and we could have an entire conference just",
    "start": "1389080",
    "end": "1395720"
  },
  {
    "text": "dedicated on that I'm going to try to summarize it into kind of two classes of",
    "start": "1395720",
    "end": "1400960"
  },
  {
    "text": "evaluation techniques that I've seen first of all you know can your quality be evaluated in a precise way",
    "start": "1400960",
    "end": "1408200"
  },
  {
    "text": "that can be automated for instance you're generating a program or SQL command that can run uh can you for",
    "start": "1408200",
    "end": "1414480"
  },
  {
    "text": "instance label or extract information or classify information in an accurate way that's a kind of pass orfill scenario",
    "start": "1414480",
    "end": "1421640"
  },
  {
    "text": "right or formatting the output into a specific Json formatting this is something that you can easily test as a",
    "start": "1421640",
    "end": "1428880"
  },
  {
    "text": "pass or fail test and then there's more of the soft evaluation for instance if I were to take an answer and say well",
    "start": "1428880",
    "end": "1435000"
  },
  {
    "text": "which output is written in a more polite or professional way you can't really write a program to evaluate this unless",
    "start": "1435000",
    "end": "1441799"
  },
  {
    "text": "you're using an llm of course right but you have to put yourself into maybe 2 200 sorry 2020 2021 mindset before GPT",
    "start": "1441799",
    "end": "1450960"
  },
  {
    "text": "was around well it' be hard to build a program that can assess this right so generally you need a human in the loop",
    "start": "1450960",
    "end": "1456720"
  },
  {
    "text": "to say which out of out of a or b is a better answer thankfully today we can use llms",
    "start": "1456720",
    "end": "1463880"
  },
  {
    "text": "to automate that evaluation but keep in mind that for instance if you're using gp4 to evaluate two answers well if",
    "start": "1463880",
    "end": "1470799"
  },
  {
    "text": "you're compare against GPT 4 it might favor its own answer and people have seen that in these kind of evaluations",
    "start": "1470799",
    "end": "1476120"
  },
  {
    "text": "so this is a whole science I mean we could have a whole conference just on this I just wanted to present the highlevel uh guidelines of this whole",
    "start": "1476120",
    "end": "1484480"
  },
  {
    "text": "cycle of deploying fine-tune llms and so really there is no Finish Line that's",
    "start": "1484480",
    "end": "1489880"
  },
  {
    "text": "what I want to convey to you all that going through a single iteration is",
    "start": "1489880",
    "end": "1495039"
  },
  {
    "text": "something that you might have to do on a regular basis Maybe once a week maybe once a year it all depends on your use",
    "start": "1495039",
    "end": "1502240"
  },
  {
    "text": "case and constraints now let's get a bit more practical let's switch over to our demo",
    "start": "1502240",
    "end": "1510240"
  },
  {
    "text": "and so for those of you who came uh a little bit late there's a QR code here",
    "start": "1510240",
    "end": "1515480"
  },
  {
    "text": "that you can scan and that will point you to our Google collab and we also",
    "start": "1515480",
    "end": "1521679"
  },
  {
    "text": "have under slack let me see if I can pull it if you're in the slack channel for AI",
    "start": "1521679",
    "end": "1529159"
  },
  {
    "text": "Engineers World Fair there is this uh quality optimization boot camp where you can ask questions here if you want to",
    "start": "1529159",
    "end": "1535159"
  },
  {
    "text": "follow along and so we're going to go we're going to try to go over the uh practical component in the next 25",
    "start": "1535159",
    "end": "1541080"
  },
  {
    "text": "minutes I just want to provide some context here the use case is uh",
    "start": "1541080",
    "end": "1546880"
  },
  {
    "text": "personally identifiable information redaction we've taken this from a data",
    "start": "1546880",
    "end": "1552039"
  },
  {
    "text": "set composed by AI for privacy called Pi masking 200k it's one of the largest",
    "start": "1552039",
    "end": "1557960"
  },
  {
    "text": "data sets of its kind it has 54 different pii classes so different kinds",
    "start": "1557960",
    "end": "1563919"
  },
  {
    "text": "of sensitive data like the name the uh email address ad you know address of",
    "start": "1563919",
    "end": "1569159"
  },
  {
    "text": "physical address of someone uh their credit card information etc etc across",
    "start": "1569159",
    "end": "1575159"
  },
  {
    "text": "229 discussion subjects so that includes conversations from customer ticket",
    "start": "1575159",
    "end": "1580600"
  },
  {
    "text": "resolution conversations with a banker conversations between individuals Etc",
    "start": "1580600",
    "end": "1586679"
  },
  {
    "text": "what this data set looks like is as follows you're going to have a message an email uh here we have you know",
    "start": "1586679",
    "end": "1594440"
  },
  {
    "text": "something that looks like it came out of an email that contains credit card information IP address maybe even a",
    "start": "1594440",
    "end": "1601240"
  },
  {
    "text": "mention of a role or or anything that is essentially personally personally",
    "start": "1601240",
    "end": "1606279"
  },
  {
    "text": "identifiable and I've highlighted those in red because they will need to be",
    "start": "1606279",
    "end": "1611440"
  },
  {
    "text": "redacted and after redaction we should get the following text that shows look here is this information that is now red",
    "start": "1611440",
    "end": "1618240"
  },
  {
    "text": "rected anonymize but instead of just masking it we're actually telling it what kind of category this information",
    "start": "1618240",
    "end": "1624520"
  },
  {
    "text": "belongs to right a credit card number an IP address or job title and this is how",
    "start": "1624520",
    "end": "1629559"
  },
  {
    "text": "we're going to redact this text so where do llms come in the way we would use it",
    "start": "1629559",
    "end": "1635480"
  },
  {
    "text": "is through function calling who here has used llms with tool calls or function",
    "start": "1635480",
    "end": "1641440"
  },
  {
    "text": "calls okay so quite a few people you know and um as as many of us are aware",
    "start": "1641440",
    "end": "1646960"
  },
  {
    "text": "this kind of what a lot of the agentic applications so this is a great use case",
    "start": "1646960",
    "end": "1652320"
  },
  {
    "text": "for people who want to do function calling and are not seeing the results you know out of the box from say",
    "start": "1652320",
    "end": "1658720"
  },
  {
    "text": "gp4 uh that they would like to to to see and in this case we're actually going to see that that these kind of state of the",
    "start": "1658720",
    "end": "1664320"
  },
  {
    "text": "models are doing quite well at fairly large uh and complex function call use",
    "start": "1664320",
    "end": "1669720"
  },
  {
    "text": "cases uh so to achieve this a redaction use case we're going to pass in a system",
    "start": "1669720",
    "end": "1674880"
  },
  {
    "text": "prompt we're going to also pass in a tool specification the system prompt says look you're an expert model trained",
    "start": "1674880",
    "end": "1680960"
  },
  {
    "text": "to do redaction and you can call this function here are all the sensitive uh pii categories for you to",
    "start": "1680960",
    "end": "1687679"
  },
  {
    "text": "redact and then as a user prompt we're going to pass in that email or that message and then the output is a tools",
    "start": "1687679",
    "end": "1694919"
  },
  {
    "text": "call so it's not the redacted text it's actually a tools call to that redact",
    "start": "1694919",
    "end": "1700440"
  },
  {
    "text": "function that's going to contain all the arguments for us to perform the redaction why am I doing this as opposed",
    "start": "1700440",
    "end": "1706200"
  },
  {
    "text": "to spitting out the redactive test well that gives us flexibility in terms of how we want to redact this text we could",
    "start": "1706200",
    "end": "1712840"
  },
  {
    "text": "choose to just replace that information with the pii class we could also",
    "start": "1712840",
    "end": "1718399"
  },
  {
    "text": "completely aisc it or we could choose to use for instance a database that Maps",
    "start": "1718399",
    "end": "1724080"
  },
  {
    "text": "each pii entry to a fake substitute so that we have an email that kind of reads",
    "start": "1724080",
    "end": "1730559"
  },
  {
    "text": "normally except the credit card the the the names the addresses are all made up",
    "start": "1730559",
    "end": "1736799"
  },
  {
    "text": "but they will always map to the same individual and so that allows us to do then more interesting processing on our",
    "start": "1736799",
    "end": "1741919"
  },
  {
    "text": "data set right so that's why we're going to use function calling here and let's",
    "start": "1741919",
    "end": "1747000"
  },
  {
    "text": "start to build the data set so I'm going to switch over to our notebook here uh this notebook is meant to be sort of uh",
    "start": "1747000",
    "end": "1753399"
  },
  {
    "text": "self explainable so there's a bit of redundance redundant context as part of",
    "start": "1753399",
    "end": "1758760"
  },
  {
    "text": "the prerequisites you're going to have to get an account on octo and open pipe um and and these are the tools that",
    "start": "1758760",
    "end": "1765080"
  },
  {
    "text": "we're going to use and if you want to run the evaluation function also provide your open AI key because we're going to",
    "start": "1765080",
    "end": "1771320"
  },
  {
    "text": "compare against gp4 so we're going to install the python packages initially only open Ai and data",
    "start": "1771320",
    "end": "1777399"
  },
  {
    "text": "sets from hugging face you can ignore this uh pip dependency error here uh which happens when you pip install data",
    "start": "1777399",
    "end": "1784679"
  },
  {
    "text": "sets in a collab notebook but that's okay we can get past that you can enter your octo AI token and open AI API key",
    "start": "1784679",
    "end": "1792880"
  },
  {
    "text": "at the beginning and I've already done this so we're going to start with the first phase which is to build a",
    "start": "1792880",
    "end": "1798080"
  },
  {
    "text": "fine-tuning data set so we have this Pi masking data set I'm going to show it from hugging face so Pi uh masking and",
    "start": "1798080",
    "end": "1807000"
  },
  {
    "text": "you can see what the data set looks like it has the source text information as",
    "start": "1807000",
    "end": "1812200"
  },
  {
    "text": "you can see these are you know exchange you know Snippets from emails for instance you have the target text that",
    "start": "1812200",
    "end": "1818240"
  },
  {
    "text": "is redacted and the Privacy mask that contains each one of the pii and the classes Associated to it so this",
    "start": "1818240",
    "end": "1825679"
  },
  {
    "text": "contains all the data all the information in put and labels that we need to build our uh our data set for",
    "start": "1825679",
    "end": "1832799"
  },
  {
    "text": "fine tuning and so really what we're going to do is that we're going to use the system",
    "start": "1832799",
    "end": "1840559"
  },
  {
    "text": "prompt here we're going to Define our system prompt here which is again telling the model you're an expert model",
    "start": "1840559",
    "end": "1846960"
  },
  {
    "text": "trained to redact information and here are the 56 categories explaining next to",
    "start": "1846960",
    "end": "1852840"
  },
  {
    "text": "each category what that corresponds to and this is really the beauty of llm and sort of natural language entry is that",
    "start": "1852840",
    "end": "1860399"
  },
  {
    "text": "in the old world when we were doing Pi redaction we had to write complex regular expressions and here this is all",
    "start": "1860399",
    "end": "1866399"
  },
  {
    "text": "done through just providing a category and a bit of a description here and the llm will naturally infer how to do the",
    "start": "1866399",
    "end": "1873720"
  },
  {
    "text": "re redaction we're also going to uh Define the tool to call right and so",
    "start": "1873720",
    "end": "1879559"
  },
  {
    "text": "this is done as a uh essentially a dictionary adjacent object and as you can see there is an array that contains",
    "start": "1879559",
    "end": "1887679"
  },
  {
    "text": "uh dictionaries containing a string and a pi type and the string is the pi",
    "start": "1887679",
    "end": "1893840"
  },
  {
    "text": "information the type is essentially one of 56 categories that we provide as an enum so right off the bat you can see",
    "start": "1893840",
    "end": "1900240"
  },
  {
    "text": "that this tool call is um you know a bit of a large function uh",
    "start": "1900240",
    "end": "1905799"
  },
  {
    "text": "specification and so let's load our data set from hugging face in this case it's going to take maybe a few seconds to",
    "start": "1905799",
    "end": "1912200"
  },
  {
    "text": "load in that data set of 200,000 entries and then what I have in the next cell",
    "start": "1912200",
    "end": "1917559"
  },
  {
    "text": "when I'm downloading this uh data set is what I'm going to use to build my",
    "start": "1917559",
    "end": "1924000"
  },
  {
    "text": "fine-tuning training data set and here's the thing about fine-tuning is that to",
    "start": "1924000",
    "end": "1929600"
  },
  {
    "text": "build your data set you need to make it seem like you've essentially logged conversations with an llm right you're",
    "start": "1929600",
    "end": "1936039"
  },
  {
    "text": "logging the prompts and the responses because that's how you're going to fine-tune it you need to tell it this is the input with system prompt uh tools",
    "start": "1936039",
    "end": "1944799"
  },
  {
    "text": "specification user prompt and here's the uh tools call response that I expect to",
    "start": "1944799",
    "end": "1951480"
  },
  {
    "text": "see and so this cell here just sets it up so that we essentially have each training sample as a message from an nlm",
    "start": "1951480",
    "end": "1959600"
  },
  {
    "text": "that's been logged we're going to see what that looks like in a second so we're going to build a 10,000",
    "start": "1959600",
    "end": "1966159"
  },
  {
    "text": "entry training data set for open pipe and that's going to be downloaded",
    "start": "1966159",
    "end": "1972519"
  },
  {
    "text": "as this open pip data set. jonl and so as I run the cell it's going",
    "start": "1972519",
    "end": "1978000"
  },
  {
    "text": "to download this uh from collab and now when you switch over to",
    "start": "1978000",
    "end": "1984559"
  },
  {
    "text": "open pipe we're going to create a new data set so once you're on open pipe console you have a project here I've",
    "start": "1984559",
    "end": "1991960"
  },
  {
    "text": "generically named IT project one you can access data sets and I already as you",
    "start": "1991960",
    "end": "1997279"
  },
  {
    "text": "can see I already have built a few data sets uh before but if you're a first-time user you're not going to see",
    "start": "1997279",
    "end": "2003360"
  },
  {
    "text": "anything under data sets so you can create a new data set here by clicking on this button",
    "start": "2003360",
    "end": "2009080"
  },
  {
    "text": "and if you go under settings we can name our data set so I'm going to call it uh lunch and",
    "start": "2009080",
    "end": "2016120"
  },
  {
    "text": "learn and today is uh June 26 all right so this is today's lunch",
    "start": "2016120",
    "end": "2022480"
  },
  {
    "text": "and learn I'm going to I'm going to call this my data set and under General I can",
    "start": "2022480",
    "end": "2027880"
  },
  {
    "text": "upload the data that I just downloaded from my notebook open pipe data set.",
    "start": "2027880",
    "end": "2033000"
  },
  {
    "text": "jonl so this upload operation is going to take a few",
    "start": "2033000",
    "end": "2038320"
  },
  {
    "text": "seconds or maybe a couple of minutes because what's going to happen on open PB is not only we're uploading this data",
    "start": "2038320",
    "end": "2044360"
  },
  {
    "text": "set but it's GNA do some uh pre-processing here to split it into uh training and validation set it's also",
    "start": "2044360",
    "end": "2051760"
  },
  {
    "text": "going to get it all formatted in a nice way so we can essentially look into the",
    "start": "2051760",
    "end": "2056960"
  },
  {
    "text": "data set uh so you can see there's this little window here that shows that you're uploading the data set and that",
    "start": "2056960",
    "end": "2062720"
  },
  {
    "text": "it is essentially being processed so while this is happening",
    "start": "2062720",
    "end": "2068960"
  },
  {
    "text": "right we've prepared our data set and we're going to take a look at it in a second while it's being processed on open pipe but let's see how we're going",
    "start": "2068960",
    "end": "2075398"
  },
  {
    "text": "to do the fine-tuning in the next stage right so once we have our data set uploaded we're going to have this view",
    "start": "2075399",
    "end": "2082240"
  },
  {
    "text": "on the data set that shows every single entry that we can Peak into and how it's split into training and test set",
    "start": "2082240",
    "end": "2088878"
  },
  {
    "text": "generally a 90 10% split and from that UI we can launch a",
    "start": "2088879",
    "end": "2094560"
  },
  {
    "text": "finetune and this is where we get to choose our base model model and what we're going to choose is a llama 3 8",
    "start": "2094560",
    "end": "2100119"
  },
  {
    "text": "billion parameter model with 32k Contex width which is a uh fine tune",
    "start": "2100119",
    "end": "2107520"
  },
  {
    "text": "from news research called the Theta model and you can see that there's",
    "start": "2107520",
    "end": "2113079"
  },
  {
    "text": "essentially a pricing here that is being estimated for this fine tune we have a substantial training set because it can",
    "start": "2113079",
    "end": "2120079"
  },
  {
    "text": "range from say hundreds of samples to thousands to hundreds of thousands and the cost can scale up as",
    "start": "2120079",
    "end": "2127240"
  },
  {
    "text": "you um as you feed in more training samples but it will improve the accuracy",
    "start": "2127240",
    "end": "2132720"
  },
  {
    "text": "and it also provides an estimated training price of $40 now that might seem like a lot especially when you're tinkering with fine-tuning but keep in",
    "start": "2132720",
    "end": "2139960"
  },
  {
    "text": "mind some of the people that we work with they tend to spend tens of thousands or maybe hundreds of thousands of dollars a month on gen I spend so",
    "start": "2139960",
    "end": "2147280"
  },
  {
    "text": "this is absolutely something that you can do up front that will pay off and I believe that on open pipe if you get",
    "start": "2147280",
    "end": "2152920"
  },
  {
    "text": "started you get $100 credit uh so that allows you to to run some fine tunes off",
    "start": "2152920",
    "end": "2158599"
  },
  {
    "text": "the bat uh without having to necessarily uh have to to pay so um let's go over to",
    "start": "2158599",
    "end": "2166160"
  },
  {
    "text": "open pipe and it is still uploading I think maybe the network is uh is a bit slow",
    "start": "2166160",
    "end": "2173440"
  },
  {
    "text": "but we're going to essentially start training at this point and once the training is happening we're going to",
    "start": "2173440",
    "end": "2180160"
  },
  {
    "text": "then deploy the fine tun llm uh when training is done and what happens on",
    "start": "2180160",
    "end": "2185319"
  },
  {
    "text": "open pipe is when you're done with training you're going to get an email when that training job is done it can",
    "start": "2185319",
    "end": "2190800"
  },
  {
    "text": "take a few minutes so I'm going to pull a jewelia child here I'm going to stick the you know the turkey in the oven and",
    "start": "2190800",
    "end": "2196040"
  },
  {
    "text": "in the second oven I'm going to have a a pre-baked turkey just so that we don't lose time but as you're going through",
    "start": "2196040",
    "end": "2201839"
  },
  {
    "text": "this on your own keep in mind it's going to take a little bit of time to just kick off that whole fine-tuning process",
    "start": "2201839",
    "end": "2207280"
  },
  {
    "text": "but it's not that long because um you know you're training a fairly small model",
    "start": "2207280",
    "end": "2212960"
  },
  {
    "text": "here all right so this is still uh saving but let let's kind of take a look",
    "start": "2212960",
    "end": "2218240"
  },
  {
    "text": "at what we've done so far right so we've built our data set using a synthetic data set from hugging face uh we format",
    "start": "2218240",
    "end": "2224440"
  },
  {
    "text": "each input output pair from the data set as logged llm messages and this is",
    "start": "2224440",
    "end": "2229760"
  },
  {
    "text": "essentially stored as a jonl file that we upload to open pipe and we produce 10,000 training samples we're",
    "start": "2229760",
    "end": "2236760"
  },
  {
    "text": "fine-tuning a model from open pipe and we're open pipe uses a parameter efficient fine-tuning which produces",
    "start": "2236760",
    "end": "2242280"
  },
  {
    "text": "aora and we choose llama 38 billon parameter model as the base",
    "start": "2242280",
    "end": "2247640"
  },
  {
    "text": "and when we deploy what we're going to use here is octo AI so let's see this didn't finish",
    "start": "2247640",
    "end": "2254240"
  },
  {
    "text": "uploading so I'm going to go into the one that I uploaded just a couple days ago just to essentially show you what",
    "start": "2254240",
    "end": "2260680"
  },
  {
    "text": "you should see on the uh user interface so as you peruse through the",
    "start": "2260680",
    "end": "2267520"
  },
  {
    "text": "training samples what you're going to see is an input column and output column and so on the left you have the input",
    "start": "2267520",
    "end": "2273640"
  },
  {
    "text": "with the system prompt as you can see it's a it's a big boy uh big because it has all these different categories right",
    "start": "2273640",
    "end": "2279760"
  },
  {
    "text": "that it needs to classify it also has the user prompt which is the message that we need to redact the tool choice",
    "start": "2279760",
    "end": "2287000"
  },
  {
    "text": "and the tool specification here with all the different categories of pii types and then the output will be will be this",
    "start": "2287000",
    "end": "2293640"
  },
  {
    "text": "tools call from the assistance response and that will have this redact call",
    "start": "2293640",
    "end": "2299200"
  },
  {
    "text": "along with these arguments field to redact as a list of dictionary entries containing string and Pi type",
    "start": "2299200",
    "end": "2305760"
  },
  {
    "text": "information right and so this is what we've passed into our",
    "start": "2305760",
    "end": "2311480"
  },
  {
    "text": "fine-tuning data set into open pipe and this is still saving so I'm",
    "start": "2311480",
    "end": "2317880"
  },
  {
    "text": "just going to go ahead and go to the model so once you have the data set uploaded",
    "start": "2317880",
    "end": "2323960"
  },
  {
    "text": "again you hit this fine-tune button and this is what's going to allow you to launch a fine-tuning job right I can",
    "start": "2323960",
    "end": "2329040"
  },
  {
    "text": "call this blah and this is where you select under this drop down the model that you want",
    "start": "2329040",
    "end": "2334079"
  },
  {
    "text": "to fine-tune uh this is again what we saw four training size is substantial I'm",
    "start": "2334079",
    "end": "2339319"
  },
  {
    "text": "not going to hit start training because I already have a trained model but when you do that it's going to kick off the training and when it's done you'll get",
    "start": "2339319",
    "end": "2345119"
  },
  {
    "text": "notified by email now let's fast forward let's assume I've already trained my model so",
    "start": "2345119",
    "end": "2350560"
  },
  {
    "text": "I'm going to have this model here that's been fine tuned from this data set I'm",
    "start": "2350560",
    "end": "2355839"
  },
  {
    "text": "going to click on it as we can see it's an llama 38b model it's been fine-tuned",
    "start": "2355839",
    "end": "2361440"
  },
  {
    "text": "over these 10,000 data sets split into 9,000 uh training samples and a00 test",
    "start": "2361440",
    "end": "2368359"
  },
  {
    "text": "samples we can even look at the evaluation but going back to the",
    "start": "2368359",
    "end": "2376280"
  },
  {
    "text": "model and the nice thing is that it's taking care of the hyperparameter like learning rate number of appbox It kind",
    "start": "2376280",
    "end": "2382560"
  },
  {
    "text": "of figures it out for you so you don't really have to tweak those settings and I find that to be very convenient especially for people who haven't yet",
    "start": "2382560",
    "end": "2388680"
  },
  {
    "text": "built an understanding of how to tweak those values and the beauty of using open pipe",
    "start": "2388680",
    "end": "2395240"
  },
  {
    "text": "is that you can now export the weights and be the owner of those weights right remember when we talked about open source it's really important to own the",
    "start": "2395240",
    "end": "2401920"
  },
  {
    "text": "result of the fine-tuning so you can download the weights in any format you want you have luras but also merg",
    "start": "2401920",
    "end": "2408160"
  },
  {
    "text": "checkpoints so you can have a a a parameter efficient representation as well as a checkpoint and so we've",
    "start": "2408160",
    "end": "2414400"
  },
  {
    "text": "selected to export our model as a fb16 Laura which is what we're going to use to upload our model on octo AI which is",
    "start": "2414400",
    "end": "2421280"
  },
  {
    "text": "where we're going to use to deploy the model so now I can download the weights as a zip file and it's Prett fairly",
    "start": "2421280",
    "end": "2427640"
  },
  {
    "text": "small only 50 megabytes but I can also copy the link copy the URL and this is",
    "start": "2427640",
    "end": "2433720"
  },
  {
    "text": "what we're going to need to do in this tutorial so to deploy the model what we",
    "start": "2433720",
    "end": "2438960"
  },
  {
    "text": "need to do is copy this URL I'm going to download in the cell the otoi CLI this",
    "start": "2438960",
    "end": "2446520"
  },
  {
    "text": "is a command line interface for users to upload their own fine tunes to what we",
    "start": "2446520",
    "end": "2451800"
  },
  {
    "text": "call our asset Library so this is a place where you can store your own checkpoints your own luras for not just",
    "start": "2451800",
    "end": "2456960"
  },
  {
    "text": "l but also models like stable diffusion if some of you are developers who also work in the image gen space and so we",
    "start": "2456960",
    "end": "2464319"
  },
  {
    "text": "can serve these customized models uh on our platform and so we're going to upload",
    "start": "2464319",
    "end": "2471359"
  },
  {
    "text": "this Laura from open pipe to octo AI so",
    "start": "2471359",
    "end": "2476800"
  },
  {
    "text": "we're going to log in just to make sure credentials are good and here we have confirmation that our token is valid and",
    "start": "2476800",
    "end": "2484040"
  },
  {
    "text": "in the cell we have to replace the Laura URL from set me to that URL that I just",
    "start": "2484040",
    "end": "2489599"
  },
  {
    "text": "copied here from download weights and keep in mind this might take a couple minutes to get the link to",
    "start": "2489599",
    "end": "2496000"
  },
  {
    "text": "appear but once you have that link and again I'm kind of skipping ahead because",
    "start": "2496000",
    "end": "2501560"
  },
  {
    "text": "when you're going to run this at your own time it might take a you know a few minutes to run the fine tune it might take a few minutes to download the",
    "start": "2501560",
    "end": "2507160"
  },
  {
    "text": "weights but everything that I'm running here is essentially the steps that you'll take yourself and what I'm doing",
    "start": "2507160",
    "end": "2512960"
  },
  {
    "text": "here is uh passing in this URL here and set setting a Laura asset name in my",
    "start": "2512960",
    "end": "2520599"
  },
  {
    "text": "octo asset Library so I can then create this asset from this Laura as a safe",
    "start": "2520599",
    "end": "2527960"
  },
  {
    "text": "tensor file and uh based on the Llama 38b model I'm going to name it uh let's",
    "start": "2527960",
    "end": "2538039"
  },
  {
    "text": "see seems like something has failed here so let's try to run it again",
    "start": "2538200",
    "end": "2547880"
  },
  {
    "text": "and so what this is doing is uh let's",
    "start": "2558640",
    "end": "2562880"
  },
  {
    "text": "see usually that that uh that should have worked so what's uh what should happen",
    "start": "2566319",
    "end": "2573640"
  },
  {
    "text": "here is at this point once you've taken the uh URL of your finetune",
    "start": "2573640",
    "end": "2580559"
  },
  {
    "text": "asset should be able to host it on our asset library and then from there serve",
    "start": "2580559",
    "end": "2585839"
  },
  {
    "text": "it to start running some inferences so this",
    "start": "2585839",
    "end": "2591559"
  },
  {
    "text": "uh this this Laura upload step didn't quite work here so Pedro are you're able",
    "start": "2591559",
    "end": "2597960"
  },
  {
    "text": "to maybe double check with product whether this capability is working this isn't a good demo unless",
    "start": "2597960",
    "end": "2604720"
  },
  {
    "text": "something fails and so so uh yeah you know I just tested it earlier today and",
    "start": "2604720",
    "end": "2609960"
  },
  {
    "text": "it was working flawlessly so uh let's see I might have to list my",
    "start": "2609960",
    "end": "2617680"
  },
  {
    "text": "assets so I can pull an old one um",
    "start": "2617680",
    "end": "2622760"
  },
  {
    "text": "actually one second Pedro can you can you tell me what the uh commend is to to",
    "start": "2622760",
    "end": "2629160"
  },
  {
    "text": "list the assets that are on I think it might be octo octo asset list all right",
    "start": "2629160",
    "end": "2636200"
  },
  {
    "text": "let's",
    "start": "2636200",
    "end": "2639040"
  },
  {
    "text": "okay there we go so I'm going to pull from an asset that I uploaded",
    "start": "2644800",
    "end": "2649880"
  },
  {
    "text": "earlier the third one all",
    "start": "2651280",
    "end": "2658440"
  },
  {
    "text": "right yeah because all right so I'm going to take an asset that I uploaded earlier I'm",
    "start": "2660000",
    "end": "2666599"
  },
  {
    "text": "going to have to look into why that step failed but uh",
    "start": "2666599",
    "end": "2672240"
  },
  {
    "text": "let's let's try this okay so I'm going to use an asset",
    "start": "2673400",
    "end": "2678720"
  },
  {
    "text": "that I uploaded earlier I'm not sure why this didn't work but I'll make sure that this is working for you all to reproduce the",
    "start": "2678720",
    "end": "2684800"
  },
  {
    "text": "step and I'm going to set Laura asset",
    "start": "2684800",
    "end": "2690040"
  },
  {
    "text": "name equals this all right so these other luras I uploaded using the exact",
    "start": "2690040",
    "end": "2696480"
  },
  {
    "text": "same steps as as I use for this uh tutorial so we'll make sure to get to the bottom of this and uh we'll use the",
    "start": "2696480",
    "end": "2702839"
  },
  {
    "text": "slack channel here for folks who want to run through this uh step but I'm just going to run an example inference here",
    "start": "2702839",
    "end": "2709440"
  },
  {
    "text": "on this asset that I pulled from open pipe and so again we have our system",
    "start": "2709440",
    "end": "2715839"
  },
  {
    "text": "prompt we're going to pass in this uh ex you know this message this email as our",
    "start": "2715839",
    "end": "2721559"
  },
  {
    "text": "test prompt and then when we're invoking this octo endpoint we're using the standard chat completions uh from open",
    "start": "2721559",
    "end": "2730000"
  },
  {
    "text": "Ai and what we're passing here is this open pipe Lama 3 8B 32k model and we",
    "start": "2730000",
    "end": "2736720"
  },
  {
    "text": "pass in this argument for parameter efficient fine-tune and pass in the Laura asset",
    "start": "2736720",
    "end": "2743319"
  },
  {
    "text": "name that we just uploaded to the asset library and as we can see the response",
    "start": "2743319",
    "end": "2748880"
  },
  {
    "text": "here contains the tool calls and the call to the function that will do the redaction so this is behaving exactly as",
    "start": "2748880",
    "end": "2755640"
  },
  {
    "text": "we intended to so now we can move on to the Quality evaluation for Quality",
    "start": "2755640",
    "end": "2760880"
  },
  {
    "text": "evaluation what we've done is use essentially an accuracy metric",
    "start": "2760880",
    "end": "2766079"
  },
  {
    "text": "thankfully we have a ground proof right from our data set all the exchanges have been labeled with privacy",
    "start": "2766079",
    "end": "2773119"
  },
  {
    "text": "mask information that we can use as ground truth so that makes evaluating scoring or results fairly easy we don't",
    "start": "2773119",
    "end": "2779599"
  },
  {
    "text": "have to use an llm for instance for that we can actually use more traditional techniques of accuracy evaluation and so",
    "start": "2779599",
    "end": "2786000"
  },
  {
    "text": "we have a metric that we've built it assigns a score that can be penalized",
    "start": "2786000",
    "end": "2791480"
  },
  {
    "text": "when pi information was missed or mistakenly added I.E false negative false positive and then we use a similar",
    "start": "2791480",
    "end": "2798680"
  },
  {
    "text": "distance metric to kind of match the responses from the llms compared to our",
    "start": "2798680",
    "end": "2803839"
  },
  {
    "text": "ground Troth so for illustration purposes we have for instance this Pi",
    "start": "2803839",
    "end": "2809119"
  },
  {
    "text": "information that's been redacted that's a score of 1.0 because it's the perfect match or fine tune my for instance miss",
    "start": "2809119",
    "end": "2816520"
  },
  {
    "text": "the fact that Billy was the middle name and my interpreted as first name in that case we're still attributing a high",
    "start": "2816520",
    "end": "2822440"
  },
  {
    "text": "score because it's close enough and probably for a practical use case that would be good enough but for instance",
    "start": "2822440",
    "end": "2828760"
  },
  {
    "text": "upon calling gbd4 it fails to identify two out of the three information that we had to redact and so the score is about",
    "start": "2828760",
    "end": "2835440"
  },
  {
    "text": "a third here right so in this case what we're going to do here I'm just going to",
    "start": "2835440",
    "end": "2840599"
  },
  {
    "text": "reduce the test size to 100 samples and I am going to run this",
    "start": "2840599",
    "end": "2845800"
  },
  {
    "text": "evaluation inside of this cell it's GNA",
    "start": "2845800",
    "end": "2850880"
  },
  {
    "text": "bring us a 100 test samples that we can then run our evaluation metric and get",
    "start": "2850880",
    "end": "2857680"
  },
  {
    "text": "our overall scoring out of uh so if we look at you know the uh output from the",
    "start": "2857680",
    "end": "2867040"
  },
  {
    "text": "cell essentially we're just invoke invoking backto back the fine-tune",
    "start": "2867040",
    "end": "2872160"
  },
  {
    "text": "running octo Ai and we're invoking gp4 on open AI to do the results collection",
    "start": "2872160",
    "end": "2879119"
  },
  {
    "text": "so we're going to collect some results here and uh once we've collected the",
    "start": "2879119",
    "end": "2884359"
  },
  {
    "text": "results once we get to 100 I think we're getting pretty close here we can run the quality valuation metric and of course I",
    "start": "2884359",
    "end": "2890280"
  },
  {
    "text": "invite you to run it on more samples maybe a th000 or 10,000 uh it just gets more expensive as you're using gp4 you",
    "start": "2890280",
    "end": "2898640"
  },
  {
    "text": "know to run a 100 samples it cost about a dollar in inference so uh then a th000",
    "start": "2898640",
    "end": "2904240"
  },
  {
    "text": "samples cost $10 and now we're going to score it all right so we're going to go through every",
    "start": "2904240",
    "end": "2910079"
  },
  {
    "text": "single entry we have our ground proof information we have our eval and labels from gp4 and our",
    "start": "2910079",
    "end": "2918319"
  },
  {
    "text": "eval and labels from our fine tune and we can see that right off the bat the fine tune is actually better at finding",
    "start": "2918319",
    "end": "2925400"
  },
  {
    "text": "the pi to redact here gbd4 scored only a score of 0.49 whereas our fine tune",
    "start": "2925400",
    "end": "2932440"
  },
  {
    "text": "achieved 0.85 and here 0.3 for GPT 4 1 .04 the",
    "start": "2932440",
    "end": "2937559"
  },
  {
    "text": "fine tune so the fine tune overall is performing better and once we Aggregate and average the score gp4 achieved",
    "start": "2937559",
    "end": "2946119"
  },
  {
    "text": "0.68 out of one whereas our fine tune achieve",
    "start": "2946119",
    "end": "2951240"
  },
  {
    "text": "0.97 and so that's a difference between prototype and production right you're expected to achieve somewhere in the",
    "start": "2951240",
    "end": "2957119"
  },
  {
    "text": "single nine or two nines of accuracy and this is what this technique shows allows you to achieve and again I want to",
    "start": "2957119",
    "end": "2963920"
  },
  {
    "text": "reiterate that in terms of cost GPD 4 costs upward to $30 per million tokens",
    "start": "2963920",
    "end": "2970480"
  },
  {
    "text": "generated whereas llama 38b on octo cost just 15 cents that's a 200x difference",
    "start": "2970480",
    "end": "2976319"
  },
  {
    "text": "right so with that I just want to conclude with some takeaways on fine on",
    "start": "2976319",
    "end": "2984440"
  },
  {
    "text": "fine tuning right fine tuning is a journey but a very rewarding Journey there's truly no Finish Line here you",
    "start": "2984440",
    "end": "2991559"
  },
  {
    "text": "need to attempt fine tuning after you already tried other techniques like prompt engineering retrieval augmented generation",
    "start": "2991559",
    "end": "2997400"
  },
  {
    "text": "but once you decide to Embark data is very important collecting your data set",
    "start": "2997400",
    "end": "3002880"
  },
  {
    "text": "because your model is only as good as a data it's trained on you need to make sure to continuously monitor quality to",
    "start": "3002880",
    "end": "3008760"
  },
  {
    "text": "retune your model as needed you also need to um you know thankfully we have",
    "start": "3008760",
    "end": "3014680"
  },
  {
    "text": "Solutions like octo and open pipe to really make this more approachable and easy to do and it's easier than ever",
    "start": "3014680",
    "end": "3021520"
  },
  {
    "text": "it's only going to get easier but maybe a year ago was only reserved for the most adventurous and sophisticated users",
    "start": "3021520",
    "end": "3027440"
  },
  {
    "text": "and now we really lower the barrier of entry and when you do it right you can achieve really significant improvements",
    "start": "3027440",
    "end": "3033040"
  },
  {
    "text": "in accuracy as well as great reduction in costs I wanted to thank you for sitting",
    "start": "3033040",
    "end": "3038799"
  },
  {
    "text": "here with me over the last 50 minutes I want to reiterate a few calls to action so go to oo. Cloud to learn how to use",
    "start": "3038799",
    "end": "3046520"
  },
  {
    "text": "our Solutions and endpoints but also come to our booth and uh so we're",
    "start": "3046520",
    "end": "3051599"
  },
  {
    "text": "located at this G7 booth and we're going to be here today and tomorrow if if you",
    "start": "3051599",
    "end": "3056760"
  },
  {
    "text": "want to chat about our SAS endpoints about our ability to deploy in an",
    "start": "3056760",
    "end": "3061880"
  },
  {
    "text": "Enterprise environment and also I want to give a shout out to my colleague here Pedro if",
    "start": "3061880",
    "end": "3066960"
  },
  {
    "text": "you're curious about all the knowhow that goes behind how we optimize our model and production because our background is in compiler optimization",
    "start": "3066960",
    "end": "3073760"
  },
  {
    "text": "is is in system optimization infrastructure optimization we've applied all of this to be able to server",
    "start": "3073760",
    "end": "3079960"
  },
  {
    "text": "models you know with positive margins we're not doing this at a loss right we're not wasting our VC money here",
    "start": "3079960",
    "end": "3086599"
  },
  {
    "text": "we're actually building all this knoow into making sure that AI inference is as",
    "start": "3086599",
    "end": "3092119"
  },
  {
    "text": "efficient as it could be so there's going to be a talk on that and also make sure if you if you get a chance assuming",
    "start": "3092119",
    "end": "3100000"
  },
  {
    "text": "you've joined our our uh slack Channel which is the",
    "start": "3100000",
    "end": "3106160"
  },
  {
    "text": "following one so if you're on the slack org for the event go to llm",
    "start": "3106160",
    "end": "3111760"
  },
  {
    "text": "Quality optimization boot camp you can ask us any questions and if you fill out",
    "start": "3111760",
    "end": "3117040"
  },
  {
    "text": "the survey that Pedro is going to post we're going to give you an additional $10 in credits uh so that doesn't seem",
    "start": "3117040",
    "end": "3124119"
  },
  {
    "text": "like a lot but that's a ton you know if it's 15 cents per million tokens that's a lot of uh tokens that you can generate",
    "start": "3124119",
    "end": "3131000"
  },
  {
    "text": "for free so we can give you an additional 10 uh dollars for uh filling out the survey which which should take",
    "start": "3131000",
    "end": "3136920"
  },
  {
    "text": "about you know 20 to 30 seconds so I'm going to be around and also you can find me at the booth this afternoon in case",
    "start": "3136920",
    "end": "3143119"
  },
  {
    "text": "you have any questions but I like you all uh to thank you for sitting through this talk and hopefully hopefully you've",
    "start": "3143119",
    "end": "3148839"
  },
  {
    "text": "learned something from this and hopefully you feel like I've I've demystified this idea of trying fine tuning on your own give this notebook a",
    "start": "3148839",
    "end": "3156079"
  },
  {
    "text": "try assuming of course we fixed this laowa upload issue and uh yeah thank you",
    "start": "3156079",
    "end": "3161160"
  },
  {
    "text": "all and maybe ask me some questions after this uh after this talk thanks",
    "start": "3161160",
    "end": "3167720"
  },
  {
    "text": "[Music]",
    "start": "3168990",
    "end": "3186579"
  }
]