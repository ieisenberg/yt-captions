[
  {
    "text": "[Music]",
    "start": "3520",
    "end": "7860"
  },
  {
    "text": "couple different things right like you",
    "start": "13559",
    "end": "14920"
  },
  {
    "text": "know people have been talking about",
    "start": "14920",
    "end": "16880"
  },
  {
    "text": "stagnation um and uh it's I I don't",
    "start": "16880",
    "end": "19720"
  },
  {
    "text": "think anyone else anyone here sees that",
    "start": "19720",
    "end": "21840"
  },
  {
    "text": "a lot of people have been talking about",
    "start": "21840",
    "end": "22880"
  },
  {
    "text": "stagnation of models and a lot of a lot",
    "start": "22880",
    "end": "25439"
  },
  {
    "text": "of that has to just do with the fact",
    "start": "25439",
    "end": "27000"
  },
  {
    "text": "that we haven't seen a big capabilities",
    "start": "27000",
    "end": "29000"
  },
  {
    "text": "leap uh the last bit uh but that that",
    "start": "29000",
    "end": "31759"
  },
  {
    "text": "comes really from uh models that we're",
    "start": "31759",
    "end": "35320"
  },
  {
    "text": "using today are largely the same as the",
    "start": "35320",
    "end": "37600"
  },
  {
    "text": "models that were trained in 2022 right",
    "start": "37600",
    "end": "39640"
  },
  {
    "text": "gp4 4 Turbo 40 those are just smaller",
    "start": "39640",
    "end": "42800"
  },
  {
    "text": "models that are trained for longer so",
    "start": "42800",
    "end": "44320"
  },
  {
    "text": "similar quality right um you know 3.5",
    "start": "44320",
    "end": "46879"
  },
  {
    "text": "CET came out recently but again that's",
    "start": "46879",
    "end": "48640"
  },
  {
    "text": "actually smaller than Opus but it's",
    "start": "48640",
    "end": "50680"
  },
  {
    "text": "somehow better because they trained it",
    "start": "50680",
    "end": "52000"
  },
  {
    "text": "for longer right but we haven't seen a",
    "start": "52000",
    "end": "54120"
  },
  {
    "text": "extremely large Model come out yet and",
    "start": "54120",
    "end": "56320"
  },
  {
    "text": "and but we will soon uh but one",
    "start": "56320",
    "end": "58199"
  },
  {
    "text": "interesting thing right is gbd4 is like",
    "start": "58199",
    "end": "59960"
  },
  {
    "text": ".8 trillion parameters it's crazy crazy",
    "start": "59960",
    "end": "62039"
  },
  {
    "text": "expensive to run right uh 200 billion",
    "start": "62039",
    "end": "65158"
  },
  {
    "text": "parameters uh each token requires you",
    "start": "65159",
    "end": "68040"
  },
  {
    "text": "know almost 600 gig flops uh but that",
    "start": "68040",
    "end": "71040"
  },
  {
    "text": "that that's almost going to be",
    "start": "71040",
    "end": "72720"
  },
  {
    "text": "considered a last generation model right",
    "start": "72720",
    "end": "74560"
  },
  {
    "text": "in in a year from now um so there's a",
    "start": "74560",
    "end": "76479"
  },
  {
    "text": "couple things that I wanted to talk",
    "start": "76479",
    "end": "77680"
  },
  {
    "text": "about regarding that right and and",
    "start": "77680",
    "end": "79560"
  },
  {
    "text": "mostly on the inference side because I",
    "start": "79560",
    "end": "81240"
  },
  {
    "text": "don't think you know anyone here is",
    "start": "81240",
    "end": "82640"
  },
  {
    "text": "going to try and train that kind of Next",
    "start": "82640",
    "end": "84040"
  },
  {
    "text": "Generation model but definitely we we",
    "start": "84040",
    "end": "86000"
  },
  {
    "text": "need to be able to run it um and so you",
    "start": "86000",
    "end": "89079"
  },
  {
    "text": "know a few things right so just just",
    "start": "89079",
    "end": "90600"
  },
  {
    "text": "going to break down inference uh in",
    "start": "90600",
    "end": "93079"
  },
  {
    "text": "detail right uh you know you know",
    "start": "93079",
    "end": "95560"
  },
  {
    "text": "there's two parts of inference right",
    "start": "95560",
    "end": "96799"
  },
  {
    "text": "there's pre-fill there's decode prefills",
    "start": "96799",
    "end": "98880"
  },
  {
    "text": "The Prompt processing right and the",
    "start": "98880",
    "end": "100880"
  },
  {
    "text": "interesting thing is if you have a 2K",
    "start": "100880",
    "end": "103399"
  },
  {
    "text": "prompt 2K uh context length prompt right",
    "start": "103399",
    "end": "105880"
  },
  {
    "text": "2,000 tokens you input into GPT um",
    "start": "105880",
    "end": "108719"
  },
  {
    "text": "that's that's a pedal flop itself right",
    "start": "108719",
    "end": "111040"
  },
  {
    "text": "um and then you know if you have 32,000",
    "start": "111040",
    "end": "113439"
  },
  {
    "text": "prompt that you enter it's 20 pedop",
    "start": "113439",
    "end": "114840"
  },
  {
    "text": "flops actually so it's a an incredible",
    "start": "114840",
    "end": "116600"
  },
  {
    "text": "amount of compute uh that's required to",
    "start": "116600",
    "end": "118799"
  },
  {
    "text": "just process the prompt",
    "start": "118799",
    "end": "120439"
  },
  {
    "text": "um and and you know while while prefill",
    "start": "120439",
    "end": "123479"
  },
  {
    "text": "is is very compute intensive right it's",
    "start": "123479",
    "end": "126439"
  },
  {
    "text": "actually the opposite of decode right",
    "start": "126439",
    "end": "128000"
  },
  {
    "text": "decode is actually generating each token",
    "start": "128000",
    "end": "130679"
  },
  {
    "text": "iteratively right so you you process the",
    "start": "130679",
    "end": "132680"
  },
  {
    "text": "prompt and you generate a token you feed",
    "start": "132680",
    "end": "134400"
  },
  {
    "text": "it back in and you keep going",
    "start": "134400",
    "end": "135480"
  },
  {
    "text": "iteratively right um and decode is",
    "start": "135480",
    "end": "138040"
  },
  {
    "text": "extremely memory bandwidth intensive",
    "start": "138040",
    "end": "140160"
  },
  {
    "text": "right um You have to load the whole",
    "start": "140160",
    "end": "142840"
  },
  {
    "text": "model from the weights the entire all",
    "start": "142840",
    "end": "145319"
  },
  {
    "text": "the weights into the uh chip right or",
    "start": "145319",
    "end": "147680"
  },
  {
    "text": "chips uh for decode um and the Big",
    "start": "147680",
    "end": "150920"
  },
  {
    "text": "Challenge here is that you know hey if",
    "start": "150920",
    "end": "153080"
  },
  {
    "text": "you have 1.8 trillion parameters if",
    "start": "153080",
    "end": "154920"
  },
  {
    "text": "you're running out a reasonable batch",
    "start": "154920",
    "end": "156000"
  },
  {
    "text": "size you're activating all the experts",
    "start": "156000",
    "end": "157959"
  },
  {
    "text": "you need to load all 1.8 trillion",
    "start": "157959",
    "end": "160920"
  },
  {
    "text": "parameters every single token generation",
    "start": "160920",
    "end": "163879"
  },
  {
    "text": "right even if you're serving multiple",
    "start": "163879",
    "end": "165040"
  },
  {
    "text": "users at once that means you're uh you",
    "start": "165040",
    "end": "167720"
  },
  {
    "text": "need you know a 1.8 uh you need",
    "start": "167720",
    "end": "170760"
  },
  {
    "text": "terabytes a second of memory bandwidth",
    "start": "170760",
    "end": "172280"
  },
  {
    "text": "you want to do 30 tokens per second I",
    "start": "172280",
    "end": "174159"
  },
  {
    "text": "think that's like a minimum bar for most",
    "start": "174159",
    "end": "175800"
  },
  {
    "text": "people right uh a lot of people want",
    "start": "175800",
    "end": "177920"
  },
  {
    "text": "hundreds of tokens per second but even",
    "start": "177920",
    "end": "179680"
  },
  {
    "text": "if if you want 30 tokens per second per",
    "start": "179680",
    "end": "181560"
  },
  {
    "text": "user 64 users you need 60 terab a second",
    "start": "181560",
    "end": "184519"
  },
  {
    "text": "of memory bandwidth if even if you look",
    "start": "184519",
    "end": "186720"
  },
  {
    "text": "at an h100 it has like three right so",
    "start": "186720",
    "end": "189000"
  },
  {
    "text": "this is a extremely challenging systems",
    "start": "189000",
    "end": "191440"
  },
  {
    "text": "problem um more you know decode while it",
    "start": "191440",
    "end": "194720"
  },
  {
    "text": "is very bandwidth intensive it's",
    "start": "194720",
    "end": "196560"
  },
  {
    "text": "actually quite cheap on the compute",
    "start": "196560",
    "end": "197840"
  },
  {
    "text": "which is why uh if you look at like open",
    "start": "197840",
    "end": "199920"
  },
  {
    "text": "AI pricing or Claud pricing you see a",
    "start": "199920",
    "end": "202400"
  },
  {
    "text": "three or 4:1 ratio between prefill",
    "start": "202400",
    "end": "205120"
  },
  {
    "text": "versus decode pricing right uh so the",
    "start": "205120",
    "end": "207519"
  },
  {
    "text": "input tokens cost you know oneir that of",
    "start": "207519",
    "end": "210599"
  },
  {
    "text": "the output tokens um or 1/4 that so so",
    "start": "210599",
    "end": "214720"
  },
  {
    "text": "you know today the best models I think",
    "start": "214720",
    "end": "216400"
  },
  {
    "text": "uh 40 and and 3.5 CET or uh I want to",
    "start": "216400",
    "end": "219720"
  },
  {
    "text": "say it's $15 per million tokens and then",
    "start": "219720",
    "end": "222120"
  },
  {
    "text": "it's $5 per million tokens for input uh",
    "start": "222120",
    "end": "224560"
  },
  {
    "text": "15 for output um so five for pre-fill 15",
    "start": "224560",
    "end": "228599"
  },
  {
    "text": "for decode um and and soon we're going",
    "start": "228599",
    "end": "231280"
  },
  {
    "text": "to have you know in the in the open",
    "start": "231280",
    "end": "232720"
  },
  {
    "text": "source you know so what everyone here",
    "start": "232720",
    "end": "234120"
  },
  {
    "text": "can touch is is llama 3 405b right and",
    "start": "234120",
    "end": "236799"
  },
  {
    "text": "that's that's going to be a real",
    "start": "236799",
    "end": "238239"
  },
  {
    "text": "capability sort of unlock",
    "start": "238239",
    "end": "240480"
  },
  {
    "text": "um for the you know open source market",
    "start": "240480",
    "end": "243879"
  },
  {
    "text": "as well as you know Builders here right",
    "start": "243879",
    "end": "245959"
  },
  {
    "text": "and I think I think there's a couple",
    "start": "245959",
    "end": "247280"
  },
  {
    "text": "things that uh people really need to be",
    "start": "247280",
    "end": "249159"
  },
  {
    "text": "able to implement right like you can't",
    "start": "249159",
    "end": "250640"
  },
  {
    "text": "just run llama CPP on llama 405b right",
    "start": "250640",
    "end": "254000"
  },
  {
    "text": "like it's just not going to work um so",
    "start": "254000",
    "end": "255920"
  },
  {
    "text": "there's a bunch of stuff that people",
    "start": "255920",
    "end": "257759"
  },
  {
    "text": "have to work on um you know whether it's",
    "start": "257759",
    "end": "259959"
  },
  {
    "text": "using you know Clos Source libraries",
    "start": "259959",
    "end": "261479"
  },
  {
    "text": "like tensor rtlm uh that only work on",
    "start": "261479",
    "end": "263960"
  },
  {
    "text": "Nvidia or like VM which is an open",
    "start": "263960",
    "end": "266199"
  },
  {
    "text": "source library that works uh on AMD and",
    "start": "266199",
    "end": "269720"
  },
  {
    "text": "inel and and soon other people's chips",
    "start": "269720",
    "end": "271800"
  },
  {
    "text": "as well um you know there's there's a",
    "start": "271800",
    "end": "273639"
  },
  {
    "text": "lot of stuff that people need to figure",
    "start": "273639",
    "end": "274880"
  },
  {
    "text": "out one one of those is is continuous",
    "start": "274880",
    "end": "276720"
  },
  {
    "text": "batching right uh because you're going",
    "start": "276720",
    "end": "278280"
  },
  {
    "text": "to get you know running inference at",
    "start": "278280",
    "end": "280199"
  },
  {
    "text": "batch size one is horrendously expensive",
    "start": "280199",
    "end": "282919"
  },
  {
    "text": "um you know it's great to run if you're",
    "start": "282919",
    "end": "284680"
  },
  {
    "text": "running it on your own personal devices",
    "start": "284680",
    "end": "286639"
  },
  {
    "text": "but if you're running it in the cloud",
    "start": "286639",
    "end": "287840"
  },
  {
    "text": "right you're renting gpus you're running",
    "start": "287840",
    "end": "289160"
  },
  {
    "text": "batch size one you're you're going to",
    "start": "289160",
    "end": "291360"
  },
  {
    "text": "cost yourself 10x more you know 10x is a",
    "start": "291360",
    "end": "294400"
  },
  {
    "text": "low bar right it's actually could be 10x",
    "start": "294400",
    "end": "295840"
  },
  {
    "text": "to 100x more than running at a high",
    "start": "295840",
    "end": "297840"
  },
  {
    "text": "batch right so you have to figure out",
    "start": "297840",
    "end": "299280"
  },
  {
    "text": "how to to run High batch sizes batch",
    "start": "299280",
    "end": "301960"
  },
  {
    "text": "sizes how many concurrent users you're",
    "start": "301960",
    "end": "303520"
  },
  {
    "text": "serving um and so one of those things",
    "start": "303520",
    "end": "306199"
  },
  {
    "text": "that makes it difficult is that users",
    "start": "306199",
    "end": "308120"
  },
  {
    "text": "requests come in at different times",
    "start": "308120",
    "end": "310080"
  },
  {
    "text": "right uh one person might send a request",
    "start": "310080",
    "end": "311919"
  },
  {
    "text": "now and then another person sends in a",
    "start": "311919",
    "end": "313240"
  },
  {
    "text": "request 5 seconds later uh but the first",
    "start": "313240",
    "end": "315680"
  },
  {
    "text": "person's request is not done so you need",
    "start": "315680",
    "end": "317000"
  },
  {
    "text": "to be able to do continuous batching I.E",
    "start": "317000",
    "end": "319600"
  },
  {
    "text": "sub uh be able to run through the model",
    "start": "319600",
    "end": "321680"
  },
  {
    "text": "iteratively uh every time right um and",
    "start": "321680",
    "end": "324680"
  },
  {
    "text": "and bring in new users so continuous",
    "start": "324680",
    "end": "326319"
  },
  {
    "text": "batching is one of the things that you",
    "start": "326319",
    "end": "327800"
  },
  {
    "text": "have to have to have support of and a",
    "start": "327800",
    "end": "329880"
  },
  {
    "text": "lot of software today like llama CPP",
    "start": "329880",
    "end": "331560"
  },
  {
    "text": "doesn't have support for that so either",
    "start": "331560",
    "end": "333120"
  },
  {
    "text": "you need to build it yourself or um you",
    "start": "333120",
    "end": "336039"
  },
  {
    "text": "know contribute to an open source",
    "start": "336039",
    "end": "337400"
  },
  {
    "text": "project that that builds this um to to",
    "start": "337400",
    "end": "340360"
  },
  {
    "text": "enable lowcost inference right for you",
    "start": "340360",
    "end": "343160"
  },
  {
    "text": "know models like llama 405b right um",
    "start": "343160",
    "end": "346280"
  },
  {
    "text": "another one of those is is uh",
    "start": "346280",
    "end": "349000"
  },
  {
    "text": "disaggregated uh prefill or",
    "start": "349000",
    "end": "351360"
  },
  {
    "text": "disaggregated batching right it depends",
    "start": "351360",
    "end": "353080"
  },
  {
    "text": "on what you call it um but you know if",
    "start": "353080",
    "end": "355639"
  },
  {
    "text": "you go back to earlier I was discussing",
    "start": "355639",
    "end": "358160"
  },
  {
    "text": "uh prefill is very very compute",
    "start": "358160",
    "end": "360240"
  },
  {
    "text": "intensive decode is very uh bandwidth",
    "start": "360240",
    "end": "362759"
  },
  {
    "text": "intensive these are two different",
    "start": "362759",
    "end": "364000"
  },
  {
    "text": "workloads but when you Ser when you're",
    "start": "364000",
    "end": "365800"
  },
  {
    "text": "serving a user right whether it's uh you",
    "start": "365800",
    "end": "368039"
  },
  {
    "text": "know in your own app or you're using an",
    "start": "368039",
    "end": "369720"
  },
  {
    "text": "API what have you right like these users",
    "start": "369720",
    "end": "372440"
  },
  {
    "text": "uh don't care that it's two different",
    "start": "372440",
    "end": "374080"
  },
  {
    "text": "workloads right it's one workload to",
    "start": "374080",
    "end": "375440"
  },
  {
    "text": "them uh I get tokens out right I submit",
    "start": "375440",
    "end": "377599"
  },
  {
    "text": "something to you and I get tokens back",
    "start": "377599",
    "end": "379479"
  },
  {
    "text": "uh but but for anyone running the infra",
    "start": "379479",
    "end": "381759"
  },
  {
    "text": "themselves uh they need to they need to",
    "start": "381759",
    "end": "383759"
  },
  {
    "text": "be keenly aware that these are two",
    "start": "383759",
    "end": "384919"
  },
  {
    "text": "different workloads um so one thing that",
    "start": "384919",
    "end": "386840"
  },
  {
    "text": "a lot of people have uh started to do um",
    "start": "386840",
    "end": "389319"
  },
  {
    "text": "gole Go's publicly said they're doing it",
    "start": "389319",
    "end": "391160"
  },
  {
    "text": "I believe opening ID and anthropic are",
    "start": "391160",
    "end": "392720"
  },
  {
    "text": "also doing it um you know uh other firms",
    "start": "392720",
    "end": "396080"
  },
  {
    "text": "like together and fireworks have hinted",
    "start": "396080",
    "end": "397680"
  },
  {
    "text": "that they're doing this uh is is",
    "start": "397680",
    "end": "399639"
  },
  {
    "text": "disaggregated pre-fill right so once",
    "start": "399639",
    "end": "402039"
  },
  {
    "text": "your inference volumes are high enough",
    "start": "402039",
    "end": "403960"
  },
  {
    "text": "you don't just run inference you know",
    "start": "403960",
    "end": "406520"
  },
  {
    "text": "you don't just replicate the model",
    "start": "406520",
    "end": "407759"
  },
  {
    "text": "across however many chips you have right",
    "start": "407759",
    "end": "409759"
  },
  {
    "text": "uh say say it takes four mod four chips",
    "start": "409759",
    "end": "411440"
  },
  {
    "text": "to serve llama 405b right in the future",
    "start": "411440",
    "end": "414560"
  },
  {
    "text": "um you wouldn't just have you know if",
    "start": "414560",
    "end": "416919"
  },
  {
    "text": "you have so many if you have enough",
    "start": "416919",
    "end": "417919"
  },
  {
    "text": "users you don't just go for and then 8",
    "start": "417919",
    "end": "420199"
  },
  {
    "text": "16 whatever right you don't just",
    "start": "420199",
    "end": "421879"
  },
  {
    "text": "replicate that across the world you",
    "start": "421879",
    "end": "423319"
  },
  {
    "text": "actually do this thing called",
    "start": "423319",
    "end": "424440"
  },
  {
    "text": "disaggregated prefill you have one set",
    "start": "424440",
    "end": "426160"
  },
  {
    "text": "of accelerators do the pre-fill which is",
    "start": "426160",
    "end": "428639"
  },
  {
    "text": "very compute intensive and then you hand",
    "start": "428639",
    "end": "430319"
  },
  {
    "text": "it off to the other set of accelerators",
    "start": "430319",
    "end": "432080"
  },
  {
    "text": "to do decode now today everyone just",
    "start": "432080",
    "end": "434319"
  },
  {
    "text": "uses the same accelerator for that right",
    "start": "434319",
    "end": "436120"
  },
  {
    "text": "h100 or a100 or you know maybe maybe l40",
    "start": "436120",
    "end": "439520"
  },
  {
    "text": "or something but mostly h100",
    "start": "439520",
    "end": "442240"
  },
  {
    "text": "uh but there's a there's a reason you do",
    "start": "442240",
    "end": "444599"
  },
  {
    "text": "this right and and and that big reason",
    "start": "444599",
    "end": "446639"
  },
  {
    "text": "is that you have a lot of noisy",
    "start": "446639",
    "end": "447840"
  },
  {
    "text": "neighbors right um so if you've evered",
    "start": "447840",
    "end": "449520"
  },
  {
    "text": "worked in like CPUs or on anything in",
    "start": "449520",
    "end": "451560"
  },
  {
    "text": "cloud computing noisy neighbors are a",
    "start": "451560",
    "end": "453080"
  },
  {
    "text": "huge huge issue um and actually like",
    "start": "453080",
    "end": "455919"
  },
  {
    "text": "there's it's very trivial to",
    "start": "455919",
    "end": "457639"
  },
  {
    "text": "dramatically slow down most inference",
    "start": "457639",
    "end": "459199"
  },
  {
    "text": "providers Services uh if if you just uh",
    "start": "459199",
    "end": "462919"
  },
  {
    "text": "send queries in a certain way like in a",
    "start": "462919",
    "end": "464759"
  },
  {
    "text": "in a sort of malicious way um you can",
    "start": "464759",
    "end": "467199"
  },
  {
    "text": "you can just slow down people's uh",
    "start": "467199",
    "end": "469919"
  },
  {
    "text": "service right whether that's you know",
    "start": "469919",
    "end": "471479"
  },
  {
    "text": "and and that'll that'll impact the users",
    "start": "471479",
    "end": "473280"
  },
  {
    "text": "time to First token right um and I think",
    "start": "473280",
    "end": "475240"
  },
  {
    "text": "that's a huge issue right if time to",
    "start": "475240",
    "end": "476680"
  },
  {
    "text": "First token is too long people will just",
    "start": "476680",
    "end": "479120"
  },
  {
    "text": "quit right using your service um if uh",
    "start": "479120",
    "end": "483440"
  },
  {
    "text": "you know the tokens per second varies a",
    "start": "483440",
    "end": "485520"
  },
  {
    "text": "lot right for a moment you're getting",
    "start": "485520",
    "end": "486759"
  },
  {
    "text": "100 tokens per second then it drops down",
    "start": "486759",
    "end": "488199"
  },
  {
    "text": "to like 30 then it drops goes back up to",
    "start": "488199",
    "end": "490520"
  },
  {
    "text": "100 that's going to be really annoying",
    "start": "490520",
    "end": "492280"
  },
  {
    "text": "to the user so so there's a lot of",
    "start": "492280",
    "end": "493960"
  },
  {
    "text": "things around you know SLA and and",
    "start": "493960",
    "end": "496479"
  },
  {
    "text": "reliability and all these things that",
    "start": "496479",
    "end": "498080"
  },
  {
    "text": "you have to guarantee and so",
    "start": "498080",
    "end": "499520"
  },
  {
    "text": "disaggregated prefill uh is is one of",
    "start": "499520",
    "end": "502599"
  },
  {
    "text": "the techniques to do that right um and",
    "start": "502599",
    "end": "505560"
  },
  {
    "text": "and so you don't want to have someone",
    "start": "505560",
    "end": "508080"
  },
  {
    "text": "submit you know for example",
    "start": "508080",
    "end": "509960"
  },
  {
    "text": "hey I have a database and I want to sub",
    "start": "509960",
    "end": "511680"
  },
  {
    "text": "I want to run an llm query across every",
    "start": "511680",
    "end": "513800"
  },
  {
    "text": "single Row in that database and I'm just",
    "start": "513800",
    "end": "515518"
  },
  {
    "text": "going to submit it to you my service",
    "start": "515519",
    "end": "517200"
  },
  {
    "text": "provider because you have this cool",
    "start": "517200",
    "end": "518399"
  },
  {
    "text": "model or what have you that's fine tuned",
    "start": "518399",
    "end": "520560"
  },
  {
    "text": "on some data set and what whatever it is",
    "start": "520560",
    "end": "522560"
  },
  {
    "text": "right if I submit 10,000 rows to you at",
    "start": "522560",
    "end": "525080"
  },
  {
    "text": "once that's going to kill everyone",
    "start": "525080",
    "end": "526279"
  },
  {
    "text": "else's performance right so so this is",
    "start": "526279",
    "end": "528040"
  },
  {
    "text": "one of the techniques that people have",
    "start": "528040",
    "end": "529600"
  },
  {
    "text": "for uh making it so you know that that",
    "start": "529600",
    "end": "532839"
  },
  {
    "text": "person who you definitely want to serve",
    "start": "532839",
    "end": "534680"
  },
  {
    "text": "uh doesn't impact everyone else's usage",
    "start": "534680",
    "end": "537560"
  },
  {
    "text": "uh because once you open up your service",
    "start": "537560",
    "end": "539120"
  },
  {
    "text": "to the real world you're not going to be",
    "start": "539120",
    "end": "540640"
  },
  {
    "text": "able to control who's submitting what",
    "start": "540640",
    "end": "542519"
  },
  {
    "text": "and rate limits are the most annoying",
    "start": "542519",
    "end": "543839"
  },
  {
    "text": "thing ever so that's not the correct way",
    "start": "543839",
    "end": "545279"
  },
  {
    "text": "to go about it um another thing is",
    "start": "545279",
    "end": "548920"
  },
  {
    "text": "context caching right so Google launched",
    "start": "548920",
    "end": "551399"
  },
  {
    "text": "this recently uh they're the only one",
    "start": "551399",
    "end": "553360"
  },
  {
    "text": "offering this today but I think this is",
    "start": "553360",
    "end": "554800"
  },
  {
    "text": "a really big deal uh cuz when people",
    "start": "554800",
    "end": "557200"
  },
  {
    "text": "talk about fine-tuning right of models",
    "start": "557200",
    "end": "558959"
  },
  {
    "text": "that's great uh but in reality the best",
    "start": "558959",
    "end": "561760"
  },
  {
    "text": "models are really expensive to fine tune",
    "start": "561760",
    "end": "563680"
  },
  {
    "text": "or impossible to fine-tune right I can't",
    "start": "563680",
    "end": "566160"
  },
  {
    "text": "go fine-tune 3.5 CET or fine tuning l",
    "start": "566160",
    "end": "569560"
  },
  {
    "text": "405b is going to take you know dozens",
    "start": "569560",
    "end": "571720"
  },
  {
    "text": "and dozens of gpus right so so instead",
    "start": "571720",
    "end": "574160"
  },
  {
    "text": "of that the the uh or you know in in",
    "start": "574160",
    "end": "576760"
  },
  {
    "text": "close Source models generally so Google",
    "start": "576760",
    "end": "578240"
  },
  {
    "text": "only does close Source models mostly for",
    "start": "578240",
    "end": "579720"
  },
  {
    "text": "the big ones right so Gemini 1.5 Pro",
    "start": "579720",
    "end": "582399"
  },
  {
    "text": "they offered this they brought this",
    "start": "582399",
    "end": "583920"
  },
  {
    "text": "recently right which is context caching",
    "start": "583920",
    "end": "586320"
  },
  {
    "text": "so instead of you know fine-tuning your",
    "start": "586320",
    "end": "588240"
  },
  {
    "text": "model why not you know just fill out a",
    "start": "588240",
    "end": "590240"
  },
  {
    "text": "context length of you know they they",
    "start": "590240",
    "end": "591880"
  },
  {
    "text": "offer I think 2 million now today right",
    "start": "591880",
    "end": "593680"
  },
  {
    "text": "2 million context length um why not fill",
    "start": "593680",
    "end": "596360"
  },
  {
    "text": "it out with your data there right um you",
    "start": "596360",
    "end": "598920"
  },
  {
    "text": "know and and there's a couple you know",
    "start": "598920",
    "end": "601040"
  },
  {
    "text": "advantages to that one is you can use",
    "start": "601040",
    "end": "603160"
  },
  {
    "text": "the best models right in the case of",
    "start": "603160",
    "end": "604680"
  },
  {
    "text": "fine-tune models you really are focused",
    "start": "604680",
    "end": "606720"
  },
  {
    "text": "on like the Llama 7B or mix draw or",
    "start": "606720",
    "end": "609720"
  },
  {
    "text": "llama uh you know 70b it's it's kind of",
    "start": "609720",
    "end": "612600"
  },
  {
    "text": "much lower quality models than what's",
    "start": "612600",
    "end": "614480"
  },
  {
    "text": "available in the close Source world uh",
    "start": "614480",
    "end": "616519"
  },
  {
    "text": "so one of the things you can do is you",
    "start": "616519",
    "end": "618040"
  },
  {
    "text": "can um Implement what Google has called",
    "start": "618040",
    "end": "620920"
  },
  {
    "text": "context caching in the in the open",
    "start": "620920",
    "end": "622279"
  },
  {
    "text": "source World we'll we'll have super long",
    "start": "622279",
    "end": "623880"
  },
  {
    "text": "context models soon enough but uh",
    "start": "623880",
    "end": "626040"
  },
  {
    "text": "economically right you know we talked",
    "start": "626040",
    "end": "627600"
  },
  {
    "text": "about $15 token per per million tokens",
    "start": "627600",
    "end": "630360"
  },
  {
    "text": "output um and 5 million per million",
    "start": "630360",
    "end": "633200"
  },
  {
    "text": "tokens input if you were to have uh on",
    "start": "633200",
    "end": "635560"
  },
  {
    "text": "on you know the best close Source models",
    "start": "635560",
    "end": "637519"
  },
  {
    "text": "today if you were to submit a prompt of",
    "start": "637519",
    "end": "640240"
  },
  {
    "text": "like you know a million tokens and and",
    "start": "640240",
    "end": "643279"
  },
  {
    "text": "most most of the times you're looking at",
    "start": "643279",
    "end": "644440"
  },
  {
    "text": "a document you get a query back right",
    "start": "644440",
    "end": "645880"
  },
  {
    "text": "you your your output is very small",
    "start": "645880",
    "end": "647760"
  },
  {
    "text": "almost all of the cost is just sending",
    "start": "647760",
    "end": "649480"
  },
  {
    "text": "them that document right so that's",
    "start": "649480",
    "end": "651200"
  },
  {
    "text": "that's going to really really hurt you",
    "start": "651200",
    "end": "652880"
  },
  {
    "text": "so for people you know targeting maybe",
    "start": "652880",
    "end": "654760"
  },
  {
    "text": "like a legal AI or like um you know some",
    "start": "654760",
    "end": "657480"
  },
  {
    "text": "sort of other contract review AI a lot",
    "start": "657480",
    "end": "659480"
  },
  {
    "text": "of these Enterprise use cases uh",
    "start": "659480",
    "end": "661600"
  },
  {
    "text": "pre-fill is going to dominate your cost",
    "start": "661600",
    "end": "663079"
  },
  {
    "text": "if you're using apis um and so Google",
    "start": "663079",
    "end": "666040"
  },
  {
    "text": "has this context caching and and open",
    "start": "666040",
    "end": "668200"
  },
  {
    "text": "source will have it so models you can",
    "start": "668200",
    "end": "669560"
  },
  {
    "text": "run yourself and and others will deploy",
    "start": "669560",
    "end": "671360"
  },
  {
    "text": "over time uh but basically you don't",
    "start": "671360",
    "end": "673600"
  },
  {
    "text": "recompute the KV cache right the the",
    "start": "673600",
    "end": "677079"
  },
  {
    "text": "context length every single time instead",
    "start": "677079",
    "end": "679240"
  },
  {
    "text": "you cache it uh but the problem is to",
    "start": "679240",
    "end": "681959"
  },
  {
    "text": "save save that takes in an incredible",
    "start": "681959",
    "end": "684480"
  },
  {
    "text": "amount of memory um so you don't save it",
    "start": "684480",
    "end": "686480"
  },
  {
    "text": "in the gpu's memory right you save it on",
    "start": "686480",
    "end": "688839"
  },
  {
    "text": "the CP use memory or storage um and so",
    "start": "688839",
    "end": "692600"
  },
  {
    "text": "uh VM uh which is an open source library",
    "start": "692600",
    "end": "695720"
  },
  {
    "text": "for inference is contributing is",
    "start": "695720",
    "end": "697600"
  },
  {
    "text": "building this currently so if you're",
    "start": "697600",
    "end": "699440"
  },
  {
    "text": "interested in contributing to that uh",
    "start": "699440",
    "end": "701320"
  },
  {
    "text": "check that out um or if you're",
    "start": "701320",
    "end": "703519"
  },
  {
    "text": "interested in using it just start a",
    "start": "703519",
    "end": "704920"
  },
  {
    "text": "project right um because you know while",
    "start": "704920",
    "end": "707639"
  },
  {
    "text": "most of the models we have in the closed",
    "start": "707639",
    "end": "708760"
  },
  {
    "text": "Source today are like only like 32 or 8K",
    "start": "708760",
    "end": "711120"
  },
  {
    "text": "or 4K context length they're coming with",
    "start": "711120",
    "end": "713040"
  },
  {
    "text": "longer um and being able to you know",
    "start": "713040",
    "end": "716440"
  },
  {
    "text": "dramatically reduce your costs um by",
    "start": "716440",
    "end": "719440"
  },
  {
    "text": "caching the context",
    "start": "719440",
    "end": "722000"
  },
  {
    "text": "um is is very is gonna is going to",
    "start": "722000",
    "end": "724399"
  },
  {
    "text": "dramatically reduce cost right um so now",
    "start": "724399",
    "end": "727959"
  },
  {
    "text": "I'm just going to talk about like head",
    "start": "727959",
    "end": "728839"
  },
  {
    "text": "in the cloud stuff instead of like real",
    "start": "728839",
    "end": "730639"
  },
  {
    "text": "usable things which is um you know",
    "start": "730639",
    "end": "733120"
  },
  {
    "text": "what's coming down the pipeline right",
    "start": "733120",
    "end": "734720"
  },
  {
    "text": "which is you know gbd4 was like 20,000",
    "start": "734720",
    "end": "736600"
  },
  {
    "text": "chips for 90 to 100 days um used you",
    "start": "736600",
    "end": "739399"
  },
  {
    "text": "know 38 gwatt hours very very expensive",
    "start": "739399",
    "end": "741639"
  },
  {
    "text": "cool um but you know what's what what",
    "start": "741639",
    "end": "744160"
  },
  {
    "text": "are they building now right uh open AI",
    "start": "744160",
    "end": "746880"
  },
  {
    "text": "xai um anthropic many others are",
    "start": "746880",
    "end": "749279"
  },
  {
    "text": "building 100,000 chip clusters right and",
    "start": "749279",
    "end": "751399"
  },
  {
    "text": "it would train gbd4 in 3 days right so",
    "start": "751399",
    "end": "753160"
  },
  {
    "text": "it's kind of irrelevant um you know and",
    "start": "753160",
    "end": "756000"
  },
  {
    "text": "and uh I'll skip over this part uh",
    "start": "756000",
    "end": "759000"
  },
  {
    "text": "because it's not really uh too relevant",
    "start": "759000",
    "end": "761519"
  },
  {
    "text": "um but you know what what what's a",
    "start": "761519",
    "end": "763800"
  },
  {
    "text": "modern system capable of right like h100",
    "start": "763800",
    "end": "765959"
  },
  {
    "text": "is is pretty uh pretty fast relative to",
    "start": "765959",
    "end": "768240"
  },
  {
    "text": "a100 and and coming down the pipeline is",
    "start": "768240",
    "end": "770320"
  },
  {
    "text": "these the new Nvidia chips but what",
    "start": "770320",
    "end": "772920"
  },
  {
    "text": "what's com you know what's coming down",
    "start": "772920",
    "end": "774040"
  },
  {
    "text": "with these 100,000 GPU clusters right um",
    "start": "774040",
    "end": "776560"
  },
  {
    "text": "it's not going to be a 1.8 trillon",
    "start": "776560",
    "end": "777680"
  },
  {
    "text": "parameter model it's actually going to",
    "start": "777680",
    "end": "778760"
  },
  {
    "text": "be you know could be in the tens of",
    "start": "778760",
    "end": "780440"
  },
  {
    "text": "trillions of parameters um you know the",
    "start": "780440",
    "end": "783160"
  },
  {
    "text": "the training flops right I talked about",
    "start": "783160",
    "end": "784600"
  },
  {
    "text": "gp4 is it's roughly 2 e25 flops right",
    "start": "784600",
    "end": "788880"
  },
  {
    "text": "which is uh you know a number that's not",
    "start": "788880",
    "end": "791720"
  },
  {
    "text": "really relevant or 2 e25 flop um but",
    "start": "791720",
    "end": "794360"
  },
  {
    "text": "with 100,000 GPU cluster you can do 10",
    "start": "794360",
    "end": "796760"
  },
  {
    "text": "e26 10 e27 flops uh and to run that",
    "start": "796760",
    "end": "799959"
  },
  {
    "text": "model is going to require 200 gigabyt or",
    "start": "799959",
    "end": "801800"
  },
  {
    "text": "terabytes of a second memory bandwidth",
    "start": "801800",
    "end": "803160"
  },
  {
    "text": "right um but what what is that like what",
    "start": "803160",
    "end": "805199"
  },
  {
    "text": "does that look like right so so this is",
    "start": "805199",
    "end": "807519"
  },
  {
    "text": "a on the top right is an image",
    "start": "807519",
    "end": "810079"
  },
  {
    "text": "of uh Microsoft's data centers in",
    "start": "810079",
    "end": "813160"
  },
  {
    "text": "Arizona where they're making GPT 5 right",
    "start": "813160",
    "end": "816639"
  },
  {
    "text": "um they have about 100,000 gpus here uh",
    "start": "816639",
    "end": "819480"
  },
  {
    "text": "it's 150 megawatts right like the",
    "start": "819480",
    "end": "821399"
  },
  {
    "text": "average home does not consume you know",
    "start": "821399",
    "end": "823360"
  },
  {
    "text": "that's like that's like like tens of",
    "start": "823360",
    "end": "825680"
  },
  {
    "text": "thousands if not hundreds of thousands",
    "start": "825680",
    "end": "826959"
  },
  {
    "text": "of homes of power consumption right it's",
    "start": "826959",
    "end": "829240"
  },
  {
    "text": "it's kind of insane um elon's talked",
    "start": "829240",
    "end": "831320"
  },
  {
    "text": "about his next Generation cluster he's",
    "start": "831320",
    "end": "832759"
  },
  {
    "text": "building 100,000 GPU cluster today uh",
    "start": "832759",
    "end": "835120"
  },
  {
    "text": "but he's talked about his next",
    "start": "835120",
    "end": "836040"
  },
  {
    "text": "Generation cluster is 300,000 gpus this",
    "start": "836040",
    "end": "838880"
  },
  {
    "text": "kind kind of insane but the the power",
    "start": "838880",
    "end": "840839"
  },
  {
    "text": "cost for that alone would be like $500",
    "start": "840839",
    "end": "843120"
  },
  {
    "text": "million a year right so it's like you",
    "start": "843120",
    "end": "845440"
  },
  {
    "text": "know people are people are kind of",
    "start": "845440",
    "end": "846519"
  },
  {
    "text": "insane but it's pretty cool um but you",
    "start": "846519",
    "end": "850040"
  },
  {
    "text": "know the the the interesting thing here",
    "start": "850040",
    "end": "852199"
  },
  {
    "text": "is you know on training we we you know",
    "start": "852199",
    "end": "854040"
  },
  {
    "text": "when when you when you try and train a",
    "start": "854040",
    "end": "856240"
  },
  {
    "text": "model today people just talk about fully",
    "start": "856240",
    "end": "857800"
  },
  {
    "text": "connected clusters uh every GPU is",
    "start": "857800",
    "end": "860120"
  },
  {
    "text": "connected to every other GPU at some",
    "start": "860120",
    "end": "861440"
  },
  {
    "text": "speed and you you know you have to do",
    "start": "861440",
    "end": "863800"
  },
  {
    "text": "you know all your operations but that's",
    "start": "863800",
    "end": "865120"
  },
  {
    "text": "not really possible when you go to these",
    "start": "865120",
    "end": "867399"
  },
  {
    "text": "super large clusters right",
    "start": "867399",
    "end": "869720"
  },
  {
    "text": "um so the 100,000 GPU clusters those are",
    "start": "869720",
    "end": "871759"
  },
  {
    "text": "being built this year and then next year",
    "start": "871759",
    "end": "873199"
  },
  {
    "text": "they're planning to build multiple",
    "start": "873199",
    "end": "874680"
  },
  {
    "text": "100,000 GPU clusters already you can see",
    "start": "874680",
    "end": "877279"
  },
  {
    "text": "that it exists across multiple buildings",
    "start": "877279",
    "end": "879399"
  },
  {
    "text": "right um and so there's a lot of",
    "start": "879399",
    "end": "882839"
  },
  {
    "text": "complicated networking uh going on right",
    "start": "882839",
    "end": "885680"
  },
  {
    "text": "to connect these data centers together",
    "start": "885680",
    "end": "888000"
  },
  {
    "text": "um and and one other thing I that I",
    "start": "888000",
    "end": "890000"
  },
  {
    "text": "think is just like kind of interesting",
    "start": "890000",
    "end": "891240"
  },
  {
    "text": "to again head in the clouds just to",
    "start": "891240",
    "end": "892519"
  },
  {
    "text": "think about is um when you connect these",
    "start": "892519",
    "end": "895240"
  },
  {
    "text": "chips together there's a lot of Optics",
    "start": "895240",
    "end": "897360"
  },
  {
    "text": "right uh you know you convert from",
    "start": "897360",
    "end": "899000"
  },
  {
    "text": "electrical to Optical uh and then you",
    "start": "899000",
    "end": "901320"
  },
  {
    "text": "know over fiber optics to connect",
    "start": "901320",
    "end": "902800"
  },
  {
    "text": "between chips transceivers Etc right uh",
    "start": "902800",
    "end": "905120"
  },
  {
    "text": "these are extremely unreliable right uh",
    "start": "905120",
    "end": "907720"
  },
  {
    "text": "they tend to have a failure rate of",
    "start": "907720",
    "end": "909000"
  },
  {
    "text": "around 5 years um and so what's",
    "start": "909000",
    "end": "912120"
  },
  {
    "text": "interesting is if you're talking about a",
    "start": "912120",
    "end": "913480"
  },
  {
    "text": "100,000 GPU cluster um or if you're",
    "start": "913480",
    "end": "915880"
  },
  {
    "text": "talking about a 500,000 GPU cluster",
    "start": "915880",
    "end": "918199"
  },
  {
    "text": "you're going to have something fail like",
    "start": "918199",
    "end": "920079"
  },
  {
    "text": "every 5 minutes right um which is insane",
    "start": "920079",
    "end": "923279"
  },
  {
    "text": "right how do you even deal with",
    "start": "923279",
    "end": "925000"
  },
  {
    "text": "something in your cluster failing every",
    "start": "925000",
    "end": "926680"
  },
  {
    "text": "5 minutes when you're training a model",
    "start": "926680",
    "end": "928800"
  },
  {
    "text": "right right um so you know this is this",
    "start": "928800",
    "end": "931839"
  },
  {
    "text": "is again more of like a hardware",
    "start": "931839",
    "end": "932959"
  },
  {
    "text": "oriented thing but uh you know the the",
    "start": "932959",
    "end": "935880"
  },
  {
    "text": "other thing that's interesting is like",
    "start": "935880",
    "end": "937480"
  },
  {
    "text": "when you get chips they're not all the",
    "start": "937480",
    "end": "938839"
  },
  {
    "text": "same speed you know h100 is not an h100",
    "start": "938839",
    "end": "941800"
  },
  {
    "text": "um they're stragglers uh so if you get a",
    "start": "941800",
    "end": "944120"
  },
  {
    "text": "large distribution of chips um what we",
    "start": "944120",
    "end": "946600"
  },
  {
    "text": "call an industry is is called the",
    "start": "946600",
    "end": "947759"
  },
  {
    "text": "Silicon Lottery um in that like you know",
    "start": "947759",
    "end": "951040"
  },
  {
    "text": "you you can buy for example a a gaming",
    "start": "951040",
    "end": "954000"
  },
  {
    "text": "GPU and and compare it to other people's",
    "start": "954000",
    "end": "956040"
  },
  {
    "text": "gaming gpus on the forms and they're",
    "start": "956040",
    "end": "957279"
  },
  {
    "text": "actually like percentages different",
    "start": "957279",
    "end": "959519"
  },
  {
    "text": "in performance but when you do a massive",
    "start": "959519",
    "end": "961360"
  },
  {
    "text": "training cluster um you end up with you",
    "start": "961360",
    "end": "964600"
  },
  {
    "text": "know training is a synchronous workload",
    "start": "964600",
    "end": "966040"
  },
  {
    "text": "right you know you you you update the",
    "start": "966040",
    "end": "969040"
  },
  {
    "text": "weights you then you pass the gradients",
    "start": "969040",
    "end": "970399"
  },
  {
    "text": "around right um and then you you know",
    "start": "970399",
    "end": "972880"
  },
  {
    "text": "then you again run through a bunch of",
    "start": "972880",
    "end": "974639"
  },
  {
    "text": "data uh update the weights or pass the",
    "start": "974639",
    "end": "976720"
  },
  {
    "text": "gradients around update the weights",
    "start": "976720",
    "end": "978000"
  },
  {
    "text": "right um so it's a synchronous workload",
    "start": "978000",
    "end": "980160"
  },
  {
    "text": "so if one of them is 10% slower then",
    "start": "980160",
    "end": "982040"
  },
  {
    "text": "everything is 10% slower and B dance had",
    "start": "982040",
    "end": "984000"
  },
  {
    "text": "a cool paper where actually they saw a",
    "start": "984000",
    "end": "985920"
  },
  {
    "text": "25% decrease in speed just because one",
    "start": "985920",
    "end": "988920"
  },
  {
    "text": "random GPU they got uh while it did",
    "start": "988920",
    "end": "991120"
  },
  {
    "text": "technically work um in Nvidia and and",
    "start": "991120",
    "end": "993680"
  },
  {
    "text": "and according to Nvidia it was fine it",
    "start": "993680",
    "end": "995880"
  },
  {
    "text": "was like 25% slower than uh what they",
    "start": "995880",
    "end": "998600"
  },
  {
    "text": "wanted right so they're you know this is",
    "start": "998600",
    "end": "1000040"
  },
  {
    "text": "like this is on like a 20,000 GPU",
    "start": "1000040",
    "end": "1002040"
  },
  {
    "text": "cluster even right um so so it's uh it's",
    "start": "1002040",
    "end": "1005680"
  },
  {
    "text": "it's quite interesting that you know",
    "start": "1005680",
    "end": "1008279"
  },
  {
    "text": "that that's these are the problems",
    "start": "1008279",
    "end": "1009480"
  },
  {
    "text": "people are running into at scale right",
    "start": "1009480",
    "end": "1011240"
  },
  {
    "text": "so they pulled that GPU out um and then",
    "start": "1011240",
    "end": "1013959"
  },
  {
    "text": "you you can sort of see their",
    "start": "1013959",
    "end": "1015000"
  },
  {
    "text": "performance dramatically uplifted right",
    "start": "1015000",
    "end": "1017639"
  },
  {
    "text": "um during during TR",
    "start": "1017639",
    "end": "1019839"
  },
  {
    "text": "um and then again this is bite dance on",
    "start": "1019839",
    "end": "1021399"
  },
  {
    "text": "a 20,000 GPU cluster so it's it's um",
    "start": "1021399",
    "end": "1025438"
  },
  {
    "text": "it's it's a big big issue um and I think",
    "start": "1025439",
    "end": "1028160"
  },
  {
    "text": "I think some of the other stuff in this",
    "start": "1028160",
    "end": "1029360"
  },
  {
    "text": "presentation is not really relevant uh",
    "start": "1029360",
    "end": "1031640"
  },
  {
    "text": "but I think I think what are these next",
    "start": "1031640",
    "end": "1034079"
  },
  {
    "text": "Generation systems look like is a very",
    "start": "1034079",
    "end": "1037360"
  },
  {
    "text": "um important question to ask yourself",
    "start": "1037360",
    "end": "1040438"
  },
  {
    "text": "right um you know and what what do I",
    "start": "1040439",
    "end": "1042880"
  },
  {
    "text": "what do I what do I do when I deal with",
    "start": "1042880",
    "end": "1044319"
  },
  {
    "text": "that right like I think a lot of the",
    "start": "1044319",
    "end": "1046199"
  },
  {
    "text": "scaffolding that people are building uh",
    "start": "1046199",
    "end": "1048280"
  },
  {
    "text": "today for M are dealing with you know is",
    "start": "1048280",
    "end": "1051440"
  },
  {
    "text": "is dealing with hallucinations and",
    "start": "1051440",
    "end": "1052840"
  },
  {
    "text": "things like that and and the hope that",
    "start": "1052840",
    "end": "1054320"
  },
  {
    "text": "everyone has or at least a lot of the",
    "start": "1054320",
    "end": "1056120"
  },
  {
    "text": "AGI people have is that you know when I",
    "start": "1056120",
    "end": "1058600"
  },
  {
    "text": "when I 100x the compute um you know when",
    "start": "1058600",
    "end": "1061360"
  },
  {
    "text": "I build a cluster that takes $500",
    "start": "1061360",
    "end": "1062720"
  },
  {
    "text": "million of electricity and I trade a",
    "start": "1062720",
    "end": "1064240"
  },
  {
    "text": "model with it it's going to make",
    "start": "1064240",
    "end": "1065480"
  },
  {
    "text": "something that uh uh you know yearly",
    "start": "1065480",
    "end": "1067840"
  },
  {
    "text": "electricity cost and make a model with",
    "start": "1067840",
    "end": "1069280"
  },
  {
    "text": "it and then the cluster itself cost over",
    "start": "1069280",
    "end": "1070720"
  },
  {
    "text": "10 billion by the way right uh it's it's",
    "start": "1070720",
    "end": "1072679"
  },
  {
    "text": "going to get rid of a lot of this um the",
    "start": "1072679",
    "end": "1075240"
  },
  {
    "text": "hallucinations it's going to let us do a",
    "start": "1075240",
    "end": "1077120"
  },
  {
    "text": "lot of interesting things uh um yeah so",
    "start": "1077120",
    "end": "1080559"
  },
  {
    "text": "so I think that's that's basically all",
    "start": "1080559",
    "end": "1082000"
  },
  {
    "text": "for the talk I just wanted to you know",
    "start": "1082000",
    "end": "1083919"
  },
  {
    "text": "uh mention you know sort of a reasonable",
    "start": "1083919",
    "end": "1086159"
  },
  {
    "text": "thing which is how do you run llama 405b",
    "start": "1086159",
    "end": "1087919"
  },
  {
    "text": "kind of some strategies that people need",
    "start": "1087919",
    "end": "1089720"
  },
  {
    "text": "to implement that aren't necessarily",
    "start": "1089720",
    "end": "1091120"
  },
  {
    "text": "implemented yet uh in the open source",
    "start": "1091120",
    "end": "1092919"
  },
  {
    "text": "that are implemented at the labs um but",
    "start": "1092919",
    "end": "1095320"
  },
  {
    "text": "then also like you know what are they",
    "start": "1095320",
    "end": "1096720"
  },
  {
    "text": "doing right because they're not worried",
    "start": "1096720",
    "end": "1097919"
  },
  {
    "text": "about you know llama 405b capable models",
    "start": "1097919",
    "end": "1102710"
  },
  {
    "text": "[Music]",
    "start": "1102710",
    "end": "1110149"
  }
]