[
  {
    "start": "0",
    "end": "79000"
  },
  {
    "text": "[Music]",
    "start": "4240",
    "end": "14039"
  },
  {
    "text": "today we will be talking about how to attack gimbals of ai power games",
    "start": "15040",
    "end": "20720"
  },
  {
    "text": "this is a joint work with my all the weather and colleague as we know",
    "start": "20720",
    "end": "27680"
  },
  {
    "text": "deep learning has become the hottest ml techniques in the past few years it has dominated",
    "start": "27680",
    "end": "34320"
  },
  {
    "text": "many supervised and unsupervised learning field in security it also outperformed the traditional",
    "start": "34320",
    "end": "41120"
  },
  {
    "text": "methods in malware detection and intuition detection going beyond supervised and unsupervised",
    "start": "41120",
    "end": "47920"
  },
  {
    "text": "learning reinforcement learning is a more powerful tool of learning technique that could handle",
    "start": "47920",
    "end": "55280"
  },
  {
    "text": "more complex tasks recently researcher combined",
    "start": "55280",
    "end": "60320"
  },
  {
    "text": "aisle with dl and developed different type of diio techniques these techniques has",
    "start": "60320",
    "end": "67600"
  },
  {
    "text": "shown extraordinary performance in many decision making tasks such as robotic control autonomous",
    "start": "67600",
    "end": "74799"
  },
  {
    "text": "vehicle finance and business management in games agent learn by drl",
    "start": "74799",
    "end": "83119"
  },
  {
    "start": "79000",
    "end": "79000"
  },
  {
    "text": "also be professional or even first class human players for example",
    "start": "83119",
    "end": "89360"
  },
  {
    "text": "demand alpha goal continual speed first class goal human goal players more",
    "start": "89360",
    "end": "95840"
  },
  {
    "text": "recently news shows that besides goal game dil agent also can",
    "start": "95840",
    "end": "101439"
  },
  {
    "text": "could beat professional poker game players in different type of poker game such as",
    "start": "101439",
    "end": "107439"
  },
  {
    "text": "texas holdem last year divment released an open",
    "start": "107439",
    "end": "112560"
  },
  {
    "text": "source package which includes multiple board game environment and a state of art diio learning",
    "start": "112560",
    "end": "119840"
  },
  {
    "text": "technique so users could learn could train their own agent to play with other il agents or even",
    "start": "119840",
    "end": "126799"
  },
  {
    "text": "human player in addition to both game diio also have",
    "start": "126799",
    "end": "132400"
  },
  {
    "text": "become the standard method of training a master agent in both simulation games and real-time",
    "start": "132400",
    "end": "139680"
  },
  {
    "text": "strategy games in simulation game open i released a package",
    "start": "139680",
    "end": "145520"
  },
  {
    "text": "called gym which includes different type of simulation game environment such as the",
    "start": "145520",
    "end": "151599"
  },
  {
    "text": "ones shown on the right hand side the atari game robot school game and musical games",
    "start": "151599",
    "end": "159519"
  },
  {
    "text": "so the user could test their diy technique on all of these environments recent news",
    "start": "159920",
    "end": "166720"
  },
  {
    "text": "also reports reported the effectiveness of drl in world famous real-time",
    "start": "166720",
    "end": "173200"
  },
  {
    "text": "strategy games such as starcraft 2 and open under dota 2.",
    "start": "173200",
    "end": "180480"
  },
  {
    "text": "along with the rapid development of the il technique researchers also start to investigate",
    "start": "181280",
    "end": "188319"
  },
  {
    "text": "the security property of drl especially at a stereo attack",
    "start": "188319",
    "end": "194000"
  },
  {
    "text": "as you may aware there have been many works about the adversarial attack on deep",
    "start": "194000",
    "end": "199599"
  },
  {
    "text": "learning so this attack can be meaning categorized as training phase attack",
    "start": "199599",
    "end": "205040"
  },
  {
    "text": "and the testing phase attacks since drl all dio systems also have a deep",
    "start": "205040",
    "end": "211920"
  },
  {
    "text": "learning involved so intuitively this iridial exam",
    "start": "211920",
    "end": "217360"
  },
  {
    "text": "should also be vulnerable to the ideal serial attack this intuition has been wedded by some",
    "start": "217360",
    "end": "224239"
  },
  {
    "text": "recent works specifically this work has shown that",
    "start": "224239",
    "end": "229599"
  },
  {
    "text": "the attacker could perturb an agent's observation action or rewards and fail the agent",
    "start": "229599",
    "end": "237599"
  },
  {
    "text": "and force the agent to fail the corresponding tasks",
    "start": "237599",
    "end": "242720"
  },
  {
    "text": "however as we will show later discuss later these attacks are not practical",
    "start": "242720",
    "end": "249599"
  },
  {
    "text": "meaning because they involve hydrating a game system which is time consuming and cannot",
    "start": "249599",
    "end": "256000"
  },
  {
    "text": "always guarantee to be succeed so in this talk we will present how to",
    "start": "256000",
    "end": "261680"
  },
  {
    "text": "enable a practical adversarial attack against a game bot or master agent in a two-party game",
    "start": "261680",
    "end": "269199"
  },
  {
    "text": "environment this is the agenda for today's talk we",
    "start": "269199",
    "end": "274560"
  },
  {
    "text": "will start with some background knowledge of drl",
    "start": "274560",
    "end": "279680"
  },
  {
    "text": "then we'll introduce some air power games and how to tune a boat for these games",
    "start": "279680",
    "end": "287360"
  },
  {
    "text": "then we will introduce existing attacks on dil game bots and discuss their",
    "start": "288000",
    "end": "294840"
  },
  {
    "text": "limitations based on the limitation of existing work we will elaborate on",
    "start": "294840",
    "end": "300320"
  },
  {
    "text": "our attack mass knowledge finally we will show the evaluation results and conclude our talk",
    "start": "300320",
    "end": "308400"
  },
  {
    "start": "308000",
    "end": "308000"
  },
  {
    "text": "at a high level an io problem is the decision making problem in",
    "start": "308639",
    "end": "315440"
  },
  {
    "text": "in way specifically it has an agent who observes and interacts with an",
    "start": "315440",
    "end": "321600"
  },
  {
    "text": "environment through a series of actions each time this agent takes action it will receive a reward",
    "start": "321600",
    "end": "329840"
  },
  {
    "text": "take this entire game as an example in this game the agent is the blue belt",
    "start": "329840",
    "end": "336160"
  },
  {
    "text": "the environment is just a game itself at each time step the agent observe the",
    "start": "336160",
    "end": "342960"
  },
  {
    "text": "environment and take action accordingly at this time the agent probably gonna go",
    "start": "342960",
    "end": "349360"
  },
  {
    "text": "right and try to catch the ball",
    "start": "349360",
    "end": "353840"
  },
  {
    "text": "after taking this action the agent will receive a reward from the environment so in this game the",
    "start": "355120",
    "end": "362240"
  },
  {
    "text": "reward probably be how many breaks the agent has hit then the game environment will receive",
    "start": "362240",
    "end": "368960"
  },
  {
    "text": "this action and translate to the next stick based on the transition dynamics",
    "start": "368960",
    "end": "375039"
  },
  {
    "text": "this transition dynamic is usually unknown to the learning algorithm",
    "start": "375039",
    "end": "381840"
  },
  {
    "start": "381000",
    "end": "381000"
  },
  {
    "text": "solving io problem is equivalent to training our io agent the goal of this",
    "start": "381840",
    "end": "388000"
  },
  {
    "text": "agent is to maximize its total amount of reward",
    "start": "388000",
    "end": "393759"
  },
  {
    "text": "so in this case the goal of an il algorithm is to learn an optimal policy",
    "start": "393759",
    "end": "400160"
  },
  {
    "text": "follow which the agent could receive a maximum maximum amount of rewards over time",
    "start": "400160",
    "end": "407759"
  },
  {
    "text": "so as we can see here the total amount of reward is very important for training an ir",
    "start": "407759",
    "end": "413919"
  },
  {
    "text": "agent in il it is represented by value function",
    "start": "413919",
    "end": "419360"
  },
  {
    "text": "specifically it has two forms and our optimal policy",
    "start": "419360",
    "end": "427360"
  },
  {
    "text": "can be obtained by maximize either of the value functions",
    "start": "427360",
    "end": "432720"
  },
  {
    "text": "take the game on the left hand side as an example here the agent who could move within the",
    "start": "432720",
    "end": "439759"
  },
  {
    "text": "check board and collect reward based on its move suppose we somehow know the value",
    "start": "439759",
    "end": "446319"
  },
  {
    "text": "function shown in the figure in the middle so based on this value function of each",
    "start": "446319",
    "end": "451680"
  },
  {
    "text": "state the agent will then choose its action for example on the top left corner the",
    "start": "451680",
    "end": "458400"
  },
  {
    "text": "best move here is to go right because it will collect more reward",
    "start": "458400",
    "end": "463440"
  },
  {
    "text": "than going down in drl an agent is usually modeled",
    "start": "463440",
    "end": "470319"
  },
  {
    "text": "as a deep neural network which is called policy network this network takes as",
    "start": "470319",
    "end": "477120"
  },
  {
    "text": "input the observation and the output the corresponding action take the figure",
    "start": "477120",
    "end": "483599"
  },
  {
    "text": "on the right hand top right hand side as an example here the agent is the policy network",
    "start": "483599",
    "end": "490319"
  },
  {
    "text": "shown here this network then takes us input the game snapshot that is",
    "start": "490319",
    "end": "496319"
  },
  {
    "text": "observation and the output the corresponding action such as going down",
    "start": "496319",
    "end": "501840"
  },
  {
    "text": "going up down under either up down or left and right so in this case",
    "start": "501840",
    "end": "509199"
  },
  {
    "text": "learning a policy is equivalent to solving the parameter of this neural",
    "start": "509199",
    "end": "514640"
  },
  {
    "text": "network the method that's used to solve the",
    "start": "514640",
    "end": "520959"
  },
  {
    "text": "network parameters are called policy grading method recall the goal of a drl algorithm is to",
    "start": "520959",
    "end": "528800"
  },
  {
    "text": "maximize the value function but surely the value function of",
    "start": "528800",
    "end": "535920"
  },
  {
    "text": "of some like complex scheme are usually unknown so the so here in dll",
    "start": "535920",
    "end": "544320"
  },
  {
    "text": "the the policy network a policy reading method usually use another network to",
    "start": "544320",
    "end": "550240"
  },
  {
    "text": "approximate the value function as such in each iteration of the",
    "start": "550240",
    "end": "555519"
  },
  {
    "text": "algorithm it will they will first update the value value function network by minimizing the",
    "start": "555519",
    "end": "562800"
  },
  {
    "text": "approximation error then they will update the policy network by maximizing",
    "start": "562800",
    "end": "568800"
  },
  {
    "text": "the value function as shown in the example on the bottom right hand side",
    "start": "568800",
    "end": "576000"
  },
  {
    "text": "these two networks the usually share parameter",
    "start": "576000",
    "end": "581279"
  },
  {
    "text": "next we will introduce some dil power games considered in our work and the code",
    "start": "581600",
    "end": "588480"
  },
  {
    "text": "structure of training and diio board for these games",
    "start": "588480",
    "end": "594480"
  },
  {
    "start": "594000",
    "end": "594000"
  },
  {
    "text": "in this work we focus on both the simulation game and real-time strategy games for",
    "start": "594959",
    "end": "601440"
  },
  {
    "text": "simulation games we are talking we are constantly we are considering two-party mutual games in this game",
    "start": "601440",
    "end": "610000"
  },
  {
    "text": "the observation usually is usually the current status of the environment including the agents",
    "start": "610000",
    "end": "616720"
  },
  {
    "text": "and its components statics the action of an agent just the agent's movement",
    "start": "616720",
    "end": "622560"
  },
  {
    "text": "including like moving direction and moving speed the reward of a game is the agent's",
    "start": "622560",
    "end": "628560"
  },
  {
    "text": "statics and a windows condition going beyond the simulation games we",
    "start": "628560",
    "end": "635920"
  },
  {
    "text": "also consider a real-time strategy game starcraft 2.",
    "start": "635920",
    "end": "641519"
  },
  {
    "text": "in this game the observation is a special condition of the map and the amount of resources has been",
    "start": "641519",
    "end": "649040"
  },
  {
    "text": "collected by the agent the action are categorized",
    "start": "649040",
    "end": "654079"
  },
  {
    "text": "into four classes building pro building unit construction",
    "start": "654079",
    "end": "661360"
  },
  {
    "text": "producing units such as workforce and armies high-risk resources and finally",
    "start": "661360",
    "end": "668160"
  },
  {
    "text": "attacking enemy the reward of this game includes",
    "start": "668160",
    "end": "675040"
  },
  {
    "text": "the game statistics and the windows condition",
    "start": "675040",
    "end": "680079"
  },
  {
    "start": "679000",
    "end": "679000"
  },
  {
    "text": "recall that the policy grading framework is the standard way of training a dil",
    "start": "680399",
    "end": "687200"
  },
  {
    "text": "agent following in this framework researchers have developed a different type of",
    "start": "687200",
    "end": "692720"
  },
  {
    "text": "learning algorithm among the among this algorithm the widely used one",
    "start": "692720",
    "end": "698640"
  },
  {
    "text": "in training a gimbals is the ppo algorithm here is the augmented workflow of",
    "start": "698640",
    "end": "705680"
  },
  {
    "text": "ppo algorithm first one need to initialize the network",
    "start": "705680",
    "end": "711680"
  },
  {
    "text": "parameter for policy network and value function networks",
    "start": "711680",
    "end": "716880"
  },
  {
    "text": "then in each iteration it first collect a set of trajectory by playing the current policy in the",
    "start": "716880",
    "end": "724240"
  },
  {
    "text": "environment after cracking this in this trajectory",
    "start": "724240",
    "end": "729680"
  },
  {
    "text": "as is mentioned above it will update the policy network and the value function network by using",
    "start": "729680",
    "end": "737040"
  },
  {
    "text": "this trajectories as the code level take starcraft 2 game as an example",
    "start": "737040",
    "end": "745040"
  },
  {
    "text": "to train a game bot to train a game board the first first we need a programmatic game",
    "start": "745040",
    "end": "751920"
  },
  {
    "text": "environment with the environment backhand the code structure",
    "start": "751920",
    "end": "757760"
  },
  {
    "text": "is shown in the figure on the right hand side so basically we need three major parts",
    "start": "757760",
    "end": "764399"
  },
  {
    "text": "first is the agent which defined on constructing the two networks the policy",
    "start": "764399",
    "end": "769920"
  },
  {
    "text": "network and the value function network for the agent then we need to write the environment",
    "start": "769920",
    "end": "775920"
  },
  {
    "text": "environment weber by using the environment package list above so this environment weber",
    "start": "775920",
    "end": "785360"
  },
  {
    "text": "are used to play the agent in in the environment and correct the",
    "start": "785360",
    "end": "790399"
  },
  {
    "text": "trajectory finally the main file used to run the agent in the environment and train the",
    "start": "790399",
    "end": "797519"
  },
  {
    "text": "networks defend above",
    "start": "797519",
    "end": "801839"
  },
  {
    "text": "to train game bots to train on drill game bots",
    "start": "802880",
    "end": "807920"
  },
  {
    "text": "for two agent or multi-agent game one standard strategy is the self-play",
    "start": "807920",
    "end": "815120"
  },
  {
    "text": "mechanism that is training an agent to play against itself",
    "start": "815120",
    "end": "820959"
  },
  {
    "text": "until the winning rate for both party are 50 specifically the main field for training",
    "start": "820959",
    "end": "828959"
  },
  {
    "text": "on board with this strategy is as follows we first need to define game environment",
    "start": "828959",
    "end": "835839"
  },
  {
    "text": "by using the environment weapon so as you can see from the upper right",
    "start": "835839",
    "end": "842639"
  },
  {
    "text": "frame in right hand side figure then we define a runner",
    "start": "842639",
    "end": "848079"
  },
  {
    "text": "to run the agent in the environment and collect the trajectory",
    "start": "848079",
    "end": "853199"
  },
  {
    "text": "finally we define a learned learner to which receive the trajectory",
    "start": "853199",
    "end": "859360"
  },
  {
    "text": "collected by runner and update the policy network and value function",
    "start": "859360",
    "end": "865440"
  },
  {
    "text": "network in the self-play mechanism it will iteratively",
    "start": "865440",
    "end": "871040"
  },
  {
    "text": "update the policy for each party until the winning rate for both party stabilized at 50 percent",
    "start": "871040",
    "end": "880720"
  },
  {
    "text": "after introducing the basics of drl and how to train a game board or master",
    "start": "880880",
    "end": "886800"
  },
  {
    "text": "agent by using a diy algorithm we now proceed to how to attack the scheme bots next",
    "start": "886800",
    "end": "894800"
  },
  {
    "text": "cm will introduce the existing attacks on drl game bought the existing attacks on deep",
    "start": "894800",
    "end": "902399"
  },
  {
    "text": "reinforcement learning can be summarized as two categories and the first one is a probation-based",
    "start": "902399",
    "end": "908399"
  },
  {
    "text": "attacks the second one is a more practical adversarial agent attack",
    "start": "908399",
    "end": "914800"
  },
  {
    "start": "914000",
    "end": "914000"
  },
  {
    "text": "firstly i will introduce the probation based text as we can see from the right picture",
    "start": "914800",
    "end": "921040"
  },
  {
    "text": "that var0 can either protect observation and thus force the post network to output a series of",
    "start": "921040",
    "end": "927680"
  },
  {
    "text": "sub-optimal actions or directly add perturbations to the output actions of the poison network",
    "start": "927680",
    "end": "935120"
  },
  {
    "text": "here is a pump game example we can see from the bottom image two agent is playing the",
    "start": "935120",
    "end": "942240"
  },
  {
    "text": "pawn with each other for the current snapshot the optimal action for the right agent",
    "start": "942240",
    "end": "947920"
  },
  {
    "text": "is done before the attack then attacker generates perturbations by using the",
    "start": "947920",
    "end": "953120"
  },
  {
    "text": "existing attacks on different neural networks and as it to the current observation to induce the",
    "start": "953120",
    "end": "960000"
  },
  {
    "text": "right agent to output a sub-optimal action by doing so the right agent may fail",
    "start": "960000",
    "end": "967360"
  },
  {
    "text": "in this game although the perturbation-based attacks",
    "start": "967360",
    "end": "973120"
  },
  {
    "text": "have achieved some success it has not limitations in the real-world setting",
    "start": "973120",
    "end": "979040"
  },
  {
    "text": "in the game set up it requires the attacker to hijack the game server to illustrate",
    "start": "979040",
    "end": "985279"
  },
  {
    "text": "this argument we again take for example the affirmation online games",
    "start": "985279",
    "end": "990880"
  },
  {
    "text": "in these examples the activities of manipulating the environment means that adversary breaks into the",
    "start": "990880",
    "end": "998240"
  },
  {
    "text": "game server or to the game coder and thus influence the environment that the agent interacts with",
    "start": "998240",
    "end": "1005600"
  },
  {
    "text": "considering this it requires professional handlers tremendous effort and time",
    "start": "1005600",
    "end": "1011199"
  },
  {
    "text": "what's more the preservation best attacks is not a practical setup for beating a",
    "start": "1011199",
    "end": "1017680"
  },
  {
    "text": "master agent of a two-party game next i will talk about the adversarial",
    "start": "1017680",
    "end": "1025280"
  },
  {
    "start": "1022000",
    "end": "1022000"
  },
  {
    "text": "agent attack as is shown in the right picture the attacker is not allowed to",
    "start": "1025280",
    "end": "1030640"
  },
  {
    "text": "hijack the information flow of the victim agent but the attacker could train an",
    "start": "1030640",
    "end": "1035760"
  },
  {
    "text": "adversarial agent by playing with the victim agent compared with the perturbation-based",
    "start": "1035760",
    "end": "1042240"
  },
  {
    "text": "attack it's more practical in games since we need not hang the game system",
    "start": "1042240",
    "end": "1048720"
  },
  {
    "text": "and any player could play with the master agent freely",
    "start": "1048720",
    "end": "1053840"
  },
  {
    "text": "there is some existing technique in adversarial agent attack in this work it treats the victim agent",
    "start": "1055520",
    "end": "1062880"
  },
  {
    "text": "as part of the environment and trains the agent to collect maximum rewards in the environment",
    "start": "1062880",
    "end": "1069600"
  },
  {
    "text": "specifically it use the ppo algorithm to maximize the training agent's value",
    "start": "1069600",
    "end": "1074880"
  },
  {
    "text": "function and expect to obtain a policy that could beat the big team",
    "start": "1074880",
    "end": "1080240"
  },
  {
    "text": "however as we will shoot later it cannot establish a high game ring",
    "start": "1080240",
    "end": "1085919"
  },
  {
    "text": "rate the reason is that it doesn't explicitly disturb the victim agent",
    "start": "1085919",
    "end": "1092160"
  },
  {
    "text": "and the training algorithm has less guidance for identifying the weakness of the",
    "start": "1092160",
    "end": "1098320"
  },
  {
    "text": "victim a next one we will talk about",
    "start": "1098320",
    "end": "1106559"
  },
  {
    "text": "as is introduced by xin existing attack either rely on unrealistic assumptions",
    "start": "1107200",
    "end": "1114559"
  },
  {
    "text": "or is not able to achieve a decent attack success rate in the following",
    "start": "1114559",
    "end": "1121760"
  },
  {
    "text": "we will elaborate on our idea of training an adversarial agent to",
    "start": "1121760",
    "end": "1127520"
  },
  {
    "text": "exploit the weakness of of its opponent and thus defeat it in a two-party game",
    "start": "1127520",
    "end": "1137440"
  },
  {
    "start": "1137000",
    "end": "1137000"
  },
  {
    "text": "our attack's rare model is the same with side-to-serial attack introduced",
    "start": "1137440",
    "end": "1143280"
  },
  {
    "text": "before but to tackle the limitation of this existing attack",
    "start": "1143280",
    "end": "1148799"
  },
  {
    "text": "we propose to argument it with new to two new designs the goal of this design",
    "start": "1148799",
    "end": "1155679"
  },
  {
    "text": "are the same that is true an agent to not only maximize its reward",
    "start": "1155679",
    "end": "1162400"
  },
  {
    "text": "but also prevent its opponent from collecting more reward at the same",
    "start": "1162400",
    "end": "1168480"
  },
  {
    "text": "time so in other words we would like we will like the idol zero agent",
    "start": "1168480",
    "end": "1174960"
  },
  {
    "text": "learns to how to perturb or disturb its opponent so specifically",
    "start": "1174960",
    "end": "1182400"
  },
  {
    "text": "our first idea is to directly change the adorable learning objective",
    "start": "1182400",
    "end": "1188880"
  },
  {
    "text": "to not only maximize its own reward but also minimize its opponent's reward",
    "start": "1188880",
    "end": "1195600"
  },
  {
    "text": "at the same time our second idea is to let the adversarial agent",
    "start": "1195600",
    "end": "1202320"
  },
  {
    "text": "take the action that deviates the victim's nest action",
    "start": "1202320",
    "end": "1210480"
  },
  {
    "text": "to realize the first design recall that the value function are unknown to",
    "start": "1210799",
    "end": "1216880"
  },
  {
    "text": "the learning algorithm so the first step is to approximate the victim value function with another",
    "start": "1216880",
    "end": "1224159"
  },
  {
    "text": "neural network with this approximation by hand with an id term to the ideal serial learning",
    "start": "1224159",
    "end": "1231919"
  },
  {
    "text": "objective this term is used to minimize this approximated",
    "start": "1231919",
    "end": "1237679"
  },
  {
    "text": "value function to give you an example of the difference",
    "start": "1237679",
    "end": "1244240"
  },
  {
    "text": "between our objective and the existing attack which only maximizing",
    "start": "1244240",
    "end": "1249919"
  },
  {
    "text": "the which only maximizes the idle zero agents a total expected reward we use the",
    "start": "1249919",
    "end": "1258159"
  },
  {
    "text": "cracking resources game as an example in this scheme the goal of an agent is collect more",
    "start": "1258159",
    "end": "1265919"
  },
  {
    "text": "resources than its opponent without newly added term the adversarial agent",
    "start": "1265919",
    "end": "1273360"
  },
  {
    "text": "focused only on itself in other words it will only optimize its policy and try",
    "start": "1273360",
    "end": "1279520"
  },
  {
    "text": "to collect more rewards or more resources however if the victim",
    "start": "1279520",
    "end": "1285520"
  },
  {
    "text": "agent or opponent or opponent agent has a better strategy of cracking",
    "start": "1285520",
    "end": "1293360"
  },
  {
    "text": "resources it will be extremely hard for the adversarial agent",
    "start": "1293360",
    "end": "1298960"
  },
  {
    "text": "learns a better strategy than the victim and thus beats 18",
    "start": "1298960",
    "end": "1306080"
  },
  {
    "text": "however with the added term in this case that was zero agent will learn to",
    "start": "1306080",
    "end": "1313919"
  },
  {
    "text": "block the victim from collecting more rewards so it will have a higher chance to beat",
    "start": "1313919",
    "end": "1321360"
  },
  {
    "text": "its opponent no matter how good the opponent's policy is",
    "start": "1321360",
    "end": "1327360"
  },
  {
    "text": "as for the second design we first leverage a model explanation method to",
    "start": "1327760",
    "end": "1334000"
  },
  {
    "text": "expand the action of the victim and find out time step",
    "start": "1334000",
    "end": "1339520"
  },
  {
    "text": "when victim takes action based on the adversarial agent",
    "start": "1339520",
    "end": "1344960"
  },
  {
    "text": "then in this critical time steps we will optimize the adversarial policy to take",
    "start": "1344960",
    "end": "1351919"
  },
  {
    "text": "an action that will introduce a maximum deviation in the victims next action",
    "start": "1351919",
    "end": "1360880"
  },
  {
    "text": "let me demonstrate this design with the example here this is a robot school punk game in",
    "start": "1360880",
    "end": "1367679"
  },
  {
    "text": "which the master agent the purple one is playing against the zero agent which is the blue",
    "start": "1367679",
    "end": "1375120"
  },
  {
    "text": "one recall the observation of an agent",
    "start": "1375120",
    "end": "1380559"
  },
  {
    "text": "is the current game statics which includes its opponent here the red flame",
    "start": "1380559",
    "end": "1387919"
  },
  {
    "text": "in in this factor indicates that what that was adversarial part in the victim",
    "start": "1387919",
    "end": "1395360"
  },
  {
    "text": "observation so this observation is then given to the victim policy",
    "start": "1395360",
    "end": "1401679"
  },
  {
    "text": "network and outputs the corresponding action with the model explanation method we",
    "start": "1401679",
    "end": "1409200"
  },
  {
    "text": "could calculate the importance or inference of each input dimension on the output prediction",
    "start": "1409200",
    "end": "1417919"
  },
  {
    "text": "so as such we could use this technique to identify the inference of the android",
    "start": "1417919",
    "end": "1424400"
  },
  {
    "text": "serial on a victim action just the red part red red part in the victim of",
    "start": "1424400",
    "end": "1431760"
  },
  {
    "text": "observation so the difference of the red part on output",
    "start": "1431760",
    "end": "1437279"
  },
  {
    "text": "output action and stack select the time step when i do",
    "start": "1437279",
    "end": "1444400"
  },
  {
    "text": "a 0 have a large impact on the victim",
    "start": "1444400",
    "end": "1449440"
  },
  {
    "text": "at this selected time steps we if we slightly change the action of the adversary agent it",
    "start": "1449520",
    "end": "1457039"
  },
  {
    "text": "will also change the observation of the victim agent so this small change in the weight",
    "start": "1457039",
    "end": "1465360"
  },
  {
    "text": "in the victims observation will then trigger a relatively large change",
    "start": "1465360",
    "end": "1472559"
  },
  {
    "text": "in the victim in the victims action so this process is similar to adding",
    "start": "1472559",
    "end": "1479200"
  },
  {
    "text": "adversarial probation to a deep learning deep neural network's input so here",
    "start": "1479200",
    "end": "1486559"
  },
  {
    "text": "instead of directly change the vector value we consider a more practical setup that",
    "start": "1486559",
    "end": "1493520"
  },
  {
    "text": "is indirectly change the observation of victim",
    "start": "1493520",
    "end": "1499039"
  },
  {
    "text": "by changing the action of the adversary in this example the trajectory in the",
    "start": "1499039",
    "end": "1506799"
  },
  {
    "text": "greek hammers represent the ordinary trajectory so here the master agent could go goes towards",
    "start": "1506799",
    "end": "1515039"
  },
  {
    "text": "the ball and try to catch it and win again however if we preterm that was serious",
    "start": "1515039",
    "end": "1522080"
  },
  {
    "text": "action that is choose a different action in a critical time step",
    "start": "1522080",
    "end": "1527200"
  },
  {
    "text": "selected above at those time step the victim will then take a different",
    "start": "1527200",
    "end": "1533200"
  },
  {
    "text": "action due to the probation like the trajectory in red canvas here",
    "start": "1533200",
    "end": "1541520"
  },
  {
    "text": "the purple agent the master agent no longer move towards the ball due to",
    "start": "1541520",
    "end": "1547360"
  },
  {
    "text": "the probation as such it cannot catch the ball and will lose the game",
    "start": "1547360",
    "end": "1554158"
  },
  {
    "text": "after introducing our attack mass knowledge we now evaluate it on some static games",
    "start": "1554320",
    "end": "1561279"
  },
  {
    "text": "and show our interesting findings specifically we choose five games four",
    "start": "1561279",
    "end": "1568400"
  },
  {
    "text": "of which are from musical game and one is real world strategic starcraft 2 game as for mutual",
    "start": "1568400",
    "end": "1576159"
  },
  {
    "text": "games the demonstration of the static game are shown on the right hand side",
    "start": "1576159",
    "end": "1581840"
  },
  {
    "text": "the first game k khan defend is just a standard penalty shootout in second you should",
    "start": "1581840",
    "end": "1589760"
  },
  {
    "text": "not pass game the runner the blue agent is running towards the red line the finish line",
    "start": "1589760",
    "end": "1597360"
  },
  {
    "text": "behind the red agent then the blocker the red agent in turns to block the blue agent",
    "start": "1597360",
    "end": "1604720"
  },
  {
    "text": "from crossing the line in both human arms and human human games the two agents",
    "start": "1604720",
    "end": "1612080"
  },
  {
    "text": "are fighting against each other on our arena and try to push its opponent",
    "start": "1612080",
    "end": "1619279"
  },
  {
    "text": "off of the arena as for the starcraft 2 game we consider",
    "start": "1619279",
    "end": "1626960"
  },
  {
    "text": "a two-party scenario verzac versus zlac recall a drl algorithm iteratively",
    "start": "1626960",
    "end": "1635279"
  },
  {
    "text": "updates the policy with a matter and report the winning rate of the adversarial",
    "start": "1635279",
    "end": "1641279"
  },
  {
    "text": "agent each time its policy is updated during the training process",
    "start": "1641279",
    "end": "1648960"
  },
  {
    "start": "1648000",
    "end": "1648000"
  },
  {
    "text": "here is the comparison of the reading rates of the adversarial agent obtained by",
    "start": "1649200",
    "end": "1655120"
  },
  {
    "text": "different attacks the red line is refers to over attack and blue line",
    "start": "1655120",
    "end": "1661360"
  },
  {
    "text": "refers to the existing attack as we can observe",
    "start": "1661360",
    "end": "1666480"
  },
  {
    "text": "from these figures our attack outperformed the existing attack on most games",
    "start": "1666480",
    "end": "1673760"
  },
  {
    "text": "the only express exception is the swimmer arms game in this game both attack cannot improve",
    "start": "1673760",
    "end": "1681760"
  },
  {
    "text": "the tax success rate of the adversarial agent",
    "start": "1681760",
    "end": "1687360"
  },
  {
    "text": "the reason is that the observation in this game is of low dimensionality which means",
    "start": "1687360",
    "end": "1694720"
  },
  {
    "text": "the probation space is relatively small as such it's kind of hard to disturb the",
    "start": "1694720",
    "end": "1701360"
  },
  {
    "text": "victim where the adversarial action and the defeat victim",
    "start": "1701360",
    "end": "1706480"
  },
  {
    "text": "accordingly however as is true in the figure in the below",
    "start": "1706480",
    "end": "1713279"
  },
  {
    "text": "or attack improves the non-lose rate of the adversary agent as we will",
    "start": "1713279",
    "end": "1719840"
  },
  {
    "text": "explanate her dispatch cannot influence the action of the victim agent our attack",
    "start": "1719840",
    "end": "1727600"
  },
  {
    "text": "could give the adversarial agent some advantages by exploit exploiting the",
    "start": "1727600",
    "end": "1734559"
  },
  {
    "text": "vulnerability of the game design after showing the quantitatively",
    "start": "1734559",
    "end": "1741279"
  },
  {
    "start": "1738000",
    "end": "1738000"
  },
  {
    "text": "evaluation we now show some game episodes of our adversarial agent play against",
    "start": "1741279",
    "end": "1748960"
  },
  {
    "text": "the victim",
    "start": "1748960",
    "end": "1753840"
  },
  {
    "text": "first as we can see from the kikhan defend and usual now pass game",
    "start": "1755039",
    "end": "1762799"
  },
  {
    "text": "our attack could exploit the victim",
    "start": "1762960",
    "end": "1768080"
  },
  {
    "text": "the weakness of the victim agent by establishing some weird behaviors",
    "start": "1768960",
    "end": "1776799"
  },
  {
    "text": "in human human video",
    "start": "1777520",
    "end": "1781840"
  },
  {
    "text": "sorry human human video all attack could help",
    "start": "1789440",
    "end": "1796799"
  },
  {
    "text": "learn stress learn a better strategy that is initialize itself near the boundary",
    "start": "1796799",
    "end": "1803520"
  },
  {
    "text": "and lower the victim to attacking it and fell from the",
    "start": "1803520",
    "end": "1810559"
  },
  {
    "text": "arena",
    "start": "1810559",
    "end": "1813039"
  },
  {
    "text": "more interestingly in the summer on scheme we can say that it will seriously",
    "start": "1816880",
    "end": "1822399"
  },
  {
    "text": "intentionally flow from the arena",
    "start": "1822399",
    "end": "1826559"
  },
  {
    "text": "the adult zero intentionally fall from the arena of the game beginning in this",
    "start": "1830480",
    "end": "1837200"
  },
  {
    "text": "case the game ends up with a draw this shows that our attack could explore the",
    "start": "1837200",
    "end": "1844240"
  },
  {
    "text": "victimly the weakness of the game rule because one of the rules is that",
    "start": "1844240",
    "end": "1850080"
  },
  {
    "text": "if one player or one party falls from the arena without touching its opponent the game",
    "start": "1850080",
    "end": "1857360"
  },
  {
    "text": "ends up with a jaw to hear that or serial agent you use this rule to force the game",
    "start": "1857360",
    "end": "1864880"
  },
  {
    "text": "ends up with the draw in addition to the attack we also",
    "start": "1864880",
    "end": "1871519"
  },
  {
    "start": "1868000",
    "end": "1868000"
  },
  {
    "text": "consider a possible defense that widely used that is widely used",
    "start": "1871519",
    "end": "1877679"
  },
  {
    "text": "the adversary training strategy specifically we play the victim agent",
    "start": "1877679",
    "end": "1883840"
  },
  {
    "text": "with our trained and also adversarial agent",
    "start": "1883840",
    "end": "1889120"
  },
  {
    "text": "and return the victim agent with our proposed attack",
    "start": "1889120",
    "end": "1894799"
  },
  {
    "text": "the results are showing the figure in this slide as we can observe from the figure as the",
    "start": "1894799",
    "end": "1902559"
  },
  {
    "text": "adverse theory training process indeed improve the performance of the victim by winning",
    "start": "1902559",
    "end": "1909360"
  },
  {
    "text": "the usual pass game and achieving a draw on the other three games",
    "start": "1909360",
    "end": "1915200"
  },
  {
    "text": "but however it doesn't work on kick hand defense game",
    "start": "1915200",
    "end": "1920240"
  },
  {
    "text": "we suspect this is caused by the unfairness of the game design because we have tried",
    "start": "1920240",
    "end": "1927039"
  },
  {
    "text": "that it's relatively hard for the kicker to earn the win the game in general",
    "start": "1927039",
    "end": "1935279"
  },
  {
    "text": "here are some videos of using the return victim agent play against the adversary",
    "start": "1935279",
    "end": "1942559"
  },
  {
    "text": "in the first video the usual now pass game the victim learns to ignore the",
    "start": "1942559",
    "end": "1948000"
  },
  {
    "text": "adversarial and directly go for the finish line which means the victim agent will be aware of his",
    "start": "1948000",
    "end": "1956080"
  },
  {
    "text": "policy weakness and patched awakenings through the retraining process",
    "start": "1956080",
    "end": "1961519"
  },
  {
    "text": "in the second video the victim recognized the trick played by the adult zero it will stay",
    "start": "1961519",
    "end": "1968240"
  },
  {
    "text": "where it is so the game will end up with the jaw so which means in this human human game",
    "start": "1968240",
    "end": "1975279"
  },
  {
    "text": "the victim also realized its policy weakness and tried to fix it in the summer arms game",
    "start": "1975279",
    "end": "1983919"
  },
  {
    "text": "since the victim cannot change the intentional intentional behavior of the adversary",
    "start": "1983919",
    "end": "1990799"
  },
  {
    "text": "so the returning process basically still keeps the original behavior",
    "start": "1990799",
    "end": "1997039"
  },
  {
    "text": "of either victim both victim and adversarial so the game still staying as",
    "start": "1997039",
    "end": "2003840"
  },
  {
    "text": "thai games finally in the k can't defend games as we mentioned above",
    "start": "2003840",
    "end": "2010000"
  },
  {
    "text": "the victim acts even worse it's more easier to fall into the ground trigger it",
    "start": "2010000",
    "end": "2017120"
  },
  {
    "text": "to lose the game",
    "start": "2017120",
    "end": "2019840"
  },
  {
    "text": "finally we conclude this talk with three conclusions first attacker could train an adult",
    "start": "2023919",
    "end": "2031760"
  },
  {
    "text": "serial agent to defeat a gimbal for for an air power game but disturb",
    "start": "2031760",
    "end": "2039360"
  },
  {
    "text": "we also found that by disturbing the victim action that was zero agent could exploit the",
    "start": "2039360",
    "end": "2046240"
  },
  {
    "text": "vulnerability of both the game victim agent and the game rule and thus fill the victim agent more",
    "start": "2046240",
    "end": "2054000"
  },
  {
    "text": "effect more effectively finally we show that adversarial training does not always",
    "start": "2054000",
    "end": "2061118"
  },
  {
    "text": "succeed we may need more advanced techniques to protect the game boss and master agent",
    "start": "2061119",
    "end": "2070320"
  },
  {
    "text": "thank you very much for your attention we now open up to answer some questions",
    "start": "2070800",
    "end": "2077760"
  },
  {
    "text": "okay hello hello everyone i'm wilbur thanks everyone for attending our",
    "start": "2079919",
    "end": "2085358"
  },
  {
    "text": "session since at the beginning of this session the self introduction was missing so",
    "start": "2085359",
    "end": "2091440"
  },
  {
    "text": "i'll just do a uh reintroduction here so i'm one phd student at penn state",
    "start": "2091440",
    "end": "2097680"
  },
  {
    "text": "so this work is a joint work with my advisor she and my colleague shia and jimmy",
    "start": "2097680",
    "end": "2104480"
  },
  {
    "text": "so the main speaker of this uh of this talk happened those was me and xian took a",
    "start": "2104480",
    "end": "2111119"
  },
  {
    "text": "small part of it so the speakers are",
    "start": "2111119",
    "end": "2116000"
  },
  {
    "text": "so before i answer any questions there there are some questions in the question chat box i just want to clarify",
    "start": "2116240",
    "end": "2124320"
  },
  {
    "text": "some things okay first this work is about attacking a master agent you know deep reinforced",
    "start": "2124320",
    "end": "2131440"
  },
  {
    "text": "learning uh uh like environment which i say we have like so many online games some",
    "start": "2131440",
    "end": "2138400"
  },
  {
    "text": "of you may aware some like news uh talking about ai both",
    "start": "2138400",
    "end": "2144000"
  },
  {
    "text": "ai game bot beating professional uh like a player in dota 2 star part 2",
    "start": "2144000",
    "end": "2150240"
  },
  {
    "text": "those type of games and with the success of ai and the like deep reinforcement in base game",
    "start": "2150240",
    "end": "2157040"
  },
  {
    "text": "balls more and more game are using this top both instead of like rule-based sports",
    "start": "2157040",
    "end": "2162560"
  },
  {
    "text": "as their master agent maybe there is some agent trained by deep enforcement the goal of our attack is to attack such",
    "start": "2162560",
    "end": "2170160"
  },
  {
    "text": "type of and so currently uh we have some like uh",
    "start": "2170160",
    "end": "2176960"
  },
  {
    "text": "we will like about the code of this attack will open something like the currently we are still under",
    "start": "2176960",
    "end": "2182160"
  },
  {
    "text": "construction i it will be open source like in like one month i think we will do",
    "start": "2182160",
    "end": "2188000"
  },
  {
    "text": "like as soon as possible okay so i've received some questions in the",
    "start": "2188000",
    "end": "2193680"
  },
  {
    "text": "in the chat box the first question i think there are like very good questions the first is about",
    "start": "2193680",
    "end": "2200480"
  },
  {
    "text": "uh whether this attack to hunter asked about this whether it's your type just for",
    "start": "2200480",
    "end": "2205599"
  },
  {
    "text": "multi-party game or or like non-cooperative games so that's",
    "start": "2205599",
    "end": "2210640"
  },
  {
    "text": "correct our attack is only only works in the multi-party",
    "start": "2210640",
    "end": "2215760"
  },
  {
    "text": "competitive game scenario so to be more specific it all currently will only do two party",
    "start": "2215760",
    "end": "2221839"
  },
  {
    "text": "game so we would like to extend the game to multiplayer game in the future",
    "start": "2221839",
    "end": "2227119"
  },
  {
    "text": "and he also asked about the idea of this attack is to affect the learning of the victim",
    "start": "2227119",
    "end": "2234160"
  },
  {
    "text": "so actually as i just mentioned earlier since we are attacking master agent",
    "start": "2234160",
    "end": "2240480"
  },
  {
    "text": "so here our victim agent in our scenario uh its policy is prediction which means",
    "start": "2240480",
    "end": "2247119"
  },
  {
    "text": "that during the attack process the policy of the victim region is not learned for our attacking just to learn our own",
    "start": "2247119",
    "end": "2254400"
  },
  {
    "text": "agent and try to prepare or like inference the action of the victim for they",
    "start": "2254400",
    "end": "2262000"
  },
  {
    "text": "to make some wrong choices and thus lose again so we are not affects the",
    "start": "2262000",
    "end": "2267760"
  },
  {
    "text": "learning process of the routine while affecting the actions of the victim agent",
    "start": "2267760",
    "end": "2273280"
  },
  {
    "text": "during the game so another question is about are you trying to train boss",
    "start": "2273280",
    "end": "2280480"
  },
  {
    "text": "model adversary so that's also a good question so uh as i mentioned earlier uh during the",
    "start": "2280480",
    "end": "2287520"
  },
  {
    "text": "attacking process we are only we only put one uh one model that's the adversarial model so you have",
    "start": "2287520",
    "end": "2293760"
  },
  {
    "text": "two parties the written party and the zero part the written part is frequent so during the tight per attack",
    "start": "2293760",
    "end": "2301280"
  },
  {
    "text": "process we only train our serial model to be try to beat at the victim model but we",
    "start": "2301280",
    "end": "2308079"
  },
  {
    "text": "also try to defense this attack so during the defense attack uh difference process we do the things",
    "start": "2308079",
    "end": "2314480"
  },
  {
    "text": "in return is that we fix that the adversarial model and try to train the victim model see if the",
    "start": "2314480",
    "end": "2321119"
  },
  {
    "text": "looking model can learn to beat the angular 7 model again so just like at one time we only found",
    "start": "2321119",
    "end": "2327920"
  },
  {
    "text": "one mode",
    "start": "2327920",
    "end": "2330400"
  },
  {
    "text": "so another uh good question is about how to transfer the attack to interface",
    "start": "2333680",
    "end": "2340400"
  },
  {
    "text": "uh of such api in the real world uh that's also a good good question so",
    "start": "2340400",
    "end": "2345760"
  },
  {
    "text": "currently we are doing starcraft 2 which means like if there is an api",
    "start": "2345760",
    "end": "2351359"
  },
  {
    "text": "a row api about starcraft 2 like online game so we could do that we can directly",
    "start": "2351359",
    "end": "2356400"
  },
  {
    "text": "apply our tag to default the master agent in the start of the pin because start pass 2 is real",
    "start": "2356400",
    "end": "2362000"
  },
  {
    "text": "game and there are also some other online apis for other games such",
    "start": "2362000",
    "end": "2368240"
  },
  {
    "text": "as go gaming poker game type like different type of poker games so",
    "start": "2368240",
    "end": "2373280"
  },
  {
    "text": "uh the ongoing world cup focus on how to external current attack on the starcraft two",
    "start": "2373280",
    "end": "2379599"
  },
  {
    "text": "games for those type of games so if we could like do build a bridge between starcraft 2 and",
    "start": "2379599",
    "end": "2385520"
  },
  {
    "text": "go and program some other games as all could be potentially",
    "start": "2385520",
    "end": "2390720"
  },
  {
    "text": "generalized to those type of apis",
    "start": "2390720",
    "end": "2397359"
  },
  {
    "text": "uh actually that's all the questions i receive in the chat box yes everyone for your time",
    "start": "2397359",
    "end": "2404800"
  },
  {
    "text": "and the if you have any other questions that want to like talk more about this works you can",
    "start": "2404800",
    "end": "2410240"
  },
  {
    "text": "contact me with any ways you prefer thanks thanks sarah for your time",
    "start": "2410240",
    "end": "2418160"
  }
]