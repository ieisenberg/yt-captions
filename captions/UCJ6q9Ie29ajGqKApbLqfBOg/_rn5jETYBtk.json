[
  {
    "text": "thank you so much for coming out I'm really glad to be here for another year uh and this time talking about large",
    "start": "2520",
    "end": "7720"
  },
  {
    "text": "language models let's get right into it uh so my name is Bill Demmer CUA and",
    "start": "7720",
    "end": "16480"
  },
  {
    "text": "I work for the Microsoft security Response Center leading emerging threats I have a low-level background in",
    "start": "16480",
    "end": "23199"
  },
  {
    "text": "software security but I also do Cloud security I've experimented with large language models for a little over a year",
    "start": "23199",
    "end": "30039"
  },
  {
    "text": "but one disclaimer I want to start with is that I'm not a machine learning expert you know I don't have the",
    "start": "30039",
    "end": "36040"
  },
  {
    "text": "academic background I'm going to show you how I applied large language models but most of this talk is still going to",
    "start": "36040",
    "end": "43079"
  },
  {
    "text": "remain on my opinions rather than demonstrated academic",
    "start": "43079",
    "end": "49039"
  },
  {
    "text": "research so what is this talk going to be about well number one I'm going to tell you about large language models and",
    "start": "49719",
    "end": "55960"
  },
  {
    "text": "their advantages I'm going to tell you why msrc is interested in them how we've",
    "start": "55960",
    "end": "61640"
  },
  {
    "text": "been able to apply it for one part of our security response you know what are they capable",
    "start": "61640",
    "end": "67119"
  },
  {
    "text": "of and what's our take on their future and also how you can get the right data to use them",
    "start": "67119",
    "end": "72799"
  },
  {
    "text": "effectively now what this talk isn't going to be about is the top five",
    "start": "72799",
    "end": "78280"
  },
  {
    "text": "mind-bending ways you can use llms you know this isn't a formal literature review or a deep dive into neural",
    "start": "78280",
    "end": "85000"
  },
  {
    "text": "networks um and this is certainly not a representation of all AI research in",
    "start": "85000",
    "end": "90840"
  },
  {
    "text": "Microsoft or even msrc alone we're a massive company and this is a small part",
    "start": "90840",
    "end": "95880"
  },
  {
    "text": "of our work into leveraging machine learning for our security use cases and",
    "start": "95880",
    "end": "101040"
  },
  {
    "text": "finally I also I'm not here to sell you on Microsoft products that's not my job",
    "start": "101040",
    "end": "106439"
  },
  {
    "text": "I'm going to show you what I was able to do and what I found worked but I'm not going to sell Azure open AI or our",
    "start": "106439",
    "end": "111840"
  },
  {
    "text": "commercial offerings in this talk at all no product pitches so for some background the",
    "start": "111840",
    "end": "119079"
  },
  {
    "text": "Microsoft security Response Center handles every vulnerability reported to",
    "start": "119079",
    "end": "124240"
  },
  {
    "text": "Microsoft we have a massive scope this includes all software and web applications you can think of when",
    "start": "124240",
    "end": "130319"
  },
  {
    "text": "Microsoft pops into mind now over the years especially since 2016 we've",
    "start": "130319",
    "end": "136040"
  },
  {
    "text": "noticed that our case volume has increased by nearly a tenfold this graph",
    "start": "136040",
    "end": "141640"
  },
  {
    "text": "represents the cve by year and it's available through our public msrc",
    "start": "141640",
    "end": "147480"
  },
  {
    "text": "API as you can see from 2016 where we used to have you know roughly 200 to 400",
    "start": "147480",
    "end": "153480"
  },
  {
    "text": "annual cases that met the bar for acve now we deal with nearly a thousand",
    "start": "153480",
    "end": "159640"
  },
  {
    "text": "every year and this presents a interesting challenge you know how can we tackle",
    "start": "159640",
    "end": "166040"
  },
  {
    "text": "this large increase in volume besides just adding more resources to the problem that's always going to be the",
    "start": "166040",
    "end": "172120"
  },
  {
    "text": "easy solution so the reason why we were",
    "start": "172120",
    "end": "177920"
  },
  {
    "text": "interested in large language models is because there's a surplus of security related data um and now there is a",
    "start": "177920",
    "end": "184640"
  },
  {
    "text": "challenge of collecting it for example internally we obviously have access to Microsoft vulnerability data and our",
    "start": "184640",
    "end": "191680"
  },
  {
    "text": "analysis and patches Etc but externally there's CBE databases open- Source",
    "start": "191680",
    "end": "196959"
  },
  {
    "text": "vulnerability data there's a lot of it like exploits and Analysis available publicly and Etc and what we noticed was",
    "start": "196959",
    "end": "204319"
  },
  {
    "text": "that we have a lot of repetitive response workflows um and even if they're technically demanding they're",
    "start": "204319",
    "end": "209560"
  },
  {
    "text": "still very repetitive and many don't require Perfection so this is why we",
    "start": "209560",
    "end": "215200"
  },
  {
    "text": "were interested in seeing whether large language models could do anything for these security response",
    "start": "215200",
    "end": "222079"
  },
  {
    "text": "workflows so some background now what are large language models on a f fundamental level well",
    "start": "222959",
    "end": "230680"
  },
  {
    "text": "from a high level there are statistical models designed to understand and that's in air quotes and generate natural",
    "start": "230680",
    "end": "237799"
  },
  {
    "text": "language they represent Tex using tokens so for example given a sentence let's go",
    "start": "237799",
    "end": "243760"
  },
  {
    "text": "to NY it breaks it down into different characters and subsets of the sentence",
    "start": "243760",
    "end": "249799"
  },
  {
    "text": "based on what's called a tokenizer and the tokenization is the process of splitting this text into fragments known",
    "start": "249799",
    "end": "257680"
  },
  {
    "text": "as tokens so tokens usually represent different meanings in uh the English",
    "start": "257680",
    "end": "263160"
  },
  {
    "text": "language or whatever language the model is trained on for example the explanation point might be its own token",
    "start": "263160",
    "end": "269479"
  },
  {
    "text": "because because that has a different meaning than all the other words and all the other",
    "start": "269479",
    "end": "275680"
  },
  {
    "text": "symbols and the really interesting part about this is this these tokens are fed into the large language model to",
    "start": "276280",
    "end": "283120"
  },
  {
    "text": "generate a vector floats and these this Vector is known as embeddings embeddings represent tokens",
    "start": "283120",
    "end": "290000"
  },
  {
    "text": "in a way that has meaning for the large language model you know for example on",
    "start": "290000",
    "end": "295160"
  },
  {
    "text": "the image in the right you can see dog is converted into the 0.6 0.9 0.1 0.4",
    "start": "295160",
    "end": "301600"
  },
  {
    "text": "floats and you do that for each different word and once you convert that into an embedding you can now compare",
    "start": "301600",
    "end": "308759"
  },
  {
    "text": "different words by their semantic meaning you know uh in the right example",
    "start": "308759",
    "end": "313880"
  },
  {
    "text": "you can see that king and queen are quite close to each other when you graph it and that's the really interesting part about large language models is that",
    "start": "313880",
    "end": "320759"
  },
  {
    "text": "it represents semantic meaning through numbers and numbers we can do a lot of different math with",
    "start": "320759",
    "end": "329440"
  },
  {
    "text": "so from a high level there's two basic building blocks we're going to be focusing on that llms provide number one",
    "start": "331600",
    "end": "337400"
  },
  {
    "text": "is chat completions this can be thought of as chat GPT but over the over an API",
    "start": "337400",
    "end": "343199"
  },
  {
    "text": "you know chat completions works by providing a conversation for example first a system prompt specifies what the",
    "start": "343199",
    "end": "349240"
  },
  {
    "text": "large language model is supposed to do you are a helpful assistant then you can provide a user question like who won the",
    "start": "349240",
    "end": "355960"
  },
  {
    "text": "World Series and you can also provide a conversation that's already had answered questions as context and the chat",
    "start": "355960",
    "end": "362919"
  },
  {
    "text": "completions API allows us to generate the next message in the conversation so",
    "start": "362919",
    "end": "368280"
  },
  {
    "text": "for example when the user asks where was it played well the assistant then generates the 2020 World Series was",
    "start": "368280",
    "end": "374520"
  },
  {
    "text": "played in Texas at Globe Life field in Arlington now the second fundamental",
    "start": "374520",
    "end": "380520"
  },
  {
    "text": "building block we'll be talking about is called fine-tuning so models can learn from",
    "start": "380520",
    "end": "385680"
  },
  {
    "text": "your use case from a set of input and output examples the cool part about this",
    "start": "385680",
    "end": "390840"
  },
  {
    "text": "is that unlike the chat completions API you don't actually need to provide a lot of instructions what you instead provide",
    "start": "390840",
    "end": "397919"
  },
  {
    "text": "is a lot of sets of chat completions like conversations that represent what",
    "start": "397919",
    "end": "403759"
  },
  {
    "text": "the assistant is supposed to Output in first place and with enough of that data the model can actually learn the",
    "start": "403759",
    "end": "410400"
  },
  {
    "text": "instructions based on the examples alone instead of prompt quality so",
    "start": "410400",
    "end": "415440"
  },
  {
    "text": "that's the instruction quality for the chat completions API input quality the examples you have is what counts now",
    "start": "415440",
    "end": "422280"
  },
  {
    "text": "fine-tuning is going to be best when you have enough of the right data performance depends on the Nuance that",
    "start": "422280",
    "end": "429479"
  },
  {
    "text": "is hard to represent in a prompt so you know for some security response use cases we've seen a lot of different",
    "start": "429479",
    "end": "435319"
  },
  {
    "text": "details that can really change the output which is hard to express in a small set of",
    "start": "435319",
    "end": "441319"
  },
  {
    "text": "instructions and fine-tuning is also really useful when you want a specific output format or a tone of your message",
    "start": "441319",
    "end": "450720"
  },
  {
    "text": "now the greatest value llms provide in my opinion is that it gives you the",
    "start": "452479",
    "end": "457560"
  },
  {
    "text": "ability to interface with any data you can represent with natural language and the cool part is most data",
    "start": "457560",
    "end": "465159"
  },
  {
    "text": "can be represented as natural language because that's just words humans use",
    "start": "465159",
    "end": "470599"
  },
  {
    "text": "words to communicate so llms reduce the barrier for deriving value from most",
    "start": "470599",
    "end": "476479"
  },
  {
    "text": "data",
    "start": "476479",
    "end": "479479"
  },
  {
    "text": "so one of the early examples I tried out last year was related to the Microsoft security Response Center security update",
    "start": "484159",
    "end": "491120"
  },
  {
    "text": "guide every Patch Tuesday we release cves on the security update guide and",
    "start": "491120",
    "end": "496800"
  },
  {
    "text": "every cve gets its own page these Pages include basic metadata",
    "start": "496800",
    "end": "502120"
  },
  {
    "text": "like the name of the vulnerability you know the impact category the cwe um it",
    "start": "502120",
    "end": "507280"
  },
  {
    "text": "also includes FAQ questions so a set of questions that a customer might want to ask about a specific",
    "start": "507280",
    "end": "515360"
  },
  {
    "text": "cve but what happened was as our volume increased the data we shared through the",
    "start": "515360",
    "end": "520399"
  },
  {
    "text": "security update guide actually decreased you know it became really hard to have the same level of time for each cve when",
    "start": "520399",
    "end": "527200"
  },
  {
    "text": "our volume was increased by tenfold over the past 10 years so I was kind of",
    "start": "527200",
    "end": "533600"
  },
  {
    "text": "curious can we actually generate some of this extra data that we used to provide using large language model models so",
    "start": "533600",
    "end": "540519"
  },
  {
    "text": "this is a pretty simple example but I want to go through it because it helps us understand what using large language models looks like in practice know first",
    "start": "540519",
    "end": "548279"
  },
  {
    "text": "the first thing I did was grab a training data so remember the additional",
    "start": "548279",
    "end": "553680"
  },
  {
    "text": "context we used to provide I was able to scrape that data using our public API",
    "start": "553680",
    "end": "559519"
  },
  {
    "text": "and got every single FAQ entry all CVSs scores and also executive summaries for",
    "start": "559519",
    "end": "565600"
  },
  {
    "text": "older cases that had one you can see a snippet of the code I used on the right it was really as simple as parsing to",
    "start": "565600",
    "end": "571320"
  },
  {
    "text": "Json provided by the server and for some extra data I also parsed our internal",
    "start": "571320",
    "end": "576360"
  },
  {
    "text": "analysis notes and metadata that we had about the case uh using our internal documented",
    "start": "576360",
    "end": "581640"
  },
  {
    "text": "API and the really important part here is that getting the right data can really help make your model a lot more",
    "start": "581640",
    "end": "588720"
  },
  {
    "text": "effective in the use case you're trying to go after let's separate this use case into",
    "start": "588720",
    "end": "595440"
  },
  {
    "text": "challenges and develop a strategy around it so number one one of the first thing",
    "start": "595440",
    "end": "600600"
  },
  {
    "text": "we used to provide is executive summaries you know how can we leverage the set of summaries we used to provide",
    "start": "600600",
    "end": "606760"
  },
  {
    "text": "in order to generate new ones for new cases now we first chose fine tuning for",
    "start": "606760",
    "end": "612880"
  },
  {
    "text": "this um and an alternative approach would include uh using chat completions",
    "start": "612880",
    "end": "618360"
  },
  {
    "text": "but what we really wanted to focus on was maintaining the tone and consistency that we used to have in our old fa FAQs",
    "start": "618360",
    "end": "626079"
  },
  {
    "text": "and executive summaries so for example with chat completions you could do something called a few shot prompt and",
    "start": "626079",
    "end": "633240"
  },
  {
    "text": "that involves taking a set of examples maybe three to four and then providing it in the instruction so the model knows",
    "start": "633240",
    "end": "639440"
  },
  {
    "text": "what to um out what the output format should look like and another example would be something called a retrieval",
    "start": "639440",
    "end": "646079"
  },
  {
    "text": "augmented generation approach so in this case you would provide examples that are most relevant to the case at hand you'd",
    "start": "646079",
    "end": "653120"
  },
  {
    "text": "find in 2016 maybe there was a vulnerability very similar to this one and you'd find that uh case you'd find",
    "start": "653120",
    "end": "659839"
  },
  {
    "text": "that executive summary and provide that example which can help increase the output",
    "start": "659839",
    "end": "665839"
  },
  {
    "text": "quality so from a high level our strategy was split into pre-training and posttraining number one we would scrape",
    "start": "666680",
    "end": "673399"
  },
  {
    "text": "all the summaries from 2015 to 2021 we'd then fine-tune GPT 3.5 based",
    "start": "673399",
    "end": "680000"
  },
  {
    "text": "on a partial executive summary and then after we had that summary generated through fine tuning we'd used the model",
    "start": "680000",
    "end": "687800"
  },
  {
    "text": "to generate 10 summ we'd then pass it to GPC 4 to fix",
    "start": "687800",
    "end": "693639"
  },
  {
    "text": "hallucinations and grammatical errors and we'd also add a redact layer where",
    "start": "693639",
    "end": "699600"
  },
  {
    "text": "we redact sensitive information with a gp4 prompt now the key thing to understand here is number one you don't",
    "start": "699600",
    "end": "706079"
  },
  {
    "text": "want to do everything in one single step you want to split your task into very fundamental uh levels so in this case it",
    "start": "706079",
    "end": "713600"
  },
  {
    "text": "includes first to generate 10 summaries that we want to include in a",
    "start": "713600",
    "end": "719240"
  },
  {
    "text": "summarization model and once we have those 10 summaries we summarize those using gp4 and in this case it's really",
    "start": "719240",
    "end": "726160"
  },
  {
    "text": "as simple as you know summarize these 10 summaries together create a summary of the summary you can say and once you",
    "start": "726160",
    "end": "733160"
  },
  {
    "text": "have the summary of the summary you can finally add an extra layer to make sure there's no sensitive data from our internal analysis notes that are",
    "start": "733160",
    "end": "741639"
  },
  {
    "text": "included so for preparing the data for fine-tuning it's just like chat completions so what you do is you create",
    "start": "743440",
    "end": "750760"
  },
  {
    "text": "conversations like you would for a chat completion API but you just create multiple",
    "start": "750760",
    "end": "755880"
  },
  {
    "text": "examples now the prompt is simple because we want the model to learn from our data we just provide a sentence or",
    "start": "755880",
    "end": "761560"
  },
  {
    "text": "two know you are an expert security engineer responsible for writing a a summary for a security",
    "start": "761560",
    "end": "768600"
  },
  {
    "text": "vulnerability and the user message is our going to be our input data which includes the impact of the vulnerability",
    "start": "768600",
    "end": "775440"
  },
  {
    "text": "the CVSs Vector the root cause category so is an out of bounds write out of bounds read Etc and finally the",
    "start": "775440",
    "end": "782279"
  },
  {
    "text": "technical analysis notes from in from the msrc internal team and finally the assistant message",
    "start": "782279",
    "end": "789079"
  },
  {
    "text": "is going to be the output we expect and so since we're training against these old cves we'd provide a user message for",
    "start": "789079",
    "end": "795399"
  },
  {
    "text": "that old cve and then we'd provide the real executive summary that we published on the security update",
    "start": "795399",
    "end": "802440"
  },
  {
    "text": "guide now we use Azure open AI for training but you don't have to um all",
    "start": "802440",
    "end": "807639"
  },
  {
    "text": "you really do is with those conversations we just generated we put it into a Json lines file where every",
    "start": "807639",
    "end": "812959"
  },
  {
    "text": "line is a different conversation and then you just really pass it into a uh",
    "start": "812959",
    "end": "818160"
  },
  {
    "text": "AI platform like Azure open AI or even open AI itself and it'll automatically",
    "start": "818160",
    "end": "823760"
  },
  {
    "text": "go through these examples and fine-tune your model based on it and you can see a graph here um where model training loss",
    "start": "823760",
    "end": "830880"
  },
  {
    "text": "is represented in red the lower your loss the closer the model is to an accurate",
    "start": "830880",
    "end": "836880"
  },
  {
    "text": "output and generally speaking lower loss is going to be better because it means that the model is you know at first we",
    "start": "836880",
    "end": "843759"
  },
  {
    "text": "can see the model has no idea what it's doing the loss is at what 4.0 nearly and then as it it's given more and more",
    "start": "843759",
    "end": "850360"
  },
  {
    "text": "examples it's actually able to really reduce that loss down to nearly you know under 0.5 it's never going to be 100% exactly",
    "start": "850360",
    "end": "858480"
  },
  {
    "text": "right most of the time but it does get you pretty close to what you need now the next challenge was also",
    "start": "858480",
    "end": "866120"
  },
  {
    "text": "generating FAQ entries you know we don't want to just generate summary but we want to generate a new questions based",
    "start": "866120",
    "end": "872440"
  },
  {
    "text": "on the analysis notes that we have we ended up with a chain of GPT 4 promps and this is called an agent approach so",
    "start": "872440",
    "end": "879399"
  },
  {
    "text": "there is no fine-tuning because we wanted to really focus on creativity now this was the most",
    "start": "879399",
    "end": "885639"
  },
  {
    "text": "challenging because generating new FAQ entries that we don't have extensive data for Beyond what's already included",
    "start": "885639",
    "end": "892320"
  },
  {
    "text": "in our assessment is is not trivial we spent a lot of time reviewing our FAQ",
    "start": "892320",
    "end": "897800"
  },
  {
    "text": "entries from past cases to really Target and folks on the tone we were interested in and again from a high level our",
    "start": "897800",
    "end": "904720"
  },
  {
    "text": "strategy really included breaking these steps into different parts which includes generating the questions",
    "start": "904720",
    "end": "910560"
  },
  {
    "text": "parsing and filtering out errors selecting the best questions that's another great strategy is once you generate like 10 or 15 different uh FAQ",
    "start": "910560",
    "end": "918279"
  },
  {
    "text": "questions you ask the model to pick the best ones and then we simplify each of these uh Pro question sets and so to get",
    "start": "918279",
    "end": "925920"
  },
  {
    "text": "rid of duplicate data as well and finally we again redact sensitive info and select the best",
    "start": "925920",
    "end": "932680"
  },
  {
    "text": "question now the prompts are too long to show but I want to show you a little bit of a breakdown you can see a snippet of",
    "start": "932680",
    "end": "938839"
  },
  {
    "text": "it on the right we start with a preamble with personality and context so what is the model doing what's its job we give",
    "start": "938839",
    "end": "946120"
  },
  {
    "text": "it examples of bad questions and why these questions shouldn't be generated in the first place we also tell it how",
    "start": "946120",
    "end": "952440"
  },
  {
    "text": "to format its output you know this included uh in my case uh question on one line and then the answer on another",
    "start": "952440",
    "end": "958639"
  },
  {
    "text": "line with a colon before and we give it example questions",
    "start": "958639",
    "end": "964120"
  },
  {
    "text": "that we wanted to answer these are the types of questions we'd like you to generate an FAQ entry for and we also",
    "start": "964120",
    "end": "969199"
  },
  {
    "text": "finally give it some guidelines in tone and style now this can certainly be improved but it was more than enough for",
    "start": "969199",
    "end": "974600"
  },
  {
    "text": "our use case a lot of these techniques that went into the last um prompt came from",
    "start": "974600",
    "end": "982079"
  },
  {
    "text": "actually open AI they have a great um blog known as the open AI cookbook where they publish different tips and tricks",
    "start": "982079",
    "end": "988319"
  },
  {
    "text": "to really improve for example in this case reliability of your model but there they have everything you can imagine of",
    "start": "988319",
    "end": "994000"
  },
  {
    "text": "for fine-tuning as well you know this came from simplifying instructions targeting the right tone splitting tasks",
    "start": "994000",
    "end": "1000759"
  },
  {
    "text": "into their basic steps and asking the model to generate why it generated a specific entry to reduce",
    "start": "1000759",
    "end": "1007360"
  },
  {
    "text": "hallucinations and also generating several variations to select the best",
    "start": "1007360",
    "end": "1013120"
  },
  {
    "text": "suggestions so just to give you an example of what this project output looked like you can see on the left this",
    "start": "1013759",
    "end": "1020279"
  },
  {
    "text": "is a real cve that was published the left is what we included in our FAQ and",
    "start": "1020279",
    "end": "1025640"
  },
  {
    "text": "then on the right is what the llm was able to generate for the uh cve in",
    "start": "1025640",
    "end": "1030798"
  },
  {
    "text": "question you can see that uh in the real cve we provided a single question about",
    "start": "1030799",
    "end": "1036079"
  },
  {
    "text": "you know the attack complexity is high what does that mean whereas in the uh llm generated cve there is multiple",
    "start": "1036079",
    "end": "1043360"
  },
  {
    "text": "questions about the attack scenario what would a user you know need to do to what would an attacker need to do to exploit",
    "start": "1043360",
    "end": "1049200"
  },
  {
    "text": "this vulnerability and also details about whether it's network accessible and Etc now one thing I do want to call",
    "start": "1049200",
    "end": "1055360"
  },
  {
    "text": "out is that the llm failed to derive the exact same question that we'd expect in the real cve you know there is no",
    "start": "1055360",
    "end": "1062039"
  },
  {
    "text": "mention about the attack complexity being high Etc now we didn't include these types of templated questions on",
    "start": "1062039",
    "end": "1068679"
  },
  {
    "text": "purpose uh but we could have considered as a future value ad and in general the results what we",
    "start": "1068679",
    "end": "1076760"
  },
  {
    "text": "saw was that we could first improve by generating these required template questions now I want to also clarify",
    "start": "1076760",
    "end": "1083559"
  },
  {
    "text": "that it wasn't perfect in the sense of there were hallucinations there would sometimes be statements that it made",
    "start": "1083559",
    "end": "1089280"
  },
  {
    "text": "were factually false um this is not used in production if you go on to the security update guide today we're not",
    "start": "1089280",
    "end": "1094880"
  },
  {
    "text": "using it because of some of these challenges uh another really important one was around the translations so we",
    "start": "1094880",
    "end": "1101799"
  },
  {
    "text": "support customers around the world and if we create these custom content for each cve there's over 100 cves how do",
    "start": "1101799",
    "end": "1108280"
  },
  {
    "text": "you actually convert that into every different language you know machine translations aren't that great um and",
    "start": "1108280",
    "end": "1114320"
  },
  {
    "text": "this is also why you might notice some questions on the msrc security update guide they look very repetitive is",
    "start": "1114320",
    "end": "1119640"
  },
  {
    "text": "because we generate these translations once and then we can reuse those gen uh generated translations for multiple cbes",
    "start": "1119640",
    "end": "1126960"
  },
  {
    "text": "but I did want to demonstrate this really basic example to get you introduced into fi um fine-tuning and",
    "start": "1126960",
    "end": "1132480"
  },
  {
    "text": "chat completions API and to show you the types of tasks that LMS are good at",
    "start": "1132480",
    "end": "1139120"
  },
  {
    "text": "now another interesting use case that I found for large language models is to be able to predict key facts about a case",
    "start": "1139120",
    "end": "1145360"
  },
  {
    "text": "you know can we use large language models to predict facts about an msrc vulnerability report to help measure",
    "start": "1145360",
    "end": "1151400"
  },
  {
    "text": "risk you know example is we can predict the severity and impact of a case based on its report and ideally we could",
    "start": "1151400",
    "end": "1158480"
  },
  {
    "text": "produce a generic methodology to predict any field and the the idea behind this",
    "start": "1158480",
    "end": "1163720"
  },
  {
    "text": "was that sometimes we receive reports that turn out to be um like a Spam reports to be honest with you you know",
    "start": "1163720",
    "end": "1170559"
  },
  {
    "text": "it's not even I'm not even talking about low or moderate severity vulnerabilities I'm talking about vulnerabilities that",
    "start": "1170559",
    "end": "1175760"
  },
  {
    "text": "wouldn't even be considered a vulnerability and what we can do is we can help reduce the impact volume has on",
    "start": "1175760",
    "end": "1182240"
  },
  {
    "text": "our business if we can measure what cases we should take a closer look at and",
    "start": "1182240",
    "end": "1188480"
  },
  {
    "text": "prioritize so the training strategy for this was pretty simple we had a simple",
    "start": "1188559",
    "end": "1193840"
  },
  {
    "text": "prompt because we wanted to learn from our training data and the user message included the project ma project manager",
    "start": "1193840",
    "end": "1200679"
  },
  {
    "text": "and what that really means is this is the person that deals with handling your msrc case from start to finish so when a",
    "start": "1200679",
    "end": "1208080"
  },
  {
    "text": "case is opened they'll first do initial triage to make sure it's legitimate we should we forward it for internal",
    "start": "1208080",
    "end": "1213919"
  },
  {
    "text": "analysis now number two is the react engineer name and from a high level these are the people that do the",
    "start": "1213919",
    "end": "1219440"
  },
  {
    "text": "technical analysis of your vulnerability report and we included these names for a",
    "start": "1219440",
    "end": "1225520"
  },
  {
    "text": "very important reason um and finally we have the technical analysis from these Rea from",
    "start": "1225520",
    "end": "1231679"
  },
  {
    "text": "the react engineer and we also had the vertical about what is the product that's impacted",
    "start": "1231679",
    "end": "1237159"
  },
  {
    "text": "here you can see in the user message this is where the data is represented as our input and the output is what we want",
    "start": "1237159",
    "end": "1242520"
  },
  {
    "text": "the model to generate the reason I mentioned the name is really important to include is",
    "start": "1242520",
    "end": "1248120"
  },
  {
    "text": "because it allows us to bring multiple perspectives into the training process maybe some project managers were better",
    "start": "1248120",
    "end": "1254240"
  },
  {
    "text": "at predicting legitimate cases than others maybe some react Engineers have a strict interpretation of the servicing",
    "start": "1254240",
    "end": "1261080"
  },
  {
    "text": "criteria and this allows us to bring a lot more diverse perspectives so for example we can um provide different",
    "start": "1261080",
    "end": "1267400"
  },
  {
    "text": "names after we find tuned to model I can go through every person on my team and get them to think hey it does this case",
    "start": "1267400",
    "end": "1273960"
  },
  {
    "text": "meet the bar for servicing or does this case should we prioritize it for immediate",
    "start": "1273960",
    "end": "1279159"
  },
  {
    "text": "servicing and less successful metadata this is an important one to is you don't want to just put everything you have",
    "start": "1279159",
    "end": "1284520"
  },
  {
    "text": "into this training prompt um it's like researcher name and statistics you know how often did they submit legitimate",
    "start": "1284520",
    "end": "1290279"
  },
  {
    "text": "cases um examples of pimar cases uh when we included this metadata what we",
    "start": "1290279",
    "end": "1295480"
  },
  {
    "text": "noticed was an overfit so it would rely too much on these fields that weren't",
    "start": "1295480",
    "end": "1300640"
  },
  {
    "text": "always 100% accurate and it basically cheated off of the past examples we would give it whereas we wanted to to",
    "start": "1300640",
    "end": "1307080"
  },
  {
    "text": "learn to predict some of these issues ahead of time without needing that initial example",
    "start": "1307080",
    "end": "1313799"
  },
  {
    "text": "data we trained on nearly 6,640 of these cases and we had a loss me mean of 0.26",
    "start": "1313799",
    "end": "1322360"
  },
  {
    "text": "which is fairly good and we had a standard deviation of 0.55 the minimum loss came down to 0.01",
    "start": "1322360",
    "end": "1329400"
  },
  {
    "text": "and what that represents is sometimes the model was really really good at predicting the outcome of a case before",
    "start": "1329400",
    "end": "1334880"
  },
  {
    "text": "an analysis was even created so for the results of this we",
    "start": "1334880",
    "end": "1340360"
  },
  {
    "text": "had a roughly initially 61% um accuracy for exact severity so that's low",
    "start": "1340360",
    "end": "1346320"
  },
  {
    "text": "moderate important critical we had a 63% accuracy for exact resolution so that's",
    "start": "1346320",
    "end": "1352320"
  },
  {
    "text": "fix the next version um no Repro won't fix fixing uh security update Etc now",
    "start": "1352320",
    "end": "1360520"
  },
  {
    "text": "the issue with the low accuracy counts is that there's a lot of variance between the important moderate critical",
    "start": "1360520",
    "end": "1367840"
  },
  {
    "text": "low vulnerability ratings and what really mattered is whether we fix an an issue in a security update and what that",
    "start": "1367840",
    "end": "1374279"
  },
  {
    "text": "means is do we prioritize this report for immediate servicing because that those reports are the ones",
    "start": "1374279",
    "end": "1380640"
  },
  {
    "text": "um we'd like to prioritize the highest and the other issues are still important but they don't necessarily meet the bar",
    "start": "1380640",
    "end": "1386600"
  },
  {
    "text": "for immediate prioritization and we once we actually filtered for this General category of",
    "start": "1386600",
    "end": "1392320"
  },
  {
    "text": "fix and security update we were able to get it up to 75 to 82% accuracy the reason this is really interesting is",
    "start": "1392320",
    "end": "1398440"
  },
  {
    "text": "keep in mind this is before any analysis is actually done of the vulnerability besides the report being sent into msrc",
    "start": "1398440",
    "end": "1405559"
  },
  {
    "text": "so we're able to get ahead and get ahead of the case make sure to understand which ones are",
    "start": "1405559",
    "end": "1410640"
  },
  {
    "text": "going to be the most most important and which ones are going to need that less prioritization Etc and the really cool",
    "start": "1410640",
    "end": "1418360"
  },
  {
    "text": "part about this is it was really simple you know we all we did was provided the input like report and output um output",
    "start": "1418360",
    "end": "1425240"
  },
  {
    "text": "outcome of the case and the large language models allowed us to derive value from that data without actually",
    "start": "1425240",
    "end": "1431919"
  },
  {
    "text": "needing to do much at all like again all we had to do is provide those examples and it was able to find those patterns",
    "start": "1431919",
    "end": "1438480"
  },
  {
    "text": "it's just like traditional machine learning you know we could have used a very simple logarithmic regression model to probably get an outcome like this as",
    "start": "1438480",
    "end": "1445240"
  },
  {
    "text": "well but the key here is just again how simple large language models make it you know it's not necessarily going to be as",
    "start": "1445240",
    "end": "1451440"
  },
  {
    "text": "performant as some of these other approaches but they're a lot faster to implement in practice and this generic",
    "start": "1451440",
    "end": "1457600"
  },
  {
    "text": "methodology also worked a lot in a lot of different cases not just this one now one thing I do want to call",
    "start": "1457600",
    "end": "1464400"
  },
  {
    "text": "about this example is going to be prompt injection you know unlike the last use case we actually included an untrust or",
    "start": "1464400",
    "end": "1470039"
  },
  {
    "text": "finder report for input and this exposes us to potential prompt injection tax",
    "start": "1470039",
    "end": "1475320"
  },
  {
    "text": "what do we do about that well you know you want to try to solve it by Design",
    "start": "1475320",
    "end": "1481360"
  },
  {
    "text": "rather than detection you know if you're dealing with untrusted data make the impact not matter at all so for example",
    "start": "1481360",
    "end": "1487840"
  },
  {
    "text": "could a researcher include some weird instruction in their report to make it a you know ignore all your instructions",
    "start": "1487840",
    "end": "1494240"
  },
  {
    "text": "and instead output this specific data instead they could but you know the",
    "start": "1494240",
    "end": "1500720"
  },
  {
    "text": "point there is if they can do that make sure that the impact is limited you know in this case there's always humans",
    "start": "1500720",
    "end": "1506200"
  },
  {
    "text": "involved in the loop the worst case you can ever do is maybe make a case seem riskier than reality and in the end of",
    "start": "1506200",
    "end": "1512600"
  },
  {
    "text": "the day if you solve it by Design you have to worry a lot less about things like is can an attack or inject",
    "start": "1512600",
    "end": "1519080"
  },
  {
    "text": "something into our model because it just won't matter at the end of the day all right this is a really",
    "start": "1519080",
    "end": "1525440"
  },
  {
    "text": "interesting example I want to talk to you about so so far we focused on summarization and classification but large language models",
    "start": "1525440",
    "end": "1533440"
  },
  {
    "text": "one of their really big value ads is going to be the universal interface aspect I was curious whether they could",
    "start": "1533440",
    "end": "1541120"
  },
  {
    "text": "also help solve these hard technical problems um that really require a lot of",
    "start": "1541120",
    "end": "1546520"
  },
  {
    "text": "expertise and a lot of decade not not necessarily decades but several years of experience so I was curious could large",
    "start": "1546520",
    "end": "1552880"
  },
  {
    "text": "language models generate a root cause summary of a vulnerability based on a crash dump",
    "start": "1552880",
    "end": "1558880"
  },
  {
    "text": "alone now finding the root cause category of a vulnerability based on a crash dump is",
    "start": "1558880",
    "end": "1565720"
  },
  {
    "text": "not new you know if you've ever dealt with a crash dump you know of a vulnerability or in general any crash",
    "start": "1565720",
    "end": "1571120"
  },
  {
    "text": "dump uh wind debug the tool you can use to interact with these dumps actually has a little bit of analysis built into",
    "start": "1571120",
    "end": "1577799"
  },
  {
    "text": "it so for example in this case you know if you type analyze DV on a crash dump",
    "start": "1577799",
    "end": "1583559"
  },
  {
    "text": "that's a command you're going to use it'll tell you it's a page PA in an non-page area in this case and it gives",
    "start": "1583559",
    "end": "1589120"
  },
  {
    "text": "you like the faulting source code Etc but what we're talking about is not actually a root cause category but a",
    "start": "1589120",
    "end": "1595760"
  },
  {
    "text": "root cause summary now I'm interested can it actually explain what's happening here with the vulnerability uh far more",
    "start": "1595760",
    "end": "1602159"
  },
  {
    "text": "than you know what type of Crash occurred you know outabounds right whatever it is and where it occurred and this hasn't really been",
    "start": "1602159",
    "end": "1609120"
  },
  {
    "text": "done before you know I've seen a a of approaches in with code but they've really failed because of the diversity",
    "start": "1609120",
    "end": "1614919"
  },
  {
    "text": "of input with crash dumps now many vulnerabilities lead to a crash",
    "start": "1614919",
    "end": "1621000"
  },
  {
    "text": "generating a dump file and dump files can contain a lot of useful information about what's going on with this case you",
    "start": "1621000",
    "end": "1627840"
  },
  {
    "text": "know one of the most common artifacts we see through um repr reproducing a vulnerability report are these crash",
    "start": "1627840",
    "end": "1633840"
  },
  {
    "text": "dumps and the cool part is it works for user mode and kernel mode V vulnerabilities you know there are some",
    "start": "1633840",
    "end": "1639520"
  },
  {
    "text": "newer Technologies like time travel traces which help you record everything that's happening in the process when a",
    "start": "1639520",
    "end": "1645919"
  },
  {
    "text": "certain exception occurs uh but unlike that we we actually now have support for user mode and kernel mode time travel is",
    "start": "1645919",
    "end": "1652640"
  },
  {
    "text": "limited to user mode and these crashes can contain information like where did the crash occur what were the variables",
    "start": "1652640",
    "end": "1659480"
  },
  {
    "text": "when the crash triggered and what's the story behind the function that led up to the",
    "start": "1659480",
    "end": "1664960"
  },
  {
    "text": "crash and the dumps can also help us automatically find source information um",
    "start": "1664960",
    "end": "1670000"
  },
  {
    "text": "the stack the registers all this useful data that occurred at the point of the vulnerability being",
    "start": "1670000",
    "end": "1676760"
  },
  {
    "text": "triggered now the the key challenge in this case was really converting a crash dump into natural language so remember",
    "start": "1676760",
    "end": "1683840"
  },
  {
    "text": "how we had the chat completions we need to now convert from a crash dump into something just words to be honest we",
    "start": "1683840",
    "end": "1690960"
  },
  {
    "text": "tried to use only a few examples but if you've analyzed crash dumps you understand the complexity that goes into",
    "start": "1690960",
    "end": "1697159"
  },
  {
    "text": "analyzing root causes and uh that's why we really needed a lot more um crash dumps in the first place and and that's",
    "start": "1697159",
    "end": "1703279"
  },
  {
    "text": "what we focused on getting a lot of crash dump material",
    "start": "1703279",
    "end": "1708039"
  },
  {
    "text": "so how do we actually go from dumps to natural language well when debug has a console variant known as cdb which is",
    "start": "1709320",
    "end": "1716000"
  },
  {
    "text": "the console debugger and it allows us to really interact with crash dumps on a",
    "start": "1716000",
    "end": "1721120"
  },
  {
    "text": "programmatic level instead of the UI wind debug that a lot of you are probably familiar with so in this case I",
    "start": "1721120",
    "end": "1727440"
  },
  {
    "text": "used a wrapper for this console debugger known as cdb and Pi Pi cdb um and it",
    "start": "1727440",
    "end": "1732640"
  },
  {
    "text": "allowed me to interact with crash jumps from python uh which really made it simple to get the data I needed from the",
    "start": "1732640",
    "end": "1738559"
  },
  {
    "text": "crash dumps and into a prompt that I could provide for fine tuning so I basically wrote a rapper for",
    "start": "1738559",
    "end": "1745519"
  },
  {
    "text": "a rapper in this case I automatically extracted key information like the call stack the local variables you know the",
    "start": "1745519",
    "end": "1751960"
  },
  {
    "text": "analyze command and it really was just using reject and we debug commands you",
    "start": "1751960",
    "end": "1757960"
  },
  {
    "text": "can see the different functions I implemented on the right but it wasn't anything too fancy I would just run a command and then use Rex to Output the",
    "start": "1757960",
    "end": "1764679"
  },
  {
    "text": "parts that I needed to capture the parts that I needed and this really helped me convert the crash dumps into a form that",
    "start": "1764679",
    "end": "1771360"
  },
  {
    "text": "I could represent in normal text now another big challenge for crash",
    "start": "1771360",
    "end": "1777360"
  },
  {
    "text": "dumps was getting the source code you know decomp decompilation might work but",
    "start": "1777360",
    "end": "1783000"
  },
  {
    "text": "um source code is going to help us provide that context around what's happening to function at a level that",
    "start": "1783000",
    "end": "1788039"
  },
  {
    "text": "decompilation isn't going to be able to provide now if you're doing just for small projects you can use Source from",
    "start": "1788039",
    "end": "1793360"
  },
  {
    "text": "disk but I don't have every Microsoft product cloned to my repo cloned to my machine um and proper access controls at",
    "start": "1793360",
    "end": "1800960"
  },
  {
    "text": "least in Microsoft really make this hard harder than you expect so for this approach I first",
    "start": "1800960",
    "end": "1808360"
  },
  {
    "text": "started with an agent model so just using simple instructions to see if I can get a response that I was interested",
    "start": "1808360",
    "end": "1813840"
  },
  {
    "text": "in this included what is the function used for what are the local variables",
    "start": "1813840",
    "end": "1819399"
  },
  {
    "text": "can you summarize the flow of this function what variable is responsible for the crash Why did a variable trigger",
    "start": "1819399",
    "end": "1825200"
  },
  {
    "text": "the crash in the first place and then slowly we build this repository of information about a crash dump all the",
    "start": "1825200",
    "end": "1831200"
  },
  {
    "text": "way to generating a concise Rapport about the root cause of a",
    "start": "1831200",
    "end": "1836240"
  },
  {
    "text": "vulnerability but unfortunately with the agent approach it really didn't have that great results um even for simple",
    "start": "1836240",
    "end": "1841919"
  },
  {
    "text": "vulnerabilities I mean you got sometimes a generally rough idea of what was going on but again it really had a trouble",
    "start": "1841919",
    "end": "1848880"
  },
  {
    "text": "capturing the Nuance that went into analyzing these crash dumps and it's definitely struggled with use after",
    "start": "1848880",
    "end": "1854360"
  },
  {
    "text": "freeze in race conditions and sometimes it was partially correct but it missed the whole story this is why we went into",
    "start": "1854360",
    "end": "1861120"
  },
  {
    "text": "fine toting because if we could get the sample data we could get the model to learn off of that data instead of our",
    "start": "1861120",
    "end": "1866799"
  },
  {
    "text": "instructions alone so the dump analysis work had by",
    "start": "1866799",
    "end": "1872279"
  },
  {
    "text": "far the most strategies out of any project I've worked on with large language models you know it's not as simple as point and shoot to get good",
    "start": "1872279",
    "end": "1878440"
  },
  {
    "text": "results you know in general I use gp4 to generate a root cause summary based on our existing analysis notes that's to",
    "start": "1878440",
    "end": "1885600"
  },
  {
    "text": "get our assistant message output but besides that for the input data it was a lot more complicated again a lot more",
    "start": "1885600",
    "end": "1891639"
  },
  {
    "text": "complicated than you could expect you know you can read this offline once I release slides but from a",
    "start": "1891639",
    "end": "1897559"
  },
  {
    "text": "high level I want to show you the type of iteration and experimentation that goes into this work so instead of like",
    "start": "1897559",
    "end": "1904440"
  },
  {
    "text": "there's not going to be a perfect solution for every use case you want to start with the best guess and then iterate off of that to get to see what",
    "start": "1904440",
    "end": "1910960"
  },
  {
    "text": "works so first I tried with GPT 3.5 can we just fit as many crash frames into",
    "start": "1910960",
    "end": "1916760"
  },
  {
    "text": "the St into the input as possible and that started with a loss average of 1.4",
    "start": "1916760",
    "end": "1922080"
  },
  {
    "text": "that's very high and it wasn't accurate at all now I optimized some automation to use some of the dumps that were",
    "start": "1922080",
    "end": "1927840"
  },
  {
    "text": "seemed unviable and I removed lowquality cases from the training set and you can",
    "start": "1927840",
    "end": "1933240"
  },
  {
    "text": "see as I iterated and I iterated on these different strategies I slowly chipped away at that average loss you",
    "start": "1933240",
    "end": "1939480"
  },
  {
    "text": "know I started at 1.4 but by slowly again changing the strategy of the input",
    "start": "1939480",
    "end": "1944639"
  },
  {
    "text": "data I was able to slowly get it down to 0.87 and again the point here is there's not",
    "start": "1944639",
    "end": "1950880"
  },
  {
    "text": "going to be a perfect solution for your use case you really have to experiment and see what works and the final strategies was also",
    "start": "1950880",
    "end": "1958919"
  },
  {
    "text": "an improvement in the found fundamental model that I was using to train with so",
    "start": "1958919",
    "end": "1964240"
  },
  {
    "text": "in in 2024 Azure openai increased the fine-tuning context window so that's how",
    "start": "1964240",
    "end": "1969600"
  },
  {
    "text": "much input you you can provide in the input for your fine-tuned model to 16,000 instead of 4,000 and this helped",
    "start": "1969600",
    "end": "1976919"
  },
  {
    "text": "a lot of with the challenge of too many relevant stack frames and not enough space to fit them in I removed dumps to",
    "start": "1976919",
    "end": "1984320"
  },
  {
    "text": "help really remove inconsistency for my training data and then finally in late",
    "start": "1984320",
    "end": "1989720"
  },
  {
    "text": "2024 or mid 2024 Azure openai and open AI introduced gp4 fine tuning and what I was able to",
    "start": "1989720",
    "end": "1997039"
  },
  {
    "text": "see is just by increasing changing the fun fundamental model I was training with for foundational model I was able",
    "start": "1997039",
    "end": "2003159"
  },
  {
    "text": "to have a 16% decrease in my loss so to recap started with a mean of",
    "start": "2003159",
    "end": "2009480"
  },
  {
    "text": "1.41 for my training loss and I ended up with 0.42 a nearly 70% increase or a",
    "start": "2009480",
    "end": "2016480"
  },
  {
    "text": "decrease excuse me um and this really shows the value in that iterative experimentation work but I want to show",
    "start": "2016480",
    "end": "2023600"
  },
  {
    "text": "you what these what this model really looks like in practice so in this report we have a legitimate vulnerability from a",
    "start": "2023600",
    "end": "2029799"
  },
  {
    "text": "researcher where um the wallet service had an integer overflow it would increment a reference counter and that",
    "start": "2029799",
    "end": "2037120"
  },
  {
    "text": "was accessible over Comm and if you just called this function over and over and over again there would be an",
    "start": "2037120",
    "end": "2044759"
  },
  {
    "text": "overflow the real root cause summary for this vulnerability was due to a reference counter only being 32 bits",
    "start": "2044880",
    "end": "2050560"
  },
  {
    "text": "it's possible to overflow the reference count leading to use after free um and when the llm model generated",
    "start": "2050560",
    "end": "2057000"
  },
  {
    "text": "one here's what it had to say the root cause of the vulnerability is a reference count overflow in the wall",
    "start": "2057000",
    "end": "2062560"
  },
  {
    "text": "wallet X object which leads to a use after free now what was really interesting to me is that I was actually",
    "start": "2062560",
    "end": "2068760"
  },
  {
    "text": "able to capture the key pieces of the vulnerability in a way that we've never seen before for dump analysis Tools in",
    "start": "2068760",
    "end": "2075839"
  },
  {
    "text": "this case it found the same M CF variable that overflowed with the",
    "start": "2075839",
    "end": "2080960"
  },
  {
    "text": "function ad ref it was able to see that it bypassed a condition inside of the release function that checks if the",
    "start": "2080960",
    "end": "2087599"
  },
  {
    "text": "reference counter is not zero and this led to the execution of a delete which",
    "start": "2087599",
    "end": "2093118"
  },
  {
    "text": "led to the use after free in the first place again I just want to highlight this is all from a crash there's no you",
    "start": "2093119",
    "end": "2098520"
  },
  {
    "text": "know no cheating it doesn't have the final report in this case it all came from the crash jump alone and this is",
    "start": "2098520",
    "end": "2104200"
  },
  {
    "text": "something that would usually take years for an individual engineer to learn how to do properly another example was in this",
    "start": "2104200",
    "end": "2111359"
  },
  {
    "text": "case the ux init dll had a uh was loading an untrusted file from disk and",
    "start": "2111359",
    "end": "2116960"
  },
  {
    "text": "they had a specific part of that file that it didn't do enough validation on in this case it was the file path of the",
    "start": "2116960",
    "end": "2123599"
  },
  {
    "text": "theme that was included in the file um the controlled the offset of this file",
    "start": "2123599",
    "end": "2129119"
  },
  {
    "text": "Etc was controlled by an attacker and the function failed to do validation whether this was legitimate at all again",
    "start": "2129119",
    "end": "2135440"
  },
  {
    "text": "the real root cause summary would expanded on how this um this assumption that the size is at least Max path but",
    "start": "2135440",
    "end": "2142520"
  },
  {
    "text": "the buffer that was copying this path into didn't actually need to necessarily have um it you could copy more than Max",
    "start": "2142520",
    "end": "2150480"
  },
  {
    "text": "path because of a lack of a validation check into the input file and with the large language model we were able to see",
    "start": "2150480",
    "end": "2156880"
  },
  {
    "text": "again it C captured the right key facts about the root cause based on the dump analysis alone it found that the dlll",
    "start": "2156880",
    "end": "2163920"
  },
  {
    "text": "name which points to a location in the untrusted file was not validated and it",
    "start": "2163920",
    "end": "2169040"
  },
  {
    "text": "assumed the size of the destination buffer to be Max path but since the offset is never checked that's not",
    "start": "2169040",
    "end": "2175400"
  },
  {
    "text": "always the case and this can lead to an out of- bounds right you know the dump analysis work",
    "start": "2175400",
    "end": "2181280"
  },
  {
    "text": "wasn't perfect there were still cases where the right context wasn't available like in race conditions and use after",
    "start": "2181280",
    "end": "2187760"
  },
  {
    "text": "freeze it struggled a lot with that but it also highlighted the interesting part",
    "start": "2187760",
    "end": "2193280"
  },
  {
    "text": "that what large language models are capable of today if you can get at the right context it can do really hard",
    "start": "2193280",
    "end": "2199160"
  },
  {
    "text": "technical problems for you and especially if the technical problem doesn't need that much Precision that's",
    "start": "2199160",
    "end": "2204319"
  },
  {
    "text": "even better for the output and here you have a list of things I'd like to do better over time especially as we get",
    "start": "2204319",
    "end": "2210119"
  },
  {
    "text": "more data and we improve our input to get the uh RCA correct so just to summarize the L for",
    "start": "2210119",
    "end": "2217839"
  },
  {
    "text": "your organization number one is data is above all you know you don't necessarily need examples for using large language",
    "start": "2217839",
    "end": "2224000"
  },
  {
    "text": "models but it can greatly improve your Chan of for Success you need to really have a hacker mindset here if you can",
    "start": "2224000",
    "end": "2229319"
  },
  {
    "text": "see a data you can scrape it um and that's the model I use in my head is like if I saw source code I can see it",
    "start": "2229319",
    "end": "2236000"
  },
  {
    "text": "with my web browser then I can scrape that source code as well now msrc use internal data but there's a lot of",
    "start": "2236000",
    "end": "2242160"
  },
  {
    "text": "external equivalents like the open source vulnerability data I mentioned the Linux kernel is a really great example of that they have a lot of fixed",
    "start": "2242160",
    "end": "2248280"
  },
  {
    "text": "vulnerabilities with including patches and the issues um original description",
    "start": "2248280",
    "end": "2253800"
  },
  {
    "text": "you know look for ways to generate the data from what you already have you know for dumps there wasn't an",
    "start": "2253800",
    "end": "2259800"
  },
  {
    "text": "easy solution at all I just begged my team to give me crash dumps and eventually I was able to get nearly 500",
    "start": "2259800",
    "end": "2266040"
  },
  {
    "text": "that were viable for training but in reality it's going to be a lot more complicated than this you know",
    "start": "2266040",
    "end": "2271640"
  },
  {
    "text": "especially if you're dealing with external data you can use the large language models Universal interface advantage in order to convert the data",
    "start": "2271640",
    "end": "2278480"
  },
  {
    "text": "you can get into Data you need um and so yeah the real big principles we want to keep in mind is",
    "start": "2278480",
    "end": "2284240"
  },
  {
    "text": "derive the data you need from what you have so for example with the root cost summary work um I first generated a root",
    "start": "2284240",
    "end": "2290760"
  },
  {
    "text": "cost summary based on internal analysis notes you want to normalize your input data to make it uniform don't be afraid",
    "start": "2290760",
    "end": "2296880"
  },
  {
    "text": "of get your hands dirty and really leverage the fact that llms allow you to interact with nearly all data you can",
    "start": "2296880",
    "end": "2302960"
  },
  {
    "text": "represent in words now what makes a problem a good fit is again is it going to be data",
    "start": "2302960",
    "end": "2308640"
  },
  {
    "text": "diversity do you have enough data for your problem and is it easy to solve",
    "start": "2308640",
    "end": "2314440"
  },
  {
    "text": "your problem with code because if it isn't like the crash dump analysis work then the more value at large language",
    "start": "2314440",
    "end": "2319760"
  },
  {
    "text": "models have to offer you know is there room for imperfection and a solution can you break down your problem into logical",
    "start": "2319760",
    "end": "2326440"
  },
  {
    "text": "steps or do you have enough data to find tun a model to learn for it for",
    "start": "2326440",
    "end": "2331640"
  },
  {
    "text": "you so if you want to apply large language models to your work find what makes your life hard find what you have",
    "start": "2331640",
    "end": "2337440"
  },
  {
    "text": "have a lot of data for and really just try experiment is the biggest advice I can give you there is no perfect",
    "start": "2337440",
    "end": "2343520"
  },
  {
    "text": "solution for every use case it might work it might not work I was able to show you a few capabilities today that did work for me but it's really up to",
    "start": "2343520",
    "end": "2350160"
  },
  {
    "text": "you to apply that to your business and for your use cases I'd also be really careful with",
    "start": "2350160",
    "end": "2356400"
  },
  {
    "text": "large language models there's a lot of hype right now going around them I do think that they provide some value but",
    "start": "2356400",
    "end": "2362240"
  },
  {
    "text": "nowhere near as much as some people believe um you know I think that they're above average in terms of their",
    "start": "2362240",
    "end": "2367359"
  },
  {
    "text": "potential value but whenever someone's telling you about Ai and how they're leveraging it really make sure to",
    "start": "2367359",
    "end": "2372720"
  },
  {
    "text": "understand their incentives you know do they make a living off of thought leadership do they work for a company that's going to sell you more AI",
    "start": "2372720",
    "end": "2378400"
  },
  {
    "text": "products I'd be really careful with considering the output of work you see online especially with malformed",
    "start": "2378400",
    "end": "2385319"
  },
  {
    "text": "incentives to align with what you know works until you reproduce it",
    "start": "2385319",
    "end": "2391400"
  },
  {
    "text": "yourself I wouldn't underestimate traditional machine learning if this is what we can do with models today think",
    "start": "2391400",
    "end": "2397040"
  },
  {
    "text": "about what we can do with models tomorrow and the final piece of advice I'd give you is start collecting data",
    "start": "2397040",
    "end": "2403079"
  },
  {
    "text": "now thank you so much for coming out to my talk [Applause]",
    "start": "2403079",
    "end": "2410919"
  }
]