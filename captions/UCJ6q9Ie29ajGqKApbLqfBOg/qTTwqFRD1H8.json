[
  {
    "text": "ok so I think it's time welcome everyone I am at Utrecht and today I will be",
    "start": "0",
    "end": "6240"
  },
  {
    "text": "talking about effective effective file format fuzzing specifically some of the",
    "start": "6240",
    "end": "11370"
  },
  {
    "text": "techniques methods and the results that I've had during the few years of passing that I've been doing at Google so a few",
    "start": "11370",
    "end": "19650"
  },
  {
    "text": "words about myself I work at project zero what is relevant to the talk is",
    "start": "19650",
    "end": "24689"
  },
  {
    "text": "that I am a part-time developer and the frequent user of the fuzzing infrastructure that we have their internally and it is very related to",
    "start": "24689",
    "end": "32430"
  },
  {
    "text": "what I will be discussing here today I also play CD f's and generally interested in all sorts of vulnerability",
    "start": "32430",
    "end": "38760"
  },
  {
    "text": "research and software exploitation so today I will be talking about what",
    "start": "38760",
    "end": "44070"
  },
  {
    "text": "continues real-life offensive buzzing so fuzzing for actually finding bugs that",
    "start": "44070",
    "end": "49289"
  },
  {
    "text": "you can probably exploit or in my case report to the vendors and I would like",
    "start": "49289",
    "end": "55680"
  },
  {
    "text": "to talk about how each of the stages that is part of fuzzing is typically implemented and what you can do better",
    "start": "55680",
    "end": "62340"
  },
  {
    "text": "to actually improve your results and not find more bugs and so this is going to",
    "start": "62340",
    "end": "67680"
  },
  {
    "text": "be all shown on the example of several different pieces of software that I have passed in the past including Adobe",
    "start": "67680",
    "end": "73740"
  },
  {
    "text": "Reader Adobe Flash Windows kernel and some other open source software so let's",
    "start": "73740",
    "end": "80189"
  },
  {
    "text": "start with some very very soft basics we probably know what fuzzing is so it's just the software testing technique",
    "start": "80189",
    "end": "86460"
  },
  {
    "text": "which is most often automated and it's all about the speeding in valid data to",
    "start": "86460",
    "end": "91740"
  },
  {
    "text": "the software that we are testing so in the case of my talk now the software",
    "start": "91740",
    "end": "96900"
  },
  {
    "text": "part is just commonly used programs in libraries which can be both open and",
    "start": "96900",
    "end": "101909"
  },
  {
    "text": "closed source the important thing is that they are written in native languages and this means that they can",
    "start": "101909",
    "end": "107970"
  },
  {
    "text": "be used for memory corruption style all zero-day attacks and on the other hand the inputs are just we assume that they",
    "start": "107970",
    "end": "114869"
  },
  {
    "text": "are files of different structure which may be documented or undocumented and",
    "start": "114869",
    "end": "120390"
  },
  {
    "text": "their process by the software so it that they can be websites outlets images",
    "start": "120390",
    "end": "125610"
  },
  {
    "text": "videos and stuff like that so",
    "start": "125610",
    "end": "130489"
  },
  {
    "text": "in general the scheme of fuzzing is very simple we just have a loop inside of which we choose an input mutated fit it",
    "start": "131340",
    "end": "137790"
  },
  {
    "text": "to the target and then see the target has actually crashed or not if it has we save the input and if it hasn't then we",
    "start": "137790",
    "end": "144450"
  },
  {
    "text": "just proceed of course this is still very simple but while the general scheme is very very easy it's that the thing",
    "start": "144450",
    "end": "151590"
  },
  {
    "text": "that is easy to learn but it's it's more hard to master so if we try to dig deep",
    "start": "151590",
    "end": "159330"
  },
  {
    "text": "into how to pass effectively we have several questions that we have to answer",
    "start": "159330",
    "end": "164360"
  },
  {
    "text": "so you have to think about how we choose the defaulting target in the first place how we generate the inputs where do we",
    "start": "164360",
    "end": "171720"
  },
  {
    "text": "take them from weak we have some base corpus how do you how do we mutate them and there is like a huge list of things",
    "start": "171720",
    "end": "177930"
  },
  {
    "text": "that we actually have to consider some of these things are taken care of by the buzzer that we are using if we are you",
    "start": "177930",
    "end": "185670"
  },
  {
    "text": "use an optical phone but if we are trying to create a filing system on our",
    "start": "185670",
    "end": "190680"
  },
  {
    "text": "own for whatever reason then we actually have to think about all of those things so I will I will discuss some of those",
    "start": "190680",
    "end": "198420"
  },
  {
    "text": "things and try to answer to the best of my knowledge how to how to make them as",
    "start": "198420",
    "end": "203700"
  },
  {
    "text": "effective as possible so let's start with gathering the initial culprit of",
    "start": "203700",
    "end": "208709"
  },
  {
    "text": "input files probably most people doing fuzzing actually go through this step because it is a desired step in a",
    "start": "208709",
    "end": "215579"
  },
  {
    "text": "majority of cases for several reasons of course it makes it possible to reach",
    "start": "215579",
    "end": "220620"
  },
  {
    "text": "some code paths immediately after starting the passing so we don't have to",
    "start": "220620",
    "end": "226340"
  },
  {
    "text": "recover all of this file structure by ourselves we already have a good starting point we have we may have some",
    "start": "226340",
    "end": "233880"
  },
  {
    "text": "complex data structure inside of those files which could be either very difficult or impossible to generate even",
    "start": "233880",
    "end": "240180"
  },
  {
    "text": "if we have code coverage information available and even if these constructs could be created by for example using",
    "start": "240180",
    "end": "247109"
  },
  {
    "text": "code coverage based passing we are still saving a lot of time by doing a the initial corpus generation and of course",
    "start": "247109",
    "end": "255870"
  },
  {
    "text": "if we have a specific purpose of of files in a in a format status for example PDF we can reuse the same",
    "start": "255870",
    "end": "262750"
  },
  {
    "text": "because 2fast different projects afterwards so this contact methods of doing this is as follows first of all",
    "start": "262750",
    "end": "271200"
  },
  {
    "text": "many open-source projects actually already include extensive sets of input data for testing that we can use for",
    "start": "271200",
    "end": "277810"
  },
  {
    "text": "example we add a ten-pack project which is a video and media processing library",
    "start": "277810",
    "end": "282940"
  },
  {
    "text": "has something called ffmpeg faith and it's a system for regression testing in",
    "start": "282940",
    "end": "288970"
  },
  {
    "text": "ffmpeg and it already has like hundreds of interesting files that you can use sometimes these files are not publicly",
    "start": "288970",
    "end": "295960"
  },
  {
    "text": "for everyone but if you reach out to the developers and let them know that you",
    "start": "295960",
    "end": "301990"
  },
  {
    "text": "want to test the software they might be happy to share some of the private private samples with you and also many",
    "start": "301990",
    "end": "307960"
  },
  {
    "text": "of the projects especially the open-source one actually include converters from format X to their own",
    "start": "307960",
    "end": "313150"
  },
  {
    "text": "format Y and the example here would be the WebP image format which set the convertor called CFP which can convert",
    "start": "313150",
    "end": "320200"
  },
  {
    "text": "from PNG JPEG and TIFF another thing that we could do is internet crawling so",
    "start": "320200",
    "end": "326979"
  },
  {
    "text": "it all of course depends on the popularity of the fast file format but it's quite an intuitive approach so we",
    "start": "326979",
    "end": "333640"
  },
  {
    "text": "could either just download files through the specific file extension or with a specific magic bite or some other way of",
    "start": "333640",
    "end": "339490"
  },
  {
    "text": "of recognizing those files the problem is that if we are targeting a very",
    "start": "339490",
    "end": "344919"
  },
  {
    "text": "popular file format we turn it up with terabytes of data but it's not really a",
    "start": "344919",
    "end": "349960"
  },
  {
    "text": "problem if we can distill it to a reasonable corpus somehow but another",
    "start": "349960",
    "end": "355960"
  },
  {
    "text": "interesting idea is that we could actually try to ask the actual target what it thinks whether it thinks that it",
    "start": "355960",
    "end": "363040"
  },
  {
    "text": "supports the specific file or not so this would be either achieved by using",
    "start": "363040",
    "end": "368919"
  },
  {
    "text": "code coverage again but this tends to slow down the process very much but in",
    "start": "368919",
    "end": "374470"
  },
  {
    "text": "some cases you can just directly ask the question to the program itself and the case study here would be either pro so",
    "start": "374470",
    "end": "381400"
  },
  {
    "text": "as you probably know ida pro has a long list of supported format this is not a complete list but it's already a lot of",
    "start": "381400",
    "end": "388660"
  },
  {
    "text": "items here probably several thousands of them and so when you try to load a file",
    "start": "388660",
    "end": "395650"
  },
  {
    "text": "in either what happens first if you see this window which lists all of the format",
    "start": "395650",
    "end": "401050"
  },
  {
    "text": "that it has recognized inside of the file and you can choose how to load this file but the real question is how does",
    "start": "401050",
    "end": "407560"
  },
  {
    "text": "this really work and to find out about it you have to",
    "start": "407560",
    "end": "413349"
  },
  {
    "text": "look into the loader architecture in Ida and it turns out after a few minutes that they have a modular design with",
    "start": "413349",
    "end": "420669"
  },
  {
    "text": "with each loader being inside of a separate file so inside of your installation directory you have a long",
    "start": "420669",
    "end": "427659"
  },
  {
    "text": "list of files which are basically just DLL or shared objects or some other file",
    "start": "427659",
    "end": "433690"
  },
  {
    "text": "depending on the architecture which exports to functions except file and load file and you have two versions of",
    "start": "433690",
    "end": "440500"
  },
  {
    "text": "each of those files for 32-bit and 64-bit version of Ida so the definitions",
    "start": "440500",
    "end": "447220"
  },
  {
    "text": "of those two functions you can see here they are also documented in the Ida SDK they are very simple they just take some",
    "start": "447220",
    "end": "454120"
  },
  {
    "text": "input stream and output some information about the file so except file is basically a function which performs some",
    "start": "454120",
    "end": "460599"
  },
  {
    "text": "very preliminary processing and it returns zero on one depending on whether either can actually handle this file or",
    "start": "460599",
    "end": "466780"
  },
  {
    "text": "not and the load file is the more complicated function which actually takes takes care of the regular",
    "start": "466780",
    "end": "473020"
  },
  {
    "text": "processing of the file when you decide to load the file and yeah as I said both functions are very well-documented and",
    "start": "473020",
    "end": "480190"
  },
  {
    "text": "this is very convenient for us so when I was passing either myself I decided that",
    "start": "480190",
    "end": "486310"
  },
  {
    "text": "I could just write a simple loader which will iterate through all of those loaders called the accept file function",
    "start": "486310",
    "end": "492639"
  },
  {
    "text": "and see which file it would recognize so here you can see an example of running",
    "start": "492639",
    "end": "499569"
  },
  {
    "text": "the loader against itself and you can see that elf dot l LX actually",
    "start": "499569",
    "end": "505300"
  },
  {
    "text": "recognized the file as else which is correct so this was very helpful to actually from a large corpus of files",
    "start": "505300",
    "end": "511810"
  },
  {
    "text": "see which files would be handled by Ida correctly without doing all those code coverage information extraction or",
    "start": "511810",
    "end": "518950"
  },
  {
    "text": "anything more complicated like that so this was very good because it was it",
    "start": "518950",
    "end": "525600"
  },
  {
    "text": "worked with a very high degree of confidence because the exit file function actually has some extensive",
    "start": "525600",
    "end": "531730"
  },
  {
    "text": "logic to determine whether the file should be accepted or not thanks to this we also knew that which loader exactly",
    "start": "531730",
    "end": "540070"
  },
  {
    "text": "would be able to load so you would know which louder we would be able to find bugs in using the specific file we",
    "start": "540070",
    "end": "546580"
  },
  {
    "text": "didn't really have to even start either because we would just call a simple function from from a single dll or",
    "start": "546580",
    "end": "552040"
  },
  {
    "text": "shared object and we didn't have to use any instrumentation so similar techniques could probably be used for",
    "start": "552040",
    "end": "558340"
  },
  {
    "text": "software which makes it possible to quickly determine whether it supports the specific file or not which can be",
    "start": "558340",
    "end": "564490"
  },
  {
    "text": "useful sometimes so the other thing is after we already try to get get like the initial large",
    "start": "564490",
    "end": "574000"
  },
  {
    "text": "set of files is that we could try to distill it and I believe that in passing",
    "start": "574000",
    "end": "579430"
  },
  {
    "text": "it is quite important to get rid of most of the redundancy into input corpus and this means both the first one that we",
    "start": "579430",
    "end": "586120"
  },
  {
    "text": "actually start fuzzing with and also the living one which is being used while we do the fuzzing and evolve the corpus to",
    "start": "586120",
    "end": "592720"
  },
  {
    "text": "get a better one so in the context of a single test case we want to maximize the",
    "start": "592720",
    "end": "598660"
  },
  {
    "text": "program States Explorer divided by the input size which means that we want to have the highest ratio of bytes to",
    "start": "598660",
    "end": "605800"
  },
  {
    "text": "program feature basically so that the highest number of functionalities touched by the smallest number of bytes",
    "start": "605800",
    "end": "611980"
  },
  {
    "text": "and likewise in terms of the whole corpus we also want to maximize the whole set of program states divided by",
    "start": "611980",
    "end": "619600"
  },
  {
    "text": "the number of input samples because we don't want to have too many of them exercising the same functionality so if",
    "start": "619600",
    "end": "628810"
  },
  {
    "text": "if there is too much data to actually process using corpus distillation with",
    "start": "628810",
    "end": "633880"
  },
  {
    "text": "code coverage information we may also try to write some kind of minimizer on",
    "start": "633880",
    "end": "639670"
  },
  {
    "text": "our own based on the basic structure of the file some sub some file formats have",
    "start": "639670",
    "end": "645490"
  },
  {
    "text": "some basic structure which is for example being divided into chunks that",
    "start": "645490",
    "end": "650710"
  },
  {
    "text": "have tags or some names this is the case for sweet files PDF PNG s and many other",
    "start": "650710",
    "end": "656620"
  },
  {
    "text": "files that you can see out there so such turn rich person can already be used to",
    "start": "656620",
    "end": "661660"
  },
  {
    "text": "do some very quick before we do anything more complex but",
    "start": "661660",
    "end": "666700"
  },
  {
    "text": "you have to be careful not to build too deep into the specifications because then it gets harder and the result is",
    "start": "666700",
    "end": "673150"
  },
  {
    "text": "not as cost-effective as it is where we just do very basic processing so now the",
    "start": "673150",
    "end": "679990"
  },
  {
    "text": "question is how do we actually define the program state because the file sizes and the cardinality which was part of",
    "start": "679990",
    "end": "686590"
  },
  {
    "text": "the expression are both people to measure but a program state is more difficult so we don't really have any",
    "start": "686590",
    "end": "693490"
  },
  {
    "text": "good metric for measuring program States especially with the characteristics that",
    "start": "693490",
    "end": "698860"
  },
  {
    "text": "we would like to have while doing fasting so we would like then the the program state to be defined such that",
    "start": "698860",
    "end": "704500"
  },
  {
    "text": "the number of them should be within a sane range so we cannot really count out combinations of based in memory it",
    "start": "704500",
    "end": "711700"
  },
  {
    "text": "should be meaningful in the context of memory safety and they should be we should be able to easily determine them",
    "start": "711700",
    "end": "718240"
  },
  {
    "text": "during program run time so the most approximations used today are just",
    "start": "718240",
    "end": "723850"
  },
  {
    "text": "assuming that code coverage is somewhat equal to program states and of course",
    "start": "723850",
    "end": "729340"
  },
  {
    "text": "this has many advantages because first of all increased code coverage is representative of new program state",
    "start": "729340",
    "end": "735940"
  },
  {
    "text": "which is good for us the same range requirement is met of course because",
    "start": "735940",
    "end": "741730"
  },
  {
    "text": "code copy information is typically linear in size to the overall program size and we can usually easily measure",
    "start": "741730",
    "end": "749260"
  },
  {
    "text": "it using compiled and external instrumentation but this is it disadvantages is that constant code",
    "start": "749260",
    "end": "756640"
  },
  {
    "text": "coverage doesn't really indicate that the program state is the program state set is also constant so we could be",
    "start": "756640",
    "end": "763150"
  },
  {
    "text": "missing some information using this method so yes the current state of arts",
    "start": "763150",
    "end": "769420"
  },
  {
    "text": "is just counting basic blocks and it's a it's a reasonably good approximation",
    "start": "769420",
    "end": "774900"
  },
  {
    "text": "because they have quite good granularity we shouldn't really measure instructions",
    "start": "774900",
    "end": "780750"
  },
  {
    "text": "typically because this is redundant information so we can measure the code",
    "start": "780750",
    "end": "786880"
  },
  {
    "text": "coverage of basic blocks using a compiler instrumentation such as G cough or external instrumentation such as",
    "start": "786880",
    "end": "793270"
  },
  {
    "text": "Intel pin and annamaria and just identify those basic books by the others of the first",
    "start": "793270",
    "end": "798880"
  },
  {
    "text": "instruction but the problem with this approach is that we could be still missing some information so here we have",
    "start": "798880",
    "end": "806050"
  },
  {
    "text": "a very simple program with a full function which has a very very simple if",
    "start": "806050",
    "end": "811570"
  },
  {
    "text": "statement so we have three calls of this function and when we perform the first",
    "start": "811570",
    "end": "817060"
  },
  {
    "text": "call we actually go through all of the basic blocks through some of the edges so we could think that we have the",
    "start": "817060",
    "end": "824860"
  },
  {
    "text": "maximum coverage of this function as of now but if we see this second call we",
    "start": "824860",
    "end": "829899"
  },
  {
    "text": "can see that a new path is actually being taken a new branch here and the same goes for the second call as well so",
    "start": "829899",
    "end": "836649"
  },
  {
    "text": "if we just measuring the the basic blocks we could be missing this information so another idea to",
    "start": "836649",
    "end": "843959"
  },
  {
    "text": "approaching a measuring code coverage would be to not measure basic blocks so not the vertices of the graph of the",
    "start": "843959",
    "end": "852839"
  },
  {
    "text": "code execution but the edges and this is of course what IFL is trying to do so",
    "start": "852839",
    "end": "860230"
  },
  {
    "text": "they were the first to introduce this and shift at large and you can write you can read in the technical void volume",
    "start": "860230",
    "end": "866079"
  },
  {
    "text": "white paper of AFL that they are noting the combination of the current location",
    "start": "866079",
    "end": "872110"
  },
  {
    "text": "that is reading the code and the previous location inside of the bitmap of all basic blocks so yes but we could",
    "start": "872110",
    "end": "882339"
  },
  {
    "text": "also extend this idea even further so in a more abstract sense recording edges is",
    "start": "882339",
    "end": "887529"
  },
  {
    "text": "recording the current block and the one previous but we could also record even more information but by tracking out the",
    "start": "887529",
    "end": "894040"
  },
  {
    "text": "only previous block but also more of them so for example n previous blocks and the current one which would provide",
    "start": "894040",
    "end": "900970"
  },
  {
    "text": "even more context as of how the program actually arrived at the current state but in my experience the diode edges are",
    "start": "900970",
    "end": "909339"
  },
  {
    "text": "quite a good approximation because they are fast to determine and they also they they don't have too much",
    "start": "909339",
    "end": "918450"
  },
  {
    "text": "redundancy but yes so you can also have even more granular information such as",
    "start": "918450",
    "end": "924550"
  },
  {
    "text": "counters and bit sets so instead of just recording whether it's specific basic",
    "start": "924550",
    "end": "929890"
  },
  {
    "text": "block or edge has been reached or not we can count how many times it has been reached or not and this can be useful for example when",
    "start": "929890",
    "end": "938680"
  },
  {
    "text": "we try to determine how many times a specific group has been has iterated over which could be helpful for example",
    "start": "938680",
    "end": "945580"
  },
  {
    "text": "in a case where we want to go to a string comparison or some other logic of",
    "start": "945580",
    "end": "952720"
  },
  {
    "text": "that kind so we have all these different kinds of coverage information that we",
    "start": "952720",
    "end": "958660"
  },
  {
    "text": "would like to extract of course we can do it using enthalpy nordyne number E or",
    "start": "958660",
    "end": "963730"
  },
  {
    "text": "some other dbi for example AFL uses a modified version of QM user for open",
    "start": "963730",
    "end": "970060"
  },
  {
    "text": "source we can use the built-in compiler of compiler instrumentation such as G cough and VM Co but what I have been",
    "start": "970060",
    "end": "978430"
  },
  {
    "text": "using personally and all the recommend to to use is sanitizer coverage so",
    "start": "978430",
    "end": "983830"
  },
  {
    "text": "sanitizer coverage is just an option of other sanitizer which you are probably familiar with which is a fast reliable",
    "start": "983830",
    "end": "991380"
  },
  {
    "text": "instrumentation for detecting memory safety issues so it's very useful for passing it by itself there are also many",
    "start": "991380",
    "end": "999100"
  },
  {
    "text": "other variants such as memory sanitizer or thread sanitizer etc and one thing you can also enable whether enabling",
    "start": "999100",
    "end": "1007080"
  },
  {
    "text": "other sanitizer is the sanitizer coverage which at the same time as validating the runtime of the right",
    "start": "1007080",
    "end": "1013530"
  },
  {
    "text": "library or program also provides information about the code coverage so I",
    "start": "1013530",
    "end": "1020700"
  },
  {
    "text": "am using it personally also the lid puzzle project which is the project of",
    "start": "1020700",
    "end": "1026160"
  },
  {
    "text": "costia which will be creator of of other sanitizer uses sanitizer coverage here",
    "start": "1026160",
    "end": "1032910"
  },
  {
    "text": "we have an example of sanitizer coverage usage so we have a very short program which has some full function which is",
    "start": "1032910",
    "end": "1038819"
  },
  {
    "text": "only calls if the number of parameters is 2 so you can just use two simple",
    "start": "1038820",
    "end": "1044400"
  },
  {
    "text": "compile time options to enable it and then you can see that after we call the program with zero with one and two",
    "start": "1044400",
    "end": "1050760"
  },
  {
    "text": "parameters the size of the output output files with the coverage information is",
    "start": "1050760",
    "end": "1056790"
  },
  {
    "text": "different it has either four or eight bytes so now that we can measure code coverage easily the question is",
    "start": "1056790",
    "end": "1064520"
  },
  {
    "text": "what do we do now first of all we have to remember that just measuring code",
    "start": "1064520",
    "end": "1069860"
  },
  {
    "text": "coverage by itself is not really a silver bullet and there are still many",
    "start": "1069860",
    "end": "1074930"
  },
  {
    "text": "code constructs which are impossible to cross with just dump you mutation based filing examples of this would be",
    "start": "1074930",
    "end": "1082300"
  },
  {
    "text": "comparisons of types that are larger than a single byte for example or",
    "start": "1082300",
    "end": "1089110"
  },
  {
    "text": "comparisons of memory blobs or ASCII strings so you can see those examples",
    "start": "1089470",
    "end": "1095420"
  },
  {
    "text": "here a comparison with a 32-bit value of course it's a very difficult to cross",
    "start": "1095420",
    "end": "1102410"
  },
  {
    "text": "with just a dump buzzer because we would have to get this this specific 32-bit value and the same thing goes with the",
    "start": "1102410",
    "end": "1109010"
  },
  {
    "text": "string comparison which would be even harder to guess so that problems are actually somewhat approachable if you",
    "start": "1109010",
    "end": "1115490"
  },
  {
    "text": "think about it now because you can for",
    "start": "1115490",
    "end": "1120560"
  },
  {
    "text": "example use approaches such as dictionaries which are supported in both AFL and lead buzzer which could be use",
    "start": "1120560",
    "end": "1128450"
  },
  {
    "text": "to find those specific values that are being searched for in the code and you",
    "start": "1128450",
    "end": "1135020"
  },
  {
    "text": "can also get some help from compiler flags but this is this is someone complicated to think about because the",
    "start": "1135020",
    "end": "1142490"
  },
  {
    "text": "somewhat unintuitive approach would be to disable all code optimization which would result in fewer hockey expressions",
    "start": "1142490",
    "end": "1149870"
  },
  {
    "text": "in assembly compressed code constructs for the basic blocks and stuff like that so we'd get more granular code coverage",
    "start": "1149870",
    "end": "1156350"
  },
  {
    "text": "information to analyze but on the contrary I'll come to have also found that if you use the oh three",
    "start": "1156350",
    "end": "1162200"
  },
  {
    "text": "optimization with unroll loops then some of the short string comparisons or",
    "start": "1162200",
    "end": "1167930"
  },
  {
    "text": "string signatures are being unrolled to to just very simple by the granular",
    "start": "1167930",
    "end": "1175040"
  },
  {
    "text": "comparisons so it's quite unclear which compilations flags should really be used",
    "start": "1175040",
    "end": "1180380"
  },
  {
    "text": "for coverage guided passing and it's probably something that you should adjust based on us the specific project",
    "start": "1180380",
    "end": "1186470"
  },
  {
    "text": "that you are trying to pass mmm so in the past also tablets or monday-- tried",
    "start": "1186470",
    "end": "1191600"
  },
  {
    "text": "to approach this problem somehow with another DDI of his that you call deep",
    "start": "1191600",
    "end": "1197210"
  },
  {
    "text": "cover another so he tried to extract a more granular information about the specific",
    "start": "1197210",
    "end": "1203310"
  },
  {
    "text": "instructions that are used for string comparisons for example so he implemented this to measure how far the",
    "start": "1203310",
    "end": "1211260"
  },
  {
    "text": "execution of the Red Sea MPB instruction went before bailing out or how many",
    "start": "1211260",
    "end": "1217470"
  },
  {
    "text": "beads were successfully compared by the CMP instruction and he was able to use",
    "start": "1217470",
    "end": "1223530"
  },
  {
    "text": "this approach to be able to actually recover crc32 checksum required by the",
    "start": "1223530",
    "end": "1228630"
  },
  {
    "text": "PNG decoders and when I was first creating this slide a few months ago I",
    "start": "1228630",
    "end": "1234660"
  },
  {
    "text": "also thought did the idea of Peter would be to have a compass Pacific compiler",
    "start": "1234660",
    "end": "1240030"
  },
  {
    "text": "design profiling which would be able to create very D optimized code with all of",
    "start": "1240030",
    "end": "1245340"
  },
  {
    "text": "the assembly being maximally simplified with all of those 32 million 16 bits and",
    "start": "1245340",
    "end": "1251100"
  },
  {
    "text": "comparisons being unrolled to byte comparisons and stuff like that so for",
    "start": "1251100",
    "end": "1256140"
  },
  {
    "text": "example the construct on the Left would be contained to the construct on the right and it turns out that this has",
    "start": "1256140",
    "end": "1263100"
  },
  {
    "text": "actually been achieved in the meanwhile so in August a research has been presented resulting in in a LLVM",
    "start": "1263100",
    "end": "1271920"
  },
  {
    "text": "compiler plugin that was actually able to perform these the optimizations and",
    "start": "1271920",
    "end": "1277380"
  },
  {
    "text": "this health ASL to discover several new bugs and the other part of the ideal",
    "start": "1277380",
    "end": "1283680"
  },
  {
    "text": "feature would be that the standard comparisons functions which are very annoying we could have a compiler based",
    "start": "1283680",
    "end": "1291390"
  },
  {
    "text": "solution for just having a separate version a separate piece of code in the",
    "start": "1291390",
    "end": "1297390"
  },
  {
    "text": "assembly for each of calls of those functions in the code which would help",
    "start": "1297390",
    "end": "1303270"
  },
  {
    "text": "us get some more granular information as well but we still had some unsolvable",
    "start": "1303270",
    "end": "1308610"
  },
  {
    "text": "problems such as when the value that are being loaded from the input is actually processed somehow before being compared",
    "start": "1308610",
    "end": "1315720"
  },
  {
    "text": "with some signatures and this we cannot really approach with any of the dump",
    "start": "1315720",
    "end": "1321620"
  },
  {
    "text": "ideas that I presented before but this is about damn pausing so we have to we",
    "start": "1321620",
    "end": "1327540"
  },
  {
    "text": "have to just agree with that so now we have lots of input files we have the",
    "start": "1327540",
    "end": "1333260"
  },
  {
    "text": "compiled target and the ability to measure code coverage the question is how do we proceed from here and this is",
    "start": "1333260",
    "end": "1342140"
  },
  {
    "text": "assuming that we would like to create some corpus management system I would like to present some algorithm of how I",
    "start": "1342140",
    "end": "1347720"
  },
  {
    "text": "do it myself so let's assume that we would like to have the system for",
    "start": "1347720",
    "end": "1353740"
  },
  {
    "text": "coverage guided corpus management which would have the following properties first of all it would be able to",
    "start": "1353740",
    "end": "1360320"
  },
  {
    "text": "minimize an initial corpus of potentially gigantic sizes to a smaller",
    "start": "1360320",
    "end": "1365659"
  },
  {
    "text": "one that is equally useful so on input we would have any input files and on",
    "start": "1365659",
    "end": "1371090"
  },
  {
    "text": "output you would have M input files and information about their code coverage and of course this should be scalable",
    "start": "1371090",
    "end": "1379480"
  },
  {
    "text": "this is the first property and the second one is that you would also like it would like to use it during fuzzing",
    "start": "1379480",
    "end": "1385520"
  },
  {
    "text": "first of all to decide if a specific mutated sample should be added to the corpus at runtime and to recalculate all",
    "start": "1385520",
    "end": "1392390"
  },
  {
    "text": "of this coverage information in if needed so the input here would be just the current corpus and its coverage and",
    "start": "1392390",
    "end": "1400100"
  },
  {
    "text": "the candidate sample and its coverage and on output we just should have the new corpus and its coverage so yes the",
    "start": "1400100",
    "end": "1409039"
  },
  {
    "text": "pure work for this is that the set cover problem so this is the set cover problem",
    "start": "1409039",
    "end": "1414980"
  },
  {
    "text": "or just a specific version of it which in itself is just np-hard so we cannot",
    "start": "1414980",
    "end": "1422510"
  },
  {
    "text": "really calculate the optimal solution in result reasonable time but we don't really have to do it and in fact it's",
    "start": "1422510",
    "end": "1429289"
  },
  {
    "text": "probably better if we just don't find the optimal solution but someone somewhat optimal one and there are",
    "start": "1429289",
    "end": "1437659"
  },
  {
    "text": "actually greedy algorithms which can find approximate in some reasonable time",
    "start": "1437659",
    "end": "1444520"
  },
  {
    "text": "an example of such an algorithm would be that we store the current corpus and the",
    "start": "1444520",
    "end": "1449779"
  },
  {
    "text": "current coverage and for every new sample we just check if it adds some you trace to the current code coverage so if",
    "start": "1449779",
    "end": "1457190"
  },
  {
    "text": "it does then we add it to the corpus otherwise we discard it and periodically",
    "start": "1457190",
    "end": "1462380"
  },
  {
    "text": "we could also optionally check if the of the samples are redundant inside of the corpus and remove them too to keep",
    "start": "1462380",
    "end": "1469520"
  },
  {
    "text": "the general size small but this has some some significant drawbacks so for",
    "start": "1469520",
    "end": "1477710"
  },
  {
    "text": "example it doesn't scale very well because we have to process all of the samples sequentially and also the size",
    "start": "1477710",
    "end": "1485990"
  },
  {
    "text": "and form of the corpus depends on the order in which the samples are processed so if we start processing the big files",
    "start": "1485990",
    "end": "1492620"
  },
  {
    "text": "at the beginning and then the smaller ones later then we would end up with a very large corpus which is not necessary",
    "start": "1492620",
    "end": "1499670"
  },
  {
    "text": "so we have very little control over the over the trade-off between volume and",
    "start": "1499670",
    "end": "1505190"
  },
  {
    "text": "redundancy in the output corpus so what I would like to propose is another design which is as follows for each",
    "start": "1505190",
    "end": "1513110"
  },
  {
    "text": "execution trace we know we saw the end smallest samples which reach that trace",
    "start": "1513110",
    "end": "1518330"
  },
  {
    "text": "and the overall corpus consists of all the files present in the structure and",
    "start": "1518330",
    "end": "1523580"
  },
  {
    "text": "the structure can be represented as a C++ STL object map which Maps a string",
    "start": "1523580",
    "end": "1530570"
  },
  {
    "text": "which is the name of the sample sorry the name of the trace into a set of pairs consisting of the names of the",
    "start": "1530570",
    "end": "1537890"
  },
  {
    "text": "samples and its sizes so here is an example on an illustration we have for",
    "start": "1537890",
    "end": "1543920"
  },
  {
    "text": "some 4 input samples which have some coverage and then we transform it into a list of traces which map to a maximum of",
    "start": "1543920",
    "end": "1551420"
  },
  {
    "text": "2 files per that trace and they are of course the smallest one here so the",
    "start": "1551420",
    "end": "1558950"
  },
  {
    "text": "advantages of this approach would be that this of course can be very trivially paralyzed with any number of",
    "start": "1558950",
    "end": "1565070"
  },
  {
    "text": "machines using the MapReduce model the extent of redundancy can be controlled",
    "start": "1565070",
    "end": "1570230"
  },
  {
    "text": "via the N parameter with during fasting the corpus will evolve to minimize the",
    "start": "1570230",
    "end": "1576590"
  },
  {
    "text": "average sample size by design because we are storing the end smallest samples we",
    "start": "1576590",
    "end": "1582260"
  },
  {
    "text": "have at least n samples for each trace which results in a very uniform code",
    "start": "1582260",
    "end": "1588500"
  },
  {
    "text": "coverage distribution across the entire set as compared to just having like one",
    "start": "1588500",
    "end": "1594110"
  },
  {
    "text": "samples per each trace and the upper limit for the number of input is limited but in practice it's much",
    "start": "1594110",
    "end": "1601580"
  },
  {
    "text": "less than actually the number of coverage traces times M this approach",
    "start": "1601580",
    "end": "1607730"
  },
  {
    "text": "also has some shortcomings so due to the fact that each trace has the smallest examples in the corpus for some basic",
    "start": "1607730",
    "end": "1614930"
  },
  {
    "text": "traces we will end up with some redundant short files which don't really exercise an interesting functionality",
    "start": "1614930",
    "end": "1621050"
  },
  {
    "text": "for example for PNG we have we have some files we just have the basic headers and",
    "start": "1621050",
    "end": "1627050"
  },
  {
    "text": "stuff like that or just basic headers and then a single chunk so very short",
    "start": "1627050",
    "end": "1632570"
  },
  {
    "text": "files which are really useful for fuzzing but I still think this this is an acceptable trade-off especially given",
    "start": "1632570",
    "end": "1639410"
  },
  {
    "text": "that having a such short input may also enable us to find some unexpected",
    "start": "1639410",
    "end": "1644510"
  },
  {
    "text": "behavior for example also handling some other types of MAGIX by the target that",
    "start": "1644510",
    "end": "1650810"
  },
  {
    "text": "we didn't initially expect so here is the algorithm in a kind of pseudocode so",
    "start": "1650810",
    "end": "1657080"
  },
  {
    "text": "the map face is really simple we just get the code coverage provided by the input data and for each trace ID we",
    "start": "1657080",
    "end": "1663380"
  },
  {
    "text": "output the trace ID and a pair of sample ID and the size of the file so here you",
    "start": "1663380",
    "end": "1668990"
  },
  {
    "text": "can see it we transform the list of the input files into a map of traces mapping",
    "start": "1668990",
    "end": "1677570"
  },
  {
    "text": "to to the samples it first looks like this and then there it does for the redo",
    "start": "1677570",
    "end": "1684020"
  },
  {
    "text": "space we just sort for each of the traces we just sort the list by the size",
    "start": "1684020",
    "end": "1690080"
  },
  {
    "text": "of the samples and then choose the end smallest one and output them so here we",
    "start": "1690080",
    "end": "1695750"
  },
  {
    "text": "start with this input we just sort the files by size and then we choose the and",
    "start": "1695750",
    "end": "1701750"
  },
  {
    "text": "smallest ones which in this case is two and this is our output so at the end we",
    "start": "1701750",
    "end": "1707720"
  },
  {
    "text": "end up with this list of files at the at the top and then we just sort it and get",
    "start": "1707720",
    "end": "1716030"
  },
  {
    "text": "a new unique list and we end up with a total of three out of four files in the",
    "start": "1716030",
    "end": "1721340"
  },
  {
    "text": "initial corpus so when it comes to actual track record for using this",
    "start": "1721340",
    "end": "1727820"
  },
  {
    "text": "algorithm in my infrastructure I've successfully used it to distill but datasets of terabytes to distill it",
    "start": "1727820",
    "end": "1736070"
  },
  {
    "text": "into somewhat reasonable corpuses and the examples are our PDF format based on",
    "start": "1736070",
    "end": "1742610"
  },
  {
    "text": "instrumented PDF you so I generated three purposes based on N equals one ten",
    "start": "1742610",
    "end": "1748250"
  },
  {
    "text": "and a hundred two to get some nice corpus is profiling with different degree of redundancy and I've done the",
    "start": "1748250",
    "end": "1755840"
  },
  {
    "text": "same for for free type too so when it comes to the to the algorithm",
    "start": "1755840",
    "end": "1761809"
  },
  {
    "text": "of determining whether a new candidate is good profiling or not the algorithm",
    "start": "1761809",
    "end": "1767210"
  },
  {
    "text": "is a little bit more complex first of all we go through each of the trays which is covered by the sample and see",
    "start": "1767210",
    "end": "1774200"
  },
  {
    "text": "if it improves the size the size of one of the traces so is its smallest smaller",
    "start": "1774200",
    "end": "1781159"
  },
  {
    "text": "than the size of the large largest file in the set and if any such improvement",
    "start": "1781159",
    "end": "1787730"
  },
  {
    "text": "is is achieved then we perform a second pass to remove all files from the set we",
    "start": "1787730",
    "end": "1795169"
  },
  {
    "text": "have which have the same size as of this file and that here is an illustration of",
    "start": "1795169",
    "end": "1801230"
  },
  {
    "text": "this approach so we have this candidate called five dot PDF which has size 20 so first",
    "start": "1801230",
    "end": "1808490"
  },
  {
    "text": "thing we do is that we try to improve some of the coverage traces here and it",
    "start": "1808490",
    "end": "1814190"
  },
  {
    "text": "turns out that some of them are improved so we insert the 5 dot PDF file for two",
    "start": "1814190",
    "end": "1819889"
  },
  {
    "text": "of the coverage traces and then we perform the second pass and also replace two other files which also had size 20",
    "start": "1819889",
    "end": "1827539"
  },
  {
    "text": "to reduce the total number of files in the set so this was about distilling the",
    "start": "1827539",
    "end": "1836269"
  },
  {
    "text": "whole corpus and merging in a single file and if we want to merge two corpora",
    "start": "1836269",
    "end": "1841389"
  },
  {
    "text": "then it's also travelled by just choosing the smallest n samples for each",
    "start": "1841389",
    "end": "1846529"
  },
  {
    "text": "of the traces that are covered side and one of the things that I passed and had",
    "start": "1846529",
    "end": "1853639"
  },
  {
    "text": "some success with using this approach was Wireshark which I've been passing since November last year using the",
    "start": "1853639",
    "end": "1859970"
  },
  {
    "text": "teacher command-line utility with built with a fan an awesome coverage I discovered 30",
    "start": "1859970",
    "end": "1865730"
  },
  {
    "text": "vulnerabilities using this approach and the interesting thing is that initially I started with some simple sample files",
    "start": "1865730",
    "end": "1872960"
  },
  {
    "text": "from the page sample captures for Wireshark and it was 300 files which had",
    "start": "1872960",
    "end": "1878960"
  },
  {
    "text": "over 200 megabytes so mostly those files were very big but upon several months of",
    "start": "1878960",
    "end": "1885890"
  },
  {
    "text": "coverage guided passing I actually was able to first create a very good corpus which consisted of almost 80,000 files",
    "start": "1885890",
    "end": "1894200"
  },
  {
    "text": "but medium file size was just 47 bytes so they were really actually optimal for",
    "start": "1894200",
    "end": "1900530"
  },
  {
    "text": "filing and if you want to test out your",
    "start": "1900530",
    "end": "1905660"
  },
  {
    "text": "file infrastructure Wireshark is actually a very good thing to do this on because the nature of the code base",
    "start": "1905660",
    "end": "1911780"
  },
  {
    "text": "makes it extremely well fit for dumb fuzzing because it has a vast number of these sectors for",
    "start": "1911780",
    "end": "1918200"
  },
  {
    "text": "different format it's mostly written in C and operate on very very simple data structurally so it just mostly compares",
    "start": "1918200",
    "end": "1925310"
  },
  {
    "text": "single bytes or or some very very simple constructs in the input file so it's",
    "start": "1925310",
    "end": "1931700"
  },
  {
    "text": "generally a very great best target for your puzzle and this is the trophy case for Wireshark in the project zero",
    "start": "1931700",
    "end": "1938180"
  },
  {
    "text": "tracker also this approach proved to be useful for Adobe Flash so I've been",
    "start": "1938180",
    "end": "1944690"
  },
  {
    "text": "fighting it for many years not not so much with coverage coverage guidance but recently I started",
    "start": "1944690",
    "end": "1952100"
  },
  {
    "text": "targeting the ActionScript loader class and in the official documentation you can see that the loader class is",
    "start": "1952100",
    "end": "1958400"
  },
  {
    "text": "supposed to only support JPEG PNG gif and SWF files but when I was filling",
    "start": "1958400",
    "end": "1966140"
  },
  {
    "text": "this with coverage information after several hours of passing I observed two sudden peaks in the number of coverage",
    "start": "1966140",
    "end": "1972440"
  },
  {
    "text": "coverage traces and when I looked into the input corpus I noticed that the",
    "start": "1972440",
    "end": "1978500"
  },
  {
    "text": "father discovered two new signatures are called ATF and III I would correspond to",
    "start": "1978500",
    "end": "1984470"
  },
  {
    "text": "a format that I didn't know before which is called Adobe texture format for stage 3d which can have embedded jxr files so",
    "start": "1984470",
    "end": "1993440"
  },
  {
    "text": "these are two very complex file formats whose support was not documented anywhere and I was able to discover",
    "start": "1993440",
    "end": "1999470"
  },
  {
    "text": "by just using this code coverage algorithm and I found seven vulnerabilities in those formats",
    "start": "1999470",
    "end": "2005559"
  },
  {
    "text": "thanks to this discovery so far so also when it comes to corpus post-processing",
    "start": "2005559",
    "end": "2012270"
  },
  {
    "text": "if the files are stored in a way which makes them difficult to mutate you can",
    "start": "2012270",
    "end": "2017470"
  },
  {
    "text": "do some pre-processing such as the compressing them if they are compressed",
    "start": "2017470",
    "end": "2022980"
  },
  {
    "text": "for example in SWF files you typically typically have them in a LD ma",
    "start": "2022980",
    "end": "2029320"
  },
  {
    "text": "compressed form such as with the CVS signature PDF documents also have most",
    "start": "2029320",
    "end": "2035289"
  },
  {
    "text": "of the binary streams compressed with deflate so we can also decompress them to make sure that the bit sleeping makes",
    "start": "2035289",
    "end": "2040990"
  },
  {
    "text": "sense and also tight bond forms are always encrypted with a simple cipher so",
    "start": "2040990",
    "end": "2047710"
  },
  {
    "text": "as several things about running the target and several trips so we have to",
    "start": "2047710",
    "end": "2053230"
  },
  {
    "text": "distinguish between command line and graphical applications it's generally preferred for at least for me for the",
    "start": "2053230",
    "end": "2059740"
  },
  {
    "text": "target program to be common light only which is quite common on Linux and less so on Windows most open-source drivers",
    "start": "2059740",
    "end": "2066908"
  },
  {
    "text": "actually shape with some already testing tools which are command line which we can use for fuzzing and this is much",
    "start": "2066909",
    "end": "2073450"
  },
  {
    "text": "cleaner in terms of interaction logging and basically everything else in fuzzy",
    "start": "2073450",
    "end": "2078510"
  },
  {
    "text": "but there are also some graphical Linux graphical applications for Linux in",
    "start": "2078510",
    "end": "2084730"
  },
  {
    "text": "which case we can use the X built well frame buffer to to deal with this and",
    "start": "2084730",
    "end": "2089878"
  },
  {
    "text": "the thing is that for some applications the amount of input data processed is actually dependent on the amount of data",
    "start": "2089879",
    "end": "2096460"
  },
  {
    "text": "which is displayed on the screen which is the case for example for Adobe Reader and my solution for that was to just",
    "start": "2096460",
    "end": "2103150"
  },
  {
    "text": "start a beautiful X frame buffer with a very huge resolution and then start",
    "start": "2103150",
    "end": "2110440"
  },
  {
    "text": "Adobe Reader such that it would have a huge window displaying all of the pages at the same time so this is the result",
    "start": "2110440",
    "end": "2116650"
  },
  {
    "text": "it was a very dumb way to make sure that as much PDF data was processed as possible and it really improved the",
    "start": "2116650",
    "end": "2124150"
  },
  {
    "text": "number of bucks that I was able to find in Adobe Reader and it's interesting",
    "start": "2124150",
    "end": "2129940"
  },
  {
    "text": "because that OB reader is supposed to be a graphical only up but if you started from command line",
    "start": "2129940",
    "end": "2135190"
  },
  {
    "text": "with the help common line argument you can see that it actually has a lot of common line arguments and while we're at",
    "start": "2135190",
    "end": "2144520"
  },
  {
    "text": "Adobe Reader I wanted to mention that I perform some fuzzing of it in 2013 12",
    "start": "2144520",
    "end": "2150100"
  },
  {
    "text": "and 13 which with some bugs pass at farms but then in 2014 after the Adobe",
    "start": "2150100",
    "end": "2157960"
  },
  {
    "text": "Reader for Linux was discontinued I still had some very like better method",
    "start": "2157960",
    "end": "2163030"
  },
  {
    "text": "than before so I still wanted to find some bugs in Adobe Reader but I couldn't really use a new version because it was",
    "start": "2163030",
    "end": "2169180"
  },
  {
    "text": "discontinued so my idea was to use the Linux version and fuzz it on Linux and",
    "start": "2169180",
    "end": "2175420"
  },
  {
    "text": "then see if any bugs would actually reproduce on the Windows version and it turned out that I ended up with over 700",
    "start": "2175420",
    "end": "2182980"
  },
  {
    "text": "crashes in total in the linux version out of which 11 reproduced on the Windows version as well",
    "start": "2182980",
    "end": "2188620"
  },
  {
    "text": "and they were fixed in in 2014 and 15 so",
    "start": "2188620",
    "end": "2195670"
  },
  {
    "text": "when the program is behaves there are certain behaviors that are undesired are doing thousand for example we don't want",
    "start": "2195670",
    "end": "2202630"
  },
  {
    "text": "to deal with some generic exception handlers which would mask the exceptions that we want to catch and stuff like",
    "start": "2202630",
    "end": "2208210"
  },
  {
    "text": "that they could attempt to establish some network connections excusal interaction and stuff like that and I",
    "start": "2208210",
    "end": "2214750"
  },
  {
    "text": "have founded on Linux it's very convenient to use the LD preload to actually block off some of the functions",
    "start": "2214750",
    "end": "2219970"
  },
  {
    "text": "that you don't want to be to have cold so for example you can disable custom exception handling by masking out two",
    "start": "2219970",
    "end": "2227140"
  },
  {
    "text": "functions that are signalling cig action just replace them with no operations you",
    "start": "2227140",
    "end": "2233440"
  },
  {
    "text": "can disable network connection by just masking out the socket function and stuff like that and you can also think",
    "start": "2233440",
    "end": "2241990"
  },
  {
    "text": "about passing the command line for some of the targets so some projects have multiple command line flags which might",
    "start": "2241990",
    "end": "2247930"
  },
  {
    "text": "we might want to sleep randomly in order to reach some interesting functionality especially in an open source project and",
    "start": "2247930",
    "end": "2255550"
  },
  {
    "text": "the solution to that would be to have an external target launcher which determines the actual common line to be",
    "start": "2255550",
    "end": "2261910"
  },
  {
    "text": "used based on some some data from the actual input files and I use that information to to peak",
    "start": "2261910",
    "end": "2269210"
  },
  {
    "text": "the command line to be used so for example at the van back where you can",
    "start": "2269210",
    "end": "2274220"
  },
  {
    "text": "specify the output format in which you want the output file to be generated in and if you list this format it turns out",
    "start": "2274220",
    "end": "2281540"
  },
  {
    "text": "that there is over 300 of them so it makes sense to try to test all of them and the logic for the wrapper would be",
    "start": "2281540",
    "end": "2289580"
  },
  {
    "text": "to just choose the encoder based on a hash which is generated out of the four",
    "start": "2289580",
    "end": "2295130"
  },
  {
    "text": "thousand bytes the first byte in the input file and then execute such such a",
    "start": "2295130",
    "end": "2300530"
  },
  {
    "text": "command line and another funny thing is that you should always make sure that",
    "start": "2300530",
    "end": "2305570"
  },
  {
    "text": "you're not actually losing cycles which was what I did so this is an example of free type which is a very convenient",
    "start": "2305570",
    "end": "2311810"
  },
  {
    "text": "command-line utility called FG bench and it runs through 12 tests exorcising",
    "start": "2311810",
    "end": "2317720"
  },
  {
    "text": "values a PA library calls and when you run it with no special parameters it",
    "start": "2317720",
    "end": "2322730"
  },
  {
    "text": "takes 25 seconds for example to just process the file and the important thing",
    "start": "2322730",
    "end": "2327980"
  },
  {
    "text": "is is the reason for this you can see that it says number of seconds for each test equals 2 which I didn't realize it",
    "start": "2327980",
    "end": "2335000"
  },
  {
    "text": "first I didn't really strike me for a long time and it turns out that you can",
    "start": "2335000",
    "end": "2340520"
  },
  {
    "text": "use a special track to actually just use run every test just once instead of for",
    "start": "2340520",
    "end": "2346910"
  },
  {
    "text": "2 seconds and this this sped up the passing by at least a hundred times for",
    "start": "2346910",
    "end": "2354050"
  },
  {
    "text": "me but I was still able to find some bugs using this very slow problem so I'm",
    "start": "2354050",
    "end": "2359450"
  },
  {
    "text": "afraid I had to skip out through some of the slides here I wanted to also talk about mutating inputs you will be able",
    "start": "2359450",
    "end": "2366109"
  },
  {
    "text": "to check out the slides later on there were some interesting thoughts about",
    "start": "2366109",
    "end": "2371170"
  },
  {
    "text": "first of all how to mutate the data and also what ratios to use for those data",
    "start": "2371170",
    "end": "2376520"
  },
  {
    "text": "because you don't always want to flip a specific number of bytes but you want to adjust it based on the nature of the",
    "start": "2376520",
    "end": "2383450"
  },
  {
    "text": "input data and then I also wanted to quickly talk about the Windows kernel",
    "start": "2383450",
    "end": "2389420"
  },
  {
    "text": "for passing effort maybe I'll try to talk about it really quickly so I decided to do inbox because I'm a very",
    "start": "2389420",
    "end": "2396680"
  },
  {
    "text": "huge fan of it it's a software emulator and I've already used it for some projects it's not very fast but we",
    "start": "2396680",
    "end": "2402890"
  },
  {
    "text": "can still scale against that if we use some more machines that just one so it has some very useful properties it can",
    "start": "2402890",
    "end": "2409700"
  },
  {
    "text": "be run on a Linux system it provides a documented instrumentation API so it",
    "start": "2409700",
    "end": "2415190"
  },
  {
    "text": "makes it possible for you to interact with the guests it runs Windows out of",
    "start": "2415190",
    "end": "2420470"
  },
  {
    "text": "the box which is very simple and you can easily configure it so let's do it when",
    "start": "2420470",
    "end": "2426589"
  },
  {
    "text": "it comes to input to the input data this part was the easiest one because I already had a input corpus of files",
    "start": "2426589",
    "end": "2432230"
  },
  {
    "text": "based on pre type 2000 so what I only had to do is to extract TrueType and OpenType files",
    "start": "2432230",
    "end": "2438039"
  },
  {
    "text": "because the other one that free type supports and supported by the Windows kernel yeah so then part of mutating TGF",
    "start": "2438039",
    "end": "2447980"
  },
  {
    "text": "in the OTS was the more interesting I decided that the mutations would be",
    "start": "2447980",
    "end": "2453619"
  },
  {
    "text": "applied in box instrumentation inside instead of the gas system itself because it would be much faster that way the",
    "start": "2453619",
    "end": "2461240"
  },
  {
    "text": "question is how to mutate them properly to get the best result because both TTFN and OTS follow a common chunk structure",
    "start": "2461240",
    "end": "2467720"
  },
  {
    "text": "called a certainty in which each file comes it's a number of tables which are",
    "start": "2467720",
    "end": "2472789"
  },
  {
    "text": "public documented so there are a total of about 50 tables in existence in total",
    "start": "2472789",
    "end": "2480950"
  },
  {
    "text": "but only 20 or so are actually important and one thing they have in common is",
    "start": "2480950",
    "end": "2487039"
  },
  {
    "text": "that they all are different so they have a different length different structure different kind of importance for the",
    "start": "2487039",
    "end": "2492380"
  },
  {
    "text": "operating system and the parser and stuff like that so it only seems reasonable to treat each of them",
    "start": "2492380",
    "end": "2497569"
  },
  {
    "text": "individually instead of pass the file as a whole so the typical scheme I've seen in",
    "start": "2497569",
    "end": "2503329"
  },
  {
    "text": "nearly every fund from fuzzing presentation would be that we just took the whole font and we would flip",
    "start": "2503329",
    "end": "2509390"
  },
  {
    "text": "somebody's in the whole file and we will just fix up that the table checks arms",
    "start": "2509390",
    "end": "2515510"
  },
  {
    "text": "in the header so that windows would actually accept it but I decided to go a different way and instead of passing or mutating a",
    "start": "2515510",
    "end": "2522890"
  },
  {
    "text": "whole form file I would be passing each of those tables individually such that I",
    "start": "2522890",
    "end": "2529400"
  },
  {
    "text": "would get the success to failure ratio at 50 % so 50% of the time windows would",
    "start": "2529400",
    "end": "2538339"
  },
  {
    "text": "actually be able to process that mutated file correctly and 50% of time it would fail which would mean that we would be",
    "start": "2538339",
    "end": "2544880"
  },
  {
    "text": "kind of at a verge of the mutated file being correct so I just wrote a very",
    "start": "2544880",
    "end": "2550339"
  },
  {
    "text": "simple program to actually determine what mutation Rachel would be best fit",
    "start": "2550339",
    "end": "2555589"
  },
  {
    "text": "for each of the tables and either each of the mutation algorithm to maintain the 93% correctness and this resulted in",
    "start": "2555589",
    "end": "2563390"
  },
  {
    "text": "the following table of mutation ratios for each of the algorithms and each of the s sng tables so I also didn't set",
    "start": "2563390",
    "end": "2572059"
  },
  {
    "text": "the mutation ratios to be fixed as shown in the previous table but I also allowed some some freedom in that so I set a",
    "start": "2572059",
    "end": "2580369"
  },
  {
    "text": "range between zero and two times the the mutation ratio that I determined and with a trivial piece of code to",
    "start": "2580369",
    "end": "2587599"
  },
  {
    "text": "disassemble modified and reassemble assessing T files I was now able to mutate them in a very meaningful way",
    "start": "2587599",
    "end": "2593299"
  },
  {
    "text": "being able to control how they behave in Windows I also wrote a very simple TDF",
    "start": "2593299",
    "end": "2598909"
  },
  {
    "text": "generator with TTFN instructions which consisted of the disassembling the PDF",
    "start": "2598909",
    "end": "2605059"
  },
  {
    "text": "files inserting new instructions with a python generator and then reassembling",
    "start": "2605059",
    "end": "2610249"
  },
  {
    "text": "it back and I managed to find find one extra back with this generator so quite",
    "start": "2610249",
    "end": "2615890"
  },
  {
    "text": "quite a quite less than with does the damn puzzle itself and when it comes to",
    "start": "2615890",
    "end": "2622640"
  },
  {
    "text": "the communication channel between the ghost host and the guest I used the Box instrumentation to detect",
    "start": "2622640",
    "end": "2629599"
  },
  {
    "text": "a specific instruction in this case it was Elton's and I used this instruction to implement",
    "start": "2629599",
    "end": "2637549"
  },
  {
    "text": "operations such as request data which would allowed the harness in the guest to request mutated phone data from the",
    "start": "2637549",
    "end": "2644209"
  },
  {
    "text": "host send status to send status whether the loading of the phone succeeded or",
    "start": "2644209",
    "end": "2650749"
  },
  {
    "text": "not and the simple debug print so of course I had to be very careful to to be",
    "start": "2650749",
    "end": "2657049"
  },
  {
    "text": "to be sure that all of the memory regions passed back to box were mapped in physical memory so I would be done",
    "start": "2657049",
    "end": "2663799"
  },
  {
    "text": "able to write to each directly I also implemented - to harness to be able to get the best",
    "start": "2663799",
    "end": "2671450"
  },
  {
    "text": "code coverage in the Windows kernel so I would call all of the available API functions which were operating on the",
    "start": "2671450",
    "end": "2677390"
  },
  {
    "text": "fonts and I would list and display all of the glyphs in the font to be to make sure that all of the mutations would",
    "start": "2677390",
    "end": "2684710"
  },
  {
    "text": "actually be triggered in the Windows kernel I also operate optimize the operating system to the maximum extent",
    "start": "2684710",
    "end": "2691160"
  },
  {
    "text": "so I disabled all of the themes disabled services that were not necessarily",
    "start": "2691160",
    "end": "2696380"
  },
  {
    "text": "removed most of the system files removed all our items from outer start disabled",
    "start": "2696380",
    "end": "2701900"
  },
  {
    "text": "paging and stuff like that to make sure that the guest system was actually very quick and we wouldn't be losing any",
    "start": "2701900",
    "end": "2708529"
  },
  {
    "text": "cycles inside of the box and of course I also had to make sure that the system",
    "start": "2708529",
    "end": "2713900"
  },
  {
    "text": "would actually restart when the crash happened so I could detect a system reset as an",
    "start": "2713900",
    "end": "2719539"
  },
  {
    "text": "indicator for a crash in the Windows kernel and then when I wanted to reproduce the crashes I would do it in a",
    "start": "2719539",
    "end": "2726260"
  },
  {
    "text": "separate beautiful box VM and I would just load the font in the same way as",
    "start": "2726260",
    "end": "2732349"
  },
  {
    "text": "doing the fuzzing and then check if there is a crash dump in the windows directory if so I would generate a",
    "start": "2732349",
    "end": "2739099"
  },
  {
    "text": "report from that and save it I also did some minimization but I won't really",
    "start": "2739099",
    "end": "2746119"
  },
  {
    "text": "talk talk about it anymore but the general result from this research is that I found all classes of bugs",
    "start": "2746119",
    "end": "2753349"
  },
  {
    "text": "basically mostly pull based buffer overflows but also sum over each user to fries and other interesting things this",
    "start": "2753349",
    "end": "2761299"
  },
  {
    "text": "was done in four iterations and this is not necessarily the end there could be",
    "start": "2761299",
    "end": "2766490"
  },
  {
    "text": "more more bugs discovered let's see in the future so the closing thoughts is that",
    "start": "2766490",
    "end": "2773650"
  },
  {
    "text": "hopefully after this effort no more bites are looking the Windows kernel from processing let's see that is",
    "start": "2773650",
    "end": "2781640"
  },
  {
    "text": "the case or not either either way Microsoft has moved from processing into a user space process which is good for",
    "start": "2781640",
    "end": "2788960"
  },
  {
    "text": "security but it could also make fuzzing easier for us and yeah we can still think about using it as an RCA II vector",
    "start": "2788960",
    "end": "2797150"
  },
  {
    "text": "because even though it's not anymore in the kernel but in user space we can still use it to get some kind of",
    "start": "2797150",
    "end": "2803560"
  },
  {
    "text": "remote code execution so with that I thank you for your attention and I'm happy to answer questions probably after",
    "start": "2803560",
    "end": "2809859"
  },
  {
    "text": "the talk [Applause]",
    "start": "2809859",
    "end": "2814950"
  },
  {
    "text": "[Music]",
    "start": "2814950",
    "end": "2818030"
  }
]