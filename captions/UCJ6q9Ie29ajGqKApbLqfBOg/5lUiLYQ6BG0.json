[
  {
    "text": "[Music]",
    "start": "1130",
    "end": "13759"
  },
  {
    "text": "hi everyone welcome to my talk on big data in security i will talk about how",
    "start": "13759",
    "end": "19279"
  },
  {
    "text": "we can analyze these big data infrastructures to better protect them",
    "start": "19279",
    "end": "24960"
  },
  {
    "text": "before starting please let me introduce myself briefly my name is hila i work as",
    "start": "24960",
    "end": "30240"
  },
  {
    "text": "head of research at dreamlab technologies a swiss infosec company and an offensive security specialist",
    "start": "30240",
    "end": "37440"
  },
  {
    "text": "with several years of experience and in the last time i have focused on security",
    "start": "37440",
    "end": "42559"
  },
  {
    "text": "in cloud environments cloud native big data and related stuff",
    "start": "42559",
    "end": "48000"
  },
  {
    "text": "okay so let's go to the important things there are some key concepts that i would",
    "start": "48000",
    "end": "54160"
  },
  {
    "text": "like to explain before jumping into the security part probably the first thing that comes to",
    "start": "54160",
    "end": "61199"
  },
  {
    "text": "mind when talking about big data is the challenge of storing large volumes of",
    "start": "61199",
    "end": "66720"
  },
  {
    "text": "information and the technology that will take care of it although that's correct around the",
    "start": "66720",
    "end": "72799"
  },
  {
    "text": "storage technology there are many others of great importance that make up the ecosystem",
    "start": "72799",
    "end": "79600"
  },
  {
    "text": "when we design big data architectures we must think about how the data will be",
    "start": "79600",
    "end": "84960"
  },
  {
    "text": "transported from the source to the storage if the data requires some type of processing to be consumed",
    "start": "84960",
    "end": "92400"
  },
  {
    "text": "how the information will be accessed right so the different processes that the data",
    "start": "92400",
    "end": "98640"
  },
  {
    "text": "go through are divided into four main layers that comprise the big data stack",
    "start": "98640",
    "end": "105920"
  },
  {
    "text": "so we have the data injection that is the transport of the information from the different oceans to the storage",
    "start": "105920",
    "end": "113520"
  },
  {
    "text": "place the storage itself the data processing layer because the",
    "start": "113520",
    "end": "118960"
  },
  {
    "text": "most common is to ingest raw information that later needs some kind of processing",
    "start": "118960",
    "end": "125600"
  },
  {
    "text": "and finally the data access layer basically how users will access and",
    "start": "125600",
    "end": "130640"
  },
  {
    "text": "consume the information and let's add one more layer here this",
    "start": "130640",
    "end": "135920"
  },
  {
    "text": "is not part of the big data stack but we have this layer in all big data",
    "start": "135920",
    "end": "141520"
  },
  {
    "text": "infrastructures the cluster management is really important",
    "start": "141520",
    "end": "146959"
  },
  {
    "text": "so for each of these layers there is a wide variety of technologies that can be",
    "start": "146959",
    "end": "152560"
  },
  {
    "text": "implemented the big data stack is hugely big this one are just a few of the most",
    "start": "152560",
    "end": "160000"
  },
  {
    "text": "popular technologies for example hadoop for the storage spark and storm for",
    "start": "160000",
    "end": "166239"
  },
  {
    "text": "processing imbala presto grid for accessing the information",
    "start": "166239",
    "end": "171360"
  },
  {
    "text": "fluent scope for data injection and zookeeper for management right",
    "start": "171360",
    "end": "179360"
  },
  {
    "text": "so when we analyze an entire big data infrastructure we can actually find many different and",
    "start": "179360",
    "end": "186000"
  },
  {
    "text": "complex technologies interacting with each other they meet different functions according",
    "start": "186000",
    "end": "191840"
  },
  {
    "text": "to the layer of the stack where they are located",
    "start": "191840",
    "end": "196800"
  },
  {
    "text": "so let's see an example of our real big data architecture here we have uh two different clouds",
    "start": "197519",
    "end": "205120"
  },
  {
    "text": "one in lvs and another one in any other cloud provider so both are running some kubernetes",
    "start": "205120",
    "end": "212640"
  },
  {
    "text": "clusters there are serving different applications so we want to store and",
    "start": "212640",
    "end": "217680"
  },
  {
    "text": "analyze the love of these applications for that we will use",
    "start": "217680",
    "end": "222799"
  },
  {
    "text": "flowing bits to collect all the applications logs and write them to kafka for the first cloud",
    "start": "222799",
    "end": "230400"
  },
  {
    "text": "and stream them using flume and kinesis to an open hadoop cluster",
    "start": "230400",
    "end": "237840"
  },
  {
    "text": "within the hadoop cluster the first component that will receive the data is spark structure streaming",
    "start": "237840",
    "end": "244560"
  },
  {
    "text": "this one will take care of in hasting and also processing the information before dumping it into the hadoop file",
    "start": "244560",
    "end": "252080"
  },
  {
    "text": "system so once we have our information here we",
    "start": "252080",
    "end": "257759"
  },
  {
    "text": "want to access it so for that we could implement for example hive and presto",
    "start": "257759",
    "end": "264560"
  },
  {
    "text": "or instead of presto we could use impala breathe or any other technology for",
    "start": "264560",
    "end": "270560"
  },
  {
    "text": "interactive queries against hadoop if we are developing our own software to",
    "start": "270560",
    "end": "276240"
  },
  {
    "text": "visualize the information then we will probably have an api talking to the",
    "start": "276240",
    "end": "281520"
  },
  {
    "text": "presto coordinator and analyze front them so finally we have the management layer",
    "start": "281520",
    "end": "289040"
  },
  {
    "text": "here it's super common to find apache zookeeper to centralize the",
    "start": "289040",
    "end": "294080"
  },
  {
    "text": "configuration of all these components and also an administration tool like embody or a centralized log system for",
    "start": "294080",
    "end": "302080"
  },
  {
    "text": "cluster monitoring so this is an example of a real big data",
    "start": "302080",
    "end": "307520"
  },
  {
    "text": "architecture and how the components interact with each other so back to security how can we analyze",
    "start": "307520",
    "end": "314800"
  },
  {
    "text": "these complex infrastructures well i would like to propose a methodology for this where the analysis",
    "start": "314800",
    "end": "322240"
  },
  {
    "text": "is based on the different layers of the big data stack i think that a good way to analyze big",
    "start": "322240",
    "end": "328880"
  },
  {
    "text": "data infrastructure is to dissect them and analyze the security of the",
    "start": "328880",
    "end": "334000"
  },
  {
    "text": "components layer by layer so in this way we can make sure that we",
    "start": "334000",
    "end": "339120"
  },
  {
    "text": "are covering all the stations that the information we want to protect goes through",
    "start": "339120",
    "end": "345039"
  },
  {
    "text": "so from now on i will explain different attack vectors that i discovered throughout this research for each of the",
    "start": "345039",
    "end": "351840"
  },
  {
    "text": "layers good so let's start with the management layer",
    "start": "351840",
    "end": "358960"
  },
  {
    "text": "zookeeper as a set is widely used to centralize the configuration of the different technologies that make up the",
    "start": "358960",
    "end": "366160"
  },
  {
    "text": "cluster this architecture is pretty simple it runs a service on all nodes and then a",
    "start": "366160",
    "end": "373440"
  },
  {
    "text": "client let's say and cluster administrator can connect to one of the nodes and",
    "start": "373440",
    "end": "380000"
  },
  {
    "text": "update a configuration so when that happens zookeeper will automatically broadcast the change",
    "start": "380000",
    "end": "386080"
  },
  {
    "text": "across all the nodes if we scan a node of the cluster we will",
    "start": "386080",
    "end": "391680"
  },
  {
    "text": "find the ports 2181 and 3888 are open",
    "start": "391680",
    "end": "397840"
  },
  {
    "text": "this port belongs to zookeeper the port 2181 is the the port that",
    "start": "397840",
    "end": "403759"
  },
  {
    "text": "accepts connection from clients should we be able to connect to it",
    "start": "403759",
    "end": "409919"
  },
  {
    "text": "well according to the official documentation of fanberry a tool that is widely used for deploying on-prem big",
    "start": "409919",
    "end": "416720"
  },
  {
    "text": "data clusters the 751 is a requirement for installing",
    "start": "416720",
    "end": "422160"
  },
  {
    "text": "big data clusters so we can probably connect to it",
    "start": "422160",
    "end": "427440"
  },
  {
    "text": "how should we do it and we can download this zookeeper client from the official website and then it's",
    "start": "427440",
    "end": "434319"
  },
  {
    "text": "just about running this command specifying the node ip address and the 2181 port",
    "start": "434319",
    "end": "441440"
  },
  {
    "text": "so once with connect as we run the hub format there is a list of actions that we can",
    "start": "441440",
    "end": "447840"
  },
  {
    "text": "execute over the c nodes the signals or zookeeper nodes are the configurations",
    "start": "447840",
    "end": "453919"
  },
  {
    "text": "that zookeeper organizes in a hierarchical structure",
    "start": "453919",
    "end": "459759"
  },
  {
    "text": "so with the ls and get commands we can browse this hierarchy card structure",
    "start": "460560",
    "end": "466879"
  },
  {
    "text": "and we can actually find it very interesting information about the configuration of all the components that",
    "start": "466879",
    "end": "473599"
  },
  {
    "text": "make up the cluster like hadoop height 8 kafka or whatever",
    "start": "473599",
    "end": "479919"
  },
  {
    "text": "and we could use it for farted attacks we can also create new configurations",
    "start": "479919",
    "end": "486319"
  },
  {
    "text": "modify existing ones and delete configurations and this will be a problem for the",
    "start": "486319",
    "end": "492639"
  },
  {
    "text": "cluster some components might go down to keeper for example is commonly used",
    "start": "492639",
    "end": "499199"
  },
  {
    "text": "to manage the hadoop high availability so if we delete everything the cluster might run into troubles",
    "start": "499199",
    "end": "507039"
  },
  {
    "text": "so i wouldn't not do a demo of this attack because it's pretty simple but",
    "start": "507039",
    "end": "513039"
  },
  {
    "text": "this is quite impactful so what about ambari this is a pretty",
    "start": "513039",
    "end": "519120"
  },
  {
    "text": "popular open source tool to install and manage big data clusters",
    "start": "519120",
    "end": "526000"
  },
  {
    "text": "it has a dashboard from which you can control everything whose default credentials are admin",
    "start": "526880",
    "end": "533040"
  },
  {
    "text": "admin of course but but if they were changed there is a second word that is",
    "start": "533040",
    "end": "539839"
  },
  {
    "text": "absolutely worried to check embody uses a positive database to store",
    "start": "539839",
    "end": "545440"
  },
  {
    "text": "the statistics and information about the cluster and in the default installation process",
    "start": "545440",
    "end": "551200"
  },
  {
    "text": "the embody wizard asks you to change the credential for this dashboard but it",
    "start": "551200",
    "end": "556399"
  },
  {
    "text": "doesn't ask you to change the default credential for the database so we could simply connect to the",
    "start": "556399",
    "end": "563200"
  },
  {
    "text": "postgres port directly using these default credentials there",
    "start": "563200",
    "end": "568800"
  },
  {
    "text": "are user and body and password big data and explore the and varied database",
    "start": "568800",
    "end": "575680"
  },
  {
    "text": "and there we will find these two tables user authentications and users",
    "start": "575680",
    "end": "583200"
  },
  {
    "text": "so to get the username and authentication key at once",
    "start": "583200",
    "end": "588399"
  },
  {
    "text": "we need to do this in a showing query between those two tables so the authentication key is a status",
    "start": "588399",
    "end": "595839"
  },
  {
    "text": "hash so the best thing that we can do here is just update the key for the admin user",
    "start": "595839",
    "end": "601760"
  },
  {
    "text": "for example i dug into the embodied source code to find a valid saturn hash and here we",
    "start": "601760",
    "end": "608880"
  },
  {
    "text": "have the hash for the admin password so now we can run an update query",
    "start": "608880",
    "end": "615120"
  },
  {
    "text": "and once done we can log into the embedded dashboard with the admin admin credentials",
    "start": "615120",
    "end": "621120"
  },
  {
    "text": "i know this example pretty stupid but it's absolutely worth to check because",
    "start": "621120",
    "end": "626560"
  },
  {
    "text": "embody controls the whole cluster if you can access this dashboard you can do whatever you want over the customer",
    "start": "626560",
    "end": "633839"
  },
  {
    "text": "and as the default installation process doesn't ask for these credentials to be changed then you can most likely",
    "start": "633839",
    "end": "640720"
  },
  {
    "text": "compromise them in this way good so the important thing in the",
    "start": "640720",
    "end": "646000"
  },
  {
    "text": "cluster management layer is to analyze the security of the administration and monitoring tools",
    "start": "646000",
    "end": "652480"
  },
  {
    "text": "let's now talk about the storage layer first of all it's good to understand how",
    "start": "652480",
    "end": "659600"
  },
  {
    "text": "hadoop works it has a master slave architecture",
    "start": "659600",
    "end": "665200"
  },
  {
    "text": "and two main components the hdf that means the hadoop distributed file system",
    "start": "665200",
    "end": "671920"
  },
  {
    "text": "and yarn the adobe distributed file system has two main components",
    "start": "671920",
    "end": "677680"
  },
  {
    "text": "nano that saves the metadata of the files stored in the cluster and runs in",
    "start": "677680",
    "end": "683279"
  },
  {
    "text": "the master nodes and the data node that stores the actual data and runs in the slaves nodes",
    "start": "683279",
    "end": "691600"
  },
  {
    "text": "on the other hand yarn consists of two components as well the resource manager located on the",
    "start": "691600",
    "end": "698000"
  },
  {
    "text": "master nodes that controls all the processing resources in the hadoop cluster and the",
    "start": "698000",
    "end": "704000"
  },
  {
    "text": "no manager installing the place nodes that take care of tracking",
    "start": "704000",
    "end": "709279"
  },
  {
    "text": "processing resources on its slave node among other stacks",
    "start": "709279",
    "end": "714880"
  },
  {
    "text": "basically what we have to know is that the hadoop file system",
    "start": "714880",
    "end": "720399"
  },
  {
    "text": "um is where the cluster information is stored and yarn is a service that",
    "start": "720399",
    "end": "725920"
  },
  {
    "text": "manages the resources for the processing shops that are executed over the",
    "start": "725920",
    "end": "731279"
  },
  {
    "text": "information store basically is that so when it comes to the storage layer we",
    "start": "731279",
    "end": "738079"
  },
  {
    "text": "are interested in the hadoop file system so let's see how we can remotely compromise it",
    "start": "738079",
    "end": "746079"
  },
  {
    "text": "hadoop exposes an ipc port on 80 20.",
    "start": "746240",
    "end": "751279"
  },
  {
    "text": "so we shall find this port open in the hadoop name nodes",
    "start": "751279",
    "end": "756560"
  },
  {
    "text": "so if we can connect we could execute hadoop commands and access to the stored",
    "start": "756560",
    "end": "762320"
  },
  {
    "text": "data however this is not as simple as the zookeeper example was",
    "start": "762320",
    "end": "767839"
  },
  {
    "text": "managed to do this is a little more complex there are four configuration files that",
    "start": "767839",
    "end": "774320"
  },
  {
    "text": "hadoop needs to perform operations over a hadoop file system",
    "start": "774320",
    "end": "779519"
  },
  {
    "text": "if we take a look at these files inside the name node we can see that they have end of configuration",
    "start": "779519",
    "end": "786160"
  },
  {
    "text": "parameters so when i saw that i wonder if i am an attacker and i don't have access to",
    "start": "786160",
    "end": "793200"
  },
  {
    "text": "these files how can i compromise the file system in a remote way",
    "start": "793200",
    "end": "798320"
  },
  {
    "text": "well part of this research was to find among those dozens configurations which",
    "start": "798320",
    "end": "804240"
  },
  {
    "text": "ones are a hundred percent required and how we can get them remotely from the",
    "start": "804240",
    "end": "809680"
  },
  {
    "text": "information that hadoop itself discloses by default so i will explain how we can",
    "start": "809680",
    "end": "816000"
  },
  {
    "text": "manually craft these files one by one let's start by the coresight xml file",
    "start": "816000",
    "end": "825360"
  },
  {
    "text": "the only information we need here is the namespace",
    "start": "825680",
    "end": "831440"
  },
  {
    "text": "and it's pretty easy to to find this information because hadoop is posted by default dashboard on the name notes on",
    "start": "831440",
    "end": "838880"
  },
  {
    "text": "the board 50 2017 pretty high port and we can access it with our",
    "start": "838880",
    "end": "845360"
  },
  {
    "text": "authentication so here we can find the namespace side in my target cluster",
    "start": "845360",
    "end": "853120"
  },
  {
    "text": "and that's all we need for this file then we need to grab the hdf site file",
    "start": "853120",
    "end": "861120"
  },
  {
    "text": "and it's necessary to know the namespace that we already have from the previous file and we also need the names nodes",
    "start": "861120",
    "end": "868639"
  },
  {
    "text": "ids and the dns of them we could have one two or more name nodes",
    "start": "868639",
    "end": "874800"
  },
  {
    "text": "and we need to provide the id and dns for all of them in this file",
    "start": "874800",
    "end": "880560"
  },
  {
    "text": "where can we get this information from the same dashboard we have the namespace and the name of id",
    "start": "880560",
    "end": "889199"
  },
  {
    "text": "and the dns here we just need to access this dashboard on each name",
    "start": "889199",
    "end": "895360"
  },
  {
    "text": "and remember that it's on the port 50 070.",
    "start": "895360",
    "end": "901360"
  },
  {
    "text": "another alternative is to enter the data now dashboard it's on board 5075",
    "start": "901360",
    "end": "908600"
  },
  {
    "text": "and there we can see all the names nodes at once",
    "start": "909040",
    "end": "914160"
  },
  {
    "text": "good um the next file is the map right side one here we need the dns of the nano that",
    "start": "914160",
    "end": "921760"
  },
  {
    "text": "holds the mapreduce shop history we can try to access the port",
    "start": "921760",
    "end": "927440"
  },
  {
    "text": "1988 on the name node and if we can see this dashboard then that's the name that we",
    "start": "927440",
    "end": "933839"
  },
  {
    "text": "are looking for and we already know it's dns from the previous dashboard",
    "start": "933839",
    "end": "941399"
  },
  {
    "text": "finally we need to craft the yarn site file again we need a name of dns in this case",
    "start": "941519",
    "end": "947519"
  },
  {
    "text": "the one that holds the yarn resource manager and to detect that we can try to access",
    "start": "947519",
    "end": "954480"
  },
  {
    "text": "the port 8088 and if we see this dashboard then that's",
    "start": "954480",
    "end": "959920"
  },
  {
    "text": "the right now and here we can get the dns as well so",
    "start": "959920",
    "end": "965440"
  },
  {
    "text": "all these dashboards are exposed by default and don't require any authentication but if for some reasons",
    "start": "965440",
    "end": "972160"
  },
  {
    "text": "we can see them we can try to get this required information through zookeeper",
    "start": "972160",
    "end": "977199"
  },
  {
    "text": "with the attack i showed you earlier because zookeeper also has all this",
    "start": "977199",
    "end": "982959"
  },
  {
    "text": "information cool so once we have the configuration",
    "start": "982959",
    "end": "988160"
  },
  {
    "text": "files we need the next step is to install hadoop in our local machine and provide it with those files",
    "start": "988160",
    "end": "994800"
  },
  {
    "text": "to perform the remote communication as i didn't want to install hadoop on my",
    "start": "994800",
    "end": "1000079"
  },
  {
    "text": "local machine i built this docker file feel free to use it it's pretty comfortable",
    "start": "1000079",
    "end": "1006800"
  },
  {
    "text": "should change the hadoop version to match the version of your target cluster right",
    "start": "1006800",
    "end": "1013120"
  },
  {
    "text": "so from now on this is going to be our hadoop hacking container running on the attacker machine right",
    "start": "1013120",
    "end": "1022000"
  },
  {
    "text": "so let's run it and get that shell inside it and we can create a config directory to",
    "start": "1022240",
    "end": "1029839"
  },
  {
    "text": "place the xml files we have crafted before and you also need to copy this",
    "start": "1029839",
    "end": "1036600"
  },
  {
    "text": "lock.property files another thing i did is was to edit the",
    "start": "1036600",
    "end": "1044000"
  },
  {
    "text": "host file to rightly resolve this namenode dns you can use the ip addresses on the xml",
    "start": "1044000",
    "end": "1051520"
  },
  {
    "text": "files but i had better results doing this",
    "start": "1051520",
    "end": "1056880"
  },
  {
    "text": "okay so we are ready to go just pass to hadoop this config directory and execute an ls command for",
    "start": "1057440",
    "end": "1064960"
  },
  {
    "text": "example so voila we can see the entire hadoop file system from a remote",
    "start": "1064960",
    "end": "1071200"
  },
  {
    "text": "attacker machine so before jumping into a demo of this i would like to mention that most likely",
    "start": "1071200",
    "end": "1078320"
  },
  {
    "text": "we will need to impersonate hdf users for example if i try to create a new",
    "start": "1078320",
    "end": "1084960"
  },
  {
    "text": "directory using the root user i cannot do it so we need to impersonate",
    "start": "1084960",
    "end": "1091520"
  },
  {
    "text": "a user that has privileges within the hadoop file system so one of these ones",
    "start": "1091520",
    "end": "1098640"
  },
  {
    "text": "fortunately that's very easy to do we just need to set this environment variable",
    "start": "1098640",
    "end": "1104240"
  },
  {
    "text": "with the hadoop username before the command and that's all that will allow",
    "start": "1104240",
    "end": "1109760"
  },
  {
    "text": "us to for example create a new directory and we can also delete directories and files",
    "start": "1109760",
    "end": "1116240"
  },
  {
    "text": "of course and we could actually wipe out the entire cluster information",
    "start": "1116240",
    "end": "1123200"
  },
  {
    "text": "okay let's see a demo of this so here i am in my",
    "start": "1123200",
    "end": "1131039"
  },
  {
    "text": "container and these are my files this is the yeah hdl",
    "start": "1131039",
    "end": "1136960"
  },
  {
    "text": "files i have to place the the namespace and several places and in in this file",
    "start": "1136960",
    "end": "1143360"
  },
  {
    "text": "you need to also provide the name nodes ids and dns if you have for",
    "start": "1143360",
    "end": "1148880"
  },
  {
    "text": "example in this case i have two so i have to provide the dns for the two uh",
    "start": "1148880",
    "end": "1154320"
  },
  {
    "text": "name nodes and for the map red side file and we",
    "start": "1154320",
    "end": "1159840"
  },
  {
    "text": "also need to provide a dns for the map shop uh history and this is",
    "start": "1159840",
    "end": "1165600"
  },
  {
    "text": "the yarn file that also needs to provide the dns for the yar resource manager node",
    "start": "1165600",
    "end": "1172160"
  },
  {
    "text": "so with that we can execute commands hadoop commands to",
    "start": "1172160",
    "end": "1177679"
  },
  {
    "text": "browse through the hadoop file system and we execute the help command we can",
    "start": "1177679",
    "end": "1183840"
  },
  {
    "text": "see that there are a lot of common commands that we can execute like copy files or remove files",
    "start": "1183840",
    "end": "1192000"
  },
  {
    "text": "most common commands that we can find in a unix system and here while i'm making the",
    "start": "1192000",
    "end": "1197760"
  },
  {
    "text": "impersonation for the hdf user so i can create a new",
    "start": "1197760",
    "end": "1204840"
  },
  {
    "text": "directory and also i could delete the directory of",
    "start": "1204840",
    "end": "1210240"
  },
  {
    "text": "course so basically we can perform any operation over the hadoop file system in",
    "start": "1210240",
    "end": "1215840"
  },
  {
    "text": "a remote way because i'm executing it from a hadoop hacking container that has",
    "start": "1215840",
    "end": "1221360"
  },
  {
    "text": "nothing to do with the original cluster right good",
    "start": "1221360",
    "end": "1227200"
  },
  {
    "text": "okay so um let's jump into the processing layer and",
    "start": "1227200",
    "end": "1233120"
  },
  {
    "text": "see how we can abuse yarn in this case",
    "start": "1233120",
    "end": "1238000"
  },
  {
    "text": "so back to the hadoop architecture just to remember yar takes care of the processing shops",
    "start": "1238159",
    "end": "1245120"
  },
  {
    "text": "over the data stored in the cluster this shop executes code in the data",
    "start": "1245120",
    "end": "1250640"
  },
  {
    "text": "nodes so our mission here is trying to find a way to remotely submitting an",
    "start": "1250640",
    "end": "1255840"
  },
  {
    "text": "application to yarn that executes a code or command that we want to execute in",
    "start": "1255840",
    "end": "1261120"
  },
  {
    "text": "the clusters node basically achieve a remote code execution through vr",
    "start": "1261120",
    "end": "1268400"
  },
  {
    "text": "and we can use the hadoop ipc that we were using in the previous attack",
    "start": "1268400",
    "end": "1274400"
  },
  {
    "text": "it's just necessary to improve a little bit our yarn site file",
    "start": "1274400",
    "end": "1279919"
  },
  {
    "text": "we need to add the yarn application class file properly",
    "start": "1279919",
    "end": "1284960"
  },
  {
    "text": "this path used to be the default path in hadoop installations so it should not be",
    "start": "1284960",
    "end": "1290559"
  },
  {
    "text": "difficult to obtain this information in the example here we can see the",
    "start": "1290559",
    "end": "1296159"
  },
  {
    "text": "default path for installations using the hortonwork packages",
    "start": "1296159",
    "end": "1303440"
  },
  {
    "text": "then this other property is optional it will specify the application's output",
    "start": "1303440",
    "end": "1309039"
  },
  {
    "text": "parts in the hadoop file system it might be useful for us to easily find",
    "start": "1309039",
    "end": "1314480"
  },
  {
    "text": "the output of the remote code execution but it's not necessary and something i would like to mention",
    "start": "1314480",
    "end": "1320960"
  },
  {
    "text": "that i did inside before if you can access these panels that we",
    "start": "1320960",
    "end": "1326880"
  },
  {
    "text": "have seen under slash conf we can find all the configuration",
    "start": "1326880",
    "end": "1331919"
  },
  {
    "text": "parameters but you cannot choose download and use this file we still need to manually craft the",
    "start": "1331919",
    "end": "1338320"
  },
  {
    "text": "files the way we were doing it however if something is not working for",
    "start": "1338320",
    "end": "1343679"
  },
  {
    "text": "you here you might find what's missing here we can see that for example we can",
    "start": "1343679",
    "end": "1350720"
  },
  {
    "text": "get the yarn application class pads that we need",
    "start": "1350720",
    "end": "1355440"
  },
  {
    "text": "okay so now that we have improved our yarn site file and we can submit",
    "start": "1355919",
    "end": "1361679"
  },
  {
    "text": "applications to yarn the question is what applications should we submit",
    "start": "1361679",
    "end": "1367520"
  },
  {
    "text": "here hortonworks provides a simple one that is enough to achieve the remote",
    "start": "1367520",
    "end": "1372960"
  },
  {
    "text": "code execution that we want it has only three java files",
    "start": "1372960",
    "end": "1378320"
  },
  {
    "text": "because yara applications are developed in java but there are a lot of hadoop libraries",
    "start": "1378320",
    "end": "1384880"
  },
  {
    "text": "necessary to include and use so it might not be easy to develop a native vr",
    "start": "1384880",
    "end": "1390240"
  },
  {
    "text": "application but we can use this one for our purpose",
    "start": "1390240",
    "end": "1395280"
  },
  {
    "text": "it takes us parameter the command that we want to execute on the cluster and",
    "start": "1395280",
    "end": "1400559"
  },
  {
    "text": "the number of instances which is basically on how many nodes our command",
    "start": "1400559",
    "end": "1405840"
  },
  {
    "text": "will be executed so um we will clone this repository in",
    "start": "1405840",
    "end": "1412159"
  },
  {
    "text": "our help hacking container and proceed to compile this java application",
    "start": "1412159",
    "end": "1418240"
  },
  {
    "text": "we need to edit the poem xml file",
    "start": "1418240",
    "end": "1423440"
  },
  {
    "text": "and change the hadoop version to match the version of our target this is pretty",
    "start": "1423440",
    "end": "1429039"
  },
  {
    "text": "important otherwise this will not work so remember to match the version and one",
    "start": "1429039",
    "end": "1436799"
  },
  {
    "text": "stand we can compile the application using maven",
    "start": "1436799",
    "end": "1442240"
  },
  {
    "text": "and the next step is to copy the compile chart into the remote hadoop file system",
    "start": "1442880",
    "end": "1449120"
  },
  {
    "text": "we can do it using the copy from local hdf command",
    "start": "1449120",
    "end": "1455440"
  },
  {
    "text": "and now we are ready to go so in this way and we can submit the",
    "start": "1455440",
    "end": "1460480"
  },
  {
    "text": "application to yarn passing as parameter the command we want to execute and the number of instances",
    "start": "1460480",
    "end": "1469039"
  },
  {
    "text": "here an example i have executed the hostname command of a free node",
    "start": "1469039",
    "end": "1475760"
  },
  {
    "text": "and we are going to receive an application id it's important to take note of it",
    "start": "1475760",
    "end": "1481679"
  },
  {
    "text": "and it's even more important to get this finished status",
    "start": "1481679",
    "end": "1486880"
  },
  {
    "text": "and that means that our application was executed successfully",
    "start": "1486880",
    "end": "1492400"
  },
  {
    "text": "and now what where can we see the application output",
    "start": "1492400",
    "end": "1497600"
  },
  {
    "text": "well we can use this command passing the application id we got in the previous step",
    "start": "1497600",
    "end": "1503679"
  },
  {
    "text": "and the output is going to be something like this and as we have executed this command",
    "start": "1503679",
    "end": "1510080"
  },
  {
    "text": "over three nodes and we have three different outputs for the hostname command",
    "start": "1510080",
    "end": "1517200"
  },
  {
    "text": "of course we can change the hosting command for any other right",
    "start": "1517200",
    "end": "1522320"
  },
  {
    "text": "so let's see a demo of this here again i have my files and i have",
    "start": "1522320",
    "end": "1529520"
  },
  {
    "text": "performed a little modification over the er file right",
    "start": "1529520",
    "end": "1534799"
  },
  {
    "text": "and i have the simple er application this is the hortonworks example",
    "start": "1534799",
    "end": "1542000"
  },
  {
    "text": "and i already copied this file into the remote hadoop file system",
    "start": "1542000",
    "end": "1547120"
  },
  {
    "text": "so the only thing i have to do now is just to execute execute it",
    "start": "1547120",
    "end": "1553440"
  },
  {
    "text": "and for that we need to pass the local path for the application the command that we want to execute the",
    "start": "1553440",
    "end": "1560000"
  },
  {
    "text": "number of instances and the remote path of the chart",
    "start": "1560000",
    "end": "1565120"
  },
  {
    "text": "so once done we get the application id and the finish status",
    "start": "1565679",
    "end": "1573200"
  },
  {
    "text": "good so take note of that id in my case i need to perfora copy",
    "start": "1575919",
    "end": "1582880"
  },
  {
    "text": "just to allow channel to then um find the the output so i just copy the the output to a",
    "start": "1582880",
    "end": "1589760"
  },
  {
    "text": "different folder in the remote file system and now we can use the yarn command with",
    "start": "1589760",
    "end": "1595600"
  },
  {
    "text": "the application id to get the real output",
    "start": "1595600",
    "end": "1601039"
  },
  {
    "text": "so um this is the output and here we can see the output of the hostname command for",
    "start": "1602080",
    "end": "1608960"
  },
  {
    "text": "how to one the first node hadoop2 and hadoop3",
    "start": "1608960",
    "end": "1614320"
  },
  {
    "text": "because we have executed it over three data nodes",
    "start": "1614320",
    "end": "1619840"
  },
  {
    "text": "and let me show just one more output of an application i run before",
    "start": "1619840",
    "end": "1625279"
  },
  {
    "text": "so you can see for example i have run a command to",
    "start": "1625279",
    "end": "1631279"
  },
  {
    "text": "dump the information of a file in the head of machines in the",
    "start": "1631279",
    "end": "1636960"
  },
  {
    "text": "data nodes so here we can get for example the password files",
    "start": "1636960",
    "end": "1642398"
  },
  {
    "text": "good so basically we can achieve our remote code execution",
    "start": "1643360",
    "end": "1648399"
  },
  {
    "text": "using yarn the yarn will always be present in hadoop clusters",
    "start": "1648399",
    "end": "1655120"
  },
  {
    "text": "and if we want to modify that application yara application is quite",
    "start": "1655120",
    "end": "1661279"
  },
  {
    "text": "simple we could modify it to perhaps execute a more complex command",
    "start": "1661279",
    "end": "1667600"
  },
  {
    "text": "just keep in mind that any changes must be made in both the application master",
    "start": "1667600",
    "end": "1673120"
  },
  {
    "text": "file and the client file right um if we want to get something like a",
    "start": "1673120",
    "end": "1679520"
  },
  {
    "text": "reverse shell on the cluster node it's possible but keep in mind that this is a",
    "start": "1679520",
    "end": "1685200"
  },
  {
    "text": "sharp process that starts unfinished so we might need to use alternative like",
    "start": "1685200",
    "end": "1691279"
  },
  {
    "text": "backdoor in the contact with the yarn application for example so you could modify the application to run this",
    "start": "1691279",
    "end": "1698559"
  },
  {
    "text": "specific command and when you submit the application it will execute this vector for the",
    "start": "1698559",
    "end": "1705200"
  },
  {
    "text": "ground tab and then you will have a reverse shell in the data nodes",
    "start": "1705200",
    "end": "1711760"
  },
  {
    "text": "this is just an example right okay and i can't help but talk about",
    "start": "1711760",
    "end": "1717520"
  },
  {
    "text": "spark in this section spark is a super popular and well implemented technology for precision",
    "start": "1717520",
    "end": "1724159"
  },
  {
    "text": "data it's generally installed on top of hadoop and developers make data",
    "start": "1724159",
    "end": "1730559"
  },
  {
    "text": "processing applications for spark for example in python using pyspark because it's easier than",
    "start": "1730559",
    "end": "1737279"
  },
  {
    "text": "developing a native application for yeah and also spark has other advantages over",
    "start": "1737279",
    "end": "1743520"
  },
  {
    "text": "yarn as well so spark has its own ipc port on 1777",
    "start": "1743520",
    "end": "1751279"
  },
  {
    "text": "and we call submit and spark application to be executed on the cluster through this port",
    "start": "1751279",
    "end": "1757600"
  },
  {
    "text": "it's easier than with yang here we have an example and this small",
    "start": "1757600",
    "end": "1763279"
  },
  {
    "text": "code will connect to the spark master to execute the hostname command on the cluster's nodes",
    "start": "1763279",
    "end": "1770080"
  },
  {
    "text": "so we have to specify the remote spark ip address",
    "start": "1770080",
    "end": "1775360"
  },
  {
    "text": "our ip address to receive the output of the command and the command itself",
    "start": "1775360",
    "end": "1781760"
  },
  {
    "text": "then we can write this script from our machine and we don't really need anything else",
    "start": "1781760",
    "end": "1787600"
  },
  {
    "text": "it's pretty simple but i'm not going to talking therefore this because there is already attack 100",
    "start": "1787600",
    "end": "1795120"
  },
  {
    "text": "dedicated to spark that was given at defcon last year i truly recommend watching this talk and",
    "start": "1795120",
    "end": "1802880"
  },
  {
    "text": "the speaker explains how to achieve remote code execution via spark ipc",
    "start": "1802880",
    "end": "1810720"
  },
  {
    "text": "this is the the equivalent of what we did with jana so",
    "start": "1810720",
    "end": "1816080"
  },
  {
    "text": "keep in mind that spark may or may not be present in the cluster",
    "start": "1816080",
    "end": "1821679"
  },
  {
    "text": "while yarn will always be present in a hadoop installation so it's good to know",
    "start": "1821679",
    "end": "1827440"
  },
  {
    "text": "how to achieve remote code execution via and also be a spark if we have the possibility to abuse this",
    "start": "1827440",
    "end": "1834960"
  },
  {
    "text": "technology as well also",
    "start": "1834960",
    "end": "1840799"
  },
  {
    "text": "let's take a look at the injection layer if you remember from our big data",
    "start": "1840799",
    "end": "1848080"
  },
  {
    "text": "architecture example at the beginning of this talk we have",
    "start": "1848080",
    "end": "1854159"
  },
  {
    "text": "sources of data and such data is injected into our cluster using data intellection",
    "start": "1854159",
    "end": "1861440"
  },
  {
    "text": "technologies there are several ones um we have some design for streaming like",
    "start": "1861440",
    "end": "1869360"
  },
  {
    "text": "floom kafka and spark tractor streaming that is a variant of spark",
    "start": "1869360",
    "end": "1876000"
  },
  {
    "text": "and then others like scoop that enhances static information for example from one",
    "start": "1876000",
    "end": "1882000"
  },
  {
    "text": "data lake to other data lake or from one database to a data lake on so on",
    "start": "1882000",
    "end": "1890159"
  },
  {
    "text": "so from a security point of view we need to make sure that these channels that the",
    "start": "1890159",
    "end": "1896480"
  },
  {
    "text": "information go through from the source to the storage are secure right",
    "start": "1896480",
    "end": "1902159"
  },
  {
    "text": "otherwise an attacker might interfere those channels and inject malicious data",
    "start": "1902159",
    "end": "1908640"
  },
  {
    "text": "let's see how this will happen this is how spark streaming or spark",
    "start": "1908640",
    "end": "1915360"
  },
  {
    "text": "structure streaming works it's a variant of spark that injects the",
    "start": "1915360",
    "end": "1920480"
  },
  {
    "text": "data and also process it before dumping everything into the hadoop file system",
    "start": "1920480",
    "end": "1926640"
  },
  {
    "text": "so it's like having two components in one and",
    "start": "1926640",
    "end": "1932320"
  },
  {
    "text": "spark streaming um it works with technologies like kafka fluent and kinesis to put or receive the data",
    "start": "1932320",
    "end": "1940320"
  },
  {
    "text": "but it also has the possibility to just inject data from a tcp socket",
    "start": "1940320",
    "end": "1946320"
  },
  {
    "text": "and that could be very dangerous here we have an example of how the code",
    "start": "1946320",
    "end": "1952720"
  },
  {
    "text": "looks like when the streaming input is chosen tcp socket",
    "start": "1952720",
    "end": "1957760"
  },
  {
    "text": "it basically binds a board on the machine so abuse this is super easy we can use",
    "start": "1957760",
    "end": "1964640"
  },
  {
    "text": "netcat or our favorite tool and should send data over the socket",
    "start": "1964640",
    "end": "1970960"
  },
  {
    "text": "and it works but what happens to the data we inject will depend on the application that",
    "start": "1970960",
    "end": "1978000"
  },
  {
    "text": "processes it most likely we will crash the application because we might be",
    "start": "1978000",
    "end": "1983279"
  },
  {
    "text": "injecting bytes that the application doesn't know how to handle",
    "start": "1983279",
    "end": "1988320"
  },
  {
    "text": "or our bite may end up inside the halo 5 system that's also likely",
    "start": "1988320",
    "end": "1995440"
  },
  {
    "text": "so it's important to check uh that the interfaces that are waiting for data to be inherited and cannot be reached by an",
    "start": "1995440",
    "end": "2003039"
  },
  {
    "text": "attacker right and regarding scope",
    "start": "2003039",
    "end": "2008240"
  },
  {
    "text": "as i said this moves static data and it's commonly used to inject information",
    "start": "2008240",
    "end": "2013279"
  },
  {
    "text": "from different sql database into halo and analyzing scope server iphone and",
    "start": "2013279",
    "end": "2020799"
  },
  {
    "text": "api exposed by default on pro 12000",
    "start": "2020799",
    "end": "2026799"
  },
  {
    "text": "and we can actually abuse it and we can get for example the",
    "start": "2026799",
    "end": "2032960"
  },
  {
    "text": "scoop server version with this query and then it's easier to use the scoop",
    "start": "2032960",
    "end": "2038640"
  },
  {
    "text": "client to interact with the api because it is actually there is not so much",
    "start": "2038640",
    "end": "2044000"
  },
  {
    "text": "documentation that is is quite complicated to use and setup it took a lot of effort for me to",
    "start": "2044000",
    "end": "2050638"
  },
  {
    "text": "actually abuse this api so i recommend to use the uh the client",
    "start": "2050639",
    "end": "2055919"
  },
  {
    "text": "and something important is to download the same client version as the server",
    "start": "2055919",
    "end": "2061200"
  },
  {
    "text": "for example this server is 1.99.7",
    "start": "2061200",
    "end": "2066040"
  },
  {
    "text": "so then we should download the version of the client the same version of the client from this riser",
    "start": "2066960",
    "end": "2075240"
  },
  {
    "text": "so what can we do well we call for example in just malicious data from a",
    "start": "2077599",
    "end": "2083280"
  },
  {
    "text": "database that belongs to the attacker into the hadoop file system",
    "start": "2083280",
    "end": "2088560"
  },
  {
    "text": "it takes some steps we have to connect to the remote scope server create some links this is provided scope",
    "start": "2088560",
    "end": "2096320"
  },
  {
    "text": "with the information to connect to the malicious database and also to connect to the target hadoop file system",
    "start": "2096320",
    "end": "2103760"
  },
  {
    "text": "and then we have to create a scope shop specifying that we want to inject data",
    "start": "2103760",
    "end": "2108880"
  },
  {
    "text": "from this database link to this other hdf link and store it",
    "start": "2108880",
    "end": "2115760"
  },
  {
    "text": "this is easier to understand with the demo so let's take a look at a video",
    "start": "2115760",
    "end": "2122400"
  },
  {
    "text": "here i have my scoop client and i have connected successfully to the remote scope server",
    "start": "2122880",
    "end": "2129119"
  },
  {
    "text": "and these are the connectors that we have available so we need to create a",
    "start": "2129119",
    "end": "2134400"
  },
  {
    "text": "link for the um my sql connection",
    "start": "2134400",
    "end": "2140160"
  },
  {
    "text": "so i will use my sql driver and i will specify the ap address of the attacker",
    "start": "2140160",
    "end": "2145359"
  },
  {
    "text": "machine that has the mysql database",
    "start": "2145359",
    "end": "2150640"
  },
  {
    "text": "which i have of course to provide the credential and it has the malicious data that i",
    "start": "2150640",
    "end": "2155920"
  },
  {
    "text": "want to inject into the cluster right and then we need to create a link for",
    "start": "2155920",
    "end": "2160960"
  },
  {
    "text": "the hdf and here we have to specify the ip address of the name node",
    "start": "2160960",
    "end": "2166480"
  },
  {
    "text": "and the import of the i the hadoop ipc in this case it's in the port 9000 but",
    "start": "2166480",
    "end": "2172560"
  },
  {
    "text": "it used to be also in 80 20 as we saw before and this conf directory is",
    "start": "2172560",
    "end": "2180400"
  },
  {
    "text": "it's going to be the default it's a remote directory and it's going to be the default parts of hadoop installation",
    "start": "2180400",
    "end": "2187040"
  },
  {
    "text": "in this case just providing a part of a vex machine but it's going to be",
    "start": "2187040",
    "end": "2192720"
  },
  {
    "text": "the default path for hadoop so",
    "start": "2192720",
    "end": "2198000"
  },
  {
    "text": "we have created the links we need and now we have to create the scoop shop",
    "start": "2198000",
    "end": "2203440"
  },
  {
    "text": "so in the shop we specify that we want to move information from this attacker",
    "start": "2203440",
    "end": "2210079"
  },
  {
    "text": "mysql database to the target httf",
    "start": "2210079",
    "end": "2215280"
  },
  {
    "text": "basically the target hadoop files it so here we create the shell we specify",
    "start": "2215280",
    "end": "2222320"
  },
  {
    "text": "the database and the table from this mysql",
    "start": "2222320",
    "end": "2227520"
  },
  {
    "text": "database and there are many values that don't actually need",
    "start": "2227520",
    "end": "2232800"
  },
  {
    "text": "any any value they can be blank and the output directory is the output",
    "start": "2232800",
    "end": "2238560"
  },
  {
    "text": "directory in the remote hadoop file system so that is important",
    "start": "2238560",
    "end": "2244000"
  },
  {
    "text": "and then the other most of the other are optional optional parameters",
    "start": "2244000",
    "end": "2249920"
  },
  {
    "text": "so here we have our manisha shop and then she's about to start it",
    "start": "2249920",
    "end": "2255200"
  },
  {
    "text": "and that's all we can see from the client we see that okay it's what's rooting and that's all",
    "start": "2255200",
    "end": "2261839"
  },
  {
    "text": "so on the other console i will just log into the machine that has the hadoop file system just to show you that",
    "start": "2261839",
    "end": "2268640"
  },
  {
    "text": "the data was actually injected here we can see that i have two files",
    "start": "2268640",
    "end": "2274720"
  },
  {
    "text": "one say hacking scope the other say hello blah blah blah that was the content of the",
    "start": "2274720",
    "end": "2282480"
  },
  {
    "text": "malicious mysql database that was injected into the hadoop file system and keep in mind that you can ingest",
    "start": "2282480",
    "end": "2289440"
  },
  {
    "text": "data but you can also export because how sculptory allows you to",
    "start": "2289440",
    "end": "2295359"
  },
  {
    "text": "import and export data so you can do this this connection but in the reverse way i just specify that you want",
    "start": "2295359",
    "end": "2302640"
  },
  {
    "text": "to export data from the htf to the mysql database for example so you can",
    "start": "2302640",
    "end": "2309520"
  },
  {
    "text": "either inject malicious data or steal the data of the cluster",
    "start": "2309520",
    "end": "2314800"
  },
  {
    "text": "good so finally let's talk a little bit about the data access layer",
    "start": "2314800",
    "end": "2322000"
  },
  {
    "text": "back to our architecture example we saw that it's possible to use different technologies for data access",
    "start": "2323760",
    "end": "2330800"
  },
  {
    "text": "in this example we are using preston together with high but there are many others and when it",
    "start": "2330800",
    "end": "2338000"
  },
  {
    "text": "comes to hive and hbase these are",
    "start": "2338000",
    "end": "2344000"
  },
  {
    "text": "httf based storage technologies but they also provide interfaces to",
    "start": "2344000",
    "end": "2349440"
  },
  {
    "text": "access the information so for example preston needs the high metastore to carry the information",
    "start": "2349440",
    "end": "2355760"
  },
  {
    "text": "stored in the hadoop file system so these technologies dispose dashboards",
    "start": "2355760",
    "end": "2361599"
  },
  {
    "text": "and interfaces that can be abused by an attacker if they are not rightly protected",
    "start": "2361599",
    "end": "2368839"
  },
  {
    "text": "hive for example exposes a dashboard on port 1002 where we can get interesting",
    "start": "2371040",
    "end": "2377119"
  },
  {
    "text": "information and also an idea of how the data is structured in the storage",
    "start": "2377119",
    "end": "2383839"
  },
  {
    "text": "the same for eight ways and regarding presto i found this qgis login form where",
    "start": "2383839",
    "end": "2390560"
  },
  {
    "text": "password is not allowed it's quite curious because it's a login form but you cannot enter a password",
    "start": "2390560",
    "end": "2397760"
  },
  {
    "text": "so i i know that you can set up one but by default seems to be this way so you",
    "start": "2397760",
    "end": "2403359"
  },
  {
    "text": "can write admin user and enter and there is a dashboard that",
    "start": "2403359",
    "end": "2409200"
  },
  {
    "text": "shows some information about the interactive queries being executed against the cluster",
    "start": "2409200",
    "end": "2416640"
  },
  {
    "text": "so um as i said these technologies both several interfaces is common to find at",
    "start": "2417680",
    "end": "2423760"
  },
  {
    "text": "least a shade dbc one for example and in hive we can find it",
    "start": "2423760",
    "end": "2429599"
  },
  {
    "text": "on on port 1000 and there are different clients that we",
    "start": "2429599",
    "end": "2435200"
  },
  {
    "text": "can use to connect to it like squirrel or even hadoop includes a beer line",
    "start": "2435200",
    "end": "2441680"
  },
  {
    "text": "and we can connect choose specifying the remote address if not the question is required",
    "start": "2441680",
    "end": "2448480"
  },
  {
    "text": "there is usually nothing by the phone so hi has its",
    "start": "2448480",
    "end": "2453680"
  },
  {
    "text": "own comments we need to know then to browse the information for example with",
    "start": "2453680",
    "end": "2459440"
  },
  {
    "text": "show databases we can see the databases on the cluster",
    "start": "2459440",
    "end": "2464960"
  },
  {
    "text": "select one and show its tables",
    "start": "2464960",
    "end": "2469838"
  },
  {
    "text": "and then we have sentences uh to insert update delete",
    "start": "2470480",
    "end": "2475839"
  },
  {
    "text": "just like any other sql database so um",
    "start": "2475839",
    "end": "2482560"
  },
  {
    "text": "i'm running out of time so some recommendations as conclusion",
    "start": "2482560",
    "end": "2487760"
  },
  {
    "text": "um many attacks that we saw throughout this talk were based on spouses interfaces",
    "start": "2487760",
    "end": "2493920"
  },
  {
    "text": "and there are many dashboards that are supposed by default as well so if they are not being used we should",
    "start": "2493920",
    "end": "2500560"
  },
  {
    "text": "either remove them or block the access to them using a firewall for example",
    "start": "2500560",
    "end": "2507040"
  },
  {
    "text": "and if some components need to talk with each other without a firewall in the middle",
    "start": "2507040",
    "end": "2512640"
  },
  {
    "text": "then we should secure the perimeter at least the firewall has to be present despite",
    "start": "2512640",
    "end": "2518400"
  },
  {
    "text": "the official documentation asked for disabling it i believe that we can investigate what port",
    "start": "2518400",
    "end": "2525200"
  },
  {
    "text": "need to be allowed in our infrastructure and design a good firewall policy and rules",
    "start": "2525200",
    "end": "2532240"
  },
  {
    "text": "so remember also to change all the four credentials and implement any kind of",
    "start": "2532240",
    "end": "2537440"
  },
  {
    "text": "authentication in all the technologies being used hadoop for example supports",
    "start": "2537440",
    "end": "2543040"
  },
  {
    "text": "authentication for the httf um it's possible to implement",
    "start": "2543040",
    "end": "2549200"
  },
  {
    "text": "authentication authorization in most of the technologies that we have seen",
    "start": "2549200",
    "end": "2554319"
  },
  {
    "text": "but we have to do it by default there is nothing implemented",
    "start": "2554319",
    "end": "2559520"
  },
  {
    "text": "and finally remember that in big data infrastructures there are many different",
    "start": "2559520",
    "end": "2564560"
  },
  {
    "text": "technologies communicating with each other so make sure that those communications are happening in a secure way",
    "start": "2564560",
    "end": "2573440"
  },
  {
    "text": "so in the next weeks i hope i will be publishing more resources about the",
    "start": "2573599",
    "end": "2579040"
  },
  {
    "text": "practical implementation of security measures so for today that's all",
    "start": "2579040",
    "end": "2585119"
  },
  {
    "text": "thank you for watching my talk and here's my contact info in case you have any question please feel free to reach",
    "start": "2585119",
    "end": "2592319"
  },
  {
    "text": "me out thank you bye",
    "start": "2592319",
    "end": "2596318"
  }
]