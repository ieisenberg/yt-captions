[
  {
    "text": "all right here we go thanks Dalton on this exciting article this is going to be a big article so I hope everyone sat",
    "start": "40",
    "end": "6000"
  },
  {
    "text": "down and strapped in apparently it's a 62 minute read okay but I you know me I",
    "start": "6000",
    "end": "12120"
  },
  {
    "text": "I hey you know me I like I like them data structures and algorithms so this makes me very excited this makes me very",
    "start": "12120",
    "end": "19320"
  },
  {
    "text": "just hot and bothered and all those things altogether all right in this post we'll Implement a static search tree an",
    "start": "19320",
    "end": "26080"
  },
  {
    "text": "s+ tree I've never even heard of one of these okay I'm all about hey you know me I could I I could binary I could",
    "start": "26080",
    "end": "32758"
  },
  {
    "text": "Implement binary search on an array in five minutes that quick I I'm that",
    "start": "32759",
    "end": "38440"
  },
  {
    "text": "confident I know the binary searches so this I this seems very exciting I just don't understand how a static search",
    "start": "38440",
    "end": "43840"
  },
  {
    "text": "tree ever beats a binary search array all right uh for high throughput searching of sorting data as introduced",
    "start": "43840",
    "end": "50360"
  },
  {
    "text": "by algorithmica uh will mostly take the code presented there as a starting point",
    "start": "50360",
    "end": "55840"
  },
  {
    "text": "and optimize it to its limits for a large part I'm simply taking future work ideas of that post and implementing them",
    "start": "55840",
    "end": "62359"
  },
  {
    "text": "and then there will be a bunch of looking at assembly code to shave off all the instructions we can no okay so",
    "start": "62359",
    "end": "68799"
  },
  {
    "text": "by the way when they say the phrase premature optimization is the root of all evil this is what is meant by that",
    "start": "68799",
    "end": "75119"
  },
  {
    "text": "phrase right there that phrase right there okay you do not need to shave off assembly instructions until you're at",
    "start": "75119",
    "end": "82040"
  },
  {
    "text": "the end okay now since he's just at the very very end yes one would one would",
    "start": "82040",
    "end": "88000"
  },
  {
    "text": "assume one would assume that it's okay okay uh lastly there will be one big addition to optimize uh throughput",
    "start": "88000",
    "end": "93720"
  },
  {
    "text": "batching all source code including benchmarks and plotting is coded at right here Ragnar Groot uh core Camp",
    "start": "93720",
    "end": "101200"
  },
  {
    "text": "suffix array searching damn okay all right problem set input a sorted list of n 32-bit unsigned integers vs VEC oh",
    "start": "101200",
    "end": "109560"
  },
  {
    "text": "gosh bros we're already in we're already rusting we're already rusted out of our mind at this moment can you believe this",
    "start": "109560",
    "end": "116600"
  },
  {
    "text": "we've already somehow Rewritten it in Rust and we haven't even started we hav't even written and we're already in",
    "start": "116600",
    "end": "123799"
  },
  {
    "text": "Rust output a data structure that supports qu uh query's Q returning the smallest element of vows that is at",
    "start": "123799",
    "end": "131200"
  },
  {
    "text": "least Q or u32 max if no such element exists oh interesting okay so you're",
    "start": "131200",
    "end": "136720"
  },
  {
    "text": "going to go with the most max value of no element okay that's a weird Sentinel value to return oh I guess if you do",
    "start": "136720",
    "end": "142800"
  },
  {
    "text": "unsigned because normally what you do is for signed you do uh you do signed integers and then you do negative one",
    "start": "142800",
    "end": "148640"
  },
  {
    "text": "for for index up you know like index of it's not there okay whatever that's fine metric we optimize throughput okay",
    "start": "148640",
    "end": "156400"
  },
  {
    "text": "pinteresting that is the number of independent queries that can be answered per second oh interesting so it's not",
    "start": "156400",
    "end": "163040"
  },
  {
    "text": "necessarily latency of query it's throughput of query are we gaming it",
    "start": "163040",
    "end": "169200"
  },
  {
    "text": "boys boys boys are we gaming it I just turn see I just turned into Randy again I can't help it once we start talking",
    "start": "169200",
    "end": "175720"
  },
  {
    "text": "about Randy I start turning into Randy uh the typical case is where we have have sufficiently long queries as input",
    "start": "175720",
    "end": "182519"
  },
  {
    "text": "and return corresponding answers okay note that we will report reciprocal throughput as NS squer or just NS is",
    "start": "182519",
    "end": "191480"
  },
  {
    "text": "that nanc per query uh instead of queries per second you can think of the amortised not average time spent per",
    "start": "191480",
    "end": "198080"
  },
  {
    "text": "query okay okay I think I I think I see what's happening so in other words because he there's some there's some",
    "start": "198080",
    "end": "203599"
  },
  {
    "text": "things that are kind of given away so far in this article right right off the rip and so one thing they said is that throughput so throughput you can measure",
    "start": "203599",
    "end": "209959"
  },
  {
    "text": "over a long period of time meaning that if you have some sort of a query that takes a long time for a moment it",
    "start": "209959",
    "end": "215879"
  },
  {
    "text": "doesn't matter you don't you kind of get rid of all that instead you just say hey we were able to do say three per second",
    "start": "215879",
    "end": "223200"
  },
  {
    "text": "instead of what we expected to be say 2.5 or some something like that right and so there we go so even if there's",
    "start": "223200",
    "end": "228360"
  },
  {
    "text": "some sort of large moment it doesn't matter because it's just about throughput and they also said something about batching so my guess is that by",
    "start": "228360",
    "end": "234920"
  },
  {
    "text": "batching you have say a bunch of queries coming in right here and then you batch all together and do it right here so",
    "start": "234920",
    "end": "241400"
  },
  {
    "text": "even though this one had to say wait the longest this one right here had to wait the longest it doesn't matter about its",
    "start": "241400",
    "end": "247400"
  },
  {
    "text": "latency it just matters that you were able to do this many in a second versus something else that's what that's what I",
    "start": "247400",
    "end": "253000"
  },
  {
    "text": "assume is going on here so forgive me if I am wrong here note that we'll be using let's see hold on uh benchmarking set up",
    "start": "253000",
    "end": "259720"
  },
  {
    "text": "for now we will assume that both the input and queries are simple uniform random sampled 31 bit integers why 31",
    "start": "259720",
    "end": "266800"
  },
  {
    "text": "you're already using U 32s um in code uh in code this can be modeled like this uh",
    "start": "266800",
    "end": "272600"
  },
  {
    "text": "trait search index oh damn we're already in traits uh search index query one self query 32 self query query this thing",
    "start": "272600",
    "end": "280479"
  },
  {
    "text": "then we have a query querious Betsy with a slice of 32s queries it map Q self",
    "start": "280479",
    "end": "286880"
  },
  {
    "text": "self collect okay hold I'm trying I'm trying to understand this here for a second in terms of yeah yeah okay two",
    "start": "286880",
    "end": "292840"
  },
  {
    "text": "functions with default implementations in terms of each other I'm very confused by what's happening here because query",
    "start": "292840",
    "end": "298080"
  },
  {
    "text": "one calls self self query self query calls queries iterator which Maps over",
    "start": "298080",
    "end": "305400"
  },
  {
    "text": "the queries and then calls query one doesn't that just keep on going am I am I stupid or does that just keep on going",
    "start": "305400",
    "end": "311560"
  },
  {
    "text": "already lost brother brother I have already lost nine lines of rust code already don't know what is going on it",
    "start": "311560",
    "end": "318639"
  },
  {
    "text": "happens it happens I'm a little bit confused as to what's going on here but that seems exciting because it seems",
    "start": "318639",
    "end": "325039"
  },
  {
    "text": "like you take this and you just keep on calling itself over and over and over again um",
    "start": "325039",
    "end": "330160"
  },
  {
    "text": "so all right motivation uh lost on is 9 lost on Isis 9 dude just let's just say",
    "start": "330160",
    "end": "335680"
  },
  {
    "text": "because what I see here so if you call query one and you pass in say 69 it will take 69 and turn it into a slice and",
    "start": "335680",
    "end": "342800"
  },
  {
    "text": "query upon that slice and then grab out the first answer upon that slice it's going to go iterate across this slice of",
    "start": "342800",
    "end": "348479"
  },
  {
    "text": "one it's going to map that slice of one with a reference or effectively D referencing whatever this value is and",
    "start": "348479",
    "end": "355600"
  },
  {
    "text": "then it's going to call itself again with that one which that one will then turn into a slice of one which will then call into query which query will then",
    "start": "355600",
    "end": "361080"
  },
  {
    "text": "map over which will then call itself as one and then it'll call back in here and it'll call back in okay whatever it does",
    "start": "361080",
    "end": "366400"
  },
  {
    "text": "not matter I uh aside from doing this project just for fun of it there is some higher goal",
    "start": "366400",
    "end": "375000"
  },
  {
    "text": "what one of the big goals of bioinformatics is to make efficient data structures to index DNA say a single",
    "start": "375000",
    "end": "381440"
  },
  {
    "text": "Human Genome three billion base pair characters or even a bunch of them one such data structure is a suffix array",
    "start": "381440",
    "end": "388680"
  },
  {
    "text": "that sorts the suffixes of the input string classically one can find the locations where a string occurs by",
    "start": "388680",
    "end": "395280"
  },
  {
    "text": "binary searching the suffix array this project is the first step towards speeding up the suffix array search also",
    "start": "395280",
    "end": "402360"
  },
  {
    "text": "note that we indeed assume that the input data is static since usually we",
    "start": "402360",
    "end": "407440"
  },
  {
    "text": "let's see since usually we use a fixed reference genome okay I mean I think that's pretty fair uh static seems",
    "start": "407440",
    "end": "414599"
  },
  {
    "text": "pretty fair that's again this is why I'm so confused by this because wouldn't searching over an array better than",
    "start": "414599",
    "end": "420160"
  },
  {
    "text": "anything to do with a tree unless if the tree is stored in the array man I'm yeah yeah okay like a heap okay because if",
    "start": "420160",
    "end": "427120"
  },
  {
    "text": "you do if you do it if you don't do that right because if you do it in Array you get this magical ability to have all of your memory like in one big old",
    "start": "427120",
    "end": "433520"
  },
  {
    "text": "contiguous chunk whereas if you do it in Array if you do do it as in a tree then you get like this whole heap problem",
    "start": "433520",
    "end": "439280"
  },
  {
    "text": "right and practically speaking this tends to be a lot faster in a lot of cases than this but I guess there is",
    "start": "439280",
    "end": "445479"
  },
  {
    "text": "probably there probably does come a point where this is no longer faster and this is faster under some set of things",
    "start": "445479",
    "end": "451919"
  },
  {
    "text": "uh we'll see all right anyways the classical solution to this problem is a binary search we will briefly visit the",
    "start": "451919",
    "end": "458240"
  },
  {
    "text": "uh in the next section a great paper on this and search uh layouts is array layouts for comparison-based searching",
    "start": "458240",
    "end": "465840"
  },
  {
    "text": "by is that uh Kong and Moren okay cool this post will focus on the s+ trees as",
    "start": "465840",
    "end": "472599"
  },
  {
    "text": "introduced by algorithmica in the follow post static be trees in the rest of my time I'll will mostly assume that you're",
    "start": "472599",
    "end": "477960"
  },
  {
    "text": "familiar with that post I think we're mostly familiar for those that don't know what a static B tree is or I mean I don't know what a static B tree is I",
    "start": "477960",
    "end": "483879"
  },
  {
    "text": "assume it's just a b tree that you can't do inserts or removes but if you don't know what a be tree is a be tree is really really simple so let's just",
    "start": "483879",
    "end": "490319"
  },
  {
    "text": "pretend it's just an Nary search tree instead of B of a binary search tree it's an Nary search tree so let's just",
    "start": "490319",
    "end": "496039"
  },
  {
    "text": "say you have the following let's just say you have 3 17 69 nice that means you",
    "start": "496039",
    "end": "503639"
  },
  {
    "text": "have three slots and that means you could have over here you have up to four children and so",
    "start": "503639",
    "end": "509039"
  },
  {
    "text": "all of the elements are less than three all of these elements are greater than",
    "start": "509039",
    "end": "514320"
  },
  {
    "text": "or equal to three and less than 17 all of these elements are less than SE or less than or greater than or equal to 17",
    "start": "514320",
    "end": "522279"
  },
  {
    "text": "and less than 69 and all of these elements are greater than or equal to 69 right so this is basically an an a",
    "start": "522279",
    "end": "529720"
  },
  {
    "text": "search tree so if I had the answer uh 68 if I wanted to see does this exist",
    "start": "529720",
    "end": "537279"
  },
  {
    "text": "within this uh this Nary search tree I'd go right here and I'd go okay I'm",
    "start": "537279",
    "end": "544440"
  },
  {
    "text": "greater than 17 but I am less than 69 so therefore if it exists within this tree",
    "start": "544440",
    "end": "550120"
  },
  {
    "text": "it's going to be right here there you go so that's like the basics of A B tree uh now it now implemented in the HTML",
    "start": "550120",
    "end": "556920"
  },
  {
    "text": "programming language that's my favorite language by the way uh let's see also helpful is the Intel intrinsic guide",
    "start": "556920",
    "end": "563480"
  },
  {
    "text": "when looking into simd instructions note that we'll be only using avx2 instructions here as in we're uming",
    "start": "563480",
    "end": "569680"
  },
  {
    "text": "Intel and we're not assuming less uh available AVX 512 instructions in particular since my laptop doesn't have",
    "start": "569680",
    "end": "576240"
  },
  {
    "text": "them dang we should probably get Casey in at some point to explain these things again all right let's see binary search",
    "start": "576240",
    "end": "583000"
  },
  {
    "text": "and uh ID singer layout oo a baseline will use uh the rust standard Library",
    "start": "583000",
    "end": "588519"
  },
  {
    "text": "binary search implementation sorted VC sorted VC binary search IDs equals binary let's see binary search Q unwrap",
    "start": "588519",
    "end": "595200"
  },
  {
    "text": "or else this one vals this guy okay cool I like it I like it the main conclusion",
    "start": "595200",
    "end": "601839"
  },
  {
    "text": "of the lay let's see the array layouts paper uh is that I singer layout is one of the best in practice this layout",
    "start": "601839",
    "end": "608040"
  },
  {
    "text": "reorders the values in memory the binary search effectively is a binary search tree on the data with a let's see with",
    "start": "608040",
    "end": "615160"
  },
  {
    "text": "as root the middle node then the nodes position this one right here I'm confused what what does it mean uh that",
    "start": "615160",
    "end": "621079"
  },
  {
    "text": "it's that it reorders them isn't that just a sorted array the main benefit let's see what is the I guess I don't",
    "start": "621079",
    "end": "627240"
  },
  {
    "text": "know the iing or layout and I I don't want to go through that paper just to figure it out um see hold on the binary",
    "start": "627240",
    "end": "634360"
  },
  {
    "text": "search uh effectively is a binary search treat on the data yeah isn't that just a binary search array is that is that all",
    "start": "634360",
    "end": "640920"
  },
  {
    "text": "we're saying binary search array uh let's see the main benefit of the I singer layout is that all values needed for the first steps of the binary search",
    "start": "640920",
    "end": "647839"
  },
  {
    "text": "are close together oh it does some sort of unique grouping",
    "start": "647839",
    "end": "654519"
  },
  {
    "text": "with let's see with the root as the middle node oh is the root the first node oh interesting",
    "start": "654519",
    "end": "659880"
  },
  {
    "text": "okay okay okay we can uh put the root at index one and have two children of the",
    "start": "659880",
    "end": "666000"
  },
  {
    "text": "note at index I are uh 2i and 2 i+ 1 this means that we can effectively prefetch the next cach line before",
    "start": "666000",
    "end": "672480"
  },
  {
    "text": "knowing whether we need index 2 or 2 i+ 1 this can be taken a step further and prefetch the cach line uh containing",
    "start": "672480",
    "end": "679040"
  },
  {
    "text": "indices 69 to 69 + 15 which are exactly the values we needed for four iterations from now for a large part this can quite",
    "start": "679040",
    "end": "686079"
  },
  {
    "text": "effectively hide the latency associated with traversal of a tree nice okay oh very exciting oh very exciting L's the",
    "start": "686079",
    "end": "694000"
  },
  {
    "text": "number of levels ahead to preit oh my gosh oh",
    "start": "694000",
    "end": "699079"
  },
  {
    "text": "my oh my gosh what is this uh okay so we got a constant size",
    "start": "699079",
    "end": "705880"
  },
  {
    "text": "we got one of those like those con siiz generics right um so while while we can",
    "start": "705880",
    "end": "712480"
  },
  {
    "text": "left uh while we can uh shift one by L multiply by the index is less than this",
    "start": "712480",
    "end": "717880"
  },
  {
    "text": "values then we go in here holy cow I really don't understand this thing I really don't understand what prefetch",
    "start": "717880",
    "end": "724240"
  },
  {
    "text": "this index even means okay so this is just its position this is just its positional offset right so all you're",
    "start": "724240",
    "end": "730120"
  },
  {
    "text": "doing right here is you're just you're just Gathering uh I I I assume prefit index just simply accesses",
    "start": "730120",
    "end": "737079"
  },
  {
    "text": "it we don't have we don't have that defined but I assume it just simply just it just simply reads the value once all",
    "start": "737079",
    "end": "744399"
  },
  {
    "text": "right let's see the let's see the last few iterations don't need pre-etching anymore while index is less than this uh less than this",
    "start": "744399",
    "end": "750079"
  },
  {
    "text": "plus Q is greater than this length get as you size huh okay zeros trailing ones",
    "start": "750079",
    "end": "756600"
  },
  {
    "text": "plus one index zeros get index yeah I really I I'm not sure if I I I actually",
    "start": "756600",
    "end": "762440"
  },
  {
    "text": "honestly I don't understand what's happening here I'm trying to understand what this is doing but it's doing",
    "start": "762440",
    "end": "768199"
  },
  {
    "text": "something to the index to effectively get it into some sort of value and then does this trailing On's business which",
    "start": "768199",
    "end": "773760"
  },
  {
    "text": "I'm a little bit confused by and then sets that L size is to make a generic over a raise yes I I know that",
    "start": "773760",
    "end": "779959"
  },
  {
    "text": "this is this is not what I'm worried about it's this because this is what it's effectively doing it's fetching out",
    "start": "779959",
    "end": "785199"
  },
  {
    "text": "some amount out to go grab that value okay well we're just going to pretend that whatever is going on here it's",
    "start": "785199",
    "end": "791680"
  },
  {
    "text": "doing exactly what it's set up here which is to be able to fetch uh up to 16x your current index and uh or to",
    "start": "791680",
    "end": "799120"
  },
  {
    "text": "effect effectively uh fetch four layers further down the tree right uh if we plot these two we see that I singer",
    "start": "799120",
    "end": "805839"
  },
  {
    "text": "layout performs as good as binary search when the array fits in the L2 cache 256 KB for me the middle uh the middle red",
    "start": "805839",
    "end": "813440"
  },
  {
    "text": "line but starts to be much better than a binary search as the array grows to be much larger than the L3 cache 12",
    "start": "813440",
    "end": "819199"
  },
  {
    "text": "megabytes in the end I singer round it uh search is around four times faster and which nicely corresponds to being",
    "start": "819199",
    "end": "825320"
  },
  {
    "text": "able to prefetch four iterations of cash lines from memory at a time all right so let's see hold on which one let's see",
    "start": "825320",
    "end": "830560"
  },
  {
    "text": "sorted VC binary search standard Library a high Wishy would have chosen slightly different colors I'm not going to lie to",
    "start": "830560",
    "end": "835839"
  },
  {
    "text": "you choosing blue and blue really make it difficult to understand the the two like which one we talking here",
    "start": "835839",
    "end": "842040"
  },
  {
    "text": "which which one we talking here is is this the blue or is that the blue am I too stupid to understand the difference one we'll just assume that this has to",
    "start": "842040",
    "end": "848480"
  },
  {
    "text": "be the I singer ones obviously the blue one yeah this truly here hold on maybe maybe the problem is is that I just have",
    "start": "848480",
    "end": "854320"
  },
  {
    "text": "this thing it was turn off dark readers for a second no it's just blue both times okay very very difficult problem",
    "start": "854320",
    "end": "861240"
  },
  {
    "text": "to see I mean this person put on Blue is also worse yeah so what I think what I",
    "start": "861240",
    "end": "866600"
  },
  {
    "text": "assume this is meaning this is the standard Library time it takes right and as the array length gets larger and",
    "start": "866600",
    "end": "872480"
  },
  {
    "text": "larger and larger into uh the gigabyte range it takes longer and longer for it to be able to F uh find it right right",
    "start": "872480",
    "end": "879199"
  },
  {
    "text": "the latency goes up that I think that makes sense right because this requires 30",
    "start": "879199",
    "end": "884279"
  },
  {
    "text": "axises you know as this thing grows okay that makes sense whereas this thing prefetches some amount ah head and",
    "start": "884279",
    "end": "890079"
  },
  {
    "text": "somehow that makes it easier but then it start it stops it stopped being able to do that because at some point the",
    "start": "890079",
    "end": "895320"
  },
  {
    "text": "prefetch ahe has to put in more than it can hold into the cach if I'm reading this correct I think so which is",
    "start": "895320",
    "end": "901920"
  },
  {
    "text": "actually shocking how well this does this just goes to show that even if you are doing this just goes to show why O",
    "start": "901920",
    "end": "908600"
  },
  {
    "text": "big O is so broken in a practical sense that this right here these are the",
    "start": "908600",
    "end": "915360"
  },
  {
    "text": "same running time right whether you're right here or right here these are the exact same Big O running time and one",
    "start": "915360",
    "end": "922000"
  },
  {
    "text": "would even probably say that the constant is larger here than it is right",
    "start": "922000",
    "end": "928279"
  },
  {
    "text": "here but because this thing takes advantage of of the magic that is modern CPUs you just get this amazing",
    "start": "928279",
    "end": "935319"
  },
  {
    "text": "performance comparatively yes I think what it says is let's see I think it also saying that the ordering of the",
    "start": "935319",
    "end": "940720"
  },
  {
    "text": "memory is making a big impact on the ability to fetch right yes ordering of the memory is very very important and",
    "start": "940720",
    "end": "946160"
  },
  {
    "text": "what it's saying is that the ordering and the ability for a memory to be prefetched and cached so that the CPU can make really fast access is extremely",
    "start": "946160",
    "end": "953759"
  },
  {
    "text": "important when it comes to this type of stuff which all right that makes sense Casey talked a lot about this for all",
    "start": "953759",
    "end": "959000"
  },
  {
    "text": "experiments we'll make sure to allocate the tree using 2 megabyte huge Pages by default instead of the 4K uh B Pages",
    "start": "959000",
    "end": "965800"
  },
  {
    "text": "this reduces pressure on the translation look a side buffer oh my gosh this is Casey Casey Casey used to talk about",
    "start": "965800",
    "end": "972079"
  },
  {
    "text": "this stuff all the time with us okay this reduces pressure on the translation look aside buffer that translates",
    "start": "972079",
    "end": "977120"
  },
  {
    "text": "virtual memory addresses to Hardware memory addresses since its internal table of pages is much smaller when using Q Pages since it could be cash",
    "start": "977120",
    "end": "983480"
  },
  {
    "text": "better okay that makes sense um with transparent huge Pages enabled that are let's see they are a automatically given",
    "start": "983480",
    "end": "989480"
  },
  {
    "text": "out whenever allocating an exact multiple of 2 megabytes and so we always round up on the allocations for the tree",
    "start": "989480",
    "end": "995079"
  },
  {
    "text": "to the next multiple of 2 megabytes however it turns out that small allocations below 32 megabytes still go",
    "start": "995079",
    "end": "1000319"
  },
  {
    "text": "on the program's Heap rather than asking for uh the kernel for new memory Pages causing them not to uh actually be huge",
    "start": "1000319",
    "end": "1007000"
  },
  {
    "text": "Pages thus all allocations we do are actually rounded up to the next multiples of 32 megabytes instead okay",
    "start": "1007000",
    "end": "1013160"
  },
  {
    "text": "so it has to do 32 byte me megabyte uh allocations at a time to be able to build up the array let's see Also let's",
    "start": "1013160",
    "end": "1019319"
  },
  {
    "text": "see also together huge Pages sometimes makes a small difference when the data set is see is indeed between 1 Megabyte",
    "start": "1019319",
    "end": "1024480"
  },
  {
    "text": "and 32 megabytes in size smaller data structures don't really need huge Pages anyways enabling them for the I singer",
    "start": "1024480",
    "end": "1030959"
  },
  {
    "text": "layout as in the plot above also gives a significant speed up for larger sizes okay let's see a note on benchmarking",
    "start": "1030959",
    "end": "1037240"
  },
  {
    "text": "the plot have let's see the plots have the size of the input data on the logarithmic bottom xais on the top they",
    "start": "1037240",
    "end": "1044199"
  },
  {
    "text": "show the corresponding number of elements in the vector which is four times less since each element is a u32",
    "start": "1044199",
    "end": "1050120"
  },
  {
    "text": "spanning 4 byes measurements are taken at values 2 to the I 1.25 2 to the I 1.5 2i 1 Point Let's",
    "start": "1050120",
    "end": "1058000"
  },
  {
    "text": "see okay the y- axis shows let's see measured time per query in the plot above it says latency since the",
    "start": "1058000",
    "end": "1064080"
  },
  {
    "text": "Benchmark uh as for Q in queries query in let's see even then the pipelining is out of order execution of the CPU will",
    "start": "1064080",
    "end": "1070960"
  },
  {
    "text": "make it execute multiple iterations in parallel specifically while it is waiting for the last cach lines of the",
    "start": "1070960",
    "end": "1076679"
  },
  {
    "text": "iteration I it can already start executing the first instruction of the next query to measure the true latency",
    "start": "1076679",
    "end": "1082760"
  },
  {
    "text": "we would have to introduce a loop Carri dependency by making query plus one independent on the result of query ey",
    "start": "1082760",
    "end": "1089559"
  },
  {
    "text": "however the main goal of this post is to optimize throughput so we don't bother with that dang this is so this is just",
    "start": "1089559",
    "end": "1097679"
  },
  {
    "text": "such a great I feel like I've just been so prepared to read this by Casey and",
    "start": "1097679",
    "end": "1103919"
  },
  {
    "text": "now that we're seeing actual practical uses for it it just feels so amazing this is truly this is truly like",
    "start": "1103919",
    "end": "1109919"
  },
  {
    "text": "the Lord's work when it comes to optimization uh thus all plots will show the throughput of doing index query all",
    "start": "1109919",
    "end": "1116280"
  },
  {
    "text": "queries okay for Benchmark I'm using my laptop's uh i7 uh 1075 CPU with the frequency of 2.6 GHz",
    "start": "1116280",
    "end": "1124480"
  },
  {
    "text": "using Code snippet right here so you can go check out the code snippet if you want to go check it out which we're not",
    "start": "1124480",
    "end": "1129520"
  },
  {
    "text": "going to check it out yet let's see also relevant are the sizes of caches 32 K uh",
    "start": "1129520",
    "end": "1135000"
  },
  {
    "text": "kilobyte uh L1 cache per core 256 kilobyte L2 and 12 megab L3 uh C shared",
    "start": "1135000",
    "end": "1141640"
  },
  {
    "text": "between the physical six cores furthermore hyperthreading is disabled all measurements are done five times the",
    "start": "1141640",
    "end": "1147559"
  },
  {
    "text": "line follows the median and will show the spread of the second and fourth value oh okay so that's why there was",
    "start": "1147559",
    "end": "1154600"
  },
  {
    "text": "that little blue thing right here that's why this spread kind of existed if you didn't notice that there's like a little bit of a variation this line is the",
    "start": "1154600",
    "end": "1160760"
  },
  {
    "text": "median of those five so it's the middle element the third the third best if you will okay okay that makes sense all",
    "start": "1160760",
    "end": "1167720"
  },
  {
    "text": "right uh the let's see M memory and the caches work at a level of cash lines consisting of 64 bytes at least on my",
    "start": "1167720",
    "end": "1174120"
  },
  {
    "text": "machine or 16 u3 uh two values thus even if you only read a single bite if the",
    "start": "1174120",
    "end": "1180640"
  },
  {
    "text": "cach line containing that bite is not yet in the L1 cach the entire thing will be fetched from Ram or L3 or L2 into L1",
    "start": "1180640",
    "end": "1188520"
  },
  {
    "text": "plain binary search typically uses o uh uses a single value of each cach line",
    "start": "1188520",
    "end": "1194200"
  },
  {
    "text": "until it gets to the end of the search whereas the last 16 values span just one or two cach lines ooh are they going to",
    "start": "1194200",
    "end": "1199640"
  },
  {
    "text": "be doing some sort of weird packing stuff is that what we're about to hear are we about to hear some strange",
    "start": "1199640",
    "end": "1205840"
  },
  {
    "text": "packing optimizations oh oh baby boys this will be great uh the iing or layout",
    "start": "1205840",
    "end": "1212320"
  },
  {
    "text": "suffers the same problem even though the next Cash Line can be prefetched it let's see it still only uses a single",
    "start": "1212320",
    "end": "1219480"
  },
  {
    "text": "value in each this fundamentally means that both these search schemes are using the available memory bandwidth quite",
    "start": "1219480",
    "end": "1225679"
  },
  {
    "text": "inefficiently and since most of what they are doing is is a waiting for memory to come through that's not great",
    "start": "1225679",
    "end": "1231480"
  },
  {
    "text": "also while that's not relevant yet when doing this with many threads in parallel or with batching single core RAM",
    "start": "1231480",
    "end": "1238080"
  },
  {
    "text": "throughput and the throughput of main memory itself becomes a bottleneck dang uh hold on hold on one second I'm going",
    "start": "1238080",
    "end": "1244039"
  },
  {
    "text": "to I'm going to see this the takeaway point of exercises like this is that intuition about performance in modern",
    "start": "1244039",
    "end": "1249440"
  },
  {
    "text": "pipelined multi-level cache world are so good as intuitions about fluid dynamics use high level abstractions let lib",
    "start": "1249440",
    "end": "1255679"
  },
  {
    "text": "writer compilers do their job so I don't think that's the main takeaway in my book I think the main takeaway in my book is that memory ordering is very",
    "start": "1255679",
    "end": "1262440"
  },
  {
    "text": "important and even if you are not great knowing that you have contiguous memory",
    "start": "1262440",
    "end": "1267600"
  },
  {
    "text": "and doing operations over contiguous memory make big different right I think this is actually what makes the inchu",
    "start": "1267600",
    "end": "1273200"
  },
  {
    "text": "ification of the internet is that people constantly never worry at all for anything they're like ah just don't",
    "start": "1273200",
    "end": "1278919"
  },
  {
    "text": "worry about it a lib writer will make it better does it do does that actually",
    "start": "1278919",
    "end": "1284320"
  },
  {
    "text": "make life better I don't know you you's react bro uh all right let's get back after this okay all right let's see the",
    "start": "1284320",
    "end": "1291919"
  },
  {
    "text": "I singer layout the ier they no the I singer layout suffers the same problem even though the next Cash Line can be",
    "start": "1291919",
    "end": "1297880"
  },
  {
    "text": "prefetched it is still only a single value in each this fundamentally means that both these search schemes are use",
    "start": "1297880",
    "end": "1303120"
  },
  {
    "text": "using the available memory band with quite inefficiently yeah I assume what what he's trying to say like forgive me",
    "start": "1303120",
    "end": "1308240"
  },
  {
    "text": "if I'm wrong here I'm still trying to catch up on this is that your cash lines are these little 64 uh 64 by cach lines",
    "start": "1308240",
    "end": "1315720"
  },
  {
    "text": "and you're storing in the 64 Cash Line a single u32 the rest of this is just like",
    "start": "1315720",
    "end": "1321320"
  },
  {
    "text": "H that sucks and you only get so much L1 right L1 is not that big so I assume",
    "start": "1321320",
    "end": "1326480"
  },
  {
    "text": "what he's about to do or he's trying to do is that he's actually going to take this and he's going to just just power",
    "start": "1326480",
    "end": "1334679"
  },
  {
    "text": "Power Fill this bad boy with all of them so that every single one of these lines actually come through completely cashed",
    "start": "1334679",
    "end": "1341600"
  },
  {
    "text": "right and so then he gets he makes much more useful or much more efficient use right able to shove 16 values into it",
    "start": "1341600",
    "end": "1348159"
  },
  {
    "text": "which seems seems like uh pretty exciting yeah some cash money some cash money also while that's not relevant yet",
    "start": "1348159",
    "end": "1354039"
  },
  {
    "text": "when doing this with many threads in parallel or batching single core RAM throughput and throughput of main memory becomes the bottleneck already read that",
    "start": "1354039",
    "end": "1360600"
  },
  {
    "text": "let's go it would be much better if somehow we could use this information in each cach line much more efficiently we",
    "start": "1360600",
    "end": "1366480"
  },
  {
    "text": "can do that by storing our data in a different way instead of storing it layer by layer so that each iteration",
    "start": "1366480",
    "end": "1372640"
  },
  {
    "text": "goes into a new layer we can store four layers of the tree at a time oh my God",
    "start": "1372640",
    "end": "1379640"
  },
  {
    "text": "uh that takes 15 values let's see and could nicely be padded into a full Cash",
    "start": "1379640",
    "end": "1385279"
  },
  {
    "text": "Line then we uh when we fetch cash lines we can use it for four iterations at",
    "start": "1385279",
    "end": "1390600"
  },
  {
    "text": "once much better uh on the other hand uh now that we can't prefetch upcoming cash",
    "start": "1390600",
    "end": "1396000"
  },
  {
    "text": "lines in advance anymore let's see so that overall the latency will be the same we F let's see but we fetch up to",
    "start": "1396000",
    "end": "1403080"
  },
  {
    "text": "four times fewer cash lines overall which should help throughput okay good unfortunately I don't have let's see I",
    "start": "1403080",
    "end": "1408880"
  },
  {
    "text": "don't have code and plots here because what I really want to focus on uh is the next bit okay so we have 1 2 3 4 5 6 15",
    "start": "1408880",
    "end": "1416039"
  },
  {
    "text": "oh look at this 8 8 42 2 6 hold on what is this one I don't understand this",
    "start": "1416039",
    "end": "1422200"
  },
  {
    "text": "layout other than the middle element right here one which means this is two that means",
    "start": "1422200",
    "end": "1427400"
  },
  {
    "text": "three should be on the other side I confused by three is here wait hold on does that make sense four why is that",
    "start": "1427400",
    "end": "1433559"
  },
  {
    "text": "why is that the case why is this the case I'm trying to like think in my head how why what what the this ordering",
    "start": "1433559",
    "end": "1438960"
  },
  {
    "text": "means all right so this is packed I singer this is an implicit tree uh s let's see s3e node I guess I don't",
    "start": "1438960",
    "end": "1446279"
  },
  {
    "text": "understand the I singer layout I'm trying to understand the I singer layout I mean I get the one I get the two I",
    "start": "1446279",
    "end": "1452240"
  },
  {
    "text": "don't understand why the four is here and then the eight I guess that's like if you think about that that is because",
    "start": "1452240",
    "end": "1458240"
  },
  {
    "text": "it' be it goes one two three so it goes one two 3 4 5 6 7 8 9 10 11 12 okay I",
    "start": "1458240",
    "end": "1469120"
  },
  {
    "text": "think I I think I kind of see that a little bit okay at least I at least see the the rightmost traversal in this a",
    "start": "1469120",
    "end": "1476480"
  },
  {
    "text": "little bit of an array yeah okay weird yeah I should have stud I should have",
    "start": "1476480",
    "end": "1482799"
  },
  {
    "text": "studied up on the old I singer layout huh uh the first two rows show how we could pack four layers of the I singer",
    "start": "1482799",
    "end": "1487919"
  },
  {
    "text": "search into a single cach line the first follows a classic binary search layout while the second applies the I singer layout recursively the third row shows",
    "start": "1487919",
    "end": "1494880"
  },
  {
    "text": "an S uh tree node instead for Simplicity and it I'm using consecutive values but",
    "start": "1494880",
    "end": "1500240"
  },
  {
    "text": "in practice this would be any list of sorted numbers all right s tier and B trees or S trees and B trees we just",
    "start": "1500240",
    "end": "1506720"
  },
  {
    "text": "ended with a note of 15 values that represent a height four search tree in which we can binary search from there",
    "start": "1506720",
    "end": "1513440"
  },
  {
    "text": "it's just a small step to estries berries but the first let's see but first I have to briefly mention berries",
    "start": "1513440",
    "end": "1519559"
  },
  {
    "text": "through Wikipedia those are the more classic Dynamic variant where nodes are linked together via pointers as",
    "start": "1519559",
    "end": "1526240"
  },
  {
    "text": "Wikipedia writes they are typically used with much larger block sizes for example 4kb since files read from dis usually",
    "start": "1526240",
    "end": "1533120"
  },
  {
    "text": "comes 4kb chunks thus they also have much larger branching factors yes B trees are typically used for like",
    "start": "1533120",
    "end": "1539080"
  },
  {
    "text": "databases and discs and being able to do efficient reads from discs because there's this whole problem where if you want to like read a very large amount if",
    "start": "1539080",
    "end": "1545520"
  },
  {
    "text": "you want to search through a huge amount of data you don't want to have to read all the data especially from disk so instead you have to be able to read",
    "start": "1545520",
    "end": "1551960"
  },
  {
    "text": "small bits of it to be able to know where to go next and so that's the like the effective idea behind be trees is",
    "start": "1551960",
    "end": "1557399"
  },
  {
    "text": "just hyper efficiency storage estries we'll instead use estries as named by",
    "start": "1557399",
    "end": "1563240"
  },
  {
    "text": "algorithmica they are a nice Middle Ground between the high branching factor of B trees and the compactness of the",
    "start": "1563240",
    "end": "1568520"
  },
  {
    "text": "icing or layout instead of interpreting the 15 values as a search tree we can also store them in a sorted way and",
    "start": "1568520",
    "end": "1574080"
  },
  {
    "text": "consider them as a 16 Aries search tree the 15 values simply split the data in the sub tree into 16 parts and we can do",
    "start": "1574080",
    "end": "1581600"
  },
  {
    "text": "a linear scan to find which part to recurse into but if we store 15 values one padded in the cach line we might uh",
    "start": "1581600",
    "end": "1588799"
  },
  {
    "text": "as well make it 16 values and have a branching factor of 17 inste yes I agree I was a little confused why there's 16",
    "start": "1588799",
    "end": "1595000"
  },
  {
    "text": "here and there's only 15 here I assume this is all the this is all the children is that what is that what that's supposed to be at least that's what I",
    "start": "1595000",
    "end": "1601279"
  },
  {
    "text": "assume and then so I was wondering why does this one have 16 versus this one doesn't have 16 I didn't quite understand that all right all right S",
    "start": "1601279",
    "end": "1607279"
  },
  {
    "text": "Plus trees B trees and S trees Only Store uh each value once either in the leaf node or in the internal node this",
    "start": "1607279",
    "end": "1613840"
  },
  {
    "text": "turns out to be somewhat annoying since we must track in which layer the result was found to simplify this we can store",
    "start": "1613840",
    "end": "1619320"
  },
  {
    "text": "all values as a leaf to duplicate them let's see and duplicate them in the internal nodes this is what we call a B+",
    "start": "1619320",
    "end": "1626000"
  },
  {
    "text": "tree or S plus3 however I will be lazy and just use S3 and include modification so we have 16 12 so everything",
    "start": "1626000",
    "end": "1632880"
  },
  {
    "text": "everything on this side is less than six as we talked about everything in between is greater than or equal to six but less than 12 everything I I believe it's",
    "start": "1632880",
    "end": "1640480"
  },
  {
    "text": "greater greater than or equal to yeah it's it is greater than equal to right you can see the six right here the six right here you can see that one right",
    "start": "1640480",
    "end": "1647000"
  },
  {
    "text": "here so there we go all right let's look at this for just a quick second and understand it blue green red blue green",
    "start": "1647000",
    "end": "1652399"
  },
  {
    "text": "red oh is this the layout is this what the I singer layout's supposed to be no this not what the I singer layout is supposed to be this is like your",
    "start": "1652399",
    "end": "1657919"
  },
  {
    "text": "standard Heap layout right yes that's your standard Heap layout right all right so then two I'm trying to",
    "start": "1657919",
    "end": "1663559"
  },
  {
    "text": "understand why these little highlights exist here first oh this is the top level value that also appears down here",
    "start": "1663559",
    "end": "1669679"
  },
  {
    "text": "this is oh this is the duplication from here to here duplication duplication duplication duplication let's see as an",
    "start": "1669679",
    "end": "1677159"
  },
  {
    "text": "example of the uh full s plus3 that I will that I will from now on just call S3 on values let's see on 18 values with",
    "start": "1677159",
    "end": "1684840"
  },
  {
    "text": "the node size of b equal 2 and the branching factor of B+ 1 equal 3 each internal node stores the smallest value",
    "start": "1684840",
    "end": "1691240"
  },
  {
    "text": "in the sub Tree on its right in memory the layers are simply packed together behind each other a full S3 can",
    "start": "1691240",
    "end": "1698679"
  },
  {
    "text": "be navigated in a way similar to an iing or layout the node uh not let's see not value right at index I has its b+1 child",
    "start": "1698679",
    "end": "1707279"
  },
  {
    "text": "nodes at index b + 1 * I + 1 plus these values the the sub values of B yeah when",
    "start": "1707279",
    "end": "1714760"
  },
  {
    "text": "the tree is only partially filled the layout can be uh can waste a lot of space instead we can pack the layers",
    "start": "1714760",
    "end": "1720039"
  },
  {
    "text": "together by sorting the offset uh o of each layer or by storing the offset oh my gosh what did I just click as three",
    "start": "1720039",
    "end": "1728760"
  },
  {
    "text": "mentioned look at that we got some infinities here all right on the children of o l plus I are then o plus I",
    "start": "1728760",
    "end": "1736720"
  },
  {
    "text": "plus b + 1 * 1 and all this layout so then we have six to infinity infinity to Infinity which means that we have six",
    "start": "1736720",
    "end": "1742919"
  },
  {
    "text": "and seven right here I don't really quite get the Infinities the full representation can be inefficient the packed representation removes empty",
    "start": "1742919",
    "end": "1749039"
  },
  {
    "text": "space and explicitly stores the offset o where each layer starts at last we have",
    "start": "1749039",
    "end": "1754080"
  },
  {
    "text": "to look at some code each node in the tree simply represents as a list of n equal 16 u32 values we explicitly ask",
    "start": "1754080",
    "end": "1760799"
  },
  {
    "text": "the nodes to be aligned 64 bite cach line boundaries oh nice okay so you do a little representation Al line 64 tree",
    "start": "1760799",
    "end": "1767159"
  },
  {
    "text": "node is just this many values and this is a constant sized item okay that makes",
    "start": "1767159",
    "end": "1772200"
  },
  {
    "text": "sense so you able to just pass in 16 and so you have a perfectly aligned you have you have a 64 byte structure being",
    "start": "1772200",
    "end": "1778799"
  },
  {
    "text": "represented here the S3 itself is simply a list of nodes and the offsets where each layer starts",
    "start": "1778799",
    "end": "1785320"
  },
  {
    "text": "S3 b n is a vector of these tree nodes of n uh let's see the root is at index",
    "start": "1785320",
    "end": "1792320"
  },
  {
    "text": "tree offset zero its children start at tree offset one and so on okay I only",
    "start": "1792320",
    "end": "1797559"
  },
  {
    "text": "imp partially getting this but I think I I think I see the general idea of just representing data in this contiguous way",
    "start": "1797559",
    "end": "1803200"
  },
  {
    "text": "that allows for much better caching so this is kind of exciting to save some space and focus on the interesting part to me at least I will not show any code",
    "start": "1803200",
    "end": "1809519"
  },
  {
    "text": "for uh constructing estries it's a whole bunch of uninteresting fiddling with indices and it takes a lot of time to",
    "start": "1809519",
    "end": "1814960"
  },
  {
    "text": "get right also construction is not optimized at all currently anyways find the code here by the way you know I'm",
    "start": "1814960",
    "end": "1820440"
  },
  {
    "text": "just throwing this out there imagine imagine if we would have used a zig I'm just throwing it out there okay Zig",
    "start": "1820440",
    "end": "1826720"
  },
  {
    "text": "probably would have been a lot easier probably not a lot of fiddling uh what we'll look at well actually there'll be tons of fiddling s trees sound like a",
    "start": "1826720",
    "end": "1833120"
  },
  {
    "text": "nightmare to construct like s trees sound nightmarish to construct all right",
    "start": "1833120",
    "end": "1838679"
  },
  {
    "text": "search uh so here's all the search stuff it does the same offset with this whole jumping to the node it does the whole",
    "start": "1838679",
    "end": "1844640"
  },
  {
    "text": "index bid bidness so it's jumping through all these okay and this is why you duplicate at the end because then",
    "start": "1844640",
    "end": "1851159"
  },
  {
    "text": "you don't have to I I assume the duplication at the end is so that uh let's see right here the duplication at the end is such that you're not checking",
    "start": "1851159",
    "end": "1857760"
  },
  {
    "text": "the keys on the way down and trying to do like a key check through here and then through here instead you're just kind of Leaping through that's my",
    "start": "1857760",
    "end": "1864240"
  },
  {
    "text": "assumption why I don't really understand why you'd want to duplicate the keys I I assume it just makes it more easy to search through all right linear first",
    "start": "1864240",
    "end": "1871200"
  },
  {
    "text": "let's precisely Define what we want to find uh to do its input is a node with 16 sorted values and the query value q",
    "start": "1871200",
    "end": "1878559"
  },
  {
    "text": "and it should return the index of the first element that is at least Q okay that seems right the results are not",
    "start": "1878559",
    "end": "1885039"
  },
  {
    "text": "very impressive yet I don't know let's see on let's see what it is sorted binary okay first off if you're that",
    "start": "1885039",
    "end": "1891240"
  },
  {
    "text": "much better than a sorted binary uh search that is very impressive okay",
    "start": "1891240",
    "end": "1896519"
  },
  {
    "text": "notice that you're not like look look at the difference that that's very impressive okay when he's just like nah",
    "start": "1896519",
    "end": "1902120"
  },
  {
    "text": "it's not that impressive it's just like I'm impressed okay look at that S3 find linear which is the one he's creating is",
    "start": "1902120",
    "end": "1907919"
  },
  {
    "text": "not as good as the I singer lookup but still very impressive I'm I'm I'm jerked out of my mind right now the initial",
    "start": "1907919",
    "end": "1914519"
  },
  {
    "text": "version of our S3 search is quite a bit slower than the I singer layout and this and following plots old lines will be uh",
    "start": "1914519",
    "end": "1920480"
  },
  {
    "text": "dimmed and the best previous uh previous and best new lines slightly highlighted colors will be consistent from one plot",
    "start": "1920480",
    "end": "1926600"
  },
  {
    "text": "to the next okay Auto autov vectorization as it turns out the break in code snippet 8 is really bad for",
    "start": "1926600",
    "end": "1932159"
  },
  {
    "text": "performance since the branch predictor can't do a good job on it instead we can count the number of values less than q",
    "start": "1932159",
    "end": "1937960"
  },
  {
    "text": "and return that as the index of the first value okay hold on let's see this",
    "start": "1937960",
    "end": "1943880"
  },
  {
    "text": "count really search the whole dang list interesting Okay so search the whole thing and just count instead okay",
    "start": "1943880",
    "end": "1951919"
  },
  {
    "text": "interesting that's a weird Okay interesting I would hey boys don't be",
    "start": "1951919",
    "end": "1957039"
  },
  {
    "text": "breaking don't be breaking okay uh in fact the code is not just Branch list but it's actually autov vectorized into",
    "start": "1957039",
    "end": "1962360"
  },
  {
    "text": "simd instructions well simd is nuts holy cow look at all those VP operations and all that to save some space you can find",
    "start": "1962360",
    "end": "1968639"
  },
  {
    "text": "this and further results in the section figure six at the end of the section this Auto vectorization version is over",
    "start": "1968639",
    "end": "1974000"
  },
  {
    "text": "two times faster than the linear find and now clearly beats iing your layout that's nuts that's these are one of",
    "start": "1974000",
    "end": "1981279"
  },
  {
    "text": "those this is one of those things where it's just like so non I would not have thought in my head you know what we",
    "start": "1981279",
    "end": "1986639"
  },
  {
    "text": "should do search the whole Space instead of searching up till the point and breaking from there right absolutely",
    "start": "1986639",
    "end": "1993799"
  },
  {
    "text": "beautiful about that just just mindboggling stuff by the way I would assume this does not work in JavaScript",
    "start": "1993799",
    "end": "2000919"
  },
  {
    "text": "so if you're taking this advice as do this in any language you probably don't want to do that unless if you can Sim",
    "start": "2000919",
    "end": "2006960"
  },
  {
    "text": "D's okay and you should probably know what that means and you should probably do that you should probably know what all of",
    "start": "2006960",
    "end": "2013120"
  },
  {
    "text": "that means before you do that okay just throwing that out there my guess is that people will just start writing stuff and it just makes no sense what you're doing",
    "start": "2013120",
    "end": "2020039"
  },
  {
    "text": "all right to save some space you can find this and further results I do actually want to see this let's show me figure six all right sorted VC uh search",
    "start": "2020039",
    "end": "2026760"
  },
  {
    "text": "Okay sorted VC search just goes sorted VC search goes off the screen the previous find L lineer goes off the",
    "start": "2026760",
    "end": "2032360"
  },
  {
    "text": "screen I singer search which was much lower goes off the screen and look at this we got pop count portable and we",
    "start": "2032360",
    "end": "2038919"
  },
  {
    "text": "got pop count and that's the lowest one look at that look at that would you look at it would you look at that the through",
    "start": "2038919",
    "end": "2046120"
  },
  {
    "text": "inverse throughput is 150 nond versus the 20 gosh 25 crazy Splat Splat the",
    "start": "2046120",
    "end": "2054760"
  },
  {
    "text": "query Val dude popping and locking that's crazy let's see uh also relevant sizes of the cash let's see hold we",
    "start": "2054760",
    "end": "2061158"
  },
  {
    "text": "already talked about that uh oh did I lose my spot oh I lost oh brother I've lost my spot nope got it first try all",
    "start": "2061159",
    "end": "2069440"
  },
  {
    "text": "right let's see this is autov vectorization version is over two times faster than the linear find and now",
    "start": "2069440",
    "end": "2075118"
  },
  {
    "text": "clearly beats I singer layout we can also roll our own simd the simd version of the original linear scan does let's",
    "start": "2075119",
    "end": "2080720"
  },
  {
    "text": "see idea does 16 comparisons in parallel converts that to a bit mask and then counts the number of trailing zeros",
    "start": "2080720",
    "end": "2086358"
  },
  {
    "text": "using uh portable simd oh my gosh fine counts fine cuns damn crazy simd uh 32",
    "start": "2086359",
    "end": "2095560"
  },
  {
    "text": "by the way this another reason why uh another reason why Zig is pretty",
    "start": "2095560",
    "end": "2101000"
  },
  {
    "text": "impressive is that it does actually have vectorized things built into it using",
    "start": "2101000",
    "end": "2106839"
  },
  {
    "text": "compile time macros and so a lot of the things you kind of have to do with incantations are kind of freely",
    "start": "2106839",
    "end": "2112359"
  },
  {
    "text": "available in Zig which is kind of which is a super interesting it's just Zig",
    "start": "2112359",
    "end": "2118119"
  },
  {
    "text": "dang it I I feel like I'm becoming a zig fan I'm trying not to become a zig fan I'm trying my literal best not to become",
    "start": "2118119",
    "end": "2124920"
  },
  {
    "text": "a zig fan and I think I'm becoming a zig fan like I'm I'm genuinely trying not to",
    "start": "2124920",
    "end": "2130160"
  },
  {
    "text": "do it and I just it's happening Zig fan dude I'm a zig fan I'm a zig fan uh it's",
    "start": "2130160",
    "end": "2135920"
  },
  {
    "text": "true for Mojo too yeah Mojo seems very interesting Chris lner obviously very smart guy uh Chris lner very smart and I",
    "start": "2135920",
    "end": "2143960"
  },
  {
    "text": "I'm very curious where Mojo goes cuz I obviously Mojo would be the ideal language if you're doing operations that",
    "start": "2143960",
    "end": "2150560"
  },
  {
    "text": "require you to constantly dip into uh simd but you don't have the technical expertise to understand it meaning that",
    "start": "2150560",
    "end": "2157040"
  },
  {
    "text": "everything you do do can just be like hey calculate the best thing for me in this location and make it and and simd",
    "start": "2157040",
    "end": "2162760"
  },
  {
    "text": "nuts and it just like does it every single time for you at all times that just seems like it's always that just",
    "start": "2162760",
    "end": "2169240"
  },
  {
    "text": "seems like that would be it's like the JavaScript of simd right yeah exactly all right see I I don't",
    "start": "2169240",
    "end": "2177400"
  },
  {
    "text": "understand why this versus this is better versus worse because like in you",
    "start": "2177400",
    "end": "2183079"
  },
  {
    "text": "can't just look at a single instruction and say one is better than the other if you're a Layman right you have to you",
    "start": "2183079",
    "end": "2188839"
  },
  {
    "text": "actually have to like know a lot of things now let's get uh let's see now let's look at this generated code in a bit more detail first up why does Sim Le",
    "start": "2188839",
    "end": "2196680"
  },
  {
    "text": "translate into Min and compare equal uh let's see from checking the Intel intrinsics guide we find out that there",
    "start": "2196680",
    "end": "2202200"
  },
  {
    "text": "are only signed comparisons while our data is unsigned for now let's just assume that all values fit into 31 bits",
    "start": "2202200",
    "end": "2208560"
  },
  {
    "text": "and are at most i32 Max there we go this is what we're talking about see this is that thing that Casey showed us earlier",
    "start": "2208560",
    "end": "2214079"
  },
  {
    "text": "remember when we did the language performance diagram do does everyone remember the language performance diagrams I don't know if you guys do",
    "start": "2214079",
    "end": "2219520"
  },
  {
    "text": "press one if you do uh but in that Casey literally makes his version of the",
    "start": "2219520",
    "end": "2226119"
  },
  {
    "text": "language performance diagram like 4X Faster by just just doing a couple weird",
    "start": "2226119",
    "end": "2232280"
  },
  {
    "text": "operations that you would never think would be faster instead of doing a modulo it does a certain set of things",
    "start": "2232280",
    "end": "2237720"
  },
  {
    "text": "just because these instructions aren't available and it just makes it like egregiously faster and then we can",
    "start": "2237720",
    "end": "2244760"
  },
  {
    "text": "transmutate our input simd i328 without changing its meaning uh both input values and queries are between 0 to i32",
    "start": "2244760",
    "end": "2251520"
  },
  {
    "text": "Max we'll eventually we will fix this by either taking i32 put indirect or i32 input directly or by shifting u32 values",
    "start": "2251520",
    "end": "2257720"
  },
  {
    "text": "to fit into i32 range bada bing bada boom all right let's see what let's see what just Insanity happens by making",
    "start": "2257720",
    "end": "2263839"
  },
  {
    "text": "this small change I don't even know what this means but okay rock on it turns out that uh there is only uh a greater than",
    "start": "2263839",
    "end": "2270480"
  },
  {
    "text": "instruction in simd and not a greater than an equal so there is no way to avoid inverting the results we also see",
    "start": "2270480",
    "end": "2275520"
  },
  {
    "text": "that uh VP shu D instruction that feels very out of place you know what wouldn't",
    "start": "2275520",
    "end": "2282359"
  },
  {
    "text": "have known Shuff in the D the VP Shu D was out of place what's happening is that while packing the results of the 16",
    "start": "2282359",
    "end": "2288880"
  },
  {
    "text": "u32 comparisons down to a single 16bit value data is interleaved in an unfortunate way we also need to fix that",
    "start": "2288880",
    "end": "2296240"
  },
  {
    "text": "here algorithmica takes the approach of pre-hinged to counter the unshuffled",
    "start": "2296240",
    "end": "2301480"
  },
  {
    "text": "instruction they also suggest that pop are using pop count instead which is indeed what we'll do next pop count pop",
    "start": "2301480",
    "end": "2307760"
  },
  {
    "text": "count counts the number of ones right if I'm not mistaken uh as we let's see as we saw the drawback of the trailing Z or",
    "start": "2307760",
    "end": "2313400"
  },
  {
    "text": "is it the amount of zeros I can't remember the let's see the drawback of trailing zero count approach is that the",
    "start": "2313400",
    "end": "2320079"
  },
  {
    "text": "order of the lanes must be preserved instead we'll need to Simply count the number of lanes with the values less",
    "start": "2320079",
    "end": "2325880"
  },
  {
    "text": "than the query simply the let's see similar to the auto vectorization simd before and that the order of the lanes",
    "start": "2325880",
    "end": "2331480"
  },
  {
    "text": "doesn't matter yeah okay count ones count ones is pop count right yeah yeah",
    "start": "2331480",
    "end": "2337520"
  },
  {
    "text": "yeah yeah we learned that from um it was like what 3 years ago Advent of code",
    "start": "2337520",
    "end": "2343800"
  },
  {
    "text": "where we were trying to make something really really fast and that really really fast solution ended up being walking in Array backwards and using pop",
    "start": "2343800",
    "end": "2351680"
  },
  {
    "text": "count if I'm not mistaken and it just it just was crazy insane fast there we go we get a little pop count in there let's",
    "start": "2351680",
    "end": "2357880"
  },
  {
    "text": "see we remove whatever these two instructions are and get a pop count instead ideally we would like to get move mask directly into the U 16 X6",
    "start": "2357880",
    "end": "2365440"
  },
  {
    "text": "output uh of the first pack instru vpac ssdw to get the highest bit of each",
    "start": "2365440",
    "end": "2370920"
  },
  {
    "text": "of the 16 16 bit values unfortunately we are again let down by the avx2 there are move mask instructions for a u8 u32 and",
    "start": "2370920",
    "end": "2378560"
  },
  {
    "text": "u64 but not for u16 damn feels man feels",
    "start": "2378560",
    "end": "2384200"
  },
  {
    "text": "also the VP shd instruction is now provably useless so it's slightly disappointing that the compiler didn't",
    "start": "2384200",
    "end": "2389800"
  },
  {
    "text": "elide it time to write the simd by hand instead said literally nobody ever",
    "start": "2389800",
    "end": "2395720"
  },
  {
    "text": "except for this guy brother the brother is giving the simd a hand job over here",
    "start": "2395720",
    "end": "2401040"
  },
  {
    "text": "all right as it turns out we by the way that's the craziest thing I've ever said in my life time I just wanted to be",
    "start": "2401040",
    "end": "2407599"
  },
  {
    "text": "known that that has never been said and will probably never be said again um as",
    "start": "2407599",
    "end": "2413760"
  },
  {
    "text": "it turns out we can get away without most of the packing instead we using VP move mask B uh move mask this one on",
    "start": "2413760",
    "end": "2420760"
  },
  {
    "text": "8bit data we actually use it directly on the 16-bit output of the vpac sswd or DW",
    "start": "2420760",
    "end": "2427400"
  },
  {
    "text": "you exclamation point I'm not going to lie to you I'm I'm forever lost at this point but it seems so exciting I'm just",
    "start": "2427400",
    "end": "2433520"
  },
  {
    "text": "so excited now since the comparison sets each lane to all zeros or all ones we can safely read the most significant and",
    "start": "2433520",
    "end": "2440000"
  },
  {
    "text": "middle bit and divide the count by two at the end all right so we got the low we got the high we do a little bit of",
    "start": "2440000",
    "end": "2445160"
  },
  {
    "text": "slicing we Splat that son of a bee uh we do a little transmute te mask low mask",
    "start": "2445160",
    "end": "2450800"
  },
  {
    "text": "High merge it mask it writing stuff by hand look at this we're hand we're hand oh we're handing",
    "start": "2450800",
    "end": "2457480"
  },
  {
    "text": "brother is right doing some some crazy stuff manual version of simd Code by explicitly using the intrinsics this is",
    "start": "2457480",
    "end": "2464000"
  },
  {
    "text": "kind of ugly now and there's a lot of transmuting casting going on between uh u32 uh 8 and simd u8 and the native M",
    "start": "2464000",
    "end": "2471839"
  },
  {
    "text": "256 I type but we'll have to live with it oh my gosh oh my god oh I don't see",
    "start": "2471839",
    "end": "2479560"
  },
  {
    "text": "in my head Less in my head less of these are better but you know I also don't",
    "start": "2479560",
    "end": "2485440"
  },
  {
    "text": "know the throughput remember we've we've been looking at a lot of those and the throughput and how many operations and the amount of ports available to be able",
    "start": "2485440",
    "end": "2491720"
  },
  {
    "text": "to operate can make a huge difference but in my head this just means this is great right only five instructions total",
    "start": "2491720",
    "end": "2497839"
  },
  {
    "text": "are left now that there are no explicit divisions by two since that is absorbed into the pointer arithmetic in the",
    "start": "2497839",
    "end": "2504319"
  },
  {
    "text": "remainder after the function is inline now let's look how it work uh work at all this okay so this is the work is",
    "start": "2504319",
    "end": "2510160"
  },
  {
    "text": "this is what he got was that that purple line right there so he went from this line right here which is",
    "start": "2510160",
    "end": "2515560"
  },
  {
    "text": "just hold let's go let's go all the way back up a little bit and find that previous let's see yeah it's this one",
    "start": "2515560",
    "end": "2521720"
  },
  {
    "text": "this one ended at what 300 so approximately 300 and now oh my gosh oh",
    "start": "2521720",
    "end": "2527720"
  },
  {
    "text": "we're we're even doing better oh my gosh he does better so it effectively 2x the performance of find linear if that if",
    "start": "2527720",
    "end": "2533560"
  },
  {
    "text": "I've correctly identified that all right as can be seen very nicely in this plot each single instruction that we remove",
    "start": "2533560",
    "end": "2539599"
  },
  {
    "text": "gives us a small but consistent Improvement in throughput the biggest Improvement comes from the last two steps where we indeed shave off three",
    "start": "2539599",
    "end": "2545079"
  },
  {
    "text": "instructions in fact we can analyze this plot a bit more for up to for input up to 2 to the 6 equals 64 bytes the",
    "start": "2545079",
    "end": "2551920"
  },
  {
    "text": "performance is consist uh constant since in this case search tree only consists of the root node uh input up to 2 to the",
    "start": "2551920",
    "end": "2558040"
  },
  {
    "text": "10 uh uh the the has two layers the the the the okay there's a there's a typo",
    "start": "2558040",
    "end": "2564319"
  },
  {
    "text": "somewhere in here the three has two layers uh and performance is consistent similarly we need latency jumping for 2",
    "start": "2564319",
    "end": "2570040"
  },
  {
    "text": "to the 14 18 22 and 26 each time because a new layer is added to the tree or rather the jumps are at a power of the",
    "start": "2570040",
    "end": "2576119"
  },
  {
    "text": "branching Factor B plus one ooh branching factor in in a way we can also hand wavely interpret the X AIS axis as",
    "start": "2576119",
    "end": "2582680"
  },
  {
    "text": "time each time of the graph jumps the height of the jump is pretty much the same let's see the time spent on",
    "start": "2582680",
    "end": "2588240"
  },
  {
    "text": "processing that one extra layer of the tree okay so every time you add a layer you get yourself a little bit of issues",
    "start": "2588240",
    "end": "2594240"
  },
  {
    "text": "once we exceed the size of the L3 cache things slow down quickly at that point each layer each extra layer of the tree",
    "start": "2594240",
    "end": "2600319"
  },
  {
    "text": "adds significant amount of time since we are waiting for Ram let see since we're since waiting for Ram is inherently slow",
    "start": "2600319",
    "end": "2606240"
  },
  {
    "text": "that phrase that's an insane phrase okay that's an insane phrase for most people's daily programming experiences",
    "start": "2606240",
    "end": "2612839"
  },
  {
    "text": "that's just not what most people say most people most people do not say that phrase on the other hand once we hit Ram",
    "start": "2612839",
    "end": "2619040"
  },
  {
    "text": "The Slowdown is more smooth rather than stepwise this is because L3 is able to cach a fraction of the data structure",
    "start": "2619040",
    "end": "2625359"
  },
  {
    "text": "and the fraction only decreases slowly again hand wavely we can also interpret the x-axis as a snapshot of space usage",
    "start": "2625359",
    "end": "2631079"
  },
  {
    "text": "at a fixed Moment In Time the first three layers of the tree fit in L1 the fourth and fifth layers fit into L2 and",
    "start": "2631079",
    "end": "2637240"
  },
  {
    "text": "L3 once uh once the three is six layers deep or once the tree not the three once",
    "start": "2637240",
    "end": "2643040"
  },
  {
    "text": "the tree is six layers deep the reads of that layer will mostly hit Ram any additional layers let's see are sure",
    "start": "2643040",
    "end": "2648800"
  },
  {
    "text": "going to ram okay that makes sense that's why you see these steps that's why I guess you see all these steps in each one of these is that that's what he",
    "start": "2648800",
    "end": "2655280"
  },
  {
    "text": "saying by these steps right so this is a step this is a step you can see each one",
    "start": "2655280",
    "end": "2660359"
  },
  {
    "text": "of the steps as it's hitting uh place where it has to go and fetch out stuff okay that's cool that that's pretty cool",
    "start": "2660359",
    "end": "2666680"
  },
  {
    "text": "all right let's so let's optimizing the search batching as promised the first Improvement we'll make is batching instead of processing one query at a",
    "start": "2666680",
    "end": "2672880"
  },
  {
    "text": "time we we can process multiple many queries at once this allows the CPU to work on multiple queries at the same",
    "start": "2672880",
    "end": "2678880"
  },
  {
    "text": "time and in particular it can mult let's see it can have multiple up to 10 to2 in progress request to Ram at a time that",
    "start": "2678880",
    "end": "2685800"
  },
  {
    "text": "way instead of waiting for let's see for a latency of 80 NCS per read we effectively wait 10 reads at the same",
    "start": "2685800",
    "end": "2691640"
  },
  {
    "text": "time oh damn uh lowering the uh amortised weight time to around 8",
    "start": "2691640",
    "end": "2698000"
  },
  {
    "text": "nond damn batching very much benefits the fact that we are using S Plus 3",
    "start": "2698000",
    "end": "2703280"
  },
  {
    "text": "instead of s uh S3 since each element is uh is found in the last layer uh at the",
    "start": "2703280",
    "end": "2709359"
  },
  {
    "text": "same depth and hence the number of search steps through the tree is uh same at every element see that whole s+ that",
    "start": "2709359",
    "end": "2715520"
  },
  {
    "text": "whole like copping of an element into the tree in which it should uh or the copping of the element at the bottom of",
    "start": "2715520",
    "end": "2721520"
  },
  {
    "text": "the tree I don't understand but that seems very exciting it just seems very exciting okay here's a bunch of that I'm",
    "start": "2721520",
    "end": "2727359"
  },
  {
    "text": "probably not going to understand let's see the results of this so now we're batching at different amounts and you can looks like batching effectively",
    "start": "2727359",
    "end": "2734200"
  },
  {
    "text": "stops being effective right in this area in fact it looks like this 16 often is",
    "start": "2734200",
    "end": "2739880"
  },
  {
    "text": "at the bottom the 16 and this 128 but I mean at this point you probably can't get reliable it's probably impossible to",
    "start": "2739880",
    "end": "2746440"
  },
  {
    "text": "get reliable stuff look at that binary search is off the screen at 2 to the 20 his first attempts are off I singers is",
    "start": "2746440",
    "end": "2753240"
  },
  {
    "text": "off the screen right here these ones are off the screen these are all off the screen look at that that's crazy L1 L2",
    "start": "2753240",
    "end": "2759240"
  },
  {
    "text": "L3 Ram so you're in ramland get Ram rotted uh one interesting observation is",
    "start": "2759240",
    "end": "2764559"
  },
  {
    "text": "going uh from batch size one to two does not double the performance I suspect this is because cpu's out of order",
    "start": "2764559",
    "end": "2770880"
  },
  {
    "text": "execution was already deep enough to uh effectively execute almost two queries in parallel anyways going to batch size",
    "start": "2770880",
    "end": "2776839"
  },
  {
    "text": "four and then eight provides a significant speed up again going from four uh going to four the speed up is",
    "start": "2776839",
    "end": "2782400"
  },
  {
    "text": "relatively a bit less than going from uh going to eight so probably even with batch sizes of four the CPU is somewhat",
    "start": "2782400",
    "end": "2788000"
  },
  {
    "text": "looking ahead into the next batch of four already mind blown cpu's black magic throughput",
    "start": "2788000",
    "end": "2794599"
  },
  {
    "text": "saturates the batch size of 16 or really around 12 already the course let's see which corresponds to CPU having 12 line",
    "start": "2794599",
    "end": "2801000"
  },
  {
    "text": "fill buffers and thus being able to read up to 12 cach lines in parallel nevertheless we'll settle on a batch",
    "start": "2801000",
    "end": "2806839"
  },
  {
    "text": "size of 128 mostly because it leads to slightly cleaner plots in the uh remainder it is also uh ever so slightly",
    "start": "2806839",
    "end": "2814599"
  },
  {
    "text": "faster probably because the constant overhead of initializing a batch is smaller when batches are larger okay",
    "start": "2814599",
    "end": "2821280"
  },
  {
    "text": "that makes sense uh prefetching the CPU is already fetching multiple reads in parallel using out of order execution",
    "start": "2821280",
    "end": "2827200"
  },
  {
    "text": "but we can also help out a bit by doing uh this explicitly using prefetching",
    "start": "2827200",
    "end": "2832960"
  },
  {
    "text": "after processing a node we determine the child the node K that we need to visit next so we can directly request the node",
    "start": "2832960",
    "end": "2838960"
  },
  {
    "text": "to be read from memory before continuing with the rest of the batch ah yes the old to do more work which allows you to",
    "start": "2838960",
    "end": "2845640"
  },
  {
    "text": "do to go faster CL classic computer performance problem this just wild just",
    "start": "2845640",
    "end": "2853559"
  },
  {
    "text": "wild greetings hey Jeremy all right okay dude that's getting this is",
    "start": "2853559",
    "end": "2860119"
  },
  {
    "text": "getting just disgusting at this point he's down to like what 36 batch prefet up to 36 Nan for this uh",
    "start": "2860119",
    "end": "2869920"
  },
  {
    "text": "prefetching uh let's see prefetching The Cash Line node for the next instruction ahead CPU design just pick true and",
    "start": "2869920",
    "end": "2875880"
  },
  {
    "text": "false for any if statement process further instruction back in time yeah I know it's it's it's all",
    "start": "2875880",
    "end": "2881960"
  },
  {
    "text": "nutty look at that you can even see each step as it's going you can see all the steps and then it kind of goes out of",
    "start": "2881960",
    "end": "2887240"
  },
  {
    "text": "control in ramland don't get caught in ramland we observe a few things first pre-etching uh prefetching slightly uh",
    "start": "2887240",
    "end": "2894640"
  },
  {
    "text": "slows things down while the data fits in L1 already since in that case the instruction doesn't do anything anyways",
    "start": "2894640",
    "end": "2900359"
  },
  {
    "text": "in L2 it makes the graph slightly more flat indicating that uh that already there the latency is already a little",
    "start": "2900359",
    "end": "2906839"
  },
  {
    "text": "bit of a bottleneck in L3 this effectively gets larger and when we get let's see and we get a nice smooth hor",
    "start": "2906839",
    "end": "2912359"
  },
  {
    "text": "horizontal graph until we hit Ram size there prefetching provides the biggest gains yeah look at that thanks gaining",
    "start": "2912359",
    "end": "2919000"
  },
  {
    "text": "so L3 it does a better job and then ramland holy cow poter arithmetic again",
    "start": "2919000",
    "end": "2924119"
  },
  {
    "text": "oh no here we go it's time to look at some assembly code now optimize the search function itself results are down",
    "start": "2924119",
    "end": "2929400"
  },
  {
    "text": "below in figure nine let's see upfront Splat first we can note that the find function Splats the query from u32 to",
    "start": "2929400",
    "end": "2936880"
  },
  {
    "text": "simd 328 on each call it's slightly nicer but not really faster actually",
    "start": "2936880",
    "end": "2942640"
  },
  {
    "text": "this let's see to Splat all the queries up front and then reuse those so we're going to hit them with the Splat we're",
    "start": "2942640",
    "end": "2949200"
  },
  {
    "text": "going to jump to this one we're going to just keep on finding them Splats the assembly code uh for each",
    "start": "2949200",
    "end": "2954920"
  },
  {
    "text": "iteration the first Loop now looks like this classic assembly code that's I mean I was just I was literally just talking",
    "start": "2954920",
    "end": "2960839"
  },
  {
    "text": "about the shackle earlier uh did AI find the Ser no no human found this fast one",
    "start": "2960839",
    "end": "2967760"
  },
  {
    "text": "looking at the code above we see uh Shack oh wow who would have guessed that I that that's the one I pointed out I",
    "start": "2967760",
    "end": "2972960"
  },
  {
    "text": "was like hey it's a shackle and that's the one instruction that multiplies the given value by 64 that's because our",
    "start": "2972960",
    "end": "2978240"
  },
  {
    "text": "search let's see our tree nodes are 64 bytes large and hence to get to the I element of the array we need to read the",
    "start": "2978240",
    "end": "2984960"
  },
  {
    "text": "bite at I * 64 for smaller elements sizes there let's see there are dedicated read instructions that inline",
    "start": "2984960",
    "end": "2991240"
  },
  {
    "text": "say an index multiplication by 8 but for this article let's see but for the stride of 64 the compiler has to",
    "start": "2991240",
    "end": "2996960"
  },
  {
    "text": "generate manual multiplications in the form of a shift additionally direct pointer based lookups can be let's say",
    "start": "2996960",
    "end": "3002640"
  },
  {
    "text": "can be slightly more efficient here than array indexing when doing self tree this we can effectively pre-compute the",
    "start": "3002640",
    "end": "3009079"
  },
  {
    "text": "pointer of the tree so that uh Ki still has to be added so let's look at that",
    "start": "3009079",
    "end": "3014680"
  },
  {
    "text": "okay so we're doing I see at this point I'm completely I'm I'm I'm lost in the sauce on whatever is happening right",
    "start": "3014680",
    "end": "3020400"
  },
  {
    "text": "here cuz this somehow looks like we're doing even more this is where the magic becomes so magical now we can avoid all",
    "start": "3020400",
    "end": "3025720"
  },
  {
    "text": "the multiplications by 64 just by multiplying all Ki by 64 to start with",
    "start": "3025720",
    "end": "3031119"
  },
  {
    "text": "okay somehow that's better let's go I guess upfront multiplication is better than doing it one at a time because it's",
    "start": "3031119",
    "end": "3037640"
  },
  {
    "text": "all linear it's all contiguous indeed the generated code now goes from 17 to 15 instructions and we can see figure",
    "start": "3037640",
    "end": "3043280"
  },
  {
    "text": "nine that this gives significant speed up again this whole like less instruction thing that part's",
    "start": "3043280",
    "end": "3051160"
  },
  {
    "text": "always confusing to me because I know that not all con not all instructions are simply one right some of them are",
    "start": "3051160",
    "end": "3056720"
  },
  {
    "text": "are five some of them are two some of them are all sorts stuff where's this uh where's this figure I want to see the figure wait what it's it's off the",
    "start": "3056720",
    "end": "3064720"
  },
  {
    "text": "screen brother brother it's off the screen all right all right let's see the",
    "start": "3064720",
    "end": "3069760"
  },
  {
    "text": "final version one let's see one peculiarity about the code above is that the andle uh this one on line six uh the",
    "start": "3069760",
    "end": "3076520"
  },
  {
    "text": "bit mask gets written there then in line seven it's pop counted life 11 does the",
    "start": "3076520",
    "end": "3082680"
  },
  {
    "text": "shell five I.E man multiplication by 32 which is a combination of the divide by",
    "start": "3082680",
    "end": "3088680"
  },
  {
    "text": "two uh to compensate for the double pop count and the Times by 64 then it does the and 64 where the mask of -64 this",
    "start": "3088680",
    "end": "3097319"
  },
  {
    "text": "which ends in six zeros but we can just let's see but we just multiplied by 32 so all this does is zeroing out a single",
    "start": "3097319",
    "end": "3104799"
  },
  {
    "text": "bit in case let's see in case the pop count was odd but we know for a fact that it can never be so we don't",
    "start": "3104799",
    "end": "3111119"
  },
  {
    "text": "actually need this and instruction to avoid this we do a little one of those to optimate to optimize or optimization",
    "start": "3111119",
    "end": "3118119"
  },
  {
    "text": "manually oh my gosh oh my gosh instead of divide by two you multiply by",
    "start": "3118119",
    "end": "3125040"
  },
  {
    "text": "32 what dude dude assembly breaks my smooth braid dude this that just like I",
    "start": "3125040",
    "end": "3131040"
  },
  {
    "text": "would just I mean how deep on an algorithm do you have to be to know that",
    "start": "3131040",
    "end": "3136119"
  },
  {
    "text": "instead of dividing by two you should multiply by 32 instead of shift left two you do a shift right five like that's",
    "start": "3136119",
    "end": "3144839"
  },
  {
    "text": "crazy okay okay batch final look at batch final it just does better the whole way through it does better all the",
    "start": "3145359",
    "end": "3151480"
  },
  {
    "text": "way through goodness gracious all right now that we know that the first three levels of the graph fit into L1 uh so",
    "start": "3151480",
    "end": "3158200"
  },
  {
    "text": "probably we can skip pre-etching of those levels okay nice skip pre-etch nice skip three look at that boom even",
    "start": "3158200",
    "end": "3165480"
  },
  {
    "text": "smoother oh my gosh as it turns out skipping the prefetch does not help probably because the prefetch is cheap",
    "start": "3165480",
    "end": "3170520"
  },
  {
    "text": "if the data is already available and there is a small chance that the data we need was evicted to make room for other things in which case prefetch is still",
    "start": "3170520",
    "end": "3177400"
  },
  {
    "text": "useful interleaf the other observation is that the first few layers of the CPU let's see are CPU bound while the last",
    "start": "3177400",
    "end": "3182880"
  },
  {
    "text": "few layers are memory throughput Bound by merging the two domains we should be able to get a higher total throughput uh",
    "start": "3182880",
    "end": "3188640"
  },
  {
    "text": "similar somewhat similar to how uh for a peie wise linear convex function classic",
    "start": "3188640",
    "end": "3194640"
  },
  {
    "text": "piecewise linear convex functions when X and Y are on different pieces thus maybe we could process two batches of queries",
    "start": "3194640",
    "end": "3201079"
  },
  {
    "text": "at the same time by processing layer I of one batch at the same time as layer I plus L / of the other batch where L is",
    "start": "3201079",
    "end": "3208119"
  },
  {
    "text": "the height of the tree I implemented this but unfortunately the result uh is not faster than what we had or maybe we",
    "start": "3208119",
    "end": "3214359"
  },
  {
    "text": "can split the work as interleaved uh the last level of one half with all but the last level of the other half since the",
    "start": "3214359",
    "end": "3220920"
  },
  {
    "text": "last level memory read takes most of the time also that turns out slower in",
    "start": "3220920",
    "end": "3226000"
  },
  {
    "text": "practice what does give a small speed up processing the first two levels of the next batch interleaved with the last",
    "start": "3226000",
    "end": "3231760"
  },
  {
    "text": "prefetch of the current batch still the result is only two nond speed up while the code uh while the code not shown",
    "start": "3231760",
    "end": "3238160"
  },
  {
    "text": "gets significantly more messy what does work great is interleaving all layers of the search when the tree has L layers we",
    "start": "3238160",
    "end": "3245520"
  },
  {
    "text": "can inter leave L batches at a time and then process layer I of the i in progress batch then we shift out the",
    "start": "3245520",
    "end": "3252839"
  },
  {
    "text": "completed batch and store the answers to those queries and shift in a new batch this way we completely average the",
    "start": "3252839",
    "end": "3259400"
  },
  {
    "text": "different workloads of all the layers and should achieve near Optimal Performance given the cpu's memory bandwidth to L3 and RAM at least that's",
    "start": "3259400",
    "end": "3266720"
  },
  {
    "text": "what I assume it's the bottleneck now inter leave all so this one just is",
    "start": "3266720",
    "end": "3271960"
  },
  {
    "text": "effectively a zero a zero zero Time Performance and then pops up and this thing is just even lower it's on the",
    "start": "3271960",
    "end": "3279280"
  },
  {
    "text": "order look at this it's on the order of like 27 28 Nan per check definitely casting a curse",
    "start": "3279280",
    "end": "3287559"
  },
  {
    "text": "on your CPU this is this is this is Mage level okay I you you have to wear an actual Mage hat to be able to do this so",
    "start": "3287559",
    "end": "3294040"
  },
  {
    "text": "far every internal node of the Tree stores the minimum of the sub Tree on its right yep okay it turns out somewhat",
    "start": "3294040",
    "end": "3300319"
  },
  {
    "text": "inefficient when searching values that are exactly in between two subt trees uh as also already suggested by",
    "start": "3300319",
    "end": "3306799"
  },
  {
    "text": "algorithmica such as 5.5 in the case of the search tree descends into the leftmost green subtree with node 2 to",
    "start": "3306799",
    "end": "3313160"
  },
  {
    "text": "four then goes into the rightmost red 4 five and then we realize 5.5 is greater",
    "start": "3313160",
    "end": "3318200"
  },
  {
    "text": "than five and thus we need the next value in the red layer which is sorted uh stored as a single array which is six",
    "start": "3318200",
    "end": "3325119"
  },
  {
    "text": "the problem now is that the red tree node uh nodes exactly correspond to uh to cash lines and thus the six will be",
    "start": "3325119",
    "end": "3332319"
  },
  {
    "text": "in a new Cash Line that needs to be fetched for memory now consider left Max oh my gosh we're about we're left maxing",
    "start": "3332319",
    "end": "3338920"
  },
  {
    "text": "left maxing holy cow we're left maxing okay what's going on here okay so the max values on the left as opposed to if",
    "start": "3338920",
    "end": "3344640"
  },
  {
    "text": "you look at the previous version the values were always right stored okay so we did right stored",
    "start": "3344640",
    "end": "3350079"
  },
  {
    "text": "instead now we're doing left store oh my gosh I I can't even imagine this being faster but here we go now if we search",
    "start": "3350079",
    "end": "3356000"
  },
  {
    "text": "for 5.5 we descend into the middle subtree rooted at 7 or 79 then we go left to 67 uh node and end up reading",
    "start": "3356000",
    "end": "3363240"
  },
  {
    "text": "six as the first value now the search directly steers toward the node that",
    "start": "3363240",
    "end": "3368599"
  },
  {
    "text": "actually contains the answer instead of the one just before Oh my gosh look at this inter leave all left Max dude left",
    "start": "3368599",
    "end": "3375000"
  },
  {
    "text": "maxing even makes it somehow faster why why are we left maxing memory layouts",
    "start": "3375000",
    "end": "3381319"
  },
  {
    "text": "now let's consider alternative memory layouts so far we're packing all layers in a forward order but I'll M post",
    "start": "3381319",
    "end": "3387280"
  },
  {
    "text": "actually stores them in Reverse so we'll try that too the query code is exactly the same but the order of the layers is",
    "start": "3387280",
    "end": "3393440"
  },
  {
    "text": "already encoded into the offsets another potential Improvement is always store the full array this may seem very",
    "start": "3393440",
    "end": "3399200"
  },
  {
    "text": "inefficient but it is actually not bad when we make sure to to use uninitialized memory in that case",
    "start": "3399200",
    "end": "3404760"
  },
  {
    "text": "untouched memory pages will simply never be mapped so we'll uh so that we waste",
    "start": "3404760",
    "end": "3409799"
  },
  {
    "text": "on average only about 2 megabytes per layer on huge pages are enabled and 14 megabytes when there are seven layers",
    "start": "3409799",
    "end": "3416039"
  },
  {
    "text": "and and the entire tree takes up 1 Gigabyte okay that's like nothing right all right so far we have been",
    "start": "3416039",
    "end": "3421960"
  },
  {
    "text": "using packed layout we will now try the rever layout used by algorithm and the",
    "start": "3421960",
    "end": "3427119"
  },
  {
    "text": "full layout that allows simple uh arithmetic for indexing yeah",
    "start": "3427119",
    "end": "3432920"
  },
  {
    "text": "see this now we've we've hit a land of just Insanity that I that just feels packed reversed full this just",
    "start": "3433319",
    "end": "3441079"
  },
  {
    "text": "seems crazy all right the benefit of storing a full array is that instead of using the offset we can simply compute",
    "start": "3441079",
    "end": "3447160"
  },
  {
    "text": "the index in the next layer directly as we did for the I singer search Okay B bam bam",
    "start": "3447160",
    "end": "3453799"
  },
  {
    "text": "bam all right that looks to be the same as it was before right left Max left Max",
    "start": "3453799",
    "end": "3459559"
  },
  {
    "text": "and and left Max Plus full seems to be the same value uh as it turns out neither of those layouts improve",
    "start": "3459559",
    "end": "3464680"
  },
  {
    "text": "performance so we will not use them going forward node size b equals 15 oh my gosh we can also try storing 15",
    "start": "3464680",
    "end": "3470480"
  },
  {
    "text": "values per node so that the branching factor is 16 this has a benefit of making multiplication B+ 1 so far",
    "start": "3470480",
    "end": "3476720"
  },
  {
    "text": "slightly simpler and it replaces x = -4 + x by xal oh Shift 4 oh oh my gosh",
    "start": "3476720",
    "end": "3482079"
  },
  {
    "text": "that's going to be huge I don't even know no it it doesn't look like it is huge well it kind of looks like it",
    "start": "3482079",
    "end": "3487640"
  },
  {
    "text": "there's some OD initial stuff but then I guess in ramland it doesn't it doesn't look like it matters when the tree has",
    "start": "3487640",
    "end": "3492680"
  },
  {
    "text": "up to uh five layers the data fits into L3 cache but B let's see using b equal 15 is indeed slightly faster when the",
    "start": "3492680",
    "end": "3498559"
  },
  {
    "text": "number of layers in the tree is the same on the other hand the lowering branching factor of 16 requires an additional layer for smaller sizes when uh then",
    "start": "3498559",
    "end": "3506280"
  },
  {
    "text": "when using a branching Factor 17 when the input is much larger than L3 cache the speed up disappears because Ram",
    "start": "3506280",
    "end": "3512599"
  },
  {
    "text": "throughput becomes the common bottleneck uh plain binary search and the ier layout have pretty much no overhead our",
    "start": "3512599",
    "end": "3518880"
  },
  {
    "text": "S3 so far has around 1/16 6.25% overhead 11/ 17th of the values in the final",
    "start": "3518880",
    "end": "3524720"
  },
  {
    "text": "layer is duplicated in the layer above and 117th of those uh is duplicated again and so on a total of this",
    "start": "3524720",
    "end": "3531880"
  },
  {
    "text": "11/16 using node sizes uh 15 instead increas es the overhead each node now",
    "start": "3531880",
    "end": "3538119"
  },
  {
    "text": "has to store 15 instead of 16 elements so that we uh that we already have an overhead of 115th oh my gosh furthermore",
    "start": "3538119",
    "end": "3544839"
  },
  {
    "text": "reduces branching Factor increases duplication to 11/16 to 115th as well overall overhead is 215 or 13.13 13.3%",
    "start": "3544839",
    "end": "3552240"
  },
  {
    "text": "while matches the dash Blue Line look at that okay I kind of get that that's kind of wild yeah see that was the whole",
    "start": "3552240",
    "end": "3559559"
  },
  {
    "text": "duplication thing I just I I'm sure there's a really great reason for why that works so much better the duplication why not just stop searching",
    "start": "3559559",
    "end": "3566240"
  },
  {
    "text": "when you hit a value right away I've never understood that all right of all the improvements so far the uh only the",
    "start": "3566240",
    "end": "3572480"
  },
  {
    "text": "interleaving is maybe a bit too much it is the only method that does not work batch by batch but really benefits from",
    "start": "3572480",
    "end": "3578480"
  },
  {
    "text": "having the full input at once and also its code is three times longer than the plain batched query methods because the",
    "start": "3578480",
    "end": "3584440"
  },
  {
    "text": "first and last uh few iterations of each Loop are handled separately prefix",
    "start": "3584440",
    "end": "3589839"
  },
  {
    "text": "partitioning so far we'll be let's see so far we've been doing a purely comparison bit search now it's time for",
    "start": "3589839",
    "end": "3595720"
  },
  {
    "text": "something new Partition the input values oh my gosh map reducing uh the simplest form of the idea is to Simply partition",
    "start": "3595720",
    "end": "3601960"
  },
  {
    "text": "the values by their top bit B bits into 2 B parts then we can uh build uh two to",
    "start": "3601960",
    "end": "3608799"
  },
  {
    "text": "the B independent search trees and search each query in one of them oh we're going to reduce the we're",
    "start": "3608799",
    "end": "3613880"
  },
  {
    "text": "effectively reducing the layers one by search or by we're reducing the amount of layers by increasing the amount of",
    "start": "3613880",
    "end": "3620640"
  },
  {
    "text": "subt trees we have is that effectively the idea this saves the first two levels of the search or slightly less uh",
    "start": "3620640",
    "end": "3625680"
  },
  {
    "text": "actually 2 16 equals 16 3 full layout all right in memory we can store these",
    "start": "3625680",
    "end": "3632160"
  },
  {
    "text": "trees very similarly to the full layout we had before the main difference that the first layers are skipped uh and that",
    "start": "3632160",
    "end": "3639119"
  },
  {
    "text": "let's see and that now there will be padding at the end of each part okay for some choices of B it could happen that",
    "start": "3639119",
    "end": "3646280"
  },
  {
    "text": "uh 15 let's see up to 15 out of 16 of each tree padding to reduce this overhead we attempt to shrink B while",
    "start": "3646280",
    "end": "3652160"
  },
  {
    "text": "keeping the height of all trees the same as long as all pairs of adjacent trees would fit together in the same space we",
    "start": "3652160",
    "end": "3658119"
  },
  {
    "text": "decrease B by one this way all parts will be filled for at least 50% when the elements are evenly distributed once",
    "start": "3658119",
    "end": "3664839"
  },
  {
    "text": "construction is done the code for quering is very similar to before we only have to start search for each query",
    "start": "3664839",
    "end": "3670160"
  },
  {
    "text": "at the index of its given part given that Q shift for some values of shift rather than the index of zero okay",
    "start": "3670160",
    "end": "3677960"
  },
  {
    "text": "interesting lot of mapping going on I don't even know like I don't understand what the what is the return value of",
    "start": "3677960",
    "end": "3684039"
  },
  {
    "text": "this is this some sort of iterator oh it does is it is it doing worse oh no but it's doing better on these smaller",
    "start": "3684039",
    "end": "3690640"
  },
  {
    "text": "sizes right what is this what is that what does this even mean we see that",
    "start": "3690640",
    "end": "3696559"
  },
  {
    "text": "indeed the partitioned tree has a space overhead varying between 0 to one making this not useful yet in practice because",
    "start": "3696559",
    "end": "3702760"
  },
  {
    "text": "let's see larger b reduced the height of the remaining trees and indeed we see that query uh queries are faster for",
    "start": "3702760",
    "end": "3709119"
  },
  {
    "text": "larger B especially for small trees there are significant speed up over interleaving somewhat surprisingly none",
    "start": "3709119",
    "end": "3714880"
  },
  {
    "text": "of the partition sizes has faster queries than interleaving for larger inputs or for large inputs also",
    "start": "3714880",
    "end": "3720240"
  },
  {
    "text": "important to note that while partitioning is very fast for sizes up to L1 cach this is only possible because",
    "start": "3720240",
    "end": "3725720"
  },
  {
    "text": "they have space overhead H uh that's the overhead I think okay trees are red yeah",
    "start": "3725720",
    "end": "3730880"
  },
  {
    "text": "trees are red it's Delta compare let's see uh compared to some let's see to some line similar visualizations was",
    "start": "3730880",
    "end": "3736920"
  },
  {
    "text": "used in the earlier figures yeah yeah I did remember seeing this in earlier figures I just don't really understand",
    "start": "3736920",
    "end": "3742760"
  },
  {
    "text": "it and why is it why is its performance worse in these ones and then slight it looks like to be I it's hard to even say",
    "start": "3742760",
    "end": "3749680"
  },
  {
    "text": "it's better at that point is it actually better you know I know compact uh subt trees just",
    "start": "3749680",
    "end": "3755960"
  },
  {
    "text": "like we used a packed layout before we can also do uh do that now by simply concatenating all the representations of",
    "start": "3755960",
    "end": "3761720"
  },
  {
    "text": "packed sub trees we ensure that all sub trees are still padded into the same total size but now we only added as much",
    "start": "3761720",
    "end": "3767920"
  },
  {
    "text": "padding as needed for the largest part rather than padding to full trees then we give each tree the same layout in",
    "start": "3767920",
    "end": "3773000"
  },
  {
    "text": "memory we'll have offsets uh o of where each layer starts in the first tree and",
    "start": "3773000",
    "end": "3778240"
  },
  {
    "text": "we store the constant size of the trees that way we can easily index each layer okay okay the code for quering does",
    "start": "3778240",
    "end": "3784760"
  },
  {
    "text": "become slightly more complicated it's already just insane as it is uh slightly more complicated now we must explicitly",
    "start": "3784760",
    "end": "3791240"
  },
  {
    "text": "track that uh the part that each query belongs to and compute all indices based on the layer offset the in layer offset",
    "start": "3791240",
    "end": "3799079"
  },
  {
    "text": "Ki and the part offset so much so much is happening right now it's like actual",
    "start": "3799079",
    "end": "3804960"
  },
  {
    "text": "magic is happening is that better I can't even tell if that's better at this point it's like it's all going right in",
    "start": "3804960",
    "end": "3811839"
  },
  {
    "text": "the same area except for this one might be like are we getting sub 25 sub 25 uh",
    "start": "3811839",
    "end": "3817680"
  },
  {
    "text": "for a fixed bmax memory overhead of the compact uh layout is small as long as",
    "start": "3817680",
    "end": "3823119"
  },
  {
    "text": "the input is sufficiently large and the trees have sufficiently many layers thus the tree could be practical unfortunately though queering them is",
    "start": "3823119",
    "end": "3829000"
  },
  {
    "text": "slightly slower than before because we must explicitly track the part of each query the best of both compact first",
    "start": "3829000",
    "end": "3835480"
  },
  {
    "text": "level as we just saw storing the trees one by one slows queries down so we would like to avoid that but on the",
    "start": "3835480",
    "end": "3841240"
  },
  {
    "text": "other hand the full layout can be waste spa or can waste space here we combine two ideas we would like to store the",
    "start": "3841240",
    "end": "3847520"
  },
  {
    "text": "horizontal concatenation of the packed trees each packed to the same size but this is complicated because then level",
    "start": "3847520",
    "end": "3853520"
  },
  {
    "text": "or then levels because then levels would have non-constant branching Factor instead we can fully emit the last two",
    "start": "3853520",
    "end": "3860359"
  },
  {
    "text": "level sub trees uh from each tree and Pad those subt trees that are present to full subt trees this way only the first",
    "start": "3860359",
    "end": "3867279"
  },
  {
    "text": "level has a configurable branching Factor B1 which can simply store after construction is done this layout takes",
    "start": "3867279",
    "end": "3873440"
  },
  {
    "text": "uh longer or this layout takes slightly more space than before because the sub trees must be full but the overhead",
    "start": "3873440",
    "end": "3879920"
  },
  {
    "text": "should typically be on the order of 116 for uniform data each tree should have",
    "start": "3879920",
    "end": "3885000"
  },
  {
    "text": "uh should have greater than nine sub trees which will only La let's see which will only which only the last is not",
    "start": "3885000",
    "end": "3891240"
  },
  {
    "text": "full okay there's the non like the uncompacted ones here's this crazy searching dude this is such a",
    "start": "3891240",
    "end": "3899079"
  },
  {
    "text": "wild thing look at this dude the bro is just trying every single method in the universe to get this sub 25 crazy crazy",
    "start": "3899079",
    "end": "3908640"
  },
  {
    "text": "overlapping trees a drawback of all the methods is that the memory usage is heavily influenced by the largest part",
    "start": "3908640",
    "end": "3914440"
  },
  {
    "text": "since all parts must be uh must be at least as large this is especially a",
    "start": "3914440",
    "end": "3920720"
  },
  {
    "text": "problem when the distribution of the part sizes is very skewed we can avoid this by sharing storage between adjacent",
    "start": "3920720",
    "end": "3926160"
  },
  {
    "text": "trees let SP be the number of sub trees for each part P and S Max equals Max P",
    "start": "3926160",
    "end": "3931480"
  },
  {
    "text": "SP then we can Define the overlap right here and let's see and depend only B1 s",
    "start": "3931480",
    "end": "3937400"
  },
  {
    "text": "Max minus V new subt trees for each new part rather than S Max as we did before",
    "start": "3937400",
    "end": "3942599"
  },
  {
    "text": "the values for each part are then simply appended to where the previous part left off unless the sub tree is Out Of Reach",
    "start": "3942599",
    "end": "3948799"
  },
  {
    "text": "for the current part in which case uh first some padding is added the this way",
    "start": "3948799",
    "end": "3953920"
  },
  {
    "text": "constructing or consecutive parts can overlap and exchange memory and we can somewhat buffer the effect of larger",
    "start": "3953920",
    "end": "3959599"
  },
  {
    "text": "Parts oh interesting you're just you're effectively shifting right yeah cuz these ones you'll notice these are in",
    "start": "3959599",
    "end": "3965920"
  },
  {
    "text": "the middle these are right here and then they all go off to the right Interesting Man wild overlapping this is I mean this",
    "start": "3965920",
    "end": "3973640"
  },
  {
    "text": "guy knows be trees probably better than anybody in the universe this guy his name is literally B tree uh when the",
    "start": "3973640",
    "end": "3980160"
  },
  {
    "text": "overlap is one the let's see as in the above example the nodes of the first layer each contain the maximum value of",
    "start": "3980160",
    "end": "3985599"
  },
  {
    "text": "B sub trees when the overlap is larger than one the nodes in the first layer would contain overlapping values instead",
    "start": "3985599",
    "end": "3991240"
  },
  {
    "text": "we store a single list of values in which we can undo aligned reads to get the right slice of B values that we",
    "start": "3991240",
    "end": "3999119"
  },
  {
    "text": "need dang is the goal just to the goal must be to reduce this part right here",
    "start": "4002799",
    "end": "4009599"
  },
  {
    "text": "we're no longer even trying to reduce this part we're just trying to reduce this part right here human data so far",
    "start": "4009599",
    "end": "4015640"
  },
  {
    "text": "we've been testing on uniformed random data where the largest part uh deviates from the mean size around square root of",
    "start": "4015640",
    "end": "4021240"
  },
  {
    "text": "n now let's look at some real data K Ms of the human genome DNA consists of AC",
    "start": "4021240",
    "end": "4026960"
  },
  {
    "text": "GT characters that can be encoded as two bits so each string of K 16 characters defines 3 two bit integer we then look",
    "start": "4026960",
    "end": "4034640"
  },
  {
    "text": "at the first nend cers of the human genome starting at chromosome 1 to give",
    "start": "4034640",
    "end": "4039680"
  },
  {
    "text": "an idea the plot uh below shows that each CER of length k equals 12 how often",
    "start": "4039680",
    "end": "4045119"
  },
  {
    "text": "it occur in the full Human Genome in total there are around three G cers wait",
    "start": "4045119",
    "end": "4050440"
  },
  {
    "text": "is that 3G is this what 5G effects there you go sorry Bill Gates got me a little",
    "start": "4050440",
    "end": "4055640"
  },
  {
    "text": "bit on the memory layout Bill Gates got me on the memory layout there for a second hold on we're back we are so back",
    "start": "4055640",
    "end": "4062960"
  },
  {
    "text": "we're in the middle of some cash missing chromosones all right all right I am curious how much faster this makes it",
    "start": "4062960",
    "end": "4068480"
  },
  {
    "text": "all right as he let's see all right to give an idea the plot below shows that each KR length K12 let's see how often",
    "start": "4068480",
    "end": "4074960"
  },
  {
    "text": "it occurs in the full in the full Human Genome in in total there are around three G cers and also the expected count",
    "start": "4074960",
    "end": "4081799"
  },
  {
    "text": "for each CER is around 200 but instead we see that Camas that occur over 2 million times so if we were to partition",
    "start": "4081799",
    "end": "4088240"
  },
  {
    "text": "on the first 24 bits uh the size of the largest part is around 2 to the -10 uh",
    "start": "4088240",
    "end": "4094839"
  },
  {
    "text": "of the input rather than 2 to the -4 the accumulated accounts are shown in Orange",
    "start": "4094839",
    "end": "4100719"
  },
  {
    "text": "where we also see the number of flat regions uh caused by unrepresented cers",
    "start": "4100719",
    "end": "4106278"
  },
  {
    "text": "okay okay okay we're going to get into some more here we go we're going into",
    "start": "4106279",
    "end": "4111920"
  },
  {
    "text": "more uh we need a way to handle unbalanced partition sizes instead of mapping everything linearly we can only",
    "start": "4111920",
    "end": "4117000"
  },
  {
    "text": "do this by simply storing the full tree compactly as we did before uh proceeded by an array in blue that points to the",
    "start": "4117000",
    "end": "4123920"
  },
  {
    "text": "indices of the first sub tree containing elements of the uh of that part like the overlapping tree before the first layer",
    "start": "4123920",
    "end": "4130238"
  },
  {
    "text": "is simply a list of larger elements of all subt trees that can be indexed anywhere potentially unlined",
    "start": "4130239",
    "end": "4136040"
  },
  {
    "text": "okay let's go let's go let's see to answer the let's see to answer a query we first find its parts and then read",
    "start": "4136040",
    "end": "4142120"
  },
  {
    "text": "the block 16 elements starting with the pointed Two element and then proceed as usual from the subtree onward doe",
    "start": "4142120",
    "end": "4147440"
  },
  {
    "text": "imagine oh my gosh she's even into unsafe land brother's just unsafe and hard",
    "start": "4147440",
    "end": "4153640"
  },
  {
    "text": "right now crazy there we go we're reading we're reading fast let's see in the code the only thing that changes",
    "start": "4153640",
    "end": "4159600"
  },
  {
    "text": "compared to the previous overlapping version is that instead of computing the start index linearly and adapting the element layout accordingly we use prefix",
    "start": "4159600",
    "end": "4166000"
  },
  {
    "text": "maps to jump directly to the right place in the packed tree representation all right let's say although the memory",
    "start": "4166000",
    "end": "4171400"
  },
  {
    "text": "usage is now similar to the unpar partition version queries for large inputs are slightly slower than those",
    "start": "4171400",
    "end": "4176719"
  },
  {
    "text": "previous layouts due to the additional index required we can also again do the interleaving queries these uh are",
    "start": "4176719",
    "end": "4182920"
  },
  {
    "text": "slightly faster for small inputs and around as fast as interleaving was without the partitioning man I assume",
    "start": "4182920",
    "end": "4189758"
  },
  {
    "text": "this is just I I assume this equates to real dollars cuz the remember this I",
    "start": "4189759",
    "end": "4194760"
  },
  {
    "text": "believe this right here no wait no no not this line that was the let's see where where's the I singer one I believe",
    "start": "4194760",
    "end": "4200840"
  },
  {
    "text": "this one right here this is what was like the known fastest one which is just out of this world slower comparatively",
    "start": "4200840",
    "end": "4207440"
  },
  {
    "text": "this is one hell of a Monday uh opening art article yeah dude it's crazy on uh human data we see that the partition",
    "start": "4207440",
    "end": "4213920"
  },
  {
    "text": "index is a bit faster in L1 and L2 and consistently saves the time roughly one layer in L3 for larger indices",
    "start": "4213920",
    "end": "4220239"
  },
  {
    "text": "performance is still very similar to not using partitioned at all okay amazing",
    "start": "4220239",
    "end": "4225400"
  },
  {
    "text": "summary okay here's the summary look at look at that poor binary search just out the window summary overall it seems that",
    "start": "4225400",
    "end": "4231440"
  },
  {
    "text": "partitioning does not provide uh does not provide when we already enter leave queries all right multi-thread M",
    "start": "4231440",
    "end": "4237199"
  },
  {
    "text": "multi-threaded comparisons oh brother we're going in on it now oh my gosh we're down to like no none of",
    "start": "4237199",
    "end": "4243880"
  },
  {
    "text": "them look at that dude that's crazy it's getting all the way down to like six seven eight 8 milliseconds when uh or",
    "start": "4243880",
    "end": "4251960"
  },
  {
    "text": "nanc sorry uh when using 16 thre or six threads runtime goes down to 27 uh to 7",
    "start": "4251960",
    "end": "4257920"
  },
  {
    "text": "wow that's fast given the speed up is less than 4X so we are now bottlenecked by total RAM throughput and indeed these",
    "start": "4257920",
    "end": "4263640"
  },
  {
    "text": "methods are slower for a single thread also reach near optimal throughput now conclusion it's crazy that it went from",
    "start": "4263640",
    "end": "4271199"
  },
  {
    "text": "that 1.1 microc to effectively 27 NCS",
    "start": "4271199",
    "end": "4277400"
  },
  {
    "text": "and then multi-threading it getting all the way down to like nothing right getting it all the way down to absolutely nothing dude that's that's",
    "start": "4277400",
    "end": "4284800"
  },
  {
    "text": "that's wild I mean that that really is like what that's like that's more than 40x like this is 40x but the other one",
    "start": "4284800",
    "end": "4292360"
  },
  {
    "text": "is much less wow anyways let's see with interwave query is 40x speed up a large part of the Improvement is due to",
    "start": "4292360",
    "end": "4298040"
  },
  {
    "text": "batching queries and prefetching upcoming notes to get even higher throughput interleaving queries at different levels helps balance the CPU",
    "start": "4298040",
    "end": "4304040"
  },
  {
    "text": "bound part of the computation with memory bounded part so that we get overall higher throughput using a 15",
    "start": "4304040",
    "end": "4309880"
  },
  {
    "text": "element per node instead of 16 to improve throughput somewhat but doubles the overhead of the data structure from 16 1.25 to 13.3% for inputs that fit in",
    "start": "4309880",
    "end": "4318239"
  },
  {
    "text": "the L3 cache that's fine and speed up is worthwhile while the larger inputs the speed is memory bounded anyway so there",
    "start": "4318239",
    "end": "4323600"
  },
  {
    "text": "is no speed up while additional memory requirements are somewhat large we also looked into partitioning the data by",
    "start": "4323600",
    "end": "4328880"
  },
  {
    "text": "prefix while this does give some speed up it turns out that it on skewed input data the benefits quickly diminish since",
    "start": "4328880",
    "end": "4335320"
  },
  {
    "text": "the tree is either requires a lot of buffer space or else requires an additional lookup to match each part to",
    "start": "4335320",
    "end": "4340560"
  },
  {
    "text": "its location in the first level of the tree in the end I'd say an additional complexity and dependency on the shape of the input of partitioning is not",
    "start": "4340560",
    "end": "4347400"
  },
  {
    "text": "worth the speed up compared to Simply using interleaved queries directly binary search all methods were",
    "start": "4347400",
    "end": "4352840"
  },
  {
    "text": "considered are branchless and use the exact number of iterations for each query especially in combination with",
    "start": "4352840",
    "end": "4358560"
  },
  {
    "text": "partitioning it may be possible to handle few large Parts independently from the usually smaller parts that way",
    "start": "4358560",
    "end": "4364400"
  },
  {
    "text": "we could answer most queries with slightly fewer iterations on the other hand the layers saved would mostly be",
    "start": "4364400",
    "end": "4370000"
  },
  {
    "text": "quick lookups near the root of the tree and introduce branches to the code possibly uh possibly cause quite a bit",
    "start": "4370000",
    "end": "4377000"
  },
  {
    "text": "of delay due to mispredictions interpolation search as we saw the last plot above total Ram throughput rather",
    "start": "4377000",
    "end": "4383520"
  },
  {
    "text": "than per core throughput becomes a bottleneck once we uh once we're using multiple threads thus the only way to",
    "start": "4383520",
    "end": "4389960"
  },
  {
    "text": "improve total query throughput is to strict use strictly fewer Ram accesses per query prefix lookups won't help",
    "start": "4389960",
    "end": "4397000"
  },
  {
    "text": "since they only replace the layers of the tree that would otherwise fit in the cache instead we use interpolation",
    "start": "4397000",
    "end": "4402040"
  },
  {
    "text": "search Wikipedia where the estimated position of query q is linearly interpretated between known positions of",
    "start": "4402040",
    "end": "4408440"
  },
  {
    "text": "surrounding elements on random data this only takes log log n iterations rather than log n for binary search and could",
    "start": "4408440",
    "end": "4415400"
  },
  {
    "text": "save Ram accesses on the other hand when the data is not random its worst case performance is n rather than",
    "start": "4415400",
    "end": "4421199"
  },
  {
    "text": "statistically bounded to log in oo you don't want that you you don't want that oen the pla index AB Med uh Medvedev uh",
    "start": "4421199",
    "end": "4432360"
  },
  {
    "text": "also uses a single interpolation St in precisely constru Ed piece wise linear approximation the airor after the",
    "start": "4432360",
    "end": "4438440"
  },
  {
    "text": "approximation is determined by some Global upper bound so that the number of remaining search steps can be bounded as well packing data smaller another option",
    "start": "4438440",
    "end": "4445320"
  },
  {
    "text": "is to use the ram lookups more efficiently would be to pack the values into 16 bits rather than 32bits we've",
    "start": "4445320",
    "end": "4452280"
  },
  {
    "text": "been using so far especially if we uh if we first do a 16bit prefix lookup we",
    "start": "4452280",
    "end": "4458760"
  },
  {
    "text": "already know that those bits uh those bits anyways it so it would suffice to",
    "start": "4458760",
    "end": "4464159"
  },
  {
    "text": "only compare the last 16 bits of the querian values this increases the branching factor from 17 to 33 which",
    "start": "4464159",
    "end": "4470239"
  },
  {
    "text": "reduces the number of layers of the tree around 1.5 for inputs of 1 Gigabyte another option would also suggest an ant",
    "start": "4470239",
    "end": "4476280"
  },
  {
    "text": "6n on Hacker News or buy ant 6n on Hacker News hey thanks an Anton uh would",
    "start": "4476280",
    "end": "4482199"
  },
  {
    "text": "be some kind of variable depth encoding where the root noes store say the top 16 bits of every value and as we go down",
    "start": "4482199",
    "end": "4489120"
  },
  {
    "text": "the tree we store some kind of middle 16 bits uh skipping the first P bits that are shared between all elements in the",
    "start": "4489120",
    "end": "4495040"
  },
  {
    "text": "the bucket oh some sort of like Jump Ahead is that like is that like some sort of iteration like jump Point search",
    "start": "4495040",
    "end": "4501000"
  },
  {
    "text": "uh returning indices to the original data uh for various applications it may be May let's see it may be helpful to",
    "start": "4501000",
    "end": "4507560"
  },
  {
    "text": "not only return the smallest value of q but also the index in the original list of the sorted values for example when",
    "start": "4507560",
    "end": "4513239"
  },
  {
    "text": "storing an array with additional data for each item since we use S Plus tree that stores all data in the bottom layer",
    "start": "4513239",
    "end": "4519360"
  },
  {
    "text": "this is mostly straightforward the prefix map partition tree also natively supports this while the other partitioned variants do not they include",
    "start": "4519360",
    "end": "4525719"
  },
  {
    "text": "buffer SL padding elements in their bottom layer and hence we do not need to store and look up position offset of",
    "start": "4525719",
    "end": "4532080"
  },
  {
    "text": "each separately this it is so wild how deep you can go like my right now my biggest takeaway is even things like",
    "start": "4532080",
    "end": "4539280"
  },
  {
    "text": "seemingly uh even something seemingly as uh simple as as a b tree you would",
    "start": "4539280",
    "end": "4545280"
  },
  {
    "text": "assume that it's kind of like figured out and this is just like going on and",
    "start": "4545280",
    "end": "4550639"
  },
  {
    "text": "going on and going on and going on just showing like how wildly uh amazing like",
    "start": "4550639",
    "end": "4556679"
  },
  {
    "text": "research can still go and I'm having s significant skill issues right now let's see we could extend the current query",
    "start": "4556679",
    "end": "4562280"
  },
  {
    "text": "method to version that returns both the first value and the first value uh Q so that the range positions corresponding",
    "start": "4562280",
    "end": "4568639"
  },
  {
    "text": "to Value Q can be determined in practice the easiest way to do this is by simply doubling the queries q and q+ one in",
    "start": "4568639",
    "end": "4575159"
  },
  {
    "text": "this case uh this will cause some CPU overhead in the initial layers but the extra query execution will remain Branch",
    "start": "4575159",
    "end": "4580880"
  },
  {
    "text": "free when Q is not found only let's see or only occurs a few times they will most mostly fetch the same",
    "start": "4580880",
    "end": "4587840"
  },
  {
    "text": "cach lines so that the memory is efficiently reused with the bandwidth oh nice oh that makes sense oh that's cool",
    "start": "4587840",
    "end": "4593679"
  },
  {
    "text": "in practice though it this seems only around 20% faster per individual query",
    "start": "4593679",
    "end": "4599480"
  },
  {
    "text": "for 4 gigabyte input so around 60% slower for a range of single query uh",
    "start": "4599480",
    "end": "4604960"
  },
  {
    "text": "for small inputs the speed up is less and sometimes querying ranges is even more than twice as slow for individual",
    "start": "4604960",
    "end": "4611000"
  },
  {
    "text": "random queries sorting queries another way we did or another thing we did not",
    "start": "4611000",
    "end": "4616159"
  },
  {
    "text": "uh at all consider so far but was brought up by uh oral on Hacker News is",
    "start": "4616159",
    "end": "4621320"
  },
  {
    "text": "to batch queries we let's see if we assume for the moment that the queries are sorted we know that we have a",
    "start": "4621320",
    "end": "4627480"
  },
  {
    "text": "maximum Poss see maximum possible reusing of all nodes so they they all need to be fetched from memory only once",
    "start": "4627480",
    "end": "4634320"
  },
  {
    "text": "if the number of queries say uh is large say at least one let's see n divided by",
    "start": "4634320",
    "end": "4639520"
  },
  {
    "text": "16 then many nodes at the last level will have more than one query hitting them and ing them only once will reduce",
    "start": "4639520",
    "end": "4646120"
  },
  {
    "text": "memory pressure similar if we have at least around n ID 256 queries we can avoid prefetching before last layer uh",
    "start": "4646120",
    "end": "4653719"
  },
  {
    "text": "nodes multiple times in practice I'm not quite sure how much time the Sorting of the queries would take but something",
    "start": "4653719",
    "end": "4660040"
  },
  {
    "text": "simple would uh be to do one or two rounds of 8 bit 8 bit radic sort so we",
    "start": "4660040",
    "end": "4665760"
  },
  {
    "text": "sort 256 2 to the 16 or 65536 uh parts and we skip the uh skip",
    "start": "4665760",
    "end": "4672760"
  },
  {
    "text": "the first two or four first layer of search huh I don't know how that actually work that seems interesting",
    "start": "4672760",
    "end": "4679400"
  },
  {
    "text": "seems neat uh suffix R searching the next step of this project is to integrate uh this into a fast suffix",
    "start": "4679400",
    "end": "4684440"
  },
  {
    "text": "array search scheme the idea is to build this S3 on say every fourth suffix and",
    "start": "4684440",
    "end": "4690159"
  },
  {
    "text": "then use the first 32 bits or maybe 64 of each suffix as the value of the est3 given a query we can then quickly",
    "start": "4690159",
    "end": "4696320"
  },
  {
    "text": "determine the range corresponding to its first 32 bits and the binary search only in the likely small remaining range to",
    "start": "4696320",
    "end": "4701719"
  },
  {
    "text": "determine the final slice of the suffix array that corresponds to the query and then a bunch of references this is crazy",
    "start": "4701719",
    "end": "4708639"
  },
  {
    "text": "wow absolutely insane research you know my brain too small my brain too small",
    "start": "4708639",
    "end": "4713800"
  },
  {
    "text": "too uh it's very interesting though it's very interesting all the things that can be done I think all this all this shows",
    "start": "4713800",
    "end": "4719679"
  },
  {
    "text": "me is just that computer science is an infinite skill Gap you can keep going",
    "start": "4719679",
    "end": "4725639"
  },
  {
    "text": "and you can keep on getting better and better to the point where whatever you're working on there's so much more",
    "start": "4725639",
    "end": "4732840"
  },
  {
    "text": "that you can go you know this guy this guy is a wizard like but here's the best part is that just because he's good in",
    "start": "4732840",
    "end": "4738360"
  },
  {
    "text": "one thing doesn't mean he's good in all things and so that's like actually kind of cool in the sense that you can pick a",
    "start": "4738360",
    "end": "4744800"
  },
  {
    "text": "topic and become really good at a specific topic yeah he's working on his",
    "start": "4744800",
    "end": "4750360"
  },
  {
    "text": "PhD in bioinformatics right so it's like really cool so it's like there's something magical about becoming an uh",
    "start": "4750360",
    "end": "4756040"
  },
  {
    "text": "an expert in something and so there's just really it just it just it it makes me excited more than anything else and",
    "start": "4756040",
    "end": "4762440"
  },
  {
    "text": "for the last couple years I've been trying to decide what what is like the next what is like the next thing I want",
    "start": "4762440",
    "end": "4768239"
  },
  {
    "text": "to become an expert on and I've been kind of playing around a whole bunch with um just different languages kind of",
    "start": "4768239",
    "end": "4774360"
  },
  {
    "text": "really trying to figure out what's the next place I really want to get good at and I can slowly feel like the place",
    "start": "4774360",
    "end": "4779679"
  },
  {
    "text": "that I want to go and expertise out is probably going to end up being Zig I think it's",
    "start": "4779679",
    "end": "4785320"
  },
  {
    "text": "probably the nicest I so I have it I have options to do it either in",
    "start": "4785320",
    "end": "4790880"
  },
  {
    "text": "Odin to do it in zigg or to do it and uh Jay and Jay's pretty interesting Odin's",
    "start": "4790880",
    "end": "4798800"
  },
  {
    "text": "really interesting and zig's really interesting but Zig just seems like it has like the zig just seems like it has",
    "start": "4798800",
    "end": "4804159"
  },
  {
    "text": "the most amount of uh ability to get really interesting right well I did I I",
    "start": "4804159",
    "end": "4809520"
  },
  {
    "text": "wanted to do like I said I was going to do one year goang for those that are like pivoting again no I said I was going to do one year of goang and we did",
    "start": "4809520",
    "end": "4816040"
  },
  {
    "text": "2024 was all goang the only thing is that I did take a one- month detour and",
    "start": "4816040",
    "end": "4821159"
  },
  {
    "text": "did zigg and I really enjoyed my time in Zig so I was like okay that was really fun that was uh in 2024 I was looking",
    "start": "4821159",
    "end": "4828520"
  },
  {
    "text": "ahead to what I wanted to do in 2025 and in 2025 I'm pretty much stuck between doing stuff in Elixir or doing stuff in",
    "start": "4828520",
    "end": "4835560"
  },
  {
    "text": "Zig I either want to go I either want to go functional which is going to be o camel or",
    "start": "4835560",
    "end": "4841400"
  },
  {
    "text": "Elixir or I want to go like manual memory which is going to be something like zigg uh Odin or ji or j i I don't",
    "start": "4841400",
    "end": "4850600"
  },
  {
    "text": "know how to say j or J I'm not sure which one to say right and so it's like I'm kind of like in this world right",
    "start": "4850600",
    "end": "4856960"
  },
  {
    "text": "now uh i' I've done C++ I don't I've done C and C++ professionally I don't",
    "start": "4856960",
    "end": "4862880"
  },
  {
    "text": "want to do that again right I've already done it I don't want to do it again right I I don't want to you know you",
    "start": "4862880",
    "end": "4869159"
  },
  {
    "text": "went de you went deep inside right and so it's like I I feel like and but but anyways I the reason why I say all these",
    "start": "4869159",
    "end": "4875360"
  },
  {
    "text": "things is to like encourage you like there's so much space that you can infinitely extend yourself on right like",
    "start": "4875360",
    "end": "4882040"
  },
  {
    "text": "you don't you don't have to you don't like programming doesn't just simply have to be you putting boxes on the screen and",
    "start": "4882040",
    "end": "4888960"
  },
  {
    "text": "making them colorful and making them pretty and then taking that and when it's clicked it goes through some",
    "start": "4888960",
    "end": "4894880"
  },
  {
    "text": "arbitrarily complex abstracted uh kind of requesting algorithm which then goes",
    "start": "4894880",
    "end": "4900360"
  },
  {
    "text": "on and goes over here onto the back end which then goes in here and calls out a database or reddis or whatever caching",
    "start": "4900360",
    "end": "4907080"
  },
  {
    "text": "layer then Returns the response and you update this to a check mark like that that doesn't this this doesn't have to",
    "start": "4907080",
    "end": "4913360"
  },
  {
    "text": "be the end programming you can keep on going right you can keep on going deeper",
    "start": "4913360",
    "end": "4919480"
  },
  {
    "text": "and deeper and deeper and it does not have to end with a little bit of crud app right somebody has to write the DB",
    "start": "4919480",
    "end": "4925719"
  },
  {
    "text": "in the first place somebody has to write the DB in the first place but the thing is is what you do professionally versus",
    "start": "4925719",
    "end": "4930800"
  },
  {
    "text": "what you do personally they can be different too because eventually if you go long enough in the personal realm and",
    "start": "4930800",
    "end": "4936719"
  },
  {
    "text": "you find the thing you really like to do and you really want to keep on keep on building stuff with it you will find",
    "start": "4936719",
    "end": "4942560"
  },
  {
    "text": "that you get really really good at it and eventually your personal life becomes your professional life and this",
    "start": "4942560",
    "end": "4948920"
  },
  {
    "text": "is where you achieve the quote unquote like dream which is amazing how about",
    "start": "4948920",
    "end": "4954000"
  },
  {
    "text": "you explore Hardware I've done I I I did a lot of hardware at one point in my life I did it professionally in C I",
    "start": "4954000",
    "end": "4959080"
  },
  {
    "text": "wrote uh network drivers I wrote flash drivers I wrote uh planetary pancake",
    "start": "4959080",
    "end": "4964159"
  },
  {
    "text": "motor algorithms uh I helped effectively the government be able to create an invisible fence and which is uses liar",
    "start": "4964159",
    "end": "4972440"
  },
  {
    "text": "uh to be able to tell if there's been volum changes within 200 M and then point a",
    "start": "4972440",
    "end": "4978199"
  },
  {
    "text": "camera at it and then take video of that and send it back and then I created a network of nodes uh this is right out of",
    "start": "4978199",
    "end": "4985280"
  },
  {
    "text": "uh College by the way create a network of nodes that would like kind of mesh forward this to a command center and so",
    "start": "4985280",
    "end": "4991440"
  },
  {
    "text": "it would just be able to figure out all those things this is kind of why I got into Network stuff to begin with network stuff has always been fairly fun because",
    "start": "4991440",
    "end": "4997000"
  },
  {
    "text": "creating your own um creating your own uh Network protocol it's just it's just cool it's just fun it's just a good time",
    "start": "4997000",
    "end": "5004880"
  },
  {
    "text": "uh but that was a that was a lot of fun so I don't really want to do but I don't really want to keep on doing um uh",
    "start": "5004880",
    "end": "5010480"
  },
  {
    "text": "what's it called I probably I I don't really want to keep doing Hardware I think Hardware is neat but it's like",
    "start": "5010480",
    "end": "5015679"
  },
  {
    "text": "neat as in it's neat as in Hobby it's not neat as in professional for me right",
    "start": "5015679",
    "end": "5021199"
  },
  {
    "text": "if that makes sense like it's not something I want to do all the time at all points in my life I really like",
    "start": "5021199",
    "end": "5026639"
  },
  {
    "text": "building out dumb programs I really want to keep on going in this and I'm going to finish out the rust version of this",
    "start": "5026639",
    "end": "5032239"
  },
  {
    "text": "program and I'm going to make this uh I'm I want all right I I am finishing out the last version of this and this is",
    "start": "5032239",
    "end": "5038960"
  },
  {
    "text": "going to be my last rewrite it in Rust uh note uh this is the last rear right",
    "start": "5038960",
    "end": "5046800"
  },
  {
    "text": "this is my last rear I'm I'm just over it right anyways there",
    "start": "5046800",
    "end": "5052159"
  },
  {
    "text": "you go that's all I wanted to say this is great this is fantastic and I hope hopefully this is more of an encouragement note at the end right",
    "start": "5052159",
    "end": "5058719"
  },
  {
    "text": "there you go the name is the primagen",
    "start": "5058719",
    "end": "5063159"
  }
]