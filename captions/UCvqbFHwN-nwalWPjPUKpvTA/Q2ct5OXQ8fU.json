[
  {
    "text": "thanks folks for coming uh today Nay and I here from Apple we're going to talk a little bit about the journey we had from",
    "start": "80",
    "end": "6799"
  },
  {
    "text": "moving our Spark workloads from legacy Hadoop clusters um and overall to the",
    "start": "6799",
    "end": "12160"
  },
  {
    "text": "Kubernetes kind of um ecosystem a number of years ago when Kubernetes was maybe",
    "start": "12160",
    "end": "19039"
  },
  {
    "text": "not as nice as it is today so you'll hear some war stories there as well",
    "start": "19039",
    "end": "24119"
  },
  {
    "text": "so so in in terms of agenda we're going to talk a bit about you know going from bare metal to Kubernetes and how we had",
    "start": "24119",
    "end": "30640"
  },
  {
    "text": "to evolve kind of Spark techniques over time to make that work and you'll kind of see like we went we went through one",
    "start": "30640",
    "end": "37280"
  },
  {
    "text": "other type of uh uh compute uh story in between going from bare metal to",
    "start": "37280",
    "end": "42680"
  },
  {
    "text": "Kubernetes and then ultimately now being on Kubernetes with u advanceduler like",
    "start": "42680",
    "end": "48600"
  },
  {
    "text": "unicorn also super excited to learn about all the developments in Q and that those have kind of come along in the",
    "start": "48600",
    "end": "54719"
  },
  {
    "text": "last couple of weeks and then kind of key takeaways for like making Spark work well on Kubernetes because you know",
    "start": "54719",
    "end": "60480"
  },
  {
    "text": "traditionally Kubernetes is not built for um let's a data inensive",
    "start": "60480",
    "end": "65720"
  },
  {
    "text": "workloads so we'll talk a little bit about best practice and pitfalls and antiatterns to kind of stay away",
    "start": "65720",
    "end": "71720"
  },
  {
    "text": "from in terms of objective objectives in the migration is like we have a lot of data engineers and um data analysts who",
    "start": "71720",
    "end": "79520"
  },
  {
    "text": "are using spark and they need interactive access to large data sets uh",
    "start": "79520",
    "end": "84799"
  },
  {
    "text": "we really kind of want to separate storage and compute like kind of that disagregation over the last six or more",
    "start": "84799",
    "end": "90799"
  },
  {
    "text": "years right being able to put your data in cloud storage or some uh storage system that doesn't have your compute",
    "start": "90799",
    "end": "97119"
  },
  {
    "text": "coupled to it gives you a lot of flexibility then we also will talk about cluster autoscaling and then",
    "start": "97119",
    "end": "102640"
  },
  {
    "text": "containerization and dependency management and then ultimately like better resource utilization is a very",
    "start": "102640",
    "end": "107920"
  },
  {
    "text": "key part of this to allow us to kind of reduce cost and then automating as much of this as possible",
    "start": "107920",
    "end": "114320"
  },
  {
    "text": "So with that I'll hand it over to one of our best minds Nha to tell you more about this journey",
    "start": "114320",
    "end": "122040"
  },
  {
    "text": "hey everyone my name is Nha Singla uh I'm a software engineer at Apple um",
    "start": "122159",
    "end": "127920"
  },
  {
    "text": "thanks for sticking with us uh this evening i appreciate it i know it's been a long day so uh I'm going to talk about",
    "start": "127920",
    "end": "136080"
  },
  {
    "text": "uh some of the special requirements uh with interactive spark so interactive",
    "start": "136080",
    "end": "141280"
  },
  {
    "text": "spark plays a cru crucial role in like big data processing data science and",
    "start": "141280",
    "end": "146720"
  },
  {
    "text": "machine learning workflows for faster uh data exploration debugging and ML experiments and uh Jupyter notebook is",
    "start": "146720",
    "end": "153519"
  },
  {
    "text": "one of the popular tool used in industry for interactive spark experiments so",
    "start": "153519",
    "end": "158640"
  },
  {
    "text": "here I'm going to talk about some of the unique requirements uh requirements which interactive spark brings in um so",
    "start": "158640",
    "end": "166480"
  },
  {
    "text": "interactive spark needs to provide uh low latency responses for querying and processing large data sets this helps",
    "start": "166480",
    "end": "173920"
  },
  {
    "text": "data scientists to uh and analysts to iterate faster on their experiment and uh without waiting for them for",
    "start": "173920",
    "end": "180480"
  },
  {
    "text": "longunning bad jobs port launch latency matters in interactive cases where you don't want",
    "start": "180480",
    "end": "187120"
  },
  {
    "text": "your users to uh wait longer for running their experiments in a multi-tenant",
    "start": "187120",
    "end": "193920"
  },
  {
    "text": "environment where users are sharing resources uh real-time monitoring as well as introspection of cube events",
    "start": "193920",
    "end": "201200"
  },
  {
    "text": "port logging spark UI messaging into live notebooks robust connectivity uh",
    "start": "201200",
    "end": "207599"
  },
  {
    "text": "becomes extra important uh to track usage and to uh prevent bottlenecks",
    "start": "207599",
    "end": "213200"
  },
  {
    "text": "between resources and um there's a special there could be special requirement for",
    "start": "213200",
    "end": "219040"
  },
  {
    "text": "non-premptive workloads like you you need to provide a Jupiter environment uh",
    "start": "219040",
    "end": "224400"
  },
  {
    "text": "which you don't want user that to be preemptable uh which is different than the normal batch uh processing system",
    "start": "224400",
    "end": "233760"
  },
  {
    "text": "uh we need fair resource allocation here uh which ensures that uh no single user",
    "start": "233760",
    "end": "239799"
  },
  {
    "text": "monopolizes the whole cluster minimum resource guarantee",
    "start": "239799",
    "end": "244879"
  },
  {
    "text": "ensures that each user is guaranteed to have some resources for starting their interactive spark",
    "start": "244879",
    "end": "252959"
  },
  {
    "text": "experiment so let's let's start with our basic baseline spark on bare metal uh",
    "start": "253000",
    "end": "258560"
  },
  {
    "text": "which is a traditional deployment model right now so when we started as you can",
    "start": "258560",
    "end": "264000"
  },
  {
    "text": "see in this diagram we have a classic setup with a name node a resource manager and worker nodes running on the",
    "start": "264000",
    "end": "270240"
  },
  {
    "text": "physical um physical servers uh in this configuration uh there's no",
    "start": "270240",
    "end": "275440"
  },
  {
    "text": "virtualization layer everything runs on the physical hardware compute resources are preallocated by admins and they have",
    "start": "275440",
    "end": "282560"
  },
  {
    "text": "to manually manage like node provisioning scaling and fall tolerance and uh um it used to run for us on u uh",
    "start": "282560",
    "end": "291759"
  },
  {
    "text": "very popular uh resource manager YAN um which handles jobuling resource",
    "start": "291759",
    "end": "297360"
  },
  {
    "text": "allocation and monitoring uh to distribute spark workloads uh across available worker nodes uh it can",
    "start": "297360",
    "end": "304240"
  },
  {
    "text": "dynamically scale executors based on the workload demands and support shuffle with external shuffle service",
    "start": "304240",
    "end": "312639"
  },
  {
    "text": "yeah uh a bit more about YAN so YAN is a core component of the Hadoop ecosystem",
    "start": "312639",
    "end": "318639"
  },
  {
    "text": "it allows multiple applications like with Spark to share cluster resources",
    "start": "318639",
    "end": "324280"
  },
  {
    "text": "efficiently uh it is important to note that YAN was not designed to orchestrate any containerized application but",
    "start": "324280",
    "end": "331440"
  },
  {
    "text": "specifically designed for orchestrating applications within Hadoop uh it would",
    "start": "331440",
    "end": "336960"
  },
  {
    "text": "provide a fine grain access control uh over CPU memory it manages the cluster",
    "start": "336960",
    "end": "342400"
  },
  {
    "text": "resources using a Q-based architecture to ensure fair and efficient resource allocation admin can set resource limits",
    "start": "342400",
    "end": "349440"
  },
  {
    "text": "and priorities for different cues to balance their workload demands uh there's a great advantage of",
    "start": "349440",
    "end": "355919"
  },
  {
    "text": "uh external shuffle service which YAN brings in um it it helps uh to maintain",
    "start": "355919",
    "end": "361600"
  },
  {
    "text": "efficient data exchange between executors ensuring better fall tolerance and reducing memory overhead it can",
    "start": "361600",
    "end": "368560"
  },
  {
    "text": "handle large scale clusters and can be scaled horizontally to accommodate increasing resource demands among",
    "start": "368560",
    "end": "374240"
  },
  {
    "text": "multiple users and applications let's talk about the",
    "start": "374240",
    "end": "379520"
  },
  {
    "text": "challenges so we face significant challenges with this approach uh while running in spark jobs on bare metal uh",
    "start": "379520",
    "end": "387919"
  },
  {
    "text": "one there was no dynamic scaling of cluster resources admin had to provision",
    "start": "387919",
    "end": "393199"
  },
  {
    "text": "resources for peak demands and storage and computes were tightly coupled",
    "start": "393199",
    "end": "398400"
  },
  {
    "text": "limiting flexibility each cluster had access to the HDFS storage connected to that cluster only and for that reasons",
    "start": "398400",
    "end": "406000"
  },
  {
    "text": "admin had to write distributed coping jobs for moving data from one cluster to another cluster and which needed SR",
    "start": "406000",
    "end": "412759"
  },
  {
    "text": "supports you know to keep the data consistent uh between multiple clusters",
    "start": "412759",
    "end": "418160"
  },
  {
    "text": "that they had to write more and more backfilling jobs there was no data lake concept everyone was just querying the",
    "start": "418160",
    "end": "424160"
  },
  {
    "text": "row data no fault tolerance one job could bring the whole cluster down and",
    "start": "424160",
    "end": "430400"
  },
  {
    "text": "impact all other users there was no built-in orchestration we had to ship entire spark distribution on bare metal",
    "start": "430400",
    "end": "437520"
  },
  {
    "text": "which was errorprone and had connectivity issues um for interactive experiment there was no notebook",
    "start": "437520",
    "end": "443840"
  },
  {
    "text": "interface uh everyone was using a shell interface for running uh interactive",
    "start": "443840",
    "end": "449039"
  },
  {
    "text": "queries which limited our data scientist productivity so our first evolution was",
    "start": "449039",
    "end": "456800"
  },
  {
    "text": "moving to virtualized environment using VMware this introduced virtualization",
    "start": "456800",
    "end": "462319"
  },
  {
    "text": "support giving us more flexibility we gained on demand scalability though not",
    "start": "462319",
    "end": "467840"
  },
  {
    "text": "fully dynamic we moved from yan to moses based uh messos based resource",
    "start": "467840",
    "end": "473680"
  },
  {
    "text": "scheduleuler uh as our deployment model was supporting meos uh at this stage we",
    "start": "473680",
    "end": "479599"
  },
  {
    "text": "had to use static resource allocation because mezos was not supporting dynamic",
    "start": "479599",
    "end": "485479"
  },
  {
    "text": "allocation we introduced Jupiter classic for interactivity which was a significant improvement for our data",
    "start": "485479",
    "end": "491759"
  },
  {
    "text": "scientist we introduced uh at that time uh data lake concepts where HDFS cluster",
    "start": "491759",
    "end": "498479"
  },
  {
    "text": "had network connectivities and same spark job could run access more than one HDFS cluster we cowed out hive meta",
    "start": "498479",
    "end": "505520"
  },
  {
    "text": "store concept we introduced iceberg tables we supported table level",
    "start": "505520",
    "end": "510879"
  },
  {
    "text": "queries u talking about the challenges uh despite this we still face a lot of",
    "start": "511320",
    "end": "517039"
  },
  {
    "text": "challenges uh resource management was still problematic we often saw overallocation and underutilization of",
    "start": "517039",
    "end": "523680"
  },
  {
    "text": "resources so our administrator uh has to track all the usage of all the resources",
    "start": "523680",
    "end": "529279"
  },
  {
    "text": "they to carry like huge spreadsheet um mentioning okay what type of workload is",
    "start": "529279",
    "end": "535360"
  },
  {
    "text": "used where so it was it was pretty heavyweight our t-shirt sizing were not",
    "start": "535360",
    "end": "541120"
  },
  {
    "text": "correct uh for a specific amount of CPU we were allocating large chunk of memory",
    "start": "541120",
    "end": "546560"
  },
  {
    "text": "and people were not uh able to get resources due to low memory presence and most people were not efficient in",
    "start": "546560",
    "end": "552880"
  },
  {
    "text": "writing their spark queries um and and they were using lot of memory heavy data",
    "start": "552880",
    "end": "558240"
  },
  {
    "text": "frame uh we we adjusted the ratio of our t-shirt sizing uh which helped a little",
    "start": "558240",
    "end": "563440"
  },
  {
    "text": "but still an issue due to lack of dynamic allocation which due to which sessions could not shrink",
    "start": "563440",
    "end": "570240"
  },
  {
    "text": "u on the the interactive side uh the Jupyter kernel configuration we were not",
    "start": "570240",
    "end": "575600"
  },
  {
    "text": "persisting and it was not sharable so everyone had to uh write their own set",
    "start": "575600",
    "end": "581680"
  },
  {
    "text": "of spark properties on on their own kernel configuration even though running for the same",
    "start": "581680",
    "end": "588319"
  },
  {
    "text": "experiment moving on the next evolution was moving to Kubernetes so as you can",
    "start": "588680",
    "end": "595519"
  },
  {
    "text": "see in this uh spark cluster is running inside a kubernetes cluster using kubernetes to manage spark resources",
    "start": "595519",
    "end": "601920"
  },
  {
    "text": "instead of yan or mezos uh Spark driver here runs inside a",
    "start": "601920",
    "end": "607519"
  },
  {
    "text": "Kubernetes port and Spark executors runs inside a separate Kubernetes port and",
    "start": "607519",
    "end": "613200"
  },
  {
    "text": "the entire environment is managed by Kubernetes giving us more flexibility and standardization and Kubernetes",
    "start": "613200",
    "end": "619399"
  },
  {
    "text": "dynamically manages these ports scaling resources up or down as needed",
    "start": "619399",
    "end": "626920"
  },
  {
    "text": "so as we know Kubernetes is the standard for deploying containerized applications on cloud and cube scheduleuler is the",
    "start": "628079",
    "end": "634720"
  },
  {
    "text": "default Kubernetes scheduleuler which runs as part of control plane it has really important",
    "start": "634720",
    "end": "640880"
  },
  {
    "text": "characteristics uh and features to understand uh to and to find visible nodes uh for identifying where your",
    "start": "640880",
    "end": "647600"
  },
  {
    "text": "application can run um and it gives pretty good nodes like uh taint",
    "start": "647600",
    "end": "653040"
  },
  {
    "text": "toleration node affinity rules node selectors which you can use to uh to define where application needs to run",
    "start": "653040",
    "end": "660640"
  },
  {
    "text": "it's primary designed for uh micros service type workload a node batch v uh",
    "start": "660640",
    "end": "666560"
  },
  {
    "text": "data processing and it uses a spreadshuling approach to distribute your workloads u uh evenly across",
    "start": "666560",
    "end": "674240"
  },
  {
    "text": "available nodes it has a scaleout architecture with autoscaling capabilities uh it operates",
    "start": "674240",
    "end": "680880"
  },
  {
    "text": "at port level uh and has somewhat limited uh flexibility uh sorry",
    "start": "680880",
    "end": "686959"
  },
  {
    "text": "functionality but it was designed to be replaceable which becomes relevant later",
    "start": "686959",
    "end": "692000"
  },
  {
    "text": "in our journey for interactive spark analysis",
    "start": "692000",
    "end": "698560"
  },
  {
    "text": "uh we introduce uh Jupyter lab which is essentially an in browser ID um to",
    "start": "698560",
    "end": "704480"
  },
  {
    "text": "enable data scientists and analysts to use notebooks allowing them to build narratives uh using both code and text",
    "start": "704480",
    "end": "712480"
  },
  {
    "text": "in the same environment so the screenshot uh is of the Jupyter lab uh environment here uh and then users can",
    "start": "712480",
    "end": "720399"
  },
  {
    "text": "create multiple notebook files uh and then run um spark kernels um either in",
    "start": "720399",
    "end": "726880"
  },
  {
    "text": "spice spark or scala spark or a pure python kernel to power their",
    "start": "726880",
    "end": "732480"
  },
  {
    "text": "notebooks uh we built kernel configuration management system uh for managing the kernel configurations so we",
    "start": "733639",
    "end": "741040"
  },
  {
    "text": "extended Jupyter kernel and supported a spark kernel where all spark properties can be added and it it includes both",
    "start": "741040",
    "end": "748480"
  },
  {
    "text": "shared and default resource configurations security configurations data access",
    "start": "748480",
    "end": "753880"
  },
  {
    "text": "configurations uh as we know spark properties are huge in numbers so uh it",
    "start": "753880",
    "end": "759839"
  },
  {
    "text": "was important for us to not overwhelm a user who really wants to just run some",
    "start": "759839",
    "end": "766480"
  },
  {
    "text": "simple spark experiment as well as allowing admin or advanced users who wants to uh look at the properties and",
    "start": "766480",
    "end": "774560"
  },
  {
    "text": "tweak some of the advanced configurations so we built capabilities to hide those configurations based on",
    "start": "774560",
    "end": "780959"
  },
  {
    "text": "the uh user and then uh as well as allowing them the flexibility to tweak",
    "start": "780959",
    "end": "786079"
  },
  {
    "text": "their configurations uh and our kernel configurations are organized as a hierarchy for easier",
    "start": "786079",
    "end": "792880"
  },
  {
    "text": "management of properties and to support different versions of Spark uh and then",
    "start": "792880",
    "end": "798480"
  },
  {
    "text": "we we flatten that in in our meta store for implementation simplicity and to better use them in the Zuper",
    "start": "798480",
    "end": "807279"
  },
  {
    "text": "environment so now talking about the challenges so with this model uh there",
    "start": "809079",
    "end": "815600"
  },
  {
    "text": "were a few challenges one uh as we as we know the cube scheduleuler was not",
    "start": "815600",
    "end": "821680"
  },
  {
    "text": "designed for batch workload as spark uh as it's not app awareuling and it",
    "start": "821680",
    "end": "827279"
  },
  {
    "text": "doesn't have queuing support so without queuing management and borrowing of resources from each other it was not as",
    "start": "827279",
    "end": "833920"
  },
  {
    "text": "effective for interactive batch use cases there's uh there's no support of",
    "start": "833920",
    "end": "839920"
  },
  {
    "text": "ganguling uh especially like libraries like XD boost where we want all executors to",
    "start": "839920",
    "end": "846959"
  },
  {
    "text": "come up at the same time and you don't want to leave them either is is something which is missing in uh cube",
    "start": "846959",
    "end": "853320"
  },
  {
    "text": "scheduleuler uh and interactive use cases like we needed uh fairness",
    "start": "853320",
    "end": "858480"
  },
  {
    "text": "preemption reservation u resource guarantee and uh resource guarantee for",
    "start": "858480",
    "end": "864800"
  },
  {
    "text": "for um better user experience um and you know bin packing for for",
    "start": "864800",
    "end": "871760"
  },
  {
    "text": "better utilization of cluster resources uh so those were the things which which",
    "start": "871760",
    "end": "876880"
  },
  {
    "text": "were not available in cube scheduleuler uh and then we faced challenges u using",
    "start": "876880",
    "end": "882320"
  },
  {
    "text": "cube scheduleuler for spark interactive workloads and for kernel configuration",
    "start": "882320",
    "end": "888160"
  },
  {
    "text": "management we built the system which is outside of zuper lab so now users have to tweak their configurations in one",
    "start": "888160",
    "end": "895120"
  },
  {
    "text": "system and then they have to use that in Jupiter lab which is which was kind of created a friction for our data",
    "start": "895120",
    "end": "902920"
  },
  {
    "text": "scientists so this brings us to our current architecture spark on kubernetes",
    "start": "902920",
    "end": "908480"
  },
  {
    "text": "with unicorn uh as you can see here unicorn sits as a specialized layer between",
    "start": "908480",
    "end": "914000"
  },
  {
    "text": "kubernetes control plane and our spark applications uh it provides yan",
    "start": "914000",
    "end": "919079"
  },
  {
    "text": "yanlikeuling capabilities uh with Kubernetes giving us best of both worlds",
    "start": "919079",
    "end": "925040"
  },
  {
    "text": "it h it uh provides hierarchal resource management with guaranteed resource quotas per Q this architecture maintains",
    "start": "925040",
    "end": "932880"
  },
  {
    "text": "all the benefits of Kubernetes while addressing the scheduleuling limitations for Spark workloads",
    "start": "932880",
    "end": "940120"
  },
  {
    "text": "u talking about unicorn scheduleuler uh so it it can work with any kind of",
    "start": "942040",
    "end": "948399"
  },
  {
    "text": "workloads through port annotation it provides hierarchal kota management uh",
    "start": "948399",
    "end": "954160"
  },
  {
    "text": "there's no custom resource definition uh is needed u it completely replaces the",
    "start": "954160",
    "end": "960800"
  },
  {
    "text": "cube default scheduleuler handling both and queuing uh both queuing anduling it supports pri priority",
    "start": "960800",
    "end": "967839"
  },
  {
    "text": "preeemption FIFO fair sharing gang scheduleuling uh resource borrowing uh",
    "start": "967839",
    "end": "973279"
  },
  {
    "text": "exactly what what we needed for",
    "start": "973279",
    "end": "977199"
  },
  {
    "text": "Spark so this is our current architecture here um you see in this",
    "start": "978360",
    "end": "983600"
  },
  {
    "text": "model we are leveraging uh node pools and unicorn cues uh we have divided our",
    "start": "983600",
    "end": "990959"
  },
  {
    "text": "workloads into three node pools um the top uh one is node where we want to run",
    "start": "990959",
    "end": "997519"
  },
  {
    "text": "our longunning Jupyter server which is providing an environment for data scientists where in on Jupiter server",
    "start": "997519",
    "end": "1004399"
  },
  {
    "text": "they can launch kernels to run their spark jobs uh since it's a longunning u",
    "start": "1004399",
    "end": "1010800"
  },
  {
    "text": "it it doesn't scale up or down it's it's mainly just one port so it is managed by the cube default scheduleuler and then",
    "start": "1010800",
    "end": "1019199"
  },
  {
    "text": "uh the there the driver the spark driver and is managed is uh part of a different",
    "start": "1019199",
    "end": "1025120"
  },
  {
    "text": "node poolool too and the third node poolool three is for spark executors and",
    "start": "1025120",
    "end": "1030319"
  },
  {
    "text": "which are preemptable and then they can scale independently so with this architecture",
    "start": "1030319",
    "end": "1036480"
  },
  {
    "text": "it provides us the workload separation uh grouping them based on the similar configuration and",
    "start": "1036480",
    "end": "1042038"
  },
  {
    "text": "streamline streamlining life cycle management for each type of workloads uh the other advantage we we",
    "start": "1042039",
    "end": "1049520"
  },
  {
    "text": "see here uh by putting the driver and executors in the same network John uh it",
    "start": "1049520",
    "end": "1055039"
  },
  {
    "text": "helped us providing better u shuffle better shuffling and and the reduced uh",
    "start": "1055039",
    "end": "1060880"
  },
  {
    "text": "in shuffle latency",
    "start": "1060880",
    "end": "1065480"
  },
  {
    "text": "uh so we improved our interactive spark experience here uh with further improvement with Zuptor lab so we built",
    "start": "1067200",
    "end": "1073360"
  },
  {
    "text": "in uh a Zuper lab plug-in uh for kernel configuration management to reduce the",
    "start": "1073360",
    "end": "1079440"
  },
  {
    "text": "friction u and to allow users to do everything in the same environment uh we made the uh",
    "start": "1079440",
    "end": "1086799"
  },
  {
    "text": "improvement in zuper kernel connectivity um so it used to happen that uh user has",
    "start": "1086799",
    "end": "1092480"
  },
  {
    "text": "launched a spark kernel from Zuptor the spark kernel is running and the code is up and running behind the scene but uh",
    "start": "1092480",
    "end": "1099360"
  },
  {
    "text": "Zuper lab doesn't have connectivity to connect to that kernel so we we had we",
    "start": "1099360",
    "end": "1105200"
  },
  {
    "text": "improved uh the kernel connectivity layer to try to uh show the the the",
    "start": "1105200",
    "end": "1110240"
  },
  {
    "text": "exact status of the drive of the spark driver kernel which is uh which is",
    "start": "1110240",
    "end": "1116000"
  },
  {
    "text": "running on kubernetes and uh we gained utilization transparency giving users better",
    "start": "1116000",
    "end": "1122679"
  },
  {
    "text": "visibility so overall this created a much more seamless experience for our data scientists",
    "start": "1122679",
    "end": "1130679"
  },
  {
    "text": "uh this slide shows our Jupyter lab kernel configuration interface uh which provides a more integrated experience",
    "start": "1130960",
    "end": "1137520"
  },
  {
    "text": "compared to our previous setup so you can see here um so this is a Jupiter lab",
    "start": "1137520",
    "end": "1143760"
  },
  {
    "text": "uh spark kernel where all the spark properties are configured and the same can be used to launch the kernel by",
    "start": "1143760",
    "end": "1150640"
  },
  {
    "text": "staying on the zuper lab environment so we have we have different presets",
    "start": "1150640",
    "end": "1155760"
  },
  {
    "text": "there's a minimal preset this is only for just a smaller set of properties and if the users goes to the the uh the full",
    "start": "1155760",
    "end": "1164160"
  },
  {
    "text": "preset they'll see advanced set of configurations only only if they want to see it and tweak it we are planning to",
    "start": "1164160",
    "end": "1171280"
  },
  {
    "text": "open source uh this sometime in future with uh",
    "start": "1171280",
    "end": "1176400"
  },
  {
    "text": "Zuptor so um with with this approach u we we gained many benefits uh one is uh",
    "start": "1177160",
    "end": "1185919"
  },
  {
    "text": "with unicorn we now have application level uh wearululing we have achieved significant costsaving throughout our uh",
    "start": "1185919",
    "end": "1193600"
  },
  {
    "text": "through our better resource utilization we have a comprehensive monitoring capabilities and uh now this",
    "start": "1193600",
    "end": "1200799"
  },
  {
    "text": "solution works both on onrem and uh a third party cloud giving us more flexibility",
    "start": "1200799",
    "end": "1208679"
  },
  {
    "text": "so looking at specific features across our journey uh so we see that we we didn't have uh",
    "start": "1209520",
    "end": "1217120"
  },
  {
    "text": "the cluster autoscaling dependency management some of the the all the data lake concepts u when we started with",
    "start": "1217120",
    "end": "1224559"
  },
  {
    "text": "bare metal uh we improved on that when we moved to VMware um solution but at",
    "start": "1224559",
    "end": "1231600"
  },
  {
    "text": "that time we lost the dynamic allocation so which which uh which helped which",
    "start": "1231600",
    "end": "1236880"
  },
  {
    "text": "didn't help in improving our resource efficiency NC uh with with Kubernetes",
    "start": "1236880",
    "end": "1242720"
  },
  {
    "text": "our resource uh utilization uh improved uh and then but it was still it was",
    "start": "1242720",
    "end": "1250159"
  },
  {
    "text": "still not up to the mark where we wanted and we we saw the advanced improvement with Kubernetes on Unicorn uh we with",
    "start": "1250159",
    "end": "1257919"
  },
  {
    "text": "queuing support we we had it available on bare metal but then we didn't have it",
    "start": "1257919",
    "end": "1262960"
  },
  {
    "text": "with our VMware and Kubernetes and now we still have it back with Unicorn uh our cost efficiency has moved from high",
    "start": "1262960",
    "end": "1270240"
  },
  {
    "text": "investment U utilization to best resource sharing with lowest cost gang",
    "start": "1270240",
    "end": "1275280"
  },
  {
    "text": "shoulduling a critical feature for uh Spark is now available with uh Unicorn",
    "start": "1275280",
    "end": "1280320"
  },
  {
    "text": "uh resourceful uh fairness and job prioritization have evolved from basic to advanced with our current",
    "start": "1280320",
    "end": "1286000"
  },
  {
    "text": "architecture now key",
    "start": "1286000",
    "end": "1292919"
  },
  {
    "text": "takeaways I think to start with if you are in your journey of moving from bare",
    "start": "1292919",
    "end": "1298720"
  },
  {
    "text": "metal to kubernetes or you're already in that in that process uh the first is you",
    "start": "1298720",
    "end": "1304320"
  },
  {
    "text": "need to assess your current u um workload pattern whether it's a uh batch",
    "start": "1304320",
    "end": "1309919"
  },
  {
    "text": "uh processing or you need interactive or AI capabilities uh uh this is really",
    "start": "1309919",
    "end": "1316000"
  },
  {
    "text": "important to decide uh what kind ofuling capabilities you will be needing and then you can optimize on",
    "start": "1316000",
    "end": "1323320"
  },
  {
    "text": "that uh design a phase strategy uh start with the hybrid approach uh in migrate",
    "start": "1323320",
    "end": "1329120"
  },
  {
    "text": "incrementally ensure you have uh proper test plans before migrating users and which is running on your both uh",
    "start": "1329120",
    "end": "1336240"
  },
  {
    "text": "previous infrastructure and the new infrastructure",
    "start": "1336240",
    "end": "1340760"
  },
  {
    "text": "optimize uh resource uh util uh um allocation u so as you know the cube",
    "start": "1342080",
    "end": "1348799"
  },
  {
    "text": "scheduleuler is not meant for uh batchuling uh jobs so this is one thing",
    "start": "1348799",
    "end": "1354720"
  },
  {
    "text": "which you would want to consider from day one like having a batch scheduleuler on top of uh the the the currentuling",
    "start": "1354720",
    "end": "1362799"
  },
  {
    "text": "capabilities which Kubernetes has you would like to enable preeemption policies to ensure uh there are prior if",
    "start": "1362799",
    "end": "1370720"
  },
  {
    "text": "there are priority jobs has to be scheduled um enable dynamic allocation uh which we",
    "start": "1370720",
    "end": "1377360"
  },
  {
    "text": "didn't have when we uh moved to VMware and then we uh we were bitten by that",
    "start": "1377360",
    "end": "1383039"
  },
  {
    "text": "because dynamic allocation ensures a more cost um resource",
    "start": "1383039",
    "end": "1388760"
  },
  {
    "text": "utilization I mean of course there are cases like XD boost where you don't want u uh to enable it and you want all the",
    "start": "1388760",
    "end": "1395679"
  },
  {
    "text": "executors to come up at the same time but most of the cases you would like to enable dynamic allocation for better",
    "start": "1395679",
    "end": "1401360"
  },
  {
    "text": "resource efficiency uh implement proper portuling",
    "start": "1401360",
    "end": "1407760"
  },
  {
    "text": "uh you use uh Kubernetes provide uh tons of configurations to ensure that your",
    "start": "1407760",
    "end": "1413520"
  },
  {
    "text": "ports are placed at the at the right node you can use node affinity rules node selectors um taint tolerations to",
    "start": "1413520",
    "end": "1421280"
  },
  {
    "text": "to customize your kubernetes specific configurations to optimize uncheduling",
    "start": "1421280",
    "end": "1428440"
  },
  {
    "text": "uh handle your storage and networking challenges if you have data sitting on",
    "start": "1430000",
    "end": "1435120"
  },
  {
    "text": "local disc you won't like to move to the distributed storage uh uh and networking could be different",
    "start": "1435120",
    "end": "1443200"
  },
  {
    "text": "like when we started we had the bare metal only u the cluster was having access to only the HDFS which it was",
    "start": "1443200",
    "end": "1449760"
  },
  {
    "text": "connected to so you would like to consider those networking um challenges when when you move uh from biometal to",
    "start": "1449760",
    "end": "1457799"
  },
  {
    "text": "Kubernetes uh configure proper proper uh configuration uh proper shuffle service",
    "start": "1457799",
    "end": "1465440"
  },
  {
    "text": "whether you want to use external shuffle service or shuffle tracking u some of the configuration might not be the same",
    "start": "1465440",
    "end": "1472000"
  },
  {
    "text": "as you are uh as biometal to kubernetes like CPU requested limits when we",
    "start": "1472000",
    "end": "1478480"
  },
  {
    "text": "started we used to have a default configuration with putting limits as way higher than request um which which by",
    "start": "1478480",
    "end": "1486720"
  },
  {
    "text": "which we got throttled because uh when when you when you set it that way only",
    "start": "1486720",
    "end": "1492799"
  },
  {
    "text": "the one which has the highest limits take over and you become the bad no neighbor so uh and we ended up setting",
    "start": "1492799",
    "end": "1500400"
  },
  {
    "text": "both requests equals to limit um to to uh make it fair uh for better",
    "start": "1500400",
    "end": "1506559"
  },
  {
    "text": "utilization and uh uh yeah this is the thing which",
    "start": "1506559",
    "end": "1512400"
  },
  {
    "text": "you don't want to miss set up comprehensive monitoring uh you if you're using Unicorn enable Unicorn's UI",
    "start": "1512400",
    "end": "1519600"
  },
  {
    "text": "metrics to trackuling efficiency u there are",
    "start": "1519600",
    "end": "1526679"
  },
  {
    "text": "uh in in terms of pitfalls and antiatterns yeah I mean you as I said",
    "start": "1526679",
    "end": "1532880"
  },
  {
    "text": "you you would like to enable uh dynamic scaling um uh there are cases where you",
    "start": "1532880",
    "end": "1538000"
  },
  {
    "text": "would not like to do that but but yeah don't don't uh forget to use the dynamic",
    "start": "1538000",
    "end": "1543600"
  },
  {
    "text": "scaling which is really efficient uh storage and networking changes might might be tricky sometimes",
    "start": "1543600",
    "end": "1550880"
  },
  {
    "text": "so don't underestimate your storage and networking changes and um uni if you're",
    "start": "1550880",
    "end": "1556960"
  },
  {
    "text": "using unicorn uh you might want to uh make sure that youruling policies are",
    "start": "1556960",
    "end": "1562960"
  },
  {
    "text": "configured correctly as uh it'll be tricky to debug once uh if you were you",
    "start": "1562960",
    "end": "1569440"
  },
  {
    "text": "if you want your port to run at specific queue or a specific location it's not it's uh landing in other other place uh",
    "start": "1569440",
    "end": "1577039"
  },
  {
    "text": "it'll be tricky to find it out so you make sure that you are configuring youruling policies correctly",
    "start": "1577039",
    "end": "1584360"
  },
  {
    "text": "yeah that's all uh thanks thank you all [Music]",
    "start": "1584400",
    "end": "1591619"
  },
  {
    "text": "questions yeah do you hear me yep yeah uh I would like",
    "start": "1594720",
    "end": "1601279"
  },
  {
    "text": "to have a two questions first one is about data so where do data lives in the",
    "start": "1601279",
    "end": "1607760"
  },
  {
    "text": "Kubernetes environment and the second one is there is no external shuffling",
    "start": "1607760",
    "end": "1613120"
  },
  {
    "text": "service in the Kubernetes environment i can yeah take the first one yeah the data lives in cloud storage so we just",
    "start": "1613120",
    "end": "1621279"
  },
  {
    "text": "in cloud storage so it can be like local cloud storage cloud cloud storage you",
    "start": "1621279",
    "end": "1627440"
  },
  {
    "text": "think about like S3 or SE for these other technologies okay so it's basically like getting things off of the",
    "start": "1627440",
    "end": "1633440"
  },
  {
    "text": "HDFS clusters into a place where you can get good access to the data okay it's",
    "start": "1633440",
    "end": "1638720"
  },
  {
    "text": "still local but then you can in if you're in different regions and things you still have access to the data thank you the second one is no external",
    "start": "1638720",
    "end": "1646240"
  },
  {
    "text": "shuffling service anymore uh yeah I think so there's been some efforts to try to build that but right now we're",
    "start": "1646240",
    "end": "1652000"
  },
  {
    "text": "using mostly shuffle tracking and we're really interested in like figuring out if there's going to be a great long-term",
    "start": "1652000",
    "end": "1657440"
  },
  {
    "text": "solution for external shuffle tracking right right now we're not using anything specific yeah okay thank you",
    "start": "1657440",
    "end": "1664960"
  },
  {
    "text": "uh hi quick couple questions so uh at the last you mentioned don't underestimate the storage requirements",
    "start": "1664960",
    "end": "1670720"
  },
  {
    "text": "so we are also in the process of migrating spark jobs from yarn to kubernetes um one of the challenges we",
    "start": "1670720",
    "end": "1676880"
  },
  {
    "text": "are trying to solve right now is the ephemeral storage requirements that these jobs have so we are looking at uh",
    "start": "1676880",
    "end": "1682240"
  },
  {
    "text": "something like ephemeral volumes for instance but curious how you went about it and if that's something that's a problem that you also faced yeah I think",
    "start": "1682240",
    "end": "1689440"
  },
  {
    "text": "we did um do you do you know more go for it if you No I mean so we have uh",
    "start": "1689440",
    "end": "1694640"
  },
  {
    "text": "different so one is the uh so one is a Jupyter interface which we're providing for Spark and where the Jupyter files",
    "start": "1694640",
    "end": "1701520"
  },
  {
    "text": "lives which is not typically the data but the the compute code which users want to use it so we are u so we are",
    "start": "1701520",
    "end": "1708799"
  },
  {
    "text": "using like uh the kubernetes persistent volumes for that one but the actual data as uh is is the one which is residing on",
    "start": "1708799",
    "end": "1716240"
  },
  {
    "text": "the cloud like which is separate from the the kubernetes infrastructure that part makes sense uh curious so one of",
    "start": "1716240",
    "end": "1722480"
  },
  {
    "text": "the thing one of the requirements we have is really high throughput for parture on spark jobs because several",
    "start": "1722480",
    "end": "1727760"
  },
  {
    "text": "jobs wanting to run we want to keep utilization high very similar system where we have a queue mechanism where",
    "start": "1727760",
    "end": "1732960"
  },
  {
    "text": "jobs are queued all the time when we looked into using persistent volumes for example the throughput slowed down",
    "start": "1732960",
    "end": "1739320"
  },
  {
    "text": "significantly like we could maintain a throughput of about 200 300 parts per second without PVs but as soon as we put",
    "start": "1739320",
    "end": "1746000"
  },
  {
    "text": "PVs in the mix performance drops by like from 200 to 50 parts per second max we",
    "start": "1746000",
    "end": "1751520"
  },
  {
    "text": "have the same problem right how do you solve it yeah we you use local disk right use local local disk as much as",
    "start": "1751520",
    "end": "1757600"
  },
  {
    "text": "you can and like configuring that disc correctly and using the right types of disc right is probably the best you're",
    "start": "1757600",
    "end": "1764399"
  },
  {
    "text": "gonna do right i see yeah like trying to get you can also end up saturating the",
    "start": "1764399",
    "end": "1771039"
  },
  {
    "text": "ability to actually mount the PVCs so like I would totally avoid that if possible right i see in that case you're",
    "start": "1771039",
    "end": "1777600"
  },
  {
    "text": "using just empty D with disk in this case uh empty dur with disk or you can configure more specific volumes like in",
    "start": "1777600",
    "end": "1783919"
  },
  {
    "text": "the size and shape that you need them in our kernel configuration that Niha showed you can specify the disk",
    "start": "1783919",
    "end": "1789200"
  },
  {
    "text": "properties you want see and at scheduling time you can we can get the right uh workloads to the right pods or",
    "start": "1789200",
    "end": "1795200"
  },
  {
    "text": "to the right nodes with the right pod sizes right I see but in that case do you do use host paths as",
    "start": "1795200",
    "end": "1800559"
  },
  {
    "text": "volumes or something else um I can't remember what that is now but let's connect after and I can get an answer",
    "start": "1800559",
    "end": "1805840"
  },
  {
    "text": "for you sounds good thanks How has been your experience with",
    "start": "1805840",
    "end": "1811760"
  },
  {
    "text": "shuffle tracking uh because we kind of ran into a few problems so I'm trying to",
    "start": "1811760",
    "end": "1817120"
  },
  {
    "text": "understand if we are the special case or it's kind of uh what type of problems did you run into um fetch uh failing at",
    "start": "1817120",
    "end": "1826159"
  },
  {
    "text": "times or um executors staying around for longer than required yeah yeah so we ran",
    "start": "1826159",
    "end": "1832960"
  },
  {
    "text": "into some of those as well um everyone's feeling the same pain do you also use decommissioning um I don't I don't think",
    "start": "1832960",
    "end": "1840880"
  },
  {
    "text": "we're using decommissioning oh you Oh yeah no we do we actually So um if I'm if I'm understanding decommissioning",
    "start": "1840880",
    "end": "1846640"
  },
  {
    "text": "correctly the way you're asking it so let me just kind of explain uh as we're using shuffle tracking shuffle tracking",
    "start": "1846640",
    "end": "1851840"
  },
  {
    "text": "has to be configured well right and there's a lot of knobs that you have to like keep data cached in the executives",
    "start": "1851840",
    "end": "1859039"
  },
  {
    "text": "right and then you have timeouts as well so you can time out those things and depending upon the type of interactive",
    "start": "1859039",
    "end": "1865520"
  },
  {
    "text": "workload and the nature someone may want to cache data if they're just caching data there there's a cache timeout and",
    "start": "1865520",
    "end": "1871520"
  },
  {
    "text": "then on shuffle tracking there's very similar concepts right so um you may want to keep those nodes uh those pods",
    "start": "1871520",
    "end": "1877919"
  },
  {
    "text": "up for a certain amount of time and then as uh as you use dynamic allocation the",
    "start": "1877919",
    "end": "1883919"
  },
  {
    "text": "shuffle tracking will basically uh wayne off because you're going to collapse the cluster right you're going to scale it",
    "start": "1883919",
    "end": "1889520"
  },
  {
    "text": "down so an example is like I may start a cluster that has two pods right as executives and one driver and it may end",
    "start": "1889520",
    "end": "1896720"
  },
  {
    "text": "up scaling up to 100 pods right but then when I stop using it I've stopped writing queries or doing data work it",
    "start": "1896720",
    "end": "1902159"
  },
  {
    "text": "can scale in and then the shuffle uh blocks will end up uh being garbage collected right so when you're doing",
    "start": "1902159",
    "end": "1909120"
  },
  {
    "text": "that basically if you're doing something really intense you need to configure your shuffle properties shuffle tracking",
    "start": "1909120",
    "end": "1914559"
  },
  {
    "text": "properties such that for the nature of your workloads your ad hoc workloads that those shuffle tracking blocks stay",
    "start": "1914559",
    "end": "1919919"
  },
  {
    "text": "around Right does that make sense yeah it it it does um and I think the",
    "start": "1919919",
    "end": "1926720"
  },
  {
    "text": "dynamic allocation as you said is influenced by multiple factors not just shuffle tracking uh blocks yeah for sure",
    "start": "1926720",
    "end": "1933360"
  },
  {
    "text": "for sure yeah I think there there are two sets of things I want to talk about right so the shuffle tracking is one aspect of it but then there's a set of",
    "start": "1933360",
    "end": "1939519"
  },
  {
    "text": "uh properties to help you keep your executives around as well right like when you go and you cache data you don't",
    "start": "1939519",
    "end": "1945200"
  },
  {
    "text": "want those uh executives to shut down because you've cached the data you're going to reuse it you might write one query and you want that data cached and",
    "start": "1945200",
    "end": "1951279"
  },
  {
    "text": "then you're going to derive another query off of that same data right so you're keeping the executives around",
    "start": "1951279",
    "end": "1957519"
  },
  {
    "text": "right for a certain amount of time for your work and then once you stop then you have shuffle tracking that's going to start cleaning things up right i",
    "start": "1957519",
    "end": "1964399"
  },
  {
    "text": "think it's also quite relevant in terms of if you have complex spark jobs exactly say if there are uh six stages",
    "start": "1964399",
    "end": "1970799"
  },
  {
    "text": "that takes 5 hours um then in that case that's where the shuffle tracking has",
    "start": "1970799",
    "end": "1976320"
  },
  {
    "text": "basically given us the most pain exactly um I think the interactive workloads",
    "start": "1976320",
    "end": "1981360"
  },
  {
    "text": "where data scientists let's say stop working in our uh setup uh the idle",
    "start": "1981360",
    "end": "1987360"
  },
  {
    "text": "timeout for Jupyter notebooks basically kills the notebooks before shuffle tracking can even you know downscale so",
    "start": "1987360",
    "end": "1994279"
  },
  {
    "text": "yeah I'm going you finish and then I can jump in i think I have a I want to add a point did you want to finish that or uh",
    "start": "1994279",
    "end": "2000320"
  },
  {
    "text": "I think yeah I think uh I will just say that um for larger interactive workloads",
    "start": "2000320",
    "end": "2005360"
  },
  {
    "text": "I highly discourage using Jupyter notebooks right anything that's going to run past 30 minutes and 30 minutes is",
    "start": "2005360",
    "end": "2012399"
  },
  {
    "text": "you mean a lot right anything that's going to run past 30 minutes put it in the background so we have the ability to",
    "start": "2012399",
    "end": "2017840"
  },
  {
    "text": "basically build a DAG and you you all gave a presentation at Airflow Summit on the same topic but we can basically",
    "start": "2017840",
    "end": "2024320"
  },
  {
    "text": "schedule those notebooks to go in the background so the users get the convenience of typing the code in the notebook and then they can hit a few",
    "start": "2024320",
    "end": "2029760"
  },
  {
    "text": "buttons and drop it in the background and run it on a schedule or just stuff it in the background and then the results will get like generated and um",
    "start": "2029760",
    "end": "2036720"
  },
  {
    "text": "the user will be notified that the results are there right and when you run those types of jobs where you need to",
    "start": "2036720",
    "end": "2042159"
  },
  {
    "text": "like maintain the shuffle data for a longer period of time across multiple stages you can tune the properties as",
    "start": "2042159",
    "end": "2047679"
  },
  {
    "text": "you need to make that job efficient and things to not get collapsed in but remember as she showed on that last",
    "start": "2047679",
    "end": "2054398"
  },
  {
    "text": "slide let me bring the slide back i think this one you have on the I think the right",
    "start": "2054399",
    "end": "2061118"
  },
  {
    "text": "hand side for you here there is in the Q3 there's a batch workload once you put",
    "start": "2061119",
    "end": "2066560"
  },
  {
    "text": "the notebook in the background it becomes a batch workload and your batch configuration with unicorn needs to be",
    "start": "2066560",
    "end": "2071919"
  },
  {
    "text": "tuned for those needs right okay so we have like uh data scientists that will go do work and schedule things nightly",
    "start": "2071919",
    "end": "2078320"
  },
  {
    "text": "or weekly and we make sure that they put those things in a batch uh queue so that",
    "start": "2078320",
    "end": "2083358"
  },
  {
    "text": "things get scheduled appropriately and they get the type of behavior they need in terms of long running work right does",
    "start": "2083359",
    "end": "2089679"
  },
  {
    "text": "that make sense that makes sense yeah uh now you reminded me of another question that No no no it's it's 6:05 so I want",
    "start": "2089679",
    "end": "2095919"
  },
  {
    "text": "to be respectful to other people's time like people don't need to stay but I'd love to have you if you want to want to stay go for it",
    "start": "2095919",
    "end": "2102200"
  },
  {
    "text": "um why did you separate out node pools for Jupyter Ps drivers and executors",
    "start": "2102200",
    "end": "2108480"
  },
  {
    "text": "yeah I think like NA mentioned earlier in the talk you want to take it you take it no no you no you take it sorry I'm talking too much i think it was it was",
    "start": "2108480",
    "end": "2114079"
  },
  {
    "text": "kind of mentioned the networking part between driver and exeutor i'm kind of wondering there are three layers in",
    "start": "2114079",
    "end": "2119200"
  },
  {
    "text": "total uh yeah sure so the Jupiter port is kind of a longunning port right so it",
    "start": "2119200",
    "end": "2126000"
  },
  {
    "text": "doesn't uh it doesn't require any special treatment uh in terms of scaling up or down so we we don't want to",
    "start": "2126000",
    "end": "2131839"
  },
  {
    "text": "preempt these Jupiter ports if any high priority job comes in so they want to",
    "start": "2131839",
    "end": "2137040"
  },
  {
    "text": "treat this differently than the the rest of the the Spark uh jobs which has a priority setup so these are so that's",
    "start": "2137040",
    "end": "2144160"
  },
  {
    "text": "why like we want this to scale up differently and don't want to mingle them with the rest of the Spark jobs if",
    "start": "2144160",
    "end": "2150480"
  },
  {
    "text": "that makes sense okay yeah i Yeah okay it makes sense thanks i can also add a little more to that like when you have",
    "start": "2150480",
    "end": "2157440"
  },
  {
    "text": "drivers right when we uh connect to those uh Jupyter kernels there the",
    "start": "2157440",
    "end": "2162720"
  },
  {
    "text": "driver is the kernel right and the driver because that's the interactive component we're connecting to to send",
    "start": "2162720",
    "end": "2168000"
  },
  {
    "text": "the code to get the get the result that that pod we don't want to go down at all right so it needs to be in its own node",
    "start": "2168000",
    "end": "2174240"
  },
  {
    "text": "pool and the life cycle of those pods is different the life cycle of the driver the driver is there longer the",
    "start": "2174240",
    "end": "2179680"
  },
  {
    "text": "executives can come and go between multiple cells in the notebook you're running different queries right so you may go from two two executives to 100",
    "start": "2179680",
    "end": "2186960"
  },
  {
    "text": "and then back down to 50 and then back up to 100 and then back down to 75 so",
    "start": "2186960",
    "end": "2192240"
  },
  {
    "text": "the churn rate in the node pool 3 there for executives is different and your autoscaling configuration can be",
    "start": "2192240",
    "end": "2198960"
  },
  {
    "text": "tailored to the nature of those workloads does that make sense can you give me specific examples of",
    "start": "2198960",
    "end": "2205760"
  },
  {
    "text": "what those configurations are because I think that is the gist of the answer uh let me think of specific configurations",
    "start": "2205760",
    "end": "2213240"
  },
  {
    "text": "um example would be I'm trying to think on the executive one is always the easy one right which",
    "start": "2213240",
    "end": "2219440"
  },
  {
    "text": "is like I may need to and I have people here waiting to maybe throw me out sorry we're running out of time okay we're running out of time so last last answer",
    "start": "2219440",
    "end": "2226079"
  },
  {
    "text": "and we can just sync up offline too um ultimately um if you want to have let's",
    "start": "2226079",
    "end": "2232240"
  },
  {
    "text": "say a 100 nodes and you want the equivalent number of pods on those 100 nodes right and at peak time right when",
    "start": "2232240",
    "end": "2239760"
  },
  {
    "text": "people are running heavy queries let's say they're coming in the office they start running those queries right and then with dynamic allocation that that",
    "start": "2239760",
    "end": "2247280"
  },
  {
    "text": "can scale in it can scale the nodes you can go down to 10 nodes if everyone goes to lunch right and then it can scale",
    "start": "2247280",
    "end": "2253280"
  },
  {
    "text": "back out when they come back and they start writing more queries but let me answer the rest of that over here on the side of the stage thank you very much",
    "start": "2253280",
    "end": "2259440"
  },
  {
    "text": "thank you",
    "start": "2259440",
    "end": "2262599"
  }
]