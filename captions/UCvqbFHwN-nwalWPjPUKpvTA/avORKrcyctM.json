[
  {
    "text": "today's hug is about the running multi schedules in kubernetes we will talk about how multi schedulers",
    "start": "60",
    "end": "7919"
  },
  {
    "text": "are supported in kubernetes and also some of my teamwork is this area to",
    "start": "7919",
    "end": "13650"
  },
  {
    "text": "provide a better spot for multi schedulers before it started a little bit",
    "start": "13650",
    "end": "19410"
  },
  {
    "text": "information about myself my name is the shining King I'm from Malawi cloud",
    "start": "19410",
    "end": "24660"
  },
  {
    "text": "computing department my team is the testing Seattle area is in paratime",
    "start": "24660",
    "end": "31470"
  },
  {
    "text": "tongue which is a critical to downtown about the like I think about the 15",
    "start": "31470",
    "end": "37590"
  },
  {
    "text": "miles away from this hotel and the the primary focus of my team is the core",
    "start": "37590",
    "end": "43950"
  },
  {
    "text": "instruction of cloud computing like scheduling and the resource management in large-scale clusters",
    "start": "43950",
    "end": "51590"
  },
  {
    "text": "where user cuneta intensively for research and also for production purpose",
    "start": "51590",
    "end": "56910"
  },
  {
    "text": "and also our humans they are actively hiring so if you are interested in this",
    "start": "56910",
    "end": "62730"
  },
  {
    "text": "payout and you are willing to move to Seattle I would be very glad to provide more information about it after this",
    "start": "62730",
    "end": "69240"
  },
  {
    "text": "session so this is a treasure gender first we",
    "start": "69240",
    "end": "77490"
  },
  {
    "text": "are give a quick recap on how they differ the scheduling works in communities in the default case there is",
    "start": "77490",
    "end": "85049"
  },
  {
    "text": "only one scheduler process ready for the schedule because we are going to talk about the",
    "start": "85049",
    "end": "91350"
  },
  {
    "text": "multiple schedulers so it's important to first understand how a single scheduler",
    "start": "91350",
    "end": "96960"
  },
  {
    "text": "works in the default case then we will talk about why we would",
    "start": "96960",
    "end": "102060"
  },
  {
    "text": "want from motive schedulers what are the scenarios and the motivations and also",
    "start": "102060",
    "end": "107670"
  },
  {
    "text": "we were - how kubernetes 11 us to run multi schedulers",
    "start": "107670",
    "end": "114049"
  },
  {
    "text": "after that I went to induce the worker waited to provide a better spot for",
    "start": "114049",
    "end": "119790"
  },
  {
    "text": "multi schedules all this work is a part of our internal",
    "start": "119790",
    "end": "126119"
  },
  {
    "text": "project named horse power expression and the scheduling system",
    "start": "126119",
    "end": "132200"
  },
  {
    "text": "during the project we also evaluated how multi scheduled are supported in other",
    "start": "132250",
    "end": "138130"
  },
  {
    "text": "systems many about the mesas it's pretty interesting to see how multi schedules",
    "start": "138130",
    "end": "144670"
  },
  {
    "text": "are supported in a totally different language so I've also spending several minutes to share I our understanding on",
    "start": "144670",
    "end": "152800"
  },
  {
    "text": "that and then give a quick comparison and the summary about the different architectures and the its support for",
    "start": "152800",
    "end": "160540"
  },
  {
    "text": "multi schedulers and after all this I will try to leave some time for Q&A and discussion",
    "start": "160540",
    "end": "167280"
  },
  {
    "text": "so let's start with the default scheduling",
    "start": "170130",
    "end": "175200"
  },
  {
    "text": "as I just mentioned in the default case there is only one scheduler process the",
    "start": "175200",
    "end": "181870"
  },
  {
    "text": "default scheduler are the Native communities schedule it firstly it's important to understand",
    "start": "181870",
    "end": "189400"
  },
  {
    "text": "that in kubernetes parts are these atomic",
    "start": "189400",
    "end": "194410"
  },
  {
    "text": "units for scheduling you probably were how several user",
    "start": "194410",
    "end": "199959"
  },
  {
    "text": "containers in one single part but all these containers will be scheduled",
    "start": "199959",
    "end": "206440"
  },
  {
    "text": "around to a same node so post rather than containers are these scheduling",
    "start": "206440",
    "end": "212019"
  },
  {
    "text": "units also in the ephoto scheduler implementation parts are scheduled a one",
    "start": "212019",
    "end": "219070"
  },
  {
    "text": "by one this diagram shows a typical process of",
    "start": "219070",
    "end": "225120"
  },
  {
    "text": "progeny first parts are submitted to a PS server",
    "start": "225120",
    "end": "230910"
  },
  {
    "text": "they can be created by any user directly are created by some kubernetes",
    "start": "230910",
    "end": "236440"
  },
  {
    "text": "controllers like the most other controllers are some other RC controller scenes",
    "start": "236440",
    "end": "243900"
  },
  {
    "text": "the in they are facilitated to a CCD and in the service table this unscheduled",
    "start": "243900",
    "end": "250720"
  },
  {
    "text": "applause will be picked up by the scheduler process we know there are many parts in the CD",
    "start": "250720",
    "end": "258000"
  },
  {
    "text": "scheduler our own schedule the reason that a scheduler process can find all",
    "start": "258000",
    "end": "264669"
  },
  {
    "text": "the on schedule the hard part is that there is a fear named no-name in the",
    "start": "264669",
    "end": "272590"
  },
  {
    "text": "pods back this fear indicates which node has been",
    "start": "272590",
    "end": "279469"
  },
  {
    "text": "assignable to the children despot by default it's empty which means this",
    "start": "279469",
    "end": "286460"
  },
  {
    "text": "part has not been scheduled or assigned to any node so what a scheduler process studies it",
    "start": "286460",
    "end": "294740"
  },
  {
    "text": "Eclipse watching all the parts with a filtering condition that no name equals",
    "start": "294740",
    "end": "300020"
  },
  {
    "text": "the empty going to find such a unscheduled apart",
    "start": "300020",
    "end": "305650"
  },
  {
    "text": "either way our evaluate all the available nodes in the",
    "start": "305650",
    "end": "311479"
  },
  {
    "text": "cluster trying to find the best one for this part in order to do this it away",
    "start": "311479",
    "end": "316669"
  },
  {
    "text": "our evaluate a lot of rules which were talked about in next slide",
    "start": "316669",
    "end": "322419"
  },
  {
    "text": "here we only to need to know that at the end of scheduling the scheduler process",
    "start": "322419",
    "end": "328099"
  },
  {
    "text": "we are identifying one of the knows as the best one for this part",
    "start": "328099",
    "end": "333879"
  },
  {
    "text": "then what it does is it was writer the name of this node into the known name",
    "start": "333879",
    "end": "339979"
  },
  {
    "text": "field and then the last in the last step step",
    "start": "339979",
    "end": "345500"
  },
  {
    "text": "for all the cube net agents are also watching all the paths but as they are",
    "start": "345500",
    "end": "351770"
  },
  {
    "text": "watching with a different filtering condition their condition is a node name equal to the name how its own node so",
    "start": "351770",
    "end": "359990"
  },
  {
    "text": "when the node name further is updated in step 3 this part will be picked up",
    "start": "359990",
    "end": "366500"
  },
  {
    "text": "automatically by Z cubed an agent in step 4 and then the accumulated agent we",
    "start": "366500",
    "end": "371839"
  },
  {
    "text": "are pour the container images and the excrement to launch it apart",
    "start": "371839",
    "end": "377379"
  },
  {
    "text": "this is a and trying to process our path scheduling in some special",
    "start": "377379",
    "end": "384199"
  },
  {
    "text": "situations when parts are submitted the no-name theater already has a value",
    "start": "384199",
    "end": "390339"
  },
  {
    "text": "it means this part helping predefined to run on a particular node this can happen",
    "start": "390339",
    "end": "396740"
  },
  {
    "text": "when the parts are created by some control like demon SATA controllers in",
    "start": "396740",
    "end": "402420"
  },
  {
    "text": "this case the pots were go from step 2 to step forth directly it was not be",
    "start": "402420",
    "end": "409260"
  },
  {
    "text": "scheduled because the scheduler is not even aware of this part because the time amid the filtering conditioning step 3",
    "start": "409260",
    "end": "418850"
  },
  {
    "text": "we just mentioned that in order to find the best node for a part the scheduler",
    "start": "422810",
    "end": "429120"
  },
  {
    "text": "Natur runs through a lots of rules",
    "start": "429120",
    "end": "434330"
  },
  {
    "text": "all the world can be divided into two groups in kubernetes the first group is",
    "start": "434810",
    "end": "441960"
  },
  {
    "text": "a set of rules that are caught up predicates which are which are kind of hard constraints",
    "start": "441960",
    "end": "450180"
  },
  {
    "text": "are filters if a node cannot meet the requirements",
    "start": "450180",
    "end": "455430"
  },
  {
    "text": "of any predicated rule this node will be regarded as not qualified and being",
    "start": "455430",
    "end": "461460"
  },
  {
    "text": "filtered out so after the scheduler evaluates all the",
    "start": "461460",
    "end": "468360"
  },
  {
    "text": "predicates only some of the nodes will remain there some of them already filled out",
    "start": "468360",
    "end": "474380"
  },
  {
    "text": "there are not they're not all will predicated rule here I just released some of them for example the first one",
    "start": "474380",
    "end": "481070"
  },
  {
    "text": "Todd Pfitzer resources which means the available resource on the know that",
    "start": "481070",
    "end": "487920"
  },
  {
    "text": "should be large enough to run a pod if the product required to kick bytes or",
    "start": "487920",
    "end": "493830"
  },
  {
    "text": "memories in the available memory and the other node should be at least 2 gigabytes",
    "start": "493830",
    "end": "499550"
  },
  {
    "text": "the second rule for the phase part some parts are using hosted level part making",
    "start": "499550",
    "end": "506370"
  },
  {
    "text": "so the scheduler will make sure the corresponding part had not been occupied on that note",
    "start": "506370",
    "end": "513770"
  },
  {
    "text": "anyway after the scheduler finishes a pretty predicate stage some of the notes",
    "start": "513770",
    "end": "520260"
  },
  {
    "text": "have been filtered out under the in scheduler we are run through the second stage which involves the other group of",
    "start": "520260",
    "end": "527340"
  },
  {
    "text": "reviewers cada priorities each priority rule work you work you ask",
    "start": "527340",
    "end": "534920"
  },
  {
    "text": "God for know the candidate this this God indicates to what extent",
    "start": "534920",
    "end": "540920"
  },
  {
    "text": "this node is preferred from the perspective of this rule",
    "start": "540920",
    "end": "546700"
  },
  {
    "text": "for example here I listed some most commonly used the priority rules",
    "start": "546700",
    "end": "552760"
  },
  {
    "text": "imager locality proud this one means if I know the candidate",
    "start": "552760",
    "end": "558890"
  },
  {
    "text": "has already downloaded the container immediately is cache then this node should receive a",
    "start": "558890",
    "end": "566510"
  },
  {
    "text": "higher score the second one balance the resource",
    "start": "566510",
    "end": "572959"
  },
  {
    "text": "allocation the scheduler will evaluate if the resource and know the candidate to see",
    "start": "572959",
    "end": "581089"
  },
  {
    "text": "if it's used in a balanced way before a party the puller is being put on that",
    "start": "581089",
    "end": "586370"
  },
  {
    "text": "note because we do not want to use up all these CPU resources on a node by the",
    "start": "586370",
    "end": "591740"
  },
  {
    "text": "Leo most of memory resource unused or vice versa we use up all the memory resources per DeLeo most of us abusers",
    "start": "591740",
    "end": "599570"
  },
  {
    "text": "are used each priority rule here is associated with the weight so it should",
    "start": "599570",
    "end": "606050"
  },
  {
    "text": "know the Canada will receive an overall score and the Lane scheduler simply",
    "start": "606050",
    "end": "611510"
  },
  {
    "text": "picks the node which has the highest score if there are more than one notes",
    "start": "611510",
    "end": "617630"
  },
  {
    "text": "have the same high school in currently implementing into a pick a random one",
    "start": "617630",
    "end": "624339"
  },
  {
    "text": "connected is very flexible it allows you to customize the set of rules you want",
    "start": "624700",
    "end": "630950"
  },
  {
    "text": "to run with a concept called the scheduling policy a second approach is",
    "start": "630950",
    "end": "636290"
  },
  {
    "text": "basically our combination our particular predicates and the priorities",
    "start": "636290",
    "end": "641440"
  },
  {
    "text": "currently you can configure the security policy you want you want in either in",
    "start": "641440",
    "end": "647899"
  },
  {
    "text": "source codes or by providing a common line parameters in other slides we talked about to some",
    "start": "647899",
    "end": "655220"
  },
  {
    "text": "walk away data to provide a dynamic scheduling policy",
    "start": "655220",
    "end": "660550"
  },
  {
    "text": "so we have discussed the how a single scheduler works now we move to the multi",
    "start": "666810",
    "end": "672480"
  },
  {
    "text": "schedule at heart probably the first question we were asked is why we would want to run",
    "start": "672480",
    "end": "678360"
  },
  {
    "text": "mattias schedulers here by multi schedule I mean multiple scheduler processes not a not only module stress",
    "start": "678360",
    "end": "686250"
  },
  {
    "text": "inside the one process so there are different processes independent the processes",
    "start": "686250",
    "end": "693080"
  },
  {
    "text": "typically we wrong motives scheduled for the benefits of engineering flexibility",
    "start": "693110",
    "end": "698970"
  },
  {
    "text": "availability and the scalability one of the main scenario is nowadays is",
    "start": "698970",
    "end": "706310"
  },
  {
    "text": "pretty common to wrong different types of workloads on a single shared cluster",
    "start": "706310",
    "end": "712940"
  },
  {
    "text": "like a long-running services user facing services batch jobs trimming jobs all",
    "start": "712940",
    "end": "719790"
  },
  {
    "text": "these things so they can share resource dynamically and we can achieve a higher",
    "start": "719790",
    "end": "726290"
  },
  {
    "text": "resource utilization but the different types of all colors have different scheduling requirements",
    "start": "726290",
    "end": "733160"
  },
  {
    "text": "for example for a long-running service like web server we probably want to find",
    "start": "733160",
    "end": "739470"
  },
  {
    "text": "a best node and the most stable node void because once it's deployed it",
    "start": "739470",
    "end": "747300"
  },
  {
    "text": "probably was wrong for months or even for years so we are willing to spend a",
    "start": "747300",
    "end": "752910"
  },
  {
    "text": "little bit more time on scheduling itself whether we want to find the most stable notes",
    "start": "752910",
    "end": "758780"
  },
  {
    "text": "but for some short jobs that probably were only wrong for several minutes are",
    "start": "758780",
    "end": "764430"
  },
  {
    "text": "even for several seconds for for those jobs circadian latency probably the most important C and for",
    "start": "764430",
    "end": "772380"
  },
  {
    "text": "some big data jobs the data locality is the most emergency of course we can combine different",
    "start": "772380",
    "end": "780090"
  },
  {
    "text": "scheduling algorithms different requirements into one schedule but from",
    "start": "780090",
    "end": "785400"
  },
  {
    "text": "engineering perspective it would be a little difficult for us to develop an element an extremely such a big giant",
    "start": "785400",
    "end": "793190"
  },
  {
    "text": "monolithic schedule people we split them into several different",
    "start": "793190",
    "end": "800089"
  },
  {
    "text": "or schedulers each focus on a particular type of workload it will be more flexible for us to try out different the",
    "start": "800089",
    "end": "808249"
  },
  {
    "text": "scheduler implementations and it will also be easier for us to develop maintain and evolve this schedulers",
    "start": "808249",
    "end": "816220"
  },
  {
    "text": "this is about the flexibility the second motivation of",
    "start": "816220",
    "end": "821959"
  },
  {
    "text": "running multi schedulers is about availability and scalability",
    "start": "821959",
    "end": "828040"
  },
  {
    "text": "if we are cluster only supports one scheduler instance and they want to",
    "start": "828069",
    "end": "833300"
  },
  {
    "text": "update their scheduled instance then there will be a some time during that period a period of time part will not be",
    "start": "833300",
    "end": "841059"
  },
  {
    "text": "scheduled but if you are Sookie across the supported motive schedule instances",
    "start": "841059",
    "end": "846290"
  },
  {
    "text": "it will be very easy for you to add remove or update some of the instances without any downtime",
    "start": "846290",
    "end": "853839"
  },
  {
    "text": "and also be able to wrong multi schedules also",
    "start": "853839",
    "end": "859339"
  },
  {
    "text": "give you the ability to be able to scare out schedulers on the processor level",
    "start": "859339",
    "end": "864879"
  },
  {
    "text": "those are the motivation that way usually romaji schedulers",
    "start": "864879",
    "end": "873759"
  },
  {
    "text": "so let's see how kubernetes elaborate us to run more tea schedules these are",
    "start": "877650",
    "end": "884690"
  },
  {
    "text": "about two features are provided by kubernetes wine job dispatch the second",
    "start": "884690",
    "end": "890610"
  },
  {
    "text": "one is conflict detection let's say now you have a new scheduler",
    "start": "890610",
    "end": "898920"
  },
  {
    "text": "you want to write along with the existing key for the scheduler process",
    "start": "898920",
    "end": "904430"
  },
  {
    "text": "if we really simple out this the first question will be for a given hot what",
    "start": "904430",
    "end": "910350"
  },
  {
    "text": "scheduler instance should have pick it up as we just discussed we usually develop",
    "start": "910350",
    "end": "918060"
  },
  {
    "text": "different scheduler for different types of the workloads so for a given part we",
    "start": "918060",
    "end": "925589"
  },
  {
    "text": "do not want it to be picked up by random instance we usually have a preference on",
    "start": "925589",
    "end": "931710"
  },
  {
    "text": "which scheduled instance chuda should schedule this part the way that the communities of this is",
    "start": "931710",
    "end": "937860"
  },
  {
    "text": "a pretty straightforward first in now we have a stable zero this",
    "start": "937860",
    "end": "946430"
  },
  {
    "text": "first way we are assigning a schedule name to each scheduled instance",
    "start": "946430",
    "end": "952820"
  },
  {
    "text": "currently in the default scheduler this is done by a command line parameter so",
    "start": "952820",
    "end": "958860"
  },
  {
    "text": "each scheduler instance we have a unique name in the system anything in the step step one weighing a",
    "start": "958860",
    "end": "967110"
  },
  {
    "text": "process submitted user can set up a notation",
    "start": "967110",
    "end": "973250"
  },
  {
    "text": "to specify the desired scheduler schedule name and in the scheduler Jesus sorry in Step",
    "start": "973250",
    "end": "981120"
  },
  {
    "text": "three when your scheduler picks up an unscheduled apart it will check the",
    "start": "981120",
    "end": "986850"
  },
  {
    "text": "annotation on that part if if the under setting value does not match its own",
    "start": "986850",
    "end": "993089"
  },
  {
    "text": "name because it knows its own name if it does not match it simply drops the support",
    "start": "993089",
    "end": "998750"
  },
  {
    "text": "because it's a note that are some other schedule where scheduled this part",
    "start": "998750",
    "end": "1003920"
  },
  {
    "text": "eventually and also in the system there is a",
    "start": "1003920",
    "end": "1009390"
  },
  {
    "text": "predefined value for the T for the schedule so if the user does not specify",
    "start": "1009390",
    "end": "1015480"
  },
  {
    "text": "this annotation which is optional the part where we pick it up by the default",
    "start": "1015480",
    "end": "1021000"
  },
  {
    "text": "schedule yes name of schedule yeah this depends on",
    "start": "1021000",
    "end": "1028350"
  },
  {
    "text": "the implementation of the scheduler for the current T for the scheduler implementation it's provided a as a",
    "start": "1028350",
    "end": "1034620"
  },
  {
    "text": "parameter during the command line parameter during the scheduler stood up",
    "start": "1034620",
    "end": "1041120"
  },
  {
    "text": "yes the scheduling your instance and this I'm attaching is optional if we do not satisfy your part will be picked up",
    "start": "1043730",
    "end": "1050640"
  },
  {
    "text": "by the T for the schedule instance if there's no schedule at all",
    "start": "1050640",
    "end": "1057740"
  },
  {
    "text": "or if it's wrong then the part will remain in the pending status because we",
    "start": "1057740",
    "end": "1063360"
  },
  {
    "text": "talked to not know how it's not is wrong or simply the scheduler instance I had not be studied right yeah",
    "start": "1063360",
    "end": "1070580"
  },
  {
    "text": "the second feature data is provided by kubernetes to support multi scheduler is",
    "start": "1072470",
    "end": "1078480"
  },
  {
    "text": "a complicated action which I explain the problem with a very simple example",
    "start": "1078480",
    "end": "1084500"
  },
  {
    "text": "let's say we have a very small kubernetes cluster which has only one",
    "start": "1084500",
    "end": "1089760"
  },
  {
    "text": "node and we have a two scheduled instances now there are two parts are",
    "start": "1089760",
    "end": "1096810"
  },
  {
    "text": "submitted to IP our server and they are picked up by scheduler one",
    "start": "1096810",
    "end": "1102180"
  },
  {
    "text": "scheduled to let's say both parts require two gigabytes of memory and on",
    "start": "1102180",
    "end": "1109980"
  },
  {
    "text": "the only on the only slave load or house three gigabytes of memory available so",
    "start": "1109980",
    "end": "1115380"
  },
  {
    "text": "what happen is that scheduler one we're saying it's okay to put the p1 only node",
    "start": "1115380",
    "end": "1121740"
  },
  {
    "text": "because there are three gigabytes of memory available why is that hard only",
    "start": "1121740",
    "end": "1126900"
  },
  {
    "text": "required to keep our memory the same thing we are having to schedule a tool because scheduler tool will see",
    "start": "1126900",
    "end": "1133440"
  },
  {
    "text": "the exactly same thing and now we can say whatever happening for both parts",
    "start": "1133440",
    "end": "1139770"
  },
  {
    "text": "are belonged on another one the resource will not be sufficient",
    "start": "1139770",
    "end": "1145820"
  },
  {
    "text": "this problem is a resource conflict caused by the by risk condition it can",
    "start": "1145820",
    "end": "1151980"
  },
  {
    "text": "happen to other resource types like like a TCP pass or something else not only",
    "start": "1151980",
    "end": "1158340"
  },
  {
    "text": "secure in the memory and this is a problem what should be pretty familiar with it right it's something we always",
    "start": "1158340",
    "end": "1163950"
  },
  {
    "text": "handle when we are doing some multi-threading programming what cognitive does to solve this",
    "start": "1163950",
    "end": "1170880"
  },
  {
    "text": "problem is simple and effective in step 4 the last step we when the cube",
    "start": "1170880",
    "end": "1178320"
  },
  {
    "text": "net tried to launch path now it aware rerun all the predicates predicate",
    "start": "1178320",
    "end": "1185240"
  },
  {
    "text": "so in this example the first part p1 will be launched",
    "start": "1185240",
    "end": "1190770"
  },
  {
    "text": "successfully and then the available memory on that node will be updated to",
    "start": "1190770",
    "end": "1196140"
  },
  {
    "text": "one gigabyte of memory because other one had already being launched and then the",
    "start": "1196140",
    "end": "1201450"
  },
  {
    "text": "kinetic continues to launch tried to launch for the tool and neither were wrong all the predicates are gear this",
    "start": "1201450",
    "end": "1208080"
  },
  {
    "text": "time the part of its resource rua remember just the discussed that rule that the rule was fair because the",
    "start": "1208080",
    "end": "1215130"
  },
  {
    "text": "available results are no the one is not enough so this is how the cubanelle currently",
    "start": "1215130",
    "end": "1220740"
  },
  {
    "text": "kubernetes detects the possible conflict detection sorry the possible results",
    "start": "1220740",
    "end": "1227310"
  },
  {
    "text": "conflicts between different scheduler schedulers and currently its are not either were not a",
    "start": "1227310",
    "end": "1234090"
  },
  {
    "text": "risk a deposit is simply mark as a failure",
    "start": "1234090",
    "end": "1237950"
  },
  {
    "text": "so we have discussed how a single scheduler works in communities and why",
    "start": "1243700",
    "end": "1248830"
  },
  {
    "text": "we would want to remote his schedule and also how kubernetes Aleppo does to remote schedules",
    "start": "1248830",
    "end": "1255330"
  },
  {
    "text": "in this part I will talk about some walk away today in our project trying to",
    "start": "1255330",
    "end": "1260350"
  },
  {
    "text": "provide an enhanced support for multi schedulers as I mentioned earlier all",
    "start": "1260350",
    "end": "1265810"
  },
  {
    "text": "this work is a part of our internal project name the house while we are stretching and scheduling system hahaha",
    "start": "1265810",
    "end": "1272910"
  },
  {
    "text": "so what exactly do we want to improve first in job dispatch area",
    "start": "1272910",
    "end": "1282090"
  },
  {
    "text": "in currently kubernetes solution if there are several scheduler instances",
    "start": "1282090",
    "end": "1289090"
  },
  {
    "text": "the user need to specify the name of the instance in pod annotation",
    "start": "1289090",
    "end": "1294960"
  },
  {
    "text": "let's say sometimes we were all wrong several instances of the same scheduler type let's say we run three instances",
    "start": "1294960",
    "end": "1301990"
  },
  {
    "text": "instance ABC they are all same have the same same functionality actually it will",
    "start": "1301990",
    "end": "1308440"
  },
  {
    "text": "be difficult for a user to space to know which instance numerator should set",
    "start": "1308440",
    "end": "1314250"
  },
  {
    "text": "because schedule a probably will be very busy has a long scheduling queue why our",
    "start": "1314250",
    "end": "1321490"
  },
  {
    "text": "scheduler B and the C probably is idle so if you specify",
    "start": "1321490",
    "end": "1326970"
  },
  {
    "text": "schedule a and attaching you have your pod we have a very long scheduling",
    "start": "1326970",
    "end": "1332170"
  },
  {
    "text": "latency if we use specialized instead of P and C it will be scheduled very quickly",
    "start": "1332170",
    "end": "1338700"
  },
  {
    "text": "and this is hard for users to know which instance is the best one so what do we",
    "start": "1338700",
    "end": "1344530"
  },
  {
    "text": "want to do here is the first we want to provide an instance level load balancing mechanism",
    "start": "1344530",
    "end": "1349740"
  },
  {
    "text": "so we will automatically have a user select instance",
    "start": "1349740",
    "end": "1355950"
  },
  {
    "text": "second is about a dynamic scheduling policy in current kubernetes as a minute",
    "start": "1355950",
    "end": "1362170"
  },
  {
    "text": "or animation you can specify the scheduling policy in source code or by",
    "start": "1362170",
    "end": "1368490"
  },
  {
    "text": "command line parameter during scheduler stava startup but after the schedule is started the",
    "start": "1368490",
    "end": "1376180"
  },
  {
    "text": "support of the policy is fixed and it's only one policy here we was the pod mod multiple",
    "start": "1376180",
    "end": "1383770"
  },
  {
    "text": "policies and the users can specify the scheduling policy it needs in the pod",
    "start": "1383770",
    "end": "1390190"
  },
  {
    "text": "annotation and our system will make sure the right instance scheduled instance is",
    "start": "1390190",
    "end": "1395620"
  },
  {
    "text": "selected a further part with this scheduling policy support",
    "start": "1395620",
    "end": "1400980"
  },
  {
    "text": "in the conflict resolution area",
    "start": "1401370",
    "end": "1406410"
  },
  {
    "text": "currently the computation animation is implemented in",
    "start": "1406770",
    "end": "1412540"
  },
  {
    "text": "cube net it's distributed but there's a problem with the scheduling",
    "start": "1412540",
    "end": "1418480"
  },
  {
    "text": "latency because the wings are complicated detect the income net it usually is already",
    "start": "1418480",
    "end": "1425610"
  },
  {
    "text": "about the 2 seconds to 10 seconds after the scheduling time depending on the",
    "start": "1425610",
    "end": "1431080"
  },
  {
    "text": "cluster loading this is not acceptable for some short",
    "start": "1431080",
    "end": "1436090"
  },
  {
    "text": "jobs or screaming jobs which have strict requirements on scheduling latency so",
    "start": "1436090",
    "end": "1441790"
  },
  {
    "text": "what do we want to do here is to provide early company with detection mechanism",
    "start": "1441790",
    "end": "1446980"
  },
  {
    "text": "so any resource conflicts should be detected in the first place and the pod",
    "start": "1446980",
    "end": "1452890"
  },
  {
    "text": "should be rescheduled as soon as possible this part of work is inspired by a paper",
    "start": "1452890",
    "end": "1460810"
  },
  {
    "text": "from Google Omega by the way adding lots of new features also we want to provide flexible",
    "start": "1460810",
    "end": "1467020"
  },
  {
    "text": "comforting the criteria so we can reduce the possibility of conflicts and the",
    "start": "1467020",
    "end": "1474190"
  },
  {
    "text": "since there are many different schedulers different work a lot of types we also want to provide a rich",
    "start": "1474190",
    "end": "1481140"
  },
  {
    "text": "policy control on this conflict the resolution in the following slides I",
    "start": "1481140",
    "end": "1486880"
  },
  {
    "text": "will explain this work in detail",
    "start": "1486880",
    "end": "1490920"
  },
  {
    "text": "first this is this diagram shows our enhanced job dispatch mechanism before",
    "start": "1492270",
    "end": "1499210"
  },
  {
    "text": "we compare this diagram to the previous diagram we will see that",
    "start": "1499210",
    "end": "1504900"
  },
  {
    "text": "first you know step 0 we need to configure the scheduler type and since names here we're on threes instead",
    "start": "1504900",
    "end": "1513040"
  },
  {
    "text": "of sorry for instance it was scheduled a type 2 and the two instances for scheduler type walk",
    "start": "1513040",
    "end": "1519600"
  },
  {
    "text": "in step 1 when you the submitter part now that no need to specify the instance",
    "start": "1519600",
    "end": "1527440"
  },
  {
    "text": "name directly the only need to specify the type and we",
    "start": "1527440",
    "end": "1534130"
  },
  {
    "text": "introduce the new module cutter scheduler controller this controller is aware of all the running scheduled",
    "start": "1534130",
    "end": "1541540"
  },
  {
    "text": "instances either way of assigning our scheduler",
    "start": "1541540",
    "end": "1546580"
  },
  {
    "text": "instance dynamically trying to balance the note among the scheduler instances",
    "start": "1546580",
    "end": "1552700"
  },
  {
    "text": "and what it does is that it will update the existing schedule name and notation",
    "start": "1552700",
    "end": "1559050"
  },
  {
    "text": "so the existing schedule caste our work in",
    "start": "1559050",
    "end": "1564670"
  },
  {
    "text": "the same way it only scheduled the process of is its own name",
    "start": "1564670",
    "end": "1571210"
  },
  {
    "text": "but with this scheduler control controller we can make sure that if",
    "start": "1571210",
    "end": "1576220"
  },
  {
    "text": "there are multi instances of the same schedule type the the cure is roughly",
    "start": "1576220",
    "end": "1582660"
  },
  {
    "text": "roughly seven days and the scheduled emergency is roughly same and we also support a dynamical",
    "start": "1582660",
    "end": "1590950"
  },
  {
    "text": "scheduler policy user can specify the polished scheduling policy to want and",
    "start": "1590950",
    "end": "1596760"
  },
  {
    "text": "the scheduler controller will make sure a right instance with that a policy support being selected for the report",
    "start": "1596760",
    "end": "1604590"
  },
  {
    "text": "this is our enhanced our support for job dispatch",
    "start": "1604590",
    "end": "1609810"
  },
  {
    "text": "for comfortingly detecting the most important changes that we",
    "start": "1613020",
    "end": "1619020"
  },
  {
    "text": "introduced a new module called a comfort lee resolve in api server this module",
    "start": "1619020",
    "end": "1625470"
  },
  {
    "text": "we're kicking wing a binding request is submitted a binding requester basically",
    "start": "1625470",
    "end": "1631140"
  },
  {
    "text": "what our pioneering casitas is writing the name of the node to the new",
    "start": "1631140",
    "end": "1637230"
  },
  {
    "text": "name field so how the works is like this wearing a",
    "start": "1637230",
    "end": "1645690"
  },
  {
    "text": "scheduler finds a person node for for apart it'll create a binding fine",
    "start": "1645690",
    "end": "1650910"
  },
  {
    "text": "binding request this binding request is submitted a PS over and they need is intercepted by our complete resolve",
    "start": "1650910",
    "end": "1657600"
  },
  {
    "text": "module it will check if there are any results conflicts based on the different",
    "start": "1657600",
    "end": "1663990"
  },
  {
    "text": "criteria that are specified by the schedulers if there is a conflict it",
    "start": "1663990",
    "end": "1669240"
  },
  {
    "text": "will return an error and the scheduler is supposed to really wrong the scheduling process immediately to find",
    "start": "1669240",
    "end": "1676050"
  },
  {
    "text": "Eliza node for that thought in this way we can catch any resource conflict in",
    "start": "1676050",
    "end": "1682710"
  },
  {
    "text": "the first place and the reschedule pot rather than writing the particles either",
    "start": "1682710",
    "end": "1688679"
  },
  {
    "text": "city and the in waiting some time being picked up by coop net and the Tsukuba netting spending some other time and",
    "start": "1688679",
    "end": "1695010"
  },
  {
    "text": "they detected the conflicts now the company detected immediately",
    "start": "1695010",
    "end": "1701240"
  },
  {
    "text": "and for the conflict the criteria we also provide a fine current model now there",
    "start": "1708050",
    "end": "1715850"
  },
  {
    "text": "are three different complicated in the system schedulers can choose the desired",
    "start": "1715850",
    "end": "1723440"
  },
  {
    "text": "one based on its knowledge on the Curan apart our workloads the first criteria is called a strong",
    "start": "1723440",
    "end": "1731780"
  },
  {
    "text": "strong criteria so don't conflict cut here it's based on the know the resource",
    "start": "1731780",
    "end": "1738920"
  },
  {
    "text": "versioning mechanism we introduce the new versioning versioning number for each node the",
    "start": "1738920",
    "end": "1745340"
  },
  {
    "text": "reason we introduce a new version number rather than reused the existing resource",
    "start": "1745340",
    "end": "1750350"
  },
  {
    "text": "working is because we our our new resource our new version",
    "start": "1750350",
    "end": "1755750"
  },
  {
    "text": "number will only change when there are any result changes but the existing resource of working in were changes",
    "start": "1755750",
    "end": "1761240"
  },
  {
    "text": "every time when the HUD beta sub C so when our pioneer request is submitted",
    "start": "1761240",
    "end": "1766940"
  },
  {
    "text": "either work area write up a previously watching language that the schedulers",
    "start": "1766940",
    "end": "1772520"
  },
  {
    "text": "saw last time so the conflict resolved module will compare if the two lumber",
    "start": "1772520",
    "end": "1779450"
  },
  {
    "text": "the same if they are different they a conflict is",
    "start": "1779450",
    "end": "1784640"
  },
  {
    "text": "a trigger so basically it means if there are any changes happening on this node",
    "start": "1784640",
    "end": "1789800"
  },
  {
    "text": "after the scheduler gets the status in last time range conflict this mechanism",
    "start": "1789800",
    "end": "1795020"
  },
  {
    "text": "is a very very strong it'll make sure that no the status result status is there exactly the same as the scheduler",
    "start": "1795020",
    "end": "1801920"
  },
  {
    "text": "saw last time the one of the possibility of conflicts",
    "start": "1801920",
    "end": "1809390"
  },
  {
    "text": "is a high the sectional conflict criteria is called a weak criteria it's not the best",
    "start": "1809390",
    "end": "1817220"
  },
  {
    "text": "on resource versioning it's based on the know the resource quantity this is for some",
    "start": "1817220",
    "end": "1824830"
  },
  {
    "text": "some job types that let another require any special resource or any parts or",
    "start": "1824830",
    "end": "1830570"
  },
  {
    "text": "something other resources they only require some CPU and memory resource to wrong so in order to reduce the",
    "start": "1830570",
    "end": "1837080"
  },
  {
    "text": "possibility of the conflict they can use this week criteria the company resolver",
    "start": "1837080",
    "end": "1843110"
  },
  {
    "text": "check even illa if the the washing number has changed on the note as long",
    "start": "1843110",
    "end": "1849380"
  },
  {
    "text": "as the remaining CPU and memory source are enough to run this part it will be",
    "start": "1849380",
    "end": "1855280"
  },
  {
    "text": "regarded as a success case not a conflict so this provider our week",
    "start": "1855280",
    "end": "1861410"
  },
  {
    "text": "guarantee but it'll reduce the possibility of conflicts the third criteria is something between",
    "start": "1861410",
    "end": "1868520"
  },
  {
    "text": "strand wig it's based on boolean expression this expression can reference",
    "start": "1868520",
    "end": "1874040"
  },
  {
    "text": "any know the properties our pod properties the company resolver we evaluated it's",
    "start": "1874040",
    "end": "1880370"
  },
  {
    "text": "fresh if the result is is truly it's it's okay if it's forced means a",
    "start": "1880370",
    "end": "1887840"
  },
  {
    "text": "conflict so in scheduler submits a pioneer",
    "start": "1887840",
    "end": "1894559"
  },
  {
    "text": "request it can choose different criteria based on its knowledge on the on the pod",
    "start": "1894559",
    "end": "1899990"
  },
  {
    "text": "and the workload",
    "start": "1899990",
    "end": "1903400"
  },
  {
    "text": "we also provide a rich policy control our own company and resolution first is part of priority based company",
    "start": "1908660",
    "end": "1916310"
  },
  {
    "text": "resolution each part we is associated with the priority field and accordingly",
    "start": "1916310",
    "end": "1923840"
  },
  {
    "text": "each pioneer request has a approach which is the pod approach we were first the process process",
    "start": "1923840",
    "end": "1931160"
  },
  {
    "text": "planning requests that has have a higher priority because the earlier by",
    "start": "1931160",
    "end": "1936770"
  },
  {
    "text": "Newcastle is processed the more possible it's being accepted",
    "start": "1936770",
    "end": "1942250"
  },
  {
    "text": "and now we are also trying to add a schedule a priority based copy",
    "start": "1942250",
    "end": "1947870"
  },
  {
    "text": "resolution this is because we find that some schedule because they have",
    "start": "1947870",
    "end": "1954980"
  },
  {
    "text": "different the implementation the cost of rescheduling is more expensive than some simple",
    "start": "1954980",
    "end": "1961340"
  },
  {
    "text": "schedules so it makes sense to give a high priority to this kind of",
    "start": "1961340",
    "end": "1966490"
  },
  {
    "text": "schedules to avoid its rescheduling and",
    "start": "1966490",
    "end": "1971860"
  },
  {
    "text": "we support a batch comfortable resolution this means when our scheduler",
    "start": "1972280",
    "end": "1979180"
  },
  {
    "text": "submits a binding request it can submit multiple Pioneer has together this is",
    "start": "1979180",
    "end": "1985280"
  },
  {
    "text": "probably not very useful for the people community for the scheduler because because it's a schedule part one by one",
    "start": "1985280",
    "end": "1991400"
  },
  {
    "text": "but for some schedule the schedule pass in batch and this is pretty useful it",
    "start": "1991400",
    "end": "1997480"
  },
  {
    "text": "improves the performance and the furthermore we provide a group best",
    "start": "1997480",
    "end": "2004090"
  },
  {
    "text": "company resolution this means let me spend with an example suppose our",
    "start": "2004090",
    "end": "2010650"
  },
  {
    "text": "scheduler sum is 10 by Network has together as a batch among these 10",
    "start": "2010650",
    "end": "2016360"
  },
  {
    "text": "binding requests probably the first verse 3 the Pioneer Augusta one two",
    "start": "2016360",
    "end": "2022780"
  },
  {
    "text": "three they have some affinity or ante affinity constraints so the scheduler will want",
    "start": "2022780",
    "end": "2029560"
  },
  {
    "text": "to this three requests succeed or fail together as a atomic",
    "start": "2029560",
    "end": "2036370"
  },
  {
    "text": "unit while the remaining seven loka's can succeed or fail in depend so it can",
    "start": "2036370",
    "end": "2044260"
  },
  {
    "text": "specify different interests in this way we can provide a require the semantics",
    "start": "2044260",
    "end": "2050169"
  },
  {
    "text": "constraints at the same time reduce the possibility our conflicts",
    "start": "2050169",
    "end": "2055320"
  },
  {
    "text": "all this would walk away I described with the the group best comfort leery",
    "start": "2055320",
    "end": "2060669"
  },
  {
    "text": "solution is the the third criteria hasn't been done all the others data have been done",
    "start": "2060669",
    "end": "2068490"
  },
  {
    "text": "this is a picture shoe shoes all the current schedule that has run on top of",
    "start": "2070290",
    "end": "2076898"
  },
  {
    "text": "our enhance the multi scheduled framework the first one is the kubernetes native",
    "start": "2076899",
    "end": "2083169"
  },
  {
    "text": "schedule with some pharmacy improvements the second one is a scheduler color",
    "start": "2083169",
    "end": "2089350"
  },
  {
    "text": "firmament it came from Cambridge University from a research project and it was",
    "start": "2089350",
    "end": "2095080"
  },
  {
    "text": "original written in C++ we have some collaboration with the outside we changed the program the implement",
    "start": "2095080",
    "end": "2102520"
  },
  {
    "text": "language from simple applause to ago and adapted it to come daddies I know that a",
    "start": "2102520",
    "end": "2108130"
  },
  {
    "text": "course is also doing the similar scene and the time medida some of their",
    "start": "2108130",
    "end": "2114040"
  },
  {
    "text": "working in one of the scheduling second meeting the sole scheduler is a kata",
    "start": "2114040",
    "end": "2120850"
  },
  {
    "text": "hacia schedule it's from a pepper from Stanford University the idea is that for",
    "start": "2120850",
    "end": "2125950"
  },
  {
    "text": "some batch jobs shop jobs with no need to evaluate all the notes in the cost",
    "start": "2125950",
    "end": "2131760"
  },
  {
    "text": "assuming the cost is very large we just evaluate some sampling notes based on",
    "start": "2131760",
    "end": "2139450"
  },
  {
    "text": "the crustal loading status and we also modified the methods of mass",
    "start": "2139450",
    "end": "2145660"
  },
  {
    "text": "time the young resource manager so there so we can support existing Mazal",
    "start": "2145660",
    "end": "2150700"
  },
  {
    "text": "schedulers and the young schedulers the they work at the available notes",
    "start": "2150700",
    "end": "2155800"
  },
  {
    "text": "resource information from us and the instrument they are scheduling requests in the same way to copy resolved just",
    "start": "2155800",
    "end": "2163780"
  },
  {
    "text": "like any other scheduled in the system this other supported schedule now",
    "start": "2163780",
    "end": "2170850"
  },
  {
    "text": "the last slide is about the motor schedule",
    "start": "2174630",
    "end": "2180430"
  },
  {
    "text": "in races as I just mentioned during the house project we also evaluate how much",
    "start": "2180430",
    "end": "2186820"
  },
  {
    "text": "schedules are supported in meters it's pretty interesting to see how it's done in a totally different architecture so",
    "start": "2186820",
    "end": "2193390"
  },
  {
    "text": "I'd like to share our understanding here too because there are many difference",
    "start": "2193390",
    "end": "2198790"
  },
  {
    "text": "between communities and the mazes from interminable scheduling our multi",
    "start": "2198790",
    "end": "2205300"
  },
  {
    "text": "schedulers there are two differences that we need to pay attention to first we know that the mazes must itself",
    "start": "2205300",
    "end": "2212410"
  },
  {
    "text": "doesn't do scheduling all the scheduling are done by different schedulers in different up firm works this diagram is",
    "start": "2212410",
    "end": "2219280"
  },
  {
    "text": "suited to application frameworks running on top of mazes and each scheduler has",
    "start": "2219280",
    "end": "2224650"
  },
  {
    "text": "its own task queue that do not share a global queue when a user is trying to submit tasks to",
    "start": "2224650",
    "end": "2232990"
  },
  {
    "text": "missus it's talking to the corresponding frameworks rather than missus must",
    "start": "2232990",
    "end": "2238170"
  },
  {
    "text": "because they do not share a global task queue so then there is no need to have a",
    "start": "2238170",
    "end": "2243580"
  },
  {
    "text": "annotation or something to do the job dispatch they are naturally dispatch",
    "start": "2243580",
    "end": "2251580"
  },
  {
    "text": "secondly resource conflicts caused by risk condition in mazes each scheduler",
    "start": "2252240",
    "end": "2259780"
  },
  {
    "text": "in not able to see the global state of all the available notes instead if the County Caesar knows",
    "start": "2259780",
    "end": "2267610"
  },
  {
    "text": "resources that are being sent by been sent by mrs. master to it which is the",
    "start": "2267610",
    "end": "2272650"
  },
  {
    "text": "cauda resource offer in this example the Mason master is",
    "start": "2272650",
    "end": "2277660"
  },
  {
    "text": "sending a result offer to schedule a 1 and the scheduler one decides to put a",
    "start": "2277660",
    "end": "2283090"
  },
  {
    "text": "two tasks from its task a cure on this piece of resource the important thing is that at the wrong",
    "start": "2283090",
    "end": "2290110"
  },
  {
    "text": "time War II saw a result offer is only sent to once one schedule so the",
    "start": "2290110",
    "end": "2295180"
  },
  {
    "text": "scheduler one doesn't need to worry about any recent conflicts called by the risk condition",
    "start": "2295180",
    "end": "2301500"
  },
  {
    "text": "so we can see that in mazes we don't have the problem job dispatcher and the",
    "start": "2301500",
    "end": "2307020"
  },
  {
    "text": "company called virus condition but does this mean it's a better architecture or a perfect heritage",
    "start": "2307020",
    "end": "2312630"
  },
  {
    "text": "this is not it's not it's basically a pessimistic style concurrency control",
    "start": "2312630",
    "end": "2318580"
  },
  {
    "text": "style why are communicated optimistic concurrency control style the problem is",
    "start": "2318580",
    "end": "2324880"
  },
  {
    "text": "this has mystical control style is that comparison control style is first",
    "start": "2324880",
    "end": "2331860"
  },
  {
    "text": "the scheduling quality is not good in some case because the scheduler simply",
    "start": "2331860",
    "end": "2337780"
  },
  {
    "text": "cannot see there are better nodes in the cluster you can only see a part of the notes so sometimes the scheduling",
    "start": "2337780",
    "end": "2344680"
  },
  {
    "text": "quality get hurt second the research because it's a kind of",
    "start": "2344680",
    "end": "2351910"
  },
  {
    "text": "pessimistic locked concurrency control the resource utilization is not a very very",
    "start": "2351910",
    "end": "2357970"
  },
  {
    "text": "high for some large tasks they're probably",
    "start": "2357970",
    "end": "2363250"
  },
  {
    "text": "where I need to wait a long time before the scheduler can get a large enough resource alpha to run this task so",
    "start": "2363250",
    "end": "2370140"
  },
  {
    "text": "naturally this scheduler tend to a cache cache resource offers even that not need",
    "start": "2370140",
    "end": "2375280"
  },
  {
    "text": "to run use it right now this is called resource utilization not high",
    "start": "2375280",
    "end": "2380790"
  },
  {
    "text": "in missile community nowadays I'm going after the called optimistic offers which",
    "start": "2380790",
    "end": "2386110"
  },
  {
    "text": "try to address these problems by providing resource offer to multiple",
    "start": "2386110",
    "end": "2391900"
  },
  {
    "text": "schedule at the same time but as this we are called conflict as well",
    "start": "2391900",
    "end": "2397650"
  },
  {
    "text": "the last page is a summary of this different three different systems we can",
    "start": "2400230",
    "end": "2407080"
  },
  {
    "text": "see that you can you can't read a horse as a enhancer communities on concurrency",
    "start": "2407080",
    "end": "2413230"
  },
  {
    "text": "control most criminals are optimistic wire missus is the pessimistic here we",
    "start": "2413230",
    "end": "2419050"
  },
  {
    "text": "do not include the ongoing optimistic offer improvement of for misses for",
    "start": "2419050",
    "end": "2425470"
  },
  {
    "text": "scheduler resource of view communities and a house they are based",
    "start": "2425470",
    "end": "2430750"
  },
  {
    "text": "on share the caliber state so a scheduler can see all the available notes resources in the cost why",
    "start": "2430750",
    "end": "2437900"
  },
  {
    "text": "mrs. masters account sorry missus of schedule accountancy patches has your notes for a job dispatch community knows about",
    "start": "2437900",
    "end": "2446060"
  },
  {
    "text": "multi type waste about the multi type and we also added a multi instance tire and a dynamic policy support",
    "start": "2446060",
    "end": "2452020"
  },
  {
    "text": "for conflict invitation community the kind of late detection mechanism and we",
    "start": "2452020",
    "end": "2457940"
  },
  {
    "text": "are based on early detection mechanism and for a comfortable model we are fine",
    "start": "2457940",
    "end": "2463460"
  },
  {
    "text": "during the model we we include different than criterias and also different policy controls",
    "start": "2463460",
    "end": "2470859"
  },
  {
    "text": "these are some reference and further readings if we are interested in this Fiat way now we are trying to finish is",
    "start": "2472150",
    "end": "2480890"
  },
  {
    "text": "a some of the work we have finished the most of all but they are still some of them not finished we are trying to finish them stabilize them and the open",
    "start": "2480890",
    "end": "2488420"
  },
  {
    "text": "source to the module framework much scheduler framework this is pretty much what I have today",
    "start": "2488420",
    "end": "2495020"
  },
  {
    "text": "thank you",
    "start": "2495020",
    "end": "2497770"
  },
  {
    "text": "I named cousin sure I'm sorry what what's wrong",
    "start": "2500940",
    "end": "2508990"
  },
  {
    "text": "[Music]",
    "start": "2508990",
    "end": "2512090"
  },
  {
    "text": "or further reading",
    "start": "2515430",
    "end": "2518599"
  },
  {
    "text": "what okay",
    "start": "2521660",
    "end": "2525140"
  },
  {
    "text": "yeah more friend also more flexibility like the policy control",
    "start": "2527900",
    "end": "2535490"
  },
  {
    "text": "yeah yes yes then",
    "start": "2544819",
    "end": "2551029"
  },
  {
    "text": "yes this is this is hot spot because even",
    "start": "2551839",
    "end": "2557819"
  },
  {
    "text": "within one scheduler process you can run more distress right this way I cost confidence too I just mention the data",
    "start": "2557819",
    "end": "2565130"
  },
  {
    "text": "in our system way we're under cuneta default scheduler with some performance improvements what we did is that away",
    "start": "2565130",
    "end": "2572250"
  },
  {
    "text": "partitioning the nose to reduce conflicts of possibilities and also in",
    "start": "2572250",
    "end": "2577589"
  },
  {
    "text": "the slide about the benefits of running multi schedulers I imagine that we have",
    "start": "2577589",
    "end": "2583680"
  },
  {
    "text": "the ability to scale out but I didn't say it Stephanie will have a better form the dancing your schedule it all depends",
    "start": "2583680",
    "end": "2590130"
  },
  {
    "text": "on your scheduling algorithm",
    "start": "2590130",
    "end": "2593720"
  },
  {
    "text": "yes [Music] yes yes yes yes if it exists our certain",
    "start": "2596119",
    "end": "2604750"
  },
  {
    "text": "amount of time we were we'd have a directory",
    "start": "2604750",
    "end": "2608730"
  },
  {
    "text": "yes",
    "start": "2612090",
    "end": "2615090"
  },
  {
    "text": "I started doing into this",
    "start": "2620780",
    "end": "2624610"
  },
  {
    "text": "well maybe sir look okay I got here look",
    "start": "2632430",
    "end": "2637770"
  },
  {
    "text": "at you Nana the question is about gain scheduling in the optimistic case yeah oh you mentioned that you can hoard",
    "start": "2637770",
    "end": "2643470"
  },
  {
    "text": "offers in the pessimistic case but how would you schedule a constellation of",
    "start": "2643470",
    "end": "2649109"
  },
  {
    "text": "containers in when you can get conflicts sorry I didn't go to cut it wait it's",
    "start": "2649109",
    "end": "2656490"
  },
  {
    "text": "like about the game scheduling header the game schedule yeah and things sorry what you came to get",
    "start": "2656490",
    "end": "2664349"
  },
  {
    "text": "you like an HPC yes",
    "start": "2664349",
    "end": "2672349"
  },
  {
    "text": "yes and nine won't do you need all ten oh yes [Music]",
    "start": "2672349",
    "end": "2679609"
  },
  {
    "text": "how would you do it in the optimistic so the question is that miles that means",
    "start": "2679609",
    "end": "2686880"
  },
  {
    "text": "that you want to try to I look at the tank enhancer as a group right yes",
    "start": "2686880",
    "end": "2692089"
  },
  {
    "text": "in in the pessimistic case it's actually still same in our model the scheduler we",
    "start": "2692089",
    "end": "2698309"
  },
  {
    "text": "submitted this telecast together and the company resolved we are evaluated if all",
    "start": "2698309",
    "end": "2703890"
  },
  {
    "text": "these helices can be fulfilled at the same time if not simply require return",
    "start": "2703890",
    "end": "2708960"
  },
  {
    "text": "the for scheduled need to take a lot of time candidates to trigger",
    "start": "2708960",
    "end": "2715400"
  },
  {
    "text": "[Music] good yes constraints that's either way it is kind",
    "start": "2715400",
    "end": "2723539"
  },
  {
    "text": "of transactional I have the last example is that I submitted 10 binding requests",
    "start": "2723539",
    "end": "2729119"
  },
  {
    "text": "but three of them are younger group authorization this is three well the company resolver will make sure the",
    "start": "2729119",
    "end": "2735089"
  },
  {
    "text": "either succeed or fail in together there will not be another independently",
    "start": "2735089",
    "end": "2741890"
  },
  {
    "text": "[Music] oh say you know you try to solve the issue",
    "start": "2746970",
    "end": "2754920"
  },
  {
    "text": "yes",
    "start": "2754920",
    "end": "2757920"
  },
  {
    "text": "yeah because we are just a finish in the data whereas they are running some funny",
    "start": "2762180",
    "end": "2768580"
  },
  {
    "text": "neural tests away we do not have exact data for that part but in our implementation who a tip in the change",
    "start": "2768580",
    "end": "2776050"
  },
  {
    "text": "the possibility our conflicts because in the original model is kind of a strong",
    "start": "2776050",
    "end": "2781090"
  },
  {
    "text": "conflict criteria we are trying to provide a more option to reduce this company the possibilities so we will not",
    "start": "2781090",
    "end": "2787690"
  },
  {
    "text": "increase the company the possibilities and also for the company greater resolution the self is it's a status in",
    "start": "2787690",
    "end": "2795430"
  },
  {
    "text": "API server so if it's a they're too much too many requests we can scare out with",
    "start": "2795430",
    "end": "2800590"
  },
  {
    "text": "the current API server a tree mechanism we will multiply our chat service that's spotted",
    "start": "2800590",
    "end": "2806550"
  },
  {
    "text": "[Music] okay so that's all thank you",
    "start": "2806550",
    "end": "2814170"
  },
  {
    "text": "[Applause]",
    "start": "2814170",
    "end": "2820369"
  }
]