[
  {
    "text": "all right uh okay thank you so much",
    "start": "60",
    "end": "2280"
  },
  {
    "text": "everyone for joining in I'm super",
    "start": "2280",
    "end": "3959"
  },
  {
    "text": "excited to be presenting at uh the HPC",
    "start": "3959",
    "end": "6540"
  },
  {
    "text": "day and I'm Mr y uh unfortunately my",
    "start": "6540",
    "end": "9360"
  },
  {
    "text": "co-speaker Rashid could not make it",
    "start": "9360",
    "end": "11099"
  },
  {
    "text": "because of Visa issues but uh we have",
    "start": "11099",
    "end": "13320"
  },
  {
    "text": "recorded some of his part of the uh talk",
    "start": "13320",
    "end": "16379"
  },
  {
    "text": "so yeah I hope that all of you enjoy so",
    "start": "16379",
    "end": "18539"
  },
  {
    "text": "the main purpose of today's talk is",
    "start": "18539",
    "end": "20400"
  },
  {
    "text": "really to understand that how can you",
    "start": "20400",
    "end": "22740"
  },
  {
    "text": "leverage the use of uh gpus inside of",
    "start": "22740",
    "end": "26460"
  },
  {
    "text": "kubernetes clusters but primarily for",
    "start": "26460",
    "end": "28260"
  },
  {
    "text": "machine learning workloads and how you",
    "start": "28260",
    "end": "30000"
  },
  {
    "text": "can maximize the efficiency of these",
    "start": "30000",
    "end": "32820"
  },
  {
    "text": "gpus uh for your machine learning",
    "start": "32820",
    "end": "35219"
  },
  {
    "text": "workloads because as we know that as the",
    "start": "35219",
    "end": "37860"
  },
  {
    "text": "model size of machine learning models",
    "start": "37860",
    "end": "39540"
  },
  {
    "text": "becomes bigger as you have more amount",
    "start": "39540",
    "end": "41460"
  },
  {
    "text": "of training data that you work with the",
    "start": "41460",
    "end": "43559"
  },
  {
    "text": "training process and the overall",
    "start": "43559",
    "end": "45300"
  },
  {
    "text": "complexity for the training process does",
    "start": "45300",
    "end": "47760"
  },
  {
    "text": "increase quite a bit right so we need to",
    "start": "47760",
    "end": "50039"
  },
  {
    "text": "use uh things like gpus or tpus and if",
    "start": "50039",
    "end": "53579"
  },
  {
    "text": "you were to scale this or orchestrate",
    "start": "53579",
    "end": "55260"
  },
  {
    "text": "this inside of a Jeep inside of a",
    "start": "55260",
    "end": "56820"
  },
  {
    "text": "communist cluster how can you do that",
    "start": "56820",
    "end": "58440"
  },
  {
    "text": "and what are some of the best ways of",
    "start": "58440",
    "end": "60059"
  },
  {
    "text": "being able to do so and use of these",
    "start": "60059",
    "end": "63120"
  },
  {
    "text": "gpus inside of container orchestration",
    "start": "63120",
    "end": "65880"
  },
  {
    "text": "technology specifically fine-tuned for",
    "start": "65880",
    "end": "67979"
  },
  {
    "text": "machine learning purposes such as cube",
    "start": "67979",
    "end": "69960"
  },
  {
    "text": "flow and flight so this is what is the",
    "start": "69960",
    "end": "71820"
  },
  {
    "text": "broad agenda for today so just a quick",
    "start": "71820",
    "end": "74280"
  },
  {
    "text": "introduction I am a developer Solutions",
    "start": "74280",
    "end": "76020"
  },
  {
    "text": "engineer at melesearch and uh Rashid who",
    "start": "76020",
    "end": "78600"
  },
  {
    "text": "is my co-speaker he's a CS undergrad at",
    "start": "78600",
    "end": "81060"
  },
  {
    "text": "University of Toronto",
    "start": "81060",
    "end": "82860"
  },
  {
    "text": "now I'll just take a quick fire of what",
    "start": "82860",
    "end": "86280"
  },
  {
    "text": "exactly is envelopes right so whenever",
    "start": "86280",
    "end": "88259"
  },
  {
    "text": "we talk about any machine learning life",
    "start": "88259",
    "end": "89700"
  },
  {
    "text": "cycle there is always we start with",
    "start": "89700",
    "end": "91320"
  },
  {
    "text": "defining the problem statement talking",
    "start": "91320",
    "end": "93119"
  },
  {
    "text": "about the training data set then we",
    "start": "93119",
    "end": "95460"
  },
  {
    "text": "train our machine learning algorithm we",
    "start": "95460",
    "end": "97140"
  },
  {
    "text": "fine tune it and we get the results and",
    "start": "97140",
    "end": "99840"
  },
  {
    "text": "if the results are good enough we'll go",
    "start": "99840",
    "end": "101220"
  },
  {
    "text": "ahead and deploy it right so this is of",
    "start": "101220",
    "end": "103020"
  },
  {
    "text": "course a typical machine learning life",
    "start": "103020",
    "end": "105420"
  },
  {
    "text": "cycle but if we talk about the entire",
    "start": "105420",
    "end": "107640"
  },
  {
    "text": "process of ml Ops which is basically",
    "start": "107640",
    "end": "109380"
  },
  {
    "text": "bringing in devops for your machine",
    "start": "109380",
    "end": "111360"
  },
  {
    "text": "learning workloads we see that the ml",
    "start": "111360",
    "end": "113700"
  },
  {
    "text": "code right that is a very small part of",
    "start": "113700",
    "end": "116640"
  },
  {
    "text": "the overall architecture and of course",
    "start": "116640",
    "end": "118320"
  },
  {
    "text": "the most important one that I feel",
    "start": "118320",
    "end": "119820"
  },
  {
    "text": "personally is the serving infrastructure",
    "start": "119820",
    "end": "121680"
  },
  {
    "text": "because if you're running major deep",
    "start": "121680",
    "end": "124140"
  },
  {
    "text": "learning models if you're running large",
    "start": "124140",
    "end": "125820"
  },
  {
    "text": "deep learning models you'll need a lot",
    "start": "125820",
    "end": "127740"
  },
  {
    "text": "of compute power and how can you effect",
    "start": "127740",
    "end": "130200"
  },
  {
    "text": "effectively and efficiently use that",
    "start": "130200",
    "end": "131879"
  },
  {
    "text": "compute power really relies on how well",
    "start": "131879",
    "end": "134640"
  },
  {
    "text": "optimized is your serving infrastructure",
    "start": "134640",
    "end": "136440"
  },
  {
    "text": "and that is what we are going to be",
    "start": "136440",
    "end": "137580"
  },
  {
    "text": "covering in today's session",
    "start": "137580",
    "end": "139379"
  },
  {
    "text": "now of course talking about uh the",
    "start": "139379",
    "end": "141840"
  },
  {
    "text": "primary Hardware that is used to run",
    "start": "141840",
    "end": "143760"
  },
  {
    "text": "your machine learning tasks so we start",
    "start": "143760",
    "end": "145800"
  },
  {
    "text": "with CPUs and then of course if you have",
    "start": "145800",
    "end": "147780"
  },
  {
    "text": "more uh compute intensive tasks then",
    "start": "147780",
    "end": "150060"
  },
  {
    "text": "gpus are a great way uh to be able to uh",
    "start": "150060",
    "end": "153060"
  },
  {
    "text": "which are of course more optimized for",
    "start": "153060",
    "end": "155040"
  },
  {
    "text": "computer intensive tasks especially",
    "start": "155040",
    "end": "157319"
  },
  {
    "text": "let's see if you're learning running a",
    "start": "157319",
    "end": "158760"
  },
  {
    "text": "lot of deep learning uh machine learning",
    "start": "158760",
    "end": "160980"
  },
  {
    "text": "models than gpus and of course tpus",
    "start": "160980",
    "end": "163800"
  },
  {
    "text": "which are tensor processing units are",
    "start": "163800",
    "end": "166379"
  },
  {
    "text": "more optimized for those workloads so as",
    "start": "166379",
    "end": "168599"
  },
  {
    "text": "you work with major models with larger",
    "start": "168599",
    "end": "170340"
  },
  {
    "text": "models you'll primarily use a",
    "start": "170340",
    "end": "171840"
  },
  {
    "text": "combination of either of CPU GPU and",
    "start": "171840",
    "end": "174360"
  },
  {
    "text": "tpus but of course as we understand that",
    "start": "174360",
    "end": "176879"
  },
  {
    "text": "let's say if you were to compare between",
    "start": "176879",
    "end": "178200"
  },
  {
    "text": "running your machine learning workloads",
    "start": "178200",
    "end": "181260"
  },
  {
    "text": "with just two CPUs or with let's say a",
    "start": "181260",
    "end": "183840"
  },
  {
    "text": "CPU and a GPU you may think that since",
    "start": "183840",
    "end": "186959"
  },
  {
    "text": "gpus are more optimized it might be",
    "start": "186959",
    "end": "189060"
  },
  {
    "text": "better to just run your machine learning",
    "start": "189060",
    "end": "190620"
  },
  {
    "text": "workloads with with the CPU and a GPU as",
    "start": "190620",
    "end": "193260"
  },
  {
    "text": "compared to just running it with two",
    "start": "193260",
    "end": "194519"
  },
  {
    "text": "CPUs but there are some challenges when",
    "start": "194519",
    "end": "196560"
  },
  {
    "text": "it comes to choosing the right kind of",
    "start": "196560",
    "end": "198599"
  },
  {
    "text": "uh configuration of your Hardware to run",
    "start": "198599",
    "end": "201180"
  },
  {
    "text": "these machine learning models and that's",
    "start": "201180",
    "end": "203280"
  },
  {
    "text": "where uh I'll just quickly play this",
    "start": "203280",
    "end": "204840"
  },
  {
    "text": "video which is uh by uh recorded by my",
    "start": "204840",
    "end": "207480"
  },
  {
    "text": "co-speaker hopefully the audio comes",
    "start": "207480",
    "end": "209940"
  },
  {
    "text": "through",
    "start": "209940",
    "end": "212060"
  },
  {
    "text": "uh I want to talk a bit about why adding",
    "start": "213540",
    "end": "216840"
  },
  {
    "text": "the GPU is not the same as adding a CPU",
    "start": "216840",
    "end": "219720"
  },
  {
    "text": "and it requires fundamentally changing",
    "start": "219720",
    "end": "222360"
  },
  {
    "text": "changing a lot of your code uh often",
    "start": "222360",
    "end": "225180"
  },
  {
    "text": "this is 100 by Frameworks under the hood",
    "start": "225180",
    "end": "226860"
  },
  {
    "text": "for you uh but sometimes it can't",
    "start": "226860",
    "end": "230040"
  },
  {
    "text": "require still changing a lot of code or",
    "start": "230040",
    "end": "232560"
  },
  {
    "text": "a better scheduling jobs as I like to",
    "start": "232560",
    "end": "236280"
  },
  {
    "text": "say and yeah so uh so that is so that is",
    "start": "236280",
    "end": "240000"
  },
  {
    "text": "what I want to show you by adding one",
    "start": "240000",
    "end": "242099"
  },
  {
    "text": "GPU is not the same as just adding one",
    "start": "242099",
    "end": "244200"
  },
  {
    "text": "CPU in terms of how you run the code on",
    "start": "244200",
    "end": "247260"
  },
  {
    "text": "what kind of strategies would miss so",
    "start": "247260",
    "end": "250019"
  },
  {
    "text": "instead of showing this in some",
    "start": "250019",
    "end": "252239"
  },
  {
    "text": "theoretical way let's go ahead and do an",
    "start": "252239",
    "end": "254220"
  },
  {
    "text": "experiment",
    "start": "254220",
    "end": "255599"
  },
  {
    "text": "so for that experiment uh but at this",
    "start": "255599",
    "end": "258000"
  },
  {
    "text": "moment let's just assume we have no xla",
    "start": "258000",
    "end": "259859"
  },
  {
    "text": "we have no gradient accumulation and if",
    "start": "259859",
    "end": "261840"
  },
  {
    "text": "you don't know what this means um that's",
    "start": "261840",
    "end": "264300"
  },
  {
    "text": "fine we're not using it at the moment at",
    "start": "264300",
    "end": "266280"
  },
  {
    "text": "the end we'll introduce this and also",
    "start": "266280",
    "end": "268320"
  },
  {
    "text": "see like how it can help especially for",
    "start": "268320",
    "end": "270960"
  },
  {
    "text": "gpus right at this moment let's just",
    "start": "270960",
    "end": "272820"
  },
  {
    "text": "focus on what we have at them one GPU",
    "start": "272820",
    "end": "274919"
  },
  {
    "text": "plus one CPU and two CPUs",
    "start": "274919",
    "end": "277080"
  },
  {
    "text": "great so now we want to train a machine",
    "start": "277080",
    "end": "279720"
  },
  {
    "text": "learning model so I'll just take resnet",
    "start": "279720",
    "end": "281940"
  },
  {
    "text": "50 one of the most popular models out",
    "start": "281940",
    "end": "283919"
  },
  {
    "text": "there and what I want to do is I want to",
    "start": "283919",
    "end": "286320"
  },
  {
    "text": "load the data this will be quite",
    "start": "286320",
    "end": "288060"
  },
  {
    "text": "expensive with my experiments are on",
    "start": "288060",
    "end": "289440"
  },
  {
    "text": "imagenet and I want to pre-process the",
    "start": "289440",
    "end": "291840"
  },
  {
    "text": "data uh this word this would if I if I",
    "start": "291840",
    "end": "296400"
  },
  {
    "text": "think about it this would be a",
    "start": "296400",
    "end": "297780"
  },
  {
    "text": "bottleneck on the CPU I'll probably copy",
    "start": "297780",
    "end": "300419"
  },
  {
    "text": "the tensors to GPU so that will be a",
    "start": "300419",
    "end": "303060"
  },
  {
    "text": "button like on the pcie and for training",
    "start": "303060",
    "end": "306000"
  },
  {
    "text": "I'll probably use the GPU in the GPU",
    "start": "306000",
    "end": "307919"
  },
  {
    "text": "setting great so this is our experiment",
    "start": "307919",
    "end": "310680"
  },
  {
    "text": "and let's see what do we run so we run",
    "start": "310680",
    "end": "314040"
  },
  {
    "text": "all these steps open open this just load",
    "start": "314040",
    "end": "316800"
  },
  {
    "text": "the data read the data pre-process and",
    "start": "316800",
    "end": "318240"
  },
  {
    "text": "then play in the data this is what our",
    "start": "318240",
    "end": "320699"
  },
  {
    "text": "current way of running it looks like and",
    "start": "320699",
    "end": "322919"
  },
  {
    "text": "this which makes sense right so you",
    "start": "322919",
    "end": "324419"
  },
  {
    "text": "first open an image you preprocess the",
    "start": "324419",
    "end": "326580"
  },
  {
    "text": "image the neutrin on the image and this",
    "start": "326580",
    "end": "328800"
  },
  {
    "text": "makes a lot of sense uh so this kind of",
    "start": "328800",
    "end": "331320"
  },
  {
    "text": "profiling looks good to me",
    "start": "331320",
    "end": "335419"
  },
  {
    "text": "because each of these steps can can be",
    "start": "335900",
    "end": "338940"
  },
  {
    "text": "individually run with two CPUs and you",
    "start": "338940",
    "end": "341220"
  },
  {
    "text": "get quite some speed up running them",
    "start": "341220",
    "end": "343020"
  },
  {
    "text": "running each of these individuals",
    "start": "343020",
    "end": "344220"
  },
  {
    "text": "episode two CPS so this one is fantastic",
    "start": "344220",
    "end": "346199"
  },
  {
    "text": "once review uh and this also is pretty",
    "start": "346199",
    "end": "348600"
  },
  {
    "text": "easy wait let's try doing the same thing",
    "start": "348600",
    "end": "351479"
  },
  {
    "text": "on a GPU and my first guess is uh the",
    "start": "351479",
    "end": "356100"
  },
  {
    "text": "parts where where my model is loading",
    "start": "356100",
    "end": "357960"
  },
  {
    "text": "the data and reading the data won't work",
    "start": "357960",
    "end": "360120"
  },
  {
    "text": "so well uh having the having a 1 GPU",
    "start": "360120",
    "end": "362940"
  },
  {
    "text": "plus one CPU setup but the training the",
    "start": "362940",
    "end": "365699"
  },
  {
    "text": "model part would probably be faster than",
    "start": "365699",
    "end": "367860"
  },
  {
    "text": "two CPUs with the GPU",
    "start": "367860",
    "end": "370500"
  },
  {
    "text": "great so let's so let's see some numbers",
    "start": "370500",
    "end": "374100"
  },
  {
    "text": "and for the one GPU I have a",
    "start": "374100",
    "end": "377100"
  },
  {
    "text": "um so for the 1GB I have a Tesla T4 and",
    "start": "377100",
    "end": "380220"
  },
  {
    "text": "I'll compare this with two CPUs so just",
    "start": "380220",
    "end": "383340"
  },
  {
    "text": "running it in the naive way for a single",
    "start": "383340",
    "end": "385979"
  },
  {
    "text": "Epoch uh on resonate 50.",
    "start": "385979",
    "end": "389220"
  },
  {
    "text": "I see two CPUs are faster than having",
    "start": "389220",
    "end": "392280"
  },
  {
    "text": "one CPU and one GPU that definitely does",
    "start": "392280",
    "end": "395220"
  },
  {
    "text": "not seem intuitive uh you just added a",
    "start": "395220",
    "end": "397919"
  },
  {
    "text": "GPU which is a lot more costly and",
    "start": "397919",
    "end": "401520"
  },
  {
    "text": "you end up showing down your process",
    "start": "401520",
    "end": "403979"
  },
  {
    "text": "so I just wanted to show you that this",
    "start": "403979",
    "end": "407039"
  },
  {
    "text": "is not the same and if you think a bit",
    "start": "407039",
    "end": "409380"
  },
  {
    "text": "about this then you have a lot of these",
    "start": "409380",
    "end": "411240"
  },
  {
    "text": "this Ideal Time where the GPU is waiting",
    "start": "411240",
    "end": "413400"
  },
  {
    "text": "for the CPU yes your green shift the",
    "start": "413400",
    "end": "415560"
  },
  {
    "text": "ones in the right do get smaller these",
    "start": "415560",
    "end": "417960"
  },
  {
    "text": "get considerably smaller but there is a",
    "start": "417960",
    "end": "420180"
  },
  {
    "text": "lot of time where the GPU is waiting for",
    "start": "420180",
    "end": "422340"
  },
  {
    "text": "the CPU and that point gets considerably",
    "start": "422340",
    "end": "424440"
  },
  {
    "text": "larger now that you only have a single",
    "start": "424440",
    "end": "426360"
  },
  {
    "text": "CPU",
    "start": "426360",
    "end": "427500"
  },
  {
    "text": "all right and of course if you have like",
    "start": "427500",
    "end": "429000"
  },
  {
    "text": "a big enough model that's why the",
    "start": "429000",
    "end": "430860"
  },
  {
    "text": "strategy might work out just way too",
    "start": "430860",
    "end": "432660"
  },
  {
    "text": "well but we'll talk more about uh but",
    "start": "432660",
    "end": "435300"
  },
  {
    "text": "we'll talk more about why this happens",
    "start": "435300",
    "end": "436620"
  },
  {
    "text": "and how you can calculate",
    "start": "436620",
    "end": "438360"
  },
  {
    "text": "yeah so what we kind of covered over",
    "start": "438360",
    "end": "440280"
  },
  {
    "text": "here was that you saw that when we used",
    "start": "440280",
    "end": "442319"
  },
  {
    "text": "like two CPUs the performance was better",
    "start": "442319",
    "end": "444419"
  },
  {
    "text": "and that is primarily because that a lot",
    "start": "444419",
    "end": "446580"
  },
  {
    "text": "of times our GPU was just waiting for",
    "start": "446580",
    "end": "448800"
  },
  {
    "text": "the system resources right so as we",
    "start": "448800",
    "end": "450720"
  },
  {
    "text": "uncover some of the strategies of making",
    "start": "450720",
    "end": "452400"
  },
  {
    "text": "this more effective we'll see that how",
    "start": "452400",
    "end": "454319"
  },
  {
    "text": "can we configure our Hardware in such a",
    "start": "454319",
    "end": "456660"
  },
  {
    "text": "way and provision the hardware at",
    "start": "456660",
    "end": "458400"
  },
  {
    "text": "runtime to make these processes more",
    "start": "458400",
    "end": "460500"
  },
  {
    "text": "efficient now of course uh coming back",
    "start": "460500",
    "end": "463080"
  },
  {
    "text": "to the next point",
    "start": "463080",
    "end": "465000"
  },
  {
    "text": "uh what's more related to today's topic",
    "start": "465000",
    "end": "467880"
  },
  {
    "text": "is how can we make the use of gpus in",
    "start": "467880",
    "end": "470160"
  },
  {
    "text": "kubernetes so this is just a bit of a",
    "start": "470160",
    "end": "471960"
  },
  {
    "text": "primer for those who might not have",
    "start": "471960",
    "end": "473400"
  },
  {
    "text": "probably configured so by default",
    "start": "473400",
    "end": "475080"
  },
  {
    "text": "communities does support stable release",
    "start": "475080",
    "end": "477240"
  },
  {
    "text": "for uh supporting your gpus and these",
    "start": "477240",
    "end": "479699"
  },
  {
    "text": "gpus can be run on your Innovation nodes",
    "start": "479699",
    "end": "481800"
  },
  {
    "text": "and primarily whether it's your Nvidia",
    "start": "481800",
    "end": "484199"
  },
  {
    "text": "based or AMD based gpus they can be run",
    "start": "484199",
    "end": "486660"
  },
  {
    "text": "across multiple nodes inside of your",
    "start": "486660",
    "end": "488580"
  },
  {
    "text": "community cluster and it's very easy to",
    "start": "488580",
    "end": "490680"
  },
  {
    "text": "just set it up you just have this",
    "start": "490680",
    "end": "492479"
  },
  {
    "text": "particular configuration that you can",
    "start": "492479",
    "end": "494220"
  },
  {
    "text": "set up as part of like your device",
    "start": "494220",
    "end": "496020"
  },
  {
    "text": "plugin that allows your pod to access",
    "start": "496020",
    "end": "498419"
  },
  {
    "text": "your specific Hardware sources for",
    "start": "498419",
    "end": "500520"
  },
  {
    "text": "communities so if you were to use like",
    "start": "500520",
    "end": "503099"
  },
  {
    "text": "let's say AKs GK any other cloud",
    "start": "503099",
    "end": "505440"
  },
  {
    "text": "provider or even run this locally so",
    "start": "505440",
    "end": "507120"
  },
  {
    "text": "you'll just have to set up the",
    "start": "507120",
    "end": "508080"
  },
  {
    "text": "particular agent inside of your node to",
    "start": "508080",
    "end": "510180"
  },
  {
    "text": "be able to access your particular",
    "start": "510180",
    "end": "511440"
  },
  {
    "text": "Hardware",
    "start": "511440",
    "end": "512279"
  },
  {
    "text": "and of course as I mentioned that",
    "start": "512279",
    "end": "514560"
  },
  {
    "text": "um when it comes to handling a lot of",
    "start": "514560",
    "end": "516599"
  },
  {
    "text": "these compute intensive tasks the most",
    "start": "516599",
    "end": "518700"
  },
  {
    "text": "important aspect is that how can we",
    "start": "518700",
    "end": "520620"
  },
  {
    "text": "effectively uh utilize these resources",
    "start": "520620",
    "end": "523020"
  },
  {
    "text": "right these gpus and these tpus in a",
    "start": "523020",
    "end": "525720"
  },
  {
    "text": "more effective manner now when you have",
    "start": "525720",
    "end": "528180"
  },
  {
    "text": "a very large workload right a very large",
    "start": "528180",
    "end": "530160"
  },
  {
    "text": "machine learning workload you might not",
    "start": "530160",
    "end": "531779"
  },
  {
    "text": "just be able to have one particular GPU",
    "start": "531779",
    "end": "534300"
  },
  {
    "text": "for your task and you might be required",
    "start": "534300",
    "end": "536459"
  },
  {
    "text": "to use multiple gpus or a combination of",
    "start": "536459",
    "end": "539040"
  },
  {
    "text": "multiple gpus and gpus so there are some",
    "start": "539040",
    "end": "541920"
  },
  {
    "text": "issues inherent issues that come when",
    "start": "541920",
    "end": "544200"
  },
  {
    "text": "you are handling multiple gpus so the",
    "start": "544200",
    "end": "546540"
  },
  {
    "text": "first one is memory right so as you",
    "start": "546540",
    "end": "548580"
  },
  {
    "text": "basically have your entire data and you",
    "start": "548580",
    "end": "551459"
  },
  {
    "text": "split your data across these multiple",
    "start": "551459",
    "end": "553140"
  },
  {
    "text": "gpus so a lot of issues might arise with",
    "start": "553140",
    "end": "556260"
  },
  {
    "text": "respect to an engine the memory and how",
    "start": "556260",
    "end": "558720"
  },
  {
    "text": "the memory is being shared between these",
    "start": "558720",
    "end": "560940"
  },
  {
    "text": "gpus and the tpus as they are processing",
    "start": "560940",
    "end": "563279"
  },
  {
    "text": "your data and similarly like when it",
    "start": "563279",
    "end": "565320"
  },
  {
    "text": "comes to synchronization of these uh",
    "start": "565320",
    "end": "567600"
  },
  {
    "text": "particular gpus and tpus and seeing that",
    "start": "567600",
    "end": "569880"
  },
  {
    "text": "how the model evaluation is being done",
    "start": "569880",
    "end": "572220"
  },
  {
    "text": "across these multiple devices how to",
    "start": "572220",
    "end": "575040"
  },
  {
    "text": "manage them effectively is also a big",
    "start": "575040",
    "end": "577200"
  },
  {
    "text": "issue that we have to face right and",
    "start": "577200",
    "end": "579839"
  },
  {
    "text": "similarly with load balancing that how",
    "start": "579839",
    "end": "581459"
  },
  {
    "text": "are you Distributing your entire data",
    "start": "581459",
    "end": "583500"
  },
  {
    "text": "load across these multiple uh multiple",
    "start": "583500",
    "end": "586380"
  },
  {
    "text": "devices right and similarly for",
    "start": "586380",
    "end": "588060"
  },
  {
    "text": "debugging as well if if you face certain",
    "start": "588060",
    "end": "590399"
  },
  {
    "text": "errors when it comes to uh some of the",
    "start": "590399",
    "end": "592800"
  },
  {
    "text": "models not running properly so how can",
    "start": "592800",
    "end": "594720"
  },
  {
    "text": "you effectively debug which particular",
    "start": "594720",
    "end": "596940"
  },
  {
    "text": "GPU is causing that particular issue so",
    "start": "596940",
    "end": "599220"
  },
  {
    "text": "these are some of the inherent issues",
    "start": "599220",
    "end": "601019"
  },
  {
    "text": "that comes when we are dealing with",
    "start": "601019",
    "end": "602940"
  },
  {
    "text": "multiple gpus and gpus but there is a",
    "start": "602940",
    "end": "605580"
  },
  {
    "text": "way to be able to resolve most of these",
    "start": "605580",
    "end": "608339"
  },
  {
    "text": "issues right and that's where",
    "start": "608339",
    "end": "609839"
  },
  {
    "text": "parallelism really comes in pixels so",
    "start": "609839",
    "end": "612000"
  },
  {
    "text": "the entire concept of parallelism is",
    "start": "612000",
    "end": "613740"
  },
  {
    "text": "that you are taking your either your",
    "start": "613740",
    "end": "616080"
  },
  {
    "text": "data or your Hardware resources and",
    "start": "616080",
    "end": "618540"
  },
  {
    "text": "dividing them into parallel tasks right",
    "start": "618540",
    "end": "620279"
  },
  {
    "text": "very similar to how we have parallel",
    "start": "620279",
    "end": "622019"
  },
  {
    "text": "processing inside of our CPUs we are",
    "start": "622019",
    "end": "624120"
  },
  {
    "text": "bringing in those same principles but",
    "start": "624120",
    "end": "626100"
  },
  {
    "text": "for being able to do our machine",
    "start": "626100",
    "end": "627480"
  },
  {
    "text": "learning evaluation so uh primarily if",
    "start": "627480",
    "end": "629700"
  },
  {
    "text": "you talk about data parallelism we are",
    "start": "629700",
    "end": "631380"
  },
  {
    "text": "essentially uh splitting our data across",
    "start": "631380",
    "end": "633899"
  },
  {
    "text": "into different shards right similar to",
    "start": "633899",
    "end": "635820"
  },
  {
    "text": "how we have database starting and then",
    "start": "635820",
    "end": "637800"
  },
  {
    "text": "we are taking each of these data shots",
    "start": "637800",
    "end": "640140"
  },
  {
    "text": "and running them in uh parallel gpus now",
    "start": "640140",
    "end": "643860"
  },
  {
    "text": "the great thing about data parallelism",
    "start": "643860",
    "end": "645480"
  },
  {
    "text": "is that each and every GPU has the",
    "start": "645480",
    "end": "647459"
  },
  {
    "text": "context for your entire model right so",
    "start": "647459",
    "end": "649920"
  },
  {
    "text": "as the model evaluation gets done uh",
    "start": "649920",
    "end": "652380"
  },
  {
    "text": "each and every GPU has the context for",
    "start": "652380",
    "end": "654060"
  },
  {
    "text": "the entire model and at the end of the",
    "start": "654060",
    "end": "656399"
  },
  {
    "text": "evaluation once the metrics have been",
    "start": "656399",
    "end": "658620"
  },
  {
    "text": "generated you kind of back propagate all",
    "start": "658620",
    "end": "660839"
  },
  {
    "text": "of these metrics into one single source",
    "start": "660839",
    "end": "663060"
  },
  {
    "text": "so that then you can get insights of how",
    "start": "663060",
    "end": "665459"
  },
  {
    "text": "the model training basically went but in",
    "start": "665459",
    "end": "667920"
  },
  {
    "text": "comparison to this we also have the",
    "start": "667920",
    "end": "669480"
  },
  {
    "text": "model parallelism where basically you",
    "start": "669480",
    "end": "671940"
  },
  {
    "text": "don't break your data but you basically",
    "start": "671940",
    "end": "673500"
  },
  {
    "text": "break your entire model into different",
    "start": "673500",
    "end": "675540"
  },
  {
    "text": "fragments and you are running them right",
    "start": "675540",
    "end": "678000"
  },
  {
    "text": "very similar to how the data parallelism",
    "start": "678000",
    "end": "679920"
  },
  {
    "text": "takes place so one of the examples is",
    "start": "679920",
    "end": "681540"
  },
  {
    "text": "tensor parallelism where you're taking",
    "start": "681540",
    "end": "683160"
  },
  {
    "text": "your main model and breaking them down",
    "start": "683160",
    "end": "685620"
  },
  {
    "text": "into individual tensors and then each of",
    "start": "685620",
    "end": "687899"
  },
  {
    "text": "these sensors are being associated with",
    "start": "687899",
    "end": "690180"
  },
  {
    "text": "the gpus and as the processor thing",
    "start": "690180",
    "end": "692640"
  },
  {
    "text": "takes place we again have the context",
    "start": "692640",
    "end": "694620"
  },
  {
    "text": "and we are able to very effectively",
    "start": "694620",
    "end": "696779"
  },
  {
    "text": "manage our resources so the idea is that",
    "start": "696779",
    "end": "698579"
  },
  {
    "text": "together with the data parallelism and",
    "start": "698579",
    "end": "700740"
  },
  {
    "text": "the model parallelism that includes like",
    "start": "700740",
    "end": "702360"
  },
  {
    "text": "tensor parallelism and pipeline",
    "start": "702360",
    "end": "703800"
  },
  {
    "text": "parallelism we are effectively able to",
    "start": "703800",
    "end": "705600"
  },
  {
    "text": "maximize the use of multiple gpus or",
    "start": "705600",
    "end": "708000"
  },
  {
    "text": "tpus in in order to improve the machine",
    "start": "708000",
    "end": "709920"
  },
  {
    "text": "learning performance because we are",
    "start": "709920",
    "end": "711480"
  },
  {
    "text": "effectively breaking down our entire",
    "start": "711480",
    "end": "713240"
  },
  {
    "text": "spectrum of our data and our model",
    "start": "713240",
    "end": "716760"
  },
  {
    "text": "itself and still being able to use",
    "start": "716760",
    "end": "719160"
  },
  {
    "text": "techniques like black back propagation",
    "start": "719160",
    "end": "721260"
  },
  {
    "text": "to be able to fine-tune our results at",
    "start": "721260",
    "end": "724500"
  },
  {
    "text": "once and now what we'll see is that how",
    "start": "724500",
    "end": "726899"
  },
  {
    "text": "can you use these techniques with your",
    "start": "726899",
    "end": "728880"
  },
  {
    "text": "gpus and inside of uh like you know a",
    "start": "728880",
    "end": "731579"
  },
  {
    "text": "cube flow or with flight",
    "start": "731579",
    "end": "733560"
  },
  {
    "text": "so now I'll head back to a pre-recorded",
    "start": "733560",
    "end": "736500"
  },
  {
    "text": "demo from my co-speaker Rashid who kind",
    "start": "736500",
    "end": "739920"
  },
  {
    "text": "of goes further into how do we actually",
    "start": "739920",
    "end": "742980"
  },
  {
    "text": "enable uh data parallelism and pipeline",
    "start": "742980",
    "end": "745920"
  },
  {
    "text": "parallelism inside of our gpus",
    "start": "745920",
    "end": "749899"
  },
  {
    "text": "okay so now that we know about body",
    "start": "751380",
    "end": "753300"
  },
  {
    "text": "religion let's do another experiment and",
    "start": "753300",
    "end": "755760"
  },
  {
    "text": "this time I'll walk you through how how",
    "start": "755760",
    "end": "757740"
  },
  {
    "text": "you would and your models better on gpus",
    "start": "757740",
    "end": "760380"
  },
  {
    "text": "and uh probably run them faster because",
    "start": "760380",
    "end": "762660"
  },
  {
    "text": "the last results where I added a GPU and",
    "start": "762660",
    "end": "764459"
  },
  {
    "text": "the model started running slow and the",
    "start": "764459",
    "end": "765660"
  },
  {
    "text": "overall training process started",
    "start": "765660",
    "end": "767100"
  },
  {
    "text": "happening a lot more slowly so I",
    "start": "767100",
    "end": "769139"
  },
  {
    "text": "probably don't want that and let's see",
    "start": "769139",
    "end": "771300"
  },
  {
    "text": "how we do this so again we come to the",
    "start": "771300",
    "end": "773579"
  },
  {
    "text": "experimentation part I again have a",
    "start": "773579",
    "end": "775440"
  },
  {
    "text": "Tesla T4 I again have two vcpus and the",
    "start": "775440",
    "end": "778620"
  },
  {
    "text": "Tesla T4 only has one CPU uh so so that",
    "start": "778620",
    "end": "781980"
  },
  {
    "text": "is the setup I have",
    "start": "781980",
    "end": "783420"
  },
  {
    "text": "and the way I do this is just for",
    "start": "783420",
    "end": "786000"
  },
  {
    "text": "fairness all of the experiments I was",
    "start": "786000",
    "end": "787620"
  },
  {
    "text": "talking about and the one I do over here",
    "start": "787620",
    "end": "788940"
  },
  {
    "text": "right now are on a single machine so so",
    "start": "788940",
    "end": "791459"
  },
  {
    "text": "I have a single machine with two CPUs",
    "start": "791459",
    "end": "793079"
  },
  {
    "text": "and one GPU and whenever I try it out",
    "start": "793079",
    "end": "795720"
  },
  {
    "text": "for two CPUs part I explicitly make sure",
    "start": "795720",
    "end": "798540"
  },
  {
    "text": "I've not used the GPU one likewise",
    "start": "798540",
    "end": "800519"
  },
  {
    "text": "so I do this with code and that",
    "start": "800519",
    "end": "802560"
  },
  {
    "text": "Illustrated it's better results uh and I",
    "start": "802560",
    "end": "805320"
  },
  {
    "text": "also make sure that none of the",
    "start": "805320",
    "end": "806579"
  },
  {
    "text": "computational graphs are being cards or",
    "start": "806579",
    "end": "808320"
  },
  {
    "text": "anything so so okay uh enough of that",
    "start": "808320",
    "end": "811560"
  },
  {
    "text": "but what we want to do is probably use",
    "start": "811560",
    "end": "814920"
  },
  {
    "text": "some kind of GPU strategy and I did some",
    "start": "814920",
    "end": "818459"
  },
  {
    "text": "GB strategy which worked out way too",
    "start": "818459",
    "end": "820800"
  },
  {
    "text": "well the time was reduced by quite a lot",
    "start": "820800",
    "end": "823680"
  },
  {
    "text": "and other things I want you to notice is",
    "start": "823680",
    "end": "827279"
  },
  {
    "text": "that uh the epoch the overall Epoch time",
    "start": "827279",
    "end": "830459"
  },
  {
    "text": "was not only reduced but the training",
    "start": "830459",
    "end": "832440"
  },
  {
    "text": "part is faster than a GPU",
    "start": "832440",
    "end": "835680"
  },
  {
    "text": "and the secret sauce to making this",
    "start": "835680",
    "end": "837839"
  },
  {
    "text": "faster than GPU are how this happens is",
    "start": "837839",
    "end": "841560"
  },
  {
    "text": "scheduling my process and my code in",
    "start": "841560",
    "end": "844200"
  },
  {
    "text": "such a way that there is very little",
    "start": "844200",
    "end": "846420"
  },
  {
    "text": "time for which the GPU is idle so over",
    "start": "846420",
    "end": "849060"
  },
  {
    "text": "here if I see in any pop there is very",
    "start": "849060",
    "end": "851100"
  },
  {
    "text": "little time for which the GPU is Idle",
    "start": "851100",
    "end": "852959"
  },
  {
    "text": "compared to our previous attempt and our",
    "start": "852959",
    "end": "855720"
  },
  {
    "text": "compared to our previous attempt where",
    "start": "855720",
    "end": "857339"
  },
  {
    "text": "we just load loaded the data read the",
    "start": "857339",
    "end": "860639"
  },
  {
    "text": "data pre-process the data and trained it",
    "start": "860639",
    "end": "862560"
  },
  {
    "text": "sequentially which seemed to work pretty",
    "start": "862560",
    "end": "864600"
  },
  {
    "text": "well on two CPUs",
    "start": "864600",
    "end": "867680"
  },
  {
    "text": "okay",
    "start": "867779",
    "end": "868860"
  },
  {
    "text": "so how do you do this so how do you will",
    "start": "868860",
    "end": "872220"
  },
  {
    "text": "do the training without with having very",
    "start": "872220",
    "end": "874560"
  },
  {
    "text": "less GPU type a very less GPU ideal time",
    "start": "874560",
    "end": "877200"
  },
  {
    "text": "so I will talk about I'll talk about a",
    "start": "877200",
    "end": "880139"
  },
  {
    "text": "few strategies to do that and the first",
    "start": "880139",
    "end": "882000"
  },
  {
    "text": "Secret Sauce to doing this is",
    "start": "882000",
    "end": "884000"
  },
  {
    "text": "prefetching uh prefetching the answers",
    "start": "884000",
    "end": "886860"
  },
  {
    "text": "and memory",
    "start": "886860",
    "end": "888120"
  },
  {
    "text": "so what I probably understand is",
    "start": "888120",
    "end": "892019"
  },
  {
    "text": "the reason the GPU was waiting for the",
    "start": "892019",
    "end": "894180"
  },
  {
    "text": "CPU was because they did not have",
    "start": "894180",
    "end": "896100"
  },
  {
    "text": "preprocessed data ready on which it",
    "start": "896100",
    "end": "898800"
  },
  {
    "text": "could print the model so the way you do",
    "start": "898800",
    "end": "901860"
  },
  {
    "text": "this is",
    "start": "901860",
    "end": "903720"
  },
  {
    "text": "every time uh so every time a model is",
    "start": "903720",
    "end": "907079"
  },
  {
    "text": "running in any block the CPU at that",
    "start": "907079",
    "end": "910680"
  },
  {
    "text": "moment is trying to",
    "start": "910680",
    "end": "913440"
  },
  {
    "text": "is trying to load the data read the data",
    "start": "913440",
    "end": "915720"
  },
  {
    "text": "pre-process the data for the N plus one",
    "start": "915720",
    "end": "917760"
  },
  {
    "text": "at the park and",
    "start": "917760",
    "end": "920519"
  },
  {
    "text": "and this is pretty interesting because",
    "start": "920519",
    "end": "922339"
  },
  {
    "text": "uh when the GPU actually gets to",
    "start": "922339",
    "end": "925260"
  },
  {
    "text": "training the model on the next batch it",
    "start": "925260",
    "end": "927839"
  },
  {
    "text": "already has it already for it and which",
    "start": "927839",
    "end": "930060"
  },
  {
    "text": "is why there are no which is why there",
    "start": "930060",
    "end": "932699"
  },
  {
    "text": "are no hdp wider types",
    "start": "932699",
    "end": "934740"
  },
  {
    "text": "after every pre-process step",
    "start": "934740",
    "end": "936899"
  },
  {
    "text": "or while every read step",
    "start": "936899",
    "end": "939899"
  },
  {
    "text": "another difference I notice is that the",
    "start": "939899",
    "end": "943260"
  },
  {
    "text": "pre-processed steps are the the",
    "start": "943260",
    "end": "946079"
  },
  {
    "text": "preprocess and reach there which do not",
    "start": "946079",
    "end": "948180"
  },
  {
    "text": "happen sequentially and and this is",
    "start": "948180",
    "end": "950940"
  },
  {
    "text": "pretty important to for our GPA strategy",
    "start": "950940",
    "end": "954180"
  },
  {
    "text": "as well because",
    "start": "954180",
    "end": "956040"
  },
  {
    "text": "we want the pre-process we want the",
    "start": "956040",
    "end": "958500"
  },
  {
    "text": "pre-process code to be ready when the",
    "start": "958500",
    "end": "960540"
  },
  {
    "text": "GPU is ideal so and the way we do this",
    "start": "960540",
    "end": "963420"
  },
  {
    "text": "is",
    "start": "963420",
    "end": "964500"
  },
  {
    "text": "pretty simple just use vectorization so",
    "start": "964500",
    "end": "967920"
  },
  {
    "text": "I read so I read for the whole batch",
    "start": "967920",
    "end": "970139"
  },
  {
    "text": "arguments and then I just I and then I",
    "start": "970139",
    "end": "974339"
  },
  {
    "text": "just started preprocessing steps",
    "start": "974339",
    "end": "976440"
  },
  {
    "text": "so so while the first preprocess is",
    "start": "976440",
    "end": "979019"
  },
  {
    "text": "being done so while some preprocesses",
    "start": "979019",
    "end": "981180"
  },
  {
    "text": "being running and preparing for the next",
    "start": "981180",
    "end": "983519"
  },
  {
    "text": "uh for the next set of data the GPU is",
    "start": "983519",
    "end": "986100"
  },
  {
    "text": "running on the previous set of data so",
    "start": "986100",
    "end": "988740"
  },
  {
    "text": "that is how this is facilitated and if",
    "start": "988740",
    "end": "992339"
  },
  {
    "text": "you see the reason why the open and read",
    "start": "992339",
    "end": "994940"
  },
  {
    "text": "uh under the uh open read and preprocess",
    "start": "994940",
    "end": "1000620"
  },
  {
    "text": "some part of it also happens in parallel",
    "start": "1000620",
    "end": "1003500"
  },
  {
    "text": "is because of parallelized data",
    "start": "1003500",
    "end": "1005120"
  },
  {
    "text": "extraction which is another secret sauce",
    "start": "1005120",
    "end": "1007639"
  },
  {
    "text": "so",
    "start": "1007639",
    "end": "1009380"
  },
  {
    "text": "so we took all the secrets of things",
    "start": "1009380",
    "end": "1012820"
  },
  {
    "text": "which allow us to uh which allow us to",
    "start": "1012820",
    "end": "1016279"
  },
  {
    "text": "get to very little GPU Ideal Time as",
    "start": "1016279",
    "end": "1018380"
  },
  {
    "text": "possible and they seems to work pretty",
    "start": "1018380",
    "end": "1020480"
  },
  {
    "text": "well for one GPU and one city",
    "start": "1020480",
    "end": "1022579"
  },
  {
    "text": "and let's see uh and let's see the",
    "start": "1022579",
    "end": "1026240"
  },
  {
    "text": "results for this if we see the results",
    "start": "1026240",
    "end": "1027918"
  },
  {
    "text": "for this I I have two ecpus taking about",
    "start": "1027919",
    "end": "1031280"
  },
  {
    "text": "the same time uh so so this takes about",
    "start": "1031280",
    "end": "1033918"
  },
  {
    "text": "the same time as earlier uh whether it",
    "start": "1033919",
    "end": "1037220"
  },
  {
    "text": "is",
    "start": "1037220",
    "end": "1038058"
  },
  {
    "text": "there is not a lot changed when you are",
    "start": "1038059",
    "end": "1040459"
  },
  {
    "text": "trying to do this on a CPU the train",
    "start": "1040459",
    "end": "1042620"
  },
  {
    "text": "time pretty much Remains the Same and",
    "start": "1042620",
    "end": "1044839"
  },
  {
    "text": "you are just changing uh when each job",
    "start": "1044839",
    "end": "1048380"
  },
  {
    "text": "is happening",
    "start": "1048380",
    "end": "1049460"
  },
  {
    "text": "so this improves the CPU Time by a very",
    "start": "1049460",
    "end": "1052520"
  },
  {
    "text": "small margin but if you see the GPU time",
    "start": "1052520",
    "end": "1055160"
  },
  {
    "text": "this has improved by quite a huge margin",
    "start": "1055160",
    "end": "1058100"
  },
  {
    "text": "and that is because",
    "start": "1058100",
    "end": "1059960"
  },
  {
    "text": "uh and that is because if you see the",
    "start": "1059960",
    "end": "1061880"
  },
  {
    "text": "profiling graph again uh you see that",
    "start": "1061880",
    "end": "1065480"
  },
  {
    "text": "this time around",
    "start": "1065480",
    "end": "1067100"
  },
  {
    "text": "the time it takes for the GPU to train",
    "start": "1067100",
    "end": "1069200"
  },
  {
    "text": "on the data is a new bottleneck so the",
    "start": "1069200",
    "end": "1072380"
  },
  {
    "text": "bottleneck is no longer how much time",
    "start": "1072380",
    "end": "1074600"
  },
  {
    "text": "the CPU take to pre-process the data",
    "start": "1074600",
    "end": "1077539"
  },
  {
    "text": "load the data the bottleneck now uh to",
    "start": "1077539",
    "end": "1080840"
  },
  {
    "text": "make the or to make epox smaller just to",
    "start": "1080840",
    "end": "1083660"
  },
  {
    "text": "keep for the time's sake I'll just",
    "start": "1083660",
    "end": "1085940"
  },
  {
    "text": "probably skip over this part but the",
    "start": "1085940",
    "end": "1087860"
  },
  {
    "text": "idea of what you saw over here is that",
    "start": "1087860",
    "end": "1090020"
  },
  {
    "text": "as soon as we added the data process the",
    "start": "1090020",
    "end": "1093140"
  },
  {
    "text": "data parallelism we saw that uh now the",
    "start": "1093140",
    "end": "1095780"
  },
  {
    "text": "GPU was no longer staying idle so really",
    "start": "1095780",
    "end": "1098179"
  },
  {
    "text": "the trick that we wanted to Showcase",
    "start": "1098179",
    "end": "1100220"
  },
  {
    "text": "over here is the dynamic allocation of",
    "start": "1100220",
    "end": "1102200"
  },
  {
    "text": "gpus at runtime when you're processing",
    "start": "1102200",
    "end": "1104840"
  },
  {
    "text": "your data right so that's the main",
    "start": "1104840",
    "end": "1107059"
  },
  {
    "text": "secret Source when it comes to being",
    "start": "1107059",
    "end": "1108860"
  },
  {
    "text": "able to make more efficient use of these",
    "start": "1108860",
    "end": "1111200"
  },
  {
    "text": "gpus and at the end what uh micro",
    "start": "1111200",
    "end": "1113720"
  },
  {
    "text": "speaker is primarily presenting is that",
    "start": "1113720",
    "end": "1115580"
  },
  {
    "text": "when you have let's say more than one",
    "start": "1115580",
    "end": "1118520"
  },
  {
    "text": "GPU as well then you can use like",
    "start": "1118520",
    "end": "1121220"
  },
  {
    "text": "distributed data parallelism so that it",
    "start": "1121220",
    "end": "1122900"
  },
  {
    "text": "can be spread across these multiple gpus",
    "start": "1122900",
    "end": "1125480"
  },
  {
    "text": "and you can of course then leverage",
    "start": "1125480",
    "end": "1127820"
  },
  {
    "text": "these different types of parallelism",
    "start": "1127820",
    "end": "1129980"
  },
  {
    "text": "techniques in order to do so",
    "start": "1129980",
    "end": "1133360"
  },
  {
    "text": "um so of course as we mentioned right",
    "start": "1135440",
    "end": "1136940"
  },
  {
    "text": "that if you have like let's say multiple",
    "start": "1136940",
    "end": "1138500"
  },
  {
    "text": "nodes as well inside of your communities",
    "start": "1138500",
    "end": "1140120"
  },
  {
    "text": "cluster so this will be primarily",
    "start": "1140120",
    "end": "1141919"
  },
  {
    "text": "focused towards Cube flow that um",
    "start": "1141919",
    "end": "1144140"
  },
  {
    "text": "whether you want to use like let's say a",
    "start": "1144140",
    "end": "1146000"
  },
  {
    "text": "tensorflow based model so you could use",
    "start": "1146000",
    "end": "1148100"
  },
  {
    "text": "like grassmodel.fit or you could",
    "start": "1148100",
    "end": "1150380"
  },
  {
    "text": "basically see all these different types",
    "start": "1150380",
    "end": "1152539"
  },
  {
    "text": "of uh techniques right so you see",
    "start": "1152539",
    "end": "1154880"
  },
  {
    "text": "mirrored uh strategy TPU strategy a",
    "start": "1154880",
    "end": "1157220"
  },
  {
    "text": "central storage strategy so these are",
    "start": "1157220",
    "end": "1158780"
  },
  {
    "text": "all the different strategies that you",
    "start": "1158780",
    "end": "1160100"
  },
  {
    "text": "can actually apply to your Q flow",
    "start": "1160100",
    "end": "1162020"
  },
  {
    "text": "cluster uh with the help of this",
    "start": "1162020",
    "end": "1164360"
  },
  {
    "text": "parallelism technique so that it can",
    "start": "1164360",
    "end": "1166460"
  },
  {
    "text": "make it more efficient for your tube",
    "start": "1166460",
    "end": "1168380"
  },
  {
    "text": "flow to be able to do the multiple",
    "start": "1168380",
    "end": "1170059"
  },
  {
    "text": "parallelism if you are running these",
    "start": "1170059",
    "end": "1171860"
  },
  {
    "text": "multiple GPU clusters and",
    "start": "1171860",
    "end": "1174380"
  },
  {
    "text": "um probably if you can just have the",
    "start": "1174380",
    "end": "1176360"
  },
  {
    "text": "last like five minutes we just have one",
    "start": "1176360",
    "end": "1178400"
  },
  {
    "text": "uh kubeflow demo that we like to just",
    "start": "1178400",
    "end": "1180440"
  },
  {
    "text": "demonstrate quickly so this is a demo",
    "start": "1180440",
    "end": "1184039"
  },
  {
    "text": "so with that let's move on to the demo",
    "start": "1184039",
    "end": "1186559"
  },
  {
    "text": "and uh",
    "start": "1186559",
    "end": "1189399"
  },
  {
    "text": "of using all these strategies you talked",
    "start": "1190600",
    "end": "1192919"
  },
  {
    "text": "about of using multi-gpu multiple gpus",
    "start": "1192919",
    "end": "1195860"
  },
  {
    "text": "spread across multiple nodes for our",
    "start": "1195860",
    "end": "1198620"
  },
  {
    "text": "jobs so I have a lot of moving Parts",
    "start": "1198620",
    "end": "1200960"
  },
  {
    "text": "over here and I'll just start by doing a",
    "start": "1200960",
    "end": "1203360"
  },
  {
    "text": "brief background about what the",
    "start": "1203360",
    "end": "1204740"
  },
  {
    "text": "experiment setup is and so so I have a",
    "start": "1204740",
    "end": "1207620"
  },
  {
    "text": "keep flow cluster and this keep flow",
    "start": "1207620",
    "end": "1210260"
  },
  {
    "text": "cluster also has two gpus these are two",
    "start": "1210260",
    "end": "1213260"
  },
  {
    "text": "SLI 100 gpus and this is again very",
    "start": "1213260",
    "end": "1216919"
  },
  {
    "text": "similar to the setup I use for my own",
    "start": "1216919",
    "end": "1218480"
  },
  {
    "text": "research are what I use uh or what I use",
    "start": "1218480",
    "end": "1221240"
  },
  {
    "text": "for often training on multiple gpus so",
    "start": "1221240",
    "end": "1224480"
  },
  {
    "text": "okay but let's start by seeing by seeing",
    "start": "1224480",
    "end": "1228620"
  },
  {
    "text": "the config file I have so since this is",
    "start": "1228620",
    "end": "1230780"
  },
  {
    "text": "queue flow I have a config file to do to",
    "start": "1230780",
    "end": "1234380"
  },
  {
    "text": "do multiple benchmarks and uh so this is",
    "start": "1234380",
    "end": "1238220"
  },
  {
    "text": "structured as a tensorflow job and a lot",
    "start": "1238220",
    "end": "1240860"
  },
  {
    "text": "of the benchmarking code so a lot of the",
    "start": "1240860",
    "end": "1244220"
  },
  {
    "text": "original benchmarking code of running",
    "start": "1244220",
    "end": "1245720"
  },
  {
    "text": "taking a model and running it is",
    "start": "1245720",
    "end": "1247700"
  },
  {
    "text": "actually borrowed from tensorflow",
    "start": "1247700",
    "end": "1249260"
  },
  {
    "text": "benchmarks the official tensorflow",
    "start": "1249260",
    "end": "1250520"
  },
  {
    "text": "benchmarks repository so we'll use much",
    "start": "1250520",
    "end": "1252860"
  },
  {
    "text": "of that I've slightly modified the code",
    "start": "1252860",
    "end": "1254780"
  },
  {
    "text": "to parse out and rectify the logs so so",
    "start": "1254780",
    "end": "1258080"
  },
  {
    "text": "I can show that share it to you as",
    "start": "1258080",
    "end": "1259580"
  },
  {
    "text": "proper benchmarks but so this is what",
    "start": "1259580",
    "end": "1262400"
  },
  {
    "text": "we'll be doing at the moment let's go",
    "start": "1262400",
    "end": "1264380"
  },
  {
    "text": "ahead and run two jobs uh two jobs will",
    "start": "1264380",
    "end": "1267080"
  },
  {
    "text": "also take quite some while which uh to",
    "start": "1267080",
    "end": "1269299"
  },
  {
    "text": "run so but but also two jobs so",
    "start": "1269299",
    "end": "1272900"
  },
  {
    "text": "I will train a resonate 50 model on the",
    "start": "1272900",
    "end": "1275480"
  },
  {
    "text": "imagenet data set once with the",
    "start": "1275480",
    "end": "1277340"
  },
  {
    "text": "parameter server strategy and uh so once",
    "start": "1277340",
    "end": "1281000"
  },
  {
    "text": "I do it with the parameter service",
    "start": "1281000",
    "end": "1282140"
  },
  {
    "text": "strategy which is a type of data",
    "start": "1282140",
    "end": "1283880"
  },
  {
    "text": "distributed uh data parallelism I'll do",
    "start": "1283880",
    "end": "1286460"
  },
  {
    "text": "it once with the data parallelism and",
    "start": "1286460",
    "end": "1288140"
  },
  {
    "text": "then I'll use the replicated strategy",
    "start": "1288140",
    "end": "1289700"
  },
  {
    "text": "where you have all the printers in one",
    "start": "1289700",
    "end": "1292940"
  },
  {
    "text": "central place and then you just copy",
    "start": "1292940",
    "end": "1294679"
  },
  {
    "text": "them to gpus as an area so these are the",
    "start": "1294679",
    "end": "1296960"
  },
  {
    "text": "strategies I'll be trying out and as I",
    "start": "1296960",
    "end": "1299419"
  },
  {
    "text": "was talking this takes quite some while",
    "start": "1299419",
    "end": "1300919"
  },
  {
    "text": "but since it's a tensorflow job you can",
    "start": "1300919",
    "end": "1303260"
  },
  {
    "text": "directly run it with kubernetes just as",
    "start": "1303260",
    "end": "1304880"
  },
  {
    "text": "yaml and and of course the code to",
    "start": "1304880",
    "end": "1308299"
  },
  {
    "text": "create the CNN architecture so",
    "start": "1308299",
    "end": "1311600"
  },
  {
    "text": "all of that was present in EFC and in",
    "start": "1311600",
    "end": "1314299"
  },
  {
    "text": "Benchmark so I already have that and",
    "start": "1314299",
    "end": "1316220"
  },
  {
    "text": "this takes quite some while so I've",
    "start": "1316220",
    "end": "1318620"
  },
  {
    "text": "already run this and keep flow and",
    "start": "1318620",
    "end": "1320780"
  },
  {
    "text": "particularly what I wanted to show you",
    "start": "1320780",
    "end": "1322340"
  },
  {
    "text": "was the results and uh so so actually",
    "start": "1322340",
    "end": "1325640"
  },
  {
    "text": "let's see the results first and",
    "start": "1325640",
    "end": "1330380"
  },
  {
    "text": "uh since it's a tensorflow job you could",
    "start": "1330380",
    "end": "1332600"
  },
  {
    "text": "most certainly just go ahead and run",
    "start": "1332600",
    "end": "1334460"
  },
  {
    "text": "this and keep flow great so these are",
    "start": "1334460",
    "end": "1337280"
  },
  {
    "text": "the past results and as I was saying",
    "start": "1337280",
    "end": "1339740"
  },
  {
    "text": "this takes quite some time because this",
    "start": "1339740",
    "end": "1340940"
  },
  {
    "text": "loads the imagenet data set which is",
    "start": "1340940",
    "end": "1342860"
  },
  {
    "text": "quite a few million images uh it has to",
    "start": "1342860",
    "end": "1345080"
  },
  {
    "text": "run this model on",
    "start": "1345080",
    "end": "1346940"
  },
  {
    "text": "so we take the image data set use some",
    "start": "1346940",
    "end": "1350539"
  },
  {
    "text": "augmentations on it and then what I then",
    "start": "1350539",
    "end": "1354020"
  },
  {
    "text": "what I want to see is especially this",
    "start": "1354020",
    "end": "1356059"
  },
  {
    "text": "metric total images per second so think",
    "start": "1356059",
    "end": "1359179"
  },
  {
    "text": "of a think of this metric as",
    "start": "1359179",
    "end": "1361580"
  },
  {
    "text": "just how many images the GPU can train",
    "start": "1361580",
    "end": "1364460"
  },
  {
    "text": "on per second and the model itself is",
    "start": "1364460",
    "end": "1366860"
  },
  {
    "text": "pretty simple just the resonate for free",
    "start": "1366860",
    "end": "1368539"
  },
  {
    "text": "but I just wanted to show this Benchmark",
    "start": "1368539",
    "end": "1370520"
  },
  {
    "text": "and this is again same to the benchmarks",
    "start": "1370520",
    "end": "1373340"
  },
  {
    "text": "I was working on earlier in our",
    "start": "1373340",
    "end": "1374480"
  },
  {
    "text": "experimental setup earlier and uh I had",
    "start": "1374480",
    "end": "1377360"
  },
  {
    "text": "actually provided those benchmarks in",
    "start": "1377360",
    "end": "1378740"
  },
  {
    "text": "this environment making few changes here",
    "start": "1378740",
    "end": "1380360"
  },
  {
    "text": "and there",
    "start": "1380360",
    "end": "1381860"
  },
  {
    "text": "so so that was for the parameter server",
    "start": "1381860",
    "end": "1385280"
  },
  {
    "text": "and then we see the replicated one so",
    "start": "1385280",
    "end": "1387679"
  },
  {
    "text": "since this is a small model this sense",
    "start": "1387679",
    "end": "1389240"
  },
  {
    "text": "is a particularly small model uh I see",
    "start": "1389240",
    "end": "1391400"
  },
  {
    "text": "that changing and also because this two",
    "start": "1391400",
    "end": "1394400"
  },
  {
    "text": "gpus so changing whether the tensors",
    "start": "1394400",
    "end": "1397580"
  },
  {
    "text": "live in a central place and are then",
    "start": "1397580",
    "end": "1399020"
  },
  {
    "text": "copy two gpus uh would not would not",
    "start": "1399020",
    "end": "1402620"
  },
  {
    "text": "have a big effect on the pcie bus and",
    "start": "1402620",
    "end": "1405260"
  },
  {
    "text": "thus as a as we were expecting it it",
    "start": "1405260",
    "end": "1408020"
  },
  {
    "text": "took about the same time for both these",
    "start": "1408020",
    "end": "1410240"
  },
  {
    "text": "strategies uh with but that's a quick",
    "start": "1410240",
    "end": "1413120"
  },
  {
    "text": "overview of how you can of how you can",
    "start": "1413120",
    "end": "1416659"
  },
  {
    "text": "so without uh",
    "start": "1416659",
    "end": "1418640"
  },
  {
    "text": "so I'll just to kind of summarize what",
    "start": "1418640",
    "end": "1420919"
  },
  {
    "text": "happened was that we showcased two",
    "start": "1420919",
    "end": "1422480"
  },
  {
    "text": "different strategies of being able to",
    "start": "1422480",
    "end": "1424340"
  },
  {
    "text": "use uh two gpus with kubeflow and we ran",
    "start": "1424340",
    "end": "1427700"
  },
  {
    "text": "these two strategies on a image net data",
    "start": "1427700",
    "end": "1430159"
  },
  {
    "text": "set and uh you can see image and model",
    "start": "1430159",
    "end": "1431900"
  },
  {
    "text": "and you could see the results right um",
    "start": "1431900",
    "end": "1433700"
  },
  {
    "text": "so another way that you can actually",
    "start": "1433700",
    "end": "1435500"
  },
  {
    "text": "configure your uh gpus is with flight so",
    "start": "1435500",
    "end": "1438320"
  },
  {
    "text": "flight is basically a community's native",
    "start": "1438320",
    "end": "1440299"
  },
  {
    "text": "workflow automation system for both data",
    "start": "1440299",
    "end": "1442880"
  },
  {
    "text": "and for machine learning processes and",
    "start": "1442880",
    "end": "1444860"
  },
  {
    "text": "uh you can very easily configure your",
    "start": "1444860",
    "end": "1447260"
  },
  {
    "text": "gpus for being able to run your machine",
    "start": "1447260",
    "end": "1450679"
  },
  {
    "text": "learning workloads as these tasks that",
    "start": "1450679",
    "end": "1453320"
  },
  {
    "text": "are embedded inside of these workflows",
    "start": "1453320",
    "end": "1454940"
  },
  {
    "text": "and you can very easily just set up by",
    "start": "1454940",
    "end": "1457700"
  },
  {
    "text": "using plugins where you're setting up",
    "start": "1457700",
    "end": "1459980"
  },
  {
    "text": "these uh these gpus and you're giving",
    "start": "1459980",
    "end": "1463039"
  },
  {
    "text": "them access to the pods which are",
    "start": "1463039",
    "end": "1464659"
  },
  {
    "text": "running these flight workloads so",
    "start": "1464659",
    "end": "1467120"
  },
  {
    "text": "whether you are using kubeflow or using",
    "start": "1467120",
    "end": "1469340"
  },
  {
    "text": "flight uh there is support for gpus for",
    "start": "1469340",
    "end": "1471980"
  },
  {
    "text": "both and you can leverage these",
    "start": "1471980",
    "end": "1474200"
  },
  {
    "text": "strategies that we talked spoke about",
    "start": "1474200",
    "end": "1476299"
  },
  {
    "text": "like you know for your data parallelism",
    "start": "1476299",
    "end": "1479900"
  },
  {
    "text": "and your tensor parallelism in case of",
    "start": "1479900",
    "end": "1482299"
  },
  {
    "text": "both kubeflow and also for flight with",
    "start": "1482299",
    "end": "1484940"
  },
  {
    "text": "that uh we'll be open to questions now",
    "start": "1484940",
    "end": "1486919"
  },
  {
    "text": "thank you so much and yeah uh I hope",
    "start": "1486919",
    "end": "1489320"
  },
  {
    "text": "that you enjoyed the presentation",
    "start": "1489320",
    "end": "1492399"
  },
  {
    "text": "[Applause]",
    "start": "1492830",
    "end": "1495419"
  }
]