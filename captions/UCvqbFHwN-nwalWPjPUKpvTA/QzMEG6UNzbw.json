[
  {
    "text": "hi all thanks for joining our first session for today is",
    "start": "359",
    "end": "5879"
  },
  {
    "text": "optimizing application performance on kubernetes by then guntala thanks and a brief introduction about",
    "start": "5879",
    "end": "13320"
  },
  {
    "text": "then occur dinakar is an architect of the cruise project Danica is focused on autonomous",
    "start": "13320",
    "end": "19380"
  },
  {
    "text": "performance tuning and exploring the usage of machine learning and Hyper parameter optimization in the",
    "start": "19380",
    "end": "25140"
  },
  {
    "text": "performance domain specific to kubernetes and Cloud previously worked on making the open j9",
    "start": "25140",
    "end": "31619"
  },
  {
    "text": "Java virtual machine run more efficiently in the cloud and was the official maintenance of the adopt open",
    "start": "31619",
    "end": "37020"
  },
  {
    "text": "jdk Docker images open source astronomy and volleyball and",
    "start": "37020",
    "end": "42059"
  },
  {
    "text": "it's a problem prolific speaker at conferences so a brief introduction about the topic that we're going to talk today so now",
    "start": "42059",
    "end": "48780"
  },
  {
    "text": "that you have applications running on kubernetes wondering how to get the response time that you need tuning applications to get the performance that",
    "start": "48780",
    "end": "55079"
  },
  {
    "text": "you need in kubernetes and can be challenging at the same time there are a number of kubernetes features that we",
    "start": "55079",
    "end": "60239"
  },
  {
    "text": "used in the right way can go a long way to get the most of underlying Hardware resources",
    "start": "60239",
    "end": "65700"
  },
  {
    "text": "this talk looks into each and every aspect of optimizing a kubernetes cluster starting from the most basic",
    "start": "65700",
    "end": "70979"
  },
  {
    "text": "node affinities to Advanced methods such as tuning microservices each with examples and a demo will also be",
    "start": "70979",
    "end": "77580"
  },
  {
    "text": "specifically looking at tools that help to not only right size your containers but also",
    "start": "77580",
    "end": "83220"
  },
  {
    "text": "optimize the runtimes so we'll start this session",
    "start": "83220",
    "end": "91040"
  },
  {
    "text": "and thank you for attending this session uh this is I work at Red Hat where my",
    "start": "94560",
    "end": "101460"
  },
  {
    "text": "primary job is to see how runtimes such as Java can be made to run better in kubernetes and that is what I'll be",
    "start": "101460",
    "end": "108180"
  },
  {
    "text": "talking about today um has it ever happened to you when you talk technology to customers only for",
    "start": "108180",
    "end": "115619"
  },
  {
    "text": "them to ask what's the mileage so today we will look at ways to improve the",
    "start": "115619",
    "end": "120899"
  },
  {
    "text": "mileage that you can get with your kubernetes cluster uh so let me take a moment to Define",
    "start": "120899",
    "end": "127500"
  },
  {
    "text": "what I mean by performance uh traditionally performance looks at three key aspects throughput response time and",
    "start": "127500",
    "end": "134700"
  },
  {
    "text": "utilization of system resources these are the criteria that we'll be looking to optimize in today's presentation as",
    "start": "134700",
    "end": "141360"
  },
  {
    "text": "well however I'll be confining myself only to compute so before we dive into the presentation",
    "start": "141360",
    "end": "148500"
  },
  {
    "text": "I thought it would be good to define the overall context here imagine that you are an SRE and you've been given this",
    "start": "148500",
    "end": "155160"
  },
  {
    "text": "problem to be solved you have a complex polyglot application such as an airline",
    "start": "155160",
    "end": "160800"
  },
  {
    "text": "booking system that is deployed onto kubernetes as you can see it has many microservices",
    "start": "160800",
    "end": "167900"
  },
  {
    "text": "a couple of databases and each of the micro Services written in a different",
    "start": "167900",
    "end": "173040"
  },
  {
    "text": "language and framework the user is having a slow response time",
    "start": "173040",
    "end": "178800"
  },
  {
    "text": "while doing a flight booking now it is up to the SRE or the it admin to try and",
    "start": "178800",
    "end": "185220"
  },
  {
    "text": "make the user experience better so let us look at in detail what are the steps",
    "start": "185220",
    "end": "190379"
  },
  {
    "text": "that an SRE can take to try to solve this problem",
    "start": "190379",
    "end": "196459"
  },
  {
    "text": "so the first aspect to be considered is observability this is something that is",
    "start": "196739",
    "end": "202019"
  },
  {
    "text": "very key how closely we observe this system and all of the metrics Associated",
    "start": "202019",
    "end": "207299"
  },
  {
    "text": "will actually help us to determine where the performance bottlenecks are and how to go about tuning them",
    "start": "207299",
    "end": "214200"
  },
  {
    "text": "there are a number of tools out there that can help you to get better metrics Prometheus and grafana are for example a",
    "start": "214200",
    "end": "221220"
  },
  {
    "text": "couple of the more popular ones I would also suggest that you can take a look at",
    "start": "221220",
    "end": "226440"
  },
  {
    "text": "open Telemetry which is you know slowly becoming industry standard when it comes",
    "start": "226440",
    "end": "232739"
  },
  {
    "text": "to observability one of the key things in observability is the granularity of observation for",
    "start": "232739",
    "end": "238860"
  },
  {
    "text": "example if you're observing the pods on a per second basis then you get very accurate information but that causes a",
    "start": "238860",
    "end": "245400"
  },
  {
    "text": "higher overhead both in terms of CPU network activity and in fact disk space as well so there's a trade-off here and",
    "start": "245400",
    "end": "252720"
  },
  {
    "text": "you need to be very careful in setting that value another aspect to consider would be to",
    "start": "252720",
    "end": "258479"
  },
  {
    "text": "export additional operational metrics on a per application basis things like the",
    "start": "258479",
    "end": "264300"
  },
  {
    "text": "spring actuator or the micrometer for quercus prom client for node.js can be",
    "start": "264300",
    "end": "271800"
  },
  {
    "text": "turned on for your application and they provide additional runtime related metrics such as the Heap which we can",
    "start": "271800",
    "end": "278580"
  },
  {
    "text": "see later you know can be used to tune the application for better performance",
    "start": "278580",
    "end": "284280"
  },
  {
    "text": "uh when you have an on-prem Cloud uh you have the luxury of tuning the hardware all the way from the BIOS in each of",
    "start": "284280",
    "end": "290759"
  },
  {
    "text": "your kubernetes nodes a common setting found in BIOS relates to the choice of performance of power choosing power",
    "start": "290759",
    "end": "298080"
  },
  {
    "text": "means you get better power savings but variable performance the same setting",
    "start": "298080",
    "end": "303240"
  },
  {
    "text": "bubbles up into the operating system or the hypervisor as well in the case of Linux it's called as the scaling",
    "start": "303240",
    "end": "309840"
  },
  {
    "text": "governor in fact I've seen performance drop by up to 30 percent with the power save option",
    "start": "309840",
    "end": "315840"
  },
  {
    "text": "for certain workloads if power saving is your goal then you know this is a good",
    "start": "315840",
    "end": "322979"
  },
  {
    "text": "setting but definitely not if performance of the application is the key goal",
    "start": "322979",
    "end": "328919"
  },
  {
    "text": "um the other thing to consider or at least be aware of is to look at hyper",
    "start": "328919",
    "end": "335639"
  },
  {
    "text": "threading or not considered hyper threading while doing capacity planning let us say a server has 16 cores and two",
    "start": "335639",
    "end": "343919"
  },
  {
    "text": "threads per core that is counted as 32 CPUs however hyper",
    "start": "343919",
    "end": "349259"
  },
  {
    "text": "threaded CPUs give at most a 20 boost over a single core and so it is best to",
    "start": "349259",
    "end": "356520"
  },
  {
    "text": "ignore this while calculating capacity now that our hypothetical SRE has set up",
    "start": "356520",
    "end": "363720"
  },
  {
    "text": "observability and has fixed the hardware what's the next step let's start simple",
    "start": "363720",
    "end": "369419"
  },
  {
    "text": "match the application to the hardware features that is needed by the application",
    "start": "369419",
    "end": "375600"
  },
  {
    "text": "so node Affinity is typically accomplished by setting the right labels",
    "start": "375600",
    "end": "381720"
  },
  {
    "text": "to a node in a kubernetes cluster it is very useful if you want to assign pods",
    "start": "381720",
    "end": "387120"
  },
  {
    "text": "to a specific Hardware feature on the Node or maybe the node is",
    "start": "387120",
    "end": "392220"
  },
  {
    "text": "reserved for a particular type of workload or namespace or a security",
    "start": "392220",
    "end": "397319"
  },
  {
    "text": "constraint in this example we see that this particular pod which is a ml",
    "start": "397319",
    "end": "403319"
  },
  {
    "text": "application will only run on nodes that have the GPU label",
    "start": "403319",
    "end": "409080"
  },
  {
    "text": "another way to constrain Parts is to use pod affinity and part anti-affinity if",
    "start": "409080",
    "end": "415680"
  },
  {
    "text": "there are parts that commonly communicate together or maybe they share some common resources then it makes",
    "start": "415680",
    "end": "421199"
  },
  {
    "text": "sense for them to run on the same node we can use port Affinity rules to make sure that they all run on the same node",
    "start": "421199",
    "end": "427979"
  },
  {
    "text": "but what if you don't want pods from one application a to run on a node if you",
    "start": "427979",
    "end": "433500"
  },
  {
    "text": "have pods from application B running on that node maybe both applications are network heavy or both use GPU",
    "start": "433500",
    "end": "440580"
  },
  {
    "text": "extensively whatever may be the case we want to make sure that pods from application and application be run on",
    "start": "440580",
    "end": "447180"
  },
  {
    "text": "different nodes so in that case we can use pod anti-daffinity to make sure that",
    "start": "447180",
    "end": "452759"
  },
  {
    "text": "they both don't run on the same node in this example we want a pod to be",
    "start": "452759",
    "end": "458699"
  },
  {
    "text": "scheduled on a node only if there are other pods that have the same security policy S1",
    "start": "458699",
    "end": "466440"
  },
  {
    "text": "and don't want to schedule it on a node that is running pods that use a different security policy S2",
    "start": "466440",
    "end": "473520"
  },
  {
    "text": "the advantage of pod affinity and anti-affinity is that it allows the admin to dynamically assign nodes for",
    "start": "473520",
    "end": "481500"
  },
  {
    "text": "certain kind of PODS without having to dedicate nodes ahead of time",
    "start": "481500",
    "end": "488300"
  },
  {
    "text": "um there are also other scheduler mechanisms such as chains and tolerations uh pod priority that you can",
    "start": "488340",
    "end": "496139"
  },
  {
    "text": "explore as well in this particular context now we come to the most important aspect",
    "start": "496139",
    "end": "502259"
  },
  {
    "text": "of performance tuning in a kubernetes cluster right sizing application greatly",
    "start": "502259",
    "end": "507780"
  },
  {
    "text": "helps to get the best possible performance so this is done primarily by setting the CPU and memory requests and",
    "start": "507780",
    "end": "514440"
  },
  {
    "text": "limits so here on the left I have an example application deployment yaml and you can",
    "start": "514440",
    "end": "522060"
  },
  {
    "text": "see the resources specified in the container spec section it is very important as a best practice",
    "start": "522060",
    "end": "528240"
  },
  {
    "text": "to always specify the resources to enable kubernetes to make the best",
    "start": "528240",
    "end": "533279"
  },
  {
    "text": "possible scheduling decisions this usually means that you have to either set the guaranteed or the bursible qos",
    "start": "533279",
    "end": "540240"
  },
  {
    "text": "class and avoid the best effort which means that you're you know in best of what you're not setting anything at all",
    "start": "540240",
    "end": "548420"
  },
  {
    "text": "um to I mean one thing that we do need to make sure is that for the best possible",
    "start": "549660",
    "end": "556380"
  },
  {
    "text": "performance we need to set the requests to cover um the consistent Peaks",
    "start": "556380",
    "end": "563480"
  },
  {
    "text": "that we observe and the limits should be set to handle any spikes",
    "start": "563480",
    "end": "568860"
  },
  {
    "text": "so do ensure that the limits are high are set high enough during observation itself to prevent any throttling also do",
    "start": "568860",
    "end": "577320"
  },
  {
    "text": "ensure that requests and limits that you're setting do not clash with any limit ranges that might apply uh to your",
    "start": "577320",
    "end": "586019"
  },
  {
    "text": "namespace now that we know that request and limits",
    "start": "586019",
    "end": "591899"
  },
  {
    "text": "are crucial to your performance you might have a question how do I arrive at the optimal values for request and",
    "start": "591899",
    "end": "598080"
  },
  {
    "text": "limits accurately ah the vertical part Auto scalar can help in that regard but",
    "start": "598080",
    "end": "603959"
  },
  {
    "text": "I suggest you use the cruise tool which I will talk about in a minute so let's",
    "start": "603959",
    "end": "609240"
  },
  {
    "text": "take a closer look at the various Auto scalers that kubernetes has",
    "start": "609240",
    "end": "615000"
  },
  {
    "text": "so application performance depends a lot on how the app is scaled and this is",
    "start": "615000",
    "end": "620820"
  },
  {
    "text": "where setting the right policies for the horizontal pod Auto scalar is very",
    "start": "620820",
    "end": "626459"
  },
  {
    "text": "important so try to use app specific metrics to set up the HPA as much as possible as",
    "start": "626459",
    "end": "634079"
  },
  {
    "text": "using just like just the average CPU utilization for example might not be the",
    "start": "634079",
    "end": "641220"
  },
  {
    "text": "best approach um so for example when a GC is triggered in Java so this might actually cause a",
    "start": "641220",
    "end": "648720"
  },
  {
    "text": "new uh pot to be instantiated instead of when actual load",
    "start": "648720",
    "end": "654600"
  },
  {
    "text": "is increased uh you can also use external metrics such as the number of concurrent users",
    "start": "654600",
    "end": "661920"
  },
  {
    "text": "that your application is handling but the best practice is to use objects that",
    "start": "661920",
    "end": "667079"
  },
  {
    "text": "are known to kubernetes as much as possible such as you know the packets per second or requests per second so in",
    "start": "667079",
    "end": "673440"
  },
  {
    "text": "this particular case uh what we are saying is that if the packets per second the average value of the packets per",
    "start": "673440",
    "end": "679740"
  },
  {
    "text": "second goes beyond 1K then start a new uh part or in this particular case a",
    "start": "679740",
    "end": "685620"
  },
  {
    "text": "request per second goes beyond 10K then we start another part",
    "start": "685620",
    "end": "690660"
  },
  {
    "text": "and so on using a cluster autoscaler you know",
    "start": "690660",
    "end": "695940"
  },
  {
    "text": "definitely helps to make the best utilization of the underlying resources especially when you're scaling down you",
    "start": "695940",
    "end": "702779"
  },
  {
    "text": "make sure that you free up the resources that are not being used but you need to be very careful not to cause any service",
    "start": "702779",
    "end": "709079"
  },
  {
    "text": "disruption in the process especially you know when you're down scaling specifying the max unavailable pods in the Pod",
    "start": "709079",
    "end": "717060"
  },
  {
    "text": "disruption budget definitely helps in this particular regard",
    "start": "717060",
    "end": "722820"
  },
  {
    "text": "so now if you're an SRE you'll know that every runtime has many many tunables so",
    "start": "722820",
    "end": "728399"
  },
  {
    "text": "Java for example has more than 100 of them but you will also know that you should never touch them why because you",
    "start": "728399",
    "end": "736019"
  },
  {
    "text": "know that uh you know who knows what kind of an impact it has and and you",
    "start": "736019",
    "end": "741540"
  },
  {
    "text": "know it has all these dependencies on other tunables and they're they're just way too many of them for you to manually",
    "start": "741540",
    "end": "747540"
  },
  {
    "text": "test and figure out uh also how these runtimes behave in kubernetes environments is not always clear so",
    "start": "747540",
    "end": "755940"
  },
  {
    "text": "guess what most of the time an SRE is just limited to tuning the app itself or",
    "start": "755940",
    "end": "761100"
  },
  {
    "text": "tuning just the CPU and memory by tuning uh you know we all know what normally happens we just end up doubling the",
    "start": "761100",
    "end": "768480"
  },
  {
    "text": "resources until the problem goes away so it is difficult to be an SRE let's be",
    "start": "768480",
    "end": "775500"
  },
  {
    "text": "honest you have the users bugging you for better response times the finance",
    "start": "775500",
    "end": "780959"
  },
  {
    "text": "wants to cut costs all the time and you have developers giving you a ton of options uh that you find it really",
    "start": "780959",
    "end": "788220"
  },
  {
    "text": "difficult to use so if you're thinking there's got to be a smarter way you're absolutely right",
    "start": "788220",
    "end": "795180"
  },
  {
    "text": "um so we're really happy to announce that we are having you know we have this",
    "start": "795180",
    "end": "801540"
  },
  {
    "text": "new tool called Cruise autotune it's available publicly it's an open source",
    "start": "801540",
    "end": "807540"
  },
  {
    "text": "project from we are from Red Hat I do encourage you to take a look at our GitHub repo given below here",
    "start": "807540",
    "end": "815760"
  },
  {
    "text": "um so let's take a deep dive into the whole process that autotune uses",
    "start": "815760",
    "end": "822060"
  },
  {
    "text": "um to tune an application so the first step here is that the SRE encapsulates",
    "start": "822060",
    "end": "827820"
  },
  {
    "text": "all of the performance requirements into an objective function uh which is an",
    "start": "827820",
    "end": "833519"
  },
  {
    "text": "algebraic expression such as you know a square divided by B plus C where maybe a",
    "start": "833519",
    "end": "839579"
  },
  {
    "text": "can be your throughput a b can be a response time C can be costs and",
    "start": "839579",
    "end": "846959"
  },
  {
    "text": "um you want to either you know maximize or minimize the whole thing in this particular case for example if it is a",
    "start": "846959",
    "end": "853139"
  },
  {
    "text": "square divided by B plus C you might want to maximize it uh and here each of the individual",
    "start": "853139",
    "end": "860100"
  },
  {
    "text": "variables of the objective function are specified as Prometheus queries and the",
    "start": "860100",
    "end": "867839"
  },
  {
    "text": "whole thing is applicable to a particular kubernetes deployment uh which can be selected using the selector",
    "start": "867839",
    "end": "874199"
  },
  {
    "text": "out here so at the heart of the autotune is the Bayesian optimization which is provided",
    "start": "874199",
    "end": "880320"
  },
  {
    "text": "by the HBO service that you see here HP is nothing but the hyper parameter optimization service",
    "start": "880320",
    "end": "887639"
  },
  {
    "text": "um so Bayesian optimization is a type of black box optimization that uses probabilistic models",
    "start": "887639",
    "end": "894240"
  },
  {
    "text": "um of the objectives function uh that that you have specified here and that is",
    "start": "894240",
    "end": "899639"
  },
  {
    "text": "searched efficiently to arrive at either the global maximum or the minimum as",
    "start": "899639",
    "end": "905399"
  },
  {
    "text": "required so essentially What's Happening Here is that um you know the Basin optimization gives",
    "start": "905399",
    "end": "913019"
  },
  {
    "text": "you a configuration for you to try out for this particular uh you know the",
    "start": "913019",
    "end": "918480"
  },
  {
    "text": "deployment so it so we have figured out what are the layers of the application what are the layers of the stack and and",
    "start": "918480",
    "end": "926699"
  },
  {
    "text": "then send all of the tunables from those layers to the base in optimization which",
    "start": "926699",
    "end": "931980"
  },
  {
    "text": "gives you a particular config value to try out the experiment manager here deploys it and then we get a response I",
    "start": "931980",
    "end": "939899"
  },
  {
    "text": "mean we monitor the a board with the trial configuration",
    "start": "939899",
    "end": "945180"
  },
  {
    "text": "under load and then we get a summary of how it performed under load and then",
    "start": "945180",
    "end": "950940"
  },
  {
    "text": "send it back to the Bayesian algorithm which will look at the results and then try to find another",
    "start": "950940",
    "end": "958339"
  },
  {
    "text": "config that will give you better results and so on so this Loop continues and you",
    "start": "958339",
    "end": "965699"
  },
  {
    "text": "know after about 100 trials you'll find that you are I mean the base and optimization has given you a config that",
    "start": "965699",
    "end": "973019"
  },
  {
    "text": "will satisfy the objective function that you came out with and then we come up with a config recommendation so that's",
    "start": "973019",
    "end": "979139"
  },
  {
    "text": "in essence how we do this so let's take a quick look at how this works",
    "start": "979139",
    "end": "985440"
  },
  {
    "text": "um so I do have a small demo out here so I have mini Cube running on my laptop",
    "start": "985440",
    "end": "991500"
  },
  {
    "text": "here as you can see and it has Prometheus and grafana installed in the mini Cube",
    "start": "991500",
    "end": "997380"
  },
  {
    "text": "cluster and I also have auto tune running here and I have a tech cam power",
    "start": "997380",
    "end": "1004839"
  },
  {
    "text": "application which is a quarkus rusty Z hibernate application that is also",
    "start": "1004839",
    "end": "1011300"
  },
  {
    "text": "running here in the cluster and so now the challenge here is to try and",
    "start": "1011300",
    "end": "1016459"
  },
  {
    "text": "optimize this particular Benchmark that is running so what are we",
    "start": "1016459",
    "end": "1022279"
  },
  {
    "text": "trying to optimize we are trying to optimize the response time to try and minimize it so response time is defined",
    "start": "1022279",
    "end": "1029780"
  },
  {
    "text": "as request some divided by request count where request sum is this particular query uh Prometheus query and request",
    "start": "1029780",
    "end": "1036500"
  },
  {
    "text": "count is this particular from this query and this applies of course to the tech Empower deployment and we are trying to",
    "start": "1036500",
    "end": "1044178"
  },
  {
    "text": "minimize response time here so let's try to apply this CML here and you can see that",
    "start": "1044179",
    "end": "1053660"
  },
  {
    "text": "um autotune starts to deploy um you know specific configurations for",
    "start": "1053660",
    "end": "1060440"
  },
  {
    "text": "and you know it comes up with different configurations that it can test and see how it is doing under load uh of course",
    "start": "1060440",
    "end": "1067640"
  },
  {
    "text": "this being a very short demo we are really uh you know not",
    "start": "1067640",
    "end": "1074600"
  },
  {
    "text": "um you know monitoring the load but just you know giving you a sense of how the",
    "start": "1074600",
    "end": "1080840"
  },
  {
    "text": "whole process works so you can see here that it is starting multiple trials and",
    "start": "1080840",
    "end": "1085940"
  },
  {
    "text": "you can also take a look at um list experiments here to see the configs",
    "start": "1085940",
    "end": "1092240"
  },
  {
    "text": "that it's actually trying out so here you can see it is trying with certain values of CPU and memory and also uh",
    "start": "1092240",
    "end": "1098780"
  },
  {
    "text": "Java options that includes the hotspot layer that it has found and the quarkus",
    "start": "1098780",
    "end": "1105200"
  },
  {
    "text": "layer so you know very quickly you can also look at all the layers that it is found in the application here so it is",
    "start": "1105200",
    "end": "1112039"
  },
  {
    "text": "found the base container hotspot and quarkus and so on so if you keep",
    "start": "1112039",
    "end": "1118460"
  },
  {
    "text": "monitoring this uh it you know runs the whole set of Trials and then comes up",
    "start": "1118460",
    "end": "1124280"
  },
  {
    "text": "with the best trial at the end of the experiment to say you know this is the one that had the best configuration and",
    "start": "1124280",
    "end": "1131419"
  },
  {
    "text": "and you can take a look at uh you know the trial according to that particular",
    "start": "1131419",
    "end": "1136460"
  },
  {
    "text": "trial number to figure out what was the best configuration",
    "start": "1136460",
    "end": "1141760"
  },
  {
    "text": "so um so that's you know a very quick demo",
    "start": "1142240",
    "end": "1147919"
  },
  {
    "text": "of auto-tune I would definitely um recommend that you check out our GitHub repos and we have this demo also",
    "start": "1147919",
    "end": "1156620"
  },
  {
    "text": "running uh I mean available on public GitHub so it is um available in this",
    "start": "1156620",
    "end": "1163160"
  },
  {
    "text": "particular repo github.com this is the one that I was running just",
    "start": "1163160",
    "end": "1168620"
  },
  {
    "text": "now you should be able to even run it on your own laptop as well and this is the uh the main GitHub repo so",
    "start": "1168620",
    "end": "1179179"
  },
  {
    "text": "um now that uh you know you've seen a very quick demo uh what's really",
    "start": "1179179",
    "end": "1184580"
  },
  {
    "text": "happening here is that you know the Bayesian optimization is quickly coming to try and uh you know find a particular",
    "start": "1184580",
    "end": "1192559"
  },
  {
    "text": "config that gives you the best result I usually compare the Bayesian optimization to a journey uh however",
    "start": "1192559",
    "end": "1199100"
  },
  {
    "text": "there's one caveat here the Jenny can only be asked for one wish so you can",
    "start": "1199100",
    "end": "1205039"
  },
  {
    "text": "invoke the journey any number of times which means that you can invoke the Bayesian optimization a for any number of experiments but for every experiment",
    "start": "1205039",
    "end": "1212240"
  },
  {
    "text": "that you're running which consists of maybe even up to 100 trials there can only be one objective function or only",
    "start": "1212240",
    "end": "1218539"
  },
  {
    "text": "one wish so you need to be really get creative with your wish you know it's something like I want to be on a beach",
    "start": "1218539",
    "end": "1225080"
  },
  {
    "text": "in Hawaii with my wife and kids and walk into my large house with this great internet and so on so basically you're",
    "start": "1225080",
    "end": "1231919"
  },
  {
    "text": "trying to put in all of your requirements into that one objective function and then the Bayesian",
    "start": "1231919",
    "end": "1239720"
  },
  {
    "text": "optimization will try to optimize for that particular objective function",
    "start": "1239720",
    "end": "1245120"
  },
  {
    "text": "so so you've heard all of the theories so far so let's take a look at some of the results",
    "start": "1245120",
    "end": "1250880"
  },
  {
    "text": "um so here you see that I mean as I mentioned um actually we were using the tech",
    "start": "1250880",
    "end": "1256280"
  },
  {
    "text": "Empower framework which is actually an industry standard framework where you have benchmarks from uh you know for all",
    "start": "1256280",
    "end": "1264380"
  },
  {
    "text": "different kinds of runtimes you know Java golang rust and node tests you name",
    "start": "1264380",
    "end": "1269840"
  },
  {
    "text": "it so we specifically picked up the quarkus rest easy Benchmark and ran this",
    "start": "1269840",
    "end": "1275000"
  },
  {
    "text": "on a openshift cluster uh you know which had this particular configuration and",
    "start": "1275000",
    "end": "1281600"
  },
  {
    "text": "you know it had all of these different um tunables that we used are two tunables",
    "start": "1281600",
    "end": "1288500"
  },
  {
    "text": "at the container layer a bunch of tunables for the hot spot layer and a",
    "start": "1288500",
    "end": "1293659"
  },
  {
    "text": "few for the quarkus layer as well and so these were the ranges within which they were operating",
    "start": "1293659",
    "end": "1300260"
  },
  {
    "text": "um and uh we had set the kubernetes requests to be the same as the limits um and we were using the given GC",
    "start": "1300260",
    "end": "1307940"
  },
  {
    "text": "garbage collector and maximum percentage set equal to 70. the incoming load was",
    "start": "1307940",
    "end": "1313520"
  },
  {
    "text": "constant at just 512 users so we started off uh initially you know",
    "start": "1313520",
    "end": "1321320"
  },
  {
    "text": "saying that okay we want to just minimize the response time but then we quickly realized that you know as I",
    "start": "1321320",
    "end": "1327320"
  },
  {
    "text": "mentioned uh Bayesian optimization only tries to optimize that one aspect at the cost of maybe other uh aspects so we",
    "start": "1327320",
    "end": "1335059"
  },
  {
    "text": "realized that the low response time came at the cost of um uh you know higher CPU usage then we",
    "start": "1335059",
    "end": "1344419"
  },
  {
    "text": "did another experiment where we said okay fix the CPU Sage um but you know give me lower responses",
    "start": "1344419",
    "end": "1351260"
  },
  {
    "text": "time but this time we found out that it was um giving us higher Max response times",
    "start": "1351260",
    "end": "1358400"
  },
  {
    "text": "or daily latencies so this was the third take where we said uh okay Jenny uh you know give me the",
    "start": "1358400",
    "end": "1367220"
  },
  {
    "text": "best response time the lowest response time uh height throughput uh and at the same time keep the max",
    "start": "1367220",
    "end": "1375200"
  },
  {
    "text": "response time or the tail latency is down and keep the resources fixed so",
    "start": "1375200",
    "end": "1381500"
  },
  {
    "text": "essentially we gave it weightages as well we said response time has the highest weightage throughput comes next",
    "start": "1381500",
    "end": "1387740"
  },
  {
    "text": "Max response time is the you know least in terms of the weightages and make sure",
    "start": "1387740",
    "end": "1394400"
  },
  {
    "text": "that you fix the CPU and memory uh so that the cost is the same",
    "start": "1394400",
    "end": "1399799"
  },
  {
    "text": "so so you know the zeroth um value here it corresponds to the default",
    "start": "1399799",
    "end": "1408080"
  },
  {
    "text": "one where there were no changes done to the application configuration uh with",
    "start": "1408080",
    "end": "1414140"
  },
  {
    "text": "the same resources as the rest of the experiment and so here we see that it is you know the default was about 14.21",
    "start": "1414140",
    "end": "1421700"
  },
  {
    "text": "milliseconds of response time and then we see the the uh the auto-tune coming",
    "start": "1421700",
    "end": "1428120"
  },
  {
    "text": "up with different configurations and trying them out and then we got the best",
    "start": "1428120",
    "end": "1433159"
  },
  {
    "text": "configuration around the 97th trial where it got a response time of 2.39",
    "start": "1433159",
    "end": "1440240"
  },
  {
    "text": "milliseconds so you can see here um that we I mean this is actually",
    "start": "1440240",
    "end": "1446360"
  },
  {
    "text": "achieved about 83 better response time with a small uh or or the throughput",
    "start": "1446360",
    "end": "1452600"
  },
  {
    "text": "being almost the same and um of course the tail latencies uh were",
    "start": "1452600",
    "end": "1458539"
  },
  {
    "text": "low as well so you can take a look at all of these results uh here these are available on github.com uh Cruise slash",
    "start": "1458539",
    "end": "1466400"
  },
  {
    "text": "autotune results repo uh these are available publicly as well so you can",
    "start": "1466400",
    "end": "1471620"
  },
  {
    "text": "see that um the max response time in the auto-tune case is down the CPU Sage is",
    "start": "1471620",
    "end": "1478340"
  },
  {
    "text": "almost the same as the default um and we got a really good response",
    "start": "1478340",
    "end": "1485000"
  },
  {
    "text": "times about 83 percent better response time as well um and we also calculate the cost",
    "start": "1485000",
    "end": "1492620"
  },
  {
    "text": "um of the hardware by uh looking at the data that we got from the previous experiment for both the default and the",
    "start": "1492620",
    "end": "1498679"
  },
  {
    "text": "autotune config we've measured how many instances it would take to handle one million transactions",
    "start": "1498679",
    "end": "1504260"
  },
  {
    "text": "and applied it on a matching AWS configuration uh aw A1 dot extra large",
    "start": "1504260",
    "end": "1509840"
  },
  {
    "text": "which has about four core eight gig and we observed with the auto tune config there's a eight percent reduction in",
    "start": "1509840",
    "end": "1516559"
  },
  {
    "text": "cost as well so this is the corresponding best configuration uh the right side column",
    "start": "1516559",
    "end": "1523220"
  },
  {
    "text": "in the is is the value for each of the tunables that we saw previously interestingly you see that auto-tune has",
    "start": "1523220",
    "end": "1531260"
  },
  {
    "text": "flipped some of the defaults uh from what the runtime itself sets",
    "start": "1531260",
    "end": "1537260"
  },
  {
    "text": "okay so um in summary if you are an SRE you know your first",
    "start": "1537260",
    "end": "1543140"
  },
  {
    "text": "uh step is to set up observability uh don't forget to tune the hardware set",
    "start": "1543140",
    "end": "1548779"
  },
  {
    "text": "the node and Port affinities ensure requests and limits are set for all app parts and they're right size",
    "start": "1548779",
    "end": "1556000"
  },
  {
    "text": "use app specific scaling metrics if possible uh ensure that there is no disruption with the Pod disruption",
    "start": "1556000",
    "end": "1563299"
  },
  {
    "text": "budget and please do check out the cruise autotune for autonomous tuning",
    "start": "1563299",
    "end": "1568400"
  },
  {
    "text": "and and we do plan to come back to you with some updates so um lastly do check",
    "start": "1568400",
    "end": "1574279"
  },
  {
    "text": "out the cruise GitHub repos you have any questions reach out to us on uh crew",
    "start": "1574279",
    "end": "1579500"
  },
  {
    "text": "slack or send us a mail uh we do look forward to hearing from you all uh thank",
    "start": "1579500",
    "end": "1584720"
  },
  {
    "text": "you so much for listening",
    "start": "1584720",
    "end": "1588500"
  },
  {
    "text": "hey thanks a lot for the uh session and it was really informative and very",
    "start": "1608320",
    "end": "1613580"
  },
  {
    "text": "helpful and I hope uh the participants are benefited by this very informative",
    "start": "1613580",
    "end": "1619100"
  },
  {
    "text": "session and if you have any questions you can pose the questions on the chat and dinakar is available",
    "start": "1619100",
    "end": "1625400"
  },
  {
    "text": "to answer",
    "start": "1625400",
    "end": "1628000"
  },
  {
    "text": "oh sorry uh yeah session and it was really informative and very helpful and I hope uh the",
    "start": "1635539",
    "end": "1643640"
  },
  {
    "text": "participants are benefited by this very incremental session and if you have any questions you can pose the questions on",
    "start": "1643640",
    "end": "1649220"
  },
  {
    "text": "the chat and denica is available to answer",
    "start": "1649220",
    "end": "1655299"
  },
  {
    "text": "thank you Ashok um happy to answer any questions books have here oh sorry uh yeah",
    "start": "1657320",
    "end": "1666340"
  },
  {
    "text": "uh looks like there are not much questions you know questions on the chat that I see but I have one question say",
    "start": "1695500",
    "end": "1702919"
  },
  {
    "text": "if um if like say if there is a fresh grad who would like to get into this uh open",
    "start": "1702919",
    "end": "1710779"
  },
  {
    "text": "source space like what is your recommendation that you would like to give or maybe someone who would like to",
    "start": "1710779",
    "end": "1716179"
  },
  {
    "text": "switch their career path maybe after 10 years of experience and if you think that okay they would like to switch their career part",
    "start": "1716179",
    "end": "1722000"
  },
  {
    "text": "what is your recommendation that you would like to give with an occur uh yeah I I think that's a general",
    "start": "1722000",
    "end": "1729740"
  },
  {
    "text": "question so I would say that you know my the first step that I would always suggest is for people to uh understand",
    "start": "1729740",
    "end": "1737000"
  },
  {
    "text": "what are their own preferences you know there is like a wide variety of Open Source software that's available uh",
    "start": "1737000",
    "end": "1743480"
  },
  {
    "text": "today I mean there's system software front-end back-end",
    "start": "1743480",
    "end": "1748640"
  },
  {
    "text": "um you know a machine learning cloud and so on so so there's multiple different uh",
    "start": "1748640",
    "end": "1756020"
  },
  {
    "text": "you know open source projects uh that are available and I think that the best way is to First understand what are your",
    "start": "1756020",
    "end": "1763159"
  },
  {
    "text": "own interests and then find out projects in that particular space so for example",
    "start": "1763159",
    "end": "1768260"
  },
  {
    "text": "uh you know I'm a guy who's been interested in Systems Technology all my life I've worked in operating systems",
    "start": "1768260",
    "end": "1774320"
  },
  {
    "text": "jvm and kubernetes now and so on so I always tend to look around in this space",
    "start": "1774320",
    "end": "1779360"
  },
  {
    "text": "and see what are the new things coming up and of course now these days I'm interested in machine learning who is",
    "start": "1779360",
    "end": "1784640"
  },
  {
    "text": "not right everybody that's the buzzword now so um and if you look around I I'm sure",
    "start": "1784640",
    "end": "1792080"
  },
  {
    "text": "you'll find an open source project that you are interested in so that's the first step next step is to find out what is the community around it find out why",
    "start": "1792080",
    "end": "1799100"
  },
  {
    "text": "you know who are the different um like stakeholders do they have a",
    "start": "1799100",
    "end": "1804559"
  },
  {
    "text": "slack channel do they have guitar is there a mailing list so go join there",
    "start": "1804559",
    "end": "1810320"
  },
  {
    "text": "find out uh what's the best way to interact you know look at GitHub obviously gitlab or anything that's",
    "start": "1810320",
    "end": "1816980"
  },
  {
    "text": "around uh look at issues most of the projects these days have something like a good first issue that has been marked",
    "start": "1816980",
    "end": "1823220"
  },
  {
    "text": "on GitHub issues so you look at that and see you know what are the issues it could be simple things like fixing",
    "start": "1823220",
    "end": "1830240"
  },
  {
    "text": "um you know language or uh or maybe some simple issues and so on so you can look",
    "start": "1830240",
    "end": "1836480"
  },
  {
    "text": "at that and see if you can start getting into the project by understanding the process how do you submit a PR you know",
    "start": "1836480",
    "end": "1843380"
  },
  {
    "text": "basics of GitHub and things like that and then you can uh you know read more",
    "start": "1843380",
    "end": "1848840"
  },
  {
    "text": "on the topic look at videos and talk to experts talk to the community folks see",
    "start": "1848840",
    "end": "1855080"
  },
  {
    "text": "if there's a meet up or attend some things like this kcd Chennai understand more about the topic and then uh you",
    "start": "1855080",
    "end": "1862700"
  },
  {
    "text": "know gradually you take it up there so that's the way I would look at it",
    "start": "1862700",
    "end": "1867700"
  },
  {
    "text": "sure thanks thanks uh that answers my question and I like the red hat on your backdrop",
    "start": "1868159",
    "end": "1873640"
  },
  {
    "text": "thank you yeah folks please make use of this time",
    "start": "1873640",
    "end": "1881539"
  },
  {
    "text": "for Q a and if you don't have any questions then we'll have dinner go uh so that they can enjoy his rest of the",
    "start": "1881539",
    "end": "1887659"
  },
  {
    "text": "weekend it's already Friday attend the rest of the talks",
    "start": "1887659",
    "end": "1893200"
  },
  {
    "text": "yeah thank you all and it's been a pleasure uh being in this uh thank you so much",
    "start": "1894519",
    "end": "1900260"
  },
  {
    "text": "for organizing this uh great event sure thanks thanks",
    "start": "1900260",
    "end": "1906019"
  },
  {
    "text": "thank you bye bye",
    "start": "1906019",
    "end": "1911080"
  },
  {
    "text": "uh we'll have the next session starting in few minutes so",
    "start": "1929899",
    "end": "1934940"
  },
  {
    "text": "uh please hang in there thanks",
    "start": "1934940",
    "end": "1939220"
  }
]