[
  {
    "start": "0",
    "end": "72000"
  },
  {
    "text": "all right uh thanks for joining this session after runch and I'm really excited a lot of people in there this",
    "start": "599",
    "end": "7080"
  },
  {
    "text": "room I actually uh moderate a couple more session yesterday but some room is",
    "start": "7080",
    "end": "12480"
  },
  {
    "text": "not quite po so I really excited about this topic and so uh my name is Daniel o",
    "start": "12480",
    "end": "19080"
  },
  {
    "text": "I'm a CNF Ambassador I'm I'm so happy to be here as moderate this session today",
    "start": "19080",
    "end": "24560"
  },
  {
    "text": "and thanks for joining once again so we're going to talk a little bit about our next topic is how to migrate seven",
    "start": "24560",
    "end": "29960"
  },
  {
    "text": "00 kubernetes cruster not 7 not 70 it's a 700 kubernetes cruster to cruster API",
    "start": "29960",
    "end": "37160"
  },
  {
    "text": "with a zero downtime so I'm really happy for here and then please welcome our",
    "start": "37160",
    "end": "42480"
  },
  {
    "text": "next great speaker Sean and Tobias from Merc B Tech Innovation please welcome",
    "start": "42480",
    "end": "48360"
  },
  {
    "text": "the speakers",
    "start": "48360",
    "end": "51559"
  },
  {
    "text": "okay nice so hello everyone and Welcome to our talk on how to migrate 700",
    "start": "61000",
    "end": "66200"
  },
  {
    "text": "kubernetes clusters to the cluster life cycle management software called cluster API so today we will present to you how",
    "start": "66200",
    "end": "73280"
  },
  {
    "text": "we replaced our Legacy kubernetes Fleet Management with cluster API formerly it",
    "start": "73280",
    "end": "78840"
  },
  {
    "text": "was based on terraform and the most important part was Zero downtime and no effort for the",
    "start": "78840",
    "end": "84960"
  },
  {
    "text": "cluster users I'm Sean schne a software engineer from Mercedes-Benz Tech Innovation also",
    "start": "84960",
    "end": "92360"
  },
  {
    "text": "I'm a maintainer of cluster API provider openstack this is my first Cube con and",
    "start": "92360",
    "end": "97399"
  },
  {
    "text": "I'm so happy to meet all of",
    "start": "97399",
    "end": "101000"
  },
  {
    "text": "you Hol hi my name is sias giz I am also a software engineer for Mercedes-Benz",
    "start": "104680",
    "end": "110600"
  },
  {
    "text": "Tech Innovation I'm also maintainer for cluster API provider open stack and I'm working with kubernetes for 5 years now",
    "start": "110600",
    "end": "118280"
  },
  {
    "text": "so this technical presentation targets software and operating engineers and all",
    "start": "118280",
    "end": "123920"
  },
  {
    "text": "who is into cluster management it's maybe a bit um bit hard to learn in the",
    "start": "123920",
    "end": "130319"
  },
  {
    "text": "beginning but it's definitely worth the effort so keep it",
    "start": "130319",
    "end": "135760"
  },
  {
    "text": "on Mercedes-Benz Tech Innovation is a subsidiary of the German car manufacturer Mercedes-Benz it's the uh",
    "start": "135760",
    "end": "143160"
  },
  {
    "start": "136000",
    "end": "161000"
  },
  {
    "text": "the headquarter is located in ol and at b c disp tech Innovation we don't build cars we build software",
    "start": "143160",
    "end": "151000"
  },
  {
    "text": "and we are formerly known as damel TSS just in case you want to check the G commit history or you already known us",
    "start": "151000",
    "end": "157720"
  },
  {
    "text": "as D TSS so yeah our platform team develops and",
    "start": "157720",
    "end": "163400"
  },
  {
    "text": "operates a huge Fleet Management and we're operating 900 clusters all over the world in four data centers in uh",
    "start": "163400",
    "end": "170840"
  },
  {
    "text": "Atlanta Beijing Frankfurt and stutgart and just a note during the time we have",
    "start": "170840",
    "end": "176720"
  },
  {
    "text": "submitted this talk we only had 700 clusters so 200 classes more in half a",
    "start": "176720",
    "end": "181760"
  },
  {
    "text": "year that's not too bad I think so the agenda uh we first step in",
    "start": "181760",
    "end": "189080"
  },
  {
    "start": "185000",
    "end": "356000"
  },
  {
    "text": "and set the stage just that everybody knows what we are talking about then we will uh talk about the Legacy",
    "start": "189080",
    "end": "195560"
  },
  {
    "text": "provisioning architecture then after this the target picture with the migration to Cluster",
    "start": "195560",
    "end": "202040"
  },
  {
    "text": "API and in the end we will talk about the lessons learned and our next",
    "start": "202040",
    "end": "207239"
  },
  {
    "text": "steps yeah okay so we want to get to know you a bit",
    "start": "207239",
    "end": "213680"
  },
  {
    "text": "better um so please raise your hand if you know clust API who of you knows clust",
    "start": "213680",
    "end": "219120"
  },
  {
    "text": "API okay nice who is using clust API in production still a few nice okay who",
    "start": "219120",
    "end": "226480"
  },
  {
    "text": "manages and doesn't have to be with clust API who manages more than 10 clusters 100",
    "start": "226480",
    "end": "232959"
  },
  {
    "text": "clusters 1,000 clusters okay oh okay quite some experience in this room um",
    "start": "232959",
    "end": "239959"
  },
  {
    "text": "really like it so cluster API provides a central cluster management for the",
    "start": "239959",
    "end": "246400"
  },
  {
    "text": "complete life cycle of a cluster so this includes creating deleting updating to a",
    "start": "246400",
    "end": "251439"
  },
  {
    "text": "new kubernetes version and scaling so adding additional nodes to your",
    "start": "251439",
    "end": "256919"
  },
  {
    "text": "cluster cluster API is a project uh maintained by the special kubernetes special interest group cluster life",
    "start": "256919",
    "end": "263600"
  },
  {
    "text": "cycle and it perfectly works with multiple Cloud providers such as AWS",
    "start": "263600",
    "end": "269280"
  },
  {
    "text": "Google Asia open SEC and many many more actually right now there's a separate",
    "start": "269280",
    "end": "274840"
  },
  {
    "text": "talk that is teaching how to create your own cloud provider plugin to clust",
    "start": "274840",
    "end": "281120"
  },
  {
    "text": "API right um so this feature of",
    "start": "281120",
    "end": "286199"
  },
  {
    "text": "supporting Cloud providers is why we chose cluster API so we can not only",
    "start": "286199",
    "end": "292080"
  },
  {
    "text": "offer private cloud with open stake but also public Cloud to our business",
    "start": "292080",
    "end": "297960"
  },
  {
    "text": "partners here you see the turtles that are about to start their Journey the clust API turtles that are about to",
    "start": "298120",
    "end": "304039"
  },
  {
    "text": "start their Journey but before that I would like to clarify some Basics the key aspects of",
    "start": "304039",
    "end": "310360"
  },
  {
    "text": "the title from our talk first off how to migrate so starting point for this is",
    "start": "310360",
    "end": "316800"
  },
  {
    "text": "any existing cluster doesn't matter if it was created with some kind of",
    "start": "316800",
    "end": "321840"
  },
  {
    "text": "automation or if you created it on your own in our case it's a legacy Pipeline",
    "start": "321840",
    "end": "328039"
  },
  {
    "text": "and then we want to manage the infrastr structure and the cluster itself um the kubernetes style so",
    "start": "328039",
    "end": "334440"
  },
  {
    "text": "normally this is with custom resources and",
    "start": "334440",
    "end": "338960"
  },
  {
    "text": "controllers so the the architecture of clust API allows an incremental transitioning path so you don't have to",
    "start": "341400",
    "end": "348199"
  },
  {
    "text": "do everything like at one point you can just split it up and just adapt those",
    "start": "348199",
    "end": "353919"
  },
  {
    "text": "parts that you really need if you take one of those three Turtles and take take a look at a single",
    "start": "353919",
    "end": "360840"
  },
  {
    "start": "356000",
    "end": "406000"
  },
  {
    "text": "Turtle this stands for a workload cluster one of these workload clusters",
    "start": "360840",
    "end": "366199"
  },
  {
    "text": "is what we usually give to our users the workload cluster consists of a dedicated",
    "start": "366199",
    "end": "371680"
  },
  {
    "text": "control plane and multiple worker nodes and we want to migrate 700 of",
    "start": "371680",
    "end": "379160"
  },
  {
    "text": "those workload clusters to be controlled by clust API our largest management cluster that is the cluster that manages",
    "start": "379160",
    "end": "385960"
  },
  {
    "text": "all the clust API resources uh controls up to 200 of those workload",
    "start": "385960",
    "end": "393680"
  },
  {
    "text": "clusters and when you look at the outer shell of the turtle that is the infrastructure the networking the",
    "start": "394400",
    "end": "400319"
  },
  {
    "text": "firewall the router load balancer and that also is managed by cluster",
    "start": "400319",
    "end": "405919"
  },
  {
    "text": "API now when migrating one really important requirement for us was Zero downtime we",
    "start": "405919",
    "end": "413720"
  },
  {
    "start": "406000",
    "end": "447000"
  },
  {
    "text": "have business critical applications and downtime it's just not acceptable for our users",
    "start": "413720",
    "end": "420199"
  },
  {
    "text": "so we definitely want to make sure that the kubernetes API and the application",
    "start": "420199",
    "end": "425440"
  },
  {
    "text": "is available throughout the immigration this is not a green field approach where you get a new empty cluster and the",
    "start": "425440",
    "end": "430800"
  },
  {
    "text": "users migrate from one to the other one now this is we will not touch the",
    "start": "430800",
    "end": "436080"
  },
  {
    "text": "users's workload this is really important so as we now have clarified",
    "start": "436080",
    "end": "442400"
  },
  {
    "text": "the basics to beas would you like to tell us where we started from okay so",
    "start": "442400",
    "end": "448440"
  },
  {
    "start": "447000",
    "end": "502000"
  },
  {
    "text": "this is the picture from from our Legacy cluster provisioning architecture so we have a custom user interface where our",
    "start": "448440",
    "end": "456840"
  },
  {
    "text": "users are able to order a cluster and we have a custom API that uh triggers",
    "start": "456840",
    "end": "462680"
  },
  {
    "text": "several Jenkins pipelines and these pipelines are trigger some terraform pipelines for the cluster provisioning",
    "start": "462680",
    "end": "469000"
  },
  {
    "text": "so uh the infrastructure control plane Works etc etc um if the cluster is",
    "start": "469000",
    "end": "474960"
  },
  {
    "text": "provisioned successfully we have ansible to deploy the run-time deployments runtime deployments in this case are for",
    "start": "474960",
    "end": "481280"
  },
  {
    "text": "example the kubernetes core deployments like cni CSI CS whatever and after this",
    "start": "481280",
    "end": "487759"
  },
  {
    "text": "we will deploy the cluster add-ons with an Bill like metrix exporters or custom",
    "start": "487759",
    "end": "492879"
  },
  {
    "text": "controllers Etc so this is the Legacy provisioning and as you know where we",
    "start": "492879",
    "end": "499919"
  },
  {
    "text": "are coming from we will now take a look at the Target picture this is the target picture the user can order a cluster",
    "start": "499919",
    "end": "507440"
  },
  {
    "start": "502000",
    "end": "540000"
  },
  {
    "text": "like before but in the background we have some changes the main change is that Jenkins will trigger um a custom",
    "start": "507440",
    "end": "516200"
  },
  {
    "text": "binary that deploys resources into our cluster API management server and",
    "start": "516200",
    "end": "521760"
  },
  {
    "text": "cluster API will reconcile the infrastructure the control planes Etc",
    "start": "521760",
    "end": "527120"
  },
  {
    "text": "and after the cluster is provisioned our custom cubular controller will hook into the correct time and deploy then the",
    "start": "527120",
    "end": "533080"
  },
  {
    "text": "runtime deployments and flux will deploy our cluster add-ons that's it okay okay",
    "start": "533080",
    "end": "540480"
  },
  {
    "start": "540000",
    "end": "580000"
  },
  {
    "text": "perfect let's start with the preparation so we are using open St as the infrastructure layer open St or the",
    "start": "540480",
    "end": "547760"
  },
  {
    "text": "cluster API provider open stake identifies the resources um by its name so we must change the resource names to",
    "start": "547760",
    "end": "556279"
  },
  {
    "text": "the cluster API provider openstack uh name matching so this is the first step",
    "start": "556279",
    "end": "562079"
  },
  {
    "text": "and can be done by Terra form for example just make sure that this is",
    "start": "562079",
    "end": "567640"
  },
  {
    "text": "renamed correctly for example in the cluster fpi provider AWS they're using IDs and not the names so this is much",
    "start": "567640",
    "end": "574240"
  },
  {
    "text": "easier just take a look at your provider how there done okay",
    "start": "574240",
    "end": "580920"
  },
  {
    "start": "580000",
    "end": "656000"
  },
  {
    "text": "so there are multiple ways to uh migrate to Cluster API we decided to do it in",
    "start": "580920",
    "end": "586560"
  },
  {
    "text": "three steps so the first step after the resources were renamed is to migrate the",
    "start": "586560",
    "end": "592839"
  },
  {
    "text": "infrastructure that cluster API is re able to reconcile we need the cluster resources and the infra cluster resource",
    "start": "592839",
    "end": "599360"
  },
  {
    "text": "in our case the open St cluster this is the first step after this we can go further to the second",
    "start": "599360",
    "end": "606839"
  },
  {
    "text": "step we decided to use um the worker machines as the Second Step because we",
    "start": "606839",
    "end": "613279"
  },
  {
    "text": "have uh during the runtime more changes and we are can we we we can have cluster",
    "start": "613279",
    "end": "619959"
  },
  {
    "text": "API to reconcile the machine deployments Etc if a user changes the replicas or",
    "start": "619959",
    "end": "625640"
  },
  {
    "text": "the image flavors it's whatever and because of this we decided to do first",
    "start": "625640",
    "end": "631040"
  },
  {
    "text": "the machine deployment or the worker node migration Okay the third step is",
    "start": "631040",
    "end": "636399"
  },
  {
    "text": "the migration of the control plane this is also really critical because if there",
    "start": "636399",
    "end": "641519"
  },
  {
    "text": "is something false or whatever um the at CD could have data loss and your cluster",
    "start": "641519",
    "end": "647760"
  },
  {
    "text": "will be broken so keep care on this step okay let's move to the first step",
    "start": "647760",
    "end": "654639"
  },
  {
    "text": "the cluster and infra cluster migration this is really really easy because you",
    "start": "654639",
    "end": "660320"
  },
  {
    "start": "656000",
    "end": "688000"
  },
  {
    "text": "can just use your INF cluster spec and use the from the Legacy provisioning the",
    "start": "660320",
    "end": "666440"
  },
  {
    "text": "specs and deploy just the resource you can you can then deploy the cluster",
    "start": "666440",
    "end": "672480"
  },
  {
    "text": "resource for the cluster API to reconcile and just reference the infr cluster object and that's it your",
    "start": "672480",
    "end": "678519"
  },
  {
    "text": "infrastructure will reconcile and cluster API is able to manage the infrastructure perfect Sean thank you",
    "start": "678519",
    "end": "689199"
  },
  {
    "start": "688000",
    "end": "724000"
  },
  {
    "text": "okay so we now migrated the infrastructure and we have continuous reconciliation by the cluster API",
    "start": "689519",
    "end": "695519"
  },
  {
    "text": "controller with metrics and Status easily available available from our management cluster Next Step will be to",
    "start": "695519",
    "end": "702800"
  },
  {
    "text": "migrate the worker nodes existing worker noes and a node typically in clust API",
    "start": "702800",
    "end": "709560"
  },
  {
    "text": "is managed by a machine therefore we will create those three resources machine deployment machine set and the",
    "start": "709560",
    "end": "717399"
  },
  {
    "text": "Machine itself and also the provided a specific implementation the open St",
    "start": "717399",
    "end": "723360"
  },
  {
    "text": "machine now the problem is the the existing control plane which is not part",
    "start": "723480",
    "end": "728519"
  },
  {
    "start": "724000",
    "end": "782000"
  },
  {
    "text": "of the Second Step it's part of the third step plus the API is not yet aware of this control plane and to overcome",
    "start": "728519",
    "end": "736160"
  },
  {
    "text": "this problem we will create a fake Cube ADM control plane the QBE ADM control plane controls the control plane for",
    "start": "736160",
    "end": "742639"
  },
  {
    "text": "clust API and this fake Cube ADM control plane um of course needs a machine so we would",
    "start": "742639",
    "end": "749519"
  },
  {
    "text": "just create a dummy machine which is not combined with any infrastructures you",
    "start": "749519",
    "end": "755480"
  },
  {
    "text": "just have to create the resource and then reference this um and",
    "start": "755480",
    "end": "760760"
  },
  {
    "text": "I'm sorry but at the control plane label which identifies this as a control plane",
    "start": "760760",
    "end": "766639"
  },
  {
    "text": "machine and on the right side also create a dummy cadm control plane and",
    "start": "766639",
    "end": "772040"
  },
  {
    "text": "Please be aware we added the post annotation to both of those resources which will prevent clust API from doing",
    "start": "772040",
    "end": "778639"
  },
  {
    "text": "anything on those resources and then the last step to get",
    "start": "778639",
    "end": "784000"
  },
  {
    "start": "782000",
    "end": "797000"
  },
  {
    "text": "your fake cadium control plane is to patch the status fields and add the condition to the cluster and just yeah",
    "start": "784000",
    "end": "793079"
  },
  {
    "text": "say okay you're ready now for cluster API and that's it for this part now we",
    "start": "793079",
    "end": "798199"
  },
  {
    "text": "can actually migrate the existing uh worker nodes to be represented by",
    "start": "798199",
    "end": "803959"
  },
  {
    "text": "cluster API so for each worker node in your cluster we'll create an open open",
    "start": "803959",
    "end": "809320"
  },
  {
    "text": "STI machine and it is connected by the provider id provided in our case from open STI and this open STI machine the",
    "start": "809320",
    "end": "816880"
  },
  {
    "text": "name of the open Stick machine is referenced in the machine the other fields such such as",
    "start": "816880",
    "end": "823120"
  },
  {
    "text": "the boo strip secret um they don't really need uh values that are correct",
    "start": "823120",
    "end": "828440"
  },
  {
    "text": "you can just create an empty secret put in the name and then we're good to",
    "start": "828440",
    "end": "834839"
  },
  {
    "start": "835000",
    "end": "868000"
  },
  {
    "text": "go okay so we now have all workload nodes repres ented by an open mstic",
    "start": "835160",
    "end": "840240"
  },
  {
    "text": "machine and a machine next off we want to create the machine set the machine set is an immutable abstraction of a",
    "start": "840240",
    "end": "848279"
  },
  {
    "text": "machines now we have labels um specifically",
    "start": "848279",
    "end": "855240"
  },
  {
    "text": "specifically the deployment name that should be added to all resources on this slide okay so the machine deployment",
    "start": "855240",
    "end": "861040"
  },
  {
    "text": "needs this label to identify those resources to be part of the machine deployment so make sure to add those",
    "start": "861040",
    "end": "867720"
  },
  {
    "text": "two and now the machine deployment as you see the name is the",
    "start": "867720",
    "end": "872920"
  },
  {
    "start": "868000",
    "end": "898000"
  },
  {
    "text": "same as in the label deployment name um should be created and match the",
    "start": "872920",
    "end": "878360"
  },
  {
    "text": "replicas to the amount of workload uh nodes you have in your existing cluster also before creating the machine",
    "start": "878360",
    "end": "885079"
  },
  {
    "text": "deployment create the cube ADM config template in the open sake machine template put it into the machine deployment at the kubernetes version",
    "start": "885079",
    "end": "892519"
  },
  {
    "text": "you're using and that's that's about it so pretty easy I guess",
    "start": "892519",
    "end": "899839"
  },
  {
    "start": "898000",
    "end": "932000"
  },
  {
    "text": "now if you have a look from the customer's perspective what he will see it looks as follows so on the top of the",
    "start": "899839",
    "end": "905920"
  },
  {
    "text": "slide you see the state when we migrated the workload clusters managed by clust API the name stays the same as we then",
    "start": "905920",
    "end": "913680"
  },
  {
    "text": "perform a rolling update to a new kubernetes version new workload nodes will be added to your cluster and will",
    "start": "913680",
    "end": "920160"
  },
  {
    "text": "have new node names as they were bootstrapped and provided by clust API okay so what you what will you",
    "start": "920160",
    "end": "929000"
  },
  {
    "text": "continue with step three then of course thank you Sean so after we have migrated",
    "start": "929000",
    "end": "934920"
  },
  {
    "text": "our worker machines we can come to the last step the migration of the control plane and the cube ADM control plane",
    "start": "934920",
    "end": "941600"
  },
  {
    "text": "plus the API resource so let's take a look at this first we have to create a new CM",
    "start": "941600",
    "end": "948600"
  },
  {
    "start": "945000",
    "end": "984000"
  },
  {
    "text": "control plane with its real data so we can Define here the version the cube ADM",
    "start": "948600",
    "end": "954000"
  },
  {
    "text": "config spec where the cluster joint configuration is in it the machine template with the INF structure",
    "start": "954000",
    "end": "959360"
  },
  {
    "text": "reference Etc and because why we are using a new CU cubm control plan is that",
    "start": "959360",
    "end": "965160"
  },
  {
    "text": "cluster API denies some changes in the kcb specs with a uh with a web hook and",
    "start": "965160",
    "end": "972519"
  },
  {
    "text": "because of this we will create a new one because it's the easiest way we can",
    "start": "972519",
    "end": "978199"
  },
  {
    "text": "create just a new name and that's it okay after we have created the cube ADM control plane we can now adopt all the",
    "start": "978199",
    "end": "988079"
  },
  {
    "start": "984000",
    "end": "1015000"
  },
  {
    "text": "Legacy provision vision control plane machines okay so we just creating a",
    "start": "988079",
    "end": "994519"
  },
  {
    "text": "secret and a cute cedm config like in the machine deployment step with a empty",
    "start": "994519",
    "end": "999920"
  },
  {
    "text": "value it's just a dummy and reference all for all Legacy provisioned machines",
    "start": "999920",
    "end": "1005800"
  },
  {
    "text": "a machine resource we can or we have to add the provider id of the openstack instance",
    "start": "1005800",
    "end": "1012680"
  },
  {
    "text": "just to reference the infrastructure itself okay after we have the machines we can create the open machines this is",
    "start": "1012680",
    "end": "1020680"
  },
  {
    "start": "1015000",
    "end": "1039000"
  },
  {
    "text": "the resource for the cluster API provider open stack to be able to reconcile the uh virtual machines itself",
    "start": "1020680",
    "end": "1029120"
  },
  {
    "text": "so we have to here reference the machine we have created we have to reference the cube ADM control plan we have",
    "start": "1029120",
    "end": "1035918"
  },
  {
    "text": "created uh and also sorry one step back we have to pause the kcp as well and",
    "start": "1035919",
    "end": "1043720"
  },
  {
    "start": "1039000",
    "end": "1055000"
  },
  {
    "text": "also the machine just to make sure that the cubm control plan and the Machine will Rec recile at the correct time so",
    "start": "1043720",
    "end": "1051760"
  },
  {
    "text": "we now have the open stack machines created and that's mostly it we only",
    "start": "1051760",
    "end": "1057760"
  },
  {
    "text": "have to change the cube ADM control plane's name in the cluster reference",
    "start": "1057760",
    "end": "1063320"
  },
  {
    "text": "unpause the cube ADM control plane unpa the machines and that's it you can see",
    "start": "1063320",
    "end": "1068440"
  },
  {
    "text": "in the screenshot below that the new Cube ADM control plane the new control plane from cluster API will join the",
    "start": "1068440",
    "end": "1074520"
  },
  {
    "text": "cluster it's not ready yet but it will get ready and after after it's ready",
    "start": "1074520",
    "end": "1079880"
  },
  {
    "text": "cluster API will remove the Legacy provision control plane and we are done",
    "start": "1079880",
    "end": "1085559"
  },
  {
    "text": "so easy okay after this step we can clean up our dummy object that's you",
    "start": "1085559",
    "end": "1092400"
  },
  {
    "text": "that's simple and we are done we have migrated our first cluster and then our 700th",
    "start": "1092400",
    "end": "1100080"
  },
  {
    "start": "1094000",
    "end": "1104000"
  },
  {
    "text": "cluster and that's it we're ready with our migration okay so how does it look are",
    "start": "1100080",
    "end": "1106200"
  },
  {
    "start": "1104000",
    "end": "1307000"
  },
  {
    "text": "you still following that I easy one to three approach I hope so um so now we want to",
    "start": "1106200",
    "end": "1113840"
  },
  {
    "text": "share some best practices some tips and tricks and Lessons Learned with you to sum this up and the first one is not",
    "start": "1113840",
    "end": "1120360"
  },
  {
    "text": "really clust API specific it's a general recommendation so whenever we talk about",
    "start": "1120360",
    "end": "1126440"
  },
  {
    "text": "zero downtime um we ensure that we can safely drain the nodes and this is an",
    "start": "1126440",
    "end": "1132240"
  },
  {
    "text": "action that has to be performed by the users so we ask our users to add pot disruption budgets to that deployments",
    "start": "1132240",
    "end": "1139760"
  },
  {
    "text": "so we can then safely drain their notes and sometimes one of those po disruption",
    "start": "1139760",
    "end": "1145799"
  },
  {
    "text": "budgets is not working or even the upgrade the kubernetes update is stuck",
    "start": "1145799",
    "end": "1151480"
  },
  {
    "text": "due to one of those pdbs then what we do is we contact the customer ask them to",
    "start": "1151480",
    "end": "1156600"
  },
  {
    "text": "fix it and then the upgrade will continue by itself through the clust API",
    "start": "1156600",
    "end": "1163559"
  },
  {
    "text": "reconciliation the next two things are really nice uh functional ities from the cluster API uh controller one is called",
    "start": "1165039",
    "end": "1172240"
  },
  {
    "text": "the pre-rain uh hook or annotation and the other pre- terminate hook or annotation the pre-rain is just executed",
    "start": "1172240",
    "end": "1180080"
  },
  {
    "text": "before draining a Noe we use this pre-rain annotation to",
    "start": "1180080",
    "end": "1185360"
  },
  {
    "text": "add a no schedule Tain to all nodes that will be anyway deleted during an update",
    "start": "1185360",
    "end": "1190440"
  },
  {
    "text": "so whenever a new whenever a port gets rescheduled to another node then it is",
    "start": "1190440",
    "end": "1197240"
  },
  {
    "text": "mostly most of the times schedule then to a node that is a new one and not the old one and this increases the update a",
    "start": "1197240",
    "end": "1204200"
  },
  {
    "text": "bit the second one is the pre terminate hook this is executed directly um before",
    "start": "1204200",
    "end": "1210720"
  },
  {
    "text": "the node is deleted and we use this for mostly infrastructure related things",
    "start": "1210720",
    "end": "1216520"
  },
  {
    "text": "such as volume attachment so we remove the volumes and the second is to remove the node from the load balancer so",
    "start": "1216520",
    "end": "1223640"
  },
  {
    "text": "highly encourage you to to make use of that functionality",
    "start": "1223640",
    "end": "1228880"
  },
  {
    "text": "so next one is nightly builds and testing testing is key so we highly emphasize and recommend to you please",
    "start": "1228880",
    "end": "1235799"
  },
  {
    "text": "test the migration but not only the migration itself from Legacy to Cluster API managed clusters also test um",
    "start": "1235799",
    "end": "1243799"
  },
  {
    "text": "creating new clusters and maybe deleting or upgrading clusters this really helped us to identify nasty bugs and issues uh",
    "start": "1243799",
    "end": "1251280"
  },
  {
    "text": "so yeah this is yeah testing is always important and the last thing is actually",
    "start": "1251280",
    "end": "1257760"
  },
  {
    "text": "a problem we face this is a caching problem the infrastructure is not always as stable as it should be you probably",
    "start": "1257760",
    "end": "1264679"
  },
  {
    "text": "know and uh sometimes open S machines failed or had like an status that was",
    "start": "1264679",
    "end": "1270080"
  },
  {
    "text": "error something like that and most of the times we enforc the deletion of one",
    "start": "1270080",
    "end": "1276520"
  },
  {
    "text": "of those open sake machines with maybe removing the finalizer the problem is the controller that watches on those",
    "start": "1276520",
    "end": "1282880"
  },
  {
    "text": "resources doesn't know that the resource was actually deleted because the cash is then invalid or has false",
    "start": "1282880",
    "end": "1289799"
  },
  {
    "text": "data um and to mitigate the problem we just restarted the ports but luckily uh",
    "start": "1289799",
    "end": "1295400"
  },
  {
    "text": "one of our colleagues um created a pull request for the controller runtime project and fixed that small buck but",
    "start": "1295400",
    "end": "1301240"
  },
  {
    "text": "nasty bug okay that's about it um toas where we are going next thanks Sean okay",
    "start": "1301240",
    "end": "1309159"
  },
  {
    "start": "1307000",
    "end": "1389000"
  },
  {
    "text": "with the mation to Cluster API it's not done our platform is under uh",
    "start": "1309159",
    "end": "1314279"
  },
  {
    "text": "permanently con uh construction with feature improvements bug fixing",
    "start": "1314279",
    "end": "1319400"
  },
  {
    "text": "user feature requests Etc so um the",
    "start": "1319400",
    "end": "1324600"
  },
  {
    "text": "replacement of enil with flux is our next thing and also in progress another thing is that during",
    "start": "1324600",
    "end": "1332679"
  },
  {
    "text": "the initial planning of the migration cluster API features like cluster class or the runtime SDK machine health check",
    "start": "1332679",
    "end": "1339840"
  },
  {
    "text": "Etc were just not there so we are also going to plan the implementation of",
    "start": "1339840",
    "end": "1345000"
  },
  {
    "text": "these features what we are really proud about is that we have uh developed a cluster",
    "start": "1345000",
    "end": "1351240"
  },
  {
    "text": "API State metrix exporter which is a metrix exporter for cluster API um and",
    "start": "1351240",
    "end": "1357200"
  },
  {
    "text": "we are going to merge this code to the call cluster API uh repository and",
    "start": "1357200",
    "end": "1363320"
  },
  {
    "text": "because of this it's a donation to The cncf and a big thank you to all of",
    "start": "1363320",
    "end": "1368960"
  },
  {
    "text": "you last but not least is that we're moving forward forward to also um offer",
    "start": "1375159",
    "end": "1380559"
  },
  {
    "text": "to our users public clouds because of cluster API this is really easy we can use another provider and that's",
    "start": "1380559",
    "end": "1388480"
  },
  {
    "text": "it yes if you want to get involved just ping us on slack or follow us on GitHub",
    "start": "1388480",
    "end": "1395640"
  },
  {
    "start": "1389000",
    "end": "1407000"
  },
  {
    "text": "and by the way there's a cluster API intro and deep dive the Next Room in a few minutes from y rush and Vince so",
    "start": "1395640",
    "end": "1404039"
  },
  {
    "text": "just join and maybe you can learn something thanks and if you have",
    "start": "1404039",
    "end": "1409400"
  },
  {
    "start": "1407000",
    "end": "1834000"
  },
  {
    "text": "questions now it's the best [Applause]",
    "start": "1409400",
    "end": "1421720"
  },
  {
    "text": "time ah hi hello is Anderson here from Brazil",
    "start": "1421720",
    "end": "1428720"
  },
  {
    "text": "I'd like to know the the time frame since the the start of the planning until the last the migration of cluster",
    "start": "1428720",
    "end": "1435320"
  },
  {
    "text": "number 700 okay so the question was um the time",
    "start": "1435320",
    "end": "1440799"
  },
  {
    "text": "frame from starting the planning and then uh migrating to 700 clusters right",
    "start": "1440799",
    "end": "1446640"
  },
  {
    "text": "yes thank you okay I just answer um so initial planning was around I say the end of",
    "start": "1446640",
    "end": "1454559"
  },
  {
    "text": "2020 and then the complete 2021 year was uh used for migrating those clusters",
    "start": "1454559",
    "end": "1461360"
  },
  {
    "text": "because usually what we do is um we wait for a kubernetes release that we anyway",
    "start": "1461360",
    "end": "1468039"
  },
  {
    "text": "where we have anyway have to redeploy all the worker machines and use that time frame to also do the migration to",
    "start": "1468039",
    "end": "1474360"
  },
  {
    "text": "clust API so with maybe you saw it on the slides with 12 120 we migrated the",
    "start": "1474360",
    "end": "1480320"
  },
  {
    "text": "work noes and then with 121 we migrated the control planes so that's about it but when",
    "start": "1480320",
    "end": "1487240"
  },
  {
    "text": "actually migrating for 700 clusters we took maybe some two weeks maybe because",
    "start": "1487240",
    "end": "1493000"
  },
  {
    "text": "you know doing batch batch updates to those workload and control plane machines",
    "start": "1493000",
    "end": "1499480"
  },
  {
    "text": "that was also another question if we are doing the update batchwise or cluster by cluster um if we release a new",
    "start": "1499480",
    "end": "1508039"
  },
  {
    "text": "kubernetes version uh we have also migrated each step so during the Terra",
    "start": "1508039",
    "end": "1515200"
  },
  {
    "text": "form renaming we used a release for kubernetes release update to do this for",
    "start": "1515200",
    "end": "1520279"
  },
  {
    "text": "the step one for the infrastructure we have used a release so it took four releases to do the complete steps to be",
    "start": "1520279",
    "end": "1528440"
  },
  {
    "text": "well honest yeah but when uh when updating the Clusters itself doesn't matter with clust now with clust API um",
    "start": "1528440",
    "end": "1537000"
  },
  {
    "text": "the cluster API controller supports a machine concurrency and a cluster concurrency and actually a concurrency",
    "start": "1537000",
    "end": "1542120"
  },
  {
    "text": "for all objects and um this is usually set to 10 for us so uh if we have 200",
    "start": "1542120",
    "end": "1549679"
  },
  {
    "text": "clusters in our largest management cluster um yeah we'll just trigger all",
    "start": "1549679",
    "end": "1555159"
  },
  {
    "text": "of those 200 at once but clust API has like like a like this concurrency and",
    "start": "1555159",
    "end": "1561480"
  },
  {
    "text": "we'll just work on 10 uh objects at the same time there's another question uh one of",
    "start": "1561480",
    "end": "1568600"
  },
  {
    "text": "the slides at the beginning showed that there were five platform teams how many engineers in total taking care of the",
    "start": "1568600",
    "end": "1574399"
  },
  {
    "text": "platform in these 900 clusters so in general um as far as I know we are a platform team or the five platform teams",
    "start": "1574399",
    "end": "1581440"
  },
  {
    "text": "are 30 people or 40 um the core platform team for the cluster API migration",
    "start": "1581440",
    "end": "1589120"
  },
  {
    "text": "uh I think we are about eight six to eight people saw the migration of the",
    "start": "1589120",
    "end": "1594600"
  },
  {
    "text": "700 clusters to Cluster API have been done by eight",
    "start": "1594600",
    "end": "1600240"
  },
  {
    "text": "people cool",
    "start": "1600240",
    "end": "1603799"
  },
  {
    "text": "okay any further questions any other questions oh yeah",
    "start": "1605320",
    "end": "1611200"
  },
  {
    "text": "there we go",
    "start": "1611200",
    "end": "1614519"
  },
  {
    "text": "I have a question about the initial provisioning so you say you manage your clusters using cluster API but what",
    "start": "1619000",
    "end": "1625159"
  },
  {
    "text": "about management clusters right so let's imagine a situation when everything is burned down and you have to deploy the",
    "start": "1625159",
    "end": "1631520"
  },
  {
    "text": "management cluster how you going to deploy the first one um to provision a",
    "start": "1631520",
    "end": "1637720"
  },
  {
    "text": "management cluster we're using kind and then we are provisioning a new cluster",
    "start": "1637720",
    "end": "1643399"
  },
  {
    "text": "and we're doing a cluster cutle move to move all the management resources to the new cluster",
    "start": "1643399",
    "end": "1648799"
  },
  {
    "text": "so this is the initial cheing act problem step um and we are doing backups",
    "start": "1648799",
    "end": "1655039"
  },
  {
    "text": "with Valero so we can do a restore with all its resources and then it's done",
    "start": "1655039",
    "end": "1661679"
  },
  {
    "text": "does that answer your question okay nice perfect the technique which we used um",
    "start": "1661679",
    "end": "1667640"
  },
  {
    "text": "while migrating the cluster is is Canary blue green or something",
    "start": "1667640",
    "end": "1672760"
  },
  {
    "text": "else as far as I know we have using Canary updates Canary testing up yeah",
    "start": "1672760",
    "end": "1678679"
  },
  {
    "text": "Canary I would say m thank you yeah this is not not blue green",
    "start": "1678679",
    "end": "1684120"
  },
  {
    "text": "yeah okay okay so we got to have more couple questions thank you hi um did you have",
    "start": "1685039",
    "end": "1693440"
  },
  {
    "text": "by any chance uh bare metal uh servers anywhere and uh bare metal workload or",
    "start": "1693440",
    "end": "1698720"
  },
  {
    "text": "any plans for it on Capo we are not we are not using bare",
    "start": "1698720",
    "end": "1703799"
  },
  {
    "text": "metal servers um we have open sag is our layer we talk to beneath its virtual",
    "start": "1703799",
    "end": "1709720"
  },
  {
    "text": "machines um yeah and the next step is public clouds like asra AWS Google and",
    "start": "1709720",
    "end": "1716799"
  },
  {
    "text": "so on yeah okay I was thinking about ironic on pack did you ever try it or no we",
    "start": "1716799",
    "end": "1724399"
  },
  {
    "text": "haven't yet but definitely interesting yeah thank you sorry to",
    "start": "1724399",
    "end": "1730279"
  },
  {
    "text": "tell all right uh just one question in the back okay",
    "start": "1731120",
    "end": "1737360"
  },
  {
    "text": "I think I think you mentioned uh 200 cluster workload clusters per management",
    "start": "1741760",
    "end": "1747640"
  },
  {
    "text": "clusters why there's a limit is there a limit for how many clusters can be managed by the uh management cluster",
    "start": "1747640",
    "end": "1755279"
  },
  {
    "text": "there's there's not really a limit but we have U multiple private Cloud regions",
    "start": "1755279",
    "end": "1760360"
  },
  {
    "text": "or zones because of this we have per region or Zone a separate management",
    "start": "1760360",
    "end": "1765679"
  },
  {
    "text": "cluster and this is the reason so we we don't have more users on a single zone",
    "start": "1765679",
    "end": "1772399"
  },
  {
    "text": "uh right now but um I think it will it will scale pretty well um until a",
    "start": "1772399",
    "end": "1778919"
  },
  {
    "text": "certain point probably but we haven't reached that point",
    "start": "1778919",
    "end": "1783720"
  },
  {
    "text": "yet okay any more questions yeah we got a one more here okay hi um was this an unattended",
    "start": "1785519",
    "end": "1793240"
  },
  {
    "text": "migration and if yes um you could decide the user when he want to migrate the cluster",
    "start": "1793240",
    "end": "1799480"
  },
  {
    "text": "it was attended migration because we want to use uh Upstream features from",
    "start": "1799480",
    "end": "1804600"
  },
  {
    "text": "cluster API and what was the last question sorry",
    "start": "1804600",
    "end": "1810000"
  },
  {
    "text": "have you heard it yeah sure okay perfect",
    "start": "1810000",
    "end": "1816080"
  },
  {
    "text": "thanks all right uh any other",
    "start": "1816080",
    "end": "1820840"
  },
  {
    "text": "question okay I think so we done and thanks for great presentation so much thank you thank you",
    "start": "1821320",
    "end": "1829960"
  },
  {
    "text": "thank you Daniel yeah thanks and then thanks for attending once again and then hopefully",
    "start": "1830000",
    "end": "1836360"
  }
]