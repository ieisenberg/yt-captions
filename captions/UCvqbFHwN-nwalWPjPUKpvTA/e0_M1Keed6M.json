[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "hi my name is sagi volkov i work for redhat as a storage performance instigator",
    "start": "1360",
    "end": "7520"
  },
  {
    "text": "in the cloud storage and data services business unit welcome to my cubecon session whatever",
    "start": "7520",
    "end": "14080"
  },
  {
    "text": "can go wrong will go wrong ru concept and storage failures",
    "start": "14080",
    "end": "19520"
  },
  {
    "start": "25000",
    "end": "59000"
  },
  {
    "text": "in our session agenda i'll talk a little bit about um kind of a storage introduction um i'll",
    "start": "26320",
    "end": "33760"
  },
  {
    "text": "explain a few resiliency terms that are commonly used in the uh storage industry we'll talk about",
    "start": "33760",
    "end": "40719"
  },
  {
    "text": "cipher storage provider rook as a storage orchestrator or manager of fan storage",
    "start": "40719",
    "end": "46239"
  },
  {
    "text": "and we'll talk about failures um supposed to be a live demo it's going to be a recorded demo",
    "start": "46239",
    "end": "51760"
  },
  {
    "text": "and i'll leave time for questions in the",
    "start": "51760",
    "end": "56640"
  },
  {
    "text": "end so storage we all uh you need it we all",
    "start": "56840",
    "end": "63199"
  },
  {
    "start": "59000",
    "end": "161000"
  },
  {
    "text": "use it every day it might be ephemeral or it might be persistent uh of course the discussion",
    "start": "63199",
    "end": "68560"
  },
  {
    "text": "today is just on the persistent storage in a kubernetes let's do a little bit of an",
    "start": "68560",
    "end": "74400"
  },
  {
    "text": "overview of storage up until like 10 years ago what we used to",
    "start": "74400",
    "end": "79520"
  },
  {
    "text": "have is storage arrays this giant's giant bare metal arrays of many drives",
    "start": "79520",
    "end": "85360"
  },
  {
    "text": "in it and they would uh connect uh into bare metal nodes or or virtual machine",
    "start": "85360",
    "end": "93920"
  },
  {
    "text": "not a lot of dynamic in a location of a volumes and about 10 years ago",
    "start": "93920",
    "end": "100640"
  },
  {
    "text": "software defined storage or sds uh started to be a big we start to see the",
    "start": "100640",
    "end": "107520"
  },
  {
    "text": "uh the ability to have uh servers have uh the ability to run some sort of",
    "start": "107520",
    "end": "114479"
  },
  {
    "text": "a software that provides storage to a cluster of servers or set of servers",
    "start": "114479",
    "end": "120320"
  },
  {
    "text": "uh in a manner that it was still a stable resilient and redundant",
    "start": "120320",
    "end": "128239"
  },
  {
    "text": "software-defined storage is probably the most common storage used in kubernetes nowadays you",
    "start": "128239",
    "end": "135520"
  },
  {
    "text": "can use it in a converge manner where we have",
    "start": "135520",
    "end": "140720"
  },
  {
    "text": "our nodes running the software defined storage and our pods and you can also use it in",
    "start": "140720",
    "end": "147040"
  },
  {
    "text": "non-converge manner where we have some nodes running the pods the application ports and some nodes are just running",
    "start": "147040",
    "end": "153280"
  },
  {
    "text": "uh the software defined storage pods",
    "start": "153280",
    "end": "158480"
  },
  {
    "start": "161000",
    "end": "289000"
  },
  {
    "text": "so let's talk about a few terms that are widely used in the storage world of course they are also used in a lot of",
    "start": "162319",
    "end": "169519"
  },
  {
    "text": "processes in the technology world and outside of the technology world availability is basically your uptime",
    "start": "169519",
    "end": "177120"
  },
  {
    "text": "in the storage domain you're gonna hear a lot about the number of nines data",
    "start": "177120",
    "end": "182400"
  },
  {
    "text": "storage solution has that's basically",
    "start": "182400",
    "end": "189120"
  },
  {
    "text": "99.99 something nines which basically impact the",
    "start": "190280",
    "end": "195360"
  },
  {
    "text": "availability of your storage the number of nines after the dot uh six nines is equivalent",
    "start": "195360",
    "end": "202080"
  },
  {
    "text": "um to 30 once 31 seconds of downtime in one year five nines is about",
    "start": "202080",
    "end": "209040"
  },
  {
    "text": "five minutes four nines is about one hour and three nines about nine hours of downtime in",
    "start": "209040",
    "end": "216640"
  },
  {
    "text": "one year durability is uh like the name basically how durable",
    "start": "216640",
    "end": "223440"
  },
  {
    "text": "is your storage uh how good is it is in in in keeping the data the data might not be accessible all the",
    "start": "223440",
    "end": "230480"
  },
  {
    "text": "time but it is still intact so for example you lost a",
    "start": "230480",
    "end": "235840"
  },
  {
    "text": "a data center um the data still resides or you or you hope still resides on your",
    "start": "235840",
    "end": "244080"
  },
  {
    "text": "storage solution when the data setter comes back up the data is still there uh",
    "start": "244080",
    "end": "249519"
  },
  {
    "text": "reliability and resiliency two terms that we are going to be focusing uh",
    "start": "249519",
    "end": "255120"
  },
  {
    "text": "more here um basically a probability you know of how good you design your",
    "start": "255120",
    "end": "260799"
  },
  {
    "text": "storage solution and and basically it gets impact by",
    "start": "260799",
    "end": "265840"
  },
  {
    "text": "uh the ability of your storage solution to heal itself so again we lost a storage device or",
    "start": "265840",
    "end": "272240"
  },
  {
    "text": "maybe we lost a node with a few storage devices uh or a data center or a switch um it's",
    "start": "272240",
    "end": "279120"
  },
  {
    "text": "the ability for the storage solution to heal itself and to continue and provide",
    "start": "279120",
    "end": "285040"
  },
  {
    "text": "the services the data services that it needs to provide in this diagram i'm trying to kind of",
    "start": "285040",
    "end": "294000"
  },
  {
    "start": "289000",
    "end": "419000"
  },
  {
    "text": "explain these terms with a couple of processes of course we want availability and",
    "start": "294000",
    "end": "299440"
  },
  {
    "text": "durability all the time throughout all the processes we have process z that starts",
    "start": "299440",
    "end": "305520"
  },
  {
    "text": "and in the end of the process basically inserts each process inserts 10 rows into some",
    "start": "305520",
    "end": "311199"
  },
  {
    "text": "sort of a database if the process ran 100 times we're supposed to see in the end a thousand rows in the",
    "start": "311199",
    "end": "318479"
  },
  {
    "text": "database uh if we're seeing that that's a basically 100 availability if we're seeing 950 it's basically a 95",
    "start": "318479",
    "end": "326560"
  },
  {
    "text": "percent availability uh of course throughout the process once we insert the data",
    "start": "326560",
    "end": "332160"
  },
  {
    "text": "we want the data to be durable we always want to be able to query it uh some in some cases",
    "start": "332160",
    "end": "340639"
  },
  {
    "text": "in the storage domain there's a term called retention policy maybe you only need it for one night",
    "start": "340639",
    "end": "346160"
  },
  {
    "text": "maybe you only need the data for one year maybe just for 10 minutes but as long",
    "start": "346160",
    "end": "351440"
  },
  {
    "text": "as uh for the duration of your retention policy that you'd define for that a piece of data",
    "start": "351440",
    "end": "358479"
  },
  {
    "text": "uh you need to be able to uh to to have durability 100 durability",
    "start": "358479",
    "end": "363680"
  },
  {
    "text": "throughout this retention time and then you know once process z ends",
    "start": "363680",
    "end": "369360"
  },
  {
    "text": "process y starts to read input from process z and and and run as some other",
    "start": "369360",
    "end": "377120"
  },
  {
    "text": "processes as well throughout all this time reliability and resiliency",
    "start": "377120",
    "end": "382720"
  },
  {
    "text": "is a must again you lost the node you lost the device you lost the switch",
    "start": "382720",
    "end": "387919"
  },
  {
    "text": "we want to be able to continue and run process z and run process y and complete them",
    "start": "387919",
    "end": "394720"
  },
  {
    "text": "successfully that's basically how reliable our our stored solution",
    "start": "394720",
    "end": "400479"
  },
  {
    "text": "and even and if we are able to complete these processes while losing some uh",
    "start": "400479",
    "end": "408000"
  },
  {
    "text": "some sort of a data storage capabilities that shows up how good our resiliency is",
    "start": "408000",
    "end": "417440"
  },
  {
    "start": "419000",
    "end": "555000"
  },
  {
    "text": "so let's continue with a couple of more terms mttr or mean time to repair a very common",
    "start": "419360",
    "end": "427280"
  },
  {
    "text": "used terminology that is directly connected to your",
    "start": "427280",
    "end": "433520"
  },
  {
    "text": "resiliency you had a problem we talked about it you lost a storage device you know it's usually either a drive or",
    "start": "433520",
    "end": "440880"
  },
  {
    "text": "flash device or or an array of storage or switch",
    "start": "440880",
    "end": "447120"
  },
  {
    "text": "and how easy your storage solution is actually",
    "start": "447120",
    "end": "452479"
  },
  {
    "text": "repairing itself yeah when the storage defined software world um",
    "start": "452479",
    "end": "459759"
  },
  {
    "text": "you can sometimes define decides between having two copies of your data versus",
    "start": "459759",
    "end": "465440"
  },
  {
    "text": "three copies of the data for example if you have two copies of your data and for some reason",
    "start": "465440",
    "end": "470639"
  },
  {
    "text": "you lost some devices and you now only have valid um one copy of the data",
    "start": "470639",
    "end": "478240"
  },
  {
    "text": "until you actually repair and go back into 100 resiliency 100 two copies you are basically exposing",
    "start": "478240",
    "end": "486479"
  },
  {
    "text": "yourself uh to data unavailability if you lost more devices and now your only copy that",
    "start": "486479",
    "end": "494160"
  },
  {
    "text": "exists is also not available so mttr is very important for your resiliency",
    "start": "494160",
    "end": "502240"
  },
  {
    "text": "um in the sds world um we talked a little bit about converge",
    "start": "502240",
    "end": "508319"
  },
  {
    "text": "and infrastructure versus a non-converge converge can be",
    "start": "508319",
    "end": "515120"
  },
  {
    "text": "more more concerned in how you design and how you spread your data versus when",
    "start": "515120",
    "end": "521039"
  },
  {
    "text": "you separate actually compute in storage in nodes mesh of course especially in sds",
    "start": "521039",
    "end": "529680"
  },
  {
    "text": "is a big help software defined storage solutions like ceph thrive on mesh thrive on having",
    "start": "529680",
    "end": "537279"
  },
  {
    "text": "a lot of nodes with a lot of devices the more devices you have the faster you can actually recover from",
    "start": "537279",
    "end": "543519"
  },
  {
    "text": "a lost of a device",
    "start": "543519",
    "end": "549839"
  },
  {
    "start": "555000",
    "end": "641000"
  },
  {
    "text": "another term is mtbf meantime between failures uh basically everything comes",
    "start": "555920",
    "end": "562000"
  },
  {
    "text": "down into the quality of the devices that you use uh to store uh to store your data",
    "start": "562000",
    "end": "569839"
  },
  {
    "text": "um so this measurement is basically um the between time from the last",
    "start": "569839",
    "end": "575839"
  },
  {
    "text": "failure to the next failure of a device or or a storage system",
    "start": "575839",
    "end": "581040"
  },
  {
    "text": "uh just for you know example uh an enterprise grade drive have about 800 000",
    "start": "581040",
    "end": "588480"
  },
  {
    "text": "hours of mtbf which is roughly uh 90 years sounds a lot",
    "start": "588480",
    "end": "594800"
  },
  {
    "text": "but then if your storage solution have 90 drives in that example that basically means",
    "start": "594800",
    "end": "600800"
  },
  {
    "text": "that you're going to have one failure every year if your stored solution have or data center",
    "start": "600800",
    "end": "606399"
  },
  {
    "text": "uh have 900 drives basically means that you're going to have a a failure every",
    "start": "606399",
    "end": "611920"
  },
  {
    "text": "five weeks in one of the drives one of the 900 drives so you need a very good um",
    "start": "611920",
    "end": "619600"
  },
  {
    "text": "software above the devices to actually uh make sure that you still have",
    "start": "619600",
    "end": "626000"
  },
  {
    "text": "availability that's where the two copies of the data are three copies",
    "start": "626000",
    "end": "631920"
  },
  {
    "text": "erasure coding of spreading the data across all the devices uh that's where all these things come",
    "start": "631920",
    "end": "638560"
  },
  {
    "text": "into play so last couple of terms rto recovery",
    "start": "638560",
    "end": "644959"
  },
  {
    "start": "641000",
    "end": "727000"
  },
  {
    "text": "time objective basically how long can your process or your company or data center can survive without access to data",
    "start": "644959",
    "end": "652079"
  },
  {
    "text": "usually these your rto needs to be as short as possible",
    "start": "652079",
    "end": "657120"
  },
  {
    "text": "you define this usually by by tiers in in enterprise organization",
    "start": "657120",
    "end": "664399"
  },
  {
    "text": "some applications cannot have a no access for example",
    "start": "664399",
    "end": "671040"
  },
  {
    "text": "no more than five minutes in one year or 30 seconds in one year other",
    "start": "671040",
    "end": "677040"
  },
  {
    "text": "applications are in lower tiers and yeah it's fine for them to not have access to the data for you know",
    "start": "677040",
    "end": "682880"
  },
  {
    "text": "one day in a year rpo is your recovery point objective",
    "start": "682880",
    "end": "690240"
  },
  {
    "text": "besides counting on your storage solution to maintain all the copies and run all the",
    "start": "690240",
    "end": "696640"
  },
  {
    "text": "time and provide adequate performance you should always thrive to do also run backups outside of this uh",
    "start": "696640",
    "end": "703839"
  },
  {
    "text": "storage solution and this basically uh uh impact your uh rpo uh how many backups you have in case",
    "start": "703839",
    "end": "712000"
  },
  {
    "text": "you lost everything in your storage solution will basically impact your recovery point object",
    "start": "712000",
    "end": "717760"
  },
  {
    "text": "objective if you're on a backup every uh one hour that means that uh your rpo is of",
    "start": "717760",
    "end": "725200"
  },
  {
    "text": "one hour all right so now let's continue and",
    "start": "725200",
    "end": "730399"
  },
  {
    "start": "727000",
    "end": "766000"
  },
  {
    "text": "start to talk about seven rook um ceph is a unified storage system",
    "start": "730399",
    "end": "735839"
  },
  {
    "text": "uh software defined storage it's been around for a long time it provides basically the ability to",
    "start": "735839",
    "end": "742240"
  },
  {
    "text": "have object and block a and file uh file from sffs which is basically in",
    "start": "742240",
    "end": "749200"
  },
  {
    "text": "the kubernetes world will be the equivalent of ada rwx uh it's a shareable size",
    "start": "749200",
    "end": "754959"
  },
  {
    "text": "file system rbd is a a a pure block device and rgw is the",
    "start": "754959",
    "end": "761120"
  },
  {
    "text": "ability to interact with object using s3 for example",
    "start": "761120",
    "end": "766959"
  },
  {
    "start": "766000",
    "end": "887000"
  },
  {
    "text": "there are a few components uh for uh for ceph and the cephmon or the",
    "start": "766959",
    "end": "773279"
  },
  {
    "text": "monitor um is basically uh um the the coordination or took the",
    "start": "773279",
    "end": "780720"
  },
  {
    "text": "coordinator of uh all data and components in",
    "start": "780720",
    "end": "786160"
  },
  {
    "text": "in the cluster uh it's using a paxos to keep itself alive there's minimum of",
    "start": "786160",
    "end": "791680"
  },
  {
    "text": "three of these processes in a cluster and you can go up to seven the manager mainly concentrate on",
    "start": "791680",
    "end": "800000"
  },
  {
    "text": "on real-time metrics um and other management functions that are for",
    "start": "800000",
    "end": "806320"
  },
  {
    "text": "the cfa cluster there's one active and you can go to have one active and one standby sephora is the easy uh are very",
    "start": "806320",
    "end": "814399"
  },
  {
    "text": "important processes these are basically the uh i process that attached into a a a device",
    "start": "814399",
    "end": "822800"
  },
  {
    "text": "that can provide storage like an lgd sdd nvme any type of a block device",
    "start": "822800",
    "end": "829600"
  },
  {
    "text": "and then in return the ceph cluster aggregates all these devices create pools out of",
    "start": "829600",
    "end": "836000"
  },
  {
    "text": "them and able to provide uh the storage capabilities for their clients in our",
    "start": "836000",
    "end": "842160"
  },
  {
    "text": "case are kubernetes and pvcs whether it's an object a block or",
    "start": "842160",
    "end": "850639"
  },
  {
    "text": "or file a couple of other processes that are not",
    "start": "850639",
    "end": "855680"
  },
  {
    "text": "showing up here on the presentation rgw is basically what allows",
    "start": "855680",
    "end": "861040"
  },
  {
    "text": "uh for object storage access um usually starts with one process and if",
    "start": "861040",
    "end": "867839"
  },
  {
    "text": "you need to scale up to you know millions of accesses or millions of object accesses you can add more",
    "start": "867839",
    "end": "874000"
  },
  {
    "text": "mds is the ability basically to access sffs you start with two of these processes",
    "start": "874000",
    "end": "881120"
  },
  {
    "text": "one is active and one in a standby uh mode",
    "start": "881120",
    "end": "887040"
  },
  {
    "start": "887000",
    "end": "1004000"
  },
  {
    "text": "and now let's talk about uh our rook uh um the rook is a basically a storage",
    "start": "887120",
    "end": "893920"
  },
  {
    "text": "manager for kubernetes in our case we use it with ceph i like to call it the orchestrator of",
    "start": "893920",
    "end": "900959"
  },
  {
    "text": "all things as you can see on these slides anything blue is kind of more on the",
    "start": "900959",
    "end": "908160"
  },
  {
    "text": "rook level we have the rook and ceph operator support that runs in one of the nodes",
    "start": "908160",
    "end": "914320"
  },
  {
    "text": "um we have discovery pods that runs on any uh node that we would like to uh that",
    "start": "914320",
    "end": "920720"
  },
  {
    "text": "will provide storage that have like block devices that cephei the ceph cluster is going to uh to use",
    "start": "920720",
    "end": "928079"
  },
  {
    "text": "you can you configure you can configure these by uh labels and things and things like that the",
    "start": "928079",
    "end": "934320"
  },
  {
    "text": "cfcsi uh level are basically the attacher uh and detach attach pods",
    "start": "934320",
    "end": "942160"
  },
  {
    "text": "that exist on the in every nodes whether it's for a block or for cfs",
    "start": "942160",
    "end": "948560"
  },
  {
    "text": "um and then you have the seth demons that rook is controlling um of course",
    "start": "948560",
    "end": "956000"
  },
  {
    "text": "each of these things are are basically uh pods you have uh mon pods uh that runs",
    "start": "956000",
    "end": "962800"
  },
  {
    "text": "on uh you have three minimum of three m1 pods so on the next node uh that we don't uh",
    "start": "962800",
    "end": "968959"
  },
  {
    "text": "show up in this slide we don't necessarily have a mon pod um osd's",
    "start": "968959",
    "end": "974240"
  },
  {
    "text": "will actually uh we will have osds wherever we will have storage that we consume",
    "start": "974240",
    "end": "981519"
  },
  {
    "text": "on these nodes and we talked about rgw's we've talked about the manager having one of them",
    "start": "981519",
    "end": "986639"
  },
  {
    "text": "mds um there's a pod here for a mirroring of rbd to a different",
    "start": "986639",
    "end": "992560"
  },
  {
    "text": "rook surf cluster on a completely different kubernetes cluster and crash collector in case",
    "start": "992560",
    "end": "998639"
  },
  {
    "text": "things go bad and we need to uh collect all kinds of logs",
    "start": "998639",
    "end": "1004800"
  },
  {
    "start": "1004000",
    "end": "1115000"
  },
  {
    "text": "so let's talk about the rook and safe resiliency in kubernetes as discussed previously every ceph",
    "start": "1005360",
    "end": "1011759"
  },
  {
    "text": "process is basically a pod so we want to understand and what",
    "start": "1011759",
    "end": "1016959"
  },
  {
    "text": "happens when a pod fails going back to all the terminology that we use",
    "start": "1016959",
    "end": "1022240"
  },
  {
    "text": "of course this impacts our resiliency um so you know the mons are uh very",
    "start": "1022240",
    "end": "1028480"
  },
  {
    "text": "important processes that's why we have three of them that spread over uh",
    "start": "1028480",
    "end": "1033839"
  },
  {
    "text": "three nodes or more uh the mds which again provides access to uh cfs we have",
    "start": "1033839",
    "end": "1041360"
  },
  {
    "text": "a minimum of two uh one is in an active active mode and one in standby uh mode",
    "start": "1041360",
    "end": "1047199"
  },
  {
    "text": "um none of the mons or the mgl the manager processes uh of ceph are actually in the data path",
    "start": "1047199",
    "end": "1054160"
  },
  {
    "text": "when we uh access it using a a rbd for example uh",
    "start": "1054160",
    "end": "1061039"
  },
  {
    "text": "you know we talked about uh uh rpos and and and backup and and and durability",
    "start": "1061039",
    "end": "1069280"
  },
  {
    "text": "if you have two data centers you have two kubernetes uh uh clusters you can",
    "start": "1069280",
    "end": "1076400"
  },
  {
    "text": "use uh self built in replication uh such as the self-rbd mirroring to",
    "start": "1076400",
    "end": "1081840"
  },
  {
    "text": "basically mirror a pv a pv uh in a certain pool from one safe cluster in kubernetes cluster a",
    "start": "1081840",
    "end": "1090080"
  },
  {
    "text": "to a rook safe cluster in kubernetes cluster a b um ceph also offer uh the same",
    "start": "1090080",
    "end": "1098000"
  },
  {
    "text": "capabilities for uh object storage it is ga in the self project it's still uh not",
    "start": "1098000",
    "end": "1106000"
  },
  {
    "text": "a ga in rook it will be ga in the next",
    "start": "1106000",
    "end": "1111360"
  },
  {
    "text": "one or two releases so let's talk about um some demos that",
    "start": "1111360",
    "end": "1119039"
  },
  {
    "text": "i want to show we're going to look at a few scenarios um basically of osd pods",
    "start": "1119039",
    "end": "1126000"
  },
  {
    "text": "failures and how rook and chef behave what we're basically going to do is",
    "start": "1126000",
    "end": "1132559"
  },
  {
    "text": "constantly create stress on the storage solution in our case",
    "start": "1132559",
    "end": "1137919"
  },
  {
    "text": "rook and ceph i and then one scenario will be um when a a developer or an admin or",
    "start": "1137919",
    "end": "1146400"
  },
  {
    "text": "person delete by mistake in osd pod what happens what happens when you reboot a node or",
    "start": "1146400",
    "end": "1152880"
  },
  {
    "text": "have osds osd pods on it remember osd is the process that consume",
    "start": "1152880",
    "end": "1160640"
  },
  {
    "text": "a blocked device on a node and then uh is part of the whole cf a cluster",
    "start": "1160640",
    "end": "1169200"
  },
  {
    "text": "that then provides a storage to our kubernetes application we're also going",
    "start": "1169200",
    "end": "1174720"
  },
  {
    "text": "to look at what happens when we lose one of the devices that the nosd",
    "start": "1174720",
    "end": "1179840"
  },
  {
    "text": "pods uh is using um i've all this demo is running on aws very",
    "start": "1179840",
    "end": "1186880"
  },
  {
    "text": "small clusters like three masters it's gonna be three nodes that are going to run rook and safe and provide the",
    "start": "1186880",
    "end": "1193039"
  },
  {
    "text": "storage and three nodes that are going to run and applications uh that will consume and",
    "start": "1193039",
    "end": "1199200"
  },
  {
    "text": "stress the storage uh i'm using a a project named sherlock",
    "start": "1199200",
    "end": "1204320"
  },
  {
    "text": "which i started a couple of months ago it's basically a",
    "start": "1204320",
    "end": "1209520"
  },
  {
    "text": "project that aims to check and test and stress uh performance",
    "start": "1209520",
    "end": "1216880"
  },
  {
    "text": "on all sorts of stored solution in the kubernetes domain so you can run",
    "start": "1216880",
    "end": "1224240"
  },
  {
    "text": "it not only on rook and surf but on any other asds provider i also collect the statistical",
    "start": "1224240",
    "end": "1231840"
  },
  {
    "text": "information from the actual nodes that provide the storage and the nodes that are consumed",
    "start": "1231840",
    "end": "1238000"
  },
  {
    "text": "with storage so you can take a look at that as well and let's move into the demos",
    "start": "1238000",
    "end": "1246799"
  },
  {
    "start": "1247000",
    "end": "1602000"
  },
  {
    "text": "all right so let's start the first demo i've kind of split my uh terminal into",
    "start": "1247760",
    "end": "1254159"
  },
  {
    "text": "four sections hopefully it's clear on the top right uh section i have a ceph command",
    "start": "1254159",
    "end": "1262159"
  },
  {
    "text": "called zephosd3 that is constantly running using a watch um it",
    "start": "1262159",
    "end": "1268400"
  },
  {
    "text": "basically shows the ceph cluster osd3 as the command says we can see that",
    "start": "1268400",
    "end": "1275760"
  },
  {
    "text": "we have three nodes that provide the storage on each of the nodes we have uh two osds that",
    "start": "1275760",
    "end": "1284799"
  },
  {
    "text": "consume two ssd devices on them and provide it back into the",
    "start": "1284799",
    "end": "1290559"
  },
  {
    "text": "safe cluster on the top left i have a cube cuddle command looking at uh the pods that exist on the",
    "start": "1290559",
    "end": "1298480"
  },
  {
    "text": "rooks f namespace i am only grabbing for the",
    "start": "1298480",
    "end": "1303520"
  },
  {
    "text": "osd pods these are the ones that we are interested in as you can see we have",
    "start": "1303520",
    "end": "1310880"
  },
  {
    "text": "six osds on the top right and we have six osd pods on",
    "start": "1310880",
    "end": "1318080"
  },
  {
    "text": "the top left i have a project called mysql",
    "start": "1318080",
    "end": "1323280"
  },
  {
    "text": "it has 12 databases uh 12 mysql pods are running on these",
    "start": "1323280",
    "end": "1330840"
  },
  {
    "text": "three nodes each mysql database is",
    "start": "1330840",
    "end": "1337600"
  },
  {
    "text": "using 100 gigabyte pvc i also have 12 syspent jobs",
    "start": "1337600",
    "end": "1345679"
  },
  {
    "text": "suspect uh pods running uh on a on these 12 mysql databases and",
    "start": "1345679",
    "end": "1352720"
  },
  {
    "text": "um this is done of course using the sherlock github project that i've previously",
    "start": "1352720",
    "end": "1358640"
  },
  {
    "text": "mentioned i'm just going to pick one of these uh you know one of these nodes",
    "start": "1358640",
    "end": "1364000"
  },
  {
    "text": "one of these mysql jobs and i'm going to basically",
    "start": "1364000",
    "end": "1372159"
  },
  {
    "text": "constantly monitor the logs of this uh job as you can see it's a",
    "start": "1372159",
    "end": "1378559"
  },
  {
    "text": "typical uh suspension output every 10 seconds shows like the number of transactions",
    "start": "1378559",
    "end": "1384720"
  },
  {
    "text": "per second uh latency and things like that um",
    "start": "1384720",
    "end": "1391039"
  },
  {
    "text": "what i'm going to do is i'm going to pick one of these uh um basically uh",
    "start": "1391039",
    "end": "1397280"
  },
  {
    "text": "uh osd pods and um i will uh delete uh one of them",
    "start": "1397280",
    "end": "1404080"
  },
  {
    "text": "um in this case i'm just going to pick a osd 3 and",
    "start": "1404080",
    "end": "1410799"
  },
  {
    "text": "what is interesting to monitor is to look at",
    "start": "1410799",
    "end": "1418400"
  },
  {
    "text": "the top left of what happens to the pod i kill it if",
    "start": "1418400",
    "end": "1424320"
  },
  {
    "text": "a new one comes in and on the top right um since we are looking at the osd 3",
    "start": "1424320",
    "end": "1432480"
  },
  {
    "text": "that's osd 3 right here what is a what is happening to the the status of",
    "start": "1432480",
    "end": "1439120"
  },
  {
    "text": "that osd component is it staying up",
    "start": "1439120",
    "end": "1444400"
  },
  {
    "text": "for how long there's another uh command here uh from ceph that basically dumping all",
    "start": "1444400",
    "end": "1450720"
  },
  {
    "text": "the uh pg's or placement groups that and the cf cluster has",
    "start": "1450720",
    "end": "1456080"
  },
  {
    "text": "as you can see now we have 81 pages and they're all at the uh all at an active and clean if seth uh",
    "start": "1456080",
    "end": "1462640"
  },
  {
    "text": "uh see or uh uh that something is wrong with one of the osds uh maybe we need to uh recover one",
    "start": "1462640",
    "end": "1469760"
  },
  {
    "text": "of those these you're gonna see that on on this uh dump command all right so",
    "start": "1469760",
    "end": "1476480"
  },
  {
    "text": "let's uh dump let's delete the pod and see what's happening",
    "start": "1476480",
    "end": "1483278"
  },
  {
    "text": "so the pod is a deleted you can see it's terminated you can see that there's",
    "start": "1483840",
    "end": "1489520"
  },
  {
    "text": "already a new osd three pod started and it's already",
    "start": "1489520",
    "end": "1495039"
  },
  {
    "text": "up and running um osd 3 was marked here as down for uh uh you know one or two seconds",
    "start": "1495039",
    "end": "1503039"
  },
  {
    "text": "and as you can see from the uh dumping of the pages in here that seph",
    "start": "1503039",
    "end": "1509120"
  },
  {
    "text": "is uh uh working in making sure that the new osd pod that is using basically",
    "start": "1509120",
    "end": "1516400"
  },
  {
    "text": "the same device as previously has a all the right placement groups and",
    "start": "1516400",
    "end": "1524320"
  },
  {
    "text": "all the pages are clean and as you can see all the placement groups are clean",
    "start": "1524320",
    "end": "1530320"
  },
  {
    "text": "and as you can see everything went back into uh um uh normal state in in terms of uh how",
    "start": "1530320",
    "end": "1537919"
  },
  {
    "text": "cfa behaves you can also see how in the output of",
    "start": "1537919",
    "end": "1543279"
  },
  {
    "text": "sysbench um you can see that right here you can see how there was",
    "start": "1543279",
    "end": "1551120"
  },
  {
    "text": "a little bit of a drop in the iops uh that did",
    "start": "1551120",
    "end": "1557440"
  },
  {
    "text": "this cisband job was running and that is uh of course acceptable there was some",
    "start": "1557440",
    "end": "1563679"
  },
  {
    "text": "kind of uh um an eye operation that internally ceph",
    "start": "1563679",
    "end": "1569360"
  },
  {
    "text": "was doing this database uh might have been reading from uh placement groups that was",
    "start": "1569360",
    "end": "1575760"
  },
  {
    "text": "on the osd that i have deleted so for a brief second it had to get the",
    "start": "1575760",
    "end": "1582000"
  },
  {
    "text": "pages from um a different osd that's uh that's the job of the sf months to",
    "start": "1582000",
    "end": "1589679"
  },
  {
    "text": "basically tell all the clients uh to get these pages from a a different",
    "start": "1589679",
    "end": "1595520"
  },
  {
    "text": "location and that's the end of this uh first uh demo",
    "start": "1595520",
    "end": "1602960"
  },
  {
    "text": "all right so let's uh do our second demo same setup what i'm going to do is",
    "start": "1603360",
    "end": "1609120"
  },
  {
    "text": "actually now reboot one of the nodes uh that are running rook and ceph",
    "start": "1609120",
    "end": "1617520"
  },
  {
    "text": "so which basically means that uh since each node have two osds and we",
    "start": "1617520",
    "end": "1623279"
  },
  {
    "text": "have total of six osds we are basically taking down uh a third of our uh storage",
    "start": "1623279",
    "end": "1632000"
  },
  {
    "text": "um for uh for a brief time i'm gonna pick",
    "start": "1632000",
    "end": "1637360"
  },
  {
    "text": "this node let me start monitoring",
    "start": "1637440",
    "end": "1644000"
  },
  {
    "text": "one of the suspense pods",
    "start": "1644000",
    "end": "1655120"
  },
  {
    "text": "243 number eight",
    "start": "1655120",
    "end": "1660158"
  },
  {
    "text": "and i'm going to uh reboot uh this node and on the bottom",
    "start": "1662000",
    "end": "1668399"
  },
  {
    "text": "left i'm going to look at the uh the node status uh since this is aws",
    "start": "1668399",
    "end": "1675600"
  },
  {
    "text": "things are probably uh you know rebooting so fast i'm not sure how much of a",
    "start": "1675600",
    "end": "1682080"
  },
  {
    "text": "cube cattle is actually going to capture that the nodes are being down but as you can see on the bottom on the",
    "start": "1682080",
    "end": "1687279"
  },
  {
    "text": "top right ceph does it definitely starts to see some rpgs that are not available it's",
    "start": "1687279",
    "end": "1695200"
  },
  {
    "text": "marking the osds as down starting to",
    "start": "1695200",
    "end": "1700320"
  },
  {
    "text": "make sure that it's understand where it needs to move these",
    "start": "1700320",
    "end": "1705760"
  },
  {
    "text": "pages that were primary on these nodes and now on the top left you can see that we have two",
    "start": "1705760",
    "end": "1713279"
  },
  {
    "text": "new osd ports starting and once they are going to be",
    "start": "1713279",
    "end": "1718799"
  },
  {
    "text": "up and running on the top right we're going to see uh that um osd one and four",
    "start": "1718799",
    "end": "1725679"
  },
  {
    "text": "they are now marked as up and peering process is starting in ceph and it's starting to",
    "start": "1725679",
    "end": "1731600"
  },
  {
    "text": "move whatever pages it needs back into uh into those",
    "start": "1731600",
    "end": "1738000"
  },
  {
    "text": "uh new sds that just uh uh came up",
    "start": "1738000",
    "end": "1743600"
  },
  {
    "text": "now again this is a third of uh the storage and as you can see",
    "start": "1743760",
    "end": "1750080"
  },
  {
    "text": "um we are uh uh looking at the logs of uh sysbench um",
    "start": "1750080",
    "end": "1756720"
  },
  {
    "text": "there was a um a brief time that um we didn't see any uh transaction",
    "start": "1756720",
    "end": "1763039"
  },
  {
    "text": "transaction were were basically posed or have a higher latencies",
    "start": "1763039",
    "end": "1769039"
  },
  {
    "text": "but now everything continues so these were basically the two demos um",
    "start": "1769039",
    "end": "1777440"
  },
  {
    "start": "1773000",
    "end": "1812000"
  },
  {
    "text": "replacing new devices for osds is basically the same thing as the",
    "start": "1777440",
    "end": "1784640"
  },
  {
    "text": "last demo that i've shown ceph is going to",
    "start": "1784640",
    "end": "1790480"
  },
  {
    "text": "have these new devices part of the osds and then move all the placement groups",
    "start": "1790480",
    "end": "1797679"
  },
  {
    "text": "into or re-spread the placement groups using these devices as well i will now answer any questions that",
    "start": "1797679",
    "end": "1807039"
  },
  {
    "text": "you guys might have and thanks for attending my session",
    "start": "1807039",
    "end": "1814480"
  }
]