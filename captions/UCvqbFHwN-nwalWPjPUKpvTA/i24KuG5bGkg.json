[
  {
    "text": "okay I'd like to thank everyone joining",
    "start": "15530",
    "end": "21980"
  },
  {
    "text": "us here today welcome to today CN CF webinar product mm upgrades later",
    "start": "21980",
    "end": "28279"
  },
  {
    "text": "lessons from a year of managed Fornetti service my name is Arielle chatib I'm a",
    "start": "28279",
    "end": "34430"
  },
  {
    "text": "business development manager for cloud native technologies that happened also a CN CF ambassador I'll be moderating",
    "start": "34430",
    "end": "41059"
  },
  {
    "text": "today's webinar I'd like to welcome our presenter Adam Wolfe Gordon he's a",
    "start": "41059",
    "end": "46579"
  },
  {
    "text": "senior software engineer at digitalocean a few housekeeping items before we get",
    "start": "46579",
    "end": "53600"
  },
  {
    "text": "started during the webinar you're not going to be able to speak as an attendee there's a Q&A box at the bottom of your screen",
    "start": "53600",
    "end": "60350"
  },
  {
    "text": "please feel free to drop your questions in there and we'll get to as many of",
    "start": "60350",
    "end": "65360"
  },
  {
    "text": "those as we can at the end this is an official webinar of the CNC app and as such is subject to the CNC",
    "start": "65360",
    "end": "72320"
  },
  {
    "text": "apps code of conduct please do not have anything to the chat or questions that",
    "start": "72320",
    "end": "77390"
  },
  {
    "text": "would be in violation of that code basically be respectful of all your fellow participants and presenters",
    "start": "77390",
    "end": "83829"
  },
  {
    "text": "please note that this record a recording of this talk and the slides will be",
    "start": "83829",
    "end": "89329"
  },
  {
    "text": "posted later today at the CN CF webinar page at CNC F Donna",
    "start": "89329",
    "end": "95700"
  },
  {
    "text": "and I'll hand it over with that Adam to kick off today's presentation great",
    "start": "95700",
    "end": "102360"
  },
  {
    "text": "thanks Olympics hi everyone for coming today as if just heard I'm mad mouth grinning and I'm an",
    "start": "102360",
    "end": "107969"
  },
  {
    "text": "engineering digitalocean currently I'm the tech lead for our managed kubernetes and container registry products and I'm",
    "start": "107969",
    "end": "114840"
  },
  {
    "text": "going to talk to the Abel covariance upgrades and some of our experience with them I'm going to talk about how we do",
    "start": "114840",
    "end": "120689"
  },
  {
    "text": "upgrades and the things that we got right and wrong in that process but more importantly I want to talk about some",
    "start": "120689",
    "end": "126390"
  },
  {
    "text": "lessons that we've learned from doing upgrades for a year and these are",
    "start": "126390",
    "end": "131760"
  },
  {
    "text": "lessons for both cluster operators so people who are doing upgrades on Cabrini's clusters but also for",
    "start": "131760",
    "end": "137040"
  },
  {
    "text": "developers and others who are deploying workloads to kubernetes and these are things that will help your",
    "start": "137040",
    "end": "143730"
  },
  {
    "text": "upgrades go better make them easier and keep your workloads running as expected as your upgrade your cluster and I want",
    "start": "143730",
    "end": "151230"
  },
  {
    "text": "to start today with a little bit of background on kind of how this talk came to be so this talk really starts about a",
    "start": "151230",
    "end": "158640"
  },
  {
    "text": "year ago in Barcelona and there I am on the slide in Barcelona at tube con tu",
    "start": "158640",
    "end": "164370"
  },
  {
    "text": "2019 and in Barcelona we at digitalocean announced the general availability of our managed Korean product and if you",
    "start": "164370",
    "end": "171600"
  },
  {
    "text": "stuck by our lovely booth in Barcelona we probably told you about how it's now GA and you probably asked us what that",
    "start": "171600",
    "end": "179010"
  },
  {
    "text": "actually meant and we would have probably told you a bunch of things I'm",
    "start": "179010",
    "end": "184920"
  },
  {
    "text": "not going to go through all of the features the front because this isn't a marketing talk but the important one that I would tell you and that I",
    "start": "184920",
    "end": "191040"
  },
  {
    "text": "definitely would tell you about at the booth because it was something I worked on was that we had automated patch version upgrades this is a very exciting",
    "start": "191040",
    "end": "197100"
  },
  {
    "text": "new feature in our product the thing that we probably didn't tell you at the booth was that you couldn't actually",
    "start": "197100",
    "end": "203400"
  },
  {
    "text": "upgrade yet because we haven't enabled any upgrade paths for our customers we",
    "start": "203400",
    "end": "208859"
  },
  {
    "text": "had tested our great process a whole bunch I had run hundreds and hundreds of",
    "start": "208859",
    "end": "213870"
  },
  {
    "text": "upgrades on test clusters but if you went to your cluster page on digitalocean you would still see that",
    "start": "213870",
    "end": "219599"
  },
  {
    "text": "your cluster was up to date regardless of whether it actually was and the reason we hadn't enabled upgrades yet",
    "start": "219599",
    "end": "225030"
  },
  {
    "text": "was because our prior upgrade process had exposed to the full richness of customer",
    "start": "225030",
    "end": "231250"
  },
  {
    "text": "configurations and workloads that are possible in kubernetes and we were pretty sure that we were gonna find some",
    "start": "231250",
    "end": "236650"
  },
  {
    "text": "unexpected things when we turn them on for customers and we did not want to be in Barcelona enjoying Cuba and have to",
    "start": "236650",
    "end": "243430"
  },
  {
    "text": "deal with those things we want to wait till we were back at normal work so we waited a little bit to determine line so",
    "start": "243430",
    "end": "251860"
  },
  {
    "text": "as you can probably guess once we turn to hide upgrades for customers we learned a whole bunch of things and",
    "start": "251860",
    "end": "257010"
  },
  {
    "text": "that's why in this talk is lessons from a year of management upgrades and that's",
    "start": "257010",
    "end": "262870"
  },
  {
    "text": "really when I started thinking about giving this talk was when we turned upgrades on and started seeing what",
    "start": "262870",
    "end": "268000"
  },
  {
    "text": "happened when I wrote the proposal for this talk last year I in the end of 2019",
    "start": "268000",
    "end": "275110"
  },
  {
    "text": "I ran some numbers and I estimated that by the time coupon Amsterdam rolled around we were supposed to give",
    "start": "275110",
    "end": "281199"
  },
  {
    "text": "this talk we would have done about 20,000 upgrades and that's a really nice",
    "start": "281199",
    "end": "286419"
  },
  {
    "text": "big round number so I put it in the title of the talk in preparing for today during this webinar I ran the numbers",
    "start": "286419",
    "end": "293440"
  },
  {
    "text": "again and we actually accelerated our upgrades a little bit we've done more like thirty five thousand cards now and",
    "start": "293440",
    "end": "299380"
  },
  {
    "text": "that's in about a year so if you run the math that's about a hundred upgrades a day across thousands and thousands of",
    "start": "299380",
    "end": "305919"
  },
  {
    "text": "clusters so we've done a lot of upgrades and we have a pretty good set of data to",
    "start": "305919",
    "end": "311080"
  },
  {
    "text": "learn from we've seen a lot of possible things that can happen during an upgrade",
    "start": "311080",
    "end": "317460"
  },
  {
    "text": "so that leads to my favorite slide which is disclaimers I have two disclaimers",
    "start": "318060",
    "end": "323199"
  },
  {
    "text": "for everything that I'm going to say today first of all the lessons I'm going to talk about today are lessons from our",
    "start": "323199",
    "end": "330070"
  },
  {
    "text": "upgrade process at do and there are lots of different ways to upgrade kubernetes",
    "start": "330070",
    "end": "335080"
  },
  {
    "text": "I'm going to talk about some of the variations in how you can do upgrades",
    "start": "335080",
    "end": "340229"
  },
  {
    "text": "but depending on how you choose to do your upgrades you might see different things than we do and some of the things",
    "start": "340229",
    "end": "346630"
  },
  {
    "text": "I'll talk about today are going to be relevant if what you take away from this talk is that you want to do upgrades a",
    "start": "346630",
    "end": "353080"
  },
  {
    "text": "different way than we do upgrades because you don't want to skip the same things that we get that's a totally valid takeaway and I I don't want any",
    "start": "353080",
    "end": "361750"
  },
  {
    "text": "what we're doing is the right process for everyone the other disclaimer is that the lessons we've learned are from",
    "start": "361750",
    "end": "369280"
  },
  {
    "text": "upgrading our customers clusters and these their workloads are probably not the same as your our clothes their",
    "start": "369280",
    "end": "375490"
  },
  {
    "text": "workloads are not the same as our workloads internally which you'll also have experience with upgrading depending",
    "start": "375490",
    "end": "381700"
  },
  {
    "text": "on how your workloads work and how they're configured you might see different things during upgrades different problems different advantages",
    "start": "381700",
    "end": "390030"
  },
  {
    "text": "so let's start by talking about what you have to do when you want to upgrade a kubernetes cluster there are basically",
    "start": "390720",
    "end": "397840"
  },
  {
    "text": "two parts of granny's cluster there's the control plane which is up that's called the master and there are the",
    "start": "397840",
    "end": "403210"
  },
  {
    "text": "worker nodes so upgrading actually sounds like a very simple process and it's on one small slide first you",
    "start": "403210",
    "end": "410050"
  },
  {
    "text": "upgrade the control plane and then you upgrade the worker nodes and then you're",
    "start": "410050",
    "end": "415210"
  },
  {
    "text": "done that's it you've got pretty do Koreans cluster it sounds of course in",
    "start": "415210",
    "end": "420910"
  },
  {
    "text": "reality it's it's bigger than matter so here's an expanded view of what you have",
    "start": "420910",
    "end": "426310"
  },
  {
    "text": "to do this is probably still incomplete and it's kind of vary depending on your exact environment but when you're",
    "start": "426310",
    "end": "432220"
  },
  {
    "text": "upgrading the control planning you're upgrading a bunch of things and there's some ordering you have to be careful about although some of these steps can",
    "start": "432220",
    "end": "438190"
  },
  {
    "text": "be done in different orders so the first thing you're going to do is read through these notes for your new kubernetes",
    "start": "438190",
    "end": "443260"
  },
  {
    "text": "release figure out whether you're using anything that's deprecated in your current version and not going to be supported in the version you're",
    "start": "443260",
    "end": "448960"
  },
  {
    "text": "upgrading to and you're going to update those things if you need to so after getting any resources that are no longer supported in your cluster then you're",
    "start": "448960",
    "end": "456490"
  },
  {
    "text": "going to upgrade at CD if you need a new n CD then you can upgrade the actual control plan components you're gonna",
    "start": "456490",
    "end": "461680"
  },
  {
    "text": "upgrade your API server and your cute controller manager in the your cheap scheduler then you can upgrade your CI",
    "start": "461680",
    "end": "468010"
  },
  {
    "text": "plug-in for networking if you're using one and then you can upgrade any provider specific things so assuming",
    "start": "468010",
    "end": "474729"
  },
  {
    "text": "you're running in the cloud you're probably going to have a cloud controller manager and CSI controller maybe some other cloud specific or",
    "start": "474729",
    "end": "481030"
  },
  {
    "text": "provider specific pieces finally assuming that you're running things on the master as pods or static pods you're",
    "start": "481030",
    "end": "489460"
  },
  {
    "text": "going to create your cube lid on the master and your CTL master for those words once you've upgraded the",
    "start": "489460",
    "end": "496990"
  },
  {
    "text": "control plane you're an upgrade your worker nodes and this is a little bit simpler because the worker nodes don't run as much stuff but it also takes a",
    "start": "496990",
    "end": "503560"
  },
  {
    "text": "lot of coordination because the workloads are where your worker nodes are where your workloads actually run and your workloads are the things you",
    "start": "503560",
    "end": "509379"
  },
  {
    "text": "care about in your kubernetes cluster those are what you don't want to go down that's your business your applications so the first thing you're gonna do is",
    "start": "509379",
    "end": "516339"
  },
  {
    "text": "curtain and drain the worker node I still get all of the workloads off of it so that they're running somewhere else then you're known as empty you can",
    "start": "516339",
    "end": "523570"
  },
  {
    "text": "update the cubelet configuration and if there have been any changes that need to be made cubed configuration once that's",
    "start": "523570",
    "end": "530500"
  },
  {
    "text": "done you can operate the cubelet and you can uncor than the node let workload start being scheduled on it again and",
    "start": "530500",
    "end": "535570"
  },
  {
    "text": "I'm gonna rinse and repeat for each of the nodes in your cluster however big your cluster is you might do a few nodes",
    "start": "535570",
    "end": "540850"
  },
  {
    "text": "at a time if you've got capacity to at a time that can help speed up the process",
    "start": "540850",
    "end": "546329"
  },
  {
    "text": "assuming that you are running kubernetes on VMs and not on bare metal and we are",
    "start": "546810",
    "end": "552490"
  },
  {
    "text": "running on beams for our managed product there's a bit of a shortcut you can take",
    "start": "552490",
    "end": "557760"
  },
  {
    "text": "rather than upgrading each component individually in place on the nodes you can just completely replace each of the",
    "start": "557760",
    "end": "564399"
  },
  {
    "text": "nodes in the cluster so before you start you still need to do that initial step",
    "start": "564399",
    "end": "569529"
  },
  {
    "text": "of making sure that everything you're using is supported in your target version but once you've done that to upgrade the control plane you're going",
    "start": "569529",
    "end": "575440"
  },
  {
    "text": "to destroy your old control plane node and creating you and that as the new versions of everything this does assume",
    "start": "575440",
    "end": "581620"
  },
  {
    "text": "that your xev data is resilient to that so either you have multiple and cg nodes",
    "start": "581620",
    "end": "587910"
  },
  {
    "text": "and they can be rebuilt when you destroy one and create a new one or your entity",
    "start": "587910",
    "end": "593470"
  },
  {
    "text": "is outside your cluster or you're storing your NCD on some kind of persistent storage but assuming that",
    "start": "593470",
    "end": "599320"
  },
  {
    "text": "your SED data is safe you can blow away your control plane node create a brand new one has all versions of everything so that's",
    "start": "599320",
    "end": "605740"
  },
  {
    "text": "a much simpler process then trying to update each individual component of the control plan in place same thing for the",
    "start": "605740",
    "end": "612279"
  },
  {
    "text": "worker nodes you still need to do the draining but once you've drained a node you can destroy it create a brand new",
    "start": "612279",
    "end": "618279"
  },
  {
    "text": "node in its place then you're noticing to have the new versions everything you cubed configuration etc so if you've",
    "start": "618279",
    "end": "625720"
  },
  {
    "text": "worked with kubernetes fair if you've upgraded clusters before you can probably already see some potential",
    "start": "625720",
    "end": "631020"
  },
  {
    "text": "issues with doing an upgrade this way and there definitely are some issues and that's a--fun who spend a lot of time talking about today but there are also",
    "start": "631020",
    "end": "637470"
  },
  {
    "text": "some advantages and this is how we chose to implement upgrades for in our managed product for our customers clusters we do",
    "start": "637470",
    "end": "644760"
  },
  {
    "text": "full node or placement of each of the nodes in the cluster rather than upgrading things in place so the reasons",
    "start": "644760",
    "end": "652230"
  },
  {
    "text": "we chose to do it that way are that there are a bunch of advantages first",
    "start": "652230",
    "end": "657750"
  },
  {
    "text": "off if you upgrade by a node replacement then every node in the upgraded cluster is a clean slate there's no chance that",
    "start": "657750",
    "end": "664350"
  },
  {
    "text": "there was a customization made to that node that's going to persist across an upgrade and cause a problem in the new version there's you know exactly what to",
    "start": "664350",
    "end": "673200"
  },
  {
    "text": "expect on a node when it's been upgraded and this is particularly important if you're managing lots of clusters you",
    "start": "673200",
    "end": "679350"
  },
  {
    "text": "want to know exactly what's going to be there and you don't want to have to deal with a lot of the various customizations",
    "start": "679350",
    "end": "686190"
  },
  {
    "text": "you can make to a worker node you wanted to to really be predictable the other",
    "start": "686190",
    "end": "691740"
  },
  {
    "text": "nice thing about doing upgrades by a motor placement is that it's easier to automate there aren't that many steps",
    "start": "691740",
    "end": "697110"
  },
  {
    "text": "there are basically four operations in this process draining a node deleting a node creating a node and waiting for a",
    "start": "697110",
    "end": "703080"
  },
  {
    "text": "node to become ready if you've built automation for managing your clusters already for example automation to create",
    "start": "703080",
    "end": "708720"
  },
  {
    "text": "a cluster operation to an automation to do maintenance on a cluster you've probably already automated these",
    "start": "708720",
    "end": "714900"
  },
  {
    "text": "operations so automating your upgrades is just combining those primitives that you already have in the right order",
    "start": "714900",
    "end": "721550"
  },
  {
    "text": "finally this process works regardless of what kind of upgrade you're doing so you don't need to worry about whether a",
    "start": "721550",
    "end": "727470"
  },
  {
    "text": "particular update or upgrade requires a CNI upgrade or not whether quire's new at CD or not whether it's a minor",
    "start": "727470",
    "end": "735240"
  },
  {
    "text": "version upgrade or a patch version upgrade all of the upgrades to components that are going to happen are",
    "start": "735240",
    "end": "740310"
  },
  {
    "text": "encapsulated in that you're using so there's less variation between different",
    "start": "740310",
    "end": "745440"
  },
  {
    "text": "upgrades you're doing less version specific work to get ready for each upgrade I say it mostly works for all",
    "start": "745440",
    "end": "753540"
  },
  {
    "text": "these types because it's not always that tiny there are some situations where you do have to do really specific stuff",
    "start": "753540",
    "end": "760410"
  },
  {
    "text": "I'll talk about some of those later but in general this process does work the same for any kind of great so I said I",
    "start": "760410",
    "end": "771300"
  },
  {
    "text": "would talk about things we've got right and things would go wrong you know I have great process and I think this is the first thing that we really got right",
    "start": "771300",
    "end": "777149"
  },
  {
    "text": "was choosing to do our upgrades by replacing the codes in the monster it's a simpler process to understand that",
    "start": "777149",
    "end": "783029"
  },
  {
    "text": "upgrading in place it's easier to automate and it was a really good choice for us since we are managing thousands",
    "start": "783029",
    "end": "788250"
  },
  {
    "text": "of customer clusters where we don't control it workloads running on them it gives us a nice predictable process it's",
    "start": "788250",
    "end": "794009"
  },
  {
    "text": "easy to understand for the developers working on it if you're going to manage a lot of clusters and you're building",
    "start": "794009",
    "end": "799110"
  },
  {
    "text": "automation to manage a lot of clusters I recommend this as an approach to at least consider depending on your needs",
    "start": "799110",
    "end": "806628"
  },
  {
    "text": "of course there are some problems with this process and the basic problem is that it's a lot of change when you do an",
    "start": "807680",
    "end": "813870"
  },
  {
    "text": "upgrade you're totally replacing each node with a brand-new totally different builder so any custom configuration that you've",
    "start": "813870",
    "end": "820529"
  },
  {
    "text": "done on a node like changing the CTL values for example is going to be reset",
    "start": "820529",
    "end": "826040"
  },
  {
    "text": "when you do your upgrade and this bit some of our customers who are doing those kinds of customizations manually",
    "start": "826040",
    "end": "832550"
  },
  {
    "text": "when they didn't upgrade their worker nodes came back and they didn't have their customer figuration likewise at",
    "start": "832550",
    "end": "839009"
  },
  {
    "text": "least on our platform when you do an upgrade every node is going to have a new name in kubernetes it's gonna have a",
    "start": "839009",
    "end": "845009"
  },
  {
    "text": "new IP address and it's not going to have any labels or taints that are on the old node this it really bit some of",
    "start": "845009",
    "end": "851579"
  },
  {
    "text": "our customers who are scheduling there were closed directly based on node names or correctly based on labels or who were",
    "start": "851579",
    "end": "856920"
  },
  {
    "text": "directly accessing their nodes by IP rather than using our managed load balancer so this was a surprise for a",
    "start": "856920",
    "end": "864509"
  },
  {
    "text": "lot of our customers and we've had to work to fix a lot of those issues and make their own use cases work so some",
    "start": "864509",
    "end": "871380"
  },
  {
    "text": "lessons for coaster operators here are if you're doing upgrade by a node or placement it is helpful to your users to",
    "start": "871380",
    "end": "879149"
  },
  {
    "text": "reuse node names and IP addresses when you replace nodes if that's possible workloads probably shouldn't expect that",
    "start": "879149",
    "end": "886230"
  },
  {
    "text": "that's going to happen it's not great to expect that a node name is going to persist forever in communities but",
    "start": "886230",
    "end": "891360"
  },
  {
    "text": "scheduling by node name is a tool that you can use and someone's use it so if you can make that work it",
    "start": "891360",
    "end": "896610"
  },
  {
    "text": "will reduce some problems regardless of whether you are going to do that or able to do that you definitely want to make",
    "start": "896610",
    "end": "902879"
  },
  {
    "text": "sure that you're retaining labels and taints in some way people who are deploying workloads to kubernetes do",
    "start": "902879",
    "end": "909300"
  },
  {
    "text": "want some level of control over how their schedule in which nodes are scheduled on and labels and tanks are",
    "start": "909300",
    "end": "914309"
  },
  {
    "text": "the right tools to use for that in career days so providing some way to set",
    "start": "914309",
    "end": "919649"
  },
  {
    "text": "persistent labels and taints that will survive an upgrade is an important thing when you're building finally and kind of",
    "start": "919649",
    "end": "927870"
  },
  {
    "text": "like line is providing a good ingress or balancing solution that works with your clusters is important getting traffic",
    "start": "927870",
    "end": "934379"
  },
  {
    "text": "into communities cluster is actually kind of tricky and that's where the whole talk on its own that I'm not going to give today but almost everyone needs",
    "start": "934379",
    "end": "942120"
  },
  {
    "text": "to do it you usually have some kind of traffic coming into your the workload you're running in kubernetes the easier",
    "start": "942120",
    "end": "947189"
  },
  {
    "text": "you make it to for people to do that the less likely people are to build their own solution for it and end up relying",
    "start": "947189",
    "end": "954059"
  },
  {
    "text": "on node IP addresses or names those are the things that can cause problems during an upgrade or any other kind of",
    "start": "954059",
    "end": "959970"
  },
  {
    "text": "maintenance let me start deleting those some lessons for developers here are these are things",
    "start": "959970",
    "end": "968999"
  },
  {
    "text": "that will really help make your workloads more resilient upgrades and other kinds of motor placement so first",
    "start": "968999",
    "end": "974519"
  },
  {
    "text": "off if you need to customize things about a node like this is CTL values that I mentioned your best to use",
    "start": "974519",
    "end": "980120"
  },
  {
    "text": "kubernetes primitives to do that two good ways to do that are either using a privileged daemon set that's going to",
    "start": "980120",
    "end": "986819"
  },
  {
    "text": "run on every node make the customization that you need to make or using an init",
    "start": "986819",
    "end": "991920"
  },
  {
    "text": "container as part of a workload that requires a customization either way what you're going to end up with is something",
    "start": "991920",
    "end": "997740"
  },
  {
    "text": "that gets scheduled by the kubernetes scheduler on each node that needs the customization it makes that",
    "start": "997740",
    "end": "1003110"
  },
  {
    "text": "customization so that you're not doing it manually and that way if your node goes away or new and gets created it's",
    "start": "1003110",
    "end": "1008269"
  },
  {
    "text": "going to get the application you need secondly don't use node names for scheduling I mentioned you can do it but",
    "start": "1008269",
    "end": "1014959"
  },
  {
    "text": "it really is not a good idea the crew Bernays philosophies that nodes are livestock not pets nodes are going to go away at some point",
    "start": "1014959",
    "end": "1022009"
  },
  {
    "text": "they will date if you're doing that crazy we are they'll go away during upgrade but they may go away in other time",
    "start": "1022009",
    "end": "1027920"
  },
  {
    "text": "for maintenance or because of hardware failure or whatever if you want some control over scheduling you're much",
    "start": "1027920",
    "end": "1034459"
  },
  {
    "text": "better off using labels and tapes and learning how to set those in a persistent way in your environment on some providers or if you're managing",
    "start": "1034460",
    "end": "1041240"
  },
  {
    "text": "your own cluster that's going to mean just setting the labels or setting the taint on the mill directly through entities on other platforms like our",
    "start": "1041240",
    "end": "1047959"
  },
  {
    "text": "miners platform you have to create a node pool or some other extraction or configure a label in the management layer so that gets applied to you so",
    "start": "1047960",
    "end": "1056210"
  },
  {
    "text": "read your providers docs if you're using a manager or a service or ask your cluster operator if you're not managing",
    "start": "1056210",
    "end": "1062360"
  },
  {
    "text": "your own Carre's cluster make sure you understand what happens when a node goes away it gets replaced and how to get",
    "start": "1062360",
    "end": "1068480"
  },
  {
    "text": "labels set appropriately likewise I you are always best off to use supportive",
    "start": "1068480",
    "end": "1074510"
  },
  {
    "text": "ingress or load balancing service that's provided by your cluster quieter if you can that's going to make sure that",
    "start": "1074510",
    "end": "1080240"
  },
  {
    "text": "traffic keeps getting to your nodes when they're replaced it's gonna make sure that your traffic keeps going through during an upgrade and that's a good best",
    "start": "1080240",
    "end": "1087740"
  },
  {
    "text": "practice to follow there are always going to be used cases where it doesn't work and you need to build your own thing then there are totally valid",
    "start": "1087740",
    "end": "1093920"
  },
  {
    "text": "reasons to do that but I would say take that as a last resort try not to point things directly at",
    "start": "1093920",
    "end": "1100460"
  },
  {
    "text": "Granny's no one's trying these services load balancers etc so there are some",
    "start": "1100460",
    "end": "1107750"
  },
  {
    "text": "things that we got definitely wrong in our app great process and I want to talk about a couple of those now the first",
    "start": "1107750",
    "end": "1112850"
  },
  {
    "text": "big one is that we implemented our motor placement process in exactly the way that I described earlier which is break",
    "start": "1112850",
    "end": "1118940"
  },
  {
    "text": "before make so we drain a node and delete that node and then we provision a replacement for it and we did it this",
    "start": "1118940",
    "end": "1125660"
  },
  {
    "text": "way for some reasons specific to how our product works internally but it really causes trouble and this is actually",
    "start": "1125660",
    "end": "1131210"
  },
  {
    "text": "something we're working on fixing right now in our product to make things better for our customers there are a few basic",
    "start": "1131210",
    "end": "1139640"
  },
  {
    "text": "problems that this causes they're all basically related to two draining nodes",
    "start": "1139640",
    "end": "1145460"
  },
  {
    "text": "so first off if a customer or users running right at the limits of their cluster their cluster is basically full",
    "start": "1145460",
    "end": "1152000"
  },
  {
    "text": "to capacity then it might not always be possible to drain in nodes worth of workloads to another node there might",
    "start": "1152000",
    "end": "1158930"
  },
  {
    "text": "not even be another node we do have some use who have single milk clusters hopefully they're not using them for production",
    "start": "1158930",
    "end": "1164230"
  },
  {
    "text": "workloads but they do exist and so if we try and drain their single worker no",
    "start": "1164230",
    "end": "1169420"
  },
  {
    "text": "there's just nowhere for those workloads to go they're gonna go down either way because the pasady or because you've",
    "start": "1169420",
    "end": "1175450"
  },
  {
    "text": "decided to have a single cluster you're gonna end up with downtime for your workloads if they can't be drained to",
    "start": "1175450",
    "end": "1180640"
  },
  {
    "text": "summer and regardless of those issues even if you don't have capacity issues",
    "start": "1180640",
    "end": "1186580"
  },
  {
    "text": "at all another issue with break before make is just extra churn that causes for workloads when you drain the first node",
    "start": "1186580",
    "end": "1193360"
  },
  {
    "text": "in a cluster in our scheme the workloads that are running on it or guaranteed to end up on another node that's still",
    "start": "1193360",
    "end": "1199360"
  },
  {
    "text": "running the old version and that node is gonna have to be drained and replaced right away as well so ever you those",
    "start": "1199360",
    "end": "1205809"
  },
  {
    "text": "workloads are going to be drained or a bit twice instead of just once and that's a just a little bit of extra time",
    "start": "1205809",
    "end": "1212290"
  },
  {
    "text": "and extra chance for things to go wrong not great for for the workloads so",
    "start": "1212290",
    "end": "1219340"
  },
  {
    "text": "listen for operators here is pretty simple if you're going to do upgrades by Notre place in that way we do its best",
    "start": "1219340",
    "end": "1225340"
  },
  {
    "text": "to figure out a way to create the new nodes before you delete the old ones this might be a little bit more complicated well complicated to automate like it is",
    "start": "1225340",
    "end": "1231910"
  },
  {
    "text": "for us for various reasons but it's really a much better experience and I would if we could go back in time this",
    "start": "1231910",
    "end": "1238270"
  },
  {
    "text": "is how I would we build our great process if you really can't do that for",
    "start": "1238270",
    "end": "1243490"
  },
  {
    "text": "example if you were running a herbert cluster then you can't really add nodes before you turn in notes you might want",
    "start": "1243490",
    "end": "1249610"
  },
  {
    "text": "to consider reserving some capacity for upgrades so having a node that's not usually scheduled that you enable during",
    "start": "1249610",
    "end": "1254800"
  },
  {
    "text": "an upgrade that gives you just somewhere for locals to drain to if you're near capacity that might be kind of expensive",
    "start": "1254800",
    "end": "1261550"
  },
  {
    "text": "but it might CDX is long for developers the lesson here isn't really specific to",
    "start": "1261550",
    "end": "1268210"
  },
  {
    "text": "upgrades it's just that your kubernetes platform is eventually going to lose a",
    "start": "1268210",
    "end": "1274960"
  },
  {
    "text": "node I've noticed going to have to be great for an upgrade or for maintenance or for some other reason so leave some",
    "start": "1274960",
    "end": "1280600"
  },
  {
    "text": "capacity make sure that at least one node worth of workload can be drained to somewhere there's somewhere for it to go",
    "start": "1280600",
    "end": "1287470"
  },
  {
    "text": "what I know it needs to go away that's a just a good practice to keep your workloads running smoothly through not",
    "start": "1287470",
    "end": "1293500"
  },
  {
    "text": "only upgrades but also failures and other kinds of operations a",
    "start": "1293500",
    "end": "1299040"
  },
  {
    "text": "related thing that we got wrong was that we replaced nodes exactly one by one so",
    "start": "1301740",
    "end": "1306790"
  },
  {
    "text": "we destroy one note create one no destroy one note create one note till they're all replaced and this is just",
    "start": "1306790",
    "end": "1312220"
  },
  {
    "text": "fine for a three node cluster or a 5 node cluster it's not great for a 300 mil Custer because it just takes a long",
    "start": "1312220",
    "end": "1318220"
  },
  {
    "text": "time and it gets really bad if you have a big cluster and the workloads don't",
    "start": "1318220",
    "end": "1323530"
  },
  {
    "text": "evict quickly they don't see your no it doesn't get drained quickly we end up hitting a drain time oh and we had",
    "start": "1323530",
    "end": "1328990"
  },
  {
    "text": "initially sent our time out for drains to an hour and then we scaled it back to 15 minutes because we decided our",
    "start": "1328990",
    "end": "1335500"
  },
  {
    "text": "workload should not need an hour to be evicted but even on say a 20 node cluster which is a you know very common",
    "start": "1335500",
    "end": "1341530"
  },
  {
    "text": "size of group Nettie's cluster if you take 15 minutes to drain each node that's 5 hours just of draining so your",
    "start": "1341530",
    "end": "1346660"
  },
  {
    "text": "upgrades gonna take more than 5 hours and that's a long time to be waiting for your cluster to do an upgrade to restate",
    "start": "1346660",
    "end": "1356140"
  },
  {
    "text": "that a little bit more concisely replacing those one by one is just slow and it can be in the floor if you have",
    "start": "1356140",
    "end": "1361570"
  },
  {
    "text": "workloads that get stuck so upgrades can only be so fast there and takes some time we want to make them as expedient",
    "start": "1361570",
    "end": "1367630"
  },
  {
    "text": "as possible most users of kubernetes are going to want to sort of watch their",
    "start": "1367630",
    "end": "1373180"
  },
  {
    "text": "upgrades or keep an eye on their cluster doing an upgrade to make sure nothing was wrong because it is a very disruptive operation and you don't want",
    "start": "1373180",
    "end": "1379600"
  },
  {
    "text": "to leave them you know watching a cluster upgrade for 5 hours or 12 hours you're something you want to make it as",
    "start": "1379600",
    "end": "1385360"
  },
  {
    "text": "fast as you you're really so for operators the lesson is really simple",
    "start": "1385360",
    "end": "1390870"
  },
  {
    "text": "replace multiple nodes at once if you can that will just help you out great a",
    "start": "1390870",
    "end": "1396040"
  },
  {
    "text": "big cluster quickly this kind of requires that you do make before break not what we did break before make",
    "start": "1396040",
    "end": "1401290"
  },
  {
    "text": "so users you know may have capacity in their cluster to absorb one node we're",
    "start": "1401290",
    "end": "1406780"
  },
  {
    "text": "kind of worth of workload when you need to drain a node they probably haven't set aside like ten nodes at capacity if",
    "start": "1406780",
    "end": "1412270"
  },
  {
    "text": "you're going to drain 10 notes at a time and replace them so you can't have to do",
    "start": "1412270",
    "end": "1417640"
  },
  {
    "text": "the Femi before breaking loads the other thing is set reasonable drain timeouts",
    "start": "1417640",
    "end": "1423190"
  },
  {
    "text": "don't wait forever for a node to drain because sometimes endo is just not so workloads don't drain instantly it",
    "start": "1423190",
    "end": "1430060"
  },
  {
    "text": "takes some time for a process to respond to a signal and be evicted but it also shouldn't need an hour if you set a good",
    "start": "1430060",
    "end": "1436270"
  },
  {
    "text": "time out and like I said I think our current time those 15 minutes that's gonna help make sure that you're you at",
    "start": "1436270",
    "end": "1442330"
  },
  {
    "text": "least have an upper bound on how long it takes to replace a node and that upper bound is somewhat reasonable for",
    "start": "1442330",
    "end": "1450610"
  },
  {
    "text": "developers there's not a lot you can do about how your cluster operator provider replaces your nodes or how they do",
    "start": "1450610",
    "end": "1456760"
  },
  {
    "text": "upgrades but you can't help with the draining aspect there are two aspects to this you want to make sure that your",
    "start": "1456760",
    "end": "1462730"
  },
  {
    "text": "workloads can be evicted safely so use pod disruption budgets and other mechanisms and kubernetes to make sure",
    "start": "1462730",
    "end": "1468160"
  },
  {
    "text": "that enough pods of your workload stay up all the time the other piece is try",
    "start": "1468160",
    "end": "1473440"
  },
  {
    "text": "to make sure that you can be victim quickly so respond to signals appropriately try and make sure that your application has chats down quickly",
    "start": "1473440",
    "end": "1479290"
  },
  {
    "text": "and safely so that when no does get drained for whatever reason including an upgrade it's a happy process of gas",
    "start": "1479290",
    "end": "1487000"
  },
  {
    "text": "prices and you should test this it's really great to try draining a node with",
    "start": "1487000",
    "end": "1493120"
  },
  {
    "text": "your workloads running on it and make sure that it doesn't cause any problems for your application make sure that it drains nicely eventually and nobody's",
    "start": "1493120",
    "end": "1500380"
  },
  {
    "text": "going to get drained if you are doing an upgrade no matter how you do your upgrade you are going to have to drain those so it's really an eventualities",
    "start": "1500380",
    "end": "1507580"
  },
  {
    "text": "you want to make sure that your workload in your application can handle back to",
    "start": "1507580",
    "end": "1514360"
  },
  {
    "text": "the sort of positive side of things this is something that we got wrong but we were very happy we got it wrong I",
    "start": "1514360",
    "end": "1520170"
  },
  {
    "text": "earlier that when we did our GA we only offered automated patch version upgrades so for example that would be 114 1 to",
    "start": "1520170",
    "end": "1527140"
  },
  {
    "text": "114 - but not 114 2 to 115 0 which is a minor version we started out with",
    "start": "1527140",
    "end": "1533740"
  },
  {
    "text": "patchwork not creates because they're a bit simpler resources and communities aren't supposed to change between patch versions so everything should basically",
    "start": "1533740",
    "end": "1539910"
  },
  {
    "text": "continue working in your cluster when you do a patch version upgrade without any changes to the stuff you have to",
    "start": "1539910",
    "end": "1545290"
  },
  {
    "text": "play it was a good idea for us to start that way they are simpler and we learned",
    "start": "1545290",
    "end": "1550390"
  },
  {
    "text": "some things that made our lives easier when we got to doing my her version upgrades on the other hand when we",
    "start": "1550390",
    "end": "1555550"
  },
  {
    "text": "started testing minor version upgrades we found they mostly just worked and we probably worried about too much about",
    "start": "1555550",
    "end": "1560769"
  },
  {
    "text": "them we did longer than we needed to to implement them the same basic process",
    "start": "1560769",
    "end": "1566110"
  },
  {
    "text": "that we used for patch version that creates works for minor version upgrades as well they're just easier than we expected in general there was very",
    "start": "1566110",
    "end": "1572260"
  },
  {
    "text": "little that we had to fix or two first versions work I will talk about some of",
    "start": "1572260",
    "end": "1578049"
  },
  {
    "text": "the things that we found we had to do specifically for certain versions the",
    "start": "1578049",
    "end": "1584740"
  },
  {
    "text": "listen here for operators is really simple just just don't worry so much little minor version upgrades it turns out that all upgrades are disruptive and",
    "start": "1584740",
    "end": "1591820"
  },
  {
    "text": "minor versions aren't that much more disruptive than patch version upgrades they're probably less scary than You're",
    "start": "1591820",
    "end": "1597039"
  },
  {
    "text": "Expecting one decision we did make that",
    "start": "1597039",
    "end": "1602289"
  },
  {
    "text": "really helped the minor version story go well was that we leave most kerbin Inez",
    "start": "1602289",
    "end": "1607690"
  },
  {
    "text": "alpha features disabled and they're disabled by default and we don't change that configuration alpha features are",
    "start": "1607690",
    "end": "1614830"
  },
  {
    "text": "the things that are most likely to change or be deprecated between releases so if you leave them disabled there's just a whole class of problems that's",
    "start": "1614830",
    "end": "1621580"
  },
  {
    "text": "not you're not gonna have to worry about specifically you're not gonna have to worry about changing things that are",
    "start": "1621580",
    "end": "1627279"
  },
  {
    "text": "using alpha features to make them work in the new version the lesson for",
    "start": "1627279",
    "end": "1636010"
  },
  {
    "text": "operators here is I've been pretty simple I would recommend leaving alpha features off by default they are much",
    "start": "1636010",
    "end": "1642010"
  },
  {
    "text": "more likely to break between releases like I said you're gonna just eliminate a class of problems by leaving them",
    "start": "1642010",
    "end": "1647769"
  },
  {
    "text": "disabled if you or your users do have a reason to use an alpha feature just",
    "start": "1647769",
    "end": "1653350"
  },
  {
    "text": "consider it as a trade-off there's value to using the feature it's also potentially going to cause pain at",
    "start": "1653350",
    "end": "1659559"
  },
  {
    "text": "upgrade time it's something you're going to have to think about there is one alpha feature that we did enable which",
    "start": "1659559",
    "end": "1664779"
  },
  {
    "text": "is CSI snapshots we felt that offered a lot of value for our users it's something they requested so we did",
    "start": "1664779",
    "end": "1670929"
  },
  {
    "text": "enable it in our clusters and we're actually doing work right now to migrate away from those alpha snapshots and it",
    "start": "1670929",
    "end": "1677200"
  },
  {
    "text": "is one extra piece we're gonna have to take care of in a future minor version upgrade to make sure that we migrate",
    "start": "1677200",
    "end": "1682630"
  },
  {
    "text": "from the alpha version of that to the beta version for developers I think the",
    "start": "1682630",
    "end": "1689559"
  },
  {
    "text": "lesson is similar this is something you have a lot of control over regard whether alpha features are enabled you",
    "start": "1689559",
    "end": "1694720"
  },
  {
    "text": "can decide whether you use them or not I would be I sort of reluctant to use them",
    "start": "1694720",
    "end": "1700900"
  },
  {
    "text": "if you can avoid it use them this kind of a last resort if you do need to use one beat extra vigilant around upgrades",
    "start": "1700900",
    "end": "1706870"
  },
  {
    "text": "look at the release notes make sure that you know when your alpha feature is becoming beta or as a breaking change",
    "start": "1706870",
    "end": "1712200"
  },
  {
    "text": "and try and make sure that your usage is compatible with the next release before you do an upgrade just so that you don't",
    "start": "1712200",
    "end": "1717940"
  },
  {
    "text": "have any surprises and you're not counting on process the yield control which is maybe the upgrade process to",
    "start": "1717940",
    "end": "1723850"
  },
  {
    "text": "take care of it for you I want it's been",
    "start": "1723850",
    "end": "1729460"
  },
  {
    "text": "basically the rest of this talk today talking about two common classes of problems that we've seen with upgrades",
    "start": "1729460",
    "end": "1735760"
  },
  {
    "text": "the first one is issues with container storage interface or CSI a component in",
    "start": "1735760",
    "end": "1741340"
  },
  {
    "text": "Carini's so for those of you who maybe aren't familiar with it CSI is a pluggable way to provide storage to",
    "start": "1741340",
    "end": "1747070"
  },
  {
    "text": "containers it's an abstraction layer between kubernetes or other orchestrators it's a Orchestrator",
    "start": "1747070",
    "end": "1752200"
  },
  {
    "text": "recognized tech framework but it's an abstraction layer that that allows you to present storage to containers for",
    "start": "1752200",
    "end": "1758200"
  },
  {
    "text": "them to use in a sort of contracted way so that you're not building it directly to Cooper days.i",
    "start": "1758200",
    "end": "1764470"
  },
  {
    "text": "crannies clusters on digitalocean whether they're using our managed offering or managed by a customer are",
    "start": "1764470",
    "end": "1771420"
  },
  {
    "text": "able to use our open source C as I plug in to attach our persistent block storage to their workloads and this is",
    "start": "1771420",
    "end": "1778120"
  },
  {
    "text": "the mechanism we recommend for any user that needs persistence in their grades on dissolution because you're your CSI",
    "start": "1778120",
    "end": "1786190"
  },
  {
    "text": "volumes are completely outside of your cluster they're going to survive an upgrade they can survive your cluster be deleted it Sarah so we've seen a few",
    "start": "1786190",
    "end": "1792940"
  },
  {
    "text": "different roblems in CSI and I'll talk about a couple of them specifically as they relate to upgrades the first issue",
    "start": "1792940",
    "end": "1801070"
  },
  {
    "text": "that we see Nancy's eyes just that it was generally immature when we started using it the first release where we",
    "start": "1801070",
    "end": "1807850"
  },
  {
    "text": "supported upgrades in Kerberos was Gray's 1.10 and that was the same release for a CSI was promoted from",
    "start": "1807850",
    "end": "1814060"
  },
  {
    "text": "alpha to beta so in that 1.10 time frame the careerist components that support CSI were relatively new and most of the",
    "start": "1814060",
    "end": "1821830"
  },
  {
    "text": "CSI drivers including our square relatively new so unsurprisingly some bugs in both of those things",
    "start": "1821830",
    "end": "1828170"
  },
  {
    "text": "upgrading a cluster like we've talked about a lot requires draining the nodes in the cluster and when you drain a node",
    "start": "1828170",
    "end": "1834390"
  },
  {
    "text": "if any workload on that node is using persistent volumes those volumes are gonna have to be detached and then",
    "start": "1834390",
    "end": "1840390"
  },
  {
    "text": "reattached to another node so that the workload can run there so there's a lot of CSI interaction going on in that",
    "start": "1840390",
    "end": "1846780"
  },
  {
    "text": "process and we hit a number of issues in both the upstream CSI components and",
    "start": "1846780",
    "end": "1853290"
  },
  {
    "text": "communities and also in our own CSI driver then essentially resulted in the state of volumes being out of sync",
    "start": "1853290",
    "end": "1859710"
  },
  {
    "text": "between communities in the real world the symptoms that we would hit or that",
    "start": "1859710",
    "end": "1865230"
  },
  {
    "text": "node I wouldn't be able to be fully drained we hit the drain timeout because yes I was trying to detach a line that",
    "start": "1865230",
    "end": "1871470"
  },
  {
    "text": "actually wasn't attached to the node or we were doing a node and try and",
    "start": "1871470",
    "end": "1877440"
  },
  {
    "text": "reschedule the workloads on another node and not be able to attach the volumes because CSI thought but no thought the",
    "start": "1877440",
    "end": "1883050"
  },
  {
    "text": "line was still attached with different note or thought it was already attached those kinds of issues the nice thing is",
    "start": "1883050",
    "end": "1889710"
  },
  {
    "text": "CSI is mature a lot in the last few Kirby haze releases so in I would say in",
    "start": "1889710",
    "end": "1895440"
  },
  {
    "text": "114 plus we see very very few CSI problems upgrades in 114 plus have been",
    "start": "1895440",
    "end": "1901679"
  },
  {
    "text": "very similar to CSI and it's really taken a lot of strides so if you're on",
    "start": "1901679",
    "end": "1907110"
  },
  {
    "text": "then you were released I wouldn't the other problem we can't related to CSI is",
    "start": "1907110",
    "end": "1914610"
  },
  {
    "text": "also kind of related to the fact that it was not that mature when we started every CSI driver has a name and the",
    "start": "1914610",
    "end": "1923190"
  },
  {
    "text": "convention that's defined in the CSI specification is to name them on a domain name basis so kind of like a",
    "start": "1923190",
    "end": "1928920"
  },
  {
    "text": "giant class really over those in the early versions of the CSI spec that convention was reversed fqdn naming so",
    "start": "1928920",
    "end": "1935670"
  },
  {
    "text": "if you were the example corporation you would call your driver to combat example that CSI in later versions that changed",
    "start": "1935670",
    "end": "1942600"
  },
  {
    "text": "to be forward FTD n so now if your example corporation you would call it",
    "start": "1942600",
    "end": "1947820"
  },
  {
    "text": "CSI on example.com and we changed our driver when the spec changed to be I",
    "start": "1947820",
    "end": "1953210"
  },
  {
    "text": "conduct a Trojan to digital ocean and this name ends up being used in a",
    "start": "1953210",
    "end": "1959560"
  },
  {
    "text": "bunch of places and creates it gets used when your driver is registered with the communities subsystem that manages",
    "start": "1959560",
    "end": "1965650"
  },
  {
    "text": "drivers it also gets used in the storage class and said as a field on all the volumes that are created by the driver",
    "start": "1965650",
    "end": "1971200"
  },
  {
    "text": "this is essentially how kubernetes correlates a particular persistent volume to the storage fiber that's",
    "start": "1971200",
    "end": "1977170"
  },
  {
    "text": "supposed to manage it and so for good reason it's immutable once you register",
    "start": "1977170",
    "end": "1982360"
  },
  {
    "text": "historic strawberry Cooper news - CSI driver you can't really change its name because that names been propagated to",
    "start": "1982360",
    "end": "1987910"
  },
  {
    "text": "all the wines it created so when we went to upgrade from a CSI driver release where we used the old name - when we hit",
    "start": "1987910",
    "end": "1994750"
  },
  {
    "text": "a new name we started hitting a problem and the problem was basically that kubernetes no longer knew that those",
    "start": "1994750",
    "end": "2001560"
  },
  {
    "text": "precision phones he created with the old driver should be managed by the new driver and those volumes became",
    "start": "2001560",
    "end": "2007290"
  },
  {
    "text": "unmanageable he could no longer attached them to workloads and if he tried to drain a node they wouldn't get detached",
    "start": "2007290",
    "end": "2013200"
  },
  {
    "text": "and reattached so our solution was to make the name configurable and her",
    "start": "2013200",
    "end": "2018600"
  },
  {
    "text": "driver it defaults to the the you spec",
    "start": "2018600",
    "end": "2024090"
  },
  {
    "text": "right thing to do which is the forward fqdn naming but it is over edible vine",
    "start": "2024090",
    "end": "2030000"
  },
  {
    "text": "if I were variable and what we do when we upgrade is we detect with our cluster risk using the old name if it was then",
    "start": "2030000",
    "end": "2035310"
  },
  {
    "text": "we configure the new version to also use the old name so the bad name doesn't change and unfortunately this will",
    "start": "2035310",
    "end": "2042000"
  },
  {
    "text": "probably be part of our upgrade automation forever since we have to keep supporting clusters that have been",
    "start": "2042000",
    "end": "2047100"
  },
  {
    "text": "upgraded through various minor versions of kubernetes and I guess that's that",
    "start": "2047100",
    "end": "2052919"
  },
  {
    "text": "that's one of the few sort of version specific things that we've had to build",
    "start": "2052919",
    "end": "2057960"
  },
  {
    "text": "into our upgrade process is detecting that change persisting it so a couple",
    "start": "2057960",
    "end": "2066030"
  },
  {
    "text": "quick lessons a trigger clickable for both operators and developers if you're using CSI I would just recommend",
    "start": "2066030",
    "end": "2071940"
  },
  {
    "text": "carefully testing your upgrades and seeing what can go wrong there is a lot",
    "start": "2071940",
    "end": "2077128"
  },
  {
    "text": "that can go wrong and coordinating blind moves between nodes and the data on your",
    "start": "2077129",
    "end": "2082138"
  },
  {
    "text": "volumes is probably important to you that's why you put a run a person volume in the first place so you want to make sure that those your data is safe in",
    "start": "2082139",
    "end": "2088408"
  },
  {
    "text": "your workloads are as expected watch out for any workloads that get stuck for nodes that get stuck",
    "start": "2088409",
    "end": "2094809"
  },
  {
    "text": "training etc like I mentioned those are the common issues and be especially vigilant if you're using an older",
    "start": "2094809",
    "end": "2100420"
  },
  {
    "text": "kubernetes release I would say before 1:14 I use a newer release if you can it is that sort of catch-22 is that it if",
    "start": "2100420",
    "end": "2108039"
  },
  {
    "text": "you upgrade you'll have fewer CSI problems in the future but also you're more likely to get a problem but like I",
    "start": "2108039",
    "end": "2114099"
  },
  {
    "text": "said in 114 plus you're much less likely to get these problems and we really have seen very few issues in your releases",
    "start": "2114099",
    "end": "2122068"
  },
  {
    "text": "I've seen big one for last this is probably the most common problem we see in kubernetes upgrades to this day it's",
    "start": "2122309",
    "end": "2129670"
  },
  {
    "text": "a problem with admission control web hooks and these problems are possible in any environment with any upgrade process",
    "start": "2129670",
    "end": "2136150"
  },
  {
    "text": "so I'm going to spend a bit of time on them I think this is a problem that's been a big pain for us and a lot of",
    "start": "2136150",
    "end": "2142390"
  },
  {
    "text": "people don't like we did it so for",
    "start": "2142390",
    "end": "2148299"
  },
  {
    "text": "anyone who hasn't seen Mission Control it hooks before I'll give a quick overview an admission control a hook is",
    "start": "2148299",
    "end": "2154150"
  },
  {
    "text": "a configuration you can make in kubernetes to have an external service determine whether a resource can be",
    "start": "2154150",
    "end": "2159910"
  },
  {
    "text": "created or not and there are two kinds of admission controller books there are validating ones and mutating ones the",
    "start": "2159910",
    "end": "2166779"
  },
  {
    "text": "mutating ones can modify a resource before it's created validating one just determines whether it can be created or not and for our purposes in the rest of",
    "start": "2166779",
    "end": "2173950"
  },
  {
    "text": "this their idea well there's no difference okay so my example is gonna be a validating with hook but the same problem applies to me Jenny what's the",
    "start": "2173950",
    "end": "2181269"
  },
  {
    "text": "sequence diagram on the slide here shows what happens when you try and create something in Kerber Nettie's with an",
    "start": "2181269",
    "end": "2186279"
  },
  {
    "text": "admission control lever complain so you make your call the API server to create your resource and the API server is",
    "start": "2186279",
    "end": "2192220"
  },
  {
    "text": "gonna make a call out to your web hooks service that you've configured and it's very common to run these women's",
    "start": "2192220",
    "end": "2198490"
  },
  {
    "text": "services inside your cluster as the communities work you can run them outside and I'll talk about reasons you",
    "start": "2198490",
    "end": "2204970"
  },
  {
    "text": "might want to do that but it's very very common to draw them inside your cluster the web hook is going to return response",
    "start": "2204970",
    "end": "2211720"
  },
  {
    "text": "that says aloud true or a lot of false and that's how the ik a server determines whether or not it's a lot to",
    "start": "2211720",
    "end": "2217029"
  },
  {
    "text": "create the object i assuming that it is allowed to it's I'm going to go ahead do it",
    "start": "2217029",
    "end": "2222180"
  },
  {
    "text": "thing create the object everything's good I would be really clear that there are lots of good use cases for admission",
    "start": "2222180",
    "end": "2228540"
  },
  {
    "text": "controller books authorization is a common one validation and enforcement at",
    "start": "2228540",
    "end": "2233580"
  },
  {
    "text": "best practices is a good one injecting sign cards for things like service meshes is also common there's",
    "start": "2233580",
    "end": "2239100"
  },
  {
    "text": "nothing wrong with using the admission control it hooks and you should definitely use them they're they're a great tool I'm gonna talk about how to",
    "start": "2239100",
    "end": "2246030"
  },
  {
    "text": "make them safe for upgrades and the problems they can cost your upgrades the",
    "start": "2246030",
    "end": "2252720"
  },
  {
    "text": "problem is all really to what happens if the webhook service is not running and it can't respond to the api server so",
    "start": "2252720",
    "end": "2258780"
  },
  {
    "text": "looking at our sequence diagram again what happens if you go to create a resource with your api server it calls",
    "start": "2258780",
    "end": "2264300"
  },
  {
    "text": "up to the web book service and it just doesn't get a response well what happens depends a little bit on how you've",
    "start": "2264300",
    "end": "2269790"
  },
  {
    "text": "configured your replica first of all it depends on the failure policy their failure policy field can be",
    "start": "2269790",
    "end": "2276720"
  },
  {
    "text": "either fail or ignore if it's failed then the web book service if the web book service isn't available in the API",
    "start": "2276720",
    "end": "2282870"
  },
  {
    "text": "server doesn't air responds it's going to act as if that we yeah as if the web hook disallowed the the creation so it's",
    "start": "2282870",
    "end": "2290520"
  },
  {
    "text": "your resource creation is going to fail if you have it set to ignore then it's",
    "start": "2290520",
    "end": "2297690"
  },
  {
    "text": "gonna act as if the live look just doesn't exist we're gonna go ahead and create the resource we'll come back to that for a minute in a minute but I want",
    "start": "2297690",
    "end": "2303840"
  },
  {
    "text": "to talk about how this affects upgrades so the problem for upgrades is that",
    "start": "2303840",
    "end": "2310410"
  },
  {
    "text": "during a cranny's upgrade we're gonna update a bunch of system components that run in a kubernetes cluster as workloads",
    "start": "2310410",
    "end": "2317780"
  },
  {
    "text": "these are mostly in the cube system namespace but they make in other databases too depending on how you configure your cluster some examples",
    "start": "2317780",
    "end": "2324750"
  },
  {
    "text": "would be like core DNS or cube proxy these are things that run on your nodes as kubernetes workloads and were",
    "start": "2324750",
    "end": "2330630"
  },
  {
    "text": "scheduled by the api server at various controllers web hooks can prevent these",
    "start": "2330630",
    "end": "2335820"
  },
  {
    "text": "updates from happening they can prevent the definitions of of your system",
    "start": "2335820",
    "end": "2342180"
  },
  {
    "text": "components for being updated they can also prevent new pods from being created for your system components and web hooks",
    "start": "2342180",
    "end": "2348960"
  },
  {
    "text": "can prevent the services that back them from being scheduled so you if you're running your web book service in your",
    "start": "2348960",
    "end": "2354720"
  },
  {
    "text": "cluster which I mentioned is a very common configuration it can potentially prevent itself from being started and then your",
    "start": "2354720",
    "end": "2360870"
  },
  {
    "text": "webmix service it's never going to work again that's a bigger problem so coming",
    "start": "2360870",
    "end": "2366930"
  },
  {
    "text": "back to this web book configuration for a minute this one applies to pod creation and it applies to pods in any namespace so when",
    "start": "2366930",
    "end": "2374100"
  },
  {
    "text": "you try and create any pod in your cluster this webbook is going to be actually and it has a failure policy of",
    "start": "2374100",
    "end": "2381270"
  },
  {
    "text": "fail so let's look at what happens during an upgrade say our web book",
    "start": "2381270",
    "end": "2386640"
  },
  {
    "text": "service is deployed in the US here is a deployment and we're gonna start doing our upgrade we have a node that's",
    "start": "2386640",
    "end": "2392700"
  },
  {
    "text": "running the web book service and also the other normal cluster stuff when we start our upgrade we're going to try and",
    "start": "2392700",
    "end": "2398430"
  },
  {
    "text": "we're gonna drain this node and the web service pod is going to be killed on the",
    "start": "2398430",
    "end": "2404340"
  },
  {
    "text": "deployment controller is not going to be able to create a new pod for the web book service because when it tries the",
    "start": "2404340",
    "end": "2410640"
  },
  {
    "text": "API server is going to try and reach out to the web book service to ask whether it converted the pod and that call is going to fail the failure policy has",
    "start": "2410640",
    "end": "2417210"
  },
  {
    "text": "failed so it's not gonna create it so when we bring up a new node the web book",
    "start": "2417210",
    "end": "2422490"
  },
  {
    "text": "service is not running because the deployment controller was not able to create new pod for it and the demon sect",
    "start": "2422490",
    "end": "2428640"
  },
  {
    "text": "controller now is going to try and create system components like you proxy and our psyllium CNI driver I it's gonna",
    "start": "2428640",
    "end": "2434610"
  },
  {
    "text": "try and create those on the new node and it's not going to be able to again because it's gonna try and create the pod it's gonna go to the white book service",
    "start": "2434610",
    "end": "2440970"
  },
  {
    "text": "that my service is running it's gonna fail so at that point your cluster has nodes that are just completely unusable",
    "start": "2440970",
    "end": "2448520"
  },
  {
    "text": "you can see a simple solution to this which is set the fill your policy to ignore and that actually causes another",
    "start": "2448550",
    "end": "2453900"
  },
  {
    "text": "problem you might not expect because of the timeout it turns out that almost all of the default timeouts and kubernetes",
    "start": "2453900",
    "end": "2460650"
  },
  {
    "text": "are 30 seconds that includes the time up for web hooks so even if you don't specify 30 seconds as the timeout for",
    "start": "2460650",
    "end": "2467250"
  },
  {
    "text": "your web hook it's gonna get 30 seconds like people it also includes the API server timeout when you make a request",
    "start": "2467250",
    "end": "2472500"
  },
  {
    "text": "to the API server the default timeout for that request is 30 seconds so if you set your failure policy to ignore but",
    "start": "2472500",
    "end": "2478800"
  },
  {
    "text": "you leave the timeout at 30 seconds you'll end up with actually the same effect as having to fill your policy set to fail because the API server is going",
    "start": "2478800",
    "end": "2485700"
  },
  {
    "text": "to wait 30 seconds to try and get a response from the web books or before it ignores the failure but by the",
    "start": "2485700",
    "end": "2492390"
  },
  {
    "text": "time it hits that 30-second timeout the request is also timed out and so the ignore doesn't even matter the cost has already failed",
    "start": "2492390",
    "end": "2501020"
  },
  {
    "text": "so I recommend keeping your timeouts much lower than 30 seconds regardless of what fill your policy or setting this is",
    "start": "2501530",
    "end": "2508860"
  },
  {
    "text": "actually was recommended in the official kubernetes Docs so this isn't just me saying if this is in the official documentation the configuration I'm",
    "start": "2508860",
    "end": "2515700"
  },
  {
    "text": "showing on the slide here will work just fine where you have a timeout seconds of five I'm gonna fill your policy they can work that's never going to cause any",
    "start": "2515700",
    "end": "2521340"
  },
  {
    "text": "problems during the upgrade let's say you really do need your failure policy to be failed because your web book is",
    "start": "2521340",
    "end": "2529620"
  },
  {
    "text": "very important you can still avoid upgrade problems by having your web would not apply to the cube system",
    "start": "2529620",
    "end": "2535590"
  },
  {
    "text": "namespace or any other system critical namespaces a good way to do this is to set a label on your cube system",
    "start": "2535590",
    "end": "2541500"
  },
  {
    "text": "namespace and have your webhook ignore namespaces with that label using a namespace selector one strategy that",
    "start": "2541500",
    "end": "2547920"
  },
  {
    "text": "some teams at digitalocean have used is actually to have a mutating webhook at mutates Webfoot configurations so that",
    "start": "2547920",
    "end": "2553230"
  },
  {
    "text": "they are forced to ignore a cube system that way you can never set up a book that's gonna cause problems we're",
    "start": "2553230",
    "end": "2558480"
  },
  {
    "text": "considering this for our managed product as well your read books should also make sure to ignore whatever namespace their",
    "start": "2558480",
    "end": "2565230"
  },
  {
    "text": "own services run in if they're running in cluster and also any other news bases that run system critical components so",
    "start": "2565230",
    "end": "2574410"
  },
  {
    "text": "lessons for operators out of this first of all check that your rep book configurations are good before you start",
    "start": "2574410",
    "end": "2581250"
  },
  {
    "text": "upgrading a cluster we have an open source tool called cluster lint that includes a check for this so that's what",
    "start": "2581250",
    "end": "2586560"
  },
  {
    "text": "we use on our customers clusters before we upgrade them and I you can also use",
    "start": "2586560",
    "end": "2592380"
  },
  {
    "text": "that tool as well like I said it's open source the other things like I mentioned you might want to configure a mutating a",
    "start": "2592380",
    "end": "2599070"
  },
  {
    "text": "book that mutates well-put configurations to make them harmless that's a great way to avoid the problem ever coming up in the first place but if",
    "start": "2599070",
    "end": "2605970"
  },
  {
    "text": "you're going to do that you might want to consider running that service for that webhook outside of your cluster I just saw that it's not susceptible to",
    "start": "2605970",
    "end": "2612210"
  },
  {
    "text": "these same calls for developers the",
    "start": "2612210",
    "end": "2617490"
  },
  {
    "text": "lesson is basically when I showed in the example be careful your failure policy and your timeouts",
    "start": "2617490",
    "end": "2623520"
  },
  {
    "text": "and be careful with my books in general they they can cause big problems for the",
    "start": "2623520",
    "end": "2628750"
  },
  {
    "text": "important components in your cluster like I said they're a great tool to use just be really mindful of your",
    "start": "2628750",
    "end": "2634720"
  },
  {
    "text": "configuration of them so that they're not going to cause problems for their teams the cube system namespace or any other system critical namespaces that's",
    "start": "2634720",
    "end": "2644859"
  },
  {
    "text": "that's all my content for today I have this slide with sort of everything we talked about I'll run through quickly",
    "start": "2644859",
    "end": "2650830"
  },
  {
    "text": "just as a recap my first lesson today was I you might want to consider",
    "start": "2650830",
    "end": "2657190"
  },
  {
    "text": "upgrading your grades cluster by an old replacement instead of upgrading your notes in place is a simpler process it",
    "start": "2657190",
    "end": "2664540"
  },
  {
    "text": "helps with automation there are some problems that can come up with that and so make sure to be aware of those",
    "start": "2664540",
    "end": "2671470"
  },
  {
    "text": "consider retaining those names and IP addresses if that's possible in your environment I have your workloads assume",
    "start": "2671470",
    "end": "2677770"
  },
  {
    "text": "that no one's are going to go away and not refer to specific node names specific norm AP addresses and create",
    "start": "2677770",
    "end": "2683560"
  },
  {
    "text": "new nodes before you destroy old ones if that's at all possible that's really going to help with the training problem and having your workloads continue",
    "start": "2683560",
    "end": "2690160"
  },
  {
    "text": "running through an upgrade secondly make sure that your workloads can be evicted they are definitely going to be evicted",
    "start": "2690160",
    "end": "2696010"
  },
  {
    "text": "during an upgrade regardless of how you do it and the more prepared you are for that the more you test that the better",
    "start": "2696010",
    "end": "2701050"
  },
  {
    "text": "your workloads right here the next lesson was upgrade more than one note at",
    "start": "2701050",
    "end": "2707710"
  },
  {
    "text": "once if you can that's really going to help when you have a big cluster it'll make the upgrade process faster and smoother and that's a good thing my",
    "start": "2707710",
    "end": "2715350"
  },
  {
    "text": "lesson out for that was minor version upgrades are probably easier than you think especially if you've avoided using",
    "start": "2715350",
    "end": "2721210"
  },
  {
    "text": "or enabling alpha features don't worry so much about minor version upgrades there are many advantages to upgrade to",
    "start": "2721210",
    "end": "2727210"
  },
  {
    "text": "the next minor version of kubernetes and why you need to be careful about any upgrade minor upgrades are not that much",
    "start": "2727210",
    "end": "2733869"
  },
  {
    "text": "harder than patch version upgrades in general the last two lessons here were around specific problems that we've seen",
    "start": "2733869",
    "end": "2740550"
  },
  {
    "text": "one is that CSI is is now becoming mature I would say is nowadays quite",
    "start": "2740550",
    "end": "2745570"
  },
  {
    "text": "matured but on older versions of Cooper nineties it was not so take special care when you're operating if you use CSI the",
    "start": "2745570",
    "end": "2752800"
  },
  {
    "text": "final one was around webhooks they can cause all kinds of trouble to have an upgrade like I said",
    "start": "2752800",
    "end": "2757829"
  },
  {
    "text": "this is the most common problem that we see with upgrades for our customers so",
    "start": "2757829",
    "end": "2763079"
  },
  {
    "text": "if you're using admission control on books check your targets check your failure policies check your timeouts",
    "start": "2763079",
    "end": "2768450"
  },
  {
    "text": "make sure that those are all configured according to that grantees Docs and what I told you today you can use our cluster",
    "start": "2768450",
    "end": "2776040"
  },
  {
    "text": "lint tool to check those if you want there's I'm sure also other tools that can check and that's what I had for",
    "start": "2776040",
    "end": "2782609"
  },
  {
    "text": "today when we go into Q&A which I think Carol is going to moderate yeah thanks",
    "start": "2782609",
    "end": "2788790"
  },
  {
    "text": "for a great presentation and a talk Adam we have a couple of questions in chat",
    "start": "2788790",
    "end": "2797069"
  },
  {
    "text": "I'll start with the first one which takes us to an earlier spot in your talk",
    "start": "2797069",
    "end": "2802079"
  },
  {
    "text": "which is can we use in it containers for node customization yeah so the way that",
    "start": "2802079",
    "end": "2808829"
  },
  {
    "text": "works is if you have it when you define a pod you can have the normal containers",
    "start": "2808829",
    "end": "2814410"
  },
  {
    "text": "for the pot and you can also have in it containers the net containers run before the normal container start and they're",
    "start": "2814410",
    "end": "2820140"
  },
  {
    "text": "gonna run on the same node so if you need to set a specific CTL value for example on a node because your work",
    "start": "2820140",
    "end": "2826859"
  },
  {
    "text": "alone wants a really big TCP buffer or something you can have any make container that goes and sets that value",
    "start": "2826859",
    "end": "2832560"
  },
  {
    "text": "before your work load starts before your application starts so that's the basic",
    "start": "2832560",
    "end": "2839660"
  },
  {
    "text": "next up David Suarez asked regarding CSI",
    "start": "2844190",
    "end": "2849660"
  },
  {
    "text": "if he fqdn order change was an adoption to update at CD data to upgrade it or",
    "start": "2849660",
    "end": "2857280"
  },
  {
    "text": "and if so why was that option not ideal or selected yeah that's a great question",
    "start": "2857280",
    "end": "2864329"
  },
  {
    "text": "that is something we considered and it's something I experimented with the the",
    "start": "2864329",
    "end": "2870540"
  },
  {
    "text": "trouble with it is we would have had to do it directly in NCE so we would have had to go around the API server to do it",
    "start": "2870540",
    "end": "2877079"
  },
  {
    "text": "the API server disallows you from changing some of the fields for that driver name gets persisted and we really",
    "start": "2877079",
    "end": "2884490"
  },
  {
    "text": "didn't want to reach directly into an CD and hope that we all the right places we much preferred",
    "start": "2884490",
    "end": "2892050"
  },
  {
    "text": "to leave that in place and just handle the fact that it had changed but yeah that's definitely a strategy we",
    "start": "2892050",
    "end": "2898830"
  },
  {
    "text": "considered and it would have probably been the preferred strategy if it was",
    "start": "2898830",
    "end": "2904290"
  },
  {
    "text": "possible to make the change via kubernetes mechanism rather than going directly density Christian Roman ask",
    "start": "2904290",
    "end": "2914190"
  },
  {
    "text": "what kind of unit test do you run prior during or after the cluster upgrade to",
    "start": "2914190",
    "end": "2919440"
  },
  {
    "text": "validate things one well maybe even in individual stages such as after a TD",
    "start": "2919440",
    "end": "2925500"
  },
  {
    "text": "upgrade control plan upgrade Exedra yeah that's so there's a variety things",
    "start": "2925500",
    "end": "2934110"
  },
  {
    "text": "that we do the biggest thing we do is we make sure that after we replace that",
    "start": "2934110",
    "end": "2939480"
  },
  {
    "text": "worker node we make sure it becomes ready before we start off the next one that that ensures that we're not gonna",
    "start": "2939480",
    "end": "2945870"
  },
  {
    "text": "like take down all the worker nodes in a cluster and have it over to schedule it'll work loads it's not a 100 percent guarantee you that the workloads are",
    "start": "2945870",
    "end": "2951570"
  },
  {
    "text": "okay there's only so much we can do about that since the burka loads we don't own and we really try not to look",
    "start": "2951570",
    "end": "2957870"
  },
  {
    "text": "at since but we do make sure that the",
    "start": "2957870",
    "end": "2963510"
  },
  {
    "text": "nodes become ready and same between the control plane upgrade and the return upgrade we make sure that the control",
    "start": "2963510",
    "end": "2969210"
  },
  {
    "text": "plane components are all up and healthy that the you know CNI is healthy our",
    "start": "2969210",
    "end": "2975060"
  },
  {
    "text": "cloud controller manager is healthy our community schedulers healthy all those things so that's the biggest the biggest",
    "start": "2975060",
    "end": "2982620"
  },
  {
    "text": "thing that we do is is just rely on the kubernetes health statuses to make sure that things are very happy if you",
    "start": "2982620",
    "end": "2990180"
  },
  {
    "text": "control both your cluster and your workloads then doing health checks on",
    "start": "2990180",
    "end": "2995220"
  },
  {
    "text": "your workloads would make a lot of sense that's not really something we can do since we don't like I said we don't",
    "start": "2995220",
    "end": "3000650"
  },
  {
    "text": "control we're clones and if someone wants to configure their workload really poorly we don't want to sort of end up",
    "start": "3000650",
    "end": "3006530"
  },
  {
    "text": "with a stuck upgrade for that it's a bit of a trade-off being a managed provider that we you know what our customers work",
    "start": "3006530",
    "end": "3016160"
  },
  {
    "text": "wants to be as safe as possible if we don't full control over them so yeah so I",
    "start": "3016160",
    "end": "3021410"
  },
  {
    "text": "guess uh the Christian follows up a little bit with I if there is an issue",
    "start": "3021410",
    "end": "3026510"
  },
  {
    "text": "with thee and I think this speaks to the managed service aspect of it which is if",
    "start": "3026510",
    "end": "3032359"
  },
  {
    "text": "something goes wrong with the upgrade is there a process for raising this as or",
    "start": "3032359",
    "end": "3039980"
  },
  {
    "text": "flagging it for a human inner human to come in and kind of troubleshoot yeah",
    "start": "3039980",
    "end": "3046400"
  },
  {
    "text": "there definitely is we leaned heavily on prometheus metrics for this so the",
    "start": "3046400",
    "end": "3052069"
  },
  {
    "text": "process we have that runs that reconciles clusters and and does the upgrade exposes a bunch of metrics",
    "start": "3052069",
    "end": "3058880"
  },
  {
    "text": "internally to us for example what's the cluster that's sort of been reconciling",
    "start": "3058880",
    "end": "3064369"
  },
  {
    "text": "for the longest so what's the slowest upgrade that's currently in progress and we have alerts on those things that go",
    "start": "3064369",
    "end": "3070309"
  },
  {
    "text": "to our ops team and then eventually get escalated to us if there's a problem so that's our most basic mechanism the most",
    "start": "3070309",
    "end": "3077960"
  },
  {
    "text": "common thing we see is great just get stuck because for example we upgrade the",
    "start": "3077960",
    "end": "3083750"
  },
  {
    "text": "control point in it every pets healthy it's about time we catch those how do",
    "start": "3083750",
    "end": "3089180"
  },
  {
    "text": "you is there a good way a good practice that you guys employ to determine at",
    "start": "3089180",
    "end": "3094549"
  },
  {
    "text": "snack point we used to struggle with this a little bit what's uh you know that the customer didn't deploy",
    "start": "3094549",
    "end": "3100059"
  },
  {
    "text": "application to the cluster leveraging best practices and then upgrades can potentially become problematic because",
    "start": "3100059",
    "end": "3106910"
  },
  {
    "text": "everything is there is there some practice or that you all employ to",
    "start": "3106910",
    "end": "3112990"
  },
  {
    "text": "evaluate whether it's you know on the customer or whether it's something in",
    "start": "3112990",
    "end": "3118490"
  },
  {
    "text": "the platform I would do that determination the mostly manual at this point when we do have something get a",
    "start": "3118490",
    "end": "3125089"
  },
  {
    "text": "stocker across problems it's really kind of human intervention we'll go and look and there's some problems we'll just fix for",
    "start": "3125089",
    "end": "3131809"
  },
  {
    "text": "customers like the webhook ones for example will temporary its temporarily disabled hook it back that's not",
    "start": "3131809",
    "end": "3139520"
  },
  {
    "text": "something we like to do because it is touching a customer configuration but it's necessary we'll do it we'll also",
    "start": "3139520",
    "end": "3144740"
  },
  {
    "text": "you know get back to a customer and say hey if you change this thing in your code with the customer crazy to proceed",
    "start": "3144740",
    "end": "3150500"
  },
  {
    "text": "we do have a mechanism pause thought greed which I mean leaves the cluster in a kind of broken state",
    "start": "3150500",
    "end": "3156749"
  },
  {
    "text": "but at least doesn't at least doesn't block our visibility to other upgrades that are going on so that's something we",
    "start": "3156749",
    "end": "3163470"
  },
  {
    "text": "also used sometimes get back to the customer and say hey you changed this and then it can proceed cool",
    "start": "3163470",
    "end": "3169950"
  },
  {
    "text": "yeah very consistent how we used to do that back then Tong Pro husky asked did",
    "start": "3169950",
    "end": "3176880"
  },
  {
    "text": "you have to roll back the upgrade during lit upgrade if so why and how did you",
    "start": "3176880",
    "end": "3182519"
  },
  {
    "text": "get this ability and control the process we can roll back upgrades mid mid",
    "start": "3182519",
    "end": "3192269"
  },
  {
    "text": "upgrade especially like if you if we upgrade the control plan and it never comes up that would be the probably kind",
    "start": "3192269",
    "end": "3199410"
  },
  {
    "text": "of the last point at which we do a rollback we we don't have any automation",
    "start": "3199410",
    "end": "3204569"
  },
  {
    "text": "for that that's a manual thing we've had to do it I can probably count on one",
    "start": "3204569",
    "end": "3209819"
  },
  {
    "text": "hand how many times we've actually done it because it's not a great thing to do the in particular if some of the control",
    "start": "3209819",
    "end": "3217710"
  },
  {
    "text": "plane has come up and it started you know converting resources to new formats and at CD or things like that there's a",
    "start": "3217710",
    "end": "3224549"
  },
  {
    "text": "lot of chance for things to go wrong if we roll back so we try not to do it but our basic mechanisms to do that is we we",
    "start": "3224549",
    "end": "3231119"
  },
  {
    "text": "do take at CD snapshots and also the end snapshots before we start the upgrade",
    "start": "3231119",
    "end": "3236489"
  },
  {
    "text": "and make sure that those snapshots are sort of in place so that if we need them",
    "start": "3236489",
    "end": "3244140"
  },
  {
    "text": "and we need to roll back to them we can cool we have a couple more minutes and a",
    "start": "3244140",
    "end": "3250230"
  },
  {
    "text": "couple more questions do you know Dejan veteran asked you how do you change",
    "start": "3250230",
    "end": "3256920"
  },
  {
    "text": "control panel control plane IPS - when replacing in my experience this is a",
    "start": "3256920",
    "end": "3263099"
  },
  {
    "text": "pretty complicated task yeah so we we do change the public IP for the control",
    "start": "3263099",
    "end": "3271170"
  },
  {
    "text": "plane like the VIP the customer connects to to use it the way we handle that is",
    "start": "3271170",
    "end": "3276210"
  },
  {
    "text": "is that we never actually exposed that IP directly to customers they connect through a host name that we manage and",
    "start": "3276210",
    "end": "3281759"
  },
  {
    "text": "we just set the TTL on that hostname super low it's like 30 so that's the basic way around it",
    "start": "3281759",
    "end": "3290279"
  },
  {
    "text": "the in terms of like internal IPS that's all handled by the CNI so as long as",
    "start": "3290279",
    "end": "3297750"
  },
  {
    "text": "someone's not like hard-coding a control plane an IP internally they shouldn't have a problem um should thing Joe I",
    "start": "3297750",
    "end": "3308279"
  },
  {
    "text": "asked how can you configure the control plane component cube API server to bleed",
    "start": "3308279",
    "end": "3314069"
  },
  {
    "text": "during the upgrade sure so the way that we do it is we bake that configuration",
    "start": "3314069",
    "end": "3320039"
  },
  {
    "text": "into our VM images that we're gonna use for provisioning or upgrading of clusters so it's actually this exactly",
    "start": "3320039",
    "end": "3325740"
  },
  {
    "text": "the same regardless of whether you're creating a new cluster on version or upgrading to a version we do some",
    "start": "3325740",
    "end": "3330960"
  },
  {
    "text": "templating in there just using like Co templating go template strings nothing",
    "start": "3330960",
    "end": "3336299"
  },
  {
    "text": "very fancy but we yeah we bake those configurations right in like I said that",
    "start": "3336299",
    "end": "3342299"
  },
  {
    "text": "that's kind of one of the advantages I see of our process is that we're not mutating those configurations they're",
    "start": "3342299",
    "end": "3348359"
  },
  {
    "text": "they're sort of write-once objects which is helpful and just keeps the process a",
    "start": "3348359",
    "end": "3355380"
  },
  {
    "text": "little bit simpler cool let's roll through a couple more as they",
    "start": "3355380",
    "end": "3361230"
  },
  {
    "text": "keep rolling in your lis romanov ask what percentage of nodes do you update",
    "start": "3361230",
    "end": "3366990"
  },
  {
    "text": "at a time then how do you check the user settings like anti affinity and such to",
    "start": "3366990",
    "end": "3373200"
  },
  {
    "text": "allow will allow to reschedule pods yeah so at the moment we are still doing I",
    "start": "3373200",
    "end": "3379740"
  },
  {
    "text": "know it's one by one we're we're working on right now moving toward a system",
    "start": "3379740",
    "end": "3386460"
  },
  {
    "text": "where a we create new nodes before we delete old nodes and then that also allows us to increase the number that we",
    "start": "3386460",
    "end": "3391890"
  },
  {
    "text": "upgrade at once I think we're still up in there I'm exactly how are is that",
    "start": "3391890",
    "end": "3397529"
  },
  {
    "text": "those numbers I think it's going to take a little bit of experimentation on our side it also depends a little bit on",
    "start": "3397529",
    "end": "3405200"
  },
  {
    "text": "this is a kind of detail of our product it's not gonna be applicable elsewhere but our in our product that worker nodes",
    "start": "3405200",
    "end": "3413339"
  },
  {
    "text": "are owned by the user there the user has like full access to them and that means that they count",
    "start": "3413339",
    "end": "3419590"
  },
  {
    "text": "against the limit of the number of VMs we allow a particular user to have on our platform so we have to be kind of",
    "start": "3419590",
    "end": "3425590"
  },
  {
    "text": "mindful of that and allow for the case where there they hit their limit and we",
    "start": "3425590",
    "end": "3430630"
  },
  {
    "text": "can't create any more for them so there's some of the complexities that you might have to manage if you have",
    "start": "3430630",
    "end": "3435850"
  },
  {
    "text": "quotas or or something to manage yeah and I think the same holds true and",
    "start": "3435850",
    "end": "3441250"
  },
  {
    "text": "other cloud providers believe it or not since we had all have experience with a multi cloud where yeah limits on what",
    "start": "3441250",
    "end": "3450730"
  },
  {
    "text": "could be used impacted operations of you will anonymous attendee ask do you",
    "start": "3450730",
    "end": "3456730"
  },
  {
    "text": "upgrade a certain percentage of nodes at a time for instance upgrade a third of the nodes each day over a three-day",
    "start": "3456730",
    "end": "3463090"
  },
  {
    "text": "period so we like I said just just now at the moment we're we're just doing one",
    "start": "3463090",
    "end": "3469270"
  },
  {
    "text": "by one but in terms of the time scale we start an upgrade and we don't pause",
    "start": "3469270",
    "end": "3475390"
  },
  {
    "text": "until it's finished so it we don't take multiple days or anything like that our goal is to do an upgrade in like less",
    "start": "3475390",
    "end": "3482320"
  },
  {
    "text": "than an hour and right I mean right now if you have a hundred mill Kuster it's definitely to take more than an hour but",
    "start": "3482320",
    "end": "3487720"
  },
  {
    "text": "hopefully in the future and I think we",
    "start": "3487720",
    "end": "3493000"
  },
  {
    "text": "have one final one we can ask here what problems do you think the other approach has I guess a little bit of context the",
    "start": "3493000",
    "end": "3499690"
  },
  {
    "text": "the approach of upgrading each component separately in each node instead of replacing the complete node I'm asking",
    "start": "3499690",
    "end": "3506440"
  },
  {
    "text": "as my previous project we used to see a lot of downtime for our workloads as unfortunately we didn't have breathing",
    "start": "3506440",
    "end": "3513730"
  },
  {
    "text": "room in our cluster for upgrade the mate before break yeah so I think that the",
    "start": "3513730",
    "end": "3522550"
  },
  {
    "text": "big challenges of doing an in-place upgrade are a it's just a lot of",
    "start": "3522550",
    "end": "3528580"
  },
  {
    "text": "components to to coordinate and it's going to be somewhat different between different upgrades sometimes you're",
    "start": "3528580",
    "end": "3535390"
  },
  {
    "text": "upgrading your C and I sometimes you're not sometimes you're upgrading you know your",
    "start": "3535390",
    "end": "3540640"
  },
  {
    "text": "Cloud Controller manager sometimes you're not so you have to kind of build the automation differently depending on",
    "start": "3540640",
    "end": "3545890"
  },
  {
    "text": "each individual upgrade you're doing the the other thing is that you're gonna",
    "start": "3545890",
    "end": "3551650"
  },
  {
    "text": "have to upgrade configuration in place and you're gonna have to deal with any customizations on the nodes anchor that's changed sort of beyond your",
    "start": "3551650",
    "end": "3558160"
  },
  {
    "text": "control so you know depending on your environment if you have a tightly managed environment where you really",
    "start": "3558160",
    "end": "3564010"
  },
  {
    "text": "control all the workloads controller cluster I mean place upgrade could be much less disruptive and like you said you don't",
    "start": "3564010",
    "end": "3569560"
  },
  {
    "text": "have to deal with that capacity concern quite as much but for our environment",
    "start": "3569560",
    "end": "3574690"
  },
  {
    "text": "where we're managing thousands of clusters and we don't control the workloads on them the node replacement",
    "start": "3574690",
    "end": "3580210"
  },
  {
    "text": "strategy seemed safer to us and I think that's played out pretty well okay great",
    "start": "3580210",
    "end": "3586270"
  },
  {
    "text": "I see another question came in but we are unfortunately at actually a little",
    "start": "3586270",
    "end": "3592210"
  },
  {
    "text": "bit past time on the screen you'll notice you can reach out to Adam via",
    "start": "3592210",
    "end": "3597700"
  },
  {
    "text": "email he's also on Twitter or lower left-hand corner so feel free to reach",
    "start": "3597700",
    "end": "3604120"
  },
  {
    "text": "out there I Adam and I want to thank all of you for joining today the webinar",
    "start": "3604120",
    "end": "3610450"
  },
  {
    "text": "recording and the slides will be online later today and we're looking forward to",
    "start": "3610450",
    "end": "3617020"
  },
  {
    "text": "seeing you at the next CN CF webinar have a great day Thanks",
    "start": "3617020",
    "end": "3624390"
  }
]