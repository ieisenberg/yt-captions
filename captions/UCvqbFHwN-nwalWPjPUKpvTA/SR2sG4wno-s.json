[
  {
    "text": "yeah uh I think we are at time so first of all thanks everyone for joining the session and today uh we will have uh",
    "start": "240",
    "end": "7480"
  },
  {
    "text": "intro level uh session for the tunnel project I will also uh cover the project",
    "start": "7480",
    "end": "13400"
  },
  {
    "text": "updates part briefly and for this talk I also want to talk talk about some common",
    "start": "13400",
    "end": "19560"
  },
  {
    "text": "pitfalls uh as a maintainer I see Tel users having and I hope this can be some",
    "start": "19560",
    "end": "25519"
  },
  {
    "text": "uh key takeaways for you if if you are using tunnels or you plan to use tunnels",
    "start": "25519",
    "end": "31720"
  },
  {
    "text": "so uh first of all a bit introduction about myself so uh I'm Ben and I'm a",
    "start": "31720",
    "end": "36879"
  },
  {
    "text": "software engineer at AWS and I'm um a maintainer for both tunel and cortex",
    "start": "36879",
    "end": "43320"
  },
  {
    "text": "project and you can find me on GitHub and I also have a",
    "start": "43320",
    "end": "49280"
  },
  {
    "text": "p so let's start with the introduction um if you want to talk",
    "start": "49360",
    "end": "54680"
  },
  {
    "text": "about SOS first we need to talk about premisus but since um people attend this",
    "start": "54680",
    "end": "60640"
  },
  {
    "text": "talk I assume everyone probably already have some kind of background for SOS so",
    "start": "60640",
    "end": "65960"
  },
  {
    "text": "I will sorry for for premisus so I will not dive too deep uh but the key thing",
    "start": "65960",
    "end": "72479"
  },
  {
    "text": "to know here is that uh premisus is known to be designed to work as a single",
    "start": "72479",
    "end": "77600"
  },
  {
    "text": "application in a single machine and it uses pool model to square Matrix it also",
    "start": "77600",
    "end": "83079"
  },
  {
    "text": "have some uh cool feature like tsdb prom car engine and alerting feature Etc",
    "start": "83079",
    "end": "90840"
  },
  {
    "text": "next uh I want to uh walk you through a user Journey who is using uh premisus",
    "start": "90840",
    "end": "98159"
  },
  {
    "text": "and how he can gradually adopt the whole uh tunnel stack so first of all maybe",
    "start": "98159",
    "end": "104640"
  },
  {
    "text": "everyone starts with premisus as a single uh server so you scrap some metrics from your exporter or",
    "start": "104640",
    "end": "110479"
  },
  {
    "text": "application and you set up your uh visualization layer for example grafana and you can see your nice dashboard and",
    "start": "110479",
    "end": "117799"
  },
  {
    "text": "graphs however you start to see some problems when there's a deployment going on for your premis server um if you if",
    "start": "117799",
    "end": "125799"
  },
  {
    "text": "you only use a single premis server it during the deployment it will take down your premis server and it will take a",
    "start": "125799",
    "end": "133040"
  },
  {
    "text": "few minutes to wait for it to get back and then you will have a few minutes of down time and you can see some gaps in",
    "start": "133040",
    "end": "140040"
  },
  {
    "text": "your grafana dashboard so a common way to solve this is to uh set up premisus ha Pairs and",
    "start": "140040",
    "end": "148959"
  },
  {
    "text": "basically it's you will deploy two preus parts and they both scrape the same data",
    "start": "148959",
    "end": "154920"
  },
  {
    "text": "so even uh in this case even you have a single prisia going down you still have",
    "start": "154920",
    "end": "160200"
  },
  {
    "text": "kind of the same snapshot in another prisia server because they are collecting the same data but this",
    "start": "160200",
    "end": "167280"
  },
  {
    "text": "solution is not perfect and the problem sometimes uh can come from your",
    "start": "167280",
    "end": "174120"
  },
  {
    "text": "data visual visualization layer site um because grafana usually you will just use a single data source s for premisus",
    "start": "174120",
    "end": "181560"
  },
  {
    "text": "so you usually put a load balancer in front of your two premisus pairs it can",
    "start": "181560",
    "end": "186720"
  },
  {
    "text": "be usually a kuin service or something else but let's say if we have a deployment going on for your premisus it",
    "start": "186720",
    "end": "193400"
  },
  {
    "text": "will try to take down your first premisus server wait for it up and running and then take down your another",
    "start": "193400",
    "end": "199319"
  },
  {
    "text": "primitive server so what will happen in this case is that if the first pris server uh went down then the load",
    "start": "199319",
    "end": "206799"
  },
  {
    "text": "balancer will rout the quiry traffic to the second premisus and it works fine but however when the",
    "start": "206799",
    "end": "213560"
  },
  {
    "text": "first primitive server comes back it it doesn't have the metrix uh when when he",
    "start": "213560",
    "end": "219920"
  },
  {
    "text": "when it was done so if the road balancer R acquir to the first premitted server",
    "start": "219920",
    "end": "224959"
  },
  {
    "text": "again you will have you can see the metric Gap in your uh grafana dashboard",
    "start": "224959",
    "end": "230319"
  },
  {
    "text": "so this is something kind of like uh inconsistent uh Matrix quare review so",
    "start": "230319",
    "end": "236040"
  },
  {
    "text": "how can tunnels help here so uh I'm going to introduce a first uh and second",
    "start": "236040",
    "end": "241640"
  },
  {
    "text": "SOS component here um so we will try to deploy a component called SOS quer and",
    "start": "241640",
    "end": "246879"
  },
  {
    "text": "SOS quirer can uh provide a consistent global view by aggregating and DC",
    "start": "246879",
    "end": "252879"
  },
  {
    "text": "duplicating data from your premisus H APS so how does it do it is that we also",
    "start": "252879",
    "end": "259239"
  },
  {
    "text": "need to deploy uh s of side car components alongside with your premisus",
    "start": "259239",
    "end": "264720"
  },
  {
    "text": "and uh this way your Sonos query is able to talk with your Sonos side car through",
    "start": "264720",
    "end": "270560"
  },
  {
    "text": "SOS uh store uh JPC API and uh with uh",
    "start": "270560",
    "end": "276120"
  },
  {
    "text": "in in this case even though you have uh single premisus going down because SOS quare always try to aggregate from all",
    "start": "276120",
    "end": "283120"
  },
  {
    "text": "your premisus so you can kind of maintain a consistent View and won't have the problem uh I mentioned",
    "start": "283120",
    "end": "289840"
  },
  {
    "text": "before so I think this is basically the most simple setup for tunnels and for a",
    "start": "289840",
    "end": "295960"
  },
  {
    "text": "lot of use cases it work really well and let's talk about something different for",
    "start": "295960",
    "end": "301120"
  },
  {
    "text": "example um rule evaluation so user usually start from using premisus to",
    "start": "301120",
    "end": "308280"
  },
  {
    "text": "have have rules to evaluate rules like recording rules and alerting rules this",
    "start": "308280",
    "end": "313440"
  },
  {
    "text": "works and this also works very well for most of the use case but for certain use case you want to have something similar",
    "start": "313440",
    "end": "319880"
  },
  {
    "text": "to your uh quiry uh global view to have the consistent view of all all your",
    "start": "319880",
    "end": "325000"
  },
  {
    "text": "cluster or all your premises so what you can do is to deploy a component called Sonos ruler and this ruler basically",
    "start": "325000",
    "end": "331319"
  },
  {
    "text": "queries SOS querer for rule evaluation and stores uh the recording rule Matrix",
    "start": "331319",
    "end": "337360"
  },
  {
    "text": "and alerts metric on its local database and it's also able to send alerts uh to",
    "start": "337360",
    "end": "343960"
  },
  {
    "text": "premisus so that's a use case uh for Rule and what about some other use case",
    "start": "343960",
    "end": "349520"
  },
  {
    "text": "for example um like quiry or storage so um by default premisus has its long uh",
    "start": "349520",
    "end": "356759"
  },
  {
    "text": "retention period set to 15 days so probably most users start from from uh",
    "start": "356759",
    "end": "362280"
  },
  {
    "text": "with this kind of setup and it it works well but maybe one day your team has a different use case and they want to look",
    "start": "362280",
    "end": "369080"
  },
  {
    "text": "at long-term trending data for uh maybe analytics or anomaly detection but it's",
    "start": "369080",
    "end": "375080"
  },
  {
    "text": "very hard to do this in premisus because premisus is just a single server it has limited local storage and it's",
    "start": "375080",
    "end": "382160"
  },
  {
    "text": "definitely not designed to have longterm uh storage but um luckily tunos has some",
    "start": "382160",
    "end": "388639"
  },
  {
    "text": "solution so SOS side car SOS ruler and some other SOS component they all support flushing data to your object",
    "start": "388639",
    "end": "396000"
  },
  {
    "text": "storage such as S3 every two hours so users can just configure a smaller",
    "start": "396000",
    "end": "401160"
  },
  {
    "text": "retention time uh in premisus and store their long-term data in",
    "start": "401160",
    "end": "406639"
  },
  {
    "text": "S3 and in order to quate all the data stored uh in S3 SOS has a component",
    "start": "406639",
    "end": "412880"
  },
  {
    "text": "called St Gateway and St Gateway is basically kind of like a proxy to serve",
    "start": "412880",
    "end": "419080"
  },
  {
    "text": "um like the SOS quar load uh from your S3 bucket and it exposes the same Sonos",
    "start": "419080",
    "end": "425639"
  },
  {
    "text": "store grpc API and now we have the long-term",
    "start": "425639",
    "end": "431000"
  },
  {
    "text": "storage and we are able to quiry data for a longterm but the problem comes uh",
    "start": "431000",
    "end": "436800"
  },
  {
    "text": "when you start to query it it's it works but you will find like the performance is really bad and",
    "start": "436800",
    "end": "443639"
  },
  {
    "text": "the reason here is that if we uh just uh uh quir a r uh every to 2 hour block",
    "start": "443639",
    "end": "450680"
  },
  {
    "text": "they are not compacted because for example if you have a block every 2 hour you will have uh 360 blocks for a month",
    "start": "450680",
    "end": "459000"
  },
  {
    "text": "and if you run a 30-day query you are basically quing all the 360 blocks back",
    "start": "459000",
    "end": "464879"
  },
  {
    "text": "and it requires a lot of IO to your S3 bucket and uh which is not really",
    "start": "464879",
    "end": "470759"
  },
  {
    "text": "efficient so SOS has a component uh which deal with this issue it's called",
    "start": "470759",
    "end": "475800"
  },
  {
    "text": "compactor and it has two main functionality Compact and down sample so",
    "start": "475800",
    "end": "482000"
  },
  {
    "text": "compaction is basically to merge uh your blocks with smaller uh time range into a",
    "start": "482000",
    "end": "488120"
  },
  {
    "text": "longer time range for example we can merge 2our data into two day uh two day",
    "start": "488120",
    "end": "494000"
  },
  {
    "text": "data into two weeks things like that and for down sampling it's something uh it's",
    "start": "494000",
    "end": "499800"
  },
  {
    "text": "also for the long-term kind of trending use case um for example like in your raw",
    "start": "499800",
    "end": "505960"
  },
  {
    "text": "block you will have your samples every 15 seconds because that's what you have for your scrap interval and but if you",
    "start": "505960",
    "end": "513959"
  },
  {
    "text": "look back your dashboard for like one year or 6 months it doesn't make any sense to quy like those data with 15",
    "start": "513959",
    "end": "522159"
  },
  {
    "text": "seconds like resolution because that's too much and you are not actually using all of them because premisus has",
    "start": "522159",
    "end": "528320"
  },
  {
    "text": "something called Step so it will only query get only one sample every some",
    "start": "528320",
    "end": "533959"
  },
  {
    "text": "interval so with compactor perform down sampling uh it will try to um redu uh",
    "start": "533959",
    "end": "541360"
  },
  {
    "text": "reduce the resolution from like 15 seconds to like 5 minutes or 1 hour so",
    "start": "541360",
    "end": "546640"
  },
  {
    "text": "when you qu longterm data your quir will become much faster so I think that's basically all",
    "start": "546640",
    "end": "552959"
  },
  {
    "text": "the kind of the kind of one architecture we called like side car pattern where",
    "start": "552959",
    "end": "559040"
  },
  {
    "text": "you have uh the the metrics come from your in in cluster premisas and it's",
    "start": "559040",
    "end": "564480"
  },
  {
    "text": "very easy to just deploy some side car to collect those metrics but",
    "start": "564480",
    "end": "570079"
  },
  {
    "text": "uh I think now there are more and more use case where the the environment are kind of erass only which means you",
    "start": "570079",
    "end": "577040"
  },
  {
    "text": "cannot deploy the side car to a different cluster and use your sound",
    "start": "577040",
    "end": "582200"
  },
  {
    "text": "quirer to find out a quiry and to send to that environment now we have more and more agent like otel collector",
    "start": "582200",
    "end": "588880"
  },
  {
    "text": "Prometheus agent and maybe just Prometheus server and they all use uh",
    "start": "588880",
    "end": "594120"
  },
  {
    "text": "remote right and uh zos also has a component that has Windows remote right",
    "start": "594120",
    "end": "601079"
  },
  {
    "text": "and it's called Uh Sonos receiver and Sonos receiver uh it mainly consists uh",
    "start": "601079",
    "end": "608000"
  },
  {
    "text": "of two components actually uh and they serve different functionality and the",
    "start": "608000",
    "end": "613839"
  },
  {
    "text": "first one is called uh sound router and as the name uh like suggests router is",
    "start": "613839",
    "end": "620399"
  },
  {
    "text": "basically it receiv receives uh the incoming remote request it doesn't have any storage but it just distributes or",
    "start": "620399",
    "end": "628399"
  },
  {
    "text": "forwards the request Quest into its backand so this component is pure stateless and you can scale up easily",
    "start": "628399",
    "end": "635480"
  },
  {
    "text": "based on some some some Matrix like HPA and uh the backend storage is called uh",
    "start": "635480",
    "end": "642320"
  },
  {
    "text": "Sonos receiver ingestor it's basically uh runs the same premisus time series",
    "start": "642320",
    "end": "647760"
  },
  {
    "text": "database locally and store the incoming metrix uh into the",
    "start": "647760",
    "end": "652959"
  },
  {
    "text": "database so it also has a component uh internal component called hash string",
    "start": "652959",
    "end": "658959"
  },
  {
    "text": "and cashing is used to distribute the time series into different uh receiver",
    "start": "658959",
    "end": "664040"
  },
  {
    "text": "node and an important feature here is called uh data replication because uh we",
    "start": "664040",
    "end": "670720"
  },
  {
    "text": "want to ensure data are high available so when the uh time series or samples",
    "start": "670720",
    "end": "676200"
  },
  {
    "text": "coming in um the receiver or router will try to replicate the data to different",
    "start": "676200",
    "end": "682360"
  },
  {
    "text": "nodes so that we keep multiple copy of your data so this is usually called another",
    "start": "682360",
    "end": "689320"
  },
  {
    "text": "kind of approach or another pattern called receiver pattern but the both pattern like side car and receiver",
    "start": "689320",
    "end": "696519"
  },
  {
    "text": "pattern they can they can like deploy together to make it a hybrid",
    "start": "696519",
    "end": "703399"
  },
  {
    "text": "approach so next I'm going to introduce some kind of U more advanced um use case",
    "start": "703399",
    "end": "711680"
  },
  {
    "text": "for example there's a component called stateless ruler and the reason why we",
    "start": "711680",
    "end": "716959"
  },
  {
    "text": "need this kind of stateless ruler um is that the existing ruler uh is like like",
    "start": "716959",
    "end": "724440"
  },
  {
    "text": "the graph had uh we deploy it in the same cluster because it needs to query",
    "start": "724440",
    "end": "730000"
  },
  {
    "text": "uh s query and uh the issue here is that it stores all the data like recording",
    "start": "730000",
    "end": "735639"
  },
  {
    "text": "rules and alerts matric into its own local time series database for tunnel",
    "start": "735639",
    "end": "741079"
  },
  {
    "text": "query to query but the issue here is that it makes it super hard for T of square rate to horizontally scale up",
    "start": "741079",
    "end": "748360"
  },
  {
    "text": "because time serious database itself is uh is stateful so um that's why we had a",
    "start": "748360",
    "end": "755360"
  },
  {
    "text": "new kind of a different mode of ruler called stateless ruler as the name suggest it doesn't have any storage what",
    "start": "755360",
    "end": "762800"
  },
  {
    "text": "it does is that the ruler will try to evaluate the quiry from um your thos",
    "start": "762800",
    "end": "768279"
  },
  {
    "text": "quirer and for the generated Matrix like recording rules and alerts matric it",
    "start": "768279",
    "end": "773839"
  },
  {
    "text": "will try to remote right into um the remote right path like like a router so",
    "start": "773839",
    "end": "779760"
  },
  {
    "text": "this way we can horizontally scale the stateless ruler component and uh yeah I",
    "start": "779760",
    "end": "786040"
  },
  {
    "text": "think it's kind of um relieve some relieve uh the operator from uh",
    "start": "786040",
    "end": "791519"
  },
  {
    "text": "horizontally scaning up existing ruler and uh so this is about Rule and",
    "start": "791519",
    "end": "799560"
  },
  {
    "text": "uh there are a lot of question coming from user about how can we improve the performance of our uh sound quar and the",
    "start": "799560",
    "end": "808079"
  },
  {
    "text": "whole sound quar pan is kind of complex complex but um here we have a new",
    "start": "808079",
    "end": "813800"
  },
  {
    "text": "component called uh sound sare for end and it's basically kind of a proxy",
    "start": "813800",
    "end": "819519"
  },
  {
    "text": "between your Sonos query and your uh your data visualization layer usually Guana so it has some kind of magic of uh",
    "start": "819519",
    "end": "827680"
  },
  {
    "text": "request uh sharding and it can Shard either vertically or either horizontally",
    "start": "827680",
    "end": "833480"
  },
  {
    "text": "like by day and it also does something like results caching because uh when",
    "start": "833480",
    "end": "838680"
  },
  {
    "text": "user use their Guana dashboard they usually maybe it's highly likely to look at maybe similar time range or they look",
    "start": "838680",
    "end": "845800"
  },
  {
    "text": "at the same dashboard so makes total sense to catch the the results so that they can reuse and to make dashboard",
    "start": "845800",
    "end": "852360"
  },
  {
    "text": "load much faster and for your long-term storage um the component called store Gateway we",
    "start": "852360",
    "end": "859240"
  },
  {
    "text": "also introduce something new uh which is the index cache and Trunks cache and",
    "start": "859240",
    "end": "864680"
  },
  {
    "text": "they are basically mam cach clusters or can be redis cluster and can basically",
    "start": "864680",
    "end": "870279"
  },
  {
    "text": "cach what you quiry um for your uh each block on S3 and make it much faster to",
    "start": "870279",
    "end": "876839"
  },
  {
    "text": "serve the quiry results next time so yeah I think that's all I have",
    "start": "876839",
    "end": "883920"
  },
  {
    "text": "for the uh introduction part sorry next um I'm going to um talk about some um",
    "start": "883920",
    "end": "891839"
  },
  {
    "text": "common pit FS I see so as a maintainer so I look at lot uh on get have issues",
    "start": "891839",
    "end": "900160"
  },
  {
    "text": "and slack messages I feel like the most kind of common Pitfall or most asked",
    "start": "900160",
    "end": "907160"
  },
  {
    "text": "question is is about compactor because compactor is something user feel like",
    "start": "907160",
    "end": "912600"
  },
  {
    "text": "very hard to handle it's very hard to operate and they don't know how to deal with this data and don't know how how it",
    "start": "912600",
    "end": "918320"
  },
  {
    "text": "works so a lot a question I got a lot is um like I have set up some kind of",
    "start": "918320",
    "end": "923680"
  },
  {
    "text": "retention for my uh data but even Beyond this kind of uh time range I still see",
    "start": "923680",
    "end": "930560"
  },
  {
    "text": "my my bucket or sorry my my blocks staying in my S3 bucket and let's say I",
    "start": "930560",
    "end": "936240"
  },
  {
    "text": "have a 10day retention but I can still see my onee data there and also",
    "start": "936240",
    "end": "941360"
  },
  {
    "text": "similarly um for down sampling and you ask like why I cannot see any of my",
    "start": "941360",
    "end": "947600"
  },
  {
    "text": "blocks getting down sampled why I can only see my raw block here so to",
    "start": "947600",
    "end": "952759"
  },
  {
    "text": "understand this question so let's first understand like how compactor itself works so comp Factor itself is basically",
    "start": "952759",
    "end": "960199"
  },
  {
    "text": "a job and it runs uh in a infinite Loop and the job has three stages the first",
    "start": "960199",
    "end": "966519"
  },
  {
    "text": "one is compaction the second one is down sampling and third one is retention but",
    "start": "966519",
    "end": "971759"
  },
  {
    "text": "this is not really deleting your data but Mark your data as uh kind of uh",
    "start": "971759",
    "end": "977120"
  },
  {
    "text": "deletable and the compactor will try to lazily delete it it after some time so",
    "start": "977120",
    "end": "983440"
  },
  {
    "text": "these three uh three stages I think the issue here is that for each stage you",
    "start": "983440",
    "end": "988920"
  },
  {
    "text": "you need to finish the first stage to go to this stage for example like if you want to have your block to be down",
    "start": "988920",
    "end": "994440"
  },
  {
    "text": "sampled you need to finish all the compaction work available in your in your bucket if you want to have your uh",
    "start": "994440",
    "end": "1000920"
  },
  {
    "text": "retention to happen you need to have all your compaction work and down sampling to finish first so that's why a lot of",
    "start": "1000920",
    "end": "1008079"
  },
  {
    "text": "uh people ask why they don't see any down sampling why they don't see any retention it's because they have some",
    "start": "1008079",
    "end": "1013480"
  },
  {
    "text": "kind of backlog in their compactor and uh like initially maybe it's just some",
    "start": "1013480",
    "end": "1019240"
  },
  {
    "text": "compactor or some blocks slow down but over time like those things piled up so",
    "start": "1019240",
    "end": "1024760"
  },
  {
    "text": "you will have a huge backlog uh there so how can we help or how can we",
    "start": "1024760",
    "end": "1030678"
  },
  {
    "text": "travel to this kind of issue so um the most important thing for me is to make",
    "start": "1030679",
    "end": "1036240"
  },
  {
    "text": "sure your compactor itself is up hand running and there's a metric called",
    "start": "1036240",
    "end": "1042038"
  },
  {
    "text": "tunnel compact Haled and I recommend uh every tunnel user to use it and common",
    "start": "1042039",
    "end": "1049160"
  },
  {
    "text": "issue I heard is that I have my tunnel uh side car running and they are up and",
    "start": "1049160",
    "end": "1054240"
  },
  {
    "text": "ready in my kuet cluster um but I don't see my compa compactor actually doing",
    "start": "1054240",
    "end": "1060120"
  },
  {
    "text": "any compaction this is usually because the compactor might encounter some uh",
    "start": "1060120",
    "end": "1065320"
  },
  {
    "text": "arrows that they it cannot handle so it might need some kind of human interaction uh intervention but it's",
    "start": "1065320",
    "end": "1072720"
  },
  {
    "text": "something like uh uh the operator need to take care and have alarm to properly",
    "start": "1072720",
    "end": "1078159"
  },
  {
    "text": "handle it other otherwise you will have your backlog growing over and over so after we ensure um uh the SOS",
    "start": "1078159",
    "end": "1087600"
  },
  {
    "text": "compaction uh compactor is running and it's not hted next we want to identify",
    "start": "1087600",
    "end": "1093720"
  },
  {
    "text": "like if we really have a backlog or and uh see how bad our backlog is and uh so",
    "start": "1093720",
    "end": "1101159"
  },
  {
    "text": "SOS provide a metric called SOS compact Todo compaction So based on its name",
    "start": "1101159",
    "end": "1108320"
  },
  {
    "text": "it's basic basically tells you how many compaction you are unfinished and you",
    "start": "1108320",
    "end": "1114000"
  },
  {
    "text": "need to finish so if you see some a pattern like this uh it goes up and down",
    "start": "1114000",
    "end": "1120360"
  },
  {
    "text": "which is usually fine because it takes time for compactor to finish its work but if you see some uh ever growing um",
    "start": "1120360",
    "end": "1129200"
  },
  {
    "text": "uh series which means you have kind of piled compaction over time and this is",
    "start": "1129200",
    "end": "1134360"
  },
  {
    "text": "something um it will be good to have some kind of alarm or um",
    "start": "1134360",
    "end": "1139400"
  },
  {
    "text": "to to remind you that the compactor has such kind of problem so so we we talk about how to",
    "start": "1139400",
    "end": "1148520"
  },
  {
    "text": "identify uh the backlog and how to uh what about how to solve it and the most",
    "start": "1148520",
    "end": "1154240"
  },
  {
    "text": "common way to solve compaction backlog is just to sh your compactor because",
    "start": "1154240",
    "end": "1159919"
  },
  {
    "text": "usually if you just want run one compactor it has limited concurrency and",
    "start": "1159919",
    "end": "1165240"
  },
  {
    "text": "uh it's only handling like limited number of compact a time but if you can",
    "start": "1165240",
    "end": "1171039"
  },
  {
    "text": "run different compactor Shard and they are able to handle like for example",
    "start": "1171039",
    "end": "1176400"
  },
  {
    "text": "different external label or different blocks from different uh clusters at the same time then you have much more higher",
    "start": "1176400",
    "end": "1183240"
  },
  {
    "text": "concurrency to catch up the whole uh compaction backlog but the issue here is that",
    "start": "1183240",
    "end": "1190760"
  },
  {
    "text": "compaction like sharding compactor Shing is not always a uh Silver Bullet and uh",
    "start": "1190760",
    "end": "1198159"
  },
  {
    "text": "the import thing here is also like we also need to find find out what is a real bottl neck for your um compaction",
    "start": "1198159",
    "end": "1205480"
  },
  {
    "text": "slowness so um take some example from our production cluster we do see some",
    "start": "1205480",
    "end": "1211200"
  },
  {
    "text": "kind of issues like this for example like we found out that it's very slow to download uh the block from your object",
    "start": "1211200",
    "end": "1218080"
  },
  {
    "text": "storage and there could be several reasons but usually it's not uh an issue",
    "start": "1218080",
    "end": "1223360"
  },
  {
    "text": "if you use some CL object storage from your cloud provider and as that's time",
    "start": "1223360",
    "end": "1228880"
  },
  {
    "text": "maybe you should double check your uh concurrency to download your uh block files and there's a flag internals to",
    "start": "1228880",
    "end": "1236000"
  },
  {
    "text": "tune this kind of uh parameter it's basically uh a Blog can have multiple",
    "start": "1236000",
    "end": "1241480"
  },
  {
    "text": "files or objects in it and with higher concurrency you can download multiple files a time for a single",
    "start": "1241480",
    "end": "1247600"
  },
  {
    "text": "block and the second reason um is slow to write to disk and this it might be uh",
    "start": "1247600",
    "end": "1254640"
  },
  {
    "text": "might be common if you use some kind of uh network storage uh in some call",
    "start": "1254640",
    "end": "1259799"
  },
  {
    "text": "provider for example EBS and we had this kind of issue before like it's very like",
    "start": "1259799",
    "end": "1265400"
  },
  {
    "text": "slow to download your block and also during your compaction time you need to write the data to your disk and uh we",
    "start": "1265400",
    "end": "1272279"
  },
  {
    "text": "found out that after uh increasing the iops for our EBS volume on AWS we cut",
    "start": "1272279",
    "end": "1278919"
  },
  {
    "text": "the compaction Time by half so the third point is slow to list",
    "start": "1278919",
    "end": "1285279"
  },
  {
    "text": "objects from bucket so compactor does this all the time because it needs to",
    "start": "1285279",
    "end": "1291120"
  },
  {
    "text": "understand what kind of blocks you have what kind of blocks uh how many blocks there are marked as uh deltion how many",
    "start": "1291120",
    "end": "1297360"
  },
  {
    "text": "blocks they are marked as uh no no compatible um and uh so they list your",
    "start": "1297360",
    "end": "1303960"
  },
  {
    "text": "S3 bucket all the time and an issue we found uh and maybe it's um kind of a",
    "start": "1303960",
    "end": "1309520"
  },
  {
    "text": "specific issue if you use something like S3 and for our use case we turn on the",
    "start": "1309520",
    "end": "1315120"
  },
  {
    "text": "object versioning in our S3 bucket and over time these list objects become",
    "start": "1315120",
    "end": "1320880"
  },
  {
    "text": "super slow because seems like S3 uh list list API implementation uh will be",
    "start": "1320880",
    "end": "1327159"
  },
  {
    "text": "impacted by the number uh by the version uh the object version you have in your whole bucket so what we did is we tried",
    "start": "1327159",
    "end": "1334080"
  },
  {
    "text": "to clean up the old old versions We don't keep too many version histories and we also reduce the sort of like the",
    "start": "1334080",
    "end": "1341760"
  },
  {
    "text": "retention time for your version and after that we found out that the list operation is much faster",
    "start": "1341760",
    "end": "1349919"
  },
  {
    "text": "so yeah so that's um oh by the way so we also have these kind of compactor",
    "start": "1350760",
    "end": "1356559"
  },
  {
    "text": "backlog uh documentation so if you really have this kind of issue in your production environment please go to uh",
    "start": "1356559",
    "end": "1363480"
  },
  {
    "text": "the s website and it has some kind of detail troubleshoot and uh tools and uh",
    "start": "1363480",
    "end": "1370120"
  },
  {
    "text": "so to help you uh fix this this issue next uh I'm going to talk about",
    "start": "1370120",
    "end": "1377200"
  },
  {
    "text": "another pit for that's related to compactor and I got this kind of question a lot because uh user will ask",
    "start": "1377200",
    "end": "1384679"
  },
  {
    "text": "like we don't have any compactor backlog but why I'm only able to see my block",
    "start": "1384679",
    "end": "1390559"
  },
  {
    "text": "down sample to five minutes why I cannot see any block uh with one hour uh",
    "start": "1390559",
    "end": "1396520"
  },
  {
    "text": "resolution um so I got a lot of um issues like this and I want to like",
    "start": "1396520",
    "end": "1402039"
  },
  {
    "text": "clarify it bit so uh first of all let me uh introduce like how this kind of down",
    "start": "1402039",
    "end": "1407520"
  },
  {
    "text": "sampling works so each block if it needs to be down sampled it need to meet uh",
    "start": "1407520",
    "end": "1413320"
  },
  {
    "text": "certain criteria for example like if we I have a uh raw block and I want to uh",
    "start": "1413320",
    "end": "1419440"
  },
  {
    "text": "convert this raw block into a five minute like down sample block and the",
    "start": "1419440",
    "end": "1425480"
  },
  {
    "text": "criteria here is that the block length or the block time range basically the",
    "start": "1425480",
    "end": "1431600"
  },
  {
    "text": "max time of this block uh minus uh meantime of this block needs to be",
    "start": "1431600",
    "end": "1436640"
  },
  {
    "text": "larger or equal uh uh 48 hour so a common um misunderstanding is that as",
    "start": "1436640",
    "end": "1444039"
  },
  {
    "text": "long as my block uh created like 40 hour ago uh it should be fine but that's not",
    "start": "1444039",
    "end": "1449720"
  },
  {
    "text": "the case so you your block needs to grow large enough so that it can be down",
    "start": "1449720",
    "end": "1454760"
  },
  {
    "text": "sampled and uh similarly for 5 minute block it's kind of similar it needs to",
    "start": "1454760",
    "end": "1460640"
  },
  {
    "text": "be uh larger or equal uh than 10 days so that it can be done sampled into one",
    "start": "1460640",
    "end": "1466640"
  },
  {
    "text": "hour it's not not like the block age uh like what was 10 day before it's it's",
    "start": "1466640",
    "end": "1472159"
  },
  {
    "text": "not the same thing but um then let's take a look at",
    "start": "1472159",
    "end": "1477960"
  },
  {
    "text": "how we convert uh a single like 2hour block and eventually it becomes a 1 hour",
    "start": "1477960",
    "end": "1483919"
  },
  {
    "text": "down sample block so the first line here is um is",
    "start": "1483919",
    "end": "1489360"
  },
  {
    "text": "something we call block ranges it's basically um what compactor use and uh",
    "start": "1489360",
    "end": "1495679"
  },
  {
    "text": "basically the block it will try to create uh during the uh compaction process so for example if you have twoh",
    "start": "1495679",
    "end": "1502360"
  },
  {
    "text": "hour block your compactor Will Wait enough time and it will try to uh",
    "start": "1502360",
    "end": "1507760"
  },
  {
    "text": "compact your 2hour block into 8 hour and then your 8 Hour block it will compact to Next Level which is 48 hours and Next",
    "start": "1507760",
    "end": "1516240"
  },
  {
    "text": "Level which is 14 days so it's like this so there's no um easy way to I think",
    "start": "1516240",
    "end": "1522159"
  },
  {
    "text": "it's something configurable but this is the default value uh compactor have so let's say we have those kind of",
    "start": "1522159",
    "end": "1529760"
  },
  {
    "text": "uh 2hour block at the beginning and they got compacted uh into 48h hour",
    "start": "1529760",
    "end": "1535559"
  },
  {
    "text": "block and now 48 hour block it already meets our criteria because it's uh",
    "start": "1535559",
    "end": "1541159"
  },
  {
    "text": "larger than 40 hour so we can down sample it into 5 minute resolution and similarly 48 hour block",
    "start": "1541159",
    "end": "1548880"
  },
  {
    "text": "and it it waits long time long time enough it will be uh down uh sorry",
    "start": "1548880",
    "end": "1554919"
  },
  {
    "text": "compacted into a 14-day block with 5 minutes uh resolution and because a",
    "start": "1554919",
    "end": "1560919"
  },
  {
    "text": "14-day Block it's already uh larger than 10day it's able to be uh down sampled to",
    "start": "1560919",
    "end": "1566399"
  },
  {
    "text": "1 hour so the issue here is that even though we allow 10 day like block blocks",
    "start": "1566399",
    "end": "1571919"
  },
  {
    "text": "larger than 10 day to be down sampled to 1 hour because of the default competion",
    "start": "1571919",
    "end": "1577399"
  },
  {
    "text": "like time range is 14 day you need to wait enough time like 14 day for your",
    "start": "1577399",
    "end": "1582559"
  },
  {
    "text": "blog to be grow large enough and then it's done sampow to 1 hour",
    "start": "1582559",
    "end": "1590240"
  },
  {
    "text": "and uh so a lot of users they will maybe come up with this kind of configuration like uh the first level maybe I just",
    "start": "1590320",
    "end": "1597120"
  },
  {
    "text": "need to keep one two day so that it's able to be down sample to 5 minutes and the second level of five minutes one I",
    "start": "1597120",
    "end": "1604320"
  },
  {
    "text": "keep like two week so it's able to be down sampled into 1 hour but um that's",
    "start": "1604320",
    "end": "1610120"
  },
  {
    "text": "usually not the case because you need to take your compaction time into account",
    "start": "1610120",
    "end": "1615240"
  },
  {
    "text": "because you will have your 2hour block merge together into uh into two-day",
    "start": "1615240",
    "end": "1620360"
  },
  {
    "text": "block and your two-day block it takes time for them to merge into um two week",
    "start": "1620360",
    "end": "1626640"
  },
  {
    "text": "block so especially you have those kind of compaction backlog it's usually like",
    "start": "1626640",
    "end": "1633559"
  },
  {
    "text": "when you when you want to um start the compaction it's usually like your block",
    "start": "1633559",
    "end": "1638799"
  },
  {
    "text": "got deleted before it's got even compacted to to be large enough to be done sampled",
    "start": "1638799",
    "end": "1646880"
  },
  {
    "text": "so that was the second um pit for I want to talk about and the last one I want to",
    "start": "1649120",
    "end": "1654679"
  },
  {
    "text": "discuss is about a different the duplication algorithm so there are two",
    "start": "1654679",
    "end": "1660480"
  },
  {
    "text": "kind of D duplication algorithm in uh inos and uh they're also used in",
    "start": "1660480",
    "end": "1667679"
  },
  {
    "text": "compactor so the first one I'm going to introduce is called One to1 D duplication so as the name suggests um",
    "start": "1667679",
    "end": "1675480"
  },
  {
    "text": "this kind of duplication algorithm it duplicates data that's exactly the same",
    "start": "1675480",
    "end": "1681399"
  },
  {
    "text": "which means they should have the same time stamp they should have the same value and this uh the duplication",
    "start": "1681399",
    "end": "1688760"
  },
  {
    "text": "algorithm is useful usually in the scenario uh of uh data replication of",
    "start": "1688760",
    "end": "1694480"
  },
  {
    "text": "your receiver for example your receiver has some uh replication Factor set to three which means a sing a single sample",
    "start": "1694480",
    "end": "1702320"
  },
  {
    "text": "will be replicated to three different uh uh receiver nodes",
    "start": "1702320",
    "end": "1708320"
  },
  {
    "text": "and uh you can see here they Bally basically have the same time stamp and the same value and even though maybe one",
    "start": "1708320",
    "end": "1714519"
  },
  {
    "text": "node they don't have these kind of they could miss one sample because we do Quorum but it's also fine and if you",
    "start": "1714519",
    "end": "1721519"
  },
  {
    "text": "have the one to one duplication at the end it will be all those samples will be merged uh for the same time stamp and",
    "start": "1721519",
    "end": "1729039"
  },
  {
    "text": "value So eventually you will have what you uh initially send to Tunnel basically a non uh duplicated a non",
    "start": "1729039",
    "end": "1737080"
  },
  {
    "text": "duplicated um series samples basically and uh yeah so if you use",
    "start": "1737080",
    "end": "1744080"
  },
  {
    "text": "receiver please try to use uh one to1 duplication algorithm and the second um duplication",
    "start": "1744080",
    "end": "1752519"
  },
  {
    "text": "algorithm is called penalty and it's usually when you have a ha premisus pair",
    "start": "1752519",
    "end": "1758039"
  },
  {
    "text": "and this algorithm uh sorry this scenario it doesn't really work well with one 121 because one 121 only works",
    "start": "1758039",
    "end": "1764480"
  },
  {
    "text": "if you have the same time stamp but for H pair use case usually the time stamp",
    "start": "1764480",
    "end": "1770279"
  },
  {
    "text": "are different because it's two peria server scraping so there are two different processes so for example they",
    "start": "1770279",
    "end": "1776760"
  },
  {
    "text": "might uh replica 2 they might have all the like like sample delay 1 second in",
    "start": "1776760",
    "end": "1782120"
  },
  {
    "text": "this case one 12 one will not be able to merge or dup duplicate these two so what",
    "start": "1782120",
    "end": "1787600"
  },
  {
    "text": "we have is called uh panalty duplication what it does is basically it try to pick",
    "start": "1787600",
    "end": "1793559"
  },
  {
    "text": "one sample from one replica on each time interval and every time you when you",
    "start": "1793559",
    "end": "1799320"
  },
  {
    "text": "have a gap it will try to pick another like replica where it has this kind of sample so for example here like",
    "start": "1799320",
    "end": "1806200"
  },
  {
    "text": "initially we start picking uh samples from uh repli replica one but then like",
    "start": "1806200",
    "end": "1811840"
  },
  {
    "text": "replica one has a gap it will try to switch to pick replica to so that's a use case for um the panel",
    "start": "1811840",
    "end": "1820480"
  },
  {
    "text": "the duplication for your uh prus H here so next I want to give some quick",
    "start": "1820480",
    "end": "1829080"
  },
  {
    "text": "uh updates for project updates uh because we don't have enough time so I will just um um cover like uh some",
    "start": "1829080",
    "end": "1836919"
  },
  {
    "text": "features so we had to main release um since last ccon EU and I want to",
    "start": "1836919",
    "end": "1842960"
  },
  {
    "text": "introduce this feature called uh cabin uh Proto replication so basically a a",
    "start": "1842960",
    "end": "1848320"
  },
  {
    "text": "request come to router and router will replicate the request to receiver and",
    "start": "1848320",
    "end": "1853440"
  },
  {
    "text": "now the uh the current protocol for uh the replication is all using prus remote",
    "start": "1853440",
    "end": "1859679"
  },
  {
    "text": "ride and uh but since uh router and node they are kind of uh and receiver they",
    "start": "1859679",
    "end": "1865880"
  },
  {
    "text": "are kind of internal uh communication protocol so tunnels can experiment uh whatever we want for example like remote",
    "start": "1865880",
    "end": "1873200"
  },
  {
    "text": "right to do zero or something else like more performant for SOS and one of the",
    "start": "1873200",
    "end": "1878320"
  },
  {
    "text": "SOS maintainer Philip experiment with this called Capen Proto uh which has",
    "start": "1878320",
    "end": "1884360"
  },
  {
    "text": "better performance on the uh deserialization of the uh request so",
    "start": "1884360",
    "end": "1890279"
  },
  {
    "text": "there are some data I can show basically they have kind of um some reduction on",
    "start": "1890279",
    "end": "1896960"
  },
  {
    "text": "the uh memory usage of their receiver cluster and I think their CPU usage of",
    "start": "1896960",
    "end": "1902159"
  },
  {
    "text": "their receiver cluster also reduced by half next I want to talk about uh Thanos",
    "start": "1902159",
    "end": "1910559"
  },
  {
    "text": "per uh Thanos per care engine so since last cucon we're mainly working on",
    "start": "1910559",
    "end": "1916279"
  },
  {
    "text": "improving the compatibility of the SOS uh PR prom care Engine with the Upstream",
    "start": "1916279",
    "end": "1922240"
  },
  {
    "text": "prom care engine so we added support for query stat statistics which is something",
    "start": "1922240",
    "end": "1928360"
  },
  {
    "text": "when you specify stats equal to all you will have you can see like how many samples you process now this is",
    "start": "1928360",
    "end": "1934399"
  },
  {
    "text": "available in the new prom care engine and we added support for warning annotation active query tracker uh and",
    "start": "1934399",
    "end": "1941919"
  },
  {
    "text": "we also uh added more um prom care function coverage natively in the prom care um engine so we added uh more",
    "start": "1941919",
    "end": "1950240"
  },
  {
    "text": "function support like native hisr functions as well as some experimental functions in premisus such as sort by",
    "start": "1950240",
    "end": "1959320"
  },
  {
    "text": "labels next I want to talk about the next step of the T project and in our",
    "start": "1959679",
    "end": "1965360"
  },
  {
    "text": "road map we still need to finish uh Native hisr support I think now the only",
    "start": "1965360",
    "end": "1970960"
  },
  {
    "text": "thing left is the down sampling support for Native histogram and we are looking for",
    "start": "1970960",
    "end": "1976679"
  },
  {
    "text": "contributor and and the next big thing is premisus 3.0 because I think uh",
    "start": "1976679",
    "end": "1982720"
  },
  {
    "text": "premisus 3.0 uh released yesterday and it's time for like ecosystem project",
    "start": "1982720",
    "end": "1988559"
  },
  {
    "text": "like SOS to catch up and uh to be more compatible with premises so we need to",
    "start": "1988559",
    "end": "1994080"
  },
  {
    "text": "work on remote R 2.0 and we want to have a kind of overhot UI for Thanos like",
    "start": "1994080",
    "end": "1999440"
  },
  {
    "text": "what prisia 3.0 does and we also want to embrace um the open Teter ecosystem to",
    "start": "1999440",
    "end": "2005399"
  },
  {
    "text": "support OTP metric and a lot more more so yeah I think that's all for",
    "start": "2005399",
    "end": "2012159"
  },
  {
    "text": "today's talk and uh thank you everyone for for joining and I think we have only like two minutes for for",
    "start": "2012159",
    "end": "2020158"
  },
  {
    "text": "question um yeah if you have question please you can just uh go to the mic and",
    "start": "2024120",
    "end": "2030039"
  },
  {
    "text": "uh ask or you can just ask",
    "start": "2030039",
    "end": "2034919"
  },
  {
    "text": "thanks for the update Ben um I see OTL support is road map for Thanos uh I'm",
    "start": "2039120",
    "end": "2044919"
  },
  {
    "text": "assuming that's on the iness path I was wondering if there's ever been any discussion of um potentially building in",
    "start": "2044919",
    "end": "2051878"
  },
  {
    "text": "like OTP remote right into the Thanos receive router some of us work at companies that have our own proprietary",
    "start": "2051879",
    "end": "2058158"
  },
  {
    "text": "tsdb backends and found it very challenging to like route Prometheus metrics into those backends without",
    "start": "2058159",
    "end": "2064358"
  },
  {
    "text": "diving deep into open Telemetry and completely replacing the collection layer there and I'm wondering if there's like any ideas floating around about",
    "start": "2064359",
    "end": "2071440"
  },
  {
    "text": "ways to like more deeply integrate those into Thanos so is your question mainly like",
    "start": "2071440",
    "end": "2078440"
  },
  {
    "text": "adding uh sorry odlp metric uh support in in receive router or that's something",
    "start": "2078440",
    "end": "2086480"
  },
  {
    "text": "else yeah potentially um I mean that's like one component in Thanos where all the matrics are flowing too and then",
    "start": "2086480",
    "end": "2093000"
  },
  {
    "text": "getting remote written to inors but I mean uh Thanos r perhaps could remote",
    "start": "2093000",
    "end": "2098960"
  },
  {
    "text": "write OTP metrics directly into proprietary backend but um uh if if it's",
    "start": "2098960",
    "end": "2104280"
  },
  {
    "text": "not being collected in the ruler uh coming directly off of Prometheus it's hard to get those metrics into a",
    "start": "2104280",
    "end": "2110800"
  },
  {
    "text": "proprietary backend without building out a otel collection layer as well yeah I think we are definitely open I think for",
    "start": "2110800",
    "end": "2117160"
  },
  {
    "text": "the uh the receiver support and the receive router support we definitely I think we already have an issue I think",
    "start": "2117160",
    "end": "2123480"
  },
  {
    "text": "there have been some kind of discussion um for the past couple months and uh I",
    "start": "2123480",
    "end": "2130280"
  },
  {
    "text": "think we it's definitely on our road map and we want to support it it's just uh we need some contributor to help to to",
    "start": "2130280",
    "end": "2136839"
  },
  {
    "text": "to to to do the work yeah cool I think it's it's time and uh",
    "start": "2136839",
    "end": "2142960"
  },
  {
    "text": "thanks everyone um for Johnny thank you",
    "start": "2142960",
    "end": "2149000"
  }
]