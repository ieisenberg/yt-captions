[
  {
    "text": "all right Grandia started so when",
    "start": "0",
    "end": "6000"
  },
  {
    "text": "designing this talk I thought that we were going to be 50-minute sessions so",
    "start": "6000",
    "end": "11670"
  },
  {
    "text": "what we're going to do is I'm going to go through all the content but what I've done is I've put everything up on github",
    "start": "11670",
    "end": "17430"
  },
  {
    "text": "as well so for some of them I'm going to have to skip over the demo portion and just talk about things in abstract so",
    "start": "17430",
    "end": "26279"
  },
  {
    "text": "the purpose of this talk around",
    "start": "26279",
    "end": "31470"
  },
  {
    "text": "kubernetes want to oh I gave a talk called real-world kubernetes which went",
    "start": "31470",
    "end": "36840"
  },
  {
    "text": "through how to manage kubernetes and how kubernetes fails and this talk is kind",
    "start": "36840",
    "end": "44280"
  },
  {
    "text": "of updated for 2017 kubernetes we now have actually pretty good monitoring systems we have ways of managing some of",
    "start": "44280",
    "end": "51059"
  },
  {
    "text": "the complexity of the system and so wanted to kind of give an update on where that stuff stands if you're not",
    "start": "51059",
    "end": "58230"
  },
  {
    "text": "familiar with core OS we build a bunch of the open source projects in this ecosystem introduce things like at CD",
    "start": "58230",
    "end": "65518"
  },
  {
    "text": "and container linux and then we have products for large organizations and enterprises like tectonic and clay if",
    "start": "65519",
    "end": "73740"
  },
  {
    "text": "you want to learn more you can find out at core Escom and so in this",
    "start": "73740",
    "end": "79320"
  },
  {
    "text": "presentation what i'm using and getting a little bit of like echo or feedback on",
    "start": "79320",
    "end": "84390"
  },
  {
    "text": "stage what what I'm what I'm using to",
    "start": "84390",
    "end": "89700"
  },
  {
    "text": "set up these clusters is we have a thing called the tectonic installer which installs kubernetes clusters in a very",
    "start": "89700",
    "end": "96840"
  },
  {
    "text": "particular way this can all be replicated with upstream kubernetes because in fact tectonic is simply",
    "start": "96840",
    "end": "103860"
  },
  {
    "text": "upstream kubernetes but just as you if you want to follow along at home later",
    "start": "103860",
    "end": "109530"
  },
  {
    "text": "that's that's what I'm doing so the outline of the architecture or the discussion is I'm going to talk through",
    "start": "109530",
    "end": "115740"
  },
  {
    "text": "cluster architecture how things fail and how the design for cell failure the",
    "start": "115740",
    "end": "121439"
  },
  {
    "text": "monitoring and alerting once you've designed out the cluster architecture backup and disaster recovery primarily focusing on a CD",
    "start": "121439",
    "end": "129470"
  },
  {
    "text": "how customer upgrades are approached particularly with how we install kubernetes with Peconic and then scaling",
    "start": "129470",
    "end": "136100"
  },
  {
    "text": "a management of the cluster over time this is going to be quite a bit and on a couple of these topics particularly the",
    "start": "136100",
    "end": "141290"
  },
  {
    "text": "scaling and management and disaster recovery those are quite in-depth topics",
    "start": "141290",
    "end": "147170"
  },
  {
    "text": "solve the skipping over most of the details there so across your architecture I like to kind of describe",
    "start": "147170",
    "end": "155450"
  },
  {
    "text": "kubernetes like this you could of course manage a lot of machines yourself by writing like a while loop with SSH and",
    "start": "155450",
    "end": "162920"
  },
  {
    "text": "then just kind of doing the stuff and landing the containers within what happens when you know machines fail",
    "start": "162920",
    "end": "169640"
  },
  {
    "text": "there's no monitoring there's no state recovery and so we have you know these machines that we call the master",
    "start": "169640",
    "end": "174650"
  },
  {
    "text": "machines that we kind of pull off and they're the things that actually end up monitoring and maintaining the",
    "start": "174650",
    "end": "180680"
  },
  {
    "text": "kubernetes cluster because they're constantly monitoring and managing the state cool and so in the end what",
    "start": "180680",
    "end": "189200"
  },
  {
    "text": "they're really in charge of is there's a few machines in your cluster there's special the master machines and then there's a bunch of machines that work",
    "start": "189200",
    "end": "194780"
  },
  {
    "text": "with machines that take workloads from the master and then as machines fail etc",
    "start": "194780",
    "end": "200600"
  },
  {
    "text": "or you may need to scale up your application over time that's what those master machines are doing is is telling",
    "start": "200600",
    "end": "207410"
  },
  {
    "text": "everyone what to do cool so of course as we all know those three those three",
    "start": "207410",
    "end": "213860"
  },
  {
    "text": "machines there are actually the kubernetes control plane or better known as master nodes and then all those",
    "start": "213860",
    "end": "222049"
  },
  {
    "text": "master nodes there's a number of control plan components this includes things like the kubernetes scheduler API server",
    "start": "222049",
    "end": "229280"
  },
  {
    "text": "controller manager at CD etc everything",
    "start": "229280",
    "end": "234530"
  },
  {
    "text": "else are just regular worker notes or more generically reduced reporters in those nodes there's master nodes and then just notes",
    "start": "234530",
    "end": "242410"
  },
  {
    "text": "okay so in today's world to give you some context on the cluster architecture",
    "start": "242410",
    "end": "249010"
  },
  {
    "text": "that we've been going towards with with psychotic in today's world the",
    "start": "249010",
    "end": "256690"
  },
  {
    "text": "infrastructure and kubernetes are tied there's a bunch of tools like Google SH and bunch of other tools that both",
    "start": "256690",
    "end": "263590"
  },
  {
    "text": "deploy the infrastructure and deploy kubernetes all at once and so you use these tools to like upgrade your cluster",
    "start": "263590",
    "end": "269620"
  },
  {
    "text": "or whatever so a better way that I would argue is that we should be separating",
    "start": "269620",
    "end": "274840"
  },
  {
    "text": "out the infrastructure from kubernetes so the tools like the tectonic installer",
    "start": "274840",
    "end": "280449"
  },
  {
    "text": "just lay down infrastructure and then we have tools in particular kubernetes api",
    "start": "280449",
    "end": "285760"
  },
  {
    "text": "is themselves to manage kubernetes which means that we're able to use as I'll",
    "start": "285760",
    "end": "292240"
  },
  {
    "text": "show you later the kubernetes api is to say upgrade the kubernetes api is totally reasonable and",
    "start": "292240",
    "end": "298360"
  },
  {
    "text": "I'll tell you why that's totally reasonable later and the advantage to those dip sauce is that we can have",
    "start": "298360",
    "end": "304810"
  },
  {
    "text": "really consistent experiences across lots of different a bunch of different",
    "start": "304810",
    "end": "310930"
  },
  {
    "text": "cloud providers and bare-metal includes in etc ok so if you want to try",
    "start": "310930",
    "end": "318270"
  },
  {
    "text": "installing a self-hosted cluster that is using kubernetes to manage kubernetes you can find it here this installer runs",
    "start": "318270",
    "end": "327130"
  },
  {
    "text": "on all these different platforms ok all the demos and stuff that I'm about to dive into are at this github URL my name",
    "start": "327130",
    "end": "336310"
  },
  {
    "text": "spelled with 1l and you can find kubernetes day too",
    "start": "336310",
    "end": "343320"
  },
  {
    "text": "ok so the first thing that we want to talk about when we're thinking about day 2 operations with kubernetes is of",
    "start": "343320",
    "end": "349930"
  },
  {
    "text": "course failure because day 3 inevitably is going to bring failure so you should probably start planning pretty",
    "start": "349930",
    "end": "356080"
  },
  {
    "text": "immediately inside of failure there's a number of topics first we'll walk",
    "start": "356080",
    "end": "361479"
  },
  {
    "text": "through the master node failures of the control plane so again master nodes control plane components the Kuban IDs",
    "start": "361479",
    "end": "369130"
  },
  {
    "text": "control plane is fairly straightforward it looks like any other web database or web application you might have ran in",
    "start": "369130",
    "end": "375520"
  },
  {
    "text": "your path there's a data store there is an application the application is talked",
    "start": "375520",
    "end": "380860"
  },
  {
    "text": "to by clients the big difference here is that instead of that application say rendering a blog",
    "start": "380860",
    "end": "387969"
  },
  {
    "text": "or something the application which is the API server tells computers other",
    "start": "387969",
    "end": "393669"
  },
  {
    "text": "computers what to do there's a bunch of different components I'll talk through some of them in depth but that's the",
    "start": "393669",
    "end": "399849"
  },
  {
    "text": "rough architecture primary data store and a primary API server okay so the",
    "start": "399849",
    "end": "406959"
  },
  {
    "text": "first demonstration they'll go through is walking you through a cluster that is",
    "start": "406959",
    "end": "412089"
  },
  {
    "text": "self hosted where we're actually using kubernetes to manage two vanities so I'm serious a little bit of latency",
    "start": "412089",
    "end": "420520"
  },
  {
    "text": "so hopefully this isn't too painful but inside of this cluster there is a number",
    "start": "420520",
    "end": "426009"
  },
  {
    "text": "of components that may look familiar there's a qv- deployment for managing the controller manager physic unity",
    "start": "426009",
    "end": "432789"
  },
  {
    "text": "deployments for managing two BNs there's a kubernetes deployment for managing the kubernetes scheduler and so if you",
    "start": "432789",
    "end": "439689"
  },
  {
    "text": "actually click in here you can start to do super useful things like Oh figure out from the kubernetes api where is the",
    "start": "439689",
    "end": "446289"
  },
  {
    "text": "pod that's running the Tsukuba nettie scheduler using Prometheus you can start to find and generate and pull back",
    "start": "446289",
    "end": "454499"
  },
  {
    "text": "metadata about how much RAM and CPU and etc the components in your control plane are using all things that you would have",
    "start": "454499",
    "end": "461050"
  },
  {
    "text": "had to do anyways and so it's very good that they are managed in this way so",
    "start": "461050",
    "end": "468430"
  },
  {
    "text": "that's the big idea is that we're managing all these things whether it's the daemon the API server which runs a",
    "start": "468430",
    "end": "474879"
  },
  {
    "text": "daemon set for a variety of reasons or the API or the scheduler or the",
    "start": "474879",
    "end": "480369"
  },
  {
    "text": "networking system which runs as a daemon set etc all these things are managed as part of kubernetes on kubernetes",
    "start": "480369",
    "end": "487619"
  },
  {
    "text": "everyone get it I just showed hands that it makes sense what's going to show them so far okay cool okay so that's the",
    "start": "487619",
    "end": "496539"
  },
  {
    "text": "high-level concept now the the one little like fly in the ointment here is",
    "start": "496539",
    "end": "502479"
  },
  {
    "text": "that we also have to run a database and who can I get a show of hands and whose",
    "start": "502479",
    "end": "508060"
  },
  {
    "text": "ran databases on kubernetes okay uh-huh so a lot less confidence there so what",
    "start": "508060",
    "end": "517360"
  },
  {
    "text": "what we've built we'll all walk through at CD so FTD is a little bit special a",
    "start": "517360",
    "end": "525030"
  },
  {
    "text": "lot of applications that are really easy to deploy on top of kubernetes are stateless apps but this is a stateful",
    "start": "525030",
    "end": "532270"
  },
  {
    "text": "app which means that all the unjust states is unique to the node that it's",
    "start": "532270",
    "end": "538330"
  },
  {
    "text": "running on sed is useful and the reason that exterminators uses it is because",
    "start": "538330",
    "end": "543730"
  },
  {
    "text": "it's clustered and fully replicated you can think of it sort of like braid for",
    "start": "543730",
    "end": "549210"
  },
  {
    "text": "database in that every right into ends up getting written out to a bunch of",
    "start": "549210",
    "end": "556330"
  },
  {
    "text": "different replicas at the same time and then it requires of 50% plus one",
    "start": "556330",
    "end": "562720"
  },
  {
    "text": "availability to do any writes into the datastore so what that means for us those people",
    "start": "562720",
    "end": "568000"
  },
  {
    "text": "who have to operate this databases that we need to ensure that it's bonded gird and that we recover if any of the",
    "start": "568000",
    "end": "574630"
  },
  {
    "text": "machines go down as quickly as possible this is generally why the recommendation is of 5 nodes because you can have one I",
    "start": "574630",
    "end": "582100"
  },
  {
    "text": "tripped over the power table moment and then also a hardware randomly failed moment and still have 50% plus one ok so",
    "start": "582100",
    "end": "591820"
  },
  {
    "text": "what we've done is we've built this piece of technology to help operate at to be on top of kubernetes called the",
    "start": "591820",
    "end": "598840"
  },
  {
    "text": "sed operator what happens in practice is that you can actually go and ask",
    "start": "598840",
    "end": "604380"
  },
  {
    "text": "kubernetes to essentially have a special",
    "start": "604380",
    "end": "610180"
  },
  {
    "text": "resource SVD cluster resource so it gets close to that FCB",
    "start": "610180",
    "end": "617010"
  },
  {
    "text": "which says things like the cluster size which version of to brunette or which version of SED to run etc thing latency",
    "start": "617010",
    "end": "625960"
  },
  {
    "text": "is killing me so you see here that inside of our consider our namespace of cube system we",
    "start": "625960",
    "end": "633390"
  },
  {
    "text": "have a cube SPD which has a number of",
    "start": "633390",
    "end": "640650"
  },
  {
    "text": "events and then a cluster size and a particular version and so what does that",
    "start": "640650",
    "end": "646590"
  },
  {
    "text": "do the operator does is it sits there sort of like the controller manager for other components in kubernetes and it",
    "start": "646590",
    "end": "652380"
  },
  {
    "text": "observes and it essentially runs this reconciliation loop of state is the",
    "start": "652380",
    "end": "658320"
  },
  {
    "text": "cluster in the states that it should be I'll observe the current cluster state I'll analyze well the user said to run",
    "start": "658320",
    "end": "664110"
  },
  {
    "text": "five but only four running and then act it'll say well in order to get back up",
    "start": "664110",
    "end": "670290"
  },
  {
    "text": "to five I need to add another one this whole thing is open source and is really",
    "start": "670290",
    "end": "676380"
  },
  {
    "text": "slick for running SED on top of kubernetes what I'm going to show you",
    "start": "676380",
    "end": "683540"
  },
  {
    "text": "next is that we're actually running it both on top of and underneath kubernetes",
    "start": "683540",
    "end": "689250"
  },
  {
    "text": "and I'll show you why that space as well so in order to survive a failure of SPD",
    "start": "689250",
    "end": "696540"
  },
  {
    "text": "you need to do a few things you need to NAT CD for network latency activity relies on being able to know that a",
    "start": "696540",
    "end": "703170"
  },
  {
    "text": "given round trip they need its peers happens within some time bound you need",
    "start": "703170",
    "end": "708300"
  },
  {
    "text": "to maintain that 50% plus one availability you ensure that it's persistent disk if a node goes away and",
    "start": "708300",
    "end": "714089"
  },
  {
    "text": "comes back the same data should be there this is true and pretty much all cloud environments at least on a single",
    "start": "714089",
    "end": "720030"
  },
  {
    "text": "machine and that's sufficient for STD's for disaster recovery we have a out of",
    "start": "720030",
    "end": "726089"
  },
  {
    "text": "bound to lolled SPD snapshot which you can use to snapshot the datastore and back it up to wherever you want to back",
    "start": "726089",
    "end": "732690"
  },
  {
    "text": "it up - and then the FTD operator natively can do things like backup at CD",
    "start": "732690",
    "end": "737760"
  },
  {
    "text": "snapshots to s3 on a schedule so I want to show you this demo that we that I did",
    "start": "737760",
    "end": "746160"
  },
  {
    "text": "last night the reason that I had to record it is because I'm using Amazon auto scaling",
    "start": "746160",
    "end": "752070"
  },
  {
    "text": "groups and I'd rather not spend 15 minutes of us just watching a spinner even though it",
    "start": "752070",
    "end": "757640"
  },
  {
    "text": "might be entertaining the last day of the conference maybe that's all the brainpower we have but so what I have",
    "start": "757640",
    "end": "764840"
  },
  {
    "text": "here is this is the exact same cluster that is running here it's a like an 8",
    "start": "764840",
    "end": "771440"
  },
  {
    "text": "member so cluster on AWS yeah so if we",
    "start": "771440",
    "end": "777080"
  },
  {
    "text": "dive in here there's there's five master nodes and then there's a few worker",
    "start": "777080",
    "end": "782870"
  },
  {
    "text": "nodes and then on each of these master nodes is aftd a member of STD and so what I'm going to",
    "start": "782870",
    "end": "790580"
  },
  {
    "text": "demonstrate is I'm going to go in and just confirm that sed is five copies of",
    "start": "790580",
    "end": "798350"
  },
  {
    "text": "that CD you're running they're running across unique five different machines and I'll talk about how that's possible",
    "start": "798350",
    "end": "803600"
  },
  {
    "text": "later and I just choose one of the machines that City is running on and I",
    "start": "803600",
    "end": "809390"
  },
  {
    "text": "just terminate it what this will do is that the machine will shut down and and",
    "start": "809390",
    "end": "814810"
  },
  {
    "text": "Amazon will permanently delete that and what will happen pretty quickly here is",
    "start": "814810",
    "end": "821480"
  },
  {
    "text": "that will see that this node will be just removed from the node list as the",
    "start": "821480",
    "end": "826670"
  },
  {
    "text": "health checks expire inside of inside of kubernetes by default it's like a 60",
    "start": "826670",
    "end": "833030"
  },
  {
    "text": "second timeout or something and what will happen is that once that timeout has reached the pods will get",
    "start": "833030",
    "end": "840070"
  },
  {
    "text": "unscheduled get removed and eventually be marked as terminated and once they do",
    "start": "840070",
    "end": "848210"
  },
  {
    "text": "get terminated the sed operator was going to get agitated gonna be like what I had five running and now there's only",
    "start": "848210",
    "end": "853520"
  },
  {
    "text": "four I need to take some action and so what will happen is Amazon will launch",
    "start": "853520",
    "end": "860330"
  },
  {
    "text": "the new machine and in the meantime this is really important I'll roll it back just a smidge STD operator has said",
    "start": "860330",
    "end": "868610"
  },
  {
    "text": "please run another copy of that CD I'm currently out of spec but kubernetes has",
    "start": "868610",
    "end": "875720"
  },
  {
    "text": "a constraint that says I can't run FTD on the same machine and so it'll just sit here unschedulable",
    "start": "875720",
    "end": "882220"
  },
  {
    "text": "until the new machine comes up and then once the new machine comes up it goes into readiness it gets scheduled to the",
    "start": "882220",
    "end": "889330"
  },
  {
    "text": "machine the machine downloads a new copy of @cv software it gets replicated the",
    "start": "889330",
    "end": "894790"
  },
  {
    "text": "data from the existing cluster and we're back into a healthy cluster state and",
    "start": "894790",
    "end": "900310"
  },
  {
    "text": "this is all possible through two reasons one is that the sed operator is essentially acting and in acting out all",
    "start": "900310",
    "end": "908500"
  },
  {
    "text": "this operational knowledge of how to add and remove machines members from the f2p cluster and it's a single piece of",
    "start": "908500",
    "end": "914890"
  },
  {
    "text": "software that's running on top of kubernetes this leader elected so we're",
    "start": "914890",
    "end": "920620"
  },
  {
    "text": "essentially relying on its ability to make decisions cool so that is kind of",
    "start": "920620",
    "end": "928120"
  },
  {
    "text": "the vision that we have for the near future here this is all the self hosted at CD and managing this database this",
    "start": "928120",
    "end": "935140"
  },
  {
    "text": "way is something that's inside of Peconic installer right now and works",
    "start": "935140",
    "end": "940390"
  },
  {
    "text": "against 1-5 and it's the way that we'll be potentially installing and managing a city over time in the coming weeks and",
    "start": "940390",
    "end": "947200"
  },
  {
    "text": "months any questions on that yes",
    "start": "947200",
    "end": "951990"
  },
  {
    "text": "so the question was if the SVD instances are using EBS or ephemeral storage I'd",
    "start": "958390",
    "end": "964839"
  },
  {
    "text": "have to look I don't know and this particular configuration as long as the machines are able to reboot and have the",
    "start": "964839",
    "end": "971290"
  },
  {
    "text": "same state it's fine fret TV so when we",
    "start": "971290",
    "end": "978910"
  },
  {
    "text": "recover a node the way SPD works is that when the nodes come up they essentially",
    "start": "978910",
    "end": "985899"
  },
  {
    "text": "tell everyone what what the latest version of the data store they have and then one of the machines not the leader",
    "start": "985899",
    "end": "993790"
  },
  {
    "text": "will catch the other node up it works very much like raid essentially great",
    "start": "993790",
    "end": "998980"
  },
  {
    "text": "question yes yeah yeah yeah so the",
    "start": "998980",
    "end": "1009300"
  },
  {
    "text": "reason that we marked the question was is there any reason to mark the nodes as master so we have this label that we put",
    "start": "1009300",
    "end": "1014310"
  },
  {
    "text": "all of those that says master equals true the reason that we do that is because we want particular workloads to",
    "start": "1014310",
    "end": "1020130"
  },
  {
    "text": "land on the master nodes and it allows us to do like things like anti affinity calculations the reason that you might",
    "start": "1020130",
    "end": "1028110"
  },
  {
    "text": "want to do that is because your master cluster should be more or less static and only be running a set of workloads",
    "start": "1028110",
    "end": "1034620"
  },
  {
    "text": "like you probably don't want your master workload to also be running your application because if somebody like",
    "start": "1034620",
    "end": "1039839"
  },
  {
    "text": "works their way back out of your application in your master node like you're going to have a bad day and the",
    "start": "1039839",
    "end": "1044880"
  },
  {
    "text": "other reason is you can put them in separate auto scaling groups or you can manage them differently like you could",
    "start": "1044880",
    "end": "1050010"
  },
  {
    "text": "manage your workers based on demands but you kind of want that core set of nodes good remain more or less setting",
    "start": "1050010",
    "end": "1058190"
  },
  {
    "text": "and Friday",
    "start": "1059530",
    "end": "1063300"
  },
  {
    "text": "correct yes there are ways of essentially ensuring certain workloads land this is mostly about operate Li",
    "start": "1068110",
    "end": "1075070"
  },
  {
    "text": "operationalizing it knowing which nodes the monitor having them of different size etc cool so that's that's node",
    "start": "1075070",
    "end": "1083630"
  },
  {
    "text": "failure in discovery there's a couple other demos that are recorded one is about scaling it up so this cluster started off as a single member cluster",
    "start": "1083630",
    "end": "1091010"
  },
  {
    "text": "and then I scaled everything up to a five member cluster and the SDV operator helped me scale it up you can also scale",
    "start": "1091010",
    "end": "1096950"
  },
  {
    "text": "it back down etc okay so the next piece of thing that you need to worry about is",
    "start": "1096950",
    "end": "1103039"
  },
  {
    "text": "the API server this one's a lot easier it's stateless it has to be back at horizonte scalable you can just add more",
    "start": "1103039",
    "end": "1110450"
  },
  {
    "text": "to get more reach capacity it does require a load balancer and this is for a few reasons one is that as a",
    "start": "1110450",
    "end": "1116870"
  },
  {
    "text": "command-line tool or if you're interacting with a graphical console you",
    "start": "1116870",
    "end": "1123080"
  },
  {
    "text": "you need a single point to interact with it the other reason is it's the sort of",
    "start": "1123080",
    "end": "1128419"
  },
  {
    "text": "discovery end point inside of kubernetes is kind of broken right now and we just need to fix it upstream so hopefully in",
    "start": "1128419",
    "end": "1135529"
  },
  {
    "text": "the future this load balancer requirement goes away but it is what it is today so the surviving failure of API server",
    "start": "1135529",
    "end": "1143299"
  },
  {
    "text": "is pretty straightforward you want to run any API servers generally to match",
    "start": "1143299",
    "end": "1148730"
  },
  {
    "text": "your sed count because a CI server is essentially relying on a 2d as its",
    "start": "1148730",
    "end": "1153830"
  },
  {
    "text": "primary data store and if you think of the API server is like a recap in trying to in front of that CD you kind of want",
    "start": "1153830",
    "end": "1159440"
  },
  {
    "text": "to have a one-to-one mapping you also want to match failure domains generally",
    "start": "1159440",
    "end": "1164720"
  },
  {
    "text": "all these systems are designed for single machine failures to be okay and so you want STD and HIV over the",
    "start": "1164720",
    "end": "1171950"
  },
  {
    "text": "wrestling B because there because STD is useless without the API server in the context of a kubernetes and the API",
    "start": "1171950",
    "end": "1179090"
  },
  {
    "text": "server is useless without that CD there really the require each other and then",
    "start": "1179090",
    "end": "1184460"
  },
  {
    "text": "monitor and maintain load balancers over time so",
    "start": "1184460",
    "end": "1189730"
  },
  {
    "text": "the cool thing that we can do with the cell phone stuff is we can actually just",
    "start": "1189730",
    "end": "1195610"
  },
  {
    "text": "come in and sale individual API server notes so I can come and then just leave",
    "start": "1195610",
    "end": "1202179"
  },
  {
    "text": "this pause it'll end up getting terminated and then kubernetes will recover from it now one of the questions",
    "start": "1202179",
    "end": "1208630"
  },
  {
    "text": "that I often get asked about this is this is really recursive I'm a system administrator I want everything to be",
    "start": "1208630",
    "end": "1214330"
  },
  {
    "text": "stable as in this really unstable the way the whole thing is designed is that",
    "start": "1214330",
    "end": "1222059"
  },
  {
    "text": "just like any other system that is executing processes under Linux we",
    "start": "1222059",
    "end": "1227380"
  },
  {
    "text": "persist things to disk so when we think about the API server or we think about sed we have a piece of software called",
    "start": "1227380",
    "end": "1234370"
  },
  {
    "text": "the check pointer that takes what the node should be running and then we persist the disk and then when the node",
    "start": "1234370",
    "end": "1240910"
  },
  {
    "text": "comes back up it launches those things even in cases where it can't talk to an API server and so we're able to",
    "start": "1240910",
    "end": "1248320"
  },
  {
    "text": "essentially have bedrock be on disk and that's the way that we make all these operations states that I'm demonstrating",
    "start": "1248320",
    "end": "1254490"
  },
  {
    "text": "on top of the cluster using the cluster to manage the resources it's no no more",
    "start": "1254490",
    "end": "1260620"
  },
  {
    "text": "or less different than you know having like an init Rama fest that has system D services then are launched later that",
    "start": "1260620",
    "end": "1268690"
  },
  {
    "text": "makes sense okay cool so what are the",
    "start": "1268690",
    "end": "1274360"
  },
  {
    "text": "risks like one of the risks if you don't maintain the Khachaturian sed if your ad",
    "start": "1274360",
    "end": "1279669"
  },
  {
    "text": "CD cluster goes below a certain threshold below 50% plus one it will go",
    "start": "1279669",
    "end": "1286059"
  },
  {
    "text": "read-only so you won't be able to do any post which means that your cluster is",
    "start": "1286059",
    "end": "1291610"
  },
  {
    "text": "fine kubernetes clusters will continue to run their workloads if they're unable to talk to the control plane well you",
    "start": "1291610",
    "end": "1299320"
  },
  {
    "text": "will lose is cluster visibility because a lot of things like events and stuff get pushed through at CD and you may",
    "start": "1299320",
    "end": "1307419"
  },
  {
    "text": "lose the ability to find and do service discovery however DNS should be cached",
    "start": "1307419",
    "end": "1313780"
  },
  {
    "text": "so as long as your DNS instances your coop dns instances remain running you",
    "start": "1313780",
    "end": "1319360"
  },
  {
    "text": "aren't at risk of the app starting to fail for enter clusters service discovery okay so the next bit",
    "start": "1319360",
    "end": "1328250"
  },
  {
    "text": "are the scheduler and the controller manager so the scheduler and troller manager is stateful but they back their",
    "start": "1328250",
    "end": "1335840"
  },
  {
    "text": "state with the API server so to a large degree what happens is that if you lose",
    "start": "1335840",
    "end": "1344180"
  },
  {
    "text": "a scheduler controller manager they will take a few seconds to come back up so they need to rebuild internal state and",
    "start": "1344180",
    "end": "1350480"
  },
  {
    "text": "then they rebuild that internal state from the API their leader elected so",
    "start": "1350480",
    "end": "1356870"
  },
  {
    "text": "it's totally safe to have multiple copies of these but they're not horizontally scalable adding more",
    "start": "1356870",
    "end": "1362030"
  },
  {
    "text": "schedulers or more controller managers does not make the scheduling faster I wish that's the way that worked but",
    "start": "1362030",
    "end": "1367850"
  },
  {
    "text": "that's just simply not the case and it's totally possible to recover from the states so we think back to API server",
    "start": "1367850",
    "end": "1375620"
  },
  {
    "text": "and sed I said that we persist these things to disk we don't persist any",
    "start": "1375620",
    "end": "1380630"
  },
  {
    "text": "other other kubernetes components to disk and but it's okay because the",
    "start": "1380630",
    "end": "1386870"
  },
  {
    "text": "scheduler and controller manager if you screw up an update or you accidentally",
    "start": "1386870",
    "end": "1393350"
  },
  {
    "text": "scale your scheduler down to zero you can you can recover from that and I'll show you how so you can run like I say",
    "start": "1393350",
    "end": "1403640"
  },
  {
    "text": "you run two more copies but their lead elected you need to have anti affinity to ensure that you survive any single",
    "start": "1403640",
    "end": "1409580"
  },
  {
    "text": "machine failure so imagine that you have the scheduler you have two copies for they're running on the same post that's",
    "start": "1409580",
    "end": "1415670"
  },
  {
    "text": "not actually going to help you when that host goes down okay so this is a pretty quick demo so I'll show you this what",
    "start": "1415670",
    "end": "1423650"
  },
  {
    "text": "we'll do is both scale the tube scheduler down to zero",
    "start": "1423650",
    "end": "1430040"
  },
  {
    "text": "which means that essentially we're simulating a outage where we lost all",
    "start": "1430040",
    "end": "1435200"
  },
  {
    "text": "the machines where the scheduler was running on and now no new forward progress can happen against kubernetes",
    "start": "1435200",
    "end": "1442100"
  },
  {
    "text": "okay so the kubernetes deployment for the scheduler has been scaled down now",
    "start": "1442100",
    "end": "1447290"
  },
  {
    "text": "we have a problem how do we how do we schedule our new scheduler anyone have an idea about we",
    "start": "1447290",
    "end": "1453169"
  },
  {
    "text": "might do that so what what yeah exactly",
    "start": "1453169",
    "end": "1459830"
  },
  {
    "text": "so you what you do is you give the deployment of the scheduler so and then",
    "start": "1459830",
    "end": "1467419"
  },
  {
    "text": "you act like you scheduler yourself I think I can figure out the name of a node to place the scheduler on I'm a",
    "start": "1467419",
    "end": "1474289"
  },
  {
    "text": "pretty smart guy I can choose randomly from a list of six options so I'll show you how that works so you take the",
    "start": "1474289",
    "end": "1480950"
  },
  {
    "text": "scheduler you get this appointment object so the API server is still running so I go in and I look at the",
    "start": "1480950",
    "end": "1487009"
  },
  {
    "text": "scheduler object the first thing I have to do is have to change the kind from deployment pod I can delete most of this",
    "start": "1487009",
    "end": "1494029"
  },
  {
    "text": "stuff I'll keep the name space and the name and then the spec mostly deals with",
    "start": "1494029",
    "end": "1500320"
  },
  {
    "text": "the deployment stuff but inside of here is the pod template so if I come in here",
    "start": "1500320",
    "end": "1509419"
  },
  {
    "text": "and just delete essentially everything else that's not the pod template I move this around and now I have pod a single",
    "start": "1509419",
    "end": "1518179"
  },
  {
    "text": "pod and the last thing that I need to do is just choose a node so if I go in and",
    "start": "1518179",
    "end": "1524739"
  },
  {
    "text": "put a node name and I may get the name of my nodes choose one about true so I",
    "start": "1524739",
    "end": "1533869"
  },
  {
    "text": "need to find a node that's obviously a master so I put it in on one of the correct machines and then I can just do",
    "start": "1533869",
    "end": "1541070"
  },
  {
    "text": "to cuddle create minus F and what will happen is that I now have a scheduler",
    "start": "1541070",
    "end": "1546080"
  },
  {
    "text": "that is running back on on the machines and then if I want to scale my",
    "start": "1546080",
    "end": "1551809"
  },
  {
    "text": "deployment backup of schedulers to two that will happily happen and I aim to leave my scheduler so in the face of",
    "start": "1551809",
    "end": "1559070"
  },
  {
    "text": "scheduler failures your worst case scenario is that you have to act like a scheduler for a second but I have",
    "start": "1559070",
    "end": "1564919"
  },
  {
    "text": "confidence that each and every one of you can do that again the other thing to",
    "start": "1564919",
    "end": "1572029"
  },
  {
    "text": "keep in mind about the scheduler and the controller manager is that there's failure risk the controller manager if",
    "start": "1572029",
    "end": "1577489"
  },
  {
    "text": "it goes down you will lose cloud provider integrations you won't be able to spin up load balancers or attach persistent",
    "start": "1577489",
    "end": "1584710"
  },
  {
    "text": "disks if the controller manager and the scheduler go down you won't be able to",
    "start": "1584710",
    "end": "1590830"
  },
  {
    "text": "do deployments you won't be a little scale as I just demonstrated and then",
    "start": "1590830",
    "end": "1595900"
  },
  {
    "text": "failed node workloads won't reschedule so if that auto scaling group like I'd",
    "start": "1595900",
    "end": "1600970"
  },
  {
    "text": "shown if I had nukes that machine when the new machine comes up it won't actually get scheduled new workloads and",
    "start": "1600970",
    "end": "1607450"
  },
  {
    "text": "the old workloads won't get unscheduled so that's your risk and generally that means that you can have like anywhere",
    "start": "1607450",
    "end": "1614200"
  },
  {
    "text": "from five minutes to hours to respond to these components inside of kubernetes",
    "start": "1614200",
    "end": "1619300"
  },
  {
    "text": "going down the last bit is DNS I like to tie two really well I used to work on",
    "start": "1619300",
    "end": "1625690"
  },
  {
    "text": "DNS early in my career it's not be enough there's no way of CNS it was DNS",
    "start": "1625690",
    "end": "1631900"
  },
  {
    "text": "I think we've all experienced this for ourselves so kubernetes dns is a little",
    "start": "1631900",
    "end": "1640180"
  },
  {
    "text": "tricky pretty much everything else in kubernetes can use service discovery but",
    "start": "1640180",
    "end": "1645610"
  },
  {
    "text": "the reason this I - is so applicable is because everything relies on service discovery and so the kubernetes dns",
    "start": "1645610",
    "end": "1652780"
  },
  {
    "text": "service lives at a predefined service IP and so you have to run this service at",
    "start": "1652780",
    "end": "1660340"
  },
  {
    "text": "that IP all the time but the risk is really low of the beam odds going down",
    "start": "1660340",
    "end": "1666600"
  },
  {
    "text": "it's used by many applications through Venus but it's not used by cube itself so it goes down your applications may",
    "start": "1666600",
    "end": "1674380"
  },
  {
    "text": "stop responding that you still start to recover your cluster and reschedule a new DNS server all right the last bit is",
    "start": "1674380",
    "end": "1681640"
  },
  {
    "text": "around nodes the places where you're the actual user applications work labels are",
    "start": "1681640",
    "end": "1688360"
  },
  {
    "text": "really flexible so with labels we're able to say you know make selections in",
    "start": "1688360",
    "end": "1693670"
  },
  {
    "text": "a variety of ways maybe front end and back end or based on user or based on",
    "start": "1693670",
    "end": "1699490"
  },
  {
    "text": "environment these same labels can be used against nodes and this is really useful where we",
    "start": "1699490",
    "end": "1706730"
  },
  {
    "text": "can start to say this node lives in this particular rack or there's no list in this particular region or this node",
    "start": "1706730",
    "end": "1712700"
  },
  {
    "text": "lives on this particular switch the way that I've been demonstrating all this",
    "start": "1712700",
    "end": "1718460"
  },
  {
    "text": "stuff with MCD in the API server etc is that I've been using a concept called anti affinity which is really important",
    "start": "1718460",
    "end": "1725210"
  },
  {
    "text": "to ensure you that your applications end up spread out across your clusters and",
    "start": "1725210",
    "end": "1730700"
  },
  {
    "text": "you don't accidentally end up failing your application because all 20 of your",
    "start": "1730700",
    "end": "1735980"
  },
  {
    "text": "horizontally scaled apps landed on one node so this is a feature called failure",
    "start": "1735980",
    "end": "1742670"
  },
  {
    "text": "domains which is in beta and it uses labels like the rest of the system to decide where to land",
    "start": "1742670",
    "end": "1748730"
  },
  {
    "text": "mushiya land workloads so again you can start to break up your system based on",
    "start": "1748730",
    "end": "1755090"
  },
  {
    "text": "node labeling so a quick demo that I wanted to give was cuckoo kettle has",
    "start": "1755090",
    "end": "1761750"
  },
  {
    "text": "this really great thing called JSON path so it allows you to query through the cluster so you can get nodes you sleep",
    "start": "1761750",
    "end": "1769400"
  },
  {
    "text": "at night there we go so what we'll do is we'll look at this thing called the failure domain region which is",
    "start": "1769400",
    "end": "1777160"
  },
  {
    "text": "automatically populated by the cloud provider integration so in this case it",
    "start": "1777160",
    "end": "1782810"
  },
  {
    "text": "lists all the AWS regions and then there's also failure domain zone which",
    "start": "1782810",
    "end": "1788030"
  },
  {
    "text": "lists the actual able availability zone so you can start the partition and this",
    "start": "1788030",
    "end": "1793730"
  },
  {
    "text": "is how like the FTD operator partitions stop by default cool and then it's all",
    "start": "1793730",
    "end": "1800480"
  },
  {
    "text": "just wrapped up in this node selector and then a topology key so in the case of FTD we say ensure that things that",
    "start": "1800480",
    "end": "1807620"
  },
  {
    "text": "are labeled F 2d cluster equals cube FTD don't end up in the same topology like each node doesn't end up in the same",
    "start": "1807620",
    "end": "1814070"
  },
  {
    "text": "topology where the hostname matches because we don't want them to land on the same machine ever all right so I",
    "start": "1814070",
    "end": "1821060"
  },
  {
    "text": "have five minutes left and five sections this is going to go super well",
    "start": "1821060",
    "end": "1826550"
  },
  {
    "text": "monitoring and alerting so nearly all components have metrics api's that are",
    "start": "1826550",
    "end": "1832550"
  },
  {
    "text": "in Prometheus format and it's primarily what this means is there's a metric there's a label there's a value and then",
    "start": "1832550",
    "end": "1839300"
  },
  {
    "text": "optionally a timestamp we of course have built this thing called a Prometheus",
    "start": "1839300",
    "end": "1845120"
  },
  {
    "text": "operator which is also open source you find it a Korolev slash Prometheus",
    "start": "1845120",
    "end": "1850580"
  },
  {
    "text": "operator on github one of the things that a kind of a lasting I'll demo here",
    "start": "1850580",
    "end": "1856640"
  },
  {
    "text": "is the thing called cou Prometheus which folks from SoundCloud have built a bunch",
    "start": "1856640",
    "end": "1862580"
  },
  {
    "text": "of rules that allow you to receive scale",
    "start": "1862580",
    "end": "1869020"
  },
  {
    "text": "which allow you to monitor and have best practices around alerting of the health",
    "start": "1869020",
    "end": "1875540"
  },
  {
    "text": "of kubernetes so it has best practices around API server the scheduler STD etc",
    "start": "1875540",
    "end": "1881510"
  },
  {
    "text": "you can see the readme for how to deploy but that's appointed on this cluster so we can do things like look at the node",
    "start": "1881510",
    "end": "1887810"
  },
  {
    "text": "status until you like a bunch of nodes are down and give you an alert to example this query will say if there's",
    "start": "1887810",
    "end": "1895940"
  },
  {
    "text": "more than 10% of my notes that are in non node ready send me an alert I could go to page or duty or email or whatever",
    "start": "1895940",
    "end": "1902590"
  },
  {
    "text": "because and the other piece that it has is it'll start to scrape metrics CPU",
    "start": "1902590",
    "end": "1908780"
  },
  {
    "text": "network storage metrics from individual nodes so that you can start to alert on a ram usage is at 80 percent coal or",
    "start": "1908780",
    "end": "1915800"
  },
  {
    "text": "whatever so this is an example of a query that is provided in queue previous",
    "start": "1915800",
    "end": "1922250"
  },
  {
    "text": "to say give me the use percentage of kubernetes memory versus currently",
    "start": "1922250",
    "end": "1928700"
  },
  {
    "text": "consumed memory in our pods and you can also start to drill down and say give me the container memory that's used by",
    "start": "1928700",
    "end": "1934670"
  },
  {
    "text": "individual applications like node exporter so the most interesting thing I",
    "start": "1934670",
    "end": "1940610"
  },
  {
    "text": "think is how do you monitor the control plane so in this repo there's there's",
    "start": "1940610",
    "end": "1946610"
  },
  {
    "text": "rules around ensuring that FTD is remaining up with that 50% plus one",
    "start": "1946610",
    "end": "1951790"
  },
  {
    "text": "checks for high turnover your election perhaps you're having a network problem",
    "start": "1951790",
    "end": "1957389"
  },
  {
    "text": "and then also API response times the key metrics that you need to look for for",
    "start": "1957389",
    "end": "1962429"
  },
  {
    "text": "thinking about the API server is essentially latency and the sed latency between the API server and also",
    "start": "1962429",
    "end": "1969179"
  },
  {
    "text": "scheduling latency our notes there are pods getting scheduled on the order of",
    "start": "1969179",
    "end": "1974339"
  },
  {
    "text": "minutes or hours or days something that you need to keep an eye on the last bit",
    "start": "1974339",
    "end": "1980279"
  },
  {
    "text": "is around apps so the Prometheus operator lets you use label queries to",
    "start": "1980279",
    "end": "1986459"
  },
  {
    "text": "find and discover and monitor your applications as well alright so disaster",
    "start": "1986459",
    "end": "1993929"
  },
  {
    "text": "recovery you can read the reading on the notes that I have on github cluster",
    "start": "1993929",
    "end": "2000589"
  },
  {
    "text": "upgrades this is where really where self-hosted shines so there's two",
    "start": "2000589",
    "end": "2005839"
  },
  {
    "text": "vulnerabilities that happen in october 20th 2016 there's dirty cow who is affected by dirty cow everyone I hope",
    "start": "2005839",
    "end": "2013009"
  },
  {
    "text": "probably everyone's phone is still unpatched and then there's also kubernetes clients or validation problem",
    "start": "2013009",
    "end": "2019809"
  },
  {
    "text": "which I gave a name it didn't catch on called mask capitalized the K is",
    "start": "2019809",
    "end": "2025249"
  },
  {
    "text": "kubernetes but the cool thing about this is that we're able to start to do things",
    "start": "2025249",
    "end": "2030799"
  },
  {
    "text": "like upgrade and downgrade all the components in kubernetes via the",
    "start": "2030799",
    "end": "2037549"
  },
  {
    "text": "kubernetes api so that scheduler deployment we can just swap out the container that's currently at one five",
    "start": "2037549",
    "end": "2043039"
  },
  {
    "text": "four and bump it up to one five size and then watch and use the rolling upgrade capabilities of kubernetes to handle",
    "start": "2043039",
    "end": "2049878"
  },
  {
    "text": "that operational concern all right last fit is a management of nodes not",
    "start": "2049879",
    "end": "2058638"
  },
  {
    "text": "going to cover it but essentially you can use with all the technology auto scaling groups or terraform to kind of",
    "start": "2058639",
    "end": "2064700"
  },
  {
    "text": "stateless we manage the nodes and then rely on kubernetes to manage the state of the kubernetes application all right",
    "start": "2064700",
    "end": "2074169"
  },
  {
    "text": "so try to get one more demo in but if you want to try this stuff out again go",
    "start": "2074169",
    "end": "2079940"
  },
  {
    "text": "to github.com slash tulips slash kubernetes day 2 and try out some hose demos if you want to get a cluster",
    "start": "2079940",
    "end": "2085608"
  },
  {
    "text": "running check out the tectonic installer and kind of play with everything that showing up today I want to thank you I",
    "start": "2085609",
    "end": "2092230"
  },
  {
    "text": "want to try to get renewed applaud thing and I'll try to get one more demo okay thanks",
    "start": "2092230",
    "end": "2099450"
  },
  {
    "text": "so let's see if this work didn't work well there you go all right well I have",
    "start": "2105599",
    "end": "2111460"
  },
  {
    "text": "like one minute for questions there's one more question yeah",
    "start": "2111460",
    "end": "2117059"
  },
  {
    "text": "[Music] yeah SUV goes to low quorum you're kind",
    "start": "2120000",
    "end": "2125190"
  },
  {
    "text": "of screed yes it you're actually in disaster recovery mode and you have to",
    "start": "2125190",
    "end": "2130800"
  },
  {
    "text": "restore from backup the backup will be really up-to-date it'll be because",
    "start": "2130800",
    "end": "2135930"
  },
  {
    "text": "everything's been read-only for such a long time but you have to actually go through and take manual steps because only a human being can make decisions at",
    "start": "2135930",
    "end": "2142740"
  },
  {
    "text": "that point it's sort of like the scheduler problem once you go below quorum the computer can't make a reasonable",
    "start": "2142740",
    "end": "2148020"
  },
  {
    "text": "decision so human has to step in but there's a disaster recovery guide on FCB",
    "start": "2148020",
    "end": "2153240"
  },
  {
    "text": "for handling that yeah yeah so the",
    "start": "2153240",
    "end": "2160710"
  },
  {
    "text": "question was about at the operator in TLS that will be supported probably in the next release or - yeah all right",
    "start": "2160710",
    "end": "2166020"
  },
  {
    "text": "thanks so much I'll I don't want to like take the room but I'll answer questions outside so thank you",
    "start": "2166020",
    "end": "2173150"
  }
]