[
  {
    "text": "welcome to the final session of the day",
    "start": "240",
    "end": "3879"
  },
  {
    "text": "for strumsy con so we have the final",
    "start": "3879",
    "end": "7439"
  },
  {
    "text": "session for the day is leveraging tiered",
    "start": "7439",
    "end": "10160"
  },
  {
    "text": "storage in strey operated Kafka and we",
    "start": "10160",
    "end": "13440"
  },
  {
    "text": "have Lan Yao Bo and rashal B here to",
    "start": "13440",
    "end": "17840"
  },
  {
    "text": "chat about that so throughout the",
    "start": "17840",
    "end": "20160"
  },
  {
    "text": "session if you have any questions please",
    "start": "20160",
    "end": "22600"
  },
  {
    "text": "put them in the Q&A and um we should",
    "start": "22600",
    "end": "25480"
  },
  {
    "text": "have some time at the end to ask some",
    "start": "25480",
    "end": "27720"
  },
  {
    "text": "questions so with that I'll hand over",
    "start": "27720",
    "end": "31520"
  },
  {
    "text": "team okay thank you uh hello everyone my",
    "start": "31520",
    "end": "35680"
  },
  {
    "text": "name is Li ya I'm a software engineer at",
    "start": "35680",
    "end": "38200"
  },
  {
    "text": "Apple uh today Bo with SH and me will",
    "start": "38200",
    "end": "41440"
  },
  {
    "text": "talk about our story of adopting tier",
    "start": "41440",
    "end": "44440"
  },
  {
    "text": "storage feature in streamy Opera kfka at",
    "start": "44440",
    "end": "48239"
  },
  {
    "text": "Apple so here is a brief agenda of our",
    "start": "48239",
    "end": "51280"
  },
  {
    "text": "talk today uh we will start with some",
    "start": "51280",
    "end": "53960"
  },
  {
    "text": "background and motivation on our",
    "start": "53960",
    "end": "56199"
  },
  {
    "text": "architecture why we are interested in",
    "start": "56199",
    "end": "58239"
  },
  {
    "text": "teer story feature and then we will walk",
    "start": "58239",
    "end": "61039"
  },
  {
    "text": "through our journey of integrating the",
    "start": "61039",
    "end": "63399"
  },
  {
    "text": "tier story feature and share some lesson",
    "start": "63399",
    "end": "65680"
  },
  {
    "text": "learned during our integration we also",
    "start": "65680",
    "end": "68759"
  },
  {
    "text": "talk about some number on performance",
    "start": "68759",
    "end": "70799"
  },
  {
    "text": "Benchmark and then share more about our",
    "start": "70799",
    "end": "73840"
  },
  {
    "text": "team's contribution with a streamy open",
    "start": "73840",
    "end": "76479"
  },
  {
    "text": "source Community I will leave the rest",
    "start": "76479",
    "end": "78680"
  },
  {
    "text": "of time for question and",
    "start": "78680",
    "end": "82000"
  },
  {
    "text": "answer okay uh hand over to rali okay",
    "start": "82000",
    "end": "86040"
  },
  {
    "text": "thanks Lin um this diagram here",
    "start": "86040",
    "end": "88880"
  },
  {
    "text": "illustrates a typical injection pipeline",
    "start": "88880",
    "end": "90920"
  },
  {
    "text": "for streaming applications it showcases",
    "start": "90920",
    "end": "93640"
  },
  {
    "text": "the flow of data from left to right on",
    "start": "93640",
    "end": "96799"
  },
  {
    "text": "the source side we have data coming in",
    "start": "96799",
    "end": "98920"
  },
  {
    "text": "from multiple sources and that includes",
    "start": "98920",
    "end": "101600"
  },
  {
    "text": "Hardware devices as well as serers side",
    "start": "101600",
    "end": "104079"
  },
  {
    "text": "log",
    "start": "104079",
    "end": "104960"
  },
  {
    "text": "events this data is ingested through a",
    "start": "104960",
    "end": "107799"
  },
  {
    "text": "Gateway service Gateway service acts as",
    "start": "107799",
    "end": "110439"
  },
  {
    "text": "a crucial entry point into the ingestion",
    "start": "110439",
    "end": "112920"
  },
  {
    "text": "pipeline it performs several functions",
    "start": "112920",
    "end": "116439"
  },
  {
    "text": "it provides a security check by ensuring",
    "start": "116439",
    "end": "118840"
  },
  {
    "text": "that only authoriz data enters the",
    "start": "118840",
    "end": "121399"
  },
  {
    "text": "pipeline it verifies the integrity and",
    "start": "121399",
    "end": "124119"
  },
  {
    "text": "quality of the incoming data it also",
    "start": "124119",
    "end": "126880"
  },
  {
    "text": "conducts schema validation by using",
    "start": "126880",
    "end": "129200"
  },
  {
    "text": "schemas from the schema registry and",
    "start": "129200",
    "end": "131840"
  },
  {
    "text": "finally it serializes the data by",
    "start": "131840",
    "end": "134680"
  },
  {
    "text": "converting it into a format more",
    "start": "134680",
    "end": "136959"
  },
  {
    "text": "suitable for Kafka consumption before",
    "start": "136959",
    "end": "139280"
  },
  {
    "text": "writing it to",
    "start": "139280",
    "end": "140640"
  },
  {
    "text": "Kafka on the consumer side um apach",
    "start": "140640",
    "end": "143760"
  },
  {
    "text": "Flink is a widely adopted choice for",
    "start": "143760",
    "end": "145519"
  },
  {
    "text": "streaming",
    "start": "145519",
    "end": "146640"
  },
  {
    "text": "applications Flink applications consume",
    "start": "146640",
    "end": "149080"
  },
  {
    "text": "the serialized dat stuff from Kafka and",
    "start": "149080",
    "end": "151360"
  },
  {
    "text": "use schemas from the schema registry to",
    "start": "151360",
    "end": "153640"
  },
  {
    "text": "deserialize it accordingly following",
    "start": "153640",
    "end": "156560"
  },
  {
    "text": "deserialization Flink enriches the data",
    "start": "156560",
    "end": "159360"
  },
  {
    "text": "with additional business logic making it",
    "start": "159360",
    "end": "161680"
  },
  {
    "text": "more valuable and contextually",
    "start": "161680",
    "end": "164519"
  },
  {
    "text": "relevant next",
    "start": "164519",
    "end": "167879"
  },
  {
    "text": "please now let's take a closer look at",
    "start": "168519",
    "end": "170920"
  },
  {
    "text": "the Kafka clusters in our injection",
    "start": "170920",
    "end": "172879"
  },
  {
    "text": "pipeline we use AWS eks for all all our",
    "start": "172879",
    "end": "176599"
  },
  {
    "text": "workloads including gafka eks giv gives",
    "start": "176599",
    "end": "180040"
  },
  {
    "text": "us a standardized kubernetes environment",
    "start": "180040",
    "end": "182400"
  },
  {
    "text": "and makes it straightforward for us to",
    "start": "182400",
    "end": "184760"
  },
  {
    "text": "install Kafka to install Kafka we use",
    "start": "184760",
    "end": "188280"
  },
  {
    "text": "the streamy kubernetes operator we use",
    "start": "188280",
    "end": "190920"
  },
  {
    "text": "streamy because it simplifies running",
    "start": "190920",
    "end": "192959"
  },
  {
    "text": "abach Kafka and",
    "start": "192959",
    "end": "195120"
  },
  {
    "text": "clusters we manage streamy and Kafka",
    "start": "195120",
    "end": "197640"
  },
  {
    "text": "deployments using",
    "start": "197640",
    "end": "199400"
  },
  {
    "text": "H for storage we use EBS volumes they",
    "start": "199400",
    "end": "203799"
  },
  {
    "text": "have been reliable since day one even",
    "start": "203799",
    "end": "205840"
  },
  {
    "text": "though they are a bit",
    "start": "205840",
    "end": "207760"
  },
  {
    "text": "expensive um to expose the kka endpoint",
    "start": "207760",
    "end": "210400"
  },
  {
    "text": "to external clients we use engine X",
    "start": "210400",
    "end": "212480"
  },
  {
    "text": "Ingress",
    "start": "212480",
    "end": "213599"
  },
  {
    "text": "controller now besides Kafka we also use",
    "start": "213599",
    "end": "217080"
  },
  {
    "text": "other tools from the Kafka ecosystem we",
    "start": "217080",
    "end": "219680"
  },
  {
    "text": "use burrow for reporting lag metrics we",
    "start": "219680",
    "end": "222599"
  },
  {
    "text": "use Kafka manager UI to give us an",
    "start": "222599",
    "end": "225360"
  },
  {
    "text": "interactive view of the Brokers and",
    "start": "225360",
    "end": "227080"
  },
  {
    "text": "topic information we also use cruise",
    "start": "227080",
    "end": "229879"
  },
  {
    "text": "control for resource tracking and",
    "start": "229879",
    "end": "231640"
  },
  {
    "text": "anomaly detection this setup gives us a",
    "start": "231640",
    "end": "234879"
  },
  {
    "text": "robust scalable and efficient data",
    "start": "234879",
    "end": "237319"
  },
  {
    "text": "streaming architecture",
    "start": "237319",
    "end": "240760"
  },
  {
    "text": "now in our use cases we often need to",
    "start": "242079",
    "end": "244560"
  },
  {
    "text": "use multiple Kafka clusters for an",
    "start": "244560",
    "end": "246480"
  },
  {
    "text": "ingestion pipeline each Kafka cluster",
    "start": "246480",
    "end": "249239"
  },
  {
    "text": "has its own dedicated name space but",
    "start": "249239",
    "end": "251720"
  },
  {
    "text": "these clusters can be in the same eks or",
    "start": "251720",
    "end": "254360"
  },
  {
    "text": "in different eks",
    "start": "254360",
    "end": "255959"
  },
  {
    "text": "environments we install one instance of",
    "start": "255959",
    "end": "258440"
  },
  {
    "text": "streamy operator per eks this operator",
    "start": "258440",
    "end": "261880"
  },
  {
    "text": "manages all the kafa Clusters within",
    "start": "261880",
    "end": "263960"
  },
  {
    "text": "that eks the streamy operator watches",
    "start": "263960",
    "end": "266840"
  },
  {
    "text": "all the name spaces in the same e and",
    "start": "266840",
    "end": "269320"
  },
  {
    "text": "therefore allows us to streamline our",
    "start": "269320",
    "end": "271479"
  },
  {
    "text": "operations this deployment model has",
    "start": "271479",
    "end": "273680"
  },
  {
    "text": "significantly reduced our overhead",
    "start": "273680",
    "end": "275520"
  },
  {
    "text": "involved in managing Kafka",
    "start": "275520",
    "end": "278840"
  },
  {
    "text": "clusters um we have been running this CF",
    "start": "281000",
    "end": "283479"
  },
  {
    "text": "architecture for quite a while now and",
    "start": "283479",
    "end": "285440"
  },
  {
    "text": "overall it's been performing great",
    "start": "285440",
    "end": "288320"
  },
  {
    "text": "however we have encountered some",
    "start": "288320",
    "end": "289720"
  },
  {
    "text": "challenges particularly around cost",
    "start": "289720",
    "end": "292160"
  },
  {
    "text": "management initially we used io1 volumes",
    "start": "292160",
    "end": "295479"
  },
  {
    "text": "which delivered excellent performance",
    "start": "295479",
    "end": "297360"
  },
  {
    "text": "but they they are quite expensive to",
    "start": "297360",
    "end": "300080"
  },
  {
    "text": "save costs we also experimented with gp3",
    "start": "300080",
    "end": "303199"
  },
  {
    "text": "volumes while this helped the costs were",
    "start": "303199",
    "end": "305800"
  },
  {
    "text": "still high and became a significant",
    "start": "305800",
    "end": "308680"
  },
  {
    "text": "concern in some use cases the concern",
    "start": "308680",
    "end": "311160"
  },
  {
    "text": "was more than the others for example",
    "start": "311160",
    "end": "314199"
  },
  {
    "text": "some applications required one week of",
    "start": "314199",
    "end": "316160"
  },
  {
    "text": "data retention period the associated",
    "start": "316160",
    "end": "318800"
  },
  {
    "text": "costs with this can be prohibitive and",
    "start": "318800",
    "end": "321400"
  },
  {
    "text": "that's where the the storage feature",
    "start": "321400",
    "end": "323039"
  },
  {
    "text": "comes in it allows us to move large",
    "start": "323039",
    "end": "326039"
  },
  {
    "text": "amounts of data from expensive storage",
    "start": "326039",
    "end": "328240"
  },
  {
    "text": "like EBS volumes to more affordable",
    "start": "328240",
    "end": "330680"
  },
  {
    "text": "options like S3 this shift addresses",
    "start": "330680",
    "end": "333800"
  },
  {
    "text": "Financial concerns enabling us to offer",
    "start": "333800",
    "end": "336680"
  },
  {
    "text": "longer retention times without breaking",
    "start": "336680",
    "end": "338520"
  },
  {
    "text": "the",
    "start": "338520",
    "end": "340039"
  },
  {
    "text": "bank performance- wise moving data from",
    "start": "340039",
    "end": "343720"
  },
  {
    "text": "moving data to remote storage hasn't",
    "start": "343720",
    "end": "345919"
  },
  {
    "text": "cause significant issues for most use",
    "start": "345919",
    "end": "349039"
  },
  {
    "text": "cases where consumers need latest data",
    "start": "349039",
    "end": "351639"
  },
  {
    "text": "performance remains unaffected because",
    "start": "351639",
    "end": "354000"
  },
  {
    "text": "the most recent data always stays on the",
    "start": "354000",
    "end": "356000"
  },
  {
    "text": "broker disk and remote reads are quite",
    "start": "356000",
    "end": "358919"
  },
  {
    "text": "in infrequent",
    "start": "358919",
    "end": "360919"
  },
  {
    "text": "additionally reducing local storage",
    "start": "360919",
    "end": "363160"
  },
  {
    "text": "volumes has improved cluster reliability",
    "start": "363160",
    "end": "365960"
  },
  {
    "text": "scaling the cluster for higher traffic",
    "start": "365960",
    "end": "367880"
  },
  {
    "text": "is now easier either by adjusting",
    "start": "367880",
    "end": "370280"
  },
  {
    "text": "retention times or adding more Brokers",
    "start": "370280",
    "end": "373479"
  },
  {
    "text": "broker startup time and application",
    "start": "373479",
    "end": "375599"
  },
  {
    "text": "times have also",
    "start": "375599",
    "end": "377400"
  },
  {
    "text": "improved in addition to this tier",
    "start": "377400",
    "end": "379960"
  },
  {
    "text": "storage has also reduced our operational",
    "start": "379960",
    "end": "382080"
  },
  {
    "text": "overhead",
    "start": "382080",
    "end": "383160"
  },
  {
    "text": "significantly previously extending",
    "start": "383160",
    "end": "385759"
  },
  {
    "text": "retention times during outages required",
    "start": "385759",
    "end": "388680"
  },
  {
    "text": "temporarily expanding persistent volumes",
    "start": "388680",
    "end": "391360"
  },
  {
    "text": "and this is a non-reversible action post",
    "start": "391360",
    "end": "394160"
  },
  {
    "text": "outage we would need to migrate traffic",
    "start": "394160",
    "end": "396880"
  },
  {
    "text": "back to a standard sized cluster which",
    "start": "396880",
    "end": "399240"
  },
  {
    "text": "again is a complex process with storage",
    "start": "399240",
    "end": "402639"
  },
  {
    "text": "there is no that is no longer necessary",
    "start": "402639",
    "end": "405560"
  },
  {
    "text": "we can adjust the total detention times",
    "start": "405560",
    "end": "407599"
  },
  {
    "text": "without frequently altering local lus",
    "start": "407599",
    "end": "410000"
  },
  {
    "text": "volumes this largely simplifies our",
    "start": "410000",
    "end": "412800"
  },
  {
    "text": "operations and reduces overhead to",
    "start": "412800",
    "end": "415720"
  },
  {
    "text": "summarize the storage has allowed us to",
    "start": "415720",
    "end": "418680"
  },
  {
    "text": "manage cost effect effectively improve",
    "start": "418680",
    "end": "420879"
  },
  {
    "text": "performance as well as streamline our",
    "start": "420879",
    "end": "423639"
  },
  {
    "text": "operations over to",
    "start": "423639",
    "end": "427120"
  },
  {
    "text": "B thanks M shy um so let's um uh let's",
    "start": "427479",
    "end": "431599"
  },
  {
    "text": "look at the Journey of our tier storage",
    "start": "431599",
    "end": "433440"
  },
  {
    "text": "integration timeline so um we have a",
    "start": "433440",
    "end": "435599"
  },
  {
    "text": "brief timeline here for for that we kick",
    "start": "435599",
    "end": "437919"
  },
  {
    "text": "off our efforts with reaching out to",
    "start": "437919",
    "end": "440160"
  },
  {
    "text": "Satish who is the author of Kip 405 um",
    "start": "440160",
    "end": "443960"
  },
  {
    "text": "starting from February",
    "start": "443960",
    "end": "445520"
  },
  {
    "text": "2023 we got a lot of help from Satish to",
    "start": "445520",
    "end": "448440"
  },
  {
    "text": "get the project boost strapped with his",
    "start": "448440",
    "end": "451039"
  },
  {
    "text": "uh private U Dev Branch uh in",
    "start": "451039",
    "end": "454319"
  },
  {
    "text": "2.8 we also get a lot of help from alen",
    "start": "454319",
    "end": "457160"
  },
  {
    "text": "guys uh who is um engineering team",
    "start": "457160",
    "end": "459759"
  },
  {
    "text": "trying to uh actively working on open",
    "start": "459759",
    "end": "462840"
  },
  {
    "text": "source their uh remote storage plugins",
    "start": "462840",
    "end": "465080"
  },
  {
    "text": "in S3 in the Prototype phase uh in April",
    "start": "465080",
    "end": "468479"
  },
  {
    "text": "2023 we were able to start 2.8 Kafka",
    "start": "468479",
    "end": "472080"
  },
  {
    "text": "cluster with a remote storage layer on",
    "start": "472080",
    "end": "474479"
  },
  {
    "text": "S3 then we spent a lot of time on",
    "start": "474479",
    "end": "477240"
  },
  {
    "text": "functional testing performance tuning",
    "start": "477240",
    "end": "479080"
  },
  {
    "text": "and other type of testing we also build",
    "start": "479080",
    "end": "481960"
  },
  {
    "text": "our own internal version of Sr remote",
    "start": "481960",
    "end": "484199"
  },
  {
    "text": "plugin with some customization and",
    "start": "484199",
    "end": "486560"
  },
  {
    "text": "optimization for our use",
    "start": "486560",
    "end": "488720"
  },
  {
    "text": "case um by September 2023 we were",
    "start": "488720",
    "end": "492800"
  },
  {
    "text": "confident with the 2.8 Dev Branch uh",
    "start": "492800",
    "end": "495800"
  },
  {
    "text": "Kafka and we consider it verified and",
    "start": "495800",
    "end": "498120"
  },
  {
    "text": "ready to deploy but it's still built up",
    "start": "498120",
    "end": "500319"
  },
  {
    "text": "on a private Dev branch and have some",
    "start": "500319",
    "end": "502599"
  },
  {
    "text": "limited support from Kafka community so",
    "start": "502599",
    "end": "505919"
  },
  {
    "text": "um we also notice um there are many new",
    "start": "505919",
    "end": "508720"
  },
  {
    "text": "fixes and improvements merging into the",
    "start": "508720",
    "end": "510759"
  },
  {
    "text": "trunk uh in the past few months is with",
    "start": "510759",
    "end": "513560"
  },
  {
    "text": "that um we um we only offer this version",
    "start": "513560",
    "end": "517080"
  },
  {
    "text": "as a special uh addition to our internal",
    "start": "517080",
    "end": "520560"
  },
  {
    "text": "customers uh in the meanwhile kfka 3.6",
    "start": "520560",
    "end": "523839"
  },
  {
    "text": "was also getting close to release so we",
    "start": "523839",
    "end": "526000"
  },
  {
    "text": "decided to take the trunk branch and",
    "start": "526000",
    "end": "528680"
  },
  {
    "text": "then uh take the 3.6 Branch uh for",
    "start": "528680",
    "end": "532519"
  },
  {
    "text": "another round of testing so the",
    "start": "532519",
    "end": "534640"
  },
  {
    "text": "rationale is that even though 3.6 is not",
    "start": "534640",
    "end": "537240"
  },
  {
    "text": "declared as production ready um we can",
    "start": "537240",
    "end": "539720"
  },
  {
    "text": "still get some basic functionality",
    "start": "539720",
    "end": "541079"
  },
  {
    "text": "working and get active Community Support",
    "start": "541079",
    "end": "543440"
  },
  {
    "text": "since then if we find um",
    "start": "543440",
    "end": "546240"
  },
  {
    "text": "issues um so beside tier storage feature",
    "start": "546240",
    "end": "549120"
  },
  {
    "text": "moving Kafka from 2.8 to 3.6 is also",
    "start": "549120",
    "end": "552079"
  },
  {
    "text": "challenging um there are many",
    "start": "552079",
    "end": "554519"
  },
  {
    "text": "incompatible uh issues surfaced which we",
    "start": "554519",
    "end": "557480"
  },
  {
    "text": "can talk about more uh more in details",
    "start": "557480",
    "end": "559519"
  },
  {
    "text": "later but all in all um by November",
    "start": "559519",
    "end": "564079"
  },
  {
    "text": "2023 by November 2023 uh we were able to",
    "start": "564079",
    "end": "567200"
  },
  {
    "text": "sort out all kinds of challenges and",
    "start": "567200",
    "end": "569399"
  },
  {
    "text": "verify 3.6 Kafka with our internal",
    "start": "569399",
    "end": "572160"
  },
  {
    "text": "remote plug-in uh implementation so we",
    "start": "572160",
    "end": "575079"
  },
  {
    "text": "have since then gradually roll out that",
    "start": "575079",
    "end": "576959"
  },
  {
    "text": "version to our production um smoothly",
    "start": "576959",
    "end": "580800"
  },
  {
    "text": "than then so um looking a little bit um",
    "start": "580800",
    "end": "585040"
  },
  {
    "text": "um down deeper down uh let's move on to",
    "start": "585040",
    "end": "587399"
  },
  {
    "text": "talk about some technical challenges at",
    "start": "587399",
    "end": "589720"
  },
  {
    "text": "the beginning we decide to try the D 2.0",
    "start": "589720",
    "end": "592160"
  },
  {
    "text": "Branch Kafka it's pretty challenging",
    "start": "592160",
    "end": "593959"
  },
  {
    "text": "task because our team uh doesn't have",
    "start": "593959",
    "end": "596640"
  },
  {
    "text": "experience building KFAN stream Z we",
    "start": "596640",
    "end": "598440"
  },
  {
    "text": "were just rely on public release the",
    "start": "598440",
    "end": "600360"
  },
  {
    "text": "streamy operator and streamy kka image",
    "start": "600360",
    "end": "603000"
  },
  {
    "text": "in deployment uh fortunately um we can",
    "start": "603000",
    "end": "606480"
  },
  {
    "text": "um the the documentation for streamz is",
    "start": "606480",
    "end": "608360"
  },
  {
    "text": "really good and with some trial and",
    "start": "608360",
    "end": "610720"
  },
  {
    "text": "error we were able to build streamz",
    "start": "610720",
    "end": "612360"
  },
  {
    "text": "including both the operator and Kat",
    "start": "612360",
    "end": "615440"
  },
  {
    "text": "image um another challenge we faced was",
    "start": "615440",
    "end": "618760"
  },
  {
    "text": "that how can we uh include the remote",
    "start": "618760",
    "end": "622560"
  },
  {
    "text": "plug-in Library into the stream z build",
    "start": "622560",
    "end": "624760"
  },
  {
    "text": "Kafka image the plug-in library was not",
    "start": "624760",
    "end": "627760"
  },
  {
    "text": "part of the Kafka Rao so it's not",
    "start": "627760",
    "end": "631120"
  },
  {
    "text": "included in the tar file generated by",
    "start": "631120",
    "end": "633000"
  },
  {
    "text": "Kafka build uh we have two options one",
    "start": "633000",
    "end": "635680"
  },
  {
    "text": "is that we could customize the tar uh",
    "start": "635680",
    "end": "638760"
  },
  {
    "text": "tar file in Kafka build and use a um",
    "start": "638760",
    "end": "641600"
  },
  {
    "text": "composed tar file as input for streamy",
    "start": "641600",
    "end": "643959"
  },
  {
    "text": "build however this option is not ideal",
    "start": "643959",
    "end": "646600"
  },
  {
    "text": "because everything uh we want to change",
    "start": "646600",
    "end": "648600"
  },
  {
    "text": "every time we want to change the uh",
    "start": "648600",
    "end": "650200"
  },
  {
    "text": "plug-in Library we will need to rebuild",
    "start": "650200",
    "end": "652279"
  },
  {
    "text": "Kafka and through the U rebuild Kaka and",
    "start": "652279",
    "end": "654800"
  },
  {
    "text": "rebuild streamy which is time consuming",
    "start": "654800",
    "end": "657079"
  },
  {
    "text": "and aprom so the other approach we",
    "start": "657079",
    "end": "659720"
  },
  {
    "text": "choose that is that um we um uh we",
    "start": "659720",
    "end": "664240"
  },
  {
    "text": "customize the docker image directly with",
    "start": "664240",
    "end": "666320"
  },
  {
    "text": "additional library for uh remote uh S3",
    "start": "666320",
    "end": "669360"
  },
  {
    "text": "plugins this way works because uh it",
    "start": "669360",
    "end": "672399"
  },
  {
    "text": "doesn't require the rebuild of streams",
    "start": "672399",
    "end": "674320"
  },
  {
    "text": "the every time uh we only make changes",
    "start": "674320",
    "end": "677160"
  },
  {
    "text": "uh to the remote plug-in uh Library so",
    "start": "677160",
    "end": "680279"
  },
  {
    "text": "we have documented this instruction uh",
    "start": "680279",
    "end": "682639"
  },
  {
    "text": "as example in streamy proposal document",
    "start": "682639",
    "end": "684800"
  },
  {
    "text": "for tier storage",
    "start": "684800",
    "end": "687800"
  },
  {
    "text": "support so um the other one is that",
    "start": "687800",
    "end": "690639"
  },
  {
    "text": "after we got a working image we need to",
    "start": "690639",
    "end": "692680"
  },
  {
    "text": "ensure all the new tier storage related",
    "start": "692680",
    "end": "695160"
  },
  {
    "text": "configuration can be configured",
    "start": "695160",
    "end": "696720"
  },
  {
    "text": "correctly and pass down to Kafka broker",
    "start": "696720",
    "end": "700160"
  },
  {
    "text": "we were able to pass everything wi kavka",
    "start": "700160",
    "end": "702519"
  },
  {
    "text": "config in stream the API is working but",
    "start": "702519",
    "end": "705320"
  },
  {
    "text": "not ideal we have thereafter make a",
    "start": "705320",
    "end": "707560"
  },
  {
    "text": "streamz proposal to support tier storage",
    "start": "707560",
    "end": "710040"
  },
  {
    "text": "natively in streamy to simplify the",
    "start": "710040",
    "end": "713000"
  },
  {
    "text": "configuration and um we'll cover that uh",
    "start": "713000",
    "end": "716680"
  },
  {
    "text": "later",
    "start": "716680",
    "end": "719680"
  },
  {
    "text": "so um in this tier storage feature we",
    "start": "721800",
    "end": "724279"
  },
  {
    "text": "need a plug-in library for handling the",
    "start": "724279",
    "end": "726360"
  },
  {
    "text": "remote storage metadata we use a default",
    "start": "726360",
    "end": "729920"
  },
  {
    "text": "implementation um using a Kafka internal",
    "start": "729920",
    "end": "732519"
  },
  {
    "text": "topic to store remote log metadata the",
    "start": "732519",
    "end": "735680"
  },
  {
    "text": "plugin come with cut directly and is",
    "start": "735680",
    "end": "738000"
  },
  {
    "text": "ready to use out of box in the plug-in",
    "start": "738000",
    "end": "740880"
  },
  {
    "text": "implementation a new internal Kafka",
    "start": "740880",
    "end": "743600"
  },
  {
    "text": "client is created to read and write the",
    "start": "743600",
    "end": "746199"
  },
  {
    "text": "internal topic we need to config the",
    "start": "746199",
    "end": "748720"
  },
  {
    "text": "necessary configuration for this Kaa",
    "start": "748720",
    "end": "750399"
  },
  {
    "text": "client and make sure its connection is",
    "start": "750399",
    "end": "753040"
  },
  {
    "text": "secured So after talking with the uh",
    "start": "753040",
    "end": "756360"
  },
  {
    "text": "streamy owners we decide to uh use the",
    "start": "756360",
    "end": "758839"
  },
  {
    "text": "internal client to talk to uh 09 um 9091",
    "start": "758839",
    "end": "763800"
  },
  {
    "text": "Port which is a reserved internal Port",
    "start": "763800",
    "end": "766120"
  },
  {
    "text": "exposed by streamz for inter broker",
    "start": "766120",
    "end": "768880"
  },
  {
    "text": "communication um the client can",
    "start": "768880",
    "end": "770680"
  },
  {
    "text": "establish MTS connection with Kafka",
    "start": "770680",
    "end": "773199"
  },
  {
    "text": "broker using streamy created uh key key",
    "start": "773199",
    "end": "776880"
  },
  {
    "text": "store and passwords",
    "start": "776880",
    "end": "780360"
  },
  {
    "text": "so as mentioned before uh we have 2.8",
    "start": "783279",
    "end": "786040"
  },
  {
    "text": "dab Branch working and verified and we",
    "start": "786040",
    "end": "788560"
  },
  {
    "text": "decided to explore 3.6 cfut directly to",
    "start": "788560",
    "end": "791519"
  },
  {
    "text": "ensure we are up to dat with all the",
    "start": "791519",
    "end": "793360"
  },
  {
    "text": "recent deployment work from the",
    "start": "793360",
    "end": "795240"
  },
  {
    "text": "community this requires to visit uh to",
    "start": "795240",
    "end": "798480"
  },
  {
    "text": "revisit our uh build pipeline first",
    "start": "798480",
    "end": "801199"
  },
  {
    "text": "challenge is that we start to use Java",
    "start": "801199",
    "end": "803560"
  },
  {
    "text": "17 now for the build uh so we need to",
    "start": "803560",
    "end": "806399"
  },
  {
    "text": "experiment a much newer uh stream Z",
    "start": "806399",
    "end": "808839"
  },
  {
    "text": "version",
    "start": "808839",
    "end": "809720"
  },
  {
    "text": "0.3 uh",
    "start": "809720",
    "end": "811639"
  },
  {
    "text": "0.38 to fit the 3.6 Kafka version the",
    "start": "811639",
    "end": "815680"
  },
  {
    "text": "rebuilding process um seems smooth but",
    "start": "815680",
    "end": "819320"
  },
  {
    "text": "we soon realize that there's a new Kafka",
    "start": "819320",
    "end": "821639"
  },
  {
    "text": "cluster is not able to establish",
    "start": "821639",
    "end": "823839"
  },
  {
    "text": "consumer SSL connection",
    "start": "823839",
    "end": "826519"
  },
  {
    "text": "consistently um with some of the digging",
    "start": "826519",
    "end": "829079"
  },
  {
    "text": "into the streamy and Kafka release",
    "start": "829079",
    "end": "830800"
  },
  {
    "text": "history in uh in a brutal force style we",
    "start": "830800",
    "end": "833480"
  },
  {
    "text": "notice a suspicious issue in streamz",
    "start": "833480",
    "end": "836639"
  },
  {
    "text": "release note so related that is related",
    "start": "836639",
    "end": "839720"
  },
  {
    "text": "with uh Ingress controller dependency",
    "start": "839720",
    "end": "842120"
  },
  {
    "text": "version which is caused by a slightly",
    "start": "842120",
    "end": "844480"
  },
  {
    "text": "TSS Behavior change in Java 17 um if you",
    "start": "844480",
    "end": "848040"
  },
  {
    "text": "have also see the similar issue then",
    "start": "848040",
    "end": "850040"
  },
  {
    "text": "probably um we can provide some of the",
    "start": "850040",
    "end": "851759"
  },
  {
    "text": "information to save your",
    "start": "851759",
    "end": "854680"
  },
  {
    "text": "time so at this point we're able to get",
    "start": "856880",
    "end": "860440"
  },
  {
    "text": "the kafa um cluster startup and running",
    "start": "860440",
    "end": "863240"
  },
  {
    "text": "healthy however there are still a lot of",
    "start": "863240",
    "end": "865519"
  },
  {
    "text": "challenges uh when enabling tier storage",
    "start": "865519",
    "end": "867880"
  },
  {
    "text": "feature initially we were we were not",
    "start": "867880",
    "end": "869920"
  },
  {
    "text": "able to get a consumer fetching from",
    "start": "869920",
    "end": "871759"
  },
  {
    "text": "remote store consistently occasionally",
    "start": "871759",
    "end": "874040"
  },
  {
    "text": "the consumer could be stocked with no",
    "start": "874040",
    "end": "875920"
  },
  {
    "text": "obvious reason uh with some research um",
    "start": "875920",
    "end": "878680"
  },
  {
    "text": "we find the trick is that we need to",
    "start": "878680",
    "end": "880040"
  },
  {
    "text": "tune the max fetch weit time to ensure",
    "start": "880040",
    "end": "882759"
  },
  {
    "text": "the consumer have enough weit time to uh",
    "start": "882759",
    "end": "885399"
  },
  {
    "text": "wait for the um remote uh remote store",
    "start": "885399",
    "end": "888199"
  },
  {
    "text": "to get the data back um this is because",
    "start": "888199",
    "end": "891000"
  },
  {
    "text": "uh remote fetch is happening the broker",
    "start": "891000",
    "end": "893399"
  },
  {
    "text": "side latency typically increase a little",
    "start": "893399",
    "end": "895279"
  },
  {
    "text": "bit uh so sometimes we got P99 latency",
    "start": "895279",
    "end": "898560"
  },
  {
    "text": "for S3 uh get API uh that could reach a",
    "start": "898560",
    "end": "902160"
  },
  {
    "text": "very high number like a few seconds so",
    "start": "902160",
    "end": "905040"
  },
  {
    "text": "uh we need to increase this number and",
    "start": "905040",
    "end": "907759"
  },
  {
    "text": "instruct the kka client to wait a bit",
    "start": "907759",
    "end": "909560"
  },
  {
    "text": "longer after increasing the",
    "start": "909560",
    "end": "911000"
  },
  {
    "text": "configuration value we were able to get",
    "start": "911000",
    "end": "913120"
  },
  {
    "text": "the consumer application running",
    "start": "913120",
    "end": "915440"
  },
  {
    "text": "consistently um another interesting",
    "start": "915440",
    "end": "918839"
  },
  {
    "text": "configuration to consider tuning is the",
    "start": "918839",
    "end": "920600"
  },
  {
    "text": "remote log reader threats and remote log",
    "start": "920600",
    "end": "923560"
  },
  {
    "text": "manager threp size um it's kind of um",
    "start": "923560",
    "end": "927800"
  },
  {
    "text": "Case by casee raring in our testing uh",
    "start": "927800",
    "end": "930600"
  },
  {
    "text": "but uh increasing the the strapo size um",
    "start": "930600",
    "end": "934399"
  },
  {
    "text": "and the uh the um the the reader spreads",
    "start": "934399",
    "end": "937560"
  },
  {
    "text": "uh we could see slightly performance uh",
    "start": "937560",
    "end": "940199"
  },
  {
    "text": "improvements um the other one uh the",
    "start": "940199",
    "end": "943800"
  },
  {
    "text": "other one configuration wor mentioning",
    "start": "943800",
    "end": "945360"
  },
  {
    "text": "is the log segment size uh so uh the log",
    "start": "945360",
    "end": "949360"
  },
  {
    "text": "segment size uh decide the segment file",
    "start": "949360",
    "end": "951800"
  },
  {
    "text": "size um saved in kafa and the file size",
    "start": "951800",
    "end": "955519"
  },
  {
    "text": "will be uploaded uh uploaded to remote",
    "start": "955519",
    "end": "958600"
  },
  {
    "text": "storage and fetch down from there so if",
    "start": "958600",
    "end": "961079"
  },
  {
    "text": "that number is too small you will end up",
    "start": "961079",
    "end": "962680"
  },
  {
    "text": "with a lot of metadata uh information",
    "start": "962680",
    "end": "965079"
  },
  {
    "text": "and a lot of um a lot of more um S3 or",
    "start": "965079",
    "end": "968759"
  },
  {
    "text": "like remote store Fetch and upload but",
    "start": "968759",
    "end": "971680"
  },
  {
    "text": "with that number became too large um",
    "start": "971680",
    "end": "974519"
  },
  {
    "text": "then uh the payload will became very",
    "start": "974519",
    "end": "976519"
  },
  {
    "text": "large and then the S3 API call will have",
    "start": "976519",
    "end": "979040"
  },
  {
    "text": "some latency and that that that have a",
    "start": "979040",
    "end": "980880"
  },
  {
    "text": "latency implication as",
    "start": "980880",
    "end": "984199"
  },
  {
    "text": "well so let's go to the the um next",
    "start": "985920",
    "end": "990120"
  },
  {
    "text": "slides okay so at this point we're able",
    "start": "990120",
    "end": "992759"
  },
  {
    "text": "to achieve basic upload and download",
    "start": "992759",
    "end": "995040"
  },
  {
    "text": "however the performance is still not up",
    "start": "995040",
    "end": "997279"
  },
  {
    "text": "to the expectation uh we did some",
    "start": "997279",
    "end": "999639"
  },
  {
    "text": "optimization in the plug-in Library so",
    "start": "999639",
    "end": "1002199"
  },
  {
    "text": "first one is we use a multiart upload",
    "start": "1002199",
    "end": "1004279"
  },
  {
    "text": "for upload pass um so the multi uploads",
    "start": "1004279",
    "end": "1007399"
  },
  {
    "text": "allow you to upload a single object uh",
    "start": "1007399",
    "end": "1010839"
  },
  {
    "text": "as a set of parts so each part is um uh",
    "start": "1010839",
    "end": "1014279"
  },
  {
    "text": "is uh is a portion of the data and you",
    "start": "1014279",
    "end": "1016959"
  },
  {
    "text": "can upload this uh object part",
    "start": "1016959",
    "end": "1019440"
  },
  {
    "text": "independently uh without uh without any",
    "start": "1019440",
    "end": "1022160"
  },
  {
    "text": "order and if the transmission of any",
    "start": "1022160",
    "end": "1024959"
  },
  {
    "text": "parts fails you can re transmit or you",
    "start": "1024959",
    "end": "1027760"
  },
  {
    "text": "resubmit um after all the parts um has",
    "start": "1027760",
    "end": "1030678"
  },
  {
    "text": "been uh submitted uh to the um uh to the",
    "start": "1030679",
    "end": "1034038"
  },
  {
    "text": "remote uh you can uh instruct Amazon S3",
    "start": "1034039",
    "end": "1037160"
  },
  {
    "text": "to assemble these parts together uh to",
    "start": "1037160",
    "end": "1039600"
  },
  {
    "text": "create the uh the original object which",
    "start": "1039600",
    "end": "1042280"
  },
  {
    "text": "your desire to upload so in our case uh",
    "start": "1042280",
    "end": "1046000"
  },
  {
    "text": "um so in our case this is a this is a",
    "start": "1046000",
    "end": "1048038"
  },
  {
    "text": "perfect uh case for for multiart uploads",
    "start": "1048039",
    "end": "1052120"
  },
  {
    "text": "um second part is that we use the um AWS",
    "start": "1052120",
    "end": "1055320"
  },
  {
    "text": "S3 range fetch API so um the behavior",
    "start": "1055320",
    "end": "1059120"
  },
  {
    "text": "compar before is that instead uh instead",
    "start": "1059120",
    "end": "1061559"
  },
  {
    "text": "of a a use a single thread fetching you",
    "start": "1061559",
    "end": "1064679"
  },
  {
    "text": "were able to uh we were able to use a",
    "start": "1064679",
    "end": "1066880"
  },
  {
    "text": "thread pool to parallel fetch um and",
    "start": "1066880",
    "end": "1069600"
  },
  {
    "text": "speed up the fetching performance and",
    "start": "1069600",
    "end": "1071679"
  },
  {
    "text": "you don't need to um fetch the entire",
    "start": "1071679",
    "end": "1074160"
  },
  {
    "text": "thing if you only want certain part of",
    "start": "1074160",
    "end": "1076159"
  },
  {
    "text": "the uh segments uh to u to be fetched so",
    "start": "1076159",
    "end": "1079400"
  },
  {
    "text": "in our test we enable this um multi",
    "start": "1079400",
    "end": "1082120"
  },
  {
    "text": "Parts upload and concurrent range fetch",
    "start": "1082120",
    "end": "1086400"
  },
  {
    "text": "that improve our performance a lot uh in",
    "start": "1086400",
    "end": "1088640"
  },
  {
    "text": "term of bites in and bites out we",
    "start": "1088640",
    "end": "1090320"
  },
  {
    "text": "observe the throughputs roughly five",
    "start": "1090320",
    "end": "1092720"
  },
  {
    "text": "times um comparing with",
    "start": "1092720",
    "end": "1097039"
  },
  {
    "text": "before so um now we're in a relatively",
    "start": "1098640",
    "end": "1101960"
  },
  {
    "text": "good State the next step is to do the",
    "start": "1101960",
    "end": "1103760"
  },
  {
    "text": "performance tun uh tuning uh we're able",
    "start": "1103760",
    "end": "1105960"
  },
  {
    "text": "to have a healthy kovka running uh we're",
    "start": "1105960",
    "end": "1108240"
  },
  {
    "text": "able to for remote fetch uh and upload",
    "start": "1108240",
    "end": "1111000"
  },
  {
    "text": "in the uh in a decent performance so we",
    "start": "1111000",
    "end": "1113400"
  },
  {
    "text": "we spend some time to do the uh testing",
    "start": "1113400",
    "end": "1115600"
  },
  {
    "text": "uh for various test cases for example",
    "start": "1115600",
    "end": "1118200"
  },
  {
    "text": "what would happen when we have many",
    "start": "1118200",
    "end": "1119640"
  },
  {
    "text": "consumer application consuming the same",
    "start": "1119640",
    "end": "1121559"
  },
  {
    "text": "topic from the earliest offset",
    "start": "1121559",
    "end": "1124240"
  },
  {
    "text": "altogether um in our test case we",
    "start": "1124240",
    "end": "1126600"
  },
  {
    "text": "noticed that the broker could became",
    "start": "1126600",
    "end": "1128360"
  },
  {
    "text": "very busy and cause Interruption to",
    "start": "1128360",
    "end": "1131120"
  },
  {
    "text": "other basic functionalities uh so",
    "start": "1131120",
    "end": "1133720"
  },
  {
    "text": "basically you have a Noisy Neighbor uh",
    "start": "1133720",
    "end": "1135360"
  },
  {
    "text": "for example the uh Kafka producer client",
    "start": "1135360",
    "end": "1137679"
  },
  {
    "text": "will start to experience is very high",
    "start": "1137679",
    "end": "1139640"
  },
  {
    "text": "latency and per broker bit in may get",
    "start": "1139640",
    "end": "1142400"
  },
  {
    "text": "impacted so um the remote upload",
    "start": "1142400",
    "end": "1145159"
  },
  {
    "text": "functionality is also affected due to",
    "start": "1145159",
    "end": "1146760"
  },
  {
    "text": "Lake of CPU Cycles um to mitigate this",
    "start": "1146760",
    "end": "1149799"
  },
  {
    "text": "issue we choose to adjust the threat",
    "start": "1149799",
    "end": "1152159"
  },
  {
    "text": "pool that fetch uh fetch um fetch",
    "start": "1152159",
    "end": "1155039"
  },
  {
    "text": "threats use",
    "start": "1155039",
    "end": "1156600"
  },
  {
    "text": "uh uh from a cash thread pool to a fixed",
    "start": "1156600",
    "end": "1160039"
  },
  {
    "text": "thread pool so the previous one uh the",
    "start": "1160039",
    "end": "1162600"
  },
  {
    "text": "problem with with the cach threat is",
    "start": "1162600",
    "end": "1164880"
  },
  {
    "text": "that it will create um up to maximum in",
    "start": "1164880",
    "end": "1168919"
  },
  {
    "text": "number of threats as it always spin up a",
    "start": "1168919",
    "end": "1171200"
  },
  {
    "text": "new threat if the um if there if there's",
    "start": "1171200",
    "end": "1174440"
  },
  {
    "text": "a new ask so however the fix of threat",
    "start": "1174440",
    "end": "1176520"
  },
  {
    "text": "pool it will have a clear upper bound on",
    "start": "1176520",
    "end": "1179080"
  },
  {
    "text": "the Str numbers to Avid exhausting all",
    "start": "1179080",
    "end": "1181320"
  },
  {
    "text": "the CPU resources so you don't became",
    "start": "1181320",
    "end": "1183559"
  },
  {
    "text": "like the bad labor and consume all the",
    "start": "1183559",
    "end": "1185919"
  },
  {
    "text": "threads which could be used by the",
    "start": "1185919",
    "end": "1188520"
  },
  {
    "text": "producer um so the fixed thread pool",
    "start": "1188520",
    "end": "1190840"
  },
  {
    "text": "have this um limitation that we only",
    "start": "1190840",
    "end": "1194919"
  },
  {
    "text": "have certain number of threads to use uh",
    "start": "1194919",
    "end": "1198200"
  },
  {
    "text": "with some of the tuning we're able to",
    "start": "1198200",
    "end": "1199520"
  },
  {
    "text": "find acceptable solution for that um",
    "start": "1199520",
    "end": "1203520"
  },
  {
    "text": "another um optimization uh we used was",
    "start": "1203520",
    "end": "1206320"
  },
  {
    "text": "that we choose to uh create two separate",
    "start": "1206320",
    "end": "1208919"
  },
  {
    "text": "strap pool for the fetching logic and",
    "start": "1208919",
    "end": "1211120"
  },
  {
    "text": "upload logic in the remote storage",
    "start": "1211120",
    "end": "1213440"
  },
  {
    "text": "manager initially we mix mix all the",
    "start": "1213440",
    "end": "1216240"
  },
  {
    "text": "functionality with a single strap pool",
    "start": "1216240",
    "end": "1218360"
  },
  {
    "text": "um in the idea that both of that",
    "start": "1218360",
    "end": "1221320"
  },
  {
    "text": "resource could be shared for both",
    "start": "1221320",
    "end": "1223159"
  },
  {
    "text": "fetching and upload but um but then we",
    "start": "1223159",
    "end": "1225640"
  },
  {
    "text": "realize um they're affecting each other",
    "start": "1225640",
    "end": "1227799"
  },
  {
    "text": "sometimes so uh to to provide a a",
    "start": "1227799",
    "end": "1230640"
  },
  {
    "text": "guaranteed upper bound um or a lower",
    "start": "1230640",
    "end": "1232919"
  },
  {
    "text": "bound of performance uh we separate them",
    "start": "1232919",
    "end": "1235640"
  },
  {
    "text": "uh so that the r are managed separately",
    "start": "1235640",
    "end": "1238159"
  },
  {
    "text": "and we're able to ensure separation of",
    "start": "1238159",
    "end": "1239840"
  },
  {
    "text": "both of the",
    "start": "1239840",
    "end": "1242240"
  },
  {
    "text": "function so here I would like to uh",
    "start": "1244559",
    "end": "1246880"
  },
  {
    "text": "share some of the numbers for Benchmark",
    "start": "1246880",
    "end": "1248880"
  },
  {
    "text": "per per testing in the test cases here",
    "start": "1248880",
    "end": "1251240"
  },
  {
    "text": "we set up Kafka cluster with six Brokers",
    "start": "1251240",
    "end": "1254400"
  },
  {
    "text": "um um each of them running 3.6 Kafka we",
    "start": "1254400",
    "end": "1258559"
  },
  {
    "text": "we use stream Z version uh",
    "start": "1258559",
    "end": "1261000"
  },
  {
    "text": "0.38 to manage Kafka clusters and each",
    "start": "1261000",
    "end": "1264200"
  },
  {
    "text": "broker is running at R5 and4 large uh E2",
    "start": "1264200",
    "end": "1268960"
  },
  {
    "text": "AWS instance each instance have 16 CPU",
    "start": "1268960",
    "end": "1272360"
  },
  {
    "text": "and 64 gig memory we use this perf to",
    "start": "1272360",
    "end": "1276799"
  },
  {
    "text": "provided by Kafka um for testing for",
    "start": "1276799",
    "end": "1280440"
  },
  {
    "text": "each of the topic we set it's partition",
    "start": "1280440",
    "end": "1282240"
  },
  {
    "text": "number to",
    "start": "1282240",
    "end": "1283600"
  },
  {
    "text": "48 um and replication Factor S3 um the",
    "start": "1283600",
    "end": "1288279"
  },
  {
    "text": "test case in during the test case we",
    "start": "1288279",
    "end": "1290400"
  },
  {
    "text": "force the consumers to fetch from the",
    "start": "1290400",
    "end": "1292279"
  },
  {
    "text": "earliest offset um so uh in this case um",
    "start": "1292279",
    "end": "1296679"
  },
  {
    "text": "we're going to make sure that um all the",
    "start": "1296679",
    "end": "1298720"
  },
  {
    "text": "consumer is triggering the remote fetch",
    "start": "1298720",
    "end": "1303080"
  },
  {
    "text": "pass so let's look at the um the the",
    "start": "1305240",
    "end": "1308559"
  },
  {
    "text": "numbers so the result um um the key",
    "start": "1308559",
    "end": "1311720"
  },
  {
    "text": "matric we monitor is a per broker bites",
    "start": "1311720",
    "end": "1313679"
  },
  {
    "text": "in and bites out so that's what we",
    "start": "1313679",
    "end": "1315880"
  },
  {
    "text": "highlight here in this table uh here we",
    "start": "1315880",
    "end": "1318080"
  },
  {
    "text": "compare three uh different cases first",
    "start": "1318080",
    "end": "1320039"
  },
  {
    "text": "one a single consumer and a single topic",
    "start": "1320039",
    "end": "1323200"
  },
  {
    "text": "with the given setup we can achieve um I",
    "start": "1323200",
    "end": "1325840"
  },
  {
    "text": "think it's 270 megabytes per second byes",
    "start": "1325840",
    "end": "1328159"
  },
  {
    "text": "out uh this number is close to the per",
    "start": "1328159",
    "end": "1330960"
  },
  {
    "text": "we can achieve without um any remote",
    "start": "1330960",
    "end": "1333240"
  },
  {
    "text": "fetch so it's um looking good uh second",
    "start": "1333240",
    "end": "1336799"
  },
  {
    "text": "one we slightly add more consumers into",
    "start": "1336799",
    "end": "1339360"
  },
  {
    "text": "that um we don't see a lot of um big",
    "start": "1339360",
    "end": "1342080"
  },
  {
    "text": "change in the in the performance number",
    "start": "1342080",
    "end": "1344400"
  },
  {
    "text": "but slightly drops because um I think um",
    "start": "1344400",
    "end": "1347799"
  },
  {
    "text": "maybe the complex of the different",
    "start": "1347799",
    "end": "1350320"
  },
  {
    "text": "consumers have have some some",
    "start": "1350320",
    "end": "1352480"
  },
  {
    "text": "implications so um we see the similar",
    "start": "1352480",
    "end": "1355279"
  },
  {
    "text": "performance at broker level uh but per",
    "start": "1355279",
    "end": "1357600"
  },
  {
    "text": "broker consumer level uh we see um",
    "start": "1357600",
    "end": "1360080"
  },
  {
    "text": "should be roughly around 80 uh 80 um",
    "start": "1360080",
    "end": "1363640"
  },
  {
    "text": "megabytes per second 200 um megabytes",
    "start": "1363640",
    "end": "1366159"
  },
  {
    "text": "per second uh last one is that we um um",
    "start": "1366159",
    "end": "1370760"
  },
  {
    "text": "we include more consumer groups and we",
    "start": "1370760",
    "end": "1373320"
  },
  {
    "text": "bump the topic numbers to 400 uh so this",
    "start": "1373320",
    "end": "1377760"
  },
  {
    "text": "time we can see increasing topic numbers",
    "start": "1377760",
    "end": "1379640"
  },
  {
    "text": "could cause a implication on the",
    "start": "1379640",
    "end": "1381919"
  },
  {
    "text": "performance in our case each consumer",
    "start": "1381919",
    "end": "1384360"
  },
  {
    "text": "can get a bit less uh around uh 20 to 30",
    "start": "1384360",
    "end": "1388640"
  },
  {
    "text": "megabytes byes out and uh um this uh",
    "start": "1388640",
    "end": "1392360"
  },
  {
    "text": "this became a concern for our use case",
    "start": "1392360",
    "end": "1394799"
  },
  {
    "text": "um we uh you may want to do some",
    "start": "1394799",
    "end": "1396679"
  },
  {
    "text": "additional tunings and optimizations to",
    "start": "1396679",
    "end": "1398480"
  },
  {
    "text": "improve the perf for a larger amount of",
    "start": "1398480",
    "end": "1401000"
  },
  {
    "text": "topics I think the key a bottom neck",
    "start": "1401000",
    "end": "1403360"
  },
  {
    "text": "here is mostly related on CPU so when we",
    "start": "1403360",
    "end": "1405880"
  },
  {
    "text": "are using a bigger um uh ec2 instance",
    "start": "1405880",
    "end": "1408919"
  },
  {
    "text": "then the the bottom act has been",
    "start": "1408919",
    "end": "1411080"
  },
  {
    "text": "relieved then we see a much big bigger",
    "start": "1411080",
    "end": "1412679"
  },
  {
    "text": "number as well but um that's uh that's",
    "start": "1412679",
    "end": "1415840"
  },
  {
    "text": "that's wrap up the the performance",
    "start": "1415840",
    "end": "1417520"
  },
  {
    "text": "benchmarking for",
    "start": "1417520",
    "end": "1420320"
  },
  {
    "text": "this okay uh now I would like to uh on",
    "start": "1420559",
    "end": "1424799"
  },
  {
    "text": "and talk about some open source",
    "start": "1424799",
    "end": "1426200"
  },
  {
    "text": "contribution we have made uh like we",
    "start": "1426200",
    "end": "1428400"
  },
  {
    "text": "mentioned before we work closely with",
    "start": "1428400",
    "end": "1430640"
  },
  {
    "text": "streamy uh maintainers on a native tier",
    "start": "1430640",
    "end": "1433400"
  },
  {
    "text": "story support proposal this is available",
    "start": "1433400",
    "end": "1436480"
  },
  {
    "text": "after 0.40 one Z release um in this",
    "start": "1436480",
    "end": "1440640"
  },
  {
    "text": "proposal we are able to group all the",
    "start": "1440640",
    "end": "1443039"
  },
  {
    "text": "tier storage related configuration into",
    "start": "1443039",
    "end": "1445760"
  },
  {
    "text": "a dedicated tier storage spec and the ca",
    "start": "1445760",
    "end": "1448480"
  },
  {
    "text": "spec the US users are able to specify",
    "start": "1448480",
    "end": "1452640"
  },
  {
    "text": "the class information for the remote",
    "start": "1452640",
    "end": "1454600"
  },
  {
    "text": "storage manager as well as some",
    "start": "1454600",
    "end": "1456600"
  },
  {
    "text": "additional configuration for remote",
    "start": "1456600",
    "end": "1458559"
  },
  {
    "text": "storage",
    "start": "1458559",
    "end": "1459480"
  },
  {
    "text": "manager so the streamy implementation um",
    "start": "1459480",
    "end": "1463000"
  },
  {
    "text": "take the assumption that the uh the",
    "start": "1463000",
    "end": "1465799"
  },
  {
    "text": "remote log metadata manager will use the",
    "start": "1465799",
    "end": "1468360"
  },
  {
    "text": "default option like what we do in in",
    "start": "1468360",
    "end": "1471200"
  },
  {
    "text": "what in what B talk about which is a",
    "start": "1471200",
    "end": "1473360"
  },
  {
    "text": "topic based remote log dat manager class",
    "start": "1473360",
    "end": "1476520"
  },
  {
    "text": "so this configuration for remote log",
    "start": "1476520",
    "end": "1479240"
  },
  {
    "text": "metata manager class can be can be",
    "start": "1479240",
    "end": "1483039"
  },
  {
    "text": "simply omitted and those necessary",
    "start": "1483039",
    "end": "1485760"
  },
  {
    "text": "configuration and all the internal",
    "start": "1485760",
    "end": "1488080"
  },
  {
    "text": "client configuration we talked about",
    "start": "1488080",
    "end": "1490120"
  },
  {
    "text": "before will be automatically generated",
    "start": "1490120",
    "end": "1492880"
  },
  {
    "text": "and passed to Capa broker configuration",
    "start": "1492880",
    "end": "1496080"
  },
  {
    "text": "by streamy cluster operator",
    "start": "1496080",
    "end": "1500320"
  },
  {
    "text": "for the configuration in the previous",
    "start": "1500520",
    "end": "1502039"
  },
  {
    "text": "slides this is the final configuration",
    "start": "1502039",
    "end": "1504480"
  },
  {
    "text": "generated by streamy cluster operator",
    "start": "1504480",
    "end": "1507480"
  },
  {
    "text": "there are three section generated the",
    "start": "1507480",
    "end": "1509640"
  },
  {
    "text": "first part",
    "start": "1509640",
    "end": "1510840"
  },
  {
    "text": "ISM config generated by streamy so this",
    "start": "1510840",
    "end": "1514600"
  },
  {
    "text": "contain the basic setup for the feuture",
    "start": "1514600",
    "end": "1517080"
  },
  {
    "text": "enablement and the RM client",
    "start": "1517080",
    "end": "1520440"
  },
  {
    "text": "configuration the second part is remote",
    "start": "1520440",
    "end": "1523360"
  },
  {
    "text": "storage manager configuration set by the",
    "start": "1523360",
    "end": "1525640"
  },
  {
    "text": "operator and by the user the additional",
    "start": "1525640",
    "end": "1528679"
  },
  {
    "text": "configuration value for example the",
    "start": "1528679",
    "end": "1530360"
  },
  {
    "text": "bucket name is placed here um the third",
    "start": "1530360",
    "end": "1533840"
  },
  {
    "text": "part is if there's any additional",
    "start": "1533840",
    "end": "1536159"
  },
  {
    "text": "configuration for the custom I config it",
    "start": "1536159",
    "end": "1539120"
  },
  {
    "text": "will be plac",
    "start": "1539120",
    "end": "1541840"
  },
  {
    "text": "here so please note that um the support",
    "start": "1541880",
    "end": "1545039"
  },
  {
    "text": "insty is only for configuration so to",
    "start": "1545039",
    "end": "1548039"
  },
  {
    "text": "make the feature work you still need to",
    "start": "1548039",
    "end": "1550440"
  },
  {
    "text": "include the library dependency in the",
    "start": "1550440",
    "end": "1552720"
  },
  {
    "text": "into the kafa library like for example",
    "start": "1552720",
    "end": "1555080"
  },
  {
    "text": "using the approach we talked about",
    "start": "1555080",
    "end": "1556679"
  },
  {
    "text": "before here is a example example how we",
    "start": "1556679",
    "end": "1559159"
  },
  {
    "text": "do this using the docker so we simply",
    "start": "1559159",
    "end": "1561720"
  },
  {
    "text": "put the preview jar file into a",
    "start": "1561720",
    "end": "1564000"
  },
  {
    "text": "dependency folder and the folder pass is",
    "start": "1564000",
    "end": "1566840"
  },
  {
    "text": "predefined at the remote log storage",
    "start": "1566840",
    "end": "1569720"
  },
  {
    "text": "manager class pass in this way the CF",
    "start": "1569720",
    "end": "1572760"
  },
  {
    "text": "car can identify the plugin Library",
    "start": "1572760",
    "end": "1574840"
  },
  {
    "text": "class and this kill the",
    "start": "1574840",
    "end": "1577600"
  },
  {
    "text": "task if you are interested to know more",
    "start": "1577600",
    "end": "1580399"
  },
  {
    "text": "about the implementation of The Proposal",
    "start": "1580399",
    "end": "1582399"
  },
  {
    "text": "you can find um additional information",
    "start": "1582399",
    "end": "1584960"
  },
  {
    "text": "in this reference link",
    "start": "1584960",
    "end": "1588399"
  },
  {
    "text": "so that's all I have prepared for the",
    "start": "1588399",
    "end": "1590440"
  },
  {
    "text": "talk uh I would like to take questions",
    "start": "1590440",
    "end": "1593679"
  },
  {
    "text": "from",
    "start": "1593679",
    "end": "1595919"
  },
  {
    "text": "audience thank you very much really",
    "start": "1603840",
    "end": "1606399"
  },
  {
    "text": "interesting to see kind of the the",
    "start": "1606399",
    "end": "1608279"
  },
  {
    "text": "different levels and then yeah great",
    "start": "1608279",
    "end": "1610760"
  },
  {
    "text": "that we've got this now being",
    "start": "1610760",
    "end": "1612679"
  },
  {
    "text": "contributed back into steny as well um",
    "start": "1612679",
    "end": "1616000"
  },
  {
    "text": "we don't Curr I don't think have any",
    "start": "1616000",
    "end": "1619279"
  },
  {
    "text": "questions um but I did have a question",
    "start": "1619279",
    "end": "1621840"
  },
  {
    "text": "um for all of you which was um in your",
    "start": "1621840",
    "end": "1624600"
  },
  {
    "text": "experience of then engaging with the",
    "start": "1624600",
    "end": "1626840"
  },
  {
    "text": "contributors what um what would you",
    "start": "1626840",
    "end": "1629880"
  },
  {
    "text": "recommend for people who were also",
    "start": "1629880",
    "end": "1631679"
  },
  {
    "text": "interest interested in contributing to",
    "start": "1631679",
    "end": "1633600"
  },
  {
    "text": "stmy what did you find were the kind of",
    "start": "1633600",
    "end": "1635360"
  },
  {
    "text": "easiest ways to engage with the",
    "start": "1635360",
    "end": "1637039"
  },
  {
    "text": "community and and make progress yeah I I",
    "start": "1637039",
    "end": "1640799"
  },
  {
    "text": "I want to call that I get really really",
    "start": "1640799",
    "end": "1643880"
  },
  {
    "text": "warm welcome from the community",
    "start": "1643880",
    "end": "1646000"
  },
  {
    "text": "especially from ja Paulo so they give us",
    "start": "1646000",
    "end": "1650559"
  },
  {
    "text": "they give our team very strong support",
    "start": "1650559",
    "end": "1652559"
  },
  {
    "text": "and guidance on the implementation so I",
    "start": "1652559",
    "end": "1655600"
  },
  {
    "text": "think it's know when we reach out so um",
    "start": "1655600",
    "end": "1658159"
  },
  {
    "text": "I think the response and the the level",
    "start": "1658159",
    "end": "1660440"
  },
  {
    "text": "of support we get is is really really",
    "start": "1660440",
    "end": "1662519"
  },
  {
    "text": "high so um yeah I would say if if other",
    "start": "1662519",
    "end": "1666240"
  },
  {
    "text": "people want to contribute to stream Z no",
    "start": "1666240",
    "end": "1669720"
  },
  {
    "text": "definitely reach out and there are from",
    "start": "1669720",
    "end": "1671919"
  },
  {
    "text": "what I learned there are some open open",
    "start": "1671919",
    "end": "1674840"
  },
  {
    "text": "topics that need help on implementation",
    "start": "1674840",
    "end": "1677640"
  },
  {
    "text": "uh and there also have some good idea on",
    "start": "1677640",
    "end": "1680279"
  },
  {
    "text": "some known future Improvement we need to",
    "start": "1680279",
    "end": "1682440"
  },
  {
    "text": "work on so yeah I think the level of",
    "start": "1682440",
    "end": "1685679"
  },
  {
    "text": "support from maintainer is is a is",
    "start": "1685679",
    "end": "1688919"
  },
  {
    "text": "definitely exceed my expectation so",
    "start": "1688919",
    "end": "1691039"
  },
  {
    "text": "thank you uh jaob and Paulo for all the",
    "start": "1691039",
    "end": "1695559"
  },
  {
    "text": "support yeah based on my experience is",
    "start": "1695919",
    "end": "1698159"
  },
  {
    "text": "also very responsive um so um it's it's",
    "start": "1698159",
    "end": "1701720"
  },
  {
    "text": "really fast actually the the response",
    "start": "1701720",
    "end": "1703600"
  },
  {
    "text": "cycle once you reach out pretty much",
    "start": "1703600",
    "end": "1705559"
  },
  {
    "text": "within a day or so you're going to get",
    "start": "1705559",
    "end": "1707120"
  },
  {
    "text": "some some response and then there will",
    "start": "1707120",
    "end": "1709080"
  },
  {
    "text": "be further instructions and they will",
    "start": "1709080",
    "end": "1710760"
  },
  {
    "text": "give you a lot of useful links to follow",
    "start": "1710760",
    "end": "1712760"
  },
  {
    "text": "up I think I think that's very",
    "start": "1712760",
    "end": "1714120"
  },
  {
    "text": "responsive which which is IND surprising",
    "start": "1714120",
    "end": "1717240"
  },
  {
    "text": "surprisingly in the good",
    "start": "1717240",
    "end": "1720360"
  },
  {
    "text": "way awesome so yeah if anyone does think",
    "start": "1723360",
    "end": "1727480"
  },
  {
    "text": "of any other questions then um yeah feel",
    "start": "1727480",
    "end": "1729760"
  },
  {
    "text": "free to put them in the Q&A in the next",
    "start": "1729760",
    "end": "1731360"
  },
  {
    "text": "few minutes also all of our speakers are",
    "start": "1731360",
    "end": "1733840"
  },
  {
    "text": "available as well in the stry slack",
    "start": "1733840",
    "end": "1736039"
  },
  {
    "text": "channel so you can go over there and ask",
    "start": "1736039",
    "end": "1738799"
  },
  {
    "text": "them any questions as",
    "start": "1738799",
    "end": "1741640"
  },
  {
    "text": "well so yeah thank you again um for",
    "start": "1741640",
    "end": "1744799"
  },
  {
    "text": "speaking",
    "start": "1744799",
    "end": "1745760"
  },
  {
    "text": "today thank you thank you",
    "start": "1745760",
    "end": "1749440"
  },
  {
    "text": "for uh so with that uh that is actually",
    "start": "1750760",
    "end": "1754279"
  },
  {
    "text": "the end of um strumsy con so the last",
    "start": "1754279",
    "end": "1759440"
  },
  {
    "text": "thing that we wanted to do is a massive",
    "start": "1759440",
    "end": "1763559"
  },
  {
    "text": "thank you to the cloud native Computing",
    "start": "1763559",
    "end": "1766360"
  },
  {
    "text": "foundation for not only accepting stmy",
    "start": "1766360",
    "end": "1769760"
  },
  {
    "text": "as an incubating project but also",
    "start": "1769760",
    "end": "1771519"
  },
  {
    "text": "helping us to host this event we",
    "start": "1771519",
    "end": "1773360"
  },
  {
    "text": "wouldn't be able to do it without them",
    "start": "1773360",
    "end": "1775159"
  },
  {
    "text": "they provided the whole platform for",
    "start": "1775159",
    "end": "1777000"
  },
  {
    "text": "this um so yeah just a massive thank you",
    "start": "1777000",
    "end": "1779480"
  },
  {
    "text": "to cncf for taking strumsy in and",
    "start": "1779480",
    "end": "1783240"
  },
  {
    "text": "helping us to run strey con as well",
    "start": "1783240",
    "end": "1788440"
  }
]