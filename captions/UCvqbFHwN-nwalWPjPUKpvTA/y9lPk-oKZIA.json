[
  {
    "start": "0",
    "end": "90000"
  },
  {
    "text": "thank you guys for joining uh we're here to talk about mastering llm delivery in",
    "start": "680",
    "end": "6200"
  },
  {
    "text": "private clouds a journey to seamless deployments mostly with kubernetes and",
    "start": "6200",
    "end": "14160"
  },
  {
    "text": "oci and for those of you who are familiar with coh here we do have a relationship with another oci we're",
    "start": "14160",
    "end": "21240"
  },
  {
    "text": "talking about open container initiative not Oracle Cloud infrastructure but hey our Oracle Cloud friends we we like you",
    "start": "21240",
    "end": "27800"
  },
  {
    "text": "guys too so uh before we get started let me give you an intro so my name is",
    "start": "27800",
    "end": "34160"
  },
  {
    "text": "Autumn Moulder I'm the director of infrastructure and security at cooh here we build large language models yes we",
    "start": "34160",
    "end": "41520"
  },
  {
    "text": "put llm in the title of the talk mostly to just bring you all here uh I won't",
    "start": "41520",
    "end": "47000"
  },
  {
    "text": "spoil what the talk actually is about but we will talk about LMS a little bit uh we build foundational models we help",
    "start": "47000",
    "end": "53239"
  },
  {
    "text": "uh help companies that're looking to deploy and use this Tech in their Enterprise and then to talk us through",
    "start": "53239",
    "end": "58519"
  },
  {
    "text": "some of the challenges we have hi everyone uh my name is Maran I am a",
    "start": "58519",
    "end": "65158"
  },
  {
    "text": "member of technical staff coh here um previously I worked on the Azure kubernetes service team at Microsoft and",
    "start": "65159",
    "end": "71119"
  },
  {
    "text": "more recently at the company formerly known as Twitter I've contributed to a lot of several cncf projects in the past",
    "start": "71119",
    "end": "77600"
  },
  {
    "text": "so it's super exciting for me to see the foundational role cncf is doing with the success of the new kid on the Block",
    "start": "77600",
    "end": "82880"
  },
  {
    "text": "large language models and we can't wait to share with you our journey so without further Ado I'll hand it back to",
    "start": "82880",
    "end": "88840"
  },
  {
    "text": "Autumn all right thanks Marvin so I'll just tee this up a little bit uh before we get started i' like to know who I'm",
    "start": "88840",
    "end": "95479"
  },
  {
    "start": "90000",
    "end": "371000"
  },
  {
    "text": "talking to so raise of hands who's a site reliability engineer or identifies",
    "start": "95479",
    "end": "102360"
  },
  {
    "text": "as such okay we've got a few of you thank you one on the stage thank goodness uh data scientists ml's out",
    "start": "102360",
    "end": "110079"
  },
  {
    "text": "there oh good number all right I have no idea what the rest of you are here for",
    "start": "110079",
    "end": "116240"
  },
  {
    "text": "but I hope you find something valuable out of our conversation awesome so",
    "start": "116240",
    "end": "122000"
  },
  {
    "text": "really briefly most of you probably know this because you saw the word llm in the talk of the title but what is a large",
    "start": "122000",
    "end": "127439"
  },
  {
    "text": "language model um in a nutshell I asked our large language model this is what it",
    "start": "127439",
    "end": "133560"
  },
  {
    "text": "is you know it's a way you can talk to computers uh with natural language um my",
    "start": "133560",
    "end": "141640"
  },
  {
    "text": "favorite definition to be honest this one uh it's a pile of linear",
    "start": "141640",
    "end": "148680"
  },
  {
    "text": "algebra on dis so for those of you who are infrastructure Engineers uh that's kind of relevant because we're just",
    "start": "148680",
    "end": "154400"
  },
  {
    "text": "dealing with uh files right files on disk that store the the probabilities",
    "start": "154400",
    "end": "160920"
  },
  {
    "text": "so we'll talk about the llm serving stack briefly just to Anchor this conversation and then uh and before we",
    "start": "160920",
    "end": "167599"
  },
  {
    "text": "get to the the technical challenges so this is this is pretty high level um",
    "start": "167599",
    "end": "173239"
  },
  {
    "text": "when we're talking about the system components for LM stack it's it's very similar to any other system that you're",
    "start": "173239",
    "end": "178760"
  },
  {
    "text": "running so you have a kind of surrounding pieces the observability layer the persistence layer artifact",
    "start": "178760",
    "end": "185720"
  },
  {
    "text": "management I mean this is this is what you get when you have to serve any kind of system uh in the middle here we have",
    "start": "185720",
    "end": "192200"
  },
  {
    "text": "what are our CPU Based Services so we have our rate limiting our authentication and points in routing um",
    "start": "192200",
    "end": "199599"
  },
  {
    "text": "batching and queing which actually turns out to be a really important problem when you're dealing with with large language models U but these are all CPU",
    "start": "199599",
    "end": "206640"
  },
  {
    "text": "based workloads that we run on on gke and and then we have over here on the",
    "start": "206640",
    "end": "211920"
  },
  {
    "text": "right sorry your right uh we have the the models and these are our GPU based",
    "start": "211920",
    "end": "217519"
  },
  {
    "text": "workloads which is where a lot of interesting challenges come into play um",
    "start": "217519",
    "end": "222560"
  },
  {
    "text": "and so yeah this is this is it this is our this is our SAS system this is kind of what we started out as uh as a",
    "start": "222560",
    "end": "228920"
  },
  {
    "text": "company because we we started out small you know we're startup uh a lot of the systems that we that we built had some",
    "start": "228920",
    "end": "236319"
  },
  {
    "text": "pretty key dependencies on on gcp because that that's how you how you start going but um what we found was in",
    "start": "236319",
    "end": "243640"
  },
  {
    "text": "the market there's a strong need to run private llms uh so just real quick why",
    "start": "243640",
    "end": "249400"
  },
  {
    "text": "do companies need a private llm I think most of you can probably guess but this is similar to when uh as a as an",
    "start": "249400",
    "end": "256880"
  },
  {
    "text": "industry we started moving into the cloud like all the same reasons that people uh gave for I can't move to out",
    "start": "256880",
    "end": "263199"
  },
  {
    "text": "of my cooca facility into the cloud it's the it's the exact same reasons we're hearing for why I need to run a private",
    "start": "263199",
    "end": "268360"
  },
  {
    "text": "llm and don't want to use a system so it's compliance and security controls you know you have those built up you",
    "start": "268360",
    "end": "274320"
  },
  {
    "text": "want to use them uh and you want those to to apply to any llms that you're building for uh for your system um",
    "start": "274320",
    "end": "282479"
  },
  {
    "text": "you've got data volumes and latency are pretty pretty key problems so if you're if you're shoving a lot of data to the",
    "start": "282479",
    "end": "287720"
  },
  {
    "text": "llm you need that collocated with the rest of your infrastructure to just kind of drop latency um or you don't want to",
    "start": "287720",
    "end": "293800"
  },
  {
    "text": "page large data eress costs and then uh for those who are infrastructure engineers you know that it's really nice",
    "start": "293800",
    "end": "300960"
  },
  {
    "text": "to have a single plane of availability uh to to monitor availability to monitor",
    "start": "300960",
    "end": "306160"
  },
  {
    "text": "reliability and just bring that all into into one place so these are a lot of the challenges we were hearing from",
    "start": "306160",
    "end": "311400"
  },
  {
    "text": "companies who were saying we don't want to use a SAS provideed system but we also don't have the expertise to run uh",
    "start": "311400",
    "end": "317840"
  },
  {
    "text": "a fully open- Source uh stack and this is just kind of an evolving space so we want we want help so as a as a",
    "start": "317840",
    "end": "323919"
  },
  {
    "text": "leadership team we brought the the challenge to our external infrastructure team uh we've got the rest of the crew",
    "start": "323919",
    "end": "329880"
  },
  {
    "text": "crew here today and then then marwin um it was a really great group effort to figure out how do you take the system",
    "start": "329880",
    "end": "336840"
  },
  {
    "text": "running on our SAS environment and make it really easy for us to to deploy into private environment so to walk through",
    "start": "336840",
    "end": "342080"
  },
  {
    "text": "those challenges and solutions marwin thanks autum um so one of the",
    "start": "342080",
    "end": "348400"
  },
  {
    "text": "first challenges we've hit was our dependency on managed services so we were startup we're trying to moves fast",
    "start": "348400",
    "end": "354160"
  },
  {
    "text": "so we took a lot of dependencies on gcp specific components in areas such as observability and persistence",
    "start": "354160",
    "end": "360280"
  },
  {
    "text": "and we found out that when we started looking at our code that effectively it did a lot of assumption that it's",
    "start": "360280",
    "end": "366120"
  },
  {
    "text": "running in gcp it was super tightly coupled to the gcp SDK as well so what",
    "start": "366120",
    "end": "371960"
  },
  {
    "start": "371000",
    "end": "539000"
  },
  {
    "text": "we need to do is first of all make our stack Cloud agnostic right so that means is we need to figure out what we going",
    "start": "371960",
    "end": "377759"
  },
  {
    "text": "to do with the manage services and then we need to make sure that this stack can run on other multiple targets as well so",
    "start": "377759",
    "end": "384280"
  },
  {
    "text": "the first lessons we've learned is to invest in abstractions and invest in those abstractions early so so for",
    "start": "384280",
    "end": "389680"
  },
  {
    "text": "services such as databases and puup sub you want a uniform API that is independent of the underlying",
    "start": "389680",
    "end": "395400"
  },
  {
    "text": "implementation and in some cases you might also want to have those interactions behind a separate",
    "start": "395400",
    "end": "401160"
  },
  {
    "text": "microservice when we started looking at our code um a lot of it a lot of the functions and the objects they had a lot",
    "start": "401160",
    "end": "407360"
  },
  {
    "text": "of configuration options that were specific to our use case so we had things like analytics code billing code",
    "start": "407360",
    "end": "412880"
  },
  {
    "text": "and feature flagging libraries as well not all of those things are going to be um relevant for every single customer so",
    "start": "412880",
    "end": "418759"
  },
  {
    "text": "we needed to simp simplify our service configuration API so that's where the functional options pattern came in handy",
    "start": "418759",
    "end": "425240"
  },
  {
    "text": "so it's a creational design pattern that lets you build complex structure using Constructor that can take zero or more",
    "start": "425240",
    "end": "431360"
  },
  {
    "text": "functions and those functions end up modifying the return State um that you return to the",
    "start": "431360",
    "end": "437360"
  },
  {
    "text": "consumers so with that function we're actually easily able to Define different options for our private Cloud",
    "start": "437360",
    "end": "442840"
  },
  {
    "text": "deployments as well as our um s deployments so we're easily able to turn off feature flagging for those private",
    "start": "442840",
    "end": "449000"
  },
  {
    "text": "cloud configurations and also made it super easy for us to Define new options without modifying the existing",
    "start": "449000",
    "end": "454800"
  },
  {
    "text": "Constructor code because all you need to do is to define the new um options interface the second lesson we've",
    "start": "454800",
    "end": "461160"
  },
  {
    "text": "learned is not to reinvent the wheel it can be tempting sometimes to Define your own abstractions over common operations",
    "start": "461160",
    "end": "467280"
  },
  {
    "text": "but there's no need to do so um when Solutions already exist so for object storage we relied on Thanos Object Store",
    "start": "467280",
    "end": "474599"
  },
  {
    "text": "Library which abstracts a lot of the operations over common storage backends and instead instead of having to worry",
    "start": "474599",
    "end": "480159"
  },
  {
    "text": "about the specifics of persistent storage and the implementation of storage devices we required a user",
    "start": "480159",
    "end": "486039"
  },
  {
    "text": "provided R write many PVC as part of the application configuration so that PVC we",
    "start": "486039",
    "end": "491639"
  },
  {
    "text": "mostly use for storing um the large model weights um as an optimization to avoid having to redownload them during",
    "start": "491639",
    "end": "498759"
  },
  {
    "text": "autoscaling for metrics uh most of our code was instrumented from day one with Prometheus metrics and it's def the",
    "start": "498759",
    "end": "505120"
  },
  {
    "text": "defacto tool in the industry so we didn't really need to do much there and for logging there's a lot of exporters out there that convert between",
    "start": "505120",
    "end": "511760"
  },
  {
    "text": "proprietary and open source formats and finally um we try to rely on workload",
    "start": "511760",
    "end": "517279"
  },
  {
    "text": "identity whenever is whenever possible because it simplifies the integration between kubernetes service accounts as",
    "start": "517279",
    "end": "522719"
  },
  {
    "text": "well as IM service accounts so that's what we started with you can see all the gcp specific blocks",
    "start": "522719",
    "end": "528760"
  },
  {
    "text": "there and it turns out all you need to do is just very little few abstractions and then you're halfway there but we",
    "start": "528760",
    "end": "534640"
  },
  {
    "text": "haven't really talked about the artifact management layer and for us our our most valuable artifact is our model weights",
    "start": "534640",
    "end": "541560"
  },
  {
    "start": "539000",
    "end": "757000"
  },
  {
    "text": "so we needed a solution to deliver our model weights securely into private Cloud um",
    "start": "541560",
    "end": "546839"
  },
  {
    "text": "scenarios we can roll we can roll our own storage service the issue with that is you need to worry about",
    "start": "546839",
    "end": "552360"
  },
  {
    "text": "authentication and security you also need to worry about scalability and finally need to worry about the cost so",
    "start": "552360",
    "end": "559279"
  },
  {
    "text": "I'm going to take a quick break from the lessons and deep dive into how we design our model waste delivery",
    "start": "559279",
    "end": "564600"
  },
  {
    "text": "solution so the first thing we considered is creating assigned urls so",
    "start": "564600",
    "end": "569640"
  },
  {
    "text": "it's a good solution but signed URLs are quite Limited in nature because they have short expiration times and they",
    "start": "569640",
    "end": "575880"
  },
  {
    "text": "only need to be for exact object U and our model weights were quite large so we had model weights that were more than",
    "start": "575880",
    "end": "581640"
  },
  {
    "text": "100 gabes so in order to deliver a solution using that approach you need to create a unique URL per customer per",
    "start": "581640",
    "end": "587839"
  },
  {
    "text": "file per model which isn't really quite scalable the second option we considered",
    "start": "587839",
    "end": "592920"
  },
  {
    "text": "was to deliver those artifacts directly into the customer's object storage so we can use workload identifi ation to give",
    "start": "592920",
    "end": "599839"
  },
  {
    "text": "them access to our you know storage back end and then um they can pull the weights directly from our end the issue",
    "start": "599839",
    "end": "606839"
  },
  {
    "text": "with that is that it requires a lot of manual setup and it's a complex setup to do it's also not supported in all Cloud",
    "start": "606839",
    "end": "613079"
  },
  {
    "text": "providers and finally it's obviously not support in on pram so that solution wasn't really a viable thing for",
    "start": "613079",
    "end": "618839"
  },
  {
    "text": "us the third option was to bundle our model waste with the container images so",
    "start": "618839",
    "end": "624000"
  },
  {
    "text": "our application consists mostly of serving images as well as the container waste and that effectively what makes um",
    "start": "624000",
    "end": "629480"
  },
  {
    "text": "the llm stack so we thought okay we can just bundle the model weights in the same container image as well and if the",
    "start": "629480",
    "end": "634760"
  },
  {
    "text": "users can pull the container images they can pull the model weights so for container images specifically we relied",
    "start": "634760",
    "end": "640399"
  },
  {
    "text": "on a reverse proxy that is sitting on top of our Google artifact registry so reverse proxy is um connected to the",
    "start": "640399",
    "end": "647399"
  },
  {
    "text": "artifactry through a service account that gives it authorized access to pull those IM those images and then the",
    "start": "647399",
    "end": "653200"
  },
  {
    "text": "customers can talk to this reverse proxy using the license key to pull those images through and we relied on replicated it's commercial product but",
    "start": "653200",
    "end": "660240"
  },
  {
    "text": "part of them is you know the licensing API plus the reverse proxy side of things the issue with that approach is",
    "start": "660240",
    "end": "666839"
  },
  {
    "text": "it's less flexible because of the tight coupling in introduces between the application layer as well as the model",
    "start": "666839",
    "end": "672399"
  },
  {
    "text": "weights our model weights have generally a quicker release Cadence and the fact that we have to create the same image",
    "start": "672399",
    "end": "679160"
  },
  {
    "text": "every time we want to release a model weight wasn't really flexible the other issue is that it negates the",
    "start": "679160",
    "end": "684440"
  },
  {
    "text": "optimizations we did with the NFS cache um for quicker Autos scaling of models so you end up like you know being not",
    "start": "684440",
    "end": "691360"
  },
  {
    "text": "being able to scale as fast and finally because of the size of those images bundled with you know the serving images",
    "start": "691360",
    "end": "697279"
  },
  {
    "text": "which are already quite large plus the model weights patching um critical vulnerabilities takes a while so we took",
    "start": "697279",
    "end": "703680"
  },
  {
    "text": "a look at that and okay it seems that we already have a mechanism to deliver container images is there a way where we",
    "start": "703680",
    "end": "709920"
  },
  {
    "text": "can use the same mechanism to deliver the model waste and achieve the desired decoupling and that's where o artifacts",
    "start": "709920",
    "end": "716600"
  },
  {
    "text": "come into place so lesson three is o artifacts are a very powerful standard like really powerful um the for those",
    "start": "716600",
    "end": "724079"
  },
  {
    "text": "who are not familiar with o artifacts they are a way to utiliz oci compliant reges to store arbitrary files and",
    "start": "724079",
    "end": "731360"
  },
  {
    "text": "because you're utilizing the same infrastructure as containers you end up consolidating the security and the management efforts into a single",
    "start": "731360",
    "end": "738920"
  },
  {
    "text": "solution the community standard um the community usage of OC artifacts is still evolving but we we believe they have",
    "start": "738920",
    "end": "745760"
  },
  {
    "text": "strong potential as a generic artifact store and Registries can be really a very good generic artifact store so I'm",
    "start": "745760",
    "end": "752560"
  },
  {
    "text": "going to Deep dive into how we use o artifacts to build our model W solution so we use the open source project orus",
    "start": "752560",
    "end": "760040"
  },
  {
    "text": "orus is the defacto tool in handling and managing o artifacts I like orus a lot I almost SE my cat orus um the focus on",
    "start": "760040",
    "end": "768160"
  },
  {
    "text": "orus is on generic artifacts so it never assumes that you're dealing with a container image it always assumes you're",
    "start": "768160",
    "end": "773720"
  },
  {
    "text": "dealing with a generic artifact type it has a very rich API and Library support",
    "start": "773720",
    "end": "779079"
  },
  {
    "text": "for building custom registry clients to handle different media types and I'll dive a bit into like some of what those",
    "start": "779079",
    "end": "784199"
  },
  {
    "text": "terminologies mean so when we start working with orus um the first thing we thought about is",
    "start": "784199",
    "end": "791320"
  },
  {
    "text": "we can just you know give the give orus the model way directory and then you know tell it orus push and then it's",
    "start": "791320",
    "end": "796800"
  },
  {
    "text": "going to push things into the OC artifact registry what that ends up doing is it's going to create a single",
    "start": "796800",
    "end": "802160"
  },
  {
    "text": "Atomic layer as you know like one entity and then is going to create a t ball and",
    "start": "802160",
    "end": "808279"
  },
  {
    "text": "Mark that for extraction on download and this is what a manifest",
    "start": "808279",
    "end": "813680"
  },
  {
    "text": "looks like built with that approach so as you can see this looks like a typical container image manifest but the media type is different and you can Define any",
    "start": "813680",
    "end": "820800"
  },
  {
    "text": "media type you like there um you can think of a media type as similar to when you know when you do an HTP request and",
    "start": "820800",
    "end": "826120"
  },
  {
    "text": "you specify the content type to be application Json the client itself knows how to handle this particular media type",
    "start": "826120",
    "end": "832839"
  },
  {
    "text": "uh when you create an artifact with auras um passing it a folder you'll see that it adds two annotations there so",
    "start": "832839",
    "end": "839639"
  },
  {
    "text": "one of them is the D um unpack true and that marks the file the folder for",
    "start": "839639",
    "end": "845320"
  },
  {
    "text": "extraction after download and the second one is effectively just a directory name where you know this particular blob will",
    "start": "845320",
    "end": "851720"
  },
  {
    "text": "be extracted the issue with that is it's because it's a single layer you aren't",
    "start": "851720",
    "end": "857279"
  },
  {
    "text": "really gaining any paralyzation efforts like the entire blob is going to be serialized which was way much slower",
    "start": "857279",
    "end": "863079"
  },
  {
    "text": "than um using object storage so we needed to to do better so the second",
    "start": "863079",
    "end": "868800"
  },
  {
    "text": "thing we did was what if we bin pack our entire model waste into separate smaller folders and then pass it to orus um to",
    "start": "868800",
    "end": "876320"
  },
  {
    "text": "upload this so first of all we tried to you know separate the folders into the",
    "start": "876320",
    "end": "882519"
  },
  {
    "text": "entire model waste into smaller different folders so we create subfolders that are like you know suffix with a certain number and then we for",
    "start": "882519",
    "end": "889560"
  },
  {
    "text": "example if you have a model weight of size 100 gigs you end up creating 20 folders of you know size 5 gigs the",
    "start": "889560",
    "end": "895399"
  },
  {
    "text": "issue with that though is on extraction your entire directory tree or entire folder structure is different so you",
    "start": "895399",
    "end": "902120"
  },
  {
    "text": "need a way to essentially rebuild the structure post download and that's where orus rich Library comes into play so",
    "start": "902120",
    "end": "910600"
  },
  {
    "start": "909000",
    "end": "994000"
  },
  {
    "text": "orus has this post copy call back that you can hook into post descriptor download So after a certain blob is",
    "start": "910600",
    "end": "917040"
  },
  {
    "text": "downloaded you can do a particular action there so it seems like the process easy right we just hook into",
    "start": "917040",
    "end": "922279"
  },
  {
    "text": "this post copy call back and then say okay everything you downloaded here just shift it up One Directory and that would",
    "start": "922279",
    "end": "927440"
  },
  {
    "text": "restore the model weight um structure as it was before but we still need a way to tell oras that you know you need to",
    "start": "927440",
    "end": "934319"
  },
  {
    "text": "actually do this process so remember the annotations I showed you before it turns out you can Define any custom annotation",
    "start": "934319",
    "end": "941000"
  },
  {
    "text": "you'd like on this um artifact layer and then you can Define any other operations you'd like based on that so this is what",
    "start": "941000",
    "end": "948720"
  },
  {
    "text": "our manifests look like with that approach so we still have the same media type and then you notice at the bottom",
    "start": "948720",
    "end": "954199"
  },
  {
    "text": "there's this annotation that says move up and this annotation is very specific to our implementation of the ORS client",
    "start": "954199",
    "end": "960240"
  },
  {
    "text": "that says okay once I see this annotation I need to shift everything up One Directory and then the rest of the",
    "start": "960240",
    "end": "965639"
  },
  {
    "text": "Manifest is just those layers duplicate where every layer is just one portion of the model",
    "start": "965639",
    "end": "971639"
  },
  {
    "text": "weights oops so that's a typical interaction with the ORS API so you first create a bunch of file descriptors",
    "start": "972199",
    "end": "979079"
  },
  {
    "text": "and then you add annotations to those and then you pass it to ORS and say okay pack these layers with these particular",
    "start": "979079",
    "end": "984560"
  },
  {
    "text": "annotations and then the download process is simple on download we just check if this annotation exists and then",
    "start": "984560",
    "end": "990079"
  },
  {
    "text": "we move the folders up the directory tree to rebuild the the structure so some of you might be asking",
    "start": "990079",
    "end": "997120"
  },
  {
    "start": "994000",
    "end": "1177000"
  },
  {
    "text": "we've had biner distribution formats and blob storage apis for years and they're just out there and supported why do we",
    "start": "997120",
    "end": "1002399"
  },
  {
    "text": "need another you know support for a container API well it turns out there's a lot of benefits you gain from using oi",
    "start": "1002399",
    "end": "1008240"
  },
  {
    "text": "and orus uh for us the main benefit is we were able to ship a solution really quickly in you know less less than two",
    "start": "1008240",
    "end": "1014000"
  },
  {
    "text": "months actually um because it turns out that all you really need to stand up kubernetes environment is a container",
    "start": "1014000",
    "end": "1020759"
  },
  {
    "text": "registry that's really the only real dependency you have out there more or less um it also unlocked a bunch of",
    "start": "1020759",
    "end": "1027720"
  },
  {
    "text": "other benefits for us and most of those benefits relate to that container Registries or Registries in general are",
    "start": "1027720",
    "end": "1034160"
  },
  {
    "text": "content addressable um so the first thing that unlocked for us it provided a way to ensure the authenticity and",
    "start": "1034160",
    "end": "1039520"
  },
  {
    "text": "integrity of the image contents so because every layer gets its own digital",
    "start": "1039520",
    "end": "1045000"
  },
  {
    "text": "fingerprint um any simple mod any modification to this particular layer would result in a different hash so",
    "start": "1045000",
    "end": "1051440"
  },
  {
    "text": "developers are actually able to easily validate if an image has been tampered with or not the second thing is thinking of",
    "start": "1051440",
    "end": "1058600"
  },
  {
    "text": "model waste as you know image layers unlocked a lot of encryption scenarios for us instead of having to encrypt the",
    "start": "1058600",
    "end": "1064960"
  },
  {
    "text": "entire model waste and then you know take the hit on the download time we can just pick a random layer from this",
    "start": "1064960",
    "end": "1070880"
  },
  {
    "text": "manifest and decide to encrypt that and that essentially avoids the problem of you having to encrypt the full",
    "start": "1070880",
    "end": "1076559"
  },
  {
    "text": "thing the third thing is because of the content addressable nature of container rishes and the fact that if a layer has",
    "start": "1076559",
    "end": "1083400"
  },
  {
    "text": "been stored before it's not going to be uploaded again if you build your model waste in a certain way to ensure that",
    "start": "1083400",
    "end": "1090000"
  },
  {
    "text": "you know unique layers are just not duplicated there um you end up having a bunch of like storage cost reductions uh",
    "start": "1090000",
    "end": "1097520"
  },
  {
    "text": "so if you're smart about it because if a layer already exists in a registry similar to like if a container layer already exist in registry it's not going",
    "start": "1097520",
    "end": "1102919"
  },
  {
    "text": "to be pulled again and that's done through like digital fingerprinting and the sh stuff that is um known in like",
    "start": "1102919",
    "end": "1108960"
  },
  {
    "text": "container Registries um orus itself has built-in retries for failed layer downloads so if",
    "start": "1108960",
    "end": "1115200"
  },
  {
    "text": "a particular layer fails to download it's not going to undo all the work has done before it's only going to be",
    "start": "1115200",
    "end": "1120240"
  },
  {
    "text": "retrying this particular layer and all of that comes for free with just using orus you don't really need to have any custom retry logic or anything built in",
    "start": "1120240",
    "end": "1128520"
  },
  {
    "text": "there another thing is because you can pre-inspect the the the Manifest of an",
    "start": "1128520",
    "end": "1134440"
  },
  {
    "text": "artifact you're able to do a bunch of smart things so for us for example uh we we use Triton INF servers heavily and",
    "start": "1134440",
    "end": "1140440"
  },
  {
    "text": "the Triton config you know is a file that tells Triton server like how to perform certain things we try and store",
    "start": "1140440",
    "end": "1145840"
  },
  {
    "text": "the Triton config as a separate layer on this um artifact so we're actually able to like pull that before and do some",
    "start": "1145840",
    "end": "1151840"
  },
  {
    "text": "changes to support running on different host different batch configs and so on it also unlocked an easier path to",
    "start": "1151840",
    "end": "1158480"
  },
  {
    "text": "air gap for us because I mentioned before a registry is really the only dependency you needs airgap customers",
    "start": "1158480",
    "end": "1164360"
  },
  {
    "text": "can just replicate both our container images and model ways directly um into the Registries and that's all they need",
    "start": "1164360",
    "end": "1169679"
  },
  {
    "text": "to um spin up our application as well as obviously like a Helm chart as well which again Helm charts can also be stored in oci",
    "start": "1169679",
    "end": "1177080"
  },
  {
    "start": "1177000",
    "end": "1235000"
  },
  {
    "text": "artifacts so it turns out like all the problem effectively just simplified into using a vendor neutral um o registry so",
    "start": "1177320",
    "end": "1184400"
  },
  {
    "text": "we got rid of the GCS dependency there and that's all we needed to simplify our artifact delivery solution and I'll hand",
    "start": "1184400",
    "end": "1190720"
  },
  {
    "text": "it back to this is literally me giving him a",
    "start": "1190720",
    "end": "1195840"
  },
  {
    "text": "chance to take a drink of water but I think I think I warned you we uh put llm in the title but really this is our love",
    "start": "1195840",
    "end": "1202400"
  },
  {
    "text": "letter to oci and orus it's a great great set of Standards it was a really fun engineering challenge to to talk",
    "start": "1202400",
    "end": "1207799"
  },
  {
    "text": "through so uh yeah so once we once we dealt with kind of this first two set of challenges that that Mara walked through",
    "start": "1207799",
    "end": "1214679"
  },
  {
    "text": "um you know we dealt with the observability side we dealt with persistence we dealt with artifact storage and so now we had to get to the actual uh kind of core set of challenges",
    "start": "1214679",
    "end": "1222640"
  },
  {
    "text": "that are specific to um models in general llms uh specifically but",
    "start": "1222640",
    "end": "1228880"
  },
  {
    "text": "dealing with gpus so what we found is there's a lot of challenges uh when",
    "start": "1228880",
    "end": "1235280"
  },
  {
    "start": "1235000",
    "end": "1326000"
  },
  {
    "text": "you're talking about going across clouds across providers uh for gpus because we we don't just talk about like the four",
    "start": "1235280",
    "end": "1241400"
  },
  {
    "text": "main clouds we're also talking about uh on Prem scenarios we're talking about uh specialized providers that that you know",
    "start": "1241400",
    "end": "1248159"
  },
  {
    "text": "are kind of hyperscaling and building out GPU specific uh data centers that kind of thing so uh we found was there",
    "start": "1248159",
    "end": "1255600"
  },
  {
    "text": "was two main areas of challenge one is just provisioning the gpus in a way that",
    "start": "1255600",
    "end": "1261159"
  },
  {
    "text": "you don't have to like build a uh you know a giant tree decision tree that",
    "start": "1261159",
    "end": "1266360"
  },
  {
    "text": "says am I running in this GP this provider then you know use these configurations yada yada y so that",
    "start": "1266360",
    "end": "1272360"
  },
  {
    "text": "provisioning is is uh not insignificant challenge that we'll talk through um and then the other is just dealing with kind",
    "start": "1272360",
    "end": "1278919"
  },
  {
    "text": "of small scale versus large scale so as a SAS provider obviously we're dealing with gpus at at Large Scale both between",
    "start": "1278919",
    "end": "1286320"
  },
  {
    "text": "our kind of internal infrastructure team with dealing with uh like superclusters but then our external infrastructure",
    "start": "1286320",
    "end": "1292039"
  },
  {
    "text": "team that has the SAS system and we're you know we're dealing with a lot more gpus and just like a few um in our",
    "start": "1292039",
    "end": "1298240"
  },
  {
    "text": "private deployment scenarios we have customers who need you know need to to spin up a lot of replicas and so they",
    "start": "1298240",
    "end": "1304520"
  },
  {
    "text": "kind of start to approach some of the challenges that we hit dealing at Large Scale but then many uh many of our",
    "start": "1304520",
    "end": "1310320"
  },
  {
    "text": "clients just you know they're talking intens of gpus they're not they're not dealing with a a lot of gpus and so the",
    "start": "1310320",
    "end": "1316120"
  },
  {
    "text": "the challenges you hit are kind of different different um and so so learning some of those last ones was was",
    "start": "1316120",
    "end": "1321159"
  },
  {
    "text": "interesting so Maron hand it back hello again um yeah so first lesson",
    "start": "1321159",
    "end": "1328679"
  },
  {
    "start": "1326000",
    "end": "1391000"
  },
  {
    "text": "that relates to gpus is that provisioning gpus reliably is tricky so",
    "start": "1328679",
    "end": "1333760"
  },
  {
    "text": "first you need some host level components such as the NVIDIA drivers and then you need some kubernetes",
    "start": "1333760",
    "end": "1339039"
  },
  {
    "text": "specific components like the device plugin and maybe DC Jam exporter if you care at all about GPU metrics um the",
    "start": "1339039",
    "end": "1345240"
  },
  {
    "text": "issue with that is there is an implicit ordering dependenc between both if you try and run the inidia device plugin",
    "start": "1345240",
    "end": "1350919"
  },
  {
    "text": "before your drives are ready you're it's likely going to crash and then if you have a l configured that's going to",
    "start": "1350919",
    "end": "1355960"
  },
  {
    "text": "cause a lot of alerting noise for you and then what we found that is different providers do this process differently so",
    "start": "1355960",
    "end": "1362080"
  },
  {
    "text": "for the longest time well they're supporting it now but for the longest time gcp let you manage the driver them",
    "start": "1362080",
    "end": "1368159"
  },
  {
    "text": "yourself and then they manage the device plugin for you and AKs adopt a different approach where they have a pre-built vhd",
    "start": "1368159",
    "end": "1374440"
  },
  {
    "text": "as an option with all those bits installed or you can also just run your own device plugin as as well there's a solution for that but it's not quite",
    "start": "1374440",
    "end": "1381360"
  },
  {
    "text": "there yet it's the Nvidia GPU operator it's not quite there yet because it's not supported across um certain operating systems specifically the cause",
    "start": "1381360",
    "end": "1388440"
  },
  {
    "text": "um operating system if you're on gcp um the next lesson is inconsistent",
    "start": "1388440",
    "end": "1394559"
  },
  {
    "start": "1391000",
    "end": "1453000"
  },
  {
    "text": "identifiers and with that I particularly mean the device name so the device name returned by the nvml can be different in",
    "start": "1394559",
    "end": "1401520"
  },
  {
    "text": "certain scenarios and this is important to us because uh we use stron for inference and we have different um",
    "start": "1401520",
    "end": "1408480"
  },
  {
    "text": "batching configs we use Dynamic batching feature in Triton to optimize for throughput and we have different batching configs depending on the GPU",
    "start": "1408480",
    "end": "1415200"
  },
  {
    "text": "instance you're running in and the way those are are defined is it's a combination of the model name as well as",
    "start": "1415200",
    "end": "1421000"
  },
  {
    "text": "the GPU type you're running on um well you found that if your cloud",
    "start": "1421000",
    "end": "1426320"
  },
  {
    "text": "provider is using pcie versus um sxmp so PCI Express versus sxmp on the physical",
    "start": "1426320",
    "end": "1431640"
  },
  {
    "text": "interconnect you can get a different device name so for the same a100 adg",
    "start": "1431640",
    "end": "1436919"
  },
  {
    "text": "node you might and I'm getting um the first one if you're running an Azure or the second one if you're running in GCB",
    "start": "1436919",
    "end": "1442600"
  },
  {
    "text": "and at this point our parsing logic failed so the bashing configs didn't really work in different environments so",
    "start": "1442600",
    "end": "1447720"
  },
  {
    "text": "we had to work around that and account for the fact that you know the vi streams return can be different the next one is regarding node",
    "start": "1447720",
    "end": "1455760"
  },
  {
    "start": "1453000",
    "end": "1508000"
  },
  {
    "text": "labels kubernetes node labels so node labels are important because you use node labels to B pack your workflows and",
    "start": "1455760",
    "end": "1461200"
  },
  {
    "text": "try and control the scheduling properties there but it turns out there's really no uniform way of knowing",
    "start": "1461200",
    "end": "1466399"
  },
  {
    "text": "if you're running on an a100 versus D4 for example without hardcoding or without knowing what the you know the VM",
    "start": "1466399",
    "end": "1472480"
  },
  {
    "text": "name or the host name is going to be in advance um some providers have an accelerator label like GC GK does it",
    "start": "1472480",
    "end": "1479799"
  },
  {
    "text": "Azure does it but the value is not super consistent so there really isn't a unform way to you know to Define that",
    "start": "1479799",
    "end": "1485679"
  },
  {
    "text": "today the GPU feature Discovery project tries and solve this problem where it",
    "start": "1485679",
    "end": "1491000"
  },
  {
    "text": "essentially adds a bunch of labels that it retrieved from the host on the Node but you're B to hit the previous problem",
    "start": "1491000",
    "end": "1496480"
  },
  {
    "text": "I mentioned because the device name return can be inconsistent so we had to sort of like ensure that our application",
    "start": "1496480",
    "end": "1501640"
  },
  {
    "text": "is flexible enough with templating that the customer is able to provide their own you know unique set of labels and values as",
    "start": "1501640",
    "end": "1507720"
  },
  {
    "text": "well so note upgrades um everyone loves note upgrades I think um you they",
    "start": "1507720",
    "end": "1513960"
  },
  {
    "start": "1508000",
    "end": "1587000"
  },
  {
    "text": "usually work most of the time um for GP or Clos there's kind of like a unique",
    "start": "1513960",
    "end": "1519039"
  },
  {
    "text": "set of challenges there you can do surge upgrades but striking the right balance between availability and speed is tricky",
    "start": "1519039",
    "end": "1526480"
  },
  {
    "text": "and ideally if you want know down time you do a blue green strategy where you know you create the new note you create",
    "start": "1526480",
    "end": "1531520"
  },
  {
    "text": "a new note pool on the new version and then you move the workloads there let it soak for a bit and then delete the old",
    "start": "1531520",
    "end": "1536720"
  },
  {
    "text": "pool after but gpus are expensive right and you're not guaranteed GPU availability so a blue green upgrade",
    "start": "1536720",
    "end": "1542919"
  },
  {
    "text": "scenario while it may work sometimes most of the time it's it's going to fail well we found out that it's best",
    "start": "1542919",
    "end": "1550120"
  },
  {
    "text": "the best approach to upgrades for GP workflow specifically that have large count is to create a new note pool on",
    "start": "1550120",
    "end": "1555559"
  },
  {
    "text": "the new version and then for the old pool to attain that pool to ensure nothing else is going to get on it and",
    "start": "1555559",
    "end": "1560880"
  },
  {
    "text": "if you're using autoscaler you set the maximum notes to one and that's what St going to do is it ensures that the",
    "start": "1560880",
    "end": "1567159"
  },
  {
    "text": "allcare isn't going to try and scale up this not pool again and then you let HBA do its work so like over time which can",
    "start": "1567159",
    "end": "1573240"
  },
  {
    "text": "be like a week or more as your model as you get like pod turns all the new workflows are going to be scheduled on",
    "start": "1573240",
    "end": "1578480"
  },
  {
    "text": "the new pool and at that point in time you can safely delete the old pool it's not ideal but we found that that's",
    "start": "1578480",
    "end": "1583640"
  },
  {
    "text": "probably like the safest approach um we've come across and speaking of autoscaling so autoscaling llms is",
    "start": "1583640",
    "end": "1590679"
  },
  {
    "start": "1587000",
    "end": "1662000"
  },
  {
    "text": "challenging um I worked on cluster osc in the past there's bunch of hacky assumptions about how dpus work so for",
    "start": "1590679",
    "end": "1597320"
  },
  {
    "text": "example there used to be a hardcoded delay before the autoscaler issues a request for scaling um a particular GPU",
    "start": "1597320",
    "end": "1604200"
  },
  {
    "text": "requesting workload so that delay was hardcoded it was 30 seconds thankfully there's a fixed Upstream now that",
    "start": "1604200",
    "end": "1610039"
  },
  {
    "text": "essentially allows to configure this value but the main motivation back then was to make sure you're not overscaling or like overspending for gpus but in",
    "start": "1610039",
    "end": "1616960"
  },
  {
    "text": "some cases if you have capacity reservations or if you're guaranteed GPU availability then you don't really need",
    "start": "1616960",
    "end": "1622200"
  },
  {
    "text": "this artificial delay added the second thing is back off delays can lead to Long scale up times",
    "start": "1622200",
    "end": "1628840"
  },
  {
    "text": "so when all scaler fails to scale up a note pool it goes into back off mode and the back off is exponential back off",
    "start": "1628840",
    "end": "1634080"
  },
  {
    "text": "it's it starts with 5 minutes it can go up to 30 minutes but it only resets this exponential back off duration after 3",
    "start": "1634080",
    "end": "1639679"
  },
  {
    "text": "hours but during that point in time you may end up getting capacity due to other work Clos down scaling so you kind of",
    "start": "1639679",
    "end": "1646240"
  },
  {
    "text": "want to be able to configure this back off um parameters so we found what's best is to try and like tweak those",
    "start": "1646240",
    "end": "1652480"
  },
  {
    "text": "Autos scale parameters regarding back off as well as obviously generally the a SC parameters to fit your workload and",
    "start": "1652480",
    "end": "1658760"
  },
  {
    "text": "to fit the nature of your workloads and so on the Autos scal Point",
    "start": "1658760",
    "end": "1664120"
  },
  {
    "start": "1662000",
    "end": "1774000"
  },
  {
    "text": "we've had quite a journey to decide what metric we use to horizontally scale our llm",
    "start": "1664120",
    "end": "1669880"
  },
  {
    "text": "workloads so first instinct would be to look at delays like latency like request",
    "start": "1669880",
    "end": "1675399"
  },
  {
    "text": "latency we tried this for a while um the metric is not ideal because with llms there's there's really is no way to",
    "start": "1675399",
    "end": "1682279"
  },
  {
    "text": "determine the output or like how long the LM output is going to take an llm can just keep generating things forever it depends on like the context lens it",
    "start": "1682279",
    "end": "1688640"
  },
  {
    "text": "depends on like what the user specifies with like Max tokens and so on so it's not really a uniform way to scale",
    "start": "1688640",
    "end": "1694519"
  },
  {
    "text": "on um and obviously different mods have different response times the second metric was the GP utilization um and",
    "start": "1694519",
    "end": "1701679"
  },
  {
    "text": "duty cycle well again sorry back to the first point the issue with the first metric as well it doesn't really take GP utilization into account so we're like",
    "start": "1701679",
    "end": "1708240"
  },
  {
    "text": "okay let's try and look at GP utilization the issue is that different models behave differently so embed models are flops bound and GPT",
    "start": "1708240",
    "end": "1715960"
  },
  {
    "text": "generative models are memory bound so looking at the duty cycle a high or low value doesn't really give you um a gener",
    "start": "1715960",
    "end": "1722519"
  },
  {
    "text": "an indication in the generic case so we moved past this and then we looked at",
    "start": "1722519",
    "end": "1727600"
  },
  {
    "text": "the inference um server Q time so Triton exposes a bunch of metric on its q that",
    "start": "1727600",
    "end": "1732640"
  },
  {
    "text": "we Tred to scale on the issue with that is it's a local View and it only tells you um you know a particular instance of",
    "start": "1732640",
    "end": "1739559"
  },
  {
    "text": "an inference server has a long que it doesn't really give you a global view of the system so what you found best for us",
    "start": "1739559",
    "end": "1745880"
  },
  {
    "text": "is to essentially because we had a batching component in play there we could have a more distributed global",
    "start": "1745880",
    "end": "1752120"
  },
  {
    "text": "view of the system looking at the number of batches that are currently running as well as number of batches being queed",
    "start": "1752120",
    "end": "1757679"
  },
  {
    "text": "and using that we built a her istic that we scale on using that metric actually allowed us to improve GP utilization and",
    "start": "1757679",
    "end": "1764200"
  },
  {
    "text": "we were actually able to serve the same um using almost half same traffic using almost half the gpus as well as tolerate",
    "start": "1764200",
    "end": "1770679"
  },
  {
    "text": "spikes and like traffic lat latencies a lot more",
    "start": "1770679",
    "end": "1774960"
  },
  {
    "start": "1774000",
    "end": "1820000"
  },
  {
    "text": "gracefully yeah now we're basically at the end here but like in summary right we've kind of got this this broad system",
    "start": "1777039",
    "end": "1782919"
  },
  {
    "text": "and by we got by the time we got to the end of uh figuring out how to take everything into a private deployment you",
    "start": "1782919",
    "end": "1788519"
  },
  {
    "text": "know we'd come across a few few challenges and uh lessons right so these are just common things right anybody",
    "start": "1788519",
    "end": "1794840"
  },
  {
    "text": "anybody's going to have to deal with those challenges when they're going cross cloud Cloud um obviously we had some unique challenges around uh around",
    "start": "1794840",
    "end": "1802519"
  },
  {
    "text": "how to deliver like large artifacts uh regularly and and that was a really fun fun engineering problem and then just",
    "start": "1802519",
    "end": "1809120"
  },
  {
    "text": "last gpus are not as well tested or uh you know",
    "start": "1809120",
    "end": "1814960"
  },
  {
    "text": "battle one as uh as CPUs but I think that'll that'll continue to get better as we go on so thank you we appreciate",
    "start": "1814960",
    "end": "1821399"
  },
  {
    "start": "1820000",
    "end": "2303000"
  },
  {
    "text": "you coming and uh I think we have five minutes",
    "start": "1821399",
    "end": "1829240"
  },
  {
    "text": "we're happy to take any questions if we've got them yeah yeah feel free yeah nice talk and uh so regarding",
    "start": "1829440",
    "end": "1837840"
  },
  {
    "text": "the GPU the challenge of provision GPU node I was wondering and uh if you folks",
    "start": "1837840",
    "end": "1844360"
  },
  {
    "text": "have look at the most recent introduced like the dynamic resource allocation Epi",
    "start": "1844360",
    "end": "1850039"
  },
  {
    "text": "a generalization of the yeah the position volume the the dynamic resource",
    "start": "1850039",
    "end": "1855760"
  },
  {
    "text": "allocation yeah think some of my engineering team cuz they're laughing over there do you do",
    "start": "1855760",
    "end": "1861279"
  },
  {
    "text": "you want to take",
    "start": "1861279",
    "end": "1863840"
  },
  {
    "text": "that yeah so part of the potential solution because we we've started looking at it but uh there's there's",
    "start": "1867240",
    "end": "1873519"
  },
  {
    "text": "other things kind of higher priority for us to look at okay thank you yes hi uh for the oci solution do you",
    "start": "1873519",
    "end": "1881039"
  },
  {
    "text": "have an open source solution available that I can use with something like uh hugging FES accelerator something that that that is",
    "start": "1881039",
    "end": "1888279"
  },
  {
    "text": "not something that's like open generally it's more an optimization we've made as part of delivering our platform but we",
    "start": "1888279",
    "end": "1893519"
  },
  {
    "text": "kind of wanted to share the the thought process um and Marvin's got something",
    "start": "1893519",
    "end": "1898679"
  },
  {
    "text": "down yeah in general like orus makes it super easy for you to build your own clients on top the reason it's not open",
    "start": "1898679",
    "end": "1904000"
  },
  {
    "text": "source is because it's very like specific on like how we're you know packaging things and there's a dependency how you're exporting and then",
    "start": "1904000",
    "end": "1910519"
  },
  {
    "text": "downloading but the library apis are like the docs are super well like I'm happy to connect after you know any",
    "start": "1910519",
    "end": "1915919"
  },
  {
    "text": "kindness thank you thank you cool first of all thank you for the talk I thought it was very uh",
    "start": "1915919",
    "end": "1921919"
  },
  {
    "text": "very well presented very informative um so obviously I guess you guys are deploying into kind of customer clouds",
    "start": "1921919",
    "end": "1928720"
  },
  {
    "text": "as well as on Prim so for the customer Cloud scenario uh autoscaling performance seems to be pretty important",
    "start": "1928720",
    "end": "1934080"
  },
  {
    "text": "right because you don't want people paying for gpus that are just sitting there downloading weights uh so do you",
    "start": "1934080",
    "end": "1939320"
  },
  {
    "text": "guys have like numbers around that that you'd be able to share or has like using the oci registry to download the weights",
    "start": "1939320",
    "end": "1945559"
  },
  {
    "text": "uh did that improve you know based like versus using something like blob storage yeah I think we got the numbers to be",
    "start": "1945559",
    "end": "1952240"
  },
  {
    "text": "pretty close to our object storage in production um obviously there's the whole thing it depends where our",
    "start": "1952240",
    "end": "1957559"
  },
  {
    "text": "artifact registry is it's in US region but if your customers downloading from like AWS there's probably like extra delays but the numbers were like pretty",
    "start": "1957559",
    "end": "1964720"
  },
  {
    "text": "close to what we have just pulling from object storage um in our SAS platform maybe if you're F like a 2 to 3 minute",
    "start": "1964720",
    "end": "1971919"
  },
  {
    "text": "extra delay um I don't think I have the official numbers off top of my head but the numbers are pretty close and I think",
    "start": "1971919",
    "end": "1977639"
  },
  {
    "text": "the gains made it worth it for us to you know incor that cost gotta and is that like after the node is already",
    "start": "1977639",
    "end": "1983120"
  },
  {
    "text": "provisioned in the cluster is that including node Auto scaling as well yeah so the it depends on which model so for",
    "start": "1983120",
    "end": "1989960"
  },
  {
    "text": "certain models like the generative large ones we have an abstraction like the NFS cach so you only get the penalty on",
    "start": "1989960",
    "end": "1995760"
  },
  {
    "text": "first download um for the smaller embed models you download per you know model",
    "start": "1995760",
    "end": "2001760"
  },
  {
    "text": "service tart um so sorry I your question say it again",
    "start": "2001760",
    "end": "2008919"
  },
  {
    "text": "yeah so I guess I was wondering if that those numbers like you know 3 to four minutes uh were including kind of no",
    "start": "2008919",
    "end": "2015519"
  },
  {
    "text": "speciically model weights okay just model weights yeah and then we download the model weights because it's you know try to knaff them first before loading",
    "start": "2015519",
    "end": "2021919"
  },
  {
    "text": "to memory awesome thank you so much thanks hi uh thanks for the great talk I",
    "start": "2021919",
    "end": "2028600"
  },
  {
    "text": "really like the idea of putting the model weights in oci but it's not very clear to me how you actually download",
    "start": "2028600",
    "end": "2034039"
  },
  {
    "text": "those uh is this something you do you have to do you you have to code in your inference server I mean use a client",
    "start": "2034039",
    "end": "2040919"
  },
  {
    "text": "library to download those so the question being how we how we actually do the download yeah in the downloader yeah",
    "start": "2040919",
    "end": "2046639"
  },
  {
    "text": "so we have an we have an init container um theit container is effectively just an oras client and then you know you",
    "start": "2046639",
    "end": "2053118"
  },
  {
    "text": "pass it inputs outputs and it just calls Aus like you know the orus API pull which does the pull from the you know",
    "start": "2053119",
    "end": "2059839"
  },
  {
    "text": "oci registry so we have an nit container as part of the you know inference server it runs first and then the inference",
    "start": "2059839",
    "end": "2065158"
  },
  {
    "text": "server starts up when the model weights are download locally okay",
    "start": "2065159",
    "end": "2070440"
  },
  {
    "text": "thanks hey good talk I was wondering um when you break the model into layers I was wondering do you actually do like",
    "start": "2070440",
    "end": "2076079"
  },
  {
    "text": "the the layers literally like the model layers each is one layer um of the data",
    "start": "2076079",
    "end": "2081118"
  },
  {
    "text": "or is there better ways of doing that particularly with fine tuning it it depends on the the model itself like",
    "start": "2081119",
    "end": "2086919"
  },
  {
    "text": "obviously some of them are are multiple layer some of them are single layer that kind of thing but yeah with the the larger ones we kind of break it up not",
    "start": "2086919",
    "end": "2092960"
  },
  {
    "text": "like every layer but uh yeah roughly roughly we UTI lers I don't know if you have anything is there any like Research",
    "start": "2092960",
    "end": "2099000"
  },
  {
    "text": "into optimizations um for uh I would say we haven't done done a lot on that again because uh there's there's kind of other",
    "start": "2099000",
    "end": "2106440"
  },
  {
    "text": "larger larger order problems to solve right now but yeah no numbers we can specifically share",
    "start": "2106440",
    "end": "2112880"
  },
  {
    "text": "thanks hi um I'm wondering I'm not sure if this is a part of the challenges you face but do you uh can you comment on um",
    "start": "2112880",
    "end": "2120560"
  },
  {
    "text": "how do you strict the balance between um throughput and latency VI employ batching of the LM inferences uh you",
    "start": "2120560",
    "end": "2127119"
  },
  {
    "text": "mean at runtime or ah um I'm actually we can chat with you offline but like",
    "start": "2127119",
    "end": "2134160"
  },
  {
    "text": "that's yeah that's kind of a little outside of the scope of what what's the challenges are for delivery privately but yeah feel free to come",
    "start": "2134160",
    "end": "2141040"
  },
  {
    "text": "up we've got like and I have a question about the performance and the oci and uh",
    "start": "2141040",
    "end": "2148599"
  },
  {
    "text": "Wars is a great idea and how to how do you compare it between like or and other",
    "start": "2148599",
    "end": "2155640"
  },
  {
    "text": "stores like I think yeah we answered that earlier it was very close performance and even the",
    "start": "2155640",
    "end": "2162599"
  },
  {
    "text": "little hit we got from you know switching from object storage to an oci artifact in those scenarios it was worth",
    "start": "2162599",
    "end": "2168560"
  },
  {
    "text": "it because it's just it's vendor neutral you can run it anywhere so so the main",
    "start": "2168560",
    "end": "2173800"
  },
  {
    "text": "benefit is sorry the main benefit or Advantage is from phone oci is what is",
    "start": "2173800",
    "end": "2180200"
  },
  {
    "text": "the encryption I think being vendor neutral were like big things for us because we don't really need to worry about any specific things in Object",
    "start": "2180200",
    "end": "2186520"
  },
  {
    "text": "Store as well as also enabling air gap the performance is very similar and the",
    "start": "2186520",
    "end": "2191720"
  },
  {
    "text": "little hit we get I think given the benefits we've gained it was worth it I got you thanks",
    "start": "2191720",
    "end": "2198640"
  },
  {
    "text": "yeah hello uh uh my question is specifically about the uh model caching",
    "start": "2198640",
    "end": "2204160"
  },
  {
    "text": "for downloads right I'm curious about the the NFS setup assuming and any",
    "start": "2204160",
    "end": "2209440"
  },
  {
    "text": "consideration SL optimizations you had to work with to make it super fast I'm",
    "start": "2209440",
    "end": "2215599"
  },
  {
    "text": "actually going to take that and like recommend the same thing cuz we've got the rest of our team over here and so we would love to answer that but it's a",
    "start": "2215599",
    "end": "2222200"
  },
  {
    "text": "little bit outside of like the private deployments themselves yeah thanks hey thanks so much for sharing",
    "start": "2222200",
    "end": "2227960"
  },
  {
    "text": "everything was was was really awesome what you guys shared um curious on the GPU layer part of it for the on-prem are",
    "start": "2227960",
    "end": "2235000"
  },
  {
    "text": "you using like like D or HP servers with Nvidia gpus or are you using something like dgx where something like base",
    "start": "2235000",
    "end": "2240800"
  },
  {
    "text": "command might help you with some of that are you speaking about like our SAS system and like what we use for for our",
    "start": "2240800",
    "end": "2246359"
  },
  {
    "text": "speci spefic servers for the stuff on Prem yeah I mean you you showed some of the examples where you were running things in either Amazon or or Google but",
    "start": "2246359",
    "end": "2253560"
  },
  {
    "text": "part of it was going to be to run it on Prem I'm just trying to press around some of the Nvidia some of the Nvidia",
    "start": "2253560",
    "end": "2258760"
  },
  {
    "text": "marketing stuff about on dgx claims that base command is going to solve things like what you guys solved for and I'm",
    "start": "2258760",
    "end": "2265640"
  },
  {
    "text": "just I'm just trying to Ping around what your experience with that was or if you were just using like Del HP servers on Prem and and like the base command",
    "start": "2265640",
    "end": "2272480"
  },
  {
    "text": "wasn't relevant yeah I would say uh a lot of the unpr examples I can can't too much just because prary I get it yeah",
    "start": "2272480",
    "end": "2279839"
  },
  {
    "text": "it's customer specific right and so when we're doing that it's kind of whatever that particular uh particular customer",
    "start": "2279839",
    "end": "2285839"
  },
  {
    "text": "is bringing to us and we're just engaging with them to say okay how do we make sure that the the gpus are like registered appropriately got it with the",
    "start": "2285839",
    "end": "2292880"
  },
  {
    "text": "the cluster that we're running our system on okay so we see a lot okay thank you awesome thank you all",
    "start": "2292880",
    "end": "2298839"
  },
  {
    "text": "appreciate it",
    "start": "2298839",
    "end": "2305839"
  }
]