[
  {
    "text": "hello everyone um my name isong U maybe you can get started because the time's up um so um so we are uh in the research",
    "start": "240",
    "end": "9000"
  },
  {
    "text": "group uh where we studied a lot about the uh sustainability and the llm My isong Le and then leading one of the",
    "start": "9000",
    "end": "16118"
  },
  {
    "text": "research group in IBM research and in I know hybrid you know Cloud",
    "start": "16119",
    "end": "21680"
  },
  {
    "text": "infrastructureone yourself hello everyone I'm very excited to be here I'm chenw from IBM research I'm a Staff",
    "start": "21680",
    "end": "29199"
  },
  {
    "text": "research scientist is working in kubernetes uh Cloud native AI platform",
    "start": "29199",
    "end": "34440"
  },
  {
    "text": "and now in LM as well uh especially in inference and I'm looking forward to",
    "start": "34440",
    "end": "41520"
  },
  {
    "text": "more deeper discussions with all of you uh offline as well hi hello everyone my name is Ban",
    "start": "41520",
    "end": "49719"
  },
  {
    "text": "I'm also a research scientist from IBN research uh I'm from the digital health group we use a large language model for",
    "start": "49719",
    "end": "57480"
  },
  {
    "text": "patient engagement and today I will show a use case how we do that in with the",
    "start": "57480",
    "end": "64119"
  },
  {
    "text": "hell with my colleague uh thank you everyone my name Isam Chen I'm from Red House emerging",
    "start": "64119",
    "end": "71560"
  },
  {
    "text": "Technology Group uh so my day-to-day work is on the sustainability how to make Energy Efficiency available for",
    "start": "71560",
    "end": "78720"
  },
  {
    "text": "cloud native workloads yeah I'm very excited about",
    "start": "78720",
    "end": "85479"
  },
  {
    "text": "you know this opportunity to have our presentation to you so as this this is the tutorial session you know I want you",
    "start": "85479",
    "end": "92159"
  },
  {
    "text": "to have some of the tangible experience and also we try to share all the GitHub",
    "start": "92159",
    "end": "97200"
  },
  {
    "text": "you know everything that we showed today in the GitHub I think you should be able to reproduce it at home as far as you",
    "start": "97200",
    "end": "103640"
  },
  {
    "text": "have Nvidia gpus at home right so um that is our expectation and um um",
    "start": "103640",
    "end": "111200"
  },
  {
    "text": "so so we will be covering you know a lot of stuff you know starting from cncf uh",
    "start": "111200",
    "end": "116240"
  },
  {
    "text": "the cloud native AI uh working group and also the Environmental ability tag we try to have very um um you know short uh",
    "start": "116240",
    "end": "124439"
  },
  {
    "text": "introduction about that and also some of the cloud native llm overview a large language model and also uh Cloud native",
    "start": "124439",
    "end": "132280"
  },
  {
    "text": "llm in action so how we have to uh code it and how you have to you know use the",
    "start": "132280",
    "end": "137319"
  },
  {
    "text": "llm in the cloud infrastructure and the real world user experience so I really",
    "start": "137319",
    "end": "142440"
  },
  {
    "text": "want to have some like end to end usually experience for you so I think po will you know have one of the really",
    "start": "142440",
    "end": "147519"
  },
  {
    "text": "nice um you know application try to to show it to you and the cloud native sustainability in general and also some",
    "start": "147519",
    "end": "154239"
  },
  {
    "text": "of the eement takeaways so we truly believe that sustainability is the key right so I",
    "start": "154239",
    "end": "161680"
  },
  {
    "text": "think you know this belief is really important because you know all the all the things are happening even you know I heard that you know today's weather is",
    "start": "161680",
    "end": "168680"
  },
  {
    "text": "really abnormal in Paris I think I heard that it's a 73 fight which is really",
    "start": "168680",
    "end": "175120"
  },
  {
    "text": "high temperature today so I think you know over the time we're seeing that this abnormal temperature happening more",
    "start": "175120",
    "end": "181000"
  },
  {
    "text": "and more so I think global warming is right now happening and also all the flooding on the fire you know all the",
    "start": "181000",
    "end": "186720"
  },
  {
    "text": "things really want to stop it or at least mitigate it over time and for our",
    "start": "186720",
    "end": "192480"
  },
  {
    "text": "next Generation so there are this is one side of the humanity side and the other side",
    "start": "192480",
    "end": "198159"
  },
  {
    "text": "could be you know the company wise so this is ESG uh environmental responsibility you know going to uh the",
    "start": "198159",
    "end": "205840"
  },
  {
    "text": "Enterprise and uh so you may already know about this like ESG you know requirement and the carbon tax so I",
    "start": "205840",
    "end": "212720"
  },
  {
    "text": "think starting from some of the uh Northern American uh northern European countries they try to have like more",
    "start": "212720",
    "end": "218840"
  },
  {
    "text": "carbon taxes let's say if you are generating more carbon and then you have to pay more tax um you know that kind of",
    "start": "218840",
    "end": "225120"
  },
  {
    "text": "thing is in action so I think one of the example was uh European you know Energy",
    "start": "225120",
    "end": "230799"
  },
  {
    "text": "Efficiency directives so they are kind of um asking for you know transparent",
    "start": "230799",
    "end": "235959"
  },
  {
    "text": "data center Energy Efficiency uh let's say I think previous ly they're blindly",
    "start": "235959",
    "end": "241159"
  },
  {
    "text": "okay Energy Efficiency for you know data center is this much or I think they are uh they are using lots of energy but I",
    "start": "241159",
    "end": "248120"
  },
  {
    "text": "think they really want to have more transparent you know report let's say you know if you're using this kind of services how much carbon you are using",
    "start": "248120",
    "end": "254640"
  },
  {
    "text": "right that requires a lot of measurement methodology verification methodology that you know how we have to verify",
    "start": "254640",
    "end": "261040"
  },
  {
    "text": "those number right um and the other thing was the AI act so I think AI Act",
    "start": "261040",
    "end": "266400"
  },
  {
    "text": "was happening in 2022 or something and uh so so uh when you train the model I",
    "start": "266400",
    "end": "271960"
  },
  {
    "text": "think you have to really you know transparent about the you know how much energy you have to spend uh I think you",
    "start": "271960",
    "end": "277520"
  },
  {
    "text": "have to really report it so that you know that model quality I think you know that has to be reported to the",
    "start": "277520",
    "end": "283720"
  },
  {
    "text": "government and uh so I think everyone may know that you know overall you know",
    "start": "283720",
    "end": "289360"
  },
  {
    "text": "uh interest about the you know llm or the AI so at the right hand side chart actually shows that you know that is",
    "start": "289360",
    "end": "296080"
  },
  {
    "text": "going really you know skyrocketing at some point right soot lot of models comes out and the gp4 even like a lot of",
    "start": "296080",
    "end": "303639"
  },
  {
    "text": "um you know Enterprise know Google Facebook even like IBM we are developing a lot of you know models and then energy",
    "start": "303639",
    "end": "309759"
  },
  {
    "text": "conc for training DOA data centers uh in the skyrocketing so I think maybe yeah",
    "start": "309759",
    "end": "315720"
  },
  {
    "text": "you may heard about this story from you know some of the keynote right so um you",
    "start": "315720",
    "end": "320880"
  },
  {
    "text": "using you know uh thousands of gpus as a same time to train GPT 4 GPT 3.5 you",
    "start": "320880",
    "end": "327560"
  },
  {
    "text": "know kind of stuff is happening right now and this is some of the buang number uh this is based on some of the uh the",
    "start": "327560",
    "end": "333960"
  },
  {
    "text": "report um so they're calculating that you know based on the average you know",
    "start": "333960",
    "end": "339280"
  },
  {
    "text": "household energy uh consumption per year um I think training gp4 model you know",
    "start": "339280",
    "end": "345400"
  },
  {
    "text": "takes up to approximately you know 10.5k uh thousand household per year so",
    "start": "345400",
    "end": "351919"
  },
  {
    "text": "which is a lot of energy for training one model right so so by the way one of the disclaimer was actually that is the",
    "start": "351919",
    "end": "358160"
  },
  {
    "text": "estimated number it's not the you know actual verified number so um so just want to uh mention",
    "start": "358160",
    "end": "366199"
  },
  {
    "text": "that so I think what we have to do um um so one of the work group uh in Cloud",
    "start": "366680",
    "end": "373319"
  },
  {
    "text": "native AI work group I think we are working on you know review promote you know the educate the cloud native AI",
    "start": "373319",
    "end": "378800"
  },
  {
    "text": "ecosystem so one thing and then at the same time we try to reduce energy at the same time um so environmental",
    "start": "378800",
    "end": "385919"
  },
  {
    "text": "sustainability and attack was um you know founded last year I think maybe two",
    "start": "385919",
    "end": "390960"
  },
  {
    "text": "years back um so uh we are kind of also actively involved in this tag uh try to",
    "start": "390960",
    "end": "397720"
  },
  {
    "text": "uh what we can do right so I think all the discussion was happening in there so",
    "start": "397720",
    "end": "403280"
  },
  {
    "text": "that is one of the um you know tag in um you know a cloud native um you know tag",
    "start": "403280",
    "end": "409599"
  },
  {
    "text": "here so we have you know app develop delivery and runtime and security and the sustainability one of the",
    "start": "409599",
    "end": "415960"
  },
  {
    "text": "tag so specifically what's your mission statement right so so our goal is to autate for develop support and help",
    "start": "415960",
    "end": "423800"
  },
  {
    "text": "evaluate environmental sustainability basically we are developing the model speciically and also we are try to um uh",
    "start": "423800",
    "end": "431840"
  },
  {
    "text": "uh the Iden value values and you know possible uh incentives for uh the",
    "start": "431840",
    "end": "436879"
  },
  {
    "text": "sustain uh service and providers to reduce uh the carbon consumption energy consumption and the carbon",
    "start": "436879",
    "end": "442800"
  },
  {
    "text": "footprint so if you want to have more of um you know information I think you can you know take a look at this you know",
    "start": "442800",
    "end": "448680"
  },
  {
    "text": "Environmental sustainability tag in CNF and then you can have a lot of information in there so this is kind of overview I",
    "start": "448680",
    "end": "456199"
  },
  {
    "text": "think since this is a tutorial session we have a luxury to uh have a you know",
    "start": "456199",
    "end": "461280"
  },
  {
    "text": "kind of you know wi is of steps in the large language model um you know process",
    "start": "461280",
    "end": "466440"
  },
  {
    "text": "basically you have to do a lot of pro pre-processing so that involve um you know clearing data clearing and also uh",
    "start": "466440",
    "end": "473759"
  },
  {
    "text": "transformation and also a lot of integration and then also reduction right so I think you can have you know",
    "start": "473759",
    "end": "479840"
  },
  {
    "text": "massage lots of data first and then you know that data goes to training right so",
    "start": "479840",
    "end": "485360"
  },
  {
    "text": "training I think you know a lot of M multiplication all this like forward pass backward pass and then you have to",
    "start": "485360",
    "end": "491280"
  },
  {
    "text": "do a lot of you know calculation in there and then once the model was built then you know you have to do inference",
    "start": "491280",
    "end": "496800"
  },
  {
    "text": "and then also you can do the fine-tuning and then you can use those model let's say you have the base model and then you can do fine-tuning those models and you",
    "start": "496800",
    "end": "503879"
  },
  {
    "text": "can reuse them so this is kind of entire life cycle and um of course you can add",
    "start": "503879",
    "end": "509159"
  },
  {
    "text": "more Cycles on on on top of this right there's some prompt tuning there are a lot of other tuning you can do it on top",
    "start": "509159",
    "end": "515080"
  },
  {
    "text": "of this uh but I think this kind of big chunk up the you know the cycle of the large language model uh but I think you",
    "start": "515080",
    "end": "521599"
  },
  {
    "text": "might wonder um you know how much energy they are consuming right so I think one of the you know recent um you know",
    "start": "521599",
    "end": "527640"
  },
  {
    "text": "Facebook Paper was saying that you know the I think the left bottom of the graph shows that yellow yellow part is about",
    "start": "527640",
    "end": "534880"
  },
  {
    "text": "the pre-processing this kind of gray part is showing the um um um you know",
    "start": "534880",
    "end": "540120"
  },
  {
    "text": "the training and um the black part is uh the inferencing inferencing or the um",
    "start": "540120",
    "end": "546279"
  },
  {
    "text": "you know deployment phase rates once you use that model how much energy was consumed uh because uh the influencing",
    "start": "546279",
    "end": "551959"
  },
  {
    "text": "part is pretty large you may uh assume that maybe training takes a lot of a lot of you know energy but uh depending on",
    "start": "551959",
    "end": "560040"
  },
  {
    "text": "the life cycle of the model I think inferencing if you reuse that model over time then you know energy consumption",
    "start": "560040",
    "end": "566040"
  },
  {
    "text": "for that model is like more for the inference so that you know that graph shows",
    "start": "566040",
    "end": "572640"
  },
  {
    "text": "it so what is ecosystem right so I think a lot of how vendors are working on this",
    "start": "572920",
    "end": "578839"
  },
  {
    "text": "you know um you know Nvidia Intel IBM AMD I think they are having their own",
    "start": "578839",
    "end": "583959"
  },
  {
    "text": "Hardwares and also you know a lot of software SK stack was developed on top of it you know of course you know mvidia",
    "start": "583959",
    "end": "589640"
  },
  {
    "text": "is kind of almost dominating the you know inference or the training a lot but I think a lot of vendors are working on",
    "start": "589640",
    "end": "595640"
  },
  {
    "text": "that and also um you know most of them are you know deployed in Cloud so I",
    "start": "595640",
    "end": "601040"
  },
  {
    "text": "think they are very um um you know close to you know Cloud native and also you",
    "start": "601040",
    "end": "606279"
  },
  {
    "text": "may have heard you know a lot of project related to you know those uh the cloud native project you know uh in in this",
    "start": "606279",
    "end": "612760"
  },
  {
    "text": "conference as well and also what are the ecosystems in",
    "start": "612760",
    "end": "619399"
  },
  {
    "text": "you know energy carbon quantification in Cloud so I think there's a uh green softer Foundation I think there is also",
    "start": "619399",
    "end": "626720"
  },
  {
    "text": "this one of another ecosystem they develop veloping some of the tools uh for example um uh depending on the date",
    "start": "626720",
    "end": "634640"
  },
  {
    "text": "of the uh day uh time of the day uh whenever you have a lot of uh sunlight",
    "start": "634640",
    "end": "640440"
  },
  {
    "text": "uh for example then your your carbon intensity will decrease significantly uh like you know the blue curve in the left",
    "start": "640440",
    "end": "646920"
  },
  {
    "text": "hand side right so then uh if you use uh energy in during that time actually you",
    "start": "646920",
    "end": "652000"
  },
  {
    "text": "can uh save lots of you know carbon footprint because you know your energy is you know given by the sun right um I",
    "start": "652000",
    "end": "658760"
  },
  {
    "text": "think you know there's some SDK uh you can have those information from that kind of skk there's some like repository",
    "start": "658760",
    "end": "665079"
  },
  {
    "text": "working on that and also some uh interesting uh project about you know based on your uh python code uh I think",
    "start": "665079",
    "end": "672040"
  },
  {
    "text": "based on the line of the code you know all the execution path you can actually estimate the you know carbon footprint",
    "start": "672040",
    "end": "677639"
  },
  {
    "text": "on that so there are some you know tools are developed but still we see that there are a lot of opportunities",
    "start": "677639",
    "end": "683639"
  },
  {
    "text": "opportunities are available for you know Cloud native application now for example you know we try to have this Kepler uh",
    "start": "683639",
    "end": "690800"
  },
  {
    "text": "Kepler is one of the project you know that juin will introduce later about the you know how you can measure the power",
    "start": "690800",
    "end": "697279"
  },
  {
    "text": "based on your you know pot level so I think based on you're running some of the pot and then based on some of the",
    "start": "697279",
    "end": "703040"
  },
  {
    "text": "other counter information we try to estimate the power consumption for uh uh running your",
    "start": "703040",
    "end": "710360"
  },
  {
    "text": "application so is it really easy uh of course no so basically you know there",
    "start": "710639",
    "end": "717240"
  },
  {
    "text": "are a lot of things has to be um you know in in the equation so for example",
    "start": "717240",
    "end": "722480"
  },
  {
    "text": "once you only take a look at energy then of course if you don't use the resource then you say save energy but you have to",
    "start": "722480",
    "end": "729000"
  },
  {
    "text": "actually compromise some of the performance but our aim is not actually compromising too much of the performance",
    "start": "729000",
    "end": "735000"
  },
  {
    "text": "so then like what kind of things you have to take a look at it so basically we have take a look at lot of other",
    "start": "735000",
    "end": "741199"
  },
  {
    "text": "metrics for example throughput you know accuracy or the latency or you know even like carbon fate energy of course and",
    "start": "741199",
    "end": "748040"
  },
  {
    "text": "also the cost right right so um you know only saving energy um is not possible",
    "start": "748040",
    "end": "754440"
  },
  {
    "text": "Right sometimes you have to sacrifice some of the performance but I think you know based on the SLA even like your",
    "start": "754440",
    "end": "760440"
  },
  {
    "text": "service requirement or service level objectives you can possibly save energy so I think we are aiming at you know",
    "start": "760440",
    "end": "766760"
  },
  {
    "text": "given the SL is given then like how much energy we can or the com can save it of course you we shouldn't do any um um you",
    "start": "766760",
    "end": "774959"
  },
  {
    "text": "know the green washing so I think uh green washing is the term where uh you you really want to show that",
    "start": "774959",
    "end": "783000"
  },
  {
    "text": "you're saving you know Carbon but actually in fact it's not so I think you know we have to be really you know",
    "start": "783000",
    "end": "789040"
  },
  {
    "text": "transparent about this all the uh you know reporting and stuff um so I think",
    "start": "789040",
    "end": "794079"
  },
  {
    "text": "what kind of Technology possible I try to um uh uh list down some of the things",
    "start": "794079",
    "end": "799440"
  },
  {
    "text": "over here uh I think one of thing is accurate quantification is really important so so when you're using your",
    "start": "799440",
    "end": "807079"
  },
  {
    "text": "resources how much energy you're using in you know you have to be really accurate about those numbers and also",
    "start": "807079",
    "end": "812560"
  },
  {
    "text": "you have to standardize let's say you know if your workload is running in the cloud even you may not want may not see",
    "start": "812560",
    "end": "819240"
  },
  {
    "text": "the power numbers right maybe you may know that what kind of hardare you're using but I think in the cloud level you",
    "start": "819240",
    "end": "825440"
  },
  {
    "text": "hardly see that what kind of hardare you're running so I think uh standardization I think among those like",
    "start": "825440",
    "end": "831720"
  },
  {
    "text": "a cloud infrastructure or what is the you know how they're using what is a you know Power number I think that has to be",
    "start": "831720",
    "end": "837160"
  },
  {
    "text": "agreed on many different C venders that that will be very important and also inefficiencies in you know identified in",
    "start": "837160",
    "end": "843560"
  },
  {
    "text": "optimization so this leads to a lot of techn techniques you know that goes later so I think you have to really",
    "start": "843560",
    "end": "849600"
  },
  {
    "text": "identify the bubbles uh bubbles meaning that when you do the uh the training or",
    "start": "849600",
    "end": "854800"
  },
  {
    "text": "the pipelining so I think backing up all the uh pipelines so that you you don't have any bubbles meaning that you know",
    "start": "854800",
    "end": "860519"
  },
  {
    "text": "some of the ideal cycle on uh your resources will be very important so I think you have to identify those bubbles",
    "start": "860519",
    "end": "866720"
  },
  {
    "text": "and also you can do uh you know a lot of um you know techniques you know for example Dynamic scaling or resour uh",
    "start": "866720",
    "end": "873320"
  },
  {
    "text": "Dynamic resource uh Dynamic scaling of the resources you Dr you may have heard",
    "start": "873320",
    "end": "879199"
  },
  {
    "text": "it from The Kino speeches and also the multiplexing will be very important and also you know power capping of the",
    "start": "879199",
    "end": "884759"
  },
  {
    "text": "frequency scaling to save energy so I give some of the example as",
    "start": "884759",
    "end": "889959"
  },
  {
    "text": "a preliminary result so I think I already mentioned about the power capping or the quantization or the",
    "start": "889959",
    "end": "896720"
  },
  {
    "text": "enabling M uh which is you know multi- instance gpus um you know right hand",
    "start": "896720",
    "end": "902000"
  },
  {
    "text": "side chart you know shows the um you know two information at the same time so I think the bar graph shows the per uh",
    "start": "902000",
    "end": "909320"
  },
  {
    "text": "shows the uh energy consumption uh uh energy consumption you know using have",
    "start": "909320",
    "end": "915399"
  },
  {
    "text": "you know different different par cap and also the line graph which is you know uh",
    "start": "915399",
    "end": "921720"
  },
  {
    "text": "line graph shows the um you know performance in latency so which is you",
    "start": "921720",
    "end": "926800"
  },
  {
    "text": "know the lower the better right so I think we try to find a sweet spot uh even like a power capping meaning you",
    "start": "926800",
    "end": "933560"
  },
  {
    "text": "can actually save energy by doing you know lower power capping let's say your uh GPU can spend 400 watt uh but uh if",
    "start": "933560",
    "end": "941920"
  },
  {
    "text": "you par kep it you 250 then you can possibly save energy but I think you have to really you know uh sacrifice",
    "start": "941920",
    "end": "948600"
  },
  {
    "text": "some of the performance but I think we try to find a sweet spot so that you know even if you're doing some of the",
    "start": "948600",
    "end": "954040"
  },
  {
    "text": "power capping still your performance still you know good as is so that is the you know this paper we published it you",
    "start": "954040",
    "end": "960720"
  },
  {
    "text": "know sometime last week uh I'm sorry last year um and also you know multiplexing gives us lots of um",
    "start": "960720",
    "end": "967360"
  },
  {
    "text": "opportunity so that once you Multiplex the resources then then you can actually save power because you can host many uh",
    "start": "967360",
    "end": "974440"
  },
  {
    "text": "users at the same time all right so I will leave this up",
    "start": "974440",
    "end": "979560"
  },
  {
    "text": "to Chen right now uh she can do more practical and handr experiences this is",
    "start": "979560",
    "end": "984639"
  },
  {
    "text": "kind of overview so let's uh ch",
    "start": "984639",
    "end": "990920"
  },
  {
    "text": "okay now so uh now it's the interesting part and uh let's do some hands on um",
    "start": "990920",
    "end": "997399"
  },
  {
    "text": "going through the basics of LM and you all know large language models has been",
    "start": "997399",
    "end": "1003519"
  },
  {
    "text": "attracting a lot of attentions and we will have um kind of revolutionize all",
    "start": "1003519",
    "end": "1009279"
  },
  {
    "text": "our use cases business use cases and applications while this large language",
    "start": "1009279",
    "end": "1014639"
  },
  {
    "text": "model so behind the scene every application needs to call an API for the",
    "start": "1014639",
    "end": "1020360"
  },
  {
    "text": "inference that's why we are discussing about discussing about the large language model serving here however",
    "start": "1020360",
    "end": "1027240"
  },
  {
    "text": "serving large language model is very expensive it's cost wise expensive and",
    "start": "1027240",
    "end": "1032640"
  },
  {
    "text": "it's also energy-wise expensive so they run in high-end accelerators gpus like",
    "start": "1032640",
    "end": "1039720"
  },
  {
    "text": "a100 and then if you consider the sequential nature of how large language",
    "start": "1039720",
    "end": "1044959"
  },
  {
    "text": "model is generating tokens it's all to like all the architectures are all too",
    "start": "1044959",
    "end": "1050320"
  },
  {
    "text": "regressive meaning you input some tokens and based on all your input you are generating token one by one and then",
    "start": "1050320",
    "end": "1058280"
  },
  {
    "text": "this sequential nature makes it very um the generation time long time and then",
    "start": "1058280",
    "end": "1064520"
  },
  {
    "text": "if you consider one a 100 then you can process like less than one request per",
    "start": "1064520",
    "end": "1071000"
  },
  {
    "text": "second and then in your production use cases you may have like hundreds of",
    "start": "1071000",
    "end": "1076120"
  },
  {
    "text": "thousands of applications uh needing to quir this large language model and then",
    "start": "1076120",
    "end": "1081280"
  },
  {
    "text": "you can imagine how many gpus you want to spend on this and then how much cost",
    "start": "1081280",
    "end": "1087360"
  },
  {
    "text": "it is so um I I want to to to briefly",
    "start": "1087360",
    "end": "1093159"
  },
  {
    "text": "introduce the vrm which is an open source framework um for production scale",
    "start": "1093159",
    "end": "1099320"
  },
  {
    "text": "like uh large language model serving because of the two techniques uh they um",
    "start": "1099320",
    "end": "1105320"
  },
  {
    "text": "actually introduce to optimize the cost to make it faster uh for production use",
    "start": "1105320",
    "end": "1111280"
  },
  {
    "text": "case the first one is uh continuous batching so it it first came comes from",
    "start": "1111280",
    "end": "1117320"
  },
  {
    "text": "the static batching meaning you can pre-allocate memory uh to batch more of",
    "start": "1117320",
    "end": "1122480"
  },
  {
    "text": "your request input request tokens so uh you make the the all the processing",
    "start": "1122480",
    "end": "1129520"
  },
  {
    "text": "those requests in parallel to utilize the GPU Computing better and then later",
    "start": "1129520",
    "end": "1136320"
  },
  {
    "text": "they derived from the static batching to Dynamic batching meaning the pre-allocation of your memory is only to",
    "start": "1136320",
    "end": "1143720"
  },
  {
    "text": "the dynamic request you will have and it batch allocate the memory up to the maximum token you will generate through",
    "start": "1143720",
    "end": "1151480"
  },
  {
    "text": "all the request you have and then they also find if you have a very long request and then all other request you B",
    "start": "1151480",
    "end": "1158400"
  },
  {
    "text": "is small uh shorter then you waste a larger memory in the long request chunk",
    "start": "1158400",
    "end": "1163960"
  },
  {
    "text": "right R and then what's next is they propose The",
    "start": "1163960",
    "end": "1169080"
  },
  {
    "text": "Continuous batching meaning they can concatenate uh the newer request to the",
    "start": "1169080",
    "end": "1174640"
  },
  {
    "text": "empty Slots of the memory so you f a memory app and fully utilize the memory",
    "start": "1174640",
    "end": "1179960"
  },
  {
    "text": "as well as the computer this is the first technique they introduced and then",
    "start": "1179960",
    "end": "1185440"
  },
  {
    "text": "so what about those each block of the token they are caching it's called KV",
    "start": "1185440",
    "end": "1191159"
  },
  {
    "text": "cach so KV cach is actually the intermediate results for when for all",
    "start": "1191159",
    "end": "1196960"
  },
  {
    "text": "the tokens you compute after each layers and then in order to compute the next",
    "start": "1196960",
    "end": "1202760"
  },
  {
    "text": "token you need all of those intermediate results cached in your GPU memory so LM",
    "start": "1202760",
    "end": "1208760"
  },
  {
    "text": "LM inference is not only computer intensive but also very memory intensive",
    "start": "1208760",
    "end": "1214960"
  },
  {
    "text": "uh that's why we need those uh high-end accelerators with a large uh High Bend",
    "start": "1214960",
    "end": "1220200"
  },
  {
    "text": "withd memory so um if you pre-allocate your",
    "start": "1220200",
    "end": "1226280"
  },
  {
    "text": "memory statically uh in your G GPU memory you need to kind of estimate",
    "start": "1226280",
    "end": "1232320"
  },
  {
    "text": "what's the maximum lens of your uh request will generate and then",
    "start": "1232320",
    "end": "1237600"
  },
  {
    "text": "preallocate memory for that and then in this way you would end up with a lot of",
    "start": "1237600",
    "end": "1243000"
  },
  {
    "text": "uh fragmented memory is not used because your prediction is never uh the best",
    "start": "1243000",
    "end": "1249240"
  },
  {
    "text": "right so practically if you will generate uh 20 tokens but the maximum",
    "start": "1249240",
    "end": "1255159"
  },
  {
    "text": "possible tokens you will generate might be uh s 24 for so you can think of how",
    "start": "1255159",
    "end": "1260960"
  },
  {
    "text": "much memory segments you are uh wasting uh so that's why we are introduce",
    "start": "1260960",
    "end": "1266919"
  },
  {
    "text": "another key technique called page attention kernel which is basically they",
    "start": "1266919",
    "end": "1272880"
  },
  {
    "text": "have some logical memory space mapping to the F physical KV cach blocks and",
    "start": "1272880",
    "end": "1279360"
  },
  {
    "text": "then to make sure to reduce the fragmentations between request and also the fragmentations due to the um uh the",
    "start": "1279360",
    "end": "1287720"
  },
  {
    "text": "the the the shter request generation so you can get into more details from this",
    "start": "1287720",
    "end": "1293120"
  },
  {
    "text": "paper and I also got the uh diagram from the paper as well so let's assume we are building a",
    "start": "1293120",
    "end": "1301279"
  },
  {
    "text": "backend production skill back end um serving engine and then",
    "start": "1301279",
    "end": "1307480"
  },
  {
    "text": "um in our research cluster one of our need is not only to build the cluster",
    "start": "1307480",
    "end": "1313960"
  },
  {
    "text": "for one model so all our researchers want to experience a b set of model for",
    "start": "1313960",
    "end": "1320080"
  },
  {
    "text": "example here we want to serve 50 uh L different large language models for like",
    "start": "1320080",
    "end": "1326120"
  },
  {
    "text": "all our researchers and then we found out like some models at certain period",
    "start": "1326120",
    "end": "1332799"
  },
  {
    "text": "some models are very popular and some models um um are less popular over time",
    "start": "1332799",
    "end": "1339480"
  },
  {
    "text": "because for example last year everybody wants to try Lama this year everybody wants to try mro and how we deal with",
    "start": "1339480",
    "end": "1346880"
  },
  {
    "text": "the shift of the lad uh at certain period so um if we allocate one at least",
    "start": "1346880",
    "end": "1354880"
  },
  {
    "text": "one GPU per model and then allocate more gpus for those popular models what we",
    "start": "1354880",
    "end": "1360520"
  },
  {
    "text": "will end up with like if we have 92 gpus and then at certain point 74 out of 92",
    "start": "1360520",
    "end": "1368679"
  },
  {
    "text": "mod uh gpus are idling at certain certain particular time and then that's",
    "start": "1368679",
    "end": "1374240"
  },
  {
    "text": "a huge waste of resources and also huge waste of energy so what what we can do about",
    "start": "1374240",
    "end": "1382240"
  },
  {
    "text": "it so the first thing to reduce the energy cost not the money cost of course",
    "start": "1382240",
    "end": "1387679"
  },
  {
    "text": "is to uh when those gpus are idling and models are not popular we can use Envia",
    "start": "1387679",
    "end": "1395400"
  },
  {
    "text": "uh the uh system management interface to they provide the GPU Cloud for tuning",
    "start": "1395400",
    "end": "1402480"
  },
  {
    "text": "tool for you and then um you you can use the same command line in your GPU server",
    "start": "1402480",
    "end": "1408840"
  },
  {
    "text": "to check how what what are the available frequency you can tune the GPU to and",
    "start": "1408840",
    "end": "1414080"
  },
  {
    "text": "then um also you can easily just use the Nvidia SMI to change the frequency of",
    "start": "1414080",
    "end": "1421039"
  },
  {
    "text": "your GPU so if we try that during idoling",
    "start": "1421039",
    "end": "1426279"
  },
  {
    "text": "period what what we can get so when the GPU is idling if we tune down the GPU",
    "start": "1426279",
    "end": "1432200"
  },
  {
    "text": "Cloud frequency from like 1,410 MHz to 500 40 megahertz it can",
    "start": "1432200",
    "end": "1440679"
  },
  {
    "text": "reduce the GPU temperature from 39 Celsius to 35 Celsius and then totally",
    "start": "1440679",
    "end": "1447120"
  },
  {
    "text": "reduce the power usage from 55 wats to 35 WS that's eight ad per what we is",
    "start": "1447120",
    "end": "1455120"
  },
  {
    "text": "basy so if the GPU is basy basically in this experiment we sent like 16",
    "start": "1455120",
    "end": "1462919"
  },
  {
    "text": "concurrent request to the LM serving engine that is served on this GPU",
    "start": "1462919",
    "end": "1468880"
  },
  {
    "text": "and then if we tune down the frequency again we reduce the temperature from 74",
    "start": "1468880",
    "end": "1474080"
  },
  {
    "text": "to 61 so you can imagine how much cooling cost you can say and also reduce",
    "start": "1474080",
    "end": "1479360"
  },
  {
    "text": "the peak power usage from 300 wats to 150",
    "start": "1479360",
    "end": "1485200"
  },
  {
    "text": "wats and then let's see if we indeed tune down and then unluckily we got a",
    "start": "1485200",
    "end": "1491200"
  },
  {
    "text": "lot of requests coming in uh what will happen to our latencies and suut on the",
    "start": "1491200",
    "end": "1498120"
  },
  {
    "text": "uh server so this diagram basically is buing the",
    "start": "1498120",
    "end": "1504120"
  },
  {
    "text": "concurrent number of users I will say here is we we we VAR the load to the",
    "start": "1504120",
    "end": "1509559"
  },
  {
    "text": "server and we also VAR the GPU clock frequencies from like the top frequency",
    "start": "1509559",
    "end": "1515039"
  },
  {
    "text": "they can afford to the lowest one not the lowest one 504 uh 5040 is kind of",
    "start": "1515039",
    "end": "1520320"
  },
  {
    "text": "like uh double of the lowest um uh frequency you can set and then the",
    "start": "1520320",
    "end": "1525880"
  },
  {
    "text": "medium per output latency uh for the uh the the lowest frequency",
    "start": "1525880",
    "end": "1532880"
  },
  {
    "text": "will be well below 50 millisecond per sec uh 50 millisecond per token so so if",
    "start": "1532880",
    "end": "1541279"
  },
  {
    "text": "you are in this domain you know like 1550 millisecond per token is pretty acceptable when you uh typing the input",
    "start": "1541279",
    "end": "1548640"
  },
  {
    "text": "and at the same time observing the input the reading speed the 50 millisecond per token is completely catching up with",
    "start": "1548640",
    "end": "1555360"
  },
  {
    "text": "your reading speed as well uh so I will say if you have a low load below uh 16",
    "start": "1555360",
    "end": "1562360"
  },
  {
    "text": "concurrent users sending request then even you T down the frequency you not",
    "start": "1562360",
    "end": "1567880"
  },
  {
    "text": "you are not sacrificing the uh service level agreement of the end user experience very",
    "start": "1567880",
    "end": "1575080"
  },
  {
    "text": "much so let's take a look at the 99 percentile uh latency so the same thing",
    "start": "1575080",
    "end": "1582640"
  },
  {
    "text": "we can see even we tun down to 5140 megahertz and then if we have a load of",
    "start": "1582640",
    "end": "1589080"
  },
  {
    "text": "16 concurrent users sending request and then our per token latency is still",
    "start": "1589080",
    "end": "1594720"
  },
  {
    "text": "about like 100 millisecond to 120 um so it's a little bit slower but",
    "start": "1594720",
    "end": "1601559"
  },
  {
    "text": "it's still popping up so the next one is what if we not",
    "start": "1601559",
    "end": "1608080"
  },
  {
    "text": "only want to reduce the energy cost we also want to reduce the money cost or we",
    "start": "1608080",
    "end": "1614320"
  },
  {
    "text": "even want to use those those gpus for something else like for more popular",
    "start": "1614320",
    "end": "1620120"
  },
  {
    "text": "models um then what can we do is the second option we can pack uh more those",
    "start": "1620120",
    "end": "1628240"
  },
  {
    "text": "small lightly used models together we hope we can pack those small lightly used models into fewer number of gpus",
    "start": "1628240",
    "end": "1636440"
  },
  {
    "text": "and then if we look at the size of different models here is an example in",
    "start": "1636440",
    "end": "1641480"
  },
  {
    "text": "the lower right chart you can see uh the memory demount of the number of",
    "start": "1641480",
    "end": "1647000"
  },
  {
    "text": "parameters they have very a lot that they can spread across eight gpus and",
    "start": "1647000",
    "end": "1652279"
  },
  {
    "text": "they can even pack into um one8 of the GPU um so we have a lot of opportunities",
    "start": "1652279",
    "end": "1660360"
  },
  {
    "text": "here so then what technology we want to choose and in um those are the default",
    "start": "1660360",
    "end": "1667600"
  },
  {
    "text": "options provided by by Nvidia GPU you can do time sharing as long as all your",
    "start": "1667600",
    "end": "1674080"
  },
  {
    "text": "models can fit into the memory you can do MPS you can do make which is static",
    "start": "1674080",
    "end": "1680159"
  },
  {
    "text": "petitioning of your high abundance memory as as well as the space",
    "start": "1680159",
    "end": "1685440"
  },
  {
    "text": "multiplexing of all your computer and then here we want to start with trying",
    "start": "1685440",
    "end": "1691039"
  },
  {
    "text": "make uh because all those mem optimizations i t before on the server",
    "start": "1691039",
    "end": "1697240"
  },
  {
    "text": "they really make the memory allocation very unpredictable because if you have",
    "start": "1697240",
    "end": "1702480"
  },
  {
    "text": "more requests they are trying to bash more requests to use the memory more efficiently so if you assuring uh the",
    "start": "1702480",
    "end": "1709880"
  },
  {
    "text": "same memory between servers uh they may easily lead to um memory uh overload of",
    "start": "1709880",
    "end": "1716760"
  },
  {
    "text": "exceptions uh that's why we start with me which is the static partitioning of the high bandb",
    "start": "1716760",
    "end": "1723880"
  },
  {
    "text": "memory so make basically allows the GPU to uh secur potion up to seven separate",
    "start": "1723880",
    "end": "1731360"
  },
  {
    "text": "GPU devices and each of those will have uh separate and isolated PA to the",
    "start": "1731360",
    "end": "1738399"
  },
  {
    "text": "entire memory system and it is supposed to be faster and is supported um",
    "start": "1738399",
    "end": "1743960"
  },
  {
    "text": "biometals containers and the kubernetes and the way to enable it I I show the",
    "start": "1743960",
    "end": "1749679"
  },
  {
    "text": "simple command here and then also this diagram shows all the possible make",
    "start": "1749679",
    "end": "1755039"
  },
  {
    "text": "partitions you would have you would merge two small make partitions to 2G 10gb which which is a little bit bigger",
    "start": "1755039",
    "end": "1762720"
  },
  {
    "text": "and then you can even merge those to 3G uh 20 GB uh as needed",
    "start": "1762720",
    "end": "1769600"
  },
  {
    "text": "so this is the way we configure make petitions on kubernetes uh basically you",
    "start": "1769600",
    "end": "1776279"
  },
  {
    "text": "need to Define your own config map and using the default GPU operator you can predefine like whether you want all",
    "start": "1776279",
    "end": "1783200"
  },
  {
    "text": "small splies on one GPU or you want some balanced configuration with one uh 1G",
    "start": "1783200",
    "end": "1790159"
  },
  {
    "text": "5gb one 2G 10gb and one 3G 20gb and um",
    "start": "1790159",
    "end": "1795919"
  },
  {
    "text": "all you need is to define those for profile and label your noes with cor corresponding profile you",
    "start": "1795919",
    "end": "1803679"
  },
  {
    "text": "want so here is a numerical analysis on our previous research cluster example so",
    "start": "1803679",
    "end": "1811120"
  },
  {
    "text": "what if we pack all those tail models into fewer number of gpus how many gpus",
    "start": "1811120",
    "end": "1818600"
  },
  {
    "text": "we can save so we Analyze That 42 models among our uh all our 50 models and then",
    "start": "1818600",
    "end": "1827080"
  },
  {
    "text": "they used to need 42 gpus as they need one GPU per model and then if they have",
    "start": "1827080",
    "end": "1832679"
  },
  {
    "text": "low load and we use me to pack them together we can totally pack them to 19",
    "start": "1832679",
    "end": "1839200"
  },
  {
    "text": "gpus so in this way we can save 23 gpus cost wise all we can also use those",
    "start": "1839200",
    "end": "1847720"
  },
  {
    "text": "22 uh 23 uh gpus to those popular models to reduce their latencies as",
    "start": "1847720",
    "end": "1855640"
  },
  {
    "text": "well so then the next question is application wise do we really need large",
    "start": "1855679",
    "end": "1862880"
  },
  {
    "text": "very large models for all those large language model applications and then what if the small models can do the same",
    "start": "1862880",
    "end": "1870279"
  },
  {
    "text": "job and later we'll talk more about uh our experience in using small models and",
    "start": "1870279",
    "end": "1877120"
  },
  {
    "text": "then here I just want to highlight some benefits of using small models so they",
    "start": "1877120",
    "end": "1882159"
  },
  {
    "text": "are first efficient second they are very they have very low cost and third you",
    "start": "1882159",
    "end": "1887960"
  },
  {
    "text": "can easily tune the small model to domain specific models and then with",
    "start": "1887960",
    "end": "1895200"
  },
  {
    "text": "much cheaper cost so you don't need a lot of gpus between a small",
    "start": "1895200",
    "end": "1901000"
  },
  {
    "text": "model and then the last option we want to see is what what if you still use",
    "start": "1901000",
    "end": "1906639"
  },
  {
    "text": "want to use a large model but you don't really have the hardware have the very",
    "start": "1906639",
    "end": "1912000"
  },
  {
    "text": "high end Hardware with a large high bandwidth memory to serve those model so",
    "start": "1912000",
    "end": "1917720"
  },
  {
    "text": "the option might be the contest model and then here um we in this tutorial we",
    "start": "1917720",
    "end": "1925200"
  },
  {
    "text": "will D demo the the model with GP TQ quantization and basically it's a",
    "start": "1925200",
    "end": "1931760"
  },
  {
    "text": "layerwise quantization algorithm Trying to minimize the uh objective function",
    "start": "1931760",
    "end": "1938120"
  },
  {
    "text": "here I show so the W is the original coefficient of those uh neural networks",
    "start": "1938120",
    "end": "1943480"
  },
  {
    "text": "and X can be the input to the neural network then the quantization is basically coming up with the w hat which",
    "start": "1943480",
    "end": "1951720"
  },
  {
    "text": "you can minimize the difference between the two in this way you don't lose the accuracy as well as you make the model",
    "start": "1951720",
    "end": "1958320"
  },
  {
    "text": "smaller and the W he can be just uh just like four bit integer while the W is a",
    "start": "1958320",
    "end": "1966399"
  },
  {
    "text": "16 uh 16 flow Point uh data and then the",
    "start": "1966399",
    "end": "1971960"
  },
  {
    "text": "the the bottom left figure is is showing uh the original gptq paper",
    "start": "1971960",
    "end": "1978480"
  },
  {
    "text": "how accurate the quantex model compared to the uh original model so for those",
    "start": "1978480",
    "end": "1983919"
  },
  {
    "text": "different opt family and Bloom family you can see the um the the blue dots",
    "start": "1983919",
    "end": "1991240"
  },
  {
    "text": "line is the original floating Point models and then the four Bas G gptq",
    "start": "1991240",
    "end": "1996679"
  },
  {
    "text": "model is the model after quantization and then then from The Benchmark their",
    "start": "1996679",
    "end": "2003240"
  },
  {
    "text": "accuracy results are really similar and what's interesting to us is we can now",
    "start": "2003240",
    "end": "2010440"
  },
  {
    "text": "use even smaller make potations for example the 2G 20 uh 10gb Mi slides or",
    "start": "2010440",
    "end": "2018600"
  },
  {
    "text": "uh oh or 3G 20gb Mi slides to Fed those Quan model instead of using the whole",
    "start": "2018600",
    "end": "2027760"
  },
  {
    "text": "GPU so when we are using the smaller models and contest models system wise",
    "start": "2029039",
    "end": "2035720"
  },
  {
    "text": "how much performance we are sacrificing in here um so let's make an assumption",
    "start": "2035720",
    "end": "2041080"
  },
  {
    "text": "if the app doesn't really need uh to generate a lot of requests at the same",
    "start": "2041080",
    "end": "2046440"
  },
  {
    "text": "time let's see if the concurrent number of users or queries sending request is",
    "start": "2046440",
    "end": "2051960"
  },
  {
    "text": "less than a then switching to the contest model as we uh stated before you",
    "start": "2051960",
    "end": "2057878"
  },
  {
    "text": "can just use one half or 1/8 of the GPU to serve the model and if the concurrent",
    "start": "2057879",
    "end": "2065919"
  },
  {
    "text": "concurrency for the load is is below a and then you can see the uh 15",
    "start": "2065919",
    "end": "2071158"
  },
  {
    "text": "millisecond uh second per token SLA line and then you can see all those latencies",
    "start": "2071159",
    "end": "2078480"
  },
  {
    "text": "are still within our SLA it's still acceptable of course if you have a lot",
    "start": "2078480",
    "end": "2084280"
  },
  {
    "text": "of request coming in you may want to switch to the a100 uh to serve the serve",
    "start": "2084280",
    "end": "2090398"
  },
  {
    "text": "those models as well I want to highlight so this chart the contest model uh",
    "start": "2090399",
    "end": "2096398"
  },
  {
    "text": "latency goes up very quickly is because we squeeze them into smaller make potions and if you use the large um the",
    "start": "2096399",
    "end": "2104520"
  },
  {
    "text": "same a100 accelerator it will uh be similar to the original model as well so",
    "start": "2104520",
    "end": "2111480"
  },
  {
    "text": "quantization is quantization and smaller models can really uh serve your goal if",
    "start": "2111480",
    "end": "2117359"
  },
  {
    "text": "you don't have the large device and you don't have enough resources so in next we will show a",
    "start": "2117359",
    "end": "2125680"
  },
  {
    "text": "quick demo on how to deploy a application this is our whole setup we",
    "start": "2125680",
    "end": "2131119"
  },
  {
    "text": "have one small quantex model which is llama 2 7B gpdq model uh served on a 2G",
    "start": "2131119",
    "end": "2140320"
  },
  {
    "text": "10gb um mix slize and the one uh larger Quant model Lama 2 13B uh gbq model",
    "start": "2140320",
    "end": "2149119"
  },
  {
    "text": "served on 3G 20gb mix size and then original uh 7B Lama 7D model uh served",
    "start": "2149119",
    "end": "2156920"
  },
  {
    "text": "on the whole a100 and before the servers uh we develop uh our VM router that will",
    "start": "2156920",
    "end": "2165599"
  },
  {
    "text": "automatically router request to the corresponding model we have and be",
    "start": "2165599",
    "end": "2171079"
  },
  {
    "text": "behind the VM vrm router we will have load generator and our application uh",
    "start": "2171079",
    "end": "2177599"
  },
  {
    "text": "both will introduce later uh in observability stack uh all those servers",
    "start": "2177599",
    "end": "2183680"
  },
  {
    "text": "are exporting those matrics to premisus and we use grafana to visualize our",
    "start": "2183680",
    "end": "2190119"
  },
  {
    "text": "results and we use um Nvidia uh dcgm exporter to export those metrics uh for",
    "start": "2190119",
    "end": "2197599"
  },
  {
    "text": "realization Pro purpose as well and last the uh juami will introduce our uh",
    "start": "2197599",
    "end": "2204040"
  },
  {
    "text": "enhancement in Kepler to export all those energy consumptions so for all",
    "start": "2204040",
    "end": "2209599"
  },
  {
    "text": "those open- source project you can scan the barcode to get the tutorial as",
    "start": "2209599",
    "end": "2215960"
  },
  {
    "text": "well so uh now I will have a",
    "start": "2216520",
    "end": "2223400"
  },
  {
    "text": "quick quick demo I'm showing",
    "start": "2223400",
    "end": "2229480"
  },
  {
    "text": "the uh demo",
    "start": "2229480",
    "end": "2233599"
  },
  {
    "text": "video",
    "start": "2246520",
    "end": "2249520"
  },
  {
    "text": "yeah sorry really takes some time to load the",
    "start": "2253440",
    "end": "2258680"
  },
  {
    "text": "video it seems we have a lot of people here so we don't have good internet",
    "start": "2271839",
    "end": "2279440"
  },
  {
    "text": "maybe in the meantime do you have any questions I think you know this is should be interactive a little",
    "start": "2300400",
    "end": "2306440"
  },
  {
    "text": "more what's",
    "start": "2306440",
    "end": "2311920"
  },
  {
    "text": "your mirror could we mirror the the screen",
    "start": "2325240",
    "end": "2330720"
  },
  {
    "text": "could we mirror the screen",
    "start": "2330720",
    "end": "2336200"
  },
  {
    "text": "yes",
    "start": "2336440",
    "end": "2339440"
  },
  {
    "text": "okay you can you have you have my video right yeah",
    "start": "2344440",
    "end": "2350520"
  },
  {
    "text": "[Music] okay okay say the",
    "start": "2355850",
    "end": "2362520"
  },
  {
    "text": "connect I don't know uh just",
    "start": "2364200",
    "end": "2370000"
  },
  {
    "text": "okay okay oh you see no it's not showing",
    "start": "2370000",
    "end": "2376119"
  },
  {
    "text": "up oh move to the screen sorry for the",
    "start": "2377359",
    "end": "2382839"
  },
  {
    "text": "trouble",
    "start": "2382839",
    "end": "2385839"
  },
  {
    "text": "okay should be working okay okay thank you",
    "start": "2390480",
    "end": "2399319"
  },
  {
    "text": "so uh so basically the if you scan the barcode in the previous slides you will",
    "start": "2400200",
    "end": "2405359"
  },
  {
    "text": "go to this tutorial and we will go through the steps of uh deploying those",
    "start": "2405359",
    "end": "2410480"
  },
  {
    "text": "servers and um applications there are two ways one way is we provide a uh",
    "start": "2410480",
    "end": "2418359"
  },
  {
    "text": "whole deployment yo to deploy everything including the router the um all the",
    "start": "2418359",
    "end": "2424800"
  },
  {
    "text": "service monitor necessary to the uh exporting the permissions metric the other way is you use the ham chart we",
    "start": "2424800",
    "end": "2432200"
  },
  {
    "text": "provide and then here I will show an uh quick step on how we uh deploy the whole",
    "start": "2432200",
    "end": "2441079"
  },
  {
    "text": "setup system setup so you can configure the models you have all the necessary parameters and",
    "start": "2441079",
    "end": "2449400"
  },
  {
    "text": "even the um model ID and then uh we also um open source",
    "start": "2449400",
    "end": "2456920"
  },
  {
    "text": "the a router as well in both GitHub and qu and the H",
    "start": "2456920",
    "end": "2464599"
  },
  {
    "text": "really helps you to the one um One Step",
    "start": "2464599",
    "end": "2471000"
  },
  {
    "text": "setup and now you can see there are three server running different models",
    "start": "2471000",
    "end": "2476440"
  },
  {
    "text": "and then we also have our uh application Twilight chatbot deploy and then as well",
    "start": "2476440",
    "end": "2483880"
  },
  {
    "text": "as the we router",
    "start": "2483880",
    "end": "2489240"
  },
  {
    "text": "so next I think I'm I'm going to Dem demonstrate how to quickly run the load",
    "start": "2489240",
    "end": "2495599"
  },
  {
    "text": "generator uh for all the experimental results have show and shared in our",
    "start": "2495599",
    "end": "2501040"
  },
  {
    "text": "slides today and this is an example dashboard for the um realm server we",
    "start": "2501040",
    "end": "2512280"
  },
  {
    "text": "have and it will show you uh performance metric such as the suput the time to",
    "start": "2513960",
    "end": "2521280"
  },
  {
    "text": "First token latencies and the per output token latency and this this is a dcgm",
    "start": "2521280",
    "end": "2528200"
  },
  {
    "text": "dashboard well you will see all your GPU uh frequencies",
    "start": "2528200",
    "end": "2534040"
  },
  {
    "text": "temperatures uh energy [Music]",
    "start": "2534040",
    "end": "2539429"
  },
  {
    "text": "consumptions so make sure you set up your uh huging F secret um before all",
    "start": "2540160",
    "end": "2547520"
  },
  {
    "text": "those steps and also make sure you uh set up the resource claims and um",
    "start": "2547520",
    "end": "2554440"
  },
  {
    "text": "resource volume for your uh vo caching of your",
    "start": "2554440",
    "end": "2559800"
  },
  {
    "text": "models so here we are uh preconfig our hogging pH secr to just fetch the",
    "start": "2563280",
    "end": "2572119"
  },
  {
    "text": "model and this capler demo is um uh tutorial",
    "start": "2576119",
    "end": "2583920"
  },
  {
    "text": "is actually in the sustainable Computing IO or in gab",
    "start": "2583920",
    "end": "2591760"
  },
  {
    "text": "um and here we are just configure the persistent volume and persistent volume",
    "start": "2595200",
    "end": "2601160"
  },
  {
    "text": "claims to catch the results of the benchmarking uh of the low generator",
    "start": "2601160",
    "end": "2609119"
  },
  {
    "text": "and to to start running the lad generator it's very simple we configure",
    "start": "2617880",
    "end": "2623800"
  },
  {
    "text": "EMV file to configure the maximum config concurrency just like the results I show",
    "start": "2623800",
    "end": "2631359"
  },
  {
    "text": "before in the chart and then some other parameters like the the models you want",
    "start": "2631359",
    "end": "2637800"
  },
  {
    "text": "to test and how many um prompts you want",
    "start": "2637800",
    "end": "2642839"
  },
  {
    "text": "to generate for this",
    "start": "2642839",
    "end": "2646200"
  },
  {
    "text": "experiment and this config map is just wrapping up the EnV file you",
    "start": "2651119",
    "end": "2658558"
  },
  {
    "text": "configure and once the config map is ready you can go ahead and create the",
    "start": "2659559",
    "end": "2666079"
  },
  {
    "text": "low test in j job the low testing job will automatically grab those parameters and",
    "start": "2666079",
    "end": "2672680"
  },
  {
    "text": "generate request so I think uh that's all of the",
    "start": "2672680",
    "end": "2681839"
  },
  {
    "text": "demo and next I think uh bo from IBM research will introduce our application",
    "start": "2681839",
    "end": "2688440"
  },
  {
    "text": "and how he developed the application connecting to those uh large language",
    "start": "2688440",
    "end": "2693559"
  },
  {
    "text": "model uh inference servers",
    "start": "2693559",
    "end": "2699040"
  },
  {
    "text": "no no no no no I want",
    "start": "2707839",
    "end": "2712319"
  },
  {
    "text": "that",
    "start": "2725960",
    "end": "2728960"
  },
  {
    "text": "okay okay so uh I will first uh give some uh background context of this use",
    "start": "2740599",
    "end": "2747400"
  },
  {
    "text": "case it's about dementia dementia is one of the biggest impact biggest uh Health",
    "start": "2747400",
    "end": "2754920"
  },
  {
    "text": "crisis impacting our society and economic nowadays uh according to the",
    "start": "2754920",
    "end": "2760400"
  },
  {
    "text": "World Health uh organization there are 15 million people have demential World why and it will be triple by",
    "start": "2760400",
    "end": "2768000"
  },
  {
    "text": "2050 reaching 150 million globally um",
    "start": "2768000",
    "end": "2773960"
  },
  {
    "text": "the economic cost is around 1 trillion around 2018 and it can and we can try to",
    "start": "2773960",
    "end": "2781680"
  },
  {
    "text": "understand that from our more personal connecting",
    "start": "2781680",
    "end": "2787640"
  },
  {
    "text": "way like all of us are middle AG young people right and some of you probably",
    "start": "2787640",
    "end": "2793599"
  },
  {
    "text": "have kids and we all have our parents so uh my grandparents actually one of them",
    "start": "2793599",
    "end": "2801319"
  },
  {
    "text": "have dementia towards the Elder days and uh throughout my grow up we can see like",
    "start": "2801319",
    "end": "2808240"
  },
  {
    "text": "well now my parents are still healthy so I'm lucky but if one day their Heth start to decline I will have to take",
    "start": "2808240",
    "end": "2814640"
  },
  {
    "text": "care of my kids while I take care of my my parents so we become like a candle openning on both ends so this is why uh",
    "start": "2814640",
    "end": "2822760"
  },
  {
    "text": "dementia is one of the things that not just from Humanity point of view that we",
    "start": "2822760",
    "end": "2828160"
  },
  {
    "text": "should love and take care of our family but from the society point of view if",
    "start": "2828160",
    "end": "2833640"
  },
  {
    "text": "this is this Health crisis is not uh being addressed it become an",
    "start": "2833640",
    "end": "2838839"
  },
  {
    "text": "unsustainable situation for the whole society because the productivity is going to go down all of us have to take",
    "start": "2838839",
    "end": "2845079"
  },
  {
    "text": "care of uh all our loved ones and no one is available to do the",
    "start": "2845079",
    "end": "2851240"
  },
  {
    "text": "jobs and prevention is the key to become sustainable in this situation so uh",
    "start": "2851240",
    "end": "2858800"
  },
  {
    "text": "there's a lot of uh um research showing that uh you just need to use your brain",
    "start": "2858800",
    "end": "2865480"
  },
  {
    "text": "more and that's the key to prevent this uh to get uh dimension in the Elder age",
    "start": "2865480",
    "end": "2874000"
  },
  {
    "text": "So reading is one of the things that can make your brain more active as well as",
    "start": "2874000",
    "end": "2880960"
  },
  {
    "text": "other cognitive active tasks like using your computer rather than just watching",
    "start": "2880960",
    "end": "2886040"
  },
  {
    "text": "TV you just need to use your brain more uh that's the key message to take away",
    "start": "2886040",
    "end": "2892440"
  },
  {
    "text": "and in order to do that one of the research project that uh IBN that my",
    "start": "2892440",
    "end": "2898839"
  },
  {
    "text": "group digital health is working with Harvard Medical School is a pilot study",
    "start": "2898839",
    "end": "2904319"
  },
  {
    "text": "to um to get the elderly people in the assistant living facility to engage in",
    "start": "2904319",
    "end": "2911440"
  },
  {
    "text": "reading and hopefully that will help them to maintain their cognitive",
    "start": "2911440",
    "end": "2917000"
  },
  {
    "text": "activity um but for all those people you can imagine uh they are actually about",
    "start": "2917000",
    "end": "2923040"
  },
  {
    "text": "65 years old or even Elder like uh sometimes uh walking around for them is",
    "start": "2923040",
    "end": "2930000"
  },
  {
    "text": "difficult so how do you engage them to like keep reading every day that become",
    "start": "2930000",
    "end": "2935559"
  },
  {
    "text": "itself become a challenge so that's where large language model become helpful here uh we developed this",
    "start": "2935559",
    "end": "2943119"
  },
  {
    "text": "uh chatbot uh the goal of the chat bar is not to test whether the reader",
    "start": "2943119",
    "end": "2950400"
  },
  {
    "text": "understands the book but is to make the reading a more enjoyable experience is to talk about the chapter they just read",
    "start": "2950400",
    "end": "2957839"
  },
  {
    "text": "and then maybe also talk about some other stories uh fun memory moments with",
    "start": "2957839",
    "end": "2965160"
  },
  {
    "text": "their life so that to make May don't feel like reading is not just uh being",
    "start": "2965160",
    "end": "2970359"
  },
  {
    "text": "alone but it's having something more interactive",
    "start": "2970359",
    "end": "2975760"
  },
  {
    "text": "um okay so let me show the next thing is the",
    "start": "2975760",
    "end": "2980799"
  },
  {
    "text": "demo let's see if it works it's working okay so uh I'm",
    "start": "2980799",
    "end": "2988520"
  },
  {
    "text": "showing you side by side two screens one on the left hand side right now is",
    "start": "2988520",
    "end": "2993760"
  },
  {
    "text": "powered by GPT 4 and the one on the right hand side is powered by Lam 13B uh",
    "start": "2993760",
    "end": "3000520"
  },
  {
    "text": "quantise model uh quantise version and then we also have the L",
    "start": "3000520",
    "end": "3006720"
  },
  {
    "text": "17b uh the non-quantized version and quantized version we will show in the next so we will start chatting with the",
    "start": "3006720",
    "end": "3016798"
  },
  {
    "text": "bot so of course a different model will response a little bit differently but uh",
    "start": "3019680",
    "end": "3025839"
  },
  {
    "text": "the main idea the same and we'll try to respond in the similar way to see how",
    "start": "3025839",
    "end": "3030880"
  },
  {
    "text": "the model responds and the idea here is to show",
    "start": "3030880",
    "end": "3036280"
  },
  {
    "text": "you like what Chen mentioned before the bigger model sometimes may not be the best choice for the task",
    "start": "3036280",
    "end": "3043040"
  },
  {
    "text": "and you will see why very",
    "start": "3043040",
    "end": "3047480"
  },
  {
    "text": "soon so we are talking about the Alice in the Wonderland the first chapter uh",
    "start": "3050319",
    "end": "3056319"
  },
  {
    "text": "the assumption is the reader already read that chapter and now we are having a conversation with them about the book",
    "start": "3056319",
    "end": "3063200"
  },
  {
    "text": "and try to make them more engaged and feel it's more fun to read",
    "start": "3063200",
    "end": "3069839"
  },
  {
    "text": "more and you can see the GPT force uh response is actually more",
    "start": "3074040",
    "end": "3080880"
  },
  {
    "text": "sophisticated just having more words",
    "start": "3080880",
    "end": "3088559"
  },
  {
    "text": "sorry maybe I can speed it up a little",
    "start": "3097880",
    "end": "3102558"
  },
  {
    "text": "bit",
    "start": "3115520",
    "end": "3118520"
  },
  {
    "text": "okay so the bigger model tends to response with a more sophisticated",
    "start": "3129280",
    "end": "3134359"
  },
  {
    "text": "answer and now we are going to show you the smaller",
    "start": "3134359",
    "end": "3140160"
  },
  {
    "text": "model and you can see the smaller model tends to give a very short sentence response",
    "start": "3141319",
    "end": "3147119"
  },
  {
    "text": "and in this use case actually that's preferable because uh for the elderly at",
    "start": "3147119",
    "end": "3152240"
  },
  {
    "text": "that point um our medical team tellers keep their reading level around five to",
    "start": "3152240",
    "end": "3159079"
  },
  {
    "text": "like four to five grade graders because at that point because they already start",
    "start": "3159079",
    "end": "3164960"
  },
  {
    "text": "to experience cognitive decline uh if you give them a long question they",
    "start": "3164960",
    "end": "3170319"
  },
  {
    "text": "actually cannot pay attention to it they will forget and then they will feel fatigue and it will make them a bad",
    "start": "3170319",
    "end": "3178119"
  },
  {
    "text": "experience with the tring um with the trap bar so the goal is uh you want to",
    "start": "3178119",
    "end": "3184119"
  },
  {
    "text": "really have a short and sweet like you're talking to kids so actually the",
    "start": "3184119",
    "end": "3189240"
  },
  {
    "text": "smaller model here the behavior is more preferable in this use case than the",
    "start": "3189240",
    "end": "3195040"
  },
  {
    "text": "larger model and that came to back to that's why we chose this for this uh demo",
    "start": "3195040",
    "end": "3201640"
  },
  {
    "text": "purpose is to show you sometimes bigger model may not be it use more energy but",
    "start": "3201640",
    "end": "3208319"
  },
  {
    "text": "it might not be what we are looking for from user experience point of",
    "start": "3208319",
    "end": "3214040"
  },
  {
    "text": "view okay so that's the end of the demo and let's move on to next I will talk",
    "start": "3214040",
    "end": "3222119"
  },
  {
    "text": "about how we build this demo um so this is the this is the",
    "start": "3222119",
    "end": "3229559"
  },
  {
    "text": "architecture of the application uh in the botton as uh CH show before we use a",
    "start": "3229559",
    "end": "3236200"
  },
  {
    "text": "v M as a engine to host our Lambda models and then uh GPT 4 of course is",
    "start": "3236200",
    "end": "3243799"
  },
  {
    "text": "through their API Cloud API and then in the middle we use a lan chain as the pre",
    "start": "3243799",
    "end": "3250280"
  },
  {
    "text": "engineering framework to uh to connect with the RM as well as uh uh a rack in",
    "start": "3250280",
    "end": "3258119"
  },
  {
    "text": "this case is the chapter of the book that the airm is checking and then uh",
    "start": "3258119",
    "end": "3263720"
  },
  {
    "text": "there's also other necessary uh uh memory uh me memory management um is the",
    "start": "3263720",
    "end": "3273160"
  },
  {
    "text": "user database so we give the profile of the user to the LM so to customize the",
    "start": "3273160",
    "end": "3280200"
  },
  {
    "text": "chatting to become more personalized um and then on top of that",
    "start": "3280200",
    "end": "3286119"
  },
  {
    "text": "uh we use the chainlet as the chat engine to host the front end that uh",
    "start": "3286119",
    "end": "3292280"
  },
  {
    "text": "connects to a web UI that you saw just now but in our for rle you will see we",
    "start": "3292280",
    "end": "3298160"
  },
  {
    "text": "also have a voice uh interface which is uh through T integration so the elderly",
    "start": "3298160",
    "end": "3304880"
  },
  {
    "text": "can actually just uh call in and then talk to the tro over the phone or through text",
    "start": "3304880",
    "end": "3310920"
  },
  {
    "text": "messages um it's just uh providing more ways for the user to to have an",
    "start": "3310920",
    "end": "3317119"
  },
  {
    "text": "interaction with the system um and here this is uh how so",
    "start": "3317119",
    "end": "3325839"
  },
  {
    "text": "train l and the lantern they are so vrm is a very popular framework so lann",
    "start": "3325839",
    "end": "3332200"
  },
  {
    "text": "already provide a very nice package the APM uh the way to callil is just a",
    "start": "3332200",
    "end": "3337920"
  },
  {
    "text": "calling one function and then you can put in the URL from the server where you",
    "start": "3337920",
    "end": "3343480"
  },
  {
    "text": "are serving the model and then the model you want to serve and then uh other parameters that you want to use to",
    "start": "3343480",
    "end": "3349760"
  },
  {
    "text": "control the eror and behavior and here is a",
    "start": "3349760",
    "end": "3356839"
  },
  {
    "text": "some simple example to show you how to do the prom engineering they already trap templates uh I will show you the uh",
    "start": "3356839",
    "end": "3366039"
  },
  {
    "text": "source code",
    "start": "3366039",
    "end": "3368720"
  },
  {
    "text": "later uh oh sorry one thing I want to mention so uh for different models uh",
    "start": "3371119",
    "end": "3377160"
  },
  {
    "text": "the template is a little bit this is a cavier between different models um for",
    "start": "3377160",
    "end": "3383799"
  },
  {
    "text": "example for uh chat and uh and uh chbt they are chain",
    "start": "3383799",
    "end": "3392440"
  },
  {
    "text": "to be a chat they are fun with uh chat Behavior so when you prompt with when",
    "start": "3392440",
    "end": "3399240"
  },
  {
    "text": "you generate a prompt mod template model uh they expect to have a chat which",
    "start": "3399240",
    "end": "3405880"
  },
  {
    "text": "means it's a wrong by wrong uh interaction with the model um but with",
    "start": "3405880",
    "end": "3411680"
  },
  {
    "text": "other models uh like mixure those are uh funun for instruction so uh if you want",
    "start": "3411680",
    "end": "3419400"
  },
  {
    "text": "to chat with you have to use some um special um Prime engineering technique",
    "start": "3419400",
    "end": "3426240"
  },
  {
    "text": "you need to tell them here is a history of the chat and I want you to respond",
    "start": "3426240",
    "end": "3431839"
  },
  {
    "text": "what is the next sentence you want to respond so you are trying to MIM me the Trap Behavior wisdom but essentially",
    "start": "3431839",
    "end": "3438720"
  },
  {
    "text": "what the model is doing is still doing uh completion meaning it's just uh",
    "start": "3438720",
    "end": "3444799"
  },
  {
    "text": "extending what's the what's the uh prompt uh what's the input prompt that",
    "start": "3444799",
    "end": "3452680"
  },
  {
    "text": "you gave it and then you try to extend",
    "start": "3452680",
    "end": "3456880"
  },
  {
    "text": "it um and train L train L is actually a starter from here from Paris um so we",
    "start": "3459000",
    "end": "3466559"
  },
  {
    "text": "thankful to the founders here they are doing a great job train L which really makes uh building a chot very easy uh",
    "start": "3466559",
    "end": "3473760"
  },
  {
    "text": "it's built on top of a fast API so so if you're familiar F API you see The Decorator that's idea main uh signature",
    "start": "3473760",
    "end": "3482599"
  },
  {
    "text": "idea from there and then uh you most of time you only need to Define two functions one is to initialize your trod",
    "start": "3482599",
    "end": "3491520"
  },
  {
    "text": "with this uh function and then this is how do you want to handle the wrong by wrong interaction with your chatbot and",
    "start": "3491520",
    "end": "3499839"
  },
  {
    "text": "after this function and then you just run the server and then uh all the UI",
    "start": "3499839",
    "end": "3505160"
  },
  {
    "text": "and everything else is taken care for you so the chap the demo you saw just now uh that we show before actually only",
    "start": "3505160",
    "end": "3512200"
  },
  {
    "text": "took half an hour to build this",
    "start": "3512200",
    "end": "3515799"
  },
  {
    "text": "part and uh here so this is the URL to the repo if you want to check out a",
    "start": "3517440",
    "end": "3523799"
  },
  {
    "text": "source Cod and uh actually we have a v variety of range of uh contributors to",
    "start": "3523799",
    "end": "3530680"
  },
  {
    "text": "us some of them is only from high school they are concerned with their grandparent parents so they're",
    "start": "3530680",
    "end": "3537640"
  },
  {
    "text": "contributing their ideas to the to this project of course also uh researchers from Harvard Medical",
    "start": "3537640",
    "end": "3545599"
  },
  {
    "text": "School and thank you and I will give it to",
    "start": "3545720",
    "end": "3551110"
  },
  {
    "text": "[Applause] me thank you um all right so let's come",
    "start": "3551110",
    "end": "3557599"
  },
  {
    "text": "to the sustainability uh what we do here for U measuring the power consumptions",
    "start": "3557599",
    "end": "3563240"
  },
  {
    "text": "used by large language models so the project cap you may already seen the projects being mentioned multiple times",
    "start": "3563240",
    "end": "3569720"
  },
  {
    "text": "in the cube count here before so there are wonderful maintenance also here in the conference um there's more talks",
    "start": "3569720",
    "end": "3577559"
  },
  {
    "text": "available uh later today and tomorrow so welcome to the uh maintenance of the cap",
    "start": "3577559",
    "end": "3583440"
  },
  {
    "text": "so the project uh is mostly about how can we using certain methodologies to",
    "start": "3583440",
    "end": "3590680"
  },
  {
    "text": "give you the idea of how much energy used by your containers your processes or virtual machines so these are very",
    "start": "3590680",
    "end": "3597319"
  },
  {
    "text": "interesting information you can use to find in your application and also finding your deployment models so that",
    "start": "3597319",
    "end": "3604359"
  },
  {
    "text": "you can achieve your sustainability goals so project Kepler uh is currently a cncf in sandbox uh we are very glad to",
    "start": "3604359",
    "end": "3612640"
  },
  {
    "text": "grow the community strong and um here is a quick introduction of the framework so",
    "start": "3612640",
    "end": "3619359"
  },
  {
    "text": "uh if you look at the different columns on the diagrams uh with par uh particularly on the left side this the",
    "start": "3619359",
    "end": "3626359"
  },
  {
    "text": "trick that we collect information from the operating system uh using ebpf as you know that ebpf has the",
    "start": "3626359",
    "end": "3633240"
  },
  {
    "text": "capabilities of being small uh being versatile is able to collect different",
    "start": "3633240",
    "end": "3638599"
  },
  {
    "text": "levels of information as operating system level and Hardware level uh specifically uh in capler we build a",
    "start": "3638599",
    "end": "3646200"
  },
  {
    "text": "multiple entry points called probes and each of the probe functions will intercept certain OS level if uh contact",
    "start": "3646200",
    "end": "3653960"
  },
  {
    "text": "switches and uh software are FES and uh memory page 30 memory pages so that we",
    "start": "3653960",
    "end": "3660799"
  },
  {
    "text": "can uh collect a whole picture of how the processes and containers uh Works",
    "start": "3660799",
    "end": "3667520"
  },
  {
    "text": "inside of the operating system using that information we move up to the actually move right to the middle column",
    "start": "3667520",
    "end": "3675599"
  },
  {
    "text": "this is the place that we um using the EF collected information to correlate",
    "start": "3675599",
    "end": "3681680"
  },
  {
    "text": "with the user space informations to paint a bigger picture of what's the the process is ownership so the process",
    "start": "3681680",
    "end": "3688760"
  },
  {
    "text": "could be inside of a container or could be inside of the virtual machine and so this is the mapping that we create in",
    "start": "3688760",
    "end": "3696319"
  },
  {
    "text": "this area and then once we got all these informations we associate the energy",
    "start": "3696319",
    "end": "3702319"
  },
  {
    "text": "consumption RIS activities about this uh processes and containers using certain",
    "start": "3702319",
    "end": "3708039"
  },
  {
    "text": "ratio method So currently in biometal environments using the CPU instructions",
    "start": "3708039",
    "end": "3714119"
  },
  {
    "text": "uh to attribute energy let just say um the whole server hypothetically using",
    "start": "3714119",
    "end": "3720720"
  },
  {
    "text": "simple numbers the whole server uses 100 CPU CPU instructions and process a using",
    "start": "3720720",
    "end": "3728119"
  },
  {
    "text": "the 30 instructions at this time and process B use 70 instructions uh at this",
    "start": "3728119",
    "end": "3734279"
  },
  {
    "text": "time so as a ratio the process a will get 30% of the energy consumed in the",
    "start": "3734279",
    "end": "3741079"
  },
  {
    "text": "collection window and process b gets 70% and the the power information we get",
    "start": "3741079",
    "end": "3747760"
  },
  {
    "text": "from different in platforms uh depends on the configurations and Hardware architectures on x86 we get the power",
    "start": "3747760",
    "end": "3755880"
  },
  {
    "text": "information from the rle the runtime average power level on certain arm platforms uh we using the hardware",
    "start": "3755880",
    "end": "3762760"
  },
  {
    "text": "sensors uh to get their CPU level of energy consumption um on Virtual",
    "start": "3762760",
    "end": "3769000"
  },
  {
    "text": "machines where you do not have access to the hardware counters to gas energy consumption we using machine learning",
    "start": "3769000",
    "end": "3775319"
  },
  {
    "text": "models uh to estimate how much energy during this Ty duration based on CPU",
    "start": "3775319",
    "end": "3781279"
  },
  {
    "text": "activities uh so that is one of the areas we activ the modeling on different Hardware architectures aside from CPU we",
    "start": "3781279",
    "end": "3788839"
  },
  {
    "text": "are also able to manage the GPU and server level energy consumptions using",
    "start": "3788839",
    "end": "3794319"
  },
  {
    "text": "different libraries at the server level using the rle and acpi to get the",
    "start": "3794319",
    "end": "3799799"
  },
  {
    "text": "platform level energy consumption and on the GPU level which is coming up next is",
    "start": "3799799",
    "end": "3804960"
  },
  {
    "text": "very PL dependence we're using uh uh currently support Nvidia GPU using the",
    "start": "3804960",
    "end": "3810760"
  },
  {
    "text": "Nvidia management uh Library uh the two libraries over there one is nvml the",
    "start": "3810760",
    "end": "3816319"
  },
  {
    "text": "other one is this data center GPU management which will coming up",
    "start": "3816319",
    "end": "3821599"
  },
  {
    "text": "next so as we are just going through the um discussions before so the level of",
    "start": "3821599",
    "end": "3829200"
  },
  {
    "text": "configuration in GPU actually varies based on the deployant models if you are",
    "start": "3829200",
    "end": "3834720"
  },
  {
    "text": "deployment one model one GPU you can get the power consumption from the GPU and",
    "start": "3834720",
    "end": "3840039"
  },
  {
    "text": "you can attribute the total energy consumption to that model completely so that is a simple case we can use in um",
    "start": "3840039",
    "end": "3847079"
  },
  {
    "text": "you know Nvidia nvml library to get all this information but if you are having",
    "start": "3847079",
    "end": "3852880"
  },
  {
    "text": "this make multi instance GPU that you are sliced up GPU into multiple sliz and",
    "start": "3852880",
    "end": "3859240"
  },
  {
    "text": "each of the slides we are just serving one model then things can be very tricky because Nvidia does not give you slice",
    "start": "3859240",
    "end": "3866480"
  },
  {
    "text": "level energy consumption which have to come to us to do certain level of",
    "start": "3866480",
    "end": "3871920"
  },
  {
    "text": "estimation and modeling to come up with energy consumption uh which is going uh",
    "start": "3871920",
    "end": "3876960"
  },
  {
    "text": "we are using multiple level of information here so if you are this is uh snap",
    "start": "3876960",
    "end": "3882760"
  },
  {
    "text": "snapshot of the mvl or Nvidia SMI output so you see here we have multiple outputs",
    "start": "3882760",
    "end": "3889359"
  },
  {
    "text": "from here on the highlighting in red uh this is a",
    "start": "3889359",
    "end": "3895000"
  },
  {
    "text": "positions this of the identifiers of different mix so we have three mix in",
    "start": "3895000",
    "end": "3900880"
  },
  {
    "text": "this picture uh so they have a GP ID GP instance ID uh insures GI ID and comput",
    "start": "3900880",
    "end": "3908440"
  },
  {
    "text": "instance ID insures it a CI ID and M uh uh Mig number so this is the information",
    "start": "3908440",
    "end": "3917000"
  },
  {
    "text": "uh in programming programmatical level we have to go using the dcgm uh Library",
    "start": "3917000",
    "end": "3923760"
  },
  {
    "text": "theit uh so highlighted in blue this the multiprocessor count so uh we are using",
    "start": "3923760",
    "end": "3929839"
  },
  {
    "text": "a 140 gig uh GPU so the multiprocessors",
    "start": "3929839",
    "end": "3935119"
  },
  {
    "text": "if you slice up the GPU as we said before there three slices uh I think",
    "start": "3935119",
    "end": "3940640"
  },
  {
    "text": "it's a 3G 20 and uh 3 G10 and 2 G10 something like that so the biggest one",
    "start": "3940640",
    "end": "3946559"
  },
  {
    "text": "has a 42 multiprocessors coming up next is 28 and 14 so this information we can",
    "start": "3946559",
    "end": "3952240"
  },
  {
    "text": "get from Nidia Library MML so the vir last one is the processes that are using",
    "start": "3952240",
    "end": "3958559"
  },
  {
    "text": "the GPU so as we see here in highlighting in green so we see the process ID uh the P ID and we also see",
    "start": "3958559",
    "end": "3966480"
  },
  {
    "text": "the uh GPU ID on the left and the GPU instance ID which is true in this case",
    "start": "3966480",
    "end": "3973359"
  },
  {
    "text": "so and then we can make a mapping between the GPU and the GP instance ID which the process ID to correlate which",
    "start": "3973359",
    "end": "3981160"
  },
  {
    "text": "which processes or containers are using the GPU slides and then with that",
    "start": "3981160",
    "end": "3986200"
  },
  {
    "text": "information we can also go into the energy estimates by using the multiprocessor account with respect to",
    "start": "3986200",
    "end": "3992640"
  },
  {
    "text": "the whole GPU in order to get the uh estimates of how much energy using by",
    "start": "3992640",
    "end": "3998119"
  },
  {
    "text": "that slid which are consumed by certain processes or containers so that's the basic",
    "start": "3998119",
    "end": "4004400"
  },
  {
    "text": "idea in short right up uh so we need to get a number of informations from the uh",
    "start": "4004400",
    "end": "4009559"
  },
  {
    "text": "GPU as well as from the operating system to match the which processes are using",
    "start": "4009559",
    "end": "4014880"
  },
  {
    "text": "different information we got some uh containers uh containerization runtime",
    "start": "4014880",
    "end": "4020079"
  },
  {
    "text": "and information from the GPU libraries to get the mappings between the GPU",
    "start": "4020079",
    "end": "4025279"
  },
  {
    "text": "slice and GPU physical GPU ID with the same uh create we create certain models",
    "start": "4025279",
    "end": "4032039"
  },
  {
    "text": "to estimate how much energy consumed by the whole GPU can be attributed to that",
    "start": "4032039",
    "end": "4037520"
  },
  {
    "text": "CPU slice so that's is something we still we are are working on a different formulas so one of the formulas totally",
    "start": "4037520",
    "end": "4044960"
  },
  {
    "text": "based on on the CPU count on the computation unit count so we using the",
    "start": "4044960",
    "end": "4050359"
  },
  {
    "text": "if you are using the Invidia GPU uh dcgm you know there's a event called tensor",
    "start": "4050359",
    "end": "4056000"
  },
  {
    "text": "utilization so the number of percentage of active tensors currently using bi GPU",
    "start": "4056000",
    "end": "4061839"
  },
  {
    "text": "slice so that's the um indicator we are using for attribution so if you are",
    "start": "4061839",
    "end": "4067640"
  },
  {
    "text": "having a process that's using a certain M and as a certain time of sampling we",
    "start": "4067640",
    "end": "4074440"
  },
  {
    "text": "find out the um the tensor utilization let's see what's the number",
    "start": "4074440",
    "end": "4081200"
  },
  {
    "text": "here tensor utilization is uh point to 20% and the uh the GPU ratio is uh 50%",
    "start": "4081200",
    "end": "4091160"
  },
  {
    "text": "meaning the biggest uh slice as we show in the last slide uh the 3020 M and uh",
    "start": "4091160",
    "end": "4097440"
  },
  {
    "text": "we can do a simple math if you are for 25 20 250 watts of power consumption we",
    "start": "4097440",
    "end": "4103560"
  },
  {
    "text": "can divide that by ratio CPU uh the processor ratio 05 50% and then within",
    "start": "4103560",
    "end": "4109560"
  },
  {
    "text": "this 50% only 20 of them are actively using so we divide times another 20 we",
    "start": "4109560",
    "end": "4115278"
  },
  {
    "text": "get the total CPU consumption on that s and thus number of energy can be",
    "start": "4115279",
    "end": "4121520"
  },
  {
    "text": "attributed to the St and eventually go into the",
    "start": "4121520",
    "end": "4126119"
  },
  {
    "text": "containers um and then we got all this information in the capler as well as in",
    "start": "4128159",
    "end": "4134440"
  },
  {
    "text": "vrm and we can use athes to plot the whole diagrams uh on the dashboard so we",
    "start": "4134440",
    "end": "4140758"
  },
  {
    "text": "are using uh so in cap you find a number of",
    "start": "4140759",
    "end": "4146120"
  },
  {
    "text": "uh Matrix available one of which is interested to you is the cap container Matrix and inside of container Matrix we",
    "start": "4146120",
    "end": "4153560"
  },
  {
    "text": "have a GPU total Jewels so this is aggregation of how many energy how much",
    "start": "4153560",
    "end": "4158960"
  },
  {
    "text": "energy has to be consumed by the uh by the part that's I use specifically on",
    "start": "4158960",
    "end": "4164120"
  },
  {
    "text": "GPU so you see here before the workloads get started uh the GPU consumption is",
    "start": "4164120",
    "end": "4169159"
  },
  {
    "text": "almost zero um there's some idle power so once the workloads gets kicked off uh we are using a single um querry on the",
    "start": "4169159",
    "end": "4177679"
  },
  {
    "text": "our language models backand VM backend we see the energy consumption just spiked up and eventually using the",
    "start": "4177679",
    "end": "4185080"
  },
  {
    "text": "amounts of energy is going to be stabilizes so that's one of the ways you can carry how much energy using by a",
    "start": "4185080",
    "end": "4192400"
  },
  {
    "text": "part um so the next thing I think most people interest is uh how much energy",
    "start": "4192400",
    "end": "4198960"
  },
  {
    "text": "you need to generate a single uh token so this is the this is very interesting",
    "start": "4198960",
    "end": "4204360"
  },
  {
    "text": "because at the end of the day is you know for people who are doing the back",
    "start": "4204360",
    "end": "4209440"
  },
  {
    "text": "managements managing this clusters and data centers they care a lot about the energy consumption in the data center as",
    "start": "4209440",
    "end": "4216480"
  },
  {
    "text": "you know um if you're running the GPU once is powered on even on a100 or h100",
    "start": "4216480",
    "end": "4221719"
  },
  {
    "text": "the power consumption is could be one anywhere between 500 to 7 or th000 Watts depends on the number of gpus you have",
    "start": "4221719",
    "end": "4228600"
  },
  {
    "text": "and the latest announced by Nvidia the B1 100 and B upcoming B 200 the energy",
    "start": "4228600",
    "end": "4235400"
  },
  {
    "text": "consumption is even higher so the data centers may not be able to may not be",
    "start": "4235400",
    "end": "4240560"
  },
  {
    "text": "configured in that way so if you are providing the token uh you know token",
    "start": "4240560",
    "end": "4245880"
  },
  {
    "text": "per what level of management it will be very intuitive for people to manage the you know models as well as the",
    "start": "4245880",
    "end": "4252560"
  },
  {
    "text": "infrastructures to match up with the workflows so this is uh Matrix We Believe will provide you such directions",
    "start": "4252560",
    "end": "4259120"
  },
  {
    "text": "so we get the token stles from VM so again this is just onus it has nothing",
    "start": "4259120",
    "end": "4265239"
  },
  {
    "text": "to do with the latency which is more on the usability side of the story so",
    "start": "4265239",
    "end": "4270679"
  },
  {
    "text": "performance wise we get a through p and energy wise we get a cap container M",
    "start": "4270679",
    "end": "4275880"
  },
  {
    "text": "container GPU energy in in terms of what in the was units because we take the",
    "start": "4275880",
    "end": "4281239"
  },
  {
    "text": "rate over the uh gouch so that's will give you the was",
    "start": "4281239",
    "end": "4287239"
  },
  {
    "text": "information and if you because GPU is not only resource that's being used by",
    "start": "4287800",
    "end": "4292960"
  },
  {
    "text": "the container we also have CPUs together so you also want to have the whole pictures of how much energy indeed using",
    "start": "4292960",
    "end": "4300320"
  },
  {
    "text": "by the containers on proten level uh service and so we also aggregate all the",
    "start": "4300320",
    "end": "4306320"
  },
  {
    "text": "resources used by the container in the another Master is called a GP uh Capital container Jews total which will include",
    "start": "4306320",
    "end": "4313719"
  },
  {
    "text": "both GPU CPU and Dam uh in more visible ways we can",
    "start": "4313719",
    "end": "4321360"
  },
  {
    "text": "visualize the whole thing in a graphon dashboard uh so in this um again this is",
    "start": "4321360",
    "end": "4326639"
  },
  {
    "text": "just for information it does not means Which models is more efficient than the other or Which models is more H High",
    "start": "4326639",
    "end": "4333360"
  },
  {
    "text": "performing than the other because I used uh same concurrency so I used 10 um",
    "start": "4333360",
    "end": "4339679"
  },
  {
    "text": "queries for all the backends you know certain backends because of the computation and memory constraints",
    "start": "4339679",
    "end": "4345719"
  },
  {
    "text": "may not support that kind of batch configuration uh so this is just let just say you have hypothetical use case",
    "start": "4345719",
    "end": "4352400"
  },
  {
    "text": "of serving 10 queries per seconds and you are serving from different workflow",
    "start": "4352400",
    "end": "4358040"
  },
  {
    "text": "different large language models from different Hardware configurations so what would be the visually",
    "start": "4358040",
    "end": "4365120"
  },
  {
    "text": "representative energ per seconds kind of a information you can get so on the top",
    "start": "4365120",
    "end": "4370520"
  },
  {
    "text": "panel this is uh uh from the VM so the throughputs based on different models on",
    "start": "4370520",
    "end": "4376840"
  },
  {
    "text": "the top one the yellow you are seeing this is from the uh Nama 27b UNC un",
    "start": "4376840",
    "end": "4384080"
  },
  {
    "text": "quantitized so this from the using the whole GPU uh this Rus could go as high as 600 um uh 600 tokens per second and",
    "start": "4384080",
    "end": "4394679"
  },
  {
    "text": "the second one actually let me I have to read it from",
    "start": "4394679",
    "end": "4403560"
  },
  {
    "text": "here okay the second one is uh 13 uh Lama 27",
    "start": "4404120",
    "end": "4409679"
  },
  {
    "text": "uh 7 13B Quant model you got a 200 uh tokens per second uh the reason that's",
    "start": "4409679",
    "end": "4417000"
  },
  {
    "text": "these bigger models in this case is doing better than smaller models uh in",
    "start": "4417000",
    "end": "4423199"
  },
  {
    "text": "terms of through put is that in my opinion is that uh because the resources uh CPU resources give to the language",
    "start": "4423199",
    "end": "4430199"
  },
  {
    "text": "models are difference the 7B un unced is using the whole GPU the 13B gets twice",
    "start": "4430199",
    "end": "4437040"
  },
  {
    "text": "as much uh GPU resources as the 7B quti version so this RP is also be higher",
    "start": "4437040",
    "end": "4444080"
  },
  {
    "text": "than the 7B even 7B is a smaller model but the because of the resource we are given to it is smaller it's actually",
    "start": "4444080",
    "end": "4450760"
  },
  {
    "text": "half of the seven 13B contest so you get even lower scus on the uh middle panel",
    "start": "4450760",
    "end": "4458159"
  },
  {
    "text": "you see is U that's a breakdown of energy spend on CPU and GPU uh the",
    "start": "4458159",
    "end": "4463600"
  },
  {
    "text": "takeway from the diagram is that the GPU even is a two only two a100 using almost",
    "start": "4463600",
    "end": "4470840"
  },
  {
    "text": "10 times more energy than CPU which is one of the reasons uh GPU optimization",
    "start": "4470840",
    "end": "4476719"
  },
  {
    "text": "make a big bang of The Bu out of the whole picture at the very bottom is the panel at the very bottom panel you see",
    "start": "4476719",
    "end": "4483199"
  },
  {
    "text": "the uh token per wat uh comparison again I'm I'm not saying which model is better",
    "start": "4483199",
    "end": "4488760"
  },
  {
    "text": "than the other in terms of sustainability just for reference if you are configured model to Ser in certain",
    "start": "4488760",
    "end": "4495080"
  },
  {
    "text": "workloads of 10 quar per second which other ones may be more efficient than the other ones so the Llama 7 7B uh",
    "start": "4495080",
    "end": "4504159"
  },
  {
    "text": "model uh using the whole GPU you can get uh three uh you can get three tokens per",
    "start": "4504159",
    "end": "4510719"
  },
  {
    "text": "watt right so which is actually High performing in terms of sustainability uh you get more um bucks per money per",
    "start": "4510719",
    "end": "4517960"
  },
  {
    "text": "dollar more W per for dollar uh versus the other the 13B quanti version I us",
    "start": "4517960",
    "end": "4524800"
  },
  {
    "text": "which using half of this GPU can give you um know one token per second W sorry",
    "start": "4524800",
    "end": "4531239"
  },
  {
    "text": "one to one token per wat again this is always have no uh relationship with the",
    "start": "4531239",
    "end": "4537960"
  },
  {
    "text": "uh latency just the purely B based on the token perspective the smallest model",
    "start": "4537960",
    "end": "4544040"
  },
  {
    "text": "in this case 7B Conti only generates almost half token per wats um so this",
    "start": "4544040",
    "end": "4550800"
  },
  {
    "text": "just give you some visual representation and some of the touch points if you are managing a language large language uh",
    "start": "4550800",
    "end": "4557639"
  },
  {
    "text": "model service influence infrastructure with certain INF certain configurations",
    "start": "4557639",
    "end": "4563159"
  },
  {
    "text": "what are the potential U metric you can consider to save energy while still maintaining your service level of",
    "start": "4563159",
    "end": "4570360"
  },
  {
    "text": "agreements right so um this is the using 10 concurrence queries and we also have",
    "start": "4570360",
    "end": "4576600"
  },
  {
    "text": "a recorded demo using just one query or one stream of queries and",
    "start": "4576600",
    "end": "4583960"
  },
  {
    "text": "the picture actually looks a little bit different the energy was numbers uh is",
    "start": "4583960",
    "end": "4589360"
  },
  {
    "text": "not as uh significant the that may not be as significant as the the one I just show I will just leave this uh YouTube",
    "start": "4589360",
    "end": "4596560"
  },
  {
    "text": "video for you to for you to um you know watch offline after this",
    "start": "4596560",
    "end": "4602719"
  },
  {
    "text": "talk I just skip this",
    "start": "4604320",
    "end": "4608320"
  },
  {
    "text": "one",
    "start": "4613840",
    "end": "4616840"
  },
  {
    "text": "I",
    "start": "4629080",
    "end": "4631239"
  },
  {
    "text": "switch",
    "start": "4643840",
    "end": "4646840"
  },
  {
    "text": "e",
    "start": "4673840",
    "end": "4676840"
  },
  {
    "text": "I think we have a we have a we have a technical issue the browser actually",
    "start": "4684120",
    "end": "4690239"
  },
  {
    "text": "crashed coming back very soon skip it skip we can skip it it's a",
    "start": "4690239",
    "end": "4697199"
  },
  {
    "text": "YouTube video so we can actually let to watch",
    "start": "4697199",
    "end": "4702120"
  },
  {
    "text": "offline sorry for the",
    "start": "4703199",
    "end": "4706760"
  },
  {
    "text": "going to switch",
    "start": "4712840",
    "end": "4715280"
  },
  {
    "text": "it all right skip this okay we're going just skip this one",
    "start": "4724199",
    "end": "4731520"
  },
  {
    "text": "um yeah let's uh come together so this is acknowledgement to all the people who",
    "start": "4731520",
    "end": "4737719"
  },
  {
    "text": "actually make this work possible uh we appreciate all the IBM team uh Ian",
    "start": "4737719",
    "end": "4743040"
  },
  {
    "text": "research team uh we collaborates closely with and some of them working with capit some of them working on the uh AI this",
    "start": "4743040",
    "end": "4749840"
  },
  {
    "text": "is a uh very substantial lineup of uh talents sitting behind the scene make",
    "start": "4749840",
    "end": "4756040"
  },
  {
    "text": "this happen and also appreciate the uh people who are volunteering for the uh",
    "start": "4756040",
    "end": "4761560"
  },
  {
    "text": "application development from toight uh chat uh we have a wonderful experiences",
    "start": "4761560",
    "end": "4766800"
  },
  {
    "text": "working with unuser experiences uh to identify the use cases of small models",
    "start": "4766800",
    "end": "4772480"
  },
  {
    "text": "large models and different uh connecting uh techniques to make the end user",
    "start": "4772480",
    "end": "4777639"
  },
  {
    "text": "experience uh better um for takeaway uh we do have a",
    "start": "4777639",
    "end": "4783880"
  },
  {
    "text": "lots of activities going on in this cucon now we have the sustain Cloud native AI working group uh we have um",
    "start": "4783880",
    "end": "4790960"
  },
  {
    "text": "the wi paper just published recently so you can see there's a lots of information you can find over there the",
    "start": "4790960",
    "end": "4796520"
  },
  {
    "text": "configuration the deployments and even background of AI and also sustainabilities how CR native AI can",
    "start": "4796520",
    "end": "4804000"
  },
  {
    "text": "help us work together it's all the information you can find over there and we uh Cloud native AI work groups also",
    "start": "4804000",
    "end": "4811440"
  },
  {
    "text": "have this bi-weekly meetings um welcome to join the meeting we also have the bi-weekly meetings on environmental",
    "start": "4811440",
    "end": "4818760"
  },
  {
    "text": "sustainability TCH I think the Neo the TCH lead is also here in the meeting um",
    "start": "4818760",
    "end": "4824040"
  },
  {
    "text": "in the conference so he has a talk on sustainability sometime tomorrow so welcome to Real",
    "start": "4824040",
    "end": "4830480"
  },
  {
    "text": "Talk um the significance of a large language models as a utility for",
    "start": "4830480",
    "end": "4836199"
  },
  {
    "text": "day-to-day life and also as the challenge in our environmental uh",
    "start": "4836199",
    "end": "4841679"
  },
  {
    "text": "sustainability are both visible and doing something to mitigate the risk and",
    "start": "4841679",
    "end": "4847239"
  },
  {
    "text": "while improving the quality of life is quite beneficial to all of us and I believe technically that is also very",
    "start": "4847239",
    "end": "4853560"
  },
  {
    "text": "encouraging to take so I welcome everybody where you know spend time looking at this Mak Technologies",
    "start": "4853560",
    "end": "4859320"
  },
  {
    "text": "available and make the use case more appropriate for your environmental",
    "start": "4859320",
    "end": "4864360"
  },
  {
    "text": "considerations so we do have a current support for NVIDIA gpus which we have already uh identified certain use cases",
    "start": "4864360",
    "end": "4871679"
  },
  {
    "text": "how you can meure the CPU power consumption how you can correlate with your large language models performance",
    "start": "4871679",
    "end": "4877719"
  },
  {
    "text": "to provide the you know interesting use cases uh to choose the best models uh while uh preserving the energy",
    "start": "4877719",
    "end": "4884239"
  },
  {
    "text": "consumption we also exploring ways to support other type of gpus and accelerators and",
    "start": "4884239",
    "end": "4891199"
  },
  {
    "text": "vendors who are interested in working with cap working with you know different",
    "start": "4891199",
    "end": "4896280"
  },
  {
    "text": "language uh serving infrastructures this could be a very exciting playground for",
    "start": "4896280",
    "end": "4902000"
  },
  {
    "text": "everybody uh very lastly um so thank you all for coming to the session and if you",
    "start": "4902000",
    "end": "4907840"
  },
  {
    "text": "have questions and this is has",
    "start": "4907840",
    "end": "4911480"
  },
  {
    "text": "time",
    "start": "4913520",
    "end": "4916520"
  },
  {
    "text": "okay yeah just a question is about how to map the the P ID to The Container ID um so there a different ways I believe",
    "start": "4933920",
    "end": "4940320"
  },
  {
    "text": "Nvidia dcgm exporter use one way and capit use in different way the the way capit use is uh using the C group file",
    "start": "4940320",
    "end": "4947600"
  },
  {
    "text": "system uh Nam space to ID conversion so if you quy you get the uh container ID",
    "start": "4947600",
    "end": "4953679"
  },
  {
    "text": "and also the process ID and then you carry the C group file system with the p",
    "start": "4953679",
    "end": "4958880"
  },
  {
    "text": "with the container ID then you can guess the uh the container uh container ID the",
    "start": "4958880",
    "end": "4964920"
  },
  {
    "text": "container name actually So eventually quiry with a KU API you have correlate",
    "start": "4964920",
    "end": "4970120"
  },
  {
    "text": "the C group ID with the container ID sorry that's a three entity C group ID",
    "start": "4970120",
    "end": "4975560"
  },
  {
    "text": "which is 64 bits uh uh numerical number container ID is a hash number and then",
    "start": "4975560",
    "end": "4981360"
  },
  {
    "text": "we going from there you can go into mapping which other the processes using that",
    "start": "4981360",
    "end": "4987880"
  },
  {
    "text": "container oh okay so the variant missing piece is the ebpf so when ebpf go into",
    "start": "4990440",
    "end": "4995760"
  },
  {
    "text": "the contact switch uh level of probe you get the C group ID you get the P ID so",
    "start": "4995760",
    "end": "5002520"
  },
  {
    "text": "that's the mapping with a c group ID to your P ID and then we pop up to the uh",
    "start": "5002520",
    "end": "5008120"
  },
  {
    "text": "user Space level with the c group ID and then we get the high ID the container ID",
    "start": "5008120",
    "end": "5015040"
  },
  {
    "text": "and then from the container ID at the C group level you car the C9 API and you",
    "start": "5015040",
    "end": "5020080"
  },
  {
    "text": "get a you know container part name so there a true level of resolution and",
    "start": "5020080",
    "end": "5025400"
  },
  {
    "text": "that's how we get it thank",
    "start": "5025400",
    "end": "5030239"
  },
  {
    "text": "you",
    "start": "5033440",
    "end": "5036440"
  },
  {
    "text": "right so everybody gets everything that is great uh if you have no more",
    "start": "5039000",
    "end": "5044159"
  },
  {
    "text": "questions maybe you can just uh you know Miss offline and get questions that's",
    "start": "5044159",
    "end": "5049679"
  },
  {
    "text": "all",
    "start": "5049679",
    "end": "5051960"
  },
  {
    "text": "welcome",
    "start": "5058560",
    "end": "5061560"
  }
]