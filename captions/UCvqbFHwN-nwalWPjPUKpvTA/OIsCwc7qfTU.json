[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "Thank You Kelsey and welcome to Berlin everyone Cooper knots this is a great time to be",
    "start": "30",
    "end": "8550"
  },
  {
    "text": "involved in the kubernetes project this project is attempting to redefine how",
    "start": "8550",
    "end": "15960"
  },
  {
    "text": "the world runs applications on distributed systems and we believe that",
    "start": "15960",
    "end": "23400"
  },
  {
    "text": "this is only possible through an open and transparent and diverse community of",
    "start": "23400",
    "end": "31679"
  },
  {
    "text": "users and contributors you all know that",
    "start": "31679",
    "end": "39690"
  },
  {
    "start": "36000",
    "end": "314000"
  },
  {
    "text": "containers are portable and that is a huge part of their value to users but",
    "start": "39690",
    "end": "46860"
  },
  {
    "text": "you also need a portable management stack to go along with that so you want",
    "start": "46860",
    "end": "54870"
  },
  {
    "text": "a stack that can run on premise in hybrid and in multi cloud environments",
    "start": "54870",
    "end": "61890"
  },
  {
    "text": "as well as locally on your laptop and maybe on raspberry PI's for the",
    "start": "61890",
    "end": "68130"
  },
  {
    "text": "hobbyists we believe that in order to achieve this level of applicability",
    "start": "68130",
    "end": "77100"
  },
  {
    "text": "across environments it takes a large community that works across the world",
    "start": "77100",
    "end": "83880"
  },
  {
    "text": "just like our users are across the world and the openness and transparency lets",
    "start": "83880",
    "end": "93210"
  },
  {
    "text": "us stay honest and it gives the community feedback about whether we're",
    "start": "93210",
    "end": "100049"
  },
  {
    "text": "building the right product and heading in the right direction so I'm going to",
    "start": "100049",
    "end": "105240"
  },
  {
    "text": "give you some examples of this kubernetes 1.6 just released sometime in",
    "start": "105240",
    "end": "113189"
  },
  {
    "text": "the middle of the night yesterday [Applause]",
    "start": "113189",
    "end": "121229"
  },
  {
    "text": "we're very excited the theme for this release is multi team multi workload at",
    "start": "124040",
    "end": "131880"
  },
  {
    "text": "scale and it is a result of five",
    "start": "131880",
    "end": "137100"
  },
  {
    "text": "thousand commits across 275 authors from everywhere around the world release",
    "start": "137100",
    "end": "145500"
  },
  {
    "text": "management for one point six was led by Dan Giuseppe of Koro s and this is the",
    "start": "145500",
    "end": "152610"
  },
  {
    "text": "first time that a non Googler has led the management of the release",
    "start": "152610",
    "end": "159530"
  },
  {
    "text": "so kubernetes now supports 5,000 node clusters that's within the same",
    "start": "164319",
    "end": "170660"
  },
  {
    "text": "stringent SLO that we had before but did you know that this feature was done here",
    "start": "170660",
    "end": "177349"
  },
  {
    "text": "in Europe the Google team in Warsaw all of whom are here in the audience today",
    "start": "177349",
    "end": "185140"
  },
  {
    "text": "contributed and led to the testing of 5,000 node clusters and if you like my",
    "start": "185140",
    "end": "191840"
  },
  {
    "text": "blue hoody this is a special Warsaw Edition kubernetes hoody and you can catch them afterwards if you want to try",
    "start": "191840",
    "end": "199010"
  },
  {
    "text": "and get one perhaps the biggest most important feature in this release is",
    "start": "199010",
    "end": "205569"
  },
  {
    "text": "role-based access control and that was led that work was led by a team from Red",
    "start": "205569",
    "end": "211880"
  },
  {
    "text": "Hat involving many other companies in the community I'm gonna do a demo of that controlled scheduling allows you to",
    "start": "211880",
    "end": "223610"
  },
  {
    "text": "control where you schedule different workloads in a large cluster this is",
    "start": "223610",
    "end": "230900"
  },
  {
    "text": "very important for multi workload clusters which ultimately helps you",
    "start": "230900",
    "end": "236780"
  },
  {
    "text": "improve the efficiency and this work was also a collaboration across many",
    "start": "236780",
    "end": "243019"
  },
  {
    "text": "companies Google Red Hat IBM a team in",
    "start": "243019",
    "end": "248030"
  },
  {
    "text": "China with Huawei and many others the",
    "start": "248030",
    "end": "253130"
  },
  {
    "text": "last one that I'm going to talk about is dynamic storage provisioning which is",
    "start": "253130",
    "end": "258340"
  },
  {
    "text": "essentially storage automation in the service of stateful applications again",
    "start": "258340",
    "end": "264080"
  },
  {
    "text": "allowing you to run multiple workloads not just state not just stateless but",
    "start": "264080",
    "end": "269870"
  },
  {
    "text": "stateful in the same cluster so new",
    "start": "269870",
    "end": "275150"
  },
  {
    "text": "features are cool and product managers and engineers like new features but how",
    "start": "275150",
    "end": "281599"
  },
  {
    "text": "many of you care about stability yeah about a hundred percent right so",
    "start": "281599",
    "end": "290000"
  },
  {
    "text": "the kubernetes community as a whole decided that in this release and in many",
    "start": "290000",
    "end": "296210"
  },
  {
    "text": "future releases we are going to focus on stability that is a core value",
    "start": "296210",
    "end": "301639"
  },
  {
    "text": "proposition for our users and so over 20 features moved from alpha to beta to",
    "start": "301639",
    "end": "309919"
  },
  {
    "text": "stable in this release 5,000 nodes what",
    "start": "309919",
    "end": "317479"
  },
  {
    "start": "314000",
    "end": "372000"
  },
  {
    "text": "does that mean it means 150,000 pods in a cluster and really what we've tested",
    "start": "317479",
    "end": "326630"
  },
  {
    "text": "is that this still works within the stringent SLO of less than one second",
    "start": "326630",
    "end": "334060"
  },
  {
    "text": "pod API latency 99th percentile and less than five seconds pod startup time also",
    "start": "334060",
    "end": "341449"
  },
  {
    "text": "99th percentile but the real hero behind this work is at CD at CD v3 specifically",
    "start": "341449",
    "end": "349190"
  },
  {
    "text": "which was developed by core OS in support of this scaling the data model",
    "start": "349190",
    "end": "355639"
  },
  {
    "text": "has been updated the communication framework has been updated so data",
    "start": "355639",
    "end": "361400"
  },
  {
    "text": "communication instead of over JSON is now using the G RPC protocol which",
    "start": "361400",
    "end": "366919"
  },
  {
    "text": "allows much greater performance so now",
    "start": "366919",
    "end": "374990"
  },
  {
    "start": "372000",
    "end": "448000"
  },
  {
    "text": "let's go to whether bigger is better so now you have a 5,000 node cluster we",
    "start": "374990",
    "end": "383240"
  },
  {
    "text": "asked users do you want to go further what do you what do you plan to do with a 5,000 node cluster is that really what",
    "start": "383240",
    "end": "391310"
  },
  {
    "text": "you're looking for and what I hear from users is they actually want global scale",
    "start": "391310",
    "end": "397550"
  },
  {
    "text": "they want to serve their users where they are with low latency and so often",
    "start": "397550",
    "end": "403580"
  },
  {
    "text": "it's multiple large clusters in different geographies around the world that's what this map is trying to show",
    "start": "403580",
    "end": "410930"
  },
  {
    "text": "and we asked users why is multi cloud important to you",
    "start": "410930",
    "end": "416140"
  },
  {
    "text": "and it wasn't just about scale a lot of it had to do with the reliability availability and ultimately reducing IT",
    "start": "416140",
    "end": "424510"
  },
  {
    "text": "infrastructure costs that's why this release and our roadmap going forward is",
    "start": "424510",
    "end": "430090"
  },
  {
    "text": "very focused on efficiency making sure that you can run multiple teams without",
    "start": "430090",
    "end": "437860"
  },
  {
    "text": "interference within one cluster and multiple different types of workloads",
    "start": "437860",
    "end": "442920"
  },
  {
    "text": "within one cluster in an efficient manner so the first feature that works",
    "start": "442920",
    "end": "452980"
  },
  {
    "start": "448000",
    "end": "481000"
  },
  {
    "text": "on this is our back without role based access controls you have a cluster",
    "start": "452980",
    "end": "458200"
  },
  {
    "text": "whether it's a three node cluster or a 5,000 node cluster that's essentially vanilla every pod has roughly the same",
    "start": "458200",
    "end": "466480"
  },
  {
    "text": "level of authorization as every other pod so whether you have a blue team or a green team for different types of",
    "start": "466480",
    "end": "472870"
  },
  {
    "text": "workloads there's not good differentiation between them in terms of the permissions and what they can do",
    "start": "472870",
    "end": "480720"
  },
  {
    "start": "481000",
    "end": "521000"
  },
  {
    "text": "also without granular control over scheduling it limits the flexibility to",
    "start": "481740",
    "end": "487540"
  },
  {
    "text": "optimize utilization that brings me to our back our back Marc's a huge shift in",
    "start": "487540",
    "end": "499020"
  },
  {
    "text": "kubernetes Tim Hawken one of the uber tiel's of this project draws the analogy",
    "start": "499020",
    "end": "506650"
  },
  {
    "text": "- it's like going from das where it's a single flat user and everyone can see",
    "start": "506650",
    "end": "513520"
  },
  {
    "text": "everyone else's as files - unix where it's user specific permissions and this",
    "start": "513520",
    "end": "522490"
  },
  {
    "text": "is now what it looks like with our back you can separate the workloads of",
    "start": "522490",
    "end": "530020"
  },
  {
    "text": "different teams into different namespaces and give them granular permissions on what they can achieve so",
    "start": "530020",
    "end": "536860"
  },
  {
    "text": "let's do a demo to show this if we could switch over to my laptop please",
    "start": "536860",
    "end": "543660"
  },
  {
    "start": "552000",
    "end": "1010000"
  },
  {
    "text": "all right we're gonna try and do a live demo here I have a cluster that is",
    "start": "552270",
    "end": "560250"
  },
  {
    "text": "running 1.6 our latest release in",
    "start": "560250",
    "end": "565490"
  },
  {
    "text": "Google's cloud in container engine it's a three node cluster and in order to do",
    "start": "565490",
    "end": "572730"
  },
  {
    "text": "a demo of role based access control I need multiple roles so I'm gonna star",
    "start": "572730",
    "end": "579120"
  },
  {
    "text": "in this movie as three different people I will be in this window a cluster admin",
    "start": "579120",
    "end": "586580"
  },
  {
    "text": "that has control over everything it's like a super user in the cluster and",
    "start": "586580",
    "end": "591660"
  },
  {
    "text": "then I'm going to create two different accounts one account for a Blue Team developer and one account for a Green",
    "start": "591660",
    "end": "597750"
  },
  {
    "text": "Team developer I'm gonna play those roles in these other tabs okay so here",
    "start": "597750",
    "end": "605310"
  },
  {
    "text": "what I've done is I've told the G cloud I am service please create an account for the blue team developer and store",
    "start": "605310",
    "end": "611130"
  },
  {
    "text": "those keys in a local file I'm going to do the same thing for the Green Team developer and the I am service is going",
    "start": "611130",
    "end": "617610"
  },
  {
    "text": "to create that service account and store their credentials in a local file and then I'm going to go to the blue team's",
    "start": "617610",
    "end": "627260"
  },
  {
    "text": "console here and configure a cube cuddle",
    "start": "627260",
    "end": "635790"
  },
  {
    "text": "in this window to get those credentials and the same thing for the green team this is all essentially set up so that",
    "start": "635790",
    "end": "646140"
  },
  {
    "text": "you can see three different three different roles in this in this cluster",
    "start": "646140",
    "end": "651690"
  },
  {
    "text": "okay now I'm gonna create a namespace for the blue team cube cuddle creates",
    "start": "651690",
    "end": "657300"
  },
  {
    "text": "namespace blue let's go over to the blue team and see if the blue team can do anything in this namespace they",
    "start": "657300",
    "end": "663330"
  },
  {
    "text": "shouldn't be able to because I haven't given them permissions so cube cuddle get pods namespace blue and we get an",
    "start": "663330",
    "end": "669570"
  },
  {
    "text": "error and this is as it should be so I'll show you how I as the cluster super",
    "start": "669570",
    "end": "676050"
  },
  {
    "text": "admin can set up these permissions and this gets to the heart of role based access control",
    "start": "676050",
    "end": "681720"
  },
  {
    "text": "so the core here is a set of cluster roles that are predefined",
    "start": "681720",
    "end": "687810"
  },
  {
    "text": "and can be can be updated to add custom roles these are some of the user roles",
    "start": "687810",
    "end": "693810"
  },
  {
    "text": "there's also system roles so some of the user roles as you see there's admin",
    "start": "693810",
    "end": "699300"
  },
  {
    "text": "there's edit there's view we're probably going to want the admin role for the blue team",
    "start": "699300",
    "end": "704370"
  },
  {
    "text": "because we want blue team developer to be able to create and delete pods and services so let's take a look at what",
    "start": "704370",
    "end": "711600"
  },
  {
    "text": "the admin role default permissions are this is a condensed version of what those permissions are just looking at a",
    "start": "711600",
    "end": "717960"
  },
  {
    "text": "few of them so the cluster role admin has access to several resources and sub",
    "start": "717960",
    "end": "725670"
  },
  {
    "text": "resources and it has the ability to execute these words these verbs it can",
    "start": "725670",
    "end": "731400"
  },
  {
    "text": "create it can list it can set up watches on all of these resources okay so that's",
    "start": "731400",
    "end": "738600"
  },
  {
    "text": "great now how do we give the blue developer this role we need to create a",
    "start": "738600",
    "end": "744480"
  },
  {
    "text": "role binding this is the amyl for the role binding and what you see here it's saying that I want to give the role",
    "start": "744480",
    "end": "753060"
  },
  {
    "text": "admin to the user Blue team developer which happens to be a service account in",
    "start": "753060",
    "end": "759330"
  },
  {
    "text": "the name space blue so let's create this",
    "start": "759330",
    "end": "765000"
  },
  {
    "text": "binding it's been created now if we go",
    "start": "765000",
    "end": "770310"
  },
  {
    "text": "to the blue developers console and we again try and get pods in the name space",
    "start": "770310",
    "end": "778050"
  },
  {
    "text": "blue we see that we don't get an error so that is now successful he has access to the blue name space what can you do",
    "start": "778050",
    "end": "784560"
  },
  {
    "text": "in the blue name space he can actually execute so now he's created an engine exit of deployment and this is what you",
    "start": "784560",
    "end": "791280"
  },
  {
    "text": "would expect he has admin access to the blue name space and now if we get get pods of course nginx has created some",
    "start": "791280",
    "end": "798330"
  },
  {
    "text": "pods and get services and so forth so that's the first thing I wanted to show",
    "start": "798330",
    "end": "805100"
  },
  {
    "text": "let's also do this for the Green namespace so we can show the difference and that you know the green user doesn't",
    "start": "805100",
    "end": "810690"
  },
  {
    "text": "have access to the blue namespace and so forth so we're going to create at the green namespace same thing and",
    "start": "810690",
    "end": "816700"
  },
  {
    "text": "create a role binding now we're giving admin cluster access to the Green Team",
    "start": "816700",
    "end": "823870"
  },
  {
    "text": "for just the green namespace just the green namespace part of the cluster",
    "start": "823870",
    "end": "829530"
  },
  {
    "text": "right so we'll go ahead and create that and let's go back to the blue admin and",
    "start": "829530",
    "end": "834730"
  },
  {
    "text": "make sure that he doesn't have access to this new namespace that we just created so get pods name space green for the",
    "start": "834730",
    "end": "842740"
  },
  {
    "text": "blue developer gives us an error he has everything he can do in the blue namespace but not in the green namespace",
    "start": "842740",
    "end": "849090"
  },
  {
    "text": "can't get services etc let's go to the green user for the first time and see if",
    "start": "849090",
    "end": "856090"
  },
  {
    "text": "our auerbach worked and it did when we do get pods name space green there's no",
    "start": "856090",
    "end": "862810"
  },
  {
    "text": "resources found we don't get an error we haven't created any resources let's make sure that this is actually admin access",
    "start": "862810",
    "end": "868480"
  },
  {
    "text": "and that as a green user I can create and nginx deployment and looks like that",
    "start": "868480",
    "end": "873910"
  },
  {
    "text": "succeeded and I've read forward a little bit I want to make sure that while he",
    "start": "873910",
    "end": "881800"
  },
  {
    "text": "can do anything he wants in the green namespace he can't actually access the blue namespace and so get pods blue",
    "start": "881800",
    "end": "888460"
  },
  {
    "text": "namespace indeed returns an error so now you see the isolation between the two",
    "start": "888460",
    "end": "893740"
  },
  {
    "text": "teams in the same cluster based on two different namespaces that's nice",
    "start": "893740",
    "end": "901030"
  },
  {
    "text": "there's a lot more you can do and I'll show you one last thing if I want to give the green team view only access to",
    "start": "901030",
    "end": "909340"
  },
  {
    "text": "the blue namespace so that let's say that the green team is doing some monitoring for the whole cluster and I",
    "start": "909340",
    "end": "915340"
  },
  {
    "text": "want them to see the pods and services in the blue namespace but not be able to modify them I can do that too",
    "start": "915340",
    "end": "921460"
  },
  {
    "text": "as the super admin for the cluster I have visibility and access to all the",
    "start": "921460",
    "end": "929290"
  },
  {
    "text": "namespaces so you can see that I have access to the blue into next deployment I can see the green nginx deployment and",
    "start": "929290",
    "end": "935650"
  },
  {
    "text": "there's a love there's a number of system deployments that are there I'm going to create the blue-green binding",
    "start": "935650",
    "end": "942190"
  },
  {
    "text": "and what that says is let's give view access so the view was one of the roles",
    "start": "942190",
    "end": "948459"
  },
  {
    "text": "I think you saw earlier to the green team developer in the blue namespace so",
    "start": "948459",
    "end": "954880"
  },
  {
    "text": "view means they shouldn't be able to create anything so we've we've established that role binding and let's",
    "start": "954880",
    "end": "962170"
  },
  {
    "text": "go back to the green developer and see that now if he has access to the blue",
    "start": "962170",
    "end": "968890"
  },
  {
    "text": "namespace and yes indeed there is red level access so the green developer can",
    "start": "968890",
    "end": "974410"
  },
  {
    "text": "read and see the nginx deployment in the blue namespace but we want to also check",
    "start": "974410",
    "end": "980019"
  },
  {
    "text": "whether they can delete this for example or or take and so cube cuttle namespace",
    "start": "980019",
    "end": "987430"
  },
  {
    "text": "blue delete deployments does not work for the green developer and that's as it should be so this just that ends my demo",
    "start": "987430",
    "end": "993730"
  },
  {
    "text": "this just illustrates that you know there's a lot of granularity because",
    "start": "993730",
    "end": "998860"
  },
  {
    "text": "we've got namespace level per resource by role permissions that you can set for",
    "start": "998860",
    "end": "1007740"
  },
  {
    "text": "different types of users and that gives you a lot of flexibility and granularity in terms of defining defining and giving",
    "start": "1007740",
    "end": "1017130"
  },
  {
    "start": "1010000",
    "end": "1138000"
  },
  {
    "text": "structure to your to your teams and in your cluster so the next feature next",
    "start": "1017130",
    "end": "1022320"
  },
  {
    "text": "set of features controlled scheduling is more about scheduling multiple different",
    "start": "1022320",
    "end": "1028260"
  },
  {
    "text": "types of workloads and optimizing efficiency across those there's actually",
    "start": "1028260",
    "end": "1035790"
  },
  {
    "text": "three different three or four different features within this that are moving to beta in the 1.6 release node affinity",
    "start": "1035790",
    "end": "1045079"
  },
  {
    "text": "allows you to define on a per pod basis",
    "start": "1045079",
    "end": "1050309"
  },
  {
    "text": "a preference or a requirement of the type of node on which that pods should schedule so you can say I want to",
    "start": "1050309",
    "end": "1058770"
  },
  {
    "text": "schedule this pod only on nodes in this zone or nodes that have SSDs or you can",
    "start": "1058770",
    "end": "1065850"
  },
  {
    "text": "actually define custom labels and you can label nodes and then decide and then set which pods should schedule on those",
    "start": "1065850",
    "end": "1072120"
  },
  {
    "text": "nodes pod affinity and anti affinity let's use schedule pods relative to",
    "start": "1072120",
    "end": "1078970"
  },
  {
    "text": "other pods that may be there this is very useful if you want to for example optimize inter pod communication such as",
    "start": "1078970",
    "end": "1086740"
  },
  {
    "text": "for east-west or Northwest or north-south communication that's pod affinity pod anti affinity is if you",
    "start": "1086740",
    "end": "1094240"
  },
  {
    "text": "want to for example separate pods from each other you want maximum spreading in the",
    "start": "1094240",
    "end": "1099430"
  },
  {
    "text": "cluster or you want to avoid antagonistic services from Co scheduling on the same node I'll give you an",
    "start": "1099430",
    "end": "1105100"
  },
  {
    "text": "example of that taints and toleration z' are on a node basis you can put a taint",
    "start": "1105100",
    "end": "1111910"
  },
  {
    "text": "on a node that says this node is dedicated for this team or this node has",
    "start": "1111910",
    "end": "1117940"
  },
  {
    "text": "a GPU and so only workloads that tolerate that taint that require a GPU",
    "start": "1117940",
    "end": "1123040"
  },
  {
    "text": "should schedule there lastly custom schedulers give you the ultimate",
    "start": "1123040",
    "end": "1128590"
  },
  {
    "text": "flexibility in scheduling you can bring your own scheduler that can work alongside or in place of the default",
    "start": "1128590",
    "end": "1135730"
  },
  {
    "text": "scheduler so I'll give you an example of",
    "start": "1135730",
    "end": "1142020"
  },
  {
    "start": "1138000",
    "end": "1345000"
  },
  {
    "text": "how to use pod anti affinity and what",
    "start": "1142020",
    "end": "1148060"
  },
  {
    "text": "we're doing here is we've got a stateful pod which is blue state and it's part of",
    "start": "1148060",
    "end": "1154240"
  },
  {
    "text": "a quorum based stateful application so we want to make sure that if there's",
    "start": "1154240",
    "end": "1159310"
  },
  {
    "text": "always the the pod is scheduled on a different node each pod is scheduled on",
    "start": "1159310",
    "end": "1166510"
  },
  {
    "text": "a different node but and not together and also that at least two of these pods",
    "start": "1166510",
    "end": "1171610"
  },
  {
    "text": "are up at all times so there's two features here that we're using pod anti",
    "start": "1171610",
    "end": "1180610"
  },
  {
    "text": "affinity where we're saying that do not schedule the effect is do not schedule",
    "start": "1180610",
    "end": "1186460"
  },
  {
    "text": "the key is hostname so do not schedule on a node where the value of the pod",
    "start": "1186460",
    "end": "1193180"
  },
  {
    "text": "that exists there is equal to blue states so if there's already a blue state pod then do not schedule this",
    "start": "1193180",
    "end": "1199990"
  },
  {
    "text": "other blue state pod on that note that's essentially what the ANSI Finity is saying and then we've also got pod",
    "start": "1199990",
    "end": "1205270"
  },
  {
    "text": "disruption budget which is a future that was already released in the kubernetes released but it's very useful",
    "start": "1205270",
    "end": "1211390"
  },
  {
    "text": "for quorum based applications which says always make sure that there are at least two such pods available in the cluster",
    "start": "1211390",
    "end": "1218110"
  },
  {
    "text": "and so if you're doing a cluster upgrade and there's a system operation doing that it will make sure that at least two",
    "start": "1218110",
    "end": "1225250"
  },
  {
    "text": "pods are always available now I'm gonna",
    "start": "1225250",
    "end": "1231490"
  },
  {
    "text": "give you an example of teens and Toleration x' and where that can be useful tips and toleration x' can be",
    "start": "1231490",
    "end": "1236920"
  },
  {
    "text": "used for example to dedicate a node to a particular type of workload so in this",
    "start": "1236920",
    "end": "1242530"
  },
  {
    "text": "case we have put a taint on node one that says this is a dedicated node please do not schedule pods here and",
    "start": "1242530",
    "end": "1249660"
  },
  {
    "text": "we're going to put a toleration for that taint on the green pod that's called green job and so that toleration says",
    "start": "1249660",
    "end": "1257710"
  },
  {
    "text": "yeah I have a toleration for the pod for the node that says it's dedicated only",
    "start": "1257710",
    "end": "1262750"
  },
  {
    "text": "for green jobs and so on and so forth there are many such features and many",
    "start": "1262750",
    "end": "1268870"
  },
  {
    "text": "such actually combinations that you can that you can use forgiveness is is a",
    "start": "1268870",
    "end": "1274480"
  },
  {
    "text": "special case of chains and toleration x' where every node has a taint that says",
    "start": "1274480",
    "end": "1280620"
  },
  {
    "text": "if the node becomes unreachable if there's some sort of event that happens then evict pods and evict them after you",
    "start": "1280620",
    "end": "1289000"
  },
  {
    "text": "know how long that you do you wait",
    "start": "1289000",
    "end": "1294130"
  },
  {
    "text": "before you evict the pods is a toleration that you can put on the pod itself so here we've got a pod that has",
    "start": "1294130",
    "end": "1301030"
  },
  {
    "text": "a toleration of 300 seconds and another pod that has a toleration of 3600 seconds so at t equal to 0 all pods are",
    "start": "1301030",
    "end": "1308410"
  },
  {
    "text": "on node 1 node 1 has had some sort of event and has gone down and it's C equal",
    "start": "1308410",
    "end": "1313930"
  },
  {
    "text": "to 300 the blue pod which has the 300 level 300 second toleration will be",
    "start": "1313930",
    "end": "1319840"
  },
  {
    "text": "removed and rescheduled elsewhere in the cluster and then at 3600 seconds the green pods will move over elsewhere in",
    "start": "1319840",
    "end": "1326080"
  },
  {
    "text": "the cluster this is very useful for ensuring high availability for example if a node goes down or a zone goes down",
    "start": "1326080",
    "end": "1333760"
  },
  {
    "text": "and you've spread your cluster across multiple zones kubernetes will automatically move your services to the",
    "start": "1333760",
    "end": "1340590"
  },
  {
    "text": "zones that are available and the notes that are available all right the last",
    "start": "1340590",
    "end": "1347039"
  },
  {
    "start": "1345000",
    "end": "1847000"
  },
  {
    "text": "feature that I'm going to demo is dynamic storage provisioning and this is",
    "start": "1347039",
    "end": "1352799"
  },
  {
    "text": "very useful for automating the management and lifecycle of storage",
    "start": "1352799",
    "end": "1358760"
  },
  {
    "text": "particularly important for stateful applications where you want to make sure that the storage is always available to",
    "start": "1358760",
    "end": "1366450"
  },
  {
    "text": "your stateful pods and for this I'd like to switch over to the demo I'm going to",
    "start": "1366450",
    "end": "1378840"
  },
  {
    "text": "use the same cluster three node cluster in Google's cloud and that's running 1.6",
    "start": "1378840",
    "end": "1389820"
  },
  {
    "text": "so the first thing I'm going to show you is actually non dynamic storage provisioning which is manual storage",
    "start": "1389820",
    "end": "1396059"
  },
  {
    "text": "provisioning the way that it used to be done all right",
    "start": "1396059",
    "end": "1404610"
  },
  {
    "text": "so in this cluster I'm first going to list what type of storage is already available I told you it was a 3 node",
    "start": "1404610",
    "end": "1410039"
  },
  {
    "text": "cluster and so there's only just the local disk associated with each nodes each node we haven't created any storage",
    "start": "1410039",
    "end": "1416730"
  },
  {
    "text": "yet I'm going to go ahead and provision a disk in Google Cloud and I'm gonna say I want a 100 gigabyte this and it should",
    "start": "1416730",
    "end": "1424140"
  },
  {
    "text": "be of type PD standard and let's name it manual disk one this is an operation that usually someone familiar with",
    "start": "1424140",
    "end": "1429929"
  },
  {
    "text": "storage you know probably a storage admin might like to do but in any case what we're doing is we're pre",
    "start": "1429929",
    "end": "1435360"
  },
  {
    "text": "provisioning the storage before our workload is ready to consume it now",
    "start": "1435360",
    "end": "1441090"
  },
  {
    "text": "let's check that this disk has been created g-cloud compute disks list and yes now in addition to the local disks",
    "start": "1441090",
    "end": "1448200"
  },
  {
    "text": "you see that the manual this one has been created all right so let's look at",
    "start": "1448200",
    "end": "1453360"
  },
  {
    "text": "different ways of using this disk in your application one way the not",
    "start": "1453360",
    "end": "1459360"
  },
  {
    "text": "recommended way from de pond is to have an inline pod spec so in this pod",
    "start": "1459360",
    "end": "1465480"
  },
  {
    "text": "specification I have actually said you know exactly what my disk is that it's a",
    "start": "1465480",
    "end": "1470970"
  },
  {
    "text": "juicy persistent disk it's called manual disc one this is the type of file system you know while this",
    "start": "1470970",
    "end": "1477099"
  },
  {
    "text": "will work there are many disadvantages to this as you can imagine this pods back is very specific and it is not",
    "start": "1477099",
    "end": "1484209"
  },
  {
    "text": "portable you can't take it to a different environment it can't use a different disc say if manual disc one is",
    "start": "1484209",
    "end": "1490059"
  },
  {
    "text": "we want to change it out and also kubernetes doesn't really yet know about manual disc one it's something that's",
    "start": "1490059",
    "end": "1496839"
  },
  {
    "text": "outside the scope of the of the of the system what we really want is a pod sect",
    "start": "1496839",
    "end": "1502659"
  },
  {
    "text": "that's modern and portable that doesn't refer to the specifics of the storage at",
    "start": "1502659",
    "end": "1507820"
  },
  {
    "text": "the end of the day I as the application admin do not want to know the specifics of the infrastructure and kubernetes is",
    "start": "1507820",
    "end": "1514329"
  },
  {
    "text": "supposed to isolate me from that so a modern pod spec would just say hey I",
    "start": "1514329",
    "end": "1519789"
  },
  {
    "text": "have a persistent volume claim and this is the name of the claim the claim has some of the details and then the",
    "start": "1519789",
    "end": "1526179"
  },
  {
    "text": "persistent volume spec has some of the details that are specific to the storage environment so what we've done is we've",
    "start": "1526179",
    "end": "1533289"
  },
  {
    "text": "taken that storage specific detail and distributed it across to other objects",
    "start": "1533289",
    "end": "1538419"
  },
  {
    "text": "let me show you those two other objects the first one is a PVC a persistent",
    "start": "1538419",
    "end": "1543789"
  },
  {
    "text": "volume claim this is really just a request it says I want five gigs of",
    "start": "1543789",
    "end": "1549489"
  },
  {
    "text": "storage that's what my pod needs please wherever you find it I don't care what type it is but give me that and this",
    "start": "1549489",
    "end": "1558999"
  },
  {
    "text": "note this thing called a storage class name in this in this example we've set it to the empty string which is really",
    "start": "1558999",
    "end": "1565629"
  },
  {
    "text": "what's saying I don't care what type it is but in dynamic storage provisioning I'll come back to this your storage admin can",
    "start": "1565629",
    "end": "1572109"
  },
  {
    "text": "set up different classes of gold silver bronze fast slow storage class and the",
    "start": "1572109",
    "end": "1578049"
  },
  {
    "text": "in the PVC I as the user can say yeah I want the gold class but for this example",
    "start": "1578049",
    "end": "1583779"
  },
  {
    "text": "I'm not going to use any class so the second object is a persistent volume",
    "start": "1583779",
    "end": "1590559"
  },
  {
    "text": "this is the object that the storage admin or cluster admin would use to give kubernetes information about the storage",
    "start": "1590559",
    "end": "1597489"
  },
  {
    "text": "that they've provisioned so here is where the details are and we've said",
    "start": "1597489",
    "end": "1602619"
  },
  {
    "text": "here yeah actually the 10-gig disk that you created I want kubernetes to have access to five gigs",
    "start": "1602619",
    "end": "1608990"
  },
  {
    "text": "of that and when you I'm done with it I'd like you to delete that disk and yes",
    "start": "1608990",
    "end": "1615020"
  },
  {
    "text": "it is a juicy a persistent disk with this name so this is the PV the persistent volume object you see how",
    "start": "1615020",
    "end": "1621530"
  },
  {
    "text": "we've done that abstraction and separation of concerns so let's go ahead",
    "start": "1621530",
    "end": "1626570"
  },
  {
    "text": "and create the persistent volume that's been created and let's also go ahead and create the persistent volume claim and",
    "start": "1626570",
    "end": "1633050"
  },
  {
    "text": "see what what has now happened so you see that the persistent volume has been",
    "start": "1633050",
    "end": "1639050"
  },
  {
    "text": "created and it has been bound to the claim and so now when the pod comes",
    "start": "1639050",
    "end": "1644600"
  },
  {
    "text": "along and it requests the storage the claim is already there the volumes already there you know the storage is",
    "start": "1644600",
    "end": "1650540"
  },
  {
    "text": "already provisioned and the pod can now directly write you know kubernetes will",
    "start": "1650540",
    "end": "1655580"
  },
  {
    "text": "mount the volume to the pod and the pod can die and move to a different node and kubernetes will automatically take care",
    "start": "1655580",
    "end": "1661580"
  },
  {
    "text": "of remounting the storage to the new node and the data will still be there",
    "start": "1661580",
    "end": "1666800"
  },
  {
    "text": "because the PV C remains the PV C is a constant and and holds that claim to the",
    "start": "1666800",
    "end": "1673760"
  },
  {
    "text": "storage okay let's clean this up and then I want to show you and so I've just",
    "start": "1673760",
    "end": "1681260"
  },
  {
    "text": "deleted the claim and when I've since I've set the policy to delete I'm gonna go ahead and show you that in fact the",
    "start": "1681260",
    "end": "1688370"
  },
  {
    "text": "disk also has been deleted so now we're back to the original cluster and I'm going to show you dynamic storage",
    "start": "1688370",
    "end": "1694400"
  },
  {
    "text": "provisioning and how simple that is so in this manual provisioning case you",
    "start": "1694400",
    "end": "1701570"
  },
  {
    "text": "know there was a storage admin that had to come along and pre provision the storage and you know there was some",
    "start": "1701570",
    "end": "1708140"
  },
  {
    "text": "communication between the user the app admin and the storage admin to know you",
    "start": "1708140",
    "end": "1713630"
  },
  {
    "text": "know that that that that storage had been provisioned in dynamic storage provisioning we want to really kind of",
    "start": "1713630",
    "end": "1719210"
  },
  {
    "text": "make them independent so that the user does not have to even communicate with",
    "start": "1719210",
    "end": "1724340"
  },
  {
    "text": "the storage admin so first let me introduce storage classes I talked about",
    "start": "1724340",
    "end": "1730160"
  },
  {
    "text": "that a little bit we're going to have the cluster admin or the storage admin",
    "start": "1730160",
    "end": "1735400"
  },
  {
    "text": "create a storage class in this case it's called storage class fast and it's an",
    "start": "1735400",
    "end": "1740890"
  },
  {
    "text": "SSD PD note that we're not actually provisioning the the storage we're just",
    "start": "1740890",
    "end": "1746110"
  },
  {
    "text": "the admin is just specifying that I have something called fast we're going to create this storage class and take a",
    "start": "1746110",
    "end": "1752530"
  },
  {
    "text": "look yes and we've got the fast storage class there's also a default storage class because this is a 1.6 cluster",
    "start": "1752530",
    "end": "1758980"
  },
  {
    "text": "standard PBE is the default storage class and so now we look at the",
    "start": "1758980",
    "end": "1767650"
  },
  {
    "text": "persistent volume claim for dynamic storage and it's it's called and we you know my PVC fast and I've requested 10",
    "start": "1767650",
    "end": "1775510"
  },
  {
    "text": "gigs for my pod and I've said I want you know whatever the fast storage classes note that so far nothing has been",
    "start": "1775510",
    "end": "1780880"
  },
  {
    "text": "provisioned there's no pre provision storage no resources lying around waiting for this application to come up and so when I create the PVC let's see",
    "start": "1780880",
    "end": "1791320"
  },
  {
    "text": "what has happened not only did I create the PVC but the PVC has automatically",
    "start": "1791320",
    "end": "1798130"
  },
  {
    "text": "dynamically created a PV the storage admin didn't have to do that and it has",
    "start": "1798130",
    "end": "1803230"
  },
  {
    "text": "also bound the PV to the PVC and if we take a look you will see that the disk",
    "start": "1803230",
    "end": "1811300"
  },
  {
    "text": "has automatically been provisioned as well so this is dynamic storage provisioning the benefit is that it's",
    "start": "1811300",
    "end": "1817540"
  },
  {
    "text": "just in time when you need the storage that's when it gets provisioned there's no wastage and then it creates a",
    "start": "1817540",
    "end": "1824050"
  },
  {
    "text": "separation of concerns you need to call the storage admin or know about the storage admin and this is as it should be",
    "start": "1824050",
    "end": "1829360"
  },
  {
    "text": "kubernetes should take care of the underlying infrastructure for you so now we're gonna clean up and the PV has been",
    "start": "1829360",
    "end": "1839560"
  },
  {
    "text": "released and then by deleting the PVC the disk has also been deleted we'll go",
    "start": "1839560",
    "end": "1846400"
  },
  {
    "text": "back to the slides and I'll conclude with a little bit more about our roadmap so you know what you saw in the demo was",
    "start": "1846400",
    "end": "1854290"
  },
  {
    "start": "1847000",
    "end": "2020000"
  },
  {
    "text": "dynamic storage provisioning in 1.6 there's a default storage class for each",
    "start": "1854290",
    "end": "1859810"
  },
  {
    "text": "of the of these cloud environments and you can see what the default",
    "start": "1859810",
    "end": "1865240"
  },
  {
    "text": "is so we saw that for gcep D was what we used for for Google cloud there's also",
    "start": "1865240",
    "end": "1872380"
  },
  {
    "text": "support for user written and user run dynamic PV provisioners which is very nice that allows us to expand the the",
    "start": "1872380",
    "end": "1879640"
  },
  {
    "text": "range of storage supporting the last thing I'm gonna touch on is our overall",
    "start": "1879640",
    "end": "1885640"
  },
  {
    "text": "roadmap and I think with 1.6 you can see our direction supporting multiple workloads multiple teams in larger",
    "start": "1885640",
    "end": "1893380"
  },
  {
    "text": "clusters so that you can get the maximum efficiency and we're going to continue that that progress our back is going to",
    "start": "1893380",
    "end": "1901090"
  },
  {
    "text": "move to default we're also going to add we've got two network policy that's going to move to stable that allows pods",
    "start": "1901090",
    "end": "1908409"
  },
  {
    "text": "to say you know this pod should only have access to this part of the network and so that you know not anybody can",
    "start": "1908409",
    "end": "1917140"
  },
  {
    "text": "reach that pod stateful application support we're going to continue and automate stateful upgrades we've just",
    "start": "1917140",
    "end": "1925690"
  },
  {
    "text": "added alpha support for GPUs and 1.6 and we're going to continue that a lot of users are starting to run machine",
    "start": "1925690",
    "end": "1931450"
  },
  {
    "text": "learning and and batch and they've been using batch workloads on kubernetes so that expands that Multi workload",
    "start": "1931450",
    "end": "1938590"
  },
  {
    "text": "scheduling which as beta will also graduate and add further features in",
    "start": "1938590",
    "end": "1943720"
  },
  {
    "text": "terms of extensibility you know at the beginning here in our keynote we saw",
    "start": "1943720",
    "end": "1948940"
  },
  {
    "text": "that there were two different runtimes donated as part of the CNC F now the",
    "start": "1948940",
    "end": "1954279"
  },
  {
    "text": "container runtime an interface is a way for kubernetes to incorporate different",
    "start": "1954279",
    "end": "1959590"
  },
  {
    "text": "runtimes and we we have in this release beta support for the docker runtime but",
    "start": "1959590",
    "end": "1966340"
  },
  {
    "text": "in future releases we will be adding container container runtime interfaces",
    "start": "1966340",
    "end": "1971830"
  },
  {
    "text": "for other runtimes as well and lastly",
    "start": "1971830",
    "end": "1977039"
  },
  {
    "text": "Service Catalog this is a new initiative as a you know a couple releases ago it",
    "start": "1977039",
    "end": "1983289"
  },
  {
    "text": "allows kubernetes to consume and communicate with services outside the cluster and as part of this we've been",
    "start": "1983289",
    "end": "1991539"
  },
  {
    "text": "using the open service broker API that a group of companies is working to",
    "start": "1991539",
    "end": "1998110"
  },
  {
    "text": "establish as a standard in the space this is just some of the roadmap but it gives you a sense of the overall themes",
    "start": "1998110",
    "end": "2004560"
  },
  {
    "text": "and again this is in development and subject to change with that I'd like to conclude and say thank you and welcome",
    "start": "2004560",
    "end": "2012030"
  },
  {
    "text": "to the other talks at cubed [Applause]",
    "start": "2012030",
    "end": "2017450"
  },
  {
    "text": "[Applause]",
    "start": "2019700",
    "end": "2022149"
  }
]