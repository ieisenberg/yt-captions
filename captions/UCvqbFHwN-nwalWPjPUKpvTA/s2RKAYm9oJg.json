[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "hello everyone my name is marcin i am a tech lead of the auto scaling team at",
    "start": "240",
    "end": "5730"
  },
  {
    "text": "Google and magic I'm a software engineer at Delta scaling team at Google and",
    "start": "5730",
    "end": "13500"
  },
  {
    "text": "today we would like you to give a deep dive into the blue sea of kubernetes",
    "start": "13500",
    "end": "19529"
  },
  {
    "text": "auto-scaling and the seek introduction two days ago we discussed that what auto",
    "start": "19529",
    "end": "25800"
  },
  {
    "start": "21000",
    "end": "38000"
  },
  {
    "text": "scaling really is what you can use to scale your application and more",
    "start": "25800",
    "end": "32488"
  },
  {
    "text": "specifically for application auto scaling we mentioned about two options that you may use the first option was",
    "start": "32489",
    "end": "39350"
  },
  {
    "start": "38000",
    "end": "78000"
  },
  {
    "text": "horizontal scaling in horizontal and scaling you change the number of your",
    "start": "39350",
    "end": "44820"
  },
  {
    "text": "application replicas based on the traffic when the traffic increase you add some instances and then when the",
    "start": "44820",
    "end": "52469"
  },
  {
    "text": "traffic decrease you remove them of course you don't want to remove and add",
    "start": "52469",
    "end": "58800"
  },
  {
    "text": "this instances manual for that we have a nice feature in kubernetes called",
    "start": "58800",
    "end": "63870"
  },
  {
    "text": "horizontal pot autoscaler that is built in the core of our system horizontal put",
    "start": "63870",
    "end": "71430"
  },
  {
    "text": "out the scanner was covered in details on two talks yesterday so I will not spend more time on it the other option",
    "start": "71430",
    "end": "79500"
  },
  {
    "start": "78000",
    "end": "108000"
  },
  {
    "text": "is to scale your application vertically with vertical auto scaling you either increase or decrease the size of",
    "start": "79500",
    "end": "86430"
  },
  {
    "text": "instances based on the current or past load for quite a long time there was no",
    "start": "86430",
    "end": "92130"
  },
  {
    "text": "automatic support for this type of activity in kubernetes however this",
    "start": "92130",
    "end": "97439"
  },
  {
    "text": "spring we managed to launch an alpha version of vertical pod autoscaler which",
    "start": "97439",
    "end": "103740"
  },
  {
    "text": "is hope to do the size setting automatically for you",
    "start": "103740",
    "end": "108920"
  },
  {
    "text": "vertical pod autoscaler is not only about the reaction to the changes in load one of the reasons why we started",
    "start": "108920",
    "end": "115920"
  },
  {
    "text": "this project was that many users had no idea how to set their pot and container",
    "start": "115920",
    "end": "121799"
  },
  {
    "text": "sizes right this didn't set them at all or they set them too small or too big or",
    "start": "121799",
    "end": "127799"
  },
  {
    "text": "only partially vertical pot autoscaler is hoped to help in this case as well to explain how",
    "start": "127799",
    "end": "136450"
  },
  {
    "start": "134000",
    "end": "166000"
  },
  {
    "text": "vertical cut autoscaler does its jobs let's start with the API as VP is in",
    "start": "136450",
    "end": "142000"
  },
  {
    "text": "alpha the API itself is currently available as custom resource definition this means that it may change a little",
    "start": "142000",
    "end": "149230"
  },
  {
    "text": "bit in the future this also means that we need your feedback to make it great",
    "start": "149230",
    "end": "156239"
  },
  {
    "text": "so if you have any questions comments concerns on nice idea please reach reach",
    "start": "156239",
    "end": "163150"
  },
  {
    "text": "to us at the end of a presentation at the very top level of vertical path",
    "start": "163150",
    "end": "169360"
  },
  {
    "start": "166000",
    "end": "379000"
  },
  {
    "text": "autoscaler looks exactly as any other object in kubernetes there is a type there is a name and this vertical",
    "start": "169360",
    "end": "176440"
  },
  {
    "text": "product is carrot belongs to some namespace it can have some annotations more importantly it contains",
    "start": "176440",
    "end": "182970"
  },
  {
    "text": "specification and status which are we'll cover right now let's start with the",
    "start": "182970",
    "end": "188829"
  },
  {
    "text": "specification in specification we have three values the first one is the",
    "start": "188829",
    "end": "194530"
  },
  {
    "text": "selector with it you can tell the vertical path autoscaler which pulse you would like to outer scales containers",
    "start": "194530",
    "end": "201910"
  },
  {
    "text": "with the same name in this pod selected with the selector well because the dirt",
    "start": "201910",
    "end": "207160"
  },
  {
    "text": "as equivalent and will receive the same recommendations the next item is the",
    "start": "207160",
    "end": "214389"
  },
  {
    "text": "update policy it tells what exactly you would like vertical port autoscaler to",
    "start": "214389",
    "end": "219880"
  },
  {
    "text": "do if you would like it to only give you recommendations and not update anything",
    "start": "219880",
    "end": "225400"
  },
  {
    "text": "you can set it to off if you want it to update the recommendations only when the",
    "start": "225400",
    "end": "232150"
  },
  {
    "text": "pods are created you can set it to initial and if you want to trust it to",
    "start": "232150",
    "end": "237880"
  },
  {
    "text": "the end and turn on vertical path autoscaler at full speed you can set it into the outer mode in this mode a",
    "start": "237880",
    "end": "245019"
  },
  {
    "text": "vertical pot autoscaler will apply its recommendation both to the newly created pots as well as to the posts that are",
    "start": "245019",
    "end": "252730"
  },
  {
    "text": "currently existing in the cluster the",
    "start": "252730",
    "end": "258130"
  },
  {
    "text": "third element in the specification allows you to decide which containers in your post you would like to auto scale",
    "start": "258130",
    "end": "265160"
  },
  {
    "text": "for these containers which which are specified by the name you can specify the minimum and maximum amount of",
    "start": "265160",
    "end": "271730"
  },
  {
    "text": "disorder sources you would like them to give you can also have put a wild card",
    "start": "271730",
    "end": "277670"
  },
  {
    "text": "and Asterix in the name and then have everything in your pot Auto scaled in such a case you may also want to exclude",
    "start": "277670",
    "end": "284990"
  },
  {
    "text": "some of these containers from auto scaling to do it you can specify the",
    "start": "284990",
    "end": "290030"
  },
  {
    "text": "mode to off and then these containers that you explicitly named will not be",
    "start": "290030",
    "end": "295730"
  },
  {
    "text": "out of scale at all okay so we know what",
    "start": "295730",
    "end": "301100"
  },
  {
    "text": "we can Berta called pot autoscaler do for you now this is the time to show how",
    "start": "301100",
    "end": "306980"
  },
  {
    "text": "it presents its status again we have preference here the first one is three",
    "start": "306980",
    "end": "314240"
  },
  {
    "text": "but it says when vertical pot autoscaler updated its recommendation for the last",
    "start": "314240",
    "end": "321350"
  },
  {
    "text": "time the second one tells us where there are all the components of vertical pot",
    "start": "321350",
    "end": "327080"
  },
  {
    "text": "autoscaler work correctly or not is there any if there are any problems and",
    "start": "327080",
    "end": "332500"
  },
  {
    "text": "the third fields contains the list of vertical put autoscaler recommendations",
    "start": "332500",
    "end": "337670"
  },
  {
    "text": "the recommendation structure obviously contained the container name however the",
    "start": "337670",
    "end": "343220"
  },
  {
    "text": "three remaining elements are a little bit more interesting the first one is the target this is the set of resources",
    "start": "343220",
    "end": "350750"
  },
  {
    "text": "that work with a pot autoscaler will set on your pots the remaining tools are",
    "start": "350750",
    "end": "358070"
  },
  {
    "text": "used to determine whether pots that don't necessarily have the exact target",
    "start": "358070",
    "end": "364810"
  },
  {
    "text": "resources that are okay or not if the pots are beyond this min and max range",
    "start": "364810",
    "end": "373669"
  },
  {
    "text": "they are the subject for the update okay",
    "start": "373669",
    "end": "380030"
  },
  {
    "start": "379000",
    "end": "389000"
  },
  {
    "text": "so hopefully everyone now understands the API let's take a look what does it",
    "start": "380030",
    "end": "386050"
  },
  {
    "text": "how does it work in practice probably the most tricky part of the design is",
    "start": "386050",
    "end": "392240"
  },
  {
    "start": "389000",
    "end": "433000"
  },
  {
    "text": "how we update pot requests as you may know deployments for example contain",
    "start": "392240",
    "end": "398039"
  },
  {
    "text": "historic of all changes that were made to them so if you update the deployment",
    "start": "398039",
    "end": "403710"
  },
  {
    "text": "and you set new requests on them it is also written to the history it may take a while until vertical pod autoscaler it",
    "start": "403710",
    "end": "411960"
  },
  {
    "text": "gets a good grasp on the right pot sizes in your deployment and probably you",
    "start": "411960",
    "end": "418020"
  },
  {
    "text": "don't want these changes to pollute your history for that reason we don't change",
    "start": "418020",
    "end": "424619"
  },
  {
    "text": "the deployment definition do while we apply the recommendations we use",
    "start": "424619",
    "end": "430800"
  },
  {
    "text": "something slightly different instead of overriding font request we",
    "start": "430800",
    "end": "437539"
  },
  {
    "start": "433000",
    "end": "443000"
  },
  {
    "text": "instead of setting pot request in deployment we override them during the admission phase",
    "start": "437539",
    "end": "443449"
  },
  {
    "start": "443000",
    "end": "500000"
  },
  {
    "text": "whenever a new object is created in kubernetes it goes through a chain of",
    "start": "443449",
    "end": "449009"
  },
  {
    "text": "admission plugins these are national plugins I check whether the object can",
    "start": "449009",
    "end": "454469"
  },
  {
    "text": "be created at all and possibly apply some small changes to them the same",
    "start": "454469",
    "end": "459899"
  },
  {
    "text": "applies to the pots when the pot is created is checked and since when the",
    "start": "459899",
    "end": "465419"
  },
  {
    "text": "time person it can also be a subject of an external admission plugin we use this",
    "start": "465419",
    "end": "471029"
  },
  {
    "text": "technology in vertical pataskala so when we create a pot it goes through a",
    "start": "471029",
    "end": "476969"
  },
  {
    "text": "provision plug-in chain and it also goes to our plugin in this plugin we check",
    "start": "476969",
    "end": "482789"
  },
  {
    "text": "what are the currently configured vertical patata scalars what are their recommendations and if the new pot is",
    "start": "482789",
    "end": "490409"
  },
  {
    "text": "the subject of some of these vertical proto scalars we update this request and then let the",
    "start": "490409",
    "end": "496349"
  },
  {
    "text": "pot go through the rest of the chain",
    "start": "496349",
    "end": "499849"
  },
  {
    "start": "500000",
    "end": "506000"
  },
  {
    "text": "calculation area of the recommendation is an important part of the vertical pot autoscaler recommendations are",
    "start": "501439",
    "end": "508680"
  },
  {
    "start": "506000",
    "end": "545000"
  },
  {
    "text": "calculated based on the observed real world usage for a vertical about how the",
    "start": "508680",
    "end": "514560"
  },
  {
    "text": "scanner record a vertical part of the scalar component periodically they ask the metric server for Mary hipster for",
    "start": "514560",
    "end": "522479"
  },
  {
    "text": "the current use of the containers it also monitors the car",
    "start": "522479",
    "end": "528150"
  },
  {
    "text": "and set of vertical pot autoscaler objects and based on that it calculates",
    "start": "528150",
    "end": "535410"
  },
  {
    "text": "what the size of the containers should be and to put this recommendation to VPA",
    "start": "535410",
    "end": "543570"
  },
  {
    "text": "status field if you allow a vertical pot autoscaler to go in the full automatic",
    "start": "543570",
    "end": "550650"
  },
  {
    "text": "mode it will try to update the existing pots currently the only way to update an",
    "start": "550650",
    "end": "556860"
  },
  {
    "text": "existing pot is to shoot it down and kill it so vertical pot autoscaler",
    "start": "556860",
    "end": "562650"
  },
  {
    "text": "constantly analyzes both the set of pots and the set of vertical pot autoscaler",
    "start": "562650",
    "end": "569010"
  },
  {
    "text": "and if any pot has size outside of this",
    "start": "569010",
    "end": "574230"
  },
  {
    "text": "mean and match boundary it is killed by the updater we do killing one by one so",
    "start": "574230",
    "end": "580680"
  },
  {
    "text": "eventually all of the pods that are outside of the recommended values will be updated by the admission plugins",
    "start": "580680",
    "end": "590360"
  },
  {
    "text": "that's all for the VPI you can find more information on how to install and use it",
    "start": "590660",
    "end": "596700"
  },
  {
    "text": "in our github repository and if you try it please please let us know whether you",
    "start": "596700",
    "end": "603240"
  },
  {
    "text": "like it what's what are your ideas for the improvements and now it's the time",
    "start": "603240",
    "end": "609000"
  },
  {
    "text": "for the second part of the presentation the one about the throat is killer thank",
    "start": "609000",
    "end": "616170"
  },
  {
    "start": "616000",
    "end": "665000"
  },
  {
    "text": "you all right so watching told you all about pulled out of scaling and I'm going to",
    "start": "616170",
    "end": "622410"
  },
  {
    "text": "cover now I'll go scaling so basically making sure that when you out of schedule pods there is still a place to",
    "start": "622410",
    "end": "628590"
  },
  {
    "text": "run them in the cluster and if you scale them down you can actually remove the VM so then don't pay for them and for that",
    "start": "628590",
    "end": "635850"
  },
  {
    "text": "we have a kubernetes component called cluster autoscaler it's in GA since 1.8 and out-of-the-box",
    "start": "635850",
    "end": "644160"
  },
  {
    "text": "we support three cloud providers that would be Google cloud platform both GK",
    "start": "644160",
    "end": "650520"
  },
  {
    "text": "and self-hosted clusters as well and AWS I know that also some folks who have",
    "start": "650520",
    "end": "656820"
  },
  {
    "text": "their own folks of cluster autoscaler and implementation for other crowd eiders but I don't know the complete",
    "start": "656820",
    "end": "663180"
  },
  {
    "text": "list right so let's see how it works and",
    "start": "663180",
    "end": "668760"
  },
  {
    "start": "665000",
    "end": "678000"
  },
  {
    "text": "to actually cover how crossed out of scale works we have to take a step back and talk a bit about the kubernetes",
    "start": "668760",
    "end": "675750"
  },
  {
    "text": "schedule so how schedule of does and",
    "start": "675750",
    "end": "681240"
  },
  {
    "start": "678000",
    "end": "752000"
  },
  {
    "text": "this is a very high-level overview is it has two things really predicate",
    "start": "681240",
    "end": "686310"
  },
  {
    "text": "functions and priority functions so predicate functions check if a given pod could be scheduled on a given node so",
    "start": "686310",
    "end": "693030"
  },
  {
    "text": "this is stuff like do we have enough resources CPU memory and so on to run a given pot on a given node so if any of",
    "start": "693030",
    "end": "700950"
  },
  {
    "text": "those predicates are not true which just simply cannot schedule a port on this",
    "start": "700950",
    "end": "706260"
  },
  {
    "text": "node so this is basically the filtering step and once we do that we actually have a bunch of nodes that could",
    "start": "706260",
    "end": "712980"
  },
  {
    "text": "potentially host our pod and we need to choose one of them and this is well priorities coming those are basically",
    "start": "712980",
    "end": "719690"
  },
  {
    "text": "functions that give a number of how good would it be to put this spot on this",
    "start": "719690",
    "end": "726150"
  },
  {
    "text": "node and they cover things like spreading pods in a single deployment among many nodes or preferred doing",
    "start": "726150",
    "end": "733440"
  },
  {
    "text": "scheduling an T affinity and affinity like this sort of things and then the",
    "start": "733440",
    "end": "738570"
  },
  {
    "text": "final decision is just a linear combination of their scores from the different priorities and what we care",
    "start": "738570",
    "end": "747930"
  },
  {
    "text": "about in cross Delta Skylar is predicates we want to make sure that I am thanks for that nice if you want that",
    "start": "747930",
    "end": "756150"
  },
  {
    "text": "that's his computer I wanted to tell you yes that we have some predicates for stuff like basically checking that we",
    "start": "756150",
    "end": "763320"
  },
  {
    "text": "have enough resources on the node to an hour post all that is a predicate that",
    "start": "763320",
    "end": "770100"
  },
  {
    "text": "checks if there are times and if the pod tolerates those things if the volumes",
    "start": "770100",
    "end": "775680"
  },
  {
    "text": "are in the right zone this of the thing I'm not actually going to list all of them there is I think slightly less than",
    "start": "775680",
    "end": "781950"
  },
  {
    "text": "20 and we need to know about each of them is C now all of them must pass for",
    "start": "781950",
    "end": "788970"
  },
  {
    "text": "the port to schedule on a given node so when the cluster to schedule us and note",
    "start": "788970",
    "end": "795180"
  },
  {
    "text": "to make sure all of those would be true so they're actually helping the pending pause so we didn't really want to",
    "start": "795180",
    "end": "805160"
  },
  {
    "text": "implement all of those predicates all of this logic because that's a lot of logic",
    "start": "805160",
    "end": "810900"
  },
  {
    "text": "we could probably get it wrong and new predicates are constantly added the old ones are changed so what we're doing is",
    "start": "810900",
    "end": "817800"
  },
  {
    "text": "we literally just import the predicate code from the scheduler and bake it into",
    "start": "817800",
    "end": "823800"
  },
  {
    "text": "our binary that's how Christ autoscaler works on some very high level we",
    "start": "823800",
    "end": "830670"
  },
  {
    "text": "simulate what would schedule he'll do if the clustered shape was different and so",
    "start": "830670",
    "end": "836610"
  },
  {
    "start": "834000",
    "end": "885000"
  },
  {
    "text": "let's that was sort of the first concept we have to cover to propel to talk about Christ autoscaler really the other",
    "start": "836610",
    "end": "842910"
  },
  {
    "text": "concept is note groups and a note group is a set of identical notes that can be",
    "start": "842910",
    "end": "848010"
  },
  {
    "text": "resized so on AWS that would be out of scaling group ASG on Google cloud",
    "start": "848010",
    "end": "854760"
  },
  {
    "text": "platform it's manage instance group or me the important point here is that",
    "start": "854760",
    "end": "861630"
  },
  {
    "text": "nodes in a single node group must be identical for all purposes that are",
    "start": "861630",
    "end": "867060"
  },
  {
    "text": "relevant for scheduling so we don't support things like multi zone aSG's but we support auto scaling multiple node",
    "start": "867060",
    "end": "873960"
  },
  {
    "text": "groups so what we were just recommending is create a separate ASG or separate",
    "start": "873960",
    "end": "879060"
  },
  {
    "text": "week in each soul and then it that we that will work just fine right so now",
    "start": "879060",
    "end": "886680"
  },
  {
    "start": "885000",
    "end": "936000"
  },
  {
    "text": "that we have those concepts that I will use a lot during talking about crossed out to scaler let's see what cast out",
    "start": "886680",
    "end": "894210"
  },
  {
    "text": "the scale of does it doesn't look at any matrix it doesn't look at CPU usage the actual",
    "start": "894210",
    "end": "899250"
  },
  {
    "text": "CPU usage because schedule of doesn't look at it just because we have a lot of",
    "start": "899250",
    "end": "904860"
  },
  {
    "text": "spare CPU in our cluster will not help us if the scheduler will not schedule",
    "start": "904860",
    "end": "910890"
  },
  {
    "text": "pause on the nose that has the CPU and also just because the nodes are",
    "start": "910890",
    "end": "916470"
  },
  {
    "text": "basically burning hot because you said the request too low we I mean the",
    "start": "916470",
    "end": "922050"
  },
  {
    "text": "scheduler can still put posters so will not react to that so what we look for our posts that were marked by scheduler",
    "start": "922050",
    "end": "928530"
  },
  {
    "text": "as unschedulable so basically the scheduler said oh yeah that guy do not fit in the cluster so we try to fix that and yeah",
    "start": "928530",
    "end": "937950"
  },
  {
    "text": "let's cover quickly how we do this and unfortunately this is fairly complex but let's try to go to it pretty quickly",
    "start": "937950",
    "end": "946160"
  },
  {
    "text": "so basically crossed out two scalar runs a loop every ten seconds by default and",
    "start": "946160",
    "end": "952080"
  },
  {
    "text": "what it does during this loop is first of all it monitors the health of the cluster we monitor the health of both",
    "start": "952080",
    "end": "959340"
  },
  {
    "text": "crust as a whole and different node groups pretty often there are some problems where we try to scale up but",
    "start": "959340",
    "end": "964950"
  },
  {
    "text": "the node doesn't actually manage the booth order a lot of unhealthy notes maybe there is some Network issue what",
    "start": "964950",
    "end": "971190"
  },
  {
    "text": "we don't want to be doing is just keep adding nodes in this sort of situation because those are still VMs on the cloud",
    "start": "971190",
    "end": "977100"
  },
  {
    "text": "provider that you pay for so we try to figure out if the cluster is healthy and if it",
    "start": "977100",
    "end": "984360"
  },
  {
    "text": "isn't what we can do about it if there is just a single node that failed to boot up or is unhealthy we may try to",
    "start": "984360",
    "end": "991650"
  },
  {
    "text": "recreate it we'll just delete it and create a new one so crassly autoscaler",
    "start": "991650",
    "end": "996750"
  },
  {
    "text": "has some limited outer healing capabilities but if we figure out that",
    "start": "996750",
    "end": "1001850"
  },
  {
    "text": "basically everything is broken at some point cast out the skeletal just give up to avoid creating an even bigger mess so",
    "start": "1001850",
    "end": "1008810"
  },
  {
    "text": "that's sort of high-level overview of this part of the logic and once we know",
    "start": "1008810",
    "end": "1013930"
  },
  {
    "text": "that the crust is in a healthy shape we will look for the unschedulable pods the",
    "start": "1013930",
    "end": "1019610"
  },
  {
    "text": "posted failed to run schedule we do some heuristic to filter out those parts that",
    "start": "1019610",
    "end": "1026720"
  },
  {
    "text": "can actually be scheduled but maybe wasn't scheduled yet and so on so it we try to avoid traces with scheduler and",
    "start": "1026720",
    "end": "1035230"
  },
  {
    "start": "1033000",
    "end": "1082000"
  },
  {
    "text": "once we have them the real auto-scaling starts up and that's where it gets complex so what we try to do is we have",
    "start": "1035290",
    "end": "1043130"
  },
  {
    "text": "this set of node groups that will auto scaling and then have a set of posts that need help so for each node group",
    "start": "1043130",
    "end": "1049760"
  },
  {
    "text": "we'd create a template node it is basically an anode object in class the autoscaler memory and we try to fit",
    "start": "1049760",
    "end": "1057290"
  },
  {
    "text": "ports on this node to see which ports could possibly be helped by",
    "start": "1057290",
    "end": "1063049"
  },
  {
    "text": "link-up a given node group and this is where we use the scheduler code because I mean we actually don't have the",
    "start": "1063049",
    "end": "1069919"
  },
  {
    "text": "scheduling clergy as something we know about in a cluster autoscaler we just import it as a black box thing just give",
    "start": "1069919",
    "end": "1076460"
  },
  {
    "text": "it at node object a bunch of ports see what happens right so once we know that",
    "start": "1076460",
    "end": "1084460"
  },
  {
    "start": "1082000",
    "end": "1138000"
  },
  {
    "text": "we have basically set of like we have a",
    "start": "1084460",
    "end": "1089960"
  },
  {
    "text": "node group and a set of ports that could possibly fit on an empty node in this node group so we know we could outer",
    "start": "1089960",
    "end": "1096139"
  },
  {
    "text": "scale add some notes to this node group and that will help thee spots but we don't know how many nodes we actually",
    "start": "1096139",
    "end": "1102499"
  },
  {
    "text": "need so we ran some basically greedy bin packing algorithm with a bunch of",
    "start": "1102499",
    "end": "1108710"
  },
  {
    "text": "heuristics on top to figure out how many nodes we need we know that each of the pods would fit on an empty node and we",
    "start": "1108710",
    "end": "1115279"
  },
  {
    "text": "just try to bin pack by adding new nodes until we get all the pods scheduled in",
    "start": "1115279",
    "end": "1121669"
  },
  {
    "text": "our simulation and we do that for every node group so I guess the point I'm",
    "start": "1121669",
    "end": "1126769"
  },
  {
    "text": "trying to make here is we're doing what the scheduler does but we do it a lot more so we have a constant battle with",
    "start": "1126769",
    "end": "1134320"
  },
  {
    "text": "performance whenever new futures come to schedule and once we have all that",
    "start": "1134320",
    "end": "1141169"
  },
  {
    "start": "1138000",
    "end": "1224000"
  },
  {
    "text": "information we basically have a set of options and the option consists of a",
    "start": "1141169",
    "end": "1147499"
  },
  {
    "text": "node group consists of a set of ports and the number of nodes and that",
    "start": "1147499",
    "end": "1153529"
  },
  {
    "text": "basically means if we add like two nodes to this node group we'll had help this set of pots",
    "start": "1153529",
    "end": "1158929"
  },
  {
    "text": "if we had five nodes to this other node group we'll have helped this other set of posts and so on so now we need to",
    "start": "1158929",
    "end": "1167029"
  },
  {
    "text": "make a decision which of those options we want to choose and this is basically",
    "start": "1167029",
    "end": "1172580"
  },
  {
    "text": "the most I guess pluggable part of classes autoscaler we have a bunch of different heuristics here those are",
    "start": "1172580",
    "end": "1179210"
  },
  {
    "text": "called expanders you can actually choose them with a flag you're free to implement your own one I think the best",
    "start": "1179210",
    "end": "1186799"
  },
  {
    "text": "one that we like the most is pricing schedule pricing expander which",
    "start": "1186799",
    "end": "1192590"
  },
  {
    "text": "basically takes tries to optimize long Tellem cost of your cluster but",
    "start": "1192590",
    "end": "1198770"
  },
  {
    "text": "unfortunately that's only available for GCP now and the other expanders would be",
    "start": "1198770",
    "end": "1204700"
  },
  {
    "text": "much simpler how they stick my blacklist waste which is just waste the least amount of resources here and now just",
    "start": "1204700",
    "end": "1211400"
  },
  {
    "text": "sum up CPU and memory that is unused after the Skylab and we also have a random one but I do not recommend using",
    "start": "1211400",
    "end": "1217430"
  },
  {
    "text": "that and it's pretty easy to actually write one of those and this is the whole",
    "start": "1217430",
    "end": "1226010"
  },
  {
    "text": "scale up but we still have to cover the scale down fortunately that's going to",
    "start": "1226010",
    "end": "1231440"
  },
  {
    "text": "be faster basically to scale down what we do is just we look for notes that are",
    "start": "1231440",
    "end": "1237440"
  },
  {
    "text": "unneeded the notes that are either completely empty or underutilized and the node is an unneeded well I mean it",
    "start": "1237440",
    "end": "1246410"
  },
  {
    "text": "can be completely empty that's easy but if there are some poles we just apply a set of criteria to the node to see if",
    "start": "1246410",
    "end": "1254210"
  },
  {
    "text": "it's unneeded it boils down really to it has low resource utilization and we",
    "start": "1254210",
    "end": "1261470"
  },
  {
    "text": "think all the pods there are safe to move somewhere else in the cluster so we will not delete a node just because it's",
    "start": "1261470",
    "end": "1267380"
  },
  {
    "text": "it has low utilization because maybe you have five nodes that are hundred percent used and then one node that is like five",
    "start": "1267380",
    "end": "1273560"
  },
  {
    "text": "percent used if we delete that you end up with this five percent that have nowhere to go",
    "start": "1273560",
    "end": "1278660"
  },
  {
    "text": "so basically for each pot once again we'll use scheduler code and we try to",
    "start": "1278660",
    "end": "1285080"
  },
  {
    "text": "figure out if it could be scheduled on any of other nodes and then there are samples we don't like to disturb because",
    "start": "1285080",
    "end": "1291590"
  },
  {
    "text": "maybe they have local storage so there would be some state that you would loss we also don't restart cube system ports",
    "start": "1291590",
    "end": "1297470"
  },
  {
    "text": "because we really don't want to restart stuff like DNS or hipster because this will cause potentially disruptions to",
    "start": "1297470",
    "end": "1303950"
  },
  {
    "text": "your service and finally we let our users opt out of basically just say",
    "start": "1303950",
    "end": "1309440"
  },
  {
    "text": "other labor actually sanitation add an annotation to the port saying oh this",
    "start": "1309440",
    "end": "1315560"
  },
  {
    "text": "port cannot be evicted or other poor disruption budget saying oh only one of those can not be not running",
    "start": "1315560",
    "end": "1322700"
  },
  {
    "text": "at a given time so we will expect that and if all of those conditions are true",
    "start": "1322700",
    "end": "1328370"
  },
  {
    "text": "we basically say the notice underneath and even note is continuously unneeded",
    "start": "1328370",
    "end": "1333539"
  },
  {
    "text": "for 10 minutes by default you can change that with flags we will delete it and",
    "start": "1333539",
    "end": "1339479"
  },
  {
    "text": "that's basically a very high overview of how Christ out of scale works and I feel",
    "start": "1339479",
    "end": "1345839"
  },
  {
    "start": "1342000",
    "end": "1406000"
  },
  {
    "text": "like that was very quick for quite complex logic and there are actually a lot of misconceptions so I try to put",
    "start": "1345839",
    "end": "1352259"
  },
  {
    "text": "the most common misconceptions as a form of a summary so this is what crossed out",
    "start": "1352259",
    "end": "1357869"
  },
  {
    "text": "to scaler and by the way CA is how we usually abbreviate that thus it looks at",
    "start": "1357869",
    "end": "1363239"
  },
  {
    "text": "the state of the cluster which really means notes and pods it looks for",
    "start": "1363239",
    "end": "1368249"
  },
  {
    "text": "pending pods this is actually the signal that we use we simulate the scheduler and we actually miss that part so it's",
    "start": "1368249",
    "end": "1376169"
  },
  {
    "text": "good I have summary the way we actually do things is we resize node willips so we are basically telling this node we",
    "start": "1376169",
    "end": "1386190"
  },
  {
    "text": "love this crowd provider underlying concept to change the site we're actually not creating node objects with",
    "start": "1386190",
    "end": "1393269"
  },
  {
    "text": "not the configuring nodes creating them all we do is just call an api that says",
    "start": "1393269",
    "end": "1400440"
  },
  {
    "text": "for this node group please add two more notes or remove this node this is all we",
    "start": "1400440",
    "end": "1405659"
  },
  {
    "text": "do and that's actually also in this other slide which says what crossed out",
    "start": "1405659",
    "end": "1411239"
  },
  {
    "start": "1406000",
    "end": "1470000"
  },
  {
    "text": "the scallop doesn't do and a lot of people think it does it as I said we don't do anything with knows other than",
    "start": "1411239",
    "end": "1419219"
  },
  {
    "text": "telling them before the duration we don't put any labels any times we don't configure them in any way we don't look",
    "start": "1419219",
    "end": "1427739"
  },
  {
    "text": "at actual resource usage all we operate on a spot requests just a scheduler and",
    "start": "1427739",
    "end": "1433369"
  },
  {
    "text": "because of how we import scheduler code and we actually need it back into our binary we do not support any form of",
    "start": "1433369",
    "end": "1440609"
  },
  {
    "text": "custom scheduling right if you write our own schedule let's not support it if you use any",
    "start": "1440609",
    "end": "1446599"
  },
  {
    "text": "scheduler extender any web hooks any basically customization mechanism",
    "start": "1446599",
    "end": "1452639"
  },
  {
    "text": "provided by stick scheduling we have no way of supporting this and we don't do",
    "start": "1452639",
    "end": "1458789"
  },
  {
    "text": "any predictive auto scaling what I described is just purely reactive we see a body spending it",
    "start": "1458789",
    "end": "1464280"
  },
  {
    "text": "cannot be scheduled we fix that we don't try to predict that at least for now",
    "start": "1464280",
    "end": "1471110"
  },
  {
    "start": "1470000",
    "end": "1491000"
  },
  {
    "text": "and that's basically my quick overview of how class level scale works and let's",
    "start": "1471110",
    "end": "1476820"
  },
  {
    "text": "click quickly try to cover what you can do if it doesn't work which usually how",
    "start": "1476820",
    "end": "1482250"
  },
  {
    "text": "it looks like is you have this pending pot and well it's like there is no new notes coming across the autoscaler is",
    "start": "1482250",
    "end": "1488610"
  },
  {
    "text": "not working it's not scaling up why could that be so the first thing to look",
    "start": "1488610",
    "end": "1493770"
  },
  {
    "start": "1491000",
    "end": "1557000"
  },
  {
    "text": "at is the crust level to scale at status config map we write this on every loop",
    "start": "1493770",
    "end": "1499770"
  },
  {
    "text": "and this is basically cluster status as perceived by crustal to scalar and",
    "start": "1499770",
    "end": "1506480"
  },
  {
    "text": "basically there is going to be a cluster white status so the status of whole cluster and the status of each node",
    "start": "1506480",
    "end": "1512700"
  },
  {
    "text": "group so if a node group is for example unhealthy we will not scale it up and a",
    "start": "1512700",
    "end": "1517980"
  },
  {
    "text": "node will be unhealthy too many nodes are broken if X 80% of nodes are broken we think the whole thing is broken we",
    "start": "1517980",
    "end": "1524370"
  },
  {
    "text": "will not touch it if maybe you can see the scale-up status and it if it says in",
    "start": "1524370",
    "end": "1531690"
  },
  {
    "text": "progress that means we actually already created a VM it's just putting up and we cannot really do anything about it other",
    "start": "1531690",
    "end": "1537390"
  },
  {
    "text": "than wait if it says back off that means we had a scale up on this group that felt recently and we put it in",
    "start": "1537390",
    "end": "1543840"
  },
  {
    "text": "exponential back-off to avoid crashing so in general this is the first thing to",
    "start": "1543840",
    "end": "1550350"
  },
  {
    "text": "look at and if you have a create an issue against autoscaler repo please include that it's very useful the other",
    "start": "1550350",
    "end": "1558360"
  },
  {
    "start": "1557000",
    "end": "1617000"
  },
  {
    "text": "thing is events we actually write a lot of events first of all whatever we do and I think whenever we scale up node",
    "start": "1558360",
    "end": "1564990"
  },
  {
    "text": "group or scale it down we will write an event we put events on pot saying this",
    "start": "1564990",
    "end": "1570210"
  },
  {
    "text": "pot has triggered scale up the disk scale up was because of this pot while we put an even saying this pot didn't",
    "start": "1570210",
    "end": "1576210"
  },
  {
    "text": "trigger scale up we notice it's bending we have not found any possible way to scale up your cluster that would",
    "start": "1576210",
    "end": "1582270"
  },
  {
    "text": "actually help this pot so I mean there is nothing we can do we just put an even to let you know and I don't think I have",
    "start": "1582270",
    "end": "1590130"
  },
  {
    "text": "a slide for that but we also have a Prometheus matrix endpoint so that is also potentially useful for monitoring",
    "start": "1590130",
    "end": "1596370"
  },
  {
    "text": "and we put the matrix both about grass the autoscaler status but also about basically class",
    "start": "1596370",
    "end": "1602990"
  },
  {
    "text": "the status as upset by class delta scalars so how many nodes we think are",
    "start": "1602990",
    "end": "1608630"
  },
  {
    "text": "needed how many pending posts you have how many parts we have evicted during scale downs",
    "start": "1608630",
    "end": "1613880"
  },
  {
    "text": "and so on and so forth and yes for more",
    "start": "1613880",
    "end": "1619160"
  },
  {
    "start": "1617000",
    "end": "1639000"
  },
  {
    "text": "info the crust Altos Khalid leaves in the same deposit RSVP a we actually have a very large FAQ file which have built",
    "start": "1619160",
    "end": "1626870"
  },
  {
    "text": "over the years basically now and it covers how to do a lot of specific configurations how to set up specific",
    "start": "1626870",
    "end": "1633770"
  },
  {
    "text": "things that you may need and also it gives some overview over how the this whole thing works other than that this",
    "start": "1633770",
    "end": "1643130"
  },
  {
    "start": "1639000",
    "end": "1664000"
  },
  {
    "text": "is a sick talk so we have Sigma T Sigma ting at 4:00 p.m. UTC on Mondays please",
    "start": "1643130",
    "end": "1651590"
  },
  {
    "text": "join us it's not very cloud that you will clearly have a chance to say something we're pretty active on the",
    "start": "1651590",
    "end": "1658370"
  },
  {
    "text": "stick out of scaling slack Channel I try to respond by proving a day at most and",
    "start": "1658370",
    "end": "1665830"
  },
  {
    "start": "1664000",
    "end": "1779000"
  },
  {
    "text": "that's it thank you",
    "start": "1665830",
    "end": "1669669"
  },
  {
    "text": "right do you have any questions thanks I",
    "start": "1672870",
    "end": "1679419"
  },
  {
    "text": "just had a quick question about the vertical part of this gala does it keep track of the set of pods selected by a",
    "start": "1679419",
    "end": "1687190"
  },
  {
    "text": "label selector in like pervy PA object so say if I've got if I've got a set of",
    "start": "1687190",
    "end": "1692860"
  },
  {
    "text": "pods that meets a vertical pod autoscaler CID does it keep track of",
    "start": "1692860",
    "end": "1699070"
  },
  {
    "text": "like the recommendation for those across pods across time so if I've got like a constant trimming of pods coming in are",
    "start": "1699070",
    "end": "1705460"
  },
  {
    "text": "they going to get a value that's based on the previous history of all of the pods yes okay thanks so my question is",
    "start": "1705460",
    "end": "1720909"
  },
  {
    "text": "about note autoscaler so you are speaking about auto scaling group of",
    "start": "1720909",
    "end": "1728950"
  },
  {
    "text": "notes is there possibility to distinguish notes with different groups",
    "start": "1728950",
    "end": "1734490"
  },
  {
    "text": "mmm for example with different resources so when I need more CPU I will increase",
    "start": "1734490",
    "end": "1744669"
  },
  {
    "text": "number of notes using CPU when I need something more from GPO I will spawn",
    "start": "1744669",
    "end": "1753309"
  },
  {
    "text": "only not spawn risk a group of hardware",
    "start": "1753309",
    "end": "1759370"
  },
  {
    "text": "which has access to GPO or SR Iove interfaces when I'm locking on the",
    "start": "1759370",
    "end": "1767409"
  },
  {
    "text": "current point of time is there a possibility to operate on different",
    "start": "1767409",
    "end": "1772840"
  },
  {
    "text": "groups on in the same time yes so yes we",
    "start": "1772840",
    "end": "1780159"
  },
  {
    "start": "1779000",
    "end": "2058000"
  },
  {
    "text": "suppose having multiple no groups at the same time I mentioned quickly the expander which is like if you can think",
    "start": "1780159",
    "end": "1787120"
  },
  {
    "text": "it is basically thankfull choosing which node will hope to expand which sort of like you can with an thanks ace you can",
    "start": "1787120",
    "end": "1794200"
  },
  {
    "text": "have multiple node groups and it's part of cloud provide a specific implementation to implement a function",
    "start": "1794200",
    "end": "1800320"
  },
  {
    "text": "that actually for each node tells us which node will be the belongs to so basically if you were to implement it",
    "start": "1800320",
    "end": "1805720"
  },
  {
    "text": "the cloud provider you will have to write you'll have to write this function and for the cloud providers we support",
    "start": "1805720",
    "end": "1812080"
  },
  {
    "text": "we have the implementation obviously so for each node we hope you can tell in which for each node we can tell in which",
    "start": "1812080",
    "end": "1817510"
  },
  {
    "text": "node we loop it is and the whole scale-up logic is performed for each node group we basically see what we can",
    "start": "1817510",
    "end": "1825370"
  },
  {
    "text": "do for different pods and then we just choose one of those options and if you'll have one port that maybe requires",
    "start": "1825370",
    "end": "1831850"
  },
  {
    "text": "GPU and can only go to the node will prove GPU and the other but maybe this node will be stented so other posts",
    "start": "1831850",
    "end": "1837700"
  },
  {
    "text": "cannot go there we will just trigger two separate scallops one for the GPU group and then one for the idle pots and those",
    "start": "1837700",
    "end": "1844900"
  },
  {
    "text": "are just sort of two separate scallops",
    "start": "1844900",
    "end": "1848820"
  },
  {
    "text": "okay my question about vertical part of the scale as you mentioned you said that",
    "start": "1854610",
    "end": "1859680"
  },
  {
    "text": "you are not changing requests I mean limits requests right sorry you're not",
    "start": "1859680",
    "end": "1868750"
  },
  {
    "text": "during this change in reservist during",
    "start": "1868750",
    "end": "1873790"
  },
  {
    "text": "scale and you are not changing requests right we are not changing requests in the deployment object okay so we don't",
    "start": "1873790",
    "end": "1881140"
  },
  {
    "text": "change the request in the template however once the pot is created based on this template we updated during the",
    "start": "1881140",
    "end": "1887500"
  },
  {
    "text": "admission phase okay and as in the stands schedule works based on requests right that's correct",
    "start": "1887500",
    "end": "1894940"
  },
  {
    "text": "so basically when next pod will be",
    "start": "1894940",
    "end": "1900160"
  },
  {
    "text": "scaled right it can't rely your own sum of requests which is currently",
    "start": "1900160",
    "end": "1906730"
  },
  {
    "text": "calculated for for example some specific node right mm-hm",
    "start": "1906730",
    "end": "1911800"
  },
  {
    "text": "so how would you deal with that so it can be situation when actually working",
    "start": "1911800",
    "end": "1919180"
  },
  {
    "text": "load on this nodes much higher but schedule decided that it enough that new",
    "start": "1919180",
    "end": "1924430"
  },
  {
    "text": "pod can be scheduled in this node yes so once the pot is updated it can go to a",
    "start": "1924430",
    "end": "1931000"
  },
  {
    "text": "different note and it's a perfectly valid situation so scheduler tries to",
    "start": "1931000",
    "end": "1936520"
  },
  {
    "text": "pick the best node to run a out with a specific requirements so for example it may land on the exactly same",
    "start": "1936520",
    "end": "1943320"
  },
  {
    "text": "note at this used to be however it may also land on some other note if the no",
    "start": "1943320",
    "end": "1949559"
  },
  {
    "text": "other note is simply better according to the scheduler to host the spot sometime",
    "start": "1949559",
    "end": "1955289"
  },
  {
    "text": "in the future we hope to have in place upgrades so we will first try to update",
    "start": "1955289",
    "end": "1961589"
  },
  {
    "text": "the pod request without actually killing it however it requires a lot of stuff in",
    "start": "1961589",
    "end": "1967229"
  },
  {
    "text": "note and scheduler code base and we are in the process of dealing with it so",
    "start": "1967229",
    "end": "1973139"
  },
  {
    "text": "stay tuned",
    "start": "1973139",
    "end": "1976159"
  },
  {
    "text": "no but this question for clarification yes I will admission the admission",
    "start": "1982099",
    "end": "1987509"
  },
  {
    "text": "plugin actually updates the pod request on admission before the scheduler ever sees the pot so it an only sees updated",
    "start": "1987509",
    "end": "1994769"
  },
  {
    "text": "pod it never sees the original one any other clarification questions let's say",
    "start": "1994769",
    "end": "2003440"
  },
  {
    "text": "there's a case when you have let's say progress with master's wife and how to prevent master to be killed instead of",
    "start": "2003440",
    "end": "2008690"
  },
  {
    "text": "slave is it possible we so you in which",
    "start": "2008690",
    "end": "2013849"
  },
  {
    "text": "autoscaler just clarified okay so there is",
    "start": "2013849",
    "end": "2018889"
  },
  {
    "text": "something called pod disruption budget that you may say that you don't want some set of pots to be to be killed and",
    "start": "2018889",
    "end": "2026690"
  },
  {
    "text": "you might designate the master to have allowed disruptions equal to zero do not",
    "start": "2026690",
    "end": "2034129"
  },
  {
    "text": "evict a notation I just want to ask your",
    "start": "2034129",
    "end": "2039139"
  },
  {
    "text": "you will optimize across node groups that correct when you're scaling up when",
    "start": "2039139",
    "end": "2044899"
  },
  {
    "text": "he was talking about node groups when you take a particular part you will sort of do the optimal optimize across the",
    "start": "2044899",
    "end": "2051648"
  },
  {
    "text": "node groups or you check on on a per node group basis or across all known",
    "start": "2051649",
    "end": "2056990"
  },
  {
    "text": "groups so in a single loop we will scale up at most a single loop of class tile",
    "start": "2056990",
    "end": "2063408"
  },
  {
    "text": "to scalable scaler at up at most one node group but if you have posted like",
    "start": "2063409",
    "end": "2069529"
  },
  {
    "text": "better fit different groups yes we can just do it one by one we have a loop every 10 seconds so if you have a GPU",
    "start": "2069529",
    "end": "2076470"
  },
  {
    "text": "port it will go in one iteration we will scale up the GPU and I'll group if you have a non GPU ports in the next",
    "start": "2076470",
    "end": "2083820"
  },
  {
    "text": "iteration we will do a scale up for those I could apart basically sort of be scheduled on either one of the node",
    "start": "2083820",
    "end": "2089429"
  },
  {
    "text": "groups that I mean how do you know which ones ideal for them so we cannot do this",
    "start": "2089429",
    "end": "2095909"
  },
  {
    "text": "sort of a pair port like everything calculation because it gets too computationally expensive and okay so",
    "start": "2095910",
    "end": "2102300"
  },
  {
    "text": "what we do is basically we use a greedy approach here for each node group we see",
    "start": "2102300",
    "end": "2107670"
  },
  {
    "text": "the maximum number of ports that we can fit into into this node group as one",
    "start": "2107670",
    "end": "2114330"
  },
  {
    "text": "option and then we pick the best option so hopefully if you have a very ineffective way of fitting ports like",
    "start": "2114330",
    "end": "2122190"
  },
  {
    "text": "this option will not be very attractive it will first do a different scale up and then on the next loop you know those",
    "start": "2122190",
    "end": "2128340"
  },
  {
    "text": "posts that were that didn't make sense has already gone somewhere else thank you one clarification here so",
    "start": "2128340",
    "end": "2134850"
  },
  {
    "text": "usually pops don't come at random so at",
    "start": "2134850",
    "end": "2139890"
  },
  {
    "text": "one 10 second slot you create one deployment and you give us the set of equality looking spot slightly later",
    "start": "2139890",
    "end": "2146910"
  },
  {
    "text": "something else kicks in and creates another set of equality looking spots so in most of the cases cluster autoscaler",
    "start": "2146910",
    "end": "2154380"
  },
  {
    "text": "works on exactly the same set of pots so if there are not they are not mixed however we support cases when they are",
    "start": "2154380",
    "end": "2161400"
  },
  {
    "text": "slightly mixed I have a question on the cluster autoscaler is if there is an",
    "start": "2161400",
    "end": "2168270"
  },
  {
    "text": "ever any way that you can get the cluster to auto scale based on the",
    "start": "2168270",
    "end": "2174060"
  },
  {
    "text": "actual utilization of the node so if you have full CPU usage but you said it big",
    "start": "2174060",
    "end": "2180750"
  },
  {
    "text": "difference between your requests and your limits so you can still technically schedule within the request range but",
    "start": "2180750",
    "end": "2188160"
  },
  {
    "text": "actually the node is on the cpu pressure so it's the task of horizontal and",
    "start": "2188160",
    "end": "2195120"
  },
  {
    "text": "vertical path autoscaler to make sure that your pots are not burning hot so horizontal potahto scalar based on your",
    "start": "2195120",
    "end": "2201900"
  },
  {
    "text": "traffic may spin up new instances and this instances may not have a place to go send in that case cluster",
    "start": "2201900",
    "end": "2208660"
  },
  {
    "text": "autoscaler Widow kicking the similar a situation vertical pot autoscaler my I",
    "start": "2208660",
    "end": "2213820"
  },
  {
    "text": "see that you are using too many resources on your pot and this resources needs to be increased in order to",
    "start": "2213820",
    "end": "2219550"
  },
  {
    "text": "reflect the reality and this pot cannot go anywhere else and then cluster autoscaler will kick in the last",
    "start": "2219550",
    "end": "2229090"
  },
  {
    "text": "question so how do you see the vertical",
    "start": "2229090",
    "end": "2236220"
  },
  {
    "text": "autoscaler being used in conjunction with the vertical autoscaler vertical",
    "start": "2236220",
    "end": "2243540"
  },
  {
    "text": "autoscaler with horizontal killer 2 together yeah also to be more specific for",
    "start": "2243540",
    "end": "2251140"
  },
  {
    "text": "example if my application actually has grown and needs more resources i would",
    "start": "2251140",
    "end": "2257470"
  },
  {
    "text": "like it to go to grow vertically up while if i have a surge of requests i",
    "start": "2257470",
    "end": "2265300"
  },
  {
    "text": "would want it to go vertical to low go horizontally sorry okay i understood so",
    "start": "2265300",
    "end": "2272760"
  },
  {
    "text": "mixing horizontal and vertical pot autoscaler is not fully supported so",
    "start": "2272760",
    "end": "2279550"
  },
  {
    "text": "both of this autoscaler for similar purpose they want to deal",
    "start": "2279550",
    "end": "2285430"
  },
  {
    "text": "with a situation when your pots are overloaded they don't talk to each other",
    "start": "2285430",
    "end": "2290560"
  },
  {
    "text": "so they can make this similar decision at the same time for example horizontal",
    "start": "2290560",
    "end": "2295960"
  },
  {
    "text": "put autoscaler my spin up a couple of new instances and vertical pot autoscaler at the same time i do a",
    "start": "2295960",
    "end": "2302350"
  },
  {
    "text": "couple bigger bigger pots in the situation you will get too much capacity and then it will have to go back so",
    "start": "2302350",
    "end": "2309310"
  },
  {
    "text": "combining cpu-based auto scaling and cpu base vertical putter scaling is not",
    "start": "2309310",
    "end": "2315670"
  },
  {
    "text": "necessarily the best idea a slightly better idea is to use custom metrics in",
    "start": "2315670",
    "end": "2321730"
  },
  {
    "text": "horizontal pot autoscaler in such a way you have separated a",
    "start": "2321730",
    "end": "2327610"
  },
  {
    "text": "little bit the cpu load from horizontal to scalar from cpu load taken into",
    "start": "2327610",
    "end": "2335020"
  },
  {
    "text": "account in vertical path of the scalar however it needs to be drawn to test that and validated on per case",
    "start": "2335020",
    "end": "2341430"
  },
  {
    "text": "basis thank you okay thank you thank you",
    "start": "2341430",
    "end": "2346859"
  },
  {
    "text": "and have a safe trip back home [Applause]",
    "start": "2346859",
    "end": "2352750"
  }
]