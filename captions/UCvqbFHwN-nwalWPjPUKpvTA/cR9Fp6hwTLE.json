[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "so I'm Fabian I will record and I'm also Prometheus maintainer I've also been a",
    "start": "79",
    "end": "6569"
  },
  {
    "text": "barista Tina before at sound out and so I've been doing the stuff for quite a while now and I want to talk about",
    "start": "6569",
    "end": "11820"
  },
  {
    "text": "alerting and cloud native environments and the original was a loading based on time series but this sounds virtual fun",
    "start": "11820",
    "end": "18359"
  },
  {
    "text": "I guess and but we actually want to talk about alerting with time series and and just",
    "start": "18359",
    "end": "24810"
  },
  {
    "start": "22000",
    "end": "121000"
  },
  {
    "text": "to start out very theoretical and boring and what's the time series a time series",
    "start": "24810",
    "end": "30060"
  },
  {
    "text": "is a stream of values and their couple of timestamp and an actual value and and",
    "start": "30060",
    "end": "36270"
  },
  {
    "text": "the tightest term is always monitoring monotonically increasing and and to give meaning to the stream of samples we have",
    "start": "36270",
    "end": "44059"
  },
  {
    "text": "some sort of identifier and that's an our case here for Prometheus a metric named HTP requests total here which we",
    "start": "44059",
    "end": "52020"
  },
  {
    "text": "then split out into more granularity using labels so not under this one",
    "start": "52020",
    "end": "57960"
  },
  {
    "text": "counter we have several counters for every instance we have for every pass we have or we said if we have for each",
    "start": "57960",
    "end": "63629"
  },
  {
    "text": "unique combination of labels and then we have to several times who is associated",
    "start": "63629",
    "end": "68640"
  },
  {
    "text": "with a single metric and if we have that",
    "start": "68640",
    "end": "74400"
  },
  {
    "text": "we can do interesting stuff right we have now data that we came through in in this case here we query all time series",
    "start": "74400",
    "end": "80340"
  },
  {
    "text": "for this particular metric and this particular top level and because they're counters we now want to get the per",
    "start": "80340",
    "end": "86850"
  },
  {
    "text": "second value so from just a counter counting up request one by one by one we now want to know what's actually window",
    "start": "86850",
    "end": "93360"
  },
  {
    "text": "of the last five minutes my beakers per second that my applications are experiencing and applying just to sound",
    "start": "93360",
    "end": "100770"
  },
  {
    "text": "the plan series gives you a pretty big result which doesn't really give you any good inside and what's what you're looking at and that's why you have a",
    "start": "100770",
    "end": "107100"
  },
  {
    "text": "creations you can aggregate along with these flavor dimensions so here we want to preserve the paths and said as I",
    "start": "107100",
    "end": "112259"
  },
  {
    "text": "mentioned and what we get back is unique topics of our past and set of labels and",
    "start": "112259",
    "end": "117479"
  },
  {
    "text": "the currently associated recurs rate and",
    "start": "117479",
    "end": "123869"
  },
  {
    "start": "121000",
    "end": "173000"
  },
  {
    "text": "this actually optimistic is doing so Prometheus is a monitoring system and Tennessee's database and it knows how to",
    "start": "123869",
    "end": "129810"
  },
  {
    "text": "talk to your service care system like kubernetes and discovers all things that are interesting to monitor",
    "start": "129810",
    "end": "135390"
  },
  {
    "text": "in your infrastructure and based on this information goes out and collects sample",
    "start": "135390",
    "end": "140640"
  },
  {
    "text": "data from these so called targets which can be routers which is nodes your micro",
    "start": "140640",
    "end": "147000"
  },
  {
    "text": "services basically anything you want and having stopped that in our database now",
    "start": "147000",
    "end": "152340"
  },
  {
    "text": "on our Prometheus node we can also query it and some of the query language you have seen before and thereby expose it",
    "start": "152340",
    "end": "158610"
  },
  {
    "text": "and our HTTP API to things like asana our own UI etc etc but it's mostly",
    "start": "158610",
    "end": "164670"
  },
  {
    "text": "geared towards basically graphing stuff either for dashboard or on-demand if you want to investigations to any knowledge",
    "start": "164670",
    "end": "173330"
  },
  {
    "start": "173000",
    "end": "205000"
  },
  {
    "text": "and now we want to do a low thing and and how we do this all right well we have a ton of data we have millions of",
    "start": "174200",
    "end": "181830"
  },
  {
    "text": "different time series and bridges data points associated with them and of course it's quite an repeating idea to",
    "start": "181830",
    "end": "188100"
  },
  {
    "text": "just do all the French things and we know from the terms like machine learning anomaly detection code rating",
    "start": "188100",
    "end": "195420"
  },
  {
    "text": "stuff making everything cell feeding because we have big data right and big data generally means we can make",
    "start": "195420",
    "end": "201450"
  },
  {
    "text": "everything magical so previously decided",
    "start": "201450",
    "end": "206970"
  },
  {
    "start": "205000",
    "end": "299000"
  },
  {
    "text": "against this and very strongly and as a simple reason is that we're actually monitoring at scale and we really Auto",
    "start": "206970",
    "end": "214440"
  },
  {
    "text": "users to instrument applications way more than they do I usually means now and which means gets way more data then",
    "start": "214440",
    "end": "222769"
  },
  {
    "text": "then it passed and when a machine learning etc might work quite well at a smaller scale if you have little data",
    "start": "222769",
    "end": "229140"
  },
  {
    "text": "and if you scale it up by a factor of 10 100 1000 and something will always",
    "start": "229140",
    "end": "236400"
  },
  {
    "text": "correlate right if you just look and look enough some curves will always sort of add up and seem to make sense without",
    "start": "236400",
    "end": "243090"
  },
  {
    "text": "having any sort of casual relation and and in the end to eliminate these false",
    "start": "243090",
    "end": "250170"
  },
  {
    "text": "positives which will be the very bad for alerting right because we get paid for nothing and you would have to put in",
    "start": "250170",
    "end": "257370"
  },
  {
    "text": "huge efforts and the problem is if you will eliminate very aggressively there's",
    "start": "257370",
    "end": "263820"
  },
  {
    "text": "a good chance that you will somehow also cut out some others that should actually be firing and because now you have false",
    "start": "263820",
    "end": "270070"
  },
  {
    "text": "negatives and if you're alerting for the negatives are not really an option so if your datacenter is burning down and you",
    "start": "270070",
    "end": "276730"
  },
  {
    "text": "don't get paged because you're machining so everything's are probably okay and doesn't really book so um the question",
    "start": "276730",
    "end": "286060"
  },
  {
    "text": "is having all this data can be still however the comprehensive approach that's easy to use and gives you full",
    "start": "286060",
    "end": "293920"
  },
  {
    "text": "coverage and insight to add on things going wrong in infrastructure and if you",
    "start": "293920",
    "end": "301090"
  },
  {
    "start": "299000",
    "end": "339000"
  },
  {
    "text": "think about it and our time series represents quite well the current state",
    "start": "301090",
    "end": "306100"
  },
  {
    "text": "of everything about notes about micro services other routers and everything",
    "start": "306100",
    "end": "311140"
  },
  {
    "text": "that's basically instrumented and for everything that's running we have an idea of how it would be running that's",
    "start": "311140",
    "end": "317350"
  },
  {
    "text": "commonly known as SLA or I said oh and if you don't have one you should probably think about what it should be",
    "start": "317350",
    "end": "325080"
  },
  {
    "text": "but we generally can define based on the current state whether the current state matches the desired State and if it does",
    "start": "325140",
    "end": "332650"
  },
  {
    "text": "not Delta of those two that's what you want to alert on it's actually pretty simple so what does this mean",
    "start": "332650",
    "end": "341890"
  },
  {
    "text": "practically and practically this means and everything doesn't alert and we consider loads usually things that page",
    "start": "341890",
    "end": "348550"
  },
  {
    "text": "me at night right or during the day and everything that's paging should be",
    "start": "348550",
    "end": "354120"
  },
  {
    "text": "original actionable right it should be indirect problem for our users if the user is not impacted and there's not",
    "start": "354120",
    "end": "361450"
  },
  {
    "text": "necessarily a problem and the user can be anything in right if you are writing a micro service and we just sort of",
    "start": "361450",
    "end": "368470"
  },
  {
    "text": "serving data for other micro services these are your users if you're placing customers directly these are users etc",
    "start": "368470",
    "end": "373750"
  },
  {
    "text": "etc and you only really care about the boundary between your system and your",
    "start": "373750",
    "end": "378790"
  },
  {
    "text": "user and they'll say exactly where I said is defined and they are for gold",
    "start": "378790",
    "end": "386860"
  },
  {
    "text": "signals and which the sort of meant to capture things to a little and one of",
    "start": "386860",
    "end": "391990"
  },
  {
    "text": "them is latency and if a user makes a request to whatever service you provide they expect a timely response",
    "start": "391990",
    "end": "398830"
  },
  {
    "text": "might be a few milliseconds a few seconds maybe even hours but they certainly some sort of timeframe in which they expect the response so that's",
    "start": "398830",
    "end": "406120"
  },
  {
    "text": "a good thing to learn on obviously Ennis traffic if your vacation should receive",
    "start": "406120",
    "end": "411310"
  },
  {
    "text": "approximately 1000 Rica's per second and is receiving zero and some things",
    "start": "411310",
    "end": "416470"
  },
  {
    "text": "probably up it's might be on the way between user and chat system but it might also be the consistently subs just",
    "start": "416470",
    "end": "422470"
  },
  {
    "text": "sort of not even accepting requests to the point where it can measure them so it's also something and to what offer",
    "start": "422470",
    "end": "428740"
  },
  {
    "text": "and it closely related we have errors if users make requests and gets not served",
    "start": "428740",
    "end": "433840"
  },
  {
    "text": "the response I expected because of errors on the server side and you're violating it at a more certainly and",
    "start": "433840",
    "end": "439930"
  },
  {
    "text": "should definitely load on that and the",
    "start": "439930",
    "end": "445000"
  },
  {
    "text": "fourth golden signal actually is not symptom based we're just kind of breaking our paradigm here and instead",
    "start": "445000",
    "end": "451780"
  },
  {
    "start": "449000",
    "end": "547000"
  },
  {
    "text": "it's cause based and there's a reason for that and but first of all what what what course based alerting and causes",
    "start": "451780",
    "end": "459460"
  },
  {
    "text": "everything that potentially lead to problems in lethal symptoms of problems",
    "start": "459460",
    "end": "464740"
  },
  {
    "text": "so if I can't connect to a database which is sort of my downstream dependency and this might cause me not",
    "start": "464740",
    "end": "472660"
  },
  {
    "text": "being able to serve a Rica to my user but does not necessarily imply it right if I'm caching so long enough to for my",
    "start": "472660",
    "end": "478540"
  },
  {
    "text": "dependency to recover and it is just a potential cause but not an imminent problem and in general these things",
    "start": "478540",
    "end": "485140"
  },
  {
    "text": "should not be something tool to notify someone about in a disruptive manner these are usually meant it about helpful",
    "start": "485140",
    "end": "492100"
  },
  {
    "text": "context if I get paged for and high error rate of my service and I can now look at what currently cost based",
    "start": "492100",
    "end": "499000"
  },
  {
    "text": "warnings and we see and the only exception is sort of our fourth column",
    "start": "499000",
    "end": "504970"
  },
  {
    "text": "signal which is saturation alleged capacity and that can be stuff like prices and running full memory running",
    "start": "504970",
    "end": "513550"
  },
  {
    "text": "for basic set of stuff running for and and the reason why we make this sort of",
    "start": "513550",
    "end": "519039"
  },
  {
    "text": "more of a cost based thing is that if we aren't run out of faces in space out of memory etc etc the effectors that are",
    "start": "519040",
    "end": "525940"
  },
  {
    "text": "usually quite critical and don't resolve themselves quite quite easily and",
    "start": "525940",
    "end": "531280"
  },
  {
    "text": "if I didn't run in full might take hours of migration work I might even corrupted data so these are",
    "start": "531280",
    "end": "537040"
  },
  {
    "text": "things that you really want to catch early and because if they happen they are not maybe cause of something they",
    "start": "537040",
    "end": "542650"
  },
  {
    "text": "are most certainly and causing severe symptoms in the end so so far for the",
    "start": "542650",
    "end": "551410"
  },
  {
    "start": "547000",
    "end": "635000"
  },
  {
    "text": "theory and this applies to everything that is not bound to permeated in any way and but we can look at how",
    "start": "551410",
    "end": "557860"
  },
  {
    "text": "Prometheus sort of implement this idea of alerting and we have our small DSL",
    "start": "557860",
    "end": "564190"
  },
  {
    "text": "where we can define an alert name and we can associate it with the condition and the convenient thing is that these",
    "start": "564190",
    "end": "572020"
  },
  {
    "text": "conditions are expressed as Prometheus query language strings and these are the same queries that you use for",
    "start": "572020",
    "end": "577720"
  },
  {
    "text": "dashboarding and investigation during an outage so everything you can graph you can alert on and that's extremely good",
    "start": "577720",
    "end": "584980"
  },
  {
    "text": "because first of all we hopefully know the Creveling is already and also if you get an alert we can directly click on the condition that triggered it and",
    "start": "584980",
    "end": "590920"
  },
  {
    "text": "we'll see a graph that relates to exactly this um and then at some other",
    "start": "590920",
    "end": "596710"
  },
  {
    "text": "seven they're like the four duration which is sort of a great window before we actually send out an alert which is",
    "start": "596710",
    "end": "602380"
  },
  {
    "text": "supposed to prevent flapping alerts right if a problem resolves itself within a reasonable time there's not",
    "start": "602380",
    "end": "607660"
  },
  {
    "text": "really reason to sort of propagate this alert to any human and their lives their",
    "start": "607660",
    "end": "612790"
  },
  {
    "text": "patients and we will see later how these are used and and if you have seen from",
    "start": "612790",
    "end": "618339"
  },
  {
    "text": "clear before it's um or in the example before we don't get just like a single result right we get back sort of a",
    "start": "618339",
    "end": "624040"
  },
  {
    "text": "vector of label Perce associated with the value and this exactly what we get in our condition here and every element",
    "start": "624040",
    "end": "631900"
  },
  {
    "text": "in our result is one sitting in a dress so at an example we have beer monitoring",
    "start": "631900",
    "end": "638620"
  },
  {
    "start": "635000",
    "end": "709000"
  },
  {
    "text": "STD and STD expose the material itself which is called SIDS leader and it's zero if X D does not it is notice not",
    "start": "638620",
    "end": "645220"
  },
  {
    "text": "see a leader in the sed quorum and one if it does and obviously not seeing a",
    "start": "645220",
    "end": "650290"
  },
  {
    "text": "leader is very bad and and one is okay and here and B obviously are in trouble",
    "start": "650290",
    "end": "655510"
  },
  {
    "text": "and in other words detecting exactly this condition would be as simple as",
    "start": "655510",
    "end": "660790"
  },
  {
    "text": "saying para SE d novena and then the condition being Exedy has that equals zero which filters every",
    "start": "660790",
    "end": "666819"
  },
  {
    "text": "time series that does have the value zero I put it out every time she was",
    "start": "666819",
    "end": "672129"
  },
  {
    "text": "having the value unequal zero and which",
    "start": "672129",
    "end": "677170"
  },
  {
    "text": "means that our result basically is exactly two instances of seen before which don't see a reader at this point",
    "start": "677170",
    "end": "683649"
  },
  {
    "text": "in time and each of these now form one in dirt and what we do before sending out derivatives we attach a label of",
    "start": "683649",
    "end": "690220"
  },
  {
    "text": "alert name and we attach the Civic's label which is part of our Energon definition and this label will use later",
    "start": "690220",
    "end": "697089"
  },
  {
    "text": "on to make routing decision on how and who should be notified who should be",
    "start": "697089",
    "end": "702910"
  },
  {
    "text": "notified and how so these are just arbitrary labels we can attach to any alerts being sent out and to make it a",
    "start": "702910",
    "end": "711220"
  },
  {
    "start": "709000",
    "end": "744000"
  },
  {
    "text": "bit more complex and see how we can actually design them bit more complex and real world kind of alert and we have",
    "start": "711220",
    "end": "719829"
  },
  {
    "text": "some time series here and or we have two metrics one counting requests and one counting the subsets of errors of these",
    "start": "719829",
    "end": "727749"
  },
  {
    "text": "requests and these are spread out by certain labels like the instance so",
    "start": "727749",
    "end": "732939"
  },
  {
    "text": "every sort of processable microservice and by the recurse pairs being used and by the whicker's method and we want to",
    "start": "732939",
    "end": "740620"
  },
  {
    "text": "alert on error rates being the errors being too high and we can start as simple as saying alert higher rate and",
    "start": "740620",
    "end": "748720"
  },
  {
    "start": "744000",
    "end": "890000"
  },
  {
    "text": "then let's take just all these sensors we have for the error rate for the errors we counted and take the per",
    "start": "748720",
    "end": "755350"
  },
  {
    "text": "second value because that's what we're interested in so we take the per second value of a 5-minute window and what this",
    "start": "755350",
    "end": "761889"
  },
  {
    "text": "will still give us for every single instance pass and method combination put one potential alert and that's of course",
    "start": "761889",
    "end": "768009"
  },
  {
    "text": "too noisy right we don't wanna get at 20,000 words if sort of the whole service is buggy and to mitigate that we",
    "start": "768009",
    "end": "775929"
  },
  {
    "text": "use an aggregation so we apply a sum which connected this whole thing nicely in a single value and we can see here",
    "start": "775929",
    "end": "784149"
  },
  {
    "text": "that apparently our service is experiencing 534 errors per second yeah",
    "start": "784149",
    "end": "790480"
  },
  {
    "text": "okay that's bad and and because we don't want this to happen we set a threshold and make this arrow trigger if",
    "start": "790480",
    "end": "798750"
  },
  {
    "text": "greater than 500 and so the result we get is sort of expected we alert us if",
    "start": "798750",
    "end": "805800"
  },
  {
    "text": "the condition hits and the problem here is this F so it settles because it's",
    "start": "805800",
    "end": "811200"
  },
  {
    "text": "quite fuzzy what does it actually mean like this is a five note service a five thousand note service how many because",
    "start": "811200",
    "end": "818100"
  },
  {
    "text": "is getting and right and whenever it's totally depend on the total traffic you are receiving so over the day we might",
    "start": "818100",
    "end": "825210"
  },
  {
    "text": "have a sort of curve which changes as if users in busy reasons rubber has a",
    "start": "825210",
    "end": "832350"
  },
  {
    "text": "system or something and we might just get more traffic over months or weeks of time or we were to send a traffic spike",
    "start": "832350",
    "end": "840480"
  },
  {
    "text": "because you were mentioned in news so there are many many different ways that can sort of change the number of",
    "start": "840480",
    "end": "847650"
  },
  {
    "text": "requests we are getting and the error rate we are seeing depends directly on these requests and the semantic meaning",
    "start": "847650",
    "end": "853710"
  },
  {
    "text": "of the load changes basically based on based on values that aren't represented",
    "start": "853710",
    "end": "859290"
  },
  {
    "text": "in the condition and so what we ideally want is we want this sort of um threshold to be adaptive to the total",
    "start": "859290",
    "end": "865920"
  },
  {
    "text": "traffic and we can do that easily enough and by just dividing the error rate",
    "start": "865920",
    "end": "871620"
  },
  {
    "text": "which we calculated by the total number of requests right and now we have a fraction which is certainly way closer",
    "start": "871620",
    "end": "878850"
  },
  {
    "text": "to what we want and so if more than one percent of our requests are errors then we want to take an alert and now the",
    "start": "878850",
    "end": "886530"
  },
  {
    "text": "picture looks like this and it's certainly way closer to what you want and it's not right anyway um because we",
    "start": "886530",
    "end": "895650"
  },
  {
    "start": "890000",
    "end": "944000"
  },
  {
    "text": "are losing out of detail right and we have this label that curry language of all these time series having a lot of",
    "start": "895650",
    "end": "901830"
  },
  {
    "text": "care and IOT and what we are we're condensing them into a single value and and this unfortunately can cause signal",
    "start": "901830",
    "end": "910440"
  },
  {
    "text": "cancellation so let's say you have one weakest pass we'll just hit very very",
    "start": "910440",
    "end": "915870"
  },
  {
    "text": "rarely like in a contact form right and it's maybe hit like once an hour",
    "start": "915870",
    "end": "921450"
  },
  {
    "text": "compared to your index pages that we hit like two one times a second and even if",
    "start": "921450",
    "end": "927000"
  },
  {
    "text": "you our contact form I wrote every single time you use I want to see it it will not show up in the",
    "start": "927000",
    "end": "932740"
  },
  {
    "text": "result because totally overshadowed by the high traffic your index page receives and the special is somewhere",
    "start": "932740",
    "end": "939070"
  },
  {
    "text": "down there so we are certain anything properly it's not detected at all luckily we can also",
    "start": "939070",
    "end": "948010"
  },
  {
    "start": "944000",
    "end": "978000"
  },
  {
    "text": "solve this and as you've seen before we can preserve certain dimensions in notifications right and so for this time",
    "start": "948010",
    "end": "954220"
  },
  {
    "text": "series and we say yes aggregate but keep certain things around so I can have more detail and it want to preserve the",
    "start": "954220",
    "end": "961810"
  },
  {
    "text": "internet the path label we achieve exactly that so what we get back here it's a nice per instance pass",
    "start": "961810",
    "end": "968830"
  },
  {
    "text": "combination of the total error rate and in this case the previous example will",
    "start": "968830",
    "end": "974800"
  },
  {
    "text": "be reliably sort of detected and it's",
    "start": "974800",
    "end": "979959"
  },
  {
    "start": "978000",
    "end": "1028000"
  },
  {
    "text": "still wrong I'm intentionally of course to just show you what can be wrong because we preserve the instance label",
    "start": "979959",
    "end": "986470"
  },
  {
    "text": "and in the case of a microservice for example that actually did I mention along which we tolerate fader right",
    "start": "986470",
    "end": "992620"
  },
  {
    "text": "because we are building within a system that can handle certain errors and in the horizontal scale of the system the",
    "start": "992620",
    "end": "998020"
  },
  {
    "text": "single instance misbehaving is certainly one of them so um we'll see load as it",
    "start": "998020",
    "end": "1004020"
  },
  {
    "text": "is if one are of one South instances misbehaves we will get paged and we have",
    "start": "1004020",
    "end": "1010709"
  },
  {
    "text": "nothing we can do we can probably shutdown the instance and bring up a new one but it's not necessary because there",
    "start": "1010709",
    "end": "1016620"
  },
  {
    "text": "was no problem for users to begin with and great sort of going to burn out because we get all these nonsensical",
    "start": "1016620",
    "end": "1023270"
  },
  {
    "text": "errors a lot so the reason the answer of",
    "start": "1023270",
    "end": "1030720"
  },
  {
    "start": "1028000",
    "end": "1137000"
  },
  {
    "text": "course is just pick different dimensions and previous comes to the video nice",
    "start": "1030720",
    "end": "1035819"
  },
  {
    "text": "query language feature which is sort of inversion of some by which is some result so we specify the dimensions we want to",
    "start": "1035819",
    "end": "1042540"
  },
  {
    "text": "get rid of which is way safer right because we know are not necessarily aware of all the dimensions that our",
    "start": "1042540",
    "end": "1047880"
  },
  {
    "text": "metric has and but we're usually aware of the dimensions we are fault tolerant along and this case here at the instance",
    "start": "1047880",
    "end": "1054450"
  },
  {
    "text": "and we say ok I agree at everything away but the instance and what we get Vegas itself gets the most insights we can",
    "start": "1054450",
    "end": "1061559"
  },
  {
    "text": "because we preserve the past as well as a method which was not considered before",
    "start": "1061559",
    "end": "1066690"
  },
  {
    "text": "and sitting sort of alert we would like to end up with and it's it is realign",
    "start": "1066690",
    "end": "1073980"
  },
  {
    "text": "alert which will cover one ten five thousand ten thousands microsoft",
    "start": "1073980",
    "end": "1078990"
  },
  {
    "text": "instances it doesn't matter because acting based on a pool of time series data and you don't have to sort of tweak",
    "start": "1078990",
    "end": "1085139"
  },
  {
    "text": "anything if your infrastructure roughly stays the same or the applies things",
    "start": "1085139",
    "end": "1090659"
  },
  {
    "text": "infrastructure is the same it will just scale automatically sort of with that's actually going to install any new checks",
    "start": "1090659",
    "end": "1096840"
  },
  {
    "text": "for every new thing you deploy it's sort of adaptive yeah and that's actually",
    "start": "1096840",
    "end": "1103379"
  },
  {
    "text": "typical alerts for symptom based pages and once the latency look pretty similar",
    "start": "1103379",
    "end": "1109080"
  },
  {
    "text": "and it will also serve that if you're sort of having it typically Microsoft architecture that almost all alerts will",
    "start": "1109080",
    "end": "1115679"
  },
  {
    "text": "look like this and that's actually a good thing and because it makes things simple right there's no black magic",
    "start": "1115679",
    "end": "1121409"
  },
  {
    "text": "involved here you have a few distinct patterns to stick to and to get reliable alerting covering you end to end and",
    "start": "1121409",
    "end": "1129179"
  },
  {
    "text": "don't have to consider and any of these potential tens of thousands of causes leading up to the program and out to an",
    "start": "1129179",
    "end": "1139470"
  },
  {
    "start": "1137000",
    "end": "1287000"
  },
  {
    "text": "interesting part and the capacity situation adults there are kind of a different class but also following a",
    "start": "1139470",
    "end": "1145620"
  },
  {
    "text": "very similar pattern every time and so a typical thing to detect is my digit",
    "start": "1145620",
    "end": "1152399"
  },
  {
    "text": "Oracle and what we typically do nowadays is we probe every sort of note or file",
    "start": "1152399",
    "end": "1158639"
  },
  {
    "text": "system and check ok how cool is it and if it's a 70 percent everything's good and if the eighty percent everything is",
    "start": "1158639",
    "end": "1164850"
  },
  {
    "text": "good and if it's at any one percent then we will page someone because obviously everything changed between 1881 and and",
    "start": "1164850",
    "end": "1172580"
  },
  {
    "text": "this has several problems first of all you are not using 20% of your disk just surface like 20% buffer to react and if",
    "start": "1172580",
    "end": "1180450"
  },
  {
    "text": "your disk that's one full and but it also triggered immediately if you just add a 2 kilobyte file putting it over",
    "start": "1180450",
    "end": "1186750"
  },
  {
    "text": "the specials even though afterwards never nothing changes ever and and also",
    "start": "1186750",
    "end": "1193049"
  },
  {
    "text": "doesn't detect if your disk is almost free but then sort of extremely quickly filled up by some sort of slightly",
    "start": "1193049",
    "end": "1199320"
  },
  {
    "text": "software and it will only let you if it's almost too late so the solution here is for",
    "start": "1199320",
    "end": "1205260"
  },
  {
    "text": "freedom Prometheus and we have history we have time series we not just don't just know what's happening now and we",
    "start": "1205260",
    "end": "1211110"
  },
  {
    "text": "don't just know that occurrence for is over disk is 80% we also know what it was 10 minutes 30 minutes and 1 hour ago",
    "start": "1211110",
    "end": "1217140"
  },
  {
    "text": "and based on that you can actually look at the development of the data over the last hour in this case here and and",
    "start": "1217140",
    "end": "1224450"
  },
  {
    "text": "interpret the metric telling us how many free bytes there on any particular file system and based on this one hour window",
    "start": "1224450",
    "end": "1231720"
  },
  {
    "text": "from now to the past we can do a linear regression and just predict into the future we are think to end up if it",
    "start": "1231720",
    "end": "1238710"
  },
  {
    "text": "continues as it does and we do so for 4 hours now okay so yeah and if this prediction has",
    "start": "1238710",
    "end": "1244380"
  },
  {
    "text": "that setting four hours will have zero or less free bytes on a disk then the",
    "start": "1244380",
    "end": "1249990"
  },
  {
    "text": "current development on this disk is indicating that we have to do something and this holds true no matter whether we",
    "start": "1249990",
    "end": "1256140"
  },
  {
    "text": "add 80 percent 90 percent or 10 percent of this usage right and if you as 95 percent and you add one megabyte file",
    "start": "1256140",
    "end": "1262890"
  },
  {
    "text": "but don't any nothing afterwards this alert rocks we can break you up because there is nothing to be done and this",
    "start": "1262890",
    "end": "1272549"
  },
  {
    "text": "sort of applies universally for all sorts of Federation capacity alerts right if you want to load on Hydra",
    "start": "1272549",
    "end": "1278309"
  },
  {
    "text": "script and running it file descriptors running out inodes running out memory running out and this sort of always applies and now",
    "start": "1278309",
    "end": "1291240"
  },
  {
    "start": "1287000",
    "end": "1331000"
  },
  {
    "text": "we're looking at annotations annotations are this other thing that we have seen before and they allow us to just check",
    "start": "1291240",
    "end": "1296270"
  },
  {
    "text": "additional non-identifying information to alert so if you want to sort of provides a more insightful human",
    "start": "1296270",
    "end": "1303090"
  },
  {
    "text": "readable message with all alert we can just add some text in you and you can begin as property we can use the labels",
    "start": "1303090",
    "end": "1310530"
  },
  {
    "text": "of our results so our results has labels for the device in question the amount point and the note is running on we can",
    "start": "1310530",
    "end": "1317730"
  },
  {
    "text": "all add this into a message and this is also again their full alert which will",
    "start": "1317730",
    "end": "1322830"
  },
  {
    "text": "cover every single file system on every single half-life or every single node of your entire infrastructure",
    "start": "1322830",
    "end": "1329570"
  },
  {
    "text": "so and that sort of okay that's how you write alert how we how we define what is",
    "start": "1330990",
    "end": "1338019"
  },
  {
    "start": "1331000",
    "end": "1515000"
  },
  {
    "text": "problematic and what should be done in the first place and and now we come to a lot manager which is sort of just thing",
    "start": "1338019",
    "end": "1344260"
  },
  {
    "text": "component in your premises ecosystem and no matter what system you have you should have some alert management layer",
    "start": "1344260",
    "end": "1350169"
  },
  {
    "text": "and how it works is that committees evaluates the rules which is specified",
    "start": "1350169",
    "end": "1356769"
  },
  {
    "text": "it's protected data started and immediately evaluate the data in alerts",
    "start": "1356769",
    "end": "1361779"
  },
  {
    "text": "which is really nice because everything is set contained right and not crossing fair domains and and if it consider some",
    "start": "1361779",
    "end": "1368620"
  },
  {
    "text": "other firing it will push it out to a res manager and there are managers then responsible to do the sort of more",
    "start": "1368620",
    "end": "1374049"
  },
  {
    "text": "fine-grained stuff like determining and which uncoil person should receive which alert which is has no real verb is being",
    "start": "1374049",
    "end": "1381010"
  },
  {
    "text": "directly in your monitoring system and it has some more important functions",
    "start": "1381010",
    "end": "1387010"
  },
  {
    "text": "actually so if you consider these rules we have defined to just be roots somewhere we don't really care about",
    "start": "1387010",
    "end": "1392590"
  },
  {
    "text": "which parameters are where they're coming from and but they're sort of they are these things that are just emitting streams of",
    "start": "1392590",
    "end": "1399130"
  },
  {
    "text": "alert going your way and you probably see below here that's probably a pattern",
    "start": "1399130",
    "end": "1405039"
  },
  {
    "text": "that you have encountered when being on call right you get woken up at 4 a.m. and you get a page telling you the",
    "start": "1405039",
    "end": "1410620"
  },
  {
    "text": "latency is high for service ex Enzo and us for this weakest person this weakest method and he said ok and ok and then",
    "start": "1410620",
    "end": "1417309"
  },
  {
    "text": "you get the next page telling you the same thing just like some slight modification and this just continues for 10 minutes you're getting pages and",
    "start": "1417309",
    "end": "1423940"
  },
  {
    "text": "pages and pages and pages and they're all sort of tying down tying back to the one single incident so the question is",
    "start": "1423940",
    "end": "1430990"
  },
  {
    "text": "why are you getting 20 pages if there's one problem and and we actually want it",
    "start": "1430990",
    "end": "1438039"
  },
  {
    "text": "to be this way after all because we want this inserting very levity right because you don't want to overshadow problems by",
    "start": "1438039",
    "end": "1444760"
  },
  {
    "text": "being too generic in our language as you've seen before and and how we can still solve this is with an alert",
    "start": "1444760",
    "end": "1451330"
  },
  {
    "text": "management layer and what your load manager does is it gets a stream of",
    "start": "1451330",
    "end": "1456580"
  },
  {
    "text": "alerts and looks at it evaluates how it should be actually",
    "start": "1456580",
    "end": "1462360"
  },
  {
    "text": "digested and brought to the user and by grouping it together along certain label",
    "start": "1462360",
    "end": "1467620"
  },
  {
    "text": "dimensions and by major disables making routing decisions so which team should",
    "start": "1467620",
    "end": "1473919"
  },
  {
    "text": "be paged about which service and and also deduplicating in case you have seven permeated servers that are sort of",
    "start": "1473919",
    "end": "1480610"
  },
  {
    "text": "emitting the same alert in a deployment and what we then get is we get one",
    "start": "1480610",
    "end": "1485679"
  },
  {
    "text": "single page telling you you have fifteen others for sex in zone a us and these are the rough problems here high latency",
    "start": "1485679",
    "end": "1492279"
  },
  {
    "text": "higher rate and Casella being slow and that should be almost everything you",
    "start": "1492279",
    "end": "1497470"
  },
  {
    "text": "need to sort of know what is up and then getting through your laptop and actually",
    "start": "1497470",
    "end": "1502539"
  },
  {
    "text": "start investigating and handing the incidence and all the individual URLs you see here it's actually not too",
    "start": "1502539",
    "end": "1508600"
  },
  {
    "text": "important and so there the big picture is what we are interested in and they",
    "start": "1508600",
    "end": "1516460"
  },
  {
    "start": "1515000",
    "end": "1528000"
  },
  {
    "text": "also some other stuff so we have on the one hand silencing right so if you take a service into maintenance you can sign",
    "start": "1516460",
    "end": "1521470"
  },
  {
    "text": "it orders that are having a certain label for example service X it's pretty",
    "start": "1521470",
    "end": "1526870"
  },
  {
    "text": "straightforward and another more advanced feature is inhibition and actually pretty cool even though rarely",
    "start": "1526870",
    "end": "1533440"
  },
  {
    "start": "1528000",
    "end": "1826000"
  },
  {
    "text": "you ready to get because that and necessary that often and but it",
    "start": "1533440",
    "end": "1539320"
  },
  {
    "text": "basically allows you to have certain innards mute other alerts and so if you have sort of a higher priority alert",
    "start": "1539320",
    "end": "1545409"
  },
  {
    "text": "like your data center burning down that's probably going to lead to all sort of other things firing but",
    "start": "1545409",
    "end": "1551620"
  },
  {
    "text": "personally sort of encore for the servers running in some rec in the data center probably won't really care or",
    "start": "1551620",
    "end": "1557559"
  },
  {
    "text": "can't do anything about this incident and so there's no reason to sort of wake someone up if there's nothing to be done",
    "start": "1557559",
    "end": "1563169"
  },
  {
    "text": "no matter how critical it is coming back",
    "start": "1563169",
    "end": "1569799"
  },
  {
    "text": "to anomaly detection so of course everybody to want it and we have seen",
    "start": "1569799",
    "end": "1575110"
  },
  {
    "text": "the Prometheus has apparently some statistical features like linear regression and everything so maybe you",
    "start": "1575110",
    "end": "1581169"
  },
  {
    "text": "can do something after all for fanciness but just first of all this is just all",
    "start": "1581169",
    "end": "1588429"
  },
  {
    "text": "bonus these are things that might be nice and informative but never make these things pay to whatsoever and don't",
    "start": "1588429",
    "end": "1594370"
  },
  {
    "text": "ever rely on these in critical conditions and that's it and let's suppose we have a request rate for",
    "start": "1594370",
    "end": "1601479"
  },
  {
    "text": "our service which we sort of and move into this new metric here called job request 285 minute and and we want to",
    "start": "1601479",
    "end": "1609009"
  },
  {
    "text": "sort of get a smoothing view of this right because it's sort of spiky but we want to get the overall trend over the",
    "start": "1609009",
    "end": "1614349"
  },
  {
    "text": "course of the day and here we have powered windows functions in parameters which allow you to set the parameters",
    "start": "1614349",
    "end": "1621070"
  },
  {
    "text": "and what you get back essentially is a smooth red curve here giving you a good representation of the overall behavior",
    "start": "1621070",
    "end": "1628889"
  },
  {
    "text": "okay so actually if we have to smooth in view we can actually build anomaly detection in a way so let's say we want",
    "start": "1629580",
    "end": "1637479"
  },
  {
    "text": "to sort of detect if the recurse rate right now is above or below a 20% sort",
    "start": "1637479",
    "end": "1644739"
  },
  {
    "text": "of window around this mutant view from one week ago and there's a collectable",
    "start": "1644739",
    "end": "1650559"
  },
  {
    "text": "fee sort of reflecting this curve in here so we take the current precursor eight and subtract the smoothing grade",
    "start": "1650559",
    "end": "1656019"
  },
  {
    "text": "from one week ago same day same time and and check if it's out of the 20% window",
    "start": "1656019",
    "end": "1663279"
  },
  {
    "text": "up or down and then we can notify you in some hopefully non-invasive way about a traffic being honest nominal and that's",
    "start": "1663279",
    "end": "1671109"
  },
  {
    "text": "good stuff to experiment with and but it will trigger on a question to you but virtually gone Superbowl all these times",
    "start": "1671109",
    "end": "1676749"
  },
  {
    "text": "right don't make these things page and",
    "start": "1676749",
    "end": "1682559"
  },
  {
    "text": "another example and that's prob'ly example why it's really really bad to do stuff after all and so no need to",
    "start": "1682559",
    "end": "1690429"
  },
  {
    "text": "understand this in detail what we're going to do here is we have the average latency over five minutes for a bunch of",
    "start": "1690429",
    "end": "1696849"
  },
  {
    "text": "micro service instances and we want to detect if one instance is out of the two",
    "start": "1696849",
    "end": "1703389"
  },
  {
    "text": "times standard deviation of the average of all these instances so one instance is being particularly slow and and two",
    "start": "1703389",
    "end": "1709809"
  },
  {
    "text": "standard deviations is a pretty good measure I guess and so that's exactly what for you're sort of representing here we take our friend latency and",
    "start": "1709809",
    "end": "1716019"
  },
  {
    "text": "check for every instance and check whether it's greater than the average of",
    "start": "1716019",
    "end": "1721690"
  },
  {
    "text": "all the agencies of all instances plus they are two times standard deviation and that's working",
    "start": "1721690",
    "end": "1728220"
  },
  {
    "text": "fantastic until you realized that if all a piece of the rough is the same your cent deviation is going to be",
    "start": "1728220",
    "end": "1734750"
  },
  {
    "text": "effectively zero and so now even the slightest sort of deviation will trigger",
    "start": "1734750",
    "end": "1739799"
  },
  {
    "text": "the alert and pretty bad so let's just change this condition with another condition and get the condition and sell",
    "start": "1739799",
    "end": "1745919"
  },
  {
    "text": "it okay do all of this before but only if we are in total twenty percent above",
    "start": "1745919",
    "end": "1751169"
  },
  {
    "text": "the overall average and good one sort of each case elided great and until you",
    "start": "1751169",
    "end": "1758039"
  },
  {
    "text": "realize that if you put up an instance Excel has a code cache and this is being slowed overall and that does matter",
    "start": "1758039",
    "end": "1764610"
  },
  {
    "text": "because it's evolved up here it and but you will still get paged so we have to sort of not consider any instance that",
    "start": "1764610",
    "end": "1771509"
  },
  {
    "text": "has not received sufficient traffic so far sorry another condition our fee",
    "start": "1771509",
    "end": "1776639"
  },
  {
    "text": "above and the only fire of this thing and if this instance has received more",
    "start": "1776639",
    "end": "1782340"
  },
  {
    "text": "than one week at the second and here we",
    "start": "1782340",
    "end": "1787590"
  },
  {
    "text": "have built a completely functioning educate considering anomaly detection and the point is that we had a very well",
    "start": "1787590",
    "end": "1795539"
  },
  {
    "text": "scoped problem we knew what we wanted to solve the data was very narrow down from millions of series and billions of",
    "start": "1795539",
    "end": "1802200"
  },
  {
    "text": "samples to just a few and even then with all our domain owns we have and knowing",
    "start": "1802200",
    "end": "1807450"
  },
  {
    "text": "the cases we want to catch it is really hard and this probably is still not",
    "start": "1807450",
    "end": "1812789"
  },
  {
    "text": "sufficient right and so just rule of thumb don't do this stick stick to",
    "start": "1812789",
    "end": "1819480"
  },
  {
    "text": "symptom based pages stick to detecting violations of ETA and and you are generally on the safe side so some depth",
    "start": "1819480",
    "end": "1829700"
  },
  {
    "start": "1826000",
    "end": "1985000"
  },
  {
    "text": "symptom based pages is what you want you can add cost based warnings it's really helpful right if I see that my service",
    "start": "1829700",
    "end": "1835950"
  },
  {
    "text": "is being slow and I can detected it because the database cannot be reached properly that's great but only page on",
    "start": "1835950",
    "end": "1842789"
  },
  {
    "text": "symptoms and design your loads to be adaptive to change right and sort of be",
    "start": "1842789",
    "end": "1848580"
  },
  {
    "text": "aware of of dependencies between certain metrics like error rate and quickest rate and and preserve as many dimensions",
    "start": "1848580",
    "end": "1856619"
  },
  {
    "text": "that you can and explicitly remove dimensions that you are fault-tolerant along - not page you if there's no actual problem",
    "start": "1856619",
    "end": "1863609"
  },
  {
    "text": "happening and for anything capacity planning related linear prediction is probably what you want to do yeah and",
    "start": "1863609",
    "end": "1871739"
  },
  {
    "text": "you can do most of that just stick with the basics and the rollers we get from",
    "start": "1871739",
    "end": "1876989"
  },
  {
    "text": "this whole thing they are not really meant for human consumption rights you need is a load management layer which digests everything and gives you this",
    "start": "1876989",
    "end": "1883710"
  },
  {
    "text": "nice precise overview of what's happening and gives you one page for all incidents and your incident is not",
    "start": "1883710",
    "end": "1888989"
  },
  {
    "text": "measured by how many pages are triggered and I think perfect four minutes any",
    "start": "1888989",
    "end": "1909119"
  },
  {
    "text": "questions",
    "start": "1909119",
    "end": "1911479"
  },
  {
    "text": "as the question is if we have no way to intelligently deduplicate alert okay",
    "start": "1928779",
    "end": "1934340"
  },
  {
    "text": "and so the dumb way is same labels if they exactly the same right we can did you forget them of course if they're",
    "start": "1934340",
    "end": "1941899"
  },
  {
    "text": "roughly the same the point is if EF is grouping they would generally go into the same group so in this case we don't",
    "start": "1941899",
    "end": "1948950"
  },
  {
    "text": "care about the individual words anyway they will just go into the same group and be one is diffic ane way so just",
    "start": "1948950",
    "end": "1954980"
  },
  {
    "text": "based on this grouping behavior it would probably not cause you to get page twice oh I have any visible effect from these",
    "start": "1954980",
    "end": "1960740"
  },
  {
    "text": "duplications if they cannot be sort of deduplicated properly",
    "start": "1960740",
    "end": "1967000"
  },
  {
    "text": "actually yeah and Exedy has in the github repository for v2 and v3 set up",
    "start": "1977299",
    "end": "1982970"
  },
  {
    "text": "alerts that are working",
    "start": "1982970",
    "end": "1986320"
  }
]