[
  {
    "text": "uh good morning thank you very much i",
    "start": "320",
    "end": "2399"
  },
  {
    "text": "appreciate the chance to talk this",
    "start": "2399",
    "end": "3840"
  },
  {
    "text": "morning along with travis on and",
    "start": "3840",
    "end": "6000"
  },
  {
    "text": "recorded mp4 uh about efficient uh",
    "start": "6000",
    "end": "9040"
  },
  {
    "text": "automl with ludwig ray and nodeless",
    "start": "9040",
    "end": "11679"
  },
  {
    "text": "kubernetes",
    "start": "11679",
    "end": "13120"
  },
  {
    "text": "uh i just wanted to give a brief shout",
    "start": "13120",
    "end": "14880"
  },
  {
    "text": "out to the community of people that",
    "start": "14880",
    "end": "17039"
  },
  {
    "text": "contributed to content of this talk",
    "start": "17039",
    "end": "19279"
  },
  {
    "text": "including people from the ray ludwig and",
    "start": "19279",
    "end": "21920"
  },
  {
    "text": "elodal communities uh there's four kind",
    "start": "21920",
    "end": "24240"
  },
  {
    "text": "of sources for the material our recent",
    "start": "24240",
    "end": "26720"
  },
  {
    "text": "from february uh blog on cncf about",
    "start": "26720",
    "end": "29760"
  },
  {
    "text": "managing public cloud resources for deep",
    "start": "29760",
    "end": "31760"
  },
  {
    "text": "learning training our medium blog from",
    "start": "31760",
    "end": "34399"
  },
  {
    "text": "february on ludwig automl for deep",
    "start": "34399",
    "end": "36559"
  },
  {
    "text": "learning this was focused on tabular",
    "start": "36559",
    "end": "38239"
  },
  {
    "text": "data sets",
    "start": "38239",
    "end": "39440"
  },
  {
    "text": "our more recent medium blog on ludwig",
    "start": "39440",
    "end": "42000"
  },
  {
    "text": "automl for text classification data sets",
    "start": "42000",
    "end": "44800"
  },
  {
    "text": "and finally our cloud native rejects",
    "start": "44800",
    "end": "46480"
  },
  {
    "text": "from this past fall where we talked",
    "start": "46480",
    "end": "48000"
  },
  {
    "text": "about a poc of running ray on public",
    "start": "48000",
    "end": "50640"
  },
  {
    "text": "cloud kubernetes",
    "start": "50640",
    "end": "52399"
  },
  {
    "text": "um so we're going to discuss the",
    "start": "52399",
    "end": "53920"
  },
  {
    "text": "efficiency of automl",
    "start": "53920",
    "end": "56000"
  },
  {
    "text": "with respect to two aspects one is the",
    "start": "56000",
    "end": "58559"
  },
  {
    "text": "efficiency of using automl rather than",
    "start": "58559",
    "end": "61039"
  },
  {
    "text": "doing your own you know untied uh",
    "start": "61039",
    "end": "63520"
  },
  {
    "text": "untuned search and the second is when",
    "start": "63520",
    "end": "66799"
  },
  {
    "text": "you're using automl how you can use it",
    "start": "66799",
    "end": "68720"
  },
  {
    "text": "efficiently in public cloud kubernetes",
    "start": "68720",
    "end": "71119"
  },
  {
    "text": "versus",
    "start": "71119",
    "end": "72479"
  },
  {
    "text": "using it you know like by directly",
    "start": "72479",
    "end": "74720"
  },
  {
    "text": "deploying on ec2 so first we'll give",
    "start": "74720",
    "end": "77040"
  },
  {
    "text": "some background information about",
    "start": "77040",
    "end": "80400"
  },
  {
    "text": "ludwig ray ludwig automl and",
    "start": "80400",
    "end": "83680"
  },
  {
    "text": "the nodeless kubernetes technology so",
    "start": "83680",
    "end": "86000"
  },
  {
    "text": "this is",
    "start": "86000",
    "end": "87040"
  },
  {
    "text": "where travis comes in let's see if this",
    "start": "87040",
    "end": "89920"
  },
  {
    "text": "can work",
    "start": "89920",
    "end": "92479"
  },
  {
    "text": "hi everyone thanks for coming to our",
    "start": "96079",
    "end": "97920"
  },
  {
    "text": "talk today my name is travis adair i'm",
    "start": "97920",
    "end": "100799"
  },
  {
    "text": "the cto of a company called predabase",
    "start": "100799",
    "end": "102720"
  },
  {
    "text": "building an enterprise low code machine",
    "start": "102720",
    "end": "104640"
  },
  {
    "text": "learning platform",
    "start": "104640",
    "end": "106320"
  },
  {
    "text": "built on top of ludwig and today i'd",
    "start": "106320",
    "end": "108159"
  },
  {
    "text": "like to tell you a little bit about the",
    "start": "108159",
    "end": "109840"
  },
  {
    "text": "background behind the ludwig project and",
    "start": "109840",
    "end": "112479"
  },
  {
    "text": "how",
    "start": "112479",
    "end": "113360"
  },
  {
    "text": "automl fits into the vision of what",
    "start": "113360",
    "end": "115439"
  },
  {
    "text": "we're doing with the open source lumia",
    "start": "115439",
    "end": "117200"
  },
  {
    "text": "project",
    "start": "117200",
    "end": "118799"
  },
  {
    "text": "to start i want to present the",
    "start": "118799",
    "end": "120640"
  },
  {
    "text": "background on why",
    "start": "120640",
    "end": "122079"
  },
  {
    "text": "we believe that ludwig is a valuable",
    "start": "122079",
    "end": "123840"
  },
  {
    "text": "addition to the ml ecosystem",
    "start": "123840",
    "end": "126399"
  },
  {
    "text": "so our observation is that if you look",
    "start": "126399",
    "end": "128640"
  },
  {
    "text": "at the way and all is done in industry",
    "start": "128640",
    "end": "130239"
  },
  {
    "text": "today there are essentially two",
    "start": "130239",
    "end": "132560"
  },
  {
    "text": "incomplete options that are available to",
    "start": "132560",
    "end": "134800"
  },
  {
    "text": "companies and organizations that want to",
    "start": "134800",
    "end": "136480"
  },
  {
    "text": "operationalize ml on the one hand you",
    "start": "136480",
    "end": "139120"
  },
  {
    "text": "have low level apis like tensorflow and",
    "start": "139120",
    "end": "141200"
  },
  {
    "text": "pytorch that provide a great deal of",
    "start": "141200",
    "end": "142800"
  },
  {
    "text": "flexibility and on the other hand you",
    "start": "142800",
    "end": "144800"
  },
  {
    "text": "have traditional automl systems that",
    "start": "144800",
    "end": "146560"
  },
  {
    "text": "provide a lot of simplicity but neither",
    "start": "146560",
    "end": "148959"
  },
  {
    "text": "of them end up being ideal because",
    "start": "148959",
    "end": "150879"
  },
  {
    "text": "oftentimes",
    "start": "150879",
    "end": "152319"
  },
  {
    "text": "the low-level apis are difficult to",
    "start": "152319",
    "end": "154959"
  },
  {
    "text": "get into production for non-expert users",
    "start": "154959",
    "end": "156959"
  },
  {
    "text": "while the automl systems end up being",
    "start": "156959",
    "end": "159440"
  },
  {
    "text": "these black boxes that you end up",
    "start": "159440",
    "end": "161519"
  },
  {
    "text": "graduating out of because they don't",
    "start": "161519",
    "end": "163040"
  },
  {
    "text": "always",
    "start": "163040",
    "end": "164239"
  },
  {
    "text": "solve the problem the first time around",
    "start": "164239",
    "end": "166879"
  },
  {
    "text": "and so when we look at",
    "start": "166879",
    "end": "168640"
  },
  {
    "text": "what we're doing with ludwig the core",
    "start": "168640",
    "end": "171280"
  },
  {
    "text": "insight is that we believe that there is",
    "start": "171280",
    "end": "173280"
  },
  {
    "text": "a third option that needs to be",
    "start": "173280",
    "end": "175680"
  },
  {
    "text": "explored which is the what we call",
    "start": "175680",
    "end": "177360"
  },
  {
    "text": "declarative machine learning systems",
    "start": "177360",
    "end": "179680"
  },
  {
    "text": "with declarative what we intend to do is",
    "start": "179680",
    "end": "181920"
  },
  {
    "text": "provide a high level of abstraction a",
    "start": "181920",
    "end": "184000"
  },
  {
    "text": "higher level abstraction that provides",
    "start": "184000",
    "end": "185599"
  },
  {
    "text": "the flexibility and automation needs of",
    "start": "185599",
    "end": "187760"
  },
  {
    "text": "use of automl",
    "start": "187760",
    "end": "189360"
  },
  {
    "text": "while still giving you",
    "start": "189360",
    "end": "190879"
  },
  {
    "text": "the flexibility of lower level tools",
    "start": "190879",
    "end": "194480"
  },
  {
    "text": "like pytorch",
    "start": "194480",
    "end": "196800"
  },
  {
    "text": "and opening the door for non-experts to",
    "start": "196800",
    "end": "198959"
  },
  {
    "text": "harness the power of ml",
    "start": "198959",
    "end": "201280"
  },
  {
    "text": "uh without needing to resort to these uh",
    "start": "201280",
    "end": "204000"
  },
  {
    "text": "more granular tools",
    "start": "204000",
    "end": "206239"
  },
  {
    "text": "and the way that ludwig",
    "start": "206239",
    "end": "208480"
  },
  {
    "text": "works um to kind of make this declare",
    "start": "208480",
    "end": "211040"
  },
  {
    "text": "division possible",
    "start": "211040",
    "end": "212640"
  },
  {
    "text": "is",
    "start": "212640",
    "end": "213440"
  },
  {
    "text": "uh similar to kind of systems that",
    "start": "213440",
    "end": "215280"
  },
  {
    "text": "provide infrastructure as code i'm sure",
    "start": "215280",
    "end": "217519"
  },
  {
    "text": "people who in the kubernetes community",
    "start": "217519",
    "end": "219280"
  },
  {
    "text": "community are very familiar with",
    "start": "219280",
    "end": "221120"
  },
  {
    "text": "we provide yaml configurations that",
    "start": "221120",
    "end": "223440"
  },
  {
    "text": "declare declaratively define",
    "start": "223440",
    "end": "226239"
  },
  {
    "text": "models that you might wish to train and",
    "start": "226239",
    "end": "228000"
  },
  {
    "text": "so for example it's very easy to get",
    "start": "228000",
    "end": "230159"
  },
  {
    "text": "started in ludwig you just say here's a",
    "start": "230159",
    "end": "232080"
  },
  {
    "text": "yaml config saying what my input",
    "start": "232080",
    "end": "233920"
  },
  {
    "text": "features and their types are what my",
    "start": "233920",
    "end": "235920"
  },
  {
    "text": "output features and their types are and",
    "start": "235920",
    "end": "238159"
  },
  {
    "text": "then everything",
    "start": "238159",
    "end": "239519"
  },
  {
    "text": "else the kind of how will get filled in",
    "start": "239519",
    "end": "241840"
  },
  {
    "text": "automatically on your behalf",
    "start": "241840",
    "end": "243840"
  },
  {
    "text": "but at the same time we provide a lot of",
    "start": "243840",
    "end": "246239"
  },
  {
    "text": "expert level control as well so if you",
    "start": "246239",
    "end": "248239"
  },
  {
    "text": "say i want to use a specific type of",
    "start": "248239",
    "end": "249680"
  },
  {
    "text": "mall architecture to encode a particular",
    "start": "249680",
    "end": "251920"
  },
  {
    "text": "feature",
    "start": "251920",
    "end": "252799"
  },
  {
    "text": "if you want to use a particular learning",
    "start": "252799",
    "end": "254239"
  },
  {
    "text": "rate or regularization or dropout all",
    "start": "254239",
    "end": "256880"
  },
  {
    "text": "those options are available to you as",
    "start": "256880",
    "end": "258720"
  },
  {
    "text": "well as more advanced features like",
    "start": "258720",
    "end": "260160"
  },
  {
    "text": "hyper parameter search on any of the",
    "start": "260160",
    "end": "262639"
  },
  {
    "text": "different parameters within the config",
    "start": "262639",
    "end": "265680"
  },
  {
    "text": "and what makes this all possible is the",
    "start": "265680",
    "end": "267280"
  },
  {
    "text": "ludwig architecture so every input",
    "start": "267280",
    "end": "269759"
  },
  {
    "text": "feature and output feature in your data",
    "start": "269759",
    "end": "271680"
  },
  {
    "text": "set",
    "start": "271680",
    "end": "272400"
  },
  {
    "text": "passes through an architecture we call",
    "start": "272400",
    "end": "274080"
  },
  {
    "text": "ecd for encoder compiler decoder",
    "start": "274080",
    "end": "277280"
  },
  {
    "text": "every feature is pre-processed according",
    "start": "277280",
    "end": "280160"
  },
  {
    "text": "to a pre-processing rules that you can",
    "start": "280160",
    "end": "282720"
  },
  {
    "text": "configure in the in the yaml config",
    "start": "282720",
    "end": "285680"
  },
  {
    "text": "and then encoded into a vector",
    "start": "285680",
    "end": "288160"
  },
  {
    "text": "which can be a machine learning model",
    "start": "288160",
    "end": "290320"
  },
  {
    "text": "pre-trained or otherwise or learned and",
    "start": "290320",
    "end": "292639"
  },
  {
    "text": "then all the different features are",
    "start": "292639",
    "end": "294080"
  },
  {
    "text": "combined into an embedding space",
    "start": "294080",
    "end": "297440"
  },
  {
    "text": "and then individual output features then",
    "start": "297440",
    "end": "300560"
  },
  {
    "text": "pass through a very similar decoding",
    "start": "300560",
    "end": "302320"
  },
  {
    "text": "step where we get the final prediction",
    "start": "302320",
    "end": "304639"
  },
  {
    "text": "and the benefit of this architecture is",
    "start": "304639",
    "end": "306320"
  },
  {
    "text": "that it provides a great deal of task",
    "start": "306320",
    "end": "307840"
  },
  {
    "text": "flexibility without a lot of additional",
    "start": "307840",
    "end": "309680"
  },
  {
    "text": "complexity so",
    "start": "309680",
    "end": "311199"
  },
  {
    "text": "if you want to do a regression problem",
    "start": "311199",
    "end": "312960"
  },
  {
    "text": "you can have any types of inputs and",
    "start": "312960",
    "end": "314400"
  },
  {
    "text": "then just specify a numerical output you",
    "start": "314400",
    "end": "316800"
  },
  {
    "text": "want to do speech verification you can",
    "start": "316800",
    "end": "318639"
  },
  {
    "text": "have two different audio inputs and then",
    "start": "318639",
    "end": "320400"
  },
  {
    "text": "have a binary",
    "start": "320400",
    "end": "321840"
  },
  {
    "text": "output which tells you whether or not",
    "start": "321840",
    "end": "323680"
  },
  {
    "text": "the audio streams are for example",
    "start": "323680",
    "end": "325600"
  },
  {
    "text": "equivalent or something to that effect",
    "start": "325600",
    "end": "327199"
  },
  {
    "text": "for the same speaker",
    "start": "327199",
    "end": "329039"
  },
  {
    "text": "and any number of other problems",
    "start": "329039",
    "end": "330479"
  },
  {
    "text": "including text or image or forecasting",
    "start": "330479",
    "end": "333759"
  },
  {
    "text": "tabular data problems they're all",
    "start": "333759",
    "end": "335840"
  },
  {
    "text": "possible with",
    "start": "335840",
    "end": "338000"
  },
  {
    "text": "another core component blue wig is",
    "start": "338000",
    "end": "339600"
  },
  {
    "text": "scalability and so because we integrate",
    "start": "339600",
    "end": "342240"
  },
  {
    "text": "heavily with kubernetes we also",
    "start": "342240",
    "end": "344400"
  },
  {
    "text": "integrate heavily with other distributed",
    "start": "344400",
    "end": "346160"
  },
  {
    "text": "systems that sit on top of the build on",
    "start": "346160",
    "end": "348080"
  },
  {
    "text": "top of kubernetes like ray and so all of",
    "start": "348080",
    "end": "350720"
  },
  {
    "text": "the pre-processing uh can be distributed",
    "start": "350720",
    "end": "353199"
  },
  {
    "text": "across a cluster of uh pods",
    "start": "353199",
    "end": "356000"
  },
  {
    "text": "uh using uh das gunray and our training",
    "start": "356000",
    "end": "359440"
  },
  {
    "text": "system um",
    "start": "359440",
    "end": "361039"
  },
  {
    "text": "uses a framework called horovod",
    "start": "361039",
    "end": "363600"
  },
  {
    "text": "that allows you to distribute training",
    "start": "363600",
    "end": "365280"
  },
  {
    "text": "across multiple nodes multiple gpus and",
    "start": "365280",
    "end": "368000"
  },
  {
    "text": "then model artifacts can then all be",
    "start": "368000",
    "end": "370400"
  },
  {
    "text": "uploaded to a registry like something",
    "start": "370400",
    "end": "372160"
  },
  {
    "text": "like mlflow which we support integration",
    "start": "372160",
    "end": "374080"
  },
  {
    "text": "with out of the box as well",
    "start": "374080",
    "end": "376560"
  },
  {
    "text": "for hyper parameter search it's very",
    "start": "376560",
    "end": "378160"
  },
  {
    "text": "similar and very modular again so we use",
    "start": "378160",
    "end": "381840"
  },
  {
    "text": "raytune which sits at a level on top of",
    "start": "381840",
    "end": "384479"
  },
  {
    "text": "the training process and can perturb",
    "start": "384479",
    "end": "386479"
  },
  {
    "text": "different parts of the config",
    "start": "386479",
    "end": "388240"
  },
  {
    "text": "and every one of those config",
    "start": "388240",
    "end": "390720"
  },
  {
    "text": "variants then becomes its own trial that",
    "start": "390720",
    "end": "392720"
  },
  {
    "text": "goes through the same training",
    "start": "392720",
    "end": "394400"
  },
  {
    "text": "pre-processing training and evaluation",
    "start": "394400",
    "end": "396240"
  },
  {
    "text": "step",
    "start": "396240",
    "end": "397280"
  },
  {
    "text": "as any other training process in mudwig",
    "start": "397280",
    "end": "399440"
  },
  {
    "text": "and then at the end of the day you can",
    "start": "399440",
    "end": "400880"
  },
  {
    "text": "get all the different model trials that",
    "start": "400880",
    "end": "403039"
  },
  {
    "text": "were explored and choose the one that",
    "start": "403039",
    "end": "405360"
  },
  {
    "text": "you would like to use in production",
    "start": "405360",
    "end": "408639"
  },
  {
    "text": "and when we started to look at building",
    "start": "408639",
    "end": "410080"
  },
  {
    "text": "an automl layer on top of this um our",
    "start": "410080",
    "end": "413120"
  },
  {
    "text": "goal was that we wanted to be something",
    "start": "413120",
    "end": "415199"
  },
  {
    "text": "that was ultimately a glass box and not",
    "start": "415199",
    "end": "417440"
  },
  {
    "text": "a black box and so one thing that",
    "start": "417440",
    "end": "419520"
  },
  {
    "text": "is very nice about the automl system in",
    "start": "419520",
    "end": "422080"
  },
  {
    "text": "moodwig is at the end of the day",
    "start": "422080",
    "end": "424880"
  },
  {
    "text": "you can see it as like a co-pilot that's",
    "start": "424880",
    "end": "427039"
  },
  {
    "text": "helping you generate an ideal lubricant",
    "start": "427039",
    "end": "429280"
  },
  {
    "text": "for your data set so you can start by",
    "start": "429280",
    "end": "430880"
  },
  {
    "text": "saying something as simple as",
    "start": "430880",
    "end": "432720"
  },
  {
    "text": "uh create a configuration from my data",
    "start": "432720",
    "end": "435039"
  },
  {
    "text": "set which can be a data frame or rk file",
    "start": "435039",
    "end": "437280"
  },
  {
    "text": "or whatever and then i want to predict",
    "start": "437280",
    "end": "439360"
  },
  {
    "text": "this particular column in this case",
    "start": "439360",
    "end": "440800"
  },
  {
    "text": "intent and then it can give you a config",
    "start": "440800",
    "end": "443039"
  },
  {
    "text": "that then you can do whatever you want",
    "start": "443039",
    "end": "444319"
  },
  {
    "text": "with modify anything and",
    "start": "444319",
    "end": "446639"
  },
  {
    "text": "to your heart's content",
    "start": "446639",
    "end": "448319"
  },
  {
    "text": "so how this works under the hood is you",
    "start": "448319",
    "end": "450000"
  },
  {
    "text": "just provide those two parameters plus",
    "start": "450000",
    "end": "451759"
  },
  {
    "text": "an optional time budget and then ludwig",
    "start": "451759",
    "end": "454000"
  },
  {
    "text": "automl will do some inference to",
    "start": "454000",
    "end": "456080"
  },
  {
    "text": "determine the input and output feature",
    "start": "456080",
    "end": "457599"
  },
  {
    "text": "types choose the appropriate model",
    "start": "457599",
    "end": "459599"
  },
  {
    "text": "architecture based on your task select",
    "start": "459599",
    "end": "462240"
  },
  {
    "text": "the parameters and hyperparameter ranges",
    "start": "462240",
    "end": "464560"
  },
  {
    "text": "that wants to explore",
    "start": "464560",
    "end": "466160"
  },
  {
    "text": "given the time constraints and resource",
    "start": "466160",
    "end": "467919"
  },
  {
    "text": "constraints and then launch the",
    "start": "467919",
    "end": "469520"
  },
  {
    "text": "hyperparameter search trial",
    "start": "469520",
    "end": "471520"
  },
  {
    "text": "trials on raytune using your gpu workers",
    "start": "471520",
    "end": "475039"
  },
  {
    "text": "and the outputs will be the best tuned",
    "start": "475039",
    "end": "477039"
  },
  {
    "text": "model along with other models that were",
    "start": "477039",
    "end": "478560"
  },
  {
    "text": "explored",
    "start": "478560",
    "end": "479599"
  },
  {
    "text": "and you can then take those results and",
    "start": "479599",
    "end": "481680"
  },
  {
    "text": "deploy them into production",
    "start": "481680",
    "end": "484000"
  },
  {
    "text": "as well",
    "start": "484000",
    "end": "485840"
  },
  {
    "text": "and now i'd like to hand it back to uh",
    "start": "485840",
    "end": "488240"
  },
  {
    "text": "and to talk a little bit more about",
    "start": "488240",
    "end": "490240"
  },
  {
    "text": "elodal",
    "start": "490240",
    "end": "491759"
  },
  {
    "text": "the important thing that i want to",
    "start": "491759",
    "end": "493039"
  },
  {
    "text": "emphasize here is that there's more to",
    "start": "493039",
    "end": "494479"
  },
  {
    "text": "this story than just the",
    "start": "494479",
    "end": "496319"
  },
  {
    "text": "automotive side because there's when",
    "start": "496319",
    "end": "498319"
  },
  {
    "text": "you're running this thing uh in",
    "start": "498319",
    "end": "500000"
  },
  {
    "text": "production or kind of in a large",
    "start": "500000",
    "end": "501759"
  },
  {
    "text": "distributed setting",
    "start": "501759",
    "end": "503360"
  },
  {
    "text": "there's also a component of how you want",
    "start": "503360",
    "end": "504879"
  },
  {
    "text": "to do this process efficiently",
    "start": "504879",
    "end": "507280"
  },
  {
    "text": "to optimize the usage of these uh",
    "start": "507280",
    "end": "510080"
  },
  {
    "text": "commodity resources like gpus so that",
    "start": "510080",
    "end": "512800"
  },
  {
    "text": "you're using them judiciously and not",
    "start": "512800",
    "end": "515919"
  },
  {
    "text": "wasting resources and so",
    "start": "515919",
    "end": "518080"
  },
  {
    "text": "this is where a little fits into the",
    "start": "518080",
    "end": "519360"
  },
  {
    "text": "picture particularly for running",
    "start": "519360",
    "end": "521120"
  },
  {
    "text": "kubernetes workloads and so now i'd like",
    "start": "521120",
    "end": "523599"
  },
  {
    "text": "to hand it back to ann to tell you more",
    "start": "523599",
    "end": "524959"
  },
  {
    "text": "about uh a little and the work that",
    "start": "524959",
    "end": "527279"
  },
  {
    "text": "she's done on combining the automl and",
    "start": "527279",
    "end": "530399"
  },
  {
    "text": "these other systems together",
    "start": "530399",
    "end": "533120"
  },
  {
    "text": "okay great",
    "start": "533120",
    "end": "534399"
  },
  {
    "text": "thanks um to travis",
    "start": "534399",
    "end": "536880"
  },
  {
    "text": "uh we'll go back to",
    "start": "536880",
    "end": "538720"
  },
  {
    "text": "um",
    "start": "538720",
    "end": "539600"
  },
  {
    "text": "the slide set now",
    "start": "539600",
    "end": "541440"
  },
  {
    "text": "um",
    "start": "541440",
    "end": "543680"
  },
  {
    "text": "skip forward to",
    "start": "543760",
    "end": "545839"
  },
  {
    "text": "pick it up where he left off",
    "start": "545839",
    "end": "548000"
  },
  {
    "text": "so",
    "start": "548000",
    "end": "549360"
  },
  {
    "text": "the last piece of the puzzle of the",
    "start": "549360",
    "end": "550959"
  },
  {
    "text": "background information for this talk is",
    "start": "550959",
    "end": "553120"
  },
  {
    "text": "the nodeless kubernetes luna",
    "start": "553120",
    "end": "554800"
  },
  {
    "text": "functionality it's a smart cluster",
    "start": "554800",
    "end": "556959"
  },
  {
    "text": "provisioner that runs in a standard",
    "start": "556959",
    "end": "558480"
  },
  {
    "text": "kubernetes cluster it monitors the",
    "start": "558480",
    "end": "560800"
  },
  {
    "text": "cluster for pod creation requests and",
    "start": "560800",
    "end": "563360"
  },
  {
    "text": "adds additional compute to the",
    "start": "563360",
    "end": "564800"
  },
  {
    "text": "kubernetes cluster to satisfy those",
    "start": "564800",
    "end": "566800"
  },
  {
    "text": "requests that compute can be in the form",
    "start": "566800",
    "end": "569040"
  },
  {
    "text": "of vms either on demand or spot vms or",
    "start": "569040",
    "end": "572000"
  },
  {
    "text": "it can be in the form of serverless",
    "start": "572000",
    "end": "573760"
  },
  {
    "text": "compute like for example aws fargate and",
    "start": "573760",
    "end": "576480"
  },
  {
    "text": "it chooses that compute based on current",
    "start": "576480",
    "end": "578640"
  },
  {
    "text": "availability in the cloud we already",
    "start": "578640",
    "end": "580000"
  },
  {
    "text": "heard about sometimes you can't get the",
    "start": "580000",
    "end": "581440"
  },
  {
    "text": "gpus you want the cost in the cloud and",
    "start": "581440",
    "end": "584800"
  },
  {
    "text": "other user requirements in fact the user",
    "start": "584800",
    "end": "586640"
  },
  {
    "text": "may have specific gpus that work well",
    "start": "586640",
    "end": "589440"
  },
  {
    "text": "for their workloads or some that do not",
    "start": "589440",
    "end": "592080"
  },
  {
    "text": "uh it's",
    "start": "592080",
    "end": "593279"
  },
  {
    "text": "luna is monitoring your cluster and if",
    "start": "593279",
    "end": "596240"
  },
  {
    "text": "node usage is becoming low it can remove",
    "start": "596240",
    "end": "599760"
  },
  {
    "text": "unneeded nodes from your kubernetes",
    "start": "599760",
    "end": "601519"
  },
  {
    "text": "cluster it's comparable to the",
    "start": "601519",
    "end": "603440"
  },
  {
    "text": "kubernetes cluster auto scaler uh but it",
    "start": "603440",
    "end": "605760"
  },
  {
    "text": "provides more flexible node selection",
    "start": "605760",
    "end": "607519"
  },
  {
    "text": "without the need to maintain what can",
    "start": "607519",
    "end": "609839"
  },
  {
    "text": "sometimes be hundreds of node groups to",
    "start": "609839",
    "end": "612079"
  },
  {
    "text": "handle the number of instance types",
    "start": "612079",
    "end": "613600"
  },
  {
    "text": "available it's somewhat like aws",
    "start": "613600",
    "end": "615680"
  },
  {
    "text": "carpenter but it works across cloud",
    "start": "615680",
    "end": "617440"
  },
  {
    "text": "vendors provides instance family",
    "start": "617440",
    "end": "619200"
  },
  {
    "text": "exclusions and provides deterministic",
    "start": "619200",
    "end": "621760"
  },
  {
    "text": "rule application",
    "start": "621760",
    "end": "623519"
  },
  {
    "text": "so now let's go into we've got this",
    "start": "623519",
    "end": "625360"
  },
  {
    "text": "background information let's go into",
    "start": "625360",
    "end": "627360"
  },
  {
    "text": "first",
    "start": "627360",
    "end": "628320"
  },
  {
    "text": "efficiency in lidway.automl for tabular",
    "start": "628320",
    "end": "631040"
  },
  {
    "text": "data sets and then we'll move on to text",
    "start": "631040",
    "end": "632800"
  },
  {
    "text": "classification data sets",
    "start": "632800",
    "end": "634959"
  },
  {
    "text": "so looking at the tabular data sets um",
    "start": "634959",
    "end": "637920"
  },
  {
    "text": "and looking at what automl can bring you",
    "start": "637920",
    "end": "639760"
  },
  {
    "text": "versus doing untuned search the automl",
    "start": "639760",
    "end": "642560"
  },
  {
    "text": "heuristics for tabular data sets were",
    "start": "642560",
    "end": "644880"
  },
  {
    "text": "developed by analyzing thousands of",
    "start": "644880",
    "end": "647120"
  },
  {
    "text": "hours of model training across 12 data",
    "start": "647120",
    "end": "649920"
  },
  {
    "text": "sets that we used as the training data",
    "start": "649920",
    "end": "651680"
  },
  {
    "text": "sets for the heuristics",
    "start": "651680",
    "end": "653360"
  },
  {
    "text": "so we ran three model architectures we",
    "start": "653360",
    "end": "655760"
  },
  {
    "text": "ran across 24 hyper parameters and",
    "start": "655760",
    "end": "659040"
  },
  {
    "text": "basically looked at all that data and",
    "start": "659040",
    "end": "661120"
  },
  {
    "text": "formed a set of heuristics that set of",
    "start": "661120",
    "end": "663120"
  },
  {
    "text": "heuristics includes um a particular",
    "start": "663120",
    "end": "665200"
  },
  {
    "text": "model architecture tabnet which we found",
    "start": "665200",
    "end": "667680"
  },
  {
    "text": "gave a good trade off of uh if accurate",
    "start": "667680",
    "end": "670560"
  },
  {
    "text": "models with uh a good leverage of the",
    "start": "670560",
    "end": "673120"
  },
  {
    "text": "training time involved",
    "start": "673120",
    "end": "674959"
  },
  {
    "text": "we",
    "start": "674959",
    "end": "676399"
  },
  {
    "text": "drilled down on what the search hyper",
    "start": "676399",
    "end": "678320"
  },
  {
    "text": "parameter search parameters that really",
    "start": "678320",
    "end": "679760"
  },
  {
    "text": "made a difference and narrowed their",
    "start": "679760",
    "end": "681440"
  },
  {
    "text": "ranges and we found something",
    "start": "681440",
    "end": "683120"
  },
  {
    "text": "interesting which we didn't expect",
    "start": "683120",
    "end": "684640"
  },
  {
    "text": "called transfer learning for tabular",
    "start": "684640",
    "end": "686720"
  },
  {
    "text": "data sets and that is if you've done a",
    "start": "686720",
    "end": "688800"
  },
  {
    "text": "hyperparameter search on data set a",
    "start": "688800",
    "end": "691760"
  },
  {
    "text": "and you found the best model with the",
    "start": "691760",
    "end": "693440"
  },
  {
    "text": "best hyper parameters those best type of",
    "start": "693440",
    "end": "695920"
  },
  {
    "text": "parameters will probably do a pretty",
    "start": "695920",
    "end": "697600"
  },
  {
    "text": "good job on data set b if it's somewhat",
    "start": "697600",
    "end": "699680"
  },
  {
    "text": "similar",
    "start": "699680",
    "end": "700720"
  },
  {
    "text": "and so we can uh front load the hyper",
    "start": "700720",
    "end": "703120"
  },
  {
    "text": "parameter search with those um settings",
    "start": "703120",
    "end": "705600"
  },
  {
    "text": "for the hyper parameters and get a good",
    "start": "705600",
    "end": "707360"
  },
  {
    "text": "model more quickly",
    "start": "707360",
    "end": "708959"
  },
  {
    "text": "and the fourth thing that we did was to",
    "start": "708959",
    "end": "711519"
  },
  {
    "text": "set things up so that automl uses async",
    "start": "711519",
    "end": "713920"
  },
  {
    "text": "hyperband when it's using raytune async",
    "start": "713920",
    "end": "716399"
  },
  {
    "text": "hyperband just continues on promising",
    "start": "716399",
    "end": "718639"
  },
  {
    "text": "trials so the resources aren't wasted on",
    "start": "718639",
    "end": "720399"
  },
  {
    "text": "them so creating this um",
    "start": "720399",
    "end": "723200"
  },
  {
    "text": "sort of set of heuristics we then ran",
    "start": "723200",
    "end": "724880"
  },
  {
    "text": "the heuristics on the original 12 data",
    "start": "724880",
    "end": "727200"
  },
  {
    "text": "sets it's kind of like running on your",
    "start": "727200",
    "end": "728320"
  },
  {
    "text": "training data but we were able to find a",
    "start": "728320",
    "end": "730399"
  },
  {
    "text": "competitive model in one hour so that",
    "start": "730399",
    "end": "732399"
  },
  {
    "text": "gave us some confidence that we were on",
    "start": "732399",
    "end": "733920"
  },
  {
    "text": "the right track and then we took an",
    "start": "733920",
    "end": "735839"
  },
  {
    "text": "additional nine data sets we hadn't used",
    "start": "735839",
    "end": "738000"
  },
  {
    "text": "for creating those heuristics and ran",
    "start": "738000",
    "end": "740560"
  },
  {
    "text": "validation of those heuristics and we",
    "start": "740560",
    "end": "742959"
  },
  {
    "text": "were able within a two-hour time budget",
    "start": "742959",
    "end": "744880"
  },
  {
    "text": "to get models that were competitive with",
    "start": "744880",
    "end": "746720"
  },
  {
    "text": "highly manually tuned models that were",
    "start": "746720",
    "end": "748959"
  },
  {
    "text": "publicly reported and so we'll use this",
    "start": "748959",
    "end": "751600"
  },
  {
    "text": "validation data set the way i ran it to",
    "start": "751600",
    "end": "753600"
  },
  {
    "text": "do validation to show how you know",
    "start": "753600",
    "end": "756480"
  },
  {
    "text": "difficult it was to do the validation",
    "start": "756480",
    "end": "758320"
  },
  {
    "text": "and how much easier it would have been",
    "start": "758320",
    "end": "759920"
  },
  {
    "text": "if i had used uh the luna nodeless",
    "start": "759920",
    "end": "762240"
  },
  {
    "text": "system on kubernetes",
    "start": "762240",
    "end": "764800"
  },
  {
    "text": "so we took three of the validation data",
    "start": "764800",
    "end": "766880"
  },
  {
    "text": "sets of those nine and we ran them on",
    "start": "766880",
    "end": "769440"
  },
  {
    "text": "automl for one hour two hour and four",
    "start": "769440",
    "end": "771839"
  },
  {
    "text": "hour time budgets and we compared two um",
    "start": "771839",
    "end": "775440"
  },
  {
    "text": "configurations running on public cloud",
    "start": "775440",
    "end": "777440"
  },
  {
    "text": "kubernetes with luna with the way i ran",
    "start": "777440",
    "end": "779680"
  },
  {
    "text": "it so the initial baseline of deploying",
    "start": "779680",
    "end": "782560"
  },
  {
    "text": "uh ray directly on uh",
    "start": "782560",
    "end": "785680"
  },
  {
    "text": "ec2 vms",
    "start": "785680",
    "end": "788079"
  },
  {
    "text": "so here's the the top configuration is",
    "start": "788079",
    "end": "790160"
  },
  {
    "text": "the configuration i ran three node ray",
    "start": "790160",
    "end": "792639"
  },
  {
    "text": "cluster to do the ray tune uh you know",
    "start": "792639",
    "end": "795120"
  },
  {
    "text": "auto ml search",
    "start": "795120",
    "end": "796639"
  },
  {
    "text": "the head in both workers were gpu",
    "start": "796639",
    "end": "798480"
  },
  {
    "text": "enabled i used nvidia t4 gpu so kind of",
    "start": "798480",
    "end": "802639"
  },
  {
    "text": "off-the-shelf gpu that",
    "start": "802639",
    "end": "804720"
  },
  {
    "text": "gives a good roi on the cost",
    "start": "804720",
    "end": "807920"
  },
  {
    "text": "and so",
    "start": "807920",
    "end": "808800"
  },
  {
    "text": "fixed size you know at a fixed amount of",
    "start": "808800",
    "end": "811279"
  },
  {
    "text": "compute",
    "start": "811279",
    "end": "812320"
  },
  {
    "text": "alternative one is that instead of doing",
    "start": "812320",
    "end": "814320"
  },
  {
    "text": "that um deploy the ray cluster into a",
    "start": "814320",
    "end": "818240"
  },
  {
    "text": "kubernetes cluster that's running lona",
    "start": "818240",
    "end": "820959"
  },
  {
    "text": "the nodeless kubernetes system so luna",
    "start": "820959",
    "end": "823680"
  },
  {
    "text": "will be handling auto scaling when the",
    "start": "823680",
    "end": "826399"
  },
  {
    "text": "ray auto scaler asks for it so basically",
    "start": "826399",
    "end": "828959"
  },
  {
    "text": "the again the ray head is a gpu enabled",
    "start": "828959",
    "end": "831440"
  },
  {
    "text": "node the ray workers are spun up when",
    "start": "831440",
    "end": "834000"
  },
  {
    "text": "the ray auto scaler says that more",
    "start": "834000",
    "end": "835839"
  },
  {
    "text": "resources are needed and then luna sees",
    "start": "835839",
    "end": "839199"
  },
  {
    "text": "those prods pending and goes out to the",
    "start": "839199",
    "end": "841600"
  },
  {
    "text": "you know club cloud resources and adds",
    "start": "841600",
    "end": "843680"
  },
  {
    "text": "resources to the kubernetes cluster",
    "start": "843680",
    "end": "846160"
  },
  {
    "text": "and then removes them when the ray auto",
    "start": "846160",
    "end": "847760"
  },
  {
    "text": "scaler removes them alternative two is",
    "start": "847760",
    "end": "850240"
  },
  {
    "text": "just like alternative one with one",
    "start": "850240",
    "end": "851920"
  },
  {
    "text": "difference which is that the head node",
    "start": "851920",
    "end": "853920"
  },
  {
    "text": "is a cpu only node this is kind of nice",
    "start": "853920",
    "end": "856399"
  },
  {
    "text": "because it means when the ray cluster is",
    "start": "856399",
    "end": "858320"
  },
  {
    "text": "idle",
    "start": "858320",
    "end": "859360"
  },
  {
    "text": "then you're not spending money on a gpu",
    "start": "859360",
    "end": "861839"
  },
  {
    "text": "node",
    "start": "861839",
    "end": "863519"
  },
  {
    "text": "okay so why did i choose this is my",
    "start": "863519",
    "end": "865199"
  },
  {
    "text": "baseline why did i run those nine data",
    "start": "865199",
    "end": "867519"
  },
  {
    "text": "sets in this way i had three things i",
    "start": "867519",
    "end": "869680"
  },
  {
    "text": "was trying to achieve one was i wanted a",
    "start": "869680",
    "end": "872079"
  },
  {
    "text": "standardized amount of compute for the",
    "start": "872079",
    "end": "874639"
  },
  {
    "text": "budget time budget so if i run automl",
    "start": "874639",
    "end": "877120"
  },
  {
    "text": "for one hour two hour four hours i want",
    "start": "877120",
    "end": "879040"
  },
  {
    "text": "to know what that means in compute and i",
    "start": "879040",
    "end": "881199"
  },
  {
    "text": "knew that three t4 gpu based machines",
    "start": "881199",
    "end": "885519"
  },
  {
    "text": "would give a standard amount which would",
    "start": "885519",
    "end": "887519"
  },
  {
    "text": "do a pretty good job with those time",
    "start": "887519",
    "end": "889199"
  },
  {
    "text": "budgets for tabular data sets",
    "start": "889199",
    "end": "891680"
  },
  {
    "text": "and you know the overall automl runs 10",
    "start": "891680",
    "end": "894160"
  },
  {
    "text": "trials they would give a reasonable",
    "start": "894160",
    "end": "895760"
  },
  {
    "text": "result on 10 trials",
    "start": "895760",
    "end": "897760"
  },
  {
    "text": "so that was one thing is i wanted",
    "start": "897760",
    "end": "899040"
  },
  {
    "text": "standard amount of compute the second",
    "start": "899040",
    "end": "900720"
  },
  {
    "text": "thing is i wanted reduced operational",
    "start": "900720",
    "end": "902880"
  },
  {
    "text": "complexity i didn't want to worry about",
    "start": "902880",
    "end": "904320"
  },
  {
    "text": "whether i got a legitimate run or not so",
    "start": "904320",
    "end": "906320"
  },
  {
    "text": "that was a big concern",
    "start": "906320",
    "end": "907839"
  },
  {
    "text": "from my time and the third thing was i",
    "start": "907839",
    "end": "909920"
  },
  {
    "text": "wanted to limit idle cost so i didn't",
    "start": "909920",
    "end": "912399"
  },
  {
    "text": "run you know these jobs in parallel i",
    "start": "912399",
    "end": "914959"
  },
  {
    "text": "didn't run the three one hour three two",
    "start": "914959",
    "end": "916800"
  },
  {
    "text": "hour and three four hour in parallel",
    "start": "916800",
    "end": "918480"
  },
  {
    "text": "because i wanted to limit the idle cost",
    "start": "918480",
    "end": "920720"
  },
  {
    "text": "so i was sensitive to idle cost because",
    "start": "920720",
    "end": "922480"
  },
  {
    "text": "after the job is over i would log in",
    "start": "922480",
    "end": "924880"
  },
  {
    "text": "make sure i really you know liked the",
    "start": "924880",
    "end": "927120"
  },
  {
    "text": "results in terms of that they were",
    "start": "927120",
    "end": "928720"
  },
  {
    "text": "credible that they had run properly and",
    "start": "928720",
    "end": "930800"
  },
  {
    "text": "so on so i wasn't going to you know spin",
    "start": "930800",
    "end": "933279"
  },
  {
    "text": "down the cluster the minute um that the",
    "start": "933279",
    "end": "935360"
  },
  {
    "text": "job completed and so for all these",
    "start": "935360",
    "end": "937440"
  },
  {
    "text": "reasons i had you know this fixed size i",
    "start": "937440",
    "end": "939759"
  },
  {
    "text": "chose g4d and 4x large g4 dn means t4 4x",
    "start": "939759",
    "end": "944160"
  },
  {
    "text": "large has to do with other aspects of",
    "start": "944160",
    "end": "946000"
  },
  {
    "text": "the of the virtual machine which are",
    "start": "946000",
    "end": "947680"
  },
  {
    "text": "less important to this workload but the",
    "start": "947680",
    "end": "949600"
  },
  {
    "text": "smaller instances were hard for me to",
    "start": "949600",
    "end": "952000"
  },
  {
    "text": "get when i tried and i didn't want to",
    "start": "952000",
    "end": "953519"
  },
  {
    "text": "retry things because that would just be",
    "start": "953519",
    "end": "955759"
  },
  {
    "text": "tedious for me",
    "start": "955759",
    "end": "958639"
  },
  {
    "text": "so of course the baseline of these three",
    "start": "958639",
    "end": "960560"
  },
  {
    "text": "models running you know one uh three",
    "start": "960560",
    "end": "962720"
  },
  {
    "text": "data sets running one hour two hour and",
    "start": "962720",
    "end": "964480"
  },
  {
    "text": "four hour uh you know produced",
    "start": "964480",
    "end": "966079"
  },
  {
    "text": "competitive models to manually tune",
    "start": "966079",
    "end": "967680"
  },
  {
    "text": "models the elapsed time was 22.6 hours",
    "start": "967680",
    "end": "971199"
  },
  {
    "text": "and you might say well why wasn't it 21",
    "start": "971199",
    "end": "973040"
  },
  {
    "text": "hours 3 times 1 plus 3 times 2 plus 3",
    "start": "973040",
    "end": "975440"
  },
  {
    "text": "times 4. the extra 1.6 hours was to load",
    "start": "975440",
    "end": "978320"
  },
  {
    "text": "up the data sets and pre-process them",
    "start": "978320",
    "end": "980480"
  },
  {
    "text": "and also run the uh evaluation of the",
    "start": "980480",
    "end": "983680"
  },
  {
    "text": "best model from each trial which is done",
    "start": "983680",
    "end": "985759"
  },
  {
    "text": "on the head node after the hyper",
    "start": "985759",
    "end": "987519"
  },
  {
    "text": "parameter search is completed um the",
    "start": "987519",
    "end": "989839"
  },
  {
    "text": "cost for running this um you know g4d",
    "start": "989839",
    "end": "992480"
  },
  {
    "text": "and 4x larges are 1.204 dollars an hour",
    "start": "992480",
    "end": "995600"
  },
  {
    "text": "so it cost me eighty one dollars point",
    "start": "995600",
    "end": "997839"
  },
  {
    "text": "six",
    "start": "997839",
    "end": "998560"
  },
  {
    "text": "three one cents to run this job and the",
    "start": "998560",
    "end": "1001040"
  },
  {
    "text": "idle cost was three dollars point six",
    "start": "1001040",
    "end": "1003519"
  },
  {
    "text": "one two per hour",
    "start": "1003519",
    "end": "1005440"
  },
  {
    "text": "so what are some base what are some",
    "start": "1005440",
    "end": "1006880"
  },
  {
    "text": "observations about this baseline well",
    "start": "1006880",
    "end": "1008800"
  },
  {
    "text": "it'd be nice to get the results quicker",
    "start": "1008800",
    "end": "1011120"
  },
  {
    "text": "than 22.6 hours um and you know the",
    "start": "1011120",
    "end": "1014480"
  },
  {
    "text": "obvious way to do that is to run them in",
    "start": "1014480",
    "end": "1016160"
  },
  {
    "text": "parallel but i didn't want to run them",
    "start": "1016160",
    "end": "1017680"
  },
  {
    "text": "in parallel because of my sensitivity to",
    "start": "1017680",
    "end": "1019680"
  },
  {
    "text": "idle cost and my concern that the ray",
    "start": "1019680",
    "end": "1021920"
  },
  {
    "text": "auto scaler wouldn't be able to get the",
    "start": "1021920",
    "end": "1024079"
  },
  {
    "text": "instance types that you have to ask for",
    "start": "1024079",
    "end": "1025839"
  },
  {
    "text": "when you're running directly on",
    "start": "1025839",
    "end": "1028160"
  },
  {
    "text": "you know",
    "start": "1028160",
    "end": "1029120"
  },
  {
    "text": "aws vms but the marriage of the ray auto",
    "start": "1029120",
    "end": "1032079"
  },
  {
    "text": "scaler and luna fixed that problem",
    "start": "1032079",
    "end": "1033760"
  },
  {
    "text": "because the reauto scaler could ask",
    "start": "1033760",
    "end": "1035280"
  },
  {
    "text": "instead for a certain amount of",
    "start": "1035280",
    "end": "1036558"
  },
  {
    "text": "resources and then luna could go and",
    "start": "1036559",
    "end": "1039520"
  },
  {
    "text": "broker and get those resources up from",
    "start": "1039520",
    "end": "1041360"
  },
  {
    "text": "what was available so that was really",
    "start": "1041360",
    "end": "1043038"
  },
  {
    "text": "the key to being able to move",
    "start": "1043039",
    "end": "1044959"
  },
  {
    "text": "to a more flexible system with auto",
    "start": "1044959",
    "end": "1047438"
  },
  {
    "text": "scaling now you might say well i can see",
    "start": "1047439",
    "end": "1049760"
  },
  {
    "text": "how",
    "start": "1049760",
    "end": "1050480"
  },
  {
    "text": "auto scaling is going to save time in",
    "start": "1050480",
    "end": "1052640"
  },
  {
    "text": "the idle time i can see how running in",
    "start": "1052640",
    "end": "1054320"
  },
  {
    "text": "parallel is going to reduce the elapsed",
    "start": "1054320",
    "end": "1056880"
  },
  {
    "text": "time but are you really going to save",
    "start": "1056880",
    "end": "1058559"
  },
  {
    "text": "anything while the workload is running",
    "start": "1058559",
    "end": "1060880"
  },
  {
    "text": "well you're going to save at least the",
    "start": "1060880",
    "end": "1062559"
  },
  {
    "text": "workers don't have to run during that",
    "start": "1062559",
    "end": "1064000"
  },
  {
    "text": "1.6 hours that the head is active and",
    "start": "1064000",
    "end": "1066880"
  },
  {
    "text": "the other",
    "start": "1066880",
    "end": "1067840"
  },
  {
    "text": "workers aren't active but actually",
    "start": "1067840",
    "end": "1069440"
  },
  {
    "text": "there's more savings here than that and",
    "start": "1069440",
    "end": "1071679"
  },
  {
    "text": "that's because we're using async",
    "start": "1071679",
    "end": "1073679"
  },
  {
    "text": "hyperband scheduler which is",
    "start": "1073679",
    "end": "1075919"
  },
  {
    "text": "discontinuing unpromising trials",
    "start": "1075919",
    "end": "1079039"
  },
  {
    "text": "and so if you're running 10 trials and",
    "start": "1079039",
    "end": "1081200"
  },
  {
    "text": "you get down to where there's fewer than",
    "start": "1081200",
    "end": "1082880"
  },
  {
    "text": "three trials left you don't need all of",
    "start": "1082880",
    "end": "1085280"
  },
  {
    "text": "those workers to give the amount of",
    "start": "1085280",
    "end": "1087039"
  },
  {
    "text": "compute that's needed and so this is a",
    "start": "1087039",
    "end": "1088960"
  },
  {
    "text": "picture of what's going on during that",
    "start": "1088960",
    "end": "1090760"
  },
  {
    "text": "22.6 hours so you've got time on the",
    "start": "1090760",
    "end": "1093840"
  },
  {
    "text": "x-axis and you've got the data sets",
    "start": "1093840",
    "end": "1096799"
  },
  {
    "text": "and their trials on the y-axis you know",
    "start": "1096799",
    "end": "1099120"
  },
  {
    "text": "the one hour one hour one hour for the",
    "start": "1099120",
    "end": "1101039"
  },
  {
    "text": "first three data sets then two hours for",
    "start": "1101039",
    "end": "1103440"
  },
  {
    "text": "each and then so on and so you can see",
    "start": "1103440",
    "end": "1105200"
  },
  {
    "text": "that that first data set only really two",
    "start": "1105200",
    "end": "1107919"
  },
  {
    "text": "trials survived to the end of the time",
    "start": "1107919",
    "end": "1109840"
  },
  {
    "text": "so you didn't need three workers while",
    "start": "1109840",
    "end": "1111440"
  },
  {
    "text": "that trial was running you just really",
    "start": "1111440",
    "end": "1112960"
  },
  {
    "text": "mostly needed two same the second data",
    "start": "1112960",
    "end": "1115600"
  },
  {
    "text": "set you only needed one the third one",
    "start": "1115600",
    "end": "1117120"
  },
  {
    "text": "you needed more but that's what auto",
    "start": "1117120",
    "end": "1118400"
  },
  {
    "text": "scaling is all about so there's a lot of",
    "start": "1118400",
    "end": "1120000"
  },
  {
    "text": "opportunity here to spin down workers",
    "start": "1120000",
    "end": "1122240"
  },
  {
    "text": "even during the run",
    "start": "1122240",
    "end": "1124400"
  },
  {
    "text": "so that moves us to the gpu uh head rake",
    "start": "1124400",
    "end": "1128799"
  },
  {
    "text": "configuration",
    "start": "1128799",
    "end": "1130320"
  },
  {
    "text": "with luna running on kubernetes",
    "start": "1130320",
    "end": "1132880"
  },
  {
    "text": "so deployed in an eks cluster with luna",
    "start": "1132880",
    "end": "1135600"
  },
  {
    "text": "there to bring things in when they ray",
    "start": "1135600",
    "end": "1137280"
  },
  {
    "text": "out a scalar asks for them and this time",
    "start": "1137280",
    "end": "1139440"
  },
  {
    "text": "we can run the uh three workloads uh",
    "start": "1139440",
    "end": "1141520"
  },
  {
    "text": "three data sets in parallel for each of",
    "start": "1141520",
    "end": "1143120"
  },
  {
    "text": "the time basis and we set max concurrent",
    "start": "1143120",
    "end": "1146240"
  },
  {
    "text": "trials to three for each of the three",
    "start": "1146240",
    "end": "1147760"
  },
  {
    "text": "data sets running in parallel so they",
    "start": "1147760",
    "end": "1149520"
  },
  {
    "text": "would get that standard amount of",
    "start": "1149520",
    "end": "1150799"
  },
  {
    "text": "compute when they ran",
    "start": "1150799",
    "end": "1153039"
  },
  {
    "text": "so again we didn't have any compromise",
    "start": "1153039",
    "end": "1155120"
  },
  {
    "text": "on model accuracy it was just like the",
    "start": "1155120",
    "end": "1157039"
  },
  {
    "text": "baseline but the elapsed time was much",
    "start": "1157039",
    "end": "1159200"
  },
  {
    "text": "shorter 8.75 hours instead of 22.6 so a",
    "start": "1159200",
    "end": "1163360"
  },
  {
    "text": "reduction of 61 percent the idle cost of",
    "start": "1163360",
    "end": "1166160"
  },
  {
    "text": "course down by 66 because only the head",
    "start": "1166160",
    "end": "1168960"
  },
  {
    "text": "is running um not the ed workers um and",
    "start": "1168960",
    "end": "1171760"
  },
  {
    "text": "the workload cost was 54 lower than the",
    "start": "1171760",
    "end": "1174480"
  },
  {
    "text": "baseline um just to break down that 54",
    "start": "1174480",
    "end": "1177840"
  },
  {
    "text": "more than half of it was coming from the",
    "start": "1177840",
    "end": "1179440"
  },
  {
    "text": "auto scaling we just talked about uh",
    "start": "1179440",
    "end": "1181600"
  },
  {
    "text": "during the the run",
    "start": "1181600",
    "end": "1183120"
  },
  {
    "text": "but then some percentage of it was talk",
    "start": "1183120",
    "end": "1185039"
  },
  {
    "text": "was obtained because luna was able to",
    "start": "1185039",
    "end": "1186880"
  },
  {
    "text": "get cheaper instances uh that i was able",
    "start": "1186880",
    "end": "1189520"
  },
  {
    "text": "to you know mess around and try to get",
    "start": "1189520",
    "end": "1191520"
  },
  {
    "text": "when i would bring things up annually so",
    "start": "1191520",
    "end": "1194400"
  },
  {
    "text": "luna put the head node on the 4x large",
    "start": "1194400",
    "end": "1197440"
  },
  {
    "text": "but it put the workers on 2x large based",
    "start": "1197440",
    "end": "1199600"
  },
  {
    "text": "on the resource requirements",
    "start": "1199600",
    "end": "1202880"
  },
  {
    "text": "so now moving to the cpu only head",
    "start": "1202880",
    "end": "1206559"
  },
  {
    "text": "this is nice because a idle is cheaper",
    "start": "1206559",
    "end": "1209919"
  },
  {
    "text": "and b if idle is cheap enough you might",
    "start": "1209919",
    "end": "1211919"
  },
  {
    "text": "even not have to um bother to spin down",
    "start": "1211919",
    "end": "1214720"
  },
  {
    "text": "the array cluster and bring it back up",
    "start": "1214720",
    "end": "1216240"
  },
  {
    "text": "if you're willing to kind of spend way",
    "start": "1216240",
    "end": "1218720"
  },
  {
    "text": "less money so that so this is the run",
    "start": "1218720",
    "end": "1221200"
  },
  {
    "text": "there",
    "start": "1221200",
    "end": "1222240"
  },
  {
    "text": "again no compromise on model accuracy",
    "start": "1222240",
    "end": "1225120"
  },
  {
    "text": "the elapsed time was a little bit longer",
    "start": "1225120",
    "end": "1226559"
  },
  {
    "text": "because the cpu head wasn't as efficient",
    "start": "1226559",
    "end": "1229039"
  },
  {
    "text": "and running the evaluation at the end of",
    "start": "1229039",
    "end": "1230960"
  },
  {
    "text": "each you know of a set of trials as a",
    "start": "1230960",
    "end": "1233120"
  },
  {
    "text": "gpu node on the head but not too much of",
    "start": "1233120",
    "end": "1235600"
  },
  {
    "text": "a compromise an extra 15 minutes",
    "start": "1235600",
    "end": "1237840"
  },
  {
    "text": "the idle cost was significantly lower",
    "start": "1237840",
    "end": "1240559"
  },
  {
    "text": "than the gpu-based machine so 0.45 2",
    "start": "1240559",
    "end": "1245679"
  },
  {
    "text": "and the workload cost was essentially",
    "start": "1245679",
    "end": "1247360"
  },
  {
    "text": "the same",
    "start": "1247360",
    "end": "1248799"
  },
  {
    "text": "so that's the story around tabular data",
    "start": "1248799",
    "end": "1250640"
  },
  {
    "text": "sets look at let's look at text",
    "start": "1250640",
    "end": "1252159"
  },
  {
    "text": "classification data sets um can you get",
    "start": "1252159",
    "end": "1254960"
  },
  {
    "text": "an advantage running automl with text",
    "start": "1254960",
    "end": "1257200"
  },
  {
    "text": "classification data sets versus your own",
    "start": "1257200",
    "end": "1260480"
  },
  {
    "text": "untuned search well i actually thought",
    "start": "1260480",
    "end": "1262480"
  },
  {
    "text": "this wouldn't be interesting i mean i",
    "start": "1262480",
    "end": "1264640"
  },
  {
    "text": "was just naive or something but i'm like",
    "start": "1264640",
    "end": "1266320"
  },
  {
    "text": "oh you know i can read a bunch of papers",
    "start": "1266320",
    "end": "1268799"
  },
  {
    "text": "i can just use bird base uh bert base",
    "start": "1268799",
    "end": "1271840"
  },
  {
    "text": "modeling is is",
    "start": "1271840",
    "end": "1273760"
  },
  {
    "text": "widely applied to text classification",
    "start": "1273760",
    "end": "1276000"
  },
  {
    "text": "you know here's the batch size ranges i",
    "start": "1276000",
    "end": "1277919"
  },
  {
    "text": "should use here's the learning rate",
    "start": "1277919",
    "end": "1279200"
  },
  {
    "text": "ranges i should use here's the optimizer",
    "start": "1279200",
    "end": "1281520"
  },
  {
    "text": "i should use i'll just do that and it'll",
    "start": "1281520",
    "end": "1284159"
  },
  {
    "text": "just work i mean there's automl won't",
    "start": "1284159",
    "end": "1286080"
  },
  {
    "text": "really be doing anything other than",
    "start": "1286080",
    "end": "1287919"
  },
  {
    "text": "stamping out what people in the",
    "start": "1287919",
    "end": "1289520"
  },
  {
    "text": "literature tell you to do and so i had",
    "start": "1289520",
    "end": "1291679"
  },
  {
    "text": "10 data sets i was going to use to",
    "start": "1291679",
    "end": "1293360"
  },
  {
    "text": "create these heuristics and i ran this",
    "start": "1293360",
    "end": "1296080"
  },
  {
    "text": "set of",
    "start": "1296080",
    "end": "1297679"
  },
  {
    "text": "parameters to to train those models to",
    "start": "1297679",
    "end": "1300000"
  },
  {
    "text": "see what would happen well three of them",
    "start": "1300000",
    "end": "1302159"
  },
  {
    "text": "got great results within in one hour and",
    "start": "1302159",
    "end": "1304080"
  },
  {
    "text": "the other seven um crashed",
    "start": "1304080",
    "end": "1306480"
  },
  {
    "text": "and they crashed because you know bert",
    "start": "1306480",
    "end": "1308640"
  },
  {
    "text": "bass is a",
    "start": "1308640",
    "end": "1310240"
  },
  {
    "text": "transformer model it's quadratic in the",
    "start": "1310240",
    "end": "1312320"
  },
  {
    "text": "input text token length and i was trying",
    "start": "1312320",
    "end": "1314799"
  },
  {
    "text": "to run on commodity you know t4 gpus and",
    "start": "1314799",
    "end": "1317840"
  },
  {
    "text": "they wouldn't fit in memory",
    "start": "1317840",
    "end": "1319600"
  },
  {
    "text": "and so spen actually spent the next two",
    "start": "1319600",
    "end": "1321440"
  },
  {
    "text": "months uh",
    "start": "1321440",
    "end": "1323360"
  },
  {
    "text": "injecting a whole bunch of heuristics",
    "start": "1323360",
    "end": "1325280"
  },
  {
    "text": "into automl so that they could be",
    "start": "1325280",
    "end": "1327280"
  },
  {
    "text": "trained on this kind of commodity gpu so",
    "start": "1327280",
    "end": "1329440"
  },
  {
    "text": "there really is a value to automl even",
    "start": "1329440",
    "end": "1331760"
  },
  {
    "text": "for pre-trained models that you're",
    "start": "1331760",
    "end": "1333280"
  },
  {
    "text": "fine-tuning so this included adding a",
    "start": "1333280",
    "end": "1335679"
  },
  {
    "text": "tune for memory option to automl that",
    "start": "1335679",
    "end": "1338559"
  },
  {
    "text": "says hey you know change the",
    "start": "1338559",
    "end": "1340720"
  },
  {
    "text": "hyperparameter search strategy and other",
    "start": "1340720",
    "end": "1342720"
  },
  {
    "text": "things if the model is projected not to",
    "start": "1342720",
    "end": "1345120"
  },
  {
    "text": "fit in memory during training also a lot",
    "start": "1345120",
    "end": "1347280"
  },
  {
    "text": "of stuff there",
    "start": "1347280",
    "end": "1348720"
  },
  {
    "text": "including the fact that limiting the",
    "start": "1348720",
    "end": "1350240"
  },
  {
    "text": "input token length is kind of a good",
    "start": "1350240",
    "end": "1352720"
  },
  {
    "text": "trade-off first before you reduce batch",
    "start": "1352720",
    "end": "1354880"
  },
  {
    "text": "size and so on so a bunch of stuff here",
    "start": "1354880",
    "end": "1357679"
  },
  {
    "text": "i'll highlight that particular choice um",
    "start": "1357679",
    "end": "1360720"
  },
  {
    "text": "also limit the number of trials needed",
    "start": "1360720",
    "end": "1362480"
  },
  {
    "text": "you remember in tabular data sets we",
    "start": "1362480",
    "end": "1364000"
  },
  {
    "text": "were running trend 10 trials by default",
    "start": "1364000",
    "end": "1366320"
  },
  {
    "text": "we run here but five here but if we do",
    "start": "1366320",
    "end": "1369440"
  },
  {
    "text": "look at the combinatorics and there's",
    "start": "1369440",
    "end": "1370880"
  },
  {
    "text": "less than five possible combinations we",
    "start": "1370880",
    "end": "1372880"
  },
  {
    "text": "reduce the number of trials further",
    "start": "1372880",
    "end": "1375360"
  },
  {
    "text": "setting the max epoch rate max epoch",
    "start": "1375360",
    "end": "1377840"
  },
  {
    "text": "count to six because uh these",
    "start": "1377840",
    "end": "1379919"
  },
  {
    "text": "pre-trained models when you're",
    "start": "1379919",
    "end": "1381280"
  },
  {
    "text": "fine-tuning them once you get past about",
    "start": "1381280",
    "end": "1383440"
  },
  {
    "text": "six you're moving into the range of",
    "start": "1383440",
    "end": "1385039"
  },
  {
    "text": "catastrophic forgetting it's not",
    "start": "1385039",
    "end": "1386559"
  },
  {
    "text": "interesting and so on so with these",
    "start": "1386559",
    "end": "1389200"
  },
  {
    "text": "heuristics the original 10 data sets so",
    "start": "1389200",
    "end": "1392159"
  },
  {
    "text": "the training set worked well um and the",
    "start": "1392159",
    "end": "1394559"
  },
  {
    "text": "other thing we did here is um because",
    "start": "1394559",
    "end": "1397039"
  },
  {
    "text": "it's combinatoric we were sort of uh",
    "start": "1397039",
    "end": "1400559"
  },
  {
    "text": "we sort of needed to move into regimen",
    "start": "1400559",
    "end": "1402159"
  },
  {
    "text": "where you had a particular time a budget",
    "start": "1402159",
    "end": "1404799"
  },
  {
    "text": "set uniquely per data set so we couldn't",
    "start": "1404799",
    "end": "1406960"
  },
  {
    "text": "stick with the kind of fixed time budget",
    "start": "1406960",
    "end": "1408960"
  },
  {
    "text": "so anyway that worked well for the 10 we",
    "start": "1408960",
    "end": "1411440"
  },
  {
    "text": "then tried it on the additional five",
    "start": "1411440",
    "end": "1413200"
  },
  {
    "text": "data sets that we hadn't seen before",
    "start": "1413200",
    "end": "1414880"
  },
  {
    "text": "when we formed these heuristics",
    "start": "1414880",
    "end": "1417520"
  },
  {
    "text": "and got accuracy competitive with",
    "start": "1417520",
    "end": "1419679"
  },
  {
    "text": "publicly reported models",
    "start": "1419679",
    "end": "1421919"
  },
  {
    "text": "so again i did the same experiment that",
    "start": "1421919",
    "end": "1424400"
  },
  {
    "text": "i did for tabular data sets which was",
    "start": "1424400",
    "end": "1426799"
  },
  {
    "text": "choose three of the data sets from the",
    "start": "1426799",
    "end": "1428320"
  },
  {
    "text": "validation and see how they would have",
    "start": "1428320",
    "end": "1430320"
  },
  {
    "text": "worked",
    "start": "1430320",
    "end": "1431679"
  },
  {
    "text": "on my baseline configuration versus um",
    "start": "1431679",
    "end": "1435279"
  },
  {
    "text": "nodeless kubernetes luna",
    "start": "1435279",
    "end": "1437600"
  },
  {
    "text": "running on a kubernetes cluster and so",
    "start": "1437600",
    "end": "1439600"
  },
  {
    "text": "doing that i uh these are the three data",
    "start": "1439600",
    "end": "1441600"
  },
  {
    "text": "sets i chose",
    "start": "1441600",
    "end": "1443279"
  },
  {
    "text": "i was also able to save a bunch of",
    "start": "1443279",
    "end": "1445520"
  },
  {
    "text": "resources during runtime but it's kind",
    "start": "1445520",
    "end": "1447039"
  },
  {
    "text": "of interesting",
    "start": "1447039",
    "end": "1448480"
  },
  {
    "text": "where those resource savings came from",
    "start": "1448480",
    "end": "1450640"
  },
  {
    "text": "remember in tabular state of data sets",
    "start": "1450640",
    "end": "1452400"
  },
  {
    "text": "it came from the idea that async",
    "start": "1452400",
    "end": "1454880"
  },
  {
    "text": "hyperband was discontinuing certain",
    "start": "1454880",
    "end": "1457039"
  },
  {
    "text": "parts of the run here it was coming from",
    "start": "1457039",
    "end": "1459120"
  },
  {
    "text": "the fact that we didn't need as many",
    "start": "1459120",
    "end": "1460720"
  },
  {
    "text": "trials all three of these data sets",
    "start": "1460720",
    "end": "1462880"
  },
  {
    "text": "because they used tuned for memory and",
    "start": "1462880",
    "end": "1464480"
  },
  {
    "text": "because it reduced",
    "start": "1464480",
    "end": "1466080"
  },
  {
    "text": "the hyper parameter search range uh only",
    "start": "1466080",
    "end": "1468320"
  },
  {
    "text": "ran two trials at max",
    "start": "1468320",
    "end": "1470799"
  },
  {
    "text": "and then several of the data sets also",
    "start": "1470799",
    "end": "1472880"
  },
  {
    "text": "didn't have to run their full time",
    "start": "1472880",
    "end": "1474400"
  },
  {
    "text": "budget because they hit the six epoch",
    "start": "1474400",
    "end": "1476640"
  },
  {
    "text": "mark",
    "start": "1476640",
    "end": "1478159"
  },
  {
    "text": "so if you look this is a picture of the",
    "start": "1478159",
    "end": "1479600"
  },
  {
    "text": "runtime for these um three data sets",
    "start": "1479600",
    "end": "1482400"
  },
  {
    "text": "being validated so again",
    "start": "1482400",
    "end": "1484559"
  },
  {
    "text": "a big savings and compute cost and",
    "start": "1484559",
    "end": "1487200"
  },
  {
    "text": "idle cluster costs as the gpu head not",
    "start": "1487200",
    "end": "1489760"
  },
  {
    "text": "so much savings and elapsed time this",
    "start": "1489760",
    "end": "1491360"
  },
  {
    "text": "time because elapsed time was dominated",
    "start": "1491360",
    "end": "1493360"
  },
  {
    "text": "by that one big data set",
    "start": "1493360",
    "end": "1495760"
  },
  {
    "text": "so anyway this is uh",
    "start": "1495760",
    "end": "1497520"
  },
  {
    "text": "the sort of in this talk we've talked",
    "start": "1497520",
    "end": "1499360"
  },
  {
    "text": "about the savings you can get in your",
    "start": "1499360",
    "end": "1501520"
  },
  {
    "text": "own efficiency by using automl rather",
    "start": "1501520",
    "end": "1504400"
  },
  {
    "text": "than your untuned search and then we've",
    "start": "1504400",
    "end": "1506799"
  },
  {
    "text": "talked about how once you are using",
    "start": "1506799",
    "end": "1508799"
  },
  {
    "text": "automl there's a lot of leverage",
    "start": "1508799",
    "end": "1511039"
  },
  {
    "text": "in using automl on top of a kubernetes",
    "start": "1511039",
    "end": "1513679"
  },
  {
    "text": "cluster with nodeless um luna technology",
    "start": "1513679",
    "end": "1517279"
  },
  {
    "text": "so in the future we're intending to",
    "start": "1517279",
    "end": "1519120"
  },
  {
    "text": "continue to extend ludwig auto and",
    "start": "1519120",
    "end": "1521039"
  },
  {
    "text": "melting new domains and we're continuing",
    "start": "1521039",
    "end": "1523600"
  },
  {
    "text": "to enhance luna as needed to handle",
    "start": "1523600",
    "end": "1526159"
  },
  {
    "text": "efficient scaling of all sorts of",
    "start": "1526159",
    "end": "1527679"
  },
  {
    "text": "workloads including machine learning",
    "start": "1527679",
    "end": "1529919"
  },
  {
    "text": "across public cloud vendors so thanks",
    "start": "1529919",
    "end": "1532159"
  },
  {
    "text": "very much appreciate it and if there's",
    "start": "1532159",
    "end": "1534720"
  },
  {
    "text": "any questions i'm happy to",
    "start": "1534720",
    "end": "1536480"
  },
  {
    "text": "hear take them",
    "start": "1536480",
    "end": "1538350"
  },
  {
    "text": "[Applause]",
    "start": "1538350",
    "end": "1545279"
  },
  {
    "text": "does anybody have any questions they'd",
    "start": "1545279",
    "end": "1546559"
  },
  {
    "text": "like to ask",
    "start": "1546559",
    "end": "1549200"
  },
  {
    "text": "we've got one question here i'm gonna",
    "start": "1550240",
    "end": "1551840"
  },
  {
    "text": "get the microphone because we haven't",
    "start": "1551840",
    "end": "1554880"
  },
  {
    "text": "we have one over here",
    "start": "1554880",
    "end": "1557840"
  },
  {
    "text": "you want to go back here do you okay he",
    "start": "1557840",
    "end": "1559520"
  },
  {
    "text": "wants to get on stage okay",
    "start": "1559520",
    "end": "1562960"
  },
  {
    "text": "so you were talking about the idle cost",
    "start": "1564320",
    "end": "1567600"
  },
  {
    "text": "savings",
    "start": "1567600",
    "end": "1568799"
  },
  {
    "text": "um but you seem to be assuming that you",
    "start": "1568799",
    "end": "1571039"
  },
  {
    "text": "had basically a single user on the",
    "start": "1571039",
    "end": "1572400"
  },
  {
    "text": "kubernetes cluster it seems like you",
    "start": "1572400",
    "end": "1573840"
  },
  {
    "text": "could also get a lot of idle cost",
    "start": "1573840",
    "end": "1575679"
  },
  {
    "text": "savings if you were able to segment",
    "start": "1575679",
    "end": "1578159"
  },
  {
    "text": "things by namespace and have multiple",
    "start": "1578159",
    "end": "1580240"
  },
  {
    "text": "users share the same cluster oh yeah",
    "start": "1580240",
    "end": "1582240"
  },
  {
    "text": "that's a really good point yeah i think",
    "start": "1582240",
    "end": "1583760"
  },
  {
    "text": "i was this is kind of a selfish view of",
    "start": "1583760",
    "end": "1585600"
  },
  {
    "text": "the world i think i was sort of spinning",
    "start": "1585600",
    "end": "1587520"
  },
  {
    "text": "up the kubernetes cluster just for",
    "start": "1587520",
    "end": "1588960"
  },
  {
    "text": "myself and then uh deploying ray there",
    "start": "1588960",
    "end": "1591600"
  },
  {
    "text": "but i think it's a very good point yeah",
    "start": "1591600",
    "end": "1593120"
  },
  {
    "text": "i think that's a",
    "start": "1593120",
    "end": "1594320"
  },
  {
    "text": "that's something we should uh think",
    "start": "1594320",
    "end": "1596000"
  },
  {
    "text": "about more yep",
    "start": "1596000",
    "end": "1599799"
  },
  {
    "text": "okay",
    "start": "1602880",
    "end": "1605120"
  },
  {
    "text": "all right well thank you very much enjoy",
    "start": "1606240",
    "end": "1608080"
  },
  {
    "text": "the rest of your day",
    "start": "1608080",
    "end": "1611559"
  },
  {
    "text": "oh yeah so aluna is um uh i guess a lot",
    "start": "1622799",
    "end": "1626240"
  },
  {
    "text": "all that",
    "start": "1626240",
    "end": "1627440"
  },
  {
    "text": "actually uh the lodal company um has",
    "start": "1627440",
    "end": "1630480"
  },
  {
    "text": "luna so uh let's see i think i don't",
    "start": "1630480",
    "end": "1633120"
  },
  {
    "text": "know if you want to share the website",
    "start": "1633120",
    "end": "1636559"
  },
  {
    "text": "what's the question again uh where can",
    "start": "1641440",
    "end": "1643440"
  },
  {
    "text": "people get more information about luna",
    "start": "1643440",
    "end": "1645520"
  },
  {
    "text": "oh illiteral.com",
    "start": "1645520",
    "end": "1647600"
  },
  {
    "text": "luna",
    "start": "1647600",
    "end": "1649200"
  },
  {
    "text": "thanks",
    "start": "1649200",
    "end": "1651519"
  },
  {
    "text": "thanks thank you",
    "start": "1652480",
    "end": "1654300"
  },
  {
    "text": "[Applause]",
    "start": "1654300",
    "end": "1658250"
  }
]