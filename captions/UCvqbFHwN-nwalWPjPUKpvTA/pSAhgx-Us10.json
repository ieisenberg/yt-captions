[
  {
    "text": "all right so let's uh let's start um the",
    "start": "280",
    "end": "4000"
  },
  {
    "text": "first talk today is uh building a",
    "start": "4000",
    "end": "5799"
  },
  {
    "text": "multicluster privately hosted llm",
    "start": "5799",
    "end": "8000"
  },
  {
    "text": "serving platform on kubernetes our",
    "start": "8000",
    "end": "10840"
  },
  {
    "text": "speakers are um Noah Yoshida and Julian",
    "start": "10840",
    "end": "14960"
  },
  {
    "text": "bright from predy base so thanks a lot",
    "start": "14960",
    "end": "17119"
  },
  {
    "text": "and let's welcome",
    "start": "17119",
    "end": "19960"
  },
  {
    "text": "them thank thank you everyone and",
    "start": "23800",
    "end": "25880"
  },
  {
    "text": "apologies for the the slow start um it's",
    "start": "25880",
    "end": "28560"
  },
  {
    "text": "not because I'm from Australia it's just",
    "start": "28560",
    "end": "30119"
  },
  {
    "text": "generally a technical issue but uh yes I",
    "start": "30119",
    "end": "32398"
  },
  {
    "text": "have arrived uh just yesterday actually",
    "start": "32399",
    "end": "34480"
  },
  {
    "text": "from Australia and it's great to be here",
    "start": "34480",
    "end": "36360"
  },
  {
    "text": "um Chicago is a fantastic City I hope",
    "start": "36360",
    "end": "38600"
  },
  {
    "text": "you all have a great time over the the",
    "start": "38600",
    "end": "40600"
  },
  {
    "text": "course of the next few days so we're",
    "start": "40600",
    "end": "42360"
  },
  {
    "text": "talking about privately hosted llms on",
    "start": "42360",
    "end": "45520"
  },
  {
    "text": "kubernetes um we're from pretty Bas uh",
    "start": "45520",
    "end": "48239"
  },
  {
    "text": "so I'm a platform engineering uh manager",
    "start": "48239",
    "end": "51480"
  },
  {
    "text": "in pretty Bas so we've both been with",
    "start": "51480",
    "end": "53480"
  },
  {
    "text": "company pretty much since the early days",
    "start": "53480",
    "end": "55680"
  },
  {
    "text": "we're a startup about 2 years into our",
    "start": "55680",
    "end": "57719"
  },
  {
    "text": "journey and we're really excited about",
    "start": "57719",
    "end": "60399"
  },
  {
    "text": "large language models and we've built",
    "start": "60399",
    "end": "62239"
  },
  {
    "text": "our platform on kubernetes so we'll be",
    "start": "62239",
    "end": "64119"
  },
  {
    "text": "talking a little bit more um today and",
    "start": "64119",
    "end": "66119"
  },
  {
    "text": "no will be diving deep on some of those",
    "start": "66119",
    "end": "68119"
  },
  {
    "text": "technical um challenges and and sort of",
    "start": "68119",
    "end": "71240"
  },
  {
    "text": "ways we've approached solving some of",
    "start": "71240",
    "end": "73720"
  },
  {
    "text": "these so briefly I was a bit surprised",
    "start": "73720",
    "end": "76560"
  },
  {
    "text": "to see llms so small on the on the sort",
    "start": "76560",
    "end": "78840"
  },
  {
    "text": "of topic slide there because you know",
    "start": "78840",
    "end": "80600"
  },
  {
    "text": "that's all we think and talk about these",
    "start": "80600",
    "end": "82119"
  },
  {
    "text": "days but for those who aren't familiar",
    "start": "82119",
    "end": "83759"
  },
  {
    "text": "with llms they stand for large language",
    "start": "83759",
    "end": "86159"
  },
  {
    "text": "models so these started you know back in",
    "start": "86159",
    "end": "88720"
  },
  {
    "text": "you know the early days of 2012 when the",
    "start": "88720",
    "end": "91000"
  },
  {
    "text": "first generative pre-trained Transformer",
    "start": "91000",
    "end": "93280"
  },
  {
    "text": "model or GPT came out but since then you",
    "start": "93280",
    "end": "96399"
  },
  {
    "text": "know there's been a sort of real",
    "start": "96399",
    "end": "98399"
  },
  {
    "text": "progression in the last couple years in",
    "start": "98399",
    "end": "100640"
  },
  {
    "text": "particular with gpt3 from open AI",
    "start": "100640",
    "end": "103040"
  },
  {
    "text": "leading to chat gbt which I'm sure",
    "start": "103040",
    "end": "104880"
  },
  {
    "text": "you've all heard of um and that really",
    "start": "104880",
    "end": "106960"
  },
  {
    "text": "powerful very large 175 billion",
    "start": "106960",
    "end": "109079"
  },
  {
    "text": "parameter model has made it possible to",
    "start": "109079",
    "end": "111200"
  },
  {
    "text": "do some incredible things um so these",
    "start": "111200",
    "end": "114000"
  },
  {
    "text": "these models are trained on vast amounts",
    "start": "114000",
    "end": "115840"
  },
  {
    "text": "of data but they're also very flexible",
    "start": "115840",
    "end": "117840"
  },
  {
    "text": "and very um you know intelligent in",
    "start": "117840",
    "end": "119920"
  },
  {
    "text": "terms of what they can do out of the box",
    "start": "119920",
    "end": "121719"
  },
  {
    "text": "with what's called like a zero shot mode",
    "start": "121719",
    "end": "123600"
  },
  {
    "text": "so effectively you provide an input a",
    "start": "123600",
    "end": "125479"
  },
  {
    "text": "prompt and you know we can do things",
    "start": "125479",
    "end": "127320"
  },
  {
    "text": "like CER analysis uh transcription many",
    "start": "127320",
    "end": "130599"
  },
  {
    "text": "other um sort of text completion tasks",
    "start": "130599",
    "end": "133160"
  },
  {
    "text": "obviously writing pros and other pieces",
    "start": "133160",
    "end": "135280"
  },
  {
    "text": "of content um but they're also use",
    "start": "135280",
    "end": "137680"
  },
  {
    "text": "useful in Enterprise context which is",
    "start": "137680",
    "end": "139440"
  },
  {
    "text": "where most of our customers are",
    "start": "139440",
    "end": "141160"
  },
  {
    "text": "interested and in fact you know using an",
    "start": "141160",
    "end": "144120"
  },
  {
    "text": "open source model and being able to fine",
    "start": "144120",
    "end": "145800"
  },
  {
    "text": "tune and present those um you know sort",
    "start": "145800",
    "end": "148519"
  },
  {
    "text": "of serving inference um insights back to",
    "start": "148519",
    "end": "151280"
  },
  {
    "text": "their business is a really critical",
    "start": "151280",
    "end": "152959"
  },
  {
    "text": "capability so what you can see on this",
    "start": "152959",
    "end": "155200"
  },
  {
    "text": "slide here is that you know since those",
    "start": "155200",
    "end": "157440"
  },
  {
    "text": "large language models have appeared",
    "start": "157440",
    "end": "158879"
  },
  {
    "text": "we've had a lot of Open Source",
    "start": "158879",
    "end": "160599"
  },
  {
    "text": "investment particularly um models like",
    "start": "160599",
    "end": "162519"
  },
  {
    "text": "metas llama and llama 2 which are",
    "start": "162519",
    "end": "164720"
  },
  {
    "text": "available in in sort of smaller sizes of",
    "start": "164720",
    "end": "167000"
  },
  {
    "text": "7 13 and 70 billion parameter but still",
    "start": "167000",
    "end": "169920"
  },
  {
    "text": "very powerful and when you fine tune",
    "start": "169920",
    "end": "172040"
  },
  {
    "text": "these models what you find is you can",
    "start": "172040",
    "end": "173200"
  },
  {
    "text": "get amazing",
    "start": "173200",
    "end": "174599"
  },
  {
    "text": "results so very quickly in terms of what",
    "start": "174599",
    "end": "177159"
  },
  {
    "text": "we do at pretty Bas we've got a platform",
    "start": "177159",
    "end": "178760"
  },
  {
    "text": "that allows you connect your Enterprise",
    "start": "178760",
    "end": "180640"
  },
  {
    "text": "data into the platform and then build",
    "start": "180640",
    "end": "182879"
  },
  {
    "text": "and train and deploy these models these",
    "start": "182879",
    "end": "184760"
  },
  {
    "text": "large language models and we do that at",
    "start": "184760",
    "end": "186920"
  },
  {
    "text": "scale in your own native Cloud as well",
    "start": "186920",
    "end": "189640"
  },
  {
    "text": "as our managed SAS",
    "start": "189640",
    "end": "191440"
  },
  {
    "text": "platform so in terms of some of the",
    "start": "191440",
    "end": "193760"
  },
  {
    "text": "challenges um you know required to sort",
    "start": "193760",
    "end": "196000"
  },
  {
    "text": "of meet these expectations of delivering",
    "start": "196000",
    "end": "198120"
  },
  {
    "text": "inference I won't go into them in too",
    "start": "198120",
    "end": "199879"
  },
  {
    "text": "much detail but the tools are complex",
    "start": "199879",
    "end": "201680"
  },
  {
    "text": "right so you're you're dealing with",
    "start": "201680",
    "end": "203319"
  },
  {
    "text": "often you know low-level python programs",
    "start": "203319",
    "end": "205959"
  },
  {
    "text": "typically um when you're talking about",
    "start": "205959",
    "end": "208280"
  },
  {
    "text": "fine-tuning these models it's very easy",
    "start": "208280",
    "end": "210439"
  },
  {
    "text": "to run out of memory um you know",
    "start": "210439",
    "end": "212239"
  },
  {
    "text": "particularly when you've got you know",
    "start": "212239",
    "end": "213640"
  },
  {
    "text": "very large models in the order of 13",
    "start": "213640",
    "end": "215400"
  },
  {
    "text": "billion or more you can often hit these",
    "start": "215400",
    "end": "217400"
  },
  {
    "text": "um out of memory exceptions and so",
    "start": "217400",
    "end": "218840"
  },
  {
    "text": "making sure you do that efficiently can",
    "start": "218840",
    "end": "220959"
  },
  {
    "text": "save you a lot of time and cost and then",
    "start": "220959",
    "end": "223159"
  },
  {
    "text": "in terms of serving even the smaller",
    "start": "223159",
    "end": "225360"
  },
  {
    "text": "models they do require GPU computes so",
    "start": "225360",
    "end": "227959"
  },
  {
    "text": "you know Nvidia T10 or or or A10 or A1",
    "start": "227959",
    "end": "231360"
  },
  {
    "text": "100s um are typically",
    "start": "231360",
    "end": "233720"
  },
  {
    "text": "required so in terms of what we uh look",
    "start": "233720",
    "end": "236360"
  },
  {
    "text": "to do with pretty bases we make this",
    "start": "236360",
    "end": "238079"
  },
  {
    "text": "easy so we're we're providing an open",
    "start": "238079",
    "end": "240040"
  },
  {
    "text": "platform for this where you can bring um",
    "start": "240040",
    "end": "243079"
  },
  {
    "text": "you know your expertise of your",
    "start": "243079",
    "end": "244599"
  },
  {
    "text": "engineering and developer staff and",
    "start": "244599",
    "end": "246079"
  },
  {
    "text": "basically just get started through our",
    "start": "246079",
    "end": "247519"
  },
  {
    "text": "API and SDK um and in terms of the",
    "start": "247519",
    "end": "250200"
  },
  {
    "text": "efficient F shuning we we're built on a",
    "start": "250200",
    "end": "252439"
  },
  {
    "text": "open source Library called Ludwick doai",
    "start": "252439",
    "end": "255200"
  },
  {
    "text": "um and and that has a lot of flexible",
    "start": "255200",
    "end": "257040"
  },
  {
    "text": "capabilities to allow training with",
    "start": "257040",
    "end": "258799"
  },
  {
    "text": "things like Lura weights which no",
    "start": "258799",
    "end": "260479"
  },
  {
    "text": "will'll talk about a little bit later in",
    "start": "260479",
    "end": "261759"
  },
  {
    "text": "the presentation and finally in terms of",
    "start": "261759",
    "end": "264120"
  },
  {
    "text": "the serving piece um we make it very",
    "start": "264120",
    "end": "266120"
  },
  {
    "text": "easy to make this cost effective by",
    "start": "266120",
    "end": "268240"
  },
  {
    "text": "having just in time um sort of serving",
    "start": "268240",
    "end": "271199"
  },
  {
    "text": "infrastructure provisioning and also",
    "start": "271199",
    "end": "273479"
  },
  {
    "text": "scaling up these and using some of our",
    "start": "273479",
    "end": "275720"
  },
  {
    "text": "own technology that we've developed",
    "start": "275720",
    "end": "276960"
  },
  {
    "text": "called Lorax again Noah will touch on",
    "start": "276960",
    "end": "279440"
  },
  {
    "text": "this so uh without further Ado I'll hand",
    "start": "279440",
    "end": "282160"
  },
  {
    "text": "over to Noah to continue um with some",
    "start": "282160",
    "end": "284880"
  },
  {
    "text": "more technical details thank you cool uh",
    "start": "284880",
    "end": "288360"
  },
  {
    "text": "thank you Julian uh so one thing Julian",
    "start": "288360",
    "end": "290160"
  },
  {
    "text": "didn't mention is at pretty base we",
    "start": "290160",
    "end": "292520"
  },
  {
    "text": "pretty much only have about you know",
    "start": "292520",
    "end": "294960"
  },
  {
    "text": "everyone came into predy base with",
    "start": "294960",
    "end": "296199"
  },
  {
    "text": "almost no kubernetes experience so uh",
    "start": "296199",
    "end": "298520"
  },
  {
    "text": "you know this has been quite a Journey",
    "start": "298520",
    "end": "299800"
  },
  {
    "text": "for us um and I think it really speaks",
    "start": "299800",
    "end": "301520"
  },
  {
    "text": "to the power of Open Source and you know",
    "start": "301520",
    "end": "303560"
  },
  {
    "text": "kubernetes and Cloud native technologies",
    "start": "303560",
    "end": "305479"
  },
  {
    "text": "that you know were able to do all this",
    "start": "305479",
    "end": "307400"
  },
  {
    "text": "uh with like a pretty small",
    "start": "307400",
    "end": "309160"
  },
  {
    "text": "team yep so uh from now on I'm going to",
    "start": "309160",
    "end": "311919"
  },
  {
    "text": "be talking about specifically the LM",
    "start": "311919",
    "end": "314280"
  },
  {
    "text": "serving portion of predy Base uh some of",
    "start": "314280",
    "end": "316360"
  },
  {
    "text": "the challenges that we've encountered",
    "start": "316360",
    "end": "317759"
  },
  {
    "text": "while building it out um and how we've",
    "start": "317759",
    "end": "319440"
  },
  {
    "text": "kind of navigated",
    "start": "319440",
    "end": "320880"
  },
  {
    "text": "those so there when once we started",
    "start": "320880",
    "end": "324120"
  },
  {
    "text": "building this platform there were about",
    "start": "324120",
    "end": "325639"
  },
  {
    "text": "three kind of like big challenges that",
    "start": "325639",
    "end": "327319"
  },
  {
    "text": "we faced uh the first was GP",
    "start": "327319",
    "end": "329680"
  },
  {
    "text": "availability and then GPU cost and",
    "start": "329680",
    "end": "331800"
  },
  {
    "text": "finally privacy um so despite uh there",
    "start": "331800",
    "end": "334759"
  },
  {
    "text": "being many breakthroughs in fine-tuning",
    "start": "334759",
    "end": "336960"
  },
  {
    "text": "and serving technology uh for you know",
    "start": "336960",
    "end": "340080"
  },
  {
    "text": "serving and fine-tuning on commodity",
    "start": "340080",
    "end": "341800"
  },
  {
    "text": "Hardware uh there's still a lot of",
    "start": "341800",
    "end": "343440"
  },
  {
    "text": "limitations around what you can do",
    "start": "343440",
    "end": "345240"
  },
  {
    "text": "unless you know you have a lot of GPU",
    "start": "345240",
    "end": "346919"
  },
  {
    "text": "memory So currently the standard for you",
    "start": "346919",
    "end": "350000"
  },
  {
    "text": "know doing all this work with large",
    "start": "350000",
    "end": "351720"
  },
  {
    "text": "language models is the Nvidia a100 uh",
    "start": "351720",
    "end": "354680"
  },
  {
    "text": "which is quite difficult to find at the",
    "start": "354680",
    "end": "356560"
  },
  {
    "text": "moment if you've ever tried to acquire",
    "start": "356560",
    "end": "358240"
  },
  {
    "text": "them in AWS or azure",
    "start": "358240",
    "end": "360120"
  },
  {
    "text": "um and even if you do uh manage to get",
    "start": "360120",
    "end": "362280"
  },
  {
    "text": "them they can be quite expensive um",
    "start": "362280",
    "end": "364800"
  },
  {
    "text": "finally a lot of customers um that you",
    "start": "364800",
    "end": "367039"
  },
  {
    "text": "know we've talked to care a lot about uh",
    "start": "367039",
    "end": "370039"
  },
  {
    "text": "having their models and training data",
    "start": "370039",
    "end": "371560"
  },
  {
    "text": "remain in their control so they're not",
    "start": "371560",
    "end": "372880"
  },
  {
    "text": "comfortable sending off all this like",
    "start": "372880",
    "end": "374840"
  },
  {
    "text": "proprietary data and they're like very",
    "start": "374840",
    "end": "376919"
  },
  {
    "text": "important models off you know to be",
    "start": "376919",
    "end": "378479"
  },
  {
    "text": "trained in someone else's Cloud they",
    "start": "378479",
    "end": "380039"
  },
  {
    "text": "really want that to all remain kind of",
    "start": "380039",
    "end": "382199"
  },
  {
    "text": "in their private VPC and their account",
    "start": "382199",
    "end": "385080"
  },
  {
    "text": "um so building around these challenges",
    "start": "385080",
    "end": "386479"
  },
  {
    "text": "and kind of overcoming them was our main",
    "start": "386479",
    "end": "388160"
  },
  {
    "text": "goal uh at predy BAS when building our",
    "start": "388160",
    "end": "390280"
  },
  {
    "text": "uh LM serving",
    "start": "390280",
    "end": "392759"
  },
  {
    "text": "platform so the pretty based serving",
    "start": "392759",
    "end": "394960"
  },
  {
    "text": "stack can be broken to broken down into",
    "start": "394960",
    "end": "397080"
  },
  {
    "text": "several distinct layers uh the first of",
    "start": "397080",
    "end": "399160"
  },
  {
    "text": "which is our multicluster service mesh",
    "start": "399160",
    "end": "401080"
  },
  {
    "text": "which I'll talk",
    "start": "401080",
    "end": "402680"
  },
  {
    "text": "about so why multicluster first of all",
    "start": "402680",
    "end": "406280"
  },
  {
    "text": "uh so the first reason as I alluded to",
    "start": "406280",
    "end": "407960"
  },
  {
    "text": "earlier um is that customers care about",
    "start": "407960",
    "end": "410280"
  },
  {
    "text": "uh privacy so building uh pretty base",
    "start": "410280",
    "end": "413680"
  },
  {
    "text": "with a multicluster service mesh has",
    "start": "413680",
    "end": "415520"
  },
  {
    "text": "allowed us to uh deploy you know data",
    "start": "415520",
    "end": "418520"
  },
  {
    "text": "plane comp uh components where the",
    "start": "418520",
    "end": "420199"
  },
  {
    "text": "serving and fine-tuning happens into the",
    "start": "420199",
    "end": "423240"
  },
  {
    "text": "customer VPC so that they remain in",
    "start": "423240",
    "end": "425199"
  },
  {
    "text": "control of their data it's also allowed",
    "start": "425199",
    "end": "427360"
  },
  {
    "text": "us uh to deploy into environments uh",
    "start": "427360",
    "end": "430280"
  },
  {
    "text": "where we can find cheap and available uh",
    "start": "430280",
    "end": "432440"
  },
  {
    "text": "gpus such as uh dedicated GPU clouds or",
    "start": "432440",
    "end": "435560"
  },
  {
    "text": "on PR providers and then finally uh",
    "start": "435560",
    "end": "438199"
  },
  {
    "text": "using a service mesh this is a little",
    "start": "438199",
    "end": "440319"
  },
  {
    "text": "like I won't talk about this too much",
    "start": "440319",
    "end": "441599"
  },
  {
    "text": "but it has allowed us to scale our",
    "start": "441599",
    "end": "443800"
  },
  {
    "text": "platform uh a lot more because now we",
    "start": "443800",
    "end": "446160"
  },
  {
    "text": "don't have to you know deploy the",
    "start": "446160",
    "end": "448120"
  },
  {
    "text": "control plane and data plane at the same",
    "start": "448120",
    "end": "449639"
  },
  {
    "text": "time we can kind of deploy independent",
    "start": "449639",
    "end": "451520"
  },
  {
    "text": "data planes for each customer all",
    "start": "451520",
    "end": "453759"
  },
  {
    "text": "managed by a single control",
    "start": "453759",
    "end": "456000"
  },
  {
    "text": "plane so here's a kind of highle view of",
    "start": "456000",
    "end": "459560"
  },
  {
    "text": "the party base architecture so it",
    "start": "459560",
    "end": "461680"
  },
  {
    "text": "consists of multiple kubernetes clusters",
    "start": "461680",
    "end": "463800"
  },
  {
    "text": "all you know registered into the same",
    "start": "463800",
    "end": "465759"
  },
  {
    "text": "ISO service mesh so each data plane uh",
    "start": "465759",
    "end": "469440"
  },
  {
    "text": "for a customer if they choose to deploy",
    "start": "469440",
    "end": "471199"
  },
  {
    "text": "like this uh lives in their VPC and",
    "start": "471199",
    "end": "474039"
  },
  {
    "text": "their Cloud account um and all of these",
    "start": "474039",
    "end": "477639"
  },
  {
    "text": "uh Cloud accounts as well have a blob",
    "start": "477639",
    "end": "479639"
  },
  {
    "text": "storage provision so that's where like",
    "start": "479639",
    "end": "481440"
  },
  {
    "text": "model artifacts will go and that has",
    "start": "481440",
    "end": "484759"
  },
  {
    "text": "allowed us to kind of alleviate some of",
    "start": "484759",
    "end": "486120"
  },
  {
    "text": "the concerns with data",
    "start": "486120",
    "end": "488000"
  },
  {
    "text": "governance also uh if customers are okay",
    "start": "488000",
    "end": "491400"
  },
  {
    "text": "with uh us hosting their compute um they",
    "start": "491400",
    "end": "493919"
  },
  {
    "text": "can register to the predy based kind of",
    "start": "493919",
    "end": "496440"
  },
  {
    "text": "AI Cloud platform so what this is is",
    "start": "496440",
    "end": "500080"
  },
  {
    "text": "compute managed by us uh where we have",
    "start": "500080",
    "end": "501960"
  },
  {
    "text": "multiple kubernetes clusters spread out",
    "start": "501960",
    "end": "503759"
  },
  {
    "text": "between both the public cloud and",
    "start": "503759",
    "end": "506319"
  },
  {
    "text": "dedicated on-prem like GPU clouds so",
    "start": "506319",
    "end": "509560"
  },
  {
    "text": "when customers are able to que when they",
    "start": "509560",
    "end": "511039"
  },
  {
    "text": "queue up jobs uh for fine-tuning or",
    "start": "511039",
    "end": "512760"
  },
  {
    "text": "serving we can reroute the job based on",
    "start": "512760",
    "end": "515680"
  },
  {
    "text": "some heuristics to either the uh you",
    "start": "515680",
    "end": "518360"
  },
  {
    "text": "know the public cloud or the on-prem",
    "start": "518360",
    "end": "520240"
  },
  {
    "text": "cloud based on you know if they need A1",
    "start": "520240",
    "end": "522880"
  },
  {
    "text": "100s essentially um and using sdo and",
    "start": "522880",
    "end": "526760"
  },
  {
    "text": "kind of this like multicluster",
    "start": "526760",
    "end": "527720"
  },
  {
    "text": "deployment has allowed us to really",
    "start": "527720",
    "end": "529120"
  },
  {
    "text": "alleviate both the uh GPU availability",
    "start": "529120",
    "end": "532160"
  },
  {
    "text": "and the cost restriction as well as the",
    "start": "532160",
    "end": "534279"
  },
  {
    "text": "kind of privacy",
    "start": "534279",
    "end": "536240"
  },
  {
    "text": "concerns and going a bit more into that",
    "start": "536240",
    "end": "538760"
  },
  {
    "text": "privacy part um if for those maybe",
    "start": "538760",
    "end": "541200"
  },
  {
    "text": "familiar witho this might be a little",
    "start": "541200",
    "end": "542920"
  },
  {
    "text": "bit familiar but for those who aren't um",
    "start": "542920",
    "end": "545440"
  },
  {
    "text": "witho you are allowed to configure um",
    "start": "545440",
    "end": "548200"
  },
  {
    "text": "authorization policies and what we have",
    "start": "548200",
    "end": "550360"
  },
  {
    "text": "is a kind of centrally managed uh ISO",
    "start": "550360",
    "end": "553040"
  },
  {
    "text": "control plane that allows us to um you",
    "start": "553040",
    "end": "556040"
  },
  {
    "text": "know manage all of this locally not in",
    "start": "556040",
    "end": "558120"
  },
  {
    "text": "the customer data planes so that we can",
    "start": "558120",
    "end": "560040"
  },
  {
    "text": "restrict communication between customer",
    "start": "560040",
    "end": "563640"
  },
  {
    "text": "deployments so next going you know a bit",
    "start": "563640",
    "end": "566680"
  },
  {
    "text": "deeper is uh serverless inference",
    "start": "566680",
    "end": "569680"
  },
  {
    "text": "so first of all why would we want",
    "start": "569680",
    "end": "572040"
  },
  {
    "text": "serverless inference at predy",
    "start": "572040",
    "end": "575200"
  },
  {
    "text": "base so the two main reasons as you",
    "start": "575200",
    "end": "578200"
  },
  {
    "text": "could probably guess are cost and",
    "start": "578200",
    "end": "580480"
  },
  {
    "text": "traffic so as I mentioned earlier gpus",
    "start": "580480",
    "end": "583480"
  },
  {
    "text": "are quite expensive even if you can find",
    "start": "583480",
    "end": "585399"
  },
  {
    "text": "them you know A1 100s aren't cheap so we",
    "start": "585399",
    "end": "588040"
  },
  {
    "text": "really don't want to be running models",
    "start": "588040",
    "end": "590680"
  },
  {
    "text": "that aren't serving traffic so this is",
    "start": "590680",
    "end": "592720"
  },
  {
    "text": "both for us and for the customers",
    "start": "592720",
    "end": "594320"
  },
  {
    "text": "because if they are deploying their data",
    "start": "594320",
    "end": "595959"
  },
  {
    "text": "plane into their account they're going",
    "start": "595959",
    "end": "596959"
  },
  {
    "text": "to be paying for all the compute uh",
    "start": "596959",
    "end": "599320"
  },
  {
    "text": "finally we also do want to be able to",
    "start": "599320",
    "end": "601160"
  },
  {
    "text": "scale up traffic uh scale up the models",
    "start": "601160",
    "end": "603519"
  },
  {
    "text": "as traffic increases because uh you know",
    "start": "603519",
    "end": "605839"
  },
  {
    "text": "LM models can be quite bursty with like",
    "start": "605839",
    "end": "607880"
  },
  {
    "text": "people you know wanting to do a bunch of",
    "start": "607880",
    "end": "610240"
  },
  {
    "text": "experimentation at one time and then",
    "start": "610240",
    "end": "611880"
  },
  {
    "text": "kind of you know for forgetting about it",
    "start": "611880",
    "end": "613720"
  },
  {
    "text": "and kind of turning it off after a",
    "start": "613720",
    "end": "616200"
  },
  {
    "text": "while so this is kind of a a little bit",
    "start": "616200",
    "end": "619200"
  },
  {
    "text": "more of a detailed look at a pry based",
    "start": "619200",
    "end": "621440"
  },
  {
    "text": "control plane and data plane um so this",
    "start": "621440",
    "end": "623880"
  },
  {
    "text": "will kind of help us understand a bit",
    "start": "623880",
    "end": "625320"
  },
  {
    "text": "more about how we first uh developed LM",
    "start": "625320",
    "end": "628200"
  },
  {
    "text": "serving at predy BAS",
    "start": "628200",
    "end": "629839"
  },
  {
    "text": "so remember of course there are multiple",
    "start": "629839",
    "end": "631240"
  },
  {
    "text": "data planes but here is just pictured",
    "start": "631240",
    "end": "633399"
  },
  {
    "text": "one um and I'll kind of go a bit into",
    "start": "633399",
    "end": "637120"
  },
  {
    "text": "how we initially created llm",
    "start": "637120",
    "end": "640519"
  },
  {
    "text": "serving so first uh when customers would",
    "start": "640519",
    "end": "644040"
  },
  {
    "text": "want to provision an llm uh the request",
    "start": "644040",
    "end": "646279"
  },
  {
    "text": "would come in through our sdo Ingress",
    "start": "646279",
    "end": "647720"
  },
  {
    "text": "Gateway in the control plane uh it would",
    "start": "647720",
    "end": "649959"
  },
  {
    "text": "just kind of go through our API Gateway",
    "start": "649959",
    "end": "652200"
  },
  {
    "text": "uh for authentication and then go to our",
    "start": "652200",
    "end": "654600"
  },
  {
    "text": "workflow orchestrator where we would",
    "start": "654600",
    "end": "656560"
  },
  {
    "text": "kick off an asynchronous workflow which",
    "start": "656560",
    "end": "658800"
  },
  {
    "text": "gets it's picked up by the predy Bas",
    "start": "658800",
    "end": "661000"
  },
  {
    "text": "agent which lives in the data plane so",
    "start": "661000",
    "end": "663920"
  },
  {
    "text": "side note this is all kind of",
    "start": "663920",
    "end": "665279"
  },
  {
    "text": "application Level workflow management uh",
    "start": "665279",
    "end": "667560"
  },
  {
    "text": "this isn't using any kind of kubernetes",
    "start": "667560",
    "end": "669160"
  },
  {
    "text": "stuff in case you're wondering um but",
    "start": "669160",
    "end": "672399"
  },
  {
    "text": "the reason why we do this is we want to",
    "start": "672399",
    "end": "674680"
  },
  {
    "text": "have um you know some work executed on",
    "start": "674680",
    "end": "677399"
  },
  {
    "text": "the data plane where we want to talk to",
    "start": "677399",
    "end": "679160"
  },
  {
    "text": "the kubernetes API server so generally",
    "start": "679160",
    "end": "681639"
  },
  {
    "text": "we weren't comfortable giving API server",
    "start": "681639",
    "end": "683440"
  },
  {
    "text": "access kind of out universally to all",
    "start": "683440",
    "end": "685360"
  },
  {
    "text": "these data planes so instead the predy",
    "start": "685360",
    "end": "687440"
  },
  {
    "text": "base agent uh kind of takes care of",
    "start": "687440",
    "end": "689560"
  },
  {
    "text": "creating deployments and whatnot on the",
    "start": "689560",
    "end": "692399"
  },
  {
    "text": "cluster so once the predy Bas agent gets",
    "start": "692399",
    "end": "695120"
  },
  {
    "text": "the uh the request to provision ele an",
    "start": "695120",
    "end": "697279"
  },
  {
    "text": "llm it will uh create the deployment and",
    "start": "697279",
    "end": "700079"
  },
  {
    "text": "then the weights for that model will be",
    "start": "700079",
    "end": "702079"
  },
  {
    "text": "downloaded from that data plan's uh",
    "start": "702079",
    "end": "705440"
  },
  {
    "text": "model store so this is this blob storage",
    "start": "705440",
    "end": "708440"
  },
  {
    "text": "is also in the that that customer's",
    "start": "708440",
    "end": "710880"
  },
  {
    "text": "account or that vpc's",
    "start": "710880",
    "end": "713680"
  },
  {
    "text": "account and then finally during llm",
    "start": "713680",
    "end": "716040"
  },
  {
    "text": "inference uh the request will come in",
    "start": "716040",
    "end": "717760"
  },
  {
    "text": "through the Gateway which will get get",
    "start": "717760",
    "end": "719079"
  },
  {
    "text": "authenticated and routed to the llm in",
    "start": "719079",
    "end": "721519"
  },
  {
    "text": "the data plane via aiso virtual",
    "start": "721519",
    "end": "725160"
  },
  {
    "text": "service so this is you know all well and",
    "start": "725160",
    "end": "729360"
  },
  {
    "text": "good but let's actually look at what you",
    "start": "729360",
    "end": "732480"
  },
  {
    "text": "know what we have to do to make this uh",
    "start": "732480",
    "end": "734120"
  },
  {
    "text": "server lless because right now this you",
    "start": "734120",
    "end": "735760"
  },
  {
    "text": "know this is pretty Bare Bones doesn't",
    "start": "735760",
    "end": "737120"
  },
  {
    "text": "Auto scale or do any of",
    "start": "737120",
    "end": "739160"
  },
  {
    "text": "that so what we did to kind of introduce",
    "start": "739160",
    "end": "742000"
  },
  {
    "text": "autoscaling uh in serverless into this",
    "start": "742000",
    "end": "744800"
  },
  {
    "text": "LM serving was using K so K stands for",
    "start": "744800",
    "end": "748639"
  },
  {
    "text": "the kubernetes event uh driven",
    "start": "748639",
    "end": "751240"
  },
  {
    "text": "autoscaler so it's a project that allows",
    "start": "751240",
    "end": "753079"
  },
  {
    "text": "you to Scale kubernetes deployments",
    "start": "753079",
    "end": "755120"
  },
  {
    "text": "based on a variety of metrics or events",
    "start": "755120",
    "end": "757720"
  },
  {
    "text": "in the",
    "start": "757720",
    "end": "758480"
  },
  {
    "text": "cluster so in our case we are using the",
    "start": "758480",
    "end": "761000"
  },
  {
    "text": "HTTP plugin so this allows us to scale",
    "start": "761000",
    "end": "764399"
  },
  {
    "text": "up and down the deployment based on the",
    "start": "764399",
    "end": "766839"
  },
  {
    "text": "number of incoming request uh although",
    "start": "766839",
    "end": "769880"
  },
  {
    "text": "we are looking at like you know a",
    "start": "769880",
    "end": "772079"
  },
  {
    "text": "variety of different scaling options um",
    "start": "772079",
    "end": "774639"
  },
  {
    "text": "for like different needs so looking a",
    "start": "774639",
    "end": "777519"
  },
  {
    "text": "bit more into what uh provisioning looks",
    "start": "777519",
    "end": "779839"
  },
  {
    "text": "like with K so instead of just creating",
    "start": "779839",
    "end": "782920"
  },
  {
    "text": "the LM service and deployment like one",
    "start": "782920",
    "end": "784880"
  },
  {
    "text": "might do in pretty standard you know",
    "start": "784880",
    "end": "786839"
  },
  {
    "text": "kubernetes environment instead we also",
    "start": "786839",
    "end": "789040"
  },
  {
    "text": "create the HTTP scaled object which is a",
    "start": "789040",
    "end": "791959"
  },
  {
    "text": "custom resource for K so what this does",
    "start": "791959",
    "end": "794880"
  },
  {
    "text": "is it kind of configures that deployment",
    "start": "794880",
    "end": "797560"
  },
  {
    "text": "the LM deployment to either scale up or",
    "start": "797560",
    "end": "799600"
  },
  {
    "text": "down based on certain parameters that",
    "start": "799600",
    "end": "802240"
  },
  {
    "text": "you can specify in this object um and of",
    "start": "802240",
    "end": "805440"
  },
  {
    "text": "course this is managed by K and this",
    "start": "805440",
    "end": "807920"
  },
  {
    "text": "does let us Scout scale down to zero or",
    "start": "807920",
    "end": "809880"
  },
  {
    "text": "scale up you know past one and",
    "start": "809880",
    "end": "812800"
  },
  {
    "text": "Beyond so let's first imagine um that we",
    "start": "812800",
    "end": "817000"
  },
  {
    "text": "have an LM deployment that has been",
    "start": "817000",
    "end": "818839"
  },
  {
    "text": "provisioned but it hasn't received",
    "start": "818839",
    "end": "820440"
  },
  {
    "text": "traffic in some time so K has scaled it",
    "start": "820440",
    "end": "822320"
  },
  {
    "text": "down to zero so when a a request would",
    "start": "822320",
    "end": "825040"
  },
  {
    "text": "come in it'll come in through our sto",
    "start": "825040",
    "end": "827040"
  },
  {
    "text": "Gateway into the data plane um and then",
    "start": "827040",
    "end": "829760"
  },
  {
    "text": "it'll get routed to the Keta proxy so",
    "start": "829760",
    "end": "832880"
  },
  {
    "text": "this routing um is actually kind of",
    "start": "832880",
    "end": "835160"
  },
  {
    "text": "dynamic uh we can configure it with sdo",
    "start": "835160",
    "end": "837360"
  },
  {
    "text": "virtual Services which we do",
    "start": "837360",
    "end": "839680"
  },
  {
    "text": "um and once the proxy receives the",
    "start": "839680",
    "end": "841880"
  },
  {
    "text": "request it will talk to the K controller",
    "start": "841880",
    "end": "844759"
  },
  {
    "text": "so the K controller will then look at",
    "start": "844759",
    "end": "846920"
  },
  {
    "text": "the HTTP scaled object determine if the",
    "start": "846920",
    "end": "850120"
  },
  {
    "text": "deployment is ready to receive request",
    "start": "850120",
    "end": "852279"
  },
  {
    "text": "or if you know there are too many in the",
    "start": "852279",
    "end": "854000"
  },
  {
    "text": "queue and it needs to scale up more than",
    "start": "854000",
    "end": "855720"
  },
  {
    "text": "one pod and once it does uh you know",
    "start": "855720",
    "end": "859560"
  },
  {
    "text": "determine all this stuff it will then",
    "start": "859560",
    "end": "861360"
  },
  {
    "text": "try to scale up the",
    "start": "861360",
    "end": "862720"
  },
  {
    "text": "llm so uh while the scaling is happening",
    "start": "862720",
    "end": "866040"
  },
  {
    "text": "the proxy actually holds on to requests",
    "start": "866040",
    "end": "868320"
  },
  {
    "text": "so then once the elm is scaled up the",
    "start": "868320",
    "end": "871079"
  },
  {
    "text": "proxy will send the request um along the",
    "start": "871079",
    "end": "873959"
  },
  {
    "text": "normal path so through the kubernetes",
    "start": "873959",
    "end": "876160"
  },
  {
    "text": "service to the deployment um and then",
    "start": "876160",
    "end": "879240"
  },
  {
    "text": "yeah onto the pods so this is kind of",
    "start": "879240",
    "end": "881959"
  },
  {
    "text": "how inference",
    "start": "881959",
    "end": "883279"
  },
  {
    "text": "happens so this is pretty cool it makes",
    "start": "883279",
    "end": "886000"
  },
  {
    "text": "it you know a fairly serverless",
    "start": "886000",
    "end": "888480"
  },
  {
    "text": "experience you can you know query an llm",
    "start": "888480",
    "end": "892040"
  },
  {
    "text": "that has been down that hasn't been used",
    "start": "892040",
    "end": "893800"
  },
  {
    "text": "for a while and might have been scaled",
    "start": "893800",
    "end": "894920"
  },
  {
    "text": "down and you'll get a a response back",
    "start": "894920",
    "end": "896959"
  },
  {
    "text": "but unfortunately the the scaling is",
    "start": "896959",
    "end": "899920"
  },
  {
    "text": "quite difficult so scaling an llm up",
    "start": "899920",
    "end": "903000"
  },
  {
    "text": "from zero we we found was very uh very",
    "start": "903000",
    "end": "906759"
  },
  {
    "text": "bad performance-wise so it would take",
    "start": "906759",
    "end": "908600"
  },
  {
    "text": "anywhere from like 15 to 20 minutes to",
    "start": "908600",
    "end": "910199"
  },
  {
    "text": "scale up which obviously is not a",
    "start": "910199",
    "end": "912720"
  },
  {
    "text": "serverless experience at all you know",
    "start": "912720",
    "end": "914720"
  },
  {
    "text": "the request would time out you'd have to",
    "start": "914720",
    "end": "916079"
  },
  {
    "text": "wait a while um the problem here is that",
    "start": "916079",
    "end": "919720"
  },
  {
    "text": "we're not only scaling the pods but",
    "start": "919720",
    "end": "921519"
  },
  {
    "text": "we're also scaling nodes so pretty much",
    "start": "921519",
    "end": "924440"
  },
  {
    "text": "a predy base uh for llms we only have",
    "start": "924440",
    "end": "927040"
  },
  {
    "text": "one pod running on a dedic at node and",
    "start": "927040",
    "end": "930079"
  },
  {
    "text": "the reason for that is essentially the",
    "start": "930079",
    "end": "932000"
  },
  {
    "text": "LM like needs to use a lot of GPU memory",
    "start": "932000",
    "end": "934120"
  },
  {
    "text": "so it's pretty much going to use the",
    "start": "934120",
    "end": "935440"
  },
  {
    "text": "entire GPU on the Node um so it doesn't",
    "start": "935440",
    "end": "937399"
  },
  {
    "text": "really make sense to run a lot of other",
    "start": "937399",
    "end": "939120"
  },
  {
    "text": "stuff there also these nodes are very",
    "start": "939120",
    "end": "941440"
  },
  {
    "text": "expensive so the whole point of",
    "start": "941440",
    "end": "942839"
  },
  {
    "text": "autoscaling was to save money so we",
    "start": "942839",
    "end": "944720"
  },
  {
    "text": "don't we don't want them sitting around",
    "start": "944720",
    "end": "946720"
  },
  {
    "text": "uh for a really long time because that's",
    "start": "946720",
    "end": "948440"
  },
  {
    "text": "incurrent cost either on our side or on",
    "start": "948440",
    "end": "950480"
  },
  {
    "text": "the customer",
    "start": "950480",
    "end": "952639"
  },
  {
    "text": "side so this introduces us into the next",
    "start": "952639",
    "end": "955839"
  },
  {
    "text": "layer which is cold start optimizations",
    "start": "955839",
    "end": "960600"
  },
  {
    "text": "so as um as mentioned earlier it's very",
    "start": "960600",
    "end": "963839"
  },
  {
    "text": "difficult to scale these LMS up and two",
    "start": "963839",
    "end": "965759"
  },
  {
    "text": "of the reasons are the containers are",
    "start": "965759",
    "end": "968079"
  },
  {
    "text": "large and the LM model weights are even",
    "start": "968079",
    "end": "971120"
  },
  {
    "text": "larger um and of course we do have to",
    "start": "971120",
    "end": "973199"
  },
  {
    "text": "acquire the node into the cluster but",
    "start": "973199",
    "end": "974720"
  },
  {
    "text": "that's kind of like a different set of",
    "start": "974720",
    "end": "977600"
  },
  {
    "text": "problems so the LM containers are quite",
    "start": "977600",
    "end": "981680"
  },
  {
    "text": "large because they have to be shipped",
    "start": "981680",
    "end": "983319"
  },
  {
    "text": "with things like Cuda and custom kernels",
    "start": "983319",
    "end": "985079"
  },
  {
    "text": "to actually serve the LMS um we've done",
    "start": "985079",
    "end": "988160"
  },
  {
    "text": "you know many different things to try to",
    "start": "988160",
    "end": "989600"
  },
  {
    "text": "slim down our container images but",
    "start": "989600",
    "end": "991120"
  },
  {
    "text": "they're still you know in the Min",
    "start": "991120",
    "end": "992800"
  },
  {
    "text": "gigabytes uh of",
    "start": "992800",
    "end": "994720"
  },
  {
    "text": "size and the second problem which is the",
    "start": "994720",
    "end": "996920"
  },
  {
    "text": "LMS um being quite large uh for those",
    "start": "996920",
    "end": "999759"
  },
  {
    "text": "who are familiar things like llama 270",
    "start": "999759",
    "end": "1001839"
  },
  {
    "text": "billion are about 150 gigs on dis so we",
    "start": "1001839",
    "end": "1005480"
  },
  {
    "text": "have to download all of those weights um",
    "start": "1005480",
    "end": "1008120"
  },
  {
    "text": "and then load them up into the GPO so",
    "start": "1008120",
    "end": "1010079"
  },
  {
    "text": "that can take some time as well so I'm",
    "start": "1010079",
    "end": "1012639"
  },
  {
    "text": "going to split this up into two",
    "start": "1012639",
    "end": "1014079"
  },
  {
    "text": "different sections so first will be the",
    "start": "1014079",
    "end": "1015959"
  },
  {
    "text": "kind of container download optimizations",
    "start": "1015959",
    "end": "1019360"
  },
  {
    "text": "so after a lot of trial and error uh we",
    "start": "1019360",
    "end": "1021240"
  },
  {
    "text": "ended up landing on a um service called",
    "start": "1021240",
    "end": "1024558"
  },
  {
    "text": "Spiegel well an open source project",
    "start": "1024559",
    "end": "1026640"
  },
  {
    "text": "called Spiegel I should say um so what",
    "start": "1026640",
    "end": "1028360"
  },
  {
    "text": "Spiegel is is an in cluster container",
    "start": "1028360",
    "end": "1031038"
  },
  {
    "text": "registry mirror so what that means is",
    "start": "1031039",
    "end": "1033839"
  },
  {
    "text": "essentially it will you know mirror a",
    "start": "1033839",
    "end": "1037079"
  },
  {
    "text": "container registry that's uh external to",
    "start": "1037079",
    "end": "1039360"
  },
  {
    "text": "the cluster but inside of it and uh",
    "start": "1039360",
    "end": "1042720"
  },
  {
    "text": "Spiegel so I'll talk a little bit about",
    "start": "1042720",
    "end": "1044678"
  },
  {
    "text": "how this works um but first it runs as",
    "start": "1044679",
    "end": "1046798"
  },
  {
    "text": "like a Damon set on all of the nodes",
    "start": "1046799",
    "end": "1049240"
  },
  {
    "text": "and what it does is when a pod is",
    "start": "1049240",
    "end": "1052120"
  },
  {
    "text": "started on on a node in kubernetes of",
    "start": "1052120",
    "end": "1054039"
  },
  {
    "text": "course the image for the Pod will you",
    "start": "1054039",
    "end": "1056039"
  },
  {
    "text": "know attempt to be downloaded on onto",
    "start": "1056039",
    "end": "1057799"
  },
  {
    "text": "the node um and this download happens",
    "start": "1057799",
    "end": "1061280"
  },
  {
    "text": "layer by layer doesn't just happen all",
    "start": "1061280",
    "end": "1062919"
  },
  {
    "text": "at once um so what Spiegel does is it",
    "start": "1062919",
    "end": "1066000"
  },
  {
    "text": "modifies the container D settings on the",
    "start": "1066000",
    "end": "1068000"
  },
  {
    "text": "Node to instead of going to like the",
    "start": "1068000",
    "end": "1070720"
  },
  {
    "text": "default registry first it'll actually go",
    "start": "1070720",
    "end": "1072440"
  },
  {
    "text": "to Spiegel when we try to download a",
    "start": "1072440",
    "end": "1074640"
  },
  {
    "text": "layer and what Spiegel will do is ask",
    "start": "1074640",
    "end": "1077880"
  },
  {
    "text": "all of the other kind of like speel pods",
    "start": "1077880",
    "end": "1080159"
  },
  {
    "text": "in the cluster if any of those nodes",
    "start": "1080159",
    "end": "1082440"
  },
  {
    "text": "that they're living on has the layer",
    "start": "1082440",
    "end": "1084640"
  },
  {
    "text": "that we're looking",
    "start": "1084640",
    "end": "1085919"
  },
  {
    "text": "for and if the layers are found in the",
    "start": "1085919",
    "end": "1088360"
  },
  {
    "text": "registry uh they will be downloaded from",
    "start": "1088360",
    "end": "1090320"
  },
  {
    "text": "the nodes in the cluster rather than",
    "start": "1090320",
    "end": "1091960"
  },
  {
    "text": "going outside uh to the external",
    "start": "1091960",
    "end": "1093960"
  },
  {
    "text": "registry so we found this you know",
    "start": "1093960",
    "end": "1095559"
  },
  {
    "text": "dramatically speeds up uh the download",
    "start": "1095559",
    "end": "1097880"
  },
  {
    "text": "process of course if a layer is not",
    "start": "1097880",
    "end": "1100400"
  },
  {
    "text": "found uh it will have to go out and",
    "start": "1100400",
    "end": "1101960"
  },
  {
    "text": "download from the remote registry but of",
    "start": "1101960",
    "end": "1104159"
  },
  {
    "text": "course this layer is now in the cluster",
    "start": "1104159",
    "end": "1105840"
  },
  {
    "text": "so uh you know any any other pod that",
    "start": "1105840",
    "end": "1109400"
  },
  {
    "text": "needs this layer to start up uh we'll be",
    "start": "1109400",
    "end": "1111320"
  },
  {
    "text": "able to download it from inside of the",
    "start": "1111320",
    "end": "1113919"
  },
  {
    "text": "cluster and another side note is this",
    "start": "1113919",
    "end": "1116200"
  },
  {
    "text": "does require container D um to be the uh",
    "start": "1116200",
    "end": "1119440"
  },
  {
    "text": "image or the container runtime in",
    "start": "1119440",
    "end": "1121200"
  },
  {
    "text": "kubernetes but we found most of the",
    "start": "1121200",
    "end": "1122600"
  },
  {
    "text": "cloud providers have switched over from",
    "start": "1122600",
    "end": "1124159"
  },
  {
    "text": "Docker so this hasn't been an issue for",
    "start": "1124159",
    "end": "1127480"
  },
  {
    "text": "us so overall um after adding Spiegel",
    "start": "1127480",
    "end": "1130960"
  },
  {
    "text": "with very minimal configurations we saw",
    "start": "1130960",
    "end": "1132840"
  },
  {
    "text": "a pretty dramatic increase uh decrease",
    "start": "1132840",
    "end": "1135120"
  },
  {
    "text": "in container container download time so",
    "start": "1135120",
    "end": "1137600"
  },
  {
    "text": "for our larger G GPU images this went",
    "start": "1137600",
    "end": "1139320"
  },
  {
    "text": "from about 8 minutes down to four so",
    "start": "1139320",
    "end": "1141760"
  },
  {
    "text": "this was quite literally with like very",
    "start": "1141760",
    "end": "1144200"
  },
  {
    "text": "very few configurations um and was",
    "start": "1144200",
    "end": "1146640"
  },
  {
    "text": "pretty plug-and playay which is why we",
    "start": "1146640",
    "end": "1148000"
  },
  {
    "text": "ended up using this as opposed to some",
    "start": "1148000",
    "end": "1149760"
  },
  {
    "text": "other uh other things we",
    "start": "1149760",
    "end": "1152000"
  },
  {
    "text": "tried so next uh a little bit more llm",
    "start": "1152000",
    "end": "1155080"
  },
  {
    "text": "specific we'll talk about uh weight",
    "start": "1155080",
    "end": "1156799"
  },
  {
    "text": "download",
    "start": "1156799",
    "end": "1158679"
  },
  {
    "text": "optimizations So before adding anything",
    "start": "1158679",
    "end": "1162400"
  },
  {
    "text": "um the weight download process of predy",
    "start": "1162400",
    "end": "1164440"
  },
  {
    "text": "Base looked like this uh so when the llm",
    "start": "1164440",
    "end": "1166360"
  },
  {
    "text": "server would start up we would download",
    "start": "1166360",
    "end": "1168760"
  },
  {
    "text": "the base model uh if it if it wasn't",
    "start": "1168760",
    "end": "1171440"
  },
  {
    "text": "from hugging face uh from hugging face",
    "start": "1171440",
    "end": "1173440"
  },
  {
    "text": "Hub of course we we do support a few",
    "start": "1173440",
    "end": "1175320"
  },
  {
    "text": "other sources well but once we download",
    "start": "1175320",
    "end": "1178440"
  },
  {
    "text": "the the weights um which could take",
    "start": "1178440",
    "end": "1180200"
  },
  {
    "text": "anywhere from like 1 to 15 minutes",
    "start": "1180200",
    "end": "1181720"
  },
  {
    "text": "depending on um how large the model was",
    "start": "1181720",
    "end": "1184799"
  },
  {
    "text": "we would have to convert the weights",
    "start": "1184799",
    "end": "1187080"
  },
  {
    "text": "into the safe tenser format if they",
    "start": "1187080",
    "end": "1188520"
  },
  {
    "text": "weren't in that format already so that",
    "start": "1188520",
    "end": "1190679"
  },
  {
    "text": "would take on the order of several",
    "start": "1190679",
    "end": "1192840"
  },
  {
    "text": "minutes uh once you know they're",
    "start": "1192840",
    "end": "1195000"
  },
  {
    "text": "converted then we can start up the llm",
    "start": "1195000",
    "end": "1196679"
  },
  {
    "text": "and uh start serving traffic",
    "start": "1196679",
    "end": "1200000"
  },
  {
    "text": "to uh to kind of fix this issue because",
    "start": "1200000",
    "end": "1202440"
  },
  {
    "text": "that was quite a bit of time um we added",
    "start": "1202440",
    "end": "1205000"
  },
  {
    "text": "three things so an init container uh",
    "start": "1205000",
    "end": "1207200"
  },
  {
    "text": "shared storage volume between the in",
    "start": "1207200",
    "end": "1208679"
  },
  {
    "text": "container and the main server and a",
    "start": "1208679",
    "end": "1210679"
  },
  {
    "text": "private cache in the data planes uh blob",
    "start": "1210679",
    "end": "1214840"
  },
  {
    "text": "storage so first when the cach is cold",
    "start": "1214840",
    "end": "1217559"
  },
  {
    "text": "obviously we're we're just going to have",
    "start": "1217559",
    "end": "1219039"
  },
  {
    "text": "to do what we did before so which is",
    "start": "1219039",
    "end": "1220720"
  },
  {
    "text": "just download the weights um but instead",
    "start": "1220720",
    "end": "1223400"
  },
  {
    "text": "this is taking place in the init",
    "start": "1223400",
    "end": "1225240"
  },
  {
    "text": "container so the init container will",
    "start": "1225240",
    "end": "1227280"
  },
  {
    "text": "handle uh converting the the weights to",
    "start": "1227280",
    "end": "1229919"
  },
  {
    "text": "Safe tensor format um and then it will",
    "start": "1229919",
    "end": "1233280"
  },
  {
    "text": "push the weights to the cache of course",
    "start": "1233280",
    "end": "1236480"
  },
  {
    "text": "once they're in the cache we don't have",
    "start": "1236480",
    "end": "1237799"
  },
  {
    "text": "to do this again so this only happen",
    "start": "1237799",
    "end": "1239640"
  },
  {
    "text": "needs to happen once per data plane for",
    "start": "1239640",
    "end": "1242240"
  },
  {
    "text": "any specific uh",
    "start": "1242240",
    "end": "1244520"
  },
  {
    "text": "llm and uh also the safe tensor uh",
    "start": "1244520",
    "end": "1248280"
  },
  {
    "text": "weights are mounted to the shared",
    "start": "1248280",
    "end": "1249799"
  },
  {
    "text": "storage volume so they're access",
    "start": "1249799",
    "end": "1251760"
  },
  {
    "text": "accessible for both the in container and",
    "start": "1251760",
    "end": "1253400"
  },
  {
    "text": "the main uh serving",
    "start": "1253400",
    "end": "1256200"
  },
  {
    "text": "container so once the cash is warmed up",
    "start": "1256200",
    "end": "1259400"
  },
  {
    "text": "uh we're able whenever we start an llm",
    "start": "1259400",
    "end": "1262440"
  },
  {
    "text": "um in the data plane we're able to",
    "start": "1262440",
    "end": "1264440"
  },
  {
    "text": "download the weights uh from this cache",
    "start": "1264440",
    "end": "1267880"
  },
  {
    "text": "now we were actually able to find a",
    "start": "1267880",
    "end": "1270279"
  },
  {
    "text": "pretty fast way to do this at least in",
    "start": "1270279",
    "end": "1272120"
  },
  {
    "text": "S3 uh which is using the S3 CRT Library",
    "start": "1272120",
    "end": "1275880"
  },
  {
    "text": "so what this is is a um a set of many of",
    "start": "1275880",
    "end": "1279520"
  },
  {
    "text": "the AWS apis written in C and it's",
    "start": "1279520",
    "end": "1282679"
  },
  {
    "text": "available in most of the sdks as well as",
    "start": "1282679",
    "end": "1284960"
  },
  {
    "text": "the CLI so by enabling this uh able to",
    "start": "1284960",
    "end": "1288720"
  },
  {
    "text": "do multi-threaded downloads which is a",
    "start": "1288720",
    "end": "1291000"
  },
  {
    "text": "lot faster and so for an example uh for",
    "start": "1291000",
    "end": "1294600"
  },
  {
    "text": "something like llama 213 billion which",
    "start": "1294600",
    "end": "1296320"
  },
  {
    "text": "is about you know 26 to 30 gigs uh",
    "start": "1296320",
    "end": "1299080"
  },
  {
    "text": "uncompressed we were we were able to",
    "start": "1299080",
    "end": "1301240"
  },
  {
    "text": "download the weights in less than a",
    "start": "1301240",
    "end": "1302440"
  },
  {
    "text": "minute uh when before it took about like",
    "start": "1302440",
    "end": "1304960"
  },
  {
    "text": "five",
    "start": "1304960",
    "end": "1306200"
  },
  {
    "text": "minutes and of course we're able to do",
    "start": "1306200",
    "end": "1308320"
  },
  {
    "text": "this as well because uh the nodes that",
    "start": "1308320",
    "end": "1310440"
  },
  {
    "text": "these LMS are running have very fast",
    "start": "1310440",
    "end": "1312279"
  },
  {
    "text": "networking and uh you know a lot of CPU",
    "start": "1312279",
    "end": "1315279"
  },
  {
    "text": "a lot of memory because they're kind of",
    "start": "1315279",
    "end": "1316960"
  },
  {
    "text": "high performance nodes for you know GPU",
    "start": "1316960",
    "end": "1320480"
  },
  {
    "text": "workloads um and of course if by chance",
    "start": "1320480",
    "end": "1323120"
  },
  {
    "text": "we do serve multiple LMS on a single",
    "start": "1323120",
    "end": "1325679"
  },
  {
    "text": "node uh these weights are kind of cached",
    "start": "1325679",
    "end": "1328000"
  },
  {
    "text": "onto the node so we're able to just uh",
    "start": "1328000",
    "end": "1330760"
  },
  {
    "text": "start up the llm almost",
    "start": "1330760",
    "end": "1334320"
  },
  {
    "text": "instantly so here's kind of like a",
    "start": "1334360",
    "end": "1336520"
  },
  {
    "text": "timeline look at what serving uh was",
    "start": "1336520",
    "end": "1339640"
  },
  {
    "text": "beforehand so you know as you can see it",
    "start": "1339640",
    "end": "1342159"
  },
  {
    "text": "took quite a bit of time everything um",
    "start": "1342159",
    "end": "1344720"
  },
  {
    "text": "all the way from acquiring the node to",
    "start": "1344720",
    "end": "1346200"
  },
  {
    "text": "converting the weights you know we had",
    "start": "1346200",
    "end": "1347720"
  },
  {
    "text": "it do which took several",
    "start": "1347720",
    "end": "1350120"
  },
  {
    "text": "minutes but after all these",
    "start": "1350120",
    "end": "1352320"
  },
  {
    "text": "optimizations we were able to get the",
    "start": "1352320",
    "end": "1354200"
  },
  {
    "text": "container download down to about like 4",
    "start": "1354200",
    "end": "1356440"
  },
  {
    "text": "minutes um and then the weight download",
    "start": "1356440",
    "end": "1359320"
  },
  {
    "text": "to under a minute uh for the uh the warm",
    "start": "1359320",
    "end": "1362720"
  },
  {
    "text": "cach scenario and of course there's no",
    "start": "1362720",
    "end": "1364559"
  },
  {
    "text": "conversion that needs to happen either",
    "start": "1364559",
    "end": "1366880"
  },
  {
    "text": "um so another kind of side note is that",
    "start": "1366880",
    "end": "1369320"
  },
  {
    "text": "you know a lot of this is now acquiring",
    "start": "1369320",
    "end": "1371679"
  },
  {
    "text": "the node in downloading the container",
    "start": "1371679",
    "end": "1373279"
  },
  {
    "text": "which can all be erased if you don't",
    "start": "1373279",
    "end": "1374799"
  },
  {
    "text": "Auto scale your nodes um but again we're",
    "start": "1374799",
    "end": "1377080"
  },
  {
    "text": "coming from um a perspective of we have",
    "start": "1377080",
    "end": "1380720"
  },
  {
    "text": "uh you know deployments and customer",
    "start": "1380720",
    "end": "1382200"
  },
  {
    "text": "clouds and in our Cloud as well and we",
    "start": "1382200",
    "end": "1384240"
  },
  {
    "text": "don't want to keep nodes around for very",
    "start": "1384240",
    "end": "1385919"
  },
  {
    "text": "long because they incur a lot of",
    "start": "1385919",
    "end": "1388440"
  },
  {
    "text": "cost finally uh we do have one more",
    "start": "1388440",
    "end": "1391159"
  },
  {
    "text": "optimization we've done around llm",
    "start": "1391159",
    "end": "1392640"
  },
  {
    "text": "serving which is something uh Julian",
    "start": "1392640",
    "end": "1394400"
  },
  {
    "text": "kind of alluded to which is called uh",
    "start": "1394400",
    "end": "1396240"
  },
  {
    "text": "lorea it's more of an application",
    "start": "1396240",
    "end": "1398240"
  },
  {
    "text": "optimization so I'm going to go off of",
    "start": "1398240",
    "end": "1400120"
  },
  {
    "text": "kubernetes a bit right here but figured",
    "start": "1400120",
    "end": "1402279"
  },
  {
    "text": "it would be very interesting for",
    "start": "1402279",
    "end": "1403279"
  },
  {
    "text": "everyone",
    "start": "1403279",
    "end": "1404440"
  },
  {
    "text": "here so when you find tuna model at",
    "start": "1404440",
    "end": "1406919"
  },
  {
    "text": "predy base uh you can fine-tune using a",
    "start": "1406919",
    "end": "1409279"
  },
  {
    "text": "method called Laura so for those who are",
    "start": "1409279",
    "end": "1411000"
  },
  {
    "text": "not familiar um the Laura method for",
    "start": "1411000",
    "end": "1413640"
  },
  {
    "text": "fine-tuning is you create a very small",
    "start": "1413640",
    "end": "1416440"
  },
  {
    "text": "kind of a subset of Weights that you're",
    "start": "1416440",
    "end": "1418320"
  },
  {
    "text": "actually fine-tuning instead of all of",
    "start": "1418320",
    "end": "1419919"
  },
  {
    "text": "the weights in the model and when you uh",
    "start": "1419919",
    "end": "1423279"
  },
  {
    "text": "use these weights uh for inference when",
    "start": "1423279",
    "end": "1425559"
  },
  {
    "text": "you want to do inference on this fine",
    "start": "1425559",
    "end": "1426679"
  },
  {
    "text": "tune model you load in these allora",
    "start": "1426679",
    "end": "1429200"
  },
  {
    "text": "weights and kind of add them to the base",
    "start": "1429200",
    "end": "1431039"
  },
  {
    "text": "model um so actually you have kind of",
    "start": "1431039",
    "end": "1433279"
  },
  {
    "text": "two distinct things you have the Laura",
    "start": "1433279",
    "end": "1434960"
  },
  {
    "text": "weights which you call the Laura adapter",
    "start": "1434960",
    "end": "1437080"
  },
  {
    "text": "and then the base mod model so at pry",
    "start": "1437080",
    "end": "1439760"
  },
  {
    "text": "Bas we created a system that kind of",
    "start": "1439760",
    "end": "1441520"
  },
  {
    "text": "lets you uh lets you dynamically load in",
    "start": "1441520",
    "end": "1444039"
  },
  {
    "text": "these lower weights at inference time",
    "start": "1444039",
    "end": "1446520"
  },
  {
    "text": "because they're so small this does not",
    "start": "1446520",
    "end": "1449080"
  },
  {
    "text": "incur a whole lot of latency so what you",
    "start": "1449080",
    "end": "1451720"
  },
  {
    "text": "can do is instead of you know deploying",
    "start": "1451720",
    "end": "1454520"
  },
  {
    "text": "a whole you know fine-tune model every",
    "start": "1454520",
    "end": "1457360"
  },
  {
    "text": "time you want to like query it you can",
    "start": "1457360",
    "end": "1458760"
  },
  {
    "text": "just have a base model and then um",
    "start": "1458760",
    "end": "1461480"
  },
  {
    "text": "specify the adapter you want at query",
    "start": "1461480",
    "end": "1463960"
  },
  {
    "text": "time so we've seen about you know 200",
    "start": "1463960",
    "end": "1466919"
  },
  {
    "text": "milliseconds of light see here uh for",
    "start": "1466919",
    "end": "1468880"
  },
  {
    "text": "model Sy waiting and of course this can",
    "start": "1468880",
    "end": "1471159"
  },
  {
    "text": "handle concurrent requests as well so if",
    "start": "1471159",
    "end": "1473399"
  },
  {
    "text": "you have multiple uh users or multiple",
    "start": "1473399",
    "end": "1476159"
  },
  {
    "text": "models you want to you know uh get",
    "start": "1476159",
    "end": "1479600"
  },
  {
    "text": "inference from uh you can send the query",
    "start": "1479600",
    "end": "1482679"
  },
  {
    "text": "to the same base model but with",
    "start": "1482679",
    "end": "1484200"
  },
  {
    "text": "different uh adapters in each uh request",
    "start": "1484200",
    "end": "1486760"
  },
  {
    "text": "and then these adapters are loaded um",
    "start": "1486760",
    "end": "1489880"
  },
  {
    "text": "while you know in the model and they can",
    "start": "1489880",
    "end": "1492320"
  },
  {
    "text": "be loaded while other requests are",
    "start": "1492320",
    "end": "1494320"
  },
  {
    "text": "happening at once so we can kind of uh",
    "start": "1494320",
    "end": "1497120"
  },
  {
    "text": "you know squ squish things together and",
    "start": "1497120",
    "end": "1499039"
  },
  {
    "text": "uh increase the throughput",
    "start": "1499039",
    "end": "1501360"
  },
  {
    "text": "here yeah so that is kind of a very",
    "start": "1501360",
    "end": "1505360"
  },
  {
    "text": "brief but I'd say pretty thorough look",
    "start": "1505360",
    "end": "1507760"
  },
  {
    "text": "at uh all the serving happening at",
    "start": "1507760",
    "end": "1509600"
  },
  {
    "text": "pretty base all the cloud native and uh",
    "start": "1509600",
    "end": "1512760"
  },
  {
    "text": "a little bit of specialized machine",
    "start": "1512760",
    "end": "1513960"
  },
  {
    "text": "learning optimizations we've done around",
    "start": "1513960",
    "end": "1516360"
  },
  {
    "text": "making serving work um so yeah thank you",
    "start": "1516360",
    "end": "1520039"
  },
  {
    "text": "and if you want to learn more about",
    "start": "1520039",
    "end": "1521000"
  },
  {
    "text": "Lorax um I linked a post about it um",
    "start": "1521000",
    "end": "1523679"
  },
  {
    "text": "here at in this slide as well as if you",
    "start": "1523679",
    "end": "1526320"
  },
  {
    "text": "want to connect with me and Julian those",
    "start": "1526320",
    "end": "1527520"
  },
  {
    "text": "are linkedin's um and there's you know a",
    "start": "1527520",
    "end": "1529760"
  },
  {
    "text": "few of us here from pry Bas so if you",
    "start": "1529760",
    "end": "1531159"
  },
  {
    "text": "see us around you know we love to chat",
    "start": "1531159",
    "end": "1533880"
  },
  {
    "text": "so you know uh we can talk about",
    "start": "1533880",
    "end": "1536360"
  },
  {
    "text": "anything from you know llm AI stuff all",
    "start": "1536360",
    "end": "1538600"
  },
  {
    "text": "the way to you know every every kind of",
    "start": "1538600",
    "end": "1540480"
  },
  {
    "text": "kubernetes thing we've done because",
    "start": "1540480",
    "end": "1541600"
  },
  {
    "text": "we've done a lot more than what was",
    "start": "1541600",
    "end": "1543000"
  },
  {
    "text": "mentioned here so thank",
    "start": "1543000",
    "end": "1546080"
  },
  {
    "text": "you",
    "start": "1546080",
    "end": "1549080"
  }
]