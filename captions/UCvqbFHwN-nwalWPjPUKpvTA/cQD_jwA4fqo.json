[
  {
    "text": "understandable good afternoon wilford spielenberg a patch unicorn a scheduled plug-in for",
    "start": "80",
    "end": "6560"
  },
  {
    "text": "batch workloads i was supposed to be here with my colleague craig but he stuck in the us with some",
    "start": "6560",
    "end": "14000"
  },
  {
    "text": "personal things that he needed to take care of so it's just me here for today",
    "start": "14000",
    "end": "20720"
  },
  {
    "text": "apache unicorn as this we've just announced that we've",
    "start": "20720",
    "end": "26080"
  },
  {
    "text": "graduated to a top level project on apache that came out yesterday so that's a big",
    "start": "26080",
    "end": "32960"
  },
  {
    "text": "thing to hear we'll go in and we'll dive into",
    "start": "32960",
    "end": "38320"
  },
  {
    "text": "what we're going to do today we're going to look at why we're doing apache unicorn what's uh",
    "start": "38320",
    "end": "46000"
  },
  {
    "text": "why did we design it why are we running it why do we need this or whether we want this for the batch workloads",
    "start": "46000",
    "end": "52399"
  },
  {
    "text": "and then we'll dive into a bit of the architecture we've been around for about four years",
    "start": "52399",
    "end": "57840"
  },
  {
    "text": "three and a half years which was before the plug-in framework was there so we come from",
    "start": "57840",
    "end": "65360"
  },
  {
    "text": "an old architecture into a new plug-in framework architecture and then we'll have a short demo and",
    "start": "65360",
    "end": "72640"
  },
  {
    "text": "i'll hope that i've got some time left over at the end for questions um and if not then we'll sort things out",
    "start": "72640",
    "end": "79600"
  },
  {
    "text": "along the way so the plugin framework um is only just released in in 1.0 so it's",
    "start": "79600",
    "end": "85200"
  },
  {
    "text": "in tech preview at the moment and we'll dive into that a little bit more",
    "start": "85200",
    "end": "90880"
  },
  {
    "text": "so this must start sounding familiar by now",
    "start": "90880",
    "end": "97200"
  },
  {
    "text": "all the talks that we've had today are talking about big data workload batch workloads needs",
    "start": "97200",
    "end": "103360"
  },
  {
    "text": "to be different it doesn't work like services on on kubernetes so from the perspective where we got where",
    "start": "103360",
    "end": "110320"
  },
  {
    "text": "we came from we're coming out of the yarn in the mesos world we look at",
    "start": "110320",
    "end": "115680"
  },
  {
    "text": "pots or or applications that get scheduled that have got a number of",
    "start": "115680",
    "end": "120880"
  },
  {
    "text": "different pieces and they belong together it's a logical grouping it's not specific for",
    "start": "120880",
    "end": "128560"
  },
  {
    "text": "anything that we do together not all workloads are equal",
    "start": "128560",
    "end": "134959"
  },
  {
    "text": "sometimes you've got things that you want to be first in first out financial data you first want",
    "start": "135680",
    "end": "141280"
  },
  {
    "text": "to process the data from yesterday before you do two days that's the first name first but if you've got interactive",
    "start": "141280",
    "end": "148319"
  },
  {
    "text": "sets of data you want to do that fair sharing you want to make sure that everybody gets their resources and",
    "start": "148319",
    "end": "153840"
  },
  {
    "text": "everybody goes through the way that they have submitted things the",
    "start": "153840",
    "end": "160160"
  },
  {
    "text": "other thing is that we if you look at batch workloads you want to do cueing it's it's been mentioned by",
    "start": "160160",
    "end": "166080"
  },
  {
    "text": "everybody you want to do some cueing you don't want to have something external needed to be need to resubmit a task if",
    "start": "166080",
    "end": "174080"
  },
  {
    "text": "it doesn't fit within the quota you want to hold on to it you want to schedule it later and just go through with all of",
    "start": "174080",
    "end": "180480"
  },
  {
    "text": "that workload queueing also because of the fact that your demand is not constant",
    "start": "180480",
    "end": "188480"
  },
  {
    "text": "often jobs run during the nights during the evenings for for the the standard",
    "start": "188480",
    "end": "194319"
  },
  {
    "text": "batches that get run but the interactive workloads that will run more during the day you've got completely different",
    "start": "194319",
    "end": "201360"
  },
  {
    "text": "demands on your cluster during the things during the time you want to do this postpone certain",
    "start": "201360",
    "end": "207280"
  },
  {
    "text": "workloads you want to hold on to them and you want to kick them off whenever you want to",
    "start": "207280",
    "end": "214000"
  },
  {
    "text": "so why apache unicorn so we looked at what",
    "start": "214000",
    "end": "219760"
  },
  {
    "text": "the default scheduler did and what we had as a demand from from the customers and what we were doing on a yarn and a",
    "start": "219760",
    "end": "226480"
  },
  {
    "text": "message kind of thing so in the default schedule we don't have an application concept you can't just say group all",
    "start": "226480",
    "end": "233519"
  },
  {
    "text": "these things together and run this as as one thing schedule them fit me resources do whatever you need to",
    "start": "233519",
    "end": "240480"
  },
  {
    "text": "do the scheduling algorithm is also limited we've got",
    "start": "240480",
    "end": "245840"
  },
  {
    "text": "one queue we sort the queue it's based on the pots that are there and then if",
    "start": "245840",
    "end": "250959"
  },
  {
    "text": "we've sorted all those spots we're gonna run them but the queue that we've got is",
    "start": "250959",
    "end": "257600"
  },
  {
    "text": "one queue for the whole cluster and i can have multiple types of workloads running at the same point in time so i",
    "start": "257600",
    "end": "264000"
  },
  {
    "text": "really would like to have multiple types of sorting algorithms for instance",
    "start": "264000",
    "end": "270160"
  },
  {
    "text": "one namespace will run with the first theme first out the other one will do fair sharing or priority based queuing",
    "start": "270160",
    "end": "278400"
  },
  {
    "text": "so that's that's the other part limits and quotas resource quotas are",
    "start": "278400",
    "end": "283440"
  },
  {
    "text": "not part of the scheduler on standard kubernetes you enforce them before you submit things so",
    "start": "283440",
    "end": "289919"
  },
  {
    "text": "somebody comes up and says i want to run this batch job oh there's no resources available oh now i need to come back and",
    "start": "289919",
    "end": "295919"
  },
  {
    "text": "retry to submit that same kind of job again and again up until they've got the resources available so that's",
    "start": "295919",
    "end": "303520"
  },
  {
    "text": "a problem the quotas because they're hard and forced",
    "start": "303520",
    "end": "308720"
  },
  {
    "text": "don't allow me to do any of the queuing so what do we do with",
    "start": "308720",
    "end": "314479"
  },
  {
    "text": "apache unicorn so we've got a different approach we pay we schedule based on applications whatever you",
    "start": "314479",
    "end": "322160"
  },
  {
    "text": "decide is an application will group together and we'll schedule that as part of one application that can be",
    "start": "322160",
    "end": "329600"
  },
  {
    "text": "anything from [Music] one pot or thousands of ports and pots",
    "start": "329600",
    "end": "335039"
  },
  {
    "text": "that are created dynamically or pots that are submitted all in one go so you decide on",
    "start": "335039",
    "end": "341360"
  },
  {
    "text": "what what to do there we create a hierarchical queue system so",
    "start": "341360",
    "end": "347280"
  },
  {
    "text": "we've we set up a queue on a route queues underneath uh and etcetera etcetera and",
    "start": "347280",
    "end": "354639"
  },
  {
    "text": "we enforce the quotas in that hierarchical hue system",
    "start": "354639",
    "end": "360000"
  },
  {
    "text": "later on we'll look a bit more on how we do that and what what that gives us uh and what the",
    "start": "360000",
    "end": "366639"
  },
  {
    "text": "the real extras get that we get out of that we can do",
    "start": "366639",
    "end": "372800"
  },
  {
    "text": "in that hierarchical uses we can do flexible quota distribution so i saw that in one of the other",
    "start": "372800",
    "end": "379280"
  },
  {
    "text": "presentations coming back too it's quota sharing i think it was in volcano that",
    "start": "379280",
    "end": "385520"
  },
  {
    "text": "a group is using only half of that quota another group within the same team wants",
    "start": "385520",
    "end": "391120"
  },
  {
    "text": "to use more so we can now share that quota flexible quota sharing and flexible",
    "start": "391120",
    "end": "396720"
  },
  {
    "text": "quota distributions we can set up in that hierarchical queue system",
    "start": "396720",
    "end": "401840"
  },
  {
    "text": "we've also got configuration configuration for our sorting policies so instead of having a sorting policy",
    "start": "401840",
    "end": "409360"
  },
  {
    "text": "that runs out of one queue we can sort at different levels we can sort",
    "start": "409360",
    "end": "415199"
  },
  {
    "text": "the queues which queue needs to be first which one needs to be later",
    "start": "415199",
    "end": "421039"
  },
  {
    "text": "how do you distribute your quotas between the queues and then we can also look at application",
    "start": "421039",
    "end": "428560"
  },
  {
    "text": "sorting so do you want to do priorities first in first out fair sharing and all",
    "start": "428560",
    "end": "434080"
  },
  {
    "text": "these things can be set up per queue so i've got the possibility to have",
    "start": "434080",
    "end": "442000"
  },
  {
    "text": "queues that are doing fare sharing queues that are doing fifo based on the applications again",
    "start": "442000",
    "end": "447680"
  },
  {
    "text": "and then the last but not least we've got the the node sorting also",
    "start": "447680",
    "end": "453440"
  },
  {
    "text": "the area where we come from we've got customers that want to run either in the cloud or on-premise and on-premise",
    "start": "453440",
    "end": "461680"
  },
  {
    "text": "you've got a steady cluster you don't scale up you don't scale down at least most people don't",
    "start": "461680",
    "end": "468160"
  },
  {
    "text": "so that means that you you want to be able to sort your notes differently",
    "start": "468160",
    "end": "473759"
  },
  {
    "text": "in the cloud you want to pack everything together make sure you get your cost under control pack all the pots that",
    "start": "473759",
    "end": "480080"
  },
  {
    "text": "you've got on one note you do bin packing and in",
    "start": "480080",
    "end": "485199"
  },
  {
    "text": "an on premise cluster you want to spread everything out as much as possible so you can use as much of the cpu that",
    "start": "485199",
    "end": "492319"
  },
  {
    "text": "you've got available and do bursting and all that kind of stuff",
    "start": "492319",
    "end": "497520"
  },
  {
    "text": "there's a there's also a couple of other things that we come up with a bit more advanced scheduling",
    "start": "497680",
    "end": "503199"
  },
  {
    "text": "requirements so apache spark has been used a number of times and as an example",
    "start": "503199",
    "end": "510080"
  },
  {
    "text": "so if you look at advanced scheduling we've got the gang scheduling you want to",
    "start": "510080",
    "end": "516240"
  },
  {
    "text": "create an application but you don't know beforehand how much it's going to use but you want to give",
    "start": "516240",
    "end": "522719"
  },
  {
    "text": "it a certain set of resources so you specify on the spark application or on",
    "start": "522719",
    "end": "529279"
  },
  {
    "text": "any application what kind of gang resource you want to use you can have",
    "start": "529279",
    "end": "535440"
  },
  {
    "text": "multiple gang definitions so multiple plot types multiple combinations of",
    "start": "535440",
    "end": "540640"
  },
  {
    "text": "things and we only start scheduling based on",
    "start": "540640",
    "end": "545680"
  },
  {
    "text": "the resources that are available so the other thing that comes up in the",
    "start": "545680",
    "end": "551120"
  },
  {
    "text": "gang scheduling and somebody mentioned that as sla scheduling i think it was called is the soft and hard scheduling the for",
    "start": "551120",
    "end": "558480"
  },
  {
    "text": "the gang if you say oh after 5 or 10 minutes or 15",
    "start": "558480",
    "end": "563600"
  },
  {
    "text": "minutes i don't care if i don't have all my gang resources available i want to start the job anyway then you can say",
    "start": "563600",
    "end": "570880"
  },
  {
    "text": "okay let's go and schedule and then we'll figure out where the rest of the",
    "start": "570880",
    "end": "576080"
  },
  {
    "text": "resources come from in other cases you want to say oh no no i really need all these resources if i",
    "start": "576080",
    "end": "582160"
  },
  {
    "text": "don't get them stop the application just fail it and let's go on but",
    "start": "582160",
    "end": "587279"
  },
  {
    "text": "that's over all those combinations are possible and we can set up all of that stuff for you",
    "start": "587279",
    "end": "594240"
  },
  {
    "text": "now look at the the application sorting again we sort perqueue",
    "start": "594240",
    "end": "599440"
  },
  {
    "text": "multiple applications are there if there are streaming applications that could be spark application a spark",
    "start": "599440",
    "end": "606560"
  },
  {
    "text": "application could also be an sql query that that somebody wants to run and wants to have a direct output so",
    "start": "606560",
    "end": "613600"
  },
  {
    "text": "depending on where we run depending what we do we can do the different swords",
    "start": "613600",
    "end": "618720"
  },
  {
    "text": "in there and the third point was the the note sorting",
    "start": "618720",
    "end": "625680"
  },
  {
    "text": "bin packing fair sharing whatever you want to do top one the bin packing",
    "start": "625680",
    "end": "632800"
  },
  {
    "text": "private is in the public clouds aws google whatever you want to run",
    "start": "632800",
    "end": "639360"
  },
  {
    "text": "the bottom you run your private cloud you share you",
    "start": "639360",
    "end": "644480"
  },
  {
    "text": "burst your your containers all these are available",
    "start": "644480",
    "end": "650560"
  },
  {
    "text": "application sorting gangster and all that kind of stuff you can set up different cues different things you can",
    "start": "650640",
    "end": "656880"
  },
  {
    "text": "do whatever you want but there's only one node sorting policy allowed for the cluster because there's there's no way",
    "start": "656880",
    "end": "663120"
  },
  {
    "text": "that you can sort nodes in a different way because you share all the nodes on on one thing",
    "start": "663120",
    "end": "670079"
  },
  {
    "text": "in the previous slides i stepped really quickly over what possibilities we've had in in",
    "start": "670959",
    "end": "678319"
  },
  {
    "text": "the hierarchical hue system so let's let's dive in a little bit more in into the hierarchical cue system i'm going to",
    "start": "678480",
    "end": "685040"
  },
  {
    "text": "take a really simple example you can make it as difficult as possible",
    "start": "685040",
    "end": "690880"
  },
  {
    "text": "again we're coming from the from the yarn perspective we've had customers that are running with",
    "start": "690880",
    "end": "696720"
  },
  {
    "text": "hundreds of queues in in five six levels deep with all kinds of",
    "start": "696720",
    "end": "702800"
  },
  {
    "text": "different combinations all kinds of different setups again all that stuff is also possible within unicorn but that's",
    "start": "702800",
    "end": "709760"
  },
  {
    "text": "not something that i want to go in too deeply in already so we take a simple setup",
    "start": "709760",
    "end": "716720"
  },
  {
    "text": "root cue with underneath that one tenant queue and we're going to schedule from from that",
    "start": "716720",
    "end": "723760"
  },
  {
    "text": "at the namespace and in kubernetes level we create three namespaces",
    "start": "725200",
    "end": "730959"
  },
  {
    "text": "we've got an unlimited and limited namespace those two namespaces will be scheduled completely according to the",
    "start": "730959",
    "end": "737279"
  },
  {
    "text": "unicorn logic and whatever we want to do with application sorting and all that kind of stuff we've got a non-unicorn namespace",
    "start": "737279",
    "end": "744959"
  },
  {
    "text": "also sitting beside that that one is not going to be scheduled by unicorn because we're running as a",
    "start": "744959",
    "end": "751519"
  },
  {
    "text": "plugin we're running as an extension of the default scheduler so what we say is",
    "start": "751519",
    "end": "757519"
  },
  {
    "text": "those two namespaces unlimited limited those will follow unicorn and whatever",
    "start": "757519",
    "end": "763839"
  },
  {
    "text": "you want to do and whatever you set up for the non-unicorn namespace we don't care we don't touch it we just go",
    "start": "763839",
    "end": "770560"
  },
  {
    "text": "through the logic that the default scheduler does with whatever you've set up there",
    "start": "770560",
    "end": "778000"
  },
  {
    "text": "in all these namespaces we run an x number of ports",
    "start": "778000",
    "end": "783519"
  },
  {
    "text": "the ports underneath the non-unicorn namespace are just separate pots they get handled",
    "start": "783519",
    "end": "789680"
  },
  {
    "text": "like that the pots under the unicorn namespaces will be handled as",
    "start": "789680",
    "end": "794800"
  },
  {
    "text": "logical groupings of in an application",
    "start": "794800",
    "end": "800360"
  },
  {
    "text": "the route has got a dynamic resource quota",
    "start": "804959",
    "end": "810880"
  },
  {
    "text": "it's the the total size of the cluster whenever you register a new node it gets picked up",
    "start": "810880",
    "end": "816639"
  },
  {
    "text": "root gets adjusted quota grows quota shrinks based on what is available in in the",
    "start": "816639",
    "end": "823360"
  },
  {
    "text": "cluster that's also important because we need that quota",
    "start": "823360",
    "end": "829199"
  },
  {
    "text": "a little bit later on when we go and do the next step because",
    "start": "829199",
    "end": "834480"
  },
  {
    "text": "the non-unicorn namespace that we've created does not have a quota within unicorn but",
    "start": "835279",
    "end": "842000"
  },
  {
    "text": "it still uses notes and and and an amount of resources in the cluster so",
    "start": "842000",
    "end": "849279"
  },
  {
    "text": "whatever is being used in non-unicorn namespace will be deducted from the root",
    "start": "849279",
    "end": "856399"
  },
  {
    "text": "quota that unicorn knows how to handle so we don't use the",
    "start": "856399",
    "end": "863519"
  },
  {
    "text": "quota that is set on the non-unicode names we use the real usage that is there so we track the ports and we",
    "start": "863519",
    "end": "870399"
  },
  {
    "text": "deduct that from whatever is there so again that makes the quota",
    "start": "870399",
    "end": "876240"
  },
  {
    "text": "at the root level dynamically adjusted",
    "start": "876240",
    "end": "882320"
  },
  {
    "text": "and then within unicorn we can set different quotas on different queues",
    "start": "882320",
    "end": "888160"
  },
  {
    "text": "in this case we've set a resource limit as an example on the tenant queue of 75",
    "start": "888160",
    "end": "893680"
  },
  {
    "text": "gigabytes 75 cpus whatever you want to set is fine we've",
    "start": "893680",
    "end": "898880"
  },
  {
    "text": "limited the limited queue also to a 50 gigabyte and a 50 cpu so with limited",
    "start": "898880",
    "end": "905760"
  },
  {
    "text": "corresponds to a namespace that namespace or the ports that will be",
    "start": "905760",
    "end": "912160"
  },
  {
    "text": "running in the namespace will be limited by unicorn to the 50 gigabyte unlimited does not have a",
    "start": "912160",
    "end": "919600"
  },
  {
    "text": "quota set directly on it but the parent of unlimited is limit is",
    "start": "919600",
    "end": "926079"
  },
  {
    "text": "the tenant queue so what happens is that when we try to schedule things in the",
    "start": "926079",
    "end": "931680"
  },
  {
    "text": "unlimited queue we're not only checking does this queue have a limit but we also look up to the parent",
    "start": "931680",
    "end": "938560"
  },
  {
    "text": "and the parent of the parent just to make sure that we keep track of that quota that is there",
    "start": "938560",
    "end": "944320"
  },
  {
    "text": "so we do a recursive check so effectively",
    "start": "944320",
    "end": "950320"
  },
  {
    "text": "in the unlimited queue we've got a 75 gigabyte 75 cpu limit set",
    "start": "950320",
    "end": "958000"
  },
  {
    "text": "however checking the the parents and the and the interaction",
    "start": "958000",
    "end": "963680"
  },
  {
    "text": "at the different levels is a bit more complex because what happens is that the usage within the tenant queue is really",
    "start": "963680",
    "end": "971440"
  },
  {
    "text": "the sum of all the queues that are below the tenant queue so whatever resources",
    "start": "971440",
    "end": "977360"
  },
  {
    "text": "are used in unlimited and limited get combined as a resource usage in the",
    "start": "977360",
    "end": "982639"
  },
  {
    "text": "tenant queue so if i use 50 gigabyte in limited then",
    "start": "982639",
    "end": "987920"
  },
  {
    "text": "really effectively unlimited can use another 25 gigabytes how we share that 75 gigabyte limit from",
    "start": "987920",
    "end": "995839"
  },
  {
    "text": "the tenant over these two queues depends on the sorting algorithm that i've set between",
    "start": "995839",
    "end": "1003440"
  },
  {
    "text": "the two queues so if i do fair sharing that means that based on",
    "start": "1003440",
    "end": "1009040"
  },
  {
    "text": "the amount of quota that i've got i get it nicely distributed over the two",
    "start": "1009040",
    "end": "1014160"
  },
  {
    "text": "queues if there's no load running in unlimited limited will be able to pick up the 50",
    "start": "1014160",
    "end": "1020399"
  },
  {
    "text": "gigabyte at the maximum and gets limited at that point there",
    "start": "1020399",
    "end": "1025839"
  },
  {
    "text": "so again looking at all these different ways of configuring things",
    "start": "1029120",
    "end": "1035120"
  },
  {
    "text": "we've got different cue sorting algorithms that we can set up we can do",
    "start": "1035120",
    "end": "1041520"
  },
  {
    "text": "the pots the applications within the unlimited queue as a fifo setup while we",
    "start": "1041520",
    "end": "1046558"
  },
  {
    "text": "do fair sharing in the limited queue combinations are endless you can set up",
    "start": "1046559",
    "end": "1053120"
  },
  {
    "text": "whatever you want um",
    "start": "1053120",
    "end": "1056960"
  },
  {
    "text": "then we'll go and have a little bit of a look at the architecture so what did we do how do we",
    "start": "1060080",
    "end": "1065520"
  },
  {
    "text": "we get where we are here for for this architecture so when we started the",
    "start": "1065520",
    "end": "1071360"
  },
  {
    "text": "the the system that was available was no extensions on the default scheduler",
    "start": "1071360",
    "end": "1078000"
  },
  {
    "text": "we couldn't do anything there was no plug-in framework the extenders were probably",
    "start": "1078000",
    "end": "1084480"
  },
  {
    "text": "still a little bit in development but that was abandoned halfway through",
    "start": "1084480",
    "end": "1089760"
  },
  {
    "text": "so we really didn't have any any options so what we did is",
    "start": "1089760",
    "end": "1096080"
  },
  {
    "text": "we built a simple architecture based on",
    "start": "1096080",
    "end": "1102879"
  },
  {
    "text": "a plug a plugin a shim and a core scheduler so because we came out of the yarn world we",
    "start": "1102960",
    "end": "1110000"
  },
  {
    "text": "thought we've had multiple resource managers nasa's yarn kubernetes all these kind of things so we want to be",
    "start": "1110000",
    "end": "1116080"
  },
  {
    "text": "able to run the scheduler on top of whatever resource manager is there",
    "start": "1116080",
    "end": "1122880"
  },
  {
    "text": "so what we decided was we built a core that does all the scheduling stuff",
    "start": "1122880",
    "end": "1130000"
  },
  {
    "text": "so it handles the queues that handles the quota checks that does all the things that we want to do of",
    "start": "1130000",
    "end": "1136720"
  },
  {
    "text": "enforcing all that stuff and we've got a shim that runs below that that hides all the resource",
    "start": "1136720",
    "end": "1143440"
  },
  {
    "text": "manager specific things for it so",
    "start": "1143440",
    "end": "1148799"
  },
  {
    "text": "the unicorn core doesn't really understand what a pot is it doesn't know what it is it just knows i've got a",
    "start": "1148799",
    "end": "1155280"
  },
  {
    "text": "resource request it has got an allocation that i need to schedule so we can remove the kubernetes shim and we",
    "start": "1155280",
    "end": "1160799"
  },
  {
    "text": "can put a yanching in place or a message him in place that does the same thing",
    "start": "1160799",
    "end": "1166000"
  },
  {
    "text": "the kubernetes sim talks to the api server and it converts whatever the api server gives inputs",
    "start": "1166000",
    "end": "1173120"
  },
  {
    "text": "into what we understand in the core as applications and all that kind of stuff",
    "start": "1173120",
    "end": "1179120"
  },
  {
    "text": "so this was the design that we had uh when we started about three three and a half years ago this is what we've",
    "start": "1179120",
    "end": "1185039"
  },
  {
    "text": "implemented um over the time that we were running the plug-in framework",
    "start": "1185039",
    "end": "1191760"
  },
  {
    "text": "became mature and we started we started using it so how did we change",
    "start": "1191760",
    "end": "1197600"
  },
  {
    "text": "our design from what was here implementing the whole scheduler doing",
    "start": "1197600",
    "end": "1202880"
  },
  {
    "text": "all the things that the default scheduler did to something a little bit more in line",
    "start": "1202880",
    "end": "1208240"
  },
  {
    "text": "with the plug-in framework so what we did is we pulled apart the shim so we replaced",
    "start": "1208240",
    "end": "1216799"
  },
  {
    "text": "all the things that we did within the shim with the plug-in framework so we integrated",
    "start": "1216799",
    "end": "1222559"
  },
  {
    "text": "instead of our own code for for handling the default for the handling the binding of",
    "start": "1222559",
    "end": "1227840"
  },
  {
    "text": "the ports and all that kind of stuff we replace that with using the plug-in framework and letting",
    "start": "1227840",
    "end": "1234480"
  },
  {
    "text": "the default scheduler handle all those things for us so the unicorn core hasn't changed it's",
    "start": "1234480",
    "end": "1240880"
  },
  {
    "text": "exactly the same as it was before we just replaced a part of our",
    "start": "1240880",
    "end": "1246320"
  },
  {
    "text": "shim with call outs and the plug-in integration",
    "start": "1246320",
    "end": "1251760"
  },
  {
    "text": "so in the current version that we've got we provide you with both options so you can",
    "start": "1251760",
    "end": "1258159"
  },
  {
    "text": "run the old model or you can run the new model both work and both are generated",
    "start": "1258159",
    "end": "1263679"
  },
  {
    "text": "from the same code it's just a different docker image that we provide for you for",
    "start": "1263679",
    "end": "1268799"
  },
  {
    "text": "running it",
    "start": "1268799",
    "end": "1271200"
  },
  {
    "text": "the plugin architecture for people that have looked at the",
    "start": "1274080",
    "end": "1280000"
  },
  {
    "text": "scheduler and have looked at the extent of the scheduler before this is a familiar picture",
    "start": "1280000",
    "end": "1286400"
  },
  {
    "text": "so we're there we started using that so what have we done what what what have we",
    "start": "1286400",
    "end": "1292960"
  },
  {
    "text": "implemented so we we've picked up",
    "start": "1292960",
    "end": "1298159"
  },
  {
    "text": "at the point that we we started and pick up the pre-filter in the pre-filter what we do is we pick out the parts that",
    "start": "1298159",
    "end": "1306480"
  },
  {
    "text": "unicorn needs to handle so everything that belongs to either namespace or things like that we want to manage as",
    "start": "1306480",
    "end": "1312320"
  },
  {
    "text": "unicorn through the unicorn code we pick those out and we we handle",
    "start": "1312320",
    "end": "1318000"
  },
  {
    "text": "them anything else we just let go we don't we we don't bother with it we just let pre-filter say yep it's all fine go",
    "start": "1318000",
    "end": "1325520"
  },
  {
    "text": "and do your thing the pre-filter ones we filter out whatever we want to do",
    "start": "1325520",
    "end": "1330960"
  },
  {
    "text": "so at that point we we get the pre-filter and",
    "start": "1330960",
    "end": "1337600"
  },
  {
    "text": "release whatever has passed so anything that passes through the",
    "start": "1338400",
    "end": "1344799"
  },
  {
    "text": "pre-filter and what needs to be handled by unicorn has passed the internal quota checks for for unicorn we know that we",
    "start": "1344799",
    "end": "1351600"
  },
  {
    "text": "are in the in the right state so we've we know that it's an application the application should be",
    "start": "1351600",
    "end": "1357039"
  },
  {
    "text": "running so it runs within the queue because not only can we limit things on",
    "start": "1357039",
    "end": "1362320"
  },
  {
    "text": "resources but we can also say oh you can only run 10 applications in that queue so we know that that application is",
    "start": "1362320",
    "end": "1368400"
  },
  {
    "text": "running and that we need to assign pod student and it fits within the quota so after the pre-filter has passed",
    "start": "1368400",
    "end": "1375919"
  },
  {
    "text": "the default scheduler will start looking for a node and assign a node to",
    "start": "1375919",
    "end": "1380960"
  },
  {
    "text": "the to the pod unicorn the core scheduler does the same we look at the node and we we also do",
    "start": "1380960",
    "end": "1388880"
  },
  {
    "text": "that in the field those two things come together so",
    "start": "1388880",
    "end": "1394799"
  },
  {
    "text": "the default scheduler says i've got a node it's for it is running this port and",
    "start": "1394799",
    "end": "1400400"
  },
  {
    "text": "we keep on rejecting the note up until we say oh no this is the note that we",
    "start": "1400400",
    "end": "1406000"
  },
  {
    "text": "have now decided unicorn has decided that the spot needs to run on and then we say yep all okay and we release",
    "start": "1406000",
    "end": "1413200"
  },
  {
    "text": "we release the pot at that point we let the default scheduler take over it will",
    "start": "1413200",
    "end": "1420240"
  },
  {
    "text": "do all the rest of the things and go through all the all the stuff up",
    "start": "1420240",
    "end": "1425440"
  },
  {
    "text": "until we get to the post binds",
    "start": "1425440",
    "end": "1431200"
  },
  {
    "text": "so what we do is internally within the shim we have our own accounting and our",
    "start": "1431200",
    "end": "1436480"
  },
  {
    "text": "tracking and our own filtering and our logging things and up until we get to the post bind",
    "start": "1436480",
    "end": "1444320"
  },
  {
    "text": "we say we we we are stopped we've got nothing to do",
    "start": "1444320",
    "end": "1449840"
  },
  {
    "text": "postponed will update our tracking",
    "start": "1449840",
    "end": "1454158"
  },
  {
    "text": "for this we've got a short demo that will show you uh two sets of parts that we do",
    "start": "1455679",
    "end": "1464400"
  },
  {
    "text": "we've got a set of parts that will be tracked by unicorn and a set of pots that won't be",
    "start": "1465039",
    "end": "1471360"
  },
  {
    "text": "tracked by unicorn and we'll show you slightly and then a quick overview in a two minute demo but how",
    "start": "1471360",
    "end": "1478880"
  },
  {
    "text": "that looks from the front end and i hope that the sound works",
    "start": "1478880",
    "end": "1485039"
  },
  {
    "text": "and now for a short demo of unicorn's new plugin interface we first create two namespaces each",
    "start": "1485039",
    "end": "1490559"
  },
  {
    "text": "having a quota of one cpu and one gigabyte of ram however unicorn has been configured so that namespace 2 will",
    "start": "1490559",
    "end": "1497600"
  },
  {
    "text": "utilize the internal default scheduling logic rather than the queueing logic that unicorn normally uses",
    "start": "1497600",
    "end": "1503760"
  },
  {
    "text": "now we create three sleep pods in the first name space there is only room for two of these to run given our cue",
    "start": "1503760",
    "end": "1509520"
  },
  {
    "text": "settings so when we list the pods out we can see here that in fact only two of the pods",
    "start": "1509520",
    "end": "1515520"
  },
  {
    "text": "have been approved for scheduling and the third remains an appending status",
    "start": "1515520",
    "end": "1520960"
  },
  {
    "text": "we can also see that a scheduler name has been set of unicorn and a default generated application id has been",
    "start": "1520960",
    "end": "1526799"
  },
  {
    "text": "assigned to these pods we can also check the web interface and see that in fact a cue has been created",
    "start": "1526799",
    "end": "1532559"
  },
  {
    "text": "automatically for this application it is in a running state and two of the pods are active",
    "start": "1532559",
    "end": "1538240"
  },
  {
    "text": "now we can clean up our pods so that we can create them in our second namespace now we create three pods in our second",
    "start": "1538240",
    "end": "1544880"
  },
  {
    "text": "namespace this namespace has been configured to bypass unicorn queuing and so all three pods will be allowed to",
    "start": "1544880",
    "end": "1551120"
  },
  {
    "text": "start at the same time and we can see that here we can also see that unicorn has been",
    "start": "1551120",
    "end": "1556480"
  },
  {
    "text": "assigned as the scheduler so we haven't completely bypassed unicorn however we have not associated an application id or",
    "start": "1556480",
    "end": "1562400"
  },
  {
    "text": "a queue to these pots we can see this in the web interface by the fact that there is no queue for",
    "start": "1562400",
    "end": "1568080"
  },
  {
    "text": "namespace 2. thank you for watching our demo have a great day",
    "start": "1568080",
    "end": "1574799"
  },
  {
    "text": "so that was done by craig who couldn't be here today so",
    "start": "1576080",
    "end": "1581279"
  },
  {
    "text": "what we saw here is we pick up the ports we schedule a part of them through unicorn and part of them we don't",
    "start": "1581279",
    "end": "1588480"
  },
  {
    "text": "um in summary so what we've what we've done",
    "start": "1588480",
    "end": "1593520"
  },
  {
    "text": "the community we've released 1.0 with a tech preview of this we're still missing a number of things",
    "start": "1594799",
    "end": "1601440"
  },
  {
    "text": "scale and performance testing we haven't done yet we started working with other",
    "start": "1601440",
    "end": "1606799"
  },
  {
    "text": "communities apache spark was already named by the volcano guys",
    "start": "1606799",
    "end": "1612559"
  },
  {
    "text": "we're working with them on the same spark jira to get things going",
    "start": "1612559",
    "end": "1618320"
  },
  {
    "text": "with six scheduling we're trying to integrate with them also and do a little",
    "start": "1618320",
    "end": "1623440"
  },
  {
    "text": "bit more we've got some improvement in future work that we we want to do",
    "start": "1623440",
    "end": "1629919"
  },
  {
    "text": "the pre-filter gives us a lot of unscheduled parts which affects",
    "start": "1629919",
    "end": "1635679"
  },
  {
    "text": "our auto scaling badly there's a pre and q hook that just popped up",
    "start": "1635679",
    "end": "1643120"
  },
  {
    "text": "about two weeks ago so a week and a half ago that is looking like a really good solution for what we do with batch",
    "start": "1643120",
    "end": "1649120"
  },
  {
    "text": "scheduling for us the other thing is that we've got a problem uh something that we need to",
    "start": "1649120",
    "end": "1655039"
  },
  {
    "text": "look at is with the autoscaler with the impact of the out of quota pots on the on the auto scaler",
    "start": "1655039",
    "end": "1661600"
  },
  {
    "text": "so that is really limited to what we do if you don't use the outscaler we don't",
    "start": "1661600",
    "end": "1667919"
  },
  {
    "text": "have that problem so but okay and then i think we've got some",
    "start": "1667919",
    "end": "1673440"
  },
  {
    "text": "time left over for q a i don't know how good i am at for time",
    "start": "1673440",
    "end": "1679880"
  },
  {
    "text": "can you handle gpu resources yes so",
    "start": "1685840",
    "end": "1692080"
  },
  {
    "text": "from a from a scheduling perspective from the core perspective we are um resource",
    "start": "1692080",
    "end": "1698559"
  },
  {
    "text": "agnostic so whatever you define as a resource if even if you say i want to uh",
    "start": "1698559",
    "end": "1703760"
  },
  {
    "text": "schedule pots that use a resource called license we don't care you can schedule whatever",
    "start": "1703760",
    "end": "1709279"
  },
  {
    "text": "you want so we do if if you can define it on the pot and you can define it",
    "start": "1709279",
    "end": "1714960"
  },
  {
    "text": "anywhere else we can schedule on that and we can see it as a resource yeah",
    "start": "1714960",
    "end": "1720880"
  },
  {
    "text": "just from your experience do you find like you mentioned that there's some customers using deep hierarchies",
    "start": "1721840",
    "end": "1728399"
  },
  {
    "text": "do you think that was a good idea do you think that they are being abused maybe we should for example force them",
    "start": "1728399",
    "end": "1734720"
  },
  {
    "text": "not to do that because i imagine to be really hard reasonable you're saying six seven levels that's",
    "start": "1734720",
    "end": "1740640"
  },
  {
    "text": "just like yes yeah yeah yeah no so yes",
    "start": "1740640",
    "end": "1745919"
  },
  {
    "text": "those q hierarchies are really difficult to talk about and really difficult to understand but",
    "start": "1745919",
    "end": "1752480"
  },
  {
    "text": "the companies that use them they what they do they've got a logical grouping of that so let's say you've got",
    "start": "1752480",
    "end": "1759279"
  },
  {
    "text": "a a multinational that says i've got the company as the top level than us as",
    "start": "1759279",
    "end": "1767279"
  },
  {
    "text": "uh the second level um and other regions around the world as the second level",
    "start": "1767279",
    "end": "1772880"
  },
  {
    "text": "then within that so you go down so they've got a really logical setup for that and they spread",
    "start": "1772880",
    "end": "1778960"
  },
  {
    "text": "and do things around that in in that way so yeah it's",
    "start": "1778960",
    "end": "1784159"
  },
  {
    "text": "it's unbelievably hard to troubleshoot and to to understand but yeah there's a logical",
    "start": "1784159",
    "end": "1790320"
  },
  {
    "text": "solution for for customers",
    "start": "1790320",
    "end": "1794519"
  },
  {
    "text": "thank you wilfred i hope you stay around for more questions i'll be around for more",
    "start": "1797360",
    "end": "1802480"
  },
  {
    "text": "questions without a problem [Applause]",
    "start": "1802480",
    "end": "1808159"
  },
  {
    "text": "you",
    "start": "1808159",
    "end": "1810240"
  }
]