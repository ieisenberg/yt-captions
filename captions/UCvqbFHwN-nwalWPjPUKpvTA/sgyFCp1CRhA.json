[
  {
    "text": "all right hello everyone uh thank you for coming to our session uh secret V2 is coming soon to a clustering area",
    "start": "120",
    "end": "5279"
  },
  {
    "text": "thank you for coming after lunch uh my name is David Porter I'm from Google I worked on the GK node team and I work in",
    "start": "5279",
    "end": "13259"
  },
  {
    "text": "Upstream signode uh this is Ronald Patel uh Hey folks I'm Ronald Patel I work for",
    "start": "13259",
    "end": "18840"
  },
  {
    "text": "Red Hat uh so I work on container runtimes I'm a maintainer of OCR runtime spec run C Grail and I also work on",
    "start": "18840",
    "end": "26699"
  },
  {
    "text": "Signal Upstream so uh first off we're gonna talk about",
    "start": "26699",
    "end": "32398"
  },
  {
    "text": "Resource Management what is Resource Management really in terms of kubernetes",
    "start": "32399",
    "end": "39260"
  },
  {
    "text": "so here's a 10 000 view uh 10 000 feet overview of resource management right clusters consist of nodes and nodes have",
    "start": "39360",
    "end": "47399"
  },
  {
    "text": "resources like CPUs memory disks gpus and resource management is about",
    "start": "47399",
    "end": "53520"
  },
  {
    "text": "management managing these resources so nodes advertise the availability to",
    "start": "53520",
    "end": "59879"
  },
  {
    "text": "the kubernetes scheduler so typically you have some amount of memory on your node say 32 GB you want to reserve some",
    "start": "59879",
    "end": "67320"
  },
  {
    "text": "for your system like the kernel and the and the processes that are there on the Node running natively a system Services",
    "start": "67320",
    "end": "73020"
  },
  {
    "text": "then you want to reserve some for the cubelet India container runtime say you take away 4 GB each you have 24",
    "start": "73020",
    "end": "80400"
  },
  {
    "text": "remaining and that's what's advertised to the scheduler as allocatable",
    "start": "80400",
    "end": "86960"
  },
  {
    "text": "so here's an example of a pod that has resources set it has request and limits",
    "start": "88020",
    "end": "94500"
  },
  {
    "text": "so the scheduler is looking at requests when it is scheduling pods on nodes so",
    "start": "94500",
    "end": "99900"
  },
  {
    "text": "when it finds a node that's able to satisfy the request it schedules the",
    "start": "99900",
    "end": "105900"
  },
  {
    "text": "part on that node and limit is what a pod cannot exceed",
    "start": "105900",
    "end": "113180"
  },
  {
    "text": "so what are some of the requirements from Resource Management so Port should",
    "start": "115259",
    "end": "120360"
  },
  {
    "text": "not be able to hurt each other right they should stay within the limits they should get consistent Performance Based",
    "start": "120360",
    "end": "126899"
  },
  {
    "text": "on what they requested we should be able to prevent infinite Loops for bombs",
    "start": "126899",
    "end": "132599"
  },
  {
    "text": "memory leaks node lockups and we should allocate the right amount of resources for parts and also we want to ensure",
    "start": "132599",
    "end": "139800"
  },
  {
    "text": "that doing all this management doesn't utilize a lot of resources ultimately we",
    "start": "139800",
    "end": "145440"
  },
  {
    "text": "want to allow as many pods to be run on a node as possible right you don't want to have",
    "start": "145440",
    "end": "151080"
  },
  {
    "text": "a lot of overhead from the system components",
    "start": "151080",
    "end": "155660"
  },
  {
    "text": "so how do we do this so we do this with something called a c groups and Linux kernel it's it's a way to group a set of",
    "start": "156959",
    "end": "164340"
  },
  {
    "text": "processes hierarchically hierarchically and then you have a set of controllers that allow you to put limits on those",
    "start": "164340",
    "end": "170580"
  },
  {
    "text": "processes so we have the CPU memory iOS these are some of the controllers that",
    "start": "170580",
    "end": "176640"
  },
  {
    "text": "can be used to put limits on the processes and c groups are controlled",
    "start": "176640",
    "end": "182220"
  },
  {
    "text": "through a pseudo file system which is called a c group FS so basically you write to these files to set limits and",
    "start": "182220",
    "end": "189060"
  },
  {
    "text": "then there are other files that you read to Monitor and get statistics back on",
    "start": "189060",
    "end": "195000"
  },
  {
    "text": "what how much memory it's using how many processes are running and so on",
    "start": "195000",
    "end": "200420"
  },
  {
    "text": "so here's a history of c groups so the first version of c groups was introduced",
    "start": "201480",
    "end": "207239"
  },
  {
    "text": "by Google and the Linux kernel in 2006 it was first called as process controllers it didn't cover everything",
    "start": "207239",
    "end": "213720"
  },
  {
    "text": "then slowly over time lot of different folks came and contributed other controllers",
    "start": "213720",
    "end": "219720"
  },
  {
    "text": "uh then uh in V2 development started in",
    "start": "219720",
    "end": "225299"
  },
  {
    "text": "2016 some of the goals of V2 was try to simplify things like things were added organically right as and so the",
    "start": "225299",
    "end": "232500"
  },
  {
    "text": "controllers weren't working well with each other and they were not unified so V2 is an attempt to simplify things and",
    "start": "232500",
    "end": "240299"
  },
  {
    "text": "also provide more features and more stability so Fedora moved to V2 in 2019 Docker and",
    "start": "240299",
    "end": "247379"
  },
  {
    "text": "run C group support for it in 2020 and in 2021 most of the destroys are now",
    "start": "247379",
    "end": "253799"
  },
  {
    "text": "enabling c groups V2 by default also like V1 is considered Legacy at this point right so kernel fixes in this area",
    "start": "253799",
    "end": "261419"
  },
  {
    "text": "are mainly going to V2 also system D is planning to remove V1 support by end of",
    "start": "261419",
    "end": "268259"
  },
  {
    "text": "2023. so the big message here really is that V2 is real it's coming if you",
    "start": "268259",
    "end": "274979"
  },
  {
    "text": "aren't already testing it you should be planning to test out V2 to make sure it works well for you and you can give us",
    "start": "274979",
    "end": "281880"
  },
  {
    "text": "feedback to fix any issues that you find",
    "start": "281880",
    "end": "287120"
  },
  {
    "text": "so here's an overview like basically the slide shows that all the popular",
    "start": "287400",
    "end": "293699"
  },
  {
    "text": "distributions that I use with kubernetes today all the container runtimes the high level ones like containerdy cryo",
    "start": "293699",
    "end": "300060"
  },
  {
    "text": "Docker the lower level ones like run CC run have support for c groups V2 and uh",
    "start": "300060",
    "end": "307020"
  },
  {
    "text": "in 125 we finally went GA with c groups V2 so kubernetes supports it now",
    "start": "307020",
    "end": "315320"
  },
  {
    "text": "so what's new in V2 right so the first thing is a single unified hierarchy so",
    "start": "315780",
    "end": "321240"
  },
  {
    "text": "I'll go over that in the next slide when there are some additional improvements so I mentioned like how the controllers",
    "start": "321240",
    "end": "327960"
  },
  {
    "text": "were added one after the other right so with V2 now work has been done so they",
    "start": "327960",
    "end": "333120"
  },
  {
    "text": "can work well together so one example is Page cache right backs right the memory",
    "start": "333120",
    "end": "338160"
  },
  {
    "text": "and the i o controllers work together to properly account which processes are charged for that",
    "start": "338160",
    "end": "343740"
  },
  {
    "text": "then on the memory side like user and memory TCP socket buffers kernel memory",
    "start": "343740",
    "end": "350220"
  },
  {
    "text": "such as inodes and entries are tracked together under the memory uh memory",
    "start": "350220",
    "end": "355500"
  },
  {
    "text": "controls also on the memory side we have way more knobs compared to before so we have more",
    "start": "355500",
    "end": "363419"
  },
  {
    "text": "control over when the kernel starts throttling your memory allocations",
    "start": "363419",
    "end": "368479"
  },
  {
    "text": "instead of just like hitting a limit and getting boom killed when there's something new called as",
    "start": "368479",
    "end": "374940"
  },
  {
    "text": "pressure stall information so it allows us to monitor how much resource pressure is there in a",
    "start": "374940",
    "end": "382620"
  },
  {
    "text": "particular C group for CPU memory or i o and finally like this better delegation",
    "start": "382620",
    "end": "390360"
  },
  {
    "text": "support so this allows us to run rootless containers well like for",
    "start": "390360",
    "end": "395400"
  },
  {
    "text": "example podman uses this for its rootless support",
    "start": "395400",
    "end": "400520"
  },
  {
    "text": "so here's an example of V1 versus V2 hierarchy so on the left you see a c",
    "start": "400919",
    "end": "406860"
  },
  {
    "text": "groups V1 and you can see like how the CPU and memory controllers are mounted",
    "start": "406860",
    "end": "412380"
  },
  {
    "text": "separately under this FSC group and then you have to go and add your",
    "start": "412380",
    "end": "418199"
  },
  {
    "text": "process to each of them separately so it it's more flexible but it's clunky right",
    "start": "418199",
    "end": "424620"
  },
  {
    "text": "and in practice most of the time we're gonna end up putting them under the same hierarchy so on the right you see that",
    "start": "424620",
    "end": "431940"
  },
  {
    "text": "with V2 there is a single hierarchy of c groups and then finally all the settings for a particular C group are under us",
    "start": "431940",
    "end": "440280"
  },
  {
    "text": "under a single directory so that's uh that's a unified hierarchy I will hand",
    "start": "440280",
    "end": "446340"
  },
  {
    "text": "it off to David to cover some more details here thank you manal for explaining some of",
    "start": "446340",
    "end": "451740"
  },
  {
    "text": "the new features in secret V2 so I'm going to talk a little bit about how kublet and kubernetes actually makes use",
    "start": "451740",
    "end": "457319"
  },
  {
    "text": "of c groups to provide Resource Management so the important thing to understand here is there's actually two components that interface with c groups",
    "start": "457319",
    "end": "464160"
  },
  {
    "text": "in the context of a node it's the kubelet and the container runtime so the way it works is that Kubla creates a c",
    "start": "464160",
    "end": "470340"
  },
  {
    "text": "group for each pod when you start a pod the Kubler actually creates a c group to house all the containers within that pod",
    "start": "470340",
    "end": "476759"
  },
  {
    "text": "next the container runtime will actually create a c group for each container so the Kubo actually passes the path of the",
    "start": "476759",
    "end": "483060"
  },
  {
    "text": "C group for the Pod to The Container runtime and the container runtime owns the C group within each pod",
    "start": "483060",
    "end": "488280"
  },
  {
    "text": "the other thing is that kublet actually manages not just the Pod c groups but the whole secret hierarchy with Qs",
    "start": "488280",
    "end": "494340"
  },
  {
    "text": "classes so kubernetes is the concept of Qs class there's burstable best effort and guaranteed pods and there's in",
    "start": "494340",
    "end": "499740"
  },
  {
    "text": "different levels of the secret hierarchy so depending on the Qs class of the Pod the C group of the Pod will be placed",
    "start": "499740",
    "end": "506280"
  },
  {
    "text": "under that qos top level C group so I want to talk a little bit about how",
    "start": "506280",
    "end": "512279"
  },
  {
    "text": "do we actually set these secret values how do they get from your pod spec into the into the kernel and set into the C",
    "start": "512279",
    "end": "518039"
  },
  {
    "text": "group FS so it all starts with a with a pod spec right you create your pod the kublet observes it and it has some some",
    "start": "518039",
    "end": "525240"
  },
  {
    "text": "amount of requests and limits set the next step is that Kubler will go ahead and create the Pod C group then",
    "start": "525240",
    "end": "531720"
  },
  {
    "text": "for the containers it's going to actually talk to the CRI the container runtime over the CRI protocol over grpc",
    "start": "531720",
    "end": "538140"
  },
  {
    "text": "and the CRI protocol actually has some definitions for all of these different values so we start to convert these",
    "start": "538140",
    "end": "543420"
  },
  {
    "text": "values from the Pod spec into the CRI kind of definition the next step is that the CRI will",
    "start": "543420",
    "end": "550500"
  },
  {
    "text": "actually be responsible to talk to the underlying container runtime and sometimes it's the same component sometimes it's a different kind of sub component and it converts basically the",
    "start": "550500",
    "end": "557519"
  },
  {
    "text": "CRI into the real container so we create an oci Json specification oci is the",
    "start": "557519",
    "end": "562920"
  },
  {
    "text": "container kind of standard and inside the oci spec there's this config.json file which is a standard and it has",
    "start": "562920",
    "end": "569399"
  },
  {
    "text": "different fields for the resources uh so it has explicit fields for the memory and CPU and it actually also has a new",
    "start": "569399",
    "end": "576180"
  },
  {
    "text": "unified field that was added for Seagram V2 to add to set secret V2 properties so",
    "start": "576180",
    "end": "581339"
  },
  {
    "text": "once we have the oci Json spec the next step is to pass it to a lower level container runtime that can run oci",
    "start": "581339",
    "end": "586980"
  },
  {
    "text": "container images so this is usually run C this is kind of the standard and depending on something called the system",
    "start": "586980",
    "end": "593339"
  },
  {
    "text": "the C group driver that's being used and I'll go into a little bit about what that means uh usually if you're using",
    "start": "593339",
    "end": "598740"
  },
  {
    "text": "cwv2 you should be using the system DC Group driver what that means is that the oci container runtime run C here will",
    "start": "598740",
    "end": "604440"
  },
  {
    "text": "actually talk to systemd on the machine to create something called a systemd scope unit so it will be managed by",
    "start": "604440",
    "end": "610380"
  },
  {
    "text": "systemd and systemd has understanding around all these properties around CPU and memory and other resource controls",
    "start": "610380",
    "end": "616980"
  },
  {
    "text": "run c will also actually talk to the Linux kernel so systemd and run c will both talk to C group FS on the Linux",
    "start": "616980",
    "end": "623220"
  },
  {
    "text": "kernel to finally set the actual values in in the kernel and set each property for for the resource requirements so",
    "start": "623220",
    "end": "630480"
  },
  {
    "text": "that's how we get from the Pod spec to C group Fest so let's talk a little bit about what",
    "start": "630480",
    "end": "637200"
  },
  {
    "text": "properties are actually set and what they do so the main properties we have right now are CPU and memory that's the",
    "start": "637200",
    "end": "642420"
  },
  {
    "text": "main resources we kind of manage so the first thing let's talk about is CPU requests so when you set a CPU request",
    "start": "642420",
    "end": "647880"
  },
  {
    "text": "what are you actually doing the first thing is that you're telling what the minimum amount of CPU your container",
    "start": "647880",
    "end": "653160"
  },
  {
    "text": "needs and so the schedule will look at the CPU request and before it schedules a pod to a node it checks that that node",
    "start": "653160",
    "end": "659820"
  },
  {
    "text": "has that CPU available this gives you the guarantee that you'll always get that CPU request even if the node is 100",
    "start": "659820",
    "end": "665459"
  },
  {
    "text": "busy everything's using CPU you'll always get the CPU request for the kublet and the scheduler never over",
    "start": "665459",
    "end": "670680"
  },
  {
    "text": "commit on CPU to actually Implement a CPU we use something called CPU shares",
    "start": "670680",
    "end": "676500"
  },
  {
    "text": "which is a Linux kernel feature it's called CPU shares in in secret V1 and cpu.weight and Cedar V2 but the concept",
    "start": "676500",
    "end": "683399"
  },
  {
    "text": "does the same and the idea is it's a unit that we use uh and each container gets some amount of these CPU shares",
    "start": "683399",
    "end": "689459"
  },
  {
    "text": "which is kind of an arbitrary unit and then the kernel when it does actually CPU scheduling it'll take a look and",
    "start": "689459",
    "end": "694920"
  },
  {
    "text": "kind of sum up all the CPU shares in a given secret hierarchy and understand the ratio of some amount of shares in",
    "start": "694920",
    "end": "700920"
  },
  {
    "text": "one C group to another and that ratio is how much CPU 1C group will get compared to a different C group so in this simple",
    "start": "700920",
    "end": "706500"
  },
  {
    "text": "example here we have kind of one CPU and we have three containers you can think one is 1024 shares another two of 512.",
    "start": "706500",
    "end": "712500"
  },
  {
    "text": "so the first one will get 50 of the CPU and the other two will get 25 and if you had more CPUs with some of this across",
    "start": "712500",
    "end": "718440"
  },
  {
    "text": "all the CPUs on the system so that's CPU limits or let's give you a",
    "start": "718440",
    "end": "724380"
  },
  {
    "text": "request now let's talk about CPU limits so CPU limits use something called CFS bandwidth control in the Linux kernel to",
    "start": "724380",
    "end": "731040"
  },
  {
    "text": "actually be implemented and unlike CPU request the scheduler actually completely ignores CPU limit when it does scheduling so this is only used to",
    "start": "731040",
    "end": "737820"
  },
  {
    "text": "be enforced uh in c groups so the way to think about CPU limits is they are the",
    "start": "737820",
    "end": "743100"
  },
  {
    "text": "ceiling for CPU you can never use more CPU than you put in the limit in fact if you use more CPU than you put in your",
    "start": "743100",
    "end": "749760"
  },
  {
    "text": "limit you will be throttled by the CFS bandwidth control in the kernel and so the important thing to know here is",
    "start": "749760",
    "end": "755279"
  },
  {
    "text": "you'll be throttled even if their spare CPU Cycles available the way this works is that uh there's two concepts called",
    "start": "755279",
    "end": "760980"
  },
  {
    "text": "CPU quota and CPU period and in secret V2 they're called CPU Max but the same properties apply and the idea is you",
    "start": "760980",
    "end": "767160"
  },
  {
    "text": "have a CPU period and a period is a unit of time usually the default and the default everyone uses 100 milliseconds",
    "start": "767160",
    "end": "772800"
  },
  {
    "text": "and then you have some amount of quota and the way to think about it is you basically get that amount of code that amount of time that you can use for CPU",
    "start": "772800",
    "end": "780240"
  },
  {
    "text": "in each wall clock kind of period so you have 100 millisecond period and you get some amount of quota and you can use that amount of quota within 100",
    "start": "780240",
    "end": "786360"
  },
  {
    "text": "milliseconds if you use up all the quota uh within that first 100 milliseconds or before the 100 milliseconds are over you",
    "start": "786360",
    "end": "792540"
  },
  {
    "text": "get throttled and then you need to wait until that 100 milliseconds expires and then in the next kind of 100 milliseconds you again can use that",
    "start": "792540",
    "end": "798899"
  },
  {
    "text": "quota so it's kind of this burst Bank where you constantly get refilled with quota and at each at each period you",
    "start": "798899",
    "end": "804899"
  },
  {
    "text": "have the ability to use that quota and if you use more quota than you're allowed to you'll get throttled so that's the idea behind CPU limits",
    "start": "804899",
    "end": "811560"
  },
  {
    "text": "so with CPU limits there's kind of been something I want to address which is kind of what I call the CPU limits debate so if you look online there's",
    "start": "811560",
    "end": "817800"
  },
  {
    "text": "kind of a debate going on you'll see tweets like this that for example say never set CPU limit and then there's",
    "start": "817800",
    "end": "823260"
  },
  {
    "text": "other tweets that say something like you know debate's raging should you set CPU limits yes or no uh there's articles say",
    "start": "823260",
    "end": "828660"
  },
  {
    "text": "you should keep using CPU limits and kubernetes and then other articles for the love of God stop using CPU limits so",
    "start": "828660",
    "end": "834779"
  },
  {
    "text": "really confusing and you might be asking yourself all right what is going on here what should I be doing with CPU limits",
    "start": "834779",
    "end": "840000"
  },
  {
    "text": "so let me try to give you my take on it so the first thing that we can kind of all agree that I don't think there's any",
    "start": "840000",
    "end": "845220"
  },
  {
    "text": "kind of uh any debate about is always setting a CPU request so CPU requests are used for scheduling you need to set",
    "start": "845220",
    "end": "851519"
  },
  {
    "text": "a CPU request to uh provide the minimum amount of CPU you need and if you don't",
    "start": "851519",
    "end": "856740"
  },
  {
    "text": "set it you'll get you'll become a best effort pod qos and you will basically not be guaranteed any amount of CPU so",
    "start": "856740",
    "end": "862380"
  },
  {
    "text": "always set a CPU request that we can all agree on now about CPU limits so like all things I can't give you a definitive",
    "start": "862380",
    "end": "868740"
  },
  {
    "text": "answer I think there's trade-offs here so let me try to explain them so the cons of CPU limits is kind of the the",
    "start": "868740",
    "end": "874800"
  },
  {
    "text": "feature of CPU limits at the same point which is that you can't use any spare CPU Cycles so if there's spare CPU",
    "start": "874800",
    "end": "880260"
  },
  {
    "text": "Cycles on the Node and you set a CPU limit and you hit that limit you can't use that spare CPU available right and",
    "start": "880260",
    "end": "886380"
  },
  {
    "text": "so this kind of translates that you're kind of throwing away unused CPU you have CPU available but your pods can't use it and you know if your pod is",
    "start": "886380",
    "end": "892079"
  },
  {
    "text": "really bursty and suddenly gets more traffic or has more things it needs to do it won't be able to use those spare CPU cycles and so if you start to",
    "start": "892079",
    "end": "898920"
  },
  {
    "text": "measure this and kind of and analyze it you and you start to graph it out you might see that you might introduce some",
    "start": "898920",
    "end": "904139"
  },
  {
    "text": "artificial throttling into your application and especially your you know P99 latency for example may increase",
    "start": "904139",
    "end": "909540"
  },
  {
    "text": "so that's kind of the cons right but the pros of setting CPU limits is that you're not actually relying on those",
    "start": "909540",
    "end": "915300"
  },
  {
    "text": "spare CPU Cycles so if you're constantly relying on spare CPU cycles and you're constantly hitting the throttling limit",
    "start": "915300",
    "end": "921060"
  },
  {
    "text": "that probably means you set a low CPU request and the problem with that is that those spare CPU Cycles are",
    "start": "921060",
    "end": "926579"
  },
  {
    "text": "unpredictable right there's no guarantee you get them if there's other pods that are scheduled later that use up a lot of CPU you will not be able to use that CPU",
    "start": "926579",
    "end": "933120"
  },
  {
    "text": "that's available right because somebody else is using it so you're kind of relying on this unpredictable CPU right",
    "start": "933120",
    "end": "939000"
  },
  {
    "text": "and that's kind of the issue there so if you do set a CPU limit you'll kind of get more predictable Behavior right",
    "start": "939000",
    "end": "944040"
  },
  {
    "text": "you'll get you'll become in the guaranteed qos tier and you'll always ensure that you're not relying on this unpredictable CPU cycle that may not be",
    "start": "944040",
    "end": "951000"
  },
  {
    "text": "there the other scenario where it's useful is a multi-tenant environment so if you have multiple teams for example scheduling pods on a node and you want",
    "start": "951000",
    "end": "957360"
  },
  {
    "text": "to do some sort of chargeback and ensure that one team can't use some amount of CPU it's useful in that scenario as well",
    "start": "957360",
    "end": "963600"
  },
  {
    "text": "so that's kind of CPU um memory is actually kind of simpler to understand memory request it's only used",
    "start": "963600",
    "end": "969660"
  },
  {
    "text": "for scheduling we don't actually set it at all in c groups in V1 and V2 that will change and I'll talk about that in",
    "start": "969660",
    "end": "975240"
  },
  {
    "text": "a second and for memory limit we kind of have two knobs memory.max and C group V2",
    "start": "975240",
    "end": "980459"
  },
  {
    "text": "and memory Max limited bytes and V1 and it's very simple you set that and it comes from your pod spec from your",
    "start": "980459",
    "end": "985800"
  },
  {
    "text": "container right if you go over that limit you get um killed really simple and so the recommendation that we have",
    "start": "985800",
    "end": "990899"
  },
  {
    "text": "generally set your memory requests equal to your limit the reason for that is uh with CPU you can kind of over commit on",
    "start": "990899",
    "end": "996779"
  },
  {
    "text": "it right it's a compressible resource but with memory it's not compressible right you can't over commit on memory so",
    "start": "996779",
    "end": "1002060"
  },
  {
    "text": "the recommendations to always set memory request equal limit that way you don't impact other pods on the system and you",
    "start": "1002060",
    "end": "1007820"
  },
  {
    "text": "might be using more resources than you can request so the other item that I kind of touched",
    "start": "1007820",
    "end": "1013579"
  },
  {
    "text": "on earlier and I want to explain a little bit more is about C group drivers so this is kind of a little bit of a misunderstood concept so I kind of want",
    "start": "1013579",
    "end": "1018680"
  },
  {
    "text": "to talk about a little bit so as I mentioned earlier there's two components that interact with c groups on the system the kubelet and the container",
    "start": "1018680",
    "end": "1024260"
  },
  {
    "text": "runtime and the kublet right owns the Pod C group and the container runtime owns the container subgroups and when",
    "start": "1024260",
    "end": "1029480"
  },
  {
    "text": "you interface with the c group subsystem there's actually two kind of apis that you can interface with it one is the C group FS where you're just talking",
    "start": "1029480",
    "end": "1035720"
  },
  {
    "text": "directly to the kernel and setting values and the second option is something called the systemd driver and the system D driver basically means that",
    "start": "1035720",
    "end": "1041780"
  },
  {
    "text": "instead of talking directly to the secret file system you're first talking to system D and system D has this",
    "start": "1041780",
    "end": "1046880"
  },
  {
    "text": "concept of slices and Scopes which are kind of abstractions for c groups and then system D will actually go ahead and",
    "start": "1046880",
    "end": "1052520"
  },
  {
    "text": "set the values in the in the C group at fast kernel and so with the secret V2",
    "start": "1052520",
    "end": "1057980"
  },
  {
    "text": "one of the requirements of C group B2 is that we only have kind of one process that manages c groups at any given level",
    "start": "1057980",
    "end": "1063440"
  },
  {
    "text": "and since systemd kind of owns that responsibility it's kind of the default baked in every distro we'd really strongly recommend that you use the",
    "start": "1063440",
    "end": "1069860"
  },
  {
    "text": "systemdc group driver on both the kublet and the container runtime when you're using secret V2 and this is something",
    "start": "1069860",
    "end": "1074900"
  },
  {
    "text": "that you need to configure in kublet and the container runtime they have to match and we really do recommend using that systemd c group driver as I mentioned",
    "start": "1074900",
    "end": "1082940"
  },
  {
    "text": "so the other item that we want to kind of talk about is monitoring so c groups provide us Resource Management right resource throttling but also they",
    "start": "1082940",
    "end": "1089780"
  },
  {
    "text": "provide us the ability to export metrics and so the way this works is that there's a project called the C advisor",
    "start": "1089780",
    "end": "1096020"
  },
  {
    "text": "I'm the maintainer of it actually and C advisor is responsible for actually scraping those metrics in C group fs and",
    "start": "1096020",
    "end": "1102740"
  },
  {
    "text": "getting them to kubl it and then other systems will kind of actually get that information and Export it out you know to Prometheus and and that's how you can",
    "start": "1102740",
    "end": "1109400"
  },
  {
    "text": "see them in all your kind of grafana and other dashboards Etc right and so the way it actually works is Kubla depends",
    "start": "1109400",
    "end": "1115100"
  },
  {
    "text": "on C advisor as a library it links it in and uh C advisor we had to update it we had to do some changes to ensure it",
    "start": "1115100",
    "end": "1121460"
  },
  {
    "text": "works with secret V2 that was done in v043 version and it's included in kind of the latest you know latest kublets",
    "start": "1121460",
    "end": "1127460"
  },
  {
    "text": "there's also some other work I wanted to mention here where we're actually moving a lot of this metric collection away from C advisor and into the Container",
    "start": "1127460",
    "end": "1133580"
  },
  {
    "text": "runtime that'll ensure that we kind of can make it uh uniform across different kind of container runtimes and not",
    "start": "1133580",
    "end": "1139100"
  },
  {
    "text": "depend on sea advisor to get stats so that work is ongoing so the other big effort is as part of",
    "start": "1139100",
    "end": "1144980"
  },
  {
    "text": "graduating cigaroo V2 to GA one of the big things that we worked on is actually testing it and making sure it works well",
    "start": "1144980",
    "end": "1150200"
  },
  {
    "text": "and so Sig node has a whole bunch of kind of tests that we run against couplet and against the different ecosystem to make sure it works well so",
    "start": "1150200",
    "end": "1156380"
  },
  {
    "text": "as part of this we wanted to ensure that all the features everything works well with secret V2 so we actually added new",
    "start": "1156380",
    "end": "1161419"
  },
  {
    "text": "test jobs in in open source here to basically test all the variants of different tests on C group V2 and you",
    "start": "1161419",
    "end": "1167720"
  },
  {
    "text": "can see those highlighted here so there's conformance serial node test cluster tests all types of different tests that we ran and we're running all",
    "start": "1167720",
    "end": "1173539"
  },
  {
    "text": "these jobs continuously we're running them actually against the latest container d as well so we're getting coverage of both container D run C and",
    "start": "1173539",
    "end": "1179840"
  },
  {
    "text": "the latest kind of kublin we also worked on working with the community in general to gather feedback to understand that",
    "start": "1179840",
    "end": "1185059"
  },
  {
    "text": "the different container runtimes are working well and making sure that you know C group V2 was going to be adopted in the broader Community as part of this",
    "start": "1185059",
    "end": "1190340"
  },
  {
    "text": "effort so a couple things about as a couple things to be aware of as you start to",
    "start": "1190340",
    "end": "1195500"
  },
  {
    "text": "migrate with C group E2 in a kubernetes one of the things that you should do is probably just use one of the latest Linux distros that enables secret V2",
    "start": "1195500",
    "end": "1201620"
  },
  {
    "text": "edit by default and I'll have that slide earlier had basically every District these days is kind of defaulting to secret V2 you also want to have a",
    "start": "1201620",
    "end": "1208520"
  },
  {
    "text": "requirement that you need the kernel to be 5.8 plus most of those kernels are already an even even newer versions you",
    "start": "1208520",
    "end": "1214520"
  },
  {
    "text": "should use an up-to-date CRI runtime uh the latest CRI runtimes containerd cryo they both supports eurov2 the other big",
    "start": "1214520",
    "end": "1221780"
  },
  {
    "text": "thing I mentioned earlier make sure you're using the systemdc grip driver on both the kublin container runtime that's some configuration you need to set and",
    "start": "1221780",
    "end": "1228500"
  },
  {
    "text": "signode really doesn't support using the C group FS driver that was commonly used on Seagram V1 we don't support it and we",
    "start": "1228500",
    "end": "1234559"
  },
  {
    "text": "don't actually test it so please just don't use it and then also for hosted kubernetes offerings you know if you're",
    "start": "1234559",
    "end": "1240080"
  },
  {
    "text": "using using a hosted kubernetes offering like gke AKs eks one of those should I work with your vendor to understand how",
    "start": "1240080",
    "end": "1245240"
  },
  {
    "text": "they're adopting C gr V2 and you can chat with me about how GK is doing that so the other thing that you should be",
    "start": "1245240",
    "end": "1250700"
  },
  {
    "text": "aware of is you know this is a big change right and so you should test your apps and make sure they work well with secret V2",
    "start": "1250700",
    "end": "1256400"
  },
  {
    "text": "from kind of the work that we did most applications they don't really have C group dependencies it's quite kind of rare but some applications do and so the",
    "start": "1256400",
    "end": "1263120"
  },
  {
    "text": "most common case is like third-party secured third-party monitoring and security agents those often have to go",
    "start": "1263120",
    "end": "1268220"
  },
  {
    "text": "in and actually scrape the secret file system and do things like collect metrics and other kind of low-level things and those might have a dependency",
    "start": "1268220",
    "end": "1274640"
  },
  {
    "text": "on c groups and because the secret V2 kind of API has changed due to the unified hierarchy and some of the other things they need to be upgraded and so a",
    "start": "1274640",
    "end": "1281360"
  },
  {
    "text": "lot of those vendors already kind of have versions that support signal V2 but you have to make sure that you're using those versions that are supported so",
    "start": "1281360",
    "end": "1286940"
  },
  {
    "text": "work with your vendor to understand what what versions have C group V2 support and make sure you're using those versions the couple other things some of",
    "start": "1286940",
    "end": "1293360"
  },
  {
    "text": "the popular projects like C advisor if you're running it as a standalone demon site you should upgrade to ensure it supports C group V2",
    "start": "1293360",
    "end": "1300200"
  },
  {
    "text": "um the version there is listed there's another project that's kind of popular called Auto Max Crocs by Uber this one",
    "start": "1300200",
    "end": "1306440"
  },
  {
    "text": "kind of automatically if you're using go it sets the max prox variable depending on the C group setting so that one is",
    "start": "1306440",
    "end": "1312020"
  },
  {
    "text": "also upgraded to support secret V2 in that version the other thing to be aware of is some kind of language runtimes",
    "start": "1312020",
    "end": "1317120"
  },
  {
    "text": "also depend on c groups so Java actually uses the jdk and when the jdk starts up it actually looks at the secret file",
    "start": "1317120",
    "end": "1323360"
  },
  {
    "text": "system to understand how much CPU is available how much memory is available and so it uses c groups for that and so",
    "start": "1323360",
    "end": "1329380"
  },
  {
    "text": "if you're using Java you should make sure to upgrade to 11016 and jdk 15 plus they back ported the C group V2 support",
    "start": "1329380",
    "end": "1335659"
  },
  {
    "text": "and so using those versions will ensure that job applications will work well too",
    "start": "1335659",
    "end": "1341120"
  },
  {
    "text": "so that's kind of the idea behind C groovy too um hopefully once you adopt it you'll kind of get a lot of those new",
    "start": "1341120",
    "end": "1346880"
  },
  {
    "text": "improvements and kind of the lower level accounting and Resource Management that we mentioned earlier and hopefully our applications will work fine that's kind",
    "start": "1346880",
    "end": "1352400"
  },
  {
    "text": "of the goal right you won't see too many big changes but the really cool thing about secret V2 is some of the opportunities that it'll provide on top",
    "start": "1352400",
    "end": "1358640"
  },
  {
    "text": "and we have many opportunities to have improved resource management in general using secret V2 and I want to talk about",
    "start": "1358640",
    "end": "1363860"
  },
  {
    "text": "that a little bit so one of the opportunities that we have is to improve kind of how we manage memory and so this",
    "start": "1363860",
    "end": "1369380"
  },
  {
    "text": "is actually a feature that's already Alpha and kubernetes like it relies on secret V2 it's called memory qos",
    "start": "1369380",
    "end": "1374840"
  },
  {
    "text": "memory quality of service so going back with secret V1 the problem is with the kernel we really only had",
    "start": "1374840",
    "end": "1380419"
  },
  {
    "text": "one knob for memory and that was memory limit right you hit the memory limit and then you're oomed and that's the end of",
    "start": "1380419",
    "end": "1385760"
  },
  {
    "text": "the story but with secret V2 we have much more we have much more kind of control over memory we have four knobs",
    "start": "1385760",
    "end": "1391340"
  },
  {
    "text": "actually Min low high and Max and these are the soft memory limits so in the",
    "start": "1391340",
    "end": "1396380"
  },
  {
    "text": "bottom right here we have a little diagram to explain how it works but basically memory.min is kind of the guarantee of the kernel please never",
    "start": "1396380",
    "end": "1401600"
  },
  {
    "text": "reclaim this amount of memory I this is the minimum amount of memory I need memory.low is kind of best effort if there's significant memory pressure the",
    "start": "1401600",
    "end": "1407600"
  },
  {
    "text": "curl will try to reclaim it but it usually we'll try not to memory.high is kind of the limit but it's not the hard",
    "start": "1407600",
    "end": "1414679"
  },
  {
    "text": "limit so as soon as you hit that your application will be throttled it'll start to reclaim memory but you won't be",
    "start": "1414679",
    "end": "1420140"
  },
  {
    "text": "um killed and then memory.max is just like the limit we were talking about earlier which if you go over that limit then you get um killed and so the idea",
    "start": "1420140",
    "end": "1426620"
  },
  {
    "text": "here is actually we are already setting in your pod spec right everyone's setting a memory request but we're not",
    "start": "1426620",
    "end": "1432260"
  },
  {
    "text": "actually using it at all for c groups at all right we're not actually using this number so the idea here is let's map the",
    "start": "1432260",
    "end": "1437360"
  },
  {
    "text": "memory request to memory.low and this will ensure that you have some amount of minimum memory uh for your application",
    "start": "1437360",
    "end": "1443120"
  },
  {
    "text": "the other idea is you're setting a memory limit and we kind of want to get the guarantee that as you approach your",
    "start": "1443120",
    "end": "1448220"
  },
  {
    "text": "memory limit you'll start to get throttled and not unkilled and set something to memory.high so the way we did that is we take your memory limit",
    "start": "1448220",
    "end": "1454520"
  },
  {
    "text": "and we multiply it by a throttling Factor uh the defaults can have 0.8 and then we set that to memory.high and the",
    "start": "1454520",
    "end": "1460220"
  },
  {
    "text": "idea is that as you approach your memory limit you'll start to get throttled because you'll hit memory.high and then",
    "start": "1460220",
    "end": "1465380"
  },
  {
    "text": "you'll you'll kind of the kernel will try to reclaim memory and if you continue to get to increase memory usage then you'll hit memory on Max and you'll",
    "start": "1465380",
    "end": "1471320"
  },
  {
    "text": "get killed so the result here is hopefully you'll get less frequent um and kind of better performance as you approach the memory limit",
    "start": "1471320",
    "end": "1477679"
  },
  {
    "text": "some of the other work we wanted to mention is a PSI pressure metrics so this stuff is kind of coming down the",
    "start": "1477679",
    "end": "1483020"
  },
  {
    "text": "pipe and we want to integrate Kubler with it this will allow us to kind of understand what resource shortages we have and improve the eviction so we can",
    "start": "1483020",
    "end": "1489799"
  },
  {
    "text": "detect things like research shortages for CPU memory and I O and this will improve node stability",
    "start": "1489799",
    "end": "1495740"
  },
  {
    "text": "the other thing we want to talk about is disk throttling so so the kublet has",
    "start": "1495740",
    "end": "1501260"
  },
  {
    "text": "really good resource control for CPU and memory and ephemeral storage but disk has really been a resource that we haven't accounted for this guyo",
    "start": "1501260",
    "end": "1507020"
  },
  {
    "text": "specifically so secret V2 has a new i o controller that helps manage i o and we want ability to limit i o of PODS so we",
    "start": "1507020",
    "end": "1513440"
  },
  {
    "text": "can ensure that pods also get kind of some amount of i o guarantees and can impact the node",
    "start": "1513440",
    "end": "1518659"
  },
  {
    "text": "the last thing we want to talk about is um D so systemd has this kind of New Concept called UMD which is a user space",
    "start": "1518659",
    "end": "1524779"
  },
  {
    "text": "oom killer it uses PSI metrics for this so the way it works right now is the kublet sets a um score and then the",
    "start": "1524779",
    "end": "1531980"
  },
  {
    "text": "kernel actually does the oom killing but the kernel has really little visibility into the pods that are running has no idea about kubernetes pod priority you",
    "start": "1531980",
    "end": "1539240"
  },
  {
    "text": "know what pods are anything like that right but if we can move the spoon killing into user space the Kublai can",
    "start": "1539240",
    "end": "1544340"
  },
  {
    "text": "make these decisions and kubler's a lot better informed around taking into account things like pod Qs pod priority Etc so we want to take the um killing",
    "start": "1544340",
    "end": "1551480"
  },
  {
    "text": "and move it out of the kernel and put it inside kublet and ensure that Kublai can do that in killing where it has a lot",
    "start": "1551480",
    "end": "1556520"
  },
  {
    "text": "more information it can deal with and PSI metrics will help with that so that's some of the future work that",
    "start": "1556520",
    "end": "1561799"
  },
  {
    "text": "we're planning to do and please join us kind of in sign mode if you're interested in any of these areas so I want to do a quick kind of demo",
    "start": "1561799",
    "end": "1567919"
  },
  {
    "text": "video here around Cedar V2 and some of those kind of Concepts I covered so uh let me kind of make this full screen",
    "start": "1567919",
    "end": "1574580"
  },
  {
    "text": "here so the first thing we're going to do here is to create a cluster with secret V2 I'm using a cluster on gke and",
    "start": "1574580",
    "end": "1581960"
  },
  {
    "text": "GK has a feature called node config that you can specify that you want to Cluster with secret V2 enabled so here we're",
    "start": "1581960",
    "end": "1587299"
  },
  {
    "text": "specifying we want to see group V2 enabled we're going to go ahead and create a cluster this is a 125 cluster on GK with that node config",
    "start": "1587299",
    "end": "1595220"
  },
  {
    "text": "so we can see we have the cluster created here the next step is we're going to have a",
    "start": "1595220",
    "end": "1601640"
  },
  {
    "text": "little workload uh well first we're going to examine the nodes that we have so I just created a one node cluster uh",
    "start": "1601640",
    "end": "1607159"
  },
  {
    "text": "this is using the latest kind of 125 build of kubernetes it's using the container optimized OS which we use on",
    "start": "1607159",
    "end": "1612980"
  },
  {
    "text": "on gke it's running container D and it's on the 515 kernel so this is kind of the",
    "start": "1612980",
    "end": "1618260"
  },
  {
    "text": "latest cost version here so the next step is we're going to deploy a workload so I just have a very",
    "start": "1618260",
    "end": "1624200"
  },
  {
    "text": "simple kind of busy box workload it doesn't do anything it just does a sleep um and the important things I'm",
    "start": "1624200",
    "end": "1629240"
  },
  {
    "text": "specifying the requests and limits uh for CPU and memory and I'm the the limits are higher than the request so",
    "start": "1629240",
    "end": "1635360"
  },
  {
    "text": "this is going to be a bursible pod so I just take this pod and I'm going to deploy here on my cluster so just your",
    "start": "1635360",
    "end": "1642020"
  },
  {
    "text": "standard standard Coupe cuddle apply cool",
    "start": "1642020",
    "end": "1648640"
  },
  {
    "text": "and then I'm going to do get pods and see it's running so cool the pod's running so the next step is I'm going to",
    "start": "1650360",
    "end": "1655580"
  },
  {
    "text": "SSH into the node to kind of examine what's going on and and what's going on in the actual node so I'm just going to SSH into the node here all right so",
    "start": "1655580",
    "end": "1661700"
  },
  {
    "text": "we're on the Node so the first thing I want to do is I ran this command called stat and you can pass in the C group fs",
    "start": "1661700",
    "end": "1667159"
  },
  {
    "text": "and you can see here that we get back C group 2fs this is the way to check that the node is actually using cdv2",
    "start": "1667159",
    "end": "1673279"
  },
  {
    "text": "so sticker V2 is being used on this node as we specified the next thing I want to do is kind of show how this kublet pod",
    "start": "1673279",
    "end": "1679760"
  },
  {
    "text": "secret hierarchy is set up so I'm going around kind of the tree command on the kubelet a coupon slice",
    "start": "1679760",
    "end": "1685880"
  },
  {
    "text": "and you can see kind of how kublet's managing the different c groups so at the top level we have the Pod slice and",
    "start": "1685880",
    "end": "1691640"
  },
  {
    "text": "so the way it works right is we're using the system DC Group driver so systemd has slices and then uh system D is",
    "start": "1691640",
    "end": "1697940"
  },
  {
    "text": "creating the c groups underneath here and so we have at the top level of best effort slice versatile slice and guaranteed slice these are the different",
    "start": "1697940",
    "end": "1703880"
  },
  {
    "text": "Qs classes and within each kind of Qs class we have a slice for the Pod C group right each of these are the Pod c",
    "start": "1703880",
    "end": "1710419"
  },
  {
    "text": "groups that we're seeing here so that's the idea here we're going to go",
    "start": "1710419",
    "end": "1715580"
  },
  {
    "text": "one level deeper uh just to see what's inside actually the fod c groups and here uh within each policy group we have",
    "start": "1715580",
    "end": "1721520"
  },
  {
    "text": "a Dot scope unit this is a secret this is the C group that's created by the container runtime for the actual container so each container gets its C",
    "start": "1721520",
    "end": "1728600"
  },
  {
    "text": "group created underneath that pod uh level C group and that's what we see here so each it might you might have you know",
    "start": "1728600",
    "end": "1735020"
  },
  {
    "text": "one C group basically for each container and so because we're using the system DC Group driver we can actually ask systemd",
    "start": "1735020",
    "end": "1740480"
  },
  {
    "text": "about this type of stuff as well so if we ask systemd hey give us all the slices that exist system D slice is",
    "start": "1740480",
    "end": "1745880"
  },
  {
    "text": "basically analogous to a c group uh then we can see here who uh system D is telling us okay here's all the slices",
    "start": "1745880",
    "end": "1751580"
  },
  {
    "text": "on the system these are all the Pod c groups that created that Kubla created so we can see all those and we can also",
    "start": "1751580",
    "end": "1757279"
  },
  {
    "text": "ask systemd hey give us all the scope units scope units will be created by the container runtime for each container",
    "start": "1757279",
    "end": "1763460"
  },
  {
    "text": "um and then you can see them here so if we do list units type scope you can see here these are I'm using containerd here",
    "start": "1763460",
    "end": "1769100"
  },
  {
    "text": "so we have a scope units for every single container the next thing is you know we deploy that BusyBox sleep workload earlier so I",
    "start": "1769100",
    "end": "1775820"
  },
  {
    "text": "just wanted to kind of see how the C group settings are set up so first thing I'm going to kind of grab just run PS",
    "start": "1775820",
    "end": "1780860"
  },
  {
    "text": "and get the PID that the Sleep command is running under so you know here's the the PID and then I'm going to use Prock",
    "start": "1780860",
    "end": "1786020"
  },
  {
    "text": "Fest to actually see hey what a c group is this processing so here's the full path and it ends with that scope that's",
    "start": "1786020",
    "end": "1792380"
  },
  {
    "text": "the container C group right so I'm going to save this into a environment variable just called the container C group just",
    "start": "1792380",
    "end": "1798620"
  },
  {
    "text": "so I can kind of play with it and it's a long path so anyways I have it here and now we're going to see how the actual",
    "start": "1798620",
    "end": "1803980"
  },
  {
    "text": "resource settings are set so we're going to take a look look at CPU so the first thing is CPU weight this is your CPU",
    "start": "1803980",
    "end": "1811520"
  },
  {
    "text": "request and you can see they're set here so these will be converted from the CPU request you send your pod spec and then",
    "start": "1811520",
    "end": "1818179"
  },
  {
    "text": "also there's a CPU Max right and that's the CFS quota and period That's set for by the CPU limit so all those settings",
    "start": "1818179",
    "end": "1824000"
  },
  {
    "text": "are being set by the container runtime here and then um that's CPU and so for memory",
    "start": "1824000",
    "end": "1829520"
  },
  {
    "text": "here I also want to condense for how this is set up and so I also enabled that memory Qs feature I mentioned earlier that sets soft memory limits so",
    "start": "1829520",
    "end": "1836120"
  },
  {
    "text": "we have memory.max set that's set to the hard memory limit on the container right that's just like before but we also have",
    "start": "1836120",
    "end": "1842360"
  },
  {
    "text": "the soft memory limit set now right we have memory.highset this is computed earlier right as the throttling Factor",
    "start": "1842360",
    "end": "1847820"
  },
  {
    "text": "times the memory Max this is as you approach the memory limit and then we also have memory.min set this is coming",
    "start": "1847820",
    "end": "1854120"
  },
  {
    "text": "from your CPU from your memory request that you have so we're actually setting a soft memory limit here as well",
    "start": "1854120",
    "end": "1860779"
  },
  {
    "text": "so that's kind of the idea here just to kind of give you an idea of how c groups are actually working on the real node",
    "start": "1860779",
    "end": "1867580"
  },
  {
    "text": "cool all right",
    "start": "1867620",
    "end": "1873559"
  },
  {
    "text": "so that's kind of our presentation I want to give a big thank you to everyone who worked on this this is looking a big effort in Sig node so big shout out and",
    "start": "1873559",
    "end": "1879860"
  },
  {
    "text": "thank you to everyone in signode who helped help work on this I want to thank the container runtime Community container runtimes are super critical",
    "start": "1879860",
    "end": "1885620"
  },
  {
    "text": "here and shout out to Giuseppe who worked on this early on it's really kind of pioneered a lot of the early Siegler",
    "start": "1885620",
    "end": "1890720"
  },
  {
    "text": "V2 work across the container runtime space the container D maintainers the cryo maintainers the movie Docker helped",
    "start": "1890720",
    "end": "1896840"
  },
  {
    "text": "a lot here to kind of start Seeger V2 support systemd is kind of a critical element of secret V2 as well so thanks",
    "start": "1896840",
    "end": "1902779"
  },
  {
    "text": "to the systemd maintainers for adding C group E2 Sport and kind of continuously iterating on it and of course it",
    "start": "1902779",
    "end": "1907880"
  },
  {
    "text": "wouldn't be possible without the Linux kernel adding secret V2 and all the work that went into secret V2 in general",
    "start": "1907880",
    "end": "1913760"
  },
  {
    "text": "um we have a couple resources here we Jade secret V2 in 125 so there's a blog post there uh that you can read to get",
    "start": "1913760",
    "end": "1919880"
  },
  {
    "text": "more information there's some kubernetes uh docs that you can read about c groups and some of the details of C group",
    "start": "1919880",
    "end": "1925640"
  },
  {
    "text": "drivers and so forth and if you're more interested uh there's internal docs that are a great resource as well as a couple",
    "start": "1925640",
    "end": "1930679"
  },
  {
    "text": "other kubecon talks there's another kubecon talk earlier this week about secret V2 and then there's another talk",
    "start": "1930679",
    "end": "1936140"
  },
  {
    "text": "from 2020 by Giuseppe that also goes into more details about secret V2 that's a good resource so with that thank you",
    "start": "1936140",
    "end": "1941600"
  },
  {
    "text": "for coming to our presentation and really hope you can start using secret V2 and please let us know any feedback thank you",
    "start": "1941600",
    "end": "1948799"
  },
  {
    "text": "foreign [Applause]",
    "start": "1948799",
    "end": "1955559"
  },
  {
    "text": "with that soft memory limit is there any way for something like a Java garbage",
    "start": "1957100",
    "end": "1962899"
  },
  {
    "text": "collector to react to that or like get a push notification like what what is the",
    "start": "1962899",
    "end": "1969380"
  },
  {
    "text": "action that a programming language can take to help mitigate that so that you don't end up hitting that hard limit",
    "start": "1969380",
    "end": "1975980"
  },
  {
    "text": "so yeah so actually in C group system right there's a file that you can listen",
    "start": "1975980",
    "end": "1981500"
  },
  {
    "text": "on and get events as you're Crossing these thresholds so maybe like the application can open",
    "start": "1981500",
    "end": "1988760"
  },
  {
    "text": "an FD on that file and look for this notification and react dynamically but I'm not aware of any language doing that",
    "start": "1988760",
    "end": "1995120"
  },
  {
    "text": "right now but that's a great thing to explore yeah the language could integrate with that yeah and right now the main thing that you'll get is that",
    "start": "1995120",
    "end": "2001779"
  },
  {
    "text": "kernel will actually reclaim the memory from the jdk or from the application right and then hopefully the GDK is",
    "start": "2001779",
    "end": "2007059"
  },
  {
    "text": "aware of that and will react appropriately yeah so right now it's more a static tuning it looks at the values and decides what the GC values",
    "start": "2007059",
    "end": "2013840"
  },
  {
    "text": "are but what you're talking about is like more dynamic as you're using how do I react when I'm getting these",
    "start": "2013840",
    "end": "2020140"
  },
  {
    "text": "notifications from a kernel that you were throtted because you crossed the low or men or nearing the high",
    "start": "2020140",
    "end": "2028019"
  },
  {
    "text": "you talked about this KO what about Network IO",
    "start": "2028720",
    "end": "2034679"
  },
  {
    "text": "yeah that's also a definitely interesting area I think it's still pretty early but I think we also want to isolate Network i o there's been some",
    "start": "2035380",
    "end": "2041500"
  },
  {
    "text": "work in the community around that and I think that's definitely an area we want to explore as well I think kubernetes is really good for support for CPU and",
    "start": "2041500",
    "end": "2047200"
  },
  {
    "text": "memory right and some of the other resources uh we definitely need to improve on so that's something we want",
    "start": "2047200",
    "end": "2052240"
  },
  {
    "text": "to explore for sure yeah and on the networking side the details have changed a bit so with V2 I think the expectation",
    "start": "2052240",
    "end": "2058658"
  },
  {
    "text": "is you attach an ebpf program and then look for this c groups associated with and then do the throttling so it'll also",
    "start": "2058659",
    "end": "2065138"
  },
  {
    "text": "depends upon your sdn plug-in providers and so on on how to do that yeah and all",
    "start": "2065139",
    "end": "2071020"
  },
  {
    "text": "the all the network TCP socket memory will be actually accounted for under the kind of main memory counter in secret V2",
    "start": "2071020",
    "end": "2077020"
  },
  {
    "text": "so the memory is accounted there but uh for for actually the network you know usage itself that's something we still",
    "start": "2077020",
    "end": "2082658"
  },
  {
    "text": "need to add yeah yeah um so I might have missed this conversation in uh signode but",
    "start": "2082659",
    "end": "2090398"
  },
  {
    "text": "um I know a lot of the stakeholders in signode like Google and red hat use systemd like heavily",
    "start": "2090399",
    "end": "2097000"
  },
  {
    "text": "um and so you mentioned that system D you know everyone was really pushing for system the C group driver what is this",
    "start": "2097000",
    "end": "2103540"
  },
  {
    "text": "is does there exist a story for distributions that don't ship system d",
    "start": "2103540",
    "end": "2109420"
  },
  {
    "text": "but I think for those distributions right like we see that majority of the",
    "start": "2109420",
    "end": "2115420"
  },
  {
    "text": "distributions are using system D we don't see as much usage that's why we concentrated on like keeping things",
    "start": "2115420",
    "end": "2120940"
  },
  {
    "text": "simple but if folks are really interested in using C group refers like we really encourage them to show up to",
    "start": "2120940",
    "end": "2127420"
  },
  {
    "text": "Signal like raise their hands and help get that support like fully baked and working yeah",
    "start": "2127420",
    "end": "2134380"
  },
  {
    "text": "I have a question about the soft memory limits you mentioned before if an application exceeds the soft memory",
    "start": "2134380",
    "end": "2139660"
  },
  {
    "text": "limits then you can throttle it down can you elaborate about what kind of applications you're targeting with that",
    "start": "2139660",
    "end": "2145420"
  },
  {
    "text": "I mean if you're not scheduling it then it's not going to release the memory so in what situation that is is that best",
    "start": "2145420",
    "end": "2151119"
  },
  {
    "text": "avoided or best used so what one idea can do is kind of if you're not using some of my memories",
    "start": "2151119",
    "end": "2157660"
  },
  {
    "text": "flush into disk right and so that also interacts with swap which is also an ongoing effort so it tries to swap memory if it can some memory can be",
    "start": "2157660",
    "end": "2164079"
  },
  {
    "text": "reclaimed by the kernel right because it's shared and it's kind of caching it so those are the kind of things that it tries to do",
    "start": "2164079",
    "end": "2171099"
  },
  {
    "text": "yeah yeah the executable file Pages can be swapped even though swap is not enabled but there's also an effort to",
    "start": "2171099",
    "end": "2178359"
  },
  {
    "text": "make swap available and then it will work better and also like for um the",
    "start": "2178359",
    "end": "2183700"
  },
  {
    "text": "integration we'll need swap so umdi has the time to react to these changes and actually make decisions",
    "start": "2183700",
    "end": "2189280"
  },
  {
    "text": "otherwise if if things are going too fast without swap then you'll end up getting home killed by the current Loom",
    "start": "2189280",
    "end": "2194800"
  },
  {
    "text": "killer uh so incredibly exciting",
    "start": "2194800",
    "end": "2201040"
  },
  {
    "text": "um over here uh I I I noted that uh cos m97 should have C group 2 by default we",
    "start": "2201040",
    "end": "2208720"
  },
  {
    "text": "notice in some situations where you're running say uh two pods in a COS environment uh memory pressure against",
    "start": "2208720",
    "end": "2213940"
  },
  {
    "text": "one can suffocate another uh I'm guessing this uh this being enabled",
    "start": "2213940",
    "end": "2219160"
  },
  {
    "text": "might help in a situation yeah so I mean if you're setting a uh it",
    "start": "2219160",
    "end": "2224440"
  },
  {
    "text": "kind of also depends on the Qwest class that you're using right so if it's a burstable pod uh the there is no there",
    "start": "2224440",
    "end": "2229960"
  },
  {
    "text": "might not be necessarily memory limit set right or it might be at the kind of kubelet top level C group that's set so",
    "start": "2229960",
    "end": "2236740"
  },
  {
    "text": "depending on that that's one memory pressure can impact two different pods right but if you do set kind of",
    "start": "2236740",
    "end": "2242380"
  },
  {
    "text": "guaranteed pod or you set a memory limit then you should kind of get full isolation between the the memory usage between two pods right and of course",
    "start": "2242380",
    "end": "2248740"
  },
  {
    "text": "memory the Qs feature will help kind of with soft memory limits as well with that",
    "start": "2248740",
    "end": "2254160"
  },
  {
    "text": "uh yeah question here so I remember that um if you have a single threaded app",
    "start": "2254920",
    "end": "2260980"
  },
  {
    "text": "like python where it can only take advantage of one CPU anyways with c",
    "start": "2260980",
    "end": "2266140"
  },
  {
    "text": "groups V1 if uh if I don't have a limit the nature of",
    "start": "2266140",
    "end": "2271900"
  },
  {
    "text": "the application automatically limits it to one CPU but if I add a limit of uh",
    "start": "2271900",
    "end": "2277359"
  },
  {
    "text": "Hey limit yourself to one CPU just the act of adding a limit period can lower",
    "start": "2277359",
    "end": "2282880"
  },
  {
    "text": "performance and I was wondering if like C group V2 helps that scenario",
    "start": "2282880",
    "end": "2289559"
  },
  {
    "text": "so that's kind of dependent on on the you know on the on the python interpreter to kind of do that so if it",
    "start": "2290140",
    "end": "2296380"
  },
  {
    "text": "always it kind of depends if it's hitting that CPU limit in the first place so I don't know if python actually can configure and tell it kind of uh you",
    "start": "2296380",
    "end": "2302440"
  },
  {
    "text": "know like jdk and and with go Max project where you can set kind of a CPU that you have that you can allocate",
    "start": "2302440",
    "end": "2308500"
  },
  {
    "text": "underneath for you um but basically you know if you set a CPU limit of one and it actually always will use less than one then you",
    "start": "2308500",
    "end": "2314200"
  },
  {
    "text": "shouldn't get throttle in the first place right but if you're setting a lower CPU limit and it's always using more CPU than that then you'll get",
    "start": "2314200",
    "end": "2319480"
  },
  {
    "text": "throttled so Security will not necessarily help with that unless uh the The Interpreter will actually integrate",
    "start": "2319480",
    "end": "2325060"
  },
  {
    "text": "with that and read those values and tune itself appropriately and that's not really secretive yeah that's like secret B1 secret V2 it's it's a kind of General",
    "start": "2325060",
    "end": "2331960"
  },
  {
    "text": "C group setting here I appreciate for your sharing so in your",
    "start": "2331960",
    "end": "2340000"
  },
  {
    "text": "slider you mentioned there is a configuration there's a fat fatal",
    "start": "2340000",
    "end": "2346359"
  },
  {
    "text": "conversion convert memory Mass to memory highlight so but that one is a global",
    "start": "2346359",
    "end": "2354040"
  },
  {
    "text": "conversion so do you consider may it support purpose configuration means okay if I",
    "start": "2354040",
    "end": "2362859"
  },
  {
    "text": "don't set this for for my port in my postback I will be forbid to use the",
    "start": "2362859",
    "end": "2369099"
  },
  {
    "text": "global configuration but if I already say in my Prospect I would prefer this",
    "start": "2369099",
    "end": "2374140"
  },
  {
    "text": "very yeah why I asked this solution is actually it costs out an incident happening in our production when we load",
    "start": "2374140",
    "end": "2381099"
  },
  {
    "text": "our SQL V2 because some application actually for the forecast is is okay but some",
    "start": "2381099",
    "end": "2389320"
  },
  {
    "text": "some education they won't don't want to be throttle yeah because they may very latent system",
    "start": "2389320",
    "end": "2395619"
  },
  {
    "text": "sensitive and when we lower actually those applications uh latency becomes",
    "start": "2395619",
    "end": "2401859"
  },
  {
    "text": "very hard and turns out we figure out actually we take at least one hour to",
    "start": "2401859",
    "end": "2407560"
  },
  {
    "text": "figure out this is caused by our SQL which because at the beginning uh we",
    "start": "2407560",
    "end": "2413020"
  },
  {
    "text": "didn't see this issue when we log out of school we took to at least uh one or two",
    "start": "2413020",
    "end": "2419260"
  },
  {
    "text": "classes yeah so yeah that's the reason why I asked this question so I really mostly focused on memory kind of then",
    "start": "2419260",
    "end": "2425200"
  },
  {
    "text": "like if the Pod is not setting memory requests and limits that's kind of your question or more on the CPU side that's",
    "start": "2425200",
    "end": "2430960"
  },
  {
    "text": "I didn't fully understand both okay so with the memory so I'll talk about memory so for memory uh with",
    "start": "2430960",
    "end": "2437140"
  },
  {
    "text": "the memory Qs feature the one thing I didn't mention actually it's not just setting the memory uh Min and uh you",
    "start": "2437140",
    "end": "2442420"
  },
  {
    "text": "know memory High settings on the Pod level it actually also sets it at the higher level at the qos level as well",
    "start": "2442420",
    "end": "2449079"
  },
  {
    "text": "um so it'll also kind of look at node allocatable and set memory settings there so that'll ensure that even if",
    "start": "2449079",
    "end": "2454480"
  },
  {
    "text": "you're not setting any kind of memory uh requests or limits if you approach node allocatable you'll also kind of getting",
    "start": "2454480",
    "end": "2459820"
  },
  {
    "text": "that behavior at the top top level C group for CPU I don't think we set it at any kind of top level we set CPU shares",
    "start": "2459820",
    "end": "2466480"
  },
  {
    "text": "based on node allocatable at the top level so that will ensure that you know we if there's some amount of CPU",
    "start": "2466480",
    "end": "2473740"
  },
  {
    "text": "available in the system like all the pods share those CPU shares right at the top level um but we don't set a new CPU limit",
    "start": "2473740",
    "end": "2479560"
  },
  {
    "text": "settings at the top level does that help answer your question at all or maybe I didn't fully get",
    "start": "2479560",
    "end": "2487660"
  },
  {
    "text": "so I have a question that's a bit related uh the you mentioned uh 0.8 uh",
    "start": "2487660",
    "end": "2494560"
  },
  {
    "text": "throttling factor is that configurable and like the Pod spec are at the node level or is it like hard-coded um how",
    "start": "2494560",
    "end": "2501280"
  },
  {
    "text": "would you go about configuring that so I don't believe that's currently configurable the feature's still in Alpha actually so this is actually where",
    "start": "2501280",
    "end": "2507520"
  },
  {
    "text": "it'd be awesome to get your feedback and try it out and see if that works for you I think that was kind of an estimation that we kind of said works for most",
    "start": "2507520",
    "end": "2512859"
  },
  {
    "text": "folks because we didn't want to ask people hey also set us a memory high on the Pod spec it's kind of like additional info we don't think is super",
    "start": "2512859",
    "end": "2519220"
  },
  {
    "text": "useful but if that is useful to be configurable that would be great feedback",
    "start": "2519220",
    "end": "2524280"
  },
  {
    "text": "is it possible to set which course on the CPU are being used",
    "start": "2528040",
    "end": "2534359"
  },
  {
    "text": "are you asking uh we can request which course can we well we have a use case",
    "start": "2534520",
    "end": "2540760"
  },
  {
    "text": "where we want different pneuma nodes to be used we don't want workloads to be",
    "start": "2540760",
    "end": "2546400"
  },
  {
    "text": "scheduled on the same pneuma node and so we're specifying cores one three five to",
    "start": "2546400",
    "end": "2552040"
  },
  {
    "text": "be used for certain pods is that possible to be set with C group B2 via a",
    "start": "2552040",
    "end": "2557619"
  },
  {
    "text": "pod spec so like c groups V2 also has CPU sets similar to V1 yeah and that should work",
    "start": "2557619",
    "end": "2565480"
  },
  {
    "text": "similarly can you set it through a pod spec no you can't specify which CPUs you",
    "start": "2565480",
    "end": "2570820"
  },
  {
    "text": "want to use in the Pod spec you can only specify the number of CPUs and then CPU",
    "start": "2570820",
    "end": "2576040"
  },
  {
    "text": "manager or something else will go and container work with the container runtime to actually pick the specific",
    "start": "2576040",
    "end": "2581800"
  },
  {
    "text": "CPUs that your pod ends up using okay",
    "start": "2581800",
    "end": "2586680"
  },
  {
    "text": "remember thank you for this I'm curious about the",
    "start": "2592119",
    "end": "2598119"
  },
  {
    "text": "uh UMD killer in user space uh would it be is it possible to I would love it if",
    "start": "2598119",
    "end": "2606160"
  },
  {
    "text": "um the monitoring has a lower priority for being um killed like the monitoring",
    "start": "2606160",
    "end": "2611260"
  },
  {
    "text": "namespace so my user uh my other workloads",
    "start": "2611260",
    "end": "2616300"
  },
  {
    "text": "would have a higher probability of being um killed when my monitoring is kept up all the time do you know if that's uh",
    "start": "2616300",
    "end": "2622420"
  },
  {
    "text": "something you're considering all right I I think that's that's what we are hoping right like when we used to an umdi like",
    "start": "2622420",
    "end": "2628900"
  },
  {
    "text": "model we can actually look at the priorities set for a pod or other signals like the qos class and make",
    "start": "2628900",
    "end": "2635200"
  },
  {
    "text": "smarter decisions and the kernel Loom killer takes today so that's our goal and like we still have to do all that",
    "start": "2635200",
    "end": "2642099"
  },
  {
    "text": "work so you're happy to like join signode and you know uh give input on your use cases",
    "start": "2642099",
    "end": "2649380"
  },
  {
    "text": "what all right folks we have way over time so maybe we can take one last",
    "start": "2649720",
    "end": "2655839"
  },
  {
    "text": "we have time",
    "start": "2655839",
    "end": "2658740"
  },
  {
    "text": "um yeah I've noticed that um the burstability of CPU seems to have",
    "start": "2660940",
    "end": "2667119"
  },
  {
    "text": "an effect on the um kill score so if I had my request and limit for my memory",
    "start": "2667119",
    "end": "2672400"
  },
  {
    "text": "set equal I would kind of expect the quality of service for the memory aspect and the um kill score to reflect that of",
    "start": "2672400",
    "end": "2679720"
  },
  {
    "text": "guaranteed do you know if that's being addressed at all in V2 and do you know",
    "start": "2679720",
    "end": "2686079"
  },
  {
    "text": "why that it behaves like that today yeah so uh you're completely correct so",
    "start": "2686079",
    "end": "2692260"
  },
  {
    "text": "when you have a burstable pod we actually look at kind of the ratio between the memory requests and memory limits and then that's how we compute",
    "start": "2692260",
    "end": "2697839"
  },
  {
    "text": "the um score that will be set on on the Pod and then in the containers",
    "start": "2697839",
    "end": "2703380"
  },
  {
    "text": "so I mean it looks at the memory not not at this right so it looks like the memory can request and limits ratio because the idea is we want to have a",
    "start": "2707260",
    "end": "2712720"
  },
  {
    "text": "different oom score for burstable pods compared to guaranteed pods",
    "start": "2712720",
    "end": "2718020"
  },
  {
    "text": "if I don't set the CPU limit equal to the request then I get an unexpected um",
    "start": "2722079",
    "end": "2728200"
  },
  {
    "text": "okay score so let's talk about that that's okay three possible bugs so you might need to investigate it yeah",
    "start": "2728200",
    "end": "2735240"
  },
  {
    "text": "cool that's it uh thank you so much for the question please come up thank you",
    "start": "2738520",
    "end": "2743880"
  }
]