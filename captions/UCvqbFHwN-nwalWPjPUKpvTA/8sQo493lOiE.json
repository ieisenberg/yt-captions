[
  {
    "text": "hi everyone I'm Alita Sharma I lead um",
    "start": "80",
    "end": "5480"
  },
  {
    "text": "observability for AIML at Apple and uh",
    "start": "5480",
    "end": "10599"
  },
  {
    "text": "super happy to be kind of looking at this topic today with all of you uh and",
    "start": "10599",
    "end": "17199"
  },
  {
    "text": "especially you know discussing um how observability is changing uh with the",
    "start": "17199",
    "end": "25000"
  },
  {
    "text": "Advent of applications which are AI enabled and um with that let's get",
    "start": "25000",
    "end": "34680"
  },
  {
    "text": "forward moving okay so a little bit about myself um I'm super excited to be",
    "start": "34680",
    "end": "40920"
  },
  {
    "text": "actually seeing the change in the observability landscape being brought",
    "start": "40920",
    "end": "46120"
  },
  {
    "text": "about by a new generation of smart gen gen applications and gen gen really",
    "start": "46120",
    "end": "52760"
  },
  {
    "text": "means that you know we are starting to add more models along with our code in",
    "start": "52760",
    "end": "59039"
  },
  {
    "text": "writing applications and application services that you know do several",
    "start": "59039",
    "end": "64920"
  },
  {
    "text": "different kinds of things could be search could be um you know just a News",
    "start": "64920",
    "end": "70520"
  },
  {
    "text": "application could be music could be many different areas but um applications are",
    "start": "70520",
    "end": "77320"
  },
  {
    "text": "now no longer just code no longer just infrastructure you know that you're",
    "start": "77320",
    "end": "83479"
  },
  {
    "text": "running on your your application in a global and distributed way but it's also",
    "start": "83479",
    "end": "89960"
  },
  {
    "text": "usage of models in in the way that we actually um do our uh",
    "start": "89960",
    "end": "98560"
  },
  {
    "text": "analysis so as I said um I have been involved in observability for quite a",
    "start": "98560",
    "end": "103880"
  },
  {
    "text": "while and um and at in the cncf I am uh",
    "start": "103880",
    "end": "109159"
  },
  {
    "text": "member of the open Telemetry governance committee have been uh uh project maintainer as well as contributor uh on",
    "start": "109159",
    "end": "117399"
  },
  {
    "text": "different parts metrics uh interrupt between Prometheus and um open Telemetry",
    "start": "117399",
    "end": "126240"
  },
  {
    "text": "protocol um and and super excited to actually see new language profiles",
    "start": "126240",
    "end": "132959"
  },
  {
    "text": "coming in new language signals coming in into the project I'm also a co-chair for",
    "start": "132959",
    "end": "138120"
  },
  {
    "text": "the observability tag and we have a a tag session later today at uh 4 where",
    "start": "138120",
    "end": "145400"
  },
  {
    "text": "we'll be kind of talking over a larger landscape of observability so to do join",
    "start": "145400",
    "end": "151000"
  },
  {
    "text": "if you're available and uh I also chair just just uh started joined in as the",
    "start": "151000",
    "end": "157480"
  },
  {
    "text": "chair for the end user technical Advisory Board of the",
    "start": "157480",
    "end": "162599"
  },
  {
    "text": "cncf uh where this will really serve as",
    "start": "162599",
    "end": "167680"
  },
  {
    "text": "a uh Core group for being able to take end user feedback and user requests and",
    "start": "167680",
    "end": "176000"
  },
  {
    "text": "bring that back to the projects so this is a new body that has been created within the cncf and uh super excited",
    "start": "176000",
    "end": "183879"
  },
  {
    "text": "again as an end user member of the cncf to be participating there and of course",
    "start": "183879",
    "end": "191879"
  },
  {
    "text": "um as part of that responsibility also part of the governing board of the cncf so with that said um just you know",
    "start": "191879",
    "end": "200959"
  },
  {
    "text": "my slides are simple but I do want to talk about some of the key areas that are really driving the change in terms",
    "start": "200959",
    "end": "208280"
  },
  {
    "text": "of observability for smart applications right so there are two ways of looking",
    "start": "208280",
    "end": "214959"
  },
  {
    "text": "at um gen gen in the observability world right there are smart applications that",
    "start": "214959",
    "end": "222720"
  },
  {
    "text": "observability uh or observable Frameworks are looking at and then there are and then there's a second part of",
    "start": "222720",
    "end": "230599"
  },
  {
    "text": "actually using AI for observability right so I'm going to kind of focus in",
    "start": "230599",
    "end": "236799"
  },
  {
    "text": "on the first part that when you're building smart applications with using AI models such",
    "start": "236799",
    "end": "244319"
  },
  {
    "text": "as llms and models can be of different types right just don't all have to be neural Nets and large language models",
    "start": "244319",
    "end": "252480"
  },
  {
    "text": "but today we also have llms in that mix and AIA apps are here to stay so if",
    "start": "252480",
    "end": "258600"
  },
  {
    "text": "you're building large distributed services or applications you will likely",
    "start": "258600",
    "end": "264600"
  },
  {
    "text": "to use an llm in that process today observability is a key part now of that",
    "start": "264600",
    "end": "271479"
  },
  {
    "text": "ecosystem because not only are you looking and observing the behavior of um",
    "start": "271479",
    "end": "278360"
  },
  {
    "text": "your applications your infrastructure but also of your models that you're",
    "start": "278360",
    "end": "283560"
  },
  {
    "text": "using along with your applications right and observability has to actually look at",
    "start": "283560",
    "end": "290840"
  },
  {
    "text": "all those different parts so what I want to talk to you about today is really",
    "start": "290840",
    "end": "296560"
  },
  {
    "text": "highlighting on three aspects of this new paradigm for observability uh which are really the",
    "start": "296560",
    "end": "304320"
  },
  {
    "text": "three pillars if you will at this point and again this is an uh evolving area so",
    "start": "304320",
    "end": "310880"
  },
  {
    "text": "when we chat again 6 months you know there'll be many more details that uh",
    "start": "310880",
    "end": "316720"
  },
  {
    "text": "we'll be kind of talking about so in this new paradigm of",
    "start": "316720",
    "end": "322360"
  },
  {
    "text": "observability um there are three aspects that have become very foundational in",
    "start": "322360",
    "end": "328720"
  },
  {
    "text": "the way that observability needs to support AI models right the first part",
    "start": "328720",
    "end": "335479"
  },
  {
    "text": "is um at a high level looking at integrating with the model training",
    "start": "335479",
    "end": "342400"
  },
  {
    "text": "process right so this is your code for your application your configurations and",
    "start": "342400",
    "end": "348840"
  },
  {
    "text": "then this is your model right so for your models what you're doing is you are",
    "start": "348840",
    "end": "354440"
  },
  {
    "text": "typically having a training process where you have you know data coming in",
    "start": "354440",
    "end": "360240"
  },
  {
    "text": "from Real World use cases that you need to operate with and you typically have",
    "start": "360240",
    "end": "366560"
  },
  {
    "text": "training pipelines where you train your model to width right so this is a new",
    "start": "366560",
    "end": "371759"
  },
  {
    "text": "dimension with which comes with the model training space where you train your models with data in order to be",
    "start": "371759",
    "end": "379199"
  },
  {
    "text": "able to give you the results for your application right the second part is",
    "start": "379199",
    "end": "384960"
  },
  {
    "text": "really understanding the inference pipelines right because once you've TR your model for the applications that you",
    "start": "384960",
    "end": "392960"
  },
  {
    "text": "want to use it for then you actually have inference pipelines which you use",
    "start": "392960",
    "end": "398440"
  },
  {
    "text": "for being able to infer and take parameters and use parameters for",
    "start": "398440",
    "end": "404120"
  },
  {
    "text": "accuracy and observability is applied at each one of these layers and then the",
    "start": "404120",
    "end": "410440"
  },
  {
    "text": "third is really considering performance and resource consumption for the",
    "start": "410440",
    "end": "416599"
  },
  {
    "text": "infrastructure that you're running these models on as well as your applications",
    "start": "416599",
    "end": "422440"
  },
  {
    "text": "on right because at the end of the day there is new hardware that is coming in",
    "start": "422440",
    "end": "428919"
  },
  {
    "text": "into the mix if you're using gpus or you're using accelerated CPUs to be able",
    "start": "428919",
    "end": "434919"
  },
  {
    "text": "to run your models and performance and resource consumption and understanding",
    "start": "434919",
    "end": "441280"
  },
  {
    "text": "that and factoring that in the overall observability monitoring uh and the",
    "start": "441280",
    "end": "447120"
  },
  {
    "text": "analysis that you do and present to run production networks is something that",
    "start": "447120",
    "end": "452520"
  },
  {
    "text": "you need to factor in so these the I I'm going to dive deep into each of these",
    "start": "452520",
    "end": "458080"
  },
  {
    "text": "areas and what that means and kind of give you an example in in each of those",
    "start": "458080",
    "end": "463599"
  },
  {
    "text": "areas so when you're uh you know the first area of integration for a new",
    "start": "463599",
    "end": "470479"
  },
  {
    "text": "paradigm of observability is that uh you are now starting to integrate",
    "start": "470479",
    "end": "478319"
  },
  {
    "text": "observability um um instrumentation as well as um you know U",
    "start": "478319",
    "end": "485319"
  },
  {
    "text": "analysis if you will for your model training process and what that means is",
    "start": "485319",
    "end": "491919"
  },
  {
    "text": "that you're taking you're try your your aim here is to understand how long it",
    "start": "491919",
    "end": "498440"
  },
  {
    "text": "takes for an AI model to train right because it depends on the size of the",
    "start": "498440",
    "end": "504840"
  },
  {
    "text": "model you're using the complexity of the parameters the number of layers you have",
    "start": "504840",
    "end": "509919"
  },
  {
    "text": "for the models and and the errors that are known errors and failures based on",
    "start": "509919",
    "end": "516360"
  },
  {
    "text": "the data you're training with right so there are several parameters that you have to look at in order to say Hey you",
    "start": "516360",
    "end": "524120"
  },
  {
    "text": "know these are factors that we are looking at in our observability Pipeline",
    "start": "524120",
    "end": "529480"
  },
  {
    "text": "and we need to collect the metrics or the traces all the profiles that we need",
    "start": "529480",
    "end": "535480"
  },
  {
    "text": "for being able to uh determine whether our training is successful or not for",
    "start": "535480",
    "end": "542240"
  },
  {
    "text": "that model right so uh the important part here is",
    "start": "542240",
    "end": "549800"
  },
  {
    "text": "also to understand errors right like what are the known errors that can occur in model training pipelines which I need",
    "start": "549800",
    "end": "556800"
  },
  {
    "text": "to observe and report back instrumenting training pipelines to view model weights",
    "start": "556800",
    "end": "562800"
  },
  {
    "text": "for example data distribution in the model itself on the different layers and",
    "start": "562800",
    "end": "569440"
  },
  {
    "text": "and and how you are actually uh determining confidence indexes for the",
    "start": "569440",
    "end": "575120"
  },
  {
    "text": "training that is being done uh is something that you know you again typically there are a whole series of",
    "start": "575120",
    "end": "581800"
  },
  {
    "text": "functional metrics as well as training specific metrics that you are actually",
    "start": "581800",
    "end": "588160"
  },
  {
    "text": "collecting as part of the observation of these pipelines and these pipelines have",
    "start": "588160",
    "end": "594200"
  },
  {
    "text": "to be instrumented based on the model that you're training with",
    "start": "594200",
    "end": "600680"
  },
  {
    "text": "the other part is also running continuous analysis and this is where you know an observability specific model",
    "start": "600680",
    "end": "609640"
  },
  {
    "text": "can be used is that you can use continuous analysis of the errors that",
    "start": "609640",
    "end": "616279"
  },
  {
    "text": "you may be seeing while training the pipeline to be able to train your own",
    "start": "616279",
    "end": "621880"
  },
  {
    "text": "observability model also and what why does that matter because if you are you",
    "start": "621880",
    "end": "628560"
  },
  {
    "text": "know Contin ously running these pipelines over time every time you use a model you will be training again with",
    "start": "628560",
    "end": "635440"
  },
  {
    "text": "new data right and especially if you're running real time across a period of",
    "start": "635440",
    "end": "640680"
  },
  {
    "text": "time you will take that and be able to reuse it and at that point having an LM",
    "start": "640680",
    "end": "647560"
  },
  {
    "text": "which is observability specific also is super helpful because that gives you",
    "start": "647560",
    "end": "652880"
  },
  {
    "text": "longterm information about the observability metrics and the um changes",
    "start": "652880",
    "end": "660440"
  },
  {
    "text": "in those metrics over time and for so for for the sake of",
    "start": "660440",
    "end": "666200"
  },
  {
    "text": "example you know don't want to get lost in a lot of details because you could easily dive into each layer and say Hey",
    "start": "666200",
    "end": "673320"
  },
  {
    "text": "you know these are the layers of the model and this is these are the metrics this is the distributions these are the",
    "start": "673320",
    "end": "679360"
  },
  {
    "text": "weights uh these are the uh numbers that you are getting for you know the",
    "start": "679360",
    "end": "684639"
  },
  {
    "text": "training time that it takes for this specific model and you're reporting that back but you're also continuously",
    "start": "684639",
    "end": "690600"
  },
  {
    "text": "learning with the uh observability model that you're using and and uh what that",
    "start": "690600",
    "end": "697079"
  },
  {
    "text": "does is for example with a news Service app right like if you have a News application where you're getting uh new",
    "start": "697079",
    "end": "705360"
  },
  {
    "text": "news updates you know from your data sources that you're picking up for your news app you can run an AI model on the",
    "start": "705360",
    "end": "713160"
  },
  {
    "text": "application site that is continuously you know harvesting these new n news data sources and being able to you know",
    "start": "713160",
    "end": "720279"
  },
  {
    "text": "on the application side hey say Hey you know okay so we're training with all this news data and now we have an",
    "start": "720279",
    "end": "726800"
  },
  {
    "text": "application that can give you current news any any any period of time but then",
    "start": "726800",
    "end": "732839"
  },
  {
    "text": "you are also categorizing the incoming news items and running smart error",
    "start": "732839",
    "end": "737920"
  },
  {
    "text": "analysis as you train continuously and that error analysis longterm is is kind",
    "start": "737920",
    "end": "745079"
  },
  {
    "text": "of what your observability llm learns with which that gives you back the kind of feedback that",
    "start": "745079",
    "end": "751880"
  },
  {
    "text": "you're looking for that hey you know my model training process is it working as expected or is it you know out of",
    "start": "751880",
    "end": "760240"
  },
  {
    "text": "thresholds so this this is one par you know aspect of your new generation of AI",
    "start": "760240",
    "end": "767399"
  },
  {
    "text": "apps where you need to have observability and that is something that's expected if you are actually",
    "start": "767399",
    "end": "774440"
  },
  {
    "text": "looking at using uh models in your application the second part is really",
    "start": "774440",
    "end": "781720"
  },
  {
    "text": "understanding the inference pipelines right because at the end of the day once you've trained your model you're using",
    "start": "781720",
    "end": "788360"
  },
  {
    "text": "it in your AI application then at that point you want",
    "start": "788360",
    "end": "793880"
  },
  {
    "text": "to understand the parameters that is your uh inference model your inference",
    "start": "793880",
    "end": "800120"
  },
  {
    "text": "pipeline accurate or not right is and this goes back to what does accuracy",
    "start": "800120",
    "end": "806040"
  },
  {
    "text": "mean here it means that is it actually uh providing fair use is it within the",
    "start": "806040",
    "end": "813920"
  },
  {
    "text": "fair data use you know guidelines that your organization may have established",
    "start": "813920",
    "end": "819440"
  },
  {
    "text": "for your applications is your data um uh",
    "start": "819440",
    "end": "825000"
  },
  {
    "text": "privacy uh within your privacy guidelines so you know any guidelines that you're are setting for fair use um",
    "start": "825000",
    "end": "833120"
  },
  {
    "text": "uh use of bias for example if you're detecting bias in your data",
    "start": "833120",
    "end": "840079"
  },
  {
    "text": "uh each one of these are parameters for the inference pipelines which are then looked at for accuracy right is it",
    "start": "840079",
    "end": "847519"
  },
  {
    "text": "within the thresholds of you know what is expected versus it's out of bounds",
    "start": "847519",
    "end": "852639"
  },
  {
    "text": "and then from an observability standpoint you want to know that right because at the end of the day that's",
    "start": "852639",
    "end": "858519"
  },
  {
    "text": "part of what you're reporting back as part of the observability metrics or the data that you are communicating now back",
    "start": "858519",
    "end": "866079"
  },
  {
    "text": "to say Hey you know everything is in order with the application so what that does also is",
    "start": "866079",
    "end": "873320"
  },
  {
    "text": "that it evaluates the latency of model in processing an",
    "start": "873320",
    "end": "878480"
  },
  {
    "text": "input that somebody asked for a news item for example and it returned back hey you know is this the result is the",
    "start": "878480",
    "end": "885240"
  },
  {
    "text": "results accurate right that the did the application actually give back the",
    "start": "885240",
    "end": "890920"
  },
  {
    "text": "response that you wanted to back using that model and that's what inference",
    "start": "890920",
    "end": "896639"
  },
  {
    "text": "does right that you you're now starting to infer whether your results that are",
    "start": "896639",
    "end": "901800"
  },
  {
    "text": "you know being uh from the model are converging to an accurate output and you",
    "start": "901800",
    "end": "908639"
  },
  {
    "text": "are looking at that in order to understand if it is within the confidence bands of inference generated",
    "start": "908639",
    "end": "915720"
  },
  {
    "text": "or not so defining and instrumenting Telemetry data for measuring latency and",
    "start": "915720",
    "end": "922440"
  },
  {
    "text": "distribution is super important because if your Telemetry data does not really",
    "start": "922440",
    "end": "929560"
  },
  {
    "text": "understand you know or measure that latency in that inference response",
    "start": "929560",
    "end": "935240"
  },
  {
    "text": "pipeline the input output pipeline uh or the uh the cycle if you will as well as",
    "start": "935240",
    "end": "942079"
  },
  {
    "text": "the use case application confidence um the use case specific that",
    "start": "942079",
    "end": "947839"
  },
  {
    "text": "is in this case it's the news service your confidence thresholds for normal",
    "start": "947839",
    "end": "953519"
  },
  {
    "text": "and edge cases is the is the result that you're getting from your inference pipeline",
    "start": "953519",
    "end": "960240"
  },
  {
    "text": "within those normal thresholds or not that's also another metric that you are",
    "start": "960240",
    "end": "966120"
  },
  {
    "text": "actually deducing from your observability pipeline and then the third thing is to",
    "start": "966120",
    "end": "973600"
  },
  {
    "text": "again apply an ongoing observability llm",
    "start": "973600",
    "end": "979000"
  },
  {
    "text": "in the back end to continuously learn from the results that are being picked up from the observation of this",
    "start": "979000",
    "end": "986680"
  },
  {
    "text": "application so it's complex right you're you're looking at not only an AI enabled application but you're also using an",
    "start": "986680",
    "end": "993519"
  },
  {
    "text": "observability AI uh you know in the background an observability model in the",
    "start": "993519",
    "end": "999720"
  },
  {
    "text": "background to continuously learn whether those results are within your normal",
    "start": "999720",
    "end": "1004920"
  },
  {
    "text": "thresholds that are established or abnormal so in the case of the news app",
    "start": "1004920",
    "end": "1010720"
  },
  {
    "text": "that I was talking about again being able to from an observability standpoint",
    "start": "1010720",
    "end": "1016639"
  },
  {
    "text": "Monitor and publish the confidence thresholds that are uh you know for each",
    "start": "1016639",
    "end": "1023160"
  },
  {
    "text": "encoder layer in the model is something that's super useful as conveying back to",
    "start": "1023160",
    "end": "1029558"
  },
  {
    "text": "the application developers because they they care about you know hey each what",
    "start": "1029559",
    "end": "1035760"
  },
  {
    "text": "is the what does the weight on each of these layers is the encoding done in",
    "start": "1035760",
    "end": "1041360"
  },
  {
    "text": "time you know it's like if I want an subse response on each layer is that",
    "start": "1041360",
    "end": "1047839"
  },
  {
    "text": "what I is what I'm getting back and being able to convey those uh values at",
    "start": "1047839",
    "end": "1055240"
  },
  {
    "text": "each layer and provide that back as part of your observability of the you know",
    "start": "1055240",
    "end": "1060960"
  },
  {
    "text": "report or your dashboard is super",
    "start": "1060960",
    "end": "1065919"
  },
  {
    "text": "useful any questions so far or we can you know finish first and then you can",
    "start": "1066120",
    "end": "1072000"
  },
  {
    "text": "ask questions moving on to the third part",
    "start": "1072000",
    "end": "1079600"
  },
  {
    "text": "which is uh considering the performance and resource consumption of models right",
    "start": "1079600",
    "end": "1086960"
  },
  {
    "text": "so what that means is again as you're running these models they're expensive",
    "start": "1086960",
    "end": "1092360"
  },
  {
    "text": "they're you typically you know especially llms and it really depends on whether",
    "start": "1092360",
    "end": "1097559"
  },
  {
    "text": "you're running these models on on the edge or in the on server side which is",
    "start": "1097559",
    "end": "1103880"
  },
  {
    "text": "typically where your Cloud infrastructure runs right so if you are running it on the edge typically your",
    "start": "1103880",
    "end": "1111520"
  },
  {
    "text": "models will have fewer parameters for what you know and the level of",
    "start": "1111520",
    "end": "1116840"
  },
  {
    "text": "complexity that you can run on on an edge device is far smaller than what you",
    "start": "1116840",
    "end": "1122280"
  },
  {
    "text": "can run on on large compute you know with a GPU form to back it up and being",
    "start": "1122280",
    "end": "1127600"
  },
  {
    "text": "able to then observe that entire workflow and it's super important to",
    "start": "1127600",
    "end": "1134000"
  },
  {
    "text": "understand what the performance latency numbers are and the resource consumption",
    "start": "1134000",
    "end": "1140440"
  },
  {
    "text": "of such you know of the hardware underneath because that kind of factors",
    "start": "1140440",
    "end": "1145520"
  },
  {
    "text": "in also in what kind of large language models you can run on the Edge versus",
    "start": "1145520",
    "end": "1151880"
  },
  {
    "text": "what you can run on on uh on your Cloud",
    "start": "1151880",
    "end": "1156960"
  },
  {
    "text": "right so in order to evaluate the efficiency of an application AI model",
    "start": "1156960",
    "end": "1164080"
  },
  {
    "text": "you want to be able to continuously also understand how to optimize resource es",
    "start": "1164080",
    "end": "1169120"
  },
  {
    "text": "for running on Edge versus running on uh Cloud because again you can have a lot",
    "start": "1169120",
    "end": "1177320"
  },
  {
    "text": "of different ways that you can run models where you can split out the functionality of each model based on",
    "start": "1177320",
    "end": "1185039"
  },
  {
    "text": "where it's running right because the hardware footprint matters and therefore",
    "start": "1185039",
    "end": "1190320"
  },
  {
    "text": "you if you run it on edge likely you will do the more uh you know uh Edge",
    "start": "1190320",
    "end": "1196000"
  },
  {
    "text": "based uh computations and in is that you would like to with larger smaller models",
    "start": "1196000",
    "end": "1202080"
  },
  {
    "text": "versus running llms on the backend so instrumenting resource",
    "start": "1202080",
    "end": "1209000"
  },
  {
    "text": "utilization metrics and this is an overloaded term because resource utilization can mean many things it can",
    "start": "1209000",
    "end": "1216720"
  },
  {
    "text": "mean um you know again measuring what is the uh CPU usage for example or the GPU",
    "start": "1216720",
    "end": "1224679"
  },
  {
    "text": "usage for the um actual um set of gpus that are being used for",
    "start": "1224679",
    "end": "1231559"
  },
  {
    "text": "running a met model and also it could mean other uh accelerated you know gpus",
    "start": "1231559",
    "end": "1238840"
  },
  {
    "text": "or CPUs that you're running on on Prem right on on cloud so instrumenting those resource",
    "start": "1238840",
    "end": "1246640"
  },
  {
    "text": "usage metrics is something that observ in the observability uh domain today we don't",
    "start": "1246640",
    "end": "1253159"
  },
  {
    "text": "do right because we have a concept of understanding the typical generation of",
    "start": "1253159",
    "end": "1260120"
  },
  {
    "text": "processors that are used under the hood and so that could be mean CPU",
    "start": "1260120",
    "end": "1266320"
  },
  {
    "text": "infrastructure CPU metric so it could mean memory but typically you don't actually",
    "start": "1266320",
    "end": "1272960"
  },
  {
    "text": "look at GPU usage um the percentage of time that",
    "start": "1272960",
    "end": "1280279"
  },
  {
    "text": "each GPU is getting used and because these you know uh resources are actually",
    "start": "1280279",
    "end": "1287679"
  },
  {
    "text": "very expensive today so you really need to consider this being as a third pillar for",
    "start": "1287679",
    "end": "1294440"
  },
  {
    "text": "observability because you want to constantly understand what is the resource utilization and can you",
    "start": "1294440",
    "end": "1301080"
  },
  {
    "text": "actually optimize the usage of your resources",
    "start": "1301080",
    "end": "1306480"
  },
  {
    "text": "better right so it's not like you can just run idle uh you know you set up a",
    "start": "1306480",
    "end": "1311760"
  },
  {
    "text": "job and you just run run an application on a particular set of CPUs CPUs are",
    "start": "1311760",
    "end": "1317880"
  },
  {
    "text": "cheap compared to gpus today so uh to consider you know again having a",
    "start": "1317880",
    "end": "1325960"
  },
  {
    "text": "continuous process as part of your observability to you know report back performance and resource consumption is",
    "start": "1325960",
    "end": "1333440"
  },
  {
    "text": "super important and what that means is also using an AI model again to continuously",
    "start": "1333440",
    "end": "1341000"
  },
  {
    "text": "learn about the usage patterns for each model because what that does for you is",
    "start": "1341000",
    "end": "1348679"
  },
  {
    "text": "over time uh enable you to kind of optimize you know based on the model",
    "start": "1348679",
    "end": "1354760"
  },
  {
    "text": "type that you're using the type of you know uh expectations for the uh GPU you know",
    "start": "1354760",
    "end": "1363880"
  },
  {
    "text": "clusters that you're using or the on uh the um sizing of the footprint of the",
    "start": "1363880",
    "end": "1370919"
  },
  {
    "text": "edge devices that you're using right it's again a delicate balance because",
    "start": "1370919",
    "end": "1376960"
  },
  {
    "text": "again your applications you can right on any application but the question is can you run your models effectively there",
    "start": "1376960",
    "end": "1383960"
  },
  {
    "text": "and can you get the results that you need with the kind of training that you need and can you report that back and",
    "start": "1383960",
    "end": "1391520"
  },
  {
    "text": "understand and observe that on a regular basis in order to complete that cycle",
    "start": "1391520",
    "end": "1396760"
  },
  {
    "text": "and that's why observability is very key in in enabling uh day one for AI you",
    "start": "1396760",
    "end": "1404240"
  },
  {
    "text": "know based applications because it really is comes down to you know",
    "start": "1404240",
    "end": "1409919"
  },
  {
    "text": "efficiency and optimization and it's way more um uh important to do this for AI",
    "start": "1409919",
    "end": "1418000"
  },
  {
    "text": "apps than for regular apps applications at this point just because of the sheer",
    "start": "1418000",
    "end": "1423919"
  },
  {
    "text": "you know size of the models especially in the Gen llm uh generation but also as",
    "start": "1423919",
    "end": "1431760"
  },
  {
    "text": "we get more and more you know bigger models with billions of parameters how",
    "start": "1431760",
    "end": "1438559"
  },
  {
    "text": "you actually optimize that for each footprint of Hardware so with that said again there",
    "start": "1438559",
    "end": "1445080"
  },
  {
    "text": "is a need you know as in in this new paradigm of observability to kind of",
    "start": "1445080",
    "end": "1450520"
  },
  {
    "text": "have our existing generation of observability Frameworks to be able to",
    "start": "1450520",
    "end": "1455799"
  },
  {
    "text": "accommodate that and what that means also is that you have to have continuous",
    "start": "1455799",
    "end": "1461760"
  },
  {
    "text": "analysis and understanding and evaluation of the efficiency of what works on the Edge versus what works on",
    "start": "1461760",
    "end": "1469520"
  },
  {
    "text": "uh Cloud so the takeaways here really are that you know can intelligent",
    "start": "1469520",
    "end": "1475679"
  },
  {
    "text": "observability requires changes in the current observability stack as well as",
    "start": "1475679",
    "end": "1481840"
  },
  {
    "text": "in the instrumentation of AI models and Edge and Cloud native",
    "start": "1481840",
    "end": "1488039"
  },
  {
    "text": "infrastructure be run on because this is the key change between writing a current generation of",
    "start": "1488039",
    "end": "1496919"
  },
  {
    "text": "application uh Services which you know do not use models versus adding models",
    "start": "1496919",
    "end": "1503679"
  },
  {
    "text": "as part of that whole uh application you know that you're building right because",
    "start": "1503679",
    "end": "1510000"
  },
  {
    "text": "the moment you add additional uh assets such as models you",
    "start": "1510000",
    "end": "1516200"
  },
  {
    "text": "immediately actually also have to have observability built in in order to just",
    "start": "1516200",
    "end": "1521480"
  },
  {
    "text": "understand what the behavior of the application is going to be and whether it is actually delivering",
    "start": "1521480",
    "end": "1528840"
  },
  {
    "text": "the results that you expect right the second part which is a",
    "start": "1528840",
    "end": "1535399"
  },
  {
    "text": "takeaway is that a new taxonomy of data for these models has to be evolved this",
    "start": "1535399",
    "end": "1540880"
  },
  {
    "text": "is still evolving it's actually um uh really not standardized today because",
    "start": "1540880",
    "end": "1548440"
  },
  {
    "text": "every vendor who has gpus and is building gpus and rolling them out into the cloud or on edge has their own",
    "start": "1548440",
    "end": "1556720"
  },
  {
    "text": "taxonomy and those metrics you know it's like um uh kubernetes did a lot in some",
    "start": "1556720",
    "end": "1563559"
  },
  {
    "text": "standard in standardizing the metrics for CPUs for example right there are",
    "start": "1563559",
    "end": "1568760"
  },
  {
    "text": "certain CPU metrics that are um you know shared today which which are all",
    "start": "1568760",
    "end": "1574840"
  },
  {
    "text": "standardized and we take it for granted but in the world of models this is not standardized yet and and for example",
    "start": "1574840",
    "end": "1582960"
  },
  {
    "text": "open Telemetry which is an you know large collection framework needs to be",
    "start": "1582960",
    "end": "1588440"
  },
  {
    "text": "way be aware of AI models the types of open AI models and understand you know",
    "start": "1588440",
    "end": "1596399"
  },
  {
    "text": "beyond the blackbox implementation right like you it's not good enough to just",
    "start": "1596399",
    "end": "1601880"
  },
  {
    "text": "say that hey you know we just going to do uh the uh we're just going to",
    "start": "1601880",
    "end": "1607120"
  },
  {
    "text": "blackbox the model we're going to assume that this is what we're going to get out of it you may not because every model is",
    "start": "1607120",
    "end": "1613000"
  },
  {
    "text": "different today and there is no standardization in the metrics or the",
    "start": "1613000",
    "end": "1619640"
  },
  {
    "text": "Telemetry data that is being emitted by these models right there's also not",
    "start": "1619640",
    "end": "1625799"
  },
  {
    "text": "enough standardization yet to be able to say that we can say it's one",
    "start": "1625799",
    "end": "1631240"
  },
  {
    "text": "instrumentation fits all it may not because the kind of data that these",
    "start": "1631240",
    "end": "1637919"
  },
  {
    "text": "these models may be uh emitting from an observability standpoint may not be the",
    "start": "1637919",
    "end": "1644520"
  },
  {
    "text": "same so you really have to keep some of these aspects in in mind when you're building and or using models for your",
    "start": "1644520",
    "end": "1651720"
  },
  {
    "text": "applications because that also filters back in into the observability Frameworks that you're using for",
    "start": "1651720",
    "end": "1658720"
  },
  {
    "text": "collecting this data and then being able to analyze it right and and and and",
    "start": "1658720",
    "end": "1664880"
  },
  {
    "text": "operate these applications in production and the third thing I'll",
    "start": "1664880",
    "end": "1671279"
  },
  {
    "text": "leave you with which is the observability stacks you know need to factor in is data bias",
    "start": "1671279",
    "end": "1678880"
  },
  {
    "text": "and security influences right because the data actually can really change the",
    "start": "1678880",
    "end": "1685720"
  },
  {
    "text": "way that your model behaves you know uh with your for your",
    "start": "1685720",
    "end": "1691360"
  },
  {
    "text": "application and and if there is you know bias in their data today observability",
    "start": "1691360",
    "end": "1697320"
  },
  {
    "text": "Frameworks have no understanding of it they just look at you know the data that",
    "start": "1697320",
    "end": "1702640"
  },
  {
    "text": "is being produced by applications very mechanically so to be able able to",
    "start": "1702640",
    "end": "1709720"
  },
  {
    "text": "understand uh those influences in your data and to be able",
    "start": "1709720",
    "end": "1715120"
  },
  {
    "text": "to report that as a first step for course correction in terms of an",
    "start": "1715120",
    "end": "1721279"
  },
  {
    "text": "application you know avoiding that bias in results because of data so it is",
    "start": "1721279",
    "end": "1729480"
  },
  {
    "text": "very connected now because you can see that the moment you introduce you know a complex structure like a model it's not",
    "start": "1729480",
    "end": "1735799"
  },
  {
    "text": "just throwing in a model it's all Al actually understanding what your data is doing and what your data looks like what",
    "start": "1735799",
    "end": "1743200"
  },
  {
    "text": "are you ingesting that data in could be bad results out right so it it really is",
    "start": "1743200",
    "end": "1751480"
  },
  {
    "text": "also has the potential of kind of biasing your observability results",
    "start": "1751480",
    "end": "1758480"
  },
  {
    "text": "Because unless your uh observability stack understands what are normal",
    "start": "1758480",
    "end": "1764880"
  },
  {
    "text": "confidence you know indexes it can't",
    "start": "1764880",
    "end": "1770440"
  },
  {
    "text": "tell right so that's why it's like a completely changing Paradigm and this",
    "start": "1770440",
    "end": "1776360"
  },
  {
    "text": "needs to be actually factored in both into the collection as well as the an",
    "start": "1776360",
    "end": "1783320"
  },
  {
    "text": "analysis of you know what you're reporting through your observability",
    "start": "1783320",
    "end": "1789519"
  },
  {
    "text": "pipelines so with that said again um that's all I had to share with you today",
    "start": "1789519",
    "end": "1796679"
  },
  {
    "text": "and I didn't want to make this disc discussion too complex because I could have gone into a lot of detail about",
    "start": "1796679",
    "end": "1801760"
  },
  {
    "text": "what models look like and uh you know what could you do with different metrics at different layers but uh happy to",
    "start": "1801760",
    "end": "1809039"
  },
  {
    "text": "answer any questions in this space",
    "start": "1809039",
    "end": "1814480"
  }
]