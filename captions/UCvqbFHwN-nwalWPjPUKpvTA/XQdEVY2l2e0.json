[
  {
    "text": "okay hey my name is Fredrik I'm a software engineer at kora and I'm",
    "start": "30",
    "end": "6299"
  },
  {
    "text": "here to talk about Newark Manager on its way to high availability I work on",
    "start": "6299",
    "end": "12059"
  },
  {
    "text": "upstream prometheus and alert manager and kubernetes and pretty much everything connecting the two worlds and",
    "start": "12059",
    "end": "18900"
  },
  {
    "text": "I'm branch on github so a lot manager on its way to high availability but what",
    "start": "18900",
    "end": "25019"
  },
  {
    "text": "does that actually mean what am I going to talk about so when they talked about and as from what happens from an alert",
    "start": "25019",
    "end": "32578"
  },
  {
    "text": "firing from prometheus to an actual notification being sent out then the",
    "start": "32579",
    "end": "39420"
  },
  {
    "text": "actual high availability contract between Prometheus and alert manager so",
    "start": "39420",
    "end": "44430"
  },
  {
    "text": "how we actually achieve high availability between the two systems the",
    "start": "44430",
    "end": "49879"
  },
  {
    "text": "actual implementation and what is that what kind of implication does that have",
    "start": "49879",
    "end": "55949"
  },
  {
    "text": "on an operator operating alert manager in a highly available manner just as a",
    "start": "55949",
    "end": "63629"
  },
  {
    "text": "before it's not talking about everything else how does cores fit in to working on",
    "start": "63629",
    "end": "69630"
  },
  {
    "text": "these projects well this all fit into our product tectonic which is health",
    "start": "69630",
    "end": "76799"
  },
  {
    "text": "driving kubernetes and as part of that we want to have self-driving monitoring systems so that's exactly why we want to",
    "start": "76799",
    "end": "83450"
  },
  {
    "text": "be in this community be active drive innovation so just to give a general",
    "start": "83450",
    "end": "92670"
  },
  {
    "text": "overview for those who may have not worked with an earth manager itself so far alert manager is the component that",
    "start": "92670",
    "end": "101299"
  },
  {
    "text": "received alerts that are fired from a Prometheus instance but technically it doesn't actually have to be Prometheus",
    "start": "101299",
    "end": "107399"
  },
  {
    "text": "it just has to speak the same protocol and behave in the same way as Prometheus does Tom what is then does is it groups",
    "start": "107399",
    "end": "115439"
  },
  {
    "text": "all of those alerts into one notification and it potentially deduplicate them so that when alerts are",
    "start": "115439",
    "end": "123420"
  },
  {
    "text": "hiring again and again that we don't actually send out notifications more often than we want to and then once it's",
    "start": "123420",
    "end": "132030"
  },
  {
    "text": "done all of those things it actually sends them out with your favorite provider so paid your duty",
    "start": "132030",
    "end": "137760"
  },
  {
    "text": "black female whatever you want pretty much and there's also a web Punk",
    "start": "137760",
    "end": "143280"
  },
  {
    "text": "integration if anything you want to use is not fair default included then you",
    "start": "143280",
    "end": "148920"
  },
  {
    "text": "can build it yourself and one of the features that are important about the",
    "start": "148920",
    "end": "153989"
  },
  {
    "text": "talk today is financing so that means if you for example has a plan maintenance window where you know your database is",
    "start": "153989",
    "end": "161400"
  },
  {
    "text": "going to be offline for let's say 8 hours you can already like you can put that into alert manager and you won't be",
    "start": "161400",
    "end": "167489"
  },
  {
    "text": "notified about that because it's an alert or a notification that you're aware it's going to happen so this is",
    "start": "167489",
    "end": "176370"
  },
  {
    "text": "the general architecture overview of Prometheus and alert manager and",
    "start": "176370",
    "end": "182060"
  },
  {
    "text": "specifically today we want to be talking about how alerts are generated from",
    "start": "182060",
    "end": "187079"
  },
  {
    "text": "Prometheus which is partly through prom ql and alerting rule and those then",
    "start": "187079",
    "end": "194609"
  },
  {
    "text": "potentially result in an alert actually being hired against alert manager and we",
    "start": "194609",
    "end": "201870"
  },
  {
    "text": "wanted to see what that contract between Prometheus and alert manager looks like so just as a as an example of how alert",
    "start": "201870",
    "end": "210510"
  },
  {
    "text": "manager could work is prometheus is constantly evaluating these alerting rules and sends out alerts for fires",
    "start": "210510",
    "end": "220590"
  },
  {
    "text": "alert against alert manager every rule interval so that could be every 10",
    "start": "220590",
    "end": "226109"
  },
  {
    "text": "seconds it actually evaluates these alerting rules and actually tires alert",
    "start": "226109",
    "end": "233160"
  },
  {
    "text": "against alert manager every every 10 seconds so that's basically the contract",
    "start": "233160",
    "end": "239340"
  },
  {
    "text": "that Prometheus and alert manager have along themselves and then when alert",
    "start": "239340",
    "end": "244409"
  },
  {
    "text": "manager receives all of these alerts as we can see here then it actually grouped",
    "start": "244409",
    "end": "249510"
  },
  {
    "text": "takes all of these and we've seen before there were multiple alerts on highlighting 3 it says ok there are 3",
    "start": "249510",
    "end": "255750"
  },
  {
    "text": "high latency alerts there are 10 high error rate alerts and to cache server",
    "start": "255750",
    "end": "262469"
  },
  {
    "text": "slow alerts and depending on your infrastructure you can group them to whatever you're needed and",
    "start": "262469",
    "end": "269660"
  },
  {
    "text": "potentially depending on how you structure your notifications you can actually include all the individual or",
    "start": "269660",
    "end": "276360"
  },
  {
    "text": "it's in your notification so shortly or",
    "start": "276360",
    "end": "282590"
  },
  {
    "text": "compact alert manager is the component that reliably sends you notifications",
    "start": "282590",
    "end": "288990"
  },
  {
    "text": "and if we're talking about rely reliability we want to talk about high",
    "start": "288990",
    "end": "295410"
  },
  {
    "text": "availability and that's essentially what we'll we're here to talk about so just",
    "start": "295410",
    "end": "302490"
  },
  {
    "text": "as an example I want to talk through why this is important to do so you start a",
    "start": "302490",
    "end": "309840"
  },
  {
    "text": "project and you write micro-service number one like service number two Micra",
    "start": "309840",
    "end": "315630"
  },
  {
    "text": "service number three and as you get start getting users they have some kind",
    "start": "315630",
    "end": "322140"
  },
  {
    "text": "of expectation that your service is running you have maybe even have an SLA",
    "start": "322140",
    "end": "327600"
  },
  {
    "text": "once you have paying customers so you want to start monitoring your application with Prometheus and at this",
    "start": "327600",
    "end": "341700"
  },
  {
    "text": "point you're only scraping the metrics from your clients or your service with",
    "start": "341700",
    "end": "348000"
  },
  {
    "text": "Prometheus and that allows you to write alerting rules so essentially what you",
    "start": "348000",
    "end": "353100"
  },
  {
    "text": "then do is you hook up Prometheus to alert manager and you actually get",
    "start": "353100",
    "end": "358770"
  },
  {
    "text": "notified if anything goes wrong well but if your product is successful right you",
    "start": "358770",
    "end": "364860"
  },
  {
    "text": "start scaling out and you want to be highly available with your service",
    "start": "364860",
    "end": "369990"
  },
  {
    "text": "because you have some kind of an SLA and then you start scaling up scaling up but",
    "start": "369990",
    "end": "376440"
  },
  {
    "text": "you realize you're scaling up all all of your users are facing services and have",
    "start": "376440",
    "end": "384450"
  },
  {
    "text": "them highly available but not your monitoring system so you add an additional Prometheus instance to",
    "start": "384450",
    "end": "390720"
  },
  {
    "text": "monitor all of those targets again so you're highly available with Prometheus and then you also send all of those",
    "start": "390720",
    "end": "398150"
  },
  {
    "text": "alerts that you developed before with your initial Prometheus instance against alert",
    "start": "398150",
    "end": "404840"
  },
  {
    "text": "manager but here in this diagram we can already see again we have a single point of failure which is alert manager in",
    "start": "404840",
    "end": "410870"
  },
  {
    "text": "this case so alert manager goes down we will not be receiving any notifications",
    "start": "410870",
    "end": "417010"
  },
  {
    "text": "and that's essentially our most crucial point at this point right we have alerts that are firing we know",
    "start": "417010",
    "end": "424310"
  },
  {
    "text": "that something is wrong it's just not reaching up so we add another alert",
    "start": "424310",
    "end": "430160"
  },
  {
    "text": "manager instance but because of the features that we that we saw earlier we still have a problem at this point",
    "start": "430160",
    "end": "435890"
  },
  {
    "text": "because alert manager in itself is deduplicating these alerts but now since",
    "start": "435890",
    "end": "441980"
  },
  {
    "text": "we have two instances it actually sends out these alerts the notifications twice that's exactly what we don't want so",
    "start": "441980",
    "end": "449980"
  },
  {
    "text": "what we've come up with four alert manager is that we implemented a gossip",
    "start": "449980",
    "end": "456320"
  },
  {
    "text": "protocol between other planet or instances and I want to talk through",
    "start": "456320",
    "end": "464660"
  },
  {
    "text": "what just how does gossip article wants to does work and what kind of",
    "start": "464660",
    "end": "470120"
  },
  {
    "text": "implications that has for you running it but first of all I want to say why did",
    "start": "470120",
    "end": "476450"
  },
  {
    "text": "we actually be couple alert manager from prometheus well it's exactly because of the",
    "start": "476450",
    "end": "481970"
  },
  {
    "text": "complexity that we just saw that we can keep all of this state sharing gossiping",
    "start": "481970",
    "end": "488470"
  },
  {
    "text": "integrations with notification providers that we cannot keep all of that separated in a separate component and",
    "start": "488470",
    "end": "496010"
  },
  {
    "text": "even that component could potentially be used by other monitoring systems they",
    "start": "496010",
    "end": "502670"
  },
  {
    "text": "just need to speak the same protocols that Prometheus sends against alert",
    "start": "502670",
    "end": "508130"
  },
  {
    "text": "manager and yeah that keeps everything simple in Prometheus from",
    "start": "508130",
    "end": "513680"
  },
  {
    "text": "alerting side so we only evaluate these rules and everything else complex the",
    "start": "513680",
    "end": "519979"
  },
  {
    "text": "distributed system can be done in the alert nature so let's see what an",
    "start": "519979",
    "end": "525950"
  },
  {
    "text": "example alerting will would look like in Prometheus so here I'm taking one of the",
    "start": "525950",
    "end": "531530"
  },
  {
    "text": "metrics that are actually exposed by a CD one of the chorus product or project and in sed a",
    "start": "531530",
    "end": "542639"
  },
  {
    "text": "problem could arise if there is no leader in an ad cluster so we say our",
    "start": "542639",
    "end": "549300"
  },
  {
    "text": "alert name is no leader and this alert triggers if some time series is does not",
    "start": "549300",
    "end": "558360"
  },
  {
    "text": "have a leader at this moment and if that stays for 10 minutes then we fire this",
    "start": "558360",
    "end": "563370"
  },
  {
    "text": "alert and we can add some description what does that what that actually means",
    "start": "563370",
    "end": "569040"
  },
  {
    "text": "we can template some things in there but this is a very simple alerting rule how we would write it for prometheus and",
    "start": "569040",
    "end": "575940"
  },
  {
    "text": "let's be clear that this is actually something that is evaluated in Prometheus not an alert manager but this",
    "start": "575940",
    "end": "584160"
  },
  {
    "text": "does result in prometheus constantly or in repeat and rule interrelation",
    "start": "584160",
    "end": "591870"
  },
  {
    "text": "interval does actually send every 15 seconds or however you have configured",
    "start": "591870",
    "end": "598529"
  },
  {
    "text": "Prometheus to evaluate the rules it actually sends these alerts every time",
    "start": "598529",
    "end": "604500"
  },
  {
    "text": "that triggers and that's very important for the high availability model between",
    "start": "604500",
    "end": "610199"
  },
  {
    "text": "the two so because we constantly send",
    "start": "610199",
    "end": "616350"
  },
  {
    "text": "these alerts how we can tell that an alert has been resolved is that if the",
    "start": "616350",
    "end": "622079"
  },
  {
    "text": "resolve timeout has been reached for example if after five minutes our others",
    "start": "622079",
    "end": "628019"
  },
  {
    "text": "are not firing anymore we know that the problem has been fixed and then this is",
    "start": "628019",
    "end": "633569"
  },
  {
    "text": "an example configuration for the alert manager which groups by the job so that",
    "start": "633569",
    "end": "639600"
  },
  {
    "text": "we can logically group all the alerts for let's see our HCD cluster and then",
    "start": "639600",
    "end": "647819"
  },
  {
    "text": "we can say that we always want to collect for 10 seconds or the alerts that are coming in and then we want to",
    "start": "647819",
    "end": "652889"
  },
  {
    "text": "send them out in a batch and then this",
    "start": "652889",
    "end": "658019"
  },
  {
    "text": "example just because it's very simple I use the web hook but it could be any",
    "start": "658019",
    "end": "663750"
  },
  {
    "text": "other integration that we have so from a high level perspective this is",
    "start": "663750",
    "end": "671640"
  },
  {
    "text": "what another it goes through once it's reached their newark manager so the very",
    "start": "671640",
    "end": "677190"
  },
  {
    "text": "first step is the financing where we decide do we actually want to continue",
    "start": "677190",
    "end": "682470"
  },
  {
    "text": "going through this notification pipeline or have we silenced that alert we don't",
    "start": "682470",
    "end": "687990"
  },
  {
    "text": "want to even consider sending it out the next point is very crucial for the high",
    "start": "687990",
    "end": "695280"
  },
  {
    "text": "availability mode and if you're not running it as high availability this is basically skipped but depending on your",
    "start": "695280",
    "end": "701940"
  },
  {
    "text": "position on an alert manager instance position in the cluster it waits for a",
    "start": "701940",
    "end": "707610"
  },
  {
    "text": "certain period of time for the gossip data to come around from up from other",
    "start": "707610",
    "end": "713190"
  },
  {
    "text": "instances and potentially deduplicate because the very next step is to deduplicate whether the notification has",
    "start": "713190",
    "end": "720510"
  },
  {
    "text": "already been sent out then it actually sends out the notification and then the",
    "start": "720510",
    "end": "726480"
  },
  {
    "text": "important part where actually sends the notification that a notification has",
    "start": "726480",
    "end": "732420"
  },
  {
    "text": "been sent out to the other alert manager instances and we will see why that is",
    "start": "732420",
    "end": "739890"
  },
  {
    "text": "very important I'll have a visual demo of that layer so what data is actually",
    "start": "739890",
    "end": "745410"
  },
  {
    "text": "actually gossiped the point that we just looked at it's the send notification",
    "start": "745410",
    "end": "752190"
  },
  {
    "text": "that data is profit but also the silences so whenever we create the silence an alert manager those are also",
    "start": "752190",
    "end": "759020"
  },
  {
    "text": "communicated via the gossip protocol what is not gossip is the alerts that",
    "start": "759020",
    "end": "766500"
  },
  {
    "text": "have been received by the firing system because that is just that's just the",
    "start": "766500",
    "end": "772050"
  },
  {
    "text": "sheer amount of that would be too large to be able to gossip in larger set up where notifications are a tiny fraction",
    "start": "772050",
    "end": "778950"
  },
  {
    "text": "of the received alerts and how did we",
    "start": "778950",
    "end": "786150"
  },
  {
    "text": "how did we achieve this well we use CRD T's so conflict-free replicated data",
    "start": "786150",
    "end": "794130"
  },
  {
    "text": "type so that that means that we basically can merge all these states without",
    "start": "794130",
    "end": "799190"
  },
  {
    "text": "conflict and we always end up with the same data basically and that's actually",
    "start": "799190",
    "end": "808640"
  },
  {
    "text": "wrong with not well-suited for CP systems but as well suited for AP systems in cap theorem because this is",
    "start": "808640",
    "end": "816860"
  },
  {
    "text": "exactly what we want to achieve with high availability we want to have an",
    "start": "816860",
    "end": "821930"
  },
  {
    "text": "available and partition tolerant system and how if we implementation wise do",
    "start": "821930",
    "end": "828950"
  },
  {
    "text": "this we use a library called mesh by week work and because we're using CR DTS",
    "start": "828950",
    "end": "836090"
  },
  {
    "text": "here or because mesh requires us to do to have a an eventual e consistent model",
    "start": "836090",
    "end": "844090"
  },
  {
    "text": "that's what we have so we have a last writer winds model where time stamp",
    "start": "844090",
    "end": "851120"
  },
  {
    "text": "based basically whatever the lightest time stamp is wins arguably we could",
    "start": "851120",
    "end": "857560"
  },
  {
    "text": "replace that with vector clocks for example but so far we haven't had any",
    "start": "857560",
    "end": "862670"
  },
  {
    "text": "problems so we're sticking with that for now but it could definitely be exchanged",
    "start": "862670",
    "end": "870130"
  },
  {
    "text": "but yeah basically we can just merge all these things based on a unique ID and",
    "start": "870130",
    "end": "876190"
  },
  {
    "text": "it's quite important to know how these unique IDs are generated to be on to be",
    "start": "876190",
    "end": "882710"
  },
  {
    "text": "able to understand the availability model but just looking back at why are",
    "start": "882710",
    "end": "890990"
  },
  {
    "text": "we actually implementing a complex distributed system ourself and not use",
    "start": "890990",
    "end": "896870"
  },
  {
    "text": "something like sed well ICD is actually not the kind of model that we want to",
    "start": "896870",
    "end": "902060"
  },
  {
    "text": "achieve ICD is CP not ap and the",
    "start": "902060",
    "end": "909470"
  },
  {
    "text": "advantage would also be that the alert manager is actually single binary so",
    "start": "909470",
    "end": "915560"
  },
  {
    "text": "it's very easy to operate and it's just less moving parts because if we had a",
    "start": "915560",
    "end": "921490"
  },
  {
    "text": "distributed data store that would be backing for our system then that would",
    "start": "921490",
    "end": "927230"
  },
  {
    "text": "be a lot harder to operate or even troubleshoot if there are any",
    "start": "927230",
    "end": "932499"
  },
  {
    "text": "albums with it so let's look at silences",
    "start": "932499",
    "end": "938309"
  },
  {
    "text": "we have to alert manager instances and we want to perform a create action on",
    "start": "939359",
    "end": "947799"
  },
  {
    "text": "those and want to see what that means in our gossip protocol so the first thing",
    "start": "947799",
    "end": "953829"
  },
  {
    "text": "that actually happens is some user actually creates or wants to create a silence through the HTTP API for example",
    "start": "953829",
    "end": "961089"
  },
  {
    "text": "and what alert manager 0 because alert manager 0 received as a crest request",
    "start": "961089",
    "end": "967719"
  },
  {
    "text": "actually does is it inserts it into its own in memory database and once it's",
    "start": "967719",
    "end": "975039"
  },
  {
    "text": "done that it basically takes the Delta that was just inserted into its own",
    "start": "975039",
    "end": "982929"
  },
  {
    "text": "database and gossips that to all other other manager instances with the unique",
    "start": "982929",
    "end": "989759"
  },
  {
    "text": "identifier which for silence is just some randomly created unique ID and then",
    "start": "989759",
    "end": "996899"
  },
  {
    "text": "since alert manager 1 just gets that Delta it merges it into it data set and",
    "start": "996899",
    "end": "1003199"
  },
  {
    "text": "eventually they all end up with the same data so how would that work with a with",
    "start": "1003199",
    "end": "1010499"
  },
  {
    "text": "updating a silence because let's say someone got sick and you are moving the",
    "start": "1010499",
    "end": "1015809"
  },
  {
    "text": "maintenance window to next week so you want to change the start and end date or",
    "start": "1015809",
    "end": "1021269"
  },
  {
    "text": "just the start date of X Island so you just need to know the unique ID which is",
    "start": "1021269",
    "end": "1027538"
  },
  {
    "text": "in all of the databases of the road manager instances and say what you want",
    "start": "1027539",
    "end": "1032610"
  },
  {
    "text": "to modify and what then happens is that the alert manager 0 merges that into its",
    "start": "1032610",
    "end": "1037649"
  },
  {
    "text": "own in memory database has the Delta that has actually changed profit that to",
    "start": "1037649",
    "end": "1044428"
  },
  {
    "text": "the other alert manager instances and they merge that into their state and we",
    "start": "1044429",
    "end": "1049500"
  },
  {
    "text": "are actually consistent so for the finances that's an easier model than for",
    "start": "1049500",
    "end": "1058230"
  },
  {
    "text": "notifications because for silences we can always just create a new unique ID but for notifications we need to be a",
    "start": "1058230",
    "end": "1066300"
  },
  {
    "text": "to recreate the ID in a consistent manner so let's just see how we want the",
    "start": "1066300",
    "end": "1076050"
  },
  {
    "text": "alert manager to behave in certain cases so in the first days I want to show a",
    "start": "1076050",
    "end": "1082770"
  },
  {
    "text": "case where there's actually no failure but so like the happy case with a non",
    "start": "1082770",
    "end": "1089430"
  },
  {
    "text": "silent alert so that it actually gets sent out so Prometheus evaluates some alerting",
    "start": "1089430",
    "end": "1096300"
  },
  {
    "text": "rule and decides that it's going to be firing so it fires them again both alert",
    "start": "1096300",
    "end": "1102150"
  },
  {
    "text": "manager instances and because alert manager zero is position zero in the",
    "start": "1102150",
    "end": "1108210"
  },
  {
    "text": "processor it waits for zero seconds and alert manager one for five seconds and",
    "start": "1108210",
    "end": "1113520"
  },
  {
    "text": "so while alert manager one waits for the five seconds the manager zero actually",
    "start": "1113520",
    "end": "1119250"
  },
  {
    "text": "continues through the notification pipeline so at first once to deduplicate",
    "start": "1119250",
    "end": "1125550"
  },
  {
    "text": "the alert but it sees that there's actually not been in the notification",
    "start": "1125550",
    "end": "1130890"
  },
  {
    "text": "for this alert so far so it actually sends them out with paid paid your duty for example and then gossip the state",
    "start": "1130890",
    "end": "1139490"
  },
  {
    "text": "that this notification has already been sent out and once alert manager one is",
    "start": "1139490",
    "end": "1146340"
  },
  {
    "text": "has waited the five seconds then it will or wallet it's waited it received the",
    "start": "1146340",
    "end": "1152400"
  },
  {
    "text": "gossip data merged it into its state and therefore once it goes on to deduplicate",
    "start": "1152400",
    "end": "1157860"
  },
  {
    "text": "it does not actually send this notification out so but what does then",
    "start": "1157860",
    "end": "1165150"
  },
  {
    "text": "happen when there's a network partition from one alert manager to the other so",
    "start": "1165150",
    "end": "1171600"
  },
  {
    "text": "that it was not able to gossip the the",
    "start": "1171600",
    "end": "1177050"
  },
  {
    "text": "notification that has been sent out well because we're we were aiming to to build",
    "start": "1177050",
    "end": "1185430"
  },
  {
    "text": "an available system we always want to say we at least once send out our notifications so the terrible thing",
    "start": "1185430",
    "end": "1192960"
  },
  {
    "text": "would be for the alert manager that it doesn't send out a notification right so in a failure case we always make sure",
    "start": "1192960",
    "end": "1199440"
  },
  {
    "text": "that it least one notification gets sent out and so while alert Major General was trying",
    "start": "1199440",
    "end": "1206970"
  },
  {
    "text": "to gossip its data it failed because of the network partition and then alert",
    "start": "1206970",
    "end": "1212519"
  },
  {
    "text": "manager one was trying to deep duplicate but actually send it because it has not",
    "start": "1212519",
    "end": "1218879"
  },
  {
    "text": "received gossip data and so we have a notion of at least one sending a",
    "start": "1218879",
    "end": "1224159"
  },
  {
    "text": "notification so how does that work let's",
    "start": "1224159",
    "end": "1229619"
  },
  {
    "text": "say that we're actually able to generate this unique identifier for each of the",
    "start": "1229619",
    "end": "1234979"
  },
  {
    "text": "notifications ad to find where Prometheus or system that behaves like",
    "start": "1234979",
    "end": "1241289"
  },
  {
    "text": "Prometheus fires an alert against the alert manager zero in this case it wants",
    "start": "1241289",
    "end": "1247919"
  },
  {
    "text": "it sent out at the notification to its database so things like is it resolved",
    "start": "1247919",
    "end": "1255989"
  },
  {
    "text": "when was the last notification sent out and it works very similar to the",
    "start": "1255989",
    "end": "1264090"
  },
  {
    "text": "silences log we're just take the Delta again and gossip that so but how does",
    "start": "1264090",
    "end": "1273809"
  },
  {
    "text": "that how do we actually get the unique identifier then so what we do is when we",
    "start": "1273809",
    "end": "1279960"
  },
  {
    "text": "look at the actual route a alert goes through in the alert manager we always",
    "start": "1279960",
    "end": "1285570"
  },
  {
    "text": "say things like group by group by job for example so at runtime we take the",
    "start": "1285570",
    "end": "1291269"
  },
  {
    "text": "values of these labels and take take all",
    "start": "1291269",
    "end": "1298649"
  },
  {
    "text": "those from all the alerts that are going to be in one notification we hash them",
    "start": "1298649",
    "end": "1303749"
  },
  {
    "text": "and then XOR them with the entire route that it's gone through because the route can be a tree so we need to go the",
    "start": "1303749",
    "end": "1311159"
  },
  {
    "text": "entire route up and then at the end we actually concatenate that with the receiver because a route can send it out",
    "start": "1311159",
    "end": "1319049"
  },
  {
    "text": "to multiple receivers so at that point we then know that we've send it out for that specific receiver for that specific",
    "start": "1319049",
    "end": "1327779"
  },
  {
    "text": "set of alerts in a notification so now that I've shown how this is",
    "start": "1327779",
    "end": "1334350"
  },
  {
    "text": "supposed to be working in theory let's see that this actually works so I've",
    "start": "1334350",
    "end": "1340799"
  },
  {
    "text": "prepared a little demo that large enough",
    "start": "1340799",
    "end": "1347898"
  },
  {
    "text": "so I have a simple alert manager configuration here very similar to the",
    "start": "1348110",
    "end": "1355139"
  },
  {
    "text": "one that I've shown in my presentation I picked up that I group by alert name",
    "start": "1355139",
    "end": "1360480"
  },
  {
    "text": "here and whether that's a good idea or not that's another discussion but just",
    "start": "1360480",
    "end": "1365940"
  },
  {
    "text": "for the sake of this presentation we have a group weight of 10 seconds so",
    "start": "1365940",
    "end": "1372179"
  },
  {
    "text": "we'll wait for 10 seconds to actually send out a notification and it wasn't that time we group notification and we",
    "start": "1372179",
    "end": "1380429"
  },
  {
    "text": "would send it out every one hour if these alerts keep firing and we send it",
    "start": "1380429",
    "end": "1386669"
  },
  {
    "text": "out with a wet work and I've prepared a",
    "start": "1386669",
    "end": "1391759"
  },
  {
    "text": "proc file here how I'll run three alert manager instances and an example webhook",
    "start": "1391759",
    "end": "1399840"
  },
  {
    "text": "which will listen on the address that I had in my configuration and the web folk",
    "start": "1399840",
    "end": "1406649"
  },
  {
    "text": "is really just a very simple very simple",
    "start": "1406649",
    "end": "1413100"
  },
  {
    "text": "go application with just listens on HTTP on port 5001 and then just writes",
    "start": "1413100",
    "end": "1420240"
  },
  {
    "text": "everything that it received to send it up so let's better string and everything's",
    "start": "1420240",
    "end": "1429899"
  },
  {
    "text": "running so let's go to the first alert manager web UI which would be 1993 and",
    "start": "1429899",
    "end": "1439879"
  },
  {
    "text": "we see that there are no silences created yet so the first thing I want to show is that gossiping of silences",
    "start": "1439879",
    "end": "1446580"
  },
  {
    "text": "actually works so we go through all of these alert manager instances and see no",
    "start": "1446580",
    "end": "1453929"
  },
  {
    "text": "there has indeed not been a silence created so far so that's we can take a",
    "start": "1453929",
    "end": "1460200"
  },
  {
    "text": "random alert manager instance and just create something so",
    "start": "1460200",
    "end": "1466320"
  },
  {
    "text": "alert name equals high latency for",
    "start": "1466320",
    "end": "1472169"
  },
  {
    "text": "example and I need to insert a an email and the comment could be something like",
    "start": "1472169",
    "end": "1479400"
  },
  {
    "text": "hi from pucon and we create that and we",
    "start": "1479400",
    "end": "1485250"
  },
  {
    "text": "see down here that the alert has been the silence has been created and now we",
    "start": "1485250",
    "end": "1490290"
  },
  {
    "text": "can check at all of our other instances that this silence has indeed been",
    "start": "1490290",
    "end": "1496100"
  },
  {
    "text": "gossiped to the other instances so now",
    "start": "1496100",
    "end": "1503460"
  },
  {
    "text": "we've seen that finances creating work so let's look at an example where there",
    "start": "1503460",
    "end": "1509880"
  },
  {
    "text": "would be actually something firing against the alert nature so we're",
    "start": "1509880",
    "end": "1515100"
  },
  {
    "text": "simulating what prometheus or comity is like system would do in this case so",
    "start": "1515100",
    "end": "1520830"
  },
  {
    "text": "I've a small script here called send alerts which just sends a couple of",
    "start": "1520830",
    "end": "1526320"
  },
  {
    "text": "alerts to my learn manager instances and we'll have to be quick here because",
    "start": "1526320",
    "end": "1532340"
  },
  {
    "text": "within ten seconds we should now see that a alert manager actually sends it",
    "start": "1532340",
    "end": "1538559"
  },
  {
    "text": "out and the webhook server printed it out to standard out and what we want to",
    "start": "1538559",
    "end": "1545460"
  },
  {
    "text": "see is that even though these alerts have been sent to all of the alert manager instances only one of them has",
    "start": "1545460",
    "end": "1552059"
  },
  {
    "text": "sent out the notification and since they're the web server prints out",
    "start": "1552059",
    "end": "1558419"
  },
  {
    "text": "everything it receives on standard out we have only gotten one JSON blob and that is exactly what we wanted to",
    "start": "1558419",
    "end": "1565290"
  },
  {
    "text": "achieve and that is basically at the end of my demo and that makes the end of my",
    "start": "1565290",
    "end": "1572150"
  },
  {
    "text": "presentation just at the very last night",
    "start": "1572150",
    "end": "1577950"
  },
  {
    "text": "we're also hiring in Berlin or all over the world and at all our foreign offices",
    "start": "1577950",
    "end": "1584330"
  },
  {
    "text": "and if you have any questions I'm happy to answer them",
    "start": "1584330",
    "end": "1589370"
  },
  {
    "text": "um okay so the question was why didn't we choose something like in in a form of",
    "start": "1609800",
    "end": "1616230"
  },
  {
    "text": "a leadership so what we wanted to achieve is that we don't have any master",
    "start": "1616230",
    "end": "1621810"
  },
  {
    "text": "we want to have a map in this system because we want any any note to be able",
    "start": "1621810",
    "end": "1628050"
  },
  {
    "text": "to go down at any point in time and this is seem to be still available because typically systems that have a leader",
    "start": "1628050",
    "end": "1637850"
  },
  {
    "text": "election are typically CP system so particles or rock for example their CP",
    "start": "1637850",
    "end": "1647550"
  },
  {
    "text": "systems right but we wanted to have a an IP system in this case",
    "start": "1647550",
    "end": "1652790"
  },
  {
    "text": "okay there are no other crushing so the",
    "start": "1657660",
    "end": "1667650"
  },
  {
    "text": "it's an in-memory database but it gets snapshot not shot at every snapshot",
    "start": "1667650",
    "end": "1673350"
  },
  {
    "text": "interval that you want but there is a there's a time time where if the last",
    "start": "1673350",
    "end": "1680430"
  },
  {
    "text": "snapshot was done and within that time and like t0 and t1 and then inter",
    "start": "1680430",
    "end": "1686880"
  },
  {
    "text": "crashes it doesn't actually process that to disk so that can happen but we are we",
    "start": "1686880",
    "end": "1692130"
  },
  {
    "text": "are on track with that we do want to implement some kind of a right head lock",
    "start": "1692130",
    "end": "1697320"
  },
  {
    "text": "or on something in that manner to be able to do things like that yes so 1,000",
    "start": "1697320",
    "end": "1711090"
  },
  {
    "text": "notifications is like there's it's essentially free so we haven't actually",
    "start": "1711090",
    "end": "1717540"
  },
  {
    "text": "extensively benchmarked storage but like",
    "start": "1717540",
    "end": "1724050"
  },
  {
    "text": "memory usage is very low because we don't actually persist anything of the alerts that are coming in and that's the",
    "start": "1724050",
    "end": "1729990"
  },
  {
    "text": "high traffic right we only gossip and store the things that notifications that we've sent out and we also garbage",
    "start": "1729990",
    "end": "1737280"
  },
  {
    "text": "collect those after some retention time so even if you do scale up to let's say",
    "start": "1737280",
    "end": "1745670"
  },
  {
    "text": "that you need 16 gigs of ram you can still tune the retention period to be so",
    "start": "1745670",
    "end": "1754440"
  },
  {
    "text": "that you only use memory within that controllable range",
    "start": "1754440",
    "end": "1763070"
  },
  {
    "text": "I mean the knowledge okay so the",
    "start": "1767710",
    "end": "1780220"
  },
  {
    "text": "question was if I now scale-up alert manager will it has no data does",
    "start": "1780220",
    "end": "1785970"
  },
  {
    "text": "send out all the alert no actually when an alert manager joins the network it",
    "start": "1785970",
    "end": "1791980"
  },
  {
    "text": "gets all the data from the other alert managers and it also periodically",
    "start": "1791980",
    "end": "1799110"
  },
  {
    "text": "periodically share all of their state and merge it into each other so that way",
    "start": "1799110",
    "end": "1804700"
  },
  {
    "text": "we make sure that it means periodically we always have the same state so we",
    "start": "1804700",
    "end": "1828340"
  },
  {
    "text": "don't currently have a plan for anything like that but you can using the web hook you can basically implement whatever you",
    "start": "1828340",
    "end": "1834220"
  },
  {
    "text": "want right I mean you still need some kind of a notification mechanism right like if you want to like emails already",
    "start": "1834220",
    "end": "1841539"
  },
  {
    "text": "implemented and other than that you'll probably have to write a web hook if you have some proprietary system let's say I",
    "start": "1841539",
    "end": "1850020"
  },
  {
    "text": "wanted to use a question so general literature we don't want to duplicate",
    "start": "1852690",
    "end": "1858190"
  },
  {
    "text": "each of like actual management of people that on call schedules expiration policy",
    "start": "1858190",
    "end": "1864059"
  },
  {
    "text": "that we need more for systems like pages opportunity just like kind of a separate",
    "start": "1864059",
    "end": "1872000"
  },
  {
    "text": "25:4 let managers really just about the routine health alert notification okay I",
    "start": "1872000",
    "end": "1883770"
  },
  {
    "text": "think that's it though thank you [Applause]",
    "start": "1883770",
    "end": "1893750"
  }
]