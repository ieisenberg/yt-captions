[
  {
    "text": "my name is eben and in my day job I",
    "start": "30",
    "end": "1709"
  },
  {
    "text": "build a lot of cool stuff at this",
    "start": "1709",
    "end": "2939"
  },
  {
    "text": "company called honeycomb i/o and I'd be",
    "start": "2939",
    "end": "5339"
  },
  {
    "text": "happy to talk to you later about tracing",
    "start": "5339",
    "end": "7529"
  },
  {
    "text": "events observability for distributed",
    "start": "7529",
    "end": "9450"
  },
  {
    "text": "systems some of the stuff we did here",
    "start": "9450",
    "end": "10800"
  },
  {
    "text": "but that's not what we're here to talk",
    "start": "10800",
    "end": "12750"
  },
  {
    "text": "about today instead we're here to talk",
    "start": "12750",
    "end": "14820"
  },
  {
    "text": "about queuing theory now I feel like",
    "start": "14820",
    "end": "17220"
  },
  {
    "text": "like a lot of things in computer science",
    "start": "17220",
    "end": "18869"
  },
  {
    "text": "queueing theory has kind of a naming",
    "start": "18869",
    "end": "20310"
  },
  {
    "text": "problem I don't know about you but I'm",
    "start": "20310",
    "end": "22230"
  },
  {
    "text": "not really big on queueing waiting",
    "start": "22230",
    "end": "23910"
  },
  {
    "text": "around in lines and I'm not too",
    "start": "23910",
    "end": "25529"
  },
  {
    "text": "enthusiastic about theory either unless",
    "start": "25529",
    "end": "27570"
  },
  {
    "text": "it has some useful application for what",
    "start": "27570",
    "end": "29250"
  },
  {
    "text": "I do but the truth is the queueing",
    "start": "29250",
    "end": "32279"
  },
  {
    "text": "theory is all about asking and then",
    "start": "32279",
    "end": "34290"
  },
  {
    "text": "answering questions that we have as",
    "start": "34290",
    "end": "36480"
  },
  {
    "text": "system operators it's common to have",
    "start": "36480",
    "end": "38670"
  },
  {
    "text": "these questions that are relatively",
    "start": "38670",
    "end": "40170"
  },
  {
    "text": "difficult to answer you know if we have",
    "start": "40170",
    "end": "45329"
  },
  {
    "text": "something that we scale in kubernetes or",
    "start": "45329",
    "end": "46920"
  },
  {
    "text": "whatever what level of utilization is",
    "start": "46920",
    "end": "48480"
  },
  {
    "text": "appropriate if we want to make changes",
    "start": "48480",
    "end": "50100"
  },
  {
    "text": "what changes should we make to improve",
    "start": "50100",
    "end": "52260"
  },
  {
    "text": "performance and if we're not willing to",
    "start": "52260",
    "end": "54899"
  },
  {
    "text": "accept just pure guesswork as an answer",
    "start": "54899",
    "end": "57210"
  },
  {
    "text": "then queueing theory gives us both the",
    "start": "57210",
    "end": "59070"
  },
  {
    "text": "vocabulary and the toolkit a set of",
    "start": "59070",
    "end": "61770"
  },
  {
    "text": "results theorems developed and presented",
    "start": "61770",
    "end": "64830"
  },
  {
    "text": "by some really smart people to help us",
    "start": "64830",
    "end": "66869"
  },
  {
    "text": "approximate the software systems that we",
    "start": "66869",
    "end": "69119"
  },
  {
    "text": "build with simpler models that we can",
    "start": "69119",
    "end": "71369"
  },
  {
    "text": "either reason about mentally or analyze",
    "start": "71369",
    "end": "74040"
  },
  {
    "text": "analytically and this helps us interpret",
    "start": "74040",
    "end": "77549"
  },
  {
    "text": "the data that we collect either from",
    "start": "77549",
    "end": "79950"
  },
  {
    "text": "instrumenting are true production",
    "start": "79950",
    "end": "81630"
  },
  {
    "text": "systems or from conducting controlled",
    "start": "81630",
    "end": "84810"
  },
  {
    "text": "benchmarks and ultimately this helps us",
    "start": "84810",
    "end": "86970"
  },
  {
    "text": "better interrogate and understands the",
    "start": "86970",
    "end": "89549"
  },
  {
    "text": "systems that we build and operate",
    "start": "89549",
    "end": "90750"
  },
  {
    "text": "everyday",
    "start": "90750",
    "end": "92119"
  },
  {
    "text": "these methods are especially relevant of",
    "start": "92119",
    "end": "95220"
  },
  {
    "text": "course when an infrastructure is",
    "start": "95220",
    "end": "97040"
  },
  {
    "text": "distributed dynamically scaled ephemeral",
    "start": "97040",
    "end": "100079"
  },
  {
    "text": "highly concurrent cloud native if you",
    "start": "100079",
    "end": "102420"
  },
  {
    "text": "will so although this is not a talk",
    "start": "102420",
    "end": "104880"
  },
  {
    "text": "about modern hip new technology in fact",
    "start": "104880",
    "end": "107880"
  },
  {
    "text": "it's to talk about results that are more",
    "start": "107880",
    "end": "109590"
  },
  {
    "text": "than 100 years old in some cases I think",
    "start": "109590",
    "end": "111869"
  },
  {
    "text": "it's one that is increasingly relevant",
    "start": "111869",
    "end": "114659"
  },
  {
    "text": "to this audience so in this talk I want",
    "start": "114659",
    "end": "117750"
  },
  {
    "text": "to make these theses concrete by talking",
    "start": "117750",
    "end": "119850"
  },
  {
    "text": "first about how you would build a model",
    "start": "119850",
    "end": "122340"
  },
  {
    "text": "of a simple serial system one that does",
    "start": "122340",
    "end": "124680"
  },
  {
    "text": "just one thing at a time and use it to",
    "start": "124680",
    "end": "126810"
  },
  {
    "text": "derive some insights about real-world",
    "start": "126810",
    "end": "128489"
  },
  {
    "text": "workloads then I want to extend that to",
    "start": "128489",
    "end": "130860"
  },
  {
    "text": "talking about parallel systems how you",
    "start": "130860",
    "end": "133170"
  },
  {
    "text": "can reason about load",
    "start": "133170",
    "end": "134310"
  },
  {
    "text": "strategies and how you can use something",
    "start": "134310",
    "end": "136470"
  },
  {
    "text": "called the universal scalability law to",
    "start": "136470",
    "end": "138690"
  },
  {
    "text": "predict how your system will scale as",
    "start": "138690",
    "end": "140790"
  },
  {
    "text": "your traffic or your capacity grows and",
    "start": "140790",
    "end": "143130"
  },
  {
    "text": "finally will summarize some of the",
    "start": "143130",
    "end": "144989"
  },
  {
    "text": "things that we see along the way and",
    "start": "144989",
    "end": "146400"
  },
  {
    "text": "some key takeaways",
    "start": "146400",
    "end": "147569"
  },
  {
    "text": "so what before we dive in an important",
    "start": "147569",
    "end": "150780"
  },
  {
    "text": "and essential caveat any model is by",
    "start": "150780",
    "end": "153720"
  },
  {
    "text": "definition a flawed representation of",
    "start": "153720",
    "end": "155940"
  },
  {
    "text": "reality it is reductive otherwise it",
    "start": "155940",
    "end": "158940"
  },
  {
    "text": "wouldn't be a model it's also worthless",
    "start": "158940",
    "end": "161400"
  },
  {
    "text": "unless backed by real data if we just",
    "start": "161400",
    "end": "163800"
  },
  {
    "text": "make up some stuff and roll with it that",
    "start": "163800",
    "end": "166920"
  },
  {
    "text": "is not helpful unless we somehow",
    "start": "166920",
    "end": "168780"
  },
  {
    "text": "validate and back that model with",
    "start": "168780",
    "end": "171299"
  },
  {
    "text": "production data instrumentation and",
    "start": "171299",
    "end": "173790"
  },
  {
    "text": "controlled experiments benchmarks",
    "start": "173790",
    "end": "176900"
  },
  {
    "text": "however if you do get gather data having",
    "start": "176900",
    "end": "180720"
  },
  {
    "text": "some sort of model helps you make sense",
    "start": "180720",
    "end": "182519"
  },
  {
    "text": "of it instead of drowning in metrics you",
    "start": "182519",
    "end": "185069"
  },
  {
    "text": "can ask useful questions and validate",
    "start": "185069",
    "end": "187140"
  },
  {
    "text": "your assumptions if your service has",
    "start": "187140",
    "end": "189269"
  },
  {
    "text": "some sort of performance bottleneck",
    "start": "189269",
    "end": "190680"
  },
  {
    "text": "perhaps you can identify that bottleneck",
    "start": "190680",
    "end": "192840"
  },
  {
    "text": "before you actually hit it and cause an",
    "start": "192840",
    "end": "194760"
  },
  {
    "text": "outage if you want to create a benchmark",
    "start": "194760",
    "end": "197430"
  },
  {
    "text": "you can use queueing theory to decide",
    "start": "197430",
    "end": "199200"
  },
  {
    "text": "whether that benchmark is a plausible",
    "start": "199200",
    "end": "200910"
  },
  {
    "text": "representation of some different",
    "start": "200910",
    "end": "202859"
  },
  {
    "text": "real-world workload and finally if",
    "start": "202859",
    "end": "205350"
  },
  {
    "text": "you're planning changes that might be",
    "start": "205350",
    "end": "206700"
  },
  {
    "text": "expensive to make re architecture is",
    "start": "206700",
    "end": "208769"
  },
  {
    "text": "something you can use queuing theoretic",
    "start": "208769",
    "end": "210690"
  },
  {
    "text": "results to help decide whether those are",
    "start": "210690",
    "end": "212819"
  },
  {
    "text": "good changes or not what their effects",
    "start": "212819",
    "end": "214829"
  },
  {
    "text": "are likely to be so let's dive in by",
    "start": "214829",
    "end": "217530"
  },
  {
    "text": "talking about serial systems to motivate",
    "start": "217530",
    "end": "220079"
  },
  {
    "text": "this section I want to talk about one of",
    "start": "220079",
    "end": "221790"
  },
  {
    "text": "the services that we operate at",
    "start": "221790",
    "end": "223290"
  },
  {
    "text": "honeycomb our ingestion API the details",
    "start": "223290",
    "end": "226049"
  },
  {
    "text": "of this thing aren't too important but",
    "start": "226049",
    "end": "227850"
  },
  {
    "text": "it looks like a lot of services that you",
    "start": "227850",
    "end": "230430"
  },
  {
    "text": "are probably familiar with or perhaps",
    "start": "230430",
    "end": "231959"
  },
  {
    "text": "you run yourself it's user facing in our",
    "start": "231959",
    "end": "235019"
  },
  {
    "text": "case it receives ingestion data from",
    "start": "235019",
    "end": "236880"
  },
  {
    "text": "customer infrastructure kind of a lot of",
    "start": "236880",
    "end": "238859"
  },
  {
    "text": "data it's stateless highly concurrent",
    "start": "238859",
    "end": "241440"
  },
  {
    "text": "runs across many servers it's mostly CPU",
    "start": "241440",
    "end": "244109"
  },
  {
    "text": "balanced and it's supposed to be low",
    "start": "244109",
    "end": "245370"
  },
  {
    "text": "latency which for us means target Layton",
    "start": "245370",
    "end": "247440"
  },
  {
    "text": "sees in the single-digit know second",
    "start": "247440",
    "end": "249810"
  },
  {
    "text": "range at worse so we couldn't really run",
    "start": "249810",
    "end": "253709"
  },
  {
    "text": "this thing on ec2 we could run it as a",
    "start": "253709",
    "end": "255480"
  },
  {
    "text": "stateless service on kubernetes as well",
    "start": "255480",
    "end": "257070"
  },
  {
    "text": "either way that means that we could in",
    "start": "257070",
    "end": "259440"
  },
  {
    "text": "principle allocate more or less infinite",
    "start": "259440",
    "end": "261600"
  },
  {
    "text": "resources to this thing unfortunately",
    "start": "261600",
    "end": "264090"
  },
  {
    "text": "that would also cost more or less",
    "start": "264090",
    "end": "265710"
  },
  {
    "text": "infinite money",
    "start": "265710",
    "end": "267750"
  },
  {
    "text": "sir question is how do we allocate",
    "start": "267750",
    "end": "269550"
  },
  {
    "text": "appropriate resources to this service",
    "start": "269550",
    "end": "272010"
  },
  {
    "text": "how do we provide high performance at a",
    "start": "272010",
    "end": "274170"
  },
  {
    "text": "reasonable price point there are a",
    "start": "274170",
    "end": "276300"
  },
  {
    "text": "couple of ways that we could approach",
    "start": "276300",
    "end": "277260"
  },
  {
    "text": "this problem we could guess but that",
    "start": "277260",
    "end": "279600"
  },
  {
    "text": "would make for a pretty short talk we",
    "start": "279600",
    "end": "281760"
  },
  {
    "text": "could do large-scale production scale",
    "start": "281760",
    "end": "284040"
  },
  {
    "text": "load testing either generate enormous",
    "start": "284040",
    "end": "286200"
  },
  {
    "text": "amounts of traffic against a real",
    "start": "286200",
    "end": "287520"
  },
  {
    "text": "infrastructure or create a full-scale",
    "start": "287520",
    "end": "289470"
  },
  {
    "text": "copy of a real infrastructure and",
    "start": "289470",
    "end": "290970"
  },
  {
    "text": "generate enormous amounts of traffic",
    "start": "290970",
    "end": "292230"
  },
  {
    "text": "against that this is obviously a pretty",
    "start": "292230",
    "end": "295230"
  },
  {
    "text": "good idea like it would be a worthy",
    "start": "295230",
    "end": "296700"
  },
  {
    "text": "undertaking but it's also a time",
    "start": "296700",
    "end": "298530"
  },
  {
    "text": "consuming thing for a resource",
    "start": "298530",
    "end": "299730"
  },
  {
    "text": "constrained engineering team to do",
    "start": "299730",
    "end": "301200"
  },
  {
    "text": "rigorously so I want to explore a",
    "start": "301200",
    "end": "303270"
  },
  {
    "text": "different strategy we'll do some small",
    "start": "303270",
    "end": "306140"
  },
  {
    "text": "experiments things that you can do with",
    "start": "306140",
    "end": "308370"
  },
  {
    "text": "a few instances with limited time and",
    "start": "308370",
    "end": "310140"
  },
  {
    "text": "then build a performance model that will",
    "start": "310140",
    "end": "312690"
  },
  {
    "text": "help us take the results of our",
    "start": "312690",
    "end": "314100"
  },
  {
    "text": "experiments and reason about what they",
    "start": "314100",
    "end": "316170"
  },
  {
    "text": "imply for our production system",
    "start": "316170",
    "end": "318270"
  },
  {
    "text": "makes sense so here's the idea we have n",
    "start": "318270",
    "end": "324030"
  },
  {
    "text": "cores available to run this thing we",
    "start": "324030",
    "end": "326190"
  },
  {
    "text": "know it's more or less CPU bounds and is",
    "start": "326190",
    "end": "328020"
  },
  {
    "text": "some large number but what can we do",
    "start": "328020",
    "end": "330600"
  },
  {
    "text": "with just one core what's the maximal",
    "start": "330600",
    "end": "332850"
  },
  {
    "text": "single core throughput of this service",
    "start": "332850",
    "end": "335330"
  },
  {
    "text": "to answer that question will simulate",
    "start": "335330",
    "end": "337680"
  },
  {
    "text": "requests arriving uniformly at random",
    "start": "337680",
    "end": "339690"
  },
  {
    "text": "kind of like the requests that we see in",
    "start": "339690",
    "end": "341790"
  },
  {
    "text": "the wild and we're metal and we'll",
    "start": "341790",
    "end": "343410"
  },
  {
    "text": "measure the observed latency at",
    "start": "343410",
    "end": "344820"
  },
  {
    "text": "different levels of throughput so here's",
    "start": "344820",
    "end": "348180"
  },
  {
    "text": "what this looks like initially we're",
    "start": "348180",
    "end": "350400"
  },
  {
    "text": "doing pretty good you know",
    "start": "350400",
    "end": "351750"
  },
  {
    "text": "single-digit millisecond latency a",
    "start": "351750",
    "end": "353160"
  },
  {
    "text": "couple thousand requests per second and",
    "start": "353160",
    "end": "354540"
  },
  {
    "text": "then at some point things start to go",
    "start": "354540",
    "end": "356010"
  },
  {
    "text": "really south so that's obviously not",
    "start": "356010",
    "end": "358140"
  },
  {
    "text": "good our question can we find a model",
    "start": "358140",
    "end": "362190"
  },
  {
    "text": "that predicts this behavior that helps",
    "start": "362190",
    "end": "364290"
  },
  {
    "text": "us identify this knee and the response",
    "start": "364290",
    "end": "366150"
  },
  {
    "text": "time curve before we actually hit it",
    "start": "366150",
    "end": "368130"
  },
  {
    "text": "that will then let us extrapolate from",
    "start": "368130",
    "end": "370410"
  },
  {
    "text": "production data to say okay this is the",
    "start": "370410",
    "end": "371970"
  },
  {
    "text": "maximum safety utilization for the",
    "start": "371970",
    "end": "374160"
  },
  {
    "text": "service here's the idea because we've",
    "start": "374160",
    "end": "380040"
  },
  {
    "text": "constrained ourselves to a single core",
    "start": "380040",
    "end": "381570"
  },
  {
    "text": "we're going to use a single Q single",
    "start": "381570",
    "end": "384060"
  },
  {
    "text": "server model so we'll say we just have a",
    "start": "384060",
    "end": "386130"
  },
  {
    "text": "server which is some sort of black box",
    "start": "386130",
    "end": "387570"
  },
  {
    "text": "that processes tasks tasks arrived",
    "start": "387570",
    "end": "390210"
  },
  {
    "text": "independently if the server is busy",
    "start": "390210",
    "end": "392010"
  },
  {
    "text": "those tasks might have to wait until the",
    "start": "392010",
    "end": "394229"
  },
  {
    "text": "server can actually work on them do its",
    "start": "394229",
    "end": "396090"
  },
  {
    "text": "data processing or whatever finally the",
    "start": "396090",
    "end": "398400"
  },
  {
    "text": "server gets to the task it does some",
    "start": "398400",
    "end": "399690"
  },
  {
    "text": "work it returns to the client",
    "start": "399690",
    "end": "401550"
  },
  {
    "text": "so intuitively it's pretty clear that in",
    "start": "401550",
    "end": "407800"
  },
  {
    "text": "this model the busier the service is the",
    "start": "407800",
    "end": "410650"
  },
  {
    "text": "longer tasks will probably have to wait",
    "start": "410650",
    "end": "412510"
  },
  {
    "text": "before they can be completed kind of",
    "start": "412510",
    "end": "414640"
  },
  {
    "text": "like what we saw in the picture the",
    "start": "414640",
    "end": "416050"
  },
  {
    "text": "busier the server was the greater the",
    "start": "416050",
    "end": "417760"
  },
  {
    "text": "latency was and our question as well as",
    "start": "417760",
    "end": "419740"
  },
  {
    "text": "a function of throughput exactly how",
    "start": "419740",
    "end": "421720"
  },
  {
    "text": "much longer so that's the question",
    "start": "421720",
    "end": "426340"
  },
  {
    "text": "here are the assumptions we're making",
    "start": "426340",
    "end": "427980"
  },
  {
    "text": "first we've forgotten completely about",
    "start": "427980",
    "end": "430810"
  },
  {
    "text": "the Linux kernel socket buffers to go",
    "start": "430810",
    "end": "432760"
  },
  {
    "text": "around time to schedule are all of that",
    "start": "432760",
    "end": "434260"
  },
  {
    "text": "we've dramatically reduced this mature",
    "start": "434260",
    "end": "436030"
  },
  {
    "text": "relatively complex system to just a",
    "start": "436030",
    "end": "437980"
  },
  {
    "text": "single more or less blackbox model later",
    "start": "437980",
    "end": "440920"
  },
  {
    "text": "we'll see whether that's actually a fair",
    "start": "440920",
    "end": "442720"
  },
  {
    "text": "simplification to make we're also making",
    "start": "442720",
    "end": "446910"
  },
  {
    "text": "these three assumptions first the tests",
    "start": "446910",
    "end": "449770"
  },
  {
    "text": "arrive independently and randomly it's",
    "start": "449770",
    "end": "451900"
  },
  {
    "text": "some average rate that we'll call lambda",
    "start": "451900",
    "end": "453400"
  },
  {
    "text": "say lambda equals 3,000 requests per",
    "start": "453400",
    "end": "455920"
  },
  {
    "text": "second and second that the server takes",
    "start": "455920",
    "end": "458050"
  },
  {
    "text": "a constant time as the service time say",
    "start": "458050",
    "end": "461380"
  },
  {
    "text": "200 microseconds to actually process",
    "start": "461380",
    "end": "464080"
  },
  {
    "text": "each task later we'll revisit these",
    "start": "464080",
    "end": "466480"
  },
  {
    "text": "assumptions and see what happens when we",
    "start": "466480",
    "end": "468160"
  },
  {
    "text": "relax them and finally like we've said",
    "start": "468160",
    "end": "470290"
  },
  {
    "text": "the server can only do one thing at once",
    "start": "470290",
    "end": "471730"
  },
  {
    "text": "and it doesn't preempt once it starts a",
    "start": "471730",
    "end": "473320"
  },
  {
    "text": "task it has to finish it so step 3 you",
    "start": "473320",
    "end": "478300"
  },
  {
    "text": "all have sets phd's right just kidding",
    "start": "478300",
    "end": "483330"
  },
  {
    "text": "to build up our intuition for this",
    "start": "483330",
    "end": "485590"
  },
  {
    "text": "system let's draw ourselves a picture of",
    "start": "485590",
    "end": "487810"
  },
  {
    "text": "how its state evolves over time the",
    "start": "487810",
    "end": "490630"
  },
  {
    "text": "state of the server changes when a task",
    "start": "490630",
    "end": "493330"
  },
  {
    "text": "arrives the server goes from idle to",
    "start": "493330",
    "end": "495310"
  },
  {
    "text": "busy the outstanding work at the server",
    "start": "495310",
    "end": "497710"
  },
  {
    "text": "goes from zero to s then the server",
    "start": "497710",
    "end": "500410"
  },
  {
    "text": "works on the task a other outstanding",
    "start": "500410",
    "end": "502690"
  },
  {
    "text": "work goes down then the server is idle",
    "start": "502690",
    "end": "504130"
  },
  {
    "text": "again another task arrives that's the",
    "start": "504130",
    "end": "506890"
  },
  {
    "text": "same thing happens now intuitively it's",
    "start": "506890",
    "end": "510070"
  },
  {
    "text": "pretty clear that when throughput is low",
    "start": "510070",
    "end": "511870"
  },
  {
    "text": "the server is mostly idle and tasks",
    "start": "511870",
    "end": "514570"
  },
  {
    "text": "almost never have to queue in general",
    "start": "514570",
    "end": "516700"
  },
  {
    "text": "they can be served immediately and",
    "start": "516700",
    "end": "518020"
  },
  {
    "text": "latency will be low",
    "start": "518020",
    "end": "519810"
  },
  {
    "text": "however as throughput increases the",
    "start": "519810",
    "end": "523000"
  },
  {
    "text": "server is busy more of the time and it's",
    "start": "523000",
    "end": "524620"
  },
  {
    "text": "more likely the tasks will have to wait",
    "start": "524620",
    "end": "526480"
  },
  {
    "text": "in this diagram let's try and",
    "start": "526480",
    "end": "528940"
  },
  {
    "text": "distinguish the waiting time for a task",
    "start": "528940",
    "end": "530740"
  },
  {
    "text": "which is blue here from the service time",
    "start": "530740",
    "end": "533470"
  },
  {
    "text": "for a task which is orange",
    "start": "533470",
    "end": "535600"
  },
  {
    "text": "here the test shows up it has to wait",
    "start": "535600",
    "end": "537370"
  },
  {
    "text": "for a little bit and then the server can",
    "start": "537370",
    "end": "538899"
  },
  {
    "text": "work on it so I'm that this picture can",
    "start": "538899",
    "end": "545199"
  },
  {
    "text": "we talk about the metric that we're",
    "start": "545199",
    "end": "546699"
  },
  {
    "text": "interested in which is average wait time",
    "start": "546699",
    "end": "548829"
  },
  {
    "text": "not just for a given arbitrary test but",
    "start": "548829",
    "end": "551800"
  },
  {
    "text": "in general over a large set of tasks how",
    "start": "551800",
    "end": "553899"
  },
  {
    "text": "long will they have to wait there are",
    "start": "553899",
    "end": "557110"
  },
  {
    "text": "two ways that we can identify the",
    "start": "557110",
    "end": "558550"
  },
  {
    "text": "average wait time in this picture the",
    "start": "558550",
    "end": "561430"
  },
  {
    "text": "first is the average weight of these",
    "start": "561430",
    "end": "563500"
  },
  {
    "text": "blue parallelograms the average time",
    "start": "563500",
    "end": "565839"
  },
  {
    "text": "between when a tasks arrives and when",
    "start": "565839",
    "end": "569170"
  },
  {
    "text": "it's ready to be serviced makes sense",
    "start": "569170",
    "end": "573480"
  },
  {
    "text": "but the other way that we can identify",
    "start": "573480",
    "end": "575589"
  },
  {
    "text": "average wait time is is the average",
    "start": "575589",
    "end": "577959"
  },
  {
    "text": "height of the graph remember tests",
    "start": "577959",
    "end": "580720"
  },
  {
    "text": "arrive independently and uniformly at",
    "start": "580720",
    "end": "582550"
  },
  {
    "text": "random so if we were task and we show up",
    "start": "582550",
    "end": "584500"
  },
  {
    "text": "at some random point in time right here",
    "start": "584500",
    "end": "586389"
  },
  {
    "text": "the height of the graph at that point",
    "start": "586389",
    "end": "588130"
  },
  {
    "text": "represents how long we'd have to wait so",
    "start": "588130",
    "end": "591220"
  },
  {
    "text": "the average height represents the",
    "start": "591220",
    "end": "592420"
  },
  {
    "text": "average wait time if this is giving you",
    "start": "592420",
    "end": "595060"
  },
  {
    "text": "like bad high school geometry flashbacks",
    "start": "595060",
    "end": "597009"
  },
  {
    "text": "that's Philly fare but bear with me",
    "start": "597009",
    "end": "598630"
  },
  {
    "text": "because there will be some cool insight",
    "start": "598630",
    "end": "600040"
  },
  {
    "text": "at the end of this little exercise the",
    "start": "600040",
    "end": "603220"
  },
  {
    "text": "idea is that will relate the average",
    "start": "603220",
    "end": "604959"
  },
  {
    "text": "width and the average height using the",
    "start": "604959",
    "end": "606490"
  },
  {
    "text": "area under the graph and then solve to",
    "start": "606490",
    "end": "608560"
  },
  {
    "text": "give us an expression a formula for the",
    "start": "608560",
    "end": "610449"
  },
  {
    "text": "wait time that we can then use to make",
    "start": "610449",
    "end": "612009"
  },
  {
    "text": "predictions so over a long time interval",
    "start": "612009",
    "end": "615220"
  },
  {
    "text": "T the area under the graph is the width",
    "start": "615220",
    "end": "617949"
  },
  {
    "text": "the time span T times the average height",
    "start": "617949",
    "end": "620620"
  },
  {
    "text": "of the graph which we already said was",
    "start": "620620",
    "end": "622420"
  },
  {
    "text": "the average wait time let's call that W",
    "start": "622420",
    "end": "624430"
  },
  {
    "text": "so the area is T times W on the other",
    "start": "624430",
    "end": "627819"
  },
  {
    "text": "hand we have all these tests that are",
    "start": "627819",
    "end": "629800"
  },
  {
    "text": "represented both by a triangle in this",
    "start": "629800",
    "end": "631630"
  },
  {
    "text": "diagram the orange triangle representing",
    "start": "631630",
    "end": "633639"
  },
  {
    "text": "when it's being serviced and a",
    "start": "633639",
    "end": "635410"
  },
  {
    "text": "parallelogram the blue parallelogram",
    "start": "635410",
    "end": "637600"
  },
  {
    "text": "that's representing when the task is",
    "start": "637600",
    "end": "639250"
  },
  {
    "text": "waiting now we can express the",
    "start": "639250",
    "end": "643029"
  },
  {
    "text": "parallelogram area as its height which",
    "start": "643029",
    "end": "646000"
  },
  {
    "text": "is s the service time times its width",
    "start": "646000",
    "end": "648250"
  },
  {
    "text": "which on average is W which you said use",
    "start": "648250",
    "end": "650889"
  },
  {
    "text": "the average weight I'm a little subtle",
    "start": "650889",
    "end": "652689"
  },
  {
    "text": "but we'll see where this gets us so the",
    "start": "652689",
    "end": "656079"
  },
  {
    "text": "area under the graph is the number of",
    "start": "656079",
    "end": "657490"
  },
  {
    "text": "tasks times the area of each triangle",
    "start": "657490",
    "end": "659769"
  },
  {
    "text": "plus the average parallelogram area",
    "start": "659769",
    "end": "662220"
  },
  {
    "text": "which we can express as escrowed over to",
    "start": "662220",
    "end": "665050"
  },
  {
    "text": "the triangle area plus s times w the",
    "start": "665050",
    "end": "667660"
  },
  {
    "text": "parallelogram area",
    "start": "667660",
    "end": "670558"
  },
  {
    "text": "now how many tests are there well over",
    "start": "670649",
    "end": "674500"
  },
  {
    "text": "sometimes Fanty the number of tests is a",
    "start": "674500",
    "end": "677410"
  },
  {
    "text": "function of the arrival rate called",
    "start": "677410",
    "end": "679209"
  },
  {
    "text": "lambda so putting it all together we",
    "start": "679209",
    "end": "681490"
  },
  {
    "text": "have this kind of gnarly expression that",
    "start": "681490",
    "end": "683410"
  },
  {
    "text": "will make a lot more sense in a minute",
    "start": "683410",
    "end": "684760"
  },
  {
    "text": "we know that the area under the graph is",
    "start": "684760",
    "end": "687339"
  },
  {
    "text": "lambda t s squared over 2 plus s times W",
    "start": "687339",
    "end": "691170"
  },
  {
    "text": "which we also know is the area under the",
    "start": "691170",
    "end": "694120"
  },
  {
    "text": "graph t times W ha this is great because",
    "start": "694120",
    "end": "699670"
  },
  {
    "text": "we can relate the two and then solve for",
    "start": "699670",
    "end": "701860"
  },
  {
    "text": "W you might have to trust me on two",
    "start": "701860",
    "end": "703959"
  },
  {
    "text": "algebraic manipulation here but if you",
    "start": "703959",
    "end": "705970"
  },
  {
    "text": "simplify the top equation what you get",
    "start": "705970",
    "end": "708010"
  },
  {
    "text": "is this expression W is equal to lambda",
    "start": "708010",
    "end": "710500"
  },
  {
    "text": "s squared over 2 times 1 minus lambda s",
    "start": "710500",
    "end": "713519"
  },
  {
    "text": "let's bust out our graphing calculators",
    "start": "713519",
    "end": "715870"
  },
  {
    "text": "and draw a picture of what this looks",
    "start": "715870",
    "end": "717130"
  },
  {
    "text": "like this is what it looks like as the",
    "start": "717130",
    "end": "720459"
  },
  {
    "text": "server becomes saturated as the",
    "start": "720459",
    "end": "722560"
  },
  {
    "text": "throughput lambda grows the wait time",
    "start": "722560",
    "end": "724750"
  },
  {
    "text": "also grows without bound",
    "start": "724750",
    "end": "727079"
  },
  {
    "text": "this looks a lot like the picture that",
    "start": "727079",
    "end": "729339"
  },
  {
    "text": "we saw in our experiment and so this is",
    "start": "729339",
    "end": "731230"
  },
  {
    "text": "encouraging even without actually",
    "start": "731230",
    "end": "733270"
  },
  {
    "text": "fitting this to the data that we",
    "start": "733270",
    "end": "734560"
  },
  {
    "text": "gathered we can identify three",
    "start": "734560",
    "end": "736690"
  },
  {
    "text": "utilization regimes you know when",
    "start": "736690",
    "end": "738910"
  },
  {
    "text": "utilization is pretty low there's no",
    "start": "738910",
    "end": "740709"
  },
  {
    "text": "problem if someone comes along and says",
    "start": "740709",
    "end": "742329"
  },
  {
    "text": "hey AB and I think we should run this",
    "start": "742329",
    "end": "743529"
  },
  {
    "text": "thing at like 60 percent utilization",
    "start": "743529",
    "end": "745000"
  },
  {
    "text": "instead that's a conversation that we",
    "start": "745000",
    "end": "747070"
  },
  {
    "text": "can have about cost of performance",
    "start": "747070",
    "end": "748660"
  },
  {
    "text": "trade-offs if someone says hey we should",
    "start": "748660",
    "end": "750550"
  },
  {
    "text": "run this thing at 90 percent utilization",
    "start": "750550",
    "end": "751839"
  },
  {
    "text": "that's probably a bad idea so even this",
    "start": "751839",
    "end": "755110"
  },
  {
    "text": "relatively crude model gives us some",
    "start": "755110",
    "end": "756670"
  },
  {
    "text": "pretty good insights for example if",
    "start": "756670",
    "end": "758050"
  },
  {
    "text": "we're choosing resource limits for our",
    "start": "758050",
    "end": "759370"
  },
  {
    "text": "pods there's a big difference between",
    "start": "759370",
    "end": "761560"
  },
  {
    "text": "going from a little bit of utilization",
    "start": "761560",
    "end": "763630"
  },
  {
    "text": "to a little bit more utilization when",
    "start": "763630",
    "end": "766570"
  },
  {
    "text": "your utilization is low and growing from",
    "start": "766570",
    "end": "768910"
  },
  {
    "text": "utilization to a little bit more",
    "start": "768910",
    "end": "770529"
  },
  {
    "text": "utilization when your utilization is",
    "start": "770529",
    "end": "772089"
  },
  {
    "text": "high the ladder is very very bad so this",
    "start": "772089",
    "end": "776230"
  },
  {
    "text": "is great but so far we've just been",
    "start": "776230",
    "end": "777940"
  },
  {
    "text": "theorizing we don't actually know that",
    "start": "777940",
    "end": "779620"
  },
  {
    "text": "this model correctly describes the data",
    "start": "779620",
    "end": "781930"
  },
  {
    "text": "that we saw in practice I mean it kind",
    "start": "781930",
    "end": "783459"
  },
  {
    "text": "of looks like the same shape same shape",
    "start": "783459",
    "end": "784930"
  },
  {
    "text": "of graph but you know is it good or is",
    "start": "784930",
    "end": "789279"
  },
  {
    "text": "it just some that we made up",
    "start": "789279",
    "end": "790570"
  },
  {
    "text": "well there's a way to assess this",
    "start": "790570",
    "end": "793089"
  },
  {
    "text": "crudely here's what we can do we'll",
    "start": "793089",
    "end": "795699"
  },
  {
    "text": "choose a subset of our real data and the",
    "start": "795699",
    "end": "798160"
  },
  {
    "text": "subset will that we'll choose is the",
    "start": "798160",
    "end": "799779"
  },
  {
    "text": "data that we're actually likely to get",
    "start": "799779",
    "end": "801730"
  },
  {
    "text": "from production",
    "start": "801730",
    "end": "802450"
  },
  {
    "text": "data from the safe regime the low",
    "start": "802450",
    "end": "804940"
  },
  {
    "text": "utilization regime then will fit our",
    "start": "804940",
    "end": "807670"
  },
  {
    "text": "model to those data points well I won't",
    "start": "807670",
    "end": "809920"
  },
  {
    "text": "go into the tedious details of how you",
    "start": "809920",
    "end": "811480"
  },
  {
    "text": "would do this it's pretty simple to do",
    "start": "811480",
    "end": "813190"
  },
  {
    "text": "with any sort of statistical software",
    "start": "813190",
    "end": "815320"
  },
  {
    "text": "package like R or numpy and finally",
    "start": "815320",
    "end": "818589"
  },
  {
    "text": "we'll compare that to the data points",
    "start": "818589",
    "end": "820360"
  },
  {
    "text": "that we've withheld from our training",
    "start": "820360",
    "end": "821980"
  },
  {
    "text": "set if you will and so in this case the",
    "start": "821980",
    "end": "824769"
  },
  {
    "text": "fit is actually strikingly good so this",
    "start": "824769",
    "end": "826899"
  },
  {
    "text": "is encouraging it means that we've made",
    "start": "826899",
    "end": "828399"
  },
  {
    "text": "some radical assumptions but still come",
    "start": "828399",
    "end": "829959"
  },
  {
    "text": "up with a parametric model that pretty",
    "start": "829959",
    "end": "832180"
  },
  {
    "text": "well describes their real system awesome",
    "start": "832180",
    "end": "835980"
  },
  {
    "text": "there are some bigger lessons that we",
    "start": "835980",
    "end": "838149"
  },
  {
    "text": "can take away from this model first in",
    "start": "838149",
    "end": "841269"
  },
  {
    "text": "this type of system and really in any",
    "start": "841269",
    "end": "844000"
  },
  {
    "text": "type of system improving baseline",
    "start": "844000",
    "end": "846279"
  },
  {
    "text": "service time helps a lot if you have a",
    "start": "846279",
    "end": "848980"
  },
  {
    "text": "service and you find some sort of",
    "start": "848980",
    "end": "850990"
  },
  {
    "text": "performance optimization that there's",
    "start": "850990",
    "end": "852850"
  },
  {
    "text": "just straight-up makes it faster that is",
    "start": "852850",
    "end": "855430"
  },
  {
    "text": "the most impactful thing you can do",
    "start": "855430",
    "end": "857970"
  },
  {
    "text": "let's see why that is here's the thought",
    "start": "857970",
    "end": "860589"
  },
  {
    "text": "experiment we find some optimization",
    "start": "860589",
    "end": "862720"
  },
  {
    "text": "that cuts the average service time in",
    "start": "862720",
    "end": "865240"
  },
  {
    "text": "half and people love our service because",
    "start": "865240",
    "end": "867190"
  },
  {
    "text": "it's super fast so now our throughput",
    "start": "867190",
    "end": "868690"
  },
  {
    "text": "doubles because we have more users what",
    "start": "868690",
    "end": "870910"
  },
  {
    "text": "happens to the wait time using our",
    "start": "870910",
    "end": "872980"
  },
  {
    "text": "equation we can see what happens because",
    "start": "872980",
    "end": "875440"
  },
  {
    "text": "there's a factor of s squared in the",
    "start": "875440",
    "end": "877420"
  },
  {
    "text": "numerator of this equation the numerator",
    "start": "877420",
    "end": "879459"
  },
  {
    "text": "is twice as small on the other hand the",
    "start": "879459",
    "end": "881589"
  },
  {
    "text": "changes in the denominator here cancel",
    "start": "881589",
    "end": "883360"
  },
  {
    "text": "each other out so the denominator stays",
    "start": "883360",
    "end": "885279"
  },
  {
    "text": "the same what does that mean even after",
    "start": "885279",
    "end": "888730"
  },
  {
    "text": "we double throughput the wait time is",
    "start": "888730",
    "end": "890740"
  },
  {
    "text": "still improved this is kind of awesome",
    "start": "890740",
    "end": "893260"
  },
  {
    "text": "right if we find some optimization that",
    "start": "893260",
    "end": "895720"
  },
  {
    "text": "lets us cut service time a half we can",
    "start": "895720",
    "end": "897490"
  },
  {
    "text": "double throughput and still be faster",
    "start": "897490",
    "end": "899170"
  },
  {
    "text": "than we were before",
    "start": "899170",
    "end": "900390"
  },
  {
    "text": "alternatively we can more than double",
    "start": "900390",
    "end": "902680"
  },
  {
    "text": "throughput almost triple throughput in",
    "start": "902680",
    "end": "904329"
  },
  {
    "text": "this case and get the same performance",
    "start": "904329",
    "end": "906550"
  },
  {
    "text": "that we had before to me this is really",
    "start": "906550",
    "end": "908740"
  },
  {
    "text": "counterintuitive and really compelling",
    "start": "908740",
    "end": "911019"
  },
  {
    "text": "it says that improving baseline service",
    "start": "911019",
    "end": "913269"
  },
  {
    "text": "time doesn't just improve user-facing",
    "start": "913269",
    "end": "915220"
  },
  {
    "text": "latency it also improves our own",
    "start": "915220",
    "end": "917079"
  },
  {
    "text": "capacity to handle many concurrent",
    "start": "917079",
    "end": "918730"
  },
  {
    "text": "requests so this is a powerful thing to",
    "start": "918730",
    "end": "920740"
  },
  {
    "text": "remember investing in straight-up",
    "start": "920740",
    "end": "922120"
  },
  {
    "text": "performance optimization is maybe the",
    "start": "922120",
    "end": "923560"
  },
  {
    "text": "best use of your time the second lesson",
    "start": "923560",
    "end": "926230"
  },
  {
    "text": "is that variability is bad remember our",
    "start": "926230",
    "end": "929290"
  },
  {
    "text": "tests were arriving at random but if",
    "start": "929290",
    "end": "931209"
  },
  {
    "text": "they just showed up at perfectly spaced",
    "start": "931209",
    "end": "932740"
  },
  {
    "text": "intervals every 200 microseconds or",
    "start": "932740",
    "end": "935140"
  },
  {
    "text": "something",
    "start": "935140",
    "end": "935980"
  },
  {
    "text": "would never have to wait even if the",
    "start": "935980",
    "end": "937840"
  },
  {
    "text": "server is highly utilized unfortunately",
    "start": "937840",
    "end": "939880"
  },
  {
    "text": "that's not what happens in reality so",
    "start": "939880",
    "end": "943090"
  },
  {
    "text": "the slowdown that we see is because our",
    "start": "943090",
    "end": "944770"
  },
  {
    "text": "rivals are variable similarly we can",
    "start": "944770",
    "end": "948550"
  },
  {
    "text": "show that if job sizes are also variable",
    "start": "948550",
    "end": "950890"
  },
  {
    "text": "things get even worse here's a little",
    "start": "950890",
    "end": "953710"
  },
  {
    "text": "empirical experiment we choose two",
    "start": "953710",
    "end": "955810"
  },
  {
    "text": "distributions of task sizes one the",
    "start": "955810",
    "end": "958540"
  },
  {
    "text": "green distribution is all jobs are the",
    "start": "958540",
    "end": "960610"
  },
  {
    "text": "same size the model we started with the",
    "start": "960610",
    "end": "962740"
  },
  {
    "text": "other the average is the same but we",
    "start": "962740",
    "end": "964600"
  },
  {
    "text": "choose from a long tail distribution so",
    "start": "964600",
    "end": "966580"
  },
  {
    "text": "job sizes are variable you can run",
    "start": "966580",
    "end": "969940"
  },
  {
    "text": "through a sort of similar geometric",
    "start": "969940",
    "end": "971080"
  },
  {
    "text": "argument and see that when task sizes",
    "start": "971080",
    "end": "973150"
  },
  {
    "text": "are variable wait time is even worse",
    "start": "973150",
    "end": "975100"
  },
  {
    "text": "than when task sizes are constant so",
    "start": "975100",
    "end": "978510"
  },
  {
    "text": "this again is a powerful lesson for us",
    "start": "978510",
    "end": "980920"
  },
  {
    "text": "as system designers and operators it",
    "start": "980920",
    "end": "982840"
  },
  {
    "text": "means that it behooves us first to",
    "start": "982840",
    "end": "984910"
  },
  {
    "text": "measure the variability in our tasks",
    "start": "984910",
    "end": "987550"
  },
  {
    "text": "using you know instrumentation",
    "start": "987550",
    "end": "989830"
  },
  {
    "text": "techniques like histograms heat maps",
    "start": "989830",
    "end": "991420"
  },
  {
    "text": "distributed tracing for outline eye",
    "start": "991420",
    "end": "993100"
  },
  {
    "text": "analysis and secondly to minimize",
    "start": "993100",
    "end": "995440"
  },
  {
    "text": "variability using design strategies like",
    "start": "995440",
    "end": "997930"
  },
  {
    "text": "batching the variability of a batch of",
    "start": "997930",
    "end": "1000120"
  },
  {
    "text": "requests will always be less than the",
    "start": "1000120",
    "end": "1001710"
  },
  {
    "text": "variability of their requests taken",
    "start": "1001710",
    "end": "1003300"
  },
  {
    "text": "singly so batching helps reduce",
    "start": "1003300",
    "end": "1004950"
  },
  {
    "text": "variability",
    "start": "1004950",
    "end": "1005780"
  },
  {
    "text": "similarly aggressively pre-empting slow",
    "start": "1005780",
    "end": "1008670"
  },
  {
    "text": "tasks or applying time outs can help",
    "start": "1008670",
    "end": "1010590"
  },
  {
    "text": "reduce the impact of outlier tasks on",
    "start": "1010590",
    "end": "1012510"
  },
  {
    "text": "the tests that queue up behind it and",
    "start": "1012510",
    "end": "1014250"
  },
  {
    "text": "finally client back pressure and",
    "start": "1014250",
    "end": "1015840"
  },
  {
    "text": "currency can concurrency control help",
    "start": "1015840",
    "end": "1018240"
  },
  {
    "text": "limit variability in arrival rates in a",
    "start": "1018240",
    "end": "1022470"
  },
  {
    "text": "non-trivial system so this is great",
    "start": "1022470",
    "end": "1027240"
  },
  {
    "text": "we've learned some stuff but we don't",
    "start": "1027240",
    "end": "1029220"
  },
  {
    "text": "actually just have one core or one",
    "start": "1029220",
    "end": "1030930"
  },
  {
    "text": "server we have lots and lots so what can",
    "start": "1030930",
    "end": "1033209"
  },
  {
    "text": "you say about the performance of our big",
    "start": "1033209",
    "end": "1034740"
  },
  {
    "text": "old fleet of servers that's what I want",
    "start": "1034740",
    "end": "1037650"
  },
  {
    "text": "to talk about a bit in the second half",
    "start": "1037650",
    "end": "1039360"
  },
  {
    "text": "of this talk parallel systems so if we",
    "start": "1039360",
    "end": "1042329"
  },
  {
    "text": "know now that one server can handle T",
    "start": "1042329",
    "end": "1044579"
  },
  {
    "text": "requests per second subject to one of",
    "start": "1044579",
    "end": "1046650"
  },
  {
    "text": "our latency SLA s do we need n servers",
    "start": "1046650",
    "end": "1049080"
  },
  {
    "text": "to handle n times T requests per second",
    "start": "1049080",
    "end": "1051710"
  },
  {
    "text": "well it depends on how we assign tasks",
    "start": "1051710",
    "end": "1054270"
  },
  {
    "text": "to servers right there are lots of",
    "start": "1054270",
    "end": "1055890"
  },
  {
    "text": "different ways we could do that we could",
    "start": "1055890",
    "end": "1057870"
  },
  {
    "text": "always assign tasks to the least busy",
    "start": "1057870",
    "end": "1059670"
  },
  {
    "text": "server we could assign tasks randomly",
    "start": "1059670",
    "end": "1061770"
  },
  {
    "text": "and round robin fashion or some other",
    "start": "1061770",
    "end": "1063990"
  },
  {
    "text": "way we could always assign tasks so the",
    "start": "1063990",
    "end": "1065580"
  },
  {
    "text": "most busy server but that's probably a",
    "start": "1065580",
    "end": "1067380"
  },
  {
    "text": "bad idea",
    "start": "1067380",
    "end": "1068100"
  },
  {
    "text": "intuitively it depends",
    "start": "1068100",
    "end": "1069790"
  },
  {
    "text": "so let's do a little simulation this is",
    "start": "1069790",
    "end": "1073810"
  },
  {
    "text": "another performance modeling technique",
    "start": "1073810",
    "end": "1075910"
  },
  {
    "text": "that I find pretty compelling if you",
    "start": "1075910",
    "end": "1077710"
  },
  {
    "text": "don't know how to reason analytically",
    "start": "1077710",
    "end": "1079000"
  },
  {
    "text": "about something you can write a little",
    "start": "1079000",
    "end": "1080200"
  },
  {
    "text": "simulation about it like I cooked up",
    "start": "1080200",
    "end": "1081730"
  },
  {
    "text": "some code for this you could do it too",
    "start": "1081730",
    "end": "1083080"
  },
  {
    "text": "here's the idea we're going to simulate",
    "start": "1083080",
    "end": "1085840"
  },
  {
    "text": "N equals say 16 servers and tasks",
    "start": "1085840",
    "end": "1088900"
  },
  {
    "text": "arriving independently at randomly and",
    "start": "1088900",
    "end": "1090580"
  },
  {
    "text": "then when a new test shows up we'll",
    "start": "1090580",
    "end": "1092890"
  },
  {
    "text": "assign it randomly to a server wait",
    "start": "1092890",
    "end": "1094930"
  },
  {
    "text": "until the server finishes it and then",
    "start": "1094930",
    "end": "1096790"
  },
  {
    "text": "measure the cumulative latency",
    "start": "1096790",
    "end": "1098260"
  },
  {
    "text": "distribution over this workload so",
    "start": "1098260",
    "end": "1100360"
  },
  {
    "text": "sometimes because we're assigning random",
    "start": "1100360",
    "end": "1101860"
  },
  {
    "text": "way a bunch of tests all arrive at the",
    "start": "1101860",
    "end": "1103540"
  },
  {
    "text": "same servers and then at the same server",
    "start": "1103540",
    "end": "1106330"
  },
  {
    "text": "and then those tasks have to wait and so",
    "start": "1106330",
    "end": "1108610"
  },
  {
    "text": "there's a latency tail on the other hand",
    "start": "1108610",
    "end": "1111550"
  },
  {
    "text": "if we take the same workload but as",
    "start": "1111550",
    "end": "1113710"
  },
  {
    "text": "scientists optimally",
    "start": "1113710",
    "end": "1115030"
  },
  {
    "text": "always choose the least busy server then",
    "start": "1115030",
    "end": "1118860"
  },
  {
    "text": "it's much less likely that a task will",
    "start": "1118860",
    "end": "1121810"
  },
  {
    "text": "have to wait we can almost always find",
    "start": "1121810",
    "end": "1123670"
  },
  {
    "text": "an idle server even when most of the",
    "start": "1123670",
    "end": "1125500"
  },
  {
    "text": "servers are busy most of the time and so",
    "start": "1125500",
    "end": "1127630"
  },
  {
    "text": "here we have near constant latency even",
    "start": "1127630",
    "end": "1130150"
  },
  {
    "text": "though the throughput is the same in",
    "start": "1130150",
    "end": "1131620"
  },
  {
    "text": "both systems obviously optimal",
    "start": "1131620",
    "end": "1134050"
  },
  {
    "text": "assignment here is better than random",
    "start": "1134050",
    "end": "1135280"
  },
  {
    "text": "assignment we can reinforce this with a",
    "start": "1135280",
    "end": "1140110"
  },
  {
    "text": "bit of a probabilistic argument if we",
    "start": "1140110",
    "end": "1142600"
  },
  {
    "text": "have one utilization it's a one server",
    "start": "1142600",
    "end": "1144640"
  },
  {
    "text": "at some utilization row so it's busy",
    "start": "1144640",
    "end": "1146620"
  },
  {
    "text": "sixty percent of the time and a new task",
    "start": "1146620",
    "end": "1148690"
  },
  {
    "text": "shows up then the probability that the",
    "start": "1148690",
    "end": "1150910"
  },
  {
    "text": "new test will have to wait is the",
    "start": "1150910",
    "end": "1152800"
  },
  {
    "text": "utilization row sixty percent but if we",
    "start": "1152800",
    "end": "1155650"
  },
  {
    "text": "have n servers at the same utilization",
    "start": "1155650",
    "end": "1158080"
  },
  {
    "text": "rail and a new task shows up the",
    "start": "1158080",
    "end": "1160240"
  },
  {
    "text": "probability that the new task will have",
    "start": "1160240",
    "end": "1161860"
  },
  {
    "text": "to wait is the probability that all",
    "start": "1161860",
    "end": "1163360"
  },
  {
    "text": "servers are busy simultaneously which is",
    "start": "1163360",
    "end": "1165940"
  },
  {
    "text": "much less than row what this means is",
    "start": "1165940",
    "end": "1169240"
  },
  {
    "text": "that in theory if we have many servers",
    "start": "1169240",
    "end": "1171540"
  },
  {
    "text": "higher utilization gives us the same",
    "start": "1171540",
    "end": "1174010"
  },
  {
    "text": "queueing probability if we have n times",
    "start": "1174010",
    "end": "1176560"
  },
  {
    "text": "more traffic",
    "start": "1176560",
    "end": "1177250"
  },
  {
    "text": "we'll need fewer than n times more",
    "start": "1177250",
    "end": "1178930"
  },
  {
    "text": "servers this is awesome right it's like",
    "start": "1178930",
    "end": "1181630"
  },
  {
    "text": "economies of scale and not just because",
    "start": "1181630",
    "end": "1183280"
  },
  {
    "text": "of vendor economies of scale but because",
    "start": "1183280",
    "end": "1185050"
  },
  {
    "text": "of actual physical improvement and the",
    "start": "1185050",
    "end": "1186580"
  },
  {
    "text": "efficiency of the system this is great",
    "start": "1186580",
    "end": "1188500"
  },
  {
    "text": "like the more the more people use our",
    "start": "1188500",
    "end": "1190150"
  },
  {
    "text": "service the cheaper it is to run awesome",
    "start": "1190150",
    "end": "1192270"
  },
  {
    "text": "there's just one problem with this",
    "start": "1192270",
    "end": "1194140"
  },
  {
    "text": "argument we're assuming that we can",
    "start": "1194140",
    "end": "1196090"
  },
  {
    "text": "optimally assign tasks to servers but",
    "start": "1196090",
    "end": "1199120"
  },
  {
    "text": "optimally assigning tasks to a pool of",
    "start": "1199120",
    "end": "1201250"
  },
  {
    "text": "servers is really a coordination problem",
    "start": "1201250",
    "end": "1203180"
  },
  {
    "text": "disguise we all have to agree on which",
    "start": "1203180",
    "end": "1205280"
  },
  {
    "text": "server gets which task and in real life",
    "start": "1205280",
    "end": "1207950"
  },
  {
    "text": "coordination is expensive there's no way",
    "start": "1207950",
    "end": "1210350"
  },
  {
    "text": "around it we can't just magically pick",
    "start": "1210350",
    "end": "1212930"
  },
  {
    "text": "the least busy server with our psychic",
    "start": "1212930",
    "end": "1214850"
  },
  {
    "text": "powers in general we need some sort of",
    "start": "1214850",
    "end": "1216740"
  },
  {
    "text": "physical coordination mechanism for this",
    "start": "1216740",
    "end": "1219440"
  },
  {
    "text": "problem of stateless request assignment",
    "start": "1219440",
    "end": "1222110"
  },
  {
    "text": "that would be a load balancer or proxy",
    "start": "1222110",
    "end": "1223700"
  },
  {
    "text": "or something for similar problems that",
    "start": "1223700",
    "end": "1225830"
  },
  {
    "text": "could be a cluster scheduler for example",
    "start": "1225830",
    "end": "1227830"
  },
  {
    "text": "either way when a test shows up your",
    "start": "1227830",
    "end": "1231200"
  },
  {
    "text": "coordinator has to decide where it wants",
    "start": "1231200",
    "end": "1233180"
  },
  {
    "text": "to assign that task first it has to",
    "start": "1233180",
    "end": "1237200"
  },
  {
    "text": "interrogate all of the backends to",
    "start": "1237200",
    "end": "1238850"
  },
  {
    "text": "figure out which is the best one then it",
    "start": "1238850",
    "end": "1240680"
  },
  {
    "text": "has to actually assign the task to the",
    "start": "1240680",
    "end": "1242330"
  },
  {
    "text": "best chosen back-end this isn't a free",
    "start": "1242330",
    "end": "1245840"
  },
  {
    "text": "process if it takes us some constant",
    "start": "1245840",
    "end": "1248510"
  },
  {
    "text": "time alpha to assign a task say just the",
    "start": "1248510",
    "end": "1251540"
  },
  {
    "text": "time it takes to forward it from the",
    "start": "1251540",
    "end": "1252830"
  },
  {
    "text": "load balancer to the backends and we",
    "start": "1252830",
    "end": "1254660"
  },
  {
    "text": "have n tasks to process then the time it",
    "start": "1254660",
    "end": "1257750"
  },
  {
    "text": "takes us overall to process n tasks in",
    "start": "1257750",
    "end": "1259850"
  },
  {
    "text": "parallel is the assignment time which is",
    "start": "1259850",
    "end": "1262490"
  },
  {
    "text": "alpha times n because you have to do",
    "start": "1262490",
    "end": "1263990"
  },
  {
    "text": "this assignment more or less serially",
    "start": "1263990",
    "end": "1265400"
  },
  {
    "text": "plus the service time which is s the",
    "start": "1265400",
    "end": "1268400"
  },
  {
    "text": "throughput here is a number of tasks n",
    "start": "1268400",
    "end": "1270490"
  },
  {
    "text": "divided by the total time as a graph",
    "start": "1270490",
    "end": "1275510"
  },
  {
    "text": "this looks like this as we try to scale",
    "start": "1275510",
    "end": "1278930"
  },
  {
    "text": "our throughput scales sub linearly we",
    "start": "1278930",
    "end": "1281240"
  },
  {
    "text": "spend more and more of our time just",
    "start": "1281240",
    "end": "1282410"
  },
  {
    "text": "assigning tasks to servers and less and",
    "start": "1282410",
    "end": "1284810"
  },
  {
    "text": "less of our time actually working on the",
    "start": "1284810",
    "end": "1286490"
  },
  {
    "text": "tasks so this is rough and if you can't",
    "start": "1286490",
    "end": "1290930"
  },
  {
    "text": "just assign tasks in constant time if we",
    "start": "1290930",
    "end": "1294410"
  },
  {
    "text": "have to do something that depends on the",
    "start": "1294410",
    "end": "1296240"
  },
  {
    "text": "number of servers for example go talk to",
    "start": "1296240",
    "end": "1297890"
  },
  {
    "text": "each one of them and turn and figure out",
    "start": "1297890",
    "end": "1299210"
  },
  {
    "text": "which one is best",
    "start": "1299210",
    "end": "1300110"
  },
  {
    "text": "then it's even grimmer if the assignment",
    "start": "1300110",
    "end": "1302420"
  },
  {
    "text": "tests grow if the assignment costs grows",
    "start": "1302420",
    "end": "1304400"
  },
  {
    "text": "with n then eventually we spend all of",
    "start": "1304400",
    "end": "1306800"
  },
  {
    "text": "our time just figuring out where to",
    "start": "1306800",
    "end": "1308630"
  },
  {
    "text": "assign tasks and none of our time",
    "start": "1308630",
    "end": "1310490"
  },
  {
    "text": "actually working on them this is one",
    "start": "1310490",
    "end": "1314540"
  },
  {
    "text": "example of what's called the universal",
    "start": "1314540",
    "end": "1316100"
  },
  {
    "text": "scalability law in action the universal",
    "start": "1316100",
    "end": "1318560"
  },
  {
    "text": "scalability law says that the",
    "start": "1318560",
    "end": "1319970"
  },
  {
    "text": "scalability of more or less any parallel",
    "start": "1319970",
    "end": "1322100"
  },
  {
    "text": "computing process looks like this graph",
    "start": "1322100",
    "end": "1324140"
  },
  {
    "text": "you find parameters alpha that describe",
    "start": "1324140",
    "end": "1327050"
  },
  {
    "text": "the constant assignment cost and beta",
    "start": "1327050",
    "end": "1329510"
  },
  {
    "text": "that describe the coordination cost and",
    "start": "1329510",
    "end": "1331490"
  },
  {
    "text": "can model the scalability using this",
    "start": "1331490",
    "end": "1333560"
  },
  {
    "text": "equation unless you can find ways to",
    "start": "1333560",
    "end": "1336320"
  },
  {
    "text": "minimize",
    "start": "1336320",
    "end": "1337040"
  },
  {
    "text": "a coordination cross costs as you try to",
    "start": "1337040",
    "end": "1339380"
  },
  {
    "text": "scale your system it will actually",
    "start": "1339380",
    "end": "1340760"
  },
  {
    "text": "perform worse and worse so this is",
    "start": "1340760",
    "end": "1343610"
  },
  {
    "text": "pretty rough right this is completely",
    "start": "1343610",
    "end": "1345080"
  },
  {
    "text": "contrary to what we saw is we try to",
    "start": "1345080",
    "end": "1346520"
  },
  {
    "text": "scale our system it gets more and more",
    "start": "1346520",
    "end": "1347840"
  },
  {
    "text": "expensive so what we've learned from",
    "start": "1347840",
    "end": "1354260"
  },
  {
    "text": "this is that making scale and variant",
    "start": "1354260",
    "end": "1356420"
  },
  {
    "text": "design decisions is really hard at low",
    "start": "1356420",
    "end": "1359210"
  },
  {
    "text": "parallelism is better for us to try and",
    "start": "1359210",
    "end": "1361430"
  },
  {
    "text": "coordinate because it makes latency more",
    "start": "1361430",
    "end": "1363740"
  },
  {
    "text": "predictable but at high parallelism",
    "start": "1363740",
    "end": "1366040"
  },
  {
    "text": "coordination degrades throughput and",
    "start": "1366040",
    "end": "1367970"
  },
  {
    "text": "we're better off just assigning things",
    "start": "1367970",
    "end": "1369320"
  },
  {
    "text": "randomly otherwise we'll spend all of",
    "start": "1369320",
    "end": "1370970"
  },
  {
    "text": "our time making choices and none of our",
    "start": "1370970",
    "end": "1373010"
  },
  {
    "text": "time actually working on them so can we",
    "start": "1373010",
    "end": "1377450"
  },
  {
    "text": "be clever can we find strategies to",
    "start": "1377450",
    "end": "1379970"
  },
  {
    "text": "trade-off these two fundamental",
    "start": "1379970",
    "end": "1383110"
  },
  {
    "text": "characteristics of system performance",
    "start": "1383110",
    "end": "1384680"
  },
  {
    "text": "and do something that performs pretty",
    "start": "1384680",
    "end": "1386510"
  },
  {
    "text": "well across all spectrums of scale well",
    "start": "1386510",
    "end": "1390950"
  },
  {
    "text": "yes we can so here are two - pretty neat",
    "start": "1390950",
    "end": "1396290"
  },
  {
    "text": "ideas for how to do that the first we do",
    "start": "1396290",
    "end": "1400670"
  },
  {
    "text": "it approximately we know that finding",
    "start": "1400670",
    "end": "1403340"
  },
  {
    "text": "the very best out of n servers is an",
    "start": "1403340",
    "end": "1405590"
  },
  {
    "text": "expensive proposition but choosing one",
    "start": "1405590",
    "end": "1407780"
  },
  {
    "text": "randomly is kind of bad so here's the",
    "start": "1407780",
    "end": "1410060"
  },
  {
    "text": "idea we'll make a trade-off we'll pick",
    "start": "1410060",
    "end": "1412130"
  },
  {
    "text": "two at random",
    "start": "1412130",
    "end": "1413270"
  },
  {
    "text": "we'll compare those two and then we'll",
    "start": "1413270",
    "end": "1415100"
  },
  {
    "text": "always choose the better one of those",
    "start": "1415100",
    "end": "1416780"
  },
  {
    "text": "two what does this what does that give",
    "start": "1416780",
    "end": "1420770"
  },
  {
    "text": "us a lot it has constant overhead right",
    "start": "1420770",
    "end": "1423380"
  },
  {
    "text": "even if we have many many servers",
    "start": "1423380",
    "end": "1424910"
  },
  {
    "text": "because we only have to talk to two and",
    "start": "1424910",
    "end": "1426770"
  },
  {
    "text": "you can show that it improves the",
    "start": "1426770",
    "end": "1428600"
  },
  {
    "text": "instantaneous maximum load on any given",
    "start": "1428600",
    "end": "1431480"
  },
  {
    "text": "server from of log n to O of log log n",
    "start": "1431480",
    "end": "1434630"
  },
  {
    "text": "which is basically o of 1 right this is",
    "start": "1434630",
    "end": "1440990"
  },
  {
    "text": "not just like some theoretical thing",
    "start": "1440990",
    "end": "1442280"
  },
  {
    "text": "that someone made up this technique is",
    "start": "1442280",
    "end": "1443900"
  },
  {
    "text": "used in the sparrow research schedule",
    "start": "1443900",
    "end": "1445910"
  },
  {
    "text": "scheduler and also implemented in hedgy",
    "start": "1445910",
    "end": "1448490"
  },
  {
    "text": "Corpse Nomad to do distributed stateless",
    "start": "1448490",
    "end": "1451280"
  },
  {
    "text": "scheduling with low latency for lots of",
    "start": "1451280",
    "end": "1453470"
  },
  {
    "text": "short tasks like cron jobs and stuff it",
    "start": "1453470",
    "end": "1455720"
  },
  {
    "text": "uses this to two random choices",
    "start": "1455720",
    "end": "1457730"
  },
  {
    "text": "assignment plus some optimizations and I",
    "start": "1457730",
    "end": "1460520"
  },
  {
    "text": "really like this because it's a",
    "start": "1460520",
    "end": "1461930"
  },
  {
    "text": "relatively straightforward very",
    "start": "1461930",
    "end": "1463700"
  },
  {
    "text": "effective strategy it's something that",
    "start": "1463700",
    "end": "1466190"
  },
  {
    "text": "anyone here could cook up unlike I don't",
    "start": "1466190",
    "end": "1468830"
  },
  {
    "text": "know like the kubernetes scheduler which",
    "start": "1468830",
    "end": "1470210"
  },
  {
    "text": "is kind of calm",
    "start": "1470210",
    "end": "1470840"
  },
  {
    "text": "pitted and scary and it's a way way",
    "start": "1470840",
    "end": "1472430"
  },
  {
    "text": "better than a strawman naive random",
    "start": "1472430",
    "end": "1474200"
  },
  {
    "text": "assignment strategy so that's cool the",
    "start": "1474200",
    "end": "1480110"
  },
  {
    "text": "second idea for beating this quadratic",
    "start": "1480110",
    "end": "1482000"
  },
  {
    "text": "beta penalty I call iterative",
    "start": "1482000",
    "end": "1484040"
  },
  {
    "text": "partitioning and it illustrates how the",
    "start": "1484040",
    "end": "1486020"
  },
  {
    "text": "universal scalability law applies not",
    "start": "1486020",
    "end": "1488000"
  },
  {
    "text": "just to this problem of task assignment",
    "start": "1488000",
    "end": "1489740"
  },
  {
    "text": "but to any parallel process here we're",
    "start": "1489740",
    "end": "1492590"
  },
  {
    "text": "going to talk not about stateless",
    "start": "1492590",
    "end": "1494630"
  },
  {
    "text": "services but about stateful services",
    "start": "1494630",
    "end": "1496900"
  },
  {
    "text": "this idea as far as I know was invented",
    "start": "1496900",
    "end": "1499370"
  },
  {
    "text": "at Facebook to empower this analytic",
    "start": "1499370",
    "end": "1503840"
  },
  {
    "text": "system called scuba and is reincarnated",
    "start": "1503840",
    "end": "1506480"
  },
  {
    "text": "in fashion at honeycomb where I work the",
    "start": "1506480",
    "end": "1509480"
  },
  {
    "text": "idea is you have a lot of analytical",
    "start": "1509480",
    "end": "1511400"
  },
  {
    "text": "data instrumentation data from systems",
    "start": "1511400",
    "end": "1513140"
  },
  {
    "text": "and you want to answer questions about",
    "start": "1513140",
    "end": "1514910"
  },
  {
    "text": "it pretty quickly you know which",
    "start": "1514910",
    "end": "1516350"
  },
  {
    "text": "requests are slow or how is my site",
    "start": "1516350",
    "end": "1518090"
  },
  {
    "text": "performance changing over time and the",
    "start": "1518090",
    "end": "1520070"
  },
  {
    "text": "way you do that is by distributing your",
    "start": "1520070",
    "end": "1521990"
  },
  {
    "text": "query over lots of storage nodes so we",
    "start": "1521990",
    "end": "1525080"
  },
  {
    "text": "have a bunch of data we have a bunch of",
    "start": "1525080",
    "end": "1527450"
  },
  {
    "text": "storage nodes each node is responsible",
    "start": "1527450",
    "end": "1529100"
  },
  {
    "text": "for some slice of the data and we want",
    "start": "1529100",
    "end": "1531200"
  },
  {
    "text": "to answer a query so say you know which",
    "start": "1531200",
    "end": "1534170"
  },
  {
    "text": "of our requests was slowest over the",
    "start": "1534170",
    "end": "1535580"
  },
  {
    "text": "past week or something all of the nodes",
    "start": "1535580",
    "end": "1537440"
  },
  {
    "text": "participate in figuring out the answer",
    "start": "1537440",
    "end": "1539780"
  },
  {
    "text": "to that query in parallel so first leaf",
    "start": "1539780",
    "end": "1542990"
  },
  {
    "text": "nodes read a different disk compute",
    "start": "1542990",
    "end": "1544610"
  },
  {
    "text": "their partial results here's my slow",
    "start": "1544610",
    "end": "1546230"
  },
  {
    "text": "score here's my slowest query here's my",
    "start": "1546230",
    "end": "1547760"
  },
  {
    "text": "slowest query and then the aggregator",
    "start": "1547760",
    "end": "1549350"
  },
  {
    "text": "node merges those partial results",
    "start": "1549350",
    "end": "1551090"
  },
  {
    "text": "together and returns a results of the",
    "start": "1551090",
    "end": "1552980"
  },
  {
    "text": "client it's sort of like a MapReduce",
    "start": "1552980",
    "end": "1554450"
  },
  {
    "text": "like thing and so we can choose the",
    "start": "1554450",
    "end": "1557150"
  },
  {
    "text": "level of fan-out here again we can",
    "start": "1557150",
    "end": "1559280"
  },
  {
    "text": "distribute the data across lots and lots",
    "start": "1559280",
    "end": "1561650"
  },
  {
    "text": "and lots of service servers and our",
    "start": "1561650",
    "end": "1563720"
  },
  {
    "text": "question is what level of fan-out is",
    "start": "1563720",
    "end": "1565310"
  },
  {
    "text": "optimal the reason that's a worthwhile",
    "start": "1565310",
    "end": "1571220"
  },
  {
    "text": "question to ask is because there are two",
    "start": "1571220",
    "end": "1573770"
  },
  {
    "text": "parts to the time it actually takes to",
    "start": "1573770",
    "end": "1575660"
  },
  {
    "text": "serve a query with this distributed",
    "start": "1575660",
    "end": "1577490"
  },
  {
    "text": "query system",
    "start": "1577490",
    "end": "1578210"
  },
  {
    "text": "first there's the scan time the time it",
    "start": "1578210",
    "end": "1580550"
  },
  {
    "text": "takes for each leaf node to read its",
    "start": "1580550",
    "end": "1582200"
  },
  {
    "text": "data off of disk and figure out its",
    "start": "1582200",
    "end": "1584210"
  },
  {
    "text": "partial result and then there's an",
    "start": "1584210",
    "end": "1586160"
  },
  {
    "text": "aggregation time the time it takes to",
    "start": "1586160",
    "end": "1588020"
  },
  {
    "text": "merge all those results together the",
    "start": "1588020",
    "end": "1591230"
  },
  {
    "text": "scan time is proportional to the total",
    "start": "1591230",
    "end": "1593750"
  },
  {
    "text": "amount of data divided by the number of",
    "start": "1593750",
    "end": "1595370"
  },
  {
    "text": "leaf nodes that we have so the more leaf",
    "start": "1595370",
    "end": "1597590"
  },
  {
    "text": "nodes we have the better the scan time",
    "start": "1597590",
    "end": "1599330"
  },
  {
    "text": "is the less work each leaf node has to",
    "start": "1599330",
    "end": "1600950"
  },
  {
    "text": "do but the aggregation time is",
    "start": "1600950",
    "end": "1603170"
  },
  {
    "text": "proportional to the number",
    "start": "1603170",
    "end": "1604310"
  },
  {
    "text": "of partial results so the more leaf",
    "start": "1604310",
    "end": "1606170"
  },
  {
    "text": "nodes we have the worse the aggregation",
    "start": "1606170",
    "end": "1609230"
  },
  {
    "text": "time guess putting those two together we",
    "start": "1609230",
    "end": "1613040"
  },
  {
    "text": "get the same scalability graph that we",
    "start": "1613040",
    "end": "1615470"
  },
  {
    "text": "saw before at first as we add more nodes",
    "start": "1615470",
    "end": "1618200"
  },
  {
    "text": "our performance improves and then as we",
    "start": "1618200",
    "end": "1620450"
  },
  {
    "text": "add more nodes our performance starts to",
    "start": "1620450",
    "end": "1622040"
  },
  {
    "text": "degrade so this is pretty rough right it",
    "start": "1622040",
    "end": "1624350"
  },
  {
    "text": "means that we can't scale beyond you",
    "start": "1624350",
    "end": "1626300"
  },
  {
    "text": "know 40 or 50 nodes without actually",
    "start": "1626300",
    "end": "1628910"
  },
  {
    "text": "regressing performance which is a tough",
    "start": "1628910",
    "end": "1631640"
  },
  {
    "text": "proposition if you're trying to be I",
    "start": "1631640",
    "end": "1633050"
  },
  {
    "text": "don't know a web scale or something so",
    "start": "1633050",
    "end": "1636100"
  },
  {
    "text": "here's the idea instead of trying to do",
    "start": "1636100",
    "end": "1639380"
  },
  {
    "text": "all the aggregation at once we'll do it",
    "start": "1639380",
    "end": "1641930"
  },
  {
    "text": "iteratively by fanning out queries",
    "start": "1641930",
    "end": "1643940"
  },
  {
    "text": "across multiple levels we know that the",
    "start": "1643940",
    "end": "1646850"
  },
  {
    "text": "throughput gets worse for a very large",
    "start": "1646850",
    "end": "1648290"
  },
  {
    "text": "fan-out so we'll make the fan-out a",
    "start": "1648290",
    "end": "1650120"
  },
  {
    "text": "constant F and we'll add intermediate",
    "start": "1650120",
    "end": "1652580"
  },
  {
    "text": "aggregators between their root and the",
    "start": "1652580",
    "end": "1654800"
  },
  {
    "text": "leaf nodes now you can work through the",
    "start": "1654800",
    "end": "1658340"
  },
  {
    "text": "math and see that the aggregation time",
    "start": "1658340",
    "end": "1660440"
  },
  {
    "text": "is no longer proportional to the number",
    "start": "1660440",
    "end": "1662870"
  },
  {
    "text": "of leaf nodes it's proportional to the",
    "start": "1662870",
    "end": "1664940"
  },
  {
    "text": "height of our aggregation tree which is",
    "start": "1664940",
    "end": "1668530"
  },
  {
    "text": "proportional to the log of the total",
    "start": "1668530",
    "end": "1671450"
  },
  {
    "text": "number of servers we have participating",
    "start": "1671450",
    "end": "1673190"
  },
  {
    "text": "in this system this might seem a little",
    "start": "1673190",
    "end": "1675170"
  },
  {
    "text": "abstract but what does it mean it means",
    "start": "1675170",
    "end": "1676820"
  },
  {
    "text": "that we as we add more servers we",
    "start": "1676820",
    "end": "1679040"
  },
  {
    "text": "continue to scale by doing this",
    "start": "1679040",
    "end": "1682550"
  },
  {
    "text": "computation iteratively and aggregating",
    "start": "1682550",
    "end": "1684620"
  },
  {
    "text": "iteratively we're able to amortize the",
    "start": "1684620",
    "end": "1686570"
  },
  {
    "text": "aggregation costs over a bunch of",
    "start": "1686570",
    "end": "1688220"
  },
  {
    "text": "servers and continue to scale even to",
    "start": "1688220",
    "end": "1690500"
  },
  {
    "text": "very very large data sets so what have",
    "start": "1690500",
    "end": "1693500"
  },
  {
    "text": "we learned",
    "start": "1693500",
    "end": "1693920"
  },
  {
    "text": "again making scale invariant design",
    "start": "1693920",
    "end": "1696140"
  },
  {
    "text": "decisions is really hard because",
    "start": "1696140",
    "end": "1697850"
  },
  {
    "text": "performance characteristics at low scale",
    "start": "1697850",
    "end": "1699890"
  },
  {
    "text": "and at high scale are very different but",
    "start": "1699890",
    "end": "1702350"
  },
  {
    "text": "with smart compromises we can produce",
    "start": "1702350",
    "end": "1704120"
  },
  {
    "text": "pretty darn good results with randomized",
    "start": "1704120",
    "end": "1706850"
  },
  {
    "text": "choice we can approximate optimal",
    "start": "1706850",
    "end": "1708740"
  },
  {
    "text": "assignment very cheaply with iterative",
    "start": "1708740",
    "end": "1710810"
  },
  {
    "text": "parallelization we can amortize",
    "start": "1710810",
    "end": "1712580"
  },
  {
    "text": "coordination cost across more of our",
    "start": "1712580",
    "end": "1715400"
  },
  {
    "text": "service and the universal scalability",
    "start": "1715400",
    "end": "1717740"
  },
  {
    "text": "line both cases helped us quantify the",
    "start": "1717740",
    "end": "1719480"
  },
  {
    "text": "effect of these choices and figure out",
    "start": "1719480",
    "end": "1721580"
  },
  {
    "text": "how to design better systems before",
    "start": "1721580",
    "end": "1723290"
  },
  {
    "text": "actually building them in conclusion",
    "start": "1723290",
    "end": "1725660"
  },
  {
    "text": "queueing theory not so bad model",
    "start": "1725660",
    "end": "1728300"
  },
  {
    "text": "building isn't some magic thing done by",
    "start": "1728300",
    "end": "1730130"
  },
  {
    "text": "Wizards with PhDs all you have to do is",
    "start": "1730130",
    "end": "1732650"
  },
  {
    "text": "state your goals state the assumptions",
    "start": "1732650",
    "end": "1734810"
  },
  {
    "text": "about the system you're making",
    "start": "1734810",
    "end": "1736650"
  },
  {
    "text": "and don't be afraid you can bust out a",
    "start": "1736650",
    "end": "1739200"
  },
  {
    "text": "textbook and do a bunch of math but you",
    "start": "1739200",
    "end": "1741210"
  },
  {
    "text": "can also draw a picture to reason about",
    "start": "1741210",
    "end": "1742650"
  },
  {
    "text": "the system you can implement a",
    "start": "1742650",
    "end": "1744600"
  },
  {
    "text": "simulation to help discover what happens",
    "start": "1744600",
    "end": "1747300"
  },
  {
    "text": "as load changes without actually",
    "start": "1747300",
    "end": "1749580"
  },
  {
    "text": "exercising your system at massive scale",
    "start": "1749580",
    "end": "1752960"
  },
  {
    "text": "when modeling latency versus throughput",
    "start": "1752960",
    "end": "1755400"
  },
  {
    "text": "it's essential to measure and to",
    "start": "1755400",
    "end": "1757530"
  },
  {
    "text": "minimize variability and to be aware of",
    "start": "1757530",
    "end": "1759720"
  },
  {
    "text": "unbounded hues which produce unbounded",
    "start": "1759720",
    "end": "1761640"
  },
  {
    "text": "latency and finally the best way to have",
    "start": "1761640",
    "end": "1763980"
  },
  {
    "text": "more capacity is to do less work in the",
    "start": "1763980",
    "end": "1766140"
  },
  {
    "text": "first place when modeling scalability",
    "start": "1766140",
    "end": "1768990"
  },
  {
    "text": "it's essential to remember that",
    "start": "1768990",
    "end": "1770370"
  },
  {
    "text": "coordination is expensive but that you",
    "start": "1770370",
    "end": "1772800"
  },
  {
    "text": "can express and predict its costs with",
    "start": "1772800",
    "end": "1774600"
  },
  {
    "text": "the universal scalability law if you",
    "start": "1774600",
    "end": "1776700"
  },
  {
    "text": "have something that you need to scale",
    "start": "1776700",
    "end": "1778440"
  },
  {
    "text": "and you can't quite quite make it work",
    "start": "1778440",
    "end": "1780270"
  },
  {
    "text": "consider consider some tricks like",
    "start": "1780270",
    "end": "1783420"
  },
  {
    "text": "randomized approximation and iterative",
    "start": "1783420",
    "end": "1785250"
  },
  {
    "text": "partitioning that's all I have thank you",
    "start": "1785250",
    "end": "1787470"
  },
  {
    "text": "very much",
    "start": "1787470",
    "end": "1789640"
  },
  {
    "text": "[Applause]",
    "start": "1789640",
    "end": "1794589"
  }
]