[
  {
    "text": "my name is cyprian Hackman I'm a software engineer at Microsoft I work on",
    "start": "80",
    "end": "5240"
  },
  {
    "text": "an internal platform so related to things like this um I'm also a",
    "start": "5240",
    "end": "11960"
  },
  {
    "text": "maintainer for various projects inside the kubernetes ecosystem um and uh usually do talks",
    "start": "11960",
    "end": "20920"
  },
  {
    "text": "with Justin on such topics hi there my name is uh Justin santabarbara I am a software engineer at",
    "start": "20920",
    "end": "27679"
  },
  {
    "text": "Google I've been in the kubernetes ecosystem for a long time uh also involved a number of projects but um",
    "start": "27679",
    "end": "34399"
  },
  {
    "text": "primarily CPR and I work together on the chops project um we've been adding bare metal support uh to that um but we've",
    "start": "34399",
    "end": "40760"
  },
  {
    "text": "also discovered this is a broader topic and so we want to talk about it with you um don't worry this is not the last",
    "start": "40760",
    "end": "46760"
  },
  {
    "text": "slide uh but we did put it in here um this is a maintainer track talk and we",
    "start": "46760",
    "end": "52399"
  },
  {
    "text": "do you know so we're not going to stand up here for 25 minutes or 30 minutes and lecture you about you know everything we've been building and how it's the",
    "start": "52399",
    "end": "58280"
  },
  {
    "text": "only one true way uh we want to find out you know what are you doing what would you like to see uh whether it's in chops",
    "start": "58280",
    "end": "65439"
  },
  {
    "text": "or cluster API or in the kubernetes project itself so that it lands in eks",
    "start": "65439",
    "end": "70840"
  },
  {
    "text": "AKs gke whatever it is you know just the whole topic um obviously you know metal",
    "start": "70840",
    "end": "76600"
  },
  {
    "text": "is not necessarily going to land in gke but we can least talk about those sorts of things so thank you in advance for",
    "start": "76600",
    "end": "82040"
  },
  {
    "text": "your participation we will be roaming through the audience so please have your",
    "start": "82040",
    "end": "87159"
  },
  {
    "text": "speaking hats on uh and be ready to",
    "start": "87159",
    "end": "91920"
  },
  {
    "text": "participate okay so quick survey what is metal so which one of you use kubernetes",
    "start": "93880",
    "end": "102159"
  },
  {
    "text": "on metal so okay nice quite a lot of",
    "start": "102159",
    "end": "107880"
  },
  {
    "text": "hands so what is metal for you right it's a",
    "start": "107880",
    "end": "114079"
  },
  {
    "text": "it's a very controversial topic could be your own machine in your data centers",
    "start": "114079",
    "end": "121360"
  },
  {
    "text": "could be collocated data center could be also a cloud providers instances that",
    "start": "121360",
    "end": "127960"
  },
  {
    "text": "you're not using some managed product or the cloud part so how do you how do you",
    "start": "127960",
    "end": "133959"
  },
  {
    "text": "think about the cloud",
    "start": "133959",
    "end": "138640"
  },
  {
    "text": "anyone I can go first I I for example I'm running a handful of um of machines",
    "start": "140360",
    "end": "145680"
  },
  {
    "text": "physical machines in my basement at home uh in the winter months it keeps warm um",
    "start": "145680",
    "end": "151239"
  },
  {
    "text": "and I would like to run kubernetes on those it's it's about five machines right now and growing but I also you know I'm playing with AI so gpus are a",
    "start": "151239",
    "end": "158000"
  },
  {
    "text": "big thing for for",
    "start": "158000",
    "end": "160840"
  },
  {
    "text": "me I said Cloud equal easy world so basically on bare metal you have a lot",
    "start": "163440",
    "end": "168879"
  },
  {
    "text": "of stuff to do to to deliver your kubernetes on it in the cloud not in",
    "start": "168879",
    "end": "174280"
  },
  {
    "text": "Cloud you already have all solution around it",
    "start": "174280",
    "end": "180159"
  },
  {
    "text": "so how easy how easy can we make it right I think that's a great one and we're going to talk a lot about the challenges um anyone else want to uh",
    "start": "180159",
    "end": "185599"
  },
  {
    "text": "speak up or should we go into some of those Justin",
    "start": "185599",
    "end": "190920"
  },
  {
    "text": "sorry uh yeah sure so um about Bal I'm running Q since 28 on on bare metal and",
    "start": "190920",
    "end": "198080"
  },
  {
    "text": "I experienced a lot of different stuff um started with Cube spray on machines",
    "start": "198080",
    "end": "204760"
  },
  {
    "text": "in the office moved over to named packet net meanwhile Phoenix",
    "start": "204760",
    "end": "210760"
  },
  {
    "text": "metal um yeah running a bunch of raspberry pies at home which it's bare metal for",
    "start": "210760",
    "end": "216599"
  },
  {
    "text": "me and yeah I also experienced a lot of pain there tried kops",
    "start": "216599",
    "end": "223640"
  },
  {
    "text": "on um open stack there you get your private",
    "start": "223640",
    "end": "231439"
  },
  {
    "text": "Cloud but open stack hurts so",
    "start": "231439",
    "end": "236720"
  },
  {
    "text": "yeah wonderful thank you thank you for sharing we will be talking a little bit more about that topic open stack or the",
    "start": "236720",
    "end": "242280"
  },
  {
    "text": "the Bas layer in a little bit anyone else or should we jump in it's hard to see at the very back if anyone but raise your hand if you want",
    "start": "242280",
    "end": "247920"
  },
  {
    "text": "to all right I go for it I might take the stairs this time so",
    "start": "247920",
    "end": "255239"
  },
  {
    "text": "we we did we did a um we do have a plan for how we suggest uh going through this",
    "start": "255239",
    "end": "261199"
  },
  {
    "text": "um the primary reason being that when we sort of talk through it answering the questions in this way uh sort of helps",
    "start": "261199",
    "end": "267759"
  },
  {
    "text": "us guide uh the future answers so you know one of the first things we're going to talk about is uh ETD volumes and",
    "start": "267759",
    "end": "273680"
  },
  {
    "text": "failover and how we deal about that on bare metal um and that informs you know a bunch of the later decisions so that's",
    "start": "273680",
    "end": "280680"
  },
  {
    "text": "this is the proposal if anyone disagrees vehemently stick your hand up high in the air but uh otherwise this seemed to",
    "start": "280680",
    "end": "286680"
  },
  {
    "text": "saw someone movement saw otherwise this this flow hopefully will will work",
    "start": "286680",
    "end": "292080"
  },
  {
    "text": "well do uh um",
    "start": "293759",
    "end": "302160"
  },
  {
    "text": "so first thing is uh with kubernetes is that you have to have at CD right if you",
    "start": "302160",
    "end": "309000"
  },
  {
    "text": "don't have atcd there's no kubernetes um so Cube API server is",
    "start": "309000",
    "end": "317520"
  },
  {
    "text": "basically a wrapper around that CD it's a it provides authentication",
    "start": "317520",
    "end": "324440"
  },
  {
    "text": "authorization and various other nice things but the main part is",
    "start": "324440",
    "end": "331000"
  },
  {
    "text": "there so if you want to run a real cluster you",
    "start": "331000",
    "end": "338280"
  },
  {
    "text": "need redundancy right etcd gives you that um but it's uh it's painful you can",
    "start": "338280",
    "end": "346080"
  },
  {
    "text": "run with three five nodes with local storage static",
    "start": "346080",
    "end": "351400"
  },
  {
    "text": "addresses um can have backups for Disaster Recovery or you can also have",
    "start": "351400",
    "end": "357680"
  },
  {
    "text": "just a single node you know like play cluster or you can you know pray that it",
    "start": "357680",
    "end": "364639"
  },
  {
    "text": "never breaks okay um you could also use NFS",
    "start": "364639",
    "end": "370840"
  },
  {
    "text": "ice cazzy or any kind of storage for bare metal but the storage is kind of",
    "start": "370840",
    "end": "378680"
  },
  {
    "text": "hard like one of the hardest things in bare metal if you get that",
    "start": "378680",
    "end": "385120"
  },
  {
    "text": "right everything is easy so",
    "start": "385120",
    "end": "390400"
  },
  {
    "text": "we've been using at CD with um local storage also as long as you have three",
    "start": "390400",
    "end": "397840"
  },
  {
    "text": "or five replicas you can lose one at a time so unless you have a major event is",
    "start": "397840",
    "end": "403560"
  },
  {
    "text": "not so bad you can also have periodic backups which should allow you to",
    "start": "403560",
    "end": "410120"
  },
  {
    "text": "recover if you do it fast enough but it involves a lot of work",
    "start": "410120",
    "end": "417479"
  },
  {
    "text": "manual work so is anyone going to run something more",
    "start": "417479",
    "end": "423919"
  },
  {
    "text": "than a fixed set of nodes with repair replacement how do you plan to deal with",
    "start": "423919",
    "end": "429720"
  },
  {
    "text": "that CD or do you do that right now any pain points",
    "start": "429720",
    "end": "435680"
  },
  {
    "text": "any I mean I'll just mention also like this is like one of the things that we did in in chops and I think in a bunch",
    "start": "435680",
    "end": "441280"
  },
  {
    "text": "of the manage providers is the automated replacement of failed CD nodes um but",
    "start": "441280",
    "end": "447440"
  },
  {
    "text": "it's not clear on bare metal that that is something that can be done should be",
    "start": "447440",
    "end": "452879"
  },
  {
    "text": "done or whether we basically are like you know I'm going to run with five because I'm willing to tolerate that you",
    "start": "452879",
    "end": "458560"
  },
  {
    "text": "know that that I can I want to lose two nodes because I know I'm going to have to physically go in and repair those",
    "start": "458560",
    "end": "463759"
  },
  {
    "text": "nodes there's no automated middle of the night repair process on on my bare metal is is that assumption does anyone",
    "start": "463759",
    "end": "470440"
  },
  {
    "text": "disagree with that assumption why anyone like to I see first disagreement that's excellent that's why we're",
    "start": "470440",
    "end": "477840"
  },
  {
    "text": "here the only thing I would mention is we might want to take a leaf out of what we see in the storage world and consider",
    "start": "478080",
    "end": "484599"
  },
  {
    "text": "a passive spare where you have three eligible members and a member on the",
    "start": "484599",
    "end": "490159"
  },
  {
    "text": "side that can be built up in an automated fashion if you lose one of them if you're not willing to go to Five",
    "start": "490159",
    "end": "496120"
  },
  {
    "text": "to get failure out of ETD or if you have different types of roles that might share a",
    "start": "496120",
    "end": "503039"
  },
  {
    "text": "spare that is an excellent excellent point yes and I think ETD is adding support for I think they call them",
    "start": "503039",
    "end": "508120"
  },
  {
    "text": "Learners is that right I think it's a term like that um and that will be very valuable there and",
    "start": "508120",
    "end": "514839"
  },
  {
    "text": "might make the maintenance easier thank you any other any other thoughts on this particular topic if we can if we can",
    "start": "514839",
    "end": "520719"
  },
  {
    "text": "basically agree that uh with 3 + 1 we can work in this mode we unlock all the",
    "start": "520719",
    "end": "527920"
  },
  {
    "text": "rest of the road map so any more disagreement here we go",
    "start": "527920",
    "end": "535360"
  },
  {
    "text": "uh um mostly just alternate option for uh Etc backup in instead of just object",
    "start": "537240",
    "end": "543399"
  },
  {
    "text": "storage sorry instead of just object storage could you know could be database",
    "start": "543399",
    "end": "549360"
  },
  {
    "text": "backup alternate database backup not just",
    "start": "549360",
    "end": "555440"
  },
  {
    "text": "um great Point absolutely yeah thank you I'm going I think yes the we'll talk a little bit about authentication as well",
    "start": "555519",
    "end": "561279"
  },
  {
    "text": "to other things",
    "start": "561279",
    "end": "564720"
  },
  {
    "text": "oh this isn't so much a disagreement but I happen to be at the at CD uh special",
    "start": "568360",
    "end": "573880"
  },
  {
    "text": "interest group meeting at lunch today and one of the topics that came up I guess is that red hat have been",
    "start": "573880",
    "end": "580120"
  },
  {
    "text": "requested and now support a four node at CD uh configuration so you don't get the",
    "start": "580120",
    "end": "586560"
  },
  {
    "text": "backup for disaster recovery but if you have two separated sort of centers you",
    "start": "586560",
    "end": "592200"
  },
  {
    "text": "have a mirror now so you'd have two in each rather than um having two in one and one in the other or vice versa so",
    "start": "592200",
    "end": "599680"
  },
  {
    "text": "just an observation of something that was kind of a passing comment thank you so much yeah I mean",
    "start": "599680",
    "end": "605120"
  },
  {
    "text": "there is a there's a ETD operator working group or group Sig actually I guess that is uh looking at all of these",
    "start": "605120",
    "end": "611480"
  },
  {
    "text": "things and hopefully we can leverage what they're",
    "start": "611480",
    "end": "615000"
  },
  {
    "text": "building uh this is related to the same question what he asked so instead of doing uh 135 uh we are also looking into",
    "start": "616720",
    "end": "624560"
  },
  {
    "text": "the options of doing just two hcd right because of the operational",
    "start": "624560",
    "end": "629839"
  },
  {
    "text": "recovery needs where you have two nearby data centers right keeping one node in one",
    "start": "629839",
    "end": "637120"
  },
  {
    "text": "side and one not on the other side so what's your thought on that I mean I my understanding is that",
    "start": "637120",
    "end": "644320"
  },
  {
    "text": "historically ETD does not do well with even numbers of nodes so they encourage",
    "start": "644320",
    "end": "650079"
  },
  {
    "text": "three um the the learner mode the plus one is another like modification on that",
    "start": "650079",
    "end": "656040"
  },
  {
    "text": "I don't know how well 1+ one works with an",
    "start": "656040",
    "end": "662839"
  },
  {
    "text": "Arbiter it's clever yes so like taking a dependency on something cloud-based or",
    "start": "663440",
    "end": "669839"
  },
  {
    "text": "another site to help yeah it sounds like broadly that we we like the general",
    "start": "669839",
    "end": "675720"
  },
  {
    "text": "model and we're tweaking the the number of nodes and the configuration of the nodes in the failover which is",
    "start": "675720",
    "end": "682000"
  },
  {
    "text": "great and theme music thank you the uh all right should we go to the next top oh sorry Sam",
    "start": "682000",
    "end": "690240"
  },
  {
    "text": "uh I've seen a variety of cases in which you actually have to move at CD entirely um and sometimes the network addressing",
    "start": "691519",
    "end": "698880"
  },
  {
    "text": "of that changes uh and it gets really complicated if you have to put uh essentially one replica in a different",
    "start": "698880",
    "end": "706519"
  },
  {
    "text": "network environment for a time um and then eventually move the rest so um in",
    "start": "706519",
    "end": "712519"
  },
  {
    "text": "the context of failover and the long-term life cycle of bare metal environments especially more",
    "start": "712519",
    "end": "717839"
  },
  {
    "text": "sophisticated ones uh you may end up with exotic cases such as that and I",
    "start": "717839",
    "end": "722959"
  },
  {
    "text": "don't know whether our tooling makes that easy yet I've seen cases where it certainly did",
    "start": "722959",
    "end": "729079"
  },
  {
    "text": "not I definitely agree that I don't think I've never seen any tooling that makes that easy I agree with you 100%",
    "start": "729360",
    "end": "734600"
  },
  {
    "text": "it's a good it's a good shout out should we move on to the next topic or think I'm giving this ill advisedly",
    "start": "734600",
    "end": "741160"
  },
  {
    "text": "got to run up here so this is sort of related actually um on the topic of how",
    "start": "741160",
    "end": "746199"
  },
  {
    "text": "we would go and discover at CD like um you want to roam you want to be the",
    "start": "746199",
    "end": "753320"
  },
  {
    "text": "runner today yeah the um ETD nodes have to find each other uh",
    "start": "753320",
    "end": "762839"
  },
  {
    "text": "the problem that we have specifically at this layer is this is before kubernetes has come up at all so this is has always",
    "start": "762839",
    "end": "769839"
  },
  {
    "text": "been a big problem to figure out uh the thing that we would like to have that we",
    "start": "769839",
    "end": "775600"
  },
  {
    "text": "get from clouds is something like an elastic endpoint um um that might be an",
    "start": "775600",
    "end": "780680"
  },
  {
    "text": "elastic IP it might be a low balancer uh on something that works uh",
    "start": "780680",
    "end": "787639"
  },
  {
    "text": "in chops or can work on bare metal is like you know modification of etsy",
    "start": "787639",
    "end": "792920"
  },
  {
    "text": "hosts uh you could use Dynamic DNS if you're willing to take a dependency on a DNS server which of course Mayer may not",
    "start": "792920",
    "end": "798800"
  },
  {
    "text": "be in your local environment um but might you know be out outside in a nice",
    "start": "798800",
    "end": "804760"
  },
  {
    "text": "way um and I think that the discussion here is like is it okay to at least",
    "start": "804760",
    "end": "811360"
  },
  {
    "text": "start with static IP addresses um do we want to take a dependency on",
    "start": "811360",
    "end": "818360"
  },
  {
    "text": "DNS uh like when people think about bare metal how important is being able to survive you know a network",
    "start": "818360",
    "end": "824760"
  },
  {
    "text": "disconnectivity event or it doesn't not matter because you're already running locally any want have anything on that",
    "start": "824760",
    "end": "830920"
  },
  {
    "text": "please raise your hands if you do see Sam again thank you",
    "start": "830920",
    "end": "838000"
  },
  {
    "text": "um at home I'm sure I could survive a bit of downtime uh in my day job no I'd rather not so I will be using static",
    "start": "841000",
    "end": "847360"
  },
  {
    "text": "addresses to the greater extent as possible um however there are cases",
    "start": "847360",
    "end": "852519"
  },
  {
    "text": "where I would prefer something like a VIP um which is not too far from an elastic IP so um it depends on the",
    "start": "852519",
    "end": "860040"
  },
  {
    "text": "capabilities of my network at that time so",
    "start": "860040",
    "end": "865120"
  },
  {
    "text": "I guess one of the questions I have is are you distinguishing between a bootstrapping condition and an",
    "start": "873079",
    "end": "879880"
  },
  {
    "text": "operational condition because it seems like how you choose that path of you need one to start with and then that",
    "start": "879880",
    "end": "886399"
  },
  {
    "text": "could potentially get you running to then drive an operational to expand to mini and once you're to mini then you're",
    "start": "886399",
    "end": "892519"
  },
  {
    "text": "less of an issue so is is this a you know an actual or is this a",
    "start": "892519",
    "end": "900320"
  },
  {
    "text": "sequencing issue I I think it's a great topic right like I I think we should focus on the uh end State and think",
    "start": "900320",
    "end": "908560"
  },
  {
    "text": "about then separately how we can get there in the easiest way like ideally we can get to the end State without introducing a second bootstrap",
    "start": "908560",
    "end": "914800"
  },
  {
    "text": "technology but yes a great a great Point like but in the in the long run when",
    "start": "914800",
    "end": "920079"
  },
  {
    "text": "things are uh working normally but sometimes maybe an ETD note is failing",
    "start": "920079",
    "end": "925240"
  },
  {
    "text": "like are you okay with you know static IP addresses perhaps virtual or semi",
    "start": "925240",
    "end": "930839"
  },
  {
    "text": "dynamic or Layer Two IP addresses um but yeah thank you that is a great topic like the the ability to bootst this is",
    "start": "930839",
    "end": "937560"
  },
  {
    "text": "very very interesting anyone else like it's difficult to see from up here oh right",
    "start": "937560",
    "end": "944199"
  },
  {
    "text": "is that",
    "start": "944199",
    "end": "947120"
  },
  {
    "text": "there there was um there was a conversation a couple days ago about",
    "start": "956639",
    "end": "961759"
  },
  {
    "text": "cries on the edge here and one of the use cases that they were talking about was ketes clusters on like shipping",
    "start": "961759",
    "end": "969680"
  },
  {
    "text": "vessels that only have like a 4G radio",
    "start": "969680",
    "end": "975800"
  },
  {
    "text": "maybe and in those kinds of cases I don't think they could rely on an external DNS",
    "start": "975800",
    "end": "980920"
  },
  {
    "text": "anyway and they probably don't have an internal one either great point we never even",
    "start": "980920",
    "end": "987480"
  },
  {
    "text": "mentioned uh Edge but is certainly a big one for for metal um so yeah thank",
    "start": "987480",
    "end": "993519"
  },
  {
    "text": "you for making Zan run I like",
    "start": "993519",
    "end": "997839"
  },
  {
    "text": "this uh yeah point to this uh as we all know it's always DS so um how to skip",
    "start": "1001319",
    "end": "1009519"
  },
  {
    "text": "this probably on the private um even running an DNS server on its own I was",
    "start": "1009519",
    "end": "1016079"
  },
  {
    "text": "thinking about host name announcing I don't something DHCP Style Reverse",
    "start": "1016079",
    "end": "1022160"
  },
  {
    "text": "announcing names but then you're relying on D um DHCP and um your own DNS server",
    "start": "1022160",
    "end": "1028480"
  },
  {
    "text": "which could also fail so more redundancy there um probably static addresses",
    "start": "1028480",
    "end": "1033798"
  },
  {
    "text": "seeding into at CD host maybe 8 yeah that's probably the only way which could",
    "start": "1033799",
    "end": "1039240"
  },
  {
    "text": "be sufficient failed safe enough with less resources than the other ones yeah thank you maybe one more and",
    "start": "1039240",
    "end": "1048280"
  },
  {
    "text": "then we yeah we want to get we're not going to get through many of our list we weren't expecting to get through the whole list but yeah we yeah I just want",
    "start": "1048280",
    "end": "1054559"
  },
  {
    "text": "to add um sorry um similar to like that dependency thing is um I would also try",
    "start": "1054559",
    "end": "1061720"
  },
  {
    "text": "to avoid as much dependencies on external systems as possible because there's already the dependency on disk",
    "start": "1061720",
    "end": "1069360"
  },
  {
    "text": "um and from my experience if you have a multi multi node SD cluster but all",
    "start": "1069360",
    "end": "1076840"
  },
  {
    "text": "depend on the same backing storage like something like an NFS server or something like that and that one goes",
    "start": "1076840",
    "end": "1083320"
  },
  {
    "text": "bad or gets very slow that could also crash your cluster then",
    "start": "1083320",
    "end": "1089200"
  },
  {
    "text": "so sounds like a lot of consensus around IP addresses with perhaps a little bit of bootstrapping mechanisms virtual IP",
    "start": "1090679",
    "end": "1098080"
  },
  {
    "text": "addresses floating IPS internally thank you this is this is great we're making good progress I think like",
    "start": "1098080",
    "end": "1105120"
  },
  {
    "text": "this okay so next we're on to Discovery for cube API server addresses right um",
    "start": "1105120",
    "end": "1114480"
  },
  {
    "text": "that's one of the smaller problems but uh there are still various issues so",
    "start": "1114480",
    "end": "1123639"
  },
  {
    "text": "um we can run this on kubernetes um we originally used the DNS",
    "start": "1124679",
    "end": "1132200"
  },
  {
    "text": "but uh has trended towards IP addresses these days um the works but it's um it's",
    "start": "1132200",
    "end": "1141880"
  },
  {
    "text": "still too hard you you have long delays between when",
    "start": "1141880",
    "end": "1149080"
  },
  {
    "text": "you change something and when you actually get it for all your",
    "start": "1149080",
    "end": "1155440"
  },
  {
    "text": "nodes um multiple IP addresses is very hard um and usually we use a load",
    "start": "1155440",
    "end": "1163280"
  },
  {
    "text": "balancer to to address these issues uh for metal DNS is is nice uh but usually",
    "start": "1163280",
    "end": "1171520"
  },
  {
    "text": "means uh an internet dependency or you have to configure your own um layer",
    "start": "1171520",
    "end": "1178919"
  },
  {
    "text": "three load balancing with metal lb um might work or Cloud load",
    "start": "1178919",
    "end": "1185600"
  },
  {
    "text": "balancer like Cloud flare but you have to pay for that and accept this external",
    "start": "1185600",
    "end": "1193760"
  },
  {
    "text": "dependencies um so do you already have DNS load back bcers should we use",
    "start": "1193760",
    "end": "1201799"
  },
  {
    "text": "IPS how how do you see this becoming easier or",
    "start": "1201799",
    "end": "1209200"
  },
  {
    "text": "better uh yeah therefore that we already tackled at CD with static IPS I would assume you the same because of we",
    "start": "1211720",
    "end": "1219080"
  },
  {
    "text": "already more or less know these same IP addresses as long we are assuming that thatd is running on these servers on the",
    "start": "1219080",
    "end": "1225480"
  },
  {
    "text": "Masters um yeah I was shortly thinking about bgp",
    "start": "1225480",
    "end": "1230960"
  },
  {
    "text": "magic but I think this could get",
    "start": "1230960",
    "end": "1235840"
  },
  {
    "text": "crazy I think that's a good point we' seem to have this consensus on IP addresses just a minute ago so that is",
    "start": "1236280",
    "end": "1241600"
  },
  {
    "text": "helpful there is a long-standing bug about multiple IP addresses in Cube config it's just not supported today and",
    "start": "1241600",
    "end": "1248320"
  },
  {
    "text": "so like personally I've found DNS to be a nice little hack around that um but uh",
    "start": "1248320",
    "end": "1254840"
  },
  {
    "text": "yeah maybe we can have a virtual IP or whatever it is anyone else want to add anything there we",
    "start": "1254840",
    "end": "1262120"
  },
  {
    "text": "go so in cluster API Al so I'm I'm from cluster API so in cluster API they're",
    "start": "1264880",
    "end": "1270760"
  },
  {
    "text": "often similar to metal lb is Cube VI used a static B with RS besides",
    "start": "1270760",
    "end": "1276880"
  },
  {
    "text": "um at CD then basically and um yeah then they try to fight one who's the the",
    "start": "1276880",
    "end": "1284039"
  },
  {
    "text": "leader or the pro provides the IP",
    "start": "1284039",
    "end": "1289000"
  },
  {
    "text": "so same same uh topic with people it CD",
    "start": "1291320",
    "end": "1297279"
  },
  {
    "text": "SD so I think it depends on business and budget option of course and Magic World",
    "start": "1297279",
    "end": "1304400"
  },
  {
    "text": "Trade off but uh if emergency uh we use",
    "start": "1304400",
    "end": "1310400"
  },
  {
    "text": "static IP because all DNS server will",
    "start": "1310400",
    "end": "1315600"
  },
  {
    "text": "when happen we don't know when it um dying or subers so we basically use",
    "start": "1315600",
    "end": "1325320"
  },
  {
    "text": "Hostess and um um sorry covering DNS raying so there",
    "start": "1325320",
    "end": "1334400"
  },
  {
    "text": "are U bottom to top so use Hostess and raying DNS and use root balancer and use",
    "start": "1334400",
    "end": "1344159"
  },
  {
    "text": "VI also and setting on top of there uh we use a physical Ray so raying two two",
    "start": "1344159",
    "end": "1352640"
  },
  {
    "text": "network interface or and use two gateways that's all budgets so it's",
    "start": "1352640",
    "end": "1359159"
  },
  {
    "text": "depends I think business um um their sites",
    "start": "1359159",
    "end": "1365080"
  },
  {
    "text": "yeah wonderful that's that's such a good point like that we can actually combine these things I suspect you're slightly",
    "start": "1365080",
    "end": "1370320"
  },
  {
    "text": "higher budget than my uh five machines in my basement but yes um thank",
    "start": "1370320",
    "end": "1375600"
  },
  {
    "text": "you nope",
    "start": "1375600",
    "end": "1379600"
  },
  {
    "text": "well actually I would like to speculate about a bit DNS and low balancer but DNS",
    "start": "1383919",
    "end": "1389720"
  },
  {
    "text": "we should care about Cas I believe because if we will change DNS record we must be sure that cash is removed so",
    "start": "1389720",
    "end": "1397080"
  },
  {
    "text": "it's unsafe to use DNS properly load balancer is another hope in our infrastructure and I think where that we",
    "start": "1397080",
    "end": "1404159"
  },
  {
    "text": "would like to have as less hopes net works as possible because it's let for our atcd and kind this cluster is loaded",
    "start": "1404159",
    "end": "1412360"
  },
  {
    "text": "enough we will have huge load on our Network and we I think would like to avoid it and since we are using bare",
    "start": "1412360",
    "end": "1419720"
  },
  {
    "text": "metal here we are unlocked to level two of network and on level two of network we have VP technology which could allow",
    "start": "1419720",
    "end": "1427480"
  },
  {
    "text": "us to use VP to switch virtual P address for our uh atcd servers so if server for",
    "start": "1427480",
    "end": "1434120"
  },
  {
    "text": "example pass some heal check uh it could switch virtual p address to proper",
    "start": "1434120",
    "end": "1440039"
  },
  {
    "text": "one great point on the uh the TTL thank you and yes like it seems like virtual IPS will be helpful there maybe we'll do",
    "start": "1440039",
    "end": "1445600"
  },
  {
    "text": "one more on this topic and then right the same one all right anyone",
    "start": "1445600",
    "end": "1450880"
  },
  {
    "text": "else or all",
    "start": "1450880",
    "end": "1454360"
  },
  {
    "text": "right I actually just wanted to add a little bit on the load balancer part um it's it's great to use internal metal lb",
    "start": "1459679",
    "end": "1467480"
  },
  {
    "text": "for the AP servers but like it's essential if something goes wrong you",
    "start": "1467480",
    "end": "1472840"
  },
  {
    "text": "need an external load balancer to like for the API server otherwise you do end up going straight to IP it's just one of",
    "start": "1472840",
    "end": "1480440"
  },
  {
    "text": "those things right so like external load balancer is preferable um just for backup",
    "start": "1480440",
    "end": "1488559"
  },
  {
    "text": "perspective thank you yes and I think there's also you know this is a this is a bootstrapping problem as well also right because we can bring it up uh with",
    "start": "1489640",
    "end": "1496799"
  },
  {
    "text": "you know local hosts and run a bunch of these services and then and go from there you can have multiple Ingress",
    "start": "1496799",
    "end": "1502000"
  },
  {
    "text": "multiple ingresses or ways to Ingress into your API server we unconscious of time so we will",
    "start": "1502000",
    "end": "1507880"
  },
  {
    "text": "we will run through this fast I think I this one for it I think we've talked about low balancer ingressing or API",
    "start": "1507880",
    "end": "1514760"
  },
  {
    "text": "server hopefully it's the same sort of thing um I I will frame the problem here by saying you know I think you brought",
    "start": "1514760",
    "end": "1520919"
  },
  {
    "text": "it up right on the cloud uh the cloud basically makes a bunch of decisions about how networking works for you and",
    "start": "1520919",
    "end": "1526880"
  },
  {
    "text": "we have a cloud controller manager that will go and configure pod Siders on your nodes for example and do all of those",
    "start": "1526880",
    "end": "1533600"
  },
  {
    "text": "there is currently no Cloud controller manager for bare metal that I know of and so like there are you know a lot of",
    "start": "1533600",
    "end": "1539120"
  },
  {
    "text": "decisions that we then have to make ourselves and things we have to do ourselves I think the big one at least for my perspective is um ipv4 IPv6 um at",
    "start": "1539120",
    "end": "1549520"
  },
  {
    "text": "least in my home I don't I have one ipv4 address and more than I can count IPv6 addresses and I think that's you know",
    "start": "1549520",
    "end": "1556360"
  },
  {
    "text": "sort of common that you have a lot more IPv6 addresses and you know is this is this the time is this the place to be",
    "start": "1556360",
    "end": "1563000"
  },
  {
    "text": "opinionated and say IPv6 is the great overlay and we should run that internally um is this something we",
    "start": "1563000",
    "end": "1570520"
  },
  {
    "text": "should we can be we have to be flexible about how do people feel about taking a dependency on IPv6 or saying IPv6 is",
    "start": "1570520",
    "end": "1577120"
  },
  {
    "text": "preferred and we don't need an overlay or do people want to run with encryption uh and basically",
    "start": "1577120",
    "end": "1584520"
  },
  {
    "text": "basically we stay out of this debate and make it maybe a little harder to run because we're being less",
    "start": "1584520",
    "end": "1590279"
  },
  {
    "text": "opinionated anyone want to weigh in does anyone need ipv4 this is always going to",
    "start": "1590279",
    "end": "1595480"
  },
  {
    "text": "be a fun one here we go there the go hands I knew I I had oh sorry thank",
    "start": "1595480",
    "end": "1602600"
  },
  {
    "text": "you when we're getting into the realm of metal and potentially dealing with inconvenient bmc's I've definitely run",
    "start": "1604960",
    "end": "1611760"
  },
  {
    "text": "into plenty that 6 to4 breaks communication with um and that would be",
    "start": "1611760",
    "end": "1616960"
  },
  {
    "text": "a major issue if the the equiv of cloud controller manager were to be provisioning machines using those",
    "start": "1616960",
    "end": "1623200"
  },
  {
    "text": "bmc's yeah I agree I'm personally like using",
    "start": "1624720",
    "end": "1630399"
  },
  {
    "text": "one which does the not not 64 not 6 to4 for me and it works for my case but yeah",
    "start": "1630399",
    "end": "1635679"
  },
  {
    "text": "it probably doesn't work for Everything For Better or Worse the uh environment I",
    "start": "1635679",
    "end": "1640919"
  },
  {
    "text": "work in often requires ipv4 uh and isn't likely to change that",
    "start": "1640919",
    "end": "1646360"
  },
  {
    "text": "due to regulations for a while fair enough yes the the technical versus",
    "start": "1646360",
    "end": "1652159"
  },
  {
    "text": "the uh regulations is is always a fund tradeoff and",
    "start": "1652159",
    "end": "1657960"
  },
  {
    "text": "yeah anything else anyone wants to bring up here about like requirements for networking I feel like basically we're going to be have to support most things",
    "start": "1658880",
    "end": "1667600"
  },
  {
    "text": "and we're going to end up in a world where you know we have we have an ipv4 configuration that that works and I pv6",
    "start": "1667600",
    "end": "1673559"
  },
  {
    "text": "configuration that works a configuration a Calico configuration and all those sorts of things it's not going to be we we'll have to each have a",
    "start": "1673559",
    "end": "1682000"
  },
  {
    "text": "configuration set and we can recommend our own and I'm sure class API will recommend one and kops will recommend",
    "start": "1682000",
    "end": "1687080"
  },
  {
    "text": "one and hopefully we can agree on which one to recommend but uh I think you know if we both choose IPv6 there's still",
    "start": "1687080",
    "end": "1693159"
  },
  {
    "text": "going to be a need for other scenarios anyone",
    "start": "1693159",
    "end": "1699320"
  },
  {
    "text": "else anyone to do the next one",
    "start": "1699320",
    "end": "1703880"
  },
  {
    "text": "about five minutes left you want do that go for",
    "start": "1711919",
    "end": "1715840"
  },
  {
    "text": "it okay so um next is storage and",
    "start": "1717240",
    "end": "1722480"
  },
  {
    "text": "persistent volumes we touched a bit on those with that",
    "start": "1722480",
    "end": "1727640"
  },
  {
    "text": "CD um but now that you have a kubernetes",
    "start": "1727640",
    "end": "1732960"
  },
  {
    "text": "clusters um you have other options so simple Lo",
    "start": "1732960",
    "end": "1739399"
  },
  {
    "text": "CSI SE or renter ABS NFS ice cazi so all",
    "start": "1739399",
    "end": "1745559"
  },
  {
    "text": "is CSI driver you can mix match",
    "start": "1745559",
    "end": "1751320"
  },
  {
    "text": "multiple do you think that's enough once you have the cluster up and running",
    "start": "1751399",
    "end": "1756600"
  },
  {
    "text": "or should be something more complex like we have persistent volumes",
    "start": "1756600",
    "end": "1765080"
  },
  {
    "text": "for instances directly",
    "start": "1765080",
    "end": "1770039"
  },
  {
    "text": "I'll also add qbfs to the list of uh software defined storage that was mentioned this morning in the keynote so uh like I we didn't update our slides I",
    "start": "1770240",
    "end": "1777240"
  },
  {
    "text": "apologize does anyone have any particularly I think this one is fairly straightforward because we basically can",
    "start": "1777240",
    "end": "1783240"
  },
  {
    "text": "just support everything type thing and it you sort of pick whatever you want and I don't think there's any like",
    "start": "1783240",
    "end": "1788880"
  },
  {
    "text": "intricate dependencies I don't know if anyone has any particular requirements that they think might throw a wrench in",
    "start": "1788880",
    "end": "1794960"
  },
  {
    "text": "that in that plan",
    "start": "1794960",
    "end": "1798600"
  },
  {
    "text": "uh this maybe gets into the not the application layer storage but into the node level storage um are we doing",
    "start": "1805159",
    "end": "1812919"
  },
  {
    "text": "anything in the way of um in this scope evaluating things like encryption of the",
    "start": "1812919",
    "end": "1819760"
  },
  {
    "text": "local drives on which you deploy it CD or you know the API servers and everything else um and even the worker",
    "start": "1819760",
    "end": "1827200"
  },
  {
    "text": "nodes um do we want security do we want raid or resilience uh of local storage",
    "start": "1827200",
    "end": "1833559"
  },
  {
    "text": "um there there's a lot of other considerations there at least in the life cycle of a given host that I don't",
    "start": "1833559",
    "end": "1839480"
  },
  {
    "text": "see on the slide and I would say I have my own opinions um but I I would like to",
    "start": "1839480",
    "end": "1846760"
  },
  {
    "text": "hear what everybody else's use case demands I think that is a great point I",
    "start": "1846760",
    "end": "1852880"
  },
  {
    "text": "think we have we have a final slide that I will sneak uh sort of uh intro about",
    "start": "1852880",
    "end": "1859679"
  },
  {
    "text": "um but uh we do have some we going about we're going to talk about that a little bit in terms of provisioning of the OS",
    "start": "1859679",
    "end": "1866120"
  },
  {
    "text": "oh there's a one right there but uh if we run out of time it's not a problem because that final slide you will see I",
    "start": "1866120",
    "end": "1872720"
  },
  {
    "text": "will give you a clue it's a working group that we're proposing my question is related to",
    "start": "1872720",
    "end": "1878159"
  },
  {
    "text": "hyper convergence uh so when you do SE for raner or whatever storage solution you bring in what is your recommendation",
    "start": "1878159",
    "end": "1885600"
  },
  {
    "text": "on running it as a hyper conversed model with direct direct attached dis or you run it as a separate SE storage",
    "start": "1885600",
    "end": "1892340"
  },
  {
    "text": "[Music] notes so we want to hear from you I don't think we have a recommendation",
    "start": "1892340",
    "end": "1897880"
  },
  {
    "text": "right I think the I think the nice thing here is it probably doesn't matter too much at the kubernetes",
    "start": "1897880",
    "end": "1904840"
  },
  {
    "text": "level it's absolutely a cost for you but like if if you're optimizing for cost you will you can use uh one of the local",
    "start": "1904840",
    "end": "1911399"
  },
  {
    "text": "CSI drivers right if you already have a net apppp you can use NFS on net app",
    "start": "1911399",
    "end": "1917080"
  },
  {
    "text": "right so I think would be sort of how we would view it and this is probably going be another one where we don't need to be",
    "start": "1917080",
    "end": "1922840"
  },
  {
    "text": "very opinionated at this level the encryption of things like the ETD volume are lower level and that's I guess the",
    "start": "1922840",
    "end": "1929080"
  },
  {
    "text": "next topic which should I talk to that one quickly uh okay but we have we have more",
    "start": "1929080",
    "end": "1935880"
  },
  {
    "text": "why don't we should we wrap it up or do you want to talk about",
    "start": "1935880",
    "end": "1940799"
  },
  {
    "text": "these let's wrap it up okay yeah I think we're we're very close to time so um we have some more topics we're we're",
    "start": "1941080",
    "end": "1948000"
  },
  {
    "text": "going to upload our slides so please feel free to take a look at them and find us on slack however in",
    "start": "1948000",
    "end": "1954519"
  },
  {
    "text": "general um the the big I think topic",
    "start": "1954519",
    "end": "1959159"
  },
  {
    "text": "is how how can we move forwards from here right like I I thank you for all for participating uh there are some",
    "start": "1962480",
    "end": "1968799"
  },
  {
    "text": "great topics here um there is a QR code there where you can leave your reviews um but we would actually much rather",
    "start": "1968799",
    "end": "1974720"
  },
  {
    "text": "that you came and uh that we were able to discuss the this in a way that is uh sort of vendor neutral so whether you're",
    "start": "1974720",
    "end": "1981519"
  },
  {
    "text": "cluster API chops whatever it is like these topics seem to be common across all of our projects and I I am proposing",
    "start": "1981519",
    "end": "1990039"
  },
  {
    "text": "with Cen that we start a working group in Sig cluster life cycle to address this uh working group metal is the sort",
    "start": "1990039",
    "end": "1996480"
  },
  {
    "text": "of working title um if you would like to participate in that um please join s cluster life cycle on slack and we will",
    "start": "1996480",
    "end": "2003200"
  },
  {
    "text": "we will go from there um but thank you all so much for participating C and I will around a little bit uh we can talk",
    "start": "2003200",
    "end": "2009639"
  },
  {
    "text": "about sort of what your um you know some more of the topics that we we've covered and um uh I really",
    "start": "2009639",
    "end": "2017679"
  },
  {
    "text": "feel like we made a ton of progress on those first topics which were the really tricky ones so thank you all very much",
    "start": "2017679",
    "end": "2024440"
  },
  {
    "text": "[Applause]",
    "start": "2024440",
    "end": "2033108"
  }
]