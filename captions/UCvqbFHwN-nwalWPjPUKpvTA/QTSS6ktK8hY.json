[
  {
    "text": "hello and welcome everyone to our talk it's really cool to see a fully packed",
    "start": "0",
    "end": "5040"
  },
  {
    "text": "room um today's talk is about better bandwidth management with ebpf",
    "start": "5040",
    "end": "10400"
  },
  {
    "text": "i'm daniel brookman i'm i co-maintain ebpf in the linux kernel and i work with",
    "start": "10400",
    "end": "15759"
  },
  {
    "text": "the psyllium team and the talkers together with christopher nikolai who are also from the psyllium team",
    "start": "15759",
    "end": "22160"
  },
  {
    "text": "before we look into bandwidth management i wanted to start out with an interesting metric",
    "start": "22160",
    "end": "28160"
  },
  {
    "text": "which you can see here on the right side the metric is basically the density like the number of uh parts per node",
    "start": "28160",
    "end": "35440"
  },
  {
    "text": "and what you can see here like the more people use kubernetes the the higher the density gets so for example",
    "start": "35440",
    "end": "41680"
  },
  {
    "text": "uh it's probably reasonable to assume in 2022 that there's that there is a median of",
    "start": "41680",
    "end": "48079"
  },
  {
    "text": "around 50 parts per node and with that increasing density it also means that there's a competition for",
    "start": "48079",
    "end": "55280"
  },
  {
    "text": "resources on a given node for example cpu and memory so basically",
    "start": "55280",
    "end": "60399"
  },
  {
    "text": "operators have to have the issue that they need to tackle how to allocate and efficiently use",
    "start": "60399",
    "end": "66960"
  },
  {
    "text": "resources one tool that you can use in the parts back for example is",
    "start": "66960",
    "end": "73119"
  },
  {
    "text": "that you can define resources requests and limits for example resource",
    "start": "73119",
    "end": "78400"
  },
  {
    "text": "requests they define how much cpu and memory a",
    "start": "78400",
    "end": "83600"
  },
  {
    "text": "given part requires and then cubelet will make sure that that part is scheduled in a node which",
    "start": "83600",
    "end": "90000"
  },
  {
    "text": "can actually satisfy that constraint limits for example if a part overshoots",
    "start": "90000",
    "end": "95280"
  },
  {
    "text": "its memory usage it will get om killed so now the question is what about",
    "start": "95280",
    "end": "100560"
  },
  {
    "text": "networking so tcp in its nature sense as fast as possible",
    "start": "100560",
    "end": "105920"
  },
  {
    "text": "what you can see here in the right picture is a typical tcp",
    "start": "105920",
    "end": "111119"
  },
  {
    "text": "congestion control algorithm that will try to send more tcp segments to the network so",
    "start": "111119",
    "end": "118960"
  },
  {
    "text": "there's an exponential growth fast in the beginning until it experiences a packet loss and then it",
    "start": "118960",
    "end": "124640"
  },
  {
    "text": "will back down and then it will try to do the same thing again so really the output contract for tcp is",
    "start": "124640",
    "end": "130879"
  },
  {
    "text": "to send as fast as possible and shaping is typically done by device output cues",
    "start": "130879",
    "end": "137760"
  },
  {
    "text": "so basically the queue limit for those output queues as well as the remotes receive window size",
    "start": "137760",
    "end": "144720"
  },
  {
    "text": "determine how many packets from tcp can be in flight and but who actually limits a parts",
    "start": "144720",
    "end": "150879"
  },
  {
    "text": "network usage in kubernetes so there's a infrastructure for that in",
    "start": "150879",
    "end": "156000"
  },
  {
    "text": "kubernetes so there's a bandwidth enforcement so far this has only been experimental unfortunately",
    "start": "156000",
    "end": "161920"
  },
  {
    "text": "so if you look into the parts back there are kubernetes specific annotations so you have an ingress",
    "start": "161920",
    "end": "168720"
  },
  {
    "text": "annotation you have an egress annotation then you can for example set something like a part should get equals bandwidth",
    "start": "168720",
    "end": "174879"
  },
  {
    "text": "or 50 megabits per second and support for that pod annotation so far has only been implemented with the",
    "start": "174879",
    "end": "181840"
  },
  {
    "text": "bandwidth meta plugin that's a plugin from the cni plugin collection",
    "start": "181840",
    "end": "188000"
  },
  {
    "text": "and that basically implements a a basic token bucket filter from the linux",
    "start": "188000",
    "end": "194239"
  },
  {
    "text": "traffic control subsystem so how does it look um [Music]",
    "start": "194239",
    "end": "199680"
  },
  {
    "text": "for example if you have a typical node that you can see here there's a part in the node and the part is typically",
    "start": "199680",
    "end": "205760"
  },
  {
    "text": "connected to the host namespace with two weave devices one lag in the hostname space one lakh of the rest of the reef",
    "start": "205760",
    "end": "211760"
  },
  {
    "text": "device in the part name space and token bucket filter cudas are attached there but the real problem is",
    "start": "211760",
    "end": "218720"
  },
  {
    "text": "it's not scalable for production use and i will show you why",
    "start": "218720",
    "end": "224159"
  },
  {
    "text": "if you for example set an ingress bandwidth rate of 50 megabits per second",
    "start": "224159",
    "end": "229200"
  },
  {
    "text": "uh in your parts back then basically that bandwidth meta plugin will then attach a token bucket filter queue disk",
    "start": "229200",
    "end": "235599"
  },
  {
    "text": "to the weave device in the host name space that is attached from the tc subsystem",
    "start": "235599",
    "end": "241040"
  },
  {
    "text": "in linux on the eco side but if you look at the logical traffic flow point of view it's uh like ingressing into the",
    "start": "241040",
    "end": "248000"
  },
  {
    "text": "part the traffic so if if there are many clients connecting from the internet",
    "start": "248000",
    "end": "254560"
  },
  {
    "text": "to that part for example they will all hit that token package filter qdisk",
    "start": "254560",
    "end": "260479"
  },
  {
    "text": "and it has actual design issues because the token bucket filter queue disk it's",
    "start": "260479",
    "end": "266560"
  },
  {
    "text": "given it has to track its state in terms of the shaping it there's basically a single log that has to be",
    "start": "266560",
    "end": "273919"
  },
  {
    "text": "taken across um all cpus and that's a huge contention point if",
    "start": "273919",
    "end": "279280"
  },
  {
    "text": "you want to if if you receive traffic from multiple cpus at the same time",
    "start": "279280",
    "end": "285600"
  },
  {
    "text": "and that basically completely defeats a multi-cue capability of physical nics because",
    "start": "285600",
    "end": "292400"
  },
  {
    "text": "typically a nic has many receive cues then once packet arrive on the different",
    "start": "292400",
    "end": "297840"
  },
  {
    "text": "receive queues cpus will pick that up in parallel they will process it they will forward it in parallel but then they",
    "start": "297840",
    "end": "304320"
  },
  {
    "text": "will all hit that single token bucket filter queue disk the other issue with that is also that",
    "start": "304320",
    "end": "310080"
  },
  {
    "text": "queuing on the receive side is actually a no-go because it it consumes resources it the packet",
    "start": "310080",
    "end": "316960"
  },
  {
    "text": "basically made it over the wire it consumed your network and then only to receive it on the node itself",
    "start": "316960",
    "end": "324400"
  },
  {
    "text": "where it then goes into that q disk it's waiting in that queue instead of being processed and only then to be dropped",
    "start": "324400",
    "end": "330000"
  },
  {
    "text": "for example so that really causes buffer plot for your network if you look at the other direction for",
    "start": "330000",
    "end": "336639"
  },
  {
    "text": "example if you install egress bandwidth or 50 megabit per second then that bandwidth meta plugin will",
    "start": "336639",
    "end": "342960"
  },
  {
    "text": "basically set up an additional device it's called an ifb device type and then all the",
    "start": "342960",
    "end": "348479"
  },
  {
    "text": "traffic from the host weave device will be redirected to that ifb device and on that ifb device there is the",
    "start": "348479",
    "end": "354240"
  },
  {
    "text": "token bucket filter qdisk installed with the given rate why is that workaround",
    "start": "354240",
    "end": "359360"
  },
  {
    "text": "you might ask but the the issue is logically that traffic in the host",
    "start": "359360",
    "end": "364639"
  },
  {
    "text": "namespace when it egresses the part it arrives in the traffic control subsystem in the linux kernel on the ingress side",
    "start": "364639",
    "end": "371759"
  },
  {
    "text": "and on the ingredient side the linux kernel cannot shape so you need to redirect it to a different device only that you're at the",
    "start": "371759",
    "end": "378400"
  },
  {
    "text": "dc egress layer and only there you can attach the token bucket filter qdisk",
    "start": "378400",
    "end": "384560"
  },
  {
    "text": "so if you have multiple applications in your part they can send traffic from multiple cpus and they will all hit that",
    "start": "384560",
    "end": "390240"
  },
  {
    "text": "same token bucket filter queue disk that also has design issues because now you're now",
    "start": "390240",
    "end": "396560"
  },
  {
    "text": "you're actually queuing twice you're queuing on the weaved like on the ifb device",
    "start": "396560",
    "end": "401759"
  },
  {
    "text": "but then also you're queuing on the physical device because there's typically also a q disk attached that is",
    "start": "401759",
    "end": "407039"
  },
  {
    "text": "handling all your outgoing traffic uh for example on linux by default that's ftq codal so that really causes buffer",
    "start": "407039",
    "end": "413919"
  },
  {
    "text": "bloat which is a big issue then again you have the single lock contention point across all the cpus so",
    "start": "413919",
    "end": "420080"
  },
  {
    "text": "you cannot actually make it scalable across cpus and then there's also a mechanism in the",
    "start": "420080",
    "end": "425199"
  },
  {
    "text": "tcp stack which is called tcp small queues tcp small qs is there to reduce",
    "start": "425199",
    "end": "430319"
  },
  {
    "text": "buffer plots so that you reduce excessive buffering in queues and tcp small queues basically works",
    "start": "430319",
    "end": "437199"
  },
  {
    "text": "that works the way that tcp stack tries to not send too much packets but now those packets are stuck in queues and",
    "start": "437199",
    "end": "444479"
  },
  {
    "text": "when like once they are processed there they go to the upper stack and it will basically fool the tcp stack that the",
    "start": "444479",
    "end": "450639"
  },
  {
    "text": "packet might might already be on the wire even though it's still on your host and last but not least you know you need",
    "start": "450639",
    "end": "457120"
  },
  {
    "text": "three net devices instead of just two so overall i would say it's a latency",
    "start": "457120",
    "end": "463520"
  },
  {
    "text": "killer because you have you have to deal so much with queuing it doesn't scale across cpus and yeah it's not",
    "start": "463520",
    "end": "471360"
  },
  {
    "text": "really ready for production use the other thing is like the nature of tcp when you send as fast as possible and",
    "start": "471360",
    "end": "478240"
  },
  {
    "text": "what you can see when you look at the cueing theory like so once you get to the point where you uh consume the bottleneck link close",
    "start": "478240",
    "end": "485759"
  },
  {
    "text": "to 100 what you can see here in that graph is that that the wait time of packets in the",
    "start": "485759",
    "end": "490879"
  },
  {
    "text": "queue they basically skyrocket to infinite so there has been some interesting",
    "start": "490879",
    "end": "497120"
  },
  {
    "text": "research from from google folks and they were asking themselves so can we get rid of queues entirely so they came up with",
    "start": "497120",
    "end": "503840"
  },
  {
    "text": "a really cool idea which is uh called the earliest departure time model so they basically said",
    "start": "503840",
    "end": "510160"
  },
  {
    "text": "let's let's replace let's replace uh queuing fully and come up with two simple core pieces",
    "start": "510160",
    "end": "516880"
  },
  {
    "text": "one is the earliest departure time so it's basically a time stamp on the network packet in the host which will",
    "start": "516880",
    "end": "522800"
  },
  {
    "text": "dictate the the time when the packet can be delivered to the network at the earliest possible point in time and the other one",
    "start": "522800",
    "end": "529920"
  },
  {
    "text": "is the timing wheel scheduler which will basically hold this constraint and then send packets out",
    "start": "529920",
    "end": "536320"
  },
  {
    "text": "so how can this model be applied to to kubernetes so enter ebpf so you've",
    "start": "536320",
    "end": "541839"
  },
  {
    "text": "probably heard ebpf many times at this conference so it's basically a way to make the kernel programmable you have a",
    "start": "541839",
    "end": "547680"
  },
  {
    "text": "small programs that get verified for safety and they can be attached to different points in the linux kernel",
    "start": "547680",
    "end": "554800"
  },
  {
    "text": "and sodium is an ebpf based cni or you can better call it like",
    "start": "554800",
    "end": "560240"
  },
  {
    "text": "networking platform because it does many things so it does takes care of part connectivity but also service load",
    "start": "560240",
    "end": "565680"
  },
  {
    "text": "balancing network policies and so on and so forth i don't want to go too much into details the one detail i want to go",
    "start": "565680",
    "end": "572480"
  },
  {
    "text": "into is the bandwidth management so we implemented the sodium bandwidth manager",
    "start": "572480",
    "end": "578560"
  },
  {
    "text": "and that bandwidth manager basically allows for lockless earliest departure time based",
    "start": "578560",
    "end": "583920"
  },
  {
    "text": "uh part rate limiting with ebpf so basically what you can see here the",
    "start": "583920",
    "end": "589360"
  },
  {
    "text": "the enforcement point we moved from the reef device to the physical device so that you don't need",
    "start": "589360",
    "end": "595440"
  },
  {
    "text": "to queue twice you can avoid this additional buffer bloat if you have it on the physical device",
    "start": "595440",
    "end": "602800"
  },
  {
    "text": "and you also don't need the upper stack so we have a mode in psyllium which is",
    "start": "602800",
    "end": "608240"
  },
  {
    "text": "called bpf host routing where all the routing can remain in the tc ebpf layer",
    "start": "608240",
    "end": "613760"
  },
  {
    "text": "and it can be forwarded there directly to the physical device and you can use certain functionality of",
    "start": "613760",
    "end": "619680"
  },
  {
    "text": "the networking stack such as for example the fip lookup you can just reuse them out of bpf itself",
    "start": "619680",
    "end": "626640"
  },
  {
    "text": "and the good thing with that is that it will keep at a tcp small queue",
    "start": "626640",
    "end": "631839"
  },
  {
    "text": "feedback properly so how does the architecture look like",
    "start": "631839",
    "end": "636959"
  },
  {
    "text": "of the bandwidth manager if you set a port a specification that it can for example",
    "start": "636959",
    "end": "642720"
  },
  {
    "text": "send 50 megabit or so then in the end like the packet arrives at the physical device there's an ebpf program attached",
    "start": "642720",
    "end": "649440"
  },
  {
    "text": "to it uh that was orchestrated by psyllium it is taking care of the packet departure",
    "start": "649440",
    "end": "655120"
  },
  {
    "text": "time stamps and psyllium also sets up a multi-cue qdisk with so-called fair q",
    "start": "655120",
    "end": "662959"
  },
  {
    "text": "leaf q discs on the device and those fair q leaf q disks they",
    "start": "662959",
    "end": "668079"
  },
  {
    "text": "basically implement this timing wheel scheduler that i mentioned earlier",
    "start": "668079",
    "end": "674360"
  },
  {
    "text": "if you look at the performance of this uh architecture compared to the token bucket filter approach it's really",
    "start": "675120",
    "end": "681200"
  },
  {
    "text": "interesting because um we run an experiment with multiple concurrent flows in parallel",
    "start": "681200",
    "end": "688000"
  },
  {
    "text": "in this case 256 and those flows they are basically ping-pong flows so it will send one byte",
    "start": "688000",
    "end": "694720"
  },
  {
    "text": "of data to one direction and one byte back to the other direction we do that because we want to stress like the the",
    "start": "694720",
    "end": "700320"
  },
  {
    "text": "latency we want to see the latency of the of the whole system and each of those concurrent flows has a",
    "start": "700320",
    "end": "706959"
  },
  {
    "text": "rate limit of 100 megabit per second so now what you can see here is the time in microseconds so",
    "start": "706959",
    "end": "714560"
  },
  {
    "text": "the yellow uh bar here is basically the token bucket filter approach and the green one is the earliest departure time",
    "start": "714560",
    "end": "721279"
  },
  {
    "text": "model that we implemented with psyllium so when you look at the median it's",
    "start": "721279",
    "end": "727360"
  },
  {
    "text": "around seven times lower latency than the uh traditional approach which is based on",
    "start": "727360",
    "end": "733440"
  },
  {
    "text": "cueing and even if you look at the p99 latency you get over 4x better latency",
    "start": "733440",
    "end": "741360"
  },
  {
    "text": "and if you look at the actual transaction rate so when you run net perf one transaction is basically",
    "start": "741680",
    "end": "747600"
  },
  {
    "text": "like a like a ping-pong so one byte of data in one direction and then the other direction",
    "start": "747600",
    "end": "753519"
  },
  {
    "text": "you get over seven times better transaction rate under this 100 megabit per second constraint",
    "start": "753519",
    "end": "760959"
  },
  {
    "text": "all right so so far we have seen how we can implement scalable bandwidth management with sodium",
    "start": "762639",
    "end": "768240"
  },
  {
    "text": "based on the earliest departure time model but what about thinking even more broadly can we do like even for the for",
    "start": "768240",
    "end": "773839"
  },
  {
    "text": "the internet band better bandwidth management so what else does the earliest departure",
    "start": "773839",
    "end": "779519"
  },
  {
    "text": "time model enable it enables bbr so bbr is a tcp congestion control",
    "start": "779519",
    "end": "785040"
  },
  {
    "text": "algorithm that has been developed by uh folks from google research and the internet",
    "start": "785040",
    "end": "791920"
  },
  {
    "text": "today is basically it's basically what you can see on the on the left side of the picture so you",
    "start": "791920",
    "end": "798000"
  },
  {
    "text": "have a loss based loss-based tcp congestion control algorithm so if",
    "start": "798000",
    "end": "803120"
  },
  {
    "text": "you don't change any defaults on your linux laptop or server for example you will have that model on the left side",
    "start": "803120",
    "end": "810240"
  },
  {
    "text": "it's a so called tcp cubic congestion control algorithm so as i mentioned",
    "start": "810240",
    "end": "815279"
  },
  {
    "text": "earlier it tries to ramp up its congestion window until it's until it experiences congestion lost somewhere",
    "start": "815279",
    "end": "821279"
  },
  {
    "text": "and then it tries to back down so you see the sawtooth pattern for a connection on the right side you have the bbr so",
    "start": "821279",
    "end": "827519"
  },
  {
    "text": "bbr is modeled in a different way so it's basically modeled around the delivery rate that",
    "start": "827519",
    "end": "834000"
  },
  {
    "text": "the traffic can be delivered to the to the receiver as well as the round trip time",
    "start": "834000",
    "end": "840160"
  },
  {
    "text": "so it's not based on packet loss basically so when would it be useful to consider",
    "start": "840160",
    "end": "845440"
  },
  {
    "text": "bbr so if you have a kubernetes cluster where your clients connect from outside of the internet uh then it would be um",
    "start": "845440",
    "end": "853120"
  },
  {
    "text": "really useful to look into that because it will significantly improve your latency",
    "start": "853120",
    "end": "858639"
  },
  {
    "text": "for low end last mile networks because typically buffer bloat happens on your on your home routers and it will improve",
    "start": "858639",
    "end": "865920"
  },
  {
    "text": "there but it will also significantly improve the throughput for for high speed networks",
    "start": "865920",
    "end": "872079"
  },
  {
    "text": "when you have uh like a like a long delay in this case",
    "start": "872079",
    "end": "877760"
  },
  {
    "text": "so i run an example for example i reserved a server on new york from from",
    "start": "877839",
    "end": "883519"
  },
  {
    "text": "the packet.net and i tried to do an iperf measurement to our lab in zurich that we have",
    "start": "883519",
    "end": "889760"
  },
  {
    "text": "and what you can see here with the defaults the default is tcp cubic and fqccaddle",
    "start": "889760",
    "end": "895279"
  },
  {
    "text": "as a qdisk what you can see here when you look at the bit rate it's trying it's slowly",
    "start": "895279",
    "end": "900800"
  },
  {
    "text": "trying to ramp up until it reaches a point where it gets to 400 something megabit per second but then it",
    "start": "900800",
    "end": "906480"
  },
  {
    "text": "experienced lost somewhere in the path and at that point it's you know like it's uh",
    "start": "906480",
    "end": "911519"
  },
  {
    "text": "slowing down again and it's trying the same thing until it reaches 400 something megabits so overall the",
    "start": "911519",
    "end": "917760"
  },
  {
    "text": "average bit rate is 270 megabits per second and now if you switch over to bbr",
    "start": "917760",
    "end": "923120"
  },
  {
    "text": "for this experiment um you will see that it will ramp up until until 400 megabit per second and",
    "start": "923120",
    "end": "929920"
  },
  {
    "text": "it will plateau at that point in time so just by changing that congestion",
    "start": "929920",
    "end": "935279"
  },
  {
    "text": "control algorithm for connections over the internet you can reach from megabit per second to over 400.",
    "start": "935279",
    "end": "942959"
  },
  {
    "text": "so now the question is can bbr actually be used with kubernetes parts",
    "start": "943120",
    "end": "948399"
  },
  {
    "text": "um so bbr works in conjunction with the with the fqdisk which implements this",
    "start": "948399",
    "end": "953519"
  },
  {
    "text": "timing wheel so you really need a timestamp for your packets but the problem is when you have network",
    "start": "953519",
    "end": "959120"
  },
  {
    "text": "namespaces then the kernel actually clears the timestamp when the packet goes from one weave device to another so",
    "start": "959120",
    "end": "966320"
  },
  {
    "text": "in that case bbr can actually not work because it will get an instable rate",
    "start": "966320",
    "end": "971360"
  },
  {
    "text": "because the timestamps are zeroed so why is that it's because of a kernel",
    "start": "971360",
    "end": "978560"
  },
  {
    "text": "limitation so in the kernel we have a packet representation which is called the socket buffer",
    "start": "978560",
    "end": "984800"
  },
  {
    "text": "and the socket buffer holds all possible metadata around the packet including a timestamp",
    "start": "984800",
    "end": "990959"
  },
  {
    "text": "and the timestamp has like for the receive side and for the transmit side it has a",
    "start": "990959",
    "end": "997040"
  },
  {
    "text": "different clock base for example for the receive side it has the so-called clock tai it's a it's an atomic clock",
    "start": "997040",
    "end": "1003839"
  },
  {
    "text": "and for traffic that is going out from the transmit side it has a clock base of clock monotonic and the problem is",
    "start": "1003839",
    "end": "1011600"
  },
  {
    "text": "the socket buffer try is like the most critical data structure in the linux kernel networking stack because you have",
    "start": "1011600",
    "end": "1017600"
  },
  {
    "text": "to keep it as small as possible just to avoid cache misses just to keep your performance high",
    "start": "1017600",
    "end": "1023279"
  },
  {
    "text": "so there's just a single 64-bit timestamp field in the kernel",
    "start": "1023279",
    "end": "1028640"
  },
  {
    "text": "and um that is why it had to be cleared and the problem is [Music]",
    "start": "1028640",
    "end": "1034558"
  },
  {
    "text": "people tried in the past to standardize just on clock tai um but it didn't work because the it's",
    "start": "1034559",
    "end": "1042000"
  },
  {
    "text": "like the tai clock when you is typically taken from",
    "start": "1042000",
    "end": "1047438"
  },
  {
    "text": "like like somewhere from the hardware for example you have a hardware clock and it will",
    "start": "1047439",
    "end": "1052720"
  },
  {
    "text": "take over the timestamp into the packet but if there are some if there's buggy hardware or if you have some clock skus",
    "start": "1052720",
    "end": "1058880"
  },
  {
    "text": "it will basically mess around with the timestamp too much when for example if you get a too high",
    "start": "1058880",
    "end": "1064160"
  },
  {
    "text": "offset for your timestamp then it will cause a drop in the fair q qdisk because it will",
    "start": "1064160",
    "end": "1070160"
  },
  {
    "text": "go over a certain horizon and that will break tcp so it has been tried in the past to just standardize on",
    "start": "1070160",
    "end": "1077200"
  },
  {
    "text": "a single clock but it had to be reverted and then when you forward from receive to transmit",
    "start": "1077200",
    "end": "1083600"
  },
  {
    "text": "that's the limitation where people had to clear the timestamp back to zero",
    "start": "1083600",
    "end": "1088640"
  },
  {
    "text": "and just as a note like this monotonic clock is not prone to such a clock skus",
    "start": "1088640",
    "end": "1094320"
  },
  {
    "text": "so yeah that's the reason why it couldn't work and we have worked together with",
    "start": "1094320",
    "end": "1100320"
  },
  {
    "text": "guys from facebook to fix that problem in the linux kernel so we presented the issue",
    "start": "1100320",
    "end": "1106720"
  },
  {
    "text": "i think it was at the last year's bloomberg conference and they run in the end into the exactly the same and so we",
    "start": "1106720",
    "end": "1113120"
  },
  {
    "text": "fixed this together and now the timestamp is actually retained for like the outgoing timestamp for sockets that",
    "start": "1113120",
    "end": "1120799"
  },
  {
    "text": "are in uh that are in parts so when packets traverse the network",
    "start": "1120799",
    "end": "1125919"
  },
  {
    "text": "namespaces it will be retained and then all the way until the physical device",
    "start": "1125919",
    "end": "1131520"
  },
  {
    "text": "so now with the bandwidth manager that we implemented in cilium with the whole architecture what basically happens is",
    "start": "1131520",
    "end": "1138160"
  },
  {
    "text": "you can do all the routing in the tc bpf layer the timestamp will be retained",
    "start": "1138160",
    "end": "1143760"
  },
  {
    "text": "and also the socket association to the packet which is needed for the tcp stack to get the proper feedback and then on",
    "start": "1143760",
    "end": "1150480"
  },
  {
    "text": "the physical device there's this multi-cue with the fact you leave qrdisks to implement this timing wheel",
    "start": "1150480",
    "end": "1156240"
  },
  {
    "text": "and then it can basically work just as a side note bbr you actually",
    "start": "1156240",
    "end": "1163280"
  },
  {
    "text": "only need to to enable this on the server side because uh so you don't it's not necessary to have it on the client",
    "start": "1163280",
    "end": "1169679"
  },
  {
    "text": "side because typically all the bulk traffic comes goes from the server to the client",
    "start": "1169679",
    "end": "1175840"
  },
  {
    "text": "so now to our demo so in our demo we are basically want to show you um a",
    "start": "1176400",
    "end": "1181840"
  },
  {
    "text": "streaming service that we implemented like a mini uh netflix in that sense",
    "start": "1181840",
    "end": "1187679"
  },
  {
    "text": "and we want to compare it for cubic and bbr under different network conditions",
    "start": "1187679",
    "end": "1193840"
  },
  {
    "text": "this is basically how our setup works how our setup looks like so we have a mini cluster that mini cluster uh has",
    "start": "1193840",
    "end": "1202080"
  },
  {
    "text": "one node on that node there are two parts and yeah",
    "start": "1202080",
    "end": "1209039"
  },
  {
    "text": "there's an ffmpeg part which basically ingests a video and it chunks it up into",
    "start": "1209039",
    "end": "1214720"
  },
  {
    "text": "smaller pieces and the engine x part is basically the the front end",
    "start": "1214720",
    "end": "1221520"
  },
  {
    "text": "which serves those video chunks to a client and it is basically behind the",
    "start": "1221520",
    "end": "1227520"
  },
  {
    "text": "kubernetes service that we expose to the internet and last but not least we have an",
    "start": "1227520",
    "end": "1233679"
  },
  {
    "text": "external client it can be somebody with a phone it can be with the laptop or just the regular workstation it connects",
    "start": "1233679",
    "end": "1240320"
  },
  {
    "text": "to that kubernetes server that we have over the internet and we we emulate",
    "start": "1240320",
    "end": "1247440"
  },
  {
    "text": "latency and then also later on latency and drop with the help of a so-called net mq disk",
    "start": "1247440",
    "end": "1254400"
  },
  {
    "text": "so netemcudus and linux traffic control subsystem is explicitly there to simulate bad network conditions",
    "start": "1254400",
    "end": "1263840"
  },
  {
    "text": "and yeah so we want to show this under two different configurations so one is the bandwidth manager with the cubic tcp",
    "start": "1264960",
    "end": "1271440"
  },
  {
    "text": "congestion control algorithm and the other one is the feature that we implemented now with bbr",
    "start": "1271440",
    "end": "1278799"
  },
  {
    "text": "and with that i'm switching over to christopher",
    "start": "1278799",
    "end": "1283480"
  },
  {
    "text": "hello hello",
    "start": "1293360",
    "end": "1298559"
  },
  {
    "text": "hello so what we have here is two clusters we try to make them as similar as possible",
    "start": "1298720",
    "end": "1304559"
  },
  {
    "text": "they're both running in madrid in the same data center the only difference between them is one",
    "start": "1304559",
    "end": "1309600"
  },
  {
    "text": "is enabled with cubic the standard configuration the other is enabled with bbr",
    "start": "1309600",
    "end": "1317360"
  },
  {
    "text": "and so what we do is we we scrape in and we see two different videos okay we got really excited when",
    "start": "1317360",
    "end": "1324000"
  },
  {
    "text": "uh we had like a lot of latency problems just walking around uh i'm personally data capped on my",
    "start": "1324000",
    "end": "1330000"
  },
  {
    "text": "smartphone so i can't watch a lot of videos or else they're constantly buffering so what we have here is one of our",
    "start": "1330000",
    "end": "1336000"
  },
  {
    "text": "colleagues and ponchocena is you know doing a little bit of sky surfing here",
    "start": "1336000",
    "end": "1342400"
  },
  {
    "text": "and on the left side here we have our cluster with uh cubic set up and what we see is that",
    "start": "1342400",
    "end": "1348960"
  },
  {
    "text": "cubic is constantly sitting inside of a low resolution",
    "start": "1348960",
    "end": "1354080"
  },
  {
    "text": "and we're loading uh everything however we did introduce a little bit of",
    "start": "1354080",
    "end": "1359120"
  },
  {
    "text": "latency so what we're doing is we're introducing packet loss like every one percent of packets are being dropped as",
    "start": "1359120",
    "end": "1365440"
  },
  {
    "text": "well as we're introducing our own um like 100 milliseconds or more of latency",
    "start": "1365440",
    "end": "1371919"
  },
  {
    "text": "and we're able to do a sustain in the right-hand side with bbr much higher resolution",
    "start": "1371919",
    "end": "1378159"
  },
  {
    "text": "even under distress so both have exactly the same amount of",
    "start": "1378159",
    "end": "1383919"
  },
  {
    "text": "injected latency being injected into them but what you see on the left side if you open up your chrome tools",
    "start": "1383919",
    "end": "1389360"
  },
  {
    "text": "is that even at 100 milliseconds we're not able to pull down the high resolution ever",
    "start": "1389360",
    "end": "1395919"
  },
  {
    "text": "with a cubic on the right side same amount of latency we always get hd quality",
    "start": "1395919",
    "end": "1404480"
  },
  {
    "text": "and we pull that down with the same amount of latency",
    "start": "1404480",
    "end": "1409840"
  },
  {
    "text": "and so the important piece here is that uh you will sometimes still you know there's obviously congestion there's",
    "start": "1409840",
    "end": "1415200"
  },
  {
    "text": "packets being lost but the important part is when vbr hits this it it'll hit the limit it maybe goes spicy for a bit",
    "start": "1415200",
    "end": "1422640"
  },
  {
    "text": "but then it keeps right back up at it and it will hit the hd limit under sustained latency cubic will never",
    "start": "1422640",
    "end": "1430400"
  },
  {
    "text": "cross over into hd range at this sustained latency and",
    "start": "1430400",
    "end": "1436159"
  },
  {
    "text": "you know it'll often have a lot more rebuffering",
    "start": "1436159",
    "end": "1440960"
  },
  {
    "text": "and we can also look into a few of the statistics using the ss utility so what",
    "start": "1442240",
    "end": "1447360"
  },
  {
    "text": "we see here in cubic is our congestion window",
    "start": "1447360",
    "end": "1453200"
  },
  {
    "text": "being sent a little higher but it will it will uh kind of sustain",
    "start": "1453200",
    "end": "1458480"
  },
  {
    "text": "around seven or so segments that it's able to send during that window whereas if we were to do the same thing over in",
    "start": "1458480",
    "end": "1464240"
  },
  {
    "text": "our bbr cluster we'll get a much higher cap and that'll be sustainable over time so",
    "start": "1464240",
    "end": "1471520"
  },
  {
    "text": "in here we're operating at around 416 so much much higher than we were able to",
    "start": "1471520",
    "end": "1476799"
  },
  {
    "text": "send before",
    "start": "1476799",
    "end": "1479360"
  },
  {
    "text": "i should uh mention that the guy in the video was our colleague tom so he was uh",
    "start": "1485440",
    "end": "1490799"
  },
  {
    "text": "flying in his paraglider at one of our on-site events when we went skiing",
    "start": "1490799",
    "end": "1497840"
  },
  {
    "text": "all right so now the question is uh",
    "start": "1498000",
    "end": "1503360"
  },
  {
    "text": "you know is pbr the golden solution well uh bpr has potential uh unfairness issues so for example if",
    "start": "1503360",
    "end": "1510960"
  },
  {
    "text": "you use it in a combination with with cubic so if some nodes are in cubic and you know in your cluster some others on",
    "start": "1510960",
    "end": "1517679"
  },
  {
    "text": "bbr um bbr is pretty aggressive so it could",
    "start": "1517679",
    "end": "1523840"
  },
  {
    "text": "overrun basically cubic you will see a higher rate of re-transmissions on tcp simply it's",
    "start": "1523840",
    "end": "1529679"
  },
  {
    "text": "trying to more aggressively probe uh the google research folks they are working on bbr",
    "start": "1529679",
    "end": "1535360"
  },
  {
    "text": "v2 um in order to overcome those limitations uh",
    "start": "1535360",
    "end": "1540960"
  },
  {
    "text": "i just want to call out there's an interesting talk from the netdev conference which from from someone from",
    "start": "1540960",
    "end": "1547679"
  },
  {
    "text": "dropbox where they deployed bbr on the edge and also bb rv2 so they did some early",
    "start": "1547679",
    "end": "1554320"
  },
  {
    "text": "measurements what you can see here like on the green side is like this there's like a three percent tcp retransmission",
    "start": "1554320",
    "end": "1560159"
  },
  {
    "text": "rate on the bbr v1 whereas the v2 stays around one percent so",
    "start": "1560159",
    "end": "1566240"
  },
  {
    "text": "um it is trying to overcome those issues but on the other hand bbr has been deployed in production on a large number",
    "start": "1566240",
    "end": "1573440"
  },
  {
    "text": "of fleets so it is definitely something interesting to consider",
    "start": "1573440",
    "end": "1579279"
  },
  {
    "text": "yeah so coming back to the earlier problem statement that we have like how can you",
    "start": "1579279",
    "end": "1584559"
  },
  {
    "text": "make sure that parts can be rate limited so the bandwidth enforcement from kubernetes really doesn't have to remain",
    "start": "1584559",
    "end": "1590960"
  },
  {
    "text": "in a poor state um so with the psyllium bandwidth manager implementation",
    "start": "1590960",
    "end": "1596799"
  },
  {
    "text": "it will be ga with the next sodium release which will go out shortly after kubecon it basically implements",
    "start": "1596799",
    "end": "1603679"
  },
  {
    "text": "efficient and efficient and scalable ebpf based enforcement with the help of",
    "start": "1603679",
    "end": "1609039"
  },
  {
    "text": "the earliest departure time model and with the work that we did around bbr",
    "start": "1609039",
    "end": "1616000"
  },
  {
    "text": "and to retain the uh timestamps across the network namespace when packets traverse them",
    "start": "1616000",
    "end": "1622880"
  },
  {
    "text": "it's we are basically like the first cni to support bbr that you can use and deploy your ports",
    "start": "1622880",
    "end": "1628320"
  },
  {
    "text": "with and as a side note i should mention that",
    "start": "1628320",
    "end": "1633360"
  },
  {
    "text": "realizing this whole architecture around the bandwidth manager is possible thanks to ebpf simply because with ebpf you can",
    "start": "1633360",
    "end": "1639760"
  },
  {
    "text": "do flexible traffic classification and also set the packet delivery timestamp",
    "start": "1639760",
    "end": "1648399"
  },
  {
    "text": "if you want to try out the bandwidth manager there's a getting started guide that we have in our selium documentation",
    "start": "1648399",
    "end": "1656000"
  },
  {
    "text": "the bandwidth manager itself if you want to use the edt based rate limiting for",
    "start": "1656000",
    "end": "1661200"
  },
  {
    "text": "parts it needs the kernel 5.1 the support for bbr is a little bit newer",
    "start": "1661200",
    "end": "1667840"
  },
  {
    "text": "because we got that merged around december in the linux kernel so it requires 518.",
    "start": "1667840",
    "end": "1675919"
  },
  {
    "text": "with that i would love to thank a couple of folks especially from google facebook and from our team",
    "start": "1677440",
    "end": "1683360"
  },
  {
    "text": "and the psyllium bpf and netdev kernel community and yeah so that's it for the talk",
    "start": "1683360",
    "end": "1690559"
  },
  {
    "text": "thanks thanks a lot and if you have questions i'm happy to answer [Applause]",
    "start": "1690559",
    "end": "1705389"
  },
  {
    "text": "yes not sure who will give you the mic",
    "start": "1706399",
    "end": "1712639"
  },
  {
    "text": "otherwise uh please shout or i will all right",
    "start": "1715600",
    "end": "1722200"
  },
  {
    "text": "uh yes so i should repeat the question um the question is if there's a",
    "start": "1736840",
    "end": "1743840"
  },
  {
    "text": "like a lot of bandwidth available and cubic is trying to ramp up quickly is this the same with bbr right",
    "start": "1743840",
    "end": "1749679"
  },
  {
    "text": "um yes it is it will just uh it just has a different uh delivery",
    "start": "1749679",
    "end": "1754960"
  },
  {
    "text": "model around it and it's not prone to packet loss but it will see when it uh it will see the delivery rate based",
    "start": "1754960",
    "end": "1761520"
  },
  {
    "text": "on the measurements and it will plateau at that point so yes",
    "start": "1761520",
    "end": "1767080"
  },
  {
    "text": "also if there's an increase in available bandwidth yes exactly they will both have the slow start ramp",
    "start": "1767760",
    "end": "1773760"
  },
  {
    "text": "up yeah cool",
    "start": "1773760",
    "end": "1779240"
  },
  {
    "text": "okay so the question is if you have a load balancer in the middle do you also have to set up bbr in the load balancer",
    "start": "1785360",
    "end": "1791039"
  },
  {
    "text": "no you don't so you only have to set up you only have to set it up on the server side so on your back ends because it's",
    "start": "1791039",
    "end": "1797919"
  },
  {
    "text": "actually transparent so it you really need to have it on the socket and that's in the back end in the application",
    "start": "1797919",
    "end": "1805440"
  },
  {
    "text": "yeah exactly because the packet is basically load balance based on l4 layer it you will just redirect it you will",
    "start": "1807440",
    "end": "1813360"
  },
  {
    "text": "forward it it's a it's a node in the middle yeah cool all right",
    "start": "1813360",
    "end": "1819600"
  },
  {
    "text": "if you have further questions we are still around here so you can just come to us and ask or ask in slack we will",
    "start": "1819600",
    "end": "1826080"
  },
  {
    "text": "try to look there and yeah with that thanks a lot [Applause]",
    "start": "1826080",
    "end": "1835339"
  }
]