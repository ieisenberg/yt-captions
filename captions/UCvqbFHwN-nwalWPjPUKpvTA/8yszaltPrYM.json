[
  {
    "start": "0",
    "end": "143000"
  },
  {
    "text": "hello everybody i'm patrick connolly i am the SEPA fests team lead at Red Hat",
    "start": "30",
    "end": "9110"
  },
  {
    "text": "my colleague Jeff Leighton couldn't make it to UConn this here so he won't be",
    "start": "9110",
    "end": "15839"
  },
  {
    "text": "joining me to give this talk so it'll just have me today I want to be talking about the new technology we've been",
    "start": "15839",
    "end": "24240"
  },
  {
    "text": "working on to have NFS servers deployed with the rook storage operator with",
    "start": "24240",
    "end": "35760"
  },
  {
    "text": "kubernetes and now these NFS servers export the set file system so to begin",
    "start": "35760",
    "end": "44239"
  },
  {
    "text": "since many of you may not be aware of SEP professed sefa Fest is a POSIX",
    "start": "44239",
    "end": "49890"
  },
  {
    "text": "distributed file system originally developed around 2005 by sage well to",
    "start": "49890",
    "end": "57210"
  },
  {
    "text": "serve as a file system for HPC clusters for the National Labs SEPA Fest is is",
    "start": "57210",
    "end": "74939"
  },
  {
    "text": "interesting because it uses the clients and MDS is cooperatively maintained a",
    "start": "74939",
    "end": "81900"
  },
  {
    "text": "distributed cache of the Dinos and and the directories that is sort of a",
    "start": "81900",
    "end": "88140"
  },
  {
    "text": "difference between many other distributed file systems where clients talk directly the servers and the server's manage all the state this was a",
    "start": "88140",
    "end": "95159"
  },
  {
    "text": "decision made early on to allow the clients to to have direct access to the",
    "start": "95159",
    "end": "103619"
  },
  {
    "text": "date to the data without needing to go through any type of gateway sefa fests",
    "start": "103619",
    "end": "110070"
  },
  {
    "text": "is provides full failover management by",
    "start": "110070",
    "end": "116939"
  },
  {
    "text": "the use of for example standby metadata servers it also provides for horizontal scale out by having multiple active",
    "start": "116939",
    "end": "123509"
  },
  {
    "text": "metadata servers served in a cluster the metadata servers do not maintain any",
    "start": "123509",
    "end": "129509"
  },
  {
    "text": "state they're very easy to containerize they a store older state in Rattus Rattus",
    "start": "129509",
    "end": "135989"
  },
  {
    "text": "being a distributed object data storage data store which is the main underlying",
    "start": "135989",
    "end": "141209"
  },
  {
    "text": "component of SEF on top of that the metadata servers represent the the",
    "start": "141209",
    "end": "150090"
  },
  {
    "start": "143000",
    "end": "196000"
  },
  {
    "text": "metadata hierarchy and the present that to the clients for access clients directly talked to the object store when",
    "start": "150090",
    "end": "156450"
  },
  {
    "text": "they're writing reading or writing to the the file objects in order to",
    "start": "156450",
    "end": "161819"
  },
  {
    "text": "maintain consistency the MDS hands out capabilities which are somewhere to leases from the academic concept they",
    "start": "161819",
    "end": "169530"
  },
  {
    "text": "hand these out to clients which allow them to delegate metadata for its",
    "start": "169530",
    "end": "175709"
  },
  {
    "text": "authority to clients in particular they can allow clients to write to data or",
    "start": "175709",
    "end": "181140"
  },
  {
    "text": "also buffer rights of of data or cache",
    "start": "181140",
    "end": "186209"
  },
  {
    "text": "reads and do this safely in such a way that it doesn't conflict or is",
    "start": "186209",
    "end": "191430"
  },
  {
    "text": "inconsistent with others clients so",
    "start": "191430",
    "end": "198150"
  },
  {
    "start": "196000",
    "end": "277000"
  },
  {
    "text": "moving on the origins of this project actually began with OpenStack there's",
    "start": "198150",
    "end": "203420"
  },
  {
    "text": "two primary services with an open stack that use SEF there is Manila which is a",
    "start": "203420",
    "end": "211670"
  },
  {
    "text": "file share service for virtual machines with an open stack and then also cinder",
    "start": "211670",
    "end": "217500"
  },
  {
    "text": "which is a block device provisioner for vm's used primarily to pray blue disks",
    "start": "217500",
    "end": "223620"
  },
  {
    "text": "for for a virtual machine to boot from and we've gone a great have done a lot",
    "start": "223620",
    "end": "235829"
  },
  {
    "text": "of integration with Stefan in this way and seth has become one of the more popular storage technologies to use with",
    "start": "235829",
    "end": "242940"
  },
  {
    "text": "an open stack Community Survey done in the past and a few years ago we found",
    "start": "242940",
    "end": "249780"
  },
  {
    "text": "that its ffs was the primary storage provider for for use within Manila and",
    "start": "249780",
    "end": "258269"
  },
  {
    "text": "the reasons for this I think are fairly straightforward Saif is a open source",
    "start": "258269",
    "end": "263840"
  },
  {
    "text": "storage solution and is easy to incorporate into your OpenStack",
    "start": "263840",
    "end": "271050"
  },
  {
    "text": "clusters and then also it's free so if",
    "start": "271050",
    "end": "278819"
  },
  {
    "start": "277000",
    "end": "373000"
  },
  {
    "text": "I'm talking about OpenStack then why what why not why am i here why are we",
    "start": "278819",
    "end": "286229"
  },
  {
    "text": "talking about Cooper why should I be talking about kubernetes so it turns out",
    "start": "286229",
    "end": "291870"
  },
  {
    "text": "that there is a big push right now as we all know that to manage distributed",
    "start": "291870",
    "end": "298919"
  },
  {
    "text": "services within the context of a container Orchestrator in this case kubernetes and there's a lot of",
    "start": "298919",
    "end": "304380"
  },
  {
    "text": "attractive reasons for doing this containers are lightweight and easy to",
    "start": "304380",
    "end": "311419"
  },
  {
    "text": "deploy in response to changing application needs importantly especially",
    "start": "311419",
    "end": "316440"
  },
  {
    "text": "for this project it allows for extensible service infrastructure so I can deploy services in a response to the",
    "start": "316440",
    "end": "323430"
  },
  {
    "text": "changing needs of the application containers are also parallel enough to",
    "start": "323430",
    "end": "329970"
  },
  {
    "text": "be are lightweight enough to allow for easy parallelism I can deploy a lot of",
    "start": "329970",
    "end": "335880"
  },
  {
    "text": "containers in response to sudden to those changing needs of the application",
    "start": "335880",
    "end": "341570"
  },
  {
    "text": "it's a much easier to do failover with with in a container orchestrating device",
    "start": "341570",
    "end": "347880"
  },
  {
    "text": "environment the instead of having a dedicated machine that is there to take",
    "start": "347880",
    "end": "355949"
  },
  {
    "text": "over when a service fails I can just spin up a new container anywhere in my infrastructure to take over that role",
    "start": "355949",
    "end": "363719"
  },
  {
    "text": "and finally kubernetes also allows us to do fast IP fell over management which",
    "start": "363719",
    "end": "369030"
  },
  {
    "text": "will become important later on in this discussion",
    "start": "369030",
    "end": "374240"
  },
  {
    "text": "okay so how we're using kubernetes",
    "start": "378660",
    "end": "383760"
  },
  {
    "text": "within SEF is through the oak storage operator within step there's now a big",
    "start": "383760",
    "end": "390030"
  },
  {
    "text": "push to deploy most SEF clusters in the future using kubernetes and rook rook is",
    "start": "390030",
    "end": "399140"
  },
  {
    "text": "one of the the main storage operators for for handle for handling storage",
    "start": "399140",
    "end": "404220"
  },
  {
    "text": "within the context of kubernetes and seth is one of the primary storage systems that it supports and the way",
    "start": "404220",
    "end": "415070"
  },
  {
    "text": "kubernetes are the way rook works within this context is I can define now a step",
    "start": "415070",
    "end": "421050"
  },
  {
    "text": "cluster that I wanted to apply within kubernetes and it's as simple as feeding it some yamo files into cube cuddle and",
    "start": "421050",
    "end": "428420"
  },
  {
    "text": "the rook or agents that are running on all the nodes will figure out where it",
    "start": "428420",
    "end": "435300"
  },
  {
    "text": "can host object storage devices with force F attaching an object storage",
    "start": "435300",
    "end": "441090"
  },
  {
    "text": "device to available disk drives or SSDs on all of your nodes in your kubernetes",
    "start": "441090",
    "end": "446370"
  },
  {
    "text": "cluster and then deploying the SEF demons as needed in order to get your",
    "start": "446370",
    "end": "451980"
  },
  {
    "text": "cluster ready the great hallmark success of this is that it allows you to deploy",
    "start": "451980",
    "end": "458100"
  },
  {
    "text": "a set cluster without knowing anything about SEF which is sort of store eclis been a very difficult thing to do",
    "start": "458100",
    "end": "464250"
  },
  {
    "text": "Steff being a distributed system had a lot of knobs to turn and a lot of upkeep",
    "start": "464250",
    "end": "471210"
  },
  {
    "text": "to do on all of your different computers that are serving SEF and now with these",
    "start": "471210",
    "end": "478140"
  },
  {
    "text": "storage operators this is becoming easier than ever to host this step cluster without knowing anything about",
    "start": "478140",
    "end": "483810"
  },
  {
    "text": "distributed storage",
    "start": "483810",
    "end": "486770"
  },
  {
    "start": "486000",
    "end": "615000"
  },
  {
    "text": "so coming back to the subject of this talk why do we want ace NFS gateway in",
    "start": "490119",
    "end": "496159"
  },
  {
    "text": "front of sev so you may remember when I was talking talking about earlier the",
    "start": "496159",
    "end": "502430"
  },
  {
    "text": "primary use case for CFS was for an HPC environment where we're running a lot of",
    "start": "502430",
    "end": "509229"
  },
  {
    "text": "high performance computing jobs that need access to fast file systems",
    "start": "509229",
    "end": "515360"
  },
  {
    "text": "especially for example for scratch space and that was his target use case and",
    "start": "515360",
    "end": "520388"
  },
  {
    "text": "because of that it was operating in a mostly trusted environment and the the",
    "start": "520389",
    "end": "525500"
  },
  {
    "text": "application could be trusted to to access the file system in a safe manner",
    "start": "525500",
    "end": "533199"
  },
  {
    "text": "in a in today's deployments and like OpenStack or kubernetes and how you may",
    "start": "533350",
    "end": "539750"
  },
  {
    "text": "want to serve your infrastructure to various tenants that may not be an",
    "start": "539750",
    "end": "545269"
  },
  {
    "text": "assumption you can safely make and there's a few reasons for that is not",
    "start": "545269",
    "end": "551029"
  },
  {
    "text": "necessarily easy to to restrict which",
    "start": "551029",
    "end": "556180"
  },
  {
    "text": "which types of clients are accessing your filesystem you may have for example",
    "start": "556180",
    "end": "561889"
  },
  {
    "text": "if I'm running with an open stack and I have virtual machines being brought up by my customers and they're running",
    "start": "561889",
    "end": "567889"
  },
  {
    "text": "older kernels it may not work without bugs against my chef cluster and I lack",
    "start": "567889",
    "end": "574880"
  },
  {
    "text": "control over what types of clients they're using to talk to Seth",
    "start": "574880",
    "end": "580449"
  },
  {
    "text": "another issue is security you may want to firewall off your storage cluster in",
    "start": "580910",
    "end": "586130"
  },
  {
    "text": "fact you probably do to prevent clients from having unrestricted access to your",
    "start": "586130",
    "end": "592279"
  },
  {
    "text": "to your storage network so there may be a firewall additionally you may want to",
    "start": "592279",
    "end": "597980"
  },
  {
    "text": "introduce some kind of authentication and authorization mechanisms for example to Kerberos which Ceph does not even",
    "start": "597980",
    "end": "604069"
  },
  {
    "text": "support and force clients to authenticate through those mechanisms so",
    "start": "604069",
    "end": "614449"
  },
  {
    "text": "moving on so for that reason it's attractive to",
    "start": "614449",
    "end": "620600"
  },
  {
    "start": "615000",
    "end": "775000"
  },
  {
    "text": "put an NFS demon in front of cephus and FS being a very common file access",
    "start": "620600",
    "end": "627980"
  },
  {
    "text": "protocol within within the industry and put that in front of SEF in order to",
    "start": "627980",
    "end": "635260"
  },
  {
    "text": "serve as a gateway between the clients and the SEF cluster NFS Ganesha is an",
    "start": "635260",
    "end": "644420"
  },
  {
    "text": "open source NFS server and it runs completely in user space without any dependencies on the kernel it's open",
    "start": "644420",
    "end": "651560"
  },
  {
    "text": "source software license under the LGPL what makes Ganesha attractive is that it",
    "start": "651560",
    "end": "658040"
  },
  {
    "text": "provides a plug-in interface for multiple different kinds of exports and then also provides support for exporting",
    "start": "658040",
    "end": "667100"
  },
  {
    "text": "different types of file systems the most obvious and usual case would be a local",
    "start": "667100",
    "end": "673280"
  },
  {
    "text": "file system having an FSO organ I should export the something in the local file",
    "start": "673280",
    "end": "678560"
  },
  {
    "text": "system but within our case we want to actually export this ffs file system to",
    "start": "678560",
    "end": "686630"
  },
  {
    "text": "do this Ganesha has these backends called the the file system abstraction",
    "start": "686630",
    "end": "691910"
  },
  {
    "text": "layer and that abstraction layer for staff uses lips ffs to actually talk to",
    "start": "691910",
    "end": "697010"
  },
  {
    "text": "the SEF cluster so any incoming NFS requests RPC will be translated into a",
    "start": "697010",
    "end": "704300"
  },
  {
    "text": "corresponding call to through lips ffs correspondingly Ganesha also uses",
    "start": "704300",
    "end": "713060"
  },
  {
    "text": "librettos which is a library Diedrich gives direct access to the object",
    "start": "713060",
    "end": "718580"
  },
  {
    "text": "storage layer to store its various state that it needs to maintain in order to",
    "start": "718580",
    "end": "723860"
  },
  {
    "text": "recover from a failover when a standby condition takes over it also uses",
    "start": "723860",
    "end": "732040"
  },
  {
    "text": "rathaus to now stores configuration files instead of using the Etsy",
    "start": "732040",
    "end": "738830"
  },
  {
    "text": "namespace within for example can is filesystem and for these reasons Ganesha",
    "start": "738830",
    "end": "745460"
  },
  {
    "text": "is very amenable to containerization we can store all of our are all the file system stages of course",
    "start": "745460",
    "end": "753160"
  },
  {
    "text": "stored in cephus we can store all the configuration and recovering information within rathaus there's no need for a",
    "start": "753160",
    "end": "760930"
  },
  {
    "text": "writable local file system [Music]",
    "start": "760930",
    "end": "768289"
  },
  {
    "text": "[Music]",
    "start": "771360",
    "end": "774419"
  },
  {
    "start": "775000",
    "end": "873000"
  },
  {
    "text": "so one of the things we wanted to have was scale out and that is we wanted to have multiple Ganesha's be able to",
    "start": "795220",
    "end": "802420"
  },
  {
    "text": "export the same cephus file system which is not as easy as you might think it to",
    "start": "802420",
    "end": "808450"
  },
  {
    "text": "be mostly for reasons of consistency we want to be able to have a services",
    "start": "808450",
    "end": "815140"
  },
  {
    "text": "container container or light container Aiza bowl so that we could use it in the",
    "start": "815140",
    "end": "821529"
  },
  {
    "text": "existing in the future work of container izing seft for use in a deployment",
    "start": "821529",
    "end": "828960"
  },
  {
    "text": "container deployment Orchestrator like kubernetes additionally we wanted to",
    "start": "828960",
    "end": "835089"
  },
  {
    "text": "support a newer and FS protocol which has several which notably allows for NFS",
    "start": "835089",
    "end": "841839"
  },
  {
    "text": "clients to store state so that they can get better performance",
    "start": "841839",
    "end": "848880"
  },
  {
    "text": "and finally the ability to talk to SEF",
    "start": "851810",
    "end": "857630"
  },
  {
    "text": "and rattles for for all this communication no need for any type of third-party cholesterin software for",
    "start": "857630",
    "end": "864020"
  },
  {
    "text": "example CT CLT DB which has normally",
    "start": "864020",
    "end": "869150"
  },
  {
    "text": "been used in the past for Ganesha to handle failover management so up to now",
    "start": "869150",
    "end": "878020"
  },
  {
    "start": "873000",
    "end": "942000"
  },
  {
    "text": "Ganesha has been deployed in a passive active fashion and this has been",
    "start": "878020",
    "end": "885640"
  },
  {
    "text": "available since cefalu - back in August 2017 you have won NFS server which all",
    "start": "885640",
    "end": "894320"
  },
  {
    "text": "of your NFS clients talk to and use the third-party tools pacemaker chorusing to",
    "start": "894320",
    "end": "902680"
  },
  {
    "text": "handle the failover between the active NFS Ganesha demon and the standby NFS",
    "start": "902680",
    "end": "909589"
  },
  {
    "text": "demon the main drawback of this approach is that it scales poorly and it requires",
    "start": "909589",
    "end": "914990"
  },
  {
    "text": "a lot of resources you need a idle NFS Ganesha server available to take over",
    "start": "914990",
    "end": "920150"
  },
  {
    "text": "when the active fails and all the NFS clients are talking to a single active",
    "start": "920150",
    "end": "925280"
  },
  {
    "text": "NFS server for all of their data needs which obviously will not scale very well",
    "start": "925280",
    "end": "932110"
  },
  {
    "text": "so one of our goals was to allow for all of these NFS clients to talk to multiple",
    "start": "932110",
    "end": "938260"
  },
  {
    "text": "NFS servers",
    "start": "938260",
    "end": "941650"
  },
  {
    "start": "942000",
    "end": "1067000"
  },
  {
    "text": "so why this is difficult to put multiple",
    "start": "944910",
    "end": "950040"
  },
  {
    "text": "NFS servers in front of the set file system in an active-active fashion is",
    "start": "950040",
    "end": "956630"
  },
  {
    "text": "because of the NFS clients now having a state in particular now and I've in",
    "start": "956630",
    "end": "971160"
  },
  {
    "text": "nfsv4 we have the the clients are now least given leases similar to",
    "start": "971160",
    "end": "976980"
  },
  {
    "text": "self-assess capabilities which allow them to read or write to files or hold locks and the clients must now contact",
    "start": "976980",
    "end": "985200"
  },
  {
    "text": "the NFS server during a police period between 45 and 60 seconds this had a few",
    "start": "985200",
    "end": "995520"
  },
  {
    "text": "problems mainly around failover of clients and in nfsv4 one which is the",
    "start": "995520",
    "end": "1004310"
  },
  {
    "text": "target protocol we're supporting they now have a sessions layer and add it on",
    "start": "1004310",
    "end": "1010880"
  },
  {
    "text": "to for the NFS clients when they're talking to the access server which provides exactly once semantics so you",
    "start": "1010880",
    "end": "1017720"
  },
  {
    "text": "don't when you're retrying an operation it doesn't happen multiple times and now",
    "start": "1017720",
    "end": "1025188"
  },
  {
    "text": "also a recant reclaim complete operation so when an NFS server goes down goes",
    "start": "1025189",
    "end": "1034188"
  },
  {
    "text": "down and it comes back up it becames a reclaimed process where all the NFS",
    "start": "1034189",
    "end": "1039319"
  },
  {
    "text": "clients can reestablish whatever state they had with the prior NFS server that",
    "start": "1039319",
    "end": "1045560"
  },
  {
    "text": "was if he waited the entire lease period that means nothing could actually happen with it in terms of i/o for any client",
    "start": "1045560",
    "end": "1051650"
  },
  {
    "text": "of the NFS server so they added a new",
    "start": "1051650",
    "end": "1056660"
  },
  {
    "text": "reclaim complete operation which allowed manifest server to lift the the grace",
    "start": "1056660",
    "end": "1062510"
  },
  {
    "text": "period early moving on",
    "start": "1062510",
    "end": "1069580"
  },
  {
    "start": "1067000",
    "end": "1163000"
  },
  {
    "text": "so after an NFS server restarts or a server takes over for four failed and a",
    "start": "1070590",
    "end": "1079269"
  },
  {
    "text": "best server it has no offense date because it's a dozen storage state",
    "start": "1079269",
    "end": "1086100"
  },
  {
    "text": "anywhere locally and so all the clients have to notify the NFS server exactly",
    "start": "1086100",
    "end": "1093399"
  },
  {
    "text": "what state they had that includes what files that opened how they had the files open any kind of locks they had and",
    "start": "1093399",
    "end": "1102570"
  },
  {
    "text": "because you don't want one client to say I had I want I I want to write to this",
    "start": "1102720",
    "end": "1111490"
  },
  {
    "text": "file and another client which is not reconnected yet has write access to that",
    "start": "1111490",
    "end": "1116649"
  },
  {
    "text": "file it may be buffering rights you need to wait a little bit to give all the clients that a chance to reconnect to",
    "start": "1116649",
    "end": "1122919"
  },
  {
    "text": "the server and that's what that's what this reclaim session period is is to lease periods at least very important of",
    "start": "1122919",
    "end": "1129789"
  },
  {
    "text": "corresponding to how long you wait for a client to say they are still holding on to a lease so during that grace period",
    "start": "1129789",
    "end": "1139360"
  },
  {
    "text": "the NFS server can't issue any new state and the clients may reclaim the state",
    "start": "1139360",
    "end": "1145000"
  },
  {
    "text": "they had prior to the crash and in fact this method of recovering all of the",
    "start": "1145000",
    "end": "1153340"
  },
  {
    "text": "state that you had open is very similar to what we do in sefa fess",
    "start": "1153340",
    "end": "1158340"
  },
  {
    "text": "[Music]",
    "start": "1158950",
    "end": "1162018"
  },
  {
    "text": "so logically this can be organized into a series of epics where the where an",
    "start": "1164610",
    "end": "1172720"
  },
  {
    "text": "epic corresponds to the lifetime of a particular instance of an ephah NFS demon beginning with a grace period",
    "start": "1172720",
    "end": "1178899"
  },
  {
    "text": "where all the NFS clients can reconnect and try to establish whatever state they had and then you have after that",
    "start": "1178899",
    "end": "1185740"
  },
  {
    "text": "completes the NFS server transitions into the normal operation which will be",
    "start": "1185740",
    "end": "1191309"
  },
  {
    "text": "generally a very long time that should be in a normal operation fail overs are not that common",
    "start": "1191309",
    "end": "1199980"
  },
  {
    "text": "[Music]",
    "start": "1200470",
    "end": "1203609"
  },
  {
    "text": "so the observation is that we can use the same logical organization of of the",
    "start": "1205590",
    "end": "1211830"
  },
  {
    "text": "grace periods and the normal operations and have this apply to a cluster of servers and why that's a difficult",
    "start": "1211830",
    "end": "1218400"
  },
  {
    "text": "problem is that if we have multiple NFS servers exporting the same backing file",
    "start": "1218400",
    "end": "1224520"
  },
  {
    "text": "system they will you may have state this",
    "start": "1224520",
    "end": "1232830"
  },
  {
    "text": "issue to one client of one NFS server and during a failover event another NFS",
    "start": "1232830",
    "end": "1240300"
  },
  {
    "text": "client of the other NFS server may try to acquire that state and so now we need",
    "start": "1240300",
    "end": "1246720"
  },
  {
    "text": "to form agreement on what state has been issued and how to reclaim it and so the way the NFS ganesha' handles that is",
    "start": "1246720",
    "end": "1253500"
  },
  {
    "start": "1250000",
    "end": "1309000"
  },
  {
    "text": "through the use of coordinating grace periods it has a central database in a rathaus object which keeps track of what",
    "start": "1253500",
    "end": "1262920"
  },
  {
    "text": "the current grace period epoch is for all of the NFS servers so that they can form consensus on on when the grace",
    "start": "1262920",
    "end": "1271470"
  },
  {
    "text": "period should happen so all the NFS servers together in the cluster will enter grace period together whenever",
    "start": "1271470",
    "end": "1280470"
  },
  {
    "text": "there's a failover event so that none of the NFS servers accidently issue state",
    "start": "1280470",
    "end": "1285750"
  },
  {
    "text": "that another client that needs to reconnect to a failed NFS server needs",
    "start": "1285750",
    "end": "1290970"
  },
  {
    "text": "to reclaim so these are some details on how that works that I won't get into due",
    "start": "1290970",
    "end": "1297570"
  },
  {
    "text": "to time but I'm willing to take questions on that later",
    "start": "1297570",
    "end": "1304580"
  },
  {
    "start": "1309000",
    "end": "1407000"
  },
  {
    "text": "so the next challenge we had to address with this work is we wanted to layer NFS",
    "start": "1309440",
    "end": "1315330"
  },
  {
    "text": "over a cluster file system and because",
    "start": "1315330",
    "end": "1320700"
  },
  {
    "text": "both of the protocols force ffs and also nfsv4 one are stateful and they use a",
    "start": "1320700",
    "end": "1327960"
  },
  {
    "text": "lease based mechanism you need some way to handle the case where an NFS server",
    "start": "1327960",
    "end": "1336590"
  },
  {
    "text": "fails and another one another server wants to come in to acquire that state and you need a way to make that failover",
    "start": "1336590",
    "end": "1346320"
  },
  {
    "text": "event happen faster in be within cephus namely sefa fest has its own timeouts",
    "start": "1346320",
    "end": "1354810"
  },
  {
    "text": "that it maintains for its clients whenever a client fails and comes back that client has a certain amount of time",
    "start": "1354810",
    "end": "1362340"
  },
  {
    "text": "to reestablish its session with SEPA fest likewise all those NFS daemons all",
    "start": "1362340",
    "end": "1368280"
  },
  {
    "text": "the NFS clients have a certain amount of time the corresponding to the least periods to to reconnect to the NFS",
    "start": "1368280",
    "end": "1376080"
  },
  {
    "text": "daemon to re-establish their state and there's some conflicts that are there's some overlap there in time which can",
    "start": "1376080",
    "end": "1382770"
  },
  {
    "text": "cause a very long delay for the NFS clients to re-establish their state and be able to continue the normal",
    "start": "1382770",
    "end": "1388740"
  },
  {
    "text": "operations they want to perform and and",
    "start": "1388740",
    "end": "1394670"
  },
  {
    "text": "how long it takes for the NFS server to reacquire the stadium means what's in in cephus",
    "start": "1394670",
    "end": "1401929"
  },
  {
    "text": "so one of the new features in Nautilus was that we allowed a client to reclaim",
    "start": "1407950",
    "end": "1413750"
  },
  {
    "text": "the state of a prior session when a cephus client dies abruptly the MDS will",
    "start": "1413750",
    "end": "1420020"
  },
  {
    "text": "keep its session around for several minutes allowing that client to come",
    "start": "1420020",
    "end": "1425210"
  },
  {
    "text": "back and say I'm still here and I want to reclaim all of these the capabilities",
    "start": "1425210",
    "end": "1433429"
  },
  {
    "text": "that I had all the file locks that I had and then with the presumption that these",
    "start": "1433429",
    "end": "1439640"
  },
  {
    "text": "clients are not actually have not actually failed and come and come back as a brand-new client there was simply a",
    "start": "1439640",
    "end": "1446059"
  },
  {
    "text": "network partition that separated them from them yes and for a while this was",
    "start": "1446059",
    "end": "1452840"
  },
  {
    "text": "fine enough for current for a kernel client which if it were to restart it",
    "start": "1452840",
    "end": "1460669"
  },
  {
    "text": "wouldn't be trying to get a new session with them yes who would create I wouldn't try to reclaim the old session",
    "start": "1460669",
    "end": "1468140"
  },
  {
    "text": "with him yes because all the applications that were running with the kernel client have also been rebooted so it just gets a fresh state is the same",
    "start": "1468140",
    "end": "1475250"
  },
  {
    "text": "thing with SEF views which is another alternative mounting mechanism for CFS",
    "start": "1475250",
    "end": "1481809"
  },
  {
    "text": "with NFS Ganesha we have this new problem where the NFS demon has state",
    "start": "1481809",
    "end": "1488600"
  },
  {
    "text": "that it's issued to the clients and if the daemon fails and comes back those it needs to reacquire all that state that",
    "start": "1488600",
    "end": "1495260"
  },
  {
    "text": "it had in the prior session so now in arles cephus supports in a",
    "start": "1495260",
    "end": "1501020"
  },
  {
    "text": "cephus client to come back and claim all the state that it had in a prior instance of the client without knowing",
    "start": "1501020",
    "end": "1508750"
  },
  {
    "text": "immediately what all that state was during the grace period for the NFS",
    "start": "1508750",
    "end": "1515330"
  },
  {
    "text": "server the clients will come in and say what state they had with the benefit the",
    "start": "1515330",
    "end": "1521809"
  },
  {
    "text": "prior instance of NFS demon what files I had opened correspondingly this the Ganesha server will talk to sefa fest",
    "start": "1521809",
    "end": "1528590"
  },
  {
    "text": "and reestablished that's the state that it had from the prior session when all of that is complete it actually",
    "start": "1528590",
    "end": "1536090"
  },
  {
    "text": "completes the session similar to the reclaim complete operation of NFS clients send",
    "start": "1536090",
    "end": "1543740"
  },
  {
    "text": "to the NFS server so our next challenge",
    "start": "1543740",
    "end": "1551929"
  },
  {
    "start": "1549000",
    "end": "1723000"
  },
  {
    "text": "is the deployment and management of NFS",
    "start": "1551929",
    "end": "1558770"
  },
  {
    "text": "clusters now we have all the basic",
    "start": "1558770",
    "end": "1564140"
  },
  {
    "text": "mechanisms in place to build a cluster of NFS servers that are in an active active configuration now what we need",
    "start": "1564140",
    "end": "1572330"
  },
  {
    "text": "next is a trivial or a simple way to deploy those NFS Kaneesha servers in an",
    "start": "1572330",
    "end": "1579110"
  },
  {
    "text": "active active configuration dynamically",
    "start": "1579110",
    "end": "1584169"
  },
  {
    "text": "in corresponding to whatever kind of load we have on the on that particular",
    "start": "1584169",
    "end": "1590840"
  },
  {
    "text": "export of the ffs file system though that the NFS clients are accessing and",
    "start": "1590840",
    "end": "1598520"
  },
  {
    "text": "then also we want to scale in another direction where we have an NFS cluster for each export in the set fest file",
    "start": "1598520",
    "end": "1607190"
  },
  {
    "text": "system we have so keep in mind this ffs can be a very large file system with billions of AI nodes and many different",
    "start": "1607190",
    "end": "1614840"
  },
  {
    "text": "users with different use cases and applications and so we want to be able",
    "start": "1614840",
    "end": "1620120"
  },
  {
    "text": "to have an NFS cluster front each sub tree within cephalus corresponding to",
    "start": "1620120",
    "end": "1627169"
  },
  {
    "text": "the application need and all of those and then you've achieved a few things by",
    "start": "1627169",
    "end": "1632900"
  },
  {
    "text": "doing that the you improve performance because now you are isolating certain",
    "start": "1632900",
    "end": "1638659"
  },
  {
    "text": "file system behaviors court with a group of NFS cluster daemons you're improving",
    "start": "1638659",
    "end": "1644059"
  },
  {
    "text": "the caching of the NFS daemons because now you have applications that are all",
    "start": "1644059",
    "end": "1649909"
  },
  {
    "text": "accessing the same manifest servers and you can also improve security because",
    "start": "1649909",
    "end": "1655460"
  },
  {
    "text": "now I am I can isolate the the cluster of NFS daemons based off of which",
    "start": "1655460",
    "end": "1661700"
  },
  {
    "text": "applications should be using them and furthermore isolate which parts of this",
    "start": "1661700",
    "end": "1666950"
  },
  {
    "text": "ffs file system tree that NFS demons have access to based off of what applications we'll be using those",
    "start": "1666950",
    "end": "1674330"
  },
  {
    "text": "NFS clusters finally another challenge",
    "start": "1674330",
    "end": "1681020"
  },
  {
    "text": "we need to address is IP main great migration and failover so when an NFS",
    "start": "1681020",
    "end": "1686390"
  },
  {
    "text": "server and evitable II may die for some for some reason we need a way to spin up a replacement very quickly and cheaply",
    "start": "1686390",
    "end": "1693260"
  },
  {
    "text": "and also we need a way a mechanism to migrate an IP address from the prior",
    "start": "1693260",
    "end": "1701030"
  },
  {
    "text": "instance of the NFS server to the new instance and these are historically",
    "start": "1701030",
    "end": "1706490"
  },
  {
    "text": "decision difficult problems but not anymore and the reason for that is",
    "start": "1706490",
    "end": "1711860"
  },
  {
    "text": "kubernetes is fairly simple to spin up new containers in response to failover",
    "start": "1711860",
    "end": "1717410"
  },
  {
    "text": "and also you can migrate the IP address to the new pod so this is where I rock",
    "start": "1717410",
    "end": "1725420"
  },
  {
    "start": "1723000",
    "end": "1855000"
  },
  {
    "text": "and communities come in rook now supports in the version 1.0 release that",
    "start": "1725420",
    "end": "1730880"
  },
  {
    "text": "just came out the ability to specify a set NFS resource type which will launch",
    "start": "1730880",
    "end": "1737800"
  },
  {
    "text": "the NFS Ganesha servers in response to",
    "start": "1737800",
    "end": "1745360"
  },
  {
    "text": "the cute cuddle command creating those objects and wrote in that and now staff",
    "start": "1745360",
    "end": "1754460"
  },
  {
    "text": "have a deep integration with each other between the the SEF Nautilus release which was just released about two months",
    "start": "1754460",
    "end": "1761780"
  },
  {
    "text": "ago and rook itself so now SEF is also extendable and extensible and with rook",
    "start": "1761780",
    "end": "1769490"
  },
  {
    "text": "in that it now is able to deploy services in response to to the changing",
    "start": "1769490",
    "end": "1776420"
  },
  {
    "text": "application needs for example creating a new file system by talking to rook and",
    "start": "1776420",
    "end": "1782390"
  },
  {
    "text": "saying I you know I need another metadata server please spin another container up and this has all been",
    "start": "1782390",
    "end": "1788300"
  },
  {
    "text": "abstracted away very nicely and the look we were playing we are able to use this",
    "start": "1788300",
    "end": "1796010"
  },
  {
    "text": "to have the manager daemon and deploy NFS clusters in response to",
    "start": "1796010",
    "end": "1801250"
  },
  {
    "text": "what the user specifies via the how they",
    "start": "1801250",
    "end": "1811240"
  },
  {
    "text": "specify the NFS closer for their for their file system furthermore we can",
    "start": "1811240",
    "end": "1817480"
  },
  {
    "text": "scale up the NFS cluster up or down in response to how the application needs",
    "start": "1817480",
    "end": "1823720"
  },
  {
    "text": "change and have this whole centrally managed from a manager in the future we",
    "start": "1823720",
    "end": "1829090"
  },
  {
    "text": "also want to use this too and it's sort of a holy grail type concept within cephalus we've always thought it would",
    "start": "1829090",
    "end": "1835179"
  },
  {
    "text": "be very nice to be able to deploy more active metadata servers in response to the need to change changing load of the",
    "start": "1835179",
    "end": "1842080"
  },
  {
    "text": "file system we also plan to use it the same the same idea imply this within",
    "start": "1842080",
    "end": "1852159"
  },
  {
    "text": "cephus as well to to scale on the SOC so",
    "start": "1852159",
    "end": "1857880"
  },
  {
    "start": "1855000",
    "end": "1949000"
  },
  {
    "text": "in summary of this work we've built several mechanisms that are available",
    "start": "1857880",
    "end": "1863380"
  },
  {
    "text": "today to deploy to deploy these NFS",
    "start": "1863380",
    "end": "1869440"
  },
  {
    "text": "clusters in front of staff now we have this ability to create volumes and sub",
    "start": "1869440",
    "end": "1876010"
  },
  {
    "text": "volumes volumes corresponding to separate file systems which multiple set file systems are not yet supported",
    "start": "1876010",
    "end": "1882460"
  },
  {
    "text": "within SEF is something we're planning to do in a future release the more common thing to do is to have the sub",
    "start": "1882460",
    "end": "1888760"
  },
  {
    "text": "volume concept where you separate or you dedicate an entire directory tree to a",
    "start": "1888760",
    "end": "1893919"
  },
  {
    "text": "volume which maybe is exported to a particular person machine or a set of",
    "start": "1893919",
    "end": "1899890"
  },
  {
    "text": "containers for sharing and then this",
    "start": "1899890",
    "end": "1905620"
  },
  {
    "text": "allows Manila for OpenStack or the CSI interface for kubernetes all of your",
    "start": "1905620",
    "end": "1910840"
  },
  {
    "text": "pods can use the same interface to talk to are to set up configure volumes and then also configure the exports of those",
    "start": "1910840",
    "end": "1918039"
  },
  {
    "text": "volumes the other mechanism we have finished and built is the real concep",
    "start": "1918039",
    "end": "1923980"
  },
  {
    "text": "integration so now self can launch these NFS clusters dynamically and configure",
    "start": "1923980",
    "end": "1930549"
  },
  {
    "text": "these and if s cluster is to have a set of exports and can face corresponding to",
    "start": "1930549",
    "end": "1937259"
  },
  {
    "text": "these sub volumes and finally we've updated NFS again I should to be cluster",
    "start": "1937259",
    "end": "1942600"
  },
  {
    "text": "aware and have faster and correct recovery with asset file system and as",
    "start": "1942600",
    "end": "1950759"
  },
  {
    "start": "1949000",
    "end": "2198000"
  },
  {
    "text": "far as vision for the future again we have all these mechanisms built what the",
    "start": "1950759",
    "end": "1957330"
  },
  {
    "text": "next stage of our work is to actually make this much very turnkey in terms of",
    "start": "1957330",
    "end": "1967049"
  },
  {
    "text": "actually configuring the volume and then setting up the NFS kanesha cluster so to",
    "start": "1967049",
    "end": "1972769"
  },
  {
    "text": "in version fourteen point two point two the next version of this FS Nautilus are",
    "start": "1972769",
    "end": "1979799"
  },
  {
    "text": "sort of Sept Nautilus release you'll be able to create these sub volumes through a centralized command and then in a",
    "start": "1979799",
    "end": "1988220"
  },
  {
    "text": "subsequent future release we're planning to make it very simple to actually deploy an NFS cluster in an",
    "start": "1988220",
    "end": "1995519"
  },
  {
    "text": "active-active fashion in front of staff with rook and kubernetes just with a",
    "start": "1995519",
    "end": "2000830"
  },
  {
    "text": "simple command enabling NFS and setting",
    "start": "2000830",
    "end": "2006320"
  },
  {
    "text": "various configuration options you might want to have for example what kind of an",
    "start": "2006320",
    "end": "2011989"
  },
  {
    "text": "effect our network namespace you want to attach the pods to in terms of say for",
    "start": "2011989",
    "end": "2017239"
  },
  {
    "text": "example if I have OpenStack virtual",
    "start": "2017239",
    "end": "2022609"
  },
  {
    "text": "machines that have been set up and I want to attach my NFS kinesha clusters",
    "start": "2022609",
    "end": "2027619"
  },
  {
    "text": "to the same network namespace that that those that those tenants are in then I",
    "start": "2027619",
    "end": "2034759"
  },
  {
    "text": "can configure that through this this command and then finally turn it on by",
    "start": "2034759",
    "end": "2040609"
  },
  {
    "text": "just setting sharing a Festa true",
    "start": "2040609",
    "end": "2044080"
  },
  {
    "text": "furthermore we also want to add support for NFS version with the NFS version for",
    "start": "2046060",
    "end": "2052250"
  },
  {
    "text": "migration of clients so we want to be able to not just expand the size of the",
    "start": "2052250",
    "end": "2058159"
  },
  {
    "text": "NFS cluster but then also shrink it and that involves shifting all the clients of the NFS server that you want to",
    "start": "2058160",
    "end": "2064159"
  },
  {
    "text": "remove and have them move on to one of the remaining NFS servers within the",
    "start": "2064160",
    "end": "2069230"
  },
  {
    "text": "cluster and then finally optimizing the",
    "start": "2069230",
    "end": "2075260"
  },
  {
    "text": "grace periods for sub volumes because right now all the NFS clusters are within the same logical cluster in terms",
    "start": "2075260",
    "end": "2081020"
  },
  {
    "text": "of grace periods when that may not be necessary because salt sub volumes within sefa that's may not share any I",
    "start": "2081020",
    "end": "2087620"
  },
  {
    "text": "knows so there is no shared state between sub volumes it would be better for the NFS server clusters to be",
    "start": "2087620",
    "end": "2095030"
  },
  {
    "text": "isolated from each other or not enter grace periods together because there would be no conflicting state between",
    "start": "2095030",
    "end": "2100370"
  },
  {
    "text": "them and then finally it would also be nice to use this for SMB so that's all",
    "start": "2100370",
    "end": "2107030"
  },
  {
    "text": "my time thank you for your attention and coming to my talk I'll take any",
    "start": "2107030",
    "end": "2112130"
  },
  {
    "text": "questions",
    "start": "2112130",
    "end": "2114250"
  },
  {
    "text": "so I am from a manifest account and F Sarah communicate aways therefore so the",
    "start": "2139060",
    "end": "2146720"
  },
  {
    "text": "AO they share the same network so the NFS",
    "start": "2146720",
    "end": "2153800"
  },
  {
    "text": "clients may have be on a different network name space then the then the storage one is everything for another",
    "start": "2153800",
    "end": "2160880"
  },
  {
    "text": "server yeah so I have a server so am i have another server have a tool network",
    "start": "2160880",
    "end": "2167839"
  },
  {
    "text": "yes it would have two networks which requires newer features as I understand",
    "start": "2167839",
    "end": "2173270"
  },
  {
    "text": "it in kubernetes to be able to enable having multiple network namespaces attached to a pod has not historically",
    "start": "2173270",
    "end": "2179720"
  },
  {
    "text": "been possible any other questions",
    "start": "2179720",
    "end": "2187599"
  },
  {
    "text": "okay thank you for your time",
    "start": "2191960",
    "end": "2195859"
  }
]