[
  {
    "text": "hello everyone my name is jeron i'm going to present this time online uh on this",
    "start": "80",
    "end": "6720"
  },
  {
    "text": "topic of how to essentially extend kubeflow which is the machine learning",
    "start": "6720",
    "end": "12240"
  },
  {
    "text": "framework on top of kubernetes how to extend it and simplify it using serverless technologies and make",
    "start": "12240",
    "end": "18560"
  },
  {
    "text": "it scalable and easy so first few words on myself i'm iran i'm cto and",
    "start": "18560",
    "end": "25680"
  },
  {
    "text": "co-founder for a company called iguazio we're delivering automated data science",
    "start": "25680",
    "end": "30880"
  },
  {
    "text": "platform oriented towards production based on kubernetes and also incorporates cube flow is part",
    "start": "30880",
    "end": "37600"
  },
  {
    "text": "of it so before i talk about kubeflow and",
    "start": "37600",
    "end": "43120"
  },
  {
    "text": "and serverless and let's examine what are the real challenges that we're trying to tackle so today what you see in most",
    "start": "43120",
    "end": "50480"
  },
  {
    "text": "organizations machine learning is still pretty much in the research environment uh research environment people take a",
    "start": "50480",
    "end": "57120"
  },
  {
    "text": "few csv files excel spreadsheets you know they upload some data run a notebook or jupyter or something",
    "start": "57120",
    "end": "64799"
  },
  {
    "text": "like that run some analysis on the data and based on that they run some training and",
    "start": "64799",
    "end": "71920"
  },
  {
    "text": "generate models in some cases taking those models and put them behind an http endpoint",
    "start": "71920",
    "end": "77520"
  },
  {
    "text": "and serving those uh the challenge is really when you're going to a real application think about",
    "start": "77520",
    "end": "83040"
  },
  {
    "text": "applications do fraud detection real-time recommendations uh things that do predictive maintenance",
    "start": "83040",
    "end": "90799"
  },
  {
    "text": "etc those are real pipelines and machine learning is just a component within this pipeline",
    "start": "90799",
    "end": "96560"
  },
  {
    "text": "and also needs to perform fast in reasonable uh performance with highly high availability and scalability and",
    "start": "96560",
    "end": "104159"
  },
  {
    "text": "and all that attributes of a production system so what we we see is that the need to",
    "start": "104159",
    "end": "109200"
  },
  {
    "text": "move from this research environment to to a stack where you have real-time",
    "start": "109200",
    "end": "114479"
  },
  {
    "text": "ingestion from variety of sources whether it's through etl process from operational databases",
    "start": "114479",
    "end": "120719"
  },
  {
    "text": "uh streaming data from you know transaction feeds logs etc integrating with external api which",
    "start": "120719",
    "end": "128080"
  },
  {
    "text": "may extend our data set quite a bit pushing it all into an analytics engine this analytic engine",
    "start": "128080",
    "end": "135840"
  },
  {
    "text": "does a data preparation at scale usually on a lot more data than what you would",
    "start": "135840",
    "end": "141520"
  },
  {
    "text": "build in your own notebook sometimes terabytes of data versus few gigabytes on",
    "start": "141520",
    "end": "146879"
  },
  {
    "text": "an in-memory notebook once you've done that you need an automated system to run",
    "start": "146879",
    "end": "152480"
  },
  {
    "text": "various experiments training with different sets of parameters with different algorithms referring to it as server",
    "start": "152480",
    "end": "159360"
  },
  {
    "text": "automl and those things generate machine learning models that are served",
    "start": "159360",
    "end": "165280"
  },
  {
    "text": "using real-time interactive and those micro services may get event",
    "start": "165280",
    "end": "172080"
  },
  {
    "text": "through http or through streaming like kafka or kinesis et cetera",
    "start": "172080",
    "end": "178159"
  },
  {
    "text": "but they also need to get some real data that is very much aligned with the data we use for for",
    "start": "178159",
    "end": "184720"
  },
  {
    "text": "training so we need data preparation also for the productization part for the serving part",
    "start": "184720",
    "end": "190640"
  },
  {
    "text": "and once we've done that we also need a monitoring essentially a feedback loop to monitor our models",
    "start": "190640",
    "end": "196080"
  },
  {
    "text": "detect drift in our model behavior and sometimes do automated retraining or",
    "start": "196080",
    "end": "201920"
  },
  {
    "text": "formation of ensembles uh changing weights and and other things to mitigate uh loss of",
    "start": "201920",
    "end": "208239"
  },
  {
    "text": "accuracy in our in our models now the main challenge is today that those are usually two different",
    "start": "208239",
    "end": "213920"
  },
  {
    "text": "departments in an organization there's one group that builds data science products or",
    "start": "213920",
    "end": "219040"
  },
  {
    "text": "models there's a different group that tries to productize all of that and that in many cases involve",
    "start": "219040",
    "end": "225280"
  },
  {
    "text": "refactoring the entire set of code and project takes forever you know according to analysts more than",
    "start": "225280",
    "end": "231280"
  },
  {
    "text": "eighty percent of the project just fail uh flat due to this transition uh",
    "start": "231280",
    "end": "236640"
  },
  {
    "text": "of essentially rewriting everything recomposing it you know taking python code changing it to",
    "start": "236640",
    "end": "242239"
  },
  {
    "text": "spark and scala sometimes moving code to c and go to make it higher performance",
    "start": "242239",
    "end": "249599"
  },
  {
    "text": "and using different paradigms so that's one of the the major challenges we need to address",
    "start": "249599",
    "end": "255200"
  },
  {
    "text": "now if you think about uh what is the basic element the basic element that data",
    "start": "255200",
    "end": "260479"
  },
  {
    "text": "scientists write their code in a notebook and in many cases they",
    "start": "260479",
    "end": "265520"
  },
  {
    "text": "they take their code and they create very long notebooks um that comprise of different steps in",
    "start": "265520",
    "end": "271759"
  },
  {
    "text": "the workflow like uh writing a single sanction that reads some data and then as some some things that",
    "start": "271759",
    "end": "278080"
  },
  {
    "text": "generate charts and some things that generate a training set and then uh running some validation all within the same",
    "start": "278080",
    "end": "284639"
  },
  {
    "text": "jumbo notebook and finally running um some surveying within the notebook so",
    "start": "284639",
    "end": "291120"
  },
  {
    "text": "the first thing we we need to think about is how to transform those",
    "start": "291120",
    "end": "296560"
  },
  {
    "text": "notebooks into microservices that we can later use as part of a queue flow pipeline or an",
    "start": "296560",
    "end": "303440"
  },
  {
    "text": "orchestrated workflow um and that's the the first challenge and that may involve some work",
    "start": "303440",
    "end": "309440"
  },
  {
    "text": "also on the data scientist part to break down his code to model our functional",
    "start": "309440",
    "end": "315120"
  },
  {
    "text": "building blocks but that's the first thing that we need to to think about",
    "start": "315120",
    "end": "322160"
  },
  {
    "text": "and and once we we're thinking okay we've created this piece of code that needs to go into production uh as a",
    "start": "322400",
    "end": "330000"
  },
  {
    "text": "training logic or a feature preparation logic or ensembl logic etc",
    "start": "330000",
    "end": "336400"
  },
  {
    "text": "those are all the steps that it needs to go through uh from just having a python code to",
    "start": "336400",
    "end": "343120"
  },
  {
    "text": "having a production worthy microservice that's automated in production so for example",
    "start": "343120",
    "end": "350080"
  },
  {
    "text": "we need to take the code and add dependencies and packages to it and build a docker container out of our",
    "start": "350080",
    "end": "356880"
  },
  {
    "text": "code with run scripts and parameters and arguments and all of that",
    "start": "356880",
    "end": "362160"
  },
  {
    "text": "which are not intuitive for too many data scientists the next thing which is very challenging",
    "start": "362160",
    "end": "367919"
  },
  {
    "text": "is how do we take a code that runs on a single uh process single container",
    "start": "367919",
    "end": "373520"
  },
  {
    "text": "and we distribute that because we have a lot more load more data or a bigger computation problem to solve",
    "start": "373520",
    "end": "380240"
  },
  {
    "text": "we want to leverage the uh the benefits of kubernetes of scaling out",
    "start": "380240",
    "end": "386160"
  },
  {
    "text": "resources to essentially break that piece of code into smaller",
    "start": "386160",
    "end": "391199"
  },
  {
    "text": "pieces that work in concert to generate a bigger to address a bigger workload so in many",
    "start": "391199",
    "end": "397280"
  },
  {
    "text": "cases that actually require refactoring of the code for distributed computing",
    "start": "397280",
    "end": "402560"
  },
  {
    "text": "now most cases the codex design in research is not fitted into production environment",
    "start": "402560",
    "end": "410080"
  },
  {
    "text": "because of simply performance people don't necessarily think about parallelism",
    "start": "410080",
    "end": "415840"
  },
  {
    "text": "about potentially incorporating caching technologies so they don't just go and read the data",
    "start": "415840",
    "end": "422080"
  },
  {
    "text": "on every request sometimes gpus make a lot of sense and we need to",
    "start": "422080",
    "end": "427680"
  },
  {
    "text": "incorporate libraries for cuda and maybe change our code to support gpus and",
    "start": "427680",
    "end": "434840"
  },
  {
    "text": "etc but when we have a code which is scalable performance package the next",
    "start": "434840",
    "end": "440720"
  },
  {
    "text": "thing we have to start thinking about is observability monitoring but it's not just monitoring in the",
    "start": "440720",
    "end": "446240"
  },
  {
    "text": "traditional sense of kubernetes and microservices of you know cpus memory etc we need to",
    "start": "446240",
    "end": "451840"
  },
  {
    "text": "monitor and log everything we need to monitor the experiments that we're doing the data that's generated with the",
    "start": "451840",
    "end": "458400"
  },
  {
    "text": "experiment the code that was used in order to generate the experiment et cetera because there is a big portion",
    "start": "458400",
    "end": "463759"
  },
  {
    "text": "of liability and explainability as part of every machine learning project",
    "start": "463759",
    "end": "468960"
  },
  {
    "text": "we run a project we generate some results we want to be able to go back to the same experiment or we",
    "start": "468960",
    "end": "474879"
  },
  {
    "text": "launched something to production we had some issues with how we predict outcomes",
    "start": "474879",
    "end": "479919"
  },
  {
    "text": "someone sues us in a court of law we need to be able to explain uh what we've done and what was the what",
    "start": "479919",
    "end": "486319"
  },
  {
    "text": "were the models and how we train the models and make sure that we haven't done any mistake in that so",
    "start": "486319",
    "end": "492240"
  },
  {
    "text": "the monitoring and the observability part in machine learning is much much more extensive than what it is when",
    "start": "492240",
    "end": "498319"
  },
  {
    "text": "you're talking about standard micro services and the other aspect is security of course",
    "start": "498319",
    "end": "503919"
  },
  {
    "text": "the security has many uh faces there's data security api security",
    "start": "503919",
    "end": "510240"
  },
  {
    "text": "uh role-based access control uh access to credentials and secrets for",
    "start": "510240",
    "end": "515518"
  },
  {
    "text": "external databases and services and so on uh and the final piece in order to make",
    "start": "515519",
    "end": "521440"
  },
  {
    "text": "this workflow not sort of a single or one-time flow but something that can reiterate",
    "start": "521440",
    "end": "527200"
  },
  {
    "text": "and go on and on is automation we're all familiar with ci cd",
    "start": "527200",
    "end": "532959"
  },
  {
    "text": "in the notion of computing computation and coding we have to apply a similar approach of",
    "start": "532959",
    "end": "538640"
  },
  {
    "text": "ci cd and githubs for machine learning we need to build workflows that incorporate machine",
    "start": "538640",
    "end": "546000"
  },
  {
    "text": "learning workflows they're slightly different than our usual ci workflows because",
    "start": "546000",
    "end": "551120"
  },
  {
    "text": "they require a lot more computation and there's a lot more data involved in uh machine",
    "start": "551120",
    "end": "556640"
  },
  {
    "text": "learning through workflow this is really where kubeflow comes into play and we'll talk about it",
    "start": "556640",
    "end": "562640"
  },
  {
    "text": "and obviously we need to think about continuous deployment ruling upgrades canary deployments a b",
    "start": "562640",
    "end": "568720"
  },
  {
    "text": "testing etc usually this part from the development to productization",
    "start": "568720",
    "end": "575200"
  },
  {
    "text": "is the biggest spark in every project so there are some companies where i work with them and it",
    "start": "575200",
    "end": "580800"
  },
  {
    "text": "takes them like 12 to 18 months with the original project to deploy into",
    "start": "580800",
    "end": "586480"
  },
  {
    "text": "production and it's very essential that we try and automate those processes one of the technologies",
    "start": "586480",
    "end": "593440"
  },
  {
    "text": "that we're going to show you how it's pretty significant can bring significant",
    "start": "593440",
    "end": "599760"
  },
  {
    "text": "advantage is serverless because this is essentially what service technologies in general do they allow you to bring code they",
    "start": "599760",
    "end": "606240"
  },
  {
    "text": "automatically do the packaging they have inherent scaling out they do the instrumentation",
    "start": "606240",
    "end": "611760"
  },
  {
    "text": "and in some cases the automation we need to add some other significant parts which are missing from",
    "start": "611760",
    "end": "617680"
  },
  {
    "text": "traditional serverless which are around performance and parallelism and data statefulness",
    "start": "617680",
    "end": "623360"
  },
  {
    "text": "and and also more specific you know tracking and observability for",
    "start": "623360",
    "end": "628480"
  },
  {
    "text": "machine learning so we'll see how those things take effect",
    "start": "628480",
    "end": "635360"
  },
  {
    "text": "so first what is uh amalops before we dive into an amalops or presentation we all know what devops is",
    "start": "636240",
    "end": "643440"
  },
  {
    "text": "essentially this is according to the the traditional uh definition is",
    "start": "643440",
    "end": "648480"
  },
  {
    "text": "essentially combining development and operations to work together and and try and solve the main business challenges so",
    "start": "648480",
    "end": "656640"
  },
  {
    "text": "mlaps is pretty much the same just embedding all the aspects of data engineering and data science",
    "start": "656640",
    "end": "662320"
  },
  {
    "text": "into the process so it's not just developers and operations it's developers data scientists data",
    "start": "662320",
    "end": "667680"
  },
  {
    "text": "engineers et cetera they need to collaborate and one of the key challenges is not everyone is a programmer",
    "start": "667680",
    "end": "673600"
  },
  {
    "text": "or a devops guy in this world because data scientists they don't have necessarily all the",
    "start": "673600",
    "end": "679760"
  },
  {
    "text": "software practices that and software engineers have and we need to be mindful of that and try and simplify not",
    "start": "679760",
    "end": "687440"
  },
  {
    "text": "necessarily force the ammos and dockerfiles on the data scientist but try and speak their own language",
    "start": "687440",
    "end": "693040"
  },
  {
    "text": "when we present them apis and tools to work with so let's before we dive into the",
    "start": "693040",
    "end": "699760"
  },
  {
    "text": "solution let's look at the typical use case this is a use case that we've deployed on several places there's also",
    "start": "699760",
    "end": "707600"
  },
  {
    "text": "a git repo that you can see here which has a of the full",
    "start": "707600",
    "end": "712959"
  },
  {
    "text": "example with all the services so if we're looking at uh this is a",
    "start": "712959",
    "end": "718000"
  },
  {
    "text": "predictive maintenance pipeline how would it look like so predictive maintenance pipeline would accept",
    "start": "718000",
    "end": "724000"
  },
  {
    "text": "data from two separate sources usually or more like an operational database that gives",
    "start": "724000",
    "end": "729760"
  },
  {
    "text": "us information about devices entities etc and we may have streaming data usually in the form of a kafka",
    "start": "729760",
    "end": "736639"
  },
  {
    "text": "stream that arrives and tell us information about like telemetry data device updates alerts etc",
    "start": "736639",
    "end": "744800"
  },
  {
    "text": "what we need to do is essentially ingest all of that data the operational data coupled with the",
    "start": "744800",
    "end": "750480"
  },
  {
    "text": "streaming and transactional data and throw it into a road data bucket road data bucket could just be",
    "start": "750480",
    "end": "756639"
  },
  {
    "text": "a shared file system with parquet files or csv files uh could also be something slightly more",
    "start": "756639",
    "end": "763519"
  },
  {
    "text": "structured like a data warehouse but the first thing we need to do is place everything",
    "start": "763519",
    "end": "768720"
  },
  {
    "text": "in this sort of road data bucket the next thing we want to do is essentially start taking those raw",
    "start": "768720",
    "end": "774800"
  },
  {
    "text": "features and convert them to something meaningful that we can run training on so we may need to join various data sets to",
    "start": "774800",
    "end": "781839"
  },
  {
    "text": "extend uh the knowledge we may need to aggregate uh things into buckets of time like hourly",
    "start": "781839",
    "end": "788959"
  },
  {
    "text": "average number of events you know what's the time from the last error to the recent error and all sorts of what is",
    "start": "788959",
    "end": "796079"
  },
  {
    "text": "referred to as features in machine learning so it requires sort of a strong and in high performance analytics capabilities",
    "start": "796079",
    "end": "803519"
  },
  {
    "text": "and then we have a new data set which is usually a single table unlike the source which could comprise",
    "start": "803519",
    "end": "809600"
  },
  {
    "text": "of multiple tables here we have a essentially a single denormalized table which is",
    "start": "809600",
    "end": "814639"
  },
  {
    "text": "raw going in through a training logic but even before going into a training logic",
    "start": "814639",
    "end": "820480"
  },
  {
    "text": "we need to sanitize those features and if we have thousands of features maybe we don't really need to train on",
    "start": "820480",
    "end": "826240"
  },
  {
    "text": "thousands of features again it's more computational power it's uh going to",
    "start": "826240",
    "end": "831760"
  },
  {
    "text": "uh have skewing in the model if you're not going to learn based on the right features so the",
    "start": "831760",
    "end": "837040"
  },
  {
    "text": "next thing is to go and filter and auto detect what are the meaningful features",
    "start": "837040",
    "end": "842480"
  },
  {
    "text": "and and leverage those and maybe scale those to the right degree once we have those prepared",
    "start": "842480",
    "end": "849839"
  },
  {
    "text": "features we may need to go and run training and the training will generate a model this model uh will be later used",
    "start": "849839",
    "end": "857680"
  },
  {
    "text": "for uh for serving but also before we we use that model we want to validate it using",
    "start": "857680",
    "end": "864320"
  },
  {
    "text": "a different uh smaller data set at the essentially splitting the entire data to",
    "start": "864320",
    "end": "870320"
  },
  {
    "text": "a training set in a validation set and then we would generate this model",
    "start": "870320",
    "end": "875440"
  },
  {
    "text": "uh we take this model and validate it actually performs and what's the accuracy if it doesn't we",
    "start": "875440",
    "end": "881440"
  },
  {
    "text": "may need to go back and forth and tune this entire pipeline now once we have a model we're not done this is",
    "start": "881440",
    "end": "888079"
  },
  {
    "text": "really what we've just covered is the model creation part which is covered nicely with with",
    "start": "888079",
    "end": "894639"
  },
  {
    "text": "kubeflow with the extension of what we've discussed essentially moving the individual steps in the pipeline",
    "start": "894639",
    "end": "901519"
  },
  {
    "text": "to a reusable high performance scalable function we'll we'll see the architecture in a",
    "start": "901519",
    "end": "906800"
  },
  {
    "text": "minute the next piece in the in the puzzle is is essentially running a serving",
    "start": "906800",
    "end": "914580"
  },
  {
    "text": "[Music] and so we take the data we take the model we can't just stay with them all we have",
    "start": "914580",
    "end": "920320"
  },
  {
    "text": "to bring data because remember what the serving logic does is essentially takes a set of",
    "start": "920320",
    "end": "925680"
  },
  {
    "text": "parameters a set of features and runs some mathematical functions and produces a",
    "start": "925680",
    "end": "931120"
  },
  {
    "text": "result so and the request coming from the user doesn't contain all those features",
    "start": "931120",
    "end": "938240"
  },
  {
    "text": "so what we would need to do is essentially take data from some operational databases in real",
    "start": "938240",
    "end": "943839"
  },
  {
    "text": "time and bring it into the serving engine and this is usually done through some sort",
    "start": "943839",
    "end": "949440"
  },
  {
    "text": "of an api service or an enrichment service run it against the model and respond back to the user",
    "start": "949440",
    "end": "956240"
  },
  {
    "text": "now once we've done that uh it's not we're not done yet we need to monitor the behavior of our",
    "start": "956240",
    "end": "963360"
  },
  {
    "text": "model uh through some sort of a tracking system how do we know that our model keeps on performing",
    "start": "963360",
    "end": "969759"
  },
  {
    "text": "and is accurate or we want to be able to monitor maybe there is misbehavior in the performance aspects maybe",
    "start": "969759",
    "end": "976399"
  },
  {
    "text": "glitches but also does it lose accuracy or do we see different data than what we've",
    "start": "976399",
    "end": "983120"
  },
  {
    "text": "expected so all the data from the serving which is also high performance so this is",
    "start": "983120",
    "end": "988639"
  },
  {
    "text": "streaming data is going to go into other microservices that monitor the behavior of our model",
    "start": "988639",
    "end": "995839"
  },
  {
    "text": "and based on that they may trigger events they may write it to a time series database and visualize it in",
    "start": "995839",
    "end": "1001600"
  },
  {
    "text": "like grafana they may also record some data into sort of batches",
    "start": "1001600",
    "end": "1007360"
  },
  {
    "text": "and then there is another set of tools that may compare the the inferencing logic of the",
    "start": "1007360",
    "end": "1012720"
  },
  {
    "text": "inferencing data with real reference data in a delay you know let's assume we",
    "start": "1012720",
    "end": "1018079"
  },
  {
    "text": "we predict something but in a day from now we'll actually know if it happened or not if our prediction was true",
    "start": "1018079",
    "end": "1024160"
  },
  {
    "text": "so we can compare our predictions with the real results and this way you also have",
    "start": "1024160",
    "end": "1029839"
  },
  {
    "text": "another way of analyzing if our model behaves properly and if it doesn't we can trigger",
    "start": "1029839",
    "end": "1036240"
  },
  {
    "text": "something to fix it retraining bringing on data changing the logic of our serving and samples",
    "start": "1036240",
    "end": "1043120"
  },
  {
    "text": "etc so this is how usually you'll see that this pipelines work again you can",
    "start": "1043120",
    "end": "1049120"
  },
  {
    "text": "see that it's not trivial and this is a predictive maintenance pipeline it will be the same",
    "start": "1049120",
    "end": "1054640"
  },
  {
    "text": "problem with other solutions and if you add things like deep learning and neural networks it may",
    "start": "1054640",
    "end": "1060559"
  },
  {
    "text": "even get complicated there are many applications where there is more than one model in a single",
    "start": "1060559",
    "end": "1065679"
  },
  {
    "text": "application so those things may get even more complicated",
    "start": "1065679",
    "end": "1071840"
  },
  {
    "text": "okay so before you you tear your uh hairs out",
    "start": "1071840",
    "end": "1076880"
  },
  {
    "text": "the most important thing is you can start you see there are a lot of different moving components in this system and there are",
    "start": "1076880",
    "end": "1084799"
  },
  {
    "text": "a lot of different software projects that you can choose from but the the right way is essentially to",
    "start": "1084799",
    "end": "1092400"
  },
  {
    "text": "try and take an ecosystem which com completes the most of the building block in the puzzle",
    "start": "1092400",
    "end": "1099360"
  },
  {
    "text": "for example if you take some tools that were designed for hadoop along with some tools that were designed for uh",
    "start": "1099360",
    "end": "1105600"
  },
  {
    "text": "kubernetes and some tools in a cloud provider and they don't work with each other you",
    "start": "1105600",
    "end": "1111039"
  },
  {
    "text": "end up essentially doing a lot of integration by yourself so one of the nice things",
    "start": "1111039",
    "end": "1116320"
  },
  {
    "text": "here is that kubeflow is a mechanism or is a is essentially a framework it's not",
    "start": "1116320",
    "end": "1121520"
  },
  {
    "text": "compressed to complete or comprehensive but at least it's something that can glue a lot of other things around it",
    "start": "1121520",
    "end": "1128799"
  },
  {
    "text": "and this is part of the things i'm going to show is how to extend q flow uh with you know monitoring with",
    "start": "1128799",
    "end": "1134720"
  },
  {
    "text": "serverless functionality with automl and other capabilities and making it a server an end-to-end machine learning",
    "start": "1134720",
    "end": "1142480"
  },
  {
    "text": "framework okay so essentially what we we want to",
    "start": "1142480",
    "end": "1148400"
  },
  {
    "text": "show is a complete pipeline where we fit in data that we do data preparation we",
    "start": "1148400",
    "end": "1154799"
  },
  {
    "text": "accelerate the training we deploy an application we monitor the application in order to do all of that together we",
    "start": "1154799",
    "end": "1161120"
  },
  {
    "text": "have to also have some a baseline of data that moves around this entire uh fabric",
    "start": "1161120",
    "end": "1169039"
  },
  {
    "text": "so how would the solution look like if we're opening that sort of pipeline and so we need three layers in the",
    "start": "1169039",
    "end": "1175120"
  },
  {
    "text": "solution the first layer we need data and the data need to have a few attributes first it needs to be",
    "start": "1175120",
    "end": "1181520"
  },
  {
    "text": "relatively fast because we're not talking about archival data we're talking about data in the move",
    "start": "1181520",
    "end": "1187360"
  },
  {
    "text": "we're building a pipeline one micro service writes data the other micro service reads data",
    "start": "1187360",
    "end": "1192559"
  },
  {
    "text": "if it takes too much time we're going to have a delay in many cases we're doing distributed",
    "start": "1192559",
    "end": "1198320"
  },
  {
    "text": "computing like in in training or sometimes in in analytics so we want to have shared data we want all",
    "start": "1198320",
    "end": "1204799"
  },
  {
    "text": "those things to work together and maybe shuffle some data around between those micro services",
    "start": "1204799",
    "end": "1210559"
  },
  {
    "text": "we need sometimes streaming data we need some key value stores for real time data we may need time series",
    "start": "1210559",
    "end": "1217440"
  },
  {
    "text": "data in order to log all sorts of matrix so this is where we need",
    "start": "1217440",
    "end": "1222480"
  },
  {
    "text": "data and it needs to be fast secure and also versioned so if we run multiple experiments we can",
    "start": "1222480",
    "end": "1228480"
  },
  {
    "text": "also go back into a specific experiment and see and recontract that from the data we",
    "start": "1228480",
    "end": "1234559"
  },
  {
    "text": "have the next layer is micro services okay and those micro services this is a",
    "start": "1234559",
    "end": "1240240"
  },
  {
    "text": "simplistic view of those micro services we need we need services that ingest data",
    "start": "1240240",
    "end": "1246799"
  },
  {
    "text": "from etl from streaming from scraping from http apis we need data preparation",
    "start": "1246799",
    "end": "1254080"
  },
  {
    "text": "so functions that will do analytics and whether it's batch or real time or stream processing",
    "start": "1254080",
    "end": "1260080"
  },
  {
    "text": "to join aggregate split and do all sorts of manipulations on data so it's going to be ready for",
    "start": "1260080",
    "end": "1267360"
  },
  {
    "text": "the training then we need to run training logic again with usually distributed frameworks",
    "start": "1267360",
    "end": "1274159"
  },
  {
    "text": "uh whether it's machine learning or deep learning and here we may also want to incorporate",
    "start": "1274159",
    "end": "1279360"
  },
  {
    "text": "something referred to as hyper parameter tuning so run the same experiment on many",
    "start": "1279360",
    "end": "1284559"
  },
  {
    "text": "different permutations and we want to leverage the power of kubernetes of scheduling lots of containers",
    "start": "1284559",
    "end": "1290320"
  },
  {
    "text": "so not necessarily all the permutations run on one container we can run multiple containers each set of",
    "start": "1290320",
    "end": "1296240"
  },
  {
    "text": "containers will run different set of parameters and then converge or combine",
    "start": "1296240",
    "end": "1301760"
  },
  {
    "text": "all the results to a single sort of best result and there's also an aspect of automl",
    "start": "1301760",
    "end": "1307840"
  },
  {
    "text": "which is essentially running a variety of algorithms not just parameters and running all those in sort of a gang",
    "start": "1307840",
    "end": "1314880"
  },
  {
    "text": "and choosing the best algorithm note that we always have to store all the data",
    "start": "1314880",
    "end": "1319919"
  },
  {
    "text": "because even though the machine chose one preferred algorithm we may want to look",
    "start": "1319919",
    "end": "1324960"
  },
  {
    "text": "back in perspective and see which model preferred how performed how and maybe even choose not only the best",
    "start": "1324960",
    "end": "1331760"
  },
  {
    "text": "algorithm or the best model but the three best models out of a list of of 10 or 20. once we have that we need",
    "start": "1331760",
    "end": "1340080"
  },
  {
    "text": "to run validation and then deployment now each individual step is a microservice",
    "start": "1340080",
    "end": "1346159"
  },
  {
    "text": "so if we are going to essentially go and create a microservice and a kubernetes deployment or kubernetes job for each",
    "start": "1346159",
    "end": "1352400"
  },
  {
    "text": "one of those that we're going to run out of steam very very quickly so the general idea is instead of going and",
    "start": "1352400",
    "end": "1359600"
  },
  {
    "text": "running and creating docker files and yaml files and and kubernetes crds and all of that for each individual",
    "start": "1359600",
    "end": "1366400"
  },
  {
    "text": "step we want to have more of a serverless mindset we want to write some code",
    "start": "1366400",
    "end": "1371679"
  },
  {
    "text": "provided minimal description and we want something else to build those services for us we",
    "start": "1371679",
    "end": "1377679"
  },
  {
    "text": "did yamls with the containers with the docker images with the dependent packages with",
    "start": "1377679",
    "end": "1384559"
  },
  {
    "text": "the considerations around scaling and and all the aspects of logging and monitoring we just want to provide some",
    "start": "1384559",
    "end": "1390000"
  },
  {
    "text": "code and let someone else deal with that and this is really where serverless technology will come in very",
    "start": "1390000",
    "end": "1395520"
  },
  {
    "text": "handy and finally when we have all those individual steps we need to orchestrate them",
    "start": "1395520",
    "end": "1401520"
  },
  {
    "text": "and we can we want to create an end-to-end pipeline and this is where kubeflow and other",
    "start": "1401520",
    "end": "1406960"
  },
  {
    "text": "tools are going to be useful so um",
    "start": "1406960",
    "end": "1413360"
  },
  {
    "text": "a typical stack would look like something like that so you have the data layer remember we we mentioned a lot of times",
    "start": "1413360",
    "end": "1420080"
  },
  {
    "text": "we need real time access from microservice to other micro services from my micro services to get obtain",
    "start": "1420080",
    "end": "1427760"
  },
  {
    "text": "features that are more uh depend you know require low latency access at the same",
    "start": "1427760",
    "end": "1432960"
  },
  {
    "text": "time we need access to big data in in the form of files object store data warehouses etc so we",
    "start": "1432960",
    "end": "1440960"
  },
  {
    "text": "need to have a layer in iguazi we have our own real time a multi-model data layer which is",
    "start": "1440960",
    "end": "1447279"
  },
  {
    "text": "serving file object tables and other interesting formats in very very high performance",
    "start": "1447279",
    "end": "1454799"
  },
  {
    "text": "and along with again object files external databases",
    "start": "1454799",
    "end": "1459840"
  },
  {
    "text": "and then we have kubernetes and there are many managed kubernetes options out there",
    "start": "1459840",
    "end": "1465039"
  },
  {
    "text": "and on top of it you need all of those variety of services to make a comprehensive solution and",
    "start": "1465039",
    "end": "1470720"
  },
  {
    "text": "probably more that are not shortlisted here you know things like spark and",
    "start": "1470720",
    "end": "1475840"
  },
  {
    "text": "tensorflow and if you're doing sequels or something like presto and for machine learning you you need",
    "start": "1475840",
    "end": "1482000"
  },
  {
    "text": "like psychic learn and pi torch and and horvath is a way to distribute uh",
    "start": "1482000",
    "end": "1487200"
  },
  {
    "text": "deep learning jobs and task is a way to distribute machine learning et cetera and jupiter is a managed",
    "start": "1487200",
    "end": "1492480"
  },
  {
    "text": "notebook so you need to install a bunch of uh services on your kubernetes cluster",
    "start": "1492480",
    "end": "1497760"
  },
  {
    "text": "and make sure they all work in our solution again it's a managed offering so you don't need to",
    "start": "1497760",
    "end": "1503520"
  },
  {
    "text": "have all the hassle it just works out of the box we are suffering from the integration",
    "start": "1503520",
    "end": "1508799"
  },
  {
    "text": "not you and then you have a set of tools for pipeline orchestration now",
    "start": "1508799",
    "end": "1514799"
  },
  {
    "text": "there is cube flow and kubeflow is mainly serve a graph execution qflo pipeline is a graph",
    "start": "1514799",
    "end": "1520640"
  },
  {
    "text": "execution entity um it is missing a lot of other aspects that we need like",
    "start": "1520640",
    "end": "1526159"
  },
  {
    "text": "feature store like automl uh auto hyperparameter tuning is not really",
    "start": "1526159",
    "end": "1531279"
  },
  {
    "text": "built into it or and even experiment tracking there are some challenges around",
    "start": "1531279",
    "end": "1537039"
  },
  {
    "text": "how you do experiment tracking you want to do in many cases experiment tracking not only within a workflow sometimes just run",
    "start": "1537039",
    "end": "1544400"
  },
  {
    "text": "experiment tracking on your laptop and that's something you cannot do with kubeflow so you need another mechanism",
    "start": "1544400",
    "end": "1550640"
  },
  {
    "text": "that can work with qflo for tracking experiments but also can work on a standalone machine or just running",
    "start": "1550640",
    "end": "1558559"
  },
  {
    "text": "individual step in the pipeline so and and also we'll we'll talk about the gaps and what we need to do so this",
    "start": "1558559",
    "end": "1565360"
  },
  {
    "text": "is really where another tool called mlrun is filling up those gaps by orchestrating a lot of the workloads and",
    "start": "1565360",
    "end": "1572400"
  },
  {
    "text": "automating and making it really really friendly to data scientists not to devops",
    "start": "1572400",
    "end": "1577520"
  },
  {
    "text": "or ml engineers the the uh one of the key uh solutions",
    "start": "1577520",
    "end": "1585760"
  },
  {
    "text": "in the as what we recommend is working with serverless function because we mentioned before that one of the",
    "start": "1585760",
    "end": "1592080"
  },
  {
    "text": "biggest challenge is moving from code to like a production artifact with",
    "start": "1592080",
    "end": "1597279"
  },
  {
    "text": "all the aspects of scaling performance tuning instrumentation logging monitoring security hardening",
    "start": "1597279",
    "end": "1604880"
  },
  {
    "text": "so one of the ways to do it is essentially leverage code and to production with serverless",
    "start": "1604880",
    "end": "1610640"
  },
  {
    "text": "technologies and for that there are several technologies that we're supporting and they're part of the open source",
    "start": "1610640",
    "end": "1616400"
  },
  {
    "text": "and nucleo is is uh one of the highest performance a very mature",
    "start": "1616400",
    "end": "1621440"
  },
  {
    "text": "serverless framework over kubernetes more than 3 000 stars on github used by",
    "start": "1621440",
    "end": "1626480"
  },
  {
    "text": "many enterprises and there's also mlran which is more for batch computation and i'll show",
    "start": "1626480",
    "end": "1632320"
  },
  {
    "text": "you how it works so one of the major advantages of nucleo that beyond being a standard serverless",
    "start": "1632320",
    "end": "1638960"
  },
  {
    "text": "it's very very high performance it's using a real-time kernel it runs about 100 times faster than",
    "start": "1638960",
    "end": "1644720"
  },
  {
    "text": "things like amazon lambda it runs about 10 times faster than most other serverless frameworks over",
    "start": "1644720",
    "end": "1650320"
  },
  {
    "text": "kubernetes including kf serving and some of those it's stateful so it can actually work",
    "start": "1650320",
    "end": "1656159"
  },
  {
    "text": "with data and gpus and things like that in the function which is very essential for",
    "start": "1656159",
    "end": "1662559"
  },
  {
    "text": "machine learning and also one of the nice things it's glue it has glueless integration to cube",
    "start": "1662559",
    "end": "1668320"
  },
  {
    "text": "so you can essentially create a graph of computation where the individual steps",
    "start": "1668320",
    "end": "1673679"
  },
  {
    "text": "are functions essentially serverless function and there is a marketplace where you actually don't need to write",
    "start": "1673679",
    "end": "1679600"
  },
  {
    "text": "most of the functions for automl for serving for training for data preparation for",
    "start": "1679600",
    "end": "1685760"
  },
  {
    "text": "feature analysis for hooks to get and slack and things like that so you",
    "start": "1685760",
    "end": "1691039"
  },
  {
    "text": "just can put a bunch of components on a canvas and get a pipeline up and running",
    "start": "1691039",
    "end": "1697360"
  },
  {
    "text": "very quickly [Music] again one of the main challenges of serverless that is very event-driven",
    "start": "1697360",
    "end": "1705039"
  },
  {
    "text": "so one of the things that we see is that we want the benefits of serverless which is resource elasticity",
    "start": "1705039",
    "end": "1711039"
  },
  {
    "text": "automated deployment automated operation but we want to extend it to new workloads that are",
    "start": "1711039",
    "end": "1717440"
  },
  {
    "text": "more like data preparation and training where jobs can be tens of minutes or hours where the way",
    "start": "1717440",
    "end": "1724080"
  },
  {
    "text": "you scale or distributed the workload is not by load balancing or you know sort of a service mesh but it's",
    "start": "1724080",
    "end": "1732000"
  },
  {
    "text": "rather through data partitioning data shuffling reduction hyperparameter",
    "start": "1732000",
    "end": "1738960"
  },
  {
    "text": "shuffling you know things like that which are different technologies and also must be stateful because if i'm",
    "start": "1738960",
    "end": "1744880"
  },
  {
    "text": "processing data at scale i have to have distributed database or distributed file system as the anchor",
    "start": "1744880",
    "end": "1750480"
  },
  {
    "text": "for moving data around between those individual containers another major difference in",
    "start": "1750480",
    "end": "1756880"
  },
  {
    "text": "jobs and and data preparation and training is instead of passing json you know as an event we are",
    "start": "1756880",
    "end": "1764320"
  },
  {
    "text": "actually passing data we're creating a pipeline uh with parameters and data that feed",
    "start": "1764320",
    "end": "1769679"
  },
  {
    "text": "each other you know we have data set output of one step feeding a data",
    "start": "1769679",
    "end": "1775120"
  },
  {
    "text": "as an input data set for another step like data preparation creating a data set",
    "start": "1775120",
    "end": "1780159"
  },
  {
    "text": "and training consumes that data set so we need to have a new serverless approach which has",
    "start": "1780159",
    "end": "1786640"
  },
  {
    "text": "those attributes and this is where we've created a new framework essentially",
    "start": "1786640",
    "end": "1791679"
  },
  {
    "text": "just for that so part of what we refer to as mlran which is open source",
    "start": "1791679",
    "end": "1797120"
  },
  {
    "text": "again relatively new but very very powerful solution it has several layers the serverless engine is essentially one",
    "start": "1797120",
    "end": "1803679"
  },
  {
    "text": "layer it also has a lot of higher level automation layers and",
    "start": "1803679",
    "end": "1809279"
  },
  {
    "text": "so the general idea is that it auto scales it allows it to scalable to do scalable computing over multiple",
    "start": "1809279",
    "end": "1815600"
  },
  {
    "text": "frameworks you can create a serverless function of spark of dusk of mpi job",
    "start": "1815600",
    "end": "1821520"
  },
  {
    "text": "you know a lot of different crds are orchestrated by ml run so you can essentially create",
    "start": "1821520",
    "end": "1826880"
  },
  {
    "text": "distributed computing and emerald knows how to break the the work and control the distributed job etc",
    "start": "1826880",
    "end": "1834640"
  },
  {
    "text": "another interesting piece is that it has auto tracking features it says she knows how to look into",
    "start": "1834640",
    "end": "1839760"
  },
  {
    "text": "the inputs and outputs of every execution and auto record that so you get auto recording of all your",
    "start": "1839760",
    "end": "1846399"
  },
  {
    "text": "experiments and runs and code and data inputs and outputs etc as i'll show in a minute and again",
    "start": "1846399",
    "end": "1852240"
  },
  {
    "text": "it runs over multiple frameworks another uh interesting uh thing is you know there are other",
    "start": "1852240",
    "end": "1858159"
  },
  {
    "text": "frameworks for a hyperparameter in kubernetes like khatib",
    "start": "1858159",
    "end": "1863279"
  },
  {
    "text": "and others but then you need to manually go and integrate them and if we have an engine here that knows how to run",
    "start": "1863279",
    "end": "1869200"
  },
  {
    "text": "many different permutations already distributed the only missing piece is having",
    "start": "1869200",
    "end": "1874240"
  },
  {
    "text": "something that knows how to create those permutations and track them this is actually built into mlran so you",
    "start": "1874240",
    "end": "1879760"
  },
  {
    "text": "don't need a separate project for hyper parameters and you all have automl built in into the solution",
    "start": "1879760",
    "end": "1887760"
  },
  {
    "text": "so which is new there's no automl solution on top of kubernetes beside the mrm",
    "start": "1887760",
    "end": "1894399"
  },
  {
    "text": "okay now where where is uh does it fit so kubeflow is something that couple",
    "start": "1894399",
    "end": "1900559"
  },
  {
    "text": "project has two main uh things or three main things one is a tuple pipeline which is this",
    "start": "1900559",
    "end": "1906320"
  },
  {
    "text": "orchestration capability and then you have the operators like mpi job or distributed tensorflow",
    "start": "1906320",
    "end": "1913120"
  },
  {
    "text": "and others which you can also install by yourself and there's also managed",
    "start": "1913120",
    "end": "1918720"
  },
  {
    "text": "notebook so but the most interesting piece is kilflow pipeline which is based on argo engine a",
    "start": "1918720",
    "end": "1925120"
  },
  {
    "text": "distributed workflow engine and it allows you to essentially run a graph the challenges with kubeflow is",
    "start": "1925120",
    "end": "1931519"
  },
  {
    "text": "very oriented towards devops it's not designed for actual data scientists to work with it",
    "start": "1931519",
    "end": "1936799"
  },
  {
    "text": "and data scientists like simple things that are very very simple they don't understand docker containers",
    "start": "1936799",
    "end": "1942240"
  },
  {
    "text": "in many cases or yamls and they don't want to build jsons that build artifacts for them like you would",
    "start": "1942240",
    "end": "1948399"
  },
  {
    "text": "do in traditional kubflow they want to make things very very intuitive and simple",
    "start": "1948399",
    "end": "1953519"
  },
  {
    "text": "in their own domain of expertise so so what amaran and nucleo",
    "start": "1953519",
    "end": "1959679"
  },
  {
    "text": "provide is a way to essentially automate the code to creation of serverless functions from",
    "start": "1959679",
    "end": "1965919"
  },
  {
    "text": "code it delivers all the integration around data access and parallelism",
    "start": "1965919",
    "end": "1970960"
  },
  {
    "text": "it allows you to do distributed training gpu acceleration and other features that are not built in",
    "start": "1970960",
    "end": "1977840"
  },
  {
    "text": "and it does end-to-end tracking of everything code execution data versioning etc all",
    "start": "1977840",
    "end": "1984399"
  },
  {
    "text": "in a serving a graph that you can actually just play around and move around between the different aspects",
    "start": "1984399",
    "end": "1992960"
  },
  {
    "text": "so how does it all work if i'm looking into it from a flow perspective take this data",
    "start": "1993760",
    "end": "2000240"
  },
  {
    "text": "scientist what he wants to do is build something in his own notebook or his own pycharm or vs code",
    "start": "2000240",
    "end": "2006399"
  },
  {
    "text": "still it doesn't know kubernetes or doesn't care about kubernetes so the first thing we want to make sure is",
    "start": "2006399",
    "end": "2011679"
  },
  {
    "text": "that whatever we're building can work in a standalone and that's part of the challenges with kubeflow",
    "start": "2011679",
    "end": "2018080"
  },
  {
    "text": "if you want to run kubeflow stuff you have to run it within kubernetes and within queue flow you",
    "start": "2018080",
    "end": "2023919"
  },
  {
    "text": "cannot run a instrument and do the tracking and all of that from your notebook",
    "start": "2023919",
    "end": "2030480"
  },
  {
    "text": "without kubernetes attached to it so um the first thing that we we want to do is",
    "start": "2030480",
    "end": "2035679"
  },
  {
    "text": "essentially just import an sdk and allow someone to run from within his notebook and still get all the",
    "start": "2035679",
    "end": "2041840"
  },
  {
    "text": "experiment tracking and all the data lineage tracking and all that done automatically for us",
    "start": "2041840",
    "end": "2048158"
  },
  {
    "text": "at a certain point we've debugged our code and we feel it's ready uh for just running it as a",
    "start": "2048159",
    "end": "2054240"
  },
  {
    "text": "container on the cluster so we don't want to build and all those things we just want to say",
    "start": "2054240",
    "end": "2059839"
  },
  {
    "text": "run but this time on the cluster so essentially just a symbol a single line saying not locally but in",
    "start": "2059839",
    "end": "2066800"
  },
  {
    "text": "the cluster what it would do it essentially automatically packages your code throws it into the cluster builds your",
    "start": "2066800",
    "end": "2073919"
  },
  {
    "text": "containers uh builds the crds for the various uh runtimes that are supported like you",
    "start": "2073919",
    "end": "2080240"
  },
  {
    "text": "know kubernetes job or spark or dusk or any of the other ones",
    "start": "2080240",
    "end": "2085839"
  },
  {
    "text": "runs it and you still get the full tracking even while you're running this job on the cluster now note that at this",
    "start": "2085839",
    "end": "2092079"
  },
  {
    "text": "stage we don't really need a pipeline we just want to run a single step of let's say training we",
    "start": "2092079",
    "end": "2098240"
  },
  {
    "text": "want to test validate see that it works and the later on in the process we we want to",
    "start": "2098240",
    "end": "2104160"
  },
  {
    "text": "turn it into a pipeline so this once we've committed our code into a repository like git",
    "start": "2104160",
    "end": "2110560"
  },
  {
    "text": "then we want to take those pieces of code and just put them in a layout or a pipeline",
    "start": "2110560",
    "end": "2116640"
  },
  {
    "text": "and in this way run the entire pipeline so someone else or the same guy can just take all those components build",
    "start": "2116640",
    "end": "2124160"
  },
  {
    "text": "a pipeline and execute that through kubeflow or mlrun apis the other nice thing is that",
    "start": "2124160",
    "end": "2131280"
  },
  {
    "text": "you don't really necessarily need to make it interactive you can make it as part of an automated",
    "start": "2131280",
    "end": "2136800"
  },
  {
    "text": "ci just like you would with any other ci product so essentially you can commit something",
    "start": "2136800",
    "end": "2143280"
  },
  {
    "text": "into a git repositories you put a hook in your commit repository and i can show you a demo in a minute we",
    "start": "2143280",
    "end": "2149920"
  },
  {
    "text": "have a full-blown demo for that so you commit something into the repository then that generates a trigger",
    "start": "2149920",
    "end": "2157040"
  },
  {
    "text": "amaran goes builds the entire kubeflow pipeline for it and you know it connects all the dots the",
    "start": "2157040",
    "end": "2162480"
  },
  {
    "text": "data and and everything runs the keyful pipeline uh tracks all the information about the",
    "start": "2162480",
    "end": "2168800"
  },
  {
    "text": "the pipeline execution with the pipeline execution terminates records everything writes it",
    "start": "2168800",
    "end": "2174240"
  },
  {
    "text": "back into the pull request as annotation in the pull request and throws in a slack notification back",
    "start": "2174240",
    "end": "2181920"
  },
  {
    "text": "with the summary of your execution so this way you can move very quickly from just developing some",
    "start": "2181920",
    "end": "2187920"
  },
  {
    "text": "code on a notebook testing an individual microservice seeing how it works",
    "start": "2187920",
    "end": "2192960"
  },
  {
    "text": "running an entire pipeline and even putting your pipeline as part of a complete",
    "start": "2192960",
    "end": "2199599"
  },
  {
    "text": "automated workflow and then if you were thinking about the entire process what we have is a",
    "start": "2199599",
    "end": "2207200"
  },
  {
    "text": "for example we we have a branch we take uh we create we create a branch out of the",
    "start": "2207200",
    "end": "2212720"
  },
  {
    "text": "like development branch we run some code locally in our environment we convert it to a",
    "start": "2212720",
    "end": "2220880"
  },
  {
    "text": "function using a single command we test it on the cluster see that it works maybe we're missing some packages or",
    "start": "2220880",
    "end": "2227839"
  },
  {
    "text": "maybe we misconfigured something we go back fix it and that's uh",
    "start": "2227839",
    "end": "2233280"
  },
  {
    "text": "we're done with the development the next thing is we need to think about integration we're trying to pull request that change",
    "start": "2233280",
    "end": "2239920"
  },
  {
    "text": "that we've made this change goes into some inspection you know we can have automation in most",
    "start": "2239920",
    "end": "2245440"
  },
  {
    "text": "git ripples of like examining the python code no flake and blake and",
    "start": "2245440",
    "end": "2250640"
  },
  {
    "text": "and all those tools you can write somehow some of your own test scripts and once it passed the",
    "start": "2250640",
    "end": "2256560"
  },
  {
    "text": "basic tests we won't run a complete machine learning pipelines using queue flow so this is where we can just",
    "start": "2256560",
    "end": "2264000"
  },
  {
    "text": "integrate that with the pull request command could pull request comments",
    "start": "2264000",
    "end": "2269680"
  },
  {
    "text": "as we've done and when and then once the pipeline runs and you get the accuracy and you",
    "start": "2269680",
    "end": "2274960"
  },
  {
    "text": "get everything you can merge the pull request and even request for a deployment",
    "start": "2274960",
    "end": "2281520"
  },
  {
    "text": "once we finish the integration we want to go and run the continuous deployment this is where we",
    "start": "2281520",
    "end": "2287119"
  },
  {
    "text": "may create a tag of our repository deploy the models and",
    "start": "2287119",
    "end": "2292240"
  },
  {
    "text": "the code that goes along with that maybe feature engineering and some other aspects and",
    "start": "2292240",
    "end": "2298079"
  },
  {
    "text": "deploy that in a canary once it's working we've examined everything as well",
    "start": "2298079",
    "end": "2303280"
  },
  {
    "text": "we're going to promote that into 100 of the traffic and monitor keep on monitoring the",
    "start": "2303280",
    "end": "2309920"
  },
  {
    "text": "service behavior concept drift etc so this is usually the the one of the the ways of",
    "start": "2309920",
    "end": "2317520"
  },
  {
    "text": "implementing that to tell you that most organizations already work in this way it's still evolving but the ones that",
    "start": "2317520",
    "end": "2325359"
  },
  {
    "text": "have managed to move into this process are becoming more agile and can drive",
    "start": "2325359",
    "end": "2330560"
  },
  {
    "text": "a lot more work with less people so that's with that and and this brings",
    "start": "2330560",
    "end": "2337119"
  },
  {
    "text": "a lot of value so people can build pipelines much faster and the pipelines could be more",
    "start": "2337119",
    "end": "2342960"
  },
  {
    "text": "interactive more online one of the examples is this uh work with one of the",
    "start": "2342960",
    "end": "2348240"
  },
  {
    "text": "customers that we've done is take for a fraud prevention application",
    "start": "2348240",
    "end": "2353599"
  },
  {
    "text": "the original solution was using hadoop where you have a database you extract",
    "start": "2353599",
    "end": "2358960"
  },
  {
    "text": "everything using etl into a data warehouse you run some queries on the data warehouse and then have r servers",
    "start": "2358960",
    "end": "2366240"
  },
  {
    "text": "and doing prediction using an r model and writing back uh you know identifying the fraud",
    "start": "2366240",
    "end": "2371920"
  },
  {
    "text": "accounts and blocking those product accounts so this process is very bad oriented very manual",
    "start": "2371920",
    "end": "2377680"
  },
  {
    "text": "uh very complicated if you've worked with hadoop and all of those frameworks and instead we want to move",
    "start": "2377680",
    "end": "2384560"
  },
  {
    "text": "it into a microservice architecture which will buy us two things first faster deployment process everything",
    "start": "2384560",
    "end": "2391680"
  },
  {
    "text": "instead of three four months deployment for every exchange it could actually be continuous",
    "start": "2391680",
    "end": "2396800"
  },
  {
    "text": "or every week we can or a couple weeks we can drive a change the second is we don't want to have such",
    "start": "2396800",
    "end": "2402960"
  },
  {
    "text": "a huge delay because everything is like battery-oriented we move i want to move to something more online so",
    "start": "2402960",
    "end": "2410079"
  },
  {
    "text": "the change that we've applied is essentially to gather the information from the server",
    "start": "2410079",
    "end": "2416000"
  },
  {
    "text": "the database server uh directly as a stream so you run serverless functions using nucleo that",
    "start": "2416000",
    "end": "2423040"
  },
  {
    "text": "essentially crunch the stream in real time nuclear listens on about 13 different",
    "start": "2423040",
    "end": "2428319"
  },
  {
    "text": "streaming protocols kafka rabbit kinesis pops up etc so you can just listen on",
    "start": "2428319",
    "end": "2435280"
  },
  {
    "text": "kafka or rabbit in this case do a real-time analysis because it's stateful",
    "start": "2435280",
    "end": "2440880"
  },
  {
    "text": "again so it can do things like real-time analysis of of data write it into a combined online and",
    "start": "2440880",
    "end": "2446960"
  },
  {
    "text": "offline feature store you can also run things like spark to do batch analytics",
    "start": "2446960",
    "end": "2452640"
  },
  {
    "text": "on that feature store and extend the features even further looking at the bigger retrospective or",
    "start": "2452640",
    "end": "2457920"
  },
  {
    "text": "bigger time window and and then periodically for example on a daily basis",
    "start": "2457920",
    "end": "2463520"
  },
  {
    "text": "you have a model training process using psychic learn that extract a feature set a training set",
    "start": "2463520",
    "end": "2471280"
  },
  {
    "text": "of a bunch of features run the training generates a model file and then you have inferencing function",
    "start": "2471280",
    "end": "2477839"
  },
  {
    "text": "which in this case they have to work in a stream because there's no http request the data",
    "start": "2477839",
    "end": "2483680"
  },
  {
    "text": "which is written is essentially generating a stream and that stream is consumed by the inferencing function",
    "start": "2483680",
    "end": "2489920"
  },
  {
    "text": "which identifies the fraud and then just based on the fraud just blocks the",
    "start": "2489920",
    "end": "2495119"
  },
  {
    "text": "account so in this process we've taken a process that was 40 minutes to identify the fraud",
    "start": "2495119",
    "end": "2501599"
  },
  {
    "text": "into more of a continuous pipeline which and knows how to identify fraud within",
    "start": "2501599",
    "end": "2506880"
  },
  {
    "text": "12 seconds so significantly faster a couple of magnitudes faster and but",
    "start": "2506880",
    "end": "2513040"
  },
  {
    "text": "the other main advantage is that each one of those steps here is a microservice or a serverless",
    "start": "2513040",
    "end": "2519359"
  },
  {
    "text": "function where in order to deploy a new version the only thing i need to do is change a little code and push deploy",
    "start": "2519359",
    "end": "2525440"
  },
  {
    "text": "or go to an api call and save deploy or even through cd uh continuous deployment tools just do",
    "start": "2525440",
    "end": "2532000"
  },
  {
    "text": "the deployment automatically so this way we are much uh",
    "start": "2532000",
    "end": "2537200"
  },
  {
    "text": "more efficient so with that let me show you uh a short demo of how all this thing",
    "start": "2537200",
    "end": "2544000"
  },
  {
    "text": "uh is really uh running in reality",
    "start": "2544000",
    "end": "2549599"
  },
  {
    "text": "okay so let me share my screen",
    "start": "2549599",
    "end": "2553440"
  },
  {
    "text": "okay i hope it's visible now",
    "start": "2556800",
    "end": "2565520"
  },
  {
    "text": "okay um so what i'm opening here is is a notebook",
    "start": "2565520",
    "end": "2571520"
  },
  {
    "text": "this notebook is going to show me how i can build a full uh pipeline so this pipeline",
    "start": "2571520",
    "end": "2578720"
  },
  {
    "text": "it's all fully documented but the general idea is i have a function in this case again we said we",
    "start": "2578720",
    "end": "2584800"
  },
  {
    "text": "want to take those jumbo notebooks and try and construct them in a way using certain functions and not",
    "start": "2584800",
    "end": "2590240"
  },
  {
    "text": "just like straight code so i have some function which is generating some fake data set this is",
    "start": "2590240",
    "end": "2597520"
  },
  {
    "text": "using the iris data set and once it finishes we want the function to log a data set",
    "start": "2597520",
    "end": "2604560"
  },
  {
    "text": "to essentially log an artifact of type data set essentially a data frame and this",
    "start": "2604560",
    "end": "2610160"
  },
  {
    "text": "logging effect will essentially record everything in a database track lineage track statistical behavior of the data",
    "start": "2610160",
    "end": "2616160"
  },
  {
    "text": "set and all those things behind the scene so this context event this part of the function execution can",
    "start": "2616160",
    "end": "2623839"
  },
  {
    "text": "carry many different things it can give us access to secrets and and other things so",
    "start": "2623839",
    "end": "2630480"
  },
  {
    "text": "at a certain point we also may want to package all those functions under a logical project",
    "start": "2630480",
    "end": "2637920"
  },
  {
    "text": "but then what i need to do is just execute this function now i can just go and execute this iris",
    "start": "2637920",
    "end": "2643680"
  },
  {
    "text": "generator function myself but what we're doing here is we're running it under a wrapper",
    "start": "2643680",
    "end": "2649119"
  },
  {
    "text": "that runs the execution it's running our function and essentially wrapping it up why do we want to wrap the execution",
    "start": "2649119",
    "end": "2655680"
  },
  {
    "text": "because we want to record everything that goes in and goes out into the function",
    "start": "2655680",
    "end": "2661200"
  },
  {
    "text": "into a database and you can see that once i'm running this function i also see some nice widget",
    "start": "2661200",
    "end": "2668000"
  },
  {
    "text": "that shows me the trend the name you know how long he took and",
    "start": "2668000",
    "end": "2673440"
  },
  {
    "text": "a bunch of labels even even uh what i can actually see the data that was",
    "start": "2673440",
    "end": "2679200"
  },
  {
    "text": "generated by by that function okay sorry",
    "start": "2679200",
    "end": "2686160"
  },
  {
    "text": "so i can just run and this generates a data set the iris data set if i really want to see that data set",
    "start": "2686319",
    "end": "2692319"
  },
  {
    "text": "also and track it i can also click the url it will open a separate ui for it the",
    "start": "2692319",
    "end": "2698160"
  },
  {
    "text": "next thing we we want to do is we'll register that we'll convert it into a microservice",
    "start": "2698160",
    "end": "2704079"
  },
  {
    "text": "essentially i promised you a single line of code so there is a single line of code that converts my",
    "start": "2704079",
    "end": "2710720"
  },
  {
    "text": "code into a function essentially into a microservice into a serverless function",
    "start": "2710720",
    "end": "2715760"
  },
  {
    "text": "now i have a function object i can use for example register that in my project or do other things with",
    "start": "2715760",
    "end": "2721440"
  },
  {
    "text": "that now i generate some data set and now i want to take that data set and run analysis against that data set",
    "start": "2721440",
    "end": "2728160"
  },
  {
    "text": "so i may write the code for analysis but then i need to package a new container",
    "start": "2728160",
    "end": "2733440"
  },
  {
    "text": "and put a lot of libraries in it and build yamls and docker files instead i can just go and grab a",
    "start": "2733440",
    "end": "2740640"
  },
  {
    "text": "function from a marketplace called describe which is a running analysis on my",
    "start": "2740640",
    "end": "2746880"
  },
  {
    "text": "data set and it's showing me the behavior of each one of the features of my data set because i don't really know how it works",
    "start": "2746880",
    "end": "2753440"
  },
  {
    "text": "i can just ask for documentation i'm going to see the full documentation of that function",
    "start": "2753440",
    "end": "2759280"
  },
  {
    "text": "and now i want to run this function on my data set so i have a data set that i just generated",
    "start": "2759280",
    "end": "2764800"
  },
  {
    "text": "in my notebook and i want to run something that analyzed the data set in my notebook",
    "start": "2764800",
    "end": "2770480"
  },
  {
    "text": "so in order to do that i need to run this function called describe",
    "start": "2770480",
    "end": "2775599"
  },
  {
    "text": "because i wanted to analyze data from my notebook i need to mount it over a shared file system so the",
    "start": "2775599",
    "end": "2782000"
  },
  {
    "text": "notebook and the job will see the same data so there is something in kubeflow called",
    "start": "2782000",
    "end": "2787760"
  },
  {
    "text": "modifiers and this is using our cluster file system but you can also use nfs or",
    "start": "2787760",
    "end": "2793200"
  },
  {
    "text": "pvc [Music] in order to share data between different",
    "start": "2793200",
    "end": "2798400"
  },
  {
    "text": "containers and then i want to run this function with a set of parameters and inputs so",
    "start": "2798400",
    "end": "2804480"
  },
  {
    "text": "i want to input of my data set that once i just generated in my notebook to be used as an input",
    "start": "2804480",
    "end": "2810560"
  },
  {
    "text": "for this task for this analysis task and i'm running it now this time there's a magic here it's actually not",
    "start": "2810560",
    "end": "2817119"
  },
  {
    "text": "running on my notebook although it looks like it's running on my notebook you see it actually allocated pods to",
    "start": "2817119",
    "end": "2823599"
  },
  {
    "text": "run this job and also responded with a bunch of interesting results so i can just go",
    "start": "2823599",
    "end": "2831280"
  },
  {
    "text": "and look in my directory remember it's a shared file system so everything lands back",
    "start": "2831280",
    "end": "2836640"
  },
  {
    "text": "into my own directory and i could just go and look at all those nice charts which were",
    "start": "2836640",
    "end": "2843440"
  },
  {
    "text": "generated by this analysis job so i can look at the histograms of the data i can look at",
    "start": "2843440",
    "end": "2850960"
  },
  {
    "text": "you know the feature correlation and all those things all this analysis doesn't have to burden my own container",
    "start": "2850960",
    "end": "2857280"
  },
  {
    "text": "another nice side effect is that those containers are allocated dynamically so i don't have to have",
    "start": "2857280",
    "end": "2864720"
  },
  {
    "text": "upfront or payment on those vms or containers like in many services the resources will get",
    "start": "2864720",
    "end": "2872000"
  },
  {
    "text": "allocated dynamically and if i i need gpus for those services and i register as part of the",
    "start": "2872000",
    "end": "2877839"
  },
  {
    "text": "requirements it will automatically allocate the gpu and they allocate the gpu when it's done so it's very",
    "start": "2877839",
    "end": "2883920"
  },
  {
    "text": "important it saves huge amount of cost now after running this i want to create a complete pipeline a",
    "start": "2883920",
    "end": "2890640"
  },
  {
    "text": "pipeline comprised of classification logic essentially training",
    "start": "2890640",
    "end": "2895839"
  },
  {
    "text": "and then validation logic then i want to deploy a service the endpoint the serving",
    "start": "2895839",
    "end": "2901760"
  },
  {
    "text": "function and once i deploy the string function i want to test that serving function to see that it has",
    "start": "2901760",
    "end": "2907680"
  },
  {
    "text": "the right performance latency etc so i can just go in and pick a bunch",
    "start": "2907680",
    "end": "2913119"
  },
  {
    "text": "of those functions already from a library you don't need to write them by the way it's all open source it's part of a there is a",
    "start": "2913119",
    "end": "2919680"
  },
  {
    "text": "git repo hosting all those uh functions for auto ml and data engineering and others",
    "start": "2919680",
    "end": "2926319"
  },
  {
    "text": "and now i create a kubeflow pipeline so this looks like a q probe pipeline",
    "start": "2926319",
    "end": "2932640"
  },
  {
    "text": "the main difference is that instead of having a kubeflow pipeline of kubeflow operators you have a cupro",
    "start": "2932640",
    "end": "2939760"
  },
  {
    "text": "pipeline of serverless functions which are automatically converted to like a kubflow pipeline",
    "start": "2939760",
    "end": "2945760"
  },
  {
    "text": "operation so i'm just going to take those individual functions that i i have i can",
    "start": "2945760",
    "end": "2951760"
  },
  {
    "text": "have build functions i can do an analysis function you know again",
    "start": "2951760",
    "end": "2957119"
  },
  {
    "text": "ingestion function analysis function a a training function with auto ml which",
    "start": "2957119",
    "end": "2962559"
  },
  {
    "text": "runs all those very variety of protocol algorithms and then i need to have a validation",
    "start": "2962559",
    "end": "2969040"
  },
  {
    "text": "function that validates the output model and the testing set of that model",
    "start": "2969040",
    "end": "2974240"
  },
  {
    "text": "and finally i want to deploy a serving function so it's not just for the training i can",
    "start": "2974240",
    "end": "2979920"
  },
  {
    "text": "actually build a real-time pipeline using this engine and i can test my",
    "start": "2979920",
    "end": "2985359"
  },
  {
    "text": "real-time pipeline with another so i incorporated data engineering",
    "start": "2985359",
    "end": "2990400"
  },
  {
    "text": "machine learning pipeline and ci pipeline all in a certain in a single workflow and then i just run this simple",
    "start": "2990400",
    "end": "2998000"
  },
  {
    "text": "python code um here locally and i see all the results",
    "start": "2998000",
    "end": "3003599"
  },
  {
    "text": "of the pipeline once it's complete uh brought back into my notebook i actually",
    "start": "3003599",
    "end": "3009599"
  },
  {
    "text": "don't have to go into kubeflow and examine the results although i can just go and see all those",
    "start": "3009599",
    "end": "3016720"
  },
  {
    "text": "results in cube flow this is part of our managed platform but any other qflo also works so the other",
    "start": "3016720",
    "end": "3024079"
  },
  {
    "text": "nice thing is that all the artifacts are automatically recorded if i go to the training i see all the artifacts",
    "start": "3024079",
    "end": "3030800"
  },
  {
    "text": "which the serverless function generates automatically recorded",
    "start": "3030800",
    "end": "3035920"
  },
  {
    "text": "also for the data analysis and you know all those nice widgets and this",
    "start": "3035920",
    "end": "3041920"
  },
  {
    "text": "is where it actually deployed the nuclear function for real-time serving and then we use the tester to test the",
    "start": "3041920",
    "end": "3048640"
  },
  {
    "text": "performance of that serving function so we can actually see the latency we see that it's around",
    "start": "3048640",
    "end": "3054319"
  },
  {
    "text": "11 milliseconds so it's really decent performance",
    "start": "3054319",
    "end": "3059680"
  },
  {
    "text": "obviously we haven't tried to optimize here anything so you have the full control now all those things",
    "start": "3059680",
    "end": "3066319"
  },
  {
    "text": "that i did are also recorded in ml run so i can just go into",
    "start": "3066319",
    "end": "3071760"
  },
  {
    "text": "this join amarant doesn't care where you're running the job whether it's running in cubeflow as a separate job within the",
    "start": "3071760",
    "end": "3078960"
  },
  {
    "text": "notebook it sees all the variety of jobs you know they can be grouped by name or by qflo",
    "start": "3078960",
    "end": "3085119"
  },
  {
    "text": "workflow and then i could just go and look into each one of those jobs you know we can",
    "start": "3085119",
    "end": "3091680"
  },
  {
    "text": "see what's running we can actually see the code commit of that code and see the actual code if we want",
    "start": "3091680",
    "end": "3099040"
  },
  {
    "text": "we can look at all the artifacts that were generated in each one of those steps we can look",
    "start": "3099040",
    "end": "3106880"
  },
  {
    "text": "at the logs in real time this one didn't really have a log let's move to the training function if",
    "start": "3106880",
    "end": "3113440"
  },
  {
    "text": "the training functions include auto ml or hyperparameter tuning you can actually see all the individual results",
    "start": "3113440",
    "end": "3120319"
  },
  {
    "text": "and then it auto selects based on your criteria um the best result to move into the next",
    "start": "3120319",
    "end": "3126720"
  },
  {
    "text": "step of the pipeline so the automl and the hyperparameter tuning is built into",
    "start": "3126720",
    "end": "3132160"
  },
  {
    "text": "this framework you don't have to go in and use another framework just to get this thing and also you have",
    "start": "3132160",
    "end": "3139280"
  },
  {
    "text": "a way to manage everything you know the functions the artifacts the jobs there's gonna be a",
    "start": "3139280",
    "end": "3144800"
  },
  {
    "text": "new ui very soon and again everything is on is on github uh",
    "start": "3144800",
    "end": "3149920"
  },
  {
    "text": "the other uh important thing is those functions which i generated",
    "start": "3149920",
    "end": "3167839"
  },
  {
    "text": "is not limited to http you can essentially go and add any trigger you want you know kafka rabbit",
    "start": "3181760",
    "end": "3188160"
  },
  {
    "text": "kinesis cron whatever you can configure a lot of things like the resource",
    "start": "3188160",
    "end": "3194720"
  },
  {
    "text": "resources auto scaling minimum maximum secrets volumes you know everything that",
    "start": "3194720",
    "end": "3200960"
  },
  {
    "text": "you can think of that kubernetes can do could be abstracted using nucleo um",
    "start": "3200960",
    "end": "3208559"
  },
  {
    "text": "and again this can this function was actually generated from a notebook i i have a notebook here this serving",
    "start": "3208559",
    "end": "3216480"
  },
  {
    "text": "notebook see this function which i can write in a notebook at just when i when i feel i'm ready",
    "start": "3216480",
    "end": "3223440"
  },
  {
    "text": "after i tested this notebook have one thing to do is just say function deploy this will automatically",
    "start": "3223440",
    "end": "3230720"
  },
  {
    "text": "create the function for me or taking that function and embedding it as part of a complete workflow",
    "start": "3230720",
    "end": "3239200"
  },
  {
    "text": "another cool uh cool thing about those function is nucleo also supports api gateways and",
    "start": "3239200",
    "end": "3245520"
  },
  {
    "text": "canaries and all those things so you can go and create a gateway new api gateway which",
    "start": "3245520",
    "end": "3254079"
  },
  {
    "text": "allows you to you know authenticate an endpoint and create canaries you know assign multiple",
    "start": "3254079",
    "end": "3259839"
  },
  {
    "text": "functions and give different percentage to which one of the functions and by the way of all the things that we",
    "start": "3259839",
    "end": "3266480"
  },
  {
    "text": "monitor are also written to prometheus and grafana all the statistics of functions",
    "start": "3266480",
    "end": "3272400"
  },
  {
    "text": "jobs everything so think about how much work we've saved you with those",
    "start": "3272400",
    "end": "3278079"
  },
  {
    "text": "serverless frameworks so all the logging monitoring all the auto scaling regardless of how you want to scale",
    "start": "3278079",
    "end": "3284640"
  },
  {
    "text": "whether it's dusk or nucleo or spark or any other architecture all the",
    "start": "3284640",
    "end": "3290319"
  },
  {
    "text": "tracking with all the linkage of all this tracking and artifact and and job all of that is fully automated",
    "start": "3290319",
    "end": "3297040"
  },
  {
    "text": "so that means that you can turn project that could have been consuming tons of devops and developers and take months",
    "start": "3297040",
    "end": "3304720"
  },
  {
    "text": "you can just implement the same project within weeks and everyone will be happy because",
    "start": "3304720",
    "end": "3309760"
  },
  {
    "text": "you have all the instrumentation the best performance you can think of etc and so with that",
    "start": "3309760",
    "end": "3317119"
  },
  {
    "text": "that's my session uh welcome any questions or",
    "start": "3317119",
    "end": "3323119"
  },
  {
    "text": "or comments um you can also a you know find me in linkedin or twitter",
    "start": "3323119",
    "end": "3330079"
  },
  {
    "text": "and i'm happy to talk to any one of you if possible",
    "start": "3330079",
    "end": "3337520"
  }
]