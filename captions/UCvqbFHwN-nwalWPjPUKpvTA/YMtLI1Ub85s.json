[
  {
    "text": "my name is jose navarro and this is praiana galif and we both are the machine learning platform team at",
    "start": "80",
    "end": "5839"
  },
  {
    "text": "cookbot today we are going to talk to you about how nvidia triton inference",
    "start": "5839",
    "end": "13200"
  },
  {
    "text": "server can help you optimize three areas of your inference engine",
    "start": "13200",
    "end": "18880"
  },
  {
    "text": "performance user experience for your machine learning teams",
    "start": "18880",
    "end": "25199"
  },
  {
    "text": "and cost but first let me introduce you to cookpad a little bit for context",
    "start": "25199",
    "end": "33200"
  },
  {
    "text": "cookpot is the largest online community for home cooking lovers",
    "start": "33200",
    "end": "39680"
  },
  {
    "text": "we are making everyday cooking fun but why",
    "start": "39680",
    "end": "45440"
  },
  {
    "text": "well the act of eating has a major impact in everyone's physical and mental health",
    "start": "45440",
    "end": "53600"
  },
  {
    "text": "but also the choices we make when we cook has also a big impact on our planet",
    "start": "53600",
    "end": "61760"
  },
  {
    "text": "and with those two in mind we believe that there is a big difference between creators and",
    "start": "61760",
    "end": "67680"
  },
  {
    "text": "consumers when you are creating or you are cooking",
    "start": "67680",
    "end": "74000"
  },
  {
    "text": "suddenly your awareness starts to grow you starting to care about where your",
    "start": "74000",
    "end": "80799"
  },
  {
    "text": "ingredients come from or how the taste changes if those",
    "start": "80799",
    "end": "86479"
  },
  {
    "text": "ingredients are in season or they are produced in a more controlled environment",
    "start": "86479",
    "end": "92320"
  },
  {
    "text": "and when people",
    "start": "95119",
    "end": "101280"
  },
  {
    "text": "start caring they tend to make better decisions that impact not only their health but",
    "start": "101280",
    "end": "107840"
  },
  {
    "text": "also our environment we are",
    "start": "107840",
    "end": "113040"
  },
  {
    "text": "an online community a global community that is available in more than 70 countries",
    "start": "113040",
    "end": "119200"
  },
  {
    "text": "and support more than 30 languages which is important to understand some of the challenges that we have as a ml",
    "start": "119200",
    "end": "125759"
  },
  {
    "text": "platform team we have more than 100 million",
    "start": "125759",
    "end": "131520"
  },
  {
    "text": "users monthly and you can find more than 6 million recipes shared globally",
    "start": "131520",
    "end": "139280"
  },
  {
    "text": "in the app you can browse recipes from ingredients that are in season to get inspired",
    "start": "141200",
    "end": "147440"
  },
  {
    "text": "or you can follow authors like craig one of my favorites who uploads amazing recipes",
    "start": "147440",
    "end": "153360"
  },
  {
    "text": "but you could also search by ingredients dish or cooking process etc",
    "start": "153360",
    "end": "161280"
  },
  {
    "text": "machine learning is more available than ever in the past few years",
    "start": "164319",
    "end": "171120"
  },
  {
    "text": "with the adoption of mlo practices and tools",
    "start": "171120",
    "end": "176560"
  },
  {
    "text": "we have removed most of the pain points to deploy ml in production",
    "start": "176560",
    "end": "181920"
  },
  {
    "text": "and that means that more ml teams have moved from working in isolation",
    "start": "182480",
    "end": "187920"
  },
  {
    "text": "creating proof of concepts towards being deployed and distributed in product teams delivery teams or",
    "start": "187920",
    "end": "194560"
  },
  {
    "text": "feature teams wherever the name your companies keep these type of teams as a result",
    "start": "194560",
    "end": "200319"
  },
  {
    "text": "more ml is running it has moved now from running heavy batch jobs in the",
    "start": "200319",
    "end": "205519"
  },
  {
    "text": "background towards running more and more online inference",
    "start": "205519",
    "end": "212159"
  },
  {
    "text": "and with more ml models complex models available",
    "start": "212159",
    "end": "219280"
  },
  {
    "text": "that means that the infrastructure requirement to run online inference in gpu comes along",
    "start": "219440",
    "end": "226560"
  },
  {
    "text": "because we need those inference to run smoothly and quick for good user",
    "start": "226560",
    "end": "232959"
  },
  {
    "text": "experience",
    "start": "232959",
    "end": "235439"
  },
  {
    "text": "also as a machine learning platform team you probably don't want to lock your ml",
    "start": "238080",
    "end": "243120"
  },
  {
    "text": "teams to use a specific ml framework to simplify your inference server",
    "start": "243120",
    "end": "249760"
  },
  {
    "text": "so multiple ml framework support is a requirement while keeping the user",
    "start": "249760",
    "end": "255519"
  },
  {
    "text": "experience for ml engineers easy for them to deploy new models in production",
    "start": "255519",
    "end": "262079"
  },
  {
    "text": "in the next few minutes i'm going to tell you how triton inference server can help you improve the performance while",
    "start": "262079",
    "end": "268880"
  },
  {
    "text": "keeping the user experience simple for dml engineers and also reduce the cost",
    "start": "268880",
    "end": "276479"
  },
  {
    "text": "and i know what you probably think you know he's gonna start talking about cost and you care about performance right but",
    "start": "277120",
    "end": "283360"
  },
  {
    "text": "i promise you that this is going to be also a key element to performance",
    "start": "283360",
    "end": "289520"
  },
  {
    "text": "in my personal experience and also talking to other ml practitioners running ml in gpus is not particularly",
    "start": "289520",
    "end": "296639"
  },
  {
    "text": "difficult you add the right nodes to your auto scaling groups",
    "start": "296639",
    "end": "301759"
  },
  {
    "text": "you add the right tolerations to your deployments",
    "start": "301759",
    "end": "306880"
  },
  {
    "text": "and then request a gpu and you got it you've got your application running on the gpu",
    "start": "306880",
    "end": "312160"
  },
  {
    "text": "the challenge is that you have to balance the value the user value that you are",
    "start": "312160",
    "end": "318960"
  },
  {
    "text": "adding by deploying a new feature with the business value that you get in return",
    "start": "318960",
    "end": "325120"
  },
  {
    "text": "with the cost which could be quite high and why is that",
    "start": "325120",
    "end": "332720"
  },
  {
    "text": "in this simplified example let's say that i want to deploy a model application into a cpu node",
    "start": "333440",
    "end": "340800"
  },
  {
    "text": "i deploy me up my application i request up an amount of resources and if i'm",
    "start": "340880",
    "end": "346240"
  },
  {
    "text": "down my my homework correctly my application will utilize a good portion",
    "start": "346240",
    "end": "351360"
  },
  {
    "text": "of that leading into a well utilized and healthy cluster",
    "start": "351360",
    "end": "357520"
  },
  {
    "text": "and what happens if i want to deploy a new model well since cpu and memory are resources",
    "start": "357520",
    "end": "365919"
  },
  {
    "text": "in which you can request a portion of your new model will share the same note",
    "start": "365919",
    "end": "372880"
  },
  {
    "text": "than your previous one so as a result you have added new user",
    "start": "372880",
    "end": "378720"
  },
  {
    "text": "new value to your users by deploying a new feature you have",
    "start": "378720",
    "end": "383919"
  },
  {
    "text": "probably increased some business metrics and the infrastructure cost has",
    "start": "383919",
    "end": "389199"
  },
  {
    "text": "maintained stable happy days but what happens if your model requires",
    "start": "389199",
    "end": "396400"
  },
  {
    "text": "gpu for inference as we have",
    "start": "396400",
    "end": "401840"
  },
  {
    "text": "listened today before gpu's resources",
    "start": "401840",
    "end": "408319"
  },
  {
    "text": "you cannot request a fractional amount of them so if your model requires gpu you have",
    "start": "408319",
    "end": "414000"
  },
  {
    "text": "to request the full gpu for it and that means low utilization",
    "start": "414000",
    "end": "419759"
  },
  {
    "text": "but okay let's say that the feature is worth it you pay the price and then you deploy your model",
    "start": "419759",
    "end": "426080"
  },
  {
    "text": "what happens the next time you want to deploy a new model exactly you have to request another gpu",
    "start": "426080",
    "end": "436000"
  },
  {
    "text": "and remember when i said before that we are a global community in more than 70 countries and supporting",
    "start": "436000",
    "end": "442880"
  },
  {
    "text": "more than 30 languages no matter how simple the feature we want",
    "start": "442880",
    "end": "448080"
  },
  {
    "text": "to deploy is there is very little chance that we can train a model",
    "start": "448080",
    "end": "453599"
  },
  {
    "text": "that will perform well in all of the regions or all the all the languages so that means that for every feature",
    "start": "453599",
    "end": "460479"
  },
  {
    "text": "that we want to deploy we end up having three four five models for it to cover",
    "start": "460479",
    "end": "467440"
  },
  {
    "text": "for the most popular regions or languages a decent gpu card in our cloud provider",
    "start": "467440",
    "end": "475520"
  },
  {
    "text": "is around three dollars an hour which will take you to two over two thousand dollars a month",
    "start": "475520",
    "end": "481360"
  },
  {
    "text": "and if you have to deploy four or five every time you want to deploy a new feature we are talking about",
    "start": "481360",
    "end": "487520"
  },
  {
    "text": "considerable money",
    "start": "487520",
    "end": "491240"
  },
  {
    "text": "and if you are thinking that maybe we could scale the cost by using multiple gpus environments",
    "start": "493919",
    "end": "501280"
  },
  {
    "text": "the cost of a node with four gpus is exactly or very similar to four times a",
    "start": "501280",
    "end": "507520"
  },
  {
    "text": "single gpu so you can even scale the cost as way",
    "start": "507520",
    "end": "512479"
  },
  {
    "text": "however if you deploy triton inference server on a gpu",
    "start": "513279",
    "end": "519518"
  },
  {
    "text": "you can concurrently host multiple models in the same gpu",
    "start": "519519",
    "end": "524560"
  },
  {
    "text": "and if you deploy it in an environment with multiple gpus triton will replicate those models in",
    "start": "524560",
    "end": "531440"
  },
  {
    "text": "each gpu so that it can balance the inference compute in each of them",
    "start": "531440",
    "end": "537680"
  },
  {
    "text": "to maximize utilization",
    "start": "537680",
    "end": "541200"
  },
  {
    "text": "you're probably wondering what this means if it's a gpu on a walk in the countryside",
    "start": "544800",
    "end": "551200"
  },
  {
    "text": "or you see gpu deploying windows xp well what i'm trying to say here is that",
    "start": "551200",
    "end": "558320"
  },
  {
    "text": "triton has enabled you a happy path to deploy",
    "start": "558320",
    "end": "563519"
  },
  {
    "text": "models in gpu in a cost effective way",
    "start": "563519",
    "end": "568720"
  },
  {
    "text": "and that is the first performance gain that you can get with triton every time you deploy your models in cpu",
    "start": "570000",
    "end": "577600"
  },
  {
    "text": "because c gpu is too expensive you could migrate them to gpu",
    "start": "577600",
    "end": "584080"
  },
  {
    "text": "but triton also comes with a few other options for optimization",
    "start": "585440",
    "end": "591760"
  },
  {
    "text": "if your model allows batching you could easily configure dynamic patching for",
    "start": "591760",
    "end": "597360"
  },
  {
    "text": "your model so that triton accumulates a number of individual",
    "start": "597360",
    "end": "603440"
  },
  {
    "text": "requests and then build a larger batch that will compute more efficiently than",
    "start": "603440",
    "end": "610560"
  },
  {
    "text": "doing it individually dynamic batching can also be",
    "start": "610560",
    "end": "616839"
  },
  {
    "text": "configured by you could select a maximum amount of batch",
    "start": "616839",
    "end": "622640"
  },
  {
    "text": "you could add a maximum delay so that tritone will run the inference of the",
    "start": "622640",
    "end": "629600"
  },
  {
    "text": "batch as it is if it reaches that delay but more like you could",
    "start": "629600",
    "end": "636240"
  },
  {
    "text": "preserve the order so that triton responses responds in the same order",
    "start": "636240",
    "end": "641680"
  },
  {
    "text": "that requests arrived",
    "start": "641680",
    "end": "646040"
  },
  {
    "text": "and moving on from dynamic batching another interesting optimization is model",
    "start": "647200",
    "end": "652560"
  },
  {
    "text": "instances you can easily configure triton to replicate",
    "start": "652560",
    "end": "658320"
  },
  {
    "text": "an amount of times each model in a given gpu",
    "start": "658320",
    "end": "665680"
  },
  {
    "text": "that allows triton to overlap the transportation of data from and to",
    "start": "665680",
    "end": "672640"
  },
  {
    "text": "cpu and gpu and overlaps with the inference compute",
    "start": "672640",
    "end": "677839"
  },
  {
    "text": "finally you could also combine the both into your model so you could",
    "start": "677839",
    "end": "684560"
  },
  {
    "text": "configure dynamic patching and model instances",
    "start": "684560",
    "end": "689839"
  },
  {
    "text": "before i also said that triton will help you to",
    "start": "694079",
    "end": "699440"
  },
  {
    "text": "improve the user experience in the worst cases scenario very simplified our ml engineers if they",
    "start": "699440",
    "end": "706560"
  },
  {
    "text": "want to deploy a new model they will have to create several resource kubernetes resources",
    "start": "706560",
    "end": "712000"
  },
  {
    "text": "for it right a deployment a service a config map service account to give some permissions hpa to make sure that we",
    "start": "712000",
    "end": "719360"
  },
  {
    "text": "scale dynamically under certain circumstances and a pot disruption budget to make sure that we always run a",
    "start": "719360",
    "end": "725839"
  },
  {
    "text": "minimal amount of replicas but allow me to repeat myself",
    "start": "725839",
    "end": "730959"
  },
  {
    "text": "when i said that we have a challenge with the amount of languages and regions that we deploy",
    "start": "730959",
    "end": "737200"
  },
  {
    "text": "that we support so that for every feature we end up replicating those",
    "start": "737200",
    "end": "742560"
  },
  {
    "text": "resources well it's it's not that bad",
    "start": "742560",
    "end": "747920"
  },
  {
    "text": "really because uh using an open source tool called customize we are able to",
    "start": "747920",
    "end": "753279"
  },
  {
    "text": "template the base of the application and then deploy new models will result in",
    "start": "753279",
    "end": "759120"
  },
  {
    "text": "just patching a config map etc but that doesn't result resolve all",
    "start": "759120",
    "end": "764160"
  },
  {
    "text": "the issues because if you also need to modify the",
    "start": "764160",
    "end": "769440"
  },
  {
    "text": "resource allocation depending on the region or more and more you end up patching more and more files and it's a",
    "start": "769440",
    "end": "775680"
  },
  {
    "text": "bit complex",
    "start": "775680",
    "end": "778320"
  },
  {
    "text": "oh now with triton our machine learning",
    "start": "782959",
    "end": "789120"
  },
  {
    "text": "engineers don't have to worry about what ml framework they use to train their model",
    "start": "789120",
    "end": "794880"
  },
  {
    "text": "because triton supports all the all the ml backends that we desire like pytorch",
    "start": "794880",
    "end": "800320"
  },
  {
    "text": "or tensorflow tensor rt on x they only need to package the model",
    "start": "800320",
    "end": "807920"
  },
  {
    "text": "following a layout so that triton understands what type of backend needs to use and that process is",
    "start": "807920",
    "end": "815920"
  },
  {
    "text": "documented in triton also once the model is available in the",
    "start": "815920",
    "end": "821600"
  },
  {
    "text": "given packet they only need to create a pr to modify the right config file",
    "start": "821600",
    "end": "828480"
  },
  {
    "text": "and our cluster automation will do the rest",
    "start": "828480",
    "end": "832399"
  },
  {
    "text": "is this working here we are",
    "start": "836399",
    "end": "841760"
  },
  {
    "text": "to summarize big thumbs up for nvidia for creating triton inference server who",
    "start": "841760",
    "end": "846959"
  },
  {
    "text": "is enable us to improve the performance of our models",
    "start": "846959",
    "end": "853279"
  },
  {
    "text": "sharing resources in gpu and improving our gpu utilization",
    "start": "853279",
    "end": "860320"
  },
  {
    "text": "while keeping the user experience easy for our machine learning engineers",
    "start": "860320",
    "end": "867120"
  },
  {
    "text": "and with that i'll leave you with bry who is gonna demo some of these features",
    "start": "867120",
    "end": "872720"
  },
  {
    "text": "thank you very much thank you [Applause]",
    "start": "872720",
    "end": "879519"
  },
  {
    "text": "thank you um this will not be much of a demo more showing you how we deploy our models on",
    "start": "879519",
    "end": "886880"
  },
  {
    "text": "gpu and how it performs versus our previous deployment which is on cpu",
    "start": "886880",
    "end": "895680"
  },
  {
    "text": "we will be using apache bench for a simple benchmarking for this",
    "start": "895680",
    "end": "901600"
  },
  {
    "text": "section we'll first but first we'll show you how easy it is to",
    "start": "901600",
    "end": "908880"
  },
  {
    "text": "deploy a model on triton and even before that a little bit of context",
    "start": "908880",
    "end": "914160"
  },
  {
    "text": "for ourselves today we are going to measure a pi torch based",
    "start": "914160",
    "end": "920399"
  },
  {
    "text": "image transformation model so it accepts an image data and then it returns that image representation in",
    "start": "920399",
    "end": "926720"
  },
  {
    "text": "embedding space so the input is with the input will be a 244 by 244 by 3 multi-dimensional array",
    "start": "926720",
    "end": "935519"
  },
  {
    "text": "and the output will be 300 floating point and",
    "start": "935519",
    "end": "941680"
  },
  {
    "text": "for those of you who've deployed model machine learning model before in some cases gpu is not even necessary",
    "start": "941680",
    "end": "949759"
  },
  {
    "text": "for real-time inferencing but the one",
    "start": "949759",
    "end": "954880"
  },
  {
    "text": "the one we are going to show you is the kind of model that get the most benefit when deployed on a gpu",
    "start": "954880",
    "end": "961759"
  },
  {
    "text": "and this is actually one of the models that we use that we actively using at cookpad",
    "start": "961759",
    "end": "969440"
  },
  {
    "text": "so the setup is that for the cpu best deployment it will be a",
    "start": "969440",
    "end": "975920"
  },
  {
    "text": "simple python service it will load the model using pytorch",
    "start": "975920",
    "end": "981199"
  },
  {
    "text": "we are going to deploy it on a compute optimized ec2 instance",
    "start": "981199",
    "end": "986320"
  },
  {
    "text": "so we got beefier cpu because model deployment is mostly a cpu bound task",
    "start": "986320",
    "end": "992160"
  },
  {
    "text": "and for the gpu based one we will drop the model in triton but as you can see",
    "start": "992160",
    "end": "997839"
  },
  {
    "text": "we are adding a front-end python service in front of it just to make it easier for apigee events to hammer it",
    "start": "997839",
    "end": "1006480"
  },
  {
    "text": "this though adds another network overhead between apache bench and triton",
    "start": "1007199",
    "end": "1013920"
  },
  {
    "text": "but you can see next that it's not significant and also in real world deployment you usually need a place to",
    "start": "1013920",
    "end": "1021199"
  },
  {
    "text": "pre and pass processing the input and output of your model and in fact this is",
    "start": "1021199",
    "end": "1026959"
  },
  {
    "text": "what we end up deploying all our services at goodpad",
    "start": "1026959",
    "end": "1031760"
  },
  {
    "text": "you're probably familiar with how to load a model and deploy using python so",
    "start": "1032559",
    "end": "1038880"
  },
  {
    "text": "we're moving next to how to deploy it on triton and a few slides ago",
    "start": "1038880",
    "end": "1046160"
  },
  {
    "text": "jose showed you this in at goodpad our our teams have to",
    "start": "1046160",
    "end": "1051520"
  },
  {
    "text": "provide their own manifest when they want to deploy a service and there are some tools available to",
    "start": "1051520",
    "end": "1058480"
  },
  {
    "text": "abstract that for you but we're not currently using that the reason for that is a whole new story",
    "start": "1058480",
    "end": "1064799"
  },
  {
    "text": "and for triton because we as the platform engine uh platform team provide and manage the",
    "start": "1064799",
    "end": "1070960"
  },
  {
    "text": "deployment we've taken those manufacturers out of the equation and then what's left for",
    "start": "1070960",
    "end": "1077520"
  },
  {
    "text": "every team we want to deploy a model is that they just need to package the models",
    "start": "1077520",
    "end": "1082799"
  },
  {
    "text": "put it somewhere accessible in this case it's our private s3 packet and put that",
    "start": "1082799",
    "end": "1088960"
  },
  {
    "text": "as a uri to that package in our shared config",
    "start": "1088960",
    "end": "1095240"
  },
  {
    "text": "and one thing to note is that we need to package each model",
    "start": "1096000",
    "end": "1102960"
  },
  {
    "text": "with a structure that triton can work with and it looks like this so it's very simple",
    "start": "1102960",
    "end": "1108960"
  },
  {
    "text": "those who've worked with tensorflow surfing probably familiar with the structure or tensorflow this is what came up if",
    "start": "1108960",
    "end": "1116000"
  },
  {
    "text": "you do a safe model using tensorflow at the root is the name of the model you want to expose it",
    "start": "1116000",
    "end": "1121679"
  },
  {
    "text": "as and the next is the fusion number of the model",
    "start": "1121679",
    "end": "1127280"
  },
  {
    "text": "and then the model file itself you can put any kinds of any model you change with multiple kinds",
    "start": "1127280",
    "end": "1133440"
  },
  {
    "text": "of platform here and this is the minimum that you need excluding that",
    "start": "1133440",
    "end": "1139320"
  },
  {
    "text": "config.pbtxd that's optional triton will try to make sense of your",
    "start": "1139320",
    "end": "1144480"
  },
  {
    "text": "model and then generate that file for you but it's there if you want to make an explicit configuration of your model",
    "start": "1144480",
    "end": "1150720"
  },
  {
    "text": "for example like this so as mentioned before",
    "start": "1150720",
    "end": "1157200"
  },
  {
    "text": "you can deploy model from multiple platform multiple framework",
    "start": "1157200",
    "end": "1164160"
  },
  {
    "text": "triton supports tensorflow pytorch scalar model",
    "start": "1164160",
    "end": "1169280"
  },
  {
    "text": "and one of the models we use is that tensor rt which is like an optimized",
    "start": "1169280",
    "end": "1176160"
  },
  {
    "text": "compiled version of a model so you free to provide this at the model",
    "start": "1176160",
    "end": "1182559"
  },
  {
    "text": "level and try to reload it after you specify the platform you",
    "start": "1182559",
    "end": "1187600"
  },
  {
    "text": "specify the input and output signature and this is also where you're going to put your models inference",
    "start": "1187600",
    "end": "1195120"
  },
  {
    "text": "tuning config which we'll see later in the presentation",
    "start": "1195120",
    "end": "1200559"
  },
  {
    "text": "okay so compress the whole directory um",
    "start": "1200559",
    "end": "1206880"
  },
  {
    "text": "and then upload it to s3 and what they need to do next is just add this line",
    "start": "1206880",
    "end": "1213039"
  },
  {
    "text": "specifying where the model is and boost the changes and then let your ci cd",
    "start": "1213039",
    "end": "1219039"
  },
  {
    "text": "process that file which is actually with in our case it's a repository full of kubernetes",
    "start": "1219039",
    "end": "1225679"
  },
  {
    "text": "manifest we are we use flux in our case to synchronize those and wait for it to",
    "start": "1225679",
    "end": "1232799"
  },
  {
    "text": "roll out this is usually when i go to procrastinate and browse stuff on the",
    "start": "1232799",
    "end": "1239120"
  },
  {
    "text": "internet but you don't want to see me doing that today now we're going to see how they perform",
    "start": "1239120",
    "end": "1246080"
  },
  {
    "text": "so simple tests at the top is the cpu deployment and at the bottom is gpu",
    "start": "1246640",
    "end": "1252640"
  },
  {
    "text": "deployment",
    "start": "1252640",
    "end": "1255120"
  },
  {
    "text": "it takes quite a bit while so let's just skip um so this is the baseline number",
    "start": "1257760",
    "end": "1263760"
  },
  {
    "text": "um we start with no concurrent requests at all this is single threaded this number are as fast as we can get",
    "start": "1263760",
    "end": "1269200"
  },
  {
    "text": "out of these services um it's already fun looking at this when we first deploy triton",
    "start": "1269200",
    "end": "1276480"
  },
  {
    "text": "we'll try next and remember this is including the network overhead between our front end service and triton",
    "start": "1276480",
    "end": "1284159"
  },
  {
    "text": "now let's try with two concurrent requests",
    "start": "1284720",
    "end": "1291039"
  },
  {
    "text": "this will take a little bit longer than the previous one so we'll skip again",
    "start": "1291039",
    "end": "1296880"
  },
  {
    "text": "and this is a result not much changes from triton which is",
    "start": "1296880",
    "end": "1302400"
  },
  {
    "text": "good but as you can see the the one running on cpu already doubles the latency",
    "start": "1302400",
    "end": "1309120"
  },
  {
    "text": "so then a little bit more work now we are going to look at how the resource",
    "start": "1309120",
    "end": "1316159"
  },
  {
    "text": "utilization with these two surveys so we're going to leave the single charter benchmark running out",
    "start": "1316159",
    "end": "1322159"
  },
  {
    "text": "for a bit longer in the background and this is what we get",
    "start": "1322159",
    "end": "1328559"
  },
  {
    "text": "let's break this down at the top you see the cpu first gpu and memory usage as",
    "start": "1328559",
    "end": "1333760"
  },
  {
    "text": "you can see on the cpu it uses one which is 100 of",
    "start": "1333760",
    "end": "1339840"
  },
  {
    "text": "a single cpu available for that deployment we don't limit the cpu resource for this",
    "start": "1339840",
    "end": "1346320"
  },
  {
    "text": "one so that's the most they can use the reason being",
    "start": "1346320",
    "end": "1351520"
  },
  {
    "text": "if you do an inference on cpu that's they only use one cpu at all",
    "start": "1351520",
    "end": "1357919"
  },
  {
    "text": "at a time and for gpu we are only using 50",
    "start": "1357919",
    "end": "1363120"
  },
  {
    "text": "of a single gpu so a little bit of more room there on the memory",
    "start": "1363120",
    "end": "1369280"
  },
  {
    "text": "we see the reverse of that um so the cpu only uses around 200 megabytes but",
    "start": "1369280",
    "end": "1374960"
  },
  {
    "text": "tryton uses almost twice that number but this is that's vram video memory",
    "start": "1374960",
    "end": "1381360"
  },
  {
    "text": "and not ram on the server itself so it's not apple to apple but you get a sense of",
    "start": "1381360",
    "end": "1387360"
  },
  {
    "text": "how you should plan your capacity with this and the fun part is at the bottom",
    "start": "1387360",
    "end": "1393120"
  },
  {
    "text": "we get almost 10 times throughput and then 10 times faster latency out of gpu",
    "start": "1393120",
    "end": "1401440"
  },
  {
    "text": "so this is fine double the double the resource from memory half the",
    "start": "1401440",
    "end": "1407760"
  },
  {
    "text": "processing time sorry processing capacity",
    "start": "1408480",
    "end": "1414640"
  },
  {
    "text": "no but you get 10 times the throughput um",
    "start": "1414640",
    "end": "1420240"
  },
  {
    "text": "also of course it's not all magic at some point you",
    "start": "1420240",
    "end": "1425360"
  },
  {
    "text": "exhaust the results at as you can see here we are starting at two concurrent requests gpu is at one hundred percent",
    "start": "1425360",
    "end": "1433440"
  },
  {
    "text": "uh request read already max at 100 requests per second and each time you double which you can see it the two",
    "start": "1433440",
    "end": "1439679"
  },
  {
    "text": "arrows there latency will only increase because requests are",
    "start": "1439679",
    "end": "1444960"
  },
  {
    "text": "waiting in queue well to be honest at this point",
    "start": "1444960",
    "end": "1450799"
  },
  {
    "text": "this is already good for us most people can just wrap it up and move on to the next model but i do understand",
    "start": "1450799",
    "end": "1458400"
  },
  {
    "text": "that we need to scale from this point you have multiple options so so",
    "start": "1458400",
    "end": "1465279"
  },
  {
    "text": "the easiest one is horizontal scaling okay",
    "start": "1465279",
    "end": "1470799"
  },
  {
    "text": "that stuff explanatory uh the other one is fortica scaling this is using single gpu on a node if you use multiple gpu",
    "start": "1470880",
    "end": "1478559"
  },
  {
    "text": "triton will spread the models on whatever gpu available so you get parallel",
    "start": "1478559",
    "end": "1484840"
  },
  {
    "text": "processing the other two is the one that jose mentioned previously",
    "start": "1484840",
    "end": "1491120"
  },
  {
    "text": "you have two options you can specify an instance group so you deploy multiple instances of your model",
    "start": "1491440",
    "end": "1497600"
  },
  {
    "text": "in one gpu that is if you have a room",
    "start": "1497600",
    "end": "1503440"
  },
  {
    "text": "in the gpu capacity so triton will try to",
    "start": "1503440",
    "end": "1509600"
  },
  {
    "text": "provide multiple instances of the model and then you get parallel processing the other one",
    "start": "1509600",
    "end": "1515039"
  },
  {
    "text": "if your models allows dynamic batching that's the next thing you can use to",
    "start": "1515039",
    "end": "1520240"
  },
  {
    "text": "squeeze more throughput out of it so you can specify the config on the right",
    "start": "1520240",
    "end": "1525520"
  },
  {
    "text": "to tell triton to pull all the incoming requests and then execute it in one pass",
    "start": "1525520",
    "end": "1531120"
  },
  {
    "text": "there's benefit in memory data transfer between",
    "start": "1531120",
    "end": "1536480"
  },
  {
    "text": "the server and the gpu and that's it you can also combine these techniques",
    "start": "1536480",
    "end": "1543840"
  },
  {
    "text": "to get more out of your deployment and that's it for me that's it from us",
    "start": "1543840",
    "end": "1549760"
  },
  {
    "text": "thank you for your time next slide",
    "start": "1549760",
    "end": "1555960"
  },
  {
    "text": "thank you [Applause] so",
    "start": "1556000",
    "end": "1561200"
  },
  {
    "text": "it's time for q a um we are hiring and also you want to mention about i",
    "start": "1561200",
    "end": "1566640"
  },
  {
    "text": "yes so we will be having some drinks later next to the",
    "start": "1566640",
    "end": "1572000"
  },
  {
    "text": "city of arts and science so if anyone wants to join us to talk about email",
    "start": "1572000",
    "end": "1577840"
  },
  {
    "text": "cooking or anything just come and see us so we can send you an invite we've got some budget from the company to pay the",
    "start": "1577840",
    "end": "1584159"
  },
  {
    "text": "first round so",
    "start": "1584159",
    "end": "1586960"
  },
  {
    "text": "that's good to take questions yeah yeah yeah of course if there's time",
    "start": "1596480",
    "end": "1601360"
  },
  {
    "text": "hi hi which back ends are currently are currently supported",
    "start": "1604320",
    "end": "1609600"
  },
  {
    "text": "in the triton uh you've got pi torch tensorflow",
    "start": "1609600",
    "end": "1614880"
  },
  {
    "text": "tensor rt on x sk um scalar you can use that as well",
    "start": "1614880",
    "end": "1621840"
  },
  {
    "text": "but in order i know pi touch but in order to run on triton you need to convert",
    "start": "1621840",
    "end": "1627679"
  },
  {
    "text": "the model to torch script or torch serve something like that do you know",
    "start": "1627679",
    "end": "1632960"
  },
  {
    "text": "uh did you did something like that need to convert your model oh so for this step forward okay sorry",
    "start": "1632960",
    "end": "1640159"
  },
  {
    "text": "um for pythords for this demo um we try two stuff actually the tritone itself and",
    "start": "1640159",
    "end": "1646320"
  },
  {
    "text": "then the tensor rt yes you're correct you need to for tensority you need a torch model pythons",
    "start": "1646320",
    "end": "1653679"
  },
  {
    "text": "model that supports touchscript so if everything works on tensor basically you",
    "start": "1653679",
    "end": "1659279"
  },
  {
    "text": "can use that is it working straightforward do you have any problem with that",
    "start": "1659279",
    "end": "1666799"
  },
  {
    "text": "identifying which which part in the graph that you stands for or not that's the hard part",
    "start": "1666799",
    "end": "1672960"
  },
  {
    "text": "but once you isolate them you can take it out for preprocessing and then move the pure tensor operation into this",
    "start": "1672960",
    "end": "1680000"
  },
  {
    "text": "thank you very much",
    "start": "1680000",
    "end": "1683480"
  },
  {
    "text": "any other questions so triton inference error is",
    "start": "1691200",
    "end": "1697840"
  },
  {
    "text": "actually one of the few uh servings that supports mixed precision",
    "start": "1697840",
    "end": "1703520"
  },
  {
    "text": "on tensorflow uh have you done any benchmarking on the mixed precision versus the tensor rt in float point 16.",
    "start": "1703520",
    "end": "1713039"
  },
  {
    "text": "um let me repeat that so benchmark the difference between the",
    "start": "1713200",
    "end": "1719679"
  },
  {
    "text": "floating point 16 operation on tensor rt and mixed precision on tensorflow because they are kind of comparable but",
    "start": "1719679",
    "end": "1725840"
  },
  {
    "text": "not entirely have you done any benchmarks between those like models yeah yeah um so what we",
    "start": "1725840",
    "end": "1732240"
  },
  {
    "text": "what we check for floating points the different floating points um 16",
    "start": "1732240",
    "end": "1737520"
  },
  {
    "text": "versus 32 for example what we do is that we check the the difference between the output",
    "start": "1737520",
    "end": "1744240"
  },
  {
    "text": "and then we compare it we find we find the difference between the two and then",
    "start": "1744240",
    "end": "1750000"
  },
  {
    "text": "we kind of compare the performance of the output and then",
    "start": "1750000",
    "end": "1755120"
  },
  {
    "text": "come to a value that we accept so there are differences so i believe",
    "start": "1755120",
    "end": "1760320"
  },
  {
    "text": "for this particular model it was 10 to the power of minus",
    "start": "1760320",
    "end": "1765440"
  },
  {
    "text": "nine and that's if you if you don't reduce the precision of the plotting point",
    "start": "1765440",
    "end": "1772640"
  },
  {
    "text": "so just by floating point three to thirty two from pythog to tensor rt you",
    "start": "1772640",
    "end": "1778399"
  },
  {
    "text": "get that much difference okay and what about the perf not like",
    "start": "1778399",
    "end": "1784480"
  },
  {
    "text": "the accuracy differences between the like floating point 32 and 40.16 like",
    "start": "1784480",
    "end": "1789840"
  },
  {
    "text": "were you like what was the scope there it's not much difference",
    "start": "1789840",
    "end": "1795840"
  },
  {
    "text": "it's not very significant but i'm not exactly remember the number yeah okay thank you",
    "start": "1795840",
    "end": "1802480"
  },
  {
    "text": "any other questions",
    "start": "1806559",
    "end": "1809840"
  },
  {
    "text": "nope",
    "start": "1812480",
    "end": "1814799"
  },
  {
    "text": "hi uh thank you for the um so i'm wondering there there are more",
    "start": "1819039",
    "end": "1824799"
  },
  {
    "text": "inference surfing frameworks out there what what made you guys pick the triton inference server",
    "start": "1824799",
    "end": "1832720"
  },
  {
    "text": "yes uh so we found with triton influence server the ability to host multiple",
    "start": "1834000",
    "end": "1840080"
  },
  {
    "text": "models in a single gpu being the killer feature for us because that allow us to share the cost",
    "start": "1840080",
    "end": "1847360"
  },
  {
    "text": "between several different applications so that",
    "start": "1847360",
    "end": "1852880"
  },
  {
    "text": "the decision between deploying a feature or not comes back to just understanding the",
    "start": "1852880",
    "end": "1858799"
  },
  {
    "text": "value that you add into the users and the the one the value that you get back whereas",
    "start": "1858799",
    "end": "1865440"
  },
  {
    "text": "the cost element of the decision has been reduced and because now you can share the cost",
    "start": "1865440",
    "end": "1872399"
  },
  {
    "text": "again with multiple multi-multiple applications as we do with the cpu workloads",
    "start": "1872399",
    "end": "1878960"
  },
  {
    "text": "and also that it supports multiple frameworks that's why we choose triton because you cannot force your machine",
    "start": "1878960",
    "end": "1886240"
  },
  {
    "text": "learning team to just use one framework yeah they have preferences so just in this case we",
    "start": "1886240",
    "end": "1891600"
  },
  {
    "text": "we initially have pi torch and tensorflow deployed on our cluster",
    "start": "1891600",
    "end": "1896880"
  },
  {
    "text": "just combining that into one instead of having to manage two deployment stack is",
    "start": "1896880",
    "end": "1902159"
  },
  {
    "text": "beneficial to us",
    "start": "1902159",
    "end": "1905880"
  },
  {
    "text": "okay thank you all right thank you very much",
    "start": "1911039",
    "end": "1917080"
  },
  {
    "text": "[Applause]",
    "start": "1917080",
    "end": "1922569"
  }
]