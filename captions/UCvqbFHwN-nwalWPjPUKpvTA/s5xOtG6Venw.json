[
  {
    "text": "maybe yeah let's get started hi everyone thanks for being here with us really uh",
    "start": "240",
    "end": "6440"
  },
  {
    "text": "excited to uh present The sparkon kubernetes Talk today uh my name is uh",
    "start": "6440",
    "end": "13280"
  },
  {
    "text": "Melody Yang I'm a senior Big Data architect in AWS with me is my co-presenter uh Joong",
    "start": "13280",
    "end": "22119"
  },
  {
    "text": "so he is the Big Data engineer from the Ali cloud and also he is the creator of",
    "start": "22119",
    "end": "28039"
  },
  {
    "text": "Apache cbone So yeah thank",
    "start": "28039",
    "end": "33280"
  },
  {
    "text": "you so um sorry I have to present in English uh because I don't want to make",
    "start": "33280",
    "end": "40360"
  },
  {
    "text": "a mistake when I'm presenting in English called the chapel Service as a",
    "start": "40360",
    "end": "47320"
  },
  {
    "text": "Cai uh so we are not really presenting about the gambling uh industry use case",
    "start": "47719",
    "end": "53960"
  },
  {
    "text": "uh actually Shuffle service means a spark data shuffling right so I'm going to present in English and um uh Kong he",
    "start": "53960",
    "end": "61840"
  },
  {
    "text": "is going to present in Chinese to Deep dive about the Apache Calon service",
    "start": "61840",
    "end": "68240"
  },
  {
    "text": "don't worry if you don't understand uh we have a lots of pictures architecture diagrams for you to understand it's all",
    "start": "68240",
    "end": "75400"
  },
  {
    "text": "written in English yeah so if you have any questions in English uh that's",
    "start": "75400",
    "end": "81000"
  },
  {
    "text": "totally fine I can translate okay thank you for joining us um yeah so let's get",
    "start": "81000",
    "end": "88520"
  },
  {
    "text": "started um so in the era of data driving everything aache spark has uh emerged as",
    "start": "88520",
    "end": "95600"
  },
  {
    "text": "a very popular Big Data framework right it can handle large scale data",
    "start": "95600",
    "end": "102079"
  },
  {
    "text": "processing needs um uh the common use cases are machine learning such as the",
    "start": "102079",
    "end": "109000"
  },
  {
    "text": "Hot Topic we are talking about today G or ETL processes right some data",
    "start": "109000",
    "end": "116640"
  },
  {
    "text": "scientists said 80% of the their workload is about ETL data",
    "start": "116640",
    "end": "123320"
  },
  {
    "text": "engineering so what is the challenge when we are",
    "start": "123320",
    "end": "128560"
  },
  {
    "text": "talking about spark on kubernetes one of the well known uh",
    "start": "128560",
    "end": "134239"
  },
  {
    "text": "issue uh challenges is uh how to support the dynamic resource allocation Dr for",
    "start": "134239",
    "end": "142599"
  },
  {
    "text": "short right so this is our talk today we will cover uh what is the key challenges",
    "start": "142599",
    "end": "150120"
  },
  {
    "text": "in spark on kubernetes especially around the shule uh data shule and Dr uh I will",
    "start": "150120",
    "end": "156200"
  },
  {
    "text": "also share with you the firstand of benchmarking result when I tested the",
    "start": "156200",
    "end": "162120"
  },
  {
    "text": "solution with Apache uh Apache caliber uh after that I will hand over to K to",
    "start": "162120",
    "end": "168800"
  },
  {
    "text": "Deep dive into the spark Shuffle and Dr calone uh project then he he will also",
    "start": "168800",
    "end": "175879"
  },
  {
    "text": "cover what is the next in the road map",
    "start": "175879",
    "end": "181760"
  },
  {
    "text": "so I want to uh just ask the room how many of you are familiar with spark on",
    "start": "182760",
    "end": "191000"
  },
  {
    "text": "kubernetes great of course okay so actually less than half",
    "start": "191000",
    "end": "197280"
  },
  {
    "text": "of the audience in the room um so I will probably just quickly go through what is a spark on kubernetes all right so uh on",
    "start": "197280",
    "end": "205519"
  },
  {
    "text": "the left hand side we have kubernetes control plan and I'm a data person so I",
    "start": "205519",
    "end": "211360"
  },
  {
    "text": "don't really know much about kubernetes internal so we'll keep it quick and simple so inside the control plan we",
    "start": "211360",
    "end": "217360"
  },
  {
    "text": "have shular and API server then when we when user request the spark job uh",
    "start": "217360",
    "end": "225760"
  },
  {
    "text": "submit the job right the control plan will schedule the driver pot in the data",
    "start": "225760",
    "end": "233799"
  },
  {
    "text": "plan and inside the driver pot we have those components then the driver pot",
    "start": "233799",
    "end": "239400"
  },
  {
    "text": "will will send the request back to the uh scheduler saying it's time to",
    "start": "239400",
    "end": "244920"
  },
  {
    "text": "schedule some executor P so this is what happened we scheduled uh three nodes uh",
    "start": "244920",
    "end": "252239"
  },
  {
    "text": "in this case then we scheduled multiple uh amount of uh Spar executors on each",
    "start": "252239",
    "end": "260560"
  },
  {
    "text": "notes then finally we send executor pod watch events from the API server to the",
    "start": "260560",
    "end": "267479"
  },
  {
    "text": "executor pod so this is the common workflow for sparkon",
    "start": "267479",
    "end": "273680"
  },
  {
    "text": "communities let's talk about Dia challenges with the sparkon communities so what's this fast about",
    "start": "273680",
    "end": "281759"
  },
  {
    "text": "right um so since the spark 3.0 we have the light way of solution to support",
    "start": "281759",
    "end": "289720"
  },
  {
    "text": "Dynamic allocation uh without external Shuffle",
    "start": "289720",
    "end": "294759"
  },
  {
    "text": "service right so that means you can scale the number of executors Up and",
    "start": "294759",
    "end": "300800"
  },
  {
    "text": "Down based on your workloads and if your executor idle it can removed from your",
    "start": "300800",
    "end": "308000"
  },
  {
    "text": "kubernetes cluster if the pending tasks exists and it will scale up uh",
    "start": "308000",
    "end": "314840"
  },
  {
    "text": "requesting more uh spark executors can we see the problem here so",
    "start": "314840",
    "end": "320759"
  },
  {
    "text": "actually spark on kubernetes doesn't fully support um the ex external Shuffle",
    "start": "320759",
    "end": "327280"
  },
  {
    "text": "service and even though yet SS so external shuel service is natively",
    "start": "327280",
    "end": "332960"
  },
  {
    "text": "support by spark on Yang but it doesn't fully support in the kubernetes",
    "start": "332960",
    "end": "339080"
  },
  {
    "text": "pattern so the lightwe solution I mentioned earlier is the shuffle",
    "start": "339080",
    "end": "344240"
  },
  {
    "text": "tracking the last one shffle tracking must turn on when you need to enable the",
    "start": "344240",
    "end": "350639"
  },
  {
    "text": "dynamic resource allocation with spark on communities what happen if we don't turn",
    "start": "350639",
    "end": "357360"
  },
  {
    "text": "on so that's the error message you will see in your spark application immediately it's a little bit misleading",
    "start": "357360",
    "end": "364479"
  },
  {
    "text": "it just say the spark context error cannot initialize the driver program uh",
    "start": "364479",
    "end": "370000"
  },
  {
    "text": "because you have to turn on the external Shuffle service but like I said we don't",
    "start": "370000",
    "end": "376120"
  },
  {
    "text": "natively support external Shuffle service so what can we do just turn on the shuffle tracking",
    "start": "376120",
    "end": "385360"
  },
  {
    "text": "right so uh it's not okay so let's deep dive uh into the",
    "start": "385639",
    "end": "394280"
  },
  {
    "text": "challenge when we do Dr with a shle tracking so what's the problem here with",
    "start": "394280",
    "end": "400960"
  },
  {
    "text": "this lightwe solution right so the dashed line means with a",
    "start": "400960",
    "end": "408039"
  },
  {
    "text": "shuffle tracking right and the blue line actually represent the actual usage of",
    "start": "408039",
    "end": "413840"
  },
  {
    "text": "your compute and memory resources so we can see there's a lot of resource Wast",
    "start": "413840",
    "end": "420039"
  },
  {
    "text": "here right the Gap highlighted in Red so our customers have to pay for those idle",
    "start": "420039",
    "end": "428360"
  },
  {
    "text": "time let's dive into some example uh when we turn on the shuffle",
    "start": "428919",
    "end": "435400"
  },
  {
    "text": "tracking uh which actually stops our spot cluster scale down so the scenario",
    "start": "435400",
    "end": "442520"
  },
  {
    "text": "here is Da setting uh we can uh SC uh scale up and down from one to 100",
    "start": "442520",
    "end": "450720"
  },
  {
    "text": "executor part and the executor idle timeout threshold by default is 60",
    "start": "450720",
    "end": "459000"
  },
  {
    "text": "seconds I will go more details into that what's that mean okay so let's take a",
    "start": "459000",
    "end": "464720"
  },
  {
    "text": "look at the right hand side so when we have a pending tasks from the spark it",
    "start": "464720",
    "end": "470159"
  },
  {
    "text": "requires to SC scale up three notes just for the Simplicity okay scale up three",
    "start": "470159",
    "end": "476240"
  },
  {
    "text": "notes easy two notes right what got to happen so the pending task will run",
    "start": "476240",
    "end": "483440"
  },
  {
    "text": "three executors uh one executor per E2 Noe just for the",
    "start": "483440",
    "end": "490759"
  },
  {
    "text": "simplicity so what happen when the pending task is finish so no more",
    "start": "490759",
    "end": "496280"
  },
  {
    "text": "pending task from our workload right and we have a two executors are Idol over 60",
    "start": "496280",
    "end": "505240"
  },
  {
    "text": "seconds what got to happen so those executors supposed to be removed right",
    "start": "505240",
    "end": "510440"
  },
  {
    "text": "and released because they the idle being being idle for a long time it's a time",
    "start": "510440",
    "end": "515680"
  },
  {
    "text": "to to uh release the resources but it's not that simple spark",
    "start": "515680",
    "end": "523800"
  },
  {
    "text": "will check has it some Shuffle data on the executors right if the answer is yes in",
    "start": "523800",
    "end": "532839"
  },
  {
    "text": "the note two there's one executor being idle more than 60 seconds but it",
    "start": "532839",
    "end": "538480"
  },
  {
    "text": "contains a sh Shuffle data what going to happen we got to stay keep the note",
    "start": "538480",
    "end": "543800"
  },
  {
    "text": "to right if there's a no Shuffle data we got to terminate those",
    "start": "543800",
    "end": "550079"
  },
  {
    "text": "executors and we can terminate the not three great we save some money on",
    "start": "550079",
    "end": "555240"
  },
  {
    "text": "running one Noe two easy to instance however the problem is the Noe",
    "start": "555240",
    "end": "560360"
  },
  {
    "text": "two we cannot uh scale",
    "start": "560360",
    "end": "564800"
  },
  {
    "text": "down so that is a DA with uh uh tracking right Shuffle tracking what's the second",
    "start": "568120",
    "end": "575640"
  },
  {
    "text": "challenge so we tried to make the shuffle tracking time out faster right",
    "start": "575640",
    "end": "581800"
  },
  {
    "text": "maybe you remember the default setting is 60 seconds what if we shorten the",
    "start": "581800",
    "end": "588240"
  },
  {
    "text": "shaffle tracking time from 60 seconds to 5 seconds so that configuration setting",
    "start": "588240",
    "end": "595839"
  },
  {
    "text": "spark Dynamic allocation Shuffle tracking timeout equals 5 5",
    "start": "595839",
    "end": "601360"
  },
  {
    "text": "Seconds this what happen fantastic we don't really waste any resources here",
    "start": "601600",
    "end": "608320"
  },
  {
    "text": "because we can up and down very close to the actual",
    "start": "608320",
    "end": "613800"
  },
  {
    "text": "usage what's the problem here this is the problem from my testing",
    "start": "613800",
    "end": "621000"
  },
  {
    "text": "so uh when you look at that red Long Bar it says the stage",
    "start": "621000",
    "end": "627640"
  },
  {
    "text": "failed the re reason is on the bottom here it says fetch failed means it",
    "start": "627640",
    "end": "635000"
  },
  {
    "text": "couldn't fetch the shuffle data why is that right because we time out the",
    "start": "635000",
    "end": "643680"
  },
  {
    "text": "executor too fast every five seconds right we kill try to kill the",
    "start": "643680",
    "end": "650279"
  },
  {
    "text": "executor so it can't find the uh Shuffle data because it's being removed so we",
    "start": "652320",
    "end": "658279"
  },
  {
    "text": "lost the shuffle for data during this frequent timeout operation and the stage of the spark",
    "start": "658279",
    "end": "666800"
  },
  {
    "text": "processing failed so it will trigger the recomputation and your job will run uh",
    "start": "666800",
    "end": "673160"
  },
  {
    "text": "slower right so the third challenge is when we",
    "start": "673160",
    "end": "678760"
  },
  {
    "text": "do the shuffle migration what's that mean so uh since spark 3.0 uh we also",
    "start": "678760",
    "end": "686320"
  },
  {
    "text": "provide some graceful decommission feature is called uh yeah with those are",
    "start": "686320",
    "end": "693880"
  },
  {
    "text": "three key configurations so when you need to do the graceful decommission you",
    "start": "693880",
    "end": "699440"
  },
  {
    "text": "need to turn those on right what",
    "start": "699440",
    "end": "704639"
  },
  {
    "text": "happened so your migration triggers extra cost such as storage compute",
    "start": "704639",
    "end": "711519"
  },
  {
    "text": "Network um because before you turn down or scale down one of the ec2 instance",
    "start": "711519",
    "end": "717560"
  },
  {
    "text": "you have to move those Shuffle data from the uh ec2 to be taken away to another",
    "start": "717560",
    "end": "725560"
  },
  {
    "text": "healthy ec2 instance or compute Noe right that takes time when you copy so",
    "start": "725560",
    "end": "732320"
  },
  {
    "text": "that triggers extra cost the second one is uh imagine in the spot instance",
    "start": "732320",
    "end": "740480"
  },
  {
    "text": "scenario we have a two minutes notification so before uh we take in the",
    "start": "740480",
    "end": "747160"
  },
  {
    "text": "interrupt the the spot instances we notify our customers in 2 minutes",
    "start": "747160",
    "end": "752480"
  },
  {
    "text": "Advance right we say we need this compute instance so it's time for you to",
    "start": "752480",
    "end": "758040"
  },
  {
    "text": "move all the data out of this uh EC tune right so the problem is the migration",
    "start": "758040",
    "end": "764880"
  },
  {
    "text": "doesn't know the two minutes threshold so if you are facing large amount of a",
    "start": "764880",
    "end": "770000"
  },
  {
    "text": "shaffle data move from A to B within that 2 minutes time frame if you didn't",
    "start": "770000",
    "end": "776800"
  },
  {
    "text": "finish copy the data what going to happen and you lose partial data so the recomputation still",
    "start": "776800",
    "end": "785279"
  },
  {
    "text": "happens the third um bullet point here is actually a quite extreme scenario but",
    "start": "785279",
    "end": "792279"
  },
  {
    "text": "we do have customers see this uh problem so sometimes when you move the shuffle",
    "start": "792279",
    "end": "797920"
  },
  {
    "text": "data from uh node a to node B then the",
    "start": "797920",
    "end": "803000"
  },
  {
    "text": "note B is about to interrupt it as well unfortunately so you have to continue ly",
    "start": "803000",
    "end": "809639"
  },
  {
    "text": "move from note B to note C from note C to note D so your decommission process",
    "start": "809639",
    "end": "815720"
  },
  {
    "text": "actually is dramatically",
    "start": "815720",
    "end": "819519"
  },
  {
    "text": "delayed okay so to solve those challenges we actually found a solution",
    "start": "821040",
    "end": "828639"
  },
  {
    "text": "I'm pretty sure in the open source Community there's lots of solutions right so recently we actually did",
    "start": "828639",
    "end": "834639"
  },
  {
    "text": "testing against the Apache calium project and we found it is quite",
    "start": "834639",
    "end": "840399"
  },
  {
    "text": "compelling in terms of the result um so inside the uh AWS this is the",
    "start": "840399",
    "end": "847040"
  },
  {
    "text": "architecture we designed and how to host this uh remote Shuffle service there are",
    "start": "847040",
    "end": "852399"
  },
  {
    "text": "two options here right the first one on the left hand side is uh uh hosting the",
    "start": "852399",
    "end": "858959"
  },
  {
    "text": "calone cluster outside of your Emi environment so it's completely",
    "start": "858959",
    "end": "865160"
  },
  {
    "text": "Standalone it can serve to any other workloads not only just the big data or",
    "start": "865160",
    "end": "871120"
  },
  {
    "text": "spark uh workloads on the right hand side it's",
    "start": "871120",
    "end": "876320"
  },
  {
    "text": "quite interesting we are hosting the calone clust cluster inside the Yama cluster so",
    "start": "876320",
    "end": "884079"
  },
  {
    "text": "the key difference between these two is the hdfs a storage that's the only",
    "start": "884079",
    "end": "889600"
  },
  {
    "text": "that's the only difference so when we host the calone cluster inside Yama they",
    "start": "889600",
    "end": "896040"
  },
  {
    "text": "actually share the storage with a spark uh executors",
    "start": "896040",
    "end": "902480"
  },
  {
    "text": "right so in in my benchmarking result actually I used the left hand side one",
    "start": "902480",
    "end": "907959"
  },
  {
    "text": "so I host my calone cluster uh along inside the kubernetes environment and my",
    "start": "907959",
    "end": "915480"
  },
  {
    "text": "yma can be run in any environment could be in um uh yma subis or yma on",
    "start": "915480",
    "end": "922320"
  },
  {
    "text": "kubernetes environment anywhere as long as they can talk to each other hey so",
    "start": "922320",
    "end": "927920"
  },
  {
    "text": "this is my um set up so I turn on my spark on kubernetes Dr Dynamic allocation with",
    "start": "927920",
    "end": "936680"
  },
  {
    "text": "aach caliber which means I send all of my Shuffle data out out of my spark",
    "start": "936680",
    "end": "943240"
  },
  {
    "text": "cluster across the network to extend along Apache spark uh sorry Apache",
    "start": "943240",
    "end": "950199"
  },
  {
    "text": "caliber so we can see even though it's not that curve lines in red very close",
    "start": "950199",
    "end": "957440"
  },
  {
    "text": "to the actual usage right it's the dashed line still very close to the",
    "start": "957440",
    "end": "962480"
  },
  {
    "text": "actual uh resource usage so this is the great result we can achieve using Apache",
    "start": "962480",
    "end": "970199"
  },
  {
    "text": "Calon so can anyone guess I did a side by side test one has a calone one",
    "start": "970199",
    "end": "977199"
  },
  {
    "text": "doesn't have a calone which one has da with calone can",
    "start": "977199",
    "end": "982880"
  },
  {
    "text": "anyone give me the answer no",
    "start": "982880",
    "end": "990519"
  },
  {
    "text": "no okay I will give you the answer so on the left hand side is the usual Shuffle",
    "start": "990800",
    "end": "996959"
  },
  {
    "text": "tracking with a slow executor release and on the right hand side we have a",
    "start": "996959",
    "end": "1003120"
  },
  {
    "text": "more responsive up and down release the executors right sorry the each blue boxes represent a spark executor",
    "start": "1003120",
    "end": "1011519"
  },
  {
    "text": "sorry yeah so you can see those highlighted two to three minutes uh time",
    "start": "1011519",
    "end": "1017560"
  },
  {
    "text": "period actually actually has some red boxes that represent the executor being",
    "start": "1017560",
    "end": "1023199"
  },
  {
    "text": "uh killed by the driver because it's being idle but this one because it's been",
    "start": "1023199",
    "end": "1028798"
  },
  {
    "text": "tracking it never shut down the executor even though executor is uh",
    "start": "1028799",
    "end": "1035880"
  },
  {
    "text": "Idol okay this is our favorite part what's the result to our customers how",
    "start": "1038439",
    "end": "1046079"
  },
  {
    "text": "much do they need to spend when when they enable the calone with a",
    "start": "1046079",
    "end": "1051160"
  },
  {
    "text": "Dr so I have two tables uh the top one is using our AWS Apache yr uh sorry",
    "start": "1051160",
    "end": "1059000"
  },
  {
    "text": "Amazon yr with spark the bottom one is the open source spark running on",
    "start": "1059000",
    "end": "1065360"
  },
  {
    "text": "kubernetes so we can see uh when we run the jobs in parallel that's a DA with",
    "start": "1065360",
    "end": "1072320"
  },
  {
    "text": "tracking the first one uh cost around $232 uh US dollars and the second run is",
    "start": "1072320",
    "end": "1081360"
  },
  {
    "text": "yemma with a caliber without tracking without Shuffle tracking right it only",
    "start": "1081360",
    "end": "1087480"
  },
  {
    "text": "cost $143 why because we can release the ECU nodes more responsively and quicker so",
    "start": "1087480",
    "end": "1095880"
  },
  {
    "text": "it actually provide uh up to 38% of cost efficiency when we are",
    "start": "1095880",
    "end": "1101919"
  },
  {
    "text": "enable uh when we enable the Dr with RSS Calon so on the bottom is is the open",
    "start": "1101919",
    "end": "1109120"
  },
  {
    "text": "source Spar on kues test uh yeah the Baseline is already over $5 anyway yeah",
    "start": "1109120",
    "end": "1115960"
  },
  {
    "text": "but you still can see when we enable the calone without Shuffle tracking it's",
    "start": "1115960",
    "end": "1121000"
  },
  {
    "text": "also around 30% of Cheaper uh cost so on the right hand side that is",
    "start": "1121000",
    "end": "1128880"
  },
  {
    "text": "the setting we use and we can see that executor idle",
    "start": "1128880",
    "end": "1134880"
  },
  {
    "text": "timeout is around 10 10 seconds so that is the sweep uh spot we used we don't",
    "start": "1134880",
    "end": "1142120"
  },
  {
    "text": "really Define the really quick timeout but it's at certain degree we time",
    "start": "1142120",
    "end": "1149080"
  },
  {
    "text": "out all right so that's all from me from the user perspective to use Apache",
    "start": "1151200",
    "end": "1156679"
  },
  {
    "text": "caliber so I'm going to um invite my co-presenter Joong um to Deep dive into",
    "start": "1156679",
    "end": "1163919"
  },
  {
    "text": "spark features with Apache calal",
    "start": "1163919",
    "end": "1170240"
  },
  {
    "text": "okay okay [Music]",
    "start": "1195960",
    "end": "1203829"
  },
  {
    "text": "external sh",
    "start": "1208039",
    "end": "1210799"
  },
  {
    "text": "service",
    "start": "1227159",
    "end": "1230159"
  },
  {
    "text": "Shuffle F time we",
    "start": "1244760",
    "end": "1248799"
  },
  {
    "text": "time fet failure out of",
    "start": "1251400",
    "end": "1256200"
  },
  {
    "text": "memory",
    "start": "1257120",
    "end": "1260120"
  },
  {
    "text": "task",
    "start": "1274080",
    "end": "1277080"
  },
  {
    "text": "taser",
    "start": "1287039",
    "end": "1290039"
  },
  {
    "text": "Tas",
    "start": "1317039",
    "end": "1320039"
  },
  {
    "text": "okay",
    "start": "1341200",
    "end": "1344200"
  },
  {
    "text": "redas",
    "start": "1346960",
    "end": "1349960"
  },
  {
    "text": "okay okay SP",
    "start": "1374120",
    "end": "1379960"
  },
  {
    "text": "EX",
    "start": "1406880",
    "end": "1409880"
  },
  {
    "text": "manager external sh",
    "start": "1421919",
    "end": "1425679"
  },
  {
    "text": "service",
    "start": "1427520",
    "end": "1430520"
  },
  {
    "text": "EX",
    "start": "1436880",
    "end": "1439880"
  },
  {
    "text": "EXE",
    "start": "1466799",
    "end": "1469799"
  },
  {
    "text": "external sh external sh",
    "start": "1478840",
    "end": "1482840"
  },
  {
    "text": "service shle checking",
    "start": "1490919",
    "end": "1496760"
  },
  {
    "text": "decommission",
    "start": "1496760",
    "end": "1499760"
  },
  {
    "text": "R",
    "start": "1502120",
    "end": "1505120"
  },
  {
    "text": "efficiency external travel",
    "start": "1513240",
    "end": "1516960"
  },
  {
    "text": "services",
    "start": "1526679",
    "end": "1529679"
  },
  {
    "text": "I",
    "start": "1556679",
    "end": "1559679"
  },
  {
    "text": "yes spark release ID",
    "start": "1580799",
    "end": "1586600"
  },
  {
    "text": "EXE",
    "start": "1586600",
    "end": "1589600"
  },
  {
    "text": "okay uh",
    "start": "1616399",
    "end": "1619600"
  },
  {
    "text": "Master worker",
    "start": "1622360",
    "end": "1629760"
  },
  {
    "text": "Spar dver life cycle manager",
    "start": "1634799",
    "end": "1642000"
  },
  {
    "text": "application uh",
    "start": "1646360",
    "end": "1649520"
  },
  {
    "text": "High",
    "start": "1663399",
    "end": "1665720"
  },
  {
    "text": "availability",
    "start": "1671080",
    "end": "1674080"
  },
  {
    "text": "okay",
    "start": "1676519",
    "end": "1679519"
  },
  {
    "text": "worker line uh control message",
    "start": "1695919",
    "end": "1705000"
  },
  {
    "text": "um anyway",
    "start": "1705399",
    "end": "1709440"
  },
  {
    "text": "masterer work",
    "start": "1714880",
    "end": "1719799"
  },
  {
    "text": "okay life cycle manager register shle life cycle",
    "start": "1736399",
    "end": "1746120"
  },
  {
    "text": "manager manager manager",
    "start": "1763039",
    "end": "1769320"
  },
  {
    "text": "task cycle manager worker commit",
    "start": "1786120",
    "end": "1795200"
  },
  {
    "text": "f",
    "start": "1796320",
    "end": "1799320"
  },
  {
    "text": "Ras server",
    "start": "1807559",
    "end": "1814240"
  },
  {
    "text": "okay okay external Travel Service",
    "start": "1821320",
    "end": "1829240"
  },
  {
    "text": "P",
    "start": "1837159",
    "end": "1840159"
  },
  {
    "text": "okay p",
    "start": "1842519",
    "end": "1846679"
  },
  {
    "text": "style",
    "start": "1856240",
    "end": "1859240"
  },
  {
    "text": "worker read",
    "start": "1885559",
    "end": "1889158"
  },
  {
    "text": "reducer one",
    "start": "1904559",
    "end": "1907919"
  },
  {
    "text": "one",
    "start": "1916159",
    "end": "1919159"
  },
  {
    "text": "for",
    "start": "1931320",
    "end": "1934320"
  },
  {
    "text": "cluster okay",
    "start": "1941480",
    "end": "1949080"
  },
  {
    "text": "worker split",
    "start": "1967399",
    "end": "1974159"
  },
  {
    "text": "size",
    "start": "1976039",
    "end": "1979039"
  },
  {
    "text": "cycle manager",
    "start": "1989240",
    "end": "1992600"
  },
  {
    "text": "reducer",
    "start": "1995919",
    "end": "1998919"
  },
  {
    "text": "okay evalu",
    "start": "2005559",
    "end": "2008960"
  },
  {
    "text": "SI",
    "start": "2035960",
    "end": "2038960"
  },
  {
    "text": "right",
    "start": "2065879",
    "end": "2068878"
  },
  {
    "text": "okay",
    "start": "2089240",
    "end": "2092240"
  },
  {
    "text": "Spark",
    "start": "2095879",
    "end": "2098879"
  },
  {
    "text": "right failed the",
    "start": "2119960",
    "end": "2123480"
  },
  {
    "text": "task",
    "start": "2125800",
    "end": "2128800"
  },
  {
    "text": "okays okay",
    "start": "2155800",
    "end": "2161400"
  },
  {
    "text": "now",
    "start": "2185720",
    "end": "2188720"
  },
  {
    "text": "like",
    "start": "2215680",
    "end": "2218680"
  },
  {
    "text": "[Music]",
    "start": "2237350",
    "end": "2240510"
  },
  {
    "text": "okay",
    "start": "2245599",
    "end": "2248599"
  },
  {
    "text": "um okay uh is standing questions I think is a q&r time yeah of course yes go",
    "start": "2250839",
    "end": "2259240"
  },
  {
    "text": "ahead thank you so much sorry it's gon to be in English I apologize um when it",
    "start": "2259319",
    "end": "2264359"
  },
  {
    "text": "comes to your node sizes when you're actually running the executors um if the",
    "start": "2264359",
    "end": "2270640"
  },
  {
    "text": "shuffle files can fit on the executors is it sometimes dis like not",
    "start": "2270640",
    "end": "2276240"
  },
  {
    "text": "ADV ages to do a backup So like um you write to the executors and then at the",
    "start": "2276240",
    "end": "2282680"
  },
  {
    "text": "same time send out to um celor like",
    "start": "2282680",
    "end": "2288040"
  },
  {
    "text": "because if you wait on celor every time that Network time might be longer than just writing to the dis on the executor",
    "start": "2288040",
    "end": "2294480"
  },
  {
    "text": "itself so you're doing a um like a backup at the same time allows for you",
    "start": "2294480",
    "end": "2300359"
  },
  {
    "text": "to actually continue with the task and not block on writing sorry yeah no no wondering if",
    "start": "2300359",
    "end": "2307040"
  },
  {
    "text": "that's like a use case cuz I think the exact example was you directly write to S yeah we directly send all the shuffle",
    "start": "2307040",
    "end": "2312839"
  },
  {
    "text": "data out of your spark cluster to caliber over the network so uh are you",
    "start": "2312839",
    "end": "2319720"
  },
  {
    "text": "asking do we have an enaged uh mechanism before sending over to the calone is",
    "start": "2319720",
    "end": "2327040"
  },
  {
    "text": "that your question yeah like if the data is small and it could fit on the node right you can just write it locally to",
    "start": "2327040",
    "end": "2332839"
  },
  {
    "text": "the executor and then okay um if I on understand correctly your question is",
    "start": "2332839",
    "end": "2338839"
  },
  {
    "text": "that can we use Calon as a backup so sometimes for example for small Shuffle",
    "start": "2338839",
    "end": "2344880"
  },
  {
    "text": "jobs we can use the traditional or typical ex uh Shuffle and",
    "start": "2344880",
    "end": "2353079"
  },
  {
    "text": "for a very big Shuffle we can use kabum and the answer is yes because um",
    "start": "2353079",
    "end": "2360200"
  },
  {
    "text": "currently colon Shuffle has a plug-in mechanism so that",
    "start": "2360200",
    "end": "2366079"
  },
  {
    "text": "you can customize uh if one Shuffle can",
    "start": "2366079",
    "end": "2371280"
  },
  {
    "text": "go you know local Shuffle and another Shuffle can go to Calum based on",
    "start": "2371280",
    "end": "2377640"
  },
  {
    "text": "multiple policies for example the shuffle size or for example the uh",
    "start": "2377640",
    "end": "2382839"
  },
  {
    "text": "status of the Calon cluster and all that you'll have a configuration that you",
    "start": "2382839",
    "end": "2388680"
  },
  {
    "text": "force force the shuffle to use the traditional shaffle mechanism and if you",
    "start": "2388680",
    "end": "2394040"
  },
  {
    "text": "have heterogeneous Hardware is it going to be aware of the actual node size too of how much",
    "start": "2394040",
    "end": "2399560"
  },
  {
    "text": "ephemeral storage is on the Pod does it know that or is the setting just like if",
    "start": "2399560",
    "end": "2404800"
  },
  {
    "text": "Shuffle larger than 2 gigabytes than local um are you referring to The Spar",
    "start": "2404800",
    "end": "2411480"
  },
  {
    "text": "Port execut prod or calbor pod so if the shuffle is bigger than 2 gigabytes you",
    "start": "2411480",
    "end": "2417440"
  },
  {
    "text": "send to calbor but if your pod has Emeral storage of 300 gigabytes you're",
    "start": "2417440",
    "end": "2422680"
  },
  {
    "text": "like oh I can fit 300 gigabytes right so is it uh okay uh yeah uh uh for now um colon",
    "start": "2422680",
    "end": "2432280"
  },
  {
    "text": "does not support you know dynamic switch between the local Shuffle and cabon that",
    "start": "2432280",
    "end": "2439720"
  },
  {
    "text": "is to say for example if if you decided that uh one shle ID uses colon then you",
    "start": "2439720",
    "end": "2448480"
  },
  {
    "text": "can you can um switch uh some tasks of",
    "start": "2448480",
    "end": "2454000"
  },
  {
    "text": "the stage to use Calon and or and other tasks to use local disk it doesn't",
    "start": "2454000",
    "end": "2460680"
  },
  {
    "text": "support this it only supports uh you know uh um either the the whole Shuffle to",
    "start": "2460680",
    "end": "2470319"
  },
  {
    "text": "for for one Shuffle ID either the sh all tasks in the shuffle stage uh uses Calon",
    "start": "2470319",
    "end": "2476880"
  },
  {
    "text": "all all tasks of the shuffle uses the local dis it can switch between uh you",
    "start": "2476880",
    "end": "2483520"
  },
  {
    "text": "know traditional Shuffle and Calon Within one Shuffle and um",
    "start": "2483520",
    "end": "2490480"
  },
  {
    "text": "yeah if I understand correctly you may refer to that you can based on the size",
    "start": "2490480",
    "end": "2496520"
  },
  {
    "text": "of the port storage to decide whether we need we should go to Clon or to uh yeah",
    "start": "2496520",
    "end": "2505960"
  },
  {
    "text": "and uh my opinion is that um it can be a",
    "start": "2505960",
    "end": "2512040"
  },
  {
    "text": "it is an it it can be a policy to to you",
    "start": "2512040",
    "end": "2517079"
  },
  {
    "text": "know um uh to can be a policy you can Implement such a policy so that it can",
    "start": "2517079",
    "end": "2525000"
  },
  {
    "text": "it can do this uh but uh I'm not sure whether it's a you know generally good",
    "start": "2525000",
    "end": "2532359"
  },
  {
    "text": "policy uh because uh for example if the task uh the number of tasks is really",
    "start": "2532359",
    "end": "2540000"
  },
  {
    "text": "large for example more than 10,000s even though your ex Port has enough",
    "start": "2540000",
    "end": "2546960"
  },
  {
    "text": "uh disk storage to hold the shaffle data it may be not efficient because of the",
    "start": "2546960",
    "end": "2554760"
  },
  {
    "text": "random AIO yeah so I think it depends yeah yeah so I just uh add a little bit",
    "start": "2554760",
    "end": "2562400"
  },
  {
    "text": "to that explanation uh Apache Calon is not a solution fit for all right so we",
    "start": "2562400",
    "end": "2569760"
  },
  {
    "text": "need to choose make decision what kind of use case do you have if you if your entire spark job only contains gigabytes",
    "start": "2569760",
    "end": "2578240"
  },
  {
    "text": "of a shuffle data at each stage right you don't need the remote Shuffle service you can just use the default",
    "start": "2578240",
    "end": "2585079"
  },
  {
    "text": "shffle tracking that's fine this is for a extremely large scale Workshop uh",
    "start": "2585079",
    "end": "2591880"
  },
  {
    "text": "workloads that that you will benefit the most right so we have that example we",
    "start": "2591880",
    "end": "2597599"
  },
  {
    "text": "have a customer have um uh hundreds terabytes of data shuffling in one",
    "start": "2597599",
    "end": "2603440"
  },
  {
    "text": "single stage that's when you need to enable the the Apache calone remote",
    "start": "2603440",
    "end": "2609359"
  },
  {
    "text": "Shuffle service for that particular job right so usually if your spark job is",
    "start": "2609359",
    "end": "2614800"
  },
  {
    "text": "okay just don't need to have a large scale of a shuffling you don't need to turn on Apaches Cal a cal no no the",
    "start": "2614800",
    "end": "2623160"
  },
  {
    "text": "reason I was asking is because you can have both right so if you allow for a backup to happen at the same time",
    "start": "2623160",
    "end": "2629480"
  },
  {
    "text": "concurrently like like a concurrent backup when you do your fetches right your Shuffle fetches you can either read",
    "start": "2629480",
    "end": "2635240"
  },
  {
    "text": "from the executor or you can read from the curn cluster the RSS yeah registry",
    "start": "2635240",
    "end": "2640680"
  },
  {
    "text": "right and so based on that if the executor goes down you can always read but you have both in terms of speed if",
    "start": "2640680",
    "end": "2647599"
  },
  {
    "text": "you don't want to have the right so so you long so you mean that uh for example",
    "start": "2647599",
    "end": "2653800"
  },
  {
    "text": "in the shuffle right uh pH uh the map Tasker will write the shuffle data both",
    "start": "2653800",
    "end": "2660760"
  },
  {
    "text": "locally and push to the C yeah this is how uh uh apach spark does uh yeah and",
    "start": "2660760",
    "end": "2670079"
  },
  {
    "text": "and I I know that that method is contributed by um people from linking",
    "start": "2670079",
    "end": "2678680"
  },
  {
    "text": "and the project is called magnet and yeah yeah okay yeah I uh I think it's uh",
    "start": "2678680",
    "end": "2688200"
  },
  {
    "text": "it's a good solution uh for uh if the yeah um if the external okay okay with",
    "start": "2688200",
    "end": "2697960"
  },
  {
    "text": "with external sh service I think it's a good solution and for k i I also think",
    "start": "2697960",
    "end": "2703800"
  },
  {
    "text": "it it can do this yeah I think it's can do we can have a try",
    "start": "2703800",
    "end": "2709040"
  },
  {
    "text": "yeah maybe yeah maybe you can contribute to the community to bring in this extra",
    "start": "2709040",
    "end": "2714640"
  },
  {
    "text": "fantastic feature as well to speed up yeah speed up the entire performance",
    "start": "2714640",
    "end": "2719920"
  },
  {
    "text": "hdfs rights are very slow so sometimes it's like yeah that's right yeah thank",
    "start": "2719920",
    "end": "2726119"
  },
  {
    "text": "you for your questions thank you and is there any other questions in the in the",
    "start": "2726119",
    "end": "2731559"
  },
  {
    "text": "audience yeah sure okay yeah thanks for your great talk I",
    "start": "2731559",
    "end": "2739839"
  },
  {
    "text": "have two questions uh first is uh about the cost down numbers does it also",
    "start": "2739839",
    "end": "2746559"
  },
  {
    "text": "include the cost on the calone cluster uh which cost is it this one uh the",
    "start": "2746559",
    "end": "2754599"
  },
  {
    "text": "bench benchmarking in in your uh",
    "start": "2754599",
    "end": "2760040"
  },
  {
    "text": "talk the this one uh yes yeah so uh",
    "start": "2760040",
    "end": "2766119"
  },
  {
    "text": "which cost are you ref up to the 38% cheaper when using yeah so the",
    "start": "2766119",
    "end": "2772839"
  },
  {
    "text": "38% cheaper is comparing purely comparing this result so this cost and",
    "start": "2772839",
    "end": "2778760"
  },
  {
    "text": "this cost contains everything so including yemma uplift premium price",
    "start": "2778760",
    "end": "2786079"
  },
  {
    "text": "plus eks compute ec2 instances Price Plus",
    "start": "2786079",
    "end": "2791800"
  },
  {
    "text": "Storage so plus the calone storage cost uh very good question actually it didn't",
    "start": "2791800",
    "end": "2797640"
  },
  {
    "text": "include calone because all of these have a calone cost behind the scene right so",
    "start": "2797640",
    "end": "2802880"
  },
  {
    "text": "I assume the cost should be highly similar uh but that's a very good point",
    "start": "2802880",
    "end": "2808240"
  },
  {
    "text": "we should add the caliber cost in it but I'm very confident that shouldn't make a big difference the main big difference",
    "start": "2808240",
    "end": "2815400"
  },
  {
    "text": "is the Dynamic allocation resource up and down right when you release a ec2",
    "start": "2815400",
    "end": "2820880"
  },
  {
    "text": "note that will save you lots of money yeah okay and my second question is uh I",
    "start": "2820880",
    "end": "2828119"
  },
  {
    "text": "I see in your slides that calone runs on directly on easy2 notes doeses it",
    "start": "2828119",
    "end": "2833400"
  },
  {
    "text": "support uh to run on kues cluster uh actually it is running on the kubernetes",
    "start": "2833400",
    "end": "2840680"
  },
  {
    "text": "environment uh probably this uh yeah this diagram wasn't uh that clear",
    "start": "2840680",
    "end": "2846480"
  },
  {
    "text": "uh actually this is uh in inside the kubernetes sorry this should be eks this",
    "start": "2846480",
    "end": "2851559"
  },
  {
    "text": "is in the yma either on E or sist also uh also does cibone support",
    "start": "2851559",
    "end": "2858400"
  },
  {
    "text": "like Auto scale for the workers correct yeah okay uh I will complement some uh",
    "start": "2858400",
    "end": "2865680"
  },
  {
    "text": "okay for the first question we have some users that exactly uses the uh spark on",
    "start": "2865680",
    "end": "2873280"
  },
  {
    "text": "communties with cabon class and the and in there that told me that",
    "start": "2873280",
    "end": "2880040"
  },
  {
    "text": "the overall cost has decreased remarkably because of the uh better",
    "start": "2880040",
    "end": "2887599"
  },
  {
    "text": "elas elasticity yeah and the the C cluster",
    "start": "2887599",
    "end": "2894559"
  },
  {
    "text": "usually do not requires much resource it it it do requires you can think of that",
    "start": "2894559",
    "end": "2903000"
  },
  {
    "text": "like um for traditional shuffle your your computer node your Port has storage",
    "start": "2903000",
    "end": "2910960"
  },
  {
    "text": "and uh uh and uh network resources for",
    "start": "2910960",
    "end": "2916200"
  },
  {
    "text": "the shuffle and with Calon you just upload the storage to the cabon cluster",
    "start": "2916200",
    "end": "2922680"
  },
  {
    "text": "and the network resource so uh it it it",
    "start": "2922680",
    "end": "2928319"
  },
  {
    "text": "does not uh add uh much more resources",
    "start": "2928319",
    "end": "2934200"
  },
  {
    "text": "and as the sometime at at the same time because it is more uh efficient and more",
    "start": "2934200",
    "end": "2941839"
  },
  {
    "text": "IO friendly it it indeed uh reduces the",
    "start": "2941839",
    "end": "2947160"
  },
  {
    "text": "resources for storing and network that's for the first question and for the",
    "start": "2947160",
    "end": "2952680"
  },
  {
    "text": "second question yes Calum supports um to be deployed on the communties and uh uh",
    "start": "2952680",
    "end": "2961400"
  },
  {
    "text": "it supports Dynamic elasticity",
    "start": "2961400",
    "end": "2966680"
  },
  {
    "text": "uh even though it is not very complete for now but it it supports you can",
    "start": "2966680",
    "end": "2973760"
  },
  {
    "text": "decommission a c worker and it will you know release the",
    "start": "2973760",
    "end": "2980280"
  },
  {
    "text": "it will release U when it does it finishes",
    "start": "2980280",
    "end": "2985760"
  },
  {
    "text": "serving the local Shuffle data yeah okay",
    "start": "2985760",
    "end": "2990839"
  },
  {
    "text": "thank you very much I I like today's talk thanks thank you",
    "start": "2990839",
    "end": "2997520"
  },
  {
    "text": "um any more questions I understand it's already lunch time I really appreciate everyone has the passion here stay with",
    "start": "3000319",
    "end": "3006920"
  },
  {
    "text": "us have all the questions yeah please go ahead so yeah thanks um I have a very",
    "start": "3006920",
    "end": "3011960"
  },
  {
    "text": "quick question I know it's probably a little bit off topic um because my company use both uh kubernetes as well",
    "start": "3011960",
    "end": "3019319"
  },
  {
    "text": "as data breaks to run spark I just wonder if there's any chance from",
    "start": "3019319",
    "end": "3024440"
  },
  {
    "text": "Upstream perspective uh the cbone can be enabled uh from the",
    "start": "3024440",
    "end": "3030440"
  },
  {
    "text": "S provider such as data breaks because we like the Simplicity of having s rather than running everything from",
    "start": "3030440",
    "end": "3036720"
  },
  {
    "text": "scratch if it's available um may I ask a little bit question so where is your data brakes",
    "start": "3036720",
    "end": "3043799"
  },
  {
    "text": "hosted so our data Brak hosted on AAR in AAR okay",
    "start": "3043799",
    "end": "3050440"
  },
  {
    "text": "uh so that means uh so data breaks uh at",
    "start": "3050440",
    "end": "3056079"
  },
  {
    "text": "this stage datab brakes doesn't support containerization yeah we know that right so your data brakes clusters has to run",
    "start": "3056079",
    "end": "3064359"
  },
  {
    "text": "on the E2 equivalent compute Noe so if you want to use calone selfhosted you",
    "start": "3064359",
    "end": "3072920"
  },
  {
    "text": "could run the cluster of calone uh either on the compute node similar to",
    "start": "3072920",
    "end": "3079040"
  },
  {
    "text": "your data brakes compute node the El laity like up and down uh scale probably",
    "start": "3079040",
    "end": "3085559"
  },
  {
    "text": "is not that good because it's fixed size of E2 instances uh however if you run",
    "start": "3085559",
    "end": "3091640"
  },
  {
    "text": "Calon cluster in the cetes environment such as uh I don't know in aour is it",
    "start": "3091640",
    "end": "3097440"
  },
  {
    "text": "called AKs AKs yes so if you host that in AKs uh it should enable you to use",
    "start": "3097440",
    "end": "3105520"
  },
  {
    "text": "calone but the downside is you need to enable the network right so your data",
    "start": "3105520",
    "end": "3112599"
  },
  {
    "text": "brakes on on the compute needs to be able will talk to your kues in",
    "start": "3112599",
    "end": "3117720"
  },
  {
    "text": "AKs because you imagine you will have large amount of data sets sending over",
    "start": "3117720",
    "end": "3123200"
  },
  {
    "text": "from dat brakes cluster into kubernetes environment and also lots of a read",
    "start": "3123200",
    "end": "3128319"
  },
  {
    "text": "happening so the performance could be the bottleneck so you have to F fully",
    "start": "3128319",
    "end": "3133640"
  },
  {
    "text": "test it is it worth it or is not okay yeah all right thank you okay I have",
    "start": "3133640",
    "end": "3139760"
  },
  {
    "text": "some compliment uh uh if you I don't know if you ask whether we need to",
    "start": "3139760",
    "end": "3146559"
  },
  {
    "text": "modify spark to SP it's not and I think it's all the schedule things of data",
    "start": "3146559",
    "end": "3154680"
  },
  {
    "text": "breaks is probably managed by data breaks itself so yeah had son of I kind",
    "start": "3154680",
    "end": "3160119"
  },
  {
    "text": "of had to answer myself that if it's not supported that's end of story unless you",
    "start": "3160119",
    "end": "3165319"
  },
  {
    "text": "wanted to build everything in Cub is or yourself right I just wonder from Upstream perspective if there's any",
    "start": "3165319",
    "end": "3172599"
  },
  {
    "text": "plans in the future that can be new because I'm interested reducing any potential cost that I can um you know to",
    "start": "3172599",
    "end": "3178960"
  },
  {
    "text": "bring it down yeah so actually uh I found AWS so actually we are introduced",
    "start": "3178960",
    "end": "3185400"
  },
  {
    "text": "this with Emma on eks so our y sorry our yr product actually do offer uh this",
    "start": "3185400",
    "end": "3193559"
  },
  {
    "text": "solution in our em ons feature yeah just let you know yeah okay thank",
    "start": "3193559",
    "end": "3200280"
  },
  {
    "text": "you more",
    "start": "3204160",
    "end": "3210559"
  },
  {
    "text": "questions",
    "start": "3215799",
    "end": "3218799"
  }
]