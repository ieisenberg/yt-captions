[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "hello everybody welcome in this is my talk called doing things Prometheus can't do with Prometheus my name is Tim",
    "start": "269",
    "end": "9599"
  },
  {
    "text": "Simmons I'm an engineer on the observability team at digitalocean my team primarily runs centralized",
    "start": "9599",
    "end": "15059"
  },
  {
    "text": "logging metrics and tracing infrastructure you can follow me on Twitter but I wouldn't recommend it",
    "start": "15059",
    "end": "20520"
  },
  {
    "text": "because that is roughly the content that you will be there for digitalocean is a cloud company that is focused on",
    "start": "20520",
    "end": "26849"
  },
  {
    "text": "simplicity we have everything you need to run the workloads that you want to run my primary job at do is caring for a",
    "start": "26849",
    "end": "34410"
  },
  {
    "text": "prometheus fleet that cares about metrics that kind of support the entire cloud first line in my CFP kind of this",
    "start": "34410",
    "end": "44700"
  },
  {
    "start": "40000",
    "end": "84000"
  },
  {
    "text": "a little was clickbait to try and get my talk accepted and entice you to come to",
    "start": "44700",
    "end": "52050"
  },
  {
    "text": "my talk nothing is like this cut-and-dried you know there's a lot of",
    "start": "52050",
    "end": "57930"
  },
  {
    "text": "nuance and saying something like that and like there might be like a cool tweet to do or whatever but like I you",
    "start": "57930",
    "end": "64198"
  },
  {
    "text": "know this is not that's nothing I could be easily convinced and I'm totally wrong here but I do kind of believe this",
    "start": "64199",
    "end": "70530"
  },
  {
    "text": "if you're willing to invest the time and energy in kind of understanding the tools and the outcomes that you're",
    "start": "70530",
    "end": "75659"
  },
  {
    "text": "looking for the old pillars that nobody is so fond of anymore I think they're",
    "start": "75659",
    "end": "81689"
  },
  {
    "text": "actually really really good and good enough for you the thing that is not good enough is us and this shouldn't",
    "start": "81689",
    "end": "89400"
  },
  {
    "start": "84000",
    "end": "152000"
  },
  {
    "text": "really come as a surprise to anybody right the problem with technology is always the people using it you know",
    "start": "89400",
    "end": "96060"
  },
  {
    "text": "people will say things like I don't have time to do good observability or you know I don't have time to learn these",
    "start": "96060",
    "end": "101460"
  },
  {
    "text": "complicated tools but these are the tools that are giving you insight into how the thing that like pays your bills",
    "start": "101460",
    "end": "108180"
  },
  {
    "text": "are performing so if there's anything that is you know complicated that is actually worth learning and",
    "start": "108180",
    "end": "114000"
  },
  {
    "text": "understanding isn't it being able to do this like understanding your customers experience I think if you spend a lot of",
    "start": "114000",
    "end": "121740"
  },
  {
    "text": "time making sure your customers are happy you know your existing customers I think the new ones will kind of figure themselves out I think the hardest",
    "start": "121740",
    "end": "129509"
  },
  {
    "text": "problem in the observability space is actually just convincing your boss or leader that it is worth doing and worth",
    "start": "129509",
    "end": "135780"
  },
  {
    "text": "doing well and the way that you can reliably serve customers and do this is",
    "start": "135780",
    "end": "141420"
  },
  {
    "text": "by intelligently instrumenting your software so you can see what's going on but to produce those best outcomes you",
    "start": "141420",
    "end": "147330"
  },
  {
    "text": "have to absolutely understand what is possible so that you can make the best inputs the goal here for us today is for",
    "start": "147330",
    "end": "155879"
  },
  {
    "start": "152000",
    "end": "199000"
  },
  {
    "text": "you all to start thinking about observability like I do at least for 30 minutes or so because I'm right and",
    "start": "155879",
    "end": "161580"
  },
  {
    "text": "hopefully that is something that is generally useful as you kind of leave here and go any lunch or whatever but I",
    "start": "161580",
    "end": "168569"
  },
  {
    "text": "think it will also kind of help you understand where I'm coming from when I start explaining how we've done some more advanced things with Prometheus and",
    "start": "168569",
    "end": "175290"
  },
  {
    "text": "that's probably the real reason that you're here anyway but while I was writing the talk and talking about those things I kind of realized there's a",
    "start": "175290",
    "end": "181410"
  },
  {
    "text": "common thread and it was more about how I'm approaching those problems and how how my team is approaching these",
    "start": "181410",
    "end": "186660"
  },
  {
    "text": "problems and I think that is something that is a lot more teachable in this kind of short format then trying to dive",
    "start": "186660",
    "end": "192989"
  },
  {
    "text": "really deep technically on something and have you kind of keep that all in your brain or reference with slides later but",
    "start": "192989",
    "end": "198150"
  },
  {
    "text": "you can totally do that just in case someone in the audience doesn't know what prometheus is or what metrics are I",
    "start": "198150",
    "end": "203730"
  },
  {
    "start": "199000",
    "end": "258000"
  },
  {
    "text": "want to do just a brief explanation metrics are real-time numeric measurements and they usually are",
    "start": "203730",
    "end": "209099"
  },
  {
    "text": "answering questions like how many things happened or how much is a thing is happening right now or how long",
    "start": "209099",
    "end": "214590"
  },
  {
    "text": "something took in code we often increment or set metrics anytime",
    "start": "214590",
    "end": "219660"
  },
  {
    "text": "something happens so if you have a system that is doing things every time something happens you're gonna set some sort of metric for it applications that",
    "start": "219660",
    "end": "227310"
  },
  {
    "text": "are utilizing Prometheus serve a literal web page with metrics and plain text that you can go to and look at and that",
    "start": "227310",
    "end": "235200"
  },
  {
    "text": "is called exporting metrics and I'm gonna I'm gonna talk about that quite a bit so this is really useful because you can",
    "start": "235200",
    "end": "240480"
  },
  {
    "text": "actually see you know what Prometheus is then gonna do is go in and gobble up these metrics and you can see what",
    "start": "240480",
    "end": "248340"
  },
  {
    "text": "Prometheus is ingesting and so Prometheus will reach out gobble up",
    "start": "248340",
    "end": "254400"
  },
  {
    "text": "those metrics and put them in a time series database that you can then go and query later looking at metrics and logs",
    "start": "254400",
    "end": "260250"
  },
  {
    "start": "258000",
    "end": "332000"
  },
  {
    "text": "and traces there are a couple of themes that kind of instantly emerge to me there are a lot of promises floating",
    "start": "260250",
    "end": "265860"
  },
  {
    "text": "around about people giving you observability for free or you know being able to add it on to",
    "start": "265860",
    "end": "271350"
  },
  {
    "text": "something at any time and you know get the kind of outcomes that you want in my experience when you just kind of rubs",
    "start": "271350",
    "end": "276660"
  },
  {
    "text": "observability on something like that you're not really gonna get the best the",
    "start": "276660",
    "end": "282000"
  },
  {
    "text": "best outcome these patterns and these tools need to be kind of thoroughly understood and then utilized",
    "start": "282000",
    "end": "287820"
  },
  {
    "text": "thoughtfully this needs to be something that is central to your software architecture and design you should be",
    "start": "287820",
    "end": "293160"
  },
  {
    "text": "designing from the beginning interactions that you can observe and test and that way you can have clear",
    "start": "293160",
    "end": "298740"
  },
  {
    "text": "signals about what is going on there and your performance and that is the best thing you can do to have maintainable",
    "start": "298740",
    "end": "304980"
  },
  {
    "text": "software and to do that I mean I'm gonna start to sound like a broken record but you have to spend the time to understand",
    "start": "304980",
    "end": "311280"
  },
  {
    "text": "the tool you know doing things like learning what to log and what not to log more importantly or what things you",
    "start": "311280",
    "end": "318450"
  },
  {
    "text": "should be creating metrics for and then learning different query languages for each of those things finding the balance",
    "start": "318450",
    "end": "323670"
  },
  {
    "text": "of what you should be tracing and what you should not and when to sample when to aggregate how to visualize all that",
    "start": "323670",
    "end": "329100"
  },
  {
    "text": "stuff this all takes time and labor and because of that time that it takes to",
    "start": "329100",
    "end": "335250"
  },
  {
    "start": "332000",
    "end": "400000"
  },
  {
    "text": "kind of learn and understand this and do it well people are starting to talk about going beyond you know we're gonna correlate these signals we're gonna",
    "start": "335250",
    "end": "341340"
  },
  {
    "text": "build simple unifying tools some sort of overarching thing that is gonna make this all easier but you can't really do",
    "start": "341340",
    "end": "350070"
  },
  {
    "text": "that because these things are all so different and also kind of nuanced and it's hard to do any of that stuff well",
    "start": "350070",
    "end": "356430"
  },
  {
    "text": "without that core understanding and I don't want anybody to sleep on you know those old kind of fundamental pillar",
    "start": "356430",
    "end": "362010"
  },
  {
    "text": "concepts and wait for the next big thing because it's going to be built on this same stuff how about making sense here",
    "start": "362010",
    "end": "368040"
  },
  {
    "text": "because what I'm trying to say is that none of this is easy you've heard it in the keynote yesterday you know it's it's",
    "start": "368040",
    "end": "373290"
  },
  {
    "text": "not easy and we're trying to make it easier but even with that top quality stuff that you're gonna get for free you",
    "start": "373290",
    "end": "378330"
  },
  {
    "text": "still have to understand how to transform that into a good result I want us here today in a headspace where we",
    "start": "378330",
    "end": "385140"
  },
  {
    "text": "are treating these tools like the critical infrastructure that they are not as something that comes later because that is the way the way that we",
    "start": "385140",
    "end": "392370"
  },
  {
    "text": "effectively maintain software and keep customers happy and keep dollars flowing getting really good at this is worth the",
    "start": "392370",
    "end": "397650"
  },
  {
    "text": "investment and it's really fun so Prometheans the reason you probably",
    "start": "397650",
    "end": "402840"
  },
  {
    "start": "400000",
    "end": "462000"
  },
  {
    "text": "added this talk to your schedule is because I was gonna be talk about doing fancy stuff with Prometheus and I will",
    "start": "402840",
    "end": "408300"
  },
  {
    "text": "but you need to be ready for a little bit of a letdown because the more I thought about this the more I realized",
    "start": "408300",
    "end": "414480"
  },
  {
    "text": "there isn't special sauce these are kind of logical thought processes that result",
    "start": "414480",
    "end": "419490"
  },
  {
    "text": "from being really familiar with this tool and these are hard problems to solve and the tools can't be good at",
    "start": "419490",
    "end": "424980"
  },
  {
    "text": "everything that's why there's so many tools so you kind of need to get good or at least fluent in all of them but you",
    "start": "424980",
    "end": "431820"
  },
  {
    "text": "know even just one will get you kind of a really good you know at least get you most of the way there having said that",
    "start": "431820",
    "end": "438540"
  },
  {
    "text": "though Prometheus is actually just about as versatile a piece of software as I have ever used knowing the supposed limitations that",
    "start": "438540",
    "end": "445470"
  },
  {
    "text": "people throw out I am constantly surprised with the how well it stands up",
    "start": "445470",
    "end": "450690"
  },
  {
    "text": "to just the just utter and complete punishment that we routinely inflict upon it and that's why I'm here because",
    "start": "450690",
    "end": "457140"
  },
  {
    "text": "I think people are glossing over how great it that we could have had this already so Prometheus is intentionally",
    "start": "457140",
    "end": "464520"
  },
  {
    "start": "462000",
    "end": "560000"
  },
  {
    "text": "not a distributed system it is beautiful in its simplicity in that way it's very easy to kind of set up and explore and",
    "start": "464520",
    "end": "470700"
  },
  {
    "text": "start to understand what the value you're gonna get out of it is in the future as long as resource usage is kept",
    "start": "470700",
    "end": "476190"
  },
  {
    "text": "in check the network is up and the things that are exporting those metrics are up you're basically good to go",
    "start": "476190",
    "end": "481820"
  },
  {
    "text": "sometimes though the network does break or queries make Prometheus sad or exporters go down so people want to",
    "start": "481820",
    "end": "488070"
  },
  {
    "text": "start talking about doing high availability Prometheus this is the kind of thing that can happen so I have an instance here that was running perfectly",
    "start": "488070",
    "end": "494310"
  },
  {
    "text": "well for a while it's a short there but you know this thing is running for weeks at a pretty constant resource",
    "start": "494310",
    "end": "499590"
  },
  {
    "text": "utilization then all of a sudden it doubles triples in resource usage",
    "start": "499590",
    "end": "505640"
  },
  {
    "text": "because somebody queried it big bad but if you've got ever and this is fine but",
    "start": "505640",
    "end": "512310"
  },
  {
    "text": "if you run out of CPU or you run out of memory to processes query then you can slow an instance down to the point where",
    "start": "512310",
    "end": "518190"
  },
  {
    "text": "it becomes unresponsive this is kind of the same the only thing that I see kind of routinely you know operationally and",
    "start": "518190",
    "end": "525330"
  },
  {
    "text": "running Prometheus for people is that sometimes you know people make it sad",
    "start": "525330",
    "end": "530620"
  },
  {
    "text": "so what happens when you do that is that you might end up with a gap in metrics when you're querying later because you",
    "start": "530620",
    "end": "536270"
  },
  {
    "text": "will have put that server in a situation where it can't go out and reach for metrics anymore it also means you know a",
    "start": "536270",
    "end": "543170"
  },
  {
    "text": "gap in a graph if it's a normal day is maybe not such a big deal but when it when it is not scraping metrics like",
    "start": "543170",
    "end": "549410"
  },
  {
    "text": "that it is also not evaluating alerts so if you do have something terrible happen in that space not only will you not be",
    "start": "549410",
    "end": "556610"
  },
  {
    "text": "able to see it you won't get alerted for it and that is actually pretty bad the way that folks generally work around",
    "start": "556610",
    "end": "562339"
  },
  {
    "start": "560000",
    "end": "709000"
  },
  {
    "text": "this is by deploying replicas of Prometheus that scrape the same things and evaluate the same alerts ideally",
    "start": "562339",
    "end": "567740"
  },
  {
    "text": "they do this in kind of separate geographical locations it's kind of air-gap yourself from a basic network",
    "start": "567740",
    "end": "573980"
  },
  {
    "text": "failure this means that you need a way to switch between querying multiple instances of Prometheus at one time if",
    "start": "573980",
    "end": "580400"
  },
  {
    "text": "one of them has an issue and a proxy is a really good answer here so putting that in front of your H a replicas and",
    "start": "580400",
    "end": "585529"
  },
  {
    "text": "then you know you can switch if one of them is having problems you can run health checks on the graph endpoint in",
    "start": "585529",
    "end": "592400"
  },
  {
    "text": "Prometheus which is just a regular page that you would use to go and dork around in the UI and my experience that is a",
    "start": "592400",
    "end": "598760"
  },
  {
    "text": "pretty good proxy for it is the Prometheus op or not as I'm reading this",
    "start": "598760",
    "end": "603830"
  },
  {
    "text": "I now realize there is a status API that you could probably also query so maybe do that too ideally you could manually",
    "start": "603830",
    "end": "610130"
  },
  {
    "text": "failover those instances in your group so because maybe there's a network issue between",
    "start": "610130",
    "end": "616070"
  },
  {
    "text": "Prometheus and the things that it is scraping and you know about that cuz you've got an alert from somewhere but Prometheus is actually like hey I'm",
    "start": "616070",
    "end": "621860"
  },
  {
    "text": "doing Gucci man like there's less there's less metrics coming in and I'm actually happier but then you query and",
    "start": "621860",
    "end": "627560"
  },
  {
    "text": "say hey we're all my metrics if you've got a network segregated Prometheus somewhere that is not subject to that",
    "start": "627560",
    "end": "633260"
  },
  {
    "text": "issue you could go and poke your proxy if they actually go look over here we need to see what's going on and then",
    "start": "633260",
    "end": "638300"
  },
  {
    "text": "ideally you would not rotate back to that other instance that has those holes until the other one has problems so yeah",
    "start": "638300",
    "end": "644570"
  },
  {
    "text": "you would still have a hole in your metrics but like if you rotate seven days later the only time you're gonna see that holes if you're actually",
    "start": "644570",
    "end": "650300"
  },
  {
    "text": "looking that far back in time you also get a few things for free with a proxy like query logging although Prometheus",
    "start": "650300",
    "end": "657140"
  },
  {
    "text": "does do that now but I've been using it for a very long time and it has been very very useful to see who's being naughty and then",
    "start": "657140",
    "end": "664220"
  },
  {
    "text": "potentially something like rate-limiting - so this is a really high availability",
    "start": "664220",
    "end": "669350"
  },
  {
    "text": "right because there are there are situations where this could become one usable but you're not really going to be",
    "start": "669350",
    "end": "676490"
  },
  {
    "text": "able to tolerate a massive network failure in a system that uses the network to go out and reach out for metrics so deal with it I guess but you",
    "start": "676490",
    "end": "685100"
  },
  {
    "text": "can't minimize those gaps right and hopefully you can be able to find them you'll find those metrics that you need",
    "start": "685100",
    "end": "690470"
  },
  {
    "text": "somewhere if it's really important and you know as bad as I made that sound",
    "start": "690470",
    "end": "695660"
  },
  {
    "text": "I've been running from me Thea's pretty large infrastructure for years and there's only been a couple of cases that",
    "start": "695660",
    "end": "701510"
  },
  {
    "text": "I can remember where I didn't actually have something that somebody needed due to either a network or you know a bunch",
    "start": "701510",
    "end": "707660"
  },
  {
    "text": "of instances going down there are options though if you are kind of wanting to go harder in that in that way",
    "start": "707660",
    "end": "714590"
  },
  {
    "text": "there are things like Thanos query or promix Y and these promise to fill those gaps in your metrics you know even if",
    "start": "714590",
    "end": "723710"
  },
  {
    "text": "one of them goes down and you can have just like one thing that you're querying the way they do this is actually by fanning out queries so if you have four",
    "start": "723710",
    "end": "731630"
  },
  {
    "text": "instances that are scraping the exact same thing two of them go down this thing will actually query all four see",
    "start": "731630",
    "end": "737360"
  },
  {
    "text": "that there's a gap in the first two and then fill it in you know by deduplicating data and giving you one solid graph there are some risks here",
    "start": "737360",
    "end": "744710"
  },
  {
    "text": "though so you will have decreased query performance at least in a little bit because there is CPU time spent once",
    "start": "744710",
    "end": "751130"
  },
  {
    "text": "that proxy goes fans out and then kind of coalesce as those things together you also can pay double the cost for latency",
    "start": "751130",
    "end": "757820"
  },
  {
    "text": "of your query so say you do a big fat query that returns like 40 Meg's of",
    "start": "757820",
    "end": "763000"
  },
  {
    "text": "results maybe your network is slow maybe it's not whatever that that result will",
    "start": "763000",
    "end": "769190"
  },
  {
    "text": "go for Prometheus - this proxy do some work correspondingly larger amount of work with the larger your data set is",
    "start": "769190",
    "end": "774860"
  },
  {
    "text": "and then send it back along to you and if you're running queries that are like 15 18 20 seconds you will kind of feel",
    "start": "774860",
    "end": "781850"
  },
  {
    "text": "this a little bit more the other risk is that if someone does a really big query",
    "start": "781850",
    "end": "786860"
  },
  {
    "text": "or loads up a big fat killer dashboard over like eight weeks or something these queries will fan out to all of",
    "start": "786860",
    "end": "793680"
  },
  {
    "text": "your Prometheus and DDoS all of them and then you are creating that gap that you were trying to avoid in the first place",
    "start": "793680",
    "end": "799560"
  },
  {
    "text": "because none of your replicas are up there's also some increased operational overhead you know this is another component to",
    "start": "799560",
    "end": "805380"
  },
  {
    "text": "run and because Prometheus is generally so easy to operate this thing can actually be as hard to operate as",
    "start": "805380",
    "end": "810540"
  },
  {
    "text": "Prometheus itself but there are other benefits too so you can get something like a global query view so you're not changing you know data sources in",
    "start": "810540",
    "end": "817380"
  },
  {
    "text": "Griffin or trying to figure out which Prometheus has the metrics that you're looking for and that is pretty cool",
    "start": "817380",
    "end": "822710"
  },
  {
    "start": "822000",
    "end": "908000"
  },
  {
    "text": "let's talk about cardinality for a second this is a quick refresher because this is not something that is intuitive",
    "start": "822710",
    "end": "828840"
  },
  {
    "text": "for people when they start using Prometheus but it's probably the most important thing for you to know every unique permutation of label values in",
    "start": "828840",
    "end": "836100"
  },
  {
    "text": "Prometheus creates a new time series so you can see in that example below I have three different time series because",
    "start": "836100",
    "end": "842250"
  },
  {
    "text": "there are three different kind of combinations of potential metric label values the official recommendation from",
    "start": "842250",
    "end": "849030"
  },
  {
    "text": "Prometheus is that any individual query should match hundreds and not thousands of time series so you see I've done a",
    "start": "849030",
    "end": "855180"
  },
  {
    "text": "query that kind of matches three here but you can maybe think about how if I was doing a sum across you know HTTP",
    "start": "855180",
    "end": "862440"
  },
  {
    "text": "requests across my entire application how this could easily you know find hundreds or thousands of time series you",
    "start": "862440",
    "end": "868610"
  },
  {
    "text": "can work out a query at a single point in time to figure out kind of how dangerous it's going to be so you can",
    "start": "868610",
    "end": "874440"
  },
  {
    "text": "see like how many time series are gonna be returned how long did your query take and you should do that before you go and graph something because the way to the",
    "start": "874440",
    "end": "880200"
  },
  {
    "text": "graph works is it evaluates your individual query at many many many points in time and then connects them",
    "start": "880200",
    "end": "886200"
  },
  {
    "text": "all together so if your grade takes 10 seconds when you evaluate it for the last 5 minutes it's not going to work to your great if you're doing it over a",
    "start": "886200",
    "end": "891480"
  },
  {
    "text": "week so basically what you should do when you're creating metrics is to try and avoid labels that have unbounded",
    "start": "891480",
    "end": "896850"
  },
  {
    "text": "potential values and then watch the number of total labels that you put on a",
    "start": "896850",
    "end": "901980"
  },
  {
    "text": "metric because you've got a big metric already then you add another label that just has 10 values you have just ten X the cardinality of that metric so if",
    "start": "901980",
    "end": "909300"
  },
  {
    "start": "908000",
    "end": "937000"
  },
  {
    "text": "you're following what I basically just told you to do is not create metrics when you're creating metrics in my",
    "start": "909300",
    "end": "914340"
  },
  {
    "text": "experience running Prometheus for people generally any query that is worth doing touches thousands of time series some of",
    "start": "914340",
    "end": "921660"
  },
  {
    "text": "them even hundreds of thousands so we've got to find a way to work with this luckily if you're querying under about",
    "start": "921660",
    "end": "927060"
  },
  {
    "text": "two thousand time series you're totally fine and then if you even go up to like 10,000 or so I'm not even worried about",
    "start": "927060",
    "end": "933420"
  },
  {
    "text": "you going beyond that though we might have to do some work if you're going to do those higher cardinality queries",
    "start": "933420",
    "end": "939750"
  },
  {
    "start": "937000",
    "end": "1125000"
  },
  {
    "text": "there are some things you can do operationally to Prometheus to help limit the worst of the pain so you need to have significant amount of memory and",
    "start": "939750",
    "end": "946560"
  },
  {
    "text": "CPU head room while prometheus is running a queried instance is a completely different animal like I showed you earlier you can set a limit",
    "start": "946560",
    "end": "954480"
  },
  {
    "text": "on how many samples I'm sorry I should give you a guideline on head room I",
    "start": "954480",
    "end": "959580"
  },
  {
    "text": "usually do 50% it still dies sometimes so you know it's just money right so you",
    "start": "959580",
    "end": "965220"
  },
  {
    "text": "know just keep on getting up or just run it in kubernetes and let it eat your entire cluster whatever you can set a",
    "start": "965220",
    "end": "971010"
  },
  {
    "text": "limit on how many samples a single query can load into memory so samples means individual values inside of the",
    "start": "971010",
    "end": "977459"
  },
  {
    "text": "individual time series so if you're evaluating a query that touches a thousand time series over two weeks it's",
    "start": "977459",
    "end": "982500"
  },
  {
    "text": "gonna have to evaluate that query potentially thousands of times so then you were you know multiplying that number of time series by a big number I",
    "start": "982500",
    "end": "989089"
  },
  {
    "text": "usually set this to like 50 million if somebody's doing something that is more than 50 million like I don't want them",
    "start": "989089",
    "end": "995940"
  },
  {
    "text": "to do it and Prometheus generally will like kind of calculate the number of samples that it's gonna load it takes",
    "start": "995940",
    "end": "1002029"
  },
  {
    "text": "like five seconds or so and then go hold on Jack are you sure and I usually say",
    "start": "1002029",
    "end": "1007160"
  },
  {
    "text": "no but I have been known onto occasion on occasion to bump up this limit for",
    "start": "1007160",
    "end": "1013490"
  },
  {
    "text": "somebody who wants to do more you can also set the limit on maximum concurrent",
    "start": "1013490",
    "end": "1020209"
  },
  {
    "text": "queries that are being done in Prometheus you should absolutely do this I usually set it to one per CPU core on",
    "start": "1020209",
    "end": "1026660"
  },
  {
    "text": "the instance the reason for this is if you are doing like 20 queries at a single point in time on a box that has",
    "start": "1026660",
    "end": "1033620"
  },
  {
    "text": "like eight cores it is going to have to do like a lot of context switching and it's not gonna go well it's better to",
    "start": "1033620",
    "end": "1039500"
  },
  {
    "text": "wait the you know quarter of a second or whatever for another query to finish and then execute your new one",
    "start": "1039500",
    "end": "1045620"
  },
  {
    "text": "rather than just kind of doing them all at once what usually happens when you do something like this without this limit is you're loading a big dashboard with",
    "start": "1045620",
    "end": "1052100"
  },
  {
    "text": "100 queries on it and they will just all go back I'll come back failing and then",
    "start": "1052100",
    "end": "1057290"
  },
  {
    "text": "if you you know this limit then they can all just roughly come in you know maybe two seconds later you should also shard",
    "start": "1057290",
    "end": "1064740"
  },
  {
    "text": "metrics or federate so do we are a cloud",
    "start": "1064740",
    "end": "1069780"
  },
  {
    "text": "with lots of regions and there are times where people want to answer questions about the entire cloud unfortunately",
    "start": "1069780",
    "end": "1076620"
  },
  {
    "text": "they're the you know it's a big cloud so they're answering that question out of Prometheus would require a query across",
    "start": "1076620",
    "end": "1083190"
  },
  {
    "text": "you know a giant global set of data so I shard this stuff regionally this means",
    "start": "1083190",
    "end": "1089160"
  },
  {
    "text": "that people are a little bit sad at first because they're like well now I have to do my query for every region but",
    "start": "1089160",
    "end": "1094530"
  },
  {
    "text": "you still get your answer and that's better than no answer at all and it's honestly it's not that hard to stitch it",
    "start": "1094530",
    "end": "1099570"
  },
  {
    "text": "all together like if you really really care isn't it worth doing a little bit of work I don't know that's how I feel",
    "start": "1099570",
    "end": "1105570"
  },
  {
    "text": "about it but you can also federates oh if you you know still you could do a regional sharding and then create kind",
    "start": "1105570",
    "end": "1111570"
  },
  {
    "text": "of a layered cake of Prometheus where you are aggregating something at a regional level and then only putting",
    "start": "1111570",
    "end": "1116760"
  },
  {
    "text": "that stuff in the next tier so then you could do that global query again just a little bit more you know kind of labor",
    "start": "1116760",
    "end": "1122610"
  },
  {
    "text": "that you have to do but hey thanks for your question when you start going really hard with queries you need to be",
    "start": "1122610",
    "end": "1128760"
  },
  {
    "start": "1125000",
    "end": "1196000"
  },
  {
    "text": "kind of careful be careful with your aggregation so scope down to only the things that you actually care about you",
    "start": "1128760",
    "end": "1134790"
  },
  {
    "text": "know increase your step size of your query so that you have at first the lowest resolution that you can stand and",
    "start": "1134790",
    "end": "1139920"
  },
  {
    "text": "then you know the same thing with a smaller kind of time window and then scale up so if you're gonna do some sort",
    "start": "1139920",
    "end": "1145140"
  },
  {
    "text": "of big query over a week you know maybe start with an hour at you know 5-minute resolution and then kind of work your",
    "start": "1145140",
    "end": "1151380"
  },
  {
    "text": "way up instead of just like punching it right in the mouth with the bigquery there's another thing that you can do is",
    "start": "1151380",
    "end": "1158480"
  },
  {
    "text": "you should be able to understand kind of your query pattern and the questions that you're gonna be trying to answer",
    "start": "1158480",
    "end": "1163920"
  },
  {
    "text": "with these metrics and then create the metrics and query to match so there's a",
    "start": "1163920",
    "end": "1169500"
  },
  {
    "text": "lot of stuff to care about but again you're doing something that's really hard so you have to care a little bit it's like you're supporting customer I",
    "start": "1169500",
    "end": "1176310"
  },
  {
    "text": "don't know like what is more important so doing these kind of things I routinely help folks query metrics that",
    "start": "1176310",
    "end": "1182490"
  },
  {
    "text": "have hundreds of thousands of time-series and if you listen to you know vendors and things this is something that you absolutely cannot do",
    "start": "1182490",
    "end": "1189000"
  },
  {
    "text": "with for me Thea's and I'm here to tell you they're lying it's just a little bit harder than it it's just a little bit",
    "start": "1189000",
    "end": "1194860"
  },
  {
    "text": "harder but it's fun I like to give you an example of crafting metrics that match desired outcomes because this is",
    "start": "1194860",
    "end": "1201370"
  },
  {
    "start": "1196000",
    "end": "1266000"
  },
  {
    "text": "something that I don't think people are doing enough so you've got some sort of web service that has potentially",
    "start": "1201370",
    "end": "1206380"
  },
  {
    "text": "thousands of endpoints that you might care about latency for you want to know per endpoint latency you want to know",
    "start": "1206380",
    "end": "1212980"
  },
  {
    "text": "latency per API version and then you want to know global latency just one single number you should create a",
    "start": "1212980",
    "end": "1219640"
  },
  {
    "text": "separate metric for each one of those questions so you will have one metric that has a label value for every",
    "start": "1219640",
    "end": "1226149"
  },
  {
    "text": "endpoint in your app and then if you tried to do a global you know latency calculation on that everything would",
    "start": "1226149",
    "end": "1232059"
  },
  {
    "text": "fall apart that's fine because now you have a separate metric for global latency that is tiny tiny tiny and it",
    "start": "1232059",
    "end": "1238389"
  },
  {
    "text": "comes back instantly and that's really good and then another one for you know some sort of group that you care about",
    "start": "1238389",
    "end": "1243720"
  },
  {
    "text": "this does create more metrics up front and it does increase your regular resource usage a little bit but you're",
    "start": "1243720",
    "end": "1249279"
  },
  {
    "text": "not gonna blow anything out at query time and then you can actually answer all the questions that you want so you have to maintain a bit of cognitive load",
    "start": "1249279",
    "end": "1255100"
  },
  {
    "text": "you got to know what to query for what but I think it's a fair trade-off to being able to you know actually answer",
    "start": "1255100",
    "end": "1260470"
  },
  {
    "text": "your questions like I said spend a little time understand your tools because they're really important",
    "start": "1260470",
    "end": "1266010"
  },
  {
    "start": "1266000",
    "end": "1343000"
  },
  {
    "text": "long-term storage is something that people routinely ask for in prometheus",
    "start": "1266010",
    "end": "1271710"
  },
  {
    "text": "Prometheus pretty recently released disk backed retention so basically if you provision a big old disk on your",
    "start": "1271710",
    "end": "1277510"
  },
  {
    "text": "prometheus instance you can tell it to just fill up the entire disk with metrics and you know start deleting them",
    "start": "1277510",
    "end": "1282970"
  },
  {
    "text": "when it runs out of disk space you can do it with a block storage volume as well if you keep the maximum block size",
    "start": "1282970",
    "end": "1288700"
  },
  {
    "text": "that is the size that Prometheus size and time where Prometheus will compact your metrics down to if you keep that",
    "start": "1288700",
    "end": "1295029"
  },
  {
    "text": "relatively low like three to seven days when you run out of disk space and for me theist needs to delete metrics it",
    "start": "1295029",
    "end": "1300309"
  },
  {
    "text": "will delete by block so if you you know let this compact down to like four weeks or something and then you run out of",
    "start": "1300309",
    "end": "1305919"
  },
  {
    "text": "disk space you're not deleting four weeks of disk space or metrics and then freeing up like half of your disk you",
    "start": "1305919",
    "end": "1312940"
  },
  {
    "text": "know you can kill little little tiny bits at a time this has a little bit more overhead when you start querying",
    "start": "1312940",
    "end": "1318760"
  },
  {
    "text": "but in my experience it's not so bad I turned this on in our infrastructure about 250 days ago so my experience doing",
    "start": "1318760",
    "end": "1325870"
  },
  {
    "text": "really long-term queries is somewhat limited but doing kind of some of the stuff I talked about before I've had",
    "start": "1325870",
    "end": "1331390"
  },
  {
    "text": "like perfect easy success doing kind of long-term queries and 250 days or a year",
    "start": "1331390",
    "end": "1337090"
  },
  {
    "text": "or 2 years is a lot more than people need even just like four to eight weeks is often enough for for every question",
    "start": "1337090",
    "end": "1342990"
  },
  {
    "text": "there are new solutions in this space though things like fan oohs m3 and cortex they",
    "start": "1342990",
    "end": "1349900"
  },
  {
    "start": "1343000",
    "end": "1399000"
  },
  {
    "text": "offer long term storage as well as a lot of other potential benefits so that like global query view I was talking about earlier multi-tenancy down sampling",
    "start": "1349900",
    "end": "1356710"
  },
  {
    "text": "compaction you know pushing metrics natively scaling horizontally these are",
    "start": "1356710",
    "end": "1361929"
  },
  {
    "text": "solving really big problems compared to just vanilla Prometheus and they come with a correspondingly large amount of",
    "start": "1361929",
    "end": "1367390"
  },
  {
    "text": "operational overhead so the cost to run any of these things is much much more than Prometheus and it needs to be",
    "start": "1367390",
    "end": "1373029"
  },
  {
    "text": "staffed appropriately if you're gonna do something like this my opinion is if you're looking at something like this just for long-term storage you should",
    "start": "1373029",
    "end": "1380289"
  },
  {
    "text": "think about whether you really need to keep that data long-term in the first place in my experience it's a thing",
    "start": "1380289",
    "end": "1385630"
  },
  {
    "text": "people think they want but almost never use and the problem is that especially if you're considering a big solution like one of these big boys the expense",
    "start": "1385630",
    "end": "1393760"
  },
  {
    "text": "and operations and money is a lot higher for the relative utility that you're actually getting my recommendation for",
    "start": "1393760",
    "end": "1400809"
  },
  {
    "start": "1399000",
    "end": "1449000"
  },
  {
    "text": "long-term storage is to throw that big fat disk Prometheus and just let it go hog-wild if that doesn't work you know",
    "start": "1400809",
    "end": "1408220"
  },
  {
    "text": "you could think about setting up a long-term prometheus instance and federating only that kind of aggregated",
    "start": "1408220",
    "end": "1413830"
  },
  {
    "text": "long-term trending data that you really want to keep and if you do that and keep that data relatively small you know even",
    "start": "1413830",
    "end": "1420789"
  },
  {
    "text": "just like four four gigs of ram and a you know 100 gig disk you're gonna keep that stuff for years what I say it again",
    "start": "1420789",
    "end": "1427720"
  },
  {
    "text": "you have to do that labor you have to think about the solution that you want and you know kind of care about your outcomes and then you've got a campaign",
    "start": "1427720",
    "end": "1433960"
  },
  {
    "text": "for the time to do that and ironically it might even be easier to convince some leaders that you should stand up a large",
    "start": "1433960",
    "end": "1439690"
  },
  {
    "text": "you know new cool system because it has kind of that cloud native cloud but if",
    "start": "1439690",
    "end": "1444760"
  },
  {
    "text": "you don't need all of that you really shouldn't because it's going to be very expensive once people start getting",
    "start": "1444760",
    "end": "1451179"
  },
  {
    "start": "1449000",
    "end": "1479000"
  },
  {
    "text": "value amount of metrics a common idea that folks have is wanting to do machine learning or anomaly detection on and I'm sure this is possible I've heard",
    "start": "1451179",
    "end": "1458179"
  },
  {
    "text": "a lot of amazing stories about what machine learning can do I've personally never seen on to even an outcome good or",
    "start": "1458179",
    "end": "1464419"
  },
  {
    "text": "bad this seems harder than it looks and it looks pretty freakin hard to me I'd",
    "start": "1464419",
    "end": "1469850"
  },
  {
    "text": "like to give you a couple of tips though so that you can do some basic kind of anomaly screening predictive analysis in Prometheus and then you can tell your",
    "start": "1469850",
    "end": "1476029"
  },
  {
    "text": "leaders you Turing machine learning and get yourself a big raise so this is a good example from the Prometheus blog",
    "start": "1476029",
    "end": "1482240"
  },
  {
    "start": "1479000",
    "end": "1525000"
  },
  {
    "text": "awhile ago so you've got a handful of instances of like a web application that are serving",
    "start": "1482240",
    "end": "1488149"
  },
  {
    "text": "you know kind of similar requests you can examine the group of latency metrics to find outlier instances that might be",
    "start": "1488149",
    "end": "1494419"
  },
  {
    "text": "running kind of slowly you can do this by checking out request durations average that are sticking out by two",
    "start": "1494419",
    "end": "1500450"
  },
  {
    "text": "standard deviations and then additionally you could mitigate any false positives if your data is really",
    "start": "1500450",
    "end": "1505610"
  },
  {
    "text": "closely clustered together by checking that the value must also be 20% higher",
    "start": "1505610",
    "end": "1511879"
  },
  {
    "text": "than average so this is an absolute unit of a query but you can break it down",
    "start": "1511879",
    "end": "1517519"
  },
  {
    "text": "into its component pieces fairly easily to kind of understand it you know spend an hour understand the tool you get it",
    "start": "1517519",
    "end": "1525460"
  },
  {
    "start": "1525000",
    "end": "1553000"
  },
  {
    "text": "another one I like to do is trying to find spikes and odd events so if you got",
    "start": "1525460",
    "end": "1530809"
  },
  {
    "text": "something that is normally very very consistent and you can examine some sort of recent amount of time like 5 minutes",
    "start": "1530809",
    "end": "1536629"
  },
  {
    "text": "or 10 minutes compared to the last 24 hours or so you can see if you've got some sort of change this is a great kind",
    "start": "1536629",
    "end": "1542600"
  },
  {
    "text": "of first alert if you were you know looking you're kind of starting on your journey toward you know doing this kind of thing and you just want to know when",
    "start": "1542600",
    "end": "1548779"
  },
  {
    "text": "something happened this is a good thing to kind of know what's going on there the only other thing that is better than",
    "start": "1548779",
    "end": "1555799"
  },
  {
    "start": "1553000",
    "end": "1592000"
  },
  {
    "text": "catching a problem right away though is catching it before it even happens the predict linear function in prometheus",
    "start": "1555799",
    "end": "1561320"
  },
  {
    "text": "uses a ray predicts a future value based on a backward-looking linear regression so the canonical example is alerting",
    "start": "1561320",
    "end": "1568970"
  },
  {
    "text": "you're gonna run out of disk space before you do imagine how useful that would be on like a database for example",
    "start": "1568970",
    "end": "1574570"
  },
  {
    "text": "this is great that whenever you have some sort of capacity or saturation metric you know it's a little bit",
    "start": "1574570",
    "end": "1580820"
  },
  {
    "text": "similar to the last example because usually something has started happening that has changed your future in a way that is that is undesirable but it's",
    "start": "1580820",
    "end": "1587509"
  },
  {
    "text": "good for longer term stuff like a disk filling up release or like a TLS certificate expiring as well you can write an application that",
    "start": "1587509",
    "end": "1595700"
  },
  {
    "text": "exposes metrics for just about anything so in fact most of the ones that you would want to write probably already",
    "start": "1595700",
    "end": "1601700"
  },
  {
    "text": "exist out there at digitalocean we've written exporters to quantify capacity and our fleet in various ways look at",
    "start": "1601700",
    "end": "1608720"
  },
  {
    "text": "our events pipeline and look at individual customer customer resources and these are all things that are",
    "start": "1608720",
    "end": "1614420"
  },
  {
    "text": "incredibly custom right only only they were useful to us a lot of times an exporter is a really good replacement",
    "start": "1614420",
    "end": "1620900"
  },
  {
    "text": "for some sort of older monitoring tool that goes and like tickles a bespoke bash script or command or something like",
    "start": "1620900",
    "end": "1626630"
  },
  {
    "text": "that because you can standardize all that stuff into one Prometheus place and you know you get a few things for free",
    "start": "1626630",
    "end": "1632780"
  },
  {
    "text": "and having all of your stuff in one place and able to kind of query and make dashboards off of and do alerts on it is",
    "start": "1632780",
    "end": "1638210"
  },
  {
    "text": "really really great there's one real gotcha that I've seen by people a lot when writing exporters for the first",
    "start": "1638210",
    "end": "1644870"
  },
  {
    "start": "1639000",
    "end": "1767000"
  },
  {
    "text": "time that I want to try and warn you about this isn't the go library and it happens to you when you are kind of",
    "start": "1644870",
    "end": "1650720"
  },
  {
    "text": "writing you're forced exporter and like reading the documentation and doing it kind of the naive way basically what",
    "start": "1650720",
    "end": "1656750"
  },
  {
    "text": "happens is you will create a new time series with some combination of labels and you will you you will choose a label",
    "start": "1656750",
    "end": "1662510"
  },
  {
    "text": "value that that represents something ephemeral so say you're exporting metrics about a queue name that is only",
    "start": "1662510",
    "end": "1669920"
  },
  {
    "text": "there for five minutes or so what will happen using this kind of naive way that is very very easy to use them in the",
    "start": "1669920",
    "end": "1676370"
  },
  {
    "text": "client is that metric will then be present in the metrics endpoint that",
    "start": "1676370",
    "end": "1682340"
  },
  {
    "text": "your application exposes as long as the application is up so until it gets restarted so then you'll be clearing",
    "start": "1682340",
    "end": "1687980"
  },
  {
    "text": "your stuff in Prometheus later or you know get an alert for something you know some queue name or whatever you're like",
    "start": "1687980",
    "end": "1693740"
  },
  {
    "text": "well oh that shouldn't be like that you know that shouldn't be there and then we go one look and it won't be there like when why are my metrics wrong the",
    "start": "1693740",
    "end": "1700850"
  },
  {
    "text": "workaround is to use a pattern called const metrics in the go library this pattern essentially gathers the metrics",
    "start": "1700850",
    "end": "1707420"
  },
  {
    "text": "at scrape time so instead of kind of just you know bopping a little function",
    "start": "1707420",
    "end": "1712730"
  },
  {
    "text": "or something inside your your code you can kind of either just leave it to",
    "start": "1712730",
    "end": "1718550"
  },
  {
    "text": "entirely a thing that happens in scrape time if it's really fast or you can keep the kind of information that you want to be exporting metrics",
    "start": "1718550",
    "end": "1724670"
  },
  {
    "text": "about in some sort of other abstraction in memory in your code and then build them at the time that the scrape happens",
    "start": "1724670",
    "end": "1730990"
  },
  {
    "text": "you should always be doing this when the source of truth for the metrics that",
    "start": "1730990",
    "end": "1736130"
  },
  {
    "text": "you're exporting is not your code itself so if you're doing something like you know doing a database query and then",
    "start": "1736130",
    "end": "1742190"
  },
  {
    "text": "kind of generating metrics from that or poking around in some sort of stats socket we're writing an exporter for some sort of large system that you're",
    "start": "1742190",
    "end": "1748220"
  },
  {
    "text": "kind of you know just exporting some kind of health stuff about this is what you should do I don't expect you to",
    "start": "1748220",
    "end": "1754130"
  },
  {
    "text": "remember that what I would hope is that you kind of remember this moment and then maybe when you're doing this someday you can like tweet at me or go",
    "start": "1754130",
    "end": "1761060"
  },
  {
    "text": "read you know you'll remember constant ryx and you can go like control with that and the docks and guns kind of see what I'm talking about for me theus an",
    "start": "1761060",
    "end": "1768650"
  },
  {
    "start": "1767000",
    "end": "1860000"
  },
  {
    "text": "alert manager can I go together like PB&J so alert Manager groups many many alerts from Prometheus and sends one",
    "start": "1768650",
    "end": "1774380"
  },
  {
    "text": "single notification I like creating team or service specific alerting levels so",
    "start": "1774380",
    "end": "1779960"
  },
  {
    "text": "that like all my team's debug alerts go to one slack channel and then all my team's critical alerts will go to page",
    "start": "1779960",
    "end": "1785510"
  },
  {
    "text": "or duty and a slack channel something like that the configuration for alert manager is very very powerful but it's a",
    "start": "1785510",
    "end": "1791480"
  },
  {
    "text": "bit of a bear too right especially when you're doing something like global for an entire kind of organization you see",
    "start": "1791480",
    "end": "1797270"
  },
  {
    "text": "up on the right and that that tweet is actually my alert manager routing tree and you wouldn't even know it's a tree",
    "start": "1797270",
    "end": "1803180"
  },
  {
    "text": "because there are so many lines on it those are all the places that the alerts can possibly go there's a lot of good",
    "start": "1803180",
    "end": "1810560"
  },
  {
    "text": "literature out there about what you should alert on I'd actually I don't know if I said this when you are",
    "start": "1810560",
    "end": "1817490"
  },
  {
    "text": "creating kind of that alert manager config make an abstraction and generate that config so like I don't I don't",
    "start": "1817490",
    "end": "1824480"
  },
  {
    "text": "really have to care if my alert manager routing tree looks like a broken record because I generate that config and it's",
    "start": "1824480",
    "end": "1830780"
  },
  {
    "text": "fine an alert manager could totally handle way except in the UI it does crash chrome but that's fine there's",
    "start": "1830780",
    "end": "1837620"
  },
  {
    "text": "some good literature out there about what should be alerted on I'm not gonna like dive deep here because I could be a whole talk but alerts should prompt",
    "start": "1837620",
    "end": "1843650"
  },
  {
    "text": "action and then have all the context attached to the alert necessary to perform that action so write a playbook",
    "start": "1843650",
    "end": "1850070"
  },
  {
    "text": "attached to your alert also if you're doing slack alerts use the tool that Ullaeus wrote to create beautiful slack",
    "start": "1850070",
    "end": "1856610"
  },
  {
    "text": "you just don't get like viscerally angry whenever you see them the pinnacle of",
    "start": "1856610",
    "end": "1861860"
  },
  {
    "start": "1860000",
    "end": "1896000"
  },
  {
    "text": "mindful observability is implementing s ellos so you should read the chapter in the SRA book on it's really like the entire SRT book but essentially the idea",
    "start": "1861860",
    "end": "1869210"
  },
  {
    "text": "is that you should be defining the type of performance experience that your users can expect measure it and then take action to maintain that level of",
    "start": "1869210",
    "end": "1875539"
  },
  {
    "text": "service this is the most important thing to measure right so talking about being outcomes oriented this is the outcome",
    "start": "1875539",
    "end": "1880850"
  },
  {
    "text": "for your customers this is the thing that you should be measuring but quantifying it is actually really hard",
    "start": "1880850",
    "end": "1885919"
  },
  {
    "text": "but it's an essential exercise and the first few times you do it it might actually require some work you might",
    "start": "1885919",
    "end": "1890990"
  },
  {
    "text": "even need to change the architecture of your application to be able to measure that thing in the right place but once",
    "start": "1890990",
    "end": "1897380"
  },
  {
    "start": "1896000",
    "end": "1938000"
  },
  {
    "text": "you've measured it you can then set an objective so this is often like some number of nines and then you can give yourself an error budget so that would",
    "start": "1897380",
    "end": "1903500"
  },
  {
    "text": "be like the amount of time that you're allowed to file your SLO you can use that then as a simple you know this one",
    "start": "1903500",
    "end": "1909169"
  },
  {
    "text": "simple thing as a as a factor in any decision-making that you might have to do so if you have a service that's consistently violating its SLO and as an",
    "start": "1909169",
    "end": "1916010"
  },
  {
    "text": "organization you decided the DES allows are important you can then make the leap that you should not be doing any work",
    "start": "1916010",
    "end": "1921500"
  },
  {
    "text": "that is not a native fixing that problem this is a great way to push back on decision makers that might be trying to",
    "start": "1921500",
    "end": "1927380"
  },
  {
    "text": "push new work on your new features or something because it's really hard to argue against not achieving your SLO",
    "start": "1927380",
    "end": "1933490"
  },
  {
    "text": "Prometheus can help you set this kind of system up and I want to show you briefly how to do that so this is the 99th",
    "start": "1933490",
    "end": "1939980"
  },
  {
    "start": "1938000",
    "end": "1972000"
  },
  {
    "text": "percentile query latency in my Prometheus infrastructure over 24 hours measured hourly say I wanted to define",
    "start": "1939980",
    "end": "1947389"
  },
  {
    "text": "availability to be that 99 percent of my requests complete in under a second I can express that in a Prometheus query",
    "start": "1947389",
    "end": "1954169"
  },
  {
    "text": "as a binary one or zero did this happen or did this not happen for a time period using this less than boolean operator in",
    "start": "1954169",
    "end": "1960470"
  },
  {
    "text": "Prometheus so you can see on the previous graph there is one kind of hour where my latency my 99 percentile",
    "start": "1960470",
    "end": "1966559"
  },
  {
    "text": "latency was over a second so now in this graph it's a zero so I failed for that",
    "start": "1966559",
    "end": "1971720"
  },
  {
    "text": "hour then you can use a Prometheus sub query which are awesome to find that",
    "start": "1971720",
    "end": "1977480"
  },
  {
    "start": "1972000",
    "end": "2032000"
  },
  {
    "text": "exact same data on that graph which was generated by you know evaluating one query at 24 points in time and drawing",
    "start": "1977480",
    "end": "1983299"
  },
  {
    "text": "all the lines together you can get all that data in one query so that you can then use it in another",
    "start": "1983299",
    "end": "1988909"
  },
  {
    "text": "later you see all those ones and zeroes there we can then take the average over time of that data set and because it's",
    "start": "1988909",
    "end": "1995840"
  },
  {
    "text": "expressing those ones or zeros we can calculate a percentage of time that we met at RS although so if you're doing a time based SLO this is a very easy way",
    "start": "1995840",
    "end": "2002200"
  },
  {
    "text": "to do that so you can see that we actually had an availability of 95.8% over those 24 hours so if we wanted 99%",
    "start": "2002200",
    "end": "2008590"
  },
  {
    "text": "availability than we'd be in violation for simplicity here I evaluated the queries every hour in real life you",
    "start": "2008590",
    "end": "2015190"
  },
  {
    "text": "probably do it every few minutes or if any of you were doing like seven days you might go hourly or something like that and intervals actually do matter",
    "start": "2015190",
    "end": "2021309"
  },
  {
    "text": "because when I dropped it from evaluating - every hour - every minute my 95 percent pass rate dropped to 71 so",
    "start": "2021309",
    "end": "2027369"
  },
  {
    "text": "a little trick out there few people trying to hack your SLO but like don't do that actually because you're only hurting yourself a couple of more",
    "start": "2027369",
    "end": "2034090"
  },
  {
    "start": "2032000",
    "end": "2085000"
  },
  {
    "text": "problem QL tricks for you in this example I'm not using a histogram quantile to estimate my 99th percentile",
    "start": "2034090",
    "end": "2040179"
  },
  {
    "text": "latency I set my SLO at one second and I am actually counting the the requests that were less than or equal to a second",
    "start": "2040179",
    "end": "2046690"
  },
  {
    "text": "and then my total number of requests so I can get an 8 a more exact number of requests that actually took less than a",
    "start": "2046690",
    "end": "2052450"
  },
  {
    "text": "second because I'm using that histogram bucket another problem that you might run into here is that if you don't have",
    "start": "2052450",
    "end": "2057700"
  },
  {
    "text": "any requests for five minutes you know you would be dividing by zero here which",
    "start": "2057700",
    "end": "2062799"
  },
  {
    "text": "is not allowed in prometheus and it would break your entire query so using this little trick with or you can",
    "start": "2062799",
    "end": "2067990"
  },
  {
    "text": "sanitize that time where you had no requests as a 1 in your kind of binary 0 to 1 by using any metric that is always",
    "start": "2067990",
    "end": "2075220"
  },
  {
    "text": "extant and then we can use the same pattern as we did in the previous example so we can calculate the",
    "start": "2075220",
    "end": "2080230"
  },
  {
    "text": "percentage of time over the last week where latency was less than one second and feed that into an SLO all right",
    "start": "2080230",
    "end": "2085868"
  },
  {
    "start": "2085000",
    "end": "2128000"
  },
  {
    "text": "everybody we did it remember doing observability well requires time and",
    "start": "2085869",
    "end": "2090940"
  },
  {
    "text": "effort but your reliability is maybe the only thing that is important enough to justify that kind of investment you need",
    "start": "2090940",
    "end": "2098170"
  },
  {
    "text": "to do it anyway if you're gonna want the best if you want the best outcomes because low effort inputs are not enough",
    "start": "2098170",
    "end": "2103270"
  },
  {
    "text": "for your customers you should pick some tools and get really good at using them I'd recommend from e theist because it's",
    "start": "2103270",
    "end": "2109240"
  },
  {
    "text": "just excellent software and that's it if you have questions you can come up and ask me afterwards over here or",
    "start": "2109240",
    "end": "2116109"
  },
  {
    "text": "outside or something but I want to let you go to lunch and I cannot take the risk of any of you embarrassing me in front of sometime",
    "start": "2116109",
    "end": "2122000"
  },
  {
    "text": "so thanks for coming I appreciate your time [Applause]",
    "start": "2122000",
    "end": "2129880"
  }
]