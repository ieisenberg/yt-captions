[
  {
    "start": "0",
    "end": "75000"
  },
  {
    "text": "I saw some notes today talking about the various techniques design decisions architectural patterns that we use to",
    "start": "0",
    "end": "6690"
  },
  {
    "text": "build a system that is both cloud native but still supports the traditional sequel semantics that you would expect",
    "start": "6690",
    "end": "12509"
  },
  {
    "text": "from a sequel database before we get into that though I want to first lay down why this sort of system would be",
    "start": "12509",
    "end": "18779"
  },
  {
    "text": "interesting why it doesn't exist and why it would be great if it did exist and so why would you want a cloud native sequel",
    "start": "18779",
    "end": "25710"
  },
  {
    "text": "database and you all know what cloud native means but specifically for databases what's important is that it's",
    "start": "25710",
    "end": "32550"
  },
  {
    "text": "horizontally scalable and it can handle and recover from failures automatically so if you think of traditional sequel",
    "start": "32550",
    "end": "39420"
  },
  {
    "text": "databases going back to post grads or my sequel or all the proprietary solutions they typically run on a single machine",
    "start": "39420",
    "end": "45530"
  },
  {
    "text": "you might shard them or you might set up master-slave replication but they're typically built from the ground up to",
    "start": "45530",
    "end": "50850"
  },
  {
    "text": "run on a single machine and that single machine becomes a single point of failure and it also limits your ability to scale up as you want to store more",
    "start": "50850",
    "end": "58050"
  },
  {
    "text": "data or process more queries you have to buy bigger and bigger machines rather than just adding more machines that's a",
    "start": "58050",
    "end": "64140"
  },
  {
    "text": "real limitation from an operational perspective so a cloud native database is one that can survive failures scale",
    "start": "64140",
    "end": "71340"
  },
  {
    "text": "out horizontally and is really easy for the operator to manage and this",
    "start": "71340",
    "end": "76380"
  },
  {
    "start": "75000",
    "end": "204000"
  },
  {
    "text": "describes a number of existing databases it just turns out that they're all no sequel systems so you think to all the",
    "start": "76380",
    "end": "82439"
  },
  {
    "text": "no sequel databases you know and love most of them have these horizontal scalability and failure recovery properties to at least some extent they",
    "start": "82439",
    "end": "89549"
  },
  {
    "text": "can replicate data they can spread across datacenters for some of them but they're all lacking a couple really",
    "start": "89549",
    "end": "95759"
  },
  {
    "text": "important features the sequel databases have traditionally offered to developers and that's going back to consistency in",
    "start": "95759",
    "end": "103259"
  },
  {
    "text": "transactions so when you're using a sequel database developers get this nice benefit that they can trust the data in",
    "start": "103259",
    "end": "109829"
  },
  {
    "text": "it is all up to date there's nothing stale no you know replication is clobbering data you can trust that",
    "start": "109829",
    "end": "116579"
  },
  {
    "text": "transactions won't interact with each other in bad ways so that everything you write and read from the database is consistent and safe and this makes",
    "start": "116579",
    "end": "123450"
  },
  {
    "text": "writing systems significantly easier if you can rely on these properties if",
    "start": "123450",
    "end": "128759"
  },
  {
    "text": "you've ever built a system that uses a no sequel database you probably spent some time up front having to",
    "start": "128759",
    "end": "133930"
  },
  {
    "text": "around this if you can think back to your experience you might have ran into bugs while you were operating it due to stale data or due to your application",
    "start": "133930",
    "end": "141430"
  },
  {
    "text": "crashing in the middle of the multi-step operation these things cost a lot of developer time a lot of developer effort",
    "start": "141430",
    "end": "147750"
  },
  {
    "text": "it also caused bugs for customers so Google created a system called spanner",
    "start": "147750",
    "end": "153700"
  },
  {
    "text": "that many of you have probably heard of that was the inspiration for cockroach TV and spanner in the paper justifying",
    "start": "153700",
    "end": "160569"
  },
  {
    "text": "it the authors of the paper wrote that we'd much rather have developers occasionally looking into latency",
    "start": "160569",
    "end": "166090"
  },
  {
    "text": "problems caused by using transactions too much then to have those developers have to work around the lack of",
    "start": "166090",
    "end": "171129"
  },
  {
    "text": "transactions all the time it's much better for Google as a business if their developers don't have to worry about the",
    "start": "171129",
    "end": "178060"
  },
  {
    "text": "lack of transactions and if they can just trust that the database works the way they expect so they went to this",
    "start": "178060",
    "end": "183129"
  },
  {
    "text": "huge expense of both building and operating the system and found it to be more than worth it for everyone to start using it and I think the same thing is",
    "start": "183129",
    "end": "189489"
  },
  {
    "text": "true of a lot of systems out there outside of Google they don't have this available to them yet but they would have a much easier",
    "start": "189489",
    "end": "195549"
  },
  {
    "text": "time building and maintaining their applications if they could trust on their database to provide them with more guarantees and you know it would be",
    "start": "195549",
    "end": "206530"
  },
  {
    "start": "204000",
    "end": "270000"
  },
  {
    "text": "great if more systems provided consistency and transactions but when no sequel databases came around the",
    "start": "206530",
    "end": "212349"
  },
  {
    "text": "fundamental motivation for building them was availability and scalability you had these large web scale companies trying",
    "start": "212349",
    "end": "218620"
  },
  {
    "text": "to run massive distributed services that couldn't work with just a single machine database backing them they needed to be",
    "start": "218620",
    "end": "224560"
  },
  {
    "text": "able to stay up all the time to scale out across multiple machines and to survive all sorts of failures because",
    "start": "224560",
    "end": "230650"
  },
  {
    "text": "these systems need to be up all the time or else the companies are losing money and so it's not the people developing",
    "start": "230650",
    "end": "236019"
  },
  {
    "text": "you know big table or Cassandra or back back when they did we're throwing",
    "start": "236019",
    "end": "241299"
  },
  {
    "text": "out consistency in transactions because they didn't think they were valuable orcs they thought they were too confusing it was purely a matter of",
    "start": "241299",
    "end": "246459"
  },
  {
    "text": "prioritization that it was too hard to package them into the database and they thought availability and scalability",
    "start": "246459",
    "end": "251799"
  },
  {
    "text": "were more important which has absolutely been true you know a lot of great systems have been built on top of these",
    "start": "251799",
    "end": "257349"
  },
  {
    "text": "databases that don't have consistency or transactions you know it's powered a lot of powerful businesses but those",
    "start": "257349",
    "end": "264130"
  },
  {
    "text": "businesses and those services would have been easier to build if they could have relied on database a little bit more and so when",
    "start": "264130",
    "end": "271060"
  },
  {
    "start": "270000",
    "end": "299000"
  },
  {
    "text": "you're choosing a database today you have to make this choice you're picking between multiple options you're making a trade-off no you're either picking a",
    "start": "271060",
    "end": "277450"
  },
  {
    "text": "sequel database that is great for your developers but probably tough on your operations team who has to deal with",
    "start": "277450",
    "end": "282610"
  },
  {
    "text": "this lack of horizontal scalability or you're picking a no sequel database which is you know easier to operate in",
    "start": "282610",
    "end": "288340"
  },
  {
    "text": "in some ways but it's a lot tougher on your developers who have to make this extra effort to build around the",
    "start": "288340",
    "end": "295240"
  },
  {
    "text": "databases issues and that's where",
    "start": "295240",
    "end": "300310"
  },
  {
    "start": "299000",
    "end": "328000"
  },
  {
    "text": "cockroach DB is trying to come in that's why we're building this new system we want a database that's scalable that can",
    "start": "300310",
    "end": "306490"
  },
  {
    "text": "survive failures that is strongly consistent and that gives the standard sequel interface that everyone is",
    "start": "306490",
    "end": "311890"
  },
  {
    "text": "familiar with and it's all being built out in the open and open source out on",
    "start": "311890",
    "end": "316930"
  },
  {
    "text": "github and we think that this is something that will make building applications easier for everyone while",
    "start": "316930",
    "end": "323200"
  },
  {
    "text": "still making the database easy to operate for your reliability teams and",
    "start": "323200",
    "end": "328660"
  },
  {
    "start": "328000",
    "end": "344000"
  },
  {
    "text": "so I'm going to talk most of today about how the system works so how you can build a system that replicates data that",
    "start": "328660",
    "end": "335979"
  },
  {
    "text": "scales horizontally the survives failures while still providing consistency without having to sacrifice",
    "start": "335979",
    "end": "341590"
  },
  {
    "text": "performance and so I'm going to focus on three main areas kind of starting from",
    "start": "341590",
    "end": "347800"
  },
  {
    "start": "344000",
    "end": "359000"
  },
  {
    "text": "the core of the system and in terms of how data gets spread around a cluster of machines then getting into how we keep",
    "start": "347800",
    "end": "353680"
  },
  {
    "text": "that data consistent amongst them and how we provide arbitrary transactions on top of that and before we get into too",
    "start": "353680",
    "end": "361090"
  },
  {
    "start": "359000",
    "end": "439000"
  },
  {
    "text": "much detail it's worth understanding how the system is laid out it's built as a",
    "start": "361090",
    "end": "366100"
  },
  {
    "text": "series of layers where we have a sequel interface on top that the clients actually talk to that sequel interface",
    "start": "366100",
    "end": "371350"
  },
  {
    "text": "relies on a fully transactional consistent key value store underneath it so they're they're very separated",
    "start": "371350",
    "end": "377020"
  },
  {
    "text": "abstractions which is helps development a lot sorry yes yes you can be",
    "start": "377020",
    "end": "397990"
  },
  {
    "text": "distributed geographically over very long very long links it's you know it's",
    "start": "397990",
    "end": "404950"
  },
  {
    "text": "still a beta product right now we're hoping to go window in the next month or so yes so the system is split up into a",
    "start": "404950",
    "end": "412030"
  },
  {
    "text": "bunch of layers you know where the transactional key V key value store can rely on this data being distributed",
    "start": "412030",
    "end": "417460"
  },
  {
    "text": "across a bunch of machines and can rely on that data being replicated consistently and can rely on you know",
    "start": "417460",
    "end": "423430"
  },
  {
    "text": "let's thought a solid layer that rep that persists the data onto disk so we're gonna start by looking at this",
    "start": "423430",
    "end": "429310"
  },
  {
    "text": "middle layer that tracks where the data is amongst all the machines in the cluster and keeps track of it so that when a query comes in we know exactly",
    "start": "429310",
    "end": "436510"
  },
  {
    "text": "where the data that it wants is located and when you're working with a distributed system that is managing data",
    "start": "436510",
    "end": "443590"
  },
  {
    "text": "the first question you should typically ask is how is that day to split up onto the machines and how do you find any",
    "start": "443590",
    "end": "448750"
  },
  {
    "text": "given data that you want because that's very fundamental to how everything else within the system works and so when",
    "start": "448750",
    "end": "455200"
  },
  {
    "text": "you're asking how its split up and how its located there are two approaches that been used in a lot of previous systems kind of as prior art the first",
    "start": "455200",
    "end": "461800"
  },
  {
    "text": "is hashing which is seen in systems like Cassandra where to find where a given piece of data is located you hash the key using",
    "start": "461800",
    "end": "468010"
  },
  {
    "text": "some hash function that's predetermined and then use that resulting value to index into the machines in the cluster and this is great",
    "start": "468010",
    "end": "474520"
  },
  {
    "text": "because you know exactly where any given key is located just based on the value of the key you don't have any sort of separate lookup structure the problem is",
    "start": "474520",
    "end": "481450"
  },
  {
    "text": "that because it's a hashing function keys with similar values are mapped all over the place so you can't do",
    "start": "481450",
    "end": "486669"
  },
  {
    "text": "sequential scans efficiently at all which is a real showstopper for a sequel database which relies heavily on ordered indexes to do",
    "start": "486669",
    "end": "493210"
  },
  {
    "text": "lookups and joins and queries so we've had to go and cockroach with what's known as an order preserving approach",
    "start": "493210",
    "end": "500099"
  },
  {
    "text": "which is great for providing efficient scans but it requires an additional indexing structure and it looks a little",
    "start": "500099",
    "end": "505780"
  },
  {
    "text": "like this so you take the key space which is just the unicode alphabet and break it up into a bunch of chunks so",
    "start": "505780",
    "end": "512800"
  },
  {
    "text": "you pick arbitrary dividing lines that create reasonably trunks for your for your data and we",
    "start": "512800",
    "end": "518669"
  },
  {
    "text": "call each of those chunks a range because it's a range of the key space and so once you have that you don't know",
    "start": "518669",
    "end": "525000"
  },
  {
    "text": "where any of those chunks live because given a key you just know that it's in some chunk somewhere but that doesn't tell you where it is so you have to add",
    "start": "525000",
    "end": "531029"
  },
  {
    "text": "this range indexing structure on top which is a little more work than you need in a hash space system so this",
    "start": "531029",
    "end": "537420"
  },
  {
    "text": "range index simply keeps track of all the different ranges that are in the system and what machines they're currently located on and given that you",
    "start": "537420",
    "end": "544589"
  },
  {
    "text": "can do a lookup pretty easily by checking the index to find which range the key you want to find is in getting",
    "start": "544589",
    "end": "551339"
  },
  {
    "text": "the locations of that range within the cluster and then going to ask one of those locations for the key it also",
    "start": "551339",
    "end": "558089"
  },
  {
    "text": "makes range lookups really easy because you just find the range where the start key is in and the range where the end key is in and read all the ranges",
    "start": "558089",
    "end": "564180"
  },
  {
    "text": "between them another nice little perk of splitting up your data in this ordered",
    "start": "564180",
    "end": "570510"
  },
  {
    "text": "way is that when you get too much data in a range you can just split it in half you know pick some point in the middle",
    "start": "570510",
    "end": "576240"
  },
  {
    "text": "break it into two new ranges and update your indexing structure in some consistent manner in a hash based",
    "start": "576240",
    "end": "581760"
  },
  {
    "text": "approach if you end up with too much data on one machine you have to either add a bunch more machines so that the data falls elsewhere or change your",
    "start": "581760",
    "end": "587640"
  },
  {
    "text": "hashing function in some way which can be much trickier operation than just splitting a range in half and so once",
    "start": "587640",
    "end": "595440"
  },
  {
    "text": "you've split this range in half they can each be managed separately moved around to different machines independently and",
    "start": "595440",
    "end": "601560"
  },
  {
    "text": "of course we keep multiple copies of you changed yes so all of this is within the",
    "start": "601560",
    "end": "611640"
  },
  {
    "text": "system it's it's it's invisible to the user data can be split up automatically",
    "start": "611640",
    "end": "616980"
  },
  {
    "text": "by the system so that they're in reasonably sized chunks the user can also request to have us pre split ranges it for performance reasons if they'd",
    "start": "616980",
    "end": "622890"
  },
  {
    "text": "like but in general we'll split things when the data gets too large to be in a single range for performance reasons and",
    "start": "622890",
    "end": "628320"
  },
  {
    "text": "availability reasons and the index gets automatically updated all the time when this happens and it's all invisible to",
    "start": "628320",
    "end": "633959"
  },
  {
    "text": "the user does that answer your question so we need to keep multiple copies of",
    "start": "633959",
    "end": "639360"
  },
  {
    "text": "everything because this is a distributed system machines can fail we need to plan for that so by default we keep three",
    "start": "639360",
    "end": "644760"
  },
  {
    "text": "copies of every range you can configure it to be more if you'd like and we rather than having some external",
    "start": "644760",
    "end": "652170"
  },
  {
    "text": "control process that manages where all the data is located it's built into the nodes themselves so each node is",
    "start": "652170",
    "end": "658650"
  },
  {
    "text": "gossiping information about itself so that you know node one has some idea of how much data is on node four how many",
    "start": "658650",
    "end": "664770"
  },
  {
    "text": "ranges that are there how much traffic is getting etc so if a new node joins",
    "start": "664770",
    "end": "669930"
  },
  {
    "text": "the cluster all of a sudden nodes one through three see there's this new node that has nothing on it I have a bunch of",
    "start": "669930",
    "end": "675360"
  },
  {
    "text": "data on me I'm gonna share some of my data with it to lighten my load and raise its load so this is a decentralized process based on",
    "start": "675360",
    "end": "681930"
  },
  {
    "text": "a gossip protocol and it looks you know it all happens in a very orderly fashion",
    "start": "681930",
    "end": "686970"
  },
  {
    "text": "where a node will say node four will you take some of this data off my hand so I'm gonna give you my copy of this range",
    "start": "686970",
    "end": "692910"
  },
  {
    "text": "node 4 will generally accept it and take it and then node 3 can get rid of it which lightens the load on node 3 and",
    "start": "692910",
    "end": "700050"
  },
  {
    "text": "makes it that if a node fails there will be less recovery that's needed and",
    "start": "700050",
    "end": "705830"
  },
  {
    "text": "similarly this will continue happening until the replica thinks or the system thinks that it's mostly balanced so",
    "start": "705830",
    "end": "711750"
  },
  {
    "text": "ekend range will get replicated over there and at this point the cluster is mostly balanced and the same thing",
    "start": "711750",
    "end": "717900"
  },
  {
    "text": "happens if a node fails just a little bit faster and more urgently so if a node fails then range is 1 and 3 no",
    "start": "717900",
    "end": "727589"
  },
  {
    "text": "longer have enough copies so the nodes that do have a copy of them will quickly try to find a node to copy them to so if",
    "start": "727589",
    "end": "735060"
  },
  {
    "text": "no 2 fails here the ranges that lost a copy will be up replicated where there",
    "start": "735060",
    "end": "740220"
  },
  {
    "text": "is space in the cluster and at this point the old node can just be forgotten about and the system is rebalanced and",
    "start": "740220",
    "end": "748050"
  },
  {
    "text": "so this is always happening in the background you know you're issuing queries it's just happening you know separately and it's it's it's done in",
    "start": "748050",
    "end": "756630"
  },
  {
    "text": "such a way that you know it doesn't happen all at once and disrupt a bunch of user queries when it happens it's done in a controlled fashion so for all",
    "start": "756630",
    "end": "764940"
  },
  {
    "start": "763000",
    "end": "889000"
  },
  {
    "text": "this we make the query the range is about 64 megabytes in size which is just",
    "start": "764940",
    "end": "770070"
  },
  {
    "text": "an experimentally determined value that performs pretty well it's kind of a fight between making them smaller which",
    "start": "770070",
    "end": "776550"
  },
  {
    "text": "makes them easier to replicate around the cluster and cheaper to do so and cheaper to split into multiple hats but",
    "start": "776550",
    "end": "781980"
  },
  {
    "text": "you don't want to have some any ranges that you're indexing overhead gets huge because this indexing table",
    "start": "781980",
    "end": "787139"
  },
  {
    "text": "has a bunch of entries and typically the entries are cached on each node so that lookups are faster and if you have too",
    "start": "787139",
    "end": "792870"
  },
  {
    "text": "many index entries here then lookups can be more expensive if the caches need to be bigger it slows down the system a",
    "start": "792870",
    "end": "799980"
  },
  {
    "text": "little and so this method of splitting up data into ranges and distributing them around a pool of nodes is you know",
    "start": "799980",
    "end": "806490"
  },
  {
    "text": "has been in a bunch of a bunch of pre-existing systems most notably such things like HBase or the Google",
    "start": "806490",
    "end": "811560"
  },
  {
    "text": "databases and that's how we keep data spread around the cluster are there any",
    "start": "811560",
    "end": "817199"
  },
  {
    "text": "questions on that before I move on to how we keep data in sync so let's take one or two questions here yeah so the",
    "start": "817199",
    "end": "825630"
  },
  {
    "text": "index structure is is spread around the different nodes in the cluster but each node keeps a cache of where it thinks",
    "start": "825630",
    "end": "831120"
  },
  {
    "text": "ranges are located which improves performance on common lookups ya know so",
    "start": "831120",
    "end": "845250"
  },
  {
    "text": "there are a couple so the this indexing structure is actually kept as a range in the cluster so it's located on three",
    "start": "845250",
    "end": "851370"
  },
  {
    "text": "five seven however many nodes and then when a note like another node receives a request to do a lookup it has to check",
    "start": "851370",
    "end": "857399"
  },
  {
    "text": "with the index to find where that is but as a basic optimization keeps a cache of the the ranges that it looked up",
    "start": "857399",
    "end": "863279"
  },
  {
    "text": "recently yeah there are a bunch of",
    "start": "863279",
    "end": "872339"
  },
  {
    "text": "consistency check sounds why don't we move on and I'll take more questions at the end on more general topics so once",
    "start": "872339",
    "end": "879810"
  },
  {
    "text": "you have a bunch of data in your cluster as soon as you have multiple copies of it the immediate question is what",
    "start": "879810",
    "end": "885870"
  },
  {
    "text": "happens if they get out of sync or how do you keep them from getting out of sync because when you only have one copy it's really easy you know exactly what",
    "start": "885870",
    "end": "891690"
  },
  {
    "start": "889000",
    "end": "970000"
  },
  {
    "text": "the truth is you have multiple copies they can get out of sync one of them can get corrupted as was mentioned and so you have these multiple copies but you",
    "start": "891690",
    "end": "899279"
  },
  {
    "text": "have to keep them in sync a traditional solution is to use a primary secondary replication this is what you see in most",
    "start": "899279",
    "end": "904290"
  },
  {
    "text": "traditional social databases some more modern systems have started using what's called consensus this is what's used in",
    "start": "904290",
    "end": "910139"
  },
  {
    "text": "@cd which backs kubernetes and primary secondary application is a",
    "start": "910139",
    "end": "916670"
  },
  {
    "text": "very simple idea on the surface the idea is that you have a primary note at all times it receives all the writes it writes its",
    "start": "916670",
    "end": "923480"
  },
  {
    "text": "incoming requests to disk and sends a belong to some secondary which also writes them it sounds great but it has a",
    "start": "923480",
    "end": "929449"
  },
  {
    "text": "lot of practical complications when you actually try to run it and you have to deal with failure isn't all of that so",
    "start": "929449",
    "end": "935000"
  },
  {
    "text": "right off the bat you have to choose whether you're doing your replications synchronously or asynchronously if you",
    "start": "935000",
    "end": "940550"
  },
  {
    "text": "choose the asynchronous approach then the data on your secondary is potentially out of date and if you're doing reads from it you'll get stale",
    "start": "940550",
    "end": "946009"
  },
  {
    "text": "reads and if you have a failure it might not have all the most recent data if",
    "start": "946009",
    "end": "951110"
  },
  {
    "text": "you're doing synchronous replication meaning that your secondary has to acknowledge rights before the primary can acknowledge the write to a client",
    "start": "951110",
    "end": "957019"
  },
  {
    "text": "then you're lowering your availability by requiring the secondary to acknowledge rights you're increasing your latency by adding a round-trip to",
    "start": "957019",
    "end": "963139"
  },
  {
    "text": "every write and it's again very tricky on failover and it's really hard to get this right",
    "start": "963139",
    "end": "969500"
  },
  {
    "text": "in practice and so consensus tries to make this simpler in practice by making",
    "start": "969500",
    "end": "974690"
  },
  {
    "start": "970000",
    "end": "1143000"
  },
  {
    "text": "it harder to understand up front yeah it's kind of flipping the tables a",
    "start": "974690",
    "end": "979819"
  },
  {
    "text": "little bit you know there's a whole bunch of consensus research that's been done there you see systems like Peck so",
    "start": "979819",
    "end": "985850"
  },
  {
    "text": "it's likely you stamp replication like graft we use raft and cockroach DB and the general idea of consensus is that",
    "start": "985850",
    "end": "993680"
  },
  {
    "text": "you're replicating to a usually an odd number of machines and something is considered committed when a majority of",
    "start": "993680",
    "end": "999680"
  },
  {
    "text": "them agree that it should be committed and so this is used in a bunch of modern database systems as you see there and it",
    "start": "999680",
    "end": "1006130"
  },
  {
    "text": "looks a little bit like this in raft a right comes into the leader a leader is",
    "start": "1006130",
    "end": "1011319"
  },
  {
    "text": "usually elected as a performance optimization so that all rights go through the leader and you don't have to",
    "start": "1011319",
    "end": "1017529"
  },
  {
    "text": "deal with multiple things proposing and conflicting data so at all times a leader has been elected by the quorum if",
    "start": "1017529",
    "end": "1023500"
  },
  {
    "text": "the leader gets a right it replicates it onto a follower as soon as any of the followers have written that right to",
    "start": "1023500",
    "end": "1029409"
  },
  {
    "text": "disk the data is considered committed even before anything's been acknowledged as soon as it's on disk the data is",
    "start": "1029409",
    "end": "1034569"
  },
  {
    "text": "committed and then in the usual case the acknowledgement will come back and the leader can acknowledge the client",
    "start": "1034569",
    "end": "1040709"
  },
  {
    "text": "notice that this runs as fast as the fastest quorum of nodes if you have a",
    "start": "1040709",
    "end": "1045970"
  },
  {
    "text": "couple slow nodes on the tail it's not a big you can have two nodes you know running together in Germany another node in",
    "start": "1045970",
    "end": "1051260"
  },
  {
    "text": "Australia and it doesn't matter if that third node is really far away your commits can proceed as fast as your two",
    "start": "1051260",
    "end": "1056960"
  },
  {
    "text": "replicas that are closest together so you can get really good performance on all your reads and writes in Germany and",
    "start": "1056960",
    "end": "1062120"
  },
  {
    "text": "just leave Australia catch up in the background so this is fundamentally a CP",
    "start": "1062120",
    "end": "1078770"
  },
  {
    "text": "system and under the cap theorem so in a court in a partition the majority side of the quorum will be the only one that can can you continue making progress so",
    "start": "1078770",
    "end": "1085700"
  },
  {
    "text": "if the leader gets partitioned from the other nodes its leadership will you won't be able to commit anything because it won't be able to get any acknowledgments and its leadership will",
    "start": "1085700",
    "end": "1092059"
  },
  {
    "text": "run out and once it runs out the other nodes can elect a new leader for the given consensus group and continue",
    "start": "1092059",
    "end": "1097940"
  },
  {
    "text": "making progress and any rights or reads that they receive and of course in all",
    "start": "1097940",
    "end": "1103760"
  },
  {
    "text": "these systems the interesting part is what happens when things go wrong and so if you have the same setup and a write",
    "start": "1103760",
    "end": "1109370"
  },
  {
    "text": "comes in but the leader crashes after it writes its disk then what will happen is that one of the other nodes will be",
    "start": "1109370",
    "end": "1115700"
  },
  {
    "text": "elected as a leader and so the second node here got elected as a leader and it",
    "start": "1115700",
    "end": "1120919"
  },
  {
    "text": "never got the key cherry so the state of truth in this cluster right now is that cherry is not part of these systems",
    "start": "1120919",
    "end": "1126140"
  },
  {
    "text": "because only one of the three nodes has it so if a read comes in for that key the second node the new leader will be",
    "start": "1126140",
    "end": "1132140"
  },
  {
    "text": "able to say that that key does not exist and they know that has it will be corrected in the background so the the",
    "start": "1132140",
    "end": "1138080"
  },
  {
    "text": "read that comes in for cherry will return that it does not exist in the system and as I mentioned we use raft we",
    "start": "1138080",
    "end": "1145220"
  },
  {
    "start": "1143000",
    "end": "1205000"
  },
  {
    "text": "share the implementation with that CD so they started it out we've been contributing to it along the way",
    "start": "1145220",
    "end": "1150399"
  },
  {
    "text": "comments you know really really well hardened implementation we run one consensus group for each range of data",
    "start": "1150399",
    "end": "1157399"
  },
  {
    "text": "so in that past example with the three and four nut read for nodes where we had three ranges each of those ranges is a",
    "start": "1157399",
    "end": "1163520"
  },
  {
    "text": "separate consensus group and in a real cluster since they're each only 64 megabytes you can easily get thousands",
    "start": "1163520",
    "end": "1170000"
  },
  {
    "text": "tens of thousands hundreds of thousands of these ranges which is a lot of consensus groups to be managing and",
    "start": "1170000",
    "end": "1175130"
  },
  {
    "text": "that's where we differ from at CD quite a bit because their whole system is one consensus group so we have to have a lot",
    "start": "1175130",
    "end": "1180649"
  },
  {
    "text": "bunch of engineering to work around this to avoid overloading the network with all sorts of raft",
    "start": "1180649",
    "end": "1185930"
  },
  {
    "text": "background traffic but Raph provides an incredibly solid foundation for the",
    "start": "1185930",
    "end": "1191330"
  },
  {
    "text": "system even though there are some engineering complications involved in using it we can truly rely on any right to RF group being atomic and durable",
    "start": "1191330",
    "end": "1198190"
  },
  {
    "text": "it's either committed or it isn't and once it's committed it sits pretty reliably there so this is a great",
    "start": "1198190",
    "end": "1204260"
  },
  {
    "text": "foundation for the system the issue is that since we're doing a different consensus group for each range of data",
    "start": "1204260",
    "end": "1210580"
  },
  {
    "text": "we only get you know atomic committing for each range you can't you know",
    "start": "1210580",
    "end": "1217010"
  },
  {
    "text": "without building something on top atomically commit something that touches data in multiple ranges so if you're writing the keys ad and Z they're",
    "start": "1217010",
    "end": "1223310"
  },
  {
    "text": "probably not all in the same range we need to do something on top to make those separate commits those separate",
    "start": "1223310",
    "end": "1228380"
  },
  {
    "text": "transactions actually be atomic and that's where the distributed transaction protocol comes in and so this is",
    "start": "1228380",
    "end": "1235580"
  },
  {
    "start": "1235000",
    "end": "1294000"
  },
  {
    "text": "something that's been built somewhat homegrown somewhat based on research it's kind of a mesh of the two but the",
    "start": "1235580",
    "end": "1242720"
  },
  {
    "text": "core problem is that we want to support traditional acid semantics for sequel transactions and we need everything to",
    "start": "1242720",
    "end": "1247880"
  },
  {
    "text": "be atomic consistent isolated from each other and durable we default to the",
    "start": "1247880",
    "end": "1253400"
  },
  {
    "text": "serializable isolation level which is the this first isolation level that provides true isolation that doesn't",
    "start": "1253400",
    "end": "1259070"
  },
  {
    "text": "have any interactions between between queries this is actually stronger than",
    "start": "1259070",
    "end": "1264920"
  },
  {
    "text": "some single no databases default to it which is kind of funny and in order to do this we can't just copy what single",
    "start": "1264920",
    "end": "1271760"
  },
  {
    "text": "node databases do we have different constraints you know we can't rely on a single disk write to you know a time a",
    "start": "1271760",
    "end": "1277730"
  },
  {
    "text": "click emit something because we have multiple disks potentially spread across the world we can't use distributed",
    "start": "1277730",
    "end": "1283010"
  },
  {
    "text": "locking very easily because it's very expensive relative to single machine locking and so the transaction the",
    "start": "1283010",
    "end": "1289280"
  },
  {
    "text": "transaction protocol looks a little different from what you see in a single node system some of the things that we",
    "start": "1289280",
    "end": "1295610"
  },
  {
    "start": "1294000",
    "end": "1357000"
  },
  {
    "text": "do to work around this are to rely on an a raft group right being somewhat analogous to",
    "start": "1295610",
    "end": "1301850"
  },
  {
    "text": "a single disk block right so when a transaction comes in we create a record for it that record has some ID for the",
    "start": "1301850",
    "end": "1309650"
  },
  {
    "text": "transaction has its state some other metadata and when we do rights within the transaction",
    "start": "1309650",
    "end": "1314930"
  },
  {
    "text": "we tagged the right the written data with that ID to indicate that it might not be committed yet but if you want to check whether it's committed go look up",
    "start": "1314930",
    "end": "1320660"
  },
  {
    "text": "that transaction ID see whether or not it's done and then when the transaction finishes we just mark that single",
    "start": "1320660",
    "end": "1326990"
  },
  {
    "text": "transaction record which lives within a raft group as finished and so that raft group right is atomic and so that makes",
    "start": "1326990",
    "end": "1333920"
  },
  {
    "text": "the transaction commit atomic and as for locking we have to use an optimistic",
    "start": "1333920",
    "end": "1339740"
  },
  {
    "text": "concurrency control protocol instead which effectively amounts to detecting",
    "start": "1339740",
    "end": "1345380"
  },
  {
    "text": "conflicts as they occur rather than using locks to prevent them from occurring so we have to detect them and",
    "start": "1345380",
    "end": "1351380"
  },
  {
    "text": "then abort one of the transactions or retry one of the transactions as necessary and so I'm gonna go through a",
    "start": "1351380",
    "end": "1358550"
  },
  {
    "start": "1357000",
    "end": "1523000"
  },
  {
    "text": "couple small examples here just to give an idea of what it looks like in the simple case where everything goes well a",
    "start": "1358550",
    "end": "1364690"
  },
  {
    "text": "transaction comes in we create a record for it somewhere usually on the same range as the first right for efficiency",
    "start": "1364690",
    "end": "1370970"
  },
  {
    "text": "reasons you can go write multiple keys that are on different ranges and when you want to commit you just flip that",
    "start": "1370970",
    "end": "1377150"
  },
  {
    "text": "transaction record to committed and so",
    "start": "1377150",
    "end": "1382190"
  },
  {
    "text": "that makes all the data considered committed and durable after the fact we",
    "start": "1382190",
    "end": "1388160"
  },
  {
    "text": "usually go back and clean up those those rights that had a transaction ID attached to them which makes later reads",
    "start": "1388160",
    "end": "1393470"
  },
  {
    "text": "on those keys more efficient but it's not needed for correctness purposes and",
    "start": "1393470",
    "end": "1399170"
  },
  {
    "text": "so again that's that's what happens when things go well the interesting part of the transaction protocol is what happens",
    "start": "1399170",
    "end": "1405020"
  },
  {
    "text": "when there are conflicts and when things go wrong so in a read conflict a transaction starts and writes a key when",
    "start": "1405020",
    "end": "1410990"
  },
  {
    "text": "other transaction comes in and tries to read that key and it sees that there's this transaction ID still attached to",
    "start": "1410990",
    "end": "1417230"
  },
  {
    "text": "that written key which means that it might not be committed it has to go check that transaction record to",
    "start": "1417230",
    "end": "1422240"
  },
  {
    "text": "determine whether the data is committed so that goes up in checks and sees that",
    "start": "1422240",
    "end": "1427550"
  },
  {
    "text": "the transaction is pending and in this case the read can ignore that uncommitted value can go on and commit",
    "start": "1427550",
    "end": "1436660"
  },
  {
    "text": "what what's interesting here is that there are some time stamps involved here so if transaction 2 is timestamp is less",
    "start": "1436660",
    "end": "1442700"
  },
  {
    "text": "than transaction ones it can't just go ahead or it can just go ahead and read the value but if",
    "start": "1442700",
    "end": "1449149"
  },
  {
    "text": "action twos timestamp is greater than transaction one says initially and might have to push the timestamp back in time",
    "start": "1449149",
    "end": "1454159"
  },
  {
    "text": "because it'd be illegal to return that the key doesn't exist if a transaction later comes in and commits that key at",
    "start": "1454159",
    "end": "1459440"
  },
  {
    "text": "an earlier time stamp so with distributed transactions the timestamps can get a little fuzzy but we're able to",
    "start": "1459440",
    "end": "1466279"
  },
  {
    "text": "to manage the timestamps as necessary to ensure correctness and a right conflict",
    "start": "1466279",
    "end": "1473089"
  },
  {
    "text": "and we have two transaction records both try to write the same key so in this case one of them is going to have to be",
    "start": "1473089",
    "end": "1479450"
  },
  {
    "text": "aborted both transactions can't write the same key at the same time you know",
    "start": "1479450",
    "end": "1485749"
  },
  {
    "text": "in a single node database the second transaction would just get blocked on the lock just like in the previous read",
    "start": "1485749",
    "end": "1491419"
  },
  {
    "text": "example the read would just get blocked on transaction ones lock but since we don't have locks one of these",
    "start": "1491419",
    "end": "1496999"
  },
  {
    "text": "transactions has to be aborted in order to to complete successfully so based on",
    "start": "1496999",
    "end": "1502399"
  },
  {
    "text": "some randomly generated priorities you know transaction two will decide whether or not to abort itself or to abort the",
    "start": "1502399",
    "end": "1508399"
  },
  {
    "text": "existing right in this case it aborts itself over writes the key and it then",
    "start": "1508399",
    "end": "1514549"
  },
  {
    "text": "commits and transaction one will come back and see that got aborted it will either have to internally retry or return an error to the client telling",
    "start": "1514549",
    "end": "1520489"
  },
  {
    "text": "the client to retry but that's you know the gist of how transactions work it's",
    "start": "1520489",
    "end": "1528409"
  },
  {
    "text": "you know there are a lot of fuzzy details but the core principle is let everything proceed detect when something",
    "start": "1528409",
    "end": "1534499"
  },
  {
    "text": "is going wrong and and abort one of the transactions if something has become a conflict and so there are a lot more",
    "start": "1534499",
    "end": "1541039"
  },
  {
    "text": "details than what I've been able to cover in this in this talk but we have a blog that's full of technical details if",
    "start": "1541039",
    "end": "1547549"
  },
  {
    "text": "you're curious to learn more and so these are some some general techniques",
    "start": "1547549",
    "end": "1552859"
  },
  {
    "text": "that we've been able to use to build a database that is very cloud native and in the way that it replicates data",
    "start": "1552859",
    "end": "1558820"
  },
  {
    "text": "scales horizontally doesn't care about individual nodes in particular but but",
    "start": "1558820",
    "end": "1565129"
  },
  {
    "text": "still provide you know these real valuable guarantees on top and so real",
    "start": "1565129",
    "end": "1571460"
  },
  {
    "start": "1570000",
    "end": "1648000"
  },
  {
    "text": "quick just want to mention how to deploy it if you want to try it out in the interest of being cloud native it's a",
    "start": "1571460",
    "end": "1577669"
  },
  {
    "text": "single statically compiled binary written and go the UI is packaged in so you can just hit a different port on the",
    "start": "1577669",
    "end": "1582860"
  },
  {
    "text": "binary to get an admin UI that gives you all sorts of insight into what's running in the cluster now a client to open up a",
    "start": "1582860",
    "end": "1588740"
  },
  {
    "text": "sequel shell is built in each node in the cluster is symmetric so there's no special roles you spin them all up and",
    "start": "1588740",
    "end": "1595070"
  },
  {
    "text": "they're all considered equal so you know there's no master that you need to protect you can let any node fail it",
    "start": "1595070",
    "end": "1601520"
  },
  {
    "text": "doesn't matter at all and it's just gonna balance and heal no matter which one you get rid of and so yeah starting",
    "start": "1601520",
    "end": "1608870"
  },
  {
    "text": "a cluster is super easy now just run cockroach start on a machine if you want to join more machines in the cluster",
    "start": "1608870",
    "end": "1614090"
  },
  {
    "text": "just start them up with the address of any of the nodes in the cluster and they'll connect gossip their information",
    "start": "1614090",
    "end": "1619280"
  },
  {
    "text": "and join the join the the consensus groups now we have a docker image we have a kubernetes configuration we have",
    "start": "1619280",
    "end": "1625970"
  },
  {
    "text": "a helmet art if you're into home we use a stateful set with kubernetes along with things like pod disruption budgets",
    "start": "1625970",
    "end": "1632120"
  },
  {
    "text": "that got added in one v to ensure that you know autom automated upgrades won't take down too many nodes at once these",
    "start": "1632120",
    "end": "1639710"
  },
  {
    "text": "configurations are all fairly solid how much time do we get 3:10 cool so I'll",
    "start": "1639710",
    "end": "1649130"
  },
  {
    "start": "1648000",
    "end": "1678000"
  },
  {
    "text": "skip a demo and leave some more time for questions if you guys want to ask me anything thank you guys",
    "start": "1649130",
    "end": "1656580"
  },
  {
    "text": "[Applause]",
    "start": "1656580",
    "end": "1662059"
  },
  {
    "text": "good all the rights go through the",
    "start": "1663020",
    "end": "1668970"
  },
  {
    "text": "leader for the range yes yes so for",
    "start": "1668970",
    "end": "1678780"
  },
  {
    "start": "1678000",
    "end": "1760000"
  },
  {
    "text": "correctness reads do have to go to the current leader and actually it's even a little stronger than that they have to",
    "start": "1678780",
    "end": "1684840"
  },
  {
    "text": "go to what we call a current lease holder which is a mechanism built on top of raft because otherwise without this",
    "start": "1684840",
    "end": "1690300"
  },
  {
    "text": "extra safety that we add in even sending all reads to the leader would be",
    "start": "1690300",
    "end": "1695520"
  },
  {
    "text": "sometimes incorrect unless those reads also go through raft which is an issue that's come up with the number of systems including STD so",
    "start": "1695520",
    "end": "1702240"
  },
  {
    "text": "reads do have to go to the leader there is some work that we're interested in doing in the future using something called egalitarian Paxos which would",
    "start": "1702240",
    "end": "1709110"
  },
  {
    "text": "enable things to not all have to go through a leader but for now and for the foreseeable future everything will have",
    "start": "1709110",
    "end": "1715050"
  },
  {
    "text": "to go through the leader yeah yeah so",
    "start": "1715050",
    "end": "1721730"
  },
  {
    "text": "Kyle worked with us last fall and you're going to hold a whole bunch of tests against us he found two bugs that he was",
    "start": "1721730",
    "end": "1729720"
  },
  {
    "text": "able to generate inconsistencies with after you know tons and tons of pounding and they were both you know logic errors",
    "start": "1729720",
    "end": "1735960"
  },
  {
    "text": "that that better unit testing could have caught and so after those two bugs were fixed he wasn't able to find any true",
    "start": "1735960",
    "end": "1742200"
  },
  {
    "text": "faults so at this point there's no known consistency issues so he was testing the",
    "start": "1742200",
    "end": "1747570"
  },
  {
    "text": "fact that transactions are serializable and all data is consistent and so the results were really great they're all",
    "start": "1747570",
    "end": "1753720"
  },
  {
    "text": "there than those two bugs he found in the back",
    "start": "1753720",
    "end": "1758149"
  },
  {
    "text": "so it is a CP system we're not claiming that in a during a partition all nodes",
    "start": "1760649",
    "end": "1766809"
  },
  {
    "text": "will be able to make forward progress if you have a partition the larger side of the partition should be able to make",
    "start": "1766809",
    "end": "1772570"
  },
  {
    "text": "forward progress but anything on a smaller side is unlikely to be able to do so so it's fully a CP system but by",
    "start": "1772570",
    "end": "1779759"
  },
  {
    "text": "you know replicating data we make it so that individual things going down doesn't doesn't hurt availability you",
    "start": "1779759",
    "end": "1786549"
  },
  {
    "text": "know if you're replicating your data across data centers entire data center failures won't bring your system down but it's still technically a CP system",
    "start": "1786549",
    "end": "1794940"
  },
  {
    "text": "[Music]",
    "start": "1795890",
    "end": "1799039"
  },
  {
    "text": "yeah so you know things like graft have made this much easier than they would have been five years ago because",
    "start": "1803999",
    "end": "1809169"
  },
  {
    "text": "previously you'd have to try to implement Paxos which is a very tough task but now you can find a good",
    "start": "1809169",
    "end": "1814360"
  },
  {
    "text": "implementation of Raft in all sorts of languages and build a system based on that I mean there's a ton of engineering",
    "start": "1814360",
    "end": "1820149"
  },
  {
    "text": "involved but it can work as a as a system if you put in enough time and optimize it right yes so that's",
    "start": "1820149",
    "end": "1836139"
  },
  {
    "text": "something that we've been working on a lot over the last six months or so it's what we're calling distributed sequel where we basically take sequel queries",
    "start": "1836139",
    "end": "1843129"
  },
  {
    "text": "that are touching a lot of data and turn them into something akin to a MapReduce where we farm out different parts of the",
    "start": "1843129",
    "end": "1848889"
  },
  {
    "text": "query to different machines and able it to all happen in parallel it's still slower than a single machine of course",
    "start": "1848889",
    "end": "1854440"
  },
  {
    "text": "because we are you know moving large amounts of data across the cluster and we offer a feature called interleaved",
    "start": "1854440",
    "end": "1860230"
  },
  {
    "text": "tables such that if one table has a foreign key constraint on another table you can actually interleave it",
    "start": "1860230",
    "end": "1866740"
  },
  {
    "text": "underneath the table that it has the foreign key constraint on so that joins on that foreign key are very very cheap",
    "start": "1866740",
    "end": "1872889"
  },
  {
    "text": "because the data is all co-located within the same range so that can help with certain use cases but in the general case yes joins can be can be",
    "start": "1872889",
    "end": "1879789"
  },
  {
    "text": "slow even with the distributed",
    "start": "1879789",
    "end": "1883289"
  },
  {
    "text": "yes so all adding fruit for nodes or for",
    "start": "1889700",
    "end": "1895980"
  },
  {
    "start": "1890000",
    "end": "1970000"
  },
  {
    "text": "ranges right yeah so for adding nodes adding a new node is pretty simple",
    "start": "1895980",
    "end": "1901440"
  },
  {
    "text": "because when you add a node it has nothing on it and so like it just introduces itself to one of the nodes in the cluster it gets information about",
    "start": "1901440",
    "end": "1907679"
  },
  {
    "text": "the cluster via that gossip Network and and once it's in that gossip network other nodes will start giving it data",
    "start": "1907679",
    "end": "1913460"
  },
  {
    "text": "splitting arrange to create a new range or creating a new range from scratch has",
    "start": "1913460",
    "end": "1919440"
  },
  {
    "text": "to go through the raft consensus protocols to ensure safety and so Etsy d",
    "start": "1919440",
    "end": "1924830"
  },
  {
    "text": "similarly has logic you know in the raft implementation for adding new members removing members in a safe way yeah so",
    "start": "1924830",
    "end": "1943350"
  },
  {
    "text": "it scales linearly we've done testing on that up to 64 notes we're adding each new node as a linear amount of",
    "start": "1943350",
    "end": "1948660"
  },
  {
    "text": "additional write and read throughput are using for a ballute numbers depend a lot on what your workload is and on what",
    "start": "1948660",
    "end": "1954270"
  },
  {
    "text": "kinds of machines you have but we've hit a hundred thousand QPS on a recent ycs b test and it does scale and yearly as",
    "start": "1954270",
    "end": "1961590"
  },
  {
    "text": "long as you're not adding more and more contention as you scale up in the back",
    "start": "1961590",
    "end": "1967940"
  },
  {
    "text": "so they don't currently it's something that we could add with a fat client but right now we're just expecting users to",
    "start": "1969989",
    "end": "1976690"
  },
  {
    "text": "use their existing postgrads clients we support a bunch of common ORM s yes so",
    "start": "1976690",
    "end": "1987339"
  },
  {
    "text": "connecting through H a proxy or something like that is recommended so that you can you can connect to any node in the cluster and query for any data",
    "start": "1987339",
    "end": "1992679"
  },
  {
    "text": "and that works but if you are connected to only a single node then that client stops working as soon as that single",
    "start": "1992679",
    "end": "1997779"
  },
  {
    "text": "node is not available here yes yeah",
    "start": "1997779",
    "end": "2004049"
  },
  {
    "text": "absolutely",
    "start": "2004049",
    "end": "2006229"
  },
  {
    "text": "so yeah so highly contended workloads don't work great in the naive design",
    "start": "2017550",
    "end": "2023220"
  },
  {
    "text": "we've added some optimizations here that adds some amount of weighting kind of simulating locks such that if you're",
    "start": "2023220",
    "end": "2029700"
  },
  {
    "text": "blocked on something that another transaction is currently writing to that it will wait for a short period of time before aborting and this is drastically",
    "start": "2029700",
    "end": "2036810"
  },
  {
    "text": "improved performance on highly highly contended workloads and that was one of the issues in the Jepsen testing was",
    "start": "2036810",
    "end": "2042030"
  },
  {
    "text": "that a fear was was hammering single rows with all of his rights and so they",
    "start": "2042030",
    "end": "2047070"
  },
  {
    "text": "were all waiting on each other I mean being aborted but adding adding the short waiting period drastically",
    "start": "2047070",
    "end": "2053340"
  },
  {
    "text": "improved the amount of retries and throughput that was able to be done yes",
    "start": "2053340",
    "end": "2062149"
  },
  {
    "text": "I mean we would definitely recommend migrating from postcards to cockroach like we know that's one of the big",
    "start": "2066639",
    "end": "2073539"
  },
  {
    "start": "2068000",
    "end": "2123000"
  },
  {
    "text": "benefits of using the same protocol so so we don't support every last feature",
    "start": "2073539",
    "end": "2079809"
  },
  {
    "text": "we've had a big push to make our ends work the object relational mapping libraries that are in a bunch of",
    "start": "2079809",
    "end": "2085450"
  },
  {
    "text": "languages and some of them use some very obscure edges of postgrads and so we've been over time adding more and more of",
    "start": "2085450",
    "end": "2091599"
  },
  {
    "text": "those edges but we certainly haven't hit all of them yet so we expect most applications to work",
    "start": "2091599",
    "end": "2097059"
  },
  {
    "text": "but it's very possible you'll hit a feature that we have not yet implemented and we hope you'll tell us and we implement it usually it's not that big",
    "start": "2097059",
    "end": "2102430"
  },
  {
    "text": "of a deal to throw them in yeah so we",
    "start": "2102430",
    "end": "2108700"
  },
  {
    "text": "have up cert and we do have one doing queries but yeah there are some some interesting features yeah so we've",
    "start": "2108700",
    "end": "2123940"
  },
  {
    "start": "2123000",
    "end": "2198000"
  },
  {
    "text": "received a bunch of funding but or announced a couple months ago what are our main business model will be which is",
    "start": "2123940",
    "end": "2128980"
  },
  {
    "text": "that well almost all of the product is Apache to licensed certain features that we don't think any startup would ever",
    "start": "2128980",
    "end": "2134410"
  },
  {
    "text": "need the only enterprises would need are going under a separate license so there's still open source but they're",
    "start": "2134410",
    "end": "2140019"
  },
  {
    "text": "under what we call the cockroach community license and so these features that are planned right now are distributed back up in your store rather",
    "start": "2140019",
    "end": "2146200"
  },
  {
    "text": "than single node like a slightly slower back up in restore that's widely available and then per row geographical",
    "start": "2146200",
    "end": "2153130"
  },
  {
    "text": "placement of your data so right now you can configure where your data lives on a table level so you say this table lives in these data centers or in these",
    "start": "2153130",
    "end": "2158980"
  },
  {
    "text": "countries or whatnot at the enterprise level it'll be you can do that on a per row basis based on their you know",
    "start": "2158980",
    "end": "2164859"
  },
  {
    "text": "certain columns in your in your rows",
    "start": "2164859",
    "end": "2168298"
  },
  {
    "text": "using what sequences ah yeah so what would yeah I I don't have a good answer",
    "start": "2176020",
    "end": "2184400"
  },
  {
    "text": "for you right now I spend most of my time in the core rather than in the sequel there so I'm not super familiar with sequences but I can talk to you",
    "start": "2184400",
    "end": "2190430"
  },
  {
    "text": "afterwards if you'd like sorry so we",
    "start": "2190430",
    "end": "2199040"
  },
  {
    "start": "2198000",
    "end": "2242000"
  },
  {
    "text": "rely much less than spanner in the sense that we use a different commit protocol so spanner spanners commit protocol",
    "start": "2199040",
    "end": "2204950"
  },
  {
    "text": "requires very very low latency like single-digit millisecond latency for commits to perform well we allow up to",
    "start": "2204950",
    "end": "2211280"
  },
  {
    "text": "hundreds of milliseconds even a second of clock offset between nodes depending",
    "start": "2211280",
    "end": "2216740"
  },
  {
    "text": "on your environment what with still with good performance so but if your clock offset between nodes",
    "start": "2216740",
    "end": "2222230"
  },
  {
    "text": "exceeds the pre-configured amounts we will kill one or more of those nodes processes to avoid potentially",
    "start": "2222230",
    "end": "2228950"
  },
  {
    "text": "inconsistent reads that can happen if clocks are incorrect so we are constantly detecting the clock offset",
    "start": "2228950",
    "end": "2233960"
  },
  {
    "text": "between nodes and will kill a process if it gets to a dangerous level all right",
    "start": "2233960",
    "end": "2240710"
  },
  {
    "text": "thanks everyone [Applause]",
    "start": "2240710",
    "end": "2244150"
  }
]