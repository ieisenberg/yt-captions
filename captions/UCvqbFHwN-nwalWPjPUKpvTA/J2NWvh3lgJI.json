[
  {
    "start": "0",
    "end": "66000"
  },
  {
    "text": "[Applause] thanks everybody I'm excited to be here let's dive right in in this talk I want",
    "start": "520",
    "end": "7770"
  },
  {
    "text": "to discuss how you can get visibility into your clusters architecture health",
    "start": "7770",
    "end": "12840"
  },
  {
    "text": "and cost using BPF and I want this talk to me top-down so I'm gonna start with a",
    "start": "12840",
    "end": "18180"
  },
  {
    "text": "demo and show you what kind of visibility you would want to get then go",
    "start": "18180",
    "end": "24359"
  },
  {
    "text": "into how you could collect this type of telemetry spoiler alert you extract data",
    "start": "24359",
    "end": "29670"
  },
  {
    "text": "from Linux and your Orchestrator and merger together together what we call flow data as opposed to your metrics",
    "start": "29670",
    "end": "35910"
  },
  {
    "text": "logs and tracing data I'm gonna destroy discuss pros and cons of flow this type",
    "start": "35910",
    "end": "42960"
  },
  {
    "text": "of flow data the challenges in collecting it and why you need BPF what led us to use BPF as the technology to",
    "start": "42960",
    "end": "50010"
  },
  {
    "text": "collect it go through a complete architecture of how you and build a system to analyze flow data evaluate",
    "start": "50010",
    "end": "57390"
  },
  {
    "text": "overhead and performance and briefly discuss one of the disadvantages and how",
    "start": "57390",
    "end": "63030"
  },
  {
    "text": "we can overcome it in application level metrics so quickly a quick note about",
    "start": "63030",
    "end": "68070"
  },
  {
    "start": "66000",
    "end": "170000"
  },
  {
    "text": "myself I'll tell you a bit about my history I first was exposed to these",
    "start": "68070",
    "end": "73110"
  },
  {
    "text": "large-scale operational challenges as part of work in a government technology agency I spent seven years there I",
    "start": "73110",
    "end": "80070"
  },
  {
    "text": "started by developing systems that were deployed at scale in the field mission",
    "start": "80070",
    "end": "85710"
  },
  {
    "text": "critical systems but it quickly became apparent that it that we have an Operations problem we had 10 to 20",
    "start": "85710",
    "end": "92850"
  },
  {
    "text": "operators who kept fire fighting waking up in the middle of the night to deploy configuration changes restore",
    "start": "92850",
    "end": "99090"
  },
  {
    "text": "connectivity and deploy new software versions so I switched to the",
    "start": "99090",
    "end": "104310"
  },
  {
    "text": "infrastructure organization and built systems to monitor and automatically and",
    "start": "104310",
    "end": "109860"
  },
  {
    "text": "semi-automatically mitigate failure I then moved to MIT to do a PhD in these",
    "start": "109860",
    "end": "114930"
  },
  {
    "text": "monitoring and mitigation systems that are extremely low overhead low latency",
    "start": "114930",
    "end": "120990"
  },
  {
    "text": "high throughput for example we had a system called Fast Pass where there was a centralized server which we called the",
    "start": "120990",
    "end": "126780"
  },
  {
    "text": "arbiter the Arbor collected from every server in the cluster all the packets that server",
    "start": "126780",
    "end": "131880"
  },
  {
    "text": "wanted to send to every other server in the network so with the complete view the arbiter could analyze this traffic",
    "start": "131880",
    "end": "138150"
  },
  {
    "text": "and then allocate which server sends to which server so according to",
    "start": "138150",
    "end": "144510"
  },
  {
    "text": "organizational policy so this arbiter allocated 2 terabytes of",
    "start": "144510",
    "end": "150260"
  },
  {
    "text": "traffic of data of traffic per second with 50 microseconds of latency with",
    "start": "150260",
    "end": "155700"
  },
  {
    "text": "deployed at Facebook in production during 2013 so this is just to give you a background into what technology were",
    "start": "155700",
    "end": "161640"
  },
  {
    "text": "bringing into this type of visibility I started flow mill two years ago in",
    "start": "161640",
    "end": "167250"
  },
  {
    "text": "order to get better visibility for operations so I'm gonna start with this demo application I think many of you",
    "start": "167250",
    "end": "174210"
  },
  {
    "start": "170000",
    "end": "237000"
  },
  {
    "text": "know this it's a great it's a great open source project I want to thank Google",
    "start": "174210",
    "end": "180680"
  },
  {
    "text": "platform for open sourcing this and this is the hipster shop so if you haven't",
    "start": "180680",
    "end": "187200"
  },
  {
    "text": "seen it it's this ecommerce site that you can deploy in communities you can go and buy these hipster items maybe buy",
    "start": "187200",
    "end": "194790"
  },
  {
    "text": "four of these terrariums it has a shopping cart checkout I could you can",
    "start": "194790",
    "end": "200550"
  },
  {
    "text": "place your order and such so how does this system look in practice let's go in",
    "start": "200550",
    "end": "206940"
  },
  {
    "text": "and see how the deployment looks so usually you have this kubernetes manifest that tells you what are the",
    "start": "206940",
    "end": "214260"
  },
  {
    "text": "services you can see here there's a front-end service the cart service the checkout service and so on usually when",
    "start": "214260",
    "end": "222300"
  },
  {
    "text": "you want to operate these systems this is not enough what you want is an image of how how your system interacts how",
    "start": "222300",
    "end": "229800"
  },
  {
    "text": "your components interact and how your system is built so luckily many organizations create these these",
    "start": "229800",
    "end": "237450"
  },
  {
    "start": "237000",
    "end": "385000"
  },
  {
    "text": "architecture diagrams and the hipster shop also has this architecture diagram",
    "start": "237450",
    "end": "242540"
  },
  {
    "text": "unfortunately these architecture diagrams are frequently not up-to-date so when you're dealing with an incident",
    "start": "242540",
    "end": "249630"
  },
  {
    "text": "it gets really hard to understand what's happening what you would want is something like this where you",
    "start": "249630",
    "end": "255270"
  },
  {
    "text": "reconstruct the architecture if your system from live data in real time here you can see",
    "start": "255270",
    "end": "263129"
  },
  {
    "text": "the front-end service the load generator and so on so if if we go back to the",
    "start": "263129",
    "end": "269280"
  },
  {
    "text": "architecture diagram what I did is I rearranged all the services in the real",
    "start": "269280",
    "end": "275340"
  },
  {
    "text": "time graph and so that we can overlay the architecture diagram I hope this is visible and what you can see that is",
    "start": "275340",
    "end": "282240"
  },
  {
    "text": "that with the vanilla deployment of the hipster shop the email service and the payment service are not in use so these",
    "start": "282240",
    "end": "288539"
  },
  {
    "text": "are the type of things that you really want when you operate services at scale the other thing you can get out of this",
    "start": "288539",
    "end": "294300"
  },
  {
    "text": "when you have real time data is when you have a deployment you make you deploy a new version of a service you want to",
    "start": "294300",
    "end": "300060"
  },
  {
    "text": "understand did the did does it have the same dependencies is it getting it does",
    "start": "300060",
    "end": "307440"
  },
  {
    "text": "it have a new dependency is the old dependent are the old dependencies all",
    "start": "307440",
    "end": "312569"
  },
  {
    "text": "working as you expect so this gives you confidence when you deploy another thing real time architecture map gives you is",
    "start": "312569",
    "end": "319889"
  },
  {
    "text": "the ability to understand your load balancing and high availability architecture so what I'm gonna do now is",
    "start": "319889",
    "end": "325650"
  },
  {
    "text": "I'm going to expand expand this front end by availability zone but jumpy but",
    "start": "325650",
    "end": "333210"
  },
  {
    "text": "here we have two nodes the front end on us to be and the front end on us to a",
    "start": "333210",
    "end": "338370"
  },
  {
    "text": "I'm going to now also expand a cart service and you can see front end is",
    "start": "338370",
    "end": "348029"
  },
  {
    "text": "communicating fronted on to B is communicating with cart service on to B and also I guess that's what it's",
    "start": "348029",
    "end": "356699"
  },
  {
    "text": "communicating to and the front end on to a is communicating with the cart service on to B but neither of these front-end",
    "start": "356699",
    "end": "363360"
  },
  {
    "text": "nodes are communicating with the cart service on to a so this is an artifact of how kubernetes load balances pods to",
    "start": "363360",
    "end": "370860"
  },
  {
    "text": "services services to pods and kind of this is something that you would want to",
    "start": "370860",
    "end": "376319"
  },
  {
    "text": "know about your deployment also if you have a high availability architecture what's running in what availability zone so what are the dependencies that you",
    "start": "376319",
    "end": "381900"
  },
  {
    "text": "have okay second item so we went through",
    "start": "381900",
    "end": "387630"
  },
  {
    "start": "385000",
    "end": "531000"
  },
  {
    "text": "architecture now I want to discuss health how can you find what's happening in your cluster using this data so I'm",
    "start": "387630",
    "end": "395040"
  },
  {
    "text": "going to introduce service degradation in this product catalog service I'm",
    "start": "395040",
    "end": "400350"
  },
  {
    "text": "going to do some manual chaos engineering so let's go to these these",
    "start": "400350",
    "end": "405810"
  },
  {
    "text": "nodes so here are the nodes the sorry",
    "start": "405810",
    "end": "411780"
  },
  {
    "text": "the pods I'm going to introduce to this product catalog service so first I want",
    "start": "411780",
    "end": "417360"
  },
  {
    "text": "to get the network interface that the",
    "start": "417360",
    "end": "422880"
  },
  {
    "text": "pod has sometimes you can have a system do this for you I'm gonna do this",
    "start": "422880",
    "end": "429060"
  },
  {
    "text": "manually for the demo so this is interface 100 142 I'm then gonna SSH to",
    "start": "429060",
    "end": "435540"
  },
  {
    "text": "the kubernetes node and get a list of IP addresses so where we want 142 here so",
    "start": "435540",
    "end": "443990"
  },
  {
    "text": "here it is so this is the network interface on the node that is matched to the pod interface so 142 so I'm gonna",
    "start": "443990",
    "end": "454080"
  },
  {
    "text": "add some delay",
    "start": "454080",
    "end": "460189"
  },
  {
    "text": "let's say 75 milliseconds before I do this I want to introduce kind of the dashboard you can you can use to look at",
    "start": "465490",
    "end": "471560"
  },
  {
    "text": "this so here is the same data that you saw in the graph the map view in",
    "start": "471560",
    "end": "477710"
  },
  {
    "text": "dashboards so you can see TCP throughput drops and round-trip time also CIN",
    "start": "477710",
    "end": "483380"
  },
  {
    "text": "timeouts I'll talk about these of it later and here I've filtered to only show the product catalog service I know",
    "start": "483380",
    "end": "488690"
  },
  {
    "text": "this must be a bit the font might be small but I don't think you need to read it right now so here I'm gonna introduce",
    "start": "488690",
    "end": "495350"
  },
  {
    "text": "the delay and refresh here and here look",
    "start": "495350",
    "end": "501620"
  },
  {
    "text": "at the round-trip time so very quickly you can see measurements so the idea is to get this in real time get all this",
    "start": "501620",
    "end": "507080"
  },
  {
    "text": "data this is one second granular D data",
    "start": "507080",
    "end": "513409"
  },
  {
    "text": "I'm gonna remove the remove this delay",
    "start": "513410",
    "end": "520659"
  },
  {
    "text": "so yeah the system should recover very quickly to remove this so the round-trip",
    "start": "520660",
    "end": "526160"
  },
  {
    "text": "time here decreases here you have a sample that has low round-trip time so",
    "start": "526160",
    "end": "531380"
  },
  {
    "text": "this is one of the one of the types of servers the gradations that you want your system to show you of course I'm",
    "start": "531380",
    "end": "536450"
  },
  {
    "text": "showing you dashboards but of course you can create alerts based on this the other thing I want to show you is maybe",
    "start": "536450",
    "end": "543790"
  },
  {
    "text": "discovering if your security groups are misconfigured so I'm going to deny traffic going to the Rena server the",
    "start": "543790",
    "end": "550820"
  },
  {
    "text": "Redis node here I'm going to do this by",
    "start": "550820",
    "end": "555950"
  },
  {
    "text": "just editing manually editing the the security group usually you'd have a typo",
    "start": "555950",
    "end": "561770"
  },
  {
    "text": "in your terraform or whatever mark thank you whatever technology you're using so this is the port six three seven nine",
    "start": "561770",
    "end": "568160"
  },
  {
    "text": "but usually used let's say I not fingered it and wrote a zero here let me",
    "start": "568160",
    "end": "573380"
  },
  {
    "text": "introduce the dashboard first so this is the kind of this is the similar dashboard where I filtered only to the",
    "start": "573380",
    "end": "579230"
  },
  {
    "text": "Redis node I'm going to save",
    "start": "579230",
    "end": "586240"
  },
  {
    "text": "refresh and very quickly we see sin timeouts so these are timeouts in",
    "start": "587790",
    "end": "593050"
  },
  {
    "text": "connection establishments and so also if you could alert on these so these",
    "start": "593050",
    "end": "599410"
  },
  {
    "text": "shouldn't really happen and there definitely shouldn't happen like multiple times in consecutive seconds so",
    "start": "599410",
    "end": "605860"
  },
  {
    "text": "this is another type of a fault that you can discover with flow data I'm gonna",
    "start": "605860",
    "end": "612610"
  },
  {
    "text": "just River this okay I'm breathing through this I'm sorry but let's head to",
    "start": "612610",
    "end": "619959"
  },
  {
    "start": "617000",
    "end": "708000"
  },
  {
    "text": "the third category which is cost so we don't we did architecture and health and now cost when you run on the public",
    "start": "619959",
    "end": "626980"
  },
  {
    "text": "cloud there is a dollar cost for communication between availability zones and an even higher cost for",
    "start": "626980",
    "end": "633670"
  },
  {
    "text": "communication between regions can you get a real-time view of how your application is behaving so you can",
    "start": "633670",
    "end": "638920"
  },
  {
    "text": "optimize it so here here's a dashboard",
    "start": "638920",
    "end": "644170"
  },
  {
    "text": "where I filtered only through the traffic from this demo I'm gonna zoom in on this throughput graph and what I'm",
    "start": "644170",
    "end": "650110"
  },
  {
    "text": "gonna do now is I'm gonna add a breakdown of source traffic by",
    "start": "650110",
    "end": "655329"
  },
  {
    "text": "availability zone this adds this column so you can see traffic going from the",
    "start": "655329",
    "end": "660850"
  },
  {
    "text": "front end on to be to any node of low generator regarding regardless of what",
    "start": "660850",
    "end": "666370"
  },
  {
    "text": "every lability zone is in now I can add to breakdown on this on the destination side so now you can see traffic from on",
    "start": "666370",
    "end": "673870"
  },
  {
    "text": "the front end from to be to the load generator to a to be to to a and so on and I'm gonna add another filter you can",
    "start": "673870",
    "end": "682120"
  },
  {
    "text": "sorry I'm gonna filter only traffic between different zones so usually you",
    "start": "682120",
    "end": "688720"
  },
  {
    "text": "do it you'd have a dashboard that does this but I wanted to show the filtering so here sort by total amount of traffic",
    "start": "688720",
    "end": "696009"
  },
  {
    "text": "you can get a list of your top talkers so you can optimize and this is by pod and availability zone you can get the",
    "start": "696009",
    "end": "701350"
  },
  {
    "text": "similar thing by by node so just to",
    "start": "701350",
    "end": "709449"
  },
  {
    "start": "708000",
    "end": "744000"
  },
  {
    "text": "summarize so far we've seen these five use cases across three categories in the architecture getting more confidence in",
    "start": "709449",
    "end": "717370"
  },
  {
    "text": "your deployments and having a real-time view of architecture understanding your high availability architecture and",
    "start": "717370",
    "end": "723269"
  },
  {
    "text": "load-balancing by the way this is if there's one slide you remember out of this talk this should be it right so for",
    "start": "723269",
    "end": "730439"
  },
  {
    "text": "health you can understand you can see service the gradation very quickly and understand security groups and for cost",
    "start": "730439",
    "end": "736949"
  },
  {
    "text": "you can see traffic across zones or regions and you can do all of this with flow data ok now we did this I want to",
    "start": "736949",
    "end": "745589"
  },
  {
    "text": "go into how you get this flow data what is in this data you assemble it from two",
    "start": "745589",
    "end": "751199"
  },
  {
    "text": "sources from the Linux kernel you can get information on very frequently let's",
    "start": "751199",
    "end": "757350"
  },
  {
    "text": "say every second on what each source is talking to for every socket right what",
    "start": "757350",
    "end": "762420"
  },
  {
    "text": "each source IP address destination IP address and ports are talking of how",
    "start": "762420",
    "end": "767670"
  },
  {
    "text": "much they're sending the number of packet drops round-trip time and sync timeouts and such from kubernetes you",
    "start": "767670",
    "end": "774899"
  },
  {
    "text": "can get metadata of the pod so the IP address the name of the deployment and",
    "start": "774899",
    "end": "780149"
  },
  {
    "text": "the pod of the image that is serving the the for each container the tag of each",
    "start": "780149",
    "end": "787649"
  },
  {
    "text": "image so this is the version of your service running and the zone that it's running in so all of these are failure",
    "start": "787649",
    "end": "794579"
  },
  {
    "text": "domains that you can then extract telemetry for and understand if that is",
    "start": "794579",
    "end": "800009"
  },
  {
    "text": "that failure domain is affecting your service so together when you combine the two this is a very powerful data set you can",
    "start": "800009",
    "end": "808829"
  },
  {
    "text": "understand is this service failing to that service is this version of this service failing to that version of that",
    "start": "808829",
    "end": "814410"
  },
  {
    "text": "service etc I'm gonna go through so I've",
    "start": "814410",
    "end": "820290"
  },
  {
    "start": "817000",
    "end": "956000"
  },
  {
    "text": "told you what you expect to get out of it how to get it and then what what are",
    "start": "820290",
    "end": "826110"
  },
  {
    "text": "the benefits and the disadvantages so the advantages are one you don't need to",
    "start": "826110",
    "end": "831629"
  },
  {
    "text": "change your application that's great unlike with custom metrics or logs or traces where developers constantly have",
    "start": "831629",
    "end": "838649"
  },
  {
    "text": "to maintain the instrumentation in the application here you get everything from the kernel and Orchestrator so there's",
    "start": "838649",
    "end": "846389"
  },
  {
    "text": "the deployment is easier in those lower organizational cost you also get",
    "start": "846389",
    "end": "852580"
  },
  {
    "text": "100% coverage because everything that passes through the operating system you can capture and that's everything",
    "start": "852580",
    "end": "859680"
  },
  {
    "text": "unlike other technologies here you don't we don't have to support many languages and many frameworks so it's very easy to",
    "start": "860130",
    "end": "867779"
  },
  {
    "text": "hyper optimize all the collection points so you can get very low overhead collection and there's more about this",
    "start": "867779",
    "end": "874750"
  },
  {
    "text": "in the evaluation section I'll show you some benchmarks and results and you can",
    "start": "874750",
    "end": "881649"
  },
  {
    "text": "get visibility into external services external api's managed services databases and so that's the same way as",
    "start": "881649",
    "end": "888040"
  },
  {
    "text": "you get for your normal services the disadvantage there's one major major",
    "start": "888040",
    "end": "894040"
  },
  {
    "text": "disadvantage the the flow flow data as I've shown you so far doesn't collect",
    "start": "894040",
    "end": "901779"
  },
  {
    "text": "application layer error codes or metrics at all and usually when kind of when you",
    "start": "901779",
    "end": "908290"
  },
  {
    "text": "operate for a business purpose you want to understand how the users are experiencing the application what's",
    "start": "908290",
    "end": "913540"
  },
  {
    "text": "failing and what's not and this doesn't do that it has these advantages but it",
    "start": "913540",
    "end": "920080"
  },
  {
    "text": "doesn't collect the error codes you can get proxies so usually we found that throughput of a",
    "start": "920080",
    "end": "925810"
  },
  {
    "text": "service is a proxy for service health because once the service starts sending 400 or 500 error codes the error",
    "start": "925810",
    "end": "932860"
  },
  {
    "text": "responses are tiny compared to regular payloads so you immediately see a drop",
    "start": "932860",
    "end": "938170"
  },
  {
    "text": "in throughput so you can use that but still it's a proxy this is something that we want to fix and I'm gonna I'm",
    "start": "938170",
    "end": "943390"
  },
  {
    "text": "going to discuss this very shortly at the end of the talk towards the end of the talk how you could solve that I wanted to front-load that a disadvantage",
    "start": "943390",
    "end": "951510"
  },
  {
    "text": "but the advantages are large ok so now I",
    "start": "951510",
    "end": "957279"
  },
  {
    "start": "956000",
    "end": "1028000"
  },
  {
    "text": "want to deep dive into how you can extract this flow data from kubernetes let's say you have a pod a that wants to",
    "start": "957279",
    "end": "964420"
  },
  {
    "text": "communicate to a service X and service X is backed by a pod B the way kubernetes",
    "start": "964420",
    "end": "971110"
  },
  {
    "text": "works is you proxy configures IP tables on the node that a is on",
    "start": "971110",
    "end": "977430"
  },
  {
    "text": "- with a virtual IP address Forex a establishes a connection to X and then",
    "start": "977450",
    "end": "984710"
  },
  {
    "text": "iptables maps that X address to B's address so map's the a to X connection",
    "start": "984710",
    "end": "991220"
  },
  {
    "text": "to a to b and b just sees packets as if they're coming directly from a so how do you get this flow data first all the",
    "start": "991220",
    "end": "998420"
  },
  {
    "text": "Padma today you can get from the kubernetes api the socket level",
    "start": "998420",
    "end": "1004810"
  },
  {
    "text": "information you can get with the SS utility and linux so this is widely available here you can see a 2x",
    "start": "1004810",
    "end": "1013140"
  },
  {
    "text": "communicates how many bytes there's also run trip time and other metrics and from",
    "start": "1013140",
    "end": "1019210"
  },
  {
    "text": "contract you get information about the IP tables the actual rose in the NAP table so here you see the mapping from a",
    "start": "1019210",
    "end": "1025240"
  },
  {
    "text": "to X to A to B so this is this is great",
    "start": "1025240",
    "end": "1031150"
  },
  {
    "start": "1028000",
    "end": "1118000"
  },
  {
    "text": "like all the tools are there you can build a solution like this cui has a problem which is what is going to lead",
    "start": "1031150",
    "end": "1036819"
  },
  {
    "text": "me to BPF first is performance that the",
    "start": "1036820",
    "end": "1042370"
  },
  {
    "text": "performances is actually not the deal breaker but I want to address it first so here is a test that we ran on a",
    "start": "1042370",
    "end": "1047880"
  },
  {
    "text": "production machine it shows on the x-axis the number of sockets and on the y-axis how many million cycles it took",
    "start": "1047880",
    "end": "1055240"
  },
  {
    "text": "to run SS you can see that kind of from this results of the regression line every socket that that SS touches cost",
    "start": "1055240",
    "end": "1063220"
  },
  {
    "text": "79 thousand cycles and when you run this every second and you have a lot of these sockets open on the machine then it's",
    "start": "1063220",
    "end": "1069340"
  },
  {
    "text": "gonna get expensive because and the reason is the tool is built for the CLI",
    "start": "1069340",
    "end": "1074430"
  },
  {
    "text": "another reason is it a great so ver all the sockets in the system even if they didn't have any data on them so you're",
    "start": "1074430",
    "end": "1080200"
  },
  {
    "text": "paying even for sockets that are not helpful to you but maybe the you know",
    "start": "1080200",
    "end": "1087700"
  },
  {
    "text": "more important problem is coverage the CLI tools are built for polling so what",
    "start": "1087700",
    "end": "1094300"
  },
  {
    "text": "you do is maybe every second or every 10 seconds you run the tool and get a snapshot of the system state and this",
    "start": "1094300",
    "end": "1100690"
  },
  {
    "text": "tends to miss a lot of events so if your sockets are 100 milliseconds last 100",
    "start": "1100690",
    "end": "1106870"
  },
  {
    "text": "milliseconds because you have these fast transactions and pull the system every once in a while you're gonna lose a lot of data and this",
    "start": "1106870",
    "end": "1115200"
  },
  {
    "text": "is one of the reasons why you want a PF BPF is a great criminal sub system",
    "start": "1115200",
    "end": "1122150"
  },
  {
    "start": "1118000",
    "end": "1228000"
  },
  {
    "text": "introduced into the linux kernel with the DPF system call in kernel version",
    "start": "1122150",
    "end": "1127470"
  },
  {
    "text": "318 so it's been around for a while what it allows you to do is it allows user",
    "start": "1127470",
    "end": "1133560"
  },
  {
    "text": "space programs to specify code snippets that the kernel then runs on behalf of",
    "start": "1133560",
    "end": "1138690"
  },
  {
    "text": "the user space programs on different kernel events and usually these are functioning executions in the kernel so",
    "start": "1138690",
    "end": "1144240"
  },
  {
    "text": "when different things happen inside the kernel so the snippet of code runs it allows you to space programs to extract",
    "start": "1144240",
    "end": "1150960"
  },
  {
    "text": "data from the kernel very efficiently one advantage is you can get only",
    "start": "1150960",
    "end": "1156000"
  },
  {
    "text": "changes into intestate in the kernel so only sockets that actually had data on",
    "start": "1156000",
    "end": "1162240"
  },
  {
    "text": "them rather than all the sockets for example and it gives you access to",
    "start": "1162240",
    "end": "1168860"
  },
  {
    "text": "internal data structures where you can extract more data that is not usually easily available to user space through",
    "start": "1168890",
    "end": "1175710"
  },
  {
    "text": "system calls the gif is very safe it's built with verifier in the kernel that",
    "start": "1175710",
    "end": "1182340"
  },
  {
    "text": "ensures that vpf code doesn't access invalid memory doesn't loop forever and",
    "start": "1182340",
    "end": "1187620"
  },
  {
    "text": "so on in fact loops are forbidden beefy at EPF code completely and the in the contexts",
    "start": "1187620",
    "end": "1197280"
  },
  {
    "text": "in all the contexts where we kind of this talk and runs BPF code in all those",
    "start": "1197280",
    "end": "1204780"
  },
  {
    "text": "contexts the memory and the kernel is read-only so you cannot change anything in the kernel BPF is also very fast the",
    "start": "1204780",
    "end": "1213140"
  },
  {
    "text": "kernel contains a just-in-time compiler that takes these byte code snippets and",
    "start": "1213140",
    "end": "1218270"
  },
  {
    "text": "compiles them into very efficient machine code so you can get this 100%",
    "start": "1218270",
    "end": "1224250"
  },
  {
    "text": "coverage with very low overhead I want to run through a quick demo of how BPF",
    "start": "1224250",
    "end": "1231900"
  },
  {
    "start": "1228000",
    "end": "1383000"
  },
  {
    "text": "utilities of some VB fit utilities I'm going to run on one of the kubernetes cluster is this utility called TCP top",
    "start": "1231900",
    "end": "1238710"
  },
  {
    "text": "from the BCC repository it's a great open source project that Python bindings and many utilities",
    "start": "1238710",
    "end": "1247099"
  },
  {
    "text": "accessing kernel data structures with you Kiev so here it is TCP top sorry you",
    "start": "1247099",
    "end": "1264929"
  },
  {
    "text": "can see every second TCP top prints all the sockets that were active during that second let's look at",
    "start": "1264929",
    "end": "1270989"
  },
  {
    "text": "it more carefully sorry I'm gonna run it",
    "start": "1270989",
    "end": "1277109"
  },
  {
    "text": "again just so we can see it so you can see here's the SSH that we're using in",
    "start": "1277109",
    "end": "1285090"
  },
  {
    "text": "order to view this data you can see the different services in the hipster",
    "start": "1285090",
    "end": "1292229"
  },
  {
    "text": "shopped communicating here's the cart service and so on and by instrumenting",
    "start": "1292229",
    "end": "1300960"
  },
  {
    "text": "these kernel a paternal functions so specifically tcp top instruments sent message and tcp cleanup are buff you can",
    "start": "1300960",
    "end": "1307619"
  },
  {
    "text": "get pretty good visibility into what's happening when you build such a system",
    "start": "1307619",
    "end": "1312809"
  },
  {
    "text": "you need to be careful as you write this instrumentation so here's the tcp top code that prints out results to the",
    "start": "1312809",
    "end": "1319619"
  },
  {
    "text": "screen here this items call actually goes and fetches data from the kernel",
    "start": "1319619",
    "end": "1326970"
  },
  {
    "text": "data structures that are shared between BPF code and user space then analyzes",
    "start": "1326970",
    "end": "1331979"
  },
  {
    "text": "them does something it puts them into a dictionary and then clears that data structure and kind of the the reason I'm",
    "start": "1331979",
    "end": "1339809"
  },
  {
    "text": "showing this is because when you write this type of code you need to be careful about types of races for example in this",
    "start": "1339809",
    "end": "1345570"
  },
  {
    "text": "case between the time the the user space program reads all the state from the",
    "start": "1345570",
    "end": "1351539"
  },
  {
    "text": "kernel and the time it clears the data structure so it doesn't have to read the same values the kernel can insert new",
    "start": "1351539",
    "end": "1356789"
  },
  {
    "text": "sockets and all those sockets are cleared here thrown away so these are",
    "start": "1356789",
    "end": "1361919"
  },
  {
    "text": "the types of things you need to be careful when you build a system for real this would give you by the way the race here is pretty small so it will",
    "start": "1361919",
    "end": "1369019"
  },
  {
    "text": "give you 99% coverage right I know depending so you'd miss 1% of traffic",
    "start": "1369019",
    "end": "1375140"
  },
  {
    "text": "but maybe you don't want to miss that like we when we build these systems we take care of all the edge conditions we",
    "start": "1375140",
    "end": "1380840"
  },
  {
    "text": "don't want to have this so we've gone through BPF and kind of demo and kind of",
    "start": "1380840",
    "end": "1389210"
  },
  {
    "start": "1383000",
    "end": "1523000"
  },
  {
    "text": "what things to pay attention to and I want to talk a little bit about how to build this end-to-end as as a system to",
    "start": "1389210",
    "end": "1395179"
  },
  {
    "text": "analyze this code so first and the agent interact with the kubernetes api and",
    "start": "1395179",
    "end": "1400639"
  },
  {
    "text": "also other orchestrators could be yes docker ec2 and extracts data from linux",
    "start": "1400639",
    "end": "1406669"
  },
  {
    "text": "it then forwards data into an analysis pipeline the goal of the analysis server",
    "start": "1406669",
    "end": "1413570"
  },
  {
    "text": "here is first if you have an agent on both sides of the connection then your agents on both sides of the connection",
    "start": "1413570",
    "end": "1420200"
  },
  {
    "text": "you want to match the two flows so that you get a hear interview of that flow from both sides it then enriches that",
    "start": "1420200",
    "end": "1427370"
  },
  {
    "text": "flow with metadata from the orchestrators and aggregates them",
    "start": "1427370",
    "end": "1432880"
  },
  {
    "text": "according to some route PI's and groups by route buys the data and materializes",
    "start": "1432880",
    "end": "1438260"
  },
  {
    "text": "some view and the idea here is you don't want to save raw socket data into your time series database because the queries",
    "start": "1438260",
    "end": "1443360"
  },
  {
    "text": "are going to be very slow pushing it to time series database we use Prometheus",
    "start": "1443360",
    "end": "1450529"
  },
  {
    "text": "right now and we found that you can",
    "start": "1450529",
    "end": "1455809"
  },
  {
    "text": "build very cool api's on top of this because usually when you have time",
    "start": "1455809",
    "end": "1461120"
  },
  {
    "text": "series databases the labels are you have one set of labels here where the this",
    "start": "1461120",
    "end": "1466370"
  },
  {
    "text": "flow data you have a set of labels for the source the set of data for the destination and a set of labels for the",
    "start": "1466370",
    "end": "1472610"
  },
  {
    "text": "flow itself so the api's could be much more convenient you can you could build",
    "start": "1472610",
    "end": "1478820"
  },
  {
    "text": "api's that are more convenient to work with this data what you'd want more is to build an analysis pipeline so for",
    "start": "1478820",
    "end": "1485990"
  },
  {
    "text": "example I to analyze this data and create events so one is like the",
    "start": "1485990",
    "end": "1492200"
  },
  {
    "text": "standard alerting that you get from for example graph ah no other types of exact",
    "start": "1492200",
    "end": "1498220"
  },
  {
    "text": "exciting you can build our for example this version detector where I have a new",
    "start": "1498220",
    "end": "1505919"
  },
  {
    "text": "deployment I detect a new version and I want to see our is the new does the new version behave the same as the old",
    "start": "1505919",
    "end": "1512909"
  },
  {
    "text": "version so these are pretty exciting things you can do and we're looking for feedback there so if you have cool ideas",
    "start": "1512909",
    "end": "1519330"
  },
  {
    "text": "or want to discuss I really love to talk about that more okay",
    "start": "1519330",
    "end": "1524909"
  },
  {
    "start": "1523000",
    "end": "1613000"
  },
  {
    "text": "so let's jump into evaluation of the system so how does it behave is it gonna be too expensive to run",
    "start": "1524909",
    "end": "1530130"
  },
  {
    "text": "we've run perform measurements I've included here kind of our methodology",
    "start": "1530130",
    "end": "1535440"
  },
  {
    "text": "perf is a sampling profiler so you get these stacks execution stacks from the kernel every here 5 million cycles and",
    "start": "1535440",
    "end": "1542730"
  },
  {
    "text": "you can analyze those and see how many cycles are statistically in each in each",
    "start": "1542730",
    "end": "1548940"
  },
  {
    "text": "function one thing to note is when you evaluate these monitoring systems you",
    "start": "1548940",
    "end": "1554279"
  },
  {
    "text": "need to evaluate not only the user space portion which is what is readily accessible but also the kernel overhead",
    "start": "1554279",
    "end": "1561799"
  },
  {
    "text": "so all of the measurements I'm going to show I mean there's not a lot but we",
    "start": "1561799",
    "end": "1566909"
  },
  {
    "text": "have we evaluate the criminal overhead as well which could be more it's frequently more than the user space part",
    "start": "1566909",
    "end": "1573260"
  },
  {
    "text": "what we've seen is the collector that we've been using has point one to point",
    "start": "1573260",
    "end": "1579179"
  },
  {
    "text": "two a quarter percent pork or CPU overhead across many deployments we do",
    "start": "1579179",
    "end": "1585630"
  },
  {
    "text": "keep track of the highest overhead we've ever seen we've had a one of our",
    "start": "1585630",
    "end": "1592350"
  },
  {
    "text": "customer have run a very aggressive load test on their servers that their",
    "start": "1592350",
    "end": "1597510"
  },
  {
    "text": "application worked at 46% which is very high for them and with that application",
    "start": "1597510",
    "end": "1603240"
  },
  {
    "text": "our collector ran with less than a percent 0.86% CPU overhead so this is",
    "start": "1603240",
    "end": "1610440"
  },
  {
    "text": "the highest that we've seen is this gonna be expensive in network overhead",
    "start": "1610440",
    "end": "1616260"
  },
  {
    "start": "1613000",
    "end": "1669000"
  },
  {
    "text": "now you're sending all the socket telemetry every second to the analysis",
    "start": "1616260",
    "end": "1621570"
  },
  {
    "text": "pipeline so here we have several different clusters that are running these are production clusters",
    "start": "1621570",
    "end": "1627299"
  },
  {
    "text": "running this is megabytes per second in throughput on the cluster and the workload itself and you know this is a",
    "start": "1627299",
    "end": "1634049"
  },
  {
    "text": "flow data collection service so it also monitors the amount of telemetry flowing",
    "start": "1634049",
    "end": "1639869"
  },
  {
    "text": "to the flow to the flow analysis pipeline so here's the amount of telemetry and megabytes per second and",
    "start": "1639869",
    "end": "1646590"
  },
  {
    "text": "we found that in most most cases you can get under 0.5% CPU overhead and if",
    "start": "1646590",
    "end": "1653519"
  },
  {
    "text": "you're running a very intense workload where you have these tiny tiny transactions and tons of them then you",
    "start": "1653519",
    "end": "1660539"
  },
  {
    "text": "get higher network overhead this is Wow 1.15 is what we've seen outliers are",
    "start": "1660539",
    "end": "1666059"
  },
  {
    "text": "yeah around 1% another point on how to",
    "start": "1666059",
    "end": "1671879"
  },
  {
    "start": "1669000",
    "end": "1751000"
  },
  {
    "text": "build these analysis pipelines what you could expect the number of events here",
    "start": "1671879",
    "end": "1677279"
  },
  {
    "text": "I've I've shown three different companies deployment and the number of",
    "start": "1677279",
    "end": "1685409"
  },
  {
    "text": "events that you get of every different types per second per agent so for example company a has 1,400 TCP events",
    "start": "1685409",
    "end": "1694440"
  },
  {
    "text": "per second per agent here might be where you want to look like total number of",
    "start": "1694440",
    "end": "1699690"
  },
  {
    "text": "events per second for agent so you could expect if you're running a batch workload to have around 100 events per",
    "start": "1699690",
    "end": "1706739"
  },
  {
    "text": "second if you're running a more intense interactive workload than several",
    "start": "1706739",
    "end": "1713070"
  },
  {
    "text": "hundreds to thousands what this means for the analysis pipeline is that you need to be ready to handle let's say if",
    "start": "1713070",
    "end": "1719609"
  },
  {
    "text": "you have a 50 node cluster you need to be ready to handle hundreds low hundreds of thousands of events per second in",
    "start": "1719609",
    "end": "1725669"
  },
  {
    "text": "this analysis system much less if you have that workload this is partly why we",
    "start": "1725669",
    "end": "1732690"
  },
  {
    "text": "were motivated to write this in C++ to make sure we can control the overhead and latency and we were able to support",
    "start": "1732690",
    "end": "1739529"
  },
  {
    "text": "several hundred nodes with a two-second latency target and hope to have",
    "start": "1739529",
    "end": "1745889"
  },
  {
    "text": "thousands so this is achievable you just need to pay attention to the details",
    "start": "1745889",
    "end": "1752150"
  },
  {
    "start": "1751000",
    "end": "1906000"
  },
  {
    "text": "so this so here gonna we've done evaluation I know this is a very",
    "start": "1752150",
    "end": "1757460"
  },
  {
    "text": "fast-paced talk so this just summarizes the evaluation and at this point I want",
    "start": "1757460",
    "end": "1763250"
  },
  {
    "text": "to touch on the disadvantage that we that I mentioned earlier",
    "start": "1763250",
    "end": "1769100"
  },
  {
    "text": "getting application-layer metrics or error codes well it turns out that BPF",
    "start": "1769100",
    "end": "1775330"
  },
  {
    "text": "supports user probes so what you could do is you go here so we have a small",
    "start": "1775330",
    "end": "1790179"
  },
  {
    "text": "hello world server here you can go look",
    "start": "1790179",
    "end": "1795740"
  },
  {
    "text": "at the symbols that it has these are all",
    "start": "1795740",
    "end": "1802370"
  },
  {
    "text": "the symbols that it has all the applications and Allah ID and BPF allows",
    "start": "1802370",
    "end": "1809000"
  },
  {
    "text": "you to hook into these functions and get telemetry we let me run it so I'm gonna",
    "start": "1809000",
    "end": "1817630"
  },
  {
    "text": "run the hello world server then use the",
    "start": "1817630",
    "end": "1822860"
  },
  {
    "text": "BPF utility funk on this PID and what",
    "start": "1822860",
    "end": "1829580"
  },
  {
    "text": "I'm gonna do is instrument root hello and get this is a go a go executable go",
    "start": "1829580",
    "end": "1839029"
  },
  {
    "text": "binary so I'm gonna get all the net HTTP calls that have the word header in them",
    "start": "1839029",
    "end": "1846398"
  },
  {
    "text": "this frequently happens when I forget to kill the old server",
    "start": "1852020",
    "end": "1859880"
  },
  {
    "text": "okay and then I'm gonna curl the the",
    "start": "1869450",
    "end": "1876150"
  },
  {
    "text": "whole world server and get some responses so here I'm gonna do it three times and as you close the the funk",
    "start": "1876150",
    "end": "1883980"
  },
  {
    "text": "count tool you can see that BPF was able to get to run three times on this function right header and specifically",
    "start": "1883980",
    "end": "1891059"
  },
  {
    "text": "the right function right header this function is where the error code goes out of the go length and at HTTP handler",
    "start": "1891059",
    "end": "1898140"
  },
  {
    "text": "so it is very possible to write these BPF probes that get application level",
    "start": "1898140",
    "end": "1904620"
  },
  {
    "text": "errors so with this i've we've talked",
    "start": "1904620",
    "end": "1909840"
  },
  {
    "text": "about flow monitoring how to get architecture health and cost visibility the benefits are very easy deployment",
    "start": "1909840",
    "end": "1919380"
  },
  {
    "text": "you don't have to change it the code low overhead and visibility into external services hopefully application",
    "start": "1919380",
    "end": "1926670"
  },
  {
    "text": "metrics hope to in the next talk to talk about that more and BPF is great",
    "start": "1926670",
    "end": "1932429"
  },
  {
    "text": "technology for that and with that I'd love to take questions and please reach out later - we'd love feedback we're in",
    "start": "1932429",
    "end": "1939720"
  },
  {
    "text": "kind of this early access mode it's very exciting we'd love to talk to the community thank you",
    "start": "1939720",
    "end": "1945240"
  },
  {
    "text": "[Applause]",
    "start": "1945240",
    "end": "1954778"
  },
  {
    "text": "okay there's a question back hi I could is it like a petite earlier for the make",
    "start": "1962090",
    "end": "1970520"
  },
  {
    "text": "ice tub or a mock so I can use my my my own program like Python or or go and use",
    "start": "1970520",
    "end": "1978260"
  },
  {
    "text": "that as a gateway for the the kubernetes cluster so I do not mock much make calls",
    "start": "1978260",
    "end": "1985190"
  },
  {
    "text": "to to that I'm sorry can you talk a little bit slower and louder I'm sorry it's hard to hear from you okay can I",
    "start": "1985190",
    "end": "1992510"
  },
  {
    "text": "use this as application layer as up Gately to uses as a stub so when I",
    "start": "1992510",
    "end": "2000660"
  },
  {
    "text": "create my tests on my application I use it I don't not mock my calls I use these guys as a particular layer",
    "start": "2000660",
    "end": "2007480"
  },
  {
    "text": "so I called Dan and then returned me so you're asking if you can use BPF in",
    "start": "2007480",
    "end": "2013270"
  },
  {
    "text": "order to interject into your code and modify it well vpf is specifically a lot of it is",
    "start": "2013270",
    "end": "2021580"
  },
  {
    "text": "written as a read-only system so if you want to write tests with it it gets a little bit hard there are places in the",
    "start": "2021580",
    "end": "2028240"
  },
  {
    "text": "kernel where you can modify packet contents for example but I think it might be heavyweight for use case if you",
    "start": "2028240",
    "end": "2034510"
  },
  {
    "text": "work a lot with packets you might be able to use a VP of that way",
    "start": "2034510",
    "end": "2040290"
  },
  {
    "text": "hi I'm thanks for the impressive tool I have two questions so it wasn't clear to",
    "start": "2049710",
    "end": "2056919"
  },
  {
    "text": "me like whether you are collecting data from the host OS of a node or the OS for",
    "start": "2056919",
    "end": "2066608"
  },
  {
    "text": "in inside docker images and the second question I have is it feels like the",
    "start": "2066609",
    "end": "2073450"
  },
  {
    "text": "correlation between the OS data and the kubernetes data might not be 100%",
    "start": "2073450",
    "end": "2081030"
  },
  {
    "text": "correct especially when there are dynamic changes to iptables I don't know",
    "start": "2081030",
    "end": "2087669"
  },
  {
    "text": "- like pod going up and down and state changes in kubernetes okay so the first",
    "start": "2087669",
    "end": "2096339"
  },
  {
    "text": "question was are we monitoring inside containers or outside containers well containers are built as a lightweight",
    "start": "2096339",
    "end": "2102839"
  },
  {
    "text": "isolation mechanism in the Linux kernel so by running as a privilege so you can",
    "start": "2102839",
    "end": "2108520"
  },
  {
    "text": "run as a privileged container or on the host and see everything that's running on the host so you get this visibility",
    "start": "2108520",
    "end": "2115359"
  },
  {
    "text": "into the containers while running one agent so this is not a sidecar you can",
    "start": "2115359",
    "end": "2121720"
  },
  {
    "text": "run just one privilege daemon set on your machines your second question was",
    "start": "2121720",
    "end": "2128130"
  },
  {
    "text": "I'm correlation between the oh yes so",
    "start": "2128130",
    "end": "2133450"
  },
  {
    "text": "how do you match the data between your Orchestrator and your notes that the",
    "start": "2133450",
    "end": "2138609"
  },
  {
    "text": "correctness the correctness is correct yeah yeah so luckily many of for example",
    "start": "2138609",
    "end": "2144579"
  },
  {
    "text": "containers have a unique ID that is shared across the operating system and",
    "start": "2144579",
    "end": "2149829"
  },
  {
    "text": "kubernetes so if you look at your kubernetes pod description when you see",
    "start": "2149829",
    "end": "2156640"
  },
  {
    "text": "the containers in there it has the same UID that is used inside kernel data",
    "start": "2156640",
    "end": "2161710"
  },
  {
    "text": "structures so you can match those and we do so kind of the the system we built it does with pids for example you can have",
    "start": "2161710",
    "end": "2170349"
  },
  {
    "text": "these problems if you collected process identifier then you'd have this problem where maybe the kernel reuses the same",
    "start": "2170349",
    "end": "2176920"
  },
  {
    "text": "PID multiple times but you rarely need to use that TCP for example so port",
    "start": "2176920",
    "end": "2184180"
  },
  {
    "text": "numbers are not reused very quickly usually so you could rely on that but frequently we just rely on container",
    "start": "2184180",
    "end": "2190120"
  },
  {
    "text": "information so you know the socket what container it was and then the other side what container the saw the other side of",
    "start": "2190120",
    "end": "2196150"
  },
  {
    "text": "the socket was so you can match the metadata that way and that's there's very little error you can do that done",
    "start": "2196150",
    "end": "2211080"
  },
  {
    "text": "thank you everybody [Applause]",
    "start": "2211080",
    "end": "2216119"
  },
  {
    "text": "how could you take more questions here",
    "start": "2216150",
    "end": "2220380"
  }
]