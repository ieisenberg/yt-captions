[
  {
    "text": "so my name is Joel verac I'm a senior system engineer at open systems um and I",
    "start": "199",
    "end": "5640"
  },
  {
    "text": "work in the observability team and what does open systems do so we",
    "start": "5640",
    "end": "10840"
  },
  {
    "text": "are actually uh let's say a managed uh connectivity company so we offer um",
    "start": "10840",
    "end": "16680"
  },
  {
    "text": "managed Network Solutions and so it's quite interesting I think to be here at cucon which is a",
    "start": "16680",
    "end": "22000"
  },
  {
    "text": "kubernetes conference um and we actually ship mostly we work with these kind of boxes these kind of devices and we don't",
    "start": "22000",
    "end": "28320"
  },
  {
    "text": "run kubernetes on these devices so maybe it's interesting to think you know why are we here in the first place um we run",
    "start": "28320",
    "end": "35480"
  },
  {
    "text": "quite a lot of these hosts currently we have just over 10,000 um situated all over the world so they plug into all our",
    "start": "35480",
    "end": "42079"
  },
  {
    "text": "customers infrastructure um but today I'm not going to talk about how we monitor these hosts is sort of another",
    "start": "42079",
    "end": "48160"
  },
  {
    "text": "topic entirely if it sounds interesting and it is interesting um there's a link here to a to a talk I gave um at cucon",
    "start": "48160",
    "end": "55440"
  },
  {
    "text": "EU um back in April uh where I go into more sort of details about this",
    "start": "55440",
    "end": "60600"
  },
  {
    "text": "um but today uh we're going to talk about Thanos at least I hope that's what everyone is expecting um otherwise I wrote my my abstract very wrongly um in",
    "start": "60600",
    "end": "69600"
  },
  {
    "text": "particular what I would like to talk about is scalability resilience and performance of the right path so for us",
    "start": "69600",
    "end": "75960"
  },
  {
    "text": "all of our metrics are sort of customer facing so if there are problems with the metrics it comes back to us very very",
    "start": "75960",
    "end": "81240"
  },
  {
    "text": "quickly you know customers complain um so we're really really focused on making sure that we get every metric which is",
    "start": "81240",
    "end": "87400"
  },
  {
    "text": "shipped to us from all of those devices um and at the end if there's time um",
    "start": "87400",
    "end": "92600"
  },
  {
    "text": "this sort of one weird trick that reduced storage costs for us by quite a significant number um it was kind of an",
    "start": "92600",
    "end": "99119"
  },
  {
    "text": "interesting uh Journey let's say debugging Journey um as to what was going wrong um so hopefully we can get",
    "start": "99119",
    "end": "105240"
  },
  {
    "text": "around to that as well so hopefully most of you are aware of what Thanos is if not you will know what Prometheus is um",
    "start": "105240",
    "end": "112680"
  },
  {
    "text": "otherwise I'll do my best to accomodate for everybody um so Thanos is a framework which is built around Prometheus okay so it's um it's you know",
    "start": "112680",
    "end": "120719"
  },
  {
    "text": "Prometheus of course is the the the sort of deao metrics back end for most kubernetes clusters I think if you spin",
    "start": "120719",
    "end": "127280"
  },
  {
    "text": "up a kubernetes cluster today uh one of the first things you do is you go and install Cube prom stack to just get",
    "start": "127280",
    "end": "132840"
  },
  {
    "text": "basic monitoring in place um so Thanos is sort of wrapping around Prometheus and extending its capabilities so it",
    "start": "132840",
    "end": "139720"
  },
  {
    "text": "offers a global query view um so this is really nice if you have multiple uh you know maybe tens or hundreds of",
    "start": "139720",
    "end": "145640"
  },
  {
    "text": "kubernetes classes which you want to look at metrics in one pane of glass um Thanos can kind of plug into all of",
    "start": "145640",
    "end": "151720"
  },
  {
    "text": "those different promethee um it offers unlimited retention so one of the let's say drawbacks of Prometheus is that you",
    "start": "151720",
    "end": "158959"
  },
  {
    "text": "cannot have um you know retention Beyond let's say a few months otherwise querying becomes sort of just unfeasible",
    "start": "158959",
    "end": "166200"
  },
  {
    "text": "due to the decompression time um it's inherently Prometheus compatible uh and",
    "start": "166200",
    "end": "171519"
  },
  {
    "text": "it also has some new features like down sampling um which yeah actually enables the unlimited retention by sort of",
    "start": "171519",
    "end": "178040"
  },
  {
    "text": "reducing the amount of time it takes to decompress long uh Long Live samples or long life",
    "start": "178040",
    "end": "184480"
  },
  {
    "text": "samples so I mean let's start with uh you know how do we actually get data into Thanos um so we're going to start",
    "start": "184480",
    "end": "191239"
  },
  {
    "text": "with you know where everyone is familiar I think with a with a Prometheus instance putting data into a Time series",
    "start": "191239",
    "end": "196640"
  },
  {
    "text": "database this can be anywhere it can be running on a host somewhere it can be running in kubernetes can be yeah it's",
    "start": "196640",
    "end": "201799"
  },
  {
    "text": "just Prometheus doing its thing um and so the the sort of classic way of running Thanos is actually to run it as",
    "start": "201799",
    "end": "207840"
  },
  {
    "text": "a side car so uh there is this sidecar um module for Thanos or sort of mode of",
    "start": "207840",
    "end": "214560"
  },
  {
    "text": "operation um which basically Scoops up the blocks which are coming from Prometheus in the TSB um and then it",
    "start": "214560",
    "end": "221080"
  },
  {
    "text": "sort of exposes them via this store API and the store API is a a concept which which sort of Thanos introduced which",
    "start": "221080",
    "end": "227959"
  },
  {
    "text": "basically lets different components plug into other components so we can plug aqueria into this sidecast store API and",
    "start": "227959",
    "end": "234599"
  },
  {
    "text": "then we can fetch data from the the side car as if we were fetching it from that Prometheus and and in addition the",
    "start": "234599",
    "end": "240480"
  },
  {
    "text": "sidecar can also put those blobs uh blocks into uh long-term storage into blob storage and then we have this",
    "start": "240480",
    "end": "246760"
  },
  {
    "text": "long-term retention which is also promised so this is really cool if you have multiple clusters um because you",
    "start": "246760",
    "end": "252560"
  },
  {
    "text": "know let's say we have Switzerland West um Us East and uh Europe north it's very",
    "start": "252560",
    "end": "258519"
  },
  {
    "text": "easy to you know make a global view of those metrics just by plugging in one quera into all of those sidecast store",
    "start": "258519",
    "end": "265759"
  },
  {
    "text": "API endpoints and at the same time all of that data gets flushed into long-term storage um and it's yeah it's a pretty",
    "start": "265759",
    "end": "272400"
  },
  {
    "text": "neat system but of course sometimes we can't use the sidecar approach um you know",
    "start": "272400",
    "end": "278560"
  },
  {
    "text": "there there are a number of cases where we can't directly plug into an external",
    "start": "278560",
    "end": "284039"
  },
  {
    "text": "um Prometheus instance maybe it's running on a host which we don't have direct access to maybe that host only",
    "start": "284039",
    "end": "289720"
  },
  {
    "text": "has egress access for example we can't get into the host um that's actually our situation we have 10,000 customer hosts",
    "start": "289720",
    "end": "296639"
  },
  {
    "text": "out there we can't plug into them directly they sort of they phone home but we don't talk to them directly um at",
    "start": "296639",
    "end": "303199"
  },
  {
    "text": "least not for the Telemetry so in this situation it's not feasible to you know to point the query",
    "start": "303199",
    "end": "310280"
  },
  {
    "text": "at the promethee so we need something different and this is where the Thanos receiver comes in so Thanos receiver is",
    "start": "310280",
    "end": "316080"
  },
  {
    "text": "a component which exposes a remote right compatible API so it's it's then very simple we",
    "start": "316080",
    "end": "322960"
  },
  {
    "text": "simply Point Prometheus at this um new receive component we tell it to remote",
    "start": "322960",
    "end": "328400"
  },
  {
    "text": "write the metrics there Prometheus is doing its thing scraping the data storing its local tsdb but we no longer",
    "start": "328400",
    "end": "334120"
  },
  {
    "text": "care really about this local tsdb so we get the metrics VI remote right um there",
    "start": "334120",
    "end": "339440"
  },
  {
    "text": "is a component called the rooting receiver and this guy is really responsible to validate the metrics",
    "start": "339440",
    "end": "344960"
  },
  {
    "text": "which are coming in just verify that everything looks okay um before sending it on to the ingesting receiver and this",
    "start": "344960",
    "end": "351440"
  },
  {
    "text": "is this is the component which is actually um writing those metrics into tstp format so the ingesting receiver is",
    "start": "351440",
    "end": "359919"
  },
  {
    "text": "um writing those metrics into a local TSB which it also stores it exposes a store API which means we can query those",
    "start": "359919",
    "end": "366840"
  },
  {
    "text": "metrics as they come in which is another very powerful thing we can see the metrics as they are coming in from the",
    "start": "366840",
    "end": "371919"
  },
  {
    "text": "external promethee eyes and it also puts those Blobs of sorry again blocks into",
    "start": "371919",
    "end": "377039"
  },
  {
    "text": "blob storage and so yeah we have a similar kind of model for expanding that to",
    "start": "377039",
    "end": "383039"
  },
  {
    "text": "multiple clusters or multiple hosts um you know we can basically Point all of",
    "start": "383039",
    "end": "388319"
  },
  {
    "text": "the prometh at one end point and then we have again the global query View and the long-term retention but just in a",
    "start": "388319",
    "end": "394080"
  },
  {
    "text": "different way",
    "start": "394080",
    "end": "397198"
  },
  {
    "text": "okay excuse me so our Thanos cluster we are actually ingesting um up to 200",
    "start": "399520",
    "end": "407319"
  },
  {
    "text": "million active time series um at any given time this is requiring 90 CPUs uh almost",
    "start": "407319",
    "end": "414680"
  },
  {
    "text": "a terabyte of memory um and 400 and sort of the the the receive and transmit out of the names space is 450 megabytes per",
    "start": "414680",
    "end": "421919"
  },
  {
    "text": "second so almost half a gigabyte of data coming in per second um and a quarter of a gigabyte of data going out per second",
    "start": "421919",
    "end": "428080"
  },
  {
    "text": "so I mean it's it's not Google levels but for our little uh company I think it's pretty",
    "start": "428080",
    "end": "433759"
  },
  {
    "text": "respectable our setup um just to sort of give you a you put it in a picture we",
    "start": "433759",
    "end": "439160"
  },
  {
    "text": "have our Fleet of 10,000 Edge devices these are really Linux machines running a promethus locally um and we also now",
    "start": "439160",
    "end": "445960"
  },
  {
    "text": "start to move into um the cloud so we have kubernetes clusters where we run customer workloads um at multiple points",
    "start": "445960",
    "end": "453520"
  },
  {
    "text": "of presence around the world um and these all need to ship their data home",
    "start": "453520",
    "end": "458960"
  },
  {
    "text": "we also have our Central kubernetes cluster based in Switzerland which is where we are and we' run uh ISO public",
    "start": "458960",
    "end": "464400"
  },
  {
    "text": "Ingress Gateway there now for actually collecting those Central uh those Central metrics we just",
    "start": "464400",
    "end": "470800"
  },
  {
    "text": "run Prometheus with a side car because that's like super easy why wouldn't we do that um but for the customer facing",
    "start": "470800",
    "end": "476680"
  },
  {
    "text": "metrics we need to have this you know push remote right um properties so the",
    "start": "476680",
    "end": "483280"
  },
  {
    "text": "customer devices push the metrics in they get sent to an otel collector where they are um this we do client",
    "start": "483280",
    "end": "490440"
  },
  {
    "text": "certificate authentication and then they are forwarded into different kind of pipelines based on uh the tency tency is",
    "start": "490440",
    "end": "496440"
  },
  {
    "text": "a topic we're going to cover here uh but you can see the type of metrics that we're collecting there proxy firewall um",
    "start": "496440",
    "end": "502400"
  },
  {
    "text": "wi airing Network estan these kind of things so things we really care about we",
    "start": "502400",
    "end": "508120"
  },
  {
    "text": "want to make sure that we can scale so to meet our our workload if we on board a new tenant if we if a tenant",
    "start": "508120",
    "end": "514599"
  },
  {
    "text": "doubles quadruples in size we need to make sure that our metrics backend can meet those and we also need to make sure",
    "start": "514599",
    "end": "520240"
  },
  {
    "text": "that it's resilient um outages are really bad like the customers really",
    "start": "520240",
    "end": "525360"
  },
  {
    "text": "complain if the graphs don't look good um this is to be expected of course now we also want to make sure that uh we",
    "start": "525360",
    "end": "531720"
  },
  {
    "text": "have kind of isolation between tenants so that one guy cannot ruin the party for everyone else so we have a basic",
    "start": "531720",
    "end": "537720"
  },
  {
    "text": "sort of quality of service for each attendant shipping metrics and we also really care about data availability",
    "start": "537720",
    "end": "543959"
  },
  {
    "text": "obviously the long-term storage data that that should be available right it's in Blob storage but what I'm talking",
    "start": "543959",
    "end": "549000"
  },
  {
    "text": "about there is actually the uh the latest the freshest data this has to be there because here we're sort of using",
    "start": "549000",
    "end": "555279"
  },
  {
    "text": "this data to calculate slos you know with recording rules um and if there are gaps in that this can really be a",
    "start": "555279",
    "end": "561839"
  },
  {
    "text": "problem so um hopefully that sets the groundwork now we're going to dive in a",
    "start": "561839",
    "end": "567120"
  },
  {
    "text": "little bit and get sort of slightly more technical um so let's think about how we might",
    "start": "567120",
    "end": "572360"
  },
  {
    "text": "deploy Thanos sort of this this receiver and let's take a very naive approach let's just deoy this as a as a",
    "start": "572360",
    "end": "578120"
  },
  {
    "text": "deployment okay so kubernetes stateless deployment um is this a good idea the",
    "start": "578120",
    "end": "583440"
  },
  {
    "text": "answer is no so if we have the incoming metrics what we need to do is of course load balance those metrics across this",
    "start": "583440",
    "end": "590519"
  },
  {
    "text": "deployment and of course we have to think you know how are we going to what's the strategy we use to load balance are we going to do round robin",
    "start": "590519",
    "end": "596040"
  },
  {
    "text": "are we going to do randomly sticky sessions maybe sticky s session sounds like a good idea right um each receiver",
    "start": "596040",
    "end": "602040"
  },
  {
    "text": "is maintaining a local TSB and if you know a bit about you know the the sort of the data structure of tstp it's",
    "start": "602040",
    "end": "608480"
  },
  {
    "text": "actually very bad to have samples kind of randomly distributed between two different tstps okay TSB really likes um",
    "start": "608480",
    "end": "615279"
  },
  {
    "text": "consistent Deltas between samples so a load balancer doesn't really work in this case this is why we have the",
    "start": "615279",
    "end": "622360"
  },
  {
    "text": "rooting receive component so now we're going to take a stateful approach we're going to make these guys a stateful set",
    "start": "622360",
    "end": "628399"
  },
  {
    "text": "we're going to store the local TSB persistently so that it survives uh",
    "start": "628399",
    "end": "633959"
  },
  {
    "text": "restarts and the rooting receiver um we're going to organize it into a hash ring so we're going to organize these",
    "start": "633959",
    "end": "640000"
  },
  {
    "text": "guys into a hash ring why would we like to do that let's say that we get um a label",
    "start": "640000",
    "end": "647720"
  },
  {
    "text": "set come in CPU usage for a given host what the routing receiver does is it hashes that label set and then it Maps",
    "start": "647720",
    "end": "654360"
  },
  {
    "text": "it to a given receiver applica this is um a very good quality because now every",
    "start": "654360",
    "end": "659639"
  },
  {
    "text": "time a sample comes in for that metric it will end up in the same tstp so this is good it solves that problem of",
    "start": "659639",
    "end": "665360"
  },
  {
    "text": "scattered samples between tsbs if we have another metric coming in with a different host ID maybe that gets mapped",
    "start": "665360",
    "end": "671480"
  },
  {
    "text": "to a different receipt maybe it gets mapped to the same one it doesn't really matter so it's kind of a way of um ensuring that we can split the load",
    "start": "671480",
    "end": "678399"
  },
  {
    "text": "between the receives but also make sure that uh you know we have consistency",
    "start": "678399",
    "end": "684040"
  },
  {
    "text": "within the the within the tstp uh instances now now in this picture how",
    "start": "684040",
    "end": "690519"
  },
  {
    "text": "can we deal with scaling so if we add another replica to this hash ring how can we sort of nicely handle this and",
    "start": "690519",
    "end": "697519"
  },
  {
    "text": "also what happens if a receiver becomes unhealthy how do we deal with this so we really need to have a system of hashing",
    "start": "697519",
    "end": "704680"
  },
  {
    "text": "management or something which can manage the hash ring for us there's a few different ways we can do it so one way",
    "start": "704680",
    "end": "709959"
  },
  {
    "text": "which is very valid is to do sort of a static definition of the hash Rings um and this is fine right so you can have a",
    "start": "709959",
    "end": "716079"
  },
  {
    "text": "Helm chart you can Define how many replicas you need you can Define um how the hash ring should be set you",
    "start": "716079",
    "end": "721600"
  },
  {
    "text": "can deploy it and that's fine but it just means that every time you want to change that hashing or every time something changes um you need to do a",
    "start": "721600",
    "end": "728360"
  },
  {
    "text": "new deployment it's not so terrible another option is to use a controller and so what the controller would do is",
    "start": "728360",
    "end": "734639"
  },
  {
    "text": "it would actually watch the replicas in the um in in the receive sort of stateful set and it would make sure that",
    "start": "734639",
    "end": "741399"
  },
  {
    "text": "you know if you change the number of replicas or if you if one of the receives becomes unhealthy then the the",
    "start": "741399",
    "end": "747199"
  },
  {
    "text": "controller should be able to respond to that and there is a controller which has been built um it's not part of the core",
    "start": "747199",
    "end": "752760"
  },
  {
    "text": "Thanos project it's part of this observatorium project but there is a few shared maintainers between the two",
    "start": "752760",
    "end": "758440"
  },
  {
    "text": "projects and so there's this Thanos receive controller and this is what we use in our production environment to",
    "start": "758440",
    "end": "763639"
  },
  {
    "text": "manage the hash rings so the way it works um we deploy the the receive controller just as a normal um you know",
    "start": "763639",
    "end": "771000"
  },
  {
    "text": "stateless deployment we have the rooting receives and the receive hash rings from",
    "start": "771000",
    "end": "776079"
  },
  {
    "text": "before and all we need to do first is to just label um which hash ring those",
    "start": "776079",
    "end": "781839"
  },
  {
    "text": "receives belong to so we just say you know hash ring is equal to Hash Zer we give this hash ring now a",
    "start": "781839",
    "end": "787160"
  },
  {
    "text": "name and then we set up a very basic config file which says hash ring is Hash zero so this is just saying that we have",
    "start": "787160",
    "end": "793240"
  },
  {
    "text": "a hash ring called hash zero we feed that into the receive controller it's called base. Json um and this receive",
    "start": "793240",
    "end": "800040"
  },
  {
    "text": "controller then generates um it generates the full hashing by looking up the end points of these receive pods so",
    "start": "800040",
    "end": "807120"
  },
  {
    "text": "the receive controller is interacting with the kubernetes API to say look I get the pods with this label I map those",
    "start": "807120",
    "end": "813000"
  },
  {
    "text": "end points into this hash ring and then I feed that into the rooting receiver okay so um what happens now when we",
    "start": "813000",
    "end": "820040"
  },
  {
    "text": "scale if a new receiver comes along E from horizontal P to scaling or we set",
    "start": "820040",
    "end": "825240"
  },
  {
    "text": "the replicas um the receive controller will recognize this and it will update that hashing generated file um and then",
    "start": "825240",
    "end": "832040"
  },
  {
    "text": "the routing receivers will load that in and they will start shipping metrics to the new uh to the new endpoint what what",
    "start": "832040",
    "end": "838839"
  },
  {
    "text": "happens if something becomes unhealthy again we have there is a bit of a let's say there is a bit of",
    "start": "838839",
    "end": "844240"
  },
  {
    "text": "crossover time or a bit of uncertainty between when the kubernetes API recognizes or when the kuet rep",
    "start": "844240",
    "end": "850920"
  },
  {
    "text": "recognizes that the pot is unhealthy it of course depends on how often your health checks are and those settings but",
    "start": "850920",
    "end": "857079"
  },
  {
    "text": "at some point that pod will be taken out of action and the receive controller will recognize that that receiver should",
    "start": "857079",
    "end": "863000"
  },
  {
    "text": "be removed from the hatching so this is really nice we have scalability and we have um resilience of",
    "start": "863000",
    "end": "869880"
  },
  {
    "text": "the hash Rings the key configuration if you want to do this um I would say this is the",
    "start": "869880",
    "end": "875519"
  },
  {
    "text": "most important thing to take away uh there is a new algorithm for cons uh",
    "start": "875519",
    "end": "880600"
  },
  {
    "text": "consistent hashing in Thanos it was I think introduced in the last year I have to check um it's called the kitama hash",
    "start": "880600",
    "end": "886600"
  },
  {
    "text": "ring previously these receives were using just a simple hash mod and of course with a hash mod the only argument",
    "start": "886600",
    "end": "892199"
  },
  {
    "text": "is the number you know the kind of sharding factor and I think if you've ever worked with this kind of hashing you know that if you remove remve one",
    "start": "892199",
    "end": "899440"
  },
  {
    "text": "instance from a hash mod it kind of scatters everything so there's no sort of consistency between replicas being",
    "start": "899440",
    "end": "905839"
  },
  {
    "text": "added or taken away from the the pool the consistent hash ring solves this um",
    "start": "905839",
    "end": "911199"
  },
  {
    "text": "we're not going to go into it fully here but it basically makes the effect of adding in or removing replicas from the",
    "start": "911199",
    "end": "917240"
  },
  {
    "text": "the hash ring much less um felt it makes it much more stable and on the receive controller",
    "start": "917240",
    "end": "923959"
  },
  {
    "text": "itself then you should definitely run with these two config options so only allow ready replicas this basically says",
    "start": "923959",
    "end": "929880"
  },
  {
    "text": "okay if the Pod is running that's not good enough it also needs to be ready it sounds obvious but you we we have to",
    "start": "929880",
    "end": "935519"
  },
  {
    "text": "configure that um that means the receiver will only be added to the hash ring once it's fully ready which means",
    "start": "935519",
    "end": "941000"
  },
  {
    "text": "the local tstv has been completely um you know created and spun up and it's ready to accept requests and another one",
    "start": "941000",
    "end": "947720"
  },
  {
    "text": "is to allow Dynamic scaling in the default configuration this received controller does not um update when",
    "start": "947720",
    "end": "953959"
  },
  {
    "text": "things are taken out of the hash ring but if you enable this new flag I think you have to make sure on one of the",
    "start": "953959",
    "end": "959079"
  },
  {
    "text": "latest versions um then it will also update this hashing dynamically and so this yeah if you if",
    "start": "959079",
    "end": "966040"
  },
  {
    "text": "you're at this point then I think you can say you know this was the end of quite some internal uh work let's say to",
    "start": "966040",
    "end": "972040"
  },
  {
    "text": "get this uh these hashing stable it was a lot of fun um but you know things can",
    "start": "972040",
    "end": "978480"
  },
  {
    "text": "still go wrong right and of course what happens is you have a beautiful platform and then you unleash users on it right",
    "start": "978480",
    "end": "984600"
  },
  {
    "text": "so users are still sending um metrics to our platform and we don't really have",
    "start": "984600",
    "end": "990079"
  },
  {
    "text": "much control over what comes in so there's something which we call the Pearl hash incident internally and it's",
    "start": "990079",
    "end": "996560"
  },
  {
    "text": "pretty simple what happened um somehow a label value was set to a hash",
    "start": "996560",
    "end": "1003399"
  },
  {
    "text": "instead of the actual value referenced by the hash it sort of you forgot to De reference a poter this led to a big problem because",
    "start": "1003399",
    "end": "1010800"
  },
  {
    "text": "we suddenly had you know once that uh deployment was live we suddenly had a very very very Noisy Neighbor within our",
    "start": "1010800",
    "end": "1018120"
  },
  {
    "text": "um hashing okay so we had a a huge this is basically textbook cardinality",
    "start": "1018120",
    "end": "1024000"
  },
  {
    "text": "explosion and as we know Prometheus and Thanos by extension does not like High cardinality um time series and so then",
    "start": "1024000",
    "end": "1031678"
  },
  {
    "text": "we very quickly ran into this sort of failure Cascade where one receiver kills over the low gets split to the next two",
    "start": "1031679",
    "end": "1038079"
  },
  {
    "text": "but of course they're already killing over and eventually you have a hash ring which is completely",
    "start": "1038079",
    "end": "1043959"
  },
  {
    "text": "unrecoverable so one Troublesome tenant can really lead to a full service outage",
    "start": "1043959",
    "end": "1049000"
  },
  {
    "text": "this is not ideal so how can we solve it um the you know let's say the obvious or",
    "start": "1049000",
    "end": "1054160"
  },
  {
    "text": "the the hopefully the obvious way to deal with it is actually just say okay the Troublesome tenant can get its own",
    "start": "1054160",
    "end": "1060720"
  },
  {
    "text": "hashing and this is um this is good because then if if that Troublesome tenant starts to make trouble again um",
    "start": "1060720",
    "end": "1068520"
  },
  {
    "text": "the outage will be contained to its own hashing right so we still get metrics from 90% of the of the you know the good",
    "start": "1068520",
    "end": "1074520"
  },
  {
    "text": "tenants um but this guy down here we can go back to the service team and say hey look you're currently not ingesting",
    "start": "1074520",
    "end": "1079840"
  },
  {
    "text": "metrics because you know you need to go and investigate so that's good this is called hard tency where you sort of",
    "start": "1079840",
    "end": "1086440"
  },
  {
    "text": "physically separate you have physically separate infrastructure for handling different metrics coming in um and to",
    "start": "1086440",
    "end": "1092360"
  },
  {
    "text": "actually create that with the receive controller it's pretty simple all you need to do is create a second hash ring",
    "start": "1092360",
    "end": "1098679"
  },
  {
    "text": "in your Json manifest and then you need to map tenants um you just need to map",
    "start": "1098679",
    "end": "1104679"
  },
  {
    "text": "the tenants to which hashing they belong to and down here you see we don't map any tenants this is saying if you can't",
    "start": "1104679",
    "end": "1111200"
  },
  {
    "text": "find a hash ring for a given tenant just send it to the default hash ring okay so this is like the soft tency hash ring",
    "start": "1111200",
    "end": "1117200"
  },
  {
    "text": "where everything will just be mixed together and this is the dedicated hash ring for the Troublesome",
    "start": "1117200",
    "end": "1123080"
  },
  {
    "text": "tenants of course you might not want to jugle uh to juggle multiple hash Rings",
    "start": "1123080",
    "end": "1128280"
  },
  {
    "text": "there is of course an additional complexity there you have to M manage multiple stateful sets um there is",
    "start": "1128280",
    "end": "1134000"
  },
  {
    "text": "another option which is called active series limiting again this is a uh a relatively recent feature of Thanos um I",
    "start": "1134000",
    "end": "1141000"
  },
  {
    "text": "think since 0. 28 maybe um but this allows us to actually um look at the",
    "start": "1141000",
    "end": "1148000"
  },
  {
    "text": "number of series coming in and then just limit dynamically based on when they get above a certain",
    "start": "1148000",
    "end": "1153720"
  },
  {
    "text": "level and this is the basic idea so we have an idea of how many series we can",
    "start": "1153720",
    "end": "1159640"
  },
  {
    "text": "manage you know based on our resources which we've deployed in the in the receives um and what we need to do is we",
    "start": "1159640",
    "end": "1165840"
  },
  {
    "text": "just need to query what the current metric value is for for that head series each of the receives like Prometheus has",
    "start": "1165840",
    "end": "1171880"
  },
  {
    "text": "a metric called um head series Prometheus time series head or head time",
    "start": "1171880",
    "end": "1178600"
  },
  {
    "text": "series something like that pops up later so we can actually count that um live",
    "start": "1178600",
    "end": "1183640"
  },
  {
    "text": "and then it's we just need to tell the rooting receiver to limit if the number of active head series is higher than",
    "start": "1183640",
    "end": "1189919"
  },
  {
    "text": "some value which we configure let's see how it looks here's our hash ring here's",
    "start": "1189919",
    "end": "1195080"
  },
  {
    "text": "our noisy tenant we have to introduce a new meta monitoring monitoring queria which is going to fetch those current",
    "start": "1195080",
    "end": "1201159"
  },
  {
    "text": "head series values and it's a query which looks exactly like this this is exactly the query which is run and so in",
    "start": "1201159",
    "end": "1207080"
  },
  {
    "text": "this example for example you can see the sdp tenant has 28 million um head series",
    "start": "1207080",
    "end": "1212919"
  },
  {
    "text": "the van tenant has 23 million head series okay the bandwidth control has 9 million and so then we just need to",
    "start": "1212919",
    "end": "1219080"
  },
  {
    "text": "configure we need to tell the rooting receiver how it should behave how it should limit those tenants so of course",
    "start": "1219080",
    "end": "1224600"
  },
  {
    "text": "in this case we're in trouble because sdp is only allowed 2,000 head series okay in this case we would have a lot of",
    "start": "1224600",
    "end": "1230159"
  },
  {
    "text": "limiting based on STP but this is where that sort of tweaking based on your operational know knowledge comes in and",
    "start": "1230159",
    "end": "1236760"
  },
  {
    "text": "in this way we can really shrink the size um of the metric coming in um we",
    "start": "1236760",
    "end": "1242679"
  },
  {
    "text": "also return a retriable error to the tenant who is trying to send too many metrics so we can alert on that we can",
    "start": "1242679",
    "end": "1248559"
  },
  {
    "text": "you know we can go and look as to why that uh that situation is started to happen and of course Nothing Stops us",
    "start": "1248559",
    "end": "1255679"
  },
  {
    "text": "from taking a hybrid approach so you can have separate hash rings and you can also um have active series limiting per",
    "start": "1255679",
    "end": "1262880"
  },
  {
    "text": "hash ring if you want to and this is actually what we do in production to um to basically keep each each hash ring",
    "start": "1262880",
    "end": "1268480"
  },
  {
    "text": "healthy um but also minimize the effect of noisy",
    "start": "1268480",
    "end": "1273679"
  },
  {
    "text": "neighbors now this is all well and good so now we have scalability we have resilience and we have um let's say",
    "start": "1273679",
    "end": "1279600"
  },
  {
    "text": "happy customers but there are still some issues so in our customer portal we have",
    "start": "1279600",
    "end": "1285679"
  },
  {
    "text": "graphs and we have metrics uh or graph which are fed from metric and one of those graphs for example is availability",
    "start": "1285679",
    "end": "1291840"
  },
  {
    "text": "of a given host and this is based on a recording rule which is constantly being evaluated on data which is coming in um",
    "start": "1291840",
    "end": "1298279"
  },
  {
    "text": "and when we have an outage I mean it's it's important because we have slas which are you know related to those",
    "start": "1298279",
    "end": "1303480"
  },
  {
    "text": "outages so if we have a lot of you know incorrect outages then that's not good for us we have to go and justify to the",
    "start": "1303480",
    "end": "1309559"
  },
  {
    "text": "customer um that this was not really an outage it was a matrix problem so we were you know reported it was reported",
    "start": "1309559",
    "end": "1316159"
  },
  {
    "text": "that there was some uh spirous outages let's say some outages which were not real outages and so we went and looked",
    "start": "1316159",
    "end": "1323400"
  },
  {
    "text": "into the data and actually it turned out that we you know the rules were being evaluated but there were gaps in those",
    "start": "1323400",
    "end": "1329120"
  },
  {
    "text": "rules so we had some mised rule evaluations or we had some time when you know data was not there so the rules",
    "start": "1329120",
    "end": "1335080"
  },
  {
    "text": "could not be evaluated so what's going on",
    "start": "1335080",
    "end": "1340159"
  },
  {
    "text": "here here's our um setup right so let's say that we have data which is required",
    "start": "1340159",
    "end": "1346400"
  },
  {
    "text": "for a given uh for a given rule to be evaluated um and that's being hashed or that's being sent to receiver one okay",
    "start": "1346400",
    "end": "1353159"
  },
  {
    "text": "the ruler is picking up the data from receiver one evaluating the rule shipping it off into blob storage",
    "start": "1353159",
    "end": "1358200"
  },
  {
    "text": "everything is working fine now if receiver one has an issue of course that data can no longer be written and then",
    "start": "1358200",
    "end": "1364840"
  },
  {
    "text": "the ruler can no longer evaluate it's going to evaluate to some missing or you know just missing metric this gives us",
    "start": "1364840",
    "end": "1370279"
  },
  {
    "text": "the gaps in the data um which we which we saw and of course you can still have",
    "start": "1370279",
    "end": "1375480"
  },
  {
    "text": "you know the receive controller working um but there is this period after you know if a if a receive crashes there is",
    "start": "1375480",
    "end": "1382440"
  },
  {
    "text": "this in between time you know the cuer has to realize that it needs to reboot the Pod and needs to take it out um it's",
    "start": "1382440",
    "end": "1388919"
  },
  {
    "text": "it's not a perfect system these things don't happen instantaneously so it can be that a rule is evaluated at the same",
    "start": "1388919",
    "end": "1395039"
  },
  {
    "text": "time as a receive is not available so we can actually make things better just by setting one config which",
    "start": "1395039",
    "end": "1401760"
  },
  {
    "text": "is a replication Factor so especially only with the uh the receives so with",
    "start": "1401760",
    "end": "1407520"
  },
  {
    "text": "the the remote right approach we can replicate the data which is coming in and so this is what happens um if we",
    "start": "1407520",
    "end": "1413919"
  },
  {
    "text": "have a replication factor of three the incoming time series still gets shipped to the same um part but it also gets",
    "start": "1413919",
    "end": "1421880"
  },
  {
    "text": "replicated to two other receives so we have three copies of the data and of",
    "start": "1421880",
    "end": "1427080"
  },
  {
    "text": "course then the ruler can read from those three copies and this is fine if one of the receives goes down then",
    "start": "1427080",
    "end": "1434159"
  },
  {
    "text": "that's okay because we still have two more copies so the ruler can still proceed and can can still evaluate its rules everything is fine so this really",
    "start": "1434159",
    "end": "1441480"
  },
  {
    "text": "in in increases your failure tolerance or tolerance to sort of PODS disappearing pods not being respons",
    "start": "1441480",
    "end": "1448400"
  },
  {
    "text": "responsive you have to be careful um because with this replication comes this",
    "start": "1448400",
    "end": "1453600"
  },
  {
    "text": "uh concept of Quorum so for a given replication Factor there is a you know",
    "start": "1453600",
    "end": "1458760"
  },
  {
    "text": "just a formula q = r 2 + 1 so the Quorum factor is equal to the replication Factor divided 2 + 1 I think this is uh",
    "start": "1458760",
    "end": "1466600"
  },
  {
    "text": "you know you've probably seen this before in other um other fields so for example with our replication factor of",
    "start": "1466600",
    "end": "1472360"
  },
  {
    "text": "three we have a quorum of two and what that means is that at least two receives must acknowledge a right in order for",
    "start": "1472360",
    "end": "1479520"
  },
  {
    "text": "that right to have been considered successful so the rooting receives will make sure that um at least two of the",
    "start": "1479520",
    "end": "1485679"
  },
  {
    "text": "received succeeded that gives us you know we can calculate then what the max unavailable is for a given replication",
    "start": "1485679",
    "end": "1491399"
  },
  {
    "text": "factor and sort of you can use that to identify you know you know what your fault tolerance tolerance is um you can",
    "start": "1491399",
    "end": "1497480"
  },
  {
    "text": "use that to uh to plan your deployments the key thing is that you should ensure that the minimum number of replicas is",
    "start": "1497480",
    "end": "1504600"
  },
  {
    "text": "larger or equal to the replication Factor so that's a it's let's say it's a",
    "start": "1504600",
    "end": "1509720"
  },
  {
    "text": "very simple thing you can do just enable replication but you should um consider whether you really need it in our case",
    "start": "1509720",
    "end": "1516279"
  },
  {
    "text": "we really need it because the the rules must be evaluated we need a very very high successful rule evaluation um it",
    "start": "1516279",
    "end": "1525000"
  },
  {
    "text": "doesn't actually increase your storage cost because you can dup the data later with the compactor we're not going to go",
    "start": "1525000",
    "end": "1531399"
  },
  {
    "text": "into that today there's not enough time I would love to do a whole talk just on the compactor maybe next year um but one",
    "start": "1531399",
    "end": "1537480"
  },
  {
    "text": "of the things you can do is use a pod disruption budget which matches your max unavailable um just to make sure that",
    "start": "1537480",
    "end": "1543840"
  },
  {
    "text": "you're kind of resilient in the face of you know nodes going down cluster reboots these kind of things which",
    "start": "1543840",
    "end": "1548880"
  },
  {
    "text": "happen uh especially with managed kubernetes providers so everything was good um of",
    "start": "1548880",
    "end": "1556960"
  },
  {
    "text": "course this is how it feels sometimes when you're using the cloud right especially kubernetes and storage",
    "start": "1556960",
    "end": "1562440"
  },
  {
    "text": "especially with metrics and we also do logs and these kind of storage costs they just they can go up so it really",
    "start": "1562440",
    "end": "1568600"
  },
  {
    "text": "feels like this and of course when your boss checks the Azure budget uh over breakfast this can happen this actually",
    "start": "1568600",
    "end": "1574240"
  },
  {
    "text": "happened so um we were asked sort of um you know politely but firmly if we could",
    "start": "1574240",
    "end": "1581720"
  },
  {
    "text": "have a look at the cloud costs and bring them down and so we started to investigate um and we really did see",
    "start": "1581720",
    "end": "1587960"
  },
  {
    "text": "sort of a big increase in Cloud cost over time which didn't seem to correlate with Pure Storage okay but they were",
    "start": "1587960",
    "end": "1593520"
  },
  {
    "text": "coming from the storage account and so we traced it down to this component called the",
    "start": "1593520",
    "end": "1598840"
  },
  {
    "text": "compactor and what it turned out was that the the cause of the storage cost increase was actually requests to the",
    "start": "1598840",
    "end": "1605320"
  },
  {
    "text": "storage account so it wasn't Pure Storage it was just sort of you know write requests read requests these kind",
    "start": "1605320",
    "end": "1610559"
  },
  {
    "text": "of things were going up and they were pretty expensive and it turned out that this component called the compactor what",
    "start": "1610559",
    "end": "1616960"
  },
  {
    "text": "it does is is it periodically every 5 minutes it goes to the blob storage and it does some sort of uh it does a lot of",
    "start": "1616960",
    "end": "1623720"
  },
  {
    "text": "housekeeping so it does retention which means it deletes old blocks it does down",
    "start": "1623720",
    "end": "1628960"
  },
  {
    "text": "sampling uh it does compaction of blocks as well but the key thing is deletion okay oops I've gone too far the key",
    "start": "1628960",
    "end": "1635360"
  },
  {
    "text": "thing is deletion so it will remove blocks which it deems are no longer necessary and the reason why a block is",
    "start": "1635360",
    "end": "1642200"
  },
  {
    "text": "deemed no no longer necessary could be it's been marked for deletion which is perfectly fine or it's a partially",
    "start": "1642200",
    "end": "1648159"
  },
  {
    "text": "uploaded block and a partially uploaded block is just you know if a receive has been uploading some data it's a big",
    "start": "1648159",
    "end": "1653960"
  },
  {
    "text": "amount of data if it gets interrupted during that process then you have a kind of Unfinished Block in Blob storage so",
    "start": "1653960",
    "end": "1660279"
  },
  {
    "text": "this is what a partially uploaded block is um it usually gets solved when the receiver reboots because then it can",
    "start": "1660279",
    "end": "1666600"
  },
  {
    "text": "kind of finish uploading that block but it gets put into a new block ID so you tend to have these you know blocks",
    "start": "1666600",
    "end": "1672159"
  },
  {
    "text": "hanging around so I'm going to just whiz through some logs you don't have to read all the",
    "start": "1672159",
    "end": "1677960"
  },
  {
    "text": "logs but this I just want to take you on the Journey of of of how we we track this down I've got 3 minutes let's",
    "start": "1677960",
    "end": "1685399"
  },
  {
    "text": "say so what usually happens the compactor realizes that we have um a",
    "start": "1685399",
    "end": "1691000"
  },
  {
    "text": "block it needs to remove this is partial equals one it will say it found the block and then it marked the block for",
    "start": "1691000",
    "end": "1696320"
  },
  {
    "text": "deletion and then it deleted the block and we see this sort of happy path with the puzzle uh so the puzzle is",
    "start": "1696320",
    "end": "1703480"
  },
  {
    "text": "why is this partials going up and up and up the number of partial blocks is keeping on increasing which means we",
    "start": "1703480",
    "end": "1709440"
  },
  {
    "text": "have a bunch of garbage basically in the blob storage and it looks like the compactor",
    "start": "1709440",
    "end": "1715440"
  },
  {
    "text": "is actually marking these blocks for deletion but they're not being deleted so what's going on when you go to the",
    "start": "1715440",
    "end": "1722159"
  },
  {
    "text": "block storage um you actually see this nice directory View and uh you know it",
    "start": "1722159",
    "end": "1727720"
  },
  {
    "text": "looks like you've got files you've got directories you can sort of Si through them like this this is actually a lie",
    "start": "1727720",
    "end": "1734360"
  },
  {
    "text": "okay so this is not real this is um a facade which the cloud providers do to",
    "start": "1734360",
    "end": "1740039"
  },
  {
    "text": "make you feel at home right um it's actually you know block storage is a flat name space and what that means is",
    "start": "1740039",
    "end": "1746559"
  },
  {
    "text": "that everything is a file there's no directories at least that's how it should be that's how we expect it to",
    "start": "1746559",
    "end": "1751799"
  },
  {
    "text": "be so it's really an illusion and the way to say is you know if you were to have files like this on your on your",
    "start": "1751799",
    "end": "1757840"
  },
  {
    "text": "local machine it would look like this a hierarchical nam space is what we expect right this is a kind of standard tree",
    "start": "1757840",
    "end": "1764360"
  },
  {
    "text": "structure um with directories and we can move between these directories and yeah it's all understood so our cloud",
    "start": "1764360",
    "end": "1770440"
  },
  {
    "text": "provider Azure actually has this hierarchical name space which can be enabled for blob storage and it turned",
    "start": "1770440",
    "end": "1775679"
  },
  {
    "text": "out it was enabled by default by our own automation so our terraform pipelines",
    "start": "1775679",
    "end": "1780799"
  },
  {
    "text": "were causing us issues so when we went to these blocks which were partially uploaded blocks",
    "start": "1780799",
    "end": "1786320"
  },
  {
    "text": "they just looked like empty directories and this didn't make sense because we're like well that doesn't make sense for",
    "start": "1786320",
    "end": "1792919"
  },
  {
    "text": "for flat kind of blob storage right so what was actually happening this was kind of a ghost Left Behind from Thanos",
    "start": "1792919",
    "end": "1799200"
  },
  {
    "text": "deleting blobs and then the Hier hierarchical name space just left Behind these these uh these empty directories",
    "start": "1799200",
    "end": "1806039"
  },
  {
    "text": "and of course Thanos there's no way to delete that right so it was really strange it's really this situation right",
    "start": "1806039",
    "end": "1812200"
  },
  {
    "text": "we just have two empty directories in plob storage so this I mean the picture kind of sums it up that the cloud was um",
    "start": "1812200",
    "end": "1819760"
  },
  {
    "text": "charging us for these uh for these delete requests to completely non-existent blocks and this was really",
    "start": "1819760",
    "end": "1826600"
  },
  {
    "text": "thousands of delet requests every 5 minutes which just got worse and worse and worse the take-home message for the",
    "start": "1826600",
    "end": "1832039"
  },
  {
    "text": "blob storage is if you're using Thanos if you're using you know if you make sure you understand which settings have",
    "start": "1832039",
    "end": "1837799"
  },
  {
    "text": "been enabled um because this really led to like 86% reduction in our storage",
    "start": "1837799",
    "end": "1843039"
  },
  {
    "text": "account costs just from transactions it was crazy so summing up I've uh there's four",
    "start": "1843039",
    "end": "1849519"
  },
  {
    "text": "minutes left for questions um yeah thank you very much this has been uh our",
    "start": "1849519",
    "end": "1855360"
  },
  {
    "text": "journey with the Thanos right path and also some fun um you know deep dives into cloud provider features and there's",
    "start": "1855360",
    "end": "1863840"
  },
  {
    "text": "my talk thank you very [Applause]",
    "start": "1863840",
    "end": "1872039"
  },
  {
    "text": "much",
    "start": "1872039",
    "end": "1875039"
  },
  {
    "text": "anyone hi uh with multiple Thanos receivers writing data into uh blob",
    "start": "1879240",
    "end": "1886159"
  },
  {
    "text": "storage um is there a way to choose uh",
    "start": "1886159",
    "end": "1891279"
  },
  {
    "text": "only to write from one of the receivers instead of multiple because when we are doing",
    "start": "1891279",
    "end": "1896600"
  },
  {
    "text": "duplication uh most of the work done by compactor is just D duplicating whatever",
    "start": "1896600",
    "end": "1903080"
  },
  {
    "text": "was replicated right so how can we avoid that there's no way to say okay this",
    "start": "1903080",
    "end": "1908880"
  },
  {
    "text": "receiver will write to blob storage this receiver won't and that won't work I think because it's it's kind of you",
    "start": "1908880",
    "end": "1916200"
  },
  {
    "text": "can't really govern how those series are are hashed between the receivers right you can't guarantee that one will always",
    "start": "1916200",
    "end": "1922480"
  },
  {
    "text": "be there because as it moves in and out the consistent hashing will update and so you can't really do that you have to",
    "start": "1922480",
    "end": "1927919"
  },
  {
    "text": "do the D duplication in the compactor I think there's no way around it um I have two question so first one",
    "start": "1927919",
    "end": "1936080"
  },
  {
    "text": "is have you clean your data before you send to the receiver uh C clean the data like a",
    "start": "1936080",
    "end": "1942960"
  },
  {
    "text": "clean the data before we send it to the receiver like a recording door and then then you only scrap the um the",
    "start": "1942960",
    "end": "1949919"
  },
  {
    "text": "data we don't do any cleaning but we have so the data goes through like an",
    "start": "1949919",
    "end": "1955440"
  },
  {
    "text": "collector pipeline where we sort of stamp it with uniform metadata and these kind of things currently you basically",
    "start": "1955440",
    "end": "1962120"
  },
  {
    "text": "send all the raw data from the pereus to the receiver yeah okay so have you ever",
    "start": "1962120",
    "end": "1967240"
  },
  {
    "text": "considered about using the uh pereus Federation so what's the difference",
    "start": "1967240",
    "end": "1972559"
  },
  {
    "text": "between the Federation perms and the thumel So Okay the reason we have to do",
    "start": "1972559",
    "end": "1979039"
  },
  {
    "text": "it with Thanos is because we want to collect the data from all these different Prometheus right or do you mean to actually use ental Prometheus",
    "start": "1979039",
    "end": "1985799"
  },
  {
    "text": "instead of Thanos um so Prometheus Federation also has ability to scrap",
    "start": "1985799",
    "end": "1992639"
  },
  {
    "text": "from the different cluster yeah yeah so I think it's a little bit similar to how",
    "start": "1992639",
    "end": "1998720"
  },
  {
    "text": "stos can uh received from the different cluster so I was wondering what's the",
    "start": "1998720",
    "end": "2004600"
  },
  {
    "text": "difference between the two and why you choose the Thanos okay so um one of the",
    "start": "2004600",
    "end": "2010639"
  },
  {
    "text": "reasons for using Thanos is because um especially for the long-term retention",
    "start": "2010639",
    "end": "2016080"
  },
  {
    "text": "so for the long-term retention of the Prometheus data you really must use Thanos um just because if you need to go",
    "start": "2016080",
    "end": "2022320"
  },
  {
    "text": "beyond a few months it's it's not feasible um to query those data from",
    "start": "2022320",
    "end": "2028080"
  },
  {
    "text": "Prometheus um yeah I'm not I'm I'm not sure I'd",
    "start": "2028080",
    "end": "2035159"
  },
  {
    "text": "have to I'd have to think about that a a bit longer if you just wanted to use base Prometheus to to receive the data",
    "start": "2035159",
    "end": "2041639"
  },
  {
    "text": "okay good thanks so what would you consider truly",
    "start": "2041639",
    "end": "2046760"
  },
  {
    "text": "High cardinality uh I saw two million uh that seems like obene as far as",
    "start": "2046760",
    "end": "2052520"
  },
  {
    "text": "cardinality but if we were looking to track cardinality explosion what would you say was a good starting point I mean",
    "start": "2052520",
    "end": "2059118"
  },
  {
    "text": "it depends how much you want to scale it really depends on your use case for example I mean it's not it's not related",
    "start": "2059119",
    "end": "2064960"
  },
  {
    "text": "to Thanos but um we also run Loki so Loki again has this you know cardinality",
    "start": "2064960",
    "end": "2070040"
  },
  {
    "text": "with chunks and stuff for Loki we have we hit almost a million streams right",
    "start": "2070040",
    "end": "2075240"
  },
  {
    "text": "which for Loki is pretty huge but for our use case it was absolutely necessary because we needed to slice by",
    "start": "2075240",
    "end": "2081480"
  },
  {
    "text": "application as well as by host right so we need it kind of horizontal across hosts and per",
    "start": "2081480",
    "end": "2087320"
  },
  {
    "text": "host um I mean it's that's that's a hard question to answer because it will depend on you know your your system",
    "start": "2087320",
    "end": "2094720"
  },
  {
    "text": "right cality kind of starting to break what a would allow for or is a sh of",
    "start": "2094720",
    "end": "2101640"
  },
  {
    "text": "data yeah that's true it's that's that's that's sums it",
    "start": "2101640",
    "end": "2107040"
  },
  {
    "text": "up um so I had a question regarding uh scaling out so what would you consider",
    "start": "2108640",
    "end": "2114079"
  },
  {
    "text": "the the bottleneck so when you're adding more and more tenants what's the first thing you're like okay now this is the",
    "start": "2114079",
    "end": "2119320"
  },
  {
    "text": "component in Tel that we have to think about and the receives for sure the receivers yeah the receivers yeah okay",
    "start": "2119320",
    "end": "2125920"
  },
  {
    "text": "so those guys they you need to be really careful with them if they if they hit their memory limits",
    "start": "2125920",
    "end": "2131839"
  },
  {
    "text": "you can really have this cascading failure situation and that's very so we have like alerts on sort of 70% of the",
    "start": "2131839",
    "end": "2139320"
  },
  {
    "text": "maximum head series for example and what we want time series The Unique time series is what you're going to exactly",
    "start": "2139320",
    "end": "2145480"
  },
  {
    "text": "yeah yeah yeah yeah exactly hey hi um I have a question",
    "start": "2145480",
    "end": "2151599"
  },
  {
    "text": "regarding cardinality limiting so you said um there is a component which queries is just head serious and then it",
    "start": "2151599",
    "end": "2158640"
  },
  {
    "text": "says that cardinality is exceeded or not so does it mean that if cardinality exceeded the whole data stream is like",
    "start": "2158640",
    "end": "2165599"
  },
  {
    "text": "blocked yes so effectively that's the way you can introduce a cardinality limit because you know each each",
    "start": "2165599",
    "end": "2172440"
  },
  {
    "text": "individual um you know let's say each individual label set would be a separate TS time series so that's one way that",
    "start": "2172440",
    "end": "2178880"
  },
  {
    "text": "yeah that's exactly how you can do that okay is there a way to block only new time series so for example we have like",
    "start": "2178880",
    "end": "2184000"
  },
  {
    "text": "1 million for this tenant and then like by adding 1,000 of new series so you",
    "start": "2184000",
    "end": "2189599"
  },
  {
    "text": "reject only those 1,000 um H I don't think so because so it's",
    "start": "2189599",
    "end": "2196800"
  },
  {
    "text": "basically uh what's in the current head block so that two within this two hours",
    "start": "2196800",
    "end": "2202240"
  },
  {
    "text": "so basically you know we usually see this sort of ramping up and then a cut and then it ramping up again so",
    "start": "2202240",
    "end": "2207440"
  },
  {
    "text": "effectively once you get to that threshold and you're ramping up then you cut everything after that and you can't",
    "start": "2207440",
    "end": "2212720"
  },
  {
    "text": "say you know you can't really distinguish which series you're going to cut or not it's just about prob pure",
    "start": "2212720",
    "end": "2217839"
  },
  {
    "text": "number thank you all right good thank you very",
    "start": "2217839",
    "end": "2223800"
  },
  {
    "text": "much",
    "start": "2224839",
    "end": "2227839"
  }
]