[
  {
    "text": "hi Welcome to our talk I'm Lou I'm from",
    "start": "60",
    "end": "4860"
  },
  {
    "text": "the Robin Hood Market I mean the",
    "start": "4860",
    "end": "6779"
  },
  {
    "text": "container or translation team at the",
    "start": "6779",
    "end": "9000"
  },
  {
    "text": "senior software engineer",
    "start": "9000",
    "end": "11400"
  },
  {
    "text": "hello my name is madhu I'm also in the",
    "start": "11400",
    "end": "14820"
  },
  {
    "text": "same team as Lou I'm a software engineer",
    "start": "14820",
    "end": "17100"
  },
  {
    "text": "on the container orchestration team at",
    "start": "17100",
    "end": "18960"
  },
  {
    "text": "Robinhood markets our team is",
    "start": "18960",
    "end": "21359"
  },
  {
    "text": "responsible for building uh offering and",
    "start": "21359",
    "end": "25980"
  },
  {
    "text": "operating the compute platform for all",
    "start": "25980",
    "end": "28740"
  },
  {
    "text": "of Robin Hood and our compute platform",
    "start": "28740",
    "end": "31320"
  },
  {
    "text": "of choice today at the company's",
    "start": "31320",
    "end": "33059"
  },
  {
    "text": "kubernetes in this talk we'll walk you",
    "start": "33059",
    "end": "35940"
  },
  {
    "text": "through how we adopted celium in one of",
    "start": "35940",
    "end": "39120"
  },
  {
    "text": "our environments the challenges that we",
    "start": "39120",
    "end": "41940"
  },
  {
    "text": "ran into and the lessons we learned from",
    "start": "41940",
    "end": "44940"
  },
  {
    "text": "the challenges and how we adapted and",
    "start": "44940",
    "end": "48660"
  },
  {
    "text": "then how we now live happily ever after",
    "start": "48660",
    "end": "52020"
  },
  {
    "text": "with psyllium in that environment next",
    "start": "52020",
    "end": "55260"
  },
  {
    "text": "slide please",
    "start": "55260",
    "end": "57739"
  },
  {
    "text": "so before we get into any of the details",
    "start": "57960",
    "end": "60719"
  },
  {
    "text": "I want to clarify or explain what near",
    "start": "60719",
    "end": "63300"
  },
  {
    "text": "production means here",
    "start": "63300",
    "end": "65338"
  },
  {
    "text": "like pretty much every other company we",
    "start": "65339",
    "end": "67680"
  },
  {
    "text": "have multiple environments there is a",
    "start": "67680",
    "end": "69720"
  },
  {
    "text": "production environment as an actual",
    "start": "69720",
    "end": "71700"
  },
  {
    "text": "production where all our user-facing",
    "start": "71700",
    "end": "75540"
  },
  {
    "text": "production traffic is served as in all",
    "start": "75540",
    "end": "78000"
  },
  {
    "text": "the Production Services are run and then",
    "start": "78000",
    "end": "80880"
  },
  {
    "text": "we have a bunch of other environments",
    "start": "80880",
    "end": "82619"
  },
  {
    "text": "but there is this one specific",
    "start": "82619",
    "end": "84479"
  },
  {
    "text": "environment where we run integration",
    "start": "84479",
    "end": "87420"
  },
  {
    "text": "tasks and personal development",
    "start": "87420",
    "end": "89479"
  },
  {
    "text": "namespaces for the products at the",
    "start": "89479",
    "end": "92580"
  },
  {
    "text": "company we call it as near production",
    "start": "92580",
    "end": "95159"
  },
  {
    "text": "because this environment is treated at",
    "start": "95159",
    "end": "98100"
  },
  {
    "text": "the same level of seriousness and has",
    "start": "98100",
    "end": "100500"
  },
  {
    "text": "the same response slos as production",
    "start": "100500",
    "end": "103500"
  },
  {
    "text": "because it's critical for the entire",
    "start": "103500",
    "end": "106860"
  },
  {
    "text": "company's engineering and development if",
    "start": "106860",
    "end": "109320"
  },
  {
    "text": "this environment stops working the",
    "start": "109320",
    "end": "111479"
  },
  {
    "text": "entire development at the company hard",
    "start": "111479",
    "end": "113759"
  },
  {
    "text": "so if there is an incident we take it as",
    "start": "113759",
    "end": "117119"
  },
  {
    "text": "at the same level of seriousness as a",
    "start": "117119",
    "end": "119579"
  },
  {
    "text": "production environment the only reason",
    "start": "119579",
    "end": "121560"
  },
  {
    "text": "why we might prioritize it lower is if",
    "start": "121560",
    "end": "124020"
  },
  {
    "text": "we have if we are also simultaneously",
    "start": "124020",
    "end": "126360"
  },
  {
    "text": "having a production incident at the time",
    "start": "126360",
    "end": "128220"
  },
  {
    "text": "although that has almost never happened",
    "start": "128220",
    "end": "130800"
  },
  {
    "text": "since I joined Robin",
    "start": "130800",
    "end": "133200"
  },
  {
    "text": "um with that context I also want to",
    "start": "133200",
    "end": "136140"
  },
  {
    "text": "explain how this environment is",
    "start": "136140",
    "end": "138599"
  },
  {
    "text": "different from the production",
    "start": "138599",
    "end": "139739"
  },
  {
    "text": "environment",
    "start": "139739",
    "end": "141060"
  },
  {
    "text": "because of the nature of the workloads",
    "start": "141060",
    "end": "144120"
  },
  {
    "text": "that we run in this environment which",
    "start": "144120",
    "end": "146400"
  },
  {
    "text": "are integration tests right for most",
    "start": "146400",
    "end": "148260"
  },
  {
    "text": "part integration tests are set up",
    "start": "148260",
    "end": "150959"
  },
  {
    "text": "infrastructure is brought up for them as",
    "start": "150959",
    "end": "153000"
  },
  {
    "text": "in the parts are kicked out for them uh",
    "start": "153000",
    "end": "155940"
  },
  {
    "text": "data is set up for the integration test",
    "start": "155940",
    "end": "158640"
  },
  {
    "text": "the tests are run and towards the end",
    "start": "158640",
    "end": "162060"
  },
  {
    "text": "everything is completely toned down",
    "start": "162060",
    "end": "164099"
  },
  {
    "text": "including the namespace that was brought",
    "start": "164099",
    "end": "166319"
  },
  {
    "text": "up for that particular integration test",
    "start": "166319",
    "end": "168780"
  },
  {
    "text": "run so because of this nature it's a",
    "start": "168780",
    "end": "171599"
  },
  {
    "text": "very high share environment and the",
    "start": "171599",
    "end": "174420"
  },
  {
    "text": "challenges that we face in this",
    "start": "174420",
    "end": "175860"
  },
  {
    "text": "environment is different from all other",
    "start": "175860",
    "end": "177840"
  },
  {
    "text": "environments next slide these",
    "start": "177840",
    "end": "182060"
  },
  {
    "text": "so what exactly ISL before we even get",
    "start": "182099",
    "end": "185640"
  },
  {
    "text": "there let's look at some of the SLO",
    "start": "185640",
    "end": "188660"
  },
  {
    "text": "prerequisites kubernetes Upstream",
    "start": "188660",
    "end": "191220"
  },
  {
    "text": "projects defined so if you go look at",
    "start": "191220",
    "end": "194340"
  },
  {
    "text": "the SLO definition page as in the SLR MD",
    "start": "194340",
    "end": "196680"
  },
  {
    "text": "for six scalability on one of the",
    "start": "196680",
    "end": "199800"
  },
  {
    "text": "kubernetes six scalability repositories",
    "start": "199800",
    "end": "201840"
  },
  {
    "text": "you will see these two specific lines",
    "start": "201840",
    "end": "204900"
  },
  {
    "text": "there which is on the left here on the",
    "start": "204900",
    "end": "207060"
  },
  {
    "text": "slide",
    "start": "207060",
    "end": "208080"
  },
  {
    "text": "the prerequisite state that for any of",
    "start": "208080",
    "end": "211379"
  },
  {
    "text": "the slos to be applicable the kubernetes",
    "start": "211379",
    "end": "214860"
  },
  {
    "text": "cluster is should be available and",
    "start": "214860",
    "end": "217379"
  },
  {
    "text": "surveying which makes sense and then the",
    "start": "217379",
    "end": "220379"
  },
  {
    "text": "second prerequisite talks about the",
    "start": "220379",
    "end": "223560"
  },
  {
    "text": "cluster that is considered to be a",
    "start": "223560",
    "end": "226200"
  },
  {
    "text": "prerequisite before looking at the",
    "start": "226200",
    "end": "228239"
  },
  {
    "text": "number let's look at what the definition",
    "start": "228239",
    "end": "229500"
  },
  {
    "text": "of churn is",
    "start": "229500",
    "end": "231000"
  },
  {
    "text": "kubernetes Upstream defines sharp as",
    "start": "231000",
    "end": "234540"
  },
  {
    "text": "number of path spect Creations updates",
    "start": "234540",
    "end": "237720"
  },
  {
    "text": "and deletions per second to us the",
    "start": "237720",
    "end": "240659"
  },
  {
    "text": "number of requests that users make to",
    "start": "240659",
    "end": "244080"
  },
  {
    "text": "the system in a given second",
    "start": "244080",
    "end": "246299"
  },
  {
    "text": "and kubernetes",
    "start": "246299",
    "end": "248900"
  },
  {
    "text": "requires this charm to be lower than 20",
    "start": "248900",
    "end": "252720"
  },
  {
    "text": "for slos to be applicable now if you",
    "start": "252720",
    "end": "256500"
  },
  {
    "text": "turn right and look at the graph that we",
    "start": "256500",
    "end": "258900"
  },
  {
    "text": "are sharing here you will see return",
    "start": "258900",
    "end": "262040"
  },
  {
    "text": "patterns for one of the Clusters that we",
    "start": "262040",
    "end": "265620"
  },
  {
    "text": "have here in the environment over a",
    "start": "265620",
    "end": "268680"
  },
  {
    "text": "typical 30-day period uh you can also",
    "start": "268680",
    "end": "271560"
  },
  {
    "text": "clearly see or say which tell which of",
    "start": "271560",
    "end": "275460"
  },
  {
    "text": "these days are work days and which days",
    "start": "275460",
    "end": "277500"
  },
  {
    "text": "are not work days because we told you",
    "start": "277500",
    "end": "279540"
  },
  {
    "text": "that this is used for integration",
    "start": "279540",
    "end": "280860"
  },
  {
    "text": "testing and personal development right",
    "start": "280860",
    "end": "282419"
  },
  {
    "text": "so you we expect traffic to be higher on",
    "start": "282419",
    "end": "285300"
  },
  {
    "text": "work days so you will see five Peaks",
    "start": "285300",
    "end": "288360"
  },
  {
    "text": "followed by travs for a couple of days",
    "start": "288360",
    "end": "290580"
  },
  {
    "text": "and then five weeks which follows the",
    "start": "290580",
    "end": "292500"
  },
  {
    "text": "pattern of five workdays and then two",
    "start": "292500",
    "end": "294840"
  },
  {
    "text": "weekend days",
    "start": "294840",
    "end": "297000"
  },
  {
    "text": "you clearly see that on work days the",
    "start": "297000",
    "end": "301680"
  },
  {
    "text": "Peaks are typically over 30 many times",
    "start": "301680",
    "end": "304800"
  },
  {
    "text": "over 40 and it Peaks almost as high as",
    "start": "304800",
    "end": "309360"
  },
  {
    "text": "70",
    "start": "309360",
    "end": "310880"
  },
  {
    "text": "pot creations and deletions per second",
    "start": "310880",
    "end": "313740"
  },
  {
    "text": "and in in this graph we are not even",
    "start": "313740",
    "end": "316320"
  },
  {
    "text": "considering the use originated requests",
    "start": "316320",
    "end": "318600"
  },
  {
    "text": "right these are just pod perspect",
    "start": "318600",
    "end": "321300"
  },
  {
    "text": "creations and deletions so this is the",
    "start": "321300",
    "end": "323940"
  },
  {
    "text": "level of churn one cluster in this",
    "start": "323940",
    "end": "327240"
  },
  {
    "text": "environment goes through next slide",
    "start": "327240",
    "end": "329639"
  },
  {
    "text": "please",
    "start": "329639",
    "end": "331880"
  },
  {
    "text": "so given all these things what exactly",
    "start": "331919",
    "end": "335520"
  },
  {
    "text": "motivated us to adopt psyllium",
    "start": "335520",
    "end": "337880"
  },
  {
    "text": "especially overlay based networking in",
    "start": "337880",
    "end": "340199"
  },
  {
    "text": "this environment so before we even get",
    "start": "340199",
    "end": "343139"
  },
  {
    "text": "there let's look at what we had in this",
    "start": "343139",
    "end": "345660"
  },
  {
    "text": "environment before we adopted CEO",
    "start": "345660",
    "end": "349800"
  },
  {
    "text": "so to begin with",
    "start": "349800",
    "end": "351720"
  },
  {
    "text": "we have AWS case CMI as our cni plugin",
    "start": "351720",
    "end": "356220"
  },
  {
    "text": "running in all the environments or that",
    "start": "356220",
    "end": "358979"
  },
  {
    "text": "was the case before we adopted celium in",
    "start": "358979",
    "end": "361020"
  },
  {
    "text": "this environment",
    "start": "361020",
    "end": "363440"
  },
  {
    "text": "networking model that we use in all the",
    "start": "366919",
    "end": "370620"
  },
  {
    "text": "other environments or in all the that we",
    "start": "370620",
    "end": "372600"
  },
  {
    "text": "used to use in this environment",
    "start": "372600",
    "end": "373740"
  },
  {
    "text": "including all other environments because",
    "start": "373740",
    "end": "376139"
  },
  {
    "text": "in many other environments we need",
    "start": "376139",
    "end": "378900"
  },
  {
    "text": "cross-custer networking so it makes",
    "start": "378900",
    "end": "380520"
  },
  {
    "text": "sense but investment in this particular",
    "start": "380520",
    "end": "382740"
  },
  {
    "text": "environment",
    "start": "382740",
    "end": "384120"
  },
  {
    "text": "the traffic is mostly isolated within",
    "start": "384120",
    "end": "386639"
  },
  {
    "text": "the namespaces so cross-cultural",
    "start": "386639",
    "end": "388680"
  },
  {
    "text": "networking is not required like even",
    "start": "388680",
    "end": "390660"
  },
  {
    "text": "within the cluster the networking across",
    "start": "390660",
    "end": "394259"
  },
  {
    "text": "namespaces is mostly not required for",
    "start": "394259",
    "end": "396000"
  },
  {
    "text": "most cases so overall networking works",
    "start": "396000",
    "end": "399060"
  },
  {
    "text": "fine for this model or for this",
    "start": "399060",
    "end": "401280"
  },
  {
    "text": "environment in addition to that the kcni",
    "start": "401280",
    "end": "405240"
  },
  {
    "text": "model with flat networking works well",
    "start": "405240",
    "end": "407880"
  },
  {
    "text": "for stable production environments but",
    "start": "407880",
    "end": "410819"
  },
  {
    "text": "has limitations on more bursty",
    "start": "410819",
    "end": "414180"
  },
  {
    "text": "environments which where we need higher",
    "start": "414180",
    "end": "416699"
  },
  {
    "text": "pot density and have very high churn",
    "start": "416699",
    "end": "420000"
  },
  {
    "text": "like this particular environment",
    "start": "420000",
    "end": "422699"
  },
  {
    "text": "so to go further go go a little bit",
    "start": "422699",
    "end": "426600"
  },
  {
    "text": "further",
    "start": "426600",
    "end": "427680"
  },
  {
    "text": "the number of parts per node for us is",
    "start": "427680",
    "end": "431280"
  },
  {
    "text": "limited in the flat networking kcni more",
    "start": "431280",
    "end": "435180"
  },
  {
    "text": "AWS kcmi model because of the",
    "start": "435180",
    "end": "438300"
  },
  {
    "text": "limitations that easy to imposes on the",
    "start": "438300",
    "end": "441120"
  },
  {
    "text": "number of secondary Enis that you can",
    "start": "441120",
    "end": "443400"
  },
  {
    "text": "attach to ec2 instances and the eyepiece",
    "start": "443400",
    "end": "446340"
  },
  {
    "text": "that you can attach to the emis because",
    "start": "446340",
    "end": "449759"
  },
  {
    "text": "we can't go over that number it leads to",
    "start": "449759",
    "end": "453720"
  },
  {
    "text": "very low CPU and memory utilization",
    "start": "453720",
    "end": "455940"
  },
  {
    "text": "because the parts that we spin up here",
    "start": "455940",
    "end": "458819"
  },
  {
    "text": "are for integration tests so they don't",
    "start": "458819",
    "end": "461580"
  },
  {
    "text": "have high utilization and because",
    "start": "461580",
    "end": "464220"
  },
  {
    "text": "utilization is so low and we cannot pack",
    "start": "464220",
    "end": "467280"
  },
  {
    "text": "more densely it leads to very poor cost",
    "start": "467280",
    "end": "470759"
  },
  {
    "text": "efficiency model",
    "start": "470759",
    "end": "472199"
  },
  {
    "text": "so we we were evaluating various",
    "start": "472199",
    "end": "474660"
  },
  {
    "text": "Solutions and we wanted to adopt a",
    "start": "474660",
    "end": "477419"
  },
  {
    "text": "solution that allowed us to get higher",
    "start": "477419",
    "end": "480599"
  },
  {
    "text": "pot density and we knew that we wanted",
    "start": "480599",
    "end": "484199"
  },
  {
    "text": "to adopt our networking for that and",
    "start": "484199",
    "end": "486960"
  },
  {
    "text": "then we looked at various Solutions and",
    "start": "486960",
    "end": "489180"
  },
  {
    "text": "we wanted to make sure that we did not",
    "start": "489180",
    "end": "491580"
  },
  {
    "text": "significantly sacrifice the performance",
    "start": "491580",
    "end": "494160"
  },
  {
    "text": "by going to the overlay model although",
    "start": "494160",
    "end": "497220"
  },
  {
    "text": "this is integration test performance is",
    "start": "497220",
    "end": "499139"
  },
  {
    "text": "still important in this environment so",
    "start": "499139",
    "end": "501300"
  },
  {
    "text": "it's not sacrificing performance was an",
    "start": "501300",
    "end": "504360"
  },
  {
    "text": "important goal here we looked at ebpf",
    "start": "504360",
    "end": "506879"
  },
  {
    "text": "and it looked like a very",
    "start": "506879",
    "end": "509400"
  },
  {
    "text": "strong candidate and the moment we knew",
    "start": "509400",
    "end": "511919"
  },
  {
    "text": "that we wanted to use ebpf dissolution",
    "start": "511919",
    "end": "514680"
  },
  {
    "text": "of choice was helium because this was",
    "start": "514680",
    "end": "517800"
  },
  {
    "text": "the most adopted solution where that",
    "start": "517800",
    "end": "521520"
  },
  {
    "text": "that was all available out there next",
    "start": "521520",
    "end": "524399"
  },
  {
    "text": "slide please",
    "start": "524399",
    "end": "526880"
  },
  {
    "text": "so with with that how did our migration",
    "start": "527220",
    "end": "530519"
  },
  {
    "text": "actually look like",
    "start": "530519",
    "end": "532140"
  },
  {
    "text": "we started from CDM 175 and then we",
    "start": "532140",
    "end": "536399"
  },
  {
    "text": "quickly adopted or we quickly had to",
    "start": "536399",
    "end": "538800"
  },
  {
    "text": "jump to a much more modern version or a",
    "start": "538800",
    "end": "541440"
  },
  {
    "text": "much more newer version of Celia so we",
    "start": "541440",
    "end": "543899"
  },
  {
    "text": "went through three version bombs",
    "start": "543899",
    "end": "546839"
  },
  {
    "text": "the way we did this was we initially",
    "start": "546839",
    "end": "549180"
  },
  {
    "text": "brought up a new cluster we we decided",
    "start": "549180",
    "end": "552360"
  },
  {
    "text": "that doing this kind of upgrade or",
    "start": "552360",
    "end": "554700"
  },
  {
    "text": "change in place was not viable so we",
    "start": "554700",
    "end": "557580"
  },
  {
    "text": "brought up a new cluster for personal",
    "start": "557580",
    "end": "559800"
  },
  {
    "text": "development and another new cluster for",
    "start": "559800",
    "end": "561839"
  },
  {
    "text": "integration tasks with CM overlay",
    "start": "561839",
    "end": "564300"
  },
  {
    "text": "networking model and then we ran a lot",
    "start": "564300",
    "end": "567839"
  },
  {
    "text": "of load tests against these clusters",
    "start": "567839",
    "end": "570120"
  },
  {
    "text": "and once we reached the stage where we",
    "start": "570120",
    "end": "572220"
  },
  {
    "text": "felt confident about the knobs that we",
    "start": "572220",
    "end": "576060"
  },
  {
    "text": "had to tune and everything we started",
    "start": "576060",
    "end": "578220"
  },
  {
    "text": "cutting over workloads onto these new",
    "start": "578220",
    "end": "580260"
  },
  {
    "text": "clusters interestingly though personal",
    "start": "580260",
    "end": "582420"
  },
  {
    "text": "development in spaces were more or less",
    "start": "582420",
    "end": "584160"
  },
  {
    "text": "fine by but in integration test",
    "start": "584160",
    "end": "586140"
  },
  {
    "text": "environment because of the height chain",
    "start": "586140",
    "end": "588060"
  },
  {
    "text": "we ran into a number of challenges that",
    "start": "588060",
    "end": "590160"
  },
  {
    "text": "we were not expecting it caused a few",
    "start": "590160",
    "end": "592860"
  },
  {
    "text": "incidents or quite a few incidents the",
    "start": "592860",
    "end": "595440"
  },
  {
    "text": "term that we use here at Robin Hood like",
    "start": "595440",
    "end": "597180"
  },
  {
    "text": "many other companies in the industries",
    "start": "597180",
    "end": "599399"
  },
  {
    "text": "cells so we ran into a number of serves",
    "start": "599399",
    "end": "603060"
  },
  {
    "text": "and we had to go back and forth between",
    "start": "603060",
    "end": "605880"
  },
  {
    "text": "our old clusters like use flat",
    "start": "605880",
    "end": "607860"
  },
  {
    "text": "networking model and the new clusters",
    "start": "607860",
    "end": "610200"
  },
  {
    "text": "that used overlay networking model with",
    "start": "610200",
    "end": "613080"
  },
  {
    "text": "CVM eventually we were able to migrate",
    "start": "613080",
    "end": "616320"
  },
  {
    "text": "all the workloads onto the CM clusters",
    "start": "616320",
    "end": "619080"
  },
  {
    "text": "next slide please",
    "start": "619080",
    "end": "622200"
  },
  {
    "text": "overall if I want if I were to talk",
    "start": "622200",
    "end": "624720"
  },
  {
    "text": "about the efficiency that we have",
    "start": "624720",
    "end": "626580"
  },
  {
    "text": "achieved because we have adopted stelium",
    "start": "626580",
    "end": "629519"
  },
  {
    "text": "with overlay networking at the end we",
    "start": "629519",
    "end": "632880"
  },
  {
    "text": "were able to",
    "start": "632880",
    "end": "635720"
  },
  {
    "text": "but at least 2x more pods than we were",
    "start": "635839",
    "end": "641279"
  },
  {
    "text": "able to pack in the uh flat networking",
    "start": "641279",
    "end": "644579"
  },
  {
    "text": "model we in in the flat networking model",
    "start": "644579",
    "end": "647160"
  },
  {
    "text": "we used to set our number of parts per",
    "start": "647160",
    "end": "650279"
  },
  {
    "text": "node the Pod density per node 210 which",
    "start": "650279",
    "end": "653459"
  },
  {
    "text": "is what kubernetes generally recommends",
    "start": "653459",
    "end": "656279"
  },
  {
    "text": "um we when we adopted celium we went all",
    "start": "656279",
    "end": "659160"
  },
  {
    "text": "the way up to 250 but it caused a number",
    "start": "659160",
    "end": "661320"
  },
  {
    "text": "of issues so we slowly dialed it back",
    "start": "661320",
    "end": "664140"
  },
  {
    "text": "down to 180. although the number was",
    "start": "664140",
    "end": "666720"
  },
  {
    "text": "previously set to 110 we never actually",
    "start": "666720",
    "end": "669420"
  },
  {
    "text": "hit the number 100 done our utilization",
    "start": "669420",
    "end": "672180"
  },
  {
    "text": "or density was more around 90 to 100 now",
    "start": "672180",
    "end": "674700"
  },
  {
    "text": "we are comfortably able to reach 180 so",
    "start": "674700",
    "end": "677700"
  },
  {
    "text": "we have doubled the pot density of our",
    "start": "677700",
    "end": "680519"
  },
  {
    "text": "clusters with that I'll hand off to Lou",
    "start": "680519",
    "end": "683579"
  },
  {
    "text": "to talk about the more specific",
    "start": "683579",
    "end": "685200"
  },
  {
    "text": "challenges that we ran into",
    "start": "685200",
    "end": "688399"
  },
  {
    "text": "so as madhu mentioned this migration",
    "start": "693440",
    "end": "697200"
  },
  {
    "text": "came up we came with a few surprises so",
    "start": "697200",
    "end": "700980"
  },
  {
    "text": "for example it reviewed several",
    "start": "700980",
    "end": "703800"
  },
  {
    "text": "previously hidden bottlenecks",
    "start": "703800",
    "end": "706200"
  },
  {
    "text": "because some other factors become new",
    "start": "706200",
    "end": "708839"
  },
  {
    "text": "bottlenecks with much higher poly",
    "start": "708839",
    "end": "711480"
  },
  {
    "text": "density on each node such as excessive",
    "start": "711480",
    "end": "714720"
  },
  {
    "text": "API server requests the CPU memory and",
    "start": "714720",
    "end": "718140"
  },
  {
    "text": "mouse exhaustion on the nodes and",
    "start": "718140",
    "end": "720959"
  },
  {
    "text": "imbalanced workload distribution",
    "start": "720959",
    "end": "723200"
  },
  {
    "text": "medicines even worse",
    "start": "723200",
    "end": "725339"
  },
  {
    "text": "we also encountered a few CM stability",
    "start": "725339",
    "end": "728339"
  },
  {
    "text": "issues although version bumps of sodium",
    "start": "728339",
    "end": "731579"
  },
  {
    "text": "fixed most of them it was kind of costly",
    "start": "731579",
    "end": "736019"
  },
  {
    "text": "and tricky to root causing and fix",
    "start": "736019",
    "end": "741019"
  },
  {
    "text": "environments is also a challenge and we",
    "start": "742760",
    "end": "745200"
  },
  {
    "text": "had to build custom scheduled extensions",
    "start": "745200",
    "end": "748079"
  },
  {
    "text": "to work around it so in the following",
    "start": "748079",
    "end": "751740"
  },
  {
    "text": "presentation we will select a few",
    "start": "751740",
    "end": "754320"
  },
  {
    "text": "interesting stories to share in more",
    "start": "754320",
    "end": "756839"
  },
  {
    "text": "details",
    "start": "756839",
    "end": "758100"
  },
  {
    "text": "so let's take a look at the new",
    "start": "758100",
    "end": "761339"
  },
  {
    "text": "bottlenecks first",
    "start": "761339",
    "end": "763560"
  },
  {
    "text": "which is actually the first few",
    "start": "763560",
    "end": "766500"
  },
  {
    "text": "surprises we got with the abundance of",
    "start": "766500",
    "end": "769800"
  },
  {
    "text": "IP addresses we decided to consolidate",
    "start": "769800",
    "end": "773040"
  },
  {
    "text": "two integration test clusters into one",
    "start": "773040",
    "end": "776959"
  },
  {
    "text": "to achieve better cause efficiency",
    "start": "776959",
    "end": "781380"
  },
  {
    "text": "and in the mean wire the cubelet on each",
    "start": "781380",
    "end": "784380"
  },
  {
    "text": "node became much busier with more parts",
    "start": "784380",
    "end": "787980"
  },
  {
    "text": "packed onto each nose as a result the",
    "start": "787980",
    "end": "791940"
  },
  {
    "text": "API server got overloaded and couldn't",
    "start": "791940",
    "end": "795180"
  },
  {
    "text": "keep up as a result we couldn't bring up",
    "start": "795180",
    "end": "799800"
  },
  {
    "text": "new parts new namespaces and essentially",
    "start": "799800",
    "end": "803100"
  },
  {
    "text": "it means the integration test",
    "start": "803100",
    "end": "804920"
  },
  {
    "text": "environments got broken",
    "start": "804920",
    "end": "807600"
  },
  {
    "text": "and we had to migrate to the previous",
    "start": "807600",
    "end": "810060"
  },
  {
    "text": "clusters",
    "start": "810060",
    "end": "811440"
  },
  {
    "text": "for the mitigation and the fix with",
    "start": "811440",
    "end": "814800"
  },
  {
    "text": "another CM based cluster for the",
    "start": "814800",
    "end": "817860"
  },
  {
    "text": "integration test environments on one",
    "start": "817860",
    "end": "820860"
  },
  {
    "text": "side it's relieved the API server by",
    "start": "820860",
    "end": "824100"
  },
  {
    "text": "reduced half of the pressure and on the",
    "start": "824100",
    "end": "826740"
  },
  {
    "text": "on the other sides we got more",
    "start": "826740",
    "end": "828600"
  },
  {
    "text": "resiliency and redundancy we also moved",
    "start": "828600",
    "end": "832139"
  },
  {
    "text": "from the deployments back to Ports back",
    "start": "832139",
    "end": "835560"
  },
  {
    "text": "for integration test names basis because",
    "start": "835560",
    "end": "838860"
  },
  {
    "text": "essentially those deployments back were",
    "start": "838860",
    "end": "842459"
  },
  {
    "text": "the single party deployment and we",
    "start": "842459",
    "end": "845579"
  },
  {
    "text": "didn't really need the auto",
    "start": "845579",
    "end": "848040"
  },
  {
    "text": "reconciliation feature of deployments",
    "start": "848040",
    "end": "850860"
  },
  {
    "text": "for the integration tests so it further",
    "start": "850860",
    "end": "854399"
  },
  {
    "text": "reduced the API server pressure",
    "start": "854399",
    "end": "857100"
  },
  {
    "text": "last but not least we routed a request",
    "start": "857100",
    "end": "860220"
  },
  {
    "text": "from the cube controller manager to the",
    "start": "860220",
    "end": "863880"
  },
  {
    "text": "network load balancer instead of choose",
    "start": "863880",
    "end": "866880"
  },
  {
    "text": "the local API server we removed the",
    "start": "866880",
    "end": "869399"
  },
  {
    "text": "stickiness and thus the request became",
    "start": "869399",
    "end": "872279"
  },
  {
    "text": "more evenly distributed across the API",
    "start": "872279",
    "end": "875940"
  },
  {
    "text": "servers instead of concentrating on the",
    "start": "875940",
    "end": "879420"
  },
  {
    "text": "one which collocates with the leader",
    "start": "879420",
    "end": "882120"
  },
  {
    "text": "controller that prevented the hottest",
    "start": "882120",
    "end": "884820"
  },
  {
    "text": "API server get crashed",
    "start": "884820",
    "end": "887540"
  },
  {
    "text": "another bottleneck is the data plane",
    "start": "887540",
    "end": "891480"
  },
  {
    "text": "nodes resource exhaustion the nodes of",
    "start": "891480",
    "end": "894779"
  },
  {
    "text": "the new cilium cluster somehow became",
    "start": "894779",
    "end": "897360"
  },
  {
    "text": "more likely to encounter resource",
    "start": "897360",
    "end": "899399"
  },
  {
    "text": "exhaustion such as running out to CPU or",
    "start": "899399",
    "end": "902519"
  },
  {
    "text": "memory resources or has too many months",
    "start": "902519",
    "end": "905300"
  },
  {
    "text": "posts on search notes were stuck at",
    "start": "905300",
    "end": "908399"
  },
  {
    "text": "creating or terminating to make things",
    "start": "908399",
    "end": "911579"
  },
  {
    "text": "worse those nodes will be churned and",
    "start": "911579",
    "end": "914880"
  },
  {
    "text": "pause on those notes will have to be",
    "start": "914880",
    "end": "917339"
  },
  {
    "text": "rescheduled to the existing nodes as new",
    "start": "917339",
    "end": "921060"
  },
  {
    "text": "nodes have to go through the",
    "start": "921060",
    "end": "923160"
  },
  {
    "text": "initialization warm-up phase before",
    "start": "923160",
    "end": "926339"
  },
  {
    "text": "joining the cluster and the migration of",
    "start": "926339",
    "end": "930120"
  },
  {
    "text": "the parts on those Channel nodes into",
    "start": "930120",
    "end": "932220"
  },
  {
    "text": "the exact existing nodes just makes",
    "start": "932220",
    "end": "935100"
  },
  {
    "text": "things worse because it put more",
    "start": "935100",
    "end": "936899"
  },
  {
    "text": "pressure on some of the already kind of",
    "start": "936899",
    "end": "939899"
  },
  {
    "text": "overloaded nodes thus models will be too",
    "start": "939899",
    "end": "943500"
  },
  {
    "text": "hot and become turns bad which is a",
    "start": "943500",
    "end": "946920"
  },
  {
    "text": "essential a cascading effect",
    "start": "946920",
    "end": "950279"
  },
  {
    "text": "as a mitigation we initially tried the",
    "start": "950279",
    "end": "954720"
  },
  {
    "text": "kubernetes system resource reservation",
    "start": "954720",
    "end": "957800"
  },
  {
    "text": "basically reserved the system resource",
    "start": "957800",
    "end": "960540"
  },
  {
    "text": "for kubernetes demons and system demons",
    "start": "960540",
    "end": "963959"
  },
  {
    "text": "however it turned out that it wasn't a",
    "start": "963959",
    "end": "967339"
  },
  {
    "text": "one-size-fits-all solution it has some",
    "start": "967339",
    "end": "970199"
  },
  {
    "text": "own implementation limitations and as",
    "start": "970199",
    "end": "974339"
  },
  {
    "text": "madhu mentioned earlier the way we use",
    "start": "974339",
    "end": "977699"
  },
  {
    "text": "the kubernetes cluster exceeded the",
    "start": "977699",
    "end": "982519"
  },
  {
    "text": "prerequisite for sros so it also makes",
    "start": "982519",
    "end": "986040"
  },
  {
    "text": "sense that the simple resource",
    "start": "986040",
    "end": "988860"
  },
  {
    "text": "reservation couldn't solve the entire",
    "start": "988860",
    "end": "990839"
  },
  {
    "text": "problem although it helped a lot thus in",
    "start": "990839",
    "end": "994260"
  },
  {
    "text": "addition we audited the applications",
    "start": "994260",
    "end": "996600"
  },
  {
    "text": "especially for those heavy heavily used",
    "start": "996600",
    "end": "999300"
  },
  {
    "text": "applications in the integration test",
    "start": "999300",
    "end": "1001540"
  },
  {
    "text": "environments we write sized the resource",
    "start": "1001540",
    "end": "1004639"
  },
  {
    "text": "request and limits parameters because in",
    "start": "1004639",
    "end": "1007639"
  },
  {
    "text": "some cases those requests",
    "start": "1007639",
    "end": "1010279"
  },
  {
    "text": "growth with time and eventually it is a",
    "start": "1010279",
    "end": "1014120"
  },
  {
    "text": "Way Beyond the sorry the actual usage is",
    "start": "1014120",
    "end": "1017480"
  },
  {
    "text": "Way Beyond the actual request causing",
    "start": "1017480",
    "end": "1020360"
  },
  {
    "text": "the cluster to be much over committed",
    "start": "1020360",
    "end": "1023439"
  },
  {
    "text": "and as a result it is easy to get burned",
    "start": "1023439",
    "end": "1027798"
  },
  {
    "text": "out",
    "start": "1027799",
    "end": "1028760"
  },
  {
    "text": "in addition we also made some",
    "start": "1028760",
    "end": "1030740"
  },
  {
    "text": "performance optimizations for those",
    "start": "1030740",
    "end": "1032720"
  },
  {
    "text": "applications on the cluster to reduce",
    "start": "1032720",
    "end": "1035360"
  },
  {
    "text": "the resource over commitment problem",
    "start": "1035360",
    "end": "1039380"
  },
  {
    "text": "in addition we also made the scheduler",
    "start": "1039380",
    "end": "1043160"
  },
  {
    "text": "training and just added some scheduler",
    "start": "1043160",
    "end": "1046160"
  },
  {
    "text": "extension plugins to Target for more",
    "start": "1046160",
    "end": "1049640"
  },
  {
    "text": "evenly distribution of the workloads",
    "start": "1049640",
    "end": "1052120"
  },
  {
    "text": "across the cluster if you turn to the",
    "start": "1052120",
    "end": "1056059"
  },
  {
    "text": "left the above 2 graph shows the CPU and",
    "start": "1056059",
    "end": "1060799"
  },
  {
    "text": "the memory usage across the nodes on a",
    "start": "1060799",
    "end": "1064039"
  },
  {
    "text": "typical day before we had any of those",
    "start": "1064039",
    "end": "1066799"
  },
  {
    "text": "optimizations and the bottom two are the",
    "start": "1066799",
    "end": "1071200"
  },
  {
    "text": "newer distributions with the both tuning",
    "start": "1071200",
    "end": "1075860"
  },
  {
    "text": "and plugin of the scheduler extension as",
    "start": "1075860",
    "end": "1080059"
  },
  {
    "text": "you can see the workload distribution",
    "start": "1080059",
    "end": "1082820"
  },
  {
    "text": "for both CPU and memory are much more",
    "start": "1082820",
    "end": "1085940"
  },
  {
    "text": "evenly compared with before thus we will",
    "start": "1085940",
    "end": "1089360"
  },
  {
    "text": "have less two hot nodes and",
    "start": "1089360",
    "end": "1093080"
  },
  {
    "text": "much fewer nodes we are encountered the",
    "start": "1093080",
    "end": "1095600"
  },
  {
    "text": "resource exhaustion problem",
    "start": "1095600",
    "end": "1098919"
  },
  {
    "text": "switch to another topic which is the",
    "start": "1099640",
    "end": "1103100"
  },
  {
    "text": "silence stability issues within contact",
    "start": "1103100",
    "end": "1105520"
  },
  {
    "text": "one of the most",
    "start": "1105520",
    "end": "1108160"
  },
  {
    "text": "costly selling bug we encountered was",
    "start": "1108160",
    "end": "1111440"
  },
  {
    "text": "the psyllium identity collection bug",
    "start": "1111440",
    "end": "1114200"
  },
  {
    "text": "uh when we were running the version",
    "start": "1114200",
    "end": "1117640"
  },
  {
    "text": "1.11.6 that bug caused the exhaustion of",
    "start": "1117640",
    "end": "1122780"
  },
  {
    "text": "the psyllium identities thus we couldn't",
    "start": "1122780",
    "end": "1125480"
  },
  {
    "text": "bring up New Ports and blocked all",
    "start": "1125480",
    "end": "1127640"
  },
  {
    "text": "integration tests",
    "start": "1127640",
    "end": "1129260"
  },
  {
    "text": "what happened was essentially the",
    "start": "1129260",
    "end": "1131720"
  },
  {
    "text": "garbage collection was",
    "start": "1131720",
    "end": "1134720"
  },
  {
    "text": "um not functioning as expected thus none",
    "start": "1134720",
    "end": "1138559"
  },
  {
    "text": "of the severe Salem identities could be",
    "start": "1138559",
    "end": "1141919"
  },
  {
    "text": "removed to mixed base for the new ones",
    "start": "1141919",
    "end": "1144679"
  },
  {
    "text": "and once it reaches the max count",
    "start": "1144679",
    "end": "1147980"
  },
  {
    "text": "no new parts could be created and what's",
    "start": "1147980",
    "end": "1151700"
  },
  {
    "text": "tricky is that the problem only started",
    "start": "1151700",
    "end": "1154520"
  },
  {
    "text": "to manifest almost one month after the",
    "start": "1154520",
    "end": "1157760"
  },
  {
    "text": "initial deployment of this version",
    "start": "1157760",
    "end": "1161299"
  },
  {
    "text": "on the other side for the personal",
    "start": "1161299",
    "end": "1164059"
  },
  {
    "text": "development fortunately because the",
    "start": "1164059",
    "end": "1166760"
  },
  {
    "text": "churn is much lower and it uses",
    "start": "1166760",
    "end": "1170440"
  },
  {
    "text": "metaphysical identities so even though",
    "start": "1170440",
    "end": "1173539"
  },
  {
    "text": "the same bug was there",
    "start": "1173539",
    "end": "1175820"
  },
  {
    "text": "the identities wasn't a garbage",
    "start": "1175820",
    "end": "1177799"
  },
  {
    "text": "collected it was not impacted in fact it",
    "start": "1177799",
    "end": "1181100"
  },
  {
    "text": "is there have a few months or even years",
    "start": "1181100",
    "end": "1183020"
  },
  {
    "text": "of Headroom before the personal",
    "start": "1183020",
    "end": "1185780"
  },
  {
    "text": "development cluster got hit by the same",
    "start": "1185780",
    "end": "1189440"
  },
  {
    "text": "behavior so what are the lessons from",
    "start": "1189440",
    "end": "1192260"
  },
  {
    "text": "encountering this bug first of all if we",
    "start": "1192260",
    "end": "1196039"
  },
  {
    "text": "could understand the key components of",
    "start": "1196039",
    "end": "1198559"
  },
  {
    "text": "psyllium better and deep dived into how",
    "start": "1198559",
    "end": "1201620"
  },
  {
    "text": "it operates when we adopting sodium it",
    "start": "1201620",
    "end": "1204679"
  },
  {
    "text": "will save us a lot of time to Identity",
    "start": "1204679",
    "end": "1208299"
  },
  {
    "text": "identify the problem and fix the bug or",
    "start": "1208299",
    "end": "1212480"
  },
  {
    "text": "just the version update to the newer new",
    "start": "1212480",
    "end": "1215000"
  },
  {
    "text": "resilient",
    "start": "1215000",
    "end": "1216460"
  },
  {
    "text": "also it will be important to properly",
    "start": "1216460",
    "end": "1220820"
  },
  {
    "text": "set up the monitoring and alerts",
    "start": "1220820",
    "end": "1223820"
  },
  {
    "text": "which could help us to catch the problem",
    "start": "1223820",
    "end": "1227179"
  },
  {
    "text": "earlier before it actually starts to",
    "start": "1227179",
    "end": "1230360"
  },
  {
    "text": "impact the neoproduction environments",
    "start": "1230360",
    "end": "1233480"
  },
  {
    "text": "film provides a lot of great metrics out",
    "start": "1233480",
    "end": "1236240"
  },
  {
    "text": "of the box however it's kind of",
    "start": "1236240",
    "end": "1238100"
  },
  {
    "text": "overwhelming in the beginning and hard",
    "start": "1238100",
    "end": "1240320"
  },
  {
    "text": "to understand which ones are the real",
    "start": "1240320",
    "end": "1242480"
  },
  {
    "text": "important ones thus we didn't have the",
    "start": "1242480",
    "end": "1245179"
  },
  {
    "text": "alerts for the ceiling identity counts",
    "start": "1245179",
    "end": "1248500"
  },
  {
    "text": "and coincidentally with the change there",
    "start": "1248500",
    "end": "1252860"
  },
  {
    "text": "was a rename of the Matrix which removed",
    "start": "1252860",
    "end": "1257000"
  },
  {
    "text": "the total postal fix from The Matrix",
    "start": "1257000",
    "end": "1260900"
  },
  {
    "text": "name and we didn't notice the change log",
    "start": "1260900",
    "end": "1264260"
  },
  {
    "text": "during that version update to",
    "start": "1264260",
    "end": "1267520"
  },
  {
    "text": "1.11.6 as a result when the issue",
    "start": "1267520",
    "end": "1271280"
  },
  {
    "text": "occurred when we we looked at the GC",
    "start": "1271280",
    "end": "1274460"
  },
  {
    "text": "interest dashboard it was empty that as",
    "start": "1274460",
    "end": "1277700"
  },
  {
    "text": "more difficult to",
    "start": "1277700",
    "end": "1280480"
  },
  {
    "text": "identify the problem and the graph to",
    "start": "1280480",
    "end": "1284900"
  },
  {
    "text": "the left actually showed how slowly the",
    "start": "1284900",
    "end": "1288860"
  },
  {
    "text": "the amount of psyllium identity is",
    "start": "1288860",
    "end": "1291080"
  },
  {
    "text": "accumulated over the month period of",
    "start": "1291080",
    "end": "1294020"
  },
  {
    "text": "time and eventually kind of reached the",
    "start": "1294020",
    "end": "1297200"
  },
  {
    "text": "limit and began to hit us",
    "start": "1297200",
    "end": "1301159"
  },
  {
    "text": "another problem we encountered it was",
    "start": "1301159",
    "end": "1305240"
  },
  {
    "text": "the egress broken on some nodes",
    "start": "1305240",
    "end": "1308120"
  },
  {
    "text": "and it only happens on a few",
    "start": "1308120",
    "end": "1311080"
  },
  {
    "text": "non-deterministic nodes when we scare up",
    "start": "1311080",
    "end": "1314480"
  },
  {
    "text": "and could always be mitigated by Asylum",
    "start": "1314480",
    "end": "1317960"
  },
  {
    "text": "agent restart",
    "start": "1317960",
    "end": "1319640"
  },
  {
    "text": "so it was a mystery for us we did fired",
    "start": "1319640",
    "end": "1323480"
  },
  {
    "text": "an issue to the Salem Community and we",
    "start": "1323480",
    "end": "1326720"
  },
  {
    "text": "got as a suggestion to update to the",
    "start": "1326720",
    "end": "1329419"
  },
  {
    "text": "latest Resident version",
    "start": "1329419",
    "end": "1331280"
  },
  {
    "text": "in the meanwhile as a mitigation we",
    "start": "1331280",
    "end": "1334760"
  },
  {
    "text": "implemented Auto coordinate for when the",
    "start": "1334760",
    "end": "1338120"
  },
  {
    "text": "problem occurs on the nodes as a stop",
    "start": "1338120",
    "end": "1341059"
  },
  {
    "text": "Gap and interestingly with the",
    "start": "1341059",
    "end": "1345200"
  },
  {
    "text": "kubernetes version update from",
    "start": "1345200",
    "end": "1348280"
  },
  {
    "text": "1.15 to 1.18",
    "start": "1348280",
    "end": "1351020"
  },
  {
    "text": "although even though before we actually",
    "start": "1351020",
    "end": "1353960"
  },
  {
    "text": "update the film version the problem no",
    "start": "1353960",
    "end": "1356840"
  },
  {
    "text": "longer shows up",
    "start": "1356840",
    "end": "1358220"
  },
  {
    "text": "and",
    "start": "1358220",
    "end": "1359900"
  },
  {
    "text": "to be honest the exact reason is their",
    "start": "1359900",
    "end": "1363140"
  },
  {
    "text": "TBD",
    "start": "1363140",
    "end": "1365860"
  },
  {
    "text": "uh one issue that there bothers us is",
    "start": "1366340",
    "end": "1370460"
  },
  {
    "text": "the silence rotary and timeouts it still",
    "start": "1370460",
    "end": "1374299"
  },
  {
    "text": "kind of happens uh from time to time",
    "start": "1374299",
    "end": "1377360"
  },
  {
    "text": "although much infrequent now",
    "start": "1377360",
    "end": "1380840"
  },
  {
    "text": "um what we see would be the rate limit",
    "start": "1380840",
    "end": "1384679"
  },
  {
    "text": "error that's in the logs we will see a",
    "start": "1384679",
    "end": "1387860"
  },
  {
    "text": "lot of 429s with signature like put in a",
    "start": "1387860",
    "end": "1391820"
  },
  {
    "text": "point ID too many requests so we kind of",
    "start": "1391820",
    "end": "1394520"
  },
  {
    "text": "understand it was due to the written",
    "start": "1394520",
    "end": "1396740"
  },
  {
    "text": "limits thus we initially tuned the",
    "start": "1396740",
    "end": "1401059"
  },
  {
    "text": "Adaptive rate limiter after that indeed",
    "start": "1401059",
    "end": "1404960"
  },
  {
    "text": "it's much less for 29 errors however",
    "start": "1404960",
    "end": "1408880"
  },
  {
    "text": "it was discovered that they essentially",
    "start": "1408880",
    "end": "1412159"
  },
  {
    "text": "turned into the psyllium API client",
    "start": "1412159",
    "end": "1415220"
  },
  {
    "text": "timeout exceeded error which is even",
    "start": "1415220",
    "end": "1417860"
  },
  {
    "text": "worse because that clearly indicates",
    "start": "1417860",
    "end": "1421280"
  },
  {
    "text": "psyllium couldn't handle the birth of",
    "start": "1421280",
    "end": "1425240"
  },
  {
    "text": "request",
    "start": "1425240",
    "end": "1427700"
  },
  {
    "text": "so",
    "start": "1427700",
    "end": "1430039"
  },
  {
    "text": "even though it can serve here slowly but",
    "start": "1430039",
    "end": "1433220"
  },
  {
    "text": "it is there cause test failures",
    "start": "1433220",
    "end": "1435820"
  },
  {
    "text": "especially it's difficult for psyllium",
    "start": "1435820",
    "end": "1438500"
  },
  {
    "text": "to keep up during the product creation",
    "start": "1438500",
    "end": "1441940"
  },
  {
    "text": "and the",
    "start": "1441940",
    "end": "1444140"
  },
  {
    "text": "IP addresses will not be assigned timely",
    "start": "1444140",
    "end": "1447620"
  },
  {
    "text": "causing the ports not healthy before",
    "start": "1447620",
    "end": "1450080"
  },
  {
    "text": "test timeout so as the mitigation we had",
    "start": "1450080",
    "end": "1455059"
  },
  {
    "text": "to introduce a new module the first",
    "start": "1455059",
    "end": "1457580"
  },
  {
    "text": "control in the customer schedule",
    "start": "1457580",
    "end": "1459500"
  },
  {
    "text": "extension to avoid scheduling too many",
    "start": "1459500",
    "end": "1462140"
  },
  {
    "text": "parts on the nodes at once",
    "start": "1462140",
    "end": "1466159"
  },
  {
    "text": "to conclude our talk",
    "start": "1466159",
    "end": "1469640"
  },
  {
    "text": "here is some of our key takeaways first",
    "start": "1469640",
    "end": "1472400"
  },
  {
    "text": "psyllium is a good technology that we",
    "start": "1472400",
    "end": "1474679"
  },
  {
    "text": "love a lot we achieved much higher cost",
    "start": "1474679",
    "end": "1477380"
  },
  {
    "text": "efficiency and ebpf opens doors for a",
    "start": "1477380",
    "end": "1481400"
  },
  {
    "text": "lot of great possibilities like the",
    "start": "1481400",
    "end": "1483620"
  },
  {
    "text": "observability security stuff however",
    "start": "1483620",
    "end": "1486620"
  },
  {
    "text": "just for solving the networking problem",
    "start": "1486620",
    "end": "1488419"
  },
  {
    "text": "won't be the golden ticket to upgrade",
    "start": "1488419",
    "end": "1491780"
  },
  {
    "text": "psyllium at its gear is not sufficient",
    "start": "1491780",
    "end": "1494659"
  },
  {
    "text": "to just adopt it we really need to have",
    "start": "1494659",
    "end": "1497120"
  },
  {
    "text": "a good understanding of psyllium under",
    "start": "1497120",
    "end": "1498799"
  },
  {
    "text": "the hood specifically how it operates",
    "start": "1498799",
    "end": "1502039"
  },
  {
    "text": "and set up monitoring and alerts",
    "start": "1502039",
    "end": "1504500"
  },
  {
    "text": "appropriate",
    "start": "1504500",
    "end": "1506659"
  },
  {
    "text": "so that's about our presentation thank",
    "start": "1506659",
    "end": "1510620"
  },
  {
    "text": "you for the time to coming to our",
    "start": "1510620",
    "end": "1512480"
  },
  {
    "text": "presentation and appreciate your",
    "start": "1512480",
    "end": "1514460"
  },
  {
    "text": "interest in this topic",
    "start": "1514460",
    "end": "1517840"
  }
]