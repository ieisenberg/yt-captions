[
  {
    "text": "okay let's get started so hi everyone super excited to be here I'm Pia I'm a",
    "start": "30",
    "end": "7230"
  },
  {
    "text": "doctor engineering at up Derrick I lead the production engineering teams which encompasses the tech ops platform and",
    "start": "7230",
    "end": "15680"
  },
  {
    "text": "developer productivity groups and I'm Alex stop software engineer at director",
    "start": "15680",
    "end": "22260"
  },
  {
    "text": "since 2015 I'm actually on the left on the picture",
    "start": "22260",
    "end": "27470"
  },
  {
    "text": "yeah I've been working on the repairs leadership for while going on four years now mostly in a DevOps role so on a",
    "start": "27470",
    "end": "35579"
  },
  {
    "text": "platform team and we've been going through a lot of migrations a lot of changes from our marlott application to",
    "start": "35579",
    "end": "42870"
  },
  {
    "text": "kubernetes so i'm probably one of the best person to guide you through our",
    "start": "42870",
    "end": "49320"
  },
  {
    "text": "changes okay so today we will talk about our beginnings and our migration - like",
    "start": "49320",
    "end": "57390"
  },
  {
    "text": "in the different steps I would say in our network architecture and and more importantly we'll talk about where did",
    "start": "57390",
    "end": "63960"
  },
  {
    "text": "we fail in all of this so hopefully some lessons learned some lessons that we learned that that you could take on and",
    "start": "63960",
    "end": "69659"
  },
  {
    "text": "yeah so whoo what is AB Derek so Derek is a cloud commerce platform",
    "start": "69659",
    "end": "76110"
  },
  {
    "text": "that was founded in 2009 in San Francisco at the month we have around 800 employees engineering is 300",
    "start": "76110",
    "end": "83030"
  },
  {
    "text": "engineers strong typical end users of",
    "start": "83030",
    "end": "88170"
  },
  {
    "text": "our platform will use us let's say you start your own company you want to get Internet but you also want to get apps",
    "start": "88170",
    "end": "94920"
  },
  {
    "text": "like let's say Google Apps you want to get Dropbox or just or box but you don't",
    "start": "94920",
    "end": "99960"
  },
  {
    "text": "want to go in different websites and you don't want to go register your credit card and manage your your employees so",
    "start": "99960",
    "end": "106710"
  },
  {
    "text": "who has access to what application when so app Derek is a kind of will help you simplify this and we think we take care",
    "start": "106710",
    "end": "113159"
  },
  {
    "text": "of the billing we take care of their user provisioning and all the I would say the annoying steps we launched our",
    "start": "113159",
    "end": "121259"
  },
  {
    "text": "first marketplace in 2011 and back then the engineering thing was really small",
    "start": "121259",
    "end": "127350"
  },
  {
    "text": "yet a lot of features fast and that meant that we had a mono it",
    "start": "127350",
    "end": "133270"
  },
  {
    "text": "so when I joined a company in 2014 the decision was made that let's move to",
    "start": "133270",
    "end": "138940"
  },
  {
    "text": "services we had some teams who wanted to deploy nodejs apps or spring boot apps so they came to us and say in an ask us",
    "start": "138940",
    "end": "148060"
  },
  {
    "text": "okay later we want to deploy this what's the pattern where do we ship this this thing and back then the the platform",
    "start": "148060",
    "end": "155110"
  },
  {
    "text": "team was quite small we were responsible for the CI and the deployment infrastructure and then we started",
    "start": "155110",
    "end": "162220"
  },
  {
    "text": "looking at the kubernetes I don't want to go too deep and the why did we select Cabrini's but we looked at messes we",
    "start": "162220",
    "end": "169450"
  },
  {
    "text": "looked at Ashley Corp nomads and communities and some dr. Campos also and",
    "start": "169450",
    "end": "175270"
  },
  {
    "text": "and back then our first version of Cabrini's was on cube 1.1 and 2015 and",
    "start": "175270",
    "end": "182860"
  },
  {
    "text": "back then there was no cops there was no cube atm so managing the cluster was not",
    "start": "182860",
    "end": "188080"
  },
  {
    "text": "easy so we built some tooling around chef with some cloud init scripts with",
    "start": "188080",
    "end": "194860"
  },
  {
    "text": "some terraform and then it l+ but we",
    "start": "194860",
    "end": "200170"
  },
  {
    "text": "still had a lot of challenges on the networking side yep because we had a",
    "start": "200170",
    "end": "205780"
  },
  {
    "text": "network before kubernetes right we were serving requests for our model at application so all traffic would",
    "start": "205780",
    "end": "213760"
  },
  {
    "text": "typically go from the internet from the cloud to a load balancer the thing is we",
    "start": "213760",
    "end": "219850"
  },
  {
    "text": "are a multi-tenant up and white-label platform so that",
    "start": "219850",
    "end": "226030"
  },
  {
    "text": "meant that we add one load balancer per partner because we're doing SSL termination for them this load balancer",
    "start": "226030",
    "end": "233380"
  },
  {
    "text": "would then forward traffic down to the model not only this but we're also a",
    "start": "233380",
    "end": "238960"
  },
  {
    "text": "multi cloud deployment application so we ship our monolith to multiple partners",
    "start": "238960",
    "end": "246970"
  },
  {
    "text": "that owes them that that o stood themselves on-premise so we had a bunch",
    "start": "246970",
    "end": "253180"
  },
  {
    "text": "of load balancers it was a big mess of a festival but eventually we stream down",
    "start": "253180",
    "end": "258700"
  },
  {
    "text": "all of this with server name identification SMI so we shrink all of",
    "start": "258700",
    "end": "264430"
  },
  {
    "text": "our little bands - just a few instances since we're on premise we also make claw diagnostic",
    "start": "264430",
    "end": "272000"
  },
  {
    "text": "choices mostly because we don't want to rely on advanced features from our cloud",
    "start": "272000",
    "end": "277010"
  },
  {
    "text": "providers because they might not be accessible everywhere back in the days",
    "start": "277010",
    "end": "283310"
  },
  {
    "text": "we also spent a lot of time on our terraform automation because we wanted",
    "start": "283310",
    "end": "288950"
  },
  {
    "text": "the developers to replicate this setup in their dev environment so for us",
    "start": "288950",
    "end": "294220"
  },
  {
    "text": "registering a DNS record sending traffic to a load balancer and splitting in to",
    "start": "294220",
    "end": "300080"
  },
  {
    "text": "multiple instances was a recipe that we had on hand and that we replicated",
    "start": "300080",
    "end": "306200"
  },
  {
    "text": "heavily so with cube 1.1 when we started we didn't have any shiny ingress",
    "start": "306200",
    "end": "313400"
  },
  {
    "text": "resource types and our mindset was well",
    "start": "313400",
    "end": "318710"
  },
  {
    "text": "if we used old man certain paths let's keep on using load balancers so we added",
    "start": "318710",
    "end": "325100"
  },
  {
    "text": "a bunch of them again now every service every micro service that we wrote",
    "start": "325100",
    "end": "330229"
  },
  {
    "text": "running in kubernetes was fronted by its own public load balancer so what we",
    "start": "330229",
    "end": "336200"
  },
  {
    "text": "would do is register a DNS record for each one of our service send it to a",
    "start": "336200",
    "end": "341210"
  },
  {
    "text": "load balancer to do as a cell termination we would then forward traffic down to the services running on",
    "start": "341210",
    "end": "347870"
  },
  {
    "text": "statically assigned node ports that meant as an individual node port for",
    "start": "347870",
    "end": "354290"
  },
  {
    "text": "each service a quickly grew up to a list who wants to manage a list of reserved",
    "start": "354290",
    "end": "360830"
  },
  {
    "text": "and old ports in 2015 we knew it was a bad partner and we needed to get rid of it",
    "start": "360830",
    "end": "367600"
  },
  {
    "text": "not only is this not easy to manage but",
    "start": "368110",
    "end": "375080"
  },
  {
    "text": "it might seem simple with only two services now on the diagram but think",
    "start": "375080",
    "end": "380660"
  },
  {
    "text": "about having 20 micro services in 10 different deployments even though we have some automated tariffs or modules",
    "start": "380660",
    "end": "387500"
  },
  {
    "text": "that automation breaks because we have to manage all of those terraform state files and it quickly gets very messy",
    "start": "387500",
    "end": "394490"
  },
  {
    "text": "duplicated and a lot of clutter all of the clients all that the clients",
    "start": "394490",
    "end": "403370"
  },
  {
    "text": "were doing is send requests down to publicly available domain names this",
    "start": "403370",
    "end": "410180"
  },
  {
    "text": "means that we weren't actually doing any routing we weren't adding any logic we weren't forwarding traffic we were only",
    "start": "410180",
    "end": "417620"
  },
  {
    "text": "relying on DNS queries for us to receive traffic and for client application to",
    "start": "417620",
    "end": "422629"
  },
  {
    "text": "actually have the knowledge we're sending traffic for me being a DevOps",
    "start": "422629",
    "end": "428930"
  },
  {
    "text": "the worst thing was the development teams were not at all autonomous because",
    "start": "428930",
    "end": "434990"
  },
  {
    "text": "they didn't have the keys of production so when they wanted to deploy a new service they needed to involve us the",
    "start": "434990",
    "end": "440569"
  },
  {
    "text": "platform team or our operations team to bootstrap that load balancer and patch the domain names so a lot of cycles were",
    "start": "440569",
    "end": "448490"
  },
  {
    "text": "lost and a lot of productivity went down the drain so we thought about actually",
    "start": "448490",
    "end": "455479"
  },
  {
    "text": "building some automation our first step was with console template and a chip",
    "start": "455479",
    "end": "460699"
  },
  {
    "text": "proxy so we replaced all of our DNS with a single one a wild-card DNS record that",
    "start": "460699",
    "end": "467300"
  },
  {
    "text": "would send all the things from start Erica come down to a single HD proxy so",
    "start": "467300",
    "end": "473180"
  },
  {
    "text": "we streamline all of our load balancer to a single instance of each a proxy the",
    "start": "473180",
    "end": "478669"
  },
  {
    "text": "a chip rotc configuration was driven by console template console template would",
    "start": "478669",
    "end": "484520"
  },
  {
    "text": "read key values keys value keys being the service name and the values being the node ports to generate the active",
    "start": "484520",
    "end": "492650"
  },
  {
    "text": "proxy configuration and reload on changes this meant that we also built a",
    "start": "492650",
    "end": "498440"
  },
  {
    "text": "small registration tool that would read the configuration from the cube API and",
    "start": "498440",
    "end": "504039"
  },
  {
    "text": "register the service down in console",
    "start": "504039",
    "end": "508870"
  },
  {
    "text": "there's already a bunch of improvements as you can see we've well cleaned up a lot of our duplications and we got rid",
    "start": "509169",
    "end": "517099"
  },
  {
    "text": "of the node port list because now they are all randomly assigned and we can",
    "start": "517099",
    "end": "522289"
  },
  {
    "text": "fetch them from the cubic yard but it's still very complex and as you",
    "start": "522289",
    "end": "529050"
  },
  {
    "text": "might have guessed by the color of the boxes for some reason we decided to build this whole network infrastructure",
    "start": "529050",
    "end": "534270"
  },
  {
    "text": "outside of kubernetes we were making mistakes but also learning a lot",
    "start": "534270",
    "end": "540710"
  },
  {
    "text": "we had performance issues because in the end we ended up saturating our network link between console template and",
    "start": "540710",
    "end": "547200"
  },
  {
    "text": "console with the number of active connections and watches that were required to support up to hundred micro",
    "start": "547200",
    "end": "554100"
  },
  {
    "text": "services also meant changing our deployment pipeline because now engineers and developers had to be aware",
    "start": "554100",
    "end": "561390"
  },
  {
    "text": "of this registration tool so they could ship their services it was the first",
    "start": "561390",
    "end": "568800"
  },
  {
    "text": "time we were playing with the cube API and we really enjoyed it so we thought about pushing things a little further we",
    "start": "568800",
    "end": "577110"
  },
  {
    "text": "came up with a solution that we called our domain router the same concept applies we still have a single DNS",
    "start": "577110",
    "end": "583290"
  },
  {
    "text": "record that's all public so we send all the micro service perfect down to the",
    "start": "583290",
    "end": "588570"
  },
  {
    "text": "single load balancer where we do SSL termination then we forward our traffic",
    "start": "588570",
    "end": "594270"
  },
  {
    "text": "down to our domain router on a single node port a single one at least it's not",
    "start": "594270",
    "end": "599430"
  },
  {
    "text": "a list then the domain router can send the traffic down to micro services sorry",
    "start": "599430",
    "end": "607860"
  },
  {
    "text": "about that using the core DNS or cube DNS back then",
    "start": "607860",
    "end": "615080"
  },
  {
    "text": "so our domain router really all it is it's still running each proxy but in",
    "start": "615080",
    "end": "621300"
  },
  {
    "text": "more popular terms now we would say that our domain router is a go control plane that's using the cube API to build an",
    "start": "621300",
    "end": "628830"
  },
  {
    "text": "h-shaped proxy data plane upon changes upon because we were watching services",
    "start": "628830",
    "end": "635400"
  },
  {
    "text": "we would simply reload the h8 proxy and we were driving all of those changes",
    "start": "635400",
    "end": "641280"
  },
  {
    "text": "through service annotations because this in our timeline fits",
    "start": "641280",
    "end": "647760"
  },
  {
    "text": "about right with the time where we implemented get ups so won't go too much",
    "start": "647760",
    "end": "653790"
  },
  {
    "text": "into details are plenty of good sessions about github and the conference but essentially whatever is",
    "start": "653790",
    "end": "659430"
  },
  {
    "text": "your manifest file in git is the actual cluster state so it's always in sync",
    "start": "659430",
    "end": "666800"
  },
  {
    "text": "developers were already aware of this and making changing themselves when they wanted to deploy a new version of their",
    "start": "666800",
    "end": "672540"
  },
  {
    "text": "services so it was very easy for them to simply add an annotation to their service and ship and make their service",
    "start": "672540",
    "end": "680130"
  },
  {
    "text": "available publicly we also use this pattern in a kind of service messy way",
    "start": "680130",
    "end": "687899"
  },
  {
    "text": "where our model is application that's actually still running on old legacy instances outside of kubernetes could",
    "start": "687899",
    "end": "694500"
  },
  {
    "text": "reach internal services on a private link using the same DNS this was great",
    "start": "694500",
    "end": "703529"
  },
  {
    "text": "we actually removed ourselves from the equation developers were autonomous and",
    "start": "703529",
    "end": "709470"
  },
  {
    "text": "our proxy configuration was entirely dynamic great wins but there was still",
    "start": "709470",
    "end": "718350"
  },
  {
    "text": "some drawbacks because all of our services were registered under the adler com domain we were essentially breaking",
    "start": "718350",
    "end": "725550"
  },
  {
    "text": "our multi-tenancy we were breaking multi-tenancy but it",
    "start": "725550",
    "end": "731850"
  },
  {
    "text": "also implied a lot of complexity for client applications that wanted to integrate with us because we were kind",
    "start": "731850",
    "end": "739080"
  },
  {
    "text": "of exposing our domain modeling to them with all of those different DNS entries",
    "start": "739080",
    "end": "745010"
  },
  {
    "text": "we needed to use something stronger we needed the force and a Strangler pattern",
    "start": "745010",
    "end": "752720"
  },
  {
    "text": "stronger pattern was popularized term by martin fowler and for us the Strangler pattern took the form of an api gateway",
    "start": "752720",
    "end": "760339"
  },
  {
    "text": "with an api gateway we could front all traffic inbound send it to the mallet or",
    "start": "760339",
    "end": "767910"
  },
  {
    "text": "if a new application a new micro service was serving the same api we could then",
    "start": "767910",
    "end": "773240"
  },
  {
    "text": "choke the traffic send it to the new micro service implementation with the",
    "start": "773240",
    "end": "778920"
  },
  {
    "text": "strangler pattern we were also able to build an authentication tracing and",
    "start": "778920",
    "end": "784620"
  },
  {
    "text": "observability because while these were mechanism does were being reimplemented",
    "start": "784620",
    "end": "791610"
  },
  {
    "text": "and over in each microservice because well it's not really business logic but",
    "start": "791610",
    "end": "797010"
  },
  {
    "text": "it's utils and boilerplate that every application needs we could all we could",
    "start": "797010",
    "end": "802230"
  },
  {
    "text": "remove all of this boilerplate put it in our API gateway well that was a strategy",
    "start": "802230",
    "end": "809570"
  },
  {
    "text": "so we looked at the landscape we asked yourself should we build it or should we",
    "start": "810080",
    "end": "815340"
  },
  {
    "text": "buy it and although we were ready to contribute some open-source software we weren't really ready to build it",
    "start": "815340",
    "end": "822420"
  },
  {
    "text": "entirely because well a direct is not in the API gateway building business so for",
    "start": "822420",
    "end": "829650"
  },
  {
    "text": "us it should have been an appliance that we simply install in our clusters we looked at quickly I called provider",
    "start": "829650",
    "end": "835860"
  },
  {
    "text": "solutions but since we're multi tenant and multi deployment we must remain",
    "start": "835860",
    "end": "841710"
  },
  {
    "text": "colin gnostic so no fancy amazonica gateway for us then we looked at open",
    "start": "841710",
    "end": "847680"
  },
  {
    "text": "source solutions including Zul tick and koong koong was our first prototype",
    "start": "847680",
    "end": "853230"
  },
  {
    "text": "anybody from Kong in the audience right I don't want to seem to alarm it but it",
    "start": "853230",
    "end": "861090"
  },
  {
    "text": "didn't really work for us because Kong uses this model of well it requires an",
    "start": "861090",
    "end": "868230"
  },
  {
    "text": "external data source to drive the actual API gateway configuration and none of us",
    "start": "868230",
    "end": "874680"
  },
  {
    "text": "a table req add operational knowledge of that database so it was already a lot of",
    "start": "874680",
    "end": "881190"
  },
  {
    "text": "complexity for us and a lot of learning to go through when we wanted to",
    "start": "881190",
    "end": "886260"
  },
  {
    "text": "contribute more connectors because we are big my school shop while our",
    "start": "886260",
    "end": "892140"
  },
  {
    "text": "contributions were really well received because it would add more maintenance and it wasn't exactly the way to come",
    "start": "892140",
    "end": "899340"
  },
  {
    "text": "faults were heading to we also faced some challenges because we need to",
    "start": "899340",
    "end": "905310"
  },
  {
    "text": "reconcile this tape in the database with this tape in the cluster so we had kind",
    "start": "905310",
    "end": "911490"
  },
  {
    "text": "of two systems to manage and keep in sync so it was challenging for us also",
    "start": "911490",
    "end": "917040"
  },
  {
    "text": "but we did spend a lot of time we built some great prototypes in the end we",
    "start": "917040",
    "end": "922050"
  },
  {
    "text": "weren't really satisfied so having been at previous cube con we",
    "start": "922050",
    "end": "927820"
  },
  {
    "text": "knew about envoi also well so it seemed",
    "start": "927820",
    "end": "932920"
  },
  {
    "text": "like a natural fit to at least explore this possibility none of us do C++ so we",
    "start": "932920",
    "end": "941050"
  },
  {
    "text": "weren't really a fan of actually getting involved in the actual invoice code and",
    "start": "941050",
    "end": "947970"
  },
  {
    "text": "building our own control plane completely seemed like a very hard task",
    "start": "947970",
    "end": "954070"
  },
  {
    "text": "to tackle with a very small team but we kept our eyes open and eventually",
    "start": "954070",
    "end": "959380"
  },
  {
    "text": "stumbled upon ambassador there a little penguin with a bowtie this is a product",
    "start": "959380",
    "end": "964810"
  },
  {
    "text": "by data wire little company based out of Boston and it is essentially a control",
    "start": "964810",
    "end": "970300"
  },
  {
    "text": "plane over invoice exactly what we were looking for building not only this but",
    "start": "970300",
    "end": "975910"
  },
  {
    "text": "they were very aligned on our vision as we've rolled out our domain router all",
    "start": "975910",
    "end": "982050"
  },
  {
    "text": "control based out of service annotations they actually adopted the exact same model for configuring their Ambassador",
    "start": "982050",
    "end": "989410"
  },
  {
    "text": "gateway so we saw there was a pretty good fit for us and that's actually what",
    "start": "989410",
    "end": "998680"
  },
  {
    "text": "it looks like very similar to our domain router because we now send the traffic",
    "start": "998680",
    "end": "1004290"
  },
  {
    "text": "from all the domain names ours and those of our partners down to this load",
    "start": "1004290",
    "end": "1010050"
  },
  {
    "text": "balancer where we do SSL termination with SLA then we forward all traffic",
    "start": "1010050",
    "end": "1015300"
  },
  {
    "text": "down to this single node port where ambassador runs ambassador will then",
    "start": "1015300",
    "end": "1021380"
  },
  {
    "text": "dispatch the request down to some service or if the service has not been migrated yet to our mount application",
    "start": "1021380",
    "end": "1028470"
  },
  {
    "text": "living in a parallel infrastructure the service annotation very simple it's",
    "start": "1028470",
    "end": "1036870"
  },
  {
    "text": "still llamó objects so they could eventually be extracted at CR DS but it",
    "start": "1036870",
    "end": "1042630"
  },
  {
    "text": "was in the concept back then the simple difference is that now instead of doing",
    "start": "1042630",
    "end": "1049110"
  },
  {
    "text": "routing based on the domain a request domain we do routing based on the",
    "start": "1049110",
    "end": "1055290"
  },
  {
    "text": "request path and as you can see by the yamo sample it has a lot of features including rate",
    "start": "1055290",
    "end": "1061890"
  },
  {
    "text": "limiting and course origin filters so",
    "start": "1061890",
    "end": "1068370"
  },
  {
    "text": "yeah let's zoom in a little bit on the Ambassador box so ambassador itself is a",
    "start": "1068370",
    "end": "1075210"
  },
  {
    "text": "Python control plane using the cube API to generate the Envoy data plane in our",
    "start": "1075210",
    "end": "1081660"
  },
  {
    "text": "deployment we ship it with two side cars the first side car is a rate limit or",
    "start": "1081660",
    "end": "1087450"
  },
  {
    "text": "service that's being invoked in G RPC and as you've probably guessed it it does our rate limiting so this is logic",
    "start": "1087450",
    "end": "1095520"
  },
  {
    "text": "that we implemented ourselves it is not part of Ambassador we can ship ambassador and provide a sidecar for our",
    "start": "1095520",
    "end": "1103080"
  },
  {
    "text": "own business logic and the same applies for our context filter our context filter injects metadata about tenant",
    "start": "1103080",
    "end": "1111150"
  },
  {
    "text": "information and also does a temptation and authorization as I'm a server",
    "start": "1111150",
    "end": "1118800"
  },
  {
    "text": "supports a lot of a lot of use cases including traceability and metrics",
    "start": "1118800",
    "end": "1126180"
  },
  {
    "text": "collection course filtering traffic shadowing it even plays very nicely with",
    "start": "1126180",
    "end": "1132360"
  },
  {
    "text": "this tile since its end boy these are all features that we ship with or we",
    "start": "1132360",
    "end": "1139110"
  },
  {
    "text": "could ship we don't use them all but we could ship with our ambassador installation yeah so moving 100% of our",
    "start": "1139110",
    "end": "1147120"
  },
  {
    "text": "traffic to this new architecture was not taking lightly so envoi provides nice integration if you want to fetch metrics",
    "start": "1147120",
    "end": "1153750"
  },
  {
    "text": "and push them to your Prometheus instance so that's what we started doing so we added especially like the the jump",
    "start": "1153750",
    "end": "1160200"
  },
  {
    "text": "to our monolith we were not sure about like the latency hit so each request would come into our API gateway and",
    "start": "1160200",
    "end": "1166020"
  },
  {
    "text": "we'll go through the context filter which will make a call to our monolith endpoint to inject the tenant",
    "start": "1166020",
    "end": "1171510"
  },
  {
    "text": "information so we were not sure how much they didn't see this will add and were we ok with this",
    "start": "1171510",
    "end": "1176520"
  },
  {
    "text": "direct doesn't have like millions of requests per second but but we still",
    "start": "1176520",
    "end": "1181710"
  },
  {
    "text": "cared about like the user experience in the end so we contributed some graph and",
    "start": "1181710",
    "end": "1187050"
  },
  {
    "text": "dashboards the links are provided at the bottom of the if you want to take a look and we also I",
    "start": "1187050",
    "end": "1192390"
  },
  {
    "text": "mean we integrated with our log systems so you supplant for logging like Alex mentioned we we have our yeah ger for",
    "start": "1192390",
    "end": "1199470"
  },
  {
    "text": "tracing integration and we have our metric system on Prometheus so it's not perfect I think a lot of companies are",
    "start": "1199470",
    "end": "1205230"
  },
  {
    "text": "still facing the challenges of observability but we're getting there yeah that's it so so this story is kind",
    "start": "1205230",
    "end": "1213600"
  },
  {
    "text": "of pretty now we have an API gateway and everything is perfect right so where did we fail in all of this because it didn't",
    "start": "1213600",
    "end": "1219840"
  },
  {
    "text": "go according to plan so the first failure slash lessons learned was that back then the platform",
    "start": "1219840",
    "end": "1226650"
  },
  {
    "text": "team was operating the community's infrastructure and the API gateway and we also have it we also had a tech ops",
    "start": "1226650",
    "end": "1232980"
  },
  {
    "text": "team that was on a different or Gunder a different leader so the fact that you have two teams not really aligned",
    "start": "1232980",
    "end": "1239070"
  },
  {
    "text": "working together added unnecessary unnecessary layers between us so they",
    "start": "1239070",
    "end": "1245220"
  },
  {
    "text": "had some a tree proxy or some elby's between the legacy monolith infrastructure and our kubernetes",
    "start": "1245220",
    "end": "1251040"
  },
  {
    "text": "infrastructure so in the end we had our Prometheus and we have our nice graph and dashboards but they had their own",
    "start": "1251040",
    "end": "1256680"
  },
  {
    "text": "monitoring systems based on sanzu and zabbix so now we're operating under one",
    "start": "1256680",
    "end": "1262110"
  },
  {
    "text": "org but I would say that yeah it's something to watch out for contributing",
    "start": "1262110",
    "end": "1268380"
  },
  {
    "text": "to open source so the fact that we were willing to put engineering manpower and contribute to an open-source project is",
    "start": "1268380",
    "end": "1273930"
  },
  {
    "text": "really nice but you have to be sure that you're aligned with the vision of the open source project so how do you reach",
    "start": "1273930",
    "end": "1281310"
  },
  {
    "text": "out to them do you use github issues and complain about missing features and wait and ping no so what we did for a data",
    "start": "1281310",
    "end": "1289800"
  },
  {
    "text": "wire and ambassador we reached out to the regular channel and then we asked like we want to contribute and we shared",
    "start": "1289800",
    "end": "1296640"
  },
  {
    "text": "our vision and they were really receptive to us and then we started sending PRS and then I guess at some",
    "start": "1296640",
    "end": "1303240"
  },
  {
    "text": "point a trust came with it so the quality of our code and how we fix the bugs because there",
    "start": "1303240",
    "end": "1309000"
  },
  {
    "text": "were bugs found obviously in features we added but I would say that like there was a huge impact on the motivation of",
    "start": "1309000",
    "end": "1315090"
  },
  {
    "text": "the team so it's hard but if you can get it to work it's it's awesome because atop the REC we have we have a lot of",
    "start": "1315090",
    "end": "1321360"
  },
  {
    "text": "internal systems into all tools and engineers don't get visibility I mean they can they can go",
    "start": "1321360",
    "end": "1327450"
  },
  {
    "text": "to meetups and present but they don't get visibility for that so if you contribute an open source project that that's great and and the other thing was",
    "start": "1327450",
    "end": "1334560"
  },
  {
    "text": "watch out for defensive decisions I don't know if you heard this term before but just to give you an example we we",
    "start": "1334560",
    "end": "1341280"
  },
  {
    "text": "spend six months on a on a POC and then after that I went to cube con last year",
    "start": "1341280",
    "end": "1346730"
  },
  {
    "text": "in in Austin and then I saw the invoice and then I started we started looking at ambassador and then we asked our self",
    "start": "1346730",
    "end": "1353310"
  },
  {
    "text": "okay do we ship our POC do we get like a Cassandra or Postgres as a service",
    "start": "1353310",
    "end": "1358950"
  },
  {
    "text": "database and do we ship this to production and then do we revisit later and so as you may know the revisit later",
    "start": "1358950",
    "end": "1365010"
  },
  {
    "text": "may never happen because there's always a new emergency or a new parody coming up so we so we made the jump and we",
    "start": "1365010",
    "end": "1372150"
  },
  {
    "text": "decided okay let's do it let's let's switch to ambassador and yeah and let's add a new contribute to it and so I",
    "start": "1372150",
    "end": "1378960"
  },
  {
    "text": "would say that it was a risk but in the end the team didn't feel like the POC was the right thing to do so it kind of",
    "start": "1378960",
    "end": "1386580"
  },
  {
    "text": "I would say I underestimated because it in a month I would say in a month of work we were back on the same level as",
    "start": "1386580",
    "end": "1391890"
  },
  {
    "text": "our first you see so first was a great experience there are things adding an epi gateway",
    "start": "1391890",
    "end": "1399450"
  },
  {
    "text": "deploying cube infrastructure and all these nice tools will not fix your culture so we had this mindset of if you",
    "start": "1399450",
    "end": "1406020"
  },
  {
    "text": "build it they will come you know we'll provide the greatest infrastructure and then all the teams are going to move their code outside of the monolith to",
    "start": "1406020",
    "end": "1413490"
  },
  {
    "text": "services and they will move the data and everything will be perfect what happen is that it's kind of a complex system so",
    "start": "1413490",
    "end": "1419490"
  },
  {
    "text": "each team has their own preferences and their own ways to approach a migration like that so it's kind of they build a",
    "start": "1419490",
    "end": "1426240"
  },
  {
    "text": "little service they learn from it and then they start to attack a real big service and then they move the data out",
    "start": "1426240",
    "end": "1432110"
  },
  {
    "text": "so ARMA that is still around some companies that the luxury to freeze code for two or three years we don't have",
    "start": "1432110",
    "end": "1438840"
  },
  {
    "text": "this luxury so the business is doing well but we have to keep shipping features ad code in our monolith and add",
    "start": "1438840",
    "end": "1444780"
  },
  {
    "text": "probably in our services and move the data out so it's still a challenge so",
    "start": "1444780",
    "end": "1450780"
  },
  {
    "text": "what's next for us like like I described we had we have extra layers that we want to simplify so ambassador as a SN I support",
    "start": "1450780",
    "end": "1458020"
  },
  {
    "text": "or will Abbess and I support really soon so we want to leverage that we want to remove some are some of our HT proxy",
    "start": "1458020",
    "end": "1464020"
  },
  {
    "text": "layers where we do SSL termination with that to ambassador we might simplify also the monitoring tools so probably",
    "start": "1464020",
    "end": "1470740"
  },
  {
    "text": "everything is going to move to Prometheus and we're also investing on CR these so for example we use rabbitmq",
    "start": "1470740",
    "end": "1476679"
  },
  {
    "text": "to configure our exchanges and queues so we want to make it self-service for the engineers via a CR these on a github on",
    "start": "1476679",
    "end": "1483910"
  },
  {
    "text": "our github pipeline same concept for databases we have my sequel and will apply we're also challenged at the",
    "start": "1483910",
    "end": "1491770"
  },
  {
    "text": "moment because we have a lot of test environments so you probably saw the post on Twitter everyone's talking about testing in production and now everything",
    "start": "1491770",
    "end": "1497710"
  },
  {
    "text": "is perfect when you test in prod so we want to explore that but it's yeah it's",
    "start": "1497710",
    "end": "1502809"
  },
  {
    "text": "going to take time I on the observability monitoring side we still need to add more eyes on it so we're",
    "start": "1502809",
    "end": "1510070"
  },
  {
    "text": "working on that at the moment and we're also interested in working with Ambassador on some features like",
    "start": "1510070",
    "end": "1516809"
  },
  {
    "text": "improving the contributor experience and also like we're starting to do graph QL at AB direct so we could leverage some",
    "start": "1516809",
    "end": "1523990"
  },
  {
    "text": "features around that that's all thank",
    "start": "1523990",
    "end": "1529570"
  },
  {
    "text": "you thanks everyone [Applause]",
    "start": "1529570",
    "end": "1537290"
  },
  {
    "text": "you have questions we can stick around and yeah",
    "start": "1537290",
    "end": "1543500"
  },
  {
    "text": "well we have in capsular exactly the question yes how do we secure basically",
    "start": "1557300",
    "end": "1563220"
  },
  {
    "text": "our network infrastructure for inbound traffic and this goes back to the layers pierre was mentioning we have so many of",
    "start": "1563220",
    "end": "1569640"
  },
  {
    "text": "them including in capsule that's a web application firewall that actually",
    "start": "1569640",
    "end": "1575190"
  },
  {
    "text": "handles the traffic even before we do SSL termination in our infrastructure",
    "start": "1575190",
    "end": "1581539"
  },
  {
    "text": "so yeah so did we evaluate moving our ingress from a chipped Roxy to another",
    "start": "1593800",
    "end": "1599410"
  },
  {
    "text": "solution we did especially when envoi came around we",
    "start": "1599410",
    "end": "1605560"
  },
  {
    "text": "wanted to build a new control plane for it and also there was some releases also",
    "start": "1605560",
    "end": "1614650"
  },
  {
    "text": "from H a proxy 1:8 I think that supported seamless reloads so that kind",
    "start": "1614650",
    "end": "1620050"
  },
  {
    "text": "of helped us in well not actually migrating but just keep the project life",
    "start": "1620050",
    "end": "1626500"
  },
  {
    "text": "a little bit because it was solving some of the issues that we would have solved by changing provider so yeah maybe we'll",
    "start": "1626500",
    "end": "1635530"
  },
  {
    "text": "remove a some HF proxies leverage and boy for for that we'll see how it goes",
    "start": "1635530",
    "end": "1642809"
  },
  {
    "text": "oh well there's a lot of reasons including everything's related to",
    "start": "1650740",
    "end": "1656409"
  },
  {
    "text": "traceability to authentication we can we provided our own filter or actually our",
    "start": "1656409",
    "end": "1662919"
  },
  {
    "text": "own implementation of a service that does authentication leveraging the invoice filter and a bunch of advanced",
    "start": "1662919",
    "end": "1672130"
  },
  {
    "text": "features including eventually SSL termination which is not supported in the ingress resource types well it",
    "start": "1672130",
    "end": "1682690"
  },
  {
    "text": "depends on your implementation but for s and I don't think they support it yet",
    "start": "1682690",
    "end": "1688450"
  },
  {
    "text": "unless there's been some recent updates",
    "start": "1688450",
    "end": "1693419"
  },
  {
    "text": "[Music]",
    "start": "1704960",
    "end": "1708109"
  },
  {
    "text": "yeah so since we're actually a more actually",
    "start": "1712279",
    "end": "1719730"
  },
  {
    "text": "part of our solution is providing I am capabilities to our clients so we",
    "start": "1719730",
    "end": "1725370"
  },
  {
    "text": "actually implement multiple authentication strategies and these are all being delegated from ambassador",
    "start": "1725370",
    "end": "1732480"
  },
  {
    "text": "using the invoice filter down to our implementation of logic so we can manage",
    "start": "1732480",
    "end": "1739380"
  },
  {
    "text": "all type of authentication as well as long as it's HTTP based and yeah so open",
    "start": "1739380",
    "end": "1747299"
  },
  {
    "text": "ID connect what one with a simple token sessions these are all",
    "start": "1747299",
    "end": "1754490"
  },
  {
    "text": "[Music]",
    "start": "1757120",
    "end": "1760190"
  },
  {
    "text": "well we are our own identity provider so that's and it depends each customer is",
    "start": "1765770",
    "end": "1773070"
  },
  {
    "text": "different so in some some telco customers we integrate with their own IDP system so it's very very I would say",
    "start": "1773070",
    "end": "1781700"
  },
  {
    "text": "so at the moment our cube clusters are not connected with each other so there are isolated and we deploy on Amazon",
    "start": "1803010",
    "end": "1808919"
  },
  {
    "text": "infrastructure probably I think we mentioned that in the presentation so we're not connecting them with each",
    "start": "1808919",
    "end": "1815970"
  },
  {
    "text": "other we're exploring that in some situations for example we're wondering at the moment we have a catalogue of applications and then what",
    "start": "1815970",
    "end": "1823830"
  },
  {
    "text": "if we just had one catalog so as some kind of global service that we could serve from one region but at the moment",
    "start": "1823830",
    "end": "1829470"
  },
  {
    "text": "everything is a little copy you know a little island and depending on our customers we don't deploy the same",
    "start": "1829470",
    "end": "1835290"
  },
  {
    "text": "services so for example we have some connected micro services to some third party systems so depending on what the",
    "start": "1835290",
    "end": "1842190"
  },
  {
    "text": "partner wants we don't yeah well I just the deployment for storage I don't know",
    "start": "1842190",
    "end": "1847500"
  },
  {
    "text": "what most of our apps are I mean we rely on my sequel MongoDB under the cover parameter says state but it's not the I",
    "start": "1847500",
    "end": "1854610"
  },
  {
    "text": "permit on the state management you know we don't do a lot of stateful applications and all of our data sources",
    "start": "1854610",
    "end": "1860910"
  },
  {
    "text": "are external from kubernetes okay thanks",
    "start": "1860910",
    "end": "1867990"
  },
  {
    "text": "thank you very much [Applause]",
    "start": "1867990",
    "end": "1874880"
  }
]