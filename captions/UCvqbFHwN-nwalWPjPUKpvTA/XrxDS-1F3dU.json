[
  {
    "text": "welcome to six scheduling deep dive my name is aldukukunder i work at google and in this session is",
    "start": "80",
    "end": "7200"
  },
  {
    "text": "also joining me mike dame from red hat",
    "start": "7200",
    "end": "12480"
  },
  {
    "text": "so we're gonna talk a little bit what we do at 6 scheduling and we are going to present you what",
    "start": "14000",
    "end": "19920"
  },
  {
    "text": "we've what we've been working on for the past few releases",
    "start": "19920",
    "end": "25760"
  },
  {
    "text": "uh and this includes the cubescaler and a newer project which is the the scheduler",
    "start": "25760",
    "end": "33119"
  },
  {
    "text": "so six scheduling is the still responsible for the components that make pot",
    "start": "36640",
    "end": "42960"
  },
  {
    "text": "placement decisions that is when you create pods in your cluster",
    "start": "42960",
    "end": "49039"
  },
  {
    "text": "our components are able to detect which nodes are capable of running switchpod",
    "start": "49039",
    "end": "55280"
  },
  {
    "text": "and among all the candidates it can select they can select the best candidate",
    "start": "55280",
    "end": "60320"
  },
  {
    "text": "according to certain criteria so our leads are away",
    "start": "60320",
    "end": "67439"
  },
  {
    "text": "one from ibm and abdullah from google and our main projects as",
    "start": "67439",
    "end": "74560"
  },
  {
    "text": "you might guess one of them is cube scheduler and we have a couple of more projects",
    "start": "74560",
    "end": "81680"
  },
  {
    "text": "which are under active development as well such as the d scheduler which is a component that tries to",
    "start": "81680",
    "end": "89280"
  },
  {
    "text": "rebalance spots or reposition parts according to different criteria",
    "start": "89280",
    "end": "95119"
  },
  {
    "text": "and we have a newer repository which is the scatter plugins",
    "start": "95119",
    "end": "101200"
  },
  {
    "text": "this is an incubation project for new scheduling behaviors that we don't yet want to have in the",
    "start": "101200",
    "end": "107119"
  },
  {
    "text": "main scheduler but we might in the future",
    "start": "107119",
    "end": "112079"
  },
  {
    "text": "these are the features we want to highlight uh for cube scheduler we're gonna jump right in",
    "start": "113680",
    "end": "121600"
  },
  {
    "text": "uh the first of them is scheduling framework this is mostly a refactoring for us as core developers",
    "start": "122719",
    "end": "130720"
  },
  {
    "text": "it facilitates extensibility and also collaboration but for other developers it can",
    "start": "130720",
    "end": "137440"
  },
  {
    "text": "facilitate to create their own customer schedulers",
    "start": "137440",
    "end": "143840"
  },
  {
    "text": "you can you can think uh of different scaling features that are",
    "start": "145360",
    "end": "150400"
  },
  {
    "text": "required to scale a pot and these features are contained in plugins the plugins would implement",
    "start": "150400",
    "end": "158400"
  },
  {
    "text": "different extension points to act on different stages of scheduling as you can see in this",
    "start": "158400",
    "end": "166239"
  },
  {
    "text": "diagram we have several extension points which are grouped in two cycles each",
    "start": "166239",
    "end": "172800"
  },
  {
    "text": "cycle corresponds to a different routine so this scanning framework was developed",
    "start": "172800",
    "end": "180959"
  },
  {
    "text": "during several releases but it was in 117 when we actually",
    "start": "180959",
    "end": "187760"
  },
  {
    "text": "started using it for our core features and in since 118 you could uh",
    "start": "187760",
    "end": "195680"
  },
  {
    "text": "configure it more easily we're going to talk about configuration a little bit later",
    "start": "195680",
    "end": "200879"
  },
  {
    "text": "but in 118 we also have had some recent changes for example we have moved the permit",
    "start": "201280",
    "end": "208560"
  },
  {
    "text": "extension point from the binding cycle to the scheduling cycle and this is to allow",
    "start": "208560",
    "end": "216319"
  },
  {
    "text": "certain permit plugins to avoid um some race conditions we have also merged",
    "start": "216319",
    "end": "224000"
  },
  {
    "text": "the reserve another serve extension points into a single one this is to reduce configuration burden",
    "start": "224000",
    "end": "231120"
  },
  {
    "text": "for cluster operators and the last feature is the new post filter",
    "start": "231120",
    "end": "238400"
  },
  {
    "text": "extension point the post filter extension point it only runs if",
    "start": "238400",
    "end": "243760"
  },
  {
    "text": "the scalar failed to find any suitable candidates any suitable",
    "start": "243760",
    "end": "249280"
  },
  {
    "text": "nodes for your pod and the intention is that the pos filter",
    "start": "249280",
    "end": "255280"
  },
  {
    "text": "plugin would act in the cluster to change change it in some way so that the",
    "start": "255280",
    "end": "261759"
  },
  {
    "text": "the pod can be scheduled um for example well our only default pulse filter",
    "start": "261759",
    "end": "268080"
  },
  {
    "text": "plugin is preemption which is a it was an existing feature for a long",
    "start": "268080",
    "end": "273840"
  },
  {
    "text": "time but we have refactored it into this plugin and what what preemption does is is",
    "start": "273840",
    "end": "280720"
  },
  {
    "text": "very easy fairly simple we just if your pod has higher priority than existing",
    "start": "280720",
    "end": "286080"
  },
  {
    "text": "pods these spots might be removed might be evicted to",
    "start": "286080",
    "end": "291440"
  },
  {
    "text": "make space for this pod that is higher priority but you can",
    "start": "291440",
    "end": "297919"
  },
  {
    "text": "extend or you can create your own field your own plugins to do to implement different",
    "start": "297919",
    "end": "303280"
  },
  {
    "text": "behaviors for the scheduler and here comes again the",
    "start": "303280",
    "end": "309840"
  },
  {
    "text": "scheduling the new scaling plugins repository uh as i said this is an incubation",
    "start": "309840",
    "end": "315280"
  },
  {
    "text": "intuition project where we will incubate plugins before they are",
    "start": "315280",
    "end": "321120"
  },
  {
    "text": "ready for production use",
    "start": "321120",
    "end": "325840"
  },
  {
    "text": "and as i said the uh the the framework is center center in",
    "start": "326560",
    "end": "333199"
  },
  {
    "text": "developers whereas for cluster operators this looks as simple as scheduling",
    "start": "333199",
    "end": "339600"
  },
  {
    "text": "profiles so this is the cluster admin facing api",
    "start": "339600",
    "end": "346240"
  },
  {
    "text": "and what they can do is disable default plugins enable custom plugins and maybe reorder",
    "start": "346240",
    "end": "353360"
  },
  {
    "text": "them this could be important for for filtering",
    "start": "353360",
    "end": "359840"
  },
  {
    "text": "but our most recent feature well not not the reason we introduced it in 118 as alpha",
    "start": "361440",
    "end": "367919"
  },
  {
    "text": "ah the multiple profiles capability this allows a single cubescaler binary",
    "start": "367919",
    "end": "374800"
  },
  {
    "text": "to act as different schedulers so in a single binary you can serve different configurations",
    "start": "374800",
    "end": "382240"
  },
  {
    "text": "which will have different behavior and the pods can select",
    "start": "382240",
    "end": "387280"
  },
  {
    "text": "which profile they want to use by using the scheduler name that is an existing api um quite all from 1.6",
    "start": "387280",
    "end": "397039"
  },
  {
    "text": "or so um and in 119 we have graduated this api to",
    "start": "397039",
    "end": "404160"
  },
  {
    "text": "beta here in the right you can see one such configuration",
    "start": "404160",
    "end": "409440"
  },
  {
    "text": "uh for example in this in this configuration we have two profiles one of them is simply named default",
    "start": "409440",
    "end": "416479"
  },
  {
    "text": "scheduler and it has no configuration meaning that it will run with the default plugins and",
    "start": "416479",
    "end": "424000"
  },
  {
    "text": "as a sample we have put here a no scoring scheduler so basically a",
    "start": "424000",
    "end": "429520"
  },
  {
    "text": "scheduler that does no scoring it will just select the first node it",
    "start": "429520",
    "end": "435039"
  },
  {
    "text": "finds so you can see that we have disabled we",
    "start": "435039",
    "end": "440479"
  },
  {
    "text": "have used star here to disable all the prescore and score plugins",
    "start": "440479",
    "end": "450000"
  },
  {
    "text": "topology aware pod spreading is a new scheduling feature that we implemented as plugging",
    "start": "450000",
    "end": "455759"
  },
  {
    "text": "and it as part of the core cube scheduler",
    "start": "455759",
    "end": "461199"
  },
  {
    "text": "basically when you have different failure domains such as zones",
    "start": "461199",
    "end": "466240"
  },
  {
    "text": "or a node or maybe if you're running on-prem you might have your rack",
    "start": "466240",
    "end": "471280"
  },
  {
    "text": "or your switch basically this this feature allows you",
    "start": "471280",
    "end": "477199"
  },
  {
    "text": "to spread your pods among all those domains such that you have you can guarantee",
    "start": "477199",
    "end": "483599"
  },
  {
    "text": "higher availability the constraints you can specify could be hard",
    "start": "483599",
    "end": "489280"
  },
  {
    "text": "meaning that if the scheduler can satisfy them the the post won't be scheduled so then",
    "start": "489280",
    "end": "496560"
  },
  {
    "text": "a preemption could happen or maybe if you have cluster of the scalar you could have an auto scale",
    "start": "496560",
    "end": "503440"
  },
  {
    "text": "or a scale up uh or it could be soft meaning that scalar will try to do its best to",
    "start": "503440",
    "end": "510160"
  },
  {
    "text": "satisfy this criteria but if if it can it will still place the pot",
    "start": "510160",
    "end": "515460"
  },
  {
    "text": "[Music] but you might not have the the best balance",
    "start": "515460",
    "end": "521599"
  },
  {
    "text": "so and one new feature we introduced in 118 is that you could also set a default",
    "start": "521599",
    "end": "528240"
  },
  {
    "text": "well default cluster level uh default uh which will be applied to",
    "start": "528240",
    "end": "536640"
  },
  {
    "text": "all deposits belong to a service or a replica set such that you can have some kind of",
    "start": "536640",
    "end": "542080"
  },
  {
    "text": "automatic spreading and in in 1819 we have",
    "start": "542080",
    "end": "548160"
  },
  {
    "text": "graduated this to to ga and we've been working in 119 in in getting",
    "start": "548160",
    "end": "555279"
  },
  {
    "text": "this score really influenced scheduling so so that you can actually uh",
    "start": "555279",
    "end": "563120"
  },
  {
    "text": "with with your soft constraints you can actually have very good spreading",
    "start": "563120",
    "end": "568320"
  },
  {
    "text": "uh but if you don't want such such strong spreading you can also use the max skew",
    "start": "568320",
    "end": "574800"
  },
  {
    "text": "field to control this this strength here on the right you can see one one",
    "start": "574800",
    "end": "581279"
  },
  {
    "text": "pod spec which uses this new feature you can see that we have one one",
    "start": "581279",
    "end": "589120"
  },
  {
    "text": "constraint with a max key of two uh this this constraint is defined for",
    "start": "589120",
    "end": "594640"
  },
  {
    "text": "zones and it's a hard constraint uh you can see do not schedule if you cannot satisfy the",
    "start": "594640",
    "end": "600560"
  },
  {
    "text": "condition and as any as most as several other um apis you can define a",
    "start": "600560",
    "end": "608240"
  },
  {
    "text": "label selector to to detect which pots are affected by by this constraint",
    "start": "608240",
    "end": "619839"
  },
  {
    "text": "and this is some work we do continuously we are always working",
    "start": "624320",
    "end": "631200"
  },
  {
    "text": "on getting our algorithms to to run faster to use less resources",
    "start": "631200",
    "end": "639600"
  },
  {
    "text": "less api calls etc such that we can continue to meet our latency goals",
    "start": "639600",
    "end": "647600"
  },
  {
    "text": "in 117 we focused in vanilla workloads that is",
    "start": "647600",
    "end": "653680"
  },
  {
    "text": "workloads that then don't use any special specialized scaling features such as affinity or",
    "start": "653680",
    "end": "661120"
  },
  {
    "text": "or the new spot spreading api uh so these workloads in this workloads",
    "start": "661120",
    "end": "667279"
  },
  {
    "text": "we obtain a 2.5 x latency improvement and",
    "start": "667279",
    "end": "672560"
  },
  {
    "text": "this improvement is actually what uh allowed us at google to achieve 100 watts per",
    "start": "672560",
    "end": "678959"
  },
  {
    "text": "second in 15k nodes um or clusters with 15 000 nodes",
    "start": "678959",
    "end": "688480"
  },
  {
    "text": "um we have also improved we also improved the latency of specialized features such as",
    "start": "688480",
    "end": "694320"
  },
  {
    "text": "port affinity here we actually obtain a very big boost",
    "start": "694320",
    "end": "699519"
  },
  {
    "text": "we achieve 24 x improvement for for preferred affinity and",
    "start": "699519",
    "end": "705600"
  },
  {
    "text": "seven times for required affinity in 118 and 119 we focused on",
    "start": "705600",
    "end": "712399"
  },
  {
    "text": "again these specialized features we even improved affinity further we",
    "start": "712399",
    "end": "718399"
  },
  {
    "text": "achieved two two times improvement in latency and as we were preparing to graduate",
    "start": "718399",
    "end": "725200"
  },
  {
    "text": "what what the ball just played into ga we also work on on its latency and we also obtain a 2x",
    "start": "725200",
    "end": "733040"
  },
  {
    "text": "improvement and now this this spreading this",
    "start": "733040",
    "end": "739040"
  },
  {
    "text": "feature is if you're familiar with the legacy selector spread",
    "start": "739040",
    "end": "744720"
  },
  {
    "text": "priority this new plugin is equivalent in in speed",
    "start": "744959",
    "end": "751839"
  },
  {
    "text": "but as i said earlier it has much stronger spreading guarantees",
    "start": "751839",
    "end": "760320"
  },
  {
    "text": "uh in in 118 and beyond we want to focus on preemption we want",
    "start": "760320",
    "end": "766480"
  },
  {
    "text": "to make it faster to uh to either preempt faster",
    "start": "766480",
    "end": "772560"
  },
  {
    "text": "or skip it if we already know that preemption is not helping",
    "start": "772560",
    "end": "778880"
  },
  {
    "text": "and we also want to focus on the effect of unscalable parts once your cluster has a lot of",
    "start": "779120",
    "end": "785680"
  },
  {
    "text": "unscalable pots it can affect the latency of the scheduler so we want to really solve that",
    "start": "785680",
    "end": "794639"
  },
  {
    "text": "next uh mike is gonna present what's new in the scheduler",
    "start": "794720",
    "end": "801839"
  },
  {
    "text": "mike all right thanks ada my name is mike",
    "start": "801839",
    "end": "809920"
  },
  {
    "text": "game i work a lot with the scheduling sig and i'm also one of the currently active",
    "start": "809920",
    "end": "816800"
  },
  {
    "text": "contributors to the descheduler project if you're not familiar with the descheduler it's actually a sub project",
    "start": "816800",
    "end": "823920"
  },
  {
    "text": "that's been around for a couple of years but it's recently started to gain a lot more attraction",
    "start": "823920",
    "end": "829279"
  },
  {
    "text": "and a lot of interest from new contributors and people that are finding that it works for their use",
    "start": "829279",
    "end": "836160"
  },
  {
    "text": "cases and basically what it does is the problem that you have with the",
    "start": "836160",
    "end": "842240"
  },
  {
    "text": "scheduler is that it will take a new pod and put it onto the cluster based on",
    "start": "842240",
    "end": "847360"
  },
  {
    "text": "the series of defined plug-ins but once the pod is scheduled nothing",
    "start": "847360",
    "end": "853920"
  },
  {
    "text": "happens to counteract for that in case the state of your cluster changes so",
    "start": "853920",
    "end": "859760"
  },
  {
    "text": "say if a pod gets scheduled onto a node and then later on that node",
    "start": "859760",
    "end": "864800"
  },
  {
    "text": "gets some sort of taint or a label gets applied to it that doesn't",
    "start": "864800",
    "end": "870079"
  },
  {
    "text": "match the affinity that the pod originally had nothing will happen with that but the descheduler",
    "start": "870079",
    "end": "875760"
  },
  {
    "text": "is a tool that will run periodically either as a job or it can",
    "start": "875760",
    "end": "882639"
  },
  {
    "text": "run in a control loop and it'll periodically look at the state of the cluster and look at the pods and make",
    "start": "882639",
    "end": "889279"
  },
  {
    "text": "sure that they match what they should and if they don't it'll evict those pods so",
    "start": "889279",
    "end": "894880"
  },
  {
    "text": "say if the pod is on a node like the example that i gave and a new label",
    "start": "894880",
    "end": "901120"
  },
  {
    "text": "gets applied to that node that doesn't match affinity the d scheduler will see that",
    "start": "901120",
    "end": "906160"
  },
  {
    "text": "and evicted so we've had a lot of improvements on it",
    "start": "906160",
    "end": "911279"
  },
  {
    "text": "recently just trying to make it more reliable and accessible to new users um anyone that's",
    "start": "911279",
    "end": "918399"
  },
  {
    "text": "interested in contributing to it is welcome to but i'm gonna go over a couple of the changes that we've done so far",
    "start": "918399",
    "end": "925519"
  },
  {
    "text": "in 118 and 119. so the first thing that we did was we have now",
    "start": "925519",
    "end": "932800"
  },
  {
    "text": "decided on aligning the deschedulers releases with upstream's releases so that means that",
    "start": "932800",
    "end": "938639"
  },
  {
    "text": "we're going to be tagging release branches along with the upstream",
    "start": "938639",
    "end": "943680"
  },
  {
    "text": "release schedule so this means that you'll know what version of",
    "start": "943680",
    "end": "948720"
  },
  {
    "text": "kubernetes the descheduler is compatible with if you're running it and it also helps us track the uh",
    "start": "948720",
    "end": "956800"
  },
  {
    "text": "the past three versions of kubernetes that we want to support with it so you can know that if you're running the 119",
    "start": "956800",
    "end": "964480"
  },
  {
    "text": "based version of the descheduler that should work with 118 117 and 116 as well so that",
    "start": "964480",
    "end": "971920"
  },
  {
    "text": "is what we're able to track a lot easier with these matching tags and it also",
    "start": "971920",
    "end": "977839"
  },
  {
    "text": "gives us a more regular release cycle to make sure that we're updating to the upstream dependencies making sure",
    "start": "977839",
    "end": "984000"
  },
  {
    "text": "that everything is compatible and that you can grab the latest descheduler and run it in the",
    "start": "984000",
    "end": "989040"
  },
  {
    "text": "latest kubernetes and it'll work another part of that is that we've published some production",
    "start": "989040",
    "end": "996079"
  },
  {
    "text": "containers for anyone to consume you don't need to build your own d",
    "start": "996079",
    "end": "1001440"
  },
  {
    "text": "scheduler now if you just want to use the default that's available these are published on gcr to io",
    "start": "1001440",
    "end": "1008480"
  },
  {
    "text": "and they'll be tagged with the matching release numbers so you can grab this and run it in your cluster",
    "start": "1008480",
    "end": "1015519"
  },
  {
    "text": "without having to build the descheduler yourself which makes it a lot easier to run",
    "start": "1015519",
    "end": "1022560"
  },
  {
    "text": "so the next thing that we did was we've added a couple new strategies the full list of",
    "start": "1022839",
    "end": "1028000"
  },
  {
    "text": "available strategies for the descheduler is on the readme for the repo",
    "start": "1028000",
    "end": "1033520"
  },
  {
    "text": "i've mentioned a couple of them so far such as no uh node affinity pod affinity there's strategies that",
    "start": "1033520",
    "end": "1041199"
  },
  {
    "text": "will remove uh if you have duplicates of pods running on a node and you want to spread",
    "start": "1041199",
    "end": "1048160"
  },
  {
    "text": "those out more it'll try to evict some of those duplicates and hopefully they get rescheduled onto a different pod so it keeps your",
    "start": "1048160",
    "end": "1056480"
  },
  {
    "text": "availability up but the new things that we added in 118 and 119 are these three strategies here one will",
    "start": "1056480",
    "end": "1063520"
  },
  {
    "text": "remove pods that have too many restarts so this can be helpful if you have pods",
    "start": "1063520",
    "end": "1069280"
  },
  {
    "text": "that are constantly crash looping for some reason and you want to get in there and evict those pods maybe",
    "start": "1069280",
    "end": "1075280"
  },
  {
    "text": "to kick them onto a new node that has better availability for them maybe",
    "start": "1075280",
    "end": "1082240"
  },
  {
    "text": "they're crash looping because they're having trouble connecting to some mounted network volume on one node",
    "start": "1082240",
    "end": "1088480"
  },
  {
    "text": "but once it gets evicted it goes on to the new node and then it works for it that's what this job does and that takes",
    "start": "1088480",
    "end": "1096000"
  },
  {
    "text": "two parameters the threshold of how many restarts you want to consider for eviction and also whether",
    "start": "1096000",
    "end": "1103600"
  },
  {
    "text": "or not the init container restarts should be factored into that the next strategy",
    "start": "1103600",
    "end": "1110160"
  },
  {
    "text": "that we added is a pod lifetime strategy which just removes pods that are",
    "start": "1110160",
    "end": "1115360"
  },
  {
    "text": "over a certain a number of seconds old so that's pretty self-explanatory if you have long running pods",
    "start": "1115360",
    "end": "1121600"
  },
  {
    "text": "and you want to evict those in favor of newer pods try to get new pods into the",
    "start": "1121600",
    "end": "1127280"
  },
  {
    "text": "system that is what that is for the third one that at the time at this time is in",
    "start": "1127280",
    "end": "1134320"
  },
  {
    "text": "progress what we're targeting is for the 119 release cycle is a topology spread strategy which",
    "start": "1134320",
    "end": "1140400"
  },
  {
    "text": "matches the topology spreading plug-in that aldo mentioned and looks at",
    "start": "1140400",
    "end": "1147600"
  },
  {
    "text": "if your topology becomes uneven at some point while the pods are running it's going to",
    "start": "1147600",
    "end": "1152799"
  },
  {
    "text": "go through and try to work on or try to affect those pods that will hopefully be rebalanced",
    "start": "1152799",
    "end": "1158559"
  },
  {
    "text": "in the way that you want with your strategy so important thing to note about the",
    "start": "1158559",
    "end": "1164400"
  },
  {
    "text": "descheduler is that it doesn't look at it doesn't try to reschedule any of the pots",
    "start": "1164400",
    "end": "1170640"
  },
  {
    "text": "what it's doing is strictly evicting pots that don't match what they should and then",
    "start": "1170640",
    "end": "1176880"
  },
  {
    "text": "it's up to the scheduler and the those pods owning controllers to recreate those pods and",
    "start": "1176880",
    "end": "1182720"
  },
  {
    "text": "it's the it's optimistic that the scheduler will put them back where they need to go so having these",
    "start": "1182720",
    "end": "1189280"
  },
  {
    "text": "matching strategies is very helpful for keeping a cluster balanced",
    "start": "1189280",
    "end": "1194559"
  },
  {
    "text": "actively and i'll come back to that in a minute with something else that we were talking about for",
    "start": "1194559",
    "end": "1200080"
  },
  {
    "text": "future improvements with the d scheduler but with these like i'm like it says",
    "start": "1200080",
    "end": "1205679"
  },
  {
    "text": "here we are always looking for new ideas and new strategies that can be added to the d scheduler",
    "start": "1205679",
    "end": "1211760"
  },
  {
    "text": "like the scheduler there's infinite number of ways that you can imagine that",
    "start": "1211760",
    "end": "1217280"
  },
  {
    "text": "you might want to come up with a strategy for how your cluster should be balanced or how that could be violated and",
    "start": "1217280",
    "end": "1224400"
  },
  {
    "text": "what you want to do to address that so any new ideas with use cases for them",
    "start": "1224400",
    "end": "1230960"
  },
  {
    "text": "are welcome in the repo and any new contributors also very helpful",
    "start": "1230960",
    "end": "1237760"
  },
  {
    "text": "so along after those changes we have a couple miscellaneous improvements to the repo",
    "start": "1238880",
    "end": "1245120"
  },
  {
    "text": "one of those is that we just finalized we're going to be publishing a helm chart",
    "start": "1245120",
    "end": "1250559"
  },
  {
    "text": "automatically with our release so this will be easier for people who are consuming home charts to publish or",
    "start": "1250559",
    "end": "1257280"
  },
  {
    "text": "to run the d scheduler they will be able to grab those from our github repo would be published with",
    "start": "1257280",
    "end": "1263919"
  },
  {
    "text": "github pages make it a little easier to deploy another thing that we're adding",
    "start": "1263919",
    "end": "1269120"
  },
  {
    "text": "is the ability to run the descheduler selectively on pods based on their",
    "start": "1269120",
    "end": "1274960"
  },
  {
    "text": "labels or on specific name spaces is something that we've had people ask for",
    "start": "1274960",
    "end": "1280240"
  },
  {
    "text": "i just want to run the descheduler to keep this one project balanced instead of running it having it affect",
    "start": "1280240",
    "end": "1285520"
  },
  {
    "text": "my whole cluster that's what these will provide for you and",
    "start": "1285520",
    "end": "1291760"
  },
  {
    "text": "another thing that we're looking at in the future is being able to control what priority",
    "start": "1291760",
    "end": "1298640"
  },
  {
    "text": "uh based on a priority class threshold what pods are eligible for eviction too",
    "start": "1298640",
    "end": "1303679"
  },
  {
    "text": "so if you want to exclude a certain number of pods based on your",
    "start": "1303679",
    "end": "1309200"
  },
  {
    "text": "custom defined priority class you'll be able to do that in the future this is something that we're working on in",
    "start": "1309200",
    "end": "1315120"
  },
  {
    "text": "progress currently it only excludes pods that are",
    "start": "1315120",
    "end": "1320880"
  },
  {
    "text": "system critical based on priority class there are various number ways that you can also exclude pods but",
    "start": "1320880",
    "end": "1327760"
  },
  {
    "text": "these are just some of the other ways that we're making it configurable we also updated the descheduler to be",
    "start": "1327760",
    "end": "1333679"
  },
  {
    "text": "based on go 114 that's with keeping in line with upstream and 119.",
    "start": "1333679",
    "end": "1339039"
  },
  {
    "text": "we added some issue templates to github so that new users can more effectively",
    "start": "1339039",
    "end": "1345679"
  },
  {
    "text": "propose issues raise up bugs and we can communicate better with how to resolve those",
    "start": "1345679",
    "end": "1351760"
  },
  {
    "text": "and then we just had some various quality of life improvements to the code some refactors one of those being that",
    "start": "1351760",
    "end": "1358960"
  },
  {
    "text": "we've improved the logging we've improved the ability to see why pods were evicted in the logs",
    "start": "1358960",
    "end": "1366480"
  },
  {
    "text": "as well as raising events when pods are evicted that specifically say it was evicted because it",
    "start": "1366480",
    "end": "1372480"
  },
  {
    "text": "violated the pod anti-affinity of the node for whatever reason happens to be and",
    "start": "1372480",
    "end": "1379919"
  },
  {
    "text": "various other code refactors that are too small to mention here but definitely",
    "start": "1379919",
    "end": "1386559"
  },
  {
    "text": "improve the readability and the ability for people to contribute to the code",
    "start": "1386559",
    "end": "1392320"
  },
  {
    "text": "looking ahead we've got some plans that we plan to contribute or continue improving",
    "start": "1392320",
    "end": "1398480"
  },
  {
    "text": "the code base one of those is we're looking at our api to configure the",
    "start": "1398480",
    "end": "1404320"
  },
  {
    "text": "deschedulers strategies we want to make that a little easier to",
    "start": "1404320",
    "end": "1409760"
  },
  {
    "text": "configure and make it a little more extendable for users to add in new strategies",
    "start": "1409760",
    "end": "1415120"
  },
  {
    "text": "another big thing that we get asked a lot that we've seen a lot of people requesting is the ability for it to be",
    "start": "1415120",
    "end": "1421440"
  },
  {
    "text": "based directly on the scheduler's plugins and use that",
    "start": "1421440",
    "end": "1427039"
  },
  {
    "text": "in use that code to be able to make its descheduling decisions and that's something that we're definitely looking at and",
    "start": "1427039",
    "end": "1433440"
  },
  {
    "text": "appreciate any help that people will be willing to join on one of the big",
    "start": "1433440",
    "end": "1439039"
  },
  {
    "text": "blockers for that right now is the ability to easily import",
    "start": "1439039",
    "end": "1444320"
  },
  {
    "text": "the scheduler plug-in code it's pretty tied in to the rest of the",
    "start": "1444320",
    "end": "1450720"
  },
  {
    "text": "upstream core kubernetes code base so part of the task that we're taking on",
    "start": "1450720",
    "end": "1456799"
  },
  {
    "text": "is trying to go through where the plugins are tied in and where we can break those dependencies so that",
    "start": "1456799",
    "end": "1462000"
  },
  {
    "text": "you can more easily import just the packages that you need to get certain plugins or all of the",
    "start": "1462000",
    "end": "1468320"
  },
  {
    "text": "plugins without pulling in the rest of kate's i o slash kubernetes and that's",
    "start": "1468320",
    "end": "1473840"
  },
  {
    "text": "something that we're working on and there's issues in uh six scheduling upstream that you can find",
    "start": "1473840",
    "end": "1479039"
  },
  {
    "text": "and help out with that if you'd like but those are all of the main",
    "start": "1479039",
    "end": "1485039"
  },
  {
    "text": "improvements that i had for the scheduler um yeah",
    "start": "1485039",
    "end": "1490559"
  },
  {
    "text": "i don't know uh that's all i had aldo did you have anything else that's all from my side as",
    "start": "1490559",
    "end": "1498840"
  },
  {
    "text": "well thank you for joining this session",
    "start": "1498840",
    "end": "1504559"
  },
  {
    "text": "yep okay um so we're now live in the q a",
    "start": "1512840",
    "end": "1520799"
  },
  {
    "text": "um we have a few questions here let me take the first one",
    "start": "1520799",
    "end": "1526158"
  },
  {
    "text": "um there is uh was there wasn't there a plan to",
    "start": "1526840",
    "end": "1532080"
  },
  {
    "text": "implement affinity policies that are enforced during execution for example required during scheduling",
    "start": "1532080",
    "end": "1539440"
  },
  {
    "text": "required during execution is the descaler a substitute for that",
    "start": "1539440",
    "end": "1545039"
  },
  {
    "text": "um so let me answer first from the q scatter perspective",
    "start": "1545039",
    "end": "1550240"
  },
  {
    "text": "um in general yeah this uh this option has been",
    "start": "1550240",
    "end": "1557120"
  },
  {
    "text": "discussed in the past and uh there is there isn't",
    "start": "1557120",
    "end": "1563039"
  },
  {
    "text": "immediate pressure for it so we are not not uh doing it in in the next release",
    "start": "1563039",
    "end": "1570080"
  },
  {
    "text": "or unless there are contributors that come in um",
    "start": "1570080",
    "end": "1576080"
  },
  {
    "text": "but but this uh there is a problem here because such such feature would require",
    "start": "1576080",
    "end": "1582880"
  },
  {
    "text": "that the cube scheduler starts watching running pots which it doesn't today",
    "start": "1582880",
    "end": "1590400"
  },
  {
    "text": "but but the api is not just about the scheduler it can be enforced by other components",
    "start": "1590400",
    "end": "1597520"
  },
  {
    "text": "for example we have we have eviction that is done from from the cubelet as well for tolerations",
    "start": "1597520",
    "end": "1605039"
  },
  {
    "text": "for example um so it is possible that that we could uh implement this feature",
    "start": "1605039",
    "end": "1612000"
  },
  {
    "text": "through the scheduler uh once it becomes more uh",
    "start": "1612000",
    "end": "1617360"
  },
  {
    "text": "um of a main project in in kubernetes i don't know if um mikey went to what",
    "start": "1617600",
    "end": "1624240"
  },
  {
    "text": "something yeah um pretty much exactly what you said it's come up a couple times before and i",
    "start": "1624240",
    "end": "1630480"
  },
  {
    "text": "think in the past when i have seen it come up people tend to point to the d scheduler for it uh i think that if you run the d",
    "start": "1630480",
    "end": "1637360"
  },
  {
    "text": "scheduler you'll basically get that behavior right now if you're looking for the required during scheduling required during execution but",
    "start": "1637360",
    "end": "1644000"
  },
  {
    "text": "it's not like i said in the presentation the scheduler isn't watching pods all the",
    "start": "1644000",
    "end": "1649120"
  },
  {
    "text": "time it's running on whatever interval you're setting so sort of like how the scheduler",
    "start": "1649120",
    "end": "1657200"
  },
  {
    "text": "would need to start watching pods all the time to get this perfectly the d scheduler will probably get you",
    "start": "1657200",
    "end": "1663039"
  },
  {
    "text": "close enough if you're looking for that behavior",
    "start": "1663039",
    "end": "1669840"
  },
  {
    "text": "let's go to the next one what do you think about potentially integrating the scaling into the scheduler",
    "start": "1670480",
    "end": "1677840"
  },
  {
    "text": "um i don't know if this has been discussed before but basically i get this as putting it",
    "start": "1677840",
    "end": "1684559"
  },
  {
    "text": "as a separate routine instead of a separate binary mike yeah",
    "start": "1684559",
    "end": "1690880"
  },
  {
    "text": "i've heard people ask about it before it i think it goes back again to look just",
    "start": "1690880",
    "end": "1697200"
  },
  {
    "text": "like what we were just saying about you know if the scheduler was going to be watching pods it's kind of taking the opposite",
    "start": "1697200",
    "end": "1704000"
  },
  {
    "text": "behavior of the schedule so it would have to run as like a separate routine or um",
    "start": "1704000",
    "end": "1710080"
  },
  {
    "text": "maybe be a separate binary but it depends on how you want to define integrating it into the scheduler i",
    "start": "1710080",
    "end": "1716720"
  },
  {
    "text": "don't think right now there's too much looking at it and we're trying to get some of the",
    "start": "1716720",
    "end": "1723919"
  },
  {
    "text": "scheduler actually integrated into the d scheduler by getting those plug-ins and making them",
    "start": "1723919",
    "end": "1729840"
  },
  {
    "text": "available to be imported and use the d scheduling strategy so in that way the d scheduler is kind of a",
    "start": "1729840",
    "end": "1734880"
  },
  {
    "text": "weird implementation of a scheduler in a way on its own so i don't know if",
    "start": "1734880",
    "end": "1742000"
  },
  {
    "text": "we're really going to be looking at moving d scheduling into the scheduler anytime soon but if there is enough interest for that",
    "start": "1742000",
    "end": "1750000"
  },
  {
    "text": "coming up i would not be against i don't have any reason to be against it but i just don't think",
    "start": "1750000",
    "end": "1756159"
  },
  {
    "text": "many people have pushed too hard for it yet",
    "start": "1756159",
    "end": "1760320"
  },
  {
    "text": "but but on the other hand um is it's also good to have some",
    "start": "1761679",
    "end": "1767279"
  },
  {
    "text": "separation so for you for more opportunities of failover um this kind of goes with the next uh",
    "start": "1767279",
    "end": "1777200"
  },
  {
    "text": "question what was the reason behind starting a different project for descaling instead of starting off by studying the",
    "start": "1777200",
    "end": "1783039"
  },
  {
    "text": "scheduler well there is first of all the uh the ability to",
    "start": "1783039",
    "end": "1789440"
  },
  {
    "text": "iterate faster if you have a separate project um you can incubate you can put it in a",
    "start": "1789440",
    "end": "1795360"
  },
  {
    "text": "separate um it's a repository and people can contribute",
    "start": "1795360",
    "end": "1800480"
  },
  {
    "text": "uh faster because the scatter is old you don't wanna you don't want to break it um",
    "start": "1800480",
    "end": "1807200"
  },
  {
    "text": "you don't want to break people's clusters as they update so it's safer to to start",
    "start": "1807200",
    "end": "1813679"
  },
  {
    "text": "a separate project and and even better for this kind of",
    "start": "1813679",
    "end": "1819039"
  },
  {
    "text": "thing where you have a separate entirely separate routine that needs to start",
    "start": "1819039",
    "end": "1824640"
  },
  {
    "text": "um so it was safer in that sense",
    "start": "1824640",
    "end": "1830159"
  },
  {
    "text": "um anything but yeah and tagging on to that the uh like you said that people can contribute",
    "start": "1830559",
    "end": "1838159"
  },
  {
    "text": "a lot faster to the d scheduler as a separate project we're not so strictly tied to",
    "start": "1838159",
    "end": "1844240"
  },
  {
    "text": "kubernetes release cycles even though we try to keep them lined up you know code freezes",
    "start": "1844240",
    "end": "1849919"
  },
  {
    "text": "and stuff we can be a little looser on and just have a faster",
    "start": "1849919",
    "end": "1855520"
  },
  {
    "text": "iteration that people can contribute to",
    "start": "1855520",
    "end": "1859919"
  },
  {
    "text": "yeah as the project progresses uh it could actually um be more more in line with",
    "start": "1861919",
    "end": "1869840"
  },
  {
    "text": "um with stable releases uh but but it could always stay as a",
    "start": "1869840",
    "end": "1876159"
  },
  {
    "text": "separate project just like the cluster of descaler for example",
    "start": "1876159",
    "end": "1881120"
  },
  {
    "text": "this is another question thanks for updates what do you think is the most exciting",
    "start": "1884640",
    "end": "1891440"
  },
  {
    "text": "challenging problem for granted scheduling today",
    "start": "1891440",
    "end": "1896480"
  },
  {
    "text": "um i'm gonna say basically two things one is scalability uh",
    "start": "1896480",
    "end": "1906080"
  },
  {
    "text": "is a lot of people in the community asking for",
    "start": "1906080",
    "end": "1911278"
  },
  {
    "text": "um for some contention i mentioned in the talk um this problem of unscheduled pots that affect",
    "start": "1911919",
    "end": "1919039"
  },
  {
    "text": "scheduling so that's one uh and it's hard because there are so many",
    "start": "1919039",
    "end": "1924799"
  },
  {
    "text": "events uh happening in the the cluster the bots both removed or pots terminated and",
    "start": "1924799",
    "end": "1933360"
  },
  {
    "text": "or nodes new nodes or remove nodes so it's not easy to come up with a",
    "start": "1933360",
    "end": "1940640"
  },
  {
    "text": "with a strategy to re retry pods that are pending",
    "start": "1940640",
    "end": "1946000"
  },
  {
    "text": "uh and another interesting problem is um batch scheduling",
    "start": "1946000",
    "end": "1952399"
  },
  {
    "text": "um basically scaling multiple parts at the same time",
    "start": "1952399",
    "end": "1959120"
  },
  {
    "text": "because they have a tight relationship um so that one can only start without",
    "start": "1959120",
    "end": "1965120"
  },
  {
    "text": "the other and that is hard because the scalar today looks at a single part at a time",
    "start": "1965120",
    "end": "1971200"
  },
  {
    "text": "uh so we are that's a problem some contributors are looking at and and they",
    "start": "1971200",
    "end": "1977519"
  },
  {
    "text": "are already uh experimenting with plugins in a separate in the in the scheduling plugins repository and",
    "start": "1977519",
    "end": "1985039"
  },
  {
    "text": "um they are iterating there as well yeah batch scheduling is definitely one",
    "start": "1985039",
    "end": "1991679"
  },
  {
    "text": "of the um bigger ones that we've been trying to crack for a while and the scheduler",
    "start": "1991679",
    "end": "1997760"
  },
  {
    "text": "plug-in framework has let people more people try to take out that challenge",
    "start": "1997760",
    "end": "2003279"
  },
  {
    "text": "i know topology aware scheduling is also another thing that we have the default topology",
    "start": "2003279",
    "end": "2009840"
  },
  {
    "text": "spreading plug-in that you talked about in the presentation but i know that there's a lot of people trying to work",
    "start": "2009840",
    "end": "2015440"
  },
  {
    "text": "on their own solutions for that too i think that there's an entirely separate topology",
    "start": "2015440",
    "end": "2020559"
  },
  {
    "text": "aware scheduling channel in the gate slack where people are working on",
    "start": "2020559",
    "end": "2026159"
  },
  {
    "text": "really interesting solutions to it and i'd pop in there every now and then to see what's going on but",
    "start": "2026159",
    "end": "2032240"
  },
  {
    "text": "it gets a little over my head yeah let's go to the next one",
    "start": "2032240",
    "end": "2040720"
  },
  {
    "text": "is there a memory type aware scaling available example kubernetes knows with dram",
    "start": "2040720",
    "end": "2046640"
  },
  {
    "text": "versus nodes with p p m e m ram sram etc",
    "start": "2046640",
    "end": "2052560"
  },
  {
    "text": "uh not that i'm aware of uh there is some discussion about node",
    "start": "2052560",
    "end": "2058839"
  },
  {
    "text": "topology um so if you have a",
    "start": "2058839",
    "end": "2064638"
  },
  {
    "text": "numa nodes there is that discussion that is already started uh and i imagine this problem",
    "start": "2064879",
    "end": "2071280"
  },
  {
    "text": "would be kind of similar but i don't i haven't seen any discussions around this",
    "start": "2071280",
    "end": "2077039"
  },
  {
    "text": "yeah i would say the same thing it's pretty much uh pneumo where scheduling has been uh some probably the closest that people",
    "start": "2077679",
    "end": "2084320"
  },
  {
    "text": "have talked to about that but i'm not very much beyond that",
    "start": "2084320",
    "end": "2091440"
  },
  {
    "text": "i want to leave the next question to you like yeah the next one um saying uh",
    "start": "2093919",
    "end": "2100800"
  },
  {
    "text": "there's the weird problem of trying to pull in scheduler code to have better default strategies",
    "start": "2100800",
    "end": "2106160"
  },
  {
    "text": "in the descheduler was this a consideration that was made during the start i wasn't actually around for the very",
    "start": "2106160",
    "end": "2113280"
  },
  {
    "text": "start of the descheduler but it's been something that was being",
    "start": "2113280",
    "end": "2118400"
  },
  {
    "text": "considered for a while and especially now that the framework has become the main way that the scheduler",
    "start": "2118400",
    "end": "2125119"
  },
  {
    "text": "configures its plugins so really once we got the switch to the scheduler framework with plugins",
    "start": "2125119",
    "end": "2131200"
  },
  {
    "text": "the consideration of bringing those plugins into the scheduler started to come up",
    "start": "2131200",
    "end": "2137520"
  },
  {
    "text": "right away so we have been thinking about that for a while but it wasn't a main driving force",
    "start": "2137520",
    "end": "2144640"
  },
  {
    "text": "of developing the framework um as far as i'm aware although but it's something that",
    "start": "2144640",
    "end": "2150560"
  },
  {
    "text": "we want to make easily and portable now so that's just a",
    "start": "2150560",
    "end": "2156160"
  },
  {
    "text": "nice side effect and now we are kind of using that to drive breaking out these plug-ins we have the",
    "start": "2156160",
    "end": "2161599"
  },
  {
    "text": "scheduler plug-ins separate repo that i think we're going to try to move some plug-ins to",
    "start": "2161599",
    "end": "2166960"
  },
  {
    "text": "eventually or at least it's an incubating repo for new plug-ins now",
    "start": "2166960",
    "end": "2172240"
  },
  {
    "text": "something that word that we've considered thought about",
    "start": "2172560",
    "end": "2176960"
  },
  {
    "text": "and as we mentioned in the talk there is also a huge drive to clean up the",
    "start": "2179040",
    "end": "2186960"
  },
  {
    "text": "dependencies basically try to bring the core",
    "start": "2186960",
    "end": "2193359"
  },
  {
    "text": "part of the algorithms into separate repository that is easier to to",
    "start": "2193359",
    "end": "2200560"
  },
  {
    "text": "import from other projects so that this this is less of a problem",
    "start": "2200560",
    "end": "2214960"
  },
  {
    "text": "there are no more questions i know it looks like that's it",
    "start": "2214960",
    "end": "2224078"
  },
  {
    "text": "all right how much time do we have left yeah yeah",
    "start": "2227119",
    "end": "2234240"
  },
  {
    "text": "who's a uh looks like we're actually just about a little over our time",
    "start": "2235119",
    "end": "2241839"
  },
  {
    "text": "actually um okay so so we are also just luck",
    "start": "2241839",
    "end": "2249200"
  },
  {
    "text": "yep if you have any extra questions you can post them there and we are going",
    "start": "2249200",
    "end": "2254240"
  },
  {
    "text": "to answer if we're in the 2 q con maintainer",
    "start": "2254240",
    "end": "2260960"
  },
  {
    "text": "track slack channel for cncf and then also obviously sig scheduling regular",
    "start": "2260960",
    "end": "2267359"
  },
  {
    "text": "kate slack so thank you um",
    "start": "2267359",
    "end": "2276560"
  },
  {
    "text": "this was great i hope it was informative and uh yeah thanks for coming",
    "start": "2276560",
    "end": "2285838"
  }
]