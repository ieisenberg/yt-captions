[
  {
    "text": "thank you everyone for attending this session on us explaining you know how um",
    "start": "199",
    "end": "6200"
  },
  {
    "text": "Cloudera build uh really scalable and robust AI inferencing platform we're",
    "start": "6200",
    "end": "12360"
  },
  {
    "text": "glad that you can make it and um hopefully uh you take away uh some",
    "start": "12360",
    "end": "17920"
  },
  {
    "text": "tidbits of information uh that can help you in your own",
    "start": "17920",
    "end": "23279"
  },
  {
    "text": "Journeys so my name is sang I'm a principal engineer at uh Cloud hi my name is Peter",
    "start": "23359",
    "end": "30800"
  },
  {
    "text": "uh I'm a director of product management uh working on Enterprise Solutions and I'm going to kick us off uh I will start",
    "start": "30800",
    "end": "38879"
  },
  {
    "text": "with some historical context uh Cloud era caters for the entire data science machine learning and AI life cycle and",
    "start": "38879",
    "end": "46039"
  },
  {
    "text": "this is the this is these are capabilities that we had uh even last",
    "start": "46039",
    "end": "51840"
  },
  {
    "text": "year how we Define the life cycle is it always starts with data engine pipelines",
    "start": "51840",
    "end": "57120"
  },
  {
    "text": "uh you need to have data uh present in your systems to drive your AI use cases",
    "start": "57120",
    "end": "62920"
  },
  {
    "text": "once you have the data uh data scientists typically do uh exploratory data science they work on understanding",
    "start": "62920",
    "end": "69320"
  },
  {
    "text": "the data they have at hand um they uh look at the characteristics shapes and",
    "start": "69320",
    "end": "75479"
  },
  {
    "text": "size of the data once they have a good understanding they can actually build a machine learning model uh they can train",
    "start": "75479",
    "end": "81400"
  },
  {
    "text": "a traditional model they can fune a model or they can do some promt engineering once they have a model ready",
    "start": "81400",
    "end": "88000"
  },
  {
    "text": "uh they need to package it uh Reg register it into a a model registry for long-term cataloging and once in the",
    "start": "88000",
    "end": "95159"
  },
  {
    "text": "registry they can deploy to a serving environment uh where uh they can uh",
    "start": "95159",
    "end": "101399"
  },
  {
    "text": "serve the incoming um mod requests and then uh once something is running in",
    "start": "101399",
    "end": "108119"
  },
  {
    "text": "production they need to make sure that it's up and running and healthy so they need to monitor everything under under",
    "start": "108119",
    "end": "113920"
  },
  {
    "text": "the hood they to make sure that it's it's up to par and at the last stage they need to build an AI application",
    "start": "113920",
    "end": "120960"
  },
  {
    "text": "data scientists don't build data science models just for the sake of it there is always one specific business problem one",
    "start": "120960",
    "end": "126680"
  },
  {
    "text": "specific use case that they are looking to solve and that's typically complete by building an application and the",
    "start": "126680",
    "end": "132360"
  },
  {
    "text": "application is a is a pretty vake concept uh it can mean anything it can be a web application that they host for",
    "start": "132360",
    "end": "138959"
  },
  {
    "text": "business stakeholders it can be a weekly report that they send out it can be a mobile application that integrates with",
    "start": "138959",
    "end": "144920"
  },
  {
    "text": "the rest end point the point here is that you need an application to complete your use case",
    "start": "144920",
    "end": "150400"
  },
  {
    "text": "for the longest time Cloud era had the cloud AI workbench as the the solution for this endtoend life cycle uh and in",
    "start": "150400",
    "end": "158400"
  },
  {
    "text": "the next couple of slides I'm going to go into detail what are the actual requirements for a real robust inference",
    "start": "158400",
    "end": "164120"
  },
  {
    "text": "platform and we are going to check how uh the AI workbench fars against those",
    "start": "164120",
    "end": "171319"
  },
  {
    "text": "requirements so first an inference platform needs to support all kinds of machine learning models it needs to",
    "start": "171319",
    "end": "177720"
  },
  {
    "text": "support the traditional models as well as the new types of models like llms and gen",
    "start": "177720",
    "end": "182760"
  },
  {
    "text": "models once you have models running you need to make sure that you are exposing the right interfaces nowadays everybody",
    "start": "182760",
    "end": "189360"
  },
  {
    "text": "expects uh to communicate and interact with open AI apis so you need to implement that um all of the libraries",
    "start": "189360",
    "end": "197480"
  },
  {
    "text": "like um Lang chain llama index or swirl AI uh expects uh models exposed by by",
    "start": "197480",
    "end": "204680"
  },
  {
    "text": "the open AI live um API protocol then there are the typical",
    "start": "204680",
    "end": "209840"
  },
  {
    "text": "Enterprise requirements the production requirements it needs to be AJ it needs to be fall tolerant it needs to support",
    "start": "209840",
    "end": "216239"
  },
  {
    "text": "zero down time upgrades uh it needs to support High scale it needs to support Auto scaling and scaling down to zero to",
    "start": "216239",
    "end": "223280"
  },
  {
    "text": "control costs it needs to support all kinds of security uh fine gting Access Control to",
    "start": "223280",
    "end": "229560"
  },
  {
    "text": "the model endpoint uh as well as like auditing absolutely everything all all",
    "start": "229560",
    "end": "235840"
  },
  {
    "text": "operations all interactions with the models and all of the inputs and out puts for the model end",
    "start": "235840",
    "end": "242120"
  },
  {
    "text": "points once you have all of this you need to make sure that you set up monitoring uh first of all you need to",
    "start": "242120",
    "end": "248560"
  },
  {
    "text": "track technical performance for traditional models those can be like latency um up time um different resource",
    "start": "248560",
    "end": "256639"
  },
  {
    "text": "usages throughput and for llms those are the uh time to First token inter token",
    "start": "256639",
    "end": "262800"
  },
  {
    "text": "latency uh and these are the typical uh technical metrics once you have the technical metrics you also need to make",
    "start": "262800",
    "end": "268960"
  },
  {
    "text": "sure that you can monitor um the the the business level performance so for",
    "start": "268960",
    "end": "275160"
  },
  {
    "text": "traditional models those are the drifts uh or llms those are the like",
    "start": "275160",
    "end": "280199"
  },
  {
    "text": "truthfulness um accuracy um semantic drift and so",
    "start": "280199",
    "end": "286560"
  },
  {
    "text": "on you need to have different operational security and fault domain",
    "start": "286560",
    "end": "291600"
  },
  {
    "text": "from development uh there are typically different uh controls put around the",
    "start": "291600",
    "end": "296919"
  },
  {
    "text": "production environment there are different people who have access to it uh there are different security measures",
    "start": "296919",
    "end": "302560"
  },
  {
    "text": "that are in place and typically it's not acceptable to host uh development and",
    "start": "302560",
    "end": "307960"
  },
  {
    "text": "production workloads within the same infrastructure it needs to be highly automatable it needs to expose the right",
    "start": "307960",
    "end": "315320"
  },
  {
    "text": "apis so you can build a cic pipeline that automates the end to end",
    "start": "315320",
    "end": "320919"
  },
  {
    "text": "process it needs to be highly customizable uh we have a wide variety",
    "start": "320919",
    "end": "327000"
  },
  {
    "text": "of customers they have very specific uh requirements and we need to make sure that uh those requirements can be met",
    "start": "327000",
    "end": "334039"
  },
  {
    "text": "for example in regulated Industries for example in the financial sector it's pretty common that there is an actual",
    "start": "334039",
    "end": "339319"
  },
  {
    "text": "user actual person uh let's call him or her as a auditor who needs to review",
    "start": "339319",
    "end": "346280"
  },
  {
    "text": "model documentation the model card and make sure that the model was trained uh",
    "start": "346280",
    "end": "351759"
  },
  {
    "text": "and the the data set uh is used by the corporate",
    "start": "351759",
    "end": "357960"
  },
  {
    "text": "policy uh it needs to run anywhere uh we have customers in the public Cloud any",
    "start": "357960",
    "end": "363000"
  },
  {
    "text": "of the public clouds or own premises and then at last it needs to support full",
    "start": "363000",
    "end": "368960"
  },
  {
    "text": "privacy most Enterprises most of our customers are typically not okay with",
    "start": "368960",
    "end": "374720"
  },
  {
    "text": "sending their prompts their proprietary Enterprise context uh data as as context",
    "start": "374720",
    "end": "381680"
  },
  {
    "text": "to to large language models to to get useful responses back they need to own the infrastructure end to end everything",
    "start": "381680",
    "end": "388160"
  },
  {
    "text": "needs to run within the VPC where they can control the security boundary and they can make sure that no data leaves",
    "start": "388160",
    "end": "393560"
  },
  {
    "text": "the premises in the next couple of slides I'm going to show you how the AI",
    "start": "393560",
    "end": "398880"
  },
  {
    "text": "workbench fares against these requirements so some of these requirements are already met or were met",
    "start": "398880",
    "end": "405280"
  },
  {
    "text": "last year we have all of the different kind of security uh we expose the right apis we support hybrid deployments and",
    "start": "405280",
    "end": "412919"
  },
  {
    "text": "we provide full privacy and then there are a set of requirements where we can claim some support for for example uh we",
    "start": "412919",
    "end": "421919"
  },
  {
    "text": "have apis but we don't Implement open AI uh we support AJ but we don't support",
    "start": "421919",
    "end": "427440"
  },
  {
    "text": "zdu uh we support High scale but we don't scale down to zero these are the things that we could invest in and",
    "start": "427440",
    "end": "434000"
  },
  {
    "text": "improve our current system um but then there is one thing the different operational security and F domain that",
    "start": "434000",
    "end": "440599"
  },
  {
    "text": "we cannot achieve in this in this in this product basically if you build a monolitic environment that supports the",
    "start": "440599",
    "end": "446960"
  },
  {
    "text": "end to-end life cycle from development to production that that inherent inherently we run in a single",
    "start": "446960",
    "end": "452479"
  },
  {
    "text": "environment so that's the reason we decided to split it out build uh a real",
    "start": "452479",
    "end": "457879"
  },
  {
    "text": "production environment the cloud F service for hosting machine learning models applications and provide",
    "start": "457879",
    "end": "464440"
  },
  {
    "text": "monitoring under all of it and we are refocusing the AI workbench for the development part of the life cycle zoram",
    "start": "464440",
    "end": "470919"
  },
  {
    "text": "is going to take over and walk you through how did we actually build this and what were the considerations that we",
    "start": "470919",
    "end": "477240"
  },
  {
    "text": "have thank you Peter so um Peter has basically laid laid out",
    "start": "477240",
    "end": "484759"
  },
  {
    "text": "you know what we need why we need to build um a new platform a new service so over the next few slides I'm",
    "start": "484759",
    "end": "492240"
  },
  {
    "text": "going to take you through um The Journey right from figuring out the",
    "start": "492240",
    "end": "498080"
  },
  {
    "text": "requirements and into um identifying the solutions and then what that solution uh",
    "start": "498080",
    "end": "504759"
  },
  {
    "text": "looks like all of you today have heard a lot about Ai and different",
    "start": "504759",
    "end": "510120"
  },
  {
    "text": "um AI serving Frameworks and uh many of you might already have a pretty good",
    "start": "510120",
    "end": "516000"
  },
  {
    "text": "idea of what the correct solution is um for your own use cases and what we're",
    "start": "516000",
    "end": "521479"
  },
  {
    "text": "going to present here is um what what fits uh our use cases and it's not going",
    "start": "521479",
    "end": "527959"
  },
  {
    "text": "to be necessarily the right um solution for everybody so just keep that in mind as we go through",
    "start": "527959",
    "end": "534920"
  },
  {
    "text": "this right so one obvious way um is you know",
    "start": "534920",
    "end": "541240"
  },
  {
    "text": "you build everything from scratch um if you had unlimited time and",
    "start": "541240",
    "end": "548079"
  },
  {
    "text": "unlimited resources and you have the right skill sets um so that's not really",
    "start": "548079",
    "end": "554240"
  },
  {
    "text": "an option right there is a question of time to Market and all that and the pace at which uh the field is uh changing uh",
    "start": "554240",
    "end": "561959"
  },
  {
    "text": "this is super hard um the next thing we could consider",
    "start": "561959",
    "end": "567560"
  },
  {
    "text": "is to uh open you know adopt an open source um project right so at this point",
    "start": "567560",
    "end": "575360"
  },
  {
    "text": "um take a good open source project and build um Enterprise security and",
    "start": "575360",
    "end": "582040"
  },
  {
    "text": "governance around that that our customers are accustomed to in the Enterprise space uh strong security",
    "start": "582040",
    "end": "589279"
  },
  {
    "text": "strong governance uh Access Control compliance and all that good stuff okay uh taking an open source",
    "start": "589279",
    "end": "597640"
  },
  {
    "text": "project sounds like a great idea but the problem is there's",
    "start": "597640",
    "end": "604920"
  },
  {
    "text": "many right so I want to I want to ask the audience",
    "start": "604920",
    "end": "612120"
  },
  {
    "text": "right if if you are in a similar uh position um and based on what you've heard",
    "start": "612120",
    "end": "618160"
  },
  {
    "text": "today um what would you choose C",
    "start": "618160",
    "end": "625720"
  },
  {
    "text": "flow what else",
    "start": "625880",
    "end": "630000"
  },
  {
    "text": "uh what was that open shift open shift AI yeah definitely interesting",
    "start": "635519",
    "end": "642199"
  },
  {
    "text": "choice andv names all right let's see um so we looked at various uh open",
    "start": "642959",
    "end": "650440"
  },
  {
    "text": "source projects at the time right this was back uh in uh the last part of 2022",
    "start": "650440",
    "end": "656399"
  },
  {
    "text": "early part of 2023 um and we decided against these for various",
    "start": "656399",
    "end": "665240"
  },
  {
    "text": "reasons it's not necessarily technical right there are other considerations that come into play especially if you",
    "start": "665240",
    "end": "672680"
  },
  {
    "text": "operate in uh the kind of business model that cloud has so uh things like",
    "start": "672680",
    "end": "678040"
  },
  {
    "text": "licensing and governance become important as well so I I just want to",
    "start": "678040",
    "end": "683800"
  },
  {
    "text": "say that all of these choices are great um they're technically strong very",
    "start": "683800",
    "end": "689560"
  },
  {
    "text": "strong too right so uh what we ended up going with",
    "start": "689560",
    "end": "694959"
  },
  {
    "text": "was um kerf and in the next slide I'm going to explain uh a little bit more",
    "start": "694959",
    "end": "700760"
  },
  {
    "text": "and why uh we went with that um so I I think there were like two",
    "start": "700760",
    "end": "706880"
  },
  {
    "text": "or three talks on kof today and uh many more talks on uh Ray and VM and things",
    "start": "706880",
    "end": "713760"
  },
  {
    "text": "like that so we're not necessarily going to go into the technical details",
    "start": "713760",
    "end": "719760"
  },
  {
    "text": "but the first important thing we saw with the cas of project was that the governance is very open right it is um",
    "start": "719760",
    "end": "727880"
  },
  {
    "text": "under the cncf uh or Linux foundation so it's sort of um very independent from um",
    "start": "727880",
    "end": "737240"
  },
  {
    "text": "corporate control so there is no single entity that is setting the uh the road map for the project so we we like that",
    "start": "737240",
    "end": "745360"
  },
  {
    "text": "right and of course there are technical reasons too like um we are very opinionated about um",
    "start": "745360",
    "end": "755360"
  },
  {
    "text": "what our platform options uh need to be um we deploy everything on kubernetes so",
    "start": "755360",
    "end": "763079"
  },
  {
    "text": "a kubernetes native project um that started out with a",
    "start": "763079",
    "end": "769279"
  },
  {
    "text": "kubernetes first uh approach that was really appealing to us serverless that's",
    "start": "769279",
    "end": "775399"
  },
  {
    "text": "a big deal um our customers run their workloads in the cloud and uh you you",
    "start": "775399",
    "end": "781839"
  },
  {
    "text": "don't want to you know consume resources that you're you're not using right and then open API surfaces like u",
    "start": "781839",
    "end": "791320"
  },
  {
    "text": "p mentioned uh open inference protocols open API things like",
    "start": "791320",
    "end": "797320"
  },
  {
    "text": "that and of course our customers use a variety of ml Frameworks and that's a",
    "start": "797320",
    "end": "804440"
  },
  {
    "text": "critical thing that we saw with ker that it has out of the box support for uh a",
    "start": "804440",
    "end": "810000"
  },
  {
    "text": "lot of uh ml Frameworks and it's also highly flexible",
    "start": "810000",
    "end": "816480"
  },
  {
    "text": "customizable so there's a well- defined framework for uh building your own um",
    "start": "816480",
    "end": "823440"
  },
  {
    "text": "model serving runtimes out of the box monitoring for",
    "start": "823440",
    "end": "828880"
  },
  {
    "text": "uh support for monitoring uh and logging and thanks to its integration",
    "start": "828880",
    "end": "834759"
  },
  {
    "text": "with Ken Ando uh it's super easy to put security defense around",
    "start": "834759",
    "end": "842120"
  },
  {
    "text": "it all right now we're going to look at a",
    "start": "842120",
    "end": "847399"
  },
  {
    "text": "really really high level um architecture view of what the resulting solution uh",
    "start": "847399",
    "end": "854000"
  },
  {
    "text": "looks like in the uh Cloudera Big Data ecosystem so we start with the open",
    "start": "854000",
    "end": "861360"
  },
  {
    "text": "source project that we picked casar of ctive and the um Upstream dependencies",
    "start": "861360",
    "end": "867720"
  },
  {
    "text": "like uh ISO put that in a box right and you get um Auto scaling in",
    "start": "867720",
    "end": "876079"
  },
  {
    "text": "both directions uh for free ha for",
    "start": "876079",
    "end": "880560"
  },
  {
    "text": "free you add monitoring by integrating with Prometheus or open",
    "start": "881399",
    "end": "888000"
  },
  {
    "text": "Telemetry and then um integrate with uh um a storage system like Iceberg",
    "start": "888000",
    "end": "896680"
  },
  {
    "text": "where you can you can uh basically write all of your uh monitoring data for",
    "start": "896680",
    "end": "901800"
  },
  {
    "text": "offline analysis and you add the security defense uh all using open source um",
    "start": "901800",
    "end": "910199"
  },
  {
    "text": "projects um nox for authentication Ranger for fine grain Access Control to",
    "start": "910199",
    "end": "916839"
  },
  {
    "text": "um the services that are running on the platform and uh Atlas for um audit",
    "start": "916839",
    "end": "924000"
  },
  {
    "text": "everything right every access is logged to atlas",
    "start": "924000",
    "end": "930120"
  },
  {
    "text": "so that's what makes up the um the new service that we built which we are calling clouder II",
    "start": "930120",
    "end": "938120"
  },
  {
    "text": "service now you couple that with the uh existing services that we have like the",
    "start": "938120",
    "end": "944079"
  },
  {
    "text": "um AI registry thei workbench that drives um development for the data",
    "start": "944079",
    "end": "952279"
  },
  {
    "text": "scientists and then finally those are driving the uh business use cases like",
    "start": "952279",
    "end": "957839"
  },
  {
    "text": "AI applications um AI systems and so on and run all of that whether you're uh",
    "start": "957839",
    "end": "965800"
  },
  {
    "text": "on Prem or in the",
    "start": "965800",
    "end": "969720"
  },
  {
    "text": "cloud okay with that out of the way let's take a closer look at um the AI",
    "start": "971360",
    "end": "979160"
  },
  {
    "text": "related services in our ecosystem so um this box represents um a",
    "start": "979160",
    "end": "986959"
  },
  {
    "text": "VPC or a data center so we have the AI",
    "start": "986959",
    "end": "992240"
  },
  {
    "text": "workbench AI registry so these were the existing uh services and we add to that",
    "start": "992240",
    "end": "998519"
  },
  {
    "text": "the um the inference platform uh where uh K serve is",
    "start": "998519",
    "end": "1004319"
  },
  {
    "text": "responsible for orchestrating the inference Services now the flexibility of kerve",
    "start": "1004319",
    "end": "1010639"
  },
  {
    "text": "that I pointed out earlier allowed us to create um custom run times that offer",
    "start": "1010639",
    "end": "1016519"
  },
  {
    "text": "the best-in-class uh inference performance by integrating DBL with",
    "start": "1016519",
    "end": "1022319"
  },
  {
    "text": "Nvidia NS for Gen use cases and uh Nvidia Triton server for um predictive",
    "start": "1022319",
    "end": "1030400"
  },
  {
    "text": "uh inferencing and those Drive the um use cases that we see in the Box um below",
    "start": "1030400",
    "end": "1038360"
  },
  {
    "text": "here um so you see the two uh API surfaces open inference protocol for",
    "start": "1038360",
    "end": "1043678"
  },
  {
    "text": "predictive models and open AI for Gen use cases and also add the ability to uh",
    "start": "1043679",
    "end": "1051320"
  },
  {
    "text": "import or uh leverage pre-trained models from any Source such as the Nvidia um",
    "start": "1051320",
    "end": "1058360"
  },
  {
    "text": "catalog or from hugging face develop fine tune and deploy to the",
    "start": "1058360",
    "end": "1065400"
  },
  {
    "text": "inference platform and all of that is orchestrated with um mlop cicd uh systems like uh Pat",
    "start": "1065400",
    "end": "1074400"
  },
  {
    "text": "air flow and the end result is you can scale uh uh like crazy hundreds of models um",
    "start": "1074400",
    "end": "1083039"
  },
  {
    "text": "hundreds of model replicas you know per model deployment and deploy those on het",
    "start": "1083039",
    "end": "1090000"
  },
  {
    "text": "heterogeneous combination of GPU and CPU nodes you can autoscale the nodes and you can autoscale the models super",
    "start": "1090000",
    "end": "1098880"
  },
  {
    "text": "cool all right so that's still fairly high level um so I want to take",
    "start": "1098880",
    "end": "1106559"
  },
  {
    "text": "the next two slides to just take a quick look at the some of the uh control flows",
    "start": "1106559",
    "end": "1113799"
  },
  {
    "text": "and inference flows so that it becomes clearer you know what what happening uh",
    "start": "1113799",
    "end": "1120559"
  },
  {
    "text": "underneath so here um the rectangle represents um a kubernetes cluster so we",
    "start": "1120559",
    "end": "1128000"
  },
  {
    "text": "have a bunch of uh components that are deployed there right so we're going to",
    "start": "1128000",
    "end": "1133080"
  },
  {
    "text": "look at an example of how um the user might deploy",
    "start": "1133080",
    "end": "1140360"
  },
  {
    "text": "model and then I I want to show the authentication and authorization flows",
    "start": "1141679",
    "end": "1147200"
  },
  {
    "text": "right all of that is built using open source uh",
    "start": "1147200",
    "end": "1153400"
  },
  {
    "text": "software so we we've configured um Apache nox as the uh authentication",
    "start": "1154039",
    "end": "1161480"
  },
  {
    "text": "Gateway that is integrated with um a JWT uh issuer that's running in the same um",
    "start": "1161480",
    "end": "1169159"
  },
  {
    "text": "data center all within the customers uh premises which is the apach nox up here",
    "start": "1169159",
    "end": "1175960"
  },
  {
    "text": "uh is also integrated with the uh corporate um elap so with uh the authentication uh",
    "start": "1175960",
    "end": "1185159"
  },
  {
    "text": "done um the request goes to a thin API service that we created that provides",
    "start": "1185159",
    "end": "1192480"
  },
  {
    "text": "the integration between um kerve on and kubernetes on one side and the rest of",
    "start": "1192480",
    "end": "1198919"
  },
  {
    "text": "the um clera big data services so in this example the API",
    "start": "1198919",
    "end": "1206880"
  },
  {
    "text": "server reaches out to our model registry you know gets uh configuration and",
    "start": "1206880",
    "end": "1212919"
  },
  {
    "text": "metadata about the model including uh which runtime that we need to use for",
    "start": "1212919",
    "end": "1219520"
  },
  {
    "text": "model and also uh the location of the model artifacts in the um model registry",
    "start": "1219520",
    "end": "1226720"
  },
  {
    "text": "storage right the next step creates a case of INF service Uh custom resource",
    "start": "1226720",
    "end": "1235360"
  },
  {
    "text": "definition and that gets translated to a kive service",
    "start": "1235360",
    "end": "1240840"
  },
  {
    "text": "definition which kive reconciles into um an actual running um model",
    "start": "1240840",
    "end": "1250320"
  },
  {
    "text": "deployment um the storage initializer unit container pulls down the the model",
    "start": "1250320",
    "end": "1255640"
  },
  {
    "text": "from any uh the storage configured for the model uh",
    "start": "1255640",
    "end": "1260960"
  },
  {
    "text": "registry and once the model is running you know you start scraping uh metrics",
    "start": "1260960",
    "end": "1266640"
  },
  {
    "text": "using uh Prometheus so that's the control flow um",
    "start": "1266640",
    "end": "1271840"
  },
  {
    "text": "next I want to take you through um inference",
    "start": "1271840",
    "end": "1277559"
  },
  {
    "text": "flow again we have the same building blocks but this time um we're going to",
    "start": "1278080",
    "end": "1285640"
  },
  {
    "text": "um basically call a model and point that is running within this kubernetes cluster",
    "start": "1285640",
    "end": "1292279"
  },
  {
    "text": "um and run some prediction or uh generation so same as before that's the",
    "start": "1292279",
    "end": "1298799"
  },
  {
    "text": "authentication flow but the what's different this time",
    "start": "1298799",
    "end": "1304240"
  },
  {
    "text": "is that the model endpoints are um protected resources where we can uh",
    "start": "1304240",
    "end": "1311400"
  },
  {
    "text": "enforce fine gra access policies which user is allowed to access which endpoint",
    "start": "1311400",
    "end": "1317039"
  },
  {
    "text": "or which group of users are allowed to access certain endpoints so after the",
    "start": "1317039",
    "end": "1324000"
  },
  {
    "text": "authentication there's an extra step for authorization so this goes to uh an",
    "start": "1324000",
    "end": "1331480"
  },
  {
    "text": "Apache Ranger server running inside the same um VPC and the access is automatically um",
    "start": "1331480",
    "end": "1339760"
  },
  {
    "text": "loged to Apache Atlas for uh auditing if all that all of that checks",
    "start": "1339760",
    "end": "1346559"
  },
  {
    "text": "out um the request test is forwarded to the actual model",
    "start": "1346559",
    "end": "1351760"
  },
  {
    "text": "server that does the um prediction or generation depending on which type of model it is and then the model server um",
    "start": "1351760",
    "end": "1361520"
  },
  {
    "text": "logs the uh inference uh request and response",
    "start": "1361520",
    "end": "1367159"
  },
  {
    "text": "payloads to a payload logger service um that's responsible for",
    "start": "1367159",
    "end": "1373000"
  },
  {
    "text": "aggregating uh all all inference um request and response payloads",
    "start": "1373000",
    "end": "1378880"
  },
  {
    "text": "and then sends that over to a service that that's running outside um but still",
    "start": "1378880",
    "end": "1384039"
  },
  {
    "text": "in the same U premises that gets uh written into one",
    "start": "1384039",
    "end": "1390919"
  },
  {
    "text": "or more um Iceberg tables which is uh used for offline uh",
    "start": "1390919",
    "end": "1399720"
  },
  {
    "text": "you know analysis for you know compliance and accuracy uh uh drift",
    "start": "1399720",
    "end": "1406760"
  },
  {
    "text": "detection and all that um basically we dump everything to these Iceberg tables",
    "start": "1406760",
    "end": "1412840"
  },
  {
    "text": "and customers can do any arbitrary uh analysis uh on that",
    "start": "1412840",
    "end": "1420240"
  },
  {
    "text": "data all right so going still a little bit deeper uh I want to briefly talk",
    "start": "1420240",
    "end": "1426600"
  },
  {
    "text": "about um the run times we have created so",
    "start": "1426600",
    "end": "1431720"
  },
  {
    "text": "far so like I mentioned before um we have customer run times for various",
    "start": "1431720",
    "end": "1437720"
  },
  {
    "text": "Nvidia NS basically um one one run time per Nim",
    "start": "1437720",
    "end": "1443840"
  },
  {
    "text": "per family right uh for llama 3.1 there's going to be one on time Mistral",
    "start": "1443840",
    "end": "1449039"
  },
  {
    "text": "Mixr and so on and then we also get the um out of",
    "start": "1449039",
    "end": "1454960"
  },
  {
    "text": "the box support for uh running arbitrary Transformer models from hugging face via",
    "start": "1454960",
    "end": "1461080"
  },
  {
    "text": "the hugging face run time that that you get for free uh from kerf which uses",
    "start": "1461080",
    "end": "1467000"
  },
  {
    "text": "either the VM back end or the hugging face back",
    "start": "1467000",
    "end": "1472200"
  },
  {
    "text": "end and finally for predictive models um we have around time that leverages",
    "start": "1472200",
    "end": "1479679"
  },
  {
    "text": "nvidia's Triton uh inference",
    "start": "1479679",
    "end": "1483960"
  },
  {
    "text": "server okay one more level then I'll stop um so this is what uh every model",
    "start": "1486720",
    "end": "1494840"
  },
  {
    "text": "replica part uh looks like so for an llm you're going to have the Nvidia n as the",
    "start": "1494840",
    "end": "1500440"
  },
  {
    "text": "main model uh container here the storage initializer is responsible for uh",
    "start": "1500440",
    "end": "1507559"
  },
  {
    "text": "fetching the model artifacts from uh an object store typically",
    "start": "1507559",
    "end": "1513720"
  },
  {
    "text": "um and then um the Q",
    "start": "1513720",
    "end": "1519520"
  },
  {
    "text": "proxy enables uh kive serverless",
    "start": "1519520",
    "end": "1525200"
  },
  {
    "text": "Magic and the envoy proxy uh is used for for us to create a filter chain in esto",
    "start": "1525200",
    "end": "1533559"
  },
  {
    "text": "such that we can route authorization request to um Ranger like I showed",
    "start": "1533559",
    "end": "1540000"
  },
  {
    "text": "before and I think that's all I have and over to Peter for some closing",
    "start": "1540000",
    "end": "1546919"
  },
  {
    "text": "thoughts so I'm going to close the session with some key takeaways um what we really want you to",
    "start": "1546919",
    "end": "1554159"
  },
  {
    "text": "take home is that you can also build a robust inference platform um we obviously built it for ourselves but",
    "start": "1554159",
    "end": "1561760"
  },
  {
    "text": "also for our customers so you might have some more specific requirements that than we do uh and you might not have all",
    "start": "1561760",
    "end": "1569520"
  },
  {
    "text": "of the requirements that that we do but the point here is that using open source",
    "start": "1569520",
    "end": "1575200"
  },
  {
    "text": "software and open source components you can integrate a system for yourself first of all you can run anywhere we are",
    "start": "1575200",
    "end": "1581320"
  },
  {
    "text": "on a kubernetes conference kubernetes gives you the right obstruction to run in any of the public clouds on premises",
    "start": "1581320",
    "end": "1587919"
  },
  {
    "text": "you can use vanilla kubernetes or uh any of the the kubernetes",
    "start": "1587919",
    "end": "1593039"
  },
  {
    "text": "providers you can achieve Enterprise scale scale Enterprise grade",
    "start": "1593039",
    "end": "1599200"
  },
  {
    "text": "scalability uh we are quite happy with with what ker offers to us uh it",
    "start": "1599200",
    "end": "1604679"
  },
  {
    "text": "supports Autos scaling uh it is AG by default it's quite robust and it offers",
    "start": "1604679",
    "end": "1612279"
  },
  {
    "text": "customizability uh and there are two key points here because ker is an open",
    "start": "1612279",
    "end": "1617480"
  },
  {
    "text": "source community project it already supports a broad range of use cases with a broad range of",
    "start": "1617480",
    "end": "1624120"
  },
  {
    "text": "functionalities most of the things that we needed are already there so we can adopt we can contribute back we can find",
    "start": "1624120",
    "end": "1630840"
  },
  {
    "text": "the issues that we find based on our or specific requirements uh and then we can",
    "start": "1630840",
    "end": "1636440"
  },
  {
    "text": "grow into be part of the ecosystem and the other big thing uh is that K serve",
    "start": "1636440",
    "end": "1643880"
  },
  {
    "text": "achieved this by flexibility by pluggability or extensibility we can and we did cherry pick the best in class of",
    "start": "1643880",
    "end": "1651320"
  },
  {
    "text": "model servers uh but we can also contribute back so for example Cloud ER",
    "start": "1651320",
    "end": "1656399"
  },
  {
    "text": "has customers who are using R like R language and they are running R models",
    "start": "1656399",
    "end": "1661760"
  },
  {
    "text": "and we haven't find uh a good r model server so that's something that we probably will need to and we will",
    "start": "1661760",
    "end": "1667799"
  },
  {
    "text": "contribute back and at last Cloud integrated with with uh the security",
    "start": "1667799",
    "end": "1673919"
  },
  {
    "text": "Library services that uh that cloud uses Ranger and nox uh but there are plenty of plenty of other options out there you",
    "start": "1673919",
    "end": "1681159"
  },
  {
    "text": "don't need to implement security from scratch uh you can build security around",
    "start": "1681159",
    "end": "1686440"
  },
  {
    "text": "around kerve so all in all we are quite happy with our with our decision to to adopt",
    "start": "1686440",
    "end": "1692919"
  },
  {
    "text": "kerve uh and be part of this community thank",
    "start": "1692919",
    "end": "1697360"
  },
  {
    "text": "you yes questions sorry hi there so um I'm",
    "start": "1700919",
    "end": "1708679"
  },
  {
    "text": "so I've used Ranger at scale early in my experience with data um when it used to work with hi and H base and all those",
    "start": "1708679",
    "end": "1714760"
  },
  {
    "text": "things so can you explain with an example how Ranger sort of uh helps you in this use case uh how did you use",
    "start": "1714760",
    "end": "1722200"
  },
  {
    "text": "Ranger do you need to have a specific Ranger plugin to sort of light this up or or is it just open source Ranger that",
    "start": "1722200",
    "end": "1729120"
  },
  {
    "text": "the people have used earlier with data yeah um thank you for the question um",
    "start": "1729120",
    "end": "1734919"
  },
  {
    "text": "there's no secret Source here uh really um we using a ranger client that's",
    "start": "1734919",
    "end": "1740120"
  },
  {
    "text": "running in inside the inference uh cluster that is preconfigured so we",
    "start": "1740120",
    "end": "1746360"
  },
  {
    "text": "basically at the time of creating a cluster we we populate uh we inject the",
    "start": "1746360",
    "end": "1751480"
  },
  {
    "text": "necessarily the necessary credentials to go and reach out to the ranger server that's managed by Cloud",
    "start": "1751480",
    "end": "1757919"
  },
  {
    "text": "manager so and then we create we have created new um Ranger service",
    "start": "1757919",
    "end": "1764480"
  },
  {
    "text": "definitions and policy rules so when you create a model",
    "start": "1764480",
    "end": "1770200"
  },
  {
    "text": "deployment we end up with a basically default set of permissions that the admin can go and you know tune so at the",
    "start": "1770200",
    "end": "1777399"
  },
  {
    "text": "time of making the inference the the ranger client running locally on the cluster reaches out to um the ranger",
    "start": "1777399",
    "end": "1784320"
  },
  {
    "text": "server that checks the uh the user credentials against the um policies",
    "start": "1784320",
    "end": "1790120"
  },
  {
    "text": "defined for that endpoint so Ranger protects uh Uris basically right so",
    "start": "1790120",
    "end": "1797000"
  },
  {
    "text": "historically it was was file file system pads storage pads but that's really",
    "start": "1797000",
    "end": "1802399"
  },
  {
    "text": "extensible to http uh URLs that's that's how we do",
    "start": "1802399",
    "end": "1808200"
  },
  {
    "text": "it uh we have a version of cloud era machine learning CML running right so",
    "start": "1808399",
    "end": "1815360"
  },
  {
    "text": "where does this AI thing fit in is it a extension of it or is it totally different",
    "start": "1815360",
    "end": "1821519"
  },
  {
    "text": "offering are uh so uh the a inference service sits next to the uh to the CMI",
    "start": "1821519",
    "end": "1828679"
  },
  {
    "text": "workspace that we are renaming to AI workbench that actually historically was",
    "start": "1828679",
    "end": "1834039"
  },
  {
    "text": "cloud data science workbench then it became CLA machine learning workspace now we are going back to the workbench",
    "start": "1834039",
    "end": "1839519"
  },
  {
    "text": "uh terminology so basically claer is building a product suit for AI uh that",
    "start": "1839519",
    "end": "1844799"
  },
  {
    "text": "we call CL cler AI the workbench is going to be the development environment you have the model registry and then we",
    "start": "1844799",
    "end": "1851760"
  },
  {
    "text": "have the the AI inference service that we talked about today so CML is your workbench yes yes a AI inference is the",
    "start": "1851760",
    "end": "1859519"
  },
  {
    "text": "new piece that we need to add adjacent to it to achieve the adjacent to it that's correct yes okay but um if we are",
    "start": "1859519",
    "end": "1867480"
  },
  {
    "text": "using an on Prem and it is a disconnected environment where we don't have access to NGC catalog or hugging",
    "start": "1867480",
    "end": "1873080"
  },
  {
    "text": "phase so what is the option here like you are expecting us to put the model in a",
    "start": "1873080",
    "end": "1881080"
  },
  {
    "text": "S3 it's our responsibility to push it into our S3 bucket and then integrate with uh all the other pieces that you",
    "start": "1881080",
    "end": "1887760"
  },
  {
    "text": "should showed or is there any other way so uh yeah you can we do support the",
    "start": "1887760",
    "end": "1893919"
  },
  {
    "text": "model registry as an on Prem U service and there are ways to you know get",
    "start": "1893919",
    "end": "1899639"
  },
  {
    "text": "pre-train models from out there yeah and sync it to that mod yeah sync it to the model registry okay and um we're also",
    "start": "1899639",
    "end": "1907480"
  },
  {
    "text": "enhancing our model registry to support a hybrid kind of deployment where you can paare it with um a service that you",
    "start": "1907480",
    "end": "1914960"
  },
  {
    "text": "run in the public Cloud so and have a private connection between",
    "start": "1914960",
    "end": "1920080"
  },
  {
    "text": "your on Prem model registry and a model registry that's sitting in the public",
    "start": "1920080",
    "end": "1925279"
  },
  {
    "text": "Cloud uh a tunnel if you will right that that allows you to um go through the air",
    "start": "1925279",
    "end": "1932440"
  },
  {
    "text": "gap type situation we have a little bit different challenge right we want a model registry",
    "start": "1932440",
    "end": "1937960"
  },
  {
    "text": "it needs to be on Prem but it needs that model which gets trained from CML or somewhere else ends up there but it",
    "start": "1937960",
    "end": "1944600"
  },
  {
    "text": "should be consumed by others I think so it's doable right if we yes you're right yeah okay both on Prem and in public",
    "start": "1944600",
    "end": "1951120"
  },
  {
    "text": "Club yes hi thanks for the talk uh was really",
    "start": "1951120",
    "end": "1956159"
  },
  {
    "text": "nice um I had a question on the um you know the part level um diagram that you",
    "start": "1956159",
    "end": "1961639"
  },
  {
    "text": "showed you showed um there is um uh the Q proxy and then the NY proxy as well uh",
    "start": "1961639",
    "end": "1967120"
  },
  {
    "text": "is the K native Q proxy on the request hot path I don't think so right or is",
    "start": "1967120",
    "end": "1973398"
  },
  {
    "text": "it yeah it it actually is okay that's that's how you know the K native",
    "start": "1973760",
    "end": "1980279"
  },
  {
    "text": "activator can keep track of the level of concurrency if you want to Scale based",
    "start": "1980279",
    "end": "1985399"
  },
  {
    "text": "on concurrency or the number of requests per second that's hitting a particular",
    "start": "1985399",
    "end": "1991399"
  },
  {
    "text": "Po got it okay uh so if I mean in that case does it do the redirection as well",
    "start": "1991399",
    "end": "1997000"
  },
  {
    "text": "to other pods um or not the Q proxy itself doesn't do it so the Q proxy is",
    "start": "1997000",
    "end": "2002039"
  },
  {
    "text": "attached to uh the Pod so it really doesn't know anything about what's happening elsewhere in in the deployment",
    "start": "2002039",
    "end": "2009240"
  },
  {
    "text": "uh cool so then um what's the uh I mean there is a caser with model mesh as well",
    "start": "2009240",
    "end": "2014519"
  },
  {
    "text": "right so what's uh why uh I mean what's the pros and cons of one or the other in",
    "start": "2014519",
    "end": "2019600"
  },
  {
    "text": "this case uh so why why did you choose this so I I can try um so we did look into",
    "start": "2019600",
    "end": "2027960"
  },
  {
    "text": "model mesh early on uh it was still early days for model mesh I think right",
    "start": "2027960",
    "end": "2035480"
  },
  {
    "text": "um I'm not ruling it out for the future we might give it another look um and see",
    "start": "2035480",
    "end": "2042480"
  },
  {
    "text": "uh you know if it makes sense for our use case it's a fine uh piece of",
    "start": "2042480",
    "end": "2049079"
  },
  {
    "text": "technology okay for sure yeah cool and um I mean in this uh particular deployment use case if you have multiple",
    "start": "2049079",
    "end": "2055280"
  },
  {
    "text": "models it's probably like one model per Triton container is what you've deployed",
    "start": "2055280",
    "end": "2060839"
  },
  {
    "text": "is that the right understanding today today yes today yes but we have plans to to like add support for multi adopters",
    "start": "2060839",
    "end": "2069638"
  },
  {
    "text": "uh for a single model so you can host a large language model with different fine tuned variants within the same container",
    "start": "2069639",
    "end": "2076480"
  },
  {
    "text": "also other uh like multimodel serving um use cases cool",
    "start": "2076480",
    "end": "2083158"
  },
  {
    "text": "thanks hi um maybe you addressed it there um but I have the payload logger",
    "start": "2083839",
    "end": "2090638"
  },
  {
    "text": "today my problem seems to be that every data science team wants to do their",
    "start": "2090639",
    "end": "2096240"
  },
  {
    "text": "metrics in a different place snowflake Athena you name it um oh maybe just use",
    "start": "2096240",
    "end": "2104000"
  },
  {
    "text": "expose a kovka stream and people can do what they want with it just send it to Kafka and you know let Kafka handle it",
    "start": "2104000",
    "end": "2110400"
  },
  {
    "text": "arrest basically um so you could write to an iceberg table or a park file um I",
    "start": "2110400",
    "end": "2116880"
  },
  {
    "text": "guess um or maybe EV evidently right yeah I guess um okay um so then the data",
    "start": "2116880",
    "end": "2125200"
  },
  {
    "text": "science team is responsible for just consuming from the the topic at this point because if I write it all to just",
    "start": "2125200",
    "end": "2130839"
  },
  {
    "text": "one table then I have multi-tenant teams coming to one table which would be kind",
    "start": "2130839",
    "end": "2136960"
  },
  {
    "text": "of a security issue so so yeah the nice thing about routing it through kfka is you can you can do all sorts of",
    "start": "2136960",
    "end": "2143240"
  },
  {
    "text": "filtering right there okay thank you right great presentation guys thanks um",
    "start": "2143240",
    "end": "2150680"
  },
  {
    "text": "it sounded like you designed this in you said late 2022 and early 2023 is there anything you'd change thinking about the",
    "start": "2150680",
    "end": "2157280"
  },
  {
    "text": "inference flow and the control flow if you redesigned it now or is there anything that you're you know hoping",
    "start": "2157280",
    "end": "2163560"
  },
  {
    "text": "that will enhance the platform in future that's a tricky one um so I I don't",
    "start": "2163560",
    "end": "2170960"
  },
  {
    "text": "think we would change the our technology and the comp component choices but if we had known that um gen",
    "start": "2170960",
    "end": "2181200"
  },
  {
    "text": "would take take over the world at that speed right we may have implemented an",
    "start": "2181200",
    "end": "2188000"
  },
  {
    "text": "some kind of a Gateway U especially for the open AI",
    "start": "2188000",
    "end": "2194400"
  },
  {
    "text": "type um API because open",
    "start": "2194400",
    "end": "2199960"
  },
  {
    "text": "AI basically serves managed models so they their use case is very very",
    "start": "2199960",
    "end": "2205920"
  },
  {
    "text": "different from most of the use cases our customers have so now that it it's",
    "start": "2205920",
    "end": "2212440"
  },
  {
    "text": "become so popular everyone is kind of forced to adopt their paradigm and you",
    "start": "2212440",
    "end": "2218079"
  },
  {
    "text": "know um it's good in a way because applications written for open AI can be",
    "start": "2218079",
    "end": "2224560"
  },
  {
    "text": "easily ported to um hitting uh models that you're serving on premise uh your",
    "start": "2224560",
    "end": "2230200"
  },
  {
    "text": "private models but yeah that's definitely something we would have maybe you know changed",
    "start": "2230200",
    "end": "2238200"
  }
]