[
  {
    "text": "all right welcome everyone to what is the name of our talk really navigating the processing unit landscape in",
    "start": "1160",
    "end": "8120"
  },
  {
    "text": "kubernetes for AI use cases um so today we're going to be talking about processing units we'll go over the",
    "start": "8120",
    "end": "14080"
  },
  {
    "text": "basics of what they are and then we're going to talk about how they're used in kubernetes especially in Ai and ml",
    "start": "14080",
    "end": "20760"
  },
  {
    "text": "workloads I'm casyn Fields I'm a developer Advocate at Google Cloud where I focus on gke and open source",
    "start": "20760",
    "end": "27519"
  },
  {
    "text": "kubernetes I'm also a co-chair of the speci interest group for contributor experience in open source kubernetes so",
    "start": "27519",
    "end": "33680"
  },
  {
    "text": "if you have any questions about contributing feel free to let me know um I'm also a cncf Ambassador and a co-host",
    "start": "33680",
    "end": "39600"
  },
  {
    "text": "of the kuber podcast from Google and my name is Mofi I am also developed Advocate at Google focusing on gke and",
    "start": "39600",
    "end": "45680"
  },
  {
    "text": "these days my focus mostly is around running AI workload on",
    "start": "45680",
    "end": "51120"
  },
  {
    "text": "kubernetes and hello everyone I'm Rob cot I'm a principal at slom",
    "start": "51559",
    "end": "58000"
  },
  {
    "text": "Bill I'm also an data hero so sorry in between amongst",
    "start": "58199",
    "end": "64080"
  },
  {
    "text": "two gers here but uh I also work um in the uh",
    "start": "64080",
    "end": "70320"
  },
  {
    "text": "cncf uh DEA and heart of hearing working group I am the co-chair here and I'd like to welcome uh the rest of the group",
    "start": "70320",
    "end": "76360"
  },
  {
    "text": "here that's uh supporting me today all right folks we're going to",
    "start": "76360",
    "end": "83040"
  },
  {
    "text": "start off with some heavy stuff going to tell you the truth about",
    "start": "83040",
    "end": "88119"
  },
  {
    "text": "containers did you know that you write containers in like a 100 lines of",
    "start": "88119",
    "end": "93600"
  },
  {
    "text": "bash containers really are primarily made up of a couple of core Linux kernel",
    "start": "93600",
    "end": "99520"
  },
  {
    "text": "components called cgroups and namespaces cgroups allow you to do resource sharing",
    "start": "99520",
    "end": "106200"
  },
  {
    "text": "so you've got so much CPU you've got so much memory on your machine using a cgroup you can say this process gets",
    "start": "106200",
    "end": "112840"
  },
  {
    "text": "this much of the processor and gets this much of the memory the other core component is nam spaces and name spaces",
    "start": "112840",
    "end": "120600"
  },
  {
    "text": "are a logical environmental isolation mechanism so they're a way of separating",
    "start": "120600",
    "end": "126479"
  },
  {
    "text": "processes from one another and aside from that a container is basically just a process and that's going to come into",
    "start": "126479",
    "end": "132319"
  },
  {
    "text": "play a lot as we go through our content today so let's start talking about",
    "start": "132319",
    "end": "138640"
  },
  {
    "text": "processing units there are a whole bunch of different types of processing units actually not sure if you're aware but",
    "start": "138640",
    "end": "145120"
  },
  {
    "text": "there are a whole bunch of them they all in in pu because they're processing units but today we're going to be",
    "start": "145120",
    "end": "150160"
  },
  {
    "text": "talking about them in the context of kubernetes so for a processing unit to",
    "start": "150160",
    "end": "155640"
  },
  {
    "text": "be uh compatible or supported by kubernetes it basically needs to be supported by uh the Linux kernel it",
    "start": "155640",
    "end": "162920"
  },
  {
    "text": "needs to have a device driver that's compatible with kubernetes and the hardware that kubernetes works with and",
    "start": "162920",
    "end": "168840"
  },
  {
    "text": "it needs to be supported by the kubernetes scheduler finding all of these components can be pretty hard there's nowhere in the docs that",
    "start": "168840",
    "end": "175120"
  },
  {
    "text": "actually has all of this uh written down but basically",
    "start": "175120",
    "end": "180400"
  },
  {
    "text": "there are three main types that are supported CPUs gpus and tpus and",
    "start": "180400",
    "end": "185959"
  },
  {
    "text": "actually fpga is also supported in kubernetes and there's a talk going on right now that is talking about fpga in",
    "start": "185959",
    "end": "193400"
  },
  {
    "text": "kubernetes So check out the recording of that later uh that one is mainly used more in like personal use cases or",
    "start": "193400",
    "end": "199760"
  },
  {
    "text": "on-prem use cases it's not used in Cloud environments really um so we won't be",
    "start": "199760",
    "end": "205080"
  },
  {
    "text": "going into detail on that but check out that other talk if you want to learn about it so first up we're going to talk",
    "start": "205080",
    "end": "211360"
  },
  {
    "text": "about CPUs so CPUs uh a lot of work is",
    "start": "211360",
    "end": "218879"
  },
  {
    "text": "happening behind the scenes there so it's working on looking through memory reading from memory writing from memory",
    "start": "218879",
    "end": "225319"
  },
  {
    "text": "and going on and on there's crazy amounts of what's going on",
    "start": "225319",
    "end": "230760"
  },
  {
    "text": "there so for the general purpose use of CPUs there's a lot of different things that you could um do with it you can WR",
    "start": "232120",
    "end": "240400"
  },
  {
    "text": "code to and have it be processed there and continually improve on your speed of",
    "start": "240400",
    "end": "246680"
  },
  {
    "text": "your messaging and all of that going through every day so seeing",
    "start": "246680",
    "end": "252560"
  },
  {
    "text": "um many different things happening per second but because of the billions of",
    "start": "252560",
    "end": "260160"
  },
  {
    "text": "executions there's not enough of um of those things happening right so we have multiple core CPUs for the purpose of",
    "start": "260160",
    "end": "267479"
  },
  {
    "text": "processing more and they can work work in parallel at the same time um running",
    "start": "267479",
    "end": "272840"
  },
  {
    "text": "all together the CPUs also are receiving a",
    "start": "272840",
    "end": "279280"
  },
  {
    "text": "lot of instruction and doing the basic math to create 3D avatars maybe or um",
    "start": "279280",
    "end": "285880"
  },
  {
    "text": "processing Graphics that way um maybe in eventually sign language who",
    "start": "285880",
    "end": "291759"
  },
  {
    "text": "knows so you can see here this kind of process of setting up um CPUs it's",
    "start": "292120",
    "end": "297479"
  },
  {
    "text": "basically a switch like an onoff switch like a one zero and it tends to then go",
    "start": "297479",
    "end": "303080"
  },
  {
    "text": "from there writing in input to the processor as you can see here and then it decides what's going to be done with",
    "start": "303080",
    "end": "308800"
  },
  {
    "text": "it with that set of instructions that's being passed so then that gets written to memory and then we go through the",
    "start": "308800",
    "end": "315919"
  },
  {
    "text": "control plane to pick up whatever is there in the memory process it and then it keeps going and going and going to",
    "start": "315919",
    "end": "322520"
  },
  {
    "text": "eventually getting to the output so this is well and good and",
    "start": "322520",
    "end": "327680"
  },
  {
    "text": "processors CPUs can do all sorts of General things which is awesome in how our phones and computers generally work",
    "start": "327680",
    "end": "334680"
  },
  {
    "text": "um but there's one little problem so the CPU is basically as Rob said either",
    "start": "334680",
    "end": "340080"
  },
  {
    "text": "doing work or it's not doing work and if it's not doing work then it needs to go figure out what the next instruction is",
    "start": "340080",
    "end": "345560"
  },
  {
    "text": "and that's stored in the memory so every time it has to go back to the memory to figure out what the next instruction is",
    "start": "345560",
    "end": "351280"
  },
  {
    "text": "that takes a lot of time and makes CPUs quite slow and that's what we call the Von noyman",
    "start": "351280",
    "end": "358400"
  },
  {
    "text": "bottleneck so so there's a way that we can adjust our architecture to make this",
    "start": "358400",
    "end": "363639"
  },
  {
    "text": "at least a little bit faster we can take the memory and instead of having it outside of the CPU put it inside of the",
    "start": "363639",
    "end": "369440"
  },
  {
    "text": "CPU the closer it is the faster it's going to be so modern CPUs generally",
    "start": "369440",
    "end": "374880"
  },
  {
    "text": "have multiple cores and each one of those cores um has some memory in it an",
    "start": "374880",
    "end": "380440"
  },
  {
    "text": "L1 and possibly an L2 cache and there can also be an L3 cache within the",
    "start": "380440",
    "end": "386560"
  },
  {
    "text": "multicore CPU so all of these different layers of memory are giving us faster",
    "start": "386560",
    "end": "391960"
  },
  {
    "text": "and faster or less fast and less fast depending on which way you're looking at it memory that the CPU can access to",
    "start": "391960",
    "end": "398840"
  },
  {
    "text": "make them feel faster so when you think about how a CPU",
    "start": "398840",
    "end": "404280"
  },
  {
    "text": "works is like what we talked about a CPU is constantly reading from memory what the next instruction needs to be and",
    "start": "404280",
    "end": "409319"
  },
  {
    "text": "Computing it in this line of code if you have a variable yal WX plus b it has W",
    "start": "409319",
    "end": "415240"
  },
  {
    "text": "and x and B stored in memory so it will multiply W with X add that to B and then stored the whole thing into y if that",
    "start": "415240",
    "end": "422120"
  },
  {
    "text": "thing is running in a loop is have to constantly go back to see what is currently W what is currently x what is currently B in your memory so in the",
    "start": "422120",
    "end": "430479"
  },
  {
    "text": "best case scenario that thing is constantly running and the memory is very close in the L1 cache but sometimes",
    "start": "430479",
    "end": "436280"
  },
  {
    "text": "they're not because CPU also does other things in your computer so if for some reason CPU looks at somewhere else onun",
    "start": "436280",
    "end": "441879"
  },
  {
    "text": "other processes it will have to go back and reload that uh instruction back into its memory so every time it goes and",
    "start": "441879",
    "end": "448599"
  },
  {
    "text": "reads stuff from m memory this is a connection being happening over the memory that creates latency as well as",
    "start": "448599",
    "end": "454479"
  },
  {
    "text": "produces heat to do that calculation over and over again so in kubernetes there are a wide",
    "start": "454479",
    "end": "462479"
  },
  {
    "text": "variety of CPUs that you can use um there are of course single core CPUs but",
    "start": "462479",
    "end": "468639"
  },
  {
    "text": "we rarely see them these days so I've have it marked out here but generally these days you'll see multicore CPUs and",
    "start": "468639",
    "end": "475199"
  },
  {
    "text": "those multicore CPUs can be architected in different ways themselves so there are arm CPUs and x86 and x64 we probably",
    "start": "475199",
    "end": "482840"
  },
  {
    "text": "learned about these in school um and kubernetes supports a wide variety of these architectures here I've got an",
    "start": "482840",
    "end": "489319"
  },
  {
    "text": "example of kubernetes by Justin Garrison um which uses Intel processors and a",
    "start": "489319",
    "end": "495639"
  },
  {
    "text": "Raspberry Pi cluster that Mofi made that uses arm processors so both kinds of CPUs work in",
    "start": "495639",
    "end": "501840"
  },
  {
    "text": "kubernetes so now let's talk about the special type of accelerators that probably a lot of we are hearing in the",
    "start": "501840",
    "end": "507520"
  },
  {
    "text": "last few months to a year uh gpus before you talk about gpus why do we care about",
    "start": "507520",
    "end": "513399"
  },
  {
    "text": "this kind of special AR hardware for AIML workload is a concept called embarrassingly parallel problems so in",
    "start": "513399",
    "end": "519800"
  },
  {
    "text": "computer science there are certain set of problems that can be computed the next iteration without looking at the",
    "start": "519800",
    "end": "525519"
  },
  {
    "text": "previs iteration or other things that are running in parallel these are also called perfectly parallel or pleasantly",
    "start": "525519",
    "end": "530880"
  },
  {
    "text": "parallel I like the other terms rather than embarrassingly parallel it kind of makes me think there is something shameful about being parallel here but",
    "start": "530880",
    "end": "537760"
  },
  {
    "text": "in reality those problems are really nice nice for us to be able to solve with multiple processor doing the work",
    "start": "537760",
    "end": "543040"
  },
  {
    "text": "at the same time so a GPU stands for graphics Processing Unit uh they actually have",
    "start": "543040",
    "end": "548880"
  },
  {
    "text": "been around for a while now they're initially created to process your render your graphics from your video games or",
    "start": "548880",
    "end": "555320"
  },
  {
    "text": "your computer monitor and we use these things to speed up our frame rate on",
    "start": "555320",
    "end": "561000"
  },
  {
    "text": "playing video games but turns out when you're doing things like machine learning workload the actual task of",
    "start": "561000",
    "end": "566560"
  },
  {
    "text": "computing the next uh cycle on your neural network is very similar of you can do multiple of those things in",
    "start": "566560",
    "end": "572360"
  },
  {
    "text": "parallel and compute that at the same time so GPU works really nicely on that kind of",
    "start": "572360",
    "end": "577760"
  },
  {
    "text": "workload so the main concept here is a CPU has a very few really fast cores",
    "start": "577760",
    "end": "583279"
  },
  {
    "text": "like we saw four core architecture you can even buy up to 16 core for a consumer machine in the cloud you can",
    "start": "583279",
    "end": "589240"
  },
  {
    "text": "even get up to few hundred cores in a single VM but what if we just did that but put like a few thousand of them in",
    "start": "589240",
    "end": "595519"
  },
  {
    "text": "the same machine so we could get the fast speed of CPU and do the work in parallel across few thousand of this",
    "start": "595519",
    "end": "601760"
  },
  {
    "text": "course and that's exactly what they did for GPU a single GPU usually have multiple",
    "start": "601760",
    "end": "608000"
  },
  {
    "text": "processing clusters so they put this course in a single processing cluster a processing cluster is made up of bunch",
    "start": "608000",
    "end": "613839"
  },
  {
    "text": "of streaming multiprocessor also known as SMS and each SM will have some L1 caches and L2 caches for the quick",
    "start": "613839",
    "end": "621440"
  },
  {
    "text": "access to memory the problem of going back to the memory for the next in instruction is still a struggle for gpus",
    "start": "621440",
    "end": "627560"
  },
  {
    "text": "to solve that problem they use something called caches within the GPU architecture and the whole processing",
    "start": "627560",
    "end": "633279"
  },
  {
    "text": "cluster communicates with high bandwidth memory so that it can handle a large variety of large volume of data at the",
    "start": "633279",
    "end": "639880"
  },
  {
    "text": "same time so this would be probably a typical archetype of what a GPU structure would look like the greens",
    "start": "639880",
    "end": "646240"
  },
  {
    "text": "here are the actual cores that made up makes up streaming processors which combined to make up a processing cluster",
    "start": "646240",
    "end": "652839"
  },
  {
    "text": "which have a shared L2 cach in between and each of the processors also have L1 cach for quicker memory access and when",
    "start": "652839",
    "end": "659959"
  },
  {
    "text": "the GPU needs to access hard like other data that's not in the cache it will go",
    "start": "659959",
    "end": "665120"
  },
  {
    "text": "to the memory controller to talk to gddr uh memory on the latest iteration of gpus you're going to see things like",
    "start": "665120",
    "end": "671519"
  },
  {
    "text": "gddr5 or gddr 6 memories and each iteration of those memories are faster and faster but also consumes a lot more",
    "start": "671519",
    "end": "677720"
  },
  {
    "text": "energy to produce the information and this is the same image but for the Nvidia h100 now the second",
    "start": "677720",
    "end": "685320"
  },
  {
    "text": "latest GPU from Nvidia like this slide became outdated in two days because Nvidia had their GTC conference like in",
    "start": "685320",
    "end": "692040"
  },
  {
    "text": "Monday but Nvidia 100 the same image that we saw here now scaled up to a lot",
    "start": "692040",
    "end": "697200"
  },
  {
    "text": "bigger because the h100 has a lot more of those green things at the course and between each of them you have the L1",
    "start": "697200",
    "end": "703240"
  },
  {
    "text": "caches between them they have a L2 cach and two of those processing cluster together make up the h100 for the b200",
    "start": "703240",
    "end": "710560"
  },
  {
    "text": "that got announced like two days ago the same structure now got scaled to twice this size right so we're making bigger",
    "start": "710560",
    "end": "716519"
  },
  {
    "text": "and bigger more and more course into the same system to be able to handle more and more data now the same task we saw for the",
    "start": "716519",
    "end": "723959"
  },
  {
    "text": "CPU the calculation of yal WX plus b which by the way is a computation we usually do for mnist which is a model",
    "start": "723959",
    "end": "731360"
  },
  {
    "text": "that can understand hand in data set so for doing the same computation One Core",
    "start": "731360",
    "end": "736600"
  },
  {
    "text": "at a time but now on thousands of those cores we can do the calculation parallell across all of them and collect",
    "start": "736600",
    "end": "743040"
  },
  {
    "text": "the information at the end of it that gives us the output at the same time so the stuff that was happening the",
    "start": "743040",
    "end": "748800"
  },
  {
    "text": "computation time is probably a little bit slower because a single CPU core is probably faster than a single GPU core",
    "start": "748800",
    "end": "755199"
  },
  {
    "text": "but when you can have 5,000 of them working at the same time your actual workload spits",
    "start": "755199",
    "end": "760480"
  },
  {
    "text": "up now let's talk about tpus so tpus are specialized Hardware built by Google",
    "start": "760480",
    "end": "766839"
  },
  {
    "text": "research for doing Matrix computation so TPU stands for tensor processing unit",
    "start": "766839",
    "end": "772680"
  },
  {
    "text": "and this is the definition of a tensor this is from mathworld from.com and it says an nth rank tensor is an m dial",
    "start": "772680",
    "end": "779680"
  },
  {
    "text": "space is a mathematical object that has n indices and M to the power n components that obey certain",
    "start": "779680",
    "end": "785920"
  },
  {
    "text": "transformation rules so who in the audience understood this definition all right uh those of you who",
    "start": "785920",
    "end": "791680"
  },
  {
    "text": "understood please explain to me after because I did not so I'm going to try to explain the definition a little bit in",
    "start": "791680",
    "end": "797680"
  },
  {
    "text": "simpler form so in the rank when what we're talking about the at rank zero uh",
    "start": "797680",
    "end": "803120"
  },
  {
    "text": "a tensor basically is just a number we all know numbers in programming or math at rank one it is an array or vector for",
    "start": "803120",
    "end": "809519"
  },
  {
    "text": "math at rank two is a 2d array an array in an array at rank two plus they're",
    "start": "809519",
    "end": "814680"
  },
  {
    "text": "generally just called the tensor so tensor is the general term for any rank of matrices and turns out in machine",
    "start": "814680",
    "end": "823480"
  },
  {
    "text": "learning and when you're doing Neal networks most of the data is very well uh presented in a tensor form and a",
    "start": "823480",
    "end": "830120"
  },
  {
    "text": "tensor flow or libraries like pytorch under tensors and that's what they compute to figure out the next process",
    "start": "830120",
    "end": "836399"
  },
  {
    "text": "in your neural network so tpus are generally designed to be really good at Computing",
    "start": "836399",
    "end": "843320"
  },
  {
    "text": "tensors so a TPU is a matrix processor designed for neural network workload a thousands of multipliers and adders",
    "start": "843320",
    "end": "849880"
  },
  {
    "text": "connected together in What's called the systolic array architecture what basically it means is it takes input in",
    "start": "849880",
    "end": "855399"
  },
  {
    "text": "one end and it spits out two things one is the multiplication of the previous two input and one is the summation that",
    "start": "855399",
    "end": "860720"
  },
  {
    "text": "is continuously calculates so it creates almost like a pipeline within your system to calculate the information that",
    "start": "860720",
    "end": "866000"
  },
  {
    "text": "you're trying to calculate so a TPU host that is sending data to TPU will send a bunch of data to",
    "start": "866000",
    "end": "873800"
  },
  {
    "text": "the TPU through the infit Q and store that in the high Bandu memory inside the TPU after calculation of is done the",
    "start": "873800",
    "end": "881639"
  },
  {
    "text": "outfit ke will then collect the information for using for later or using on a different infit Q to perform Matrix",
    "start": "881639",
    "end": "889240"
  },
  {
    "text": "operation TPU loads the parameter from hbm into matrix multiplication units those are internal cores of TPU that",
    "start": "889240",
    "end": "896279"
  },
  {
    "text": "does the calculation so if you have some information like previously we had instead of computing one at a time we're",
    "start": "896279",
    "end": "902279"
  },
  {
    "text": "going to just load all the input parameters inside the TP at the same time so all the numbers gets chunked out",
    "start": "902279",
    "end": "908680"
  },
  {
    "text": "and sent to the TP at the same time that's just happened there in the next step we T load the data the other so the",
    "start": "908680",
    "end": "916079"
  },
  {
    "text": "you had WX plus b we just did the X now we're going to do the B that we're going to add to this TPU from the hbm as each",
    "start": "916079",
    "end": "923120"
  },
  {
    "text": "multiplication is executed the result is passed to The Next Step the output is the summation of all multiplic results",
    "start": "923120",
    "end": "930079"
  },
  {
    "text": "and during the whole process the benefit of TPU in this case is that your machine is actually no longer going back to the",
    "start": "930079",
    "end": "936440"
  },
  {
    "text": "memory to for the next instruction because everything was loaded in the beginning so we have all of the numbers",
    "start": "936440",
    "end": "942079"
  },
  {
    "text": "loaded up inside the TPU and the input parameters now all gets pushed into the",
    "start": "942079",
    "end": "947440"
  },
  {
    "text": "TPU calculated both the multiplication and summation at the same time and the pipeline progresses until it reaches the",
    "start": "947440",
    "end": "954319"
  },
  {
    "text": "end of your systolic array and at the end you can collect all the results at the same time so because TPU does not",
    "start": "954319",
    "end": "959959"
  },
  {
    "text": "have to go back and forth to the memory it becomes a lot more efficient in terms of energy usage because it's not reading",
    "start": "959959",
    "end": "965680"
  },
  {
    "text": "Through the Wire back and forth so now let's talk about processing units that exist in the AIML space and",
    "start": "965680",
    "end": "972560"
  },
  {
    "text": "the first one is CPU CPU is kind of like the main unit of compute for any kind of workload and as casin explained before",
    "start": "972560",
    "end": "979399"
  },
  {
    "text": "CPUs are just cgroups and name spaces kubernetes does not really reinvent the wheel with containers it just has apis",
    "start": "979399",
    "end": "986600"
  },
  {
    "text": "to talk to the underlying uh runc and container run time to talk to The Container underneath so if you looked at any",
    "start": "986600",
    "end": "993480"
  },
  {
    "text": "kubernetes yaml definition before you probably have seen some file like this uh yaml is everybody's favorite yes yes",
    "start": "993480",
    "end": "1000440"
  },
  {
    "text": "yes everybody loves yaml uh but in kubernetes we Define this yaml to Define our workload in this case in this case",
    "start": "1000440",
    "end": "1007600"
  },
  {
    "text": "what we're looking at is this block of resource and request which we have defined for memory which is our Ram",
    "start": "1007600",
    "end": "1013759"
  },
  {
    "text": "we're not talking about that as a computer unit per se but the thing we're interested in this case our CPU we're",
    "start": "1013759",
    "end": "1019199"
  },
  {
    "text": "saying I need 250 mcpu what that means we're going to talk about in a",
    "start": "1019199",
    "end": "1024558"
  },
  {
    "text": "second so if you want to know what happens when you click like c c will apply a particular uh yo file like this",
    "start": "1024559",
    "end": "1032720"
  },
  {
    "text": "if you go to our sketch page there is going to be the PDF of the slides you",
    "start": "1032720",
    "end": "1038079"
  },
  {
    "text": "can click this link it goes into a lot more detail exactly what happens but the part we're interested in is how does",
    "start": "1038079",
    "end": "1044798"
  },
  {
    "text": "kubernetes then schedule our workload onto the hardware underneath so when a new node gets attached to our kubernetes",
    "start": "1044799",
    "end": "1051520"
  },
  {
    "text": "cluster it lets the cubet know what kind of Hardware how much CPU and memory it",
    "start": "1051520",
    "end": "1056600"
  },
  {
    "text": "has available for doing work on it API server then learns that information from our cuet and when a new workload comes",
    "start": "1056600",
    "end": "1063640"
  },
  {
    "text": "like a yaml file gets applied to our kubernetes context API server will talk to the scheduler we ask okay I need .25",
    "start": "1063640",
    "end": "1072120"
  },
  {
    "text": "CPU I need 2 GB of RAM what do you have available that I can run this workload",
    "start": "1072120",
    "end": "1077200"
  },
  {
    "text": "on if the scheduler finds something it will then create that pod onto the node",
    "start": "1077200",
    "end": "1082919"
  },
  {
    "text": "a pod is uh multiple containers that have the same name space for network and",
    "start": "1082919",
    "end": "1088600"
  },
  {
    "text": "mount so a pod is just abstraction on a container and once that starts that",
    "start": "1088600",
    "end": "1094960"
  },
  {
    "text": "process will then directly talk to the hardware onto the node and use the resources to run the workload you're",
    "start": "1094960",
    "end": "1100159"
  },
  {
    "text": "trying to run now when you say request and limit basically what happens",
    "start": "1100159",
    "end": "1106120"
  },
  {
    "text": "is to kubernetes API server we ask give me Max of X which is the limit in Y",
    "start": "1106120",
    "end": "1112400"
  },
  {
    "text": "which is the request increment if the scheduler can find a node with resources greater than request it will schedule it",
    "start": "1112400",
    "end": "1119320"
  },
  {
    "text": "on that note if your pod asks for more while it's running and you have limit over that it can restart the particular",
    "start": "1119320",
    "end": "1126480"
  },
  {
    "text": "pod to start with more resources and if it can't get any more uh resources it",
    "start": "1126480",
    "end": "1132280"
  },
  {
    "text": "will fail so if you have run Java workload ever on kubernetes you probably have seen this dreaded om killed on your",
    "start": "1132280",
    "end": "1139000"
  },
  {
    "text": "pod because it asked for more RAM and kubernetes scheduler said I can no longer give you any more RAM so it gets",
    "start": "1139000",
    "end": "1144120"
  },
  {
    "text": "out of memory killed now let's take like a step back and think about what it means when you",
    "start": "1144120",
    "end": "1150000"
  },
  {
    "text": "say something like 025 CPU you can't really go to Best Buy or some hardware store and be like I want a laptop with",
    "start": "1150000",
    "end": "1156640"
  },
  {
    "text": "11.5 CPU that doesn't exist right so in the world of kubernetes when you say something like I want a fractional CPU",
    "start": "1156640",
    "end": "1163679"
  },
  {
    "text": "what that basically means is the scheduler and the uh container runtime with see groups and names spaces is",
    "start": "1163679",
    "end": "1169840"
  },
  {
    "text": "going to limit that particular process from using more than 0.25 CPU cycle",
    "start": "1169840",
    "end": "1175840"
  },
  {
    "text": "every second so it is not necessarily saying I'm going to chunk out and cut out 025 CPU to give to this process it",
    "start": "1175840",
    "end": "1182159"
  },
  {
    "text": "the process still runs on the entire node is just a software level isolation that stops the same process from using",
    "start": "1182159",
    "end": "1189320"
  },
  {
    "text": "more CPU cycle every second so let's try to see that in",
    "start": "1189320",
    "end": "1194440"
  },
  {
    "text": "action and hopefully that will work okay o Recon",
    "start": "1194440",
    "end": "1199480"
  },
  {
    "text": "conference",
    "start": "1199480",
    "end": "1202000"
  },
  {
    "text": "Wi-Fi that is not doing it fantastic OH Close welcome to the live de demo",
    "start": "1206880",
    "end": "1216158"
  },
  {
    "text": "segment okay so the code we're running here it's fairly straightforward but",
    "start": "1219120",
    "end": "1225760"
  },
  {
    "text": "what we're trying to do here is I want to run a go routine is written in go uh but the main idea is I want to run this",
    "start": "1225760",
    "end": "1232200"
  },
  {
    "text": "code in a thread that's going to do a bunch of calculation it's just going to do a multiplication and every time it",
    "start": "1232200",
    "end": "1237240"
  },
  {
    "text": "does it I'm going to count how many times I have done it so if everything goes well at the end of this program",
    "start": "1237240",
    "end": "1242440"
  },
  {
    "text": "running which runs for about 10 seconds I'm going to get an print out that says the calculation ran for 10 seconds and",
    "start": "1242440",
    "end": "1249320"
  },
  {
    "text": "it did that many counts of calculation and what I have",
    "start": "1249320",
    "end": "1255200"
  },
  {
    "text": "here is a kubernetes away okay is a kubernetes cluster that is made up of",
    "start": "1256559",
    "end": "1263159"
  },
  {
    "text": "bunch of not pools",
    "start": "1263159",
    "end": "1267600"
  },
  {
    "text": "okay and the note pools I have there are four different I have a E2 standard which happens to be general purpose uh",
    "start": "1268240",
    "end": "1275320"
  },
  {
    "text": "CPUs has two CPU on this one eight on this one and 32 on this one okay so I'm",
    "start": "1275320",
    "end": "1282799"
  },
  {
    "text": "going to run the workload CU C oh before I have to show the workload so cat job",
    "start": "1282799",
    "end": "1290279"
  },
  {
    "text": "1.o so the job itself is says I have a node selector that selects the particular uh node under Target and I",
    "start": "1290279",
    "end": "1298279"
  },
  {
    "text": "have made the go Application that I had into a container image and I'm giving it a request of 500 uh M core of CPU for",
    "start": "1298279",
    "end": "1305480"
  },
  {
    "text": "each of the job that I'm running and this is the same exact job definition that is targeting different",
    "start": "1305480",
    "end": "1312440"
  },
  {
    "text": "uh note",
    "start": "1312440",
    "end": "1315759"
  },
  {
    "text": "types",
    "start": "1317520",
    "end": "1320520"
  },
  {
    "text": "okay",
    "start": "1326120",
    "end": "1329120"
  },
  {
    "text": "F okay don't do live demos",
    "start": "1335960",
    "end": "1340120"
  },
  {
    "text": "people okay you know what I'm going to abandon",
    "start": "1341440",
    "end": "1347000"
  },
  {
    "text": "that particular demo real quick don't worry there's another",
    "start": "1347000",
    "end": "1352600"
  },
  {
    "text": "one so I'm going to come back to that demo in a second I'm going to continue on the other part and we're going to get to that demo in a second um so basically",
    "start": "1352960",
    "end": "1360760"
  },
  {
    "text": "what was supposed to happen what we're going to show here is that when you run a workload on kubernetes and that asks",
    "start": "1360760",
    "end": "1366240"
  },
  {
    "text": "for how many uh CPU I have in this particular node even though you said we",
    "start": "1366240",
    "end": "1371360"
  },
  {
    "text": "want to give it only like 0.1 CPU or 0.5 CPU because it's running as a process",
    "start": "1371360",
    "end": "1376840"
  },
  {
    "text": "onto the node itself it will actually see all the CPUs that exist in that particular Hardware so what will end up",
    "start": "1376840",
    "end": "1382200"
  },
  {
    "text": "happening is if I have a go routine that says okay spin up as many go routines as possible in this particular node it will",
    "start": "1382200",
    "end": "1388520"
  },
  {
    "text": "try to spin up that many like go routines but because I have a cgroup",
    "start": "1388520",
    "end": "1393919"
  },
  {
    "text": "limit that says don't give this particular process more than 05 CPU your CPU Kel will constantly swap out your",
    "start": "1393919",
    "end": "1401360"
  },
  {
    "text": "work every half a cycle so that even though you are supposed to get like all",
    "start": "1401360",
    "end": "1406600"
  },
  {
    "text": "32 you see all 32 CPU being aailable your workload does not actually get alternative CPU gets constantly swapped",
    "start": "1406600",
    "end": "1412159"
  },
  {
    "text": "out so if you have workload that requires to have constant CPU workload you probably want to give it the full",
    "start": "1412159",
    "end": "1418200"
  },
  {
    "text": "node so if you have really CPU intensive workload although a single node can run multiple pod you probably want to get in",
    "start": "1418200",
    "end": "1424600"
  },
  {
    "text": "the world of running a single pod per node when it requires the whole Hardware underneath okay so we'll come back and",
    "start": "1424600",
    "end": "1432320"
  },
  {
    "text": "see what happened to the demo in a second but CPUs are pretty well understood kubernetes understand CPU",
    "start": "1432320",
    "end": "1437559"
  },
  {
    "text": "from the get-go because that's how kubernetes processes run but gpus and tpus and all the other pus that can",
    "start": "1437559",
    "end": "1442960"
  },
  {
    "text": "exist are not something that is available in every single node so kubernetes core does not and should not",
    "start": "1442960",
    "end": "1449120"
  },
  {
    "text": "care about this other TPU other pus typ types so this is what is known as being out of tree for kubernetes so kubernetes",
    "start": "1449120",
    "end": "1456520"
  },
  {
    "text": "if you go to the kubernetes source code and search for GPU or TPU you're not going to find anything now how is then",
    "start": "1456520",
    "end": "1463679"
  },
  {
    "text": "these things are found and used in kubernetes so scheduler needs to know which node has this housing units a",
    "start": "1463679",
    "end": "1469640"
  },
  {
    "text": "container runtime to be able to talk to this hardware and finally cubble it to know this quote unquote General devices",
    "start": "1469640",
    "end": "1476720"
  },
  {
    "text": "if you want to learn more about how exactly this works there is a great talk from last cucon from David uh I forget",
    "start": "1476720",
    "end": "1483399"
  },
  {
    "text": "the other name and the link is here you can go take a look they go in a lot more detail and I think they're giving a",
    "start": "1483399",
    "end": "1488559"
  },
  {
    "text": "second talk this conference uh even going further about devices but in a",
    "start": "1488559",
    "end": "1494039"
  },
  {
    "text": "nutshell what basically happens is when you have GPU uh drivers at the bottom here",
    "start": "1494039",
    "end": "1499120"
  },
  {
    "text": "um and that GPU driver talks to something called device plug-in so these device plugins are something your pro if",
    "start": "1499120",
    "end": "1505640"
  },
  {
    "text": "you are a hardware provider you will write yourself and that can talk to then cuet which will basically register",
    "start": "1505640",
    "end": "1511240"
  },
  {
    "text": "itself as a named entity in this case of a GPU driver the name could be something like nvidia.com GPU if it's an MD device",
    "start": "1511240",
    "end": "1518960"
  },
  {
    "text": "it will have some different name but most of the cases these days are Nvidia and the API server now knows there is a",
    "start": "1518960",
    "end": "1524799"
  },
  {
    "text": "certain device called nvidia.com GPU now now when a new workload comes to",
    "start": "1524799",
    "end": "1530240"
  },
  {
    "text": "the API server saying I want some resources API server now tells the scheduler that okay I know of a one node",
    "start": "1530240",
    "end": "1537080"
  },
  {
    "text": "that has this nvidia.com GPU available scheduler then will create this pod onto",
    "start": "1537080",
    "end": "1542120"
  },
  {
    "text": "that node and the uh the Pod will then talk to container runtime and runy to",
    "start": "1542120",
    "end": "1548120"
  },
  {
    "text": "create containers onto the node these containers talks to the hardware directly because they're just processes",
    "start": "1548120",
    "end": "1553919"
  },
  {
    "text": "running on that particular VM so a lot of people have concerned about using containers and kubernetes for AML",
    "start": "1553919",
    "end": "1559960"
  },
  {
    "text": "workload because they think they might lose out on some performance because they're going through some sort of virtualization in reality that",
    "start": "1559960",
    "end": "1566320"
  },
  {
    "text": "performance difference between using a container versus directly talking to these devices there shouldn't be any",
    "start": "1566320",
    "end": "1571960"
  },
  {
    "text": "because containers are just processes that are talking directly to the hardware so in the example we saw for",
    "start": "1571960",
    "end": "1579840"
  },
  {
    "text": "CPUs is the very same for gpus and tpus you have resources that you can set with",
    "start": "1579840",
    "end": "1585880"
  },
  {
    "text": "named entity for example for NVIDIA GP these are just nv.com gpus and the",
    "start": "1585880",
    "end": "1590960"
  },
  {
    "text": "number of gpus you'd want in this case I want eight gpus you also have something called node selector that selects a",
    "start": "1590960",
    "end": "1597120"
  },
  {
    "text": "particular node this is to help the scheduler figure out which node it knows to have uh the gpus that we're looking",
    "start": "1597120",
    "end": "1604679"
  },
  {
    "text": "for and these labels are self created so this is something either your cloud provider is attaching to the node poool",
    "start": "1604679",
    "end": "1611440"
  },
  {
    "text": "or you yourself if you know that that that particular node has a particular GPU you can add that yourself uh for",
    "start": "1611440",
    "end": "1617559"
  },
  {
    "text": "this and the same same thing for tpus the example is very similar where you have this google.com/ TPU and that's the",
    "start": "1617559",
    "end": "1623880"
  },
  {
    "text": "name the TPU driver is attaching itself with CU cuet and you have the node uh selected to do the same thing so the",
    "start": "1623880",
    "end": "1631000"
  },
  {
    "text": "second part of the demo which okay great",
    "start": "1631000",
    "end": "1635960"
  },
  {
    "text": "um I'm going to have to go back okay so I have this yaml definition",
    "start": "1637399",
    "end": "1644279"
  },
  {
    "text": "of a TPU job which is going to be something that runs fine-tuning workload",
    "start": "1644279",
    "end": "1649480"
  },
  {
    "text": "on a large language model uh I'm going to try to kick this job off if it wants to work with me this time but even if it",
    "start": "1649480",
    "end": "1655399"
  },
  {
    "text": "doesn't if you want to see how that is working if you come by the Google Booth we have actually a couple of demos running on the screen of that same",
    "start": "1655399",
    "end": "1661399"
  },
  {
    "text": "workload that we can show you how this actually works underneath but another workload I'm running here is an",
    "start": "1661399",
    "end": "1667120"
  },
  {
    "text": "inference workload which is uh serving a large language model to talk to you can you have examples of talking to gemini",
    "start": "1667120",
    "end": "1673200"
  },
  {
    "text": "or talking to Chad jpt we have a very similar example running so this is running a open model called Gemma I'm",
    "start": "1673200",
    "end": "1679480"
  },
  {
    "text": "going to just reload this and I'm going to ask it a",
    "start": "1679480",
    "end": "1683760"
  },
  {
    "text": "question and it should probably know what is kubernetes it's a cubec con so if it doesn't know we should teach it",
    "start": "1684679",
    "end": "1690760"
  },
  {
    "text": "but I asked it what is kubernetes and this is a very small model compared to the bigger ones like a two billion parameter model and it gave it back with",
    "start": "1690760",
    "end": "1698080"
  },
  {
    "text": "something and I'm going to ask it something uh fun uh right a PO",
    "start": "1698080",
    "end": "1704399"
  },
  {
    "text": "about kubernetes for its 10 year",
    "start": "1704399",
    "end": "1710760"
  },
  {
    "text": "anniversary and it's going to come back so the way this is working is I have a pod running on my cluster hopefully it",
    "start": "1710760",
    "end": "1717080"
  },
  {
    "text": "comes back I have a pod running on my cluster O Okay I I can't tell if it's good or bad",
    "start": "1717080",
    "end": "1724200"
  },
  {
    "text": "because I can't read all that that quickly but it looks to be rhyming for the most part so I'm going to guess it's good um so what is happening here is I",
    "start": "1724200",
    "end": "1731279"
  },
  {
    "text": "have a pod running on this cluster Cube CTX GPU I'm switching context and Cube",
    "start": "1731279",
    "end": "1738559"
  },
  {
    "text": "get PF so I have this one pod hopefully comes back running in this cluster called TGI Jamma deployment which has",
    "start": "1738559",
    "end": "1745519"
  },
  {
    "text": "GPU attach which loads up the memory which loads the entire model onto GPU memory and then I have a gradio service",
    "start": "1745519",
    "end": "1752720"
  },
  {
    "text": "that we are talking to to communicate and get information so in terms of like talking to this uh particular GPU bound",
    "start": "1752720",
    "end": "1760159"
  },
  {
    "text": "workload the actual process of doing that is fairly similar to what we already used to we are setting up limits",
    "start": "1760159",
    "end": "1765679"
  },
  {
    "text": "and resources and talking to GPU bound work very similar way we can talk to CPU",
    "start": "1765679",
    "end": "1770760"
  },
  {
    "text": "and memory bound workload with that we're going to move on",
    "start": "1770760",
    "end": "1777080"
  },
  {
    "text": "to yeah sorry that the demo got a little bit away from you this time seems to be the way with live demos right but um in",
    "start": "1777919",
    "end": "1784840"
  },
  {
    "text": "general the cpu's tasks is to write the code CPU runs and understands what",
    "start": "1784840",
    "end": "1790559"
  },
  {
    "text": "you've written there and then writing to memory and going through that with billions of of transactions there with",
    "start": "1790559",
    "end": "1798480"
  },
  {
    "text": "the GPU you can you know play a game with smooth graphics and Avatar",
    "start": "1798480",
    "end": "1804679"
  },
  {
    "text": "generation and all of that is really wonderful for generating those images with",
    "start": "1804679",
    "end": "1811720"
  },
  {
    "text": "repurposing that concept for machine learning it's making the process a lot",
    "start": "1811720",
    "end": "1817200"
  },
  {
    "text": "faster and with that on you know machine learning on GP or on CPUs rather would",
    "start": "1817200",
    "end": "1824200"
  },
  {
    "text": "be days uh to process all of that with this can go down to seconds as you can",
    "start": "1824200",
    "end": "1829640"
  },
  {
    "text": "see so adding that TPU it will be more of a specific use case specific software",
    "start": "1829640",
    "end": "1837200"
  },
  {
    "text": "um tensorflow libraries Etc that can be used in parallel and so processing those",
    "start": "1837200",
    "end": "1843200"
  },
  {
    "text": "Texs will be a lot faster so reasons you should use kubernetes for AIML kubernetes of course",
    "start": "1843200",
    "end": "1850760"
  },
  {
    "text": "generally uses containers and containers are language and framework agnostic they're just a process running on a",
    "start": "1850760",
    "end": "1857200"
  },
  {
    "text": "machine so whichever ever framework and language you use is fine containers generally and also kubernetes are open",
    "start": "1857200",
    "end": "1863799"
  },
  {
    "text": "source which means that they can run on a wide variety of hardware and environments and kubernetes is meant to",
    "start": "1863799",
    "end": "1871200"
  },
  {
    "text": "manage many machines so when you have ai workloads which sometimes need a whole lot of resources and sometimes they",
    "start": "1871200",
    "end": "1877519"
  },
  {
    "text": "don't need as much they're very bursty um kubernetes can be a really good way to make sure that you're using your",
    "start": "1877519",
    "end": "1882679"
  },
  {
    "text": "resources efficiently yeah we have so many varieties of Hardware out there that",
    "start": "1882679",
    "end": "1889039"
  },
  {
    "text": "could be used for very specific use cases and a lot of times um they can run parallel tasks for optimization of",
    "start": "1889039",
    "end": "1896360"
  },
  {
    "text": "hardware and things like that you're already running machine learning processes and it costs time and money so",
    "start": "1896360",
    "end": "1904039"
  },
  {
    "text": "the hardware can make that uh reduce costs and reduce time efficiency and get",
    "start": "1904039",
    "end": "1909880"
  },
  {
    "text": "those uh products out to the marker Market faster so you know that's what we",
    "start": "1909880",
    "end": "1915000"
  },
  {
    "text": "all want right is to get everything faster and get the fast answer out out there so couple of tips and tricks for",
    "start": "1915000",
    "end": "1920720"
  },
  {
    "text": "using CPU so for critical batch worker as I said before often times having that swaps in the process can be very costly",
    "start": "1920720",
    "end": "1927639"
  },
  {
    "text": "so if you're running a lot of batch workloads parall you might want to run uh one pod per node so we have like",
    "start": "1927639",
    "end": "1933919"
  },
  {
    "text": "customers that run all 5,000 not pods or even 15,000 not not pods on gke uh and",
    "start": "1933919",
    "end": "1939080"
  },
  {
    "text": "consider using CPUs with higher clock cycle for more demanding workloads so the most basic CP you can get from your",
    "start": "1939080",
    "end": "1945320"
  },
  {
    "text": "cloud provider are probably the cheaper ones that does pretty well for most kind of workloads but if you want faster",
    "start": "1945320",
    "end": "1951120"
  },
  {
    "text": "clock cycle most Cloud providers or even data centers can offer more expensive faster clock cycle CPUs and finally",
    "start": "1951120",
    "end": "1957960"
  },
  {
    "text": "number of ml workloads can offload to CPU and RAM so if you're running a g GPU and TPU workload with a lot of memory",
    "start": "1957960",
    "end": "1965039"
  },
  {
    "text": "and you give it like a less CPU Sometimes some of the libraries will want to use offload some of the work so",
    "start": "1965039",
    "end": "1970760"
  },
  {
    "text": "you want to use that couple of the things for gpus and tpus is that label your nose appropriately so that you can find them easily and use tains to to",
    "start": "1970760",
    "end": "1978000"
  },
  {
    "text": "stop from non GPU TPU workload to be scheduled onto GPU TPU Hardware because again if you're using up that resource",
    "start": "1978000",
    "end": "1984440"
  },
  {
    "text": "for non-ml workload you're just wasting resources at that point and if you're using a multi note setup if you want you",
    "start": "1984440",
    "end": "1989519"
  },
  {
    "text": "want to try to keep them as closely geographically located as possible so either if you if it's your own data",
    "start": "1989519",
    "end": "1995279"
  },
  {
    "text": "center schedule them appropriately if you are using a cloud provider use their semantics to find out how to get them",
    "start": "1995279",
    "end": "2000679"
  },
  {
    "text": "closer and finally using gpus they have things like multi- instance GPU time slicing and multi-process services to",
    "start": "2000679",
    "end": "2008200"
  },
  {
    "text": "use the same GPU for more workload so that leads us to uh we I",
    "start": "2008200",
    "end": "2015600"
  },
  {
    "text": "know we have a short amount of time left but I'll try to condense this slide a little bit so that leads us to the sustainability portion of things so we",
    "start": "2015600",
    "end": "2021840"
  },
  {
    "text": "want to be able to use you know I'm sure you've seen arms they laptops are being used out in um you know Apple M1 M2 and",
    "start": "2021840",
    "end": "2030279"
  },
  {
    "text": "we've got uh Google Chromebooks as well as um you know service arms so there's a lot of um um benefit to that with our",
    "start": "2030279",
    "end": "2038320"
  },
  {
    "text": "batteries lasting a lot longer so we've got all of the cloud provider you know Amazon Google and other Cloud providers",
    "start": "2038320",
    "end": "2045320"
  },
  {
    "text": "out there that are working really hard to reduce that footprint trying to go a",
    "start": "2045320",
    "end": "2051000"
  },
  {
    "text": "little bit more carbon neutral and so they're adding them to their pillars of well architected Frameworks and various",
    "start": "2051000",
    "end": "2056919"
  },
  {
    "text": "things like that now Google I know has within their UI you could pick a specific region to show you where it's",
    "start": "2056919",
    "end": "2062800"
  },
  {
    "text": "more carbon neutral so that's really nice um and then you know this this is",
    "start": "2062800",
    "end": "2068040"
  },
  {
    "text": "kind of a a lot of information here but we probably have more and more pus going",
    "start": "2068040",
    "end": "2075720"
  },
  {
    "text": "coming in the future so this is just a slide kind of emphasizing that um feel free to check that out",
    "start": "2075720",
    "end": "2081720"
  },
  {
    "text": "later so in review this was a lot of information and I know that you probably",
    "start": "2081720",
    "end": "2086919"
  },
  {
    "text": "weren't able to consume all of it so if there are a few things that you take away I hope that you learned that",
    "start": "2086919",
    "end": "2093398"
  },
  {
    "text": "containers essentially are just cgroups and name spaces and basic are just processes on uh your machine CPUs are",
    "start": "2093399",
    "end": "2101839"
  },
  {
    "text": "great multi-purpose processors while gpus make processing power even stronger",
    "start": "2101839",
    "end": "2108160"
  },
  {
    "text": "through parallelization tpus both do parallelization and they load in all of",
    "start": "2108160",
    "end": "2114240"
  },
  {
    "text": "their instructions all at once to make them faster so that they're not going to memory as often and through the power of",
    "start": "2114240",
    "end": "2121599"
  },
  {
    "text": "device plugins all of these accelerators can work with kubernetes kubernetes",
    "start": "2121599",
    "end": "2126640"
  },
  {
    "text": "doesn't add anything in between it's just enabling you to make use of what's",
    "start": "2126640",
    "end": "2131680"
  },
  {
    "text": "there with the applications that you're running so with that uh that's going to be the end of the talk of course provide",
    "start": "2131680",
    "end": "2138359"
  },
  {
    "text": "your feedback if you want to uh it's going to be on the same search page that you saw the talks in and with that I do",
    "start": "2138359",
    "end": "2144599"
  },
  {
    "text": "we have any time for questions not really but I don't there's nothing after this so if you want to come we're going",
    "start": "2144599",
    "end": "2150680"
  },
  {
    "text": "to be hanging out here and outside if you have any more question about specific task with that thank you for coming to this talk thank you",
    "start": "2150680",
    "end": "2157680"
  },
  {
    "text": "[Applause]",
    "start": "2157680",
    "end": "2163619"
  }
]