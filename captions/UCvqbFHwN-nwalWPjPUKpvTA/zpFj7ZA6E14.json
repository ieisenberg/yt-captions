[
  {
    "text": "all right well thank you all for joining us today um uh this is a discussion of",
    "start": "1599",
    "end": "7040"
  },
  {
    "text": "the a new working group we've created in the kubernetes open source uh Community",
    "start": "7040",
    "end": "12200"
  },
  {
    "text": "uh and we're focused on device management um my name is John bamer I'm from Google",
    "start": "12200",
    "end": "18800"
  },
  {
    "text": "and uh um these are my two co-chairs yeah my name is Patrick Uli I work for",
    "start": "18800",
    "end": "25199"
  },
  {
    "text": "Intel I've been the original author and architect if you want of dynamic resource allocation before it became a",
    "start": "25199",
    "end": "32558"
  },
  {
    "text": "working group I've been driving it forward for a few years now and I'm Kevin Clues I work for NVIDIA uh and I",
    "start": "32559",
    "end": "39840"
  },
  {
    "text": "started working with Patrick in the early days on this as well uh mostly to push forward the Nvidia use cases for",
    "start": "39840",
    "end": "46360"
  },
  {
    "text": "for Dr so what is uh the working group device",
    "start": "46360",
    "end": "53800"
  },
  {
    "text": "management and why are we why do we exist I mean the the the short answer this is our sort of mission statement",
    "start": "53800",
    "end": "58960"
  },
  {
    "text": "it's enabling um simple and efficient configuration sharing and allocation of accelerators",
    "start": "58960",
    "end": "65320"
  },
  {
    "text": "and other specialized devices so this grew out of uh discussions Dr has been worked on for a few years as Patrick was",
    "start": "65320",
    "end": "71479"
  },
  {
    "text": "saying but we realized that we we needed a place in the community um to kind of bring together all the use cases and all",
    "start": "71479",
    "end": "77280"
  },
  {
    "text": "the different vendors and all the different stakeholders so back in um cucon EU um earlier this year uh after",
    "start": "77280",
    "end": "85040"
  },
  {
    "text": "those discussions we decided to form a working group um and the primary thing we're working on is Dr right now and we",
    "start": "85040",
    "end": "91280"
  },
  {
    "text": "may disband after Dr is complete but there are potentially other um areas",
    "start": "91280",
    "end": "97159"
  },
  {
    "text": "that are within scope so we'll make that decision uh in a year or two when we've finished what we're working on um in any",
    "start": "97159",
    "end": "105119"
  },
  {
    "text": "case uh it's a joint effort between a bunch of different sigs and um I do want to emphasize that Although our primary",
    "start": "105119",
    "end": "111600"
  },
  {
    "text": "use cases because of where our uh Market is today is around gpus um this also",
    "start": "111600",
    "end": "118360"
  },
  {
    "text": "includes NX fpgas even network attached devices um mental model is usually cards",
    "start": "118360",
    "end": "125479"
  },
  {
    "text": "in a in a not in a in a in a box but it actually can be other things as",
    "start": "125479",
    "end": "132040"
  },
  {
    "text": "well so as I said Dr is the main thing we've been focusing on and what exactly",
    "start": "132040",
    "end": "138120"
  },
  {
    "text": "is is this Dynamic resource allocation I think the easiest way to conceive of it is sort of these these four parts that",
    "start": "138120",
    "end": "144000"
  },
  {
    "text": "are on the screen here one is an API a new kubernetes API which vendors and uh",
    "start": "144000",
    "end": "150120"
  },
  {
    "text": "can use to describe their devices so rather than just saying this is we have five Nvidia gpus which is how we would",
    "start": "150120",
    "end": "156840"
  },
  {
    "text": "do it say today with Device plug-in um we have a much richer uh API for",
    "start": "156840",
    "end": "162040"
  },
  {
    "text": "describing the devices adding a bunch of metadata and um other information about",
    "start": "162040",
    "end": "167319"
  },
  {
    "text": "about the devices and how they're connected and interconnected within the node um the the flip side of that um is the a",
    "start": "167319",
    "end": "176000"
  },
  {
    "text": "new KU kubernetes API for users to make requests of those devices so this is",
    "start": "176000",
    "end": "181560"
  },
  {
    "text": "really the same pattern we had already in kubernetes with Device plug-in device plug-in published basic information",
    "start": "181560",
    "end": "187560"
  },
  {
    "text": "about the number of devices you had you could use your your your requests and limits to ask for a number of those",
    "start": "187560",
    "end": "193920"
  },
  {
    "text": "devices but the API there was very thin it was basically just a name and account",
    "start": "193920",
    "end": "199640"
  },
  {
    "text": "you know as an extended resource and then maybe node labels um compos the",
    "start": "199640",
    "end": "204799"
  },
  {
    "text": "entire API and it didn't offer a lot of flexibility in the of um devices you",
    "start": "204799",
    "end": "211200"
  },
  {
    "text": "could represent in that model uh nor in sort of the way that you could ask for them so as an example here you can see",
    "start": "211200",
    "end": "218760"
  },
  {
    "text": "rather than asking for you know an Nvidia GPU you can say I want an Nvidia",
    "start": "218760",
    "end": "223879"
  },
  {
    "text": "GPU with at least 30 gigs of memory and that could that request could be fulfilled by a node that's got you know",
    "start": "223879",
    "end": "230439"
  },
  {
    "text": "A1 100s or h100s uh and you don't have to change your your request so part one",
    "start": "230439",
    "end": "236200"
  },
  {
    "text": "is how you describe the devices part two is how you request the devices of course that means we need auler the",
    "start": "236200",
    "end": "242280"
  },
  {
    "text": "other sort of big change we made and in addition to making that API more expressive those two apis we changed the",
    "start": "242280",
    "end": "249879"
  },
  {
    "text": "algorithm so that the choice of which devices get assigned to your pod is done",
    "start": "249879",
    "end": "255360"
  },
  {
    "text": "in the scheduler rather than in the driver on the Node so device plugin makes that decision on the node of which",
    "start": "255360",
    "end": "261479"
  },
  {
    "text": "specific devices are are chosen whereas Dr makes that makes that decision in",
    "start": "261479",
    "end": "267600"
  },
  {
    "text": "theuer and that allows us to add additional logic scoring and satisfy",
    "start": "267600",
    "end": "272759"
  },
  {
    "text": "those sort of under specified requests in part two and then part four of course is we have to actually actuate this we",
    "start": "272759",
    "end": "279680"
  },
  {
    "text": "have to we have to Plum the devices into the pods and containers and so that's",
    "start": "279680",
    "end": "285400"
  },
  {
    "text": "that's requires a new API on the uh on the on the Node",
    "start": "285400",
    "end": "291800"
  },
  {
    "text": "side so I now do have the honor and the pleasure to announce that Dr actually is",
    "start": "291800",
    "end": "299199"
  },
  {
    "text": "beta in 132 the corresponding pool request got",
    "start": "299199",
    "end": "305280"
  },
  {
    "text": "merged middle of last week well before cold freeze that's a new one for me and",
    "start": "305280",
    "end": "310919"
  },
  {
    "text": "it hasn't caused any problems so it just stayed in um what that means in practice",
    "start": "310919",
    "end": "317520"
  },
  {
    "text": "is now two things we are making a promise that this beta API will be in",
    "start": "317520",
    "end": "323199"
  },
  {
    "text": "kubernetes for at least three releases um at some point that's the",
    "start": "323199",
    "end": "328720"
  },
  {
    "text": "guarantee that's the stability guarantee we may keep it a long bit longer before we replace it then it may even be",
    "start": "328720",
    "end": "334199"
  },
  {
    "text": "available beyond that but the key thing is if you have been worried about using Dr because it was Alpha now is the time",
    "start": "334199",
    "end": "341680"
  },
  {
    "text": "to really look at this API and start using and start asking your vendors for support for Dr because it is now",
    "start": "341680",
    "end": "349000"
  },
  {
    "text": "beta um it's not turned on by default",
    "start": "349000",
    "end": "355520"
  },
  {
    "text": "that's another thing that kubernetes does with new API groups they are off by",
    "start": "355520",
    "end": "360560"
  },
  {
    "text": "default so you need to work with your Cloud providers or local deployments to get this feature",
    "start": "360560",
    "end": "366720"
  },
  {
    "text": "enabled but these discussions are now also ongoing gke I think will support it",
    "start": "366720",
    "end": "372840"
  },
  {
    "text": "so we are fine on that front um the feature gate is the other aspect",
    "start": "372840",
    "end": "380319"
  },
  {
    "text": "that you need to enable so there are two things that you need to flip and but that that is now possible the other the",
    "start": "380319",
    "end": "386759"
  },
  {
    "text": "other good thing of going beta is that we are now allowed to backport Buck",
    "start": "386759",
    "end": "391960"
  },
  {
    "text": "fixes so if we find something in master that's broken we can backport to 1321",
    "start": "391960",
    "end": "398199"
  },
  {
    "text": "two three whenever we find an an issue and and fix it also in that stable supported release we couldn't do that",
    "start": "398199",
    "end": "404000"
  },
  {
    "text": "earlier in Alpha because there is a guideline against back porting uh features of buck fixes for Alpha things",
    "start": "404000",
    "end": "411479"
  },
  {
    "text": "so that's a good Milestone to reach and I think it will get give us a good base",
    "start": "411479",
    "end": "417000"
  },
  {
    "text": "to move forward because if we see here on this slide we are not yet done what got merged uh in 132 uh well I mentioned",
    "start": "417000",
    "end": "426400"
  },
  {
    "text": "the beta promotion let's go into more details what actually got promoted it is what we called structured parameters the",
    "start": "426400",
    "end": "434479"
  },
  {
    "text": "API is almost the same as the one as it was in",
    "start": "434479",
    "end": "439680"
  },
  {
    "text": "131 so moving from the alpha to the beta is very simple there's one small field",
    "start": "439680",
    "end": "446840"
  },
  {
    "text": "that changed but it's uh it's Dil you will see it if you if you're writing a Dr driver um we removed one for in what2",
    "start": "446840",
    "end": "456520"
  },
  {
    "text": "the classic Dr that one was kept around because some people were still doing work with it but ultimately we decided",
    "start": "456520",
    "end": "463919"
  },
  {
    "text": "that because it's not supporting cluster Auto scaling we can't support it any further and it doesn't have a good",
    "start": "463919",
    "end": "469440"
  },
  {
    "text": "future incub so it got it got removed um some other things related to Beta",
    "start": "469440",
    "end": "475960"
  },
  {
    "text": "graduation you probably won't notice but there were some change in in API limits",
    "start": "475960",
    "end": "482159"
  },
  {
    "text": "the size of your cell Expressions the cost of cell Expressions these are just basically things that we had to do for",
    "start": "482159",
    "end": "487520"
  },
  {
    "text": "beta it's it's probably not even usable visible so let's ignore that perhaps worth calling out is that scheduling is",
    "start": "487520",
    "end": "494240"
  },
  {
    "text": "quite a bit faster the initial implementation in 131 was really just about doing it correctly very simplistic",
    "start": "494240",
    "end": "502039"
  },
  {
    "text": "and uh with a lot of profiling and looking at bottlenecks uh in some scenarios I was seeing speed up of 16 a",
    "start": "502039",
    "end": "511000"
  },
  {
    "text": "factor of 16 it's not always that much faster but throughput is is certainly better now so if if you had concerns",
    "start": "511000",
    "end": "517560"
  },
  {
    "text": "about performance it might be a good thing to re-evaluate and and check out uh the the 132",
    "start": "517560",
    "end": "523320"
  },
  {
    "text": "implementation so other things that we got into 132 is the first a first thing",
    "start": "523320",
    "end": "528839"
  },
  {
    "text": "uh a cap and an implementation from from someone else we are certainly hoping",
    "start": "528839",
    "end": "534560"
  },
  {
    "text": "more to get more of these caps done by more by a broader Community now that",
    "start": "534560",
    "end": "539640"
  },
  {
    "text": "it's the core is stable um and alongside with that the this one cap was about",
    "start": "539640",
    "end": "546839"
  },
  {
    "text": "resource claim status in particular for network devices so you can post from the",
    "start": "546839",
    "end": "552720"
  },
  {
    "text": "Dr driver the IP address assigned to some some additional network card for",
    "start": "552720",
    "end": "557800"
  },
  {
    "text": "the port um and then the other thing is also for Rel for beta is that we now have a",
    "start": "557800",
    "end": "564640"
  },
  {
    "text": "good plan we have all the code in 132 for the cluster autoscaler to support",
    "start": "564640",
    "end": "569760"
  },
  {
    "text": "Dray and there is a prototype in the autoscaler code itself that uses that so",
    "start": "569760",
    "end": "575760"
  },
  {
    "text": "we we are expecting cluster autoscaler support for Dr shortly after the the 132",
    "start": "575760",
    "end": "581839"
  },
  {
    "text": "release so that's the entry part the out of three part is that we of course need",
    "start": "581839",
    "end": "588200"
  },
  {
    "text": "drra drivers because without drivers Dr doesn't do anything we do have the example driver but we are updating as a",
    "start": "588200",
    "end": "594640"
  },
  {
    "text": "as a reference for potential Dr driver developers uh like the Intel Dr drivers",
    "start": "594640",
    "end": "601839"
  },
  {
    "text": "and the Nvidia D drivers both drivers are currently based on the 141 Alpha API",
    "start": "601839",
    "end": "608800"
  },
  {
    "text": "they still need to be updated to support beta at which point uh they'll probably",
    "start": "608800",
    "end": "615800"
  },
  {
    "text": "only support the beta there is no good way to support Alpha and beta in the",
    "start": "615800",
    "end": "620839"
  },
  {
    "text": "same drivers so you would need we would need to maintain different different relases but that pain is going away",
    "start": "620839",
    "end": "626720"
  },
  {
    "text": "until uh as soon as we all focus on on the beta API I um and this is planned for the Intel",
    "start": "626720",
    "end": "633200"
  },
  {
    "text": "Dr drivers for gpus scudi and qat and as well for for the Nvidia uh",
    "start": "633200",
    "end": "640320"
  },
  {
    "text": "drivers then in progress for the first time is is a a driver that does uh",
    "start": "640320",
    "end": "646200"
  },
  {
    "text": "handles networking and that is under exploration so that's that's a bit less mature than",
    "start": "646200",
    "end": "652680"
  },
  {
    "text": "the other drivers and the same for the Google TPU driver and with that I think",
    "start": "652680",
    "end": "657880"
  },
  {
    "text": "we can move on to next steps sure so so as Patrick said um",
    "start": "657880",
    "end": "664040"
  },
  {
    "text": "we're we're nowhere near done um what we've done in 132 in the beta is what I",
    "start": "664040",
    "end": "670320"
  },
  {
    "text": "would consider a minimum viable product it's it gets us Dr out there it gets us able to do the drivers it gets some",
    "start": "670320",
    "end": "676320"
  },
  {
    "text": "additional functionality beyond what you could do in device plug-in but U there's a ton of work to do and so part of the",
    "start": "676320",
    "end": "684279"
  },
  {
    "text": "idea behind these maintainer track sessions is to encourage people in the community who have use case",
    "start": "684279",
    "end": "690040"
  },
  {
    "text": "uh and who have an interest in a particular topic to come and join us and this is a sample of some of the things",
    "start": "690040",
    "end": "695519"
  },
  {
    "text": "we're working on so we talked about those four four parts right the claim API that's our way to request things",
    "start": "695519",
    "end": "703360"
  },
  {
    "text": "well we have a few ways to request things right now but there's a lot of ideas about other ways you can create um",
    "start": "703360",
    "end": "711959"
  },
  {
    "text": "uh these these requests that either add constraints so one of the constraints we support now is to say align a GPU and a",
    "start": "711959",
    "end": "718360"
  },
  {
    "text": "Nick on the same piece C route but um there are other kind of constraints you might want to request in uh uh across",
    "start": "718360",
    "end": "725959"
  },
  {
    "text": "devices that you're asking for for a pod another way is is sort of a aggregate requests so uh right now you can ask for",
    "start": "725959",
    "end": "734720"
  },
  {
    "text": "a number of devices or all the devices that meet a criteria on a node but we",
    "start": "734720",
    "end": "740560"
  },
  {
    "text": "have use cases or interest we'll see how strong the use case is but we have interest in being able to specify",
    "start": "740560",
    "end": "747240"
  },
  {
    "text": "requests like you know what I'll take any Nvidia GPU or any I'll take anywhere",
    "start": "747240",
    "end": "753160"
  },
  {
    "text": "between one and four Nvidia gpus as long as in aggregate they have more than 120",
    "start": "753160",
    "end": "758399"
  },
  {
    "text": "gigs of memory right so that's one way to UND specify a request and when you have scarce resource availability within",
    "start": "758399",
    "end": "765480"
  },
  {
    "text": "your cluster then that that flexibility can allow the platform to meet your",
    "start": "765480",
    "end": "770959"
  },
  {
    "text": "request uh in multiple ways which makes it more likely you'll get scheduled um",
    "start": "770959",
    "end": "776320"
  },
  {
    "text": "so there's a whole bunch of ideas around that and around aligning native resources so tons of work there",
    "start": "776320",
    "end": "781720"
  },
  {
    "text": "similarly in the publishing of the capacity or the uh",
    "start": "781720",
    "end": "787639"
  },
  {
    "text": "advertising of the devices that's I think was part one in the in the description that's our slice API",
    "start": "787639",
    "end": "793079"
  },
  {
    "text": "resource slice API and today we have a very uh simple model that's basically",
    "start": "793079",
    "end": "798880"
  },
  {
    "text": "here's a list of devices and a bunch of metadata about them but um we have a couple of uh different ideas around how",
    "start": "798880",
    "end": "806120"
  },
  {
    "text": "to model things like uh Nvidia Mig or valid TPU topologies in in Google tpus",
    "start": "806120",
    "end": "812320"
  },
  {
    "text": "so the idea here would be that you can Dynamic you can take a request that says something like I need uh an Nvidia GPU",
    "start": "812320",
    "end": "819959"
  },
  {
    "text": "with four gigs or more and the platform can say oh you know what I I only have",
    "start": "819959",
    "end": "826000"
  },
  {
    "text": "these giant machine giant ones but these are reconfigurable I'll create a new Partition for you and I'll give you that",
    "start": "826000",
    "end": "831399"
  },
  {
    "text": "new Partition so that you're only consuming a small piece of it and that could be done dynamically and heterogeneously across a node um so that",
    "start": "831399",
    "end": "839240"
  },
  {
    "text": "super interesting we we hope to get that into Alpha in 133 we have a whole bunch of similar uh device models that that",
    "start": "839240",
    "end": "845880"
  },
  {
    "text": "that could be interesting um so I won't go into all these things but you see the",
    "start": "845880",
    "end": "850920"
  },
  {
    "text": "point is that we're just really getting started with what we have in 132 and we would love help um I think my next slide",
    "start": "850920",
    "end": "858040"
  },
  {
    "text": "is exactly that yes we would love help to uh help move those things forward um",
    "start": "858040",
    "end": "863480"
  },
  {
    "text": "we have um a bunch of them in process and we're working over the next uh few",
    "start": "863480",
    "end": "870240"
  },
  {
    "text": "weeks and months to prioritize and identify the next things we'll be doing in one in 133 and of course the more of",
    "start": "870240",
    "end": "877399"
  },
  {
    "text": "you that come and join us uh either with use cases not you don't have to re write in code but come and help us inform us",
    "start": "877399",
    "end": "884000"
  },
  {
    "text": "as to what will be useful to you uh to in your workloads um would be super",
    "start": "884000",
    "end": "889399"
  },
  {
    "text": "helpful so there's a PDF of this deck on the skedge and all these links should",
    "start": "889399",
    "end": "894560"
  },
  {
    "text": "work there so uh I'd love it if um if you could come join us",
    "start": "894560",
    "end": "900240"
  },
  {
    "text": "with that uh Kevin is going to dive into and show you a deep dive of how uh how",
    "start": "900240",
    "end": "906160"
  },
  {
    "text": "Dr works with Nvidia gpus yeah so a lot of you may have seen me give",
    "start": "906160",
    "end": "912279"
  },
  {
    "text": "talks on Dr in the past and the different apis that we used to use Dr to both select and configure gpus behind",
    "start": "912279",
    "end": "919160"
  },
  {
    "text": "the scenes once you're given access to them the the purpose of this is really just to show now that we have this",
    "start": "919160",
    "end": "924680"
  },
  {
    "text": "stable API in 132 what does this API now look like because it's changed quite",
    "start": "924680",
    "end": "930639"
  },
  {
    "text": "significantly from The Original Classic Dr and even through some of the different iterations of structured parameters that we had um and as we",
    "start": "930639",
    "end": "938759"
  },
  {
    "text": "talked about at the beginning you know Dr itself is a new way of requesting resources that has been available as an alpha feature since 126 and it's only",
    "start": "938759",
    "end": "945480"
  },
  {
    "text": "now uh reached beta and one thing I always want to highlight is that it provides an alternative to the",
    "start": "945480",
    "end": "950639"
  },
  {
    "text": "count-based interface of for example nvidia.com gpu2 it's not meant as a replacement for the device plug-in API",
    "start": "950639",
    "end": "957000"
  },
  {
    "text": "it's really just an alternative to use it so I see in the future that you know at least for gpus most things will migrate",
    "start": "957000",
    "end": "963800"
  },
  {
    "text": "to um to to a Dr based system but that doesn't that isn't necessarily true for",
    "start": "963800",
    "end": "969240"
  },
  {
    "text": "all types of devices so it's just important to always point out that Dr is meant as an alternative not a replacement um and it really does just",
    "start": "969240",
    "end": "975079"
  },
  {
    "text": "provide a much richer API for requesting and configuring resources uh and it was inspired by the persistent volume API so",
    "start": "975079",
    "end": "981639"
  },
  {
    "text": "for those of you that are new to this and haven't actually seen examples of what Dr looks like if you are uh",
    "start": "981639",
    "end": "987279"
  },
  {
    "text": "familiar with persistent volumes it should look pretty familiar um so just really quickly",
    "start": "987279",
    "end": "995319"
  },
  {
    "text": "before I go through and show some of the yl um for how you make use of Dr both in terms of advertising gpus from the node",
    "start": "995319",
    "end": "1002560"
  },
  {
    "text": "side and then you know requesting them uh on the on the claim side I just want to talk about some of the limitations",
    "start": "1002560",
    "end": "1007920"
  },
  {
    "text": "that Dr um overcomes from what the device plugins used to provide you um",
    "start": "1007920",
    "end": "1013399"
  },
  {
    "text": "one of the big ones is that you can dynamically subdivide large devices uh which was just",
    "start": "1013399",
    "end": "1020040"
  },
  {
    "text": "uh for the most part not possible with the with the standard device plugin um and you can also configure devices individually so there's an API that I'll",
    "start": "1020040",
    "end": "1027079"
  },
  {
    "text": "show a little bit later on that you know in addition to being able to select a a specific device you can also say once",
    "start": "1027079",
    "end": "1033640"
  },
  {
    "text": "that device has been given to you how do you want it configured do you want it configured to be able to share via time slicing do you want it to be able to be",
    "start": "1033640",
    "end": "1039880"
  },
  {
    "text": "configured um sharing with sharing via technology we have called uh MPS or",
    "start": "1039880",
    "end": "1045760"
  },
  {
    "text": "other types of um configuration that that might emerge in the future um um",
    "start": "1045760",
    "end": "1051840"
  },
  {
    "text": "and then yeah just being able to share them in general the the the main thing that Dr gives you that device plugins",
    "start": "1051840",
    "end": "1057000"
  },
  {
    "text": "don't is that there's a separation of declaring what devices you want from actually how you consume them and so you",
    "start": "1057000",
    "end": "1063080"
  },
  {
    "text": "you kind of say here's a here's uh a GPU that I want and then in individual containers or individual um you know",
    "start": "1063080",
    "end": "1070360"
  },
  {
    "text": "containers either within the same pod or across pods you can reference that one single uh device and know that you're",
    "start": "1070360",
    "end": "1076520"
  },
  {
    "text": "going to be sharing it in a controlled way um um and some of the the the",
    "start": "1076520",
    "end": "1082640"
  },
  {
    "text": "foundational new functionality that it that it brings is allowing you to have you know workload specific accelerators",
    "start": "1082640",
    "end": "1088600"
  },
  {
    "text": "uh via the different sharing configurations that you want to apply um specific to Nvidia we have Dynamic Mig",
    "start": "1088600",
    "end": "1094640"
  },
  {
    "text": "uh and TPU use cases that are enabled uh and one of the the brand new features that we never even uh properly solved",
    "start": "1094640",
    "end": "1101559"
  },
  {
    "text": "for in classic Dr is the ability to do alignment of different devices uh using",
    "start": "1101559",
    "end": "1106880"
  },
  {
    "text": "a concept that we call match attribut and in the future we might even have more sophisticated ways of doing this",
    "start": "1106880",
    "end": "1111960"
  },
  {
    "text": "and I'll demonstrate this a little bit later on what exactly I mean by that um and also the consumption of multiple",
    "start": "1111960",
    "end": "1117720"
  },
  {
    "text": "Associated devices at a unit so you know you could imagine building a driver that said you know I know that I always want",
    "start": "1117720",
    "end": "1123919"
  },
  {
    "text": "to bundle this Nick with this GPU so I'll just advertise that as a single device uh and that's what you ask for",
    "start": "1123919",
    "end": "1130480"
  },
  {
    "text": "and that's what you get it's also relevant to to things like making sure that you always get two gpus connected",
    "start": "1130480",
    "end": "1135799"
  },
  {
    "text": "by an NV link you could advertise those as a bundle and then when you request it that's what you get rather than having",
    "start": "1135799",
    "end": "1141440"
  },
  {
    "text": "to request two separate devices and somehow leave it up to the schedule to figure out how to link those",
    "start": "1141440",
    "end": "1147159"
  },
  {
    "text": "together um in terms of advertising resources this is what it kind of looks",
    "start": "1147159",
    "end": "1152320"
  },
  {
    "text": "like uh on the right you have um the the Dr uh resource driver itself uh consists",
    "start": "1152320",
    "end": "1158720"
  },
  {
    "text": "of a demon Set uh of a kuet plugin whose sole not sole purpose but main purpose",
    "start": "1158720",
    "end": "1164360"
  },
  {
    "text": "is to advertise uh this in tree object called a resource slice which enumerates",
    "start": "1164360",
    "end": "1169480"
  },
  {
    "text": "the set of devices uh that you want to be able to request access to inside a um",
    "start": "1169480",
    "end": "1174919"
  },
  {
    "text": "a resource claim later on from within your podspec uh this resource slice is you know at the moment mostly consern",
    "start": "1174919",
    "end": "1181520"
  },
  {
    "text": "consumed by the kubernetes scheduler and it uses this to when it sees a request come in for a device it can consult the",
    "start": "1181520",
    "end": "1188200"
  },
  {
    "text": "resource slices that exist across the cluster pick a node that has a device of the type that you're asking for uh and",
    "start": "1188200",
    "end": "1194799"
  },
  {
    "text": "then schedule your pod to that node um but it's also consumable by uh cluster Auto scalers in the future once we have",
    "start": "1194799",
    "end": "1201600"
  },
  {
    "text": "support for that merge which Patrick alluded to a little bit earlier um on the request side uh this",
    "start": "1201600",
    "end": "1208880"
  },
  {
    "text": "is uh for the most part what it looks like again on the right uh you have the podspec uh and there's a new section",
    "start": "1208880",
    "end": "1215440"
  },
  {
    "text": "inside the uh the the resources stanza from your podspec called claims which",
    "start": "1215440",
    "end": "1221640"
  },
  {
    "text": "points at a separate object called a resource claim which is used to provide all of the selection and configuration",
    "start": "1221640",
    "end": "1228200"
  },
  {
    "text": "criteria for the device that you're trying to get access to um and the resource claim is then you know consumed",
    "start": "1228200",
    "end": "1234640"
  },
  {
    "text": "by both the scheduler and the kuet in order to uh you know um actually do the allocation of",
    "start": "1234640",
    "end": "1241840"
  },
  {
    "text": "the devices to your pod and as well write the the status back um um for the",
    "start": "1241840",
    "end": "1248440"
  },
  {
    "text": "for the KU to consume when the when the time is appropriate for that um so what does this device and",
    "start": "1248440",
    "end": "1255760"
  },
  {
    "text": "numeration look like um this is an example of of what it would look like uh",
    "start": "1255760",
    "end": "1260880"
  },
  {
    "text": "in terms of Nvidia gpus so uh the main thing to note on this is that you know at the top we see that you know this is",
    "start": "1260880",
    "end": "1266840"
  },
  {
    "text": "a resource SCE that we're advertising uh It's associated with some specific driver in this case it's the GPU",
    "start": "1266840",
    "end": "1272640"
  },
  {
    "text": "nvidia.com driver and um what it does is it enumerates a named list of different",
    "start": "1272640",
    "end": "1278760"
  },
  {
    "text": "devices that have a set of attributes and capacities associated with it and the example I'm showing here I'm just",
    "start": "1278760",
    "end": "1283799"
  },
  {
    "text": "going to highlight uh two of those attributes and capacities uh one is the",
    "start": "1283799",
    "end": "1289159"
  },
  {
    "text": "model name for this GPU uh in this example I'm showing a gh2 96 gigabyte uh",
    "start": "1289159",
    "end": "1295360"
  },
  {
    "text": "GPU just just happened to be the machine I was running on when I uh generated this uh and you know underneath the",
    "start": "1295360",
    "end": "1302520"
  },
  {
    "text": "capacity section you see that it has 96 gigabytes of memory um I'm not going to go into the details today of why we",
    "start": "1302520",
    "end": "1309080"
  },
  {
    "text": "separated this notion of attributes and capacities out but from from from at a high level things that you could imagine",
    "start": "1309080",
    "end": "1315240"
  },
  {
    "text": "being consumed uh by an enduser get put into the capacity section and things",
    "start": "1315240",
    "end": "1320440"
  },
  {
    "text": "that um are kind of just generically associated with the device you put in the attributes",
    "start": "1320440",
    "end": "1327760"
  },
  {
    "text": "section um so on the flip side of this uh you know something that's also install installed by the specific driver",
    "start": "1328080",
    "end": "1335480"
  },
  {
    "text": "itself is this notion of a device class and so the device class that I'm showing here um has the name gp. nvidia.com",
    "start": "1335480",
    "end": "1342480"
  },
  {
    "text": "um that's different than the driver name most of the time you'll probably have",
    "start": "1342480",
    "end": "1347640"
  },
  {
    "text": "these two being called the same but they don't have to be called the same um and within that you write uh this cell",
    "start": "1347640",
    "end": "1353880"
  },
  {
    "text": "expression um that defines what it means when you request devices of this class",
    "start": "1353880",
    "end": "1360120"
  },
  {
    "text": "what default properties does that device have to have in order for it to satisfy your request and so in the example that",
    "start": "1360120",
    "end": "1365600"
  },
  {
    "text": "we're seeing here I'm basically just saying that associated with this device class it has to have the driver name of",
    "start": "1365600",
    "end": "1371440"
  },
  {
    "text": "GPU nvidia.com which means that if I have a resource slice associated with gp. nvidia.com any of the device that",
    "start": "1371440",
    "end": "1378840"
  },
  {
    "text": "are listed in that resource slice can be matched by this device class so you know for all intents and purposes with with",
    "start": "1378840",
    "end": "1385200"
  },
  {
    "text": "this one I would have a bunch of gpus gpu0 gpu1 gpu2 and so on and then if a user put a request in for the specific",
    "start": "1385200",
    "end": "1391960"
  },
  {
    "text": "device class the schedule would be free to allocate any of those devices to me if no extra criteria was was",
    "start": "1391960",
    "end": "1399480"
  },
  {
    "text": "specified um this is an example of actually um making use of this from the",
    "start": "1399480",
    "end": "1405799"
  },
  {
    "text": "from the consumer side um so at the top creating a resource claim which I'm calling shared GPU I'm then associating",
    "start": "1405799",
    "end": "1413000"
  },
  {
    "text": "this resource claim with this GPU nvidia.com uh resource class which I",
    "start": "1413000",
    "end": "1418039"
  },
  {
    "text": "just showed on the last slide which you know given the example that I showed would allow the scheduler to allocate",
    "start": "1418039",
    "end": "1423480"
  },
  {
    "text": "any of the devices that were in a slice that was associated with this driver and then at the bottom I have a pod um which",
    "start": "1423480",
    "end": "1430880"
  },
  {
    "text": "has this new section at the bottom called resource claims which has a reference back to that top level",
    "start": "1430880",
    "end": "1436039"
  },
  {
    "text": "resource claim object it has a local name associated with that called GPU and",
    "start": "1436039",
    "end": "1441279"
  },
  {
    "text": "then any of the containers that I want to have access to the shared GPU just reference uh that resource Claim by that",
    "start": "1441279",
    "end": "1447480"
  },
  {
    "text": "local name and under the hood they're getting shared access to the exact same underlying GPU and so you know as I",
    "start": "1447480",
    "end": "1453640"
  },
  {
    "text": "mentioned before one of the main advantages of this over something that like the device plugin could give you is that you get fine grain control of how",
    "start": "1453640",
    "end": "1460240"
  },
  {
    "text": "you share these gpus right um because you know that when you reference that",
    "start": "1460240",
    "end": "1465559"
  },
  {
    "text": "you're actually running on the exact same piece of Hardware that's been bound to that claim um this isn't necessarily just",
    "start": "1465559",
    "end": "1472520"
  },
  {
    "text": "within a single pod you can do this across pods so this you know top level resource claim object can both be",
    "start": "1472520",
    "end": "1477640"
  },
  {
    "text": "referenced in the resource claim section of two separate pods and then you in the containers associated with those pods",
    "start": "1477640",
    "end": "1484120"
  },
  {
    "text": "they just have a reference to the local name and they once again are running on the same underlying uh GPU that's been",
    "start": "1484120",
    "end": "1491440"
  },
  {
    "text": "bound to that claim um I don't show it in in these slides at all but one important thing to note is that resource",
    "start": "1491440",
    "end": "1499039"
  },
  {
    "text": "claims are um within a specific namespace so if you have a resource",
    "start": "1499039",
    "end": "1504159"
  },
  {
    "text": "claim created in one namespace it can't be used in another namespace and so it adds an extra level of um um of security",
    "start": "1504159",
    "end": "1511640"
  },
  {
    "text": "in terms of who actually can access the resource claims that you've had or the gpus that you've had bound to one of the",
    "start": "1511640",
    "end": "1516919"
  },
  {
    "text": "resource claims that you've created um so going back to this example",
    "start": "1516919",
    "end": "1522279"
  },
  {
    "text": "I showed about uh with the device class that has this cell expression saying that you know if you get if you ask for",
    "start": "1522279",
    "end": "1527559"
  },
  {
    "text": "a G GPU from the GPU nvidia.com device class you will get any of the devices",
    "start": "1527559",
    "end": "1533279"
  },
  {
    "text": "that are associated with that um you in addition to you know just having that uh",
    "start": "1533279",
    "end": "1539279"
  },
  {
    "text": "that GPU uh granted to you you can also configure that device that's been given to you in in in a certain way and so um",
    "start": "1539279",
    "end": "1546080"
  },
  {
    "text": "what I'm showing here in the example on the right is that um um at the bottom there you can see that you know I have a",
    "start": "1546080",
    "end": "1553760"
  },
  {
    "text": "uh request uh for GPU nvidia.com the requests name name local name is called",
    "start": "1553760",
    "end": "1559440"
  },
  {
    "text": "GPU and then in a new section called config there is a reference to this uh",
    "start": "1559440",
    "end": "1564880"
  },
  {
    "text": "the GPU that you've been given access to and um a set of configuration uh",
    "start": "1564880",
    "end": "1571279"
  },
  {
    "text": "parameters that can be applied to that GPU so that when it's actually granted to you it's set up in this example um",
    "start": "1571279",
    "end": "1578000"
  },
  {
    "text": "using this technology that we have called MPS to give each client that happens to reference the Shar GPU uh",
    "start": "1578000",
    "end": "1584159"
  },
  {
    "text": "dedicated access to 10 gbt of memory and 20% of active compute on on that GPU and",
    "start": "1584159",
    "end": "1589960"
  },
  {
    "text": "so you can imagine that you know if you have two different um um clients that",
    "start": "1589960",
    "end": "1596200"
  },
  {
    "text": "that that reference this shared GPU they're each going to get 10 gigabytes and there's still um I think this was a",
    "start": "1596200",
    "end": "1602320"
  },
  {
    "text": "96 gigabyte GPU so there're still going to be you know just under 70 gigabytes available to to the",
    "start": "1602320",
    "end": "1609039"
  },
  {
    "text": "others other clients that may attach to it as well um a more complex example uh that",
    "start": "1609039",
    "end": "1616200"
  },
  {
    "text": "that goes through some of this these this new alignment functionality that we have built in um uh I'm going to show",
    "start": "1616200",
    "end": "1624440"
  },
  {
    "text": "now uh in this example it's I'm calling it The Big GPU with an aligned Nick um",
    "start": "1624440",
    "end": "1629840"
  },
  {
    "text": "where I can put in a request for uh a GPU device for the from the driver from the device class GPU nvidia.com and I'm",
    "start": "1629840",
    "end": "1637480"
  },
  {
    "text": "going to add extra selection criteria to that um which says that I want to have",
    "start": "1637480",
    "end": "1643080"
  },
  {
    "text": "not just any GPU from the GPU nvidia.com device class but specifically one that",
    "start": "1643080",
    "end": "1648559"
  },
  {
    "text": "has at least 80 gbt of memory it's a little bit convoluted syntax here but because we're bound to doing this via",
    "start": "1648559",
    "end": "1655120"
  },
  {
    "text": "cell Expressions this is uh how making a request like that would look um",
    "start": "1655120",
    "end": "1661760"
  },
  {
    "text": "additionally I want to put in a separate request within this same claim uh for a",
    "start": "1661760",
    "end": "1667120"
  },
  {
    "text": "a Nick from the hypothetical rd. nvidia.com driver and specifically I",
    "start": "1667120",
    "end": "1672720"
  },
  {
    "text": "want a um uh an RDMA virtual function to be allocated from that",
    "start": "1672720",
    "end": "1679320"
  },
  {
    "text": "and then on top of that I can add this extra section called constraints which says that for the GPU and Nick devices",
    "start": "1679320",
    "end": "1685799"
  },
  {
    "text": "that I've made requests for I want to make sure that they're aligned on the same PCI route complex and so you could",
    "start": "1685799",
    "end": "1691960"
  },
  {
    "text": "imagine that both of these devices were advertised in the resource slice with this exact same attribute sets the same",
    "start": "1691960",
    "end": "1698760"
  },
  {
    "text": "value um uh in that resource slice and so the scheduler can use this information to make sure that when it",
    "start": "1698760",
    "end": "1704640"
  },
  {
    "text": "makes a decision on What GPU and what Nick should I buy to this claim it will make sure that they come uh from devices",
    "start": "1704640",
    "end": "1711279"
  },
  {
    "text": "that have the same uh same value for the attribute that you're matching there",
    "start": "1711279",
    "end": "1718360"
  },
  {
    "text": "um uh so one thing that's kind of interesting to point out is that you know we we talked about having classic",
    "start": "1718360",
    "end": "1724080"
  },
  {
    "text": "Dr versus this new model based on structured parameters um and even when we made that initial switch from classic",
    "start": "1724080",
    "end": "1730440"
  },
  {
    "text": "dra to structured parameters uh I wrote this document called Nvidia GPU use cases for dynamic resource allocation um",
    "start": "1730440",
    "end": "1738240"
  },
  {
    "text": "and we already covered six out of 12 of the initial use cases that we had um control GPU sharing was supported GPU",
    "start": "1738240",
    "end": "1744679"
  },
  {
    "text": "selection via complex constraints multiple GPU types per node you know all the ones that you see listed here but",
    "start": "1744679",
    "end": "1750840"
  },
  {
    "text": "there was still you know half of them weren't weren't supported um in 131 we were actually able to add you know three",
    "start": "1750840",
    "end": "1758039"
  },
  {
    "text": "more of these of these 12 use cases uh and by 133 we plan to have all but one",
    "start": "1758039",
    "end": "1763720"
  },
  {
    "text": "of them supported where the last one is really hard to support in general and probably most likely never will",
    "start": "1763720",
    "end": "1770320"
  },
  {
    "text": "be um but one thing I want to highlight uh just really quickly is a new use case that was kind of um identified a after",
    "start": "1770320",
    "end": "1778320"
  },
  {
    "text": "the point in time when I wrote that initial document is that um with the new gb200 systems that we have coming out",
    "start": "1778320",
    "end": "1784679"
  },
  {
    "text": "there's this notion of something called multinode in vlink associated with them um and in order to make use of multinode",
    "start": "1784679",
    "end": "1790519"
  },
  {
    "text": "in vlink you have to actually allocate a non- node local resource to allow a GPU",
    "start": "1790519",
    "end": "1795720"
  },
  {
    "text": "on one node to securely communicate with with a GPU on another node over this Hive bandwidth in vlink and Dr enables",
    "start": "1795720",
    "end": "1802880"
  },
  {
    "text": "this use case whereas there's no other inry native primitive you know kind of kubernetes native way to do this um and",
    "start": "1802880",
    "end": "1809840"
  },
  {
    "text": "so it's just an interesting use case especially because it's one of the first concrete use cases we have for non- node",
    "start": "1809840",
    "end": "1815039"
  },
  {
    "text": "local resources uh and I'm going to be talking about this at the Google booth on Friday um in conjunction with with a",
    "start": "1815039",
    "end": "1822120"
  },
  {
    "text": "with a bunch of other people um um uh specifically as it pertains to to gke",
    "start": "1822120",
    "end": "1829320"
  },
  {
    "text": "um there's also a bunch of other uh talks that we have uh related to Dr at this conference um there's one later",
    "start": "1829320",
    "end": "1835440"
  },
  {
    "text": "this afternoon um at 3:25 a tale of two drivers GPU reconfiguration on the Fly",
    "start": "1835440",
    "end": "1841279"
  },
  {
    "text": "using Dr it's another some of my colleagues here at Nvidia have um expanded the initial GPU driver that",
    "start": "1841279",
    "end": "1846919"
  },
  {
    "text": "that I wrote uh to add additional to to support additional use cases specifically as it pertains to GeForce",
    "start": "1846919",
    "end": "1853559"
  },
  {
    "text": "now um I'm also giving a talk tomorrow which GPU sharing stry is right for you",
    "start": "1853559",
    "end": "1859000"
  },
  {
    "text": "where we go through a comprehensive Benchmark study to help you figure out how and why you should use a given um",
    "start": "1859000",
    "end": "1864519"
  },
  {
    "text": "GPU sharing strategy using the different techniques I talked about with Dr earlier uh in this talk um there's also",
    "start": "1864519",
    "end": "1871080"
  },
  {
    "text": "a talk on Friday from John and Patrick um Better Together GPU TPU and Nick topology alignment um it kind of walks",
    "start": "1871080",
    "end": "1877679"
  },
  {
    "text": "through in more detail some of the the more details of that last example that I showed where you can do alignment",
    "start": "1877679",
    "end": "1883480"
  },
  {
    "text": "between gpus Nicks and other types of devices uh and then the last one is the the talk is the the booth talk that I",
    "start": "1883480",
    "end": "1889639"
  },
  {
    "text": "just mentioned in the in the previous slide um so yeah uh qar code here on the",
    "start": "1889639",
    "end": "1895120"
  },
  {
    "text": "right uh to give us feedback uh and we're happy to answer any questions in the next uh three to four minutes that",
    "start": "1895120",
    "end": "1901399"
  },
  {
    "text": "we have thank",
    "start": "1901399",
    "end": "1904120"
  },
  {
    "text": "you hi um I'm a potential Dr driver developers and I see can you speak a",
    "start": "1915000",
    "end": "1922200"
  },
  {
    "text": "little more closely to the yeah um I'm potential Dr driver developer maybe I'm",
    "start": "1922200",
    "end": "1927679"
  },
  {
    "text": "going to work on in near future but I'm curious about CDI",
    "start": "1927679",
    "end": "1933880"
  },
  {
    "text": "implementation um Intel no I mean Nvidia uh open source their CDI",
    "start": "1933880",
    "end": "1941159"
  },
  {
    "text": "implementation but I cannot find any other real world example from Googles or",
    "start": "1941159",
    "end": "1947279"
  },
  {
    "text": "intels so there are definitely Intel Dr drivers if you get the CDI CDI okay oh sorry um",
    "start": "1947279",
    "end": "1957679"
  },
  {
    "text": "good you also have CDI stuff right well we are certainly using CDI I mean that",
    "start": "1957679",
    "end": "1963039"
  },
  {
    "text": "that is the underlying technology that we are expecting from the container",
    "start": "1963039",
    "end": "1968600"
  },
  {
    "text": "runtime uh the how it works is with the cupet plug-in the Dr driver gives for",
    "start": "1968600",
    "end": "1974880"
  },
  {
    "text": "cupet a CDI device ID string we are passing that into the Container runtime",
    "start": "1974880",
    "end": "1979919"
  },
  {
    "text": "through CRI and we are expecting that when the container runtime injects those devices",
    "start": "1979919",
    "end": "1987039"
  },
  {
    "text": "or whatever is specified in the CDI file into the uh container and that works",
    "start": "1987039",
    "end": "1993639"
  },
  {
    "text": "with container D and it works with cryo uh support for other run times if if the",
    "start": "1993639",
    "end": "1999120"
  },
  {
    "text": "relevance certainly would be welcome but that's the current Focus that we have your your driver has to understand how",
    "start": "1999120",
    "end": "2004799"
  },
  {
    "text": "to map those device IDs back to whatever it the the the claim and and the the device uh",
    "start": "2004799",
    "end": "2012480"
  },
  {
    "text": "um as it's represented inside your driver yeah okay",
    "start": "2012480",
    "end": "2020120"
  },
  {
    "text": "thanks hi I have a question on the I have a question on the Dr what are",
    "start": "2020919",
    "end": "2028519"
  },
  {
    "text": "your thoughts on Dr for um not CPU topology but mode of back in network",
    "start": "2028519",
    "end": "2034200"
  },
  {
    "text": "topology um with the multiple hierarchies of networking that's going to be introduced in the new",
    "start": "2034200",
    "end": "2040960"
  },
  {
    "text": "architectures so I I couldn't quite follow i c CPU is actually the talk the",
    "start": "2040960",
    "end": "2046440"
  },
  {
    "text": "the first one on there that's today they are using Dr for CPU um uh topology for Network topology",
    "start": "2046440",
    "end": "2055118"
  },
  {
    "text": "is that what you said are you talking about the backend networking like the uh NV link getting into the next switches",
    "start": "2055119",
    "end": "2061358"
  },
  {
    "text": "NV switch Yeah we can represent that as in the in the metadata and and it's certainly something we're interested in",
    "start": "2061359",
    "end": "2067878"
  },
  {
    "text": "so um you know be great if we can talk afterwards or if you want to come to one of our meetings we can have a um uh you",
    "start": "2067879",
    "end": "2073960"
  },
  {
    "text": "can present your use cases and we can we can take a look at it thank you I think we're just about out of time",
    "start": "2073960",
    "end": "2082720"
  },
  {
    "text": "but we can maybe I think we can take the last question please one more question in yeah thank you for the talk it's very",
    "start": "2082720",
    "end": "2089839"
  },
  {
    "text": "interesting I have our questions about alignment features that is very um very interesting to me um you have the",
    "start": "2089839",
    "end": "2097160"
  },
  {
    "text": "constraint to make sure that the for example the N the GPU is in the same um node is that decide their schedulers",
    "start": "2097160",
    "end": "2105040"
  },
  {
    "text": "part to make sure that um you will schedule the uh job or the part to the",
    "start": "2105040",
    "end": "2110200"
  },
  {
    "text": "know that availables for both ni and GPU right and is is their decisions also",
    "start": "2110200",
    "end": "2115839"
  },
  {
    "text": "like um reflect at the time of the scheduling like after the scheduling or",
    "start": "2115839",
    "end": "2121040"
  },
  {
    "text": "for the next job if you never allocate for the same not okay that's just my",
    "start": "2121040",
    "end": "2126079"
  },
  {
    "text": "questions thank you very nice work the schedular tracks the devices and allocates them in Dr in device plug-in",
    "start": "2126079",
    "end": "2132560"
  },
  {
    "text": "it doesn't and so you run into some race conditions potentially in that case but with TR it should work that's great",
    "start": "2132560",
    "end": "2138040"
  },
  {
    "text": "thank you all right yep thank you everyone thanks a lot",
    "start": "2138040",
    "end": "2145079"
  }
]