[
  {
    "text": "okay thank you thank you for having us here so it's a great to see so much people",
    "start": "5060",
    "end": "10559"
  },
  {
    "text": "here um short introduction so we are both software Engineers working at IBM",
    "start": "10559",
    "end": "18199"
  },
  {
    "text": "Germany the our picture of our lab it's kind of has its roots in in Mainframe like",
    "start": "18199",
    "end": "24900"
  },
  {
    "text": "everything in IBM Linux on the Mainframe start as a skunk work project there",
    "start": "24900",
    "end": "31320"
  },
  {
    "text": "and now it's missions from Quantum to Cloud so and we are working",
    "start": "31320",
    "end": "38640"
  },
  {
    "text": "on IBM Cloud code engine and this is where we're here it's kind of a it's based on Native and provides a",
    "start": "38640",
    "end": "47399"
  },
  {
    "text": "simple user experience on top of connective and several other Open Source Products that we are using",
    "start": "47399",
    "end": "55620"
  },
  {
    "text": "so we're providing kind of running your control enable to running your containers in the",
    "start": "55620",
    "end": "62039"
  },
  {
    "text": "cloud easily with also a batch experience",
    "start": "62039",
    "end": "67320"
  },
  {
    "text": "we're using tecton to to deploy source code into the cloud",
    "start": "67320",
    "end": "74280"
  },
  {
    "text": "so for example if we if you know it from cloud Foundry we provide a kind of a",
    "start": "74280",
    "end": "79560"
  },
  {
    "text": "similar push experience for your source code for your projects",
    "start": "79560",
    "end": "86540"
  },
  {
    "text": "so this is kind of a glimpse in the user interface so it's the easy pass is just you type your name of your",
    "start": "87720",
    "end": "93840"
  },
  {
    "text": "application you type your container press create and your service is running and you can access it",
    "start": "93840",
    "end": "100579"
  },
  {
    "text": "secured via https in the cloud I think for this audience the second",
    "start": "100579",
    "end": "106200"
  },
  {
    "text": "thing is probably kind of interesting you can get a cube config using the",
    "start": "106200",
    "end": "112380"
  },
  {
    "text": "command shown Above So to can select your project you're getting a cube config",
    "start": "112380",
    "end": "119340"
  },
  {
    "text": "and then you can use the usual tools you're using so for example you can then KN service",
    "start": "119340",
    "end": "124439"
  },
  {
    "text": "create and you can run your application in the cloud easily",
    "start": "124439",
    "end": "130020"
  },
  {
    "text": "and for that you don't have to care about you clusters at all you don't see it",
    "start": "130020",
    "end": "135959"
  },
  {
    "text": "you just get these experience by getting your namespace where you can run things on",
    "start": "135959",
    "end": "142459"
  },
  {
    "text": "code engine started a while ago 28 in 2020",
    "start": "142980",
    "end": "151140"
  },
  {
    "text": "went better and 20. NGA early 21.",
    "start": "151140",
    "end": "156599"
  },
  {
    "text": "and I think we're using now Connecticut since at longer times since 011 and now on one on one six so we made",
    "start": "156599",
    "end": "164700"
  },
  {
    "text": "a long journey with it and same is for istio as you can see which you use as a service mesh",
    "start": "164700",
    "end": "171060"
  },
  {
    "text": "started also with one five an hour on Feb 115 and at the moment we are also working with some here with the",
    "start": "171060",
    "end": "177120"
  },
  {
    "text": "community on getting a connective conformance certification running",
    "start": "177120",
    "end": "183379"
  },
  {
    "text": "so earthens code engine is running globally I think now in nine data centers it runs",
    "start": "185340",
    "end": "191640"
  },
  {
    "text": "the world with lots of users lots of services we have seen that this use case is kind",
    "start": "191640",
    "end": "198780"
  },
  {
    "text": "of special so you don't get not you don't see lots of blocks or documentation how to run it in a kind of",
    "start": "198780",
    "end": "204780"
  },
  {
    "text": "this High number of services and also the documentation of running things multi-tenant is um",
    "start": "204780",
    "end": "211920"
  },
  {
    "text": "kind of spots so we want to try to to tell something about the things we learned in running",
    "start": "211920",
    "end": "217140"
  },
  {
    "text": "both as aspects the multi-tenant aspect and the as a scaling to these lots of services",
    "start": "217140",
    "end": "225420"
  },
  {
    "text": "and share that with the community hence what what you might can learn on",
    "start": "225420",
    "end": "232080"
  },
  {
    "text": "running both those so I will cover the the first part",
    "start": "232080",
    "end": "237840"
  },
  {
    "text": "which is you it's um using kinetic in a multi-tenant way so sharing one clusters with lots of",
    "start": "237840",
    "end": "244440"
  },
  {
    "text": "users and avoiding that they interfere with each other too much and better not see the other person's",
    "start": "244440",
    "end": "251939"
  },
  {
    "text": "services and data uh we're talking about three aspects why",
    "start": "251939",
    "end": "258660"
  },
  {
    "text": "we did this first is encryption",
    "start": "258660",
    "end": "264180"
  },
  {
    "text": "then we have some Network isolation and also we have to care about the resources we share",
    "start": "264180",
    "end": "270780"
  },
  {
    "text": "uh start with the encryption um we decided to use mtls encryption",
    "start": "270780",
    "end": "278280"
  },
  {
    "text": "provided by our service majestio for all connections for example from the",
    "start": "278280",
    "end": "284040"
  },
  {
    "text": "service to the activator from the activator to the single service",
    "start": "284040",
    "end": "289680"
  },
  {
    "text": "so that the traffic is secured inside the Clusters and separate from",
    "start": "289680",
    "end": "296400"
  },
  {
    "text": "each other and um since though istio is a native",
    "start": "296400",
    "end": "305520"
  },
  {
    "text": "has a number of methods you can choose from when we started we had to make a",
    "start": "305520",
    "end": "310740"
  },
  {
    "text": "decision because we decided that every customer project has its own certificate and at",
    "start": "310740",
    "end": "317160"
  },
  {
    "text": "that point of time at least it narrowed down or narrowed down our choice of service measures I think to",
    "start": "317160",
    "end": "322440"
  },
  {
    "text": "one this was istio um supporting that configuration so if you",
    "start": "322440",
    "end": "328020"
  },
  {
    "text": "we have to configure our Gateway configuration you see here for example for one of our",
    "start": "328020",
    "end": "333840"
  },
  {
    "text": "services that one Service as an example we see each of the projects which is a",
    "start": "333840",
    "end": "341400"
  },
  {
    "text": "namespace in kubernetes language gets its own certificate",
    "start": "341400",
    "end": "346979"
  },
  {
    "text": "we're using let's encrypt there and we configure that here in the in the Gateway file",
    "start": "346979",
    "end": "354139"
  },
  {
    "text": "so we have this slightly with this teacher on this is kind of the lessons learned",
    "start": "357840",
    "end": "363240"
  },
  {
    "text": "um we're not on the way and we might want to share so this is though there's a lot of",
    "start": "363240",
    "end": "368280"
  },
  {
    "text": "service measures around your Tech your requirements",
    "start": "368280",
    "end": "373620"
  },
  {
    "text": "sometimes limited to a very low number in our case at least at this point of time",
    "start": "373620",
    "end": "378960"
  },
  {
    "text": "to once and um interestingly we have to look at this",
    "start": "378960",
    "end": "385919"
  },
  {
    "text": "definition so if you have this Gateway file and we have for each namespace we are supporting with one of those",
    "start": "385919",
    "end": "391860"
  },
  {
    "text": "sections in at least it's somehow gets one of our limitations that we have in the customer",
    "start": "391860",
    "end": "398220"
  },
  {
    "text": "because this file because we have each of these entries repeats the cipher Suites and the GLS information gets",
    "start": "398220",
    "end": "406020"
  },
  {
    "text": "bigger and bigger with each service and in some point of time it will",
    "start": "406020",
    "end": "411720"
  },
  {
    "text": "um extend the limits of for example LCD entry size that you're allowed to do",
    "start": "411720",
    "end": "418680"
  },
  {
    "text": "luckily for us with one is the 115 Envoy gets send defaults so we don't",
    "start": "418680",
    "end": "426600"
  },
  {
    "text": "have to repeat we have to read it much less things but these are the things you have to look on also that's your",
    "start": "426600",
    "end": "433080"
  },
  {
    "text": "um your yaml fights didn't grow too much and you can you're not able to store them anymore",
    "start": "433080",
    "end": "439460"
  },
  {
    "text": "Network of course we have we put Network policies in place to shield namespaces",
    "start": "441120",
    "end": "449039"
  },
  {
    "text": "from each other so one customer cannot talk to um for example to pots and services another",
    "start": "449039",
    "end": "455580"
  },
  {
    "text": "namespace this looked good at the first glance",
    "start": "455580",
    "end": "460919"
  },
  {
    "text": "but then we looked on local Canada services",
    "start": "460919",
    "end": "466800"
  },
  {
    "text": "and found out that this whole concept isn't work",
    "start": "466800",
    "end": "472440"
  },
  {
    "text": "anymore because it's kind of course like the name says it's cluster local",
    "start": "472440",
    "end": "478199"
  },
  {
    "text": "but not namespace local this was kind of okay as long as we only provided public accessible services so",
    "start": "478199",
    "end": "485099"
  },
  {
    "text": "there was this public bus anyhow you can assess it but we also provide a private",
    "start": "485099",
    "end": "491099"
  },
  {
    "text": "access plot to services from the customer's VPC and then that gets a real problem that",
    "start": "491099",
    "end": "496740"
  },
  {
    "text": "you connect something you're not allowed to do and we worked around that I'll show you",
    "start": "496740",
    "end": "504539"
  },
  {
    "text": "actually out of bands of connective and istio by putting filters into Envoy",
    "start": "504539",
    "end": "513240"
  },
  {
    "text": "which is it's specifically to Simply fight code here adds a secret to which outgoing connective local call",
    "start": "513240",
    "end": "521099"
  },
  {
    "text": "and when we receive that call in the same in the namespace we check that this",
    "start": "521099",
    "end": "527160"
  },
  {
    "text": "added Secret is fitting the namespace that it originated from",
    "start": "527160",
    "end": "533839"
  },
  {
    "text": "and of course you make sure that this is not handed out to the customer and we overwrite it so it's",
    "start": "533940",
    "end": "539640"
  },
  {
    "text": "it's a question so Lessons Learned using network policy",
    "start": "539640",
    "end": "546959"
  },
  {
    "text": "is good but not enough",
    "start": "546959",
    "end": "550040"
  },
  {
    "text": "last thing I think just I think Skip over seeing the time um of course we have to if we hand out",
    "start": "552779",
    "end": "559080"
  },
  {
    "text": "resources we have to make sure that one customer cannot take the whole cluster",
    "start": "559080",
    "end": "564540"
  },
  {
    "text": "so we limit things number of apps you can provide the number of revisions because",
    "start": "564540",
    "end": "569940"
  },
  {
    "text": "revisions each service resources IP addresses in the end which are limited",
    "start": "569940",
    "end": "574980"
  },
  {
    "text": "so yeah we have to limit things and make sure that the customers play",
    "start": "574980",
    "end": "580800"
  },
  {
    "text": "well with each other and but you will have some flexibility there to react to customer demands",
    "start": "580800",
    "end": "590100"
  },
  {
    "text": "yes the last thing we had to do is kind of where to massage the things that were",
    "start": "592620",
    "end": "598560"
  },
  {
    "text": "created um I think a good thing to explain is the image pool policies or of course the",
    "start": "598560",
    "end": "605040"
  },
  {
    "text": "best for us and for the for the speed would be if you could",
    "start": "605040",
    "end": "610440"
  },
  {
    "text": "uh take the image that was on the cluster you don't have to do any outside call to",
    "start": "610440",
    "end": "615899"
  },
  {
    "text": "any registry but use that image but it's actually a bad idea in the security problem because the customer",
    "start": "615899",
    "end": "621660"
  },
  {
    "text": "another customer on the cost on the on the cluster could kind of guess the name",
    "start": "621660",
    "end": "628260"
  },
  {
    "text": "of a customer the image of another customer and there would be no Authentication and could leaks just could use that",
    "start": "628260",
    "end": "634620"
  },
  {
    "text": "image so we massage every invisible policy to always",
    "start": "634620",
    "end": "640620"
  },
  {
    "text": "which is not as bad as it sounds because it doesn't actually pull that image if it's already there but only does the",
    "start": "640620",
    "end": "647100"
  },
  {
    "text": "authentication part with the registry so and there's work going on in the the",
    "start": "647100",
    "end": "653160"
  },
  {
    "text": "community or to enhance that part",
    "start": "653160",
    "end": "658380"
  },
  {
    "text": "um so this is always there's something which checks on the image Pro this is driven by the IBM OSS folks",
    "start": "658380",
    "end": "667079"
  },
  {
    "text": "and skip over it well so I think chains and tolerations we we also have",
    "start": "667079",
    "end": "672959"
  },
  {
    "text": "to somehow control so Lessons Learned",
    "start": "672959",
    "end": "678120"
  },
  {
    "text": "um it was purposely if not present is fast but security problem",
    "start": "678120",
    "end": "684120"
  },
  {
    "text": "good thing is always what really uses these connective really provides A fine grain control",
    "start": "684120",
    "end": "689640"
  },
  {
    "text": "by by this capability settings you can do which helps us a lot controlling",
    "start": "689640",
    "end": "697079"
  },
  {
    "text": "something some aspects of connective and with that I'm through the",
    "start": "697079",
    "end": "704940"
  },
  {
    "text": "multi-10 part and normal will take the scaling section thank you Martin",
    "start": "704940",
    "end": "711180"
  },
  {
    "text": "yeah quite maybe questions through that spot Maybe",
    "start": "711180",
    "end": "715519"
  },
  {
    "text": "yeah first of all thank you for the insights that you gave us the one question more",
    "start": "723480",
    "end": "729600"
  },
  {
    "text": "and yeah I love a question would would this be also possible for any kubernetes kinetic installation so like the the",
    "start": "729600",
    "end": "737940"
  },
  {
    "text": "recipe how to set up Network policies and also this extra Android filter there's something that would be helpful",
    "start": "737940",
    "end": "744420"
  },
  {
    "text": "for for the community because I'm asking whether it might make sense at this to the documentation or what a blog post",
    "start": "744420",
    "end": "750360"
  },
  {
    "text": "around that so yes definitely it would make sense yeah we I think we have some blog posts out",
    "start": "750360",
    "end": "756600"
  },
  {
    "text": "in the world about the setup I'm not sure whether we cover these Network policy already",
    "start": "756600",
    "end": "763519"
  },
  {
    "text": "but it might be a good thing because this is asking also we get often and",
    "start": "763519",
    "end": "768779"
  },
  {
    "text": "would be super awesome if we find some yeah yeah thanks a lot you're welcome",
    "start": "768779",
    "end": "776000"
  },
  {
    "text": "no okay good okay phenomenon okay thank you Martin",
    "start": "780480",
    "end": "787260"
  },
  {
    "text": "so after we've secured our tenants and they are isolated from each other the",
    "start": "787260",
    "end": "792600"
  },
  {
    "text": "next thing we will want to do is to actually scale up so I want to give you some insights on",
    "start": "792600",
    "end": "799440"
  },
  {
    "text": "what we do what we change in the native configuration or how we configure K native to scale to the level we are",
    "start": "799440",
    "end": "806519"
  },
  {
    "text": "currently at which is multiple thousand Services per per cluster",
    "start": "806519",
    "end": "811980"
  },
  {
    "text": "and the things I want to talk about are basically three the cold start time so",
    "start": "811980",
    "end": "818160"
  },
  {
    "text": "as you've seen we are using istio as a service mesh which comes with overhead on top",
    "start": "818160",
    "end": "824160"
  },
  {
    "text": "and we want to get that down as much as possible and also some particular configuration",
    "start": "824160",
    "end": "830639"
  },
  {
    "text": "information for k-native and the special section 4K native plus istio what you",
    "start": "830639",
    "end": "837240"
  },
  {
    "text": "may need to be mindful of when you use istio with K native and how",
    "start": "837240",
    "end": "842880"
  },
  {
    "text": "to get the cluster to a sufficient size",
    "start": "842880",
    "end": "848420"
  },
  {
    "text": "so for the cold start time there are basically two things with istio or we",
    "start": "849540",
    "end": "855779"
  },
  {
    "text": "can obviously pre-pool all sidecar images out of band to get basically pre-warm all nodes we have so that the",
    "start": "855779",
    "end": "862980"
  },
  {
    "text": "image does not the sidecar images for istio proxy and Q proxy proxy which we need do not need to be pulled extra on",
    "start": "862980",
    "end": "869880"
  },
  {
    "text": "each node when scaling up but that's actually not the bigger overhead the the second part is the mesh",
    "start": "869880",
    "end": "876660"
  },
  {
    "text": "tuning so most of the overhead when scaling up with running with istio is getting the",
    "start": "876660",
    "end": "883260"
  },
  {
    "text": "service information to All Parts when scaling up and this will be most of the overhead you have when scaling up a surface from zero to",
    "start": "883260",
    "end": "891000"
  },
  {
    "text": "one instance and this is where you can basically configure istio in a way",
    "start": "891000",
    "end": "898500"
  },
  {
    "text": "which I come to in the separate section at the end on what you need to look out for",
    "start": "898500",
    "end": "903899"
  },
  {
    "text": "we also experimented with istio in a traditional way so that the istio proxy",
    "start": "903899",
    "end": "909959"
  },
  {
    "text": "which runs as a sidecar is configuring the IP tables as a",
    "start": "909959",
    "end": "915779"
  },
  {
    "text": "networking layer and basically run it against or performance tested it against",
    "start": "915779",
    "end": "922500"
  },
  {
    "text": "the istiocni plugin but there was no noticeable difference in starter cold",
    "start": "922500",
    "end": "928260"
  },
  {
    "text": "start time the only thing that mattered in the end was the mesh tuning with getting the",
    "start": "928260",
    "end": "934320"
  },
  {
    "text": "push in the discovery information for the services as quickly as possible to the activator that is in the path and to",
    "start": "934320",
    "end": "941220"
  },
  {
    "text": "the service endpoint yeah this is uh the lesson uh there of",
    "start": "941220",
    "end": "948480"
  },
  {
    "text": "the minor overhead of the actual sidecar image pooling both images are pretty are",
    "start": "948480",
    "end": "953699"
  },
  {
    "text": "pulled pretty quickly and even if you do use the cni plugin and configure the IP",
    "start": "953699",
    "end": "960540"
  },
  {
    "text": "tables not in the actual init container of a smart K native application or a",
    "start": "960540",
    "end": "966060"
  },
  {
    "text": "native service it does not matter in the end for the code start time what matters",
    "start": "966060",
    "end": "971579"
  },
  {
    "text": "is the istio state synchronization that needs to happen so the istio control plane needs to get the information what",
    "start": "971579",
    "end": "978660"
  },
  {
    "text": "the activator needs to know what the connective service needs to know and needs to push it there and the receiving",
    "start": "978660",
    "end": "984839"
  },
  {
    "text": "end needs to actually be able to get that in time and set it up",
    "start": "984839",
    "end": "990420"
  },
  {
    "text": "otherwise you will see a lot of Errors when scaling up or scaling down",
    "start": "990420",
    "end": "996680"
  },
  {
    "text": "so from a pure connective perspective we deactivate the HBA for the activator",
    "start": "999240",
    "end": "1007459"
  },
  {
    "text": "why we saw that when we have the activator HPA enabled what will happen is",
    "start": "1007459",
    "end": "1013040"
  },
  {
    "text": "dynamically if it's necessary the activator instances will spin up or down depending on need",
    "start": "1013040",
    "end": "1020120"
  },
  {
    "text": "and what will then happen is the activator service will be changed and the endpoints for all services in",
    "start": "1020120",
    "end": "1026000"
  },
  {
    "text": "the cluster will be changed which means istio needs to pick up all that information",
    "start": "1026000",
    "end": "1031520"
  },
  {
    "text": "and send it out again which is a huge pain that's we need to avoid at all costs",
    "start": "1031520",
    "end": "1039079"
  },
  {
    "text": "that's why we have for our clusters basically a static configuration for the activator at least",
    "start": "1039079",
    "end": "1045380"
  },
  {
    "text": "to limit the scaling or the changing of the activator Service as much as possible",
    "start": "1045380",
    "end": "1050960"
  },
  {
    "text": "so do not overwhelm our cluster with istio service synchronization information",
    "start": "1050960",
    "end": "1056240"
  },
  {
    "text": "the second thing is you see there we are using proxy mode so we have the activator for all services",
    "start": "1056240",
    "end": "1063080"
  },
  {
    "text": "in our cluster always in path it will never be taken out because that again will trigger istio to need to push more",
    "start": "1063080",
    "end": "1070760"
  },
  {
    "text": "information and the one thing is still currently only does it only pushes state of the",
    "start": "1070760",
    "end": "1077419"
  },
  {
    "text": "world information not a Delta so even if one service in your cluster changes",
    "start": "1077419",
    "end": "1083360"
  },
  {
    "text": "it will not send one service if that service is visible by other services it will send every cluster",
    "start": "1083360",
    "end": "1090440"
  },
  {
    "text": "every information to that service which is a lot of information",
    "start": "1090440",
    "end": "1095799"
  },
  {
    "text": "yeah then we added an ha setup for all the K native components so that we have",
    "start": "1096140",
    "end": "1101240"
  },
  {
    "text": "run multiple and different availability zones to BHA with the that's not currently in",
    "start": "1101240",
    "end": "1107960"
  },
  {
    "text": "the current native defaults then another thing to maybe be",
    "start": "1107960",
    "end": "1113120"
  },
  {
    "text": "mindful of the Q sidecar size is defaulted in K native and when you",
    "start": "1113120",
    "end": "1118700"
  },
  {
    "text": "have a smart installed a multiple tenants that have different resource quotas and can adjust",
    "start": "1118700",
    "end": "1123860"
  },
  {
    "text": "those or requests to adjust this might have an impact on the Q side car if you don't pin it to something",
    "start": "1123860",
    "end": "1130880"
  },
  {
    "text": "specific or make it tied to the size of the service itself",
    "start": "1130880",
    "end": "1136880"
  },
  {
    "text": "then the kubernetes API calls that the controller does there are two things the one is you need",
    "start": "1136880",
    "end": "1144200"
  },
  {
    "text": "to increase if you want to scale the QPS and burst they are set pretty low and",
    "start": "1144200",
    "end": "1149960"
  },
  {
    "text": "you should increase it to a sufficient amount and what's currently not possible is the controller we give them",
    "start": "1149960",
    "end": "1158179"
  },
  {
    "text": "without patching more worker threads and we currently have an issue open for discussion if that should be included in",
    "start": "1158179",
    "end": "1164539"
  },
  {
    "text": "Canada because the controller will get a pretty big cue when resyncing and that",
    "start": "1164539",
    "end": "1170840"
  },
  {
    "text": "might be a might cause some hiccup in service provisioning times and revision creation",
    "start": "1170840",
    "end": "1178480"
  },
  {
    "text": "in addition to that we don't want to only Auto scale pots or",
    "start": "1180200",
    "end": "1185539"
  },
  {
    "text": "Auto scale native services for our customers we are also using a standard cluster autoscaler with a provisioning",
    "start": "1185539",
    "end": "1192620"
  },
  {
    "text": "feature on top of it to basically order nodes ahead of time",
    "start": "1192620",
    "end": "1197720"
  },
  {
    "text": "based on workload demand we anticipate and then pre-warm all those nodes that",
    "start": "1197720",
    "end": "1203539"
  },
  {
    "text": "we can get okay native services on that app as quickly as possible",
    "start": "1203539",
    "end": "1209740"
  },
  {
    "text": "yeah and these are basically as summarized again the Lessons Learned we had we want to avoid the state sinking",
    "start": "1212600",
    "end": "1219020"
  },
  {
    "text": "4K native that's based when we when we activate our instances change then we increase the API limits",
    "start": "1219020",
    "end": "1228020"
  },
  {
    "text": "and one future item is we notice that K native is doing a lot of API calls and",
    "start": "1228020",
    "end": "1234860"
  },
  {
    "text": "we notice at least some of them seem to be unnecessary for example we have an issue you will see it in the references",
    "start": "1234860",
    "end": "1241220"
  },
  {
    "text": "section if you want to discuss or take part in the discussion where can native tries to update all the",
    "start": "1241220",
    "end": "1247280"
  },
  {
    "text": "deployments in the cluster when we're thinking even though nothing changes and all that takes time and if you have",
    "start": "1247280",
    "end": "1252860"
  },
  {
    "text": "constant monitoring on your cluster you will see those three things basically as increases in a lot of code",
    "start": "1252860",
    "end": "1259400"
  },
  {
    "text": "start time and provisioning times",
    "start": "1259400",
    "end": "1262720"
  },
  {
    "text": "coming to the the istio section I think what was it K native 1.4 that supported",
    "start": "1264440",
    "end": "1270500"
  },
  {
    "text": "mesh mode so we don't want to or we don't have pot addressability so we always run with",
    "start": "1270500",
    "end": "1277580"
  },
  {
    "text": "mesh mode enable to basically stop from the defaulting behavior that was present where first",
    "start": "1277580",
    "end": "1283280"
  },
  {
    "text": "can native try to go directly to the pot and then default it back to the cluster IP so we're running with mesh mode",
    "start": "1283280",
    "end": "1290840"
  },
  {
    "text": "and as already mentioned the proxy mode we run in so the activator is always in path to avoid",
    "start": "1290840",
    "end": "1297440"
  },
  {
    "text": "reasons on canadif and issue size if that's not present what we saw",
    "start": "1297440",
    "end": "1303020"
  },
  {
    "text": "when scaling out further and further is basically 503s a lot of them if you take",
    "start": "1303020",
    "end": "1308179"
  },
  {
    "text": "the activator back in path when it was out again the activator needs to have service information for",
    "start": "1308179",
    "end": "1314720"
  },
  {
    "text": "that service it's going to be in path4 and if that's not present at that time okay native switches the endpoint but",
    "start": "1314720",
    "end": "1320960"
  },
  {
    "text": "istio does not have did not send the correct information yet the user will see 503s on the end and we",
    "start": "1320960",
    "end": "1327980"
  },
  {
    "text": "want to avoid that another thing is this is just the the",
    "start": "1327980",
    "end": "1334039"
  },
  {
    "text": "standard artifacts when we have a service a configuration and basically a route and the route",
    "start": "1334039",
    "end": "1342380"
  },
  {
    "text": "traffic to different revisions and when we have revisions",
    "start": "1342380",
    "end": "1348020"
  },
  {
    "text": "what will be created by k-native R2 services to cube Services a public one",
    "start": "1348020",
    "end": "1353539"
  },
  {
    "text": "and a private one and they always look like that it's not configurable and this is for an active revision",
    "start": "1353539",
    "end": "1360260"
  },
  {
    "text": "and at least part of that is always necessary what will also happen is if you have",
    "start": "1360260",
    "end": "1366559"
  },
  {
    "text": "users that create multiple revisions create a new revision use traffic management you will have multiple revisions",
    "start": "1366559",
    "end": "1372440"
  },
  {
    "text": "and more importantly sometimes you get to non-active revisions there are defaults where they are",
    "start": "1372440",
    "end": "1378380"
  },
  {
    "text": "retained for some amount of time or you can pin it to a maximum but for a non-active revision K native",
    "start": "1378380",
    "end": "1384919"
  },
  {
    "text": "keeps all artifacts in place there is a deployment there is a replica set there's a port outer scaler there's",
    "start": "1384919",
    "end": "1391400"
  },
  {
    "text": "a serverless service and there are those two Cube services istio does not know that those are not",
    "start": "1391400",
    "end": "1397820"
  },
  {
    "text": "active and those are not rootable East you will send those informations with",
    "start": "1397820",
    "end": "1403940"
  },
  {
    "text": "every push even though this is not necessary",
    "start": "1403940",
    "end": "1408220"
  },
  {
    "text": "so to come to the last part we want to reduce istio mesh",
    "start": "1411080",
    "end": "1416120"
  },
  {
    "text": "synchronization because that takes up a huge amount of CPU and causes all the delays in cold start time",
    "start": "1416120",
    "end": "1423020"
  },
  {
    "text": "and for that as just saw the Canadian garbage collection we also limited it to maximum one non-active revision per",
    "start": "1423020",
    "end": "1429440"
  },
  {
    "text": "customer so or pep so the app all the connective servers can only have",
    "start": "1429440",
    "end": "1435860"
  },
  {
    "text": "one non-active revision this gives each user some kind of safety net to go back",
    "start": "1435860",
    "end": "1441500"
  },
  {
    "text": "if any anything goes wrong but it prevents us from having all those Services lying around that are not",
    "start": "1441500",
    "end": "1447320"
  },
  {
    "text": "actively rooted because we don't want to send that information out and istio supports basically a lot of",
    "start": "1447320",
    "end": "1454520"
  },
  {
    "text": "features I want to concentrate onto and this is the mesh debounce where you",
    "start": "1454520",
    "end": "1460700"
  },
  {
    "text": "can specify amounts of time on how often is your pushes and how to aggregate pushes so that you don't",
    "start": "1460700",
    "end": "1467480"
  },
  {
    "text": "overwhelm all your receivers in the cluster because the issue control plane does nothing else but push and the",
    "start": "1467480",
    "end": "1473120"
  },
  {
    "text": "receivers have to actually handle Network so the istio control player will likely",
    "start": "1473120",
    "end": "1478400"
  },
  {
    "text": "win and overwhelm the receivers with a lot of pushes and we want to avoid those",
    "start": "1478400",
    "end": "1483679"
  },
  {
    "text": "at all cost yeah and that's the summary of that to",
    "start": "1483679",
    "end": "1489620"
  },
  {
    "text": "limit the native garbage collection or at least if you're scaling the cluster be mindful when running with istio what the impact",
    "start": "1489620",
    "end": "1497000"
  },
  {
    "text": "of having non-active revisions will be and to keep an eye on istio settings",
    "start": "1497000",
    "end": "1503840"
  },
  {
    "text": "especially for d-bounds d-bonds after means every push will be delayed by that",
    "start": "1503840",
    "end": "1509720"
  },
  {
    "text": "amount of time that will have a direct impact on provisioning time on revision creation",
    "start": "1509720",
    "end": "1516500"
  },
  {
    "text": "and d-bonds Maxes how long to keep aggregating pushes until you finally send it out",
    "start": "1516500",
    "end": "1523159"
  },
  {
    "text": "and as a future istio does have something in plan to support Delta it",
    "start": "1523159",
    "end": "1529220"
  },
  {
    "text": "pushes that Envoy already supports to get rid of all this unnecessary synchronization",
    "start": "1529220",
    "end": "1536659"
  },
  {
    "text": "effort uh yeah",
    "start": "1536659",
    "end": "1541700"
  },
  {
    "text": "short Outlook from our side for a canative means we want to support custom domains we currently do but not vrk",
    "start": "1541700",
    "end": "1548240"
  },
  {
    "text": "native and we want to switch to the K native support of custom domains we also want to support the prostitute",
    "start": "1548240",
    "end": "1554720"
  },
  {
    "text": "proxy protocol together with istio for enhanced audit auditability",
    "start": "1554720",
    "end": "1560840"
  },
  {
    "text": "and we want to adopt the Delta pushes once available and with that I want to thank well the",
    "start": "1560840",
    "end": "1567740"
  },
  {
    "text": "rest of our team and especially our open technology and developer advocacy team that is doing",
    "start": "1567740",
    "end": "1574279"
  },
  {
    "text": "most of our contributions to Canada if you all know Max Paul and Angelo we are",
    "start": "1574279",
    "end": "1579860"
  },
  {
    "text": "mostly from the operation side and we want to thank the rest of the community for all their contributions",
    "start": "1579860",
    "end": "1585820"
  },
  {
    "text": "and to highlight from all the components you've seen in the beginning that Martin showed you K",
    "start": "1585820",
    "end": "1592039"
  },
  {
    "text": "native is the center part and you saw the size was it's basically the centerpiece of",
    "start": "1592039",
    "end": "1598760"
  },
  {
    "text": "everything together with istio and the yeah the upgrade experience and the",
    "start": "1598760",
    "end": "1603980"
  },
  {
    "text": "stability for K native is is good it's better than most of the other components we use and I want to thank the community",
    "start": "1603980",
    "end": "1610880"
  },
  {
    "text": "for that yeah yeah thank you it warms my heart to hear that",
    "start": "1610880",
    "end": "1618220"
  },
  {
    "text": "yeah there are questions do we have time for okay yeah",
    "start": "1619400",
    "end": "1624940"
  },
  {
    "text": "I have a question um have you experienced any size limitations besides those that you have",
    "start": "1628220",
    "end": "1633679"
  },
  {
    "text": "seen like for example the maximum number of services that you can deploy on a single cluster I'm asking because for",
    "start": "1633679",
    "end": "1638779"
  },
  {
    "text": "example IP tables have a size limit and if you say that a lot of services are",
    "start": "1638779",
    "end": "1644659"
  },
  {
    "text": "not used anymore they all still populate these this kind of",
    "start": "1644659",
    "end": "1650140"
  },
  {
    "text": "data and I wonder whether you have seen some ceiling in with respect to the",
    "start": "1650140",
    "end": "1655760"
  },
  {
    "text": "number of objects that you can deploy on a single cluster actually yeah actually I think we didn't hit a",
    "start": "1655760",
    "end": "1662179"
  },
  {
    "text": "heart limits somewhere yet I think we we knew I think the the nearest thing actually was this Gateway",
    "start": "1662179",
    "end": "1668539"
  },
  {
    "text": "configuration we knew that at some point of time will at least Force us to do something on the database side to to",
    "start": "1668539",
    "end": "1675080"
  },
  {
    "text": "store it yeah but I think our experience is that",
    "start": "1675080",
    "end": "1680360"
  },
  {
    "text": "we are kind of we have a kind of a ongoing fight with with his service mesh to get the data around and connected to",
    "start": "1680360",
    "end": "1687440"
  },
  {
    "text": "that is also this um marshalling and marshalling of this this information",
    "start": "1687440",
    "end": "1692659"
  },
  {
    "text": "center around this is astonishly CPU and memory intensive so you have to also cater for that here",
    "start": "1692659",
    "end": "1699799"
  },
  {
    "text": "so your default sizes will not be enough if you're if you hit it but we have one I think we don't have any hard numbers",
    "start": "1699799",
    "end": "1706100"
  },
  {
    "text": "where the end is I was just going to say great great presentation I just had two questions so",
    "start": "1706100",
    "end": "1712880"
  },
  {
    "text": "with the limits with the istio there um I guess how do you guys handle rollouts",
    "start": "1712880",
    "end": "1719059"
  },
  {
    "text": "were you doing upgrades to istio potentially still running thousands of workloads that could be in polite when",
    "start": "1719059",
    "end": "1724460"
  },
  {
    "text": "you have to do the rollout of those the second thing is with the chatter of istio with XDS",
    "start": "1724460",
    "end": "1729980"
  },
  {
    "text": "so obviously you've had to fine-tune that would that chatter be like reduced significantly with ambient mesh when",
    "start": "1729980",
    "end": "1736520"
  },
  {
    "text": "that comes out that capability rather than running sidecars everywhere so I guess that yeah they're just those",
    "start": "1736520",
    "end": "1741980"
  },
  {
    "text": "two questions yeah upgrade experience so we do upgrade the whole cluster in Flight it's working uh we have had to",
    "start": "1741980",
    "end": "1749900"
  },
  {
    "text": "express massage to not delete the some important things",
    "start": "1749900",
    "end": "1755659"
  },
  {
    "text": "so they have to be careful what you're doing there because we I think we are able to update the old cluster in flight",
    "start": "1755659",
    "end": "1762919"
  },
  {
    "text": "so no turn time for that ambient meshes is the thing I think we will look in because it's exactly uh I",
    "start": "1762919",
    "end": "1769520"
  },
  {
    "text": "think not for this string problem but for the for the resources used in the cluster",
    "start": "1769520",
    "end": "1774679"
  },
  {
    "text": "so if much less sidecars that you have to care for and so it would be in the end less less expensive to to run",
    "start": "1774679",
    "end": "1782240"
  },
  {
    "text": "the whole system yeah but the ambient mesh even with that the control plane still stays the same",
    "start": "1782240",
    "end": "1789679"
  },
  {
    "text": "if not using Delta and it still needs to send all that information so it will maybe help but probably not solve all of",
    "start": "1789679",
    "end": "1797899"
  },
  {
    "text": "the problems we see so far this is a quick question so you",
    "start": "1797899",
    "end": "1804620"
  },
  {
    "text": "mentioned that a lot of your work is focused on getting reducing what istio is pushing out and",
    "start": "1804620",
    "end": "1810320"
  },
  {
    "text": "reducing the number of activation six what sort of trade-off are you making by doing that is there some part of the",
    "start": "1810320",
    "end": "1817460"
  },
  {
    "text": "experience of using K native that you lose by trying to like prevent istio",
    "start": "1817460",
    "end": "1824000"
  },
  {
    "text": "from sending out all that data so it can hit the cold start time that",
    "start": "1824000",
    "end": "1830120"
  },
  {
    "text": "will you need to find a balance between cold start time provisioning time and so",
    "start": "1830120",
    "end": "1835640"
  },
  {
    "text": "or if you're creating new revision or updating your service because those times will take a hit if you try to",
    "start": "1835640",
    "end": "1843200"
  },
  {
    "text": "reduce what istio needs to think so depending on how many services you have in your cluster what rate of changes or",
    "start": "1843200",
    "end": "1850760"
  },
  {
    "text": "how many services are provisioned per minute per second and how many cold starts you have you",
    "start": "1850760",
    "end": "1857480"
  },
  {
    "text": "need to find a balance but yeah there will be a trade-off made if I can add one more bit to that um",
    "start": "1857480",
    "end": "1865100"
  },
  {
    "text": "they turned off a bunch of the auto scaling and things that are efficiency gains",
    "start": "1865100",
    "end": "1870200"
  },
  {
    "text": "to avoid istio stealing all of that back by you know having to do an XDS sync so",
    "start": "1870200",
    "end": "1877220"
  },
  {
    "text": "they turned off auto scaling on the activator and they turned off taking the activator out of the path when you get a",
    "start": "1877220",
    "end": "1883039"
  },
  {
    "text": "big you know when you get a service that's getting a thousand requests per second or something like that the activator will still be",
    "start": "1883039",
    "end": "1889399"
  },
  {
    "text": "in the path so I'd say that resource resource efficiency in some cases",
    "start": "1889399",
    "end": "1895640"
  },
  {
    "text": "they've had to sacrifice in order to exactly if that's the trait we're making that we have this process of course to",
    "start": "1895640",
    "end": "1901460"
  },
  {
    "text": "to have this resources available uh to handle the the traffic we have and the",
    "start": "1901460",
    "end": "1907520"
  },
  {
    "text": "information we have and the marshalling we do so this is the cost we pay",
    "start": "1907520",
    "end": "1915159"
  },
  {
    "text": "um one more question um in terms of advice of running this",
    "start": "1916340",
    "end": "1921500"
  },
  {
    "text": "large cluster are you running one single large cluster with all the customers in one region or you're",
    "start": "1921500",
    "end": "1928159"
  },
  {
    "text": "charting and having multiple clusters maybe some advice for people that want to run at this scale yeah actually we",
    "start": "1928159",
    "end": "1933799"
  },
  {
    "text": "have a shotting solution behind but of course to to reduce your overhead so",
    "start": "1933799",
    "end": "1939260"
  },
  {
    "text": "cluster having sorting classes overhead of the having a control plane and management component on that we try to",
    "start": "1939260",
    "end": "1947299"
  },
  {
    "text": "use the Clusters we have as much as possible so this is still a thing to to",
    "start": "1947299",
    "end": "1953720"
  },
  {
    "text": "get um as most as possible effective running those clusters but we have a sharding",
    "start": "1953720",
    "end": "1961220"
  },
  {
    "text": "concept to kind of adapt to growth and customer numbers that we what we have",
    "start": "1961220",
    "end": "1967898"
  },
  {
    "text": "thank you thank you so much",
    "start": "1975320",
    "end": "1978519"
  }
]