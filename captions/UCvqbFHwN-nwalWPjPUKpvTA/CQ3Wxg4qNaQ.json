[
  {
    "text": "hi",
    "start": "719",
    "end": "2280"
  },
  {
    "text": "everyone thank you all for coming to our",
    "start": "2280",
    "end": "5040"
  },
  {
    "text": "talk",
    "start": "5040",
    "end": "5720"
  },
  {
    "text": "today uh our talk is called scaling",
    "start": "5720",
    "end": "9040"
  },
  {
    "text": "smarter not harder how extending cluster",
    "start": "9040",
    "end": "11519"
  },
  {
    "text": "autoscaler saves",
    "start": "11519",
    "end": "13480"
  },
  {
    "text": "millions my name is Ben and I'm here",
    "start": "13480",
    "end": "15839"
  },
  {
    "text": "with my colleague Rahul and we are both",
    "start": "15839",
    "end": "17760"
  },
  {
    "text": "software engineers at data",
    "start": "17760",
    "end": "20920"
  },
  {
    "text": "dog so we're going to start today by",
    "start": "20920",
    "end": "23119"
  },
  {
    "text": "talking a little bit about how we do",
    "start": "23119",
    "end": "24720"
  },
  {
    "text": "node autoscaling at data dog before",
    "start": "24720",
    "end": "27279"
  },
  {
    "text": "diving into the cluster autoscaler and",
    "start": "27279",
    "end": "29359"
  },
  {
    "text": "the concept of",
    "start": "29359",
    "end": "30840"
  },
  {
    "text": "expanders and then we're going to move",
    "start": "30840",
    "end": "32640"
  },
  {
    "text": "into talking about how we identify",
    "start": "32640",
    "end": "34640"
  },
  {
    "text": "optimal instance types and then how we",
    "start": "34640",
    "end": "36640"
  },
  {
    "text": "scale those optimal instance",
    "start": "36640",
    "end": "39480"
  },
  {
    "text": "types so first a bit about data dog we",
    "start": "39480",
    "end": "43040"
  },
  {
    "text": "run Kubernetes from scratch in a multi",
    "start": "43040",
    "end": "45200"
  },
  {
    "text": "cloud environment we run across dozens",
    "start": "45200",
    "end": "47440"
  },
  {
    "text": "of clusters with tens of thousands of",
    "start": "47440",
    "end": "49520"
  },
  {
    "text": "nodes and hundreds of thousands of pods",
    "start": "49520",
    "end": "52399"
  },
  {
    "text": "and within this infrastructure we serve",
    "start": "52399",
    "end": "54399"
  },
  {
    "text": "trillions of data points per hour for",
    "start": "54399",
    "end": "56320"
  },
  {
    "text": "over 30,000",
    "start": "56320",
    "end": "59120"
  },
  {
    "text": "customers at Data Dog Rahul and I both",
    "start": "59239",
    "end": "61840"
  },
  {
    "text": "work on a team called compute",
    "start": "61840",
    "end": "63960"
  },
  {
    "text": "autoscaling at a high level the goal of",
    "start": "63960",
    "end": "66400"
  },
  {
    "text": "our team is to manage the node",
    "start": "66400",
    "end": "68080"
  },
  {
    "text": "infrastructure for product teams to",
    "start": "68080",
    "end": "70159"
  },
  {
    "text": "enable product teams to focus on product",
    "start": "70159",
    "end": "72080"
  },
  {
    "text": "development while we focus on the node",
    "start": "72080",
    "end": "73840"
  },
  {
    "text": "infrastructure",
    "start": "73840",
    "end": "75600"
  },
  {
    "text": "in doing so we focus on things like",
    "start": "75600",
    "end": "77840"
  },
  {
    "text": "scheduling and scaling efficiency to",
    "start": "77840",
    "end": "79680"
  },
  {
    "text": "deliver nodes as fast as possible as",
    "start": "79680",
    "end": "81840"
  },
  {
    "text": "well as binacking and cost optimizations",
    "start": "81840",
    "end": "83840"
  },
  {
    "text": "across our",
    "start": "83840",
    "end": "86399"
  },
  {
    "text": "fleet so a key offering of our platform",
    "start": "86759",
    "end": "89680"
  },
  {
    "text": "is something that we call a node group",
    "start": "89680",
    "end": "91880"
  },
  {
    "text": "set so first a node group we can think",
    "start": "91880",
    "end": "94880"
  },
  {
    "text": "of as a cloud provider agnostic",
    "start": "94880",
    "end": "96640"
  },
  {
    "text": "representation for something like an",
    "start": "96640",
    "end": "98320"
  },
  {
    "text": "autoscaling group or a manage instance",
    "start": "98320",
    "end": "100079"
  },
  {
    "text": "group if you're familiar with the",
    "start": "100079",
    "end": "101759"
  },
  {
    "text": "cluster autoscaler you might be familiar",
    "start": "101759",
    "end": "103520"
  },
  {
    "text": "with their concept of a node group and",
    "start": "103520",
    "end": "105200"
  },
  {
    "text": "ours is similar but we have a custom",
    "start": "105200",
    "end": "107280"
  },
  {
    "text": "resource definition defined in our",
    "start": "107280",
    "end": "108799"
  },
  {
    "text": "environment that we use for node",
    "start": "108799",
    "end": "111079"
  },
  {
    "text": "groupoups we have one more abstraction",
    "start": "111079",
    "end": "113759"
  },
  {
    "text": "on top of a node group which we call a",
    "start": "113759",
    "end": "115920"
  },
  {
    "text": "node groupoup set a node groupoup set is",
    "start": "115920",
    "end": "118960"
  },
  {
    "text": "a set of node groupoups that falls under",
    "start": "118960",
    "end": "120880"
  },
  {
    "text": "the same scheduling domain so what that",
    "start": "120880",
    "end": "123520"
  },
  {
    "text": "means if an application specifies a",
    "start": "123520",
    "end": "125600"
  },
  {
    "text": "toleration or a node affinity for our",
    "start": "125600",
    "end": "127759"
  },
  {
    "text": "node groupoup set then they could",
    "start": "127759",
    "end": "129360"
  },
  {
    "text": "schedule onto any of the underlying node",
    "start": "129360",
    "end": "131280"
  },
  {
    "text": "groups that would fit their",
    "start": "131280",
    "end": "133560"
  },
  {
    "text": "pod so node groupoup sets are beneficial",
    "start": "133560",
    "end": "136319"
  },
  {
    "text": "for us for a few reasons one of them is",
    "start": "136319",
    "end": "139760"
  },
  {
    "text": "it makes it a lot easier for us to",
    "start": "139760",
    "end": "141200"
  },
  {
    "text": "onboard and manage many users and many",
    "start": "141200",
    "end": "143440"
  },
  {
    "text": "applications across data",
    "start": "143440",
    "end": "145319"
  },
  {
    "text": "dog because they can just set the",
    "start": "145319",
    "end": "147840"
  },
  {
    "text": "specification for the node group set and",
    "start": "147840",
    "end": "149760"
  },
  {
    "text": "then we can change out the underlying",
    "start": "149760",
    "end": "151120"
  },
  {
    "text": "node groups or instance types beneath",
    "start": "151120",
    "end": "152720"
  },
  {
    "text": "them without them changing anything else",
    "start": "152720",
    "end": "154319"
  },
  {
    "text": "on their",
    "start": "154319",
    "end": "156120"
  },
  {
    "text": "application additionally node group sets",
    "start": "156120",
    "end": "158800"
  },
  {
    "text": "are an easy way for us to provide",
    "start": "158800",
    "end": "160160"
  },
  {
    "text": "multiple instance type options so for",
    "start": "160160",
    "end": "162480"
  },
  {
    "text": "example we want to serve a diverse set",
    "start": "162480",
    "end": "164400"
  },
  {
    "text": "of pods with lots of different resource",
    "start": "164400",
    "end": "166560"
  },
  {
    "text": "requests so we can provide large nodes",
    "start": "166560",
    "end": "168239"
  },
  {
    "text": "to serve those or smaller nodes as well",
    "start": "168239",
    "end": "171599"
  },
  {
    "text": "if we were to run out of capacity for",
    "start": "171599",
    "end": "173120"
  },
  {
    "text": "any one instance type we want to be able",
    "start": "173120",
    "end": "174959"
  },
  {
    "text": "to quickly fall back to another instance",
    "start": "174959",
    "end": "176640"
  },
  {
    "text": "type and node group sets give us an easy",
    "start": "176640",
    "end": "178640"
  },
  {
    "text": "way to do",
    "start": "178640",
    "end": "181120"
  },
  {
    "text": "that so for node autoscaling at data dog",
    "start": "181640",
    "end": "184480"
  },
  {
    "text": "we use the cluster autoscaler and for",
    "start": "184480",
    "end": "187200"
  },
  {
    "text": "the sake of time I won't get into too",
    "start": "187200",
    "end": "189440"
  },
  {
    "text": "much detail about why we use the cluster",
    "start": "189440",
    "end": "190800"
  },
  {
    "text": "autoscaler over other autoscaling",
    "start": "190800",
    "end": "192400"
  },
  {
    "text": "alternatives but at a high level one of",
    "start": "192400",
    "end": "195040"
  },
  {
    "text": "the main reasons we use cluster",
    "start": "195040",
    "end": "196080"
  },
  {
    "text": "autoscaler is because it has support for",
    "start": "196080",
    "end": "198480"
  },
  {
    "text": "all the cloud providers that we need",
    "start": "198480",
    "end": "199920"
  },
  {
    "text": "within our infrastructure as well we've",
    "start": "199920",
    "end": "202480"
  },
  {
    "text": "been running cluster autoscaler for a",
    "start": "202480",
    "end": "203760"
  },
  {
    "text": "while now and we're happy with the",
    "start": "203760",
    "end": "205040"
  },
  {
    "text": "operational experience we've been having",
    "start": "205040",
    "end": "206560"
  },
  {
    "text": "in our clusters and have been able to",
    "start": "206560",
    "end": "208239"
  },
  {
    "text": "contribute upstream with features when",
    "start": "208239",
    "end": "209680"
  },
  {
    "text": "we need",
    "start": "209680",
    "end": "211799"
  },
  {
    "text": "them so to dive into how instance type",
    "start": "211799",
    "end": "214159"
  },
  {
    "text": "selection works within the cluster",
    "start": "214159",
    "end": "215480"
  },
  {
    "text": "autoscaler we can say let's say we have",
    "start": "215480",
    "end": "217440"
  },
  {
    "text": "a pending pod that needs to scale up the",
    "start": "217440",
    "end": "220640"
  },
  {
    "text": "autoscaler will first start with all of",
    "start": "220640",
    "end": "222560"
  },
  {
    "text": "the node group",
    "start": "222560",
    "end": "224040"
  },
  {
    "text": "options and then it will filter down to",
    "start": "224040",
    "end": "226400"
  },
  {
    "text": "the node groups that are able to",
    "start": "226400",
    "end": "227519"
  },
  {
    "text": "schedule this pod via scheduling",
    "start": "227519",
    "end": "230040"
  },
  {
    "text": "simulations and then if there are still",
    "start": "230040",
    "end": "231920"
  },
  {
    "text": "multiple node groups that can schedule",
    "start": "231920",
    "end": "233200"
  },
  {
    "text": "the pod it will use the concept of",
    "start": "233200",
    "end": "235760"
  },
  {
    "text": "expanders to pick the best node group to",
    "start": "235760",
    "end": "238120"
  },
  {
    "text": "scale so we can think of an expander as",
    "start": "238120",
    "end": "240720"
  },
  {
    "text": "a strategy to pick the best node group",
    "start": "240720",
    "end": "242560"
  },
  {
    "text": "in order to trigger a scaleup to",
    "start": "242560",
    "end": "244080"
  },
  {
    "text": "schedule the",
    "start": "244080",
    "end": "246560"
  },
  {
    "text": "pot within the cluster autoscaler there",
    "start": "246760",
    "end": "249360"
  },
  {
    "text": "are a few built-in expanders the most",
    "start": "249360",
    "end": "251920"
  },
  {
    "text": "simple one is random which will just",
    "start": "251920",
    "end": "254000"
  },
  {
    "text": "select a random node group another",
    "start": "254000",
    "end": "256799"
  },
  {
    "text": "option is the least waste expander which",
    "start": "256799",
    "end": "258959"
  },
  {
    "text": "will select the node group that will",
    "start": "258959",
    "end": "260160"
  },
  {
    "text": "waste the fewest amount of resources",
    "start": "260160",
    "end": "261600"
  },
  {
    "text": "when fitting those",
    "start": "261600",
    "end": "263320"
  },
  {
    "text": "pods another is price which selects the",
    "start": "263320",
    "end": "266080"
  },
  {
    "text": "cheapest node group option another one",
    "start": "266080",
    "end": "268160"
  },
  {
    "text": "is priority where users can assign",
    "start": "268160",
    "end": "270080"
  },
  {
    "text": "specific priorities to node groupoups",
    "start": "270080",
    "end": "271759"
  },
  {
    "text": "and the cluster autoscaler will respect",
    "start": "271759",
    "end": "273280"
  },
  {
    "text": "those priorities",
    "start": "273280",
    "end": "275120"
  },
  {
    "text": "you can also stack expanders meaning",
    "start": "275120",
    "end": "277360"
  },
  {
    "text": "that you can specify one and then",
    "start": "277360",
    "end": "279040"
  },
  {
    "text": "another for example if I specify the",
    "start": "279040",
    "end": "280880"
  },
  {
    "text": "least waste expander and there's still a",
    "start": "280880",
    "end": "282720"
  },
  {
    "text": "tie it would fall back to the random",
    "start": "282720",
    "end": "284880"
  },
  {
    "text": "expander and pick randomly to ensure",
    "start": "284880",
    "end": "286960"
  },
  {
    "text": "there's always one",
    "start": "286960",
    "end": "289600"
  },
  {
    "text": "choice so for us as a reminder one of",
    "start": "289720",
    "end": "292960"
  },
  {
    "text": "our goals is to optimize for bin packing",
    "start": "292960",
    "end": "295360"
  },
  {
    "text": "efficiency and reduce waste so we",
    "start": "295360",
    "end": "297120"
  },
  {
    "text": "thought the least waste expander would",
    "start": "297120",
    "end": "298560"
  },
  {
    "text": "be a good choice for us",
    "start": "298560",
    "end": "301040"
  },
  {
    "text": "so to dive into a bit of a case study we",
    "start": "301040",
    "end": "303120"
  },
  {
    "text": "can see we have a node group set here",
    "start": "303120",
    "end": "304400"
  },
  {
    "text": "that's very simple offering one 16 core",
    "start": "304400",
    "end": "306560"
  },
  {
    "text": "machine and one 8 core machine and if a",
    "start": "306560",
    "end": "309440"
  },
  {
    "text": "pod comes along requesting six cores the",
    "start": "309440",
    "end": "311520"
  },
  {
    "text": "least waste waste expander will tell us",
    "start": "311520",
    "end": "313759"
  },
  {
    "text": "that we should scale up the 8 core",
    "start": "313759",
    "end": "315360"
  },
  {
    "text": "machine and this pod will",
    "start": "315360",
    "end": "317560"
  },
  {
    "text": "schedule now if another pod comes along",
    "start": "317560",
    "end": "320000"
  },
  {
    "text": "requesting nine cores well now that",
    "start": "320000",
    "end": "322080"
  },
  {
    "text": "can't fit on the 8 core machine so we",
    "start": "322080",
    "end": "323680"
  },
  {
    "text": "need to scale up a 16 core machine",
    "start": "323680",
    "end": "326479"
  },
  {
    "text": "and we can quickly see that in this case",
    "start": "326479",
    "end": "328320"
  },
  {
    "text": "we now have nine wasted",
    "start": "328320",
    "end": "331080"
  },
  {
    "text": "cores so in our optimal scenario we",
    "start": "331080",
    "end": "334000"
  },
  {
    "text": "would have been packed both of these",
    "start": "334000",
    "end": "335199"
  },
  {
    "text": "pods onto a single 16 core machine and",
    "start": "335199",
    "end": "337520"
  },
  {
    "text": "now only have one wasted",
    "start": "337520",
    "end": "339320"
  },
  {
    "text": "core so there are a few ways that we",
    "start": "339320",
    "end": "341440"
  },
  {
    "text": "could get to this you might notice that",
    "start": "341440",
    "end": "343520"
  },
  {
    "text": "if the nore pod came first we would have",
    "start": "343520",
    "end": "345919"
  },
  {
    "text": "scaled up a 16 core machine and then the",
    "start": "345919",
    "end": "347680"
  },
  {
    "text": "six core pod could have fit on it",
    "start": "347680",
    "end": "349919"
  },
  {
    "text": "similarly we could rely on repacking",
    "start": "349919",
    "end": "352000"
  },
  {
    "text": "with the cluster autoscaler in order to",
    "start": "352000",
    "end": "353759"
  },
  {
    "text": "pack these pods together after the fact",
    "start": "353759",
    "end": "356560"
  },
  {
    "text": "however this example still gave us",
    "start": "356560",
    "end": "359199"
  },
  {
    "text": "motivation to take a look at our fleet",
    "start": "359199",
    "end": "360960"
  },
  {
    "text": "in practice to figure out if this",
    "start": "360960",
    "end": "362639"
  },
  {
    "text": "theoretical bin packing optimization was",
    "start": "362639",
    "end": "366039"
  },
  {
    "text": "possible so we took a look at all of our",
    "start": "366039",
    "end": "368080"
  },
  {
    "text": "node group sets in all of our clusters",
    "start": "368080",
    "end": "369680"
  },
  {
    "text": "and specifically at our requested CPU",
    "start": "369680",
    "end": "371759"
  },
  {
    "text": "percent for these node groupoup sets and",
    "start": "371759",
    "end": "374560"
  },
  {
    "text": "we realized that while we're pretty good",
    "start": "374560",
    "end": "376160"
  },
  {
    "text": "about binacking in many of our node",
    "start": "376160",
    "end": "377440"
  },
  {
    "text": "groupoup sets there's clear opportunity",
    "start": "377440",
    "end": "379120"
  },
  {
    "text": "for improvement in many clusters and",
    "start": "379120",
    "end": "380960"
  },
  {
    "text": "many node groupoup sets and our clusters",
    "start": "380960",
    "end": "383680"
  },
  {
    "text": "are lots of different sizes so to be",
    "start": "383680",
    "end": "385360"
  },
  {
    "text": "more concrete we translated this CPU",
    "start": "385360",
    "end": "388560"
  },
  {
    "text": "request percentage into the actual",
    "start": "388560",
    "end": "391120"
  },
  {
    "text": "dollar impact and we realized that if we",
    "start": "391120",
    "end": "393120"
  },
  {
    "text": "were to improve our bin packing there's",
    "start": "393120",
    "end": "394560"
  },
  {
    "text": "some significant cost impact that we",
    "start": "394560",
    "end": "395919"
  },
  {
    "text": "could",
    "start": "395919",
    "end": "398080"
  },
  {
    "text": "bring however we also know that instance",
    "start": "398440",
    "end": "401360"
  },
  {
    "text": "type selection goes beyond just",
    "start": "401360",
    "end": "403080"
  },
  {
    "text": "binacking we also care about things like",
    "start": "403080",
    "end": "405600"
  },
  {
    "text": "the performance of each instance type we",
    "start": "405600",
    "end": "407919"
  },
  {
    "text": "care about whether the cloud provider",
    "start": "407919",
    "end": "409039"
  },
  {
    "text": "has enough capacity to serve our entire",
    "start": "409039",
    "end": "410720"
  },
  {
    "text": "fleet and we also know that within",
    "start": "410720",
    "end": "412960"
  },
  {
    "text": "specific clusters we might have specific",
    "start": "412960",
    "end": "414720"
  },
  {
    "text": "instance type preferences to meet the",
    "start": "414720",
    "end": "416319"
  },
  {
    "text": "application needs that run in those",
    "start": "416319",
    "end": "419600"
  },
  {
    "text": "clusters and as a reminder we run across",
    "start": "419960",
    "end": "423360"
  },
  {
    "text": "multiple cloud providers in dozens of",
    "start": "423360",
    "end": "424880"
  },
  {
    "text": "clusters so we have to make this",
    "start": "424880",
    "end": "426319"
  },
  {
    "text": "decision in more than just a single",
    "start": "426319",
    "end": "429000"
  },
  {
    "text": "cluster and as I mentioned each of these",
    "start": "429000",
    "end": "431759"
  },
  {
    "text": "clusters might be unique in the",
    "start": "431759",
    "end": "432960"
  },
  {
    "text": "applications that they run for example",
    "start": "432960",
    "end": "435360"
  },
  {
    "text": "one might serve more network intensive",
    "start": "435360",
    "end": "437039"
  },
  {
    "text": "applications and therefore benefit from",
    "start": "437039",
    "end": "438400"
  },
  {
    "text": "network intensive instant types etc",
    "start": "438400",
    "end": "442800"
  },
  {
    "text": "so at this point it's might seem like",
    "start": "442880",
    "end": "445680"
  },
  {
    "text": "there's no one expander fits solution",
    "start": "445680",
    "end": "447840"
  },
  {
    "text": "that we can use across our entire",
    "start": "447840",
    "end": "450280"
  },
  {
    "text": "fleet however the cluster autoscaler",
    "start": "450280",
    "end": "453440"
  },
  {
    "text": "provides something called the gRPC",
    "start": "453440",
    "end": "456280"
  },
  {
    "text": "expander so the gRPC expander allows",
    "start": "456280",
    "end": "459120"
  },
  {
    "text": "users to build a gRPC service to make",
    "start": "459120",
    "end": "461840"
  },
  {
    "text": "custom node groupoup decisions",
    "start": "461840",
    "end": "464240"
  },
  {
    "text": "so when it comes time for the cluster",
    "start": "464240",
    "end": "465759"
  },
  {
    "text": "autoscaler to pick the best option it",
    "start": "465759",
    "end": "468560"
  },
  {
    "text": "will make a request to the gRPC expander",
    "start": "468560",
    "end": "471280"
  },
  {
    "text": "and within that service you can return",
    "start": "471280",
    "end": "473360"
  },
  {
    "text": "what you have determined is the best",
    "start": "473360",
    "end": "474800"
  },
  {
    "text": "option to scale",
    "start": "474800",
    "end": "477400"
  },
  {
    "text": "up here's an upstream example that the",
    "start": "477400",
    "end": "479919"
  },
  {
    "text": "cluster autoscaler provides for the gRPC",
    "start": "479919",
    "end": "482240"
  },
  {
    "text": "expander",
    "start": "482240",
    "end": "483840"
  },
  {
    "text": "in this example the strategy is simply",
    "start": "483840",
    "end": "486000"
  },
  {
    "text": "to choose the option that has the",
    "start": "486000",
    "end": "488000"
  },
  {
    "text": "longest node group ID name but you could",
    "start": "488000",
    "end": "490160"
  },
  {
    "text": "replace this with whatever arbitrary",
    "start": "490160",
    "end": "491840"
  },
  {
    "text": "logic would fit your",
    "start": "491840",
    "end": "494800"
  },
  {
    "text": "needs so at this point we've recognized",
    "start": "495479",
    "end": "498560"
  },
  {
    "text": "that we have an opportunity to improve",
    "start": "498560",
    "end": "500080"
  },
  {
    "text": "our fleet efficiency via better instance",
    "start": "500080",
    "end": "501919"
  },
  {
    "text": "type selection and the gRPC expander",
    "start": "501919",
    "end": "504639"
  },
  {
    "text": "gives us the ultimate flexibility to",
    "start": "504639",
    "end": "506319"
  },
  {
    "text": "make these instance type",
    "start": "506319",
    "end": "508440"
  },
  {
    "text": "choices so our goals became simple we",
    "start": "508440",
    "end": "511440"
  },
  {
    "text": "wanted to identify the best instance",
    "start": "511440",
    "end": "512880"
  },
  {
    "text": "types and we wanted to scale the best",
    "start": "512880",
    "end": "514560"
  },
  {
    "text": "instance",
    "start": "514560",
    "end": "516880"
  },
  {
    "text": "types so to simplify the criteria that",
    "start": "518279",
    "end": "520800"
  },
  {
    "text": "we'll use for instance type selection",
    "start": "520800",
    "end": "522479"
  },
  {
    "text": "we'll bucket into three highle ideas",
    "start": "522479",
    "end": "524240"
  },
  {
    "text": "that we care about in instance type",
    "start": "524240",
    "end": "525600"
  },
  {
    "text": "selection we care about the cost of our",
    "start": "525600",
    "end": "527680"
  },
  {
    "text": "fleet we care about the performance of",
    "start": "527680",
    "end": "529440"
  },
  {
    "text": "the applications that run on them and we",
    "start": "529440",
    "end": "531200"
  },
  {
    "text": "care about the reliability of our",
    "start": "531200",
    "end": "534279"
  },
  {
    "text": "fleet so beginning with cost I want to",
    "start": "534279",
    "end": "537200"
  },
  {
    "text": "return back to the binacking problem",
    "start": "537200",
    "end": "538480"
  },
  {
    "text": "that I showed earlier but in reality",
    "start": "538480",
    "end": "541680"
  },
  {
    "text": "this problem is a lot more complicated",
    "start": "541680",
    "end": "543040"
  },
  {
    "text": "than just two pods requesting CPU we",
    "start": "543040",
    "end": "545839"
  },
  {
    "text": "actually serve lots of different pods",
    "start": "545839",
    "end": "547600"
  },
  {
    "text": "having lots of different CPU or memory",
    "start": "547600",
    "end": "550200"
  },
  {
    "text": "requests and as a reminder one of our",
    "start": "550200",
    "end": "552560"
  },
  {
    "text": "goals is to abstract any worries of node",
    "start": "552560",
    "end": "555120"
  },
  {
    "text": "infrastructure away from application",
    "start": "555120",
    "end": "556560"
  },
  {
    "text": "teams so we want to be able to serve",
    "start": "556560",
    "end": "558399"
  },
  {
    "text": "applications that request all different",
    "start": "558399",
    "end": "559839"
  },
  {
    "text": "types of CPU and memory in addition with",
    "start": "559839",
    "end": "562800"
  },
  {
    "text": "things like VPA we know that the",
    "start": "562800",
    "end": "564640"
  },
  {
    "text": "resource requests of pods might not be",
    "start": "564640",
    "end": "566080"
  },
  {
    "text": "static they might be dynamic and we",
    "start": "566080",
    "end": "567760"
  },
  {
    "text": "still want to be able to binack these",
    "start": "567760",
    "end": "569800"
  },
  {
    "text": "workloads so the question then becomes",
    "start": "569800",
    "end": "572320"
  },
  {
    "text": "how can we binack thousands of unique",
    "start": "572320",
    "end": "576160"
  },
  {
    "text": "workloads to solve this problem rather",
    "start": "576360",
    "end": "579120"
  },
  {
    "text": "than approach it from a theoretical",
    "start": "579120",
    "end": "580399"
  },
  {
    "text": "point of view we decided to use",
    "start": "580399",
    "end": "582000"
  },
  {
    "text": "scheduling",
    "start": "582000",
    "end": "584040"
  },
  {
    "text": "simulations we built a component called",
    "start": "584040",
    "end": "586399"
  },
  {
    "text": "the instance type adviser the goal of",
    "start": "586399",
    "end": "588880"
  },
  {
    "text": "the instance type adviser is to run",
    "start": "588880",
    "end": "590880"
  },
  {
    "text": "scheduling simulations for the sets of",
    "start": "590880",
    "end": "592880"
  },
  {
    "text": "pods that run on our node group sets",
    "start": "592880",
    "end": "595120"
  },
  {
    "text": "against every possible instance type",
    "start": "595120",
    "end": "596800"
  },
  {
    "text": "that we could use and then rank those",
    "start": "596800",
    "end": "599360"
  },
  {
    "text": "instance types based on which are best",
    "start": "599360",
    "end": "600880"
  },
  {
    "text": "for bin packing",
    "start": "600880",
    "end": "603160"
  },
  {
    "text": "efficiency so as an input to the adviser",
    "start": "603160",
    "end": "605600"
  },
  {
    "text": "we have our set of pods which we can",
    "start": "605600",
    "end": "608080"
  },
  {
    "text": "select via a node selector for example",
    "start": "608080",
    "end": "610720"
  },
  {
    "text": "get all pods running on nodes that have",
    "start": "610720",
    "end": "612399"
  },
  {
    "text": "label fu equals bar or concretely for us",
    "start": "612399",
    "end": "615680"
  },
  {
    "text": "get all pods that are running on the",
    "start": "615680",
    "end": "617120"
  },
  {
    "text": "nodes of our node group",
    "start": "617120",
    "end": "619880"
  },
  {
    "text": "set the next thing we'll pass to the",
    "start": "619880",
    "end": "621920"
  },
  {
    "text": "expander is an instance catalog which is",
    "start": "621920",
    "end": "624720"
  },
  {
    "text": "a catalog of all possible instance types",
    "start": "624720",
    "end": "626399"
  },
  {
    "text": "that we could use including their",
    "start": "626399",
    "end": "627760"
  },
  {
    "text": "specifications like cost per hour and",
    "start": "627760",
    "end": "629839"
  },
  {
    "text": "CPU and memory",
    "start": "629839",
    "end": "631560"
  },
  {
    "text": "capacity within the adviser we also have",
    "start": "631560",
    "end": "634240"
  },
  {
    "text": "a virtual node builder which is just an",
    "start": "634240",
    "end": "636640"
  },
  {
    "text": "interface to build virtual nodes in",
    "start": "636640",
    "end": "638800"
  },
  {
    "text": "order for us to schedule onto them that",
    "start": "638800",
    "end": "640640"
  },
  {
    "text": "match the specs of the instance",
    "start": "640640",
    "end": "643800"
  },
  {
    "text": "catalog and for the actual scheduling",
    "start": "643800",
    "end": "645959"
  },
  {
    "text": "simulations we use the upstream",
    "start": "645959",
    "end": "648000"
  },
  {
    "text": "Kubernetes scheduling framework but we",
    "start": "648000",
    "end": "650079"
  },
  {
    "text": "could use otheruler plugins if we needed",
    "start": "650079",
    "end": "651839"
  },
  {
    "text": "to",
    "start": "651839",
    "end": "654839"
  },
  {
    "text": "so the results of the instance type",
    "start": "655040",
    "end": "656480"
  },
  {
    "text": "adviser in a single cluster might look",
    "start": "656480",
    "end": "658240"
  },
  {
    "text": "something like this where we have our",
    "start": "658240",
    "end": "660800"
  },
  {
    "text": "current instance type that we're using",
    "start": "660800",
    "end": "662160"
  },
  {
    "text": "which is m6a.8x large and we can see",
    "start": "662160",
    "end": "665120"
  },
  {
    "text": "that with this instance type our",
    "start": "665120",
    "end": "666480"
  },
  {
    "text": "requested CPU percentage is down at",
    "start": "666480",
    "end": "669800"
  },
  {
    "text": "62% what the adviser tells us is that if",
    "start": "669800",
    "end": "672720"
  },
  {
    "text": "we were to reschedu all of those pods",
    "start": "672720",
    "end": "674720"
  },
  {
    "text": "currently running on mainly M6As to",
    "start": "674720",
    "end": "677440"
  },
  {
    "text": "R6A.4x 4x large we would increase our",
    "start": "677440",
    "end": "680640"
  },
  {
    "text": "requested CPU percentage all the way up",
    "start": "680640",
    "end": "682240"
  },
  {
    "text": "to 95 improving our bin packing",
    "start": "682240",
    "end": "685160"
  },
  {
    "text": "efficiency and therefore we can see this",
    "start": "685160",
    "end": "687600"
  },
  {
    "text": "translate to a significant cost decrease",
    "start": "687600",
    "end": "689440"
  },
  {
    "text": "in our",
    "start": "689440",
    "end": "690920"
  },
  {
    "text": "fleet so we run scheduling simulations",
    "start": "690920",
    "end": "693519"
  },
  {
    "text": "like this in every single cluster and we",
    "start": "693519",
    "end": "695680"
  },
  {
    "text": "store the results of them as a CRD that",
    "start": "695680",
    "end": "697600"
  },
  {
    "text": "we can consume within the",
    "start": "697600",
    "end": "700639"
  },
  {
    "text": "cluster beyond bin packing as I",
    "start": "701240",
    "end": "703680"
  },
  {
    "text": "mentioned we also care about performance",
    "start": "703680",
    "end": "706399"
  },
  {
    "text": "so to get a more granular look at the",
    "start": "706399",
    "end": "708560"
  },
  {
    "text": "performance of each instance type we run",
    "start": "708560",
    "end": "710800"
  },
  {
    "text": "performance benchmarks for network",
    "start": "710800",
    "end": "712480"
  },
  {
    "text": "performance CPU memory and storage and",
    "start": "712480",
    "end": "716399"
  },
  {
    "text": "this allows us to weigh cost versus",
    "start": "716399",
    "end": "718000"
  },
  {
    "text": "performance for every instance type so",
    "start": "718000",
    "end": "720320"
  },
  {
    "text": "for example before we make a migration",
    "start": "720320",
    "end": "721920"
  },
  {
    "text": "from instance type A to instance type E",
    "start": "721920",
    "end": "723920"
  },
  {
    "text": "B we could take a look at these",
    "start": "723920",
    "end": "725360"
  },
  {
    "text": "benchmarks and make sure that they match",
    "start": "725360",
    "end": "727440"
  },
  {
    "text": "up and we won't introduce any",
    "start": "727440",
    "end": "728639"
  },
  {
    "text": "performance",
    "start": "728639",
    "end": "730200"
  },
  {
    "text": "degradations",
    "start": "730200",
    "end": "731720"
  },
  {
    "text": "furthermore we can account for these in",
    "start": "731720",
    "end": "734160"
  },
  {
    "text": "our cost calculations so for example if",
    "start": "734160",
    "end": "736720"
  },
  {
    "text": "we look at CPU performance we might see",
    "start": "736720",
    "end": "739200"
  },
  {
    "text": "something like instance B is 20% more",
    "start": "739200",
    "end": "741760"
  },
  {
    "text": "expensive than instance A but in fact",
    "start": "741760",
    "end": "743760"
  },
  {
    "text": "it's 30% more performant so what that",
    "start": "743760",
    "end": "746639"
  },
  {
    "text": "could mean is that if we're properly",
    "start": "746639",
    "end": "748639"
  },
  {
    "text": "autoscaled on CPU then if we use",
    "start": "748639",
    "end": "751839"
  },
  {
    "text": "instance type B we might be able to run",
    "start": "751839",
    "end": "754240"
  },
  {
    "text": "30% fewer instances than instance type A",
    "start": "754240",
    "end": "757120"
  },
  {
    "text": "and that affects our cost calculations",
    "start": "757120",
    "end": "758639"
  },
  {
    "text": "in the end",
    "start": "758639",
    "end": "761959"
  },
  {
    "text": "for reliability we know that there's a",
    "start": "762320",
    "end": "764240"
  },
  {
    "text": "lot of different reliability concerns",
    "start": "764240",
    "end": "765440"
  },
  {
    "text": "that go into instance type selection for",
    "start": "765440",
    "end": "768079"
  },
  {
    "text": "us we care about things like the",
    "start": "768079",
    "end": "770600"
  },
  {
    "text": "capacity of each instance type and as I",
    "start": "770600",
    "end": "773279"
  },
  {
    "text": "mentioned before we also care about the",
    "start": "773279",
    "end": "775760"
  },
  {
    "text": "size ranges of our instance types that",
    "start": "775760",
    "end": "777200"
  },
  {
    "text": "we use to serve a diverse set of pods",
    "start": "777200",
    "end": "779120"
  },
  {
    "text": "and we also might want to express",
    "start": "779120",
    "end": "780399"
  },
  {
    "text": "performance",
    "start": "780399",
    "end": "781560"
  },
  {
    "text": "preferences additionally these",
    "start": "781560",
    "end": "783440"
  },
  {
    "text": "reliability concerns might be different",
    "start": "783440",
    "end": "785120"
  },
  {
    "text": "per environment that you're running in",
    "start": "785120",
    "end": "787040"
  },
  {
    "text": "for example the capacity of instance",
    "start": "787040",
    "end": "788480"
  },
  {
    "text": "type A might be different in region X",
    "start": "788480",
    "end": "790480"
  },
  {
    "text": "than it is in region",
    "start": "790480",
    "end": "792200"
  },
  {
    "text": "Y and so to give us flexibility in",
    "start": "792200",
    "end": "794959"
  },
  {
    "text": "making these reliability choices we",
    "start": "794959",
    "end": "796639"
  },
  {
    "text": "built a small library that allows us to",
    "start": "796639",
    "end": "798639"
  },
  {
    "text": "build cellgo selectors against instance",
    "start": "798639",
    "end": "801040"
  },
  {
    "text": "type attributes as well as attributes",
    "start": "801040",
    "end": "802880"
  },
  {
    "text": "within the",
    "start": "802880",
    "end": "805000"
  },
  {
    "text": "environment and so this allows us to",
    "start": "805000",
    "end": "806959"
  },
  {
    "text": "build selectors like the following",
    "start": "806959",
    "end": "809519"
  },
  {
    "text": "for example if I know that I want to use",
    "start": "809519",
    "end": "811360"
  },
  {
    "text": "M6G R 6G or C6G because I have a deep",
    "start": "811360",
    "end": "814480"
  },
  {
    "text": "capacity pool and I know I want to serve",
    "start": "814480",
    "end": "816399"
  },
  {
    "text": "applications between 8 and 32 CPUs I can",
    "start": "816399",
    "end": "818560"
  },
  {
    "text": "build a selector to represent",
    "start": "818560",
    "end": "821000"
  },
  {
    "text": "that if I know that I have network uh",
    "start": "821000",
    "end": "824079"
  },
  {
    "text": "network intensive applications in edge",
    "start": "824079",
    "end": "825839"
  },
  {
    "text": "environments I can build a selector to",
    "start": "825839",
    "end": "827519"
  },
  {
    "text": "select network intensive instance types",
    "start": "827519",
    "end": "829440"
  },
  {
    "text": "for that environment",
    "start": "829440",
    "end": "831519"
  },
  {
    "text": "if I know that a specific cluster can",
    "start": "831519",
    "end": "834800"
  },
  {
    "text": "benefit from newer generation instance",
    "start": "834800",
    "end": "836480"
  },
  {
    "text": "types that might be more expensive but",
    "start": "836480",
    "end": "838079"
  },
  {
    "text": "are also more CPU performant then I can",
    "start": "838079",
    "end": "840160"
  },
  {
    "text": "build a selector for that as well and",
    "start": "840160",
    "end": "842480"
  },
  {
    "text": "even if the cloud provider releases some",
    "start": "842480",
    "end": "844480"
  },
  {
    "text": "sort of fancy new type that I'm not sure",
    "start": "844480",
    "end": "846079"
  },
  {
    "text": "I want to use in prod yet but I want to",
    "start": "846079",
    "end": "847760"
  },
  {
    "text": "test out in my experimental environment",
    "start": "847760",
    "end": "849360"
  },
  {
    "text": "I can build a selector for that too",
    "start": "849360",
    "end": "853560"
  },
  {
    "text": "so with these three tools we now have",
    "start": "854079",
    "end": "856000"
  },
  {
    "text": "the adviser to be able to calculate the",
    "start": "856000",
    "end": "858560"
  },
  {
    "text": "bin packing efficiency of all of our",
    "start": "858560",
    "end": "860160"
  },
  {
    "text": "node group sets for cost optimization we",
    "start": "860160",
    "end": "862800"
  },
  {
    "text": "have benchmarks to factor in the",
    "start": "862800",
    "end": "864320"
  },
  {
    "text": "specific performance of every instance",
    "start": "864320",
    "end": "865839"
  },
  {
    "text": "type and we have attribute selectors on",
    "start": "865839",
    "end": "868000"
  },
  {
    "text": "the instance type and environment to",
    "start": "868000",
    "end": "869360"
  },
  {
    "text": "give us flexibility and the reliability",
    "start": "869360",
    "end": "871040"
  },
  {
    "text": "concerns that we care about for our",
    "start": "871040",
    "end": "873720"
  },
  {
    "text": "fleet so at this point we feel like we",
    "start": "873720",
    "end": "876480"
  },
  {
    "text": "have a good set of tools to be able to",
    "start": "876480",
    "end": "877839"
  },
  {
    "text": "identify optimal instance types and so",
    "start": "877839",
    "end": "879680"
  },
  {
    "text": "now I'm going to pass it to Rahul to",
    "start": "879680",
    "end": "881040"
  },
  {
    "text": "figure out how we scale those optimal",
    "start": "881040",
    "end": "882399"
  },
  {
    "text": "instance",
    "start": "882399",
    "end": "884680"
  },
  {
    "text": "types all right so now we're going to",
    "start": "884680",
    "end": "887440"
  },
  {
    "text": "scale the best instance",
    "start": "887440",
    "end": "889160"
  },
  {
    "text": "types so right now this is our",
    "start": "889160",
    "end": "891600"
  },
  {
    "text": "autoscaling ecosystem uh we're going to",
    "start": "891600",
    "end": "894480"
  },
  {
    "text": "build on this diagram as we go through",
    "start": "894480",
    "end": "896480"
  },
  {
    "text": "the presentation but for now we have the",
    "start": "896480",
    "end": "898959"
  },
  {
    "text": "instance analysis tools on your left uh",
    "start": "898959",
    "end": "901199"
  },
  {
    "text": "that Ben just went over and this will",
    "start": "901199",
    "end": "903440"
  },
  {
    "text": "let us know what the best instance types",
    "start": "903440",
    "end": "905199"
  },
  {
    "text": "are and then on the right we have our",
    "start": "905199",
    "end": "907519"
  },
  {
    "text": "simple autoscaling ecosystem with the",
    "start": "907519",
    "end": "909920"
  },
  {
    "text": "cluster autoscaler and some node groups",
    "start": "909920",
    "end": "912560"
  },
  {
    "text": "so we as humans through the instance",
    "start": "912560",
    "end": "914480"
  },
  {
    "text": "analysis tools know what the best",
    "start": "914480",
    "end": "916079"
  },
  {
    "text": "instance type is but now the question is",
    "start": "916079",
    "end": "918560"
  },
  {
    "text": "how does the cluster autoscaler know",
    "start": "918560",
    "end": "920079"
  },
  {
    "text": "what the best instance type is and this",
    "start": "920079",
    "end": "921839"
  },
  {
    "text": "is important so we can apply the uh",
    "start": "921839",
    "end": "924800"
  },
  {
    "text": "analysis and I want everyone to remember",
    "start": "924800",
    "end": "927040"
  },
  {
    "text": "this question because it's going to come",
    "start": "927040",
    "end": "929040"
  },
  {
    "text": "uh back later on in the presentation",
    "start": "929040",
    "end": "932959"
  },
  {
    "text": "so to tell the cluster autoscaler what",
    "start": "932959",
    "end": "935760"
  },
  {
    "text": "the best instance type is we use the",
    "start": "935760",
    "end": "937760"
  },
  {
    "text": "gRPC expander like Ben mentioned it's a",
    "start": "937760",
    "end": "940240"
  },
  {
    "text": "gRPC service where you can put whatever",
    "start": "940240",
    "end": "943040"
  },
  {
    "text": "custom code you want in there uh and",
    "start": "943040",
    "end": "945040"
  },
  {
    "text": "it'll influence the scaling decisions of",
    "start": "945040",
    "end": "947040"
  },
  {
    "text": "cluster autoscaler but this kind of just",
    "start": "947040",
    "end": "949920"
  },
  {
    "text": "pushes down the question to the gRPC",
    "start": "949920",
    "end": "952160"
  },
  {
    "text": "expander we actually have to write this",
    "start": "952160",
    "end": "954000"
  },
  {
    "text": "custom code so that I can properly",
    "start": "954000",
    "end": "955759"
  },
  {
    "text": "influence cluster autoscaler",
    "start": "955759",
    "end": "959639"
  },
  {
    "text": "so a simple way to influence the gRPC",
    "start": "959920",
    "end": "962880"
  },
  {
    "text": "expander is just you a human operator so",
    "start": "962880",
    "end": "966079"
  },
  {
    "text": "the human operator can get the instance",
    "start": "966079",
    "end": "968480"
  },
  {
    "text": "analysis results themselves and write",
    "start": "968480",
    "end": "970639"
  },
  {
    "text": "that in a format which the gRPC expander",
    "start": "970639",
    "end": "973040"
  },
  {
    "text": "can read uh something like a config map",
    "start": "973040",
    "end": "975519"
  },
  {
    "text": "or CRDs so in this example uh node group",
    "start": "975519",
    "end": "978800"
  },
  {
    "text": "C has the highest score of 100 so gRPC",
    "start": "978800",
    "end": "982000"
  },
  {
    "text": "expander will tell cluster autoscaler to",
    "start": "982000",
    "end": "984079"
  },
  {
    "text": "scale that instance type",
    "start": "984079",
    "end": "987279"
  },
  {
    "text": "so we wanted to validate this whole um",
    "start": "987279",
    "end": "989920"
  },
  {
    "text": "approach that we've been taking so far",
    "start": "989920",
    "end": "992000"
  },
  {
    "text": "of optimizing instance types uh so we",
    "start": "992000",
    "end": "994720"
  },
  {
    "text": "picked one of our clusters that we saw",
    "start": "994720",
    "end": "996320"
  },
  {
    "text": "had the most bin packing potential based",
    "start": "996320",
    "end": "998480"
  },
  {
    "text": "on our analysis and in this case we are",
    "start": "998480",
    "end": "1000959"
  },
  {
    "text": "running on M6G.8X large instance types",
    "start": "1000959",
    "end": "1003920"
  },
  {
    "text": "which are have a more balanced CPU to",
    "start": "1003920",
    "end": "1006720"
  },
  {
    "text": "memory ratio and are meant for general",
    "start": "1006720",
    "end": "1009120"
  },
  {
    "text": "purpose workloads on AWS",
    "start": "1009120",
    "end": "1011920"
  },
  {
    "text": "but uh through our simulations we found",
    "start": "1011920",
    "end": "1014320"
  },
  {
    "text": "that there are more memory intensive",
    "start": "1014320",
    "end": "1016320"
  },
  {
    "text": "workloads on this cluster and that our",
    "start": "1016320",
    "end": "1018399"
  },
  {
    "text": "6G.8X large would be a better fit",
    "start": "1018399",
    "end": "1020399"
  },
  {
    "text": "because it has a higher memory to CPU",
    "start": "1020399",
    "end": "1022639"
  },
  {
    "text": "ratio so you can see on the top graph we",
    "start": "1022639",
    "end": "1025839"
  },
  {
    "text": "significantly reduced the number of",
    "start": "1025839",
    "end": "1027600"
  },
  {
    "text": "instances we needed and on the bottom",
    "start": "1027600",
    "end": "1030240"
  },
  {
    "text": "you can see we significantly reduced our",
    "start": "1030240",
    "end": "1032160"
  },
  {
    "text": "cost by over 60% in just this one",
    "start": "1032160",
    "end": "1034558"
  },
  {
    "text": "cluster so this validated to us that our",
    "start": "1034559",
    "end": "1037678"
  },
  {
    "text": "approach of choosing the optimal",
    "start": "1037679",
    "end": "1039600"
  },
  {
    "text": "instance type uh did in fact work and we",
    "start": "1039600",
    "end": "1042400"
  },
  {
    "text": "wanted to expand this to the dozens of",
    "start": "1042400",
    "end": "1044558"
  },
  {
    "text": "clusters that we run",
    "start": "1044559",
    "end": "1046839"
  },
  {
    "text": "on so if you're running in a small shop",
    "start": "1046839",
    "end": "1049760"
  },
  {
    "text": "with just one or two Kubernetes clusters",
    "start": "1049760",
    "end": "1053840"
  },
  {
    "text": "this approach works perfectly fine with",
    "start": "1053840",
    "end": "1056640"
  },
  {
    "text": "a human operator it's not too much",
    "start": "1056640",
    "end": "1059200"
  },
  {
    "text": "manual toil you can kind of get away",
    "start": "1059200",
    "end": "1061360"
  },
  {
    "text": "with it but at our scale we run on",
    "start": "1061360",
    "end": "1064640"
  },
  {
    "text": "dozens of clusters so the best instance",
    "start": "1064640",
    "end": "1067840"
  },
  {
    "text": "type is not going to be the same across",
    "start": "1067840",
    "end": "1069520"
  },
  {
    "text": "clusters you could have one cluster that",
    "start": "1069520",
    "end": "1071600"
  },
  {
    "text": "runs a bunch of memory intensive",
    "start": "1071600",
    "end": "1074160"
  },
  {
    "text": "applications another one that runs CPU",
    "start": "1074160",
    "end": "1076240"
  },
  {
    "text": "intensive applications so it's going to",
    "start": "1076240",
    "end": "1077919"
  },
  {
    "text": "have completely different uh best",
    "start": "1077919",
    "end": "1079840"
  },
  {
    "text": "instance types also across regions",
    "start": "1079840",
    "end": "1082480"
  },
  {
    "text": "there's different instance type",
    "start": "1082480",
    "end": "1083720"
  },
  {
    "text": "availability across cloud providers you",
    "start": "1083720",
    "end": "1086240"
  },
  {
    "text": "have completely different instance types",
    "start": "1086240",
    "end": "1088240"
  },
  {
    "text": "so a human managing the best instance",
    "start": "1088240",
    "end": "1091039"
  },
  {
    "text": "types across all these clusters is not",
    "start": "1091039",
    "end": "1093360"
  },
  {
    "text": "going to scale well the other thing is",
    "start": "1093360",
    "end": "1095280"
  },
  {
    "text": "that these scores can potentially change",
    "start": "1095280",
    "end": "1097280"
  },
  {
    "text": "frequently let's say in the middle of",
    "start": "1097280",
    "end": "1099039"
  },
  {
    "text": "the night an instance type runs out of",
    "start": "1099039",
    "end": "1101039"
  },
  {
    "text": "capacity you're going to have to change",
    "start": "1101039",
    "end": "1103200"
  },
  {
    "text": "the score of that instance type or if a",
    "start": "1103200",
    "end": "1105840"
  },
  {
    "text": "new application gets deployed or scaled",
    "start": "1105840",
    "end": "1107760"
  },
  {
    "text": "up you're going to have to change the",
    "start": "1107760",
    "end": "1109280"
  },
  {
    "text": "score for that too so a human operator",
    "start": "1109280",
    "end": "1111840"
  },
  {
    "text": "can't really handle all of this",
    "start": "1111840",
    "end": "1115039"
  },
  {
    "text": "we're going to have to go back and",
    "start": "1115039",
    "end": "1116480"
  },
  {
    "text": "replace the human operator with",
    "start": "1116480",
    "end": "1118240"
  },
  {
    "text": "something that's more scalable and",
    "start": "1118240",
    "end": "1119760"
  },
  {
    "text": "dynamic so that I can optimize costs",
    "start": "1119760",
    "end": "1122640"
  },
  {
    "text": "across our dozens of",
    "start": "1122640",
    "end": "1125000"
  },
  {
    "text": "clusters so what we did was we created a",
    "start": "1125000",
    "end": "1127919"
  },
  {
    "text": "new custom controller called the",
    "start": "1127919",
    "end": "1129360"
  },
  {
    "text": "instance score this will basically",
    "start": "1129360",
    "end": "1131440"
  },
  {
    "text": "automate what the human was previously",
    "start": "1131440",
    "end": "1133440"
  },
  {
    "text": "doing and that'll watch the instance",
    "start": "1133440",
    "end": "1135360"
  },
  {
    "text": "analysis results and then write those to",
    "start": "1135360",
    "end": "1137919"
  },
  {
    "text": "config maps which the gRPC expander can",
    "start": "1137919",
    "end": "1140320"
  },
  {
    "text": "read and then influence the cluster",
    "start": "1140320",
    "end": "1142160"
  },
  {
    "text": "autoscaler",
    "start": "1142160",
    "end": "1144880"
  },
  {
    "text": "so because of this new automation uh we",
    "start": "1144880",
    "end": "1147600"
  },
  {
    "text": "are able to do this migration across",
    "start": "1147600",
    "end": "1149840"
  },
  {
    "text": "dozens of clusters on the top you can",
    "start": "1149840",
    "end": "1152000"
  },
  {
    "text": "see the number of instances we migrated",
    "start": "1152000",
    "end": "1154240"
  },
  {
    "text": "and on the bottom you can see our yearly",
    "start": "1154240",
    "end": "1156320"
  },
  {
    "text": "potential cost savings going down so",
    "start": "1156320",
    "end": "1158880"
  },
  {
    "text": "potential cost savings is a calculation",
    "start": "1158880",
    "end": "1160799"
  },
  {
    "text": "we do where we take the current cost of",
    "start": "1160799",
    "end": "1163039"
  },
  {
    "text": "our clusters and subtract the optimal",
    "start": "1163039",
    "end": "1165360"
  },
  {
    "text": "cost if we are using the optimal",
    "start": "1165360",
    "end": "1167440"
  },
  {
    "text": "instance type so let's say like a",
    "start": "1167440",
    "end": "1169200"
  },
  {
    "text": "cluster costed $10 million running on a",
    "start": "1169200",
    "end": "1172160"
  },
  {
    "text": "nonoptimal instance type but if we were",
    "start": "1172160",
    "end": "1174480"
  },
  {
    "text": "to change it to the optimal instance",
    "start": "1174480",
    "end": "1176000"
  },
  {
    "text": "type it would only cost $6 million that",
    "start": "1176000",
    "end": "1178240"
  },
  {
    "text": "means that we have a potential cost",
    "start": "1178240",
    "end": "1180400"
  },
  {
    "text": "savings of $4 million so in this case",
    "start": "1180400",
    "end": "1183280"
  },
  {
    "text": "across our dozens of clusters we had $4",
    "start": "1183280",
    "end": "1185679"
  },
  {
    "text": "million in potential savings and we were",
    "start": "1185679",
    "end": "1187840"
  },
  {
    "text": "able to save this money uh automatically",
    "start": "1187840",
    "end": "1192160"
  },
  {
    "text": "so when I say automatically um you can",
    "start": "1192160",
    "end": "1194559"
  },
  {
    "text": "see on the dates of the graph it's from",
    "start": "1194559",
    "end": "1197520"
  },
  {
    "text": "late July to early September so around",
    "start": "1197520",
    "end": "1199840"
  },
  {
    "text": "one and a half months so that's still",
    "start": "1199840",
    "end": "1201679"
  },
  {
    "text": "quite a while and although a lot of the",
    "start": "1201679",
    "end": "1204400"
  },
  {
    "text": "process was automated we still had some",
    "start": "1204400",
    "end": "1207200"
  },
  {
    "text": "manual work to",
    "start": "1207200",
    "end": "1209160"
  },
  {
    "text": "do so taking a step back this is just a",
    "start": "1209160",
    "end": "1212320"
  },
  {
    "text": "list of AWS instances there's hundreds",
    "start": "1212320",
    "end": "1214640"
  },
  {
    "text": "of them and they each have their own",
    "start": "1214640",
    "end": "1216960"
  },
  {
    "text": "characteristics",
    "start": "1216960",
    "end": "1219760"
  },
  {
    "text": "and in our diagram that we have so far",
    "start": "1219760",
    "end": "1222960"
  },
  {
    "text": "uh if instance type A B C or D happen to",
    "start": "1222960",
    "end": "1225840"
  },
  {
    "text": "be the best then we're just fine cluster",
    "start": "1225840",
    "end": "1228159"
  },
  {
    "text": "autoscaler will be able to scale it up",
    "start": "1228159",
    "end": "1230240"
  },
  {
    "text": "but what happens if instance type N",
    "start": "1230240",
    "end": "1232159"
  },
  {
    "text": "happens to be the best well it doesn't",
    "start": "1232159",
    "end": "1234159"
  },
  {
    "text": "exist in our cluster so cluster",
    "start": "1234159",
    "end": "1236159"
  },
  {
    "text": "autoscaler can't scale it up it has to",
    "start": "1236159",
    "end": "1238320"
  },
  {
    "text": "scale up the next best available option",
    "start": "1238320",
    "end": "1240640"
  },
  {
    "text": "and you might not get the full cost",
    "start": "1240640",
    "end": "1242240"
  },
  {
    "text": "savings through that",
    "start": "1242240",
    "end": "1244159"
  },
  {
    "text": "so now we come across the question how",
    "start": "1244159",
    "end": "1246159"
  },
  {
    "text": "do we make sure the best node group",
    "start": "1246159",
    "end": "1247679"
  },
  {
    "text": "exists in our cluster so that we can get",
    "start": "1247679",
    "end": "1249520"
  },
  {
    "text": "the max",
    "start": "1249520",
    "end": "1251159"
  },
  {
    "text": "savings so one easy approach is just",
    "start": "1251159",
    "end": "1254159"
  },
  {
    "text": "create a node group for every single",
    "start": "1254159",
    "end": "1255600"
  },
  {
    "text": "instance type so you'd have hundreds of",
    "start": "1255600",
    "end": "1257360"
  },
  {
    "text": "node groups on your cluster and no",
    "start": "1257360",
    "end": "1259360"
  },
  {
    "text": "matter what the best instance type is",
    "start": "1259360",
    "end": "1261039"
  },
  {
    "text": "you're guaranteed to have it and cluster",
    "start": "1261039",
    "end": "1262720"
  },
  {
    "text": "autoscaler can scale it but the problem",
    "start": "1262720",
    "end": "1265520"
  },
  {
    "text": "with this is it significantly impacts",
    "start": "1265520",
    "end": "1267840"
  },
  {
    "text": "the cluster autoscaler performance in",
    "start": "1267840",
    "end": "1270320"
  },
  {
    "text": "this case in one of our clusters we had",
    "start": "1270320",
    "end": "1272080"
  },
  {
    "text": "a few hundred node groups and you can",
    "start": "1272080",
    "end": "1274159"
  },
  {
    "text": "see on the bottom the main loop P99",
    "start": "1274159",
    "end": "1276799"
  },
  {
    "text": "duration uh sometimes it took over 5",
    "start": "1276799",
    "end": "1279600"
  },
  {
    "text": "minutes so imagine you're in an incident",
    "start": "1279600",
    "end": "1281919"
  },
  {
    "text": "and you need an urgent scale up and",
    "start": "1281919",
    "end": "1284000"
  },
  {
    "text": "you're waiting over 5 minutes to get",
    "start": "1284000",
    "end": "1285840"
  },
  {
    "text": "your nodes you're probably going to be",
    "start": "1285840",
    "end": "1287480"
  },
  {
    "text": "upset so we realized that we can't just",
    "start": "1287480",
    "end": "1290880"
  },
  {
    "text": "simply create a bunch of node groups for",
    "start": "1290880",
    "end": "1293200"
  },
  {
    "text": "no reason we have to be efficient with",
    "start": "1293200",
    "end": "1294880"
  },
  {
    "text": "it so we did some cleanup and you can",
    "start": "1294880",
    "end": "1297200"
  },
  {
    "text": "see uh the P99 duration dramatically",
    "start": "1297200",
    "end": "1300000"
  },
  {
    "text": "went down from over 5 minutes to less",
    "start": "1300000",
    "end": "1302240"
  },
  {
    "text": "than 10",
    "start": "1302240",
    "end": "1304360"
  },
  {
    "text": "seconds so as a platform we have some",
    "start": "1304360",
    "end": "1307679"
  },
  {
    "text": "requirements that we need to have to for",
    "start": "1307679",
    "end": "1310320"
  },
  {
    "text": "our users um and based on those",
    "start": "1310320",
    "end": "1312799"
  },
  {
    "text": "requirements we can create a node",
    "start": "1312799",
    "end": "1314320"
  },
  {
    "text": "groupoup set and see which node groups",
    "start": "1314320",
    "end": "1317200"
  },
  {
    "text": "we actually",
    "start": "1317200",
    "end": "1318600"
  },
  {
    "text": "need so one requirement is we need to",
    "start": "1318600",
    "end": "1321120"
  },
  {
    "text": "scale the most costefficient instance",
    "start": "1321120",
    "end": "1322960"
  },
  {
    "text": "type so we can have a node group for",
    "start": "1322960",
    "end": "1325320"
  },
  {
    "text": "that another requirement is we need to",
    "start": "1325320",
    "end": "1328240"
  },
  {
    "text": "schedule pods requesting up to 64 CPUs",
    "start": "1328240",
    "end": "1330880"
  },
  {
    "text": "and 256 gigs of memory and the best",
    "start": "1330880",
    "end": "1333840"
  },
  {
    "text": "instance type might not necessarily be",
    "start": "1333840",
    "end": "1335760"
  },
  {
    "text": "able to do that it could only have 16",
    "start": "1335760",
    "end": "1337919"
  },
  {
    "text": "CPUs so it can't schedule a pod",
    "start": "1337919",
    "end": "1340320"
  },
  {
    "text": "requesting 64 CPUs so we're going to",
    "start": "1340320",
    "end": "1342799"
  },
  {
    "text": "need another node group that has an",
    "start": "1342799",
    "end": "1344240"
  },
  {
    "text": "instance type that's big and is also",
    "start": "1344240",
    "end": "1346400"
  },
  {
    "text": "costefficient",
    "start": "1346400",
    "end": "1348799"
  },
  {
    "text": "another thing is at our scale uh we",
    "start": "1348799",
    "end": "1351280"
  },
  {
    "text": "often can't rely on the cloud provider",
    "start": "1351280",
    "end": "1353280"
  },
  {
    "text": "having enough capacity for a single",
    "start": "1353280",
    "end": "1354960"
  },
  {
    "text": "instance type uh we're going to have to",
    "start": "1354960",
    "end": "1356880"
  },
  {
    "text": "have some fallbacks just in case so",
    "start": "1356880",
    "end": "1359679"
  },
  {
    "text": "we're going to create two fallback node",
    "start": "1359679",
    "end": "1361280"
  },
  {
    "text": "groups for each of these main instance",
    "start": "1361280",
    "end": "1362799"
  },
  {
    "text": "types so now we have six node groups in",
    "start": "1362799",
    "end": "1364799"
  },
  {
    "text": "our node group set um and we're going to",
    "start": "1364799",
    "end": "1367039"
  },
  {
    "text": "guarantee we always have the best node",
    "start": "1367039",
    "end": "1368880"
  },
  {
    "text": "group while not creating too many",
    "start": "1368880",
    "end": "1372720"
  },
  {
    "text": "so in the past during that one and a",
    "start": "1372720",
    "end": "1374799"
  },
  {
    "text": "half month migration I was showing uh a",
    "start": "1374799",
    "end": "1377600"
  },
  {
    "text": "lot of it was automated because in a lot",
    "start": "1377600",
    "end": "1380000"
  },
  {
    "text": "of cases we did already have the best",
    "start": "1380000",
    "end": "1381840"
  },
  {
    "text": "instance type on the cluster but in a",
    "start": "1381840",
    "end": "1383919"
  },
  {
    "text": "lot of cases we didn't and we'd have to",
    "start": "1383919",
    "end": "1386000"
  },
  {
    "text": "manually go in and create node groups",
    "start": "1386000",
    "end": "1387760"
  },
  {
    "text": "for them so we already know that our at",
    "start": "1387760",
    "end": "1391120"
  },
  {
    "text": "our scale we can't continue manually",
    "start": "1391120",
    "end": "1393200"
  },
  {
    "text": "doing this we need to automate it so it",
    "start": "1393200",
    "end": "1394880"
  },
  {
    "text": "works across all of our clusters so what",
    "start": "1394880",
    "end": "1397440"
  },
  {
    "text": "we did was we created a new component",
    "start": "1397440",
    "end": "1399840"
  },
  {
    "text": "called the node groupoup set controller",
    "start": "1399840",
    "end": "1401840"
  },
  {
    "text": "this will see the node groupoup set see",
    "start": "1401840",
    "end": "1403679"
  },
  {
    "text": "its requirements and reconcile with the",
    "start": "1403679",
    "end": "1405919"
  },
  {
    "text": "cluster what node group should actually",
    "start": "1405919",
    "end": "1408200"
  },
  {
    "text": "belong so now we come to the question",
    "start": "1408200",
    "end": "1411039"
  },
  {
    "text": "how does the node groupoup set",
    "start": "1411039",
    "end": "1412240"
  },
  {
    "text": "controller know what the best instance",
    "start": "1412240",
    "end": "1413760"
  },
  {
    "text": "type is and if you remember I told",
    "start": "1413760",
    "end": "1415760"
  },
  {
    "text": "everyone to remember a certain question",
    "start": "1415760",
    "end": "1417600"
  },
  {
    "text": "before and the old question was how does",
    "start": "1417600",
    "end": "1419760"
  },
  {
    "text": "the cluster autoscaler know what the",
    "start": "1419760",
    "end": "1421200"
  },
  {
    "text": "best instance type is and now I've just",
    "start": "1421200",
    "end": "1423200"
  },
  {
    "text": "replaced the cluster autoscaler with no",
    "start": "1423200",
    "end": "1425360"
  },
  {
    "text": "group set controller it's asking the",
    "start": "1425360",
    "end": "1427520"
  },
  {
    "text": "exact same",
    "start": "1427520",
    "end": "1428679"
  },
  {
    "text": "question and when we answered it for",
    "start": "1428679",
    "end": "1431200"
  },
  {
    "text": "cluster autoscaler the answer was the",
    "start": "1431200",
    "end": "1433120"
  },
  {
    "text": "gRPC expander so you might be wondering",
    "start": "1433120",
    "end": "1435840"
  },
  {
    "text": "what's going to tell the node group set",
    "start": "1435840",
    "end": "1437280"
  },
  {
    "text": "controller what to",
    "start": "1437280",
    "end": "1438520"
  },
  {
    "text": "do and it's a gRPC expander yet again so",
    "start": "1438520",
    "end": "1442799"
  },
  {
    "text": "you might be a little surprised or",
    "start": "1442799",
    "end": "1444559"
  },
  {
    "text": "curious like hey I thought this gRPC",
    "start": "1444559",
    "end": "1446720"
  },
  {
    "text": "expander was a cluster autoscaler thing",
    "start": "1446720",
    "end": "1448559"
  },
  {
    "text": "like how are you using it with this new",
    "start": "1448559",
    "end": "1450400"
  },
  {
    "text": "component that you created",
    "start": "1450400",
    "end": "1452799"
  },
  {
    "text": "well that's another benefit um of the",
    "start": "1452799",
    "end": "1455679"
  },
  {
    "text": "gRPC expander you can put whatever",
    "start": "1455679",
    "end": "1458320"
  },
  {
    "text": "custom code you want in sure but it's",
    "start": "1458320",
    "end": "1460159"
  },
  {
    "text": "just a gRPC service there's nothing",
    "start": "1460159",
    "end": "1463039"
  },
  {
    "text": "restricting the cluster autoscaler from",
    "start": "1463039",
    "end": "1465200"
  },
  {
    "text": "being the only thing that can",
    "start": "1465200",
    "end": "1466720"
  },
  {
    "text": "communicate with it we even communicate",
    "start": "1466720",
    "end": "1469120"
  },
  {
    "text": "with it from our own laptops when we",
    "start": "1469120",
    "end": "1471200"
  },
  {
    "text": "want to sanity check that our instance",
    "start": "1471200",
    "end": "1472960"
  },
  {
    "text": "type rankings make sense so through that",
    "start": "1472960",
    "end": "1475840"
  },
  {
    "text": "any service component you make can make",
    "start": "1475840",
    "end": "1479039"
  },
  {
    "text": "a gRPC request to the gRPC expander and",
    "start": "1479039",
    "end": "1482480"
  },
  {
    "text": "ask it what are the best instance types",
    "start": "1482480",
    "end": "1485360"
  },
  {
    "text": "so this closes the loop on our",
    "start": "1485360",
    "end": "1487520"
  },
  {
    "text": "autoscaling ecosystem on the left we",
    "start": "1487520",
    "end": "1490240"
  },
  {
    "text": "have our instance analysis tools we get",
    "start": "1490240",
    "end": "1492720"
  },
  {
    "text": "the results instance score watches these",
    "start": "1492720",
    "end": "1494960"
  },
  {
    "text": "results and writes them into config maps",
    "start": "1494960",
    "end": "1497520"
  },
  {
    "text": "which the gRPC expander reads so now the",
    "start": "1497520",
    "end": "1500080"
  },
  {
    "text": "gRPC expander knows what the best",
    "start": "1500080",
    "end": "1501919"
  },
  {
    "text": "instance types are uh through that the",
    "start": "1501919",
    "end": "1505039"
  },
  {
    "text": "node group set controller can now create",
    "start": "1505039",
    "end": "1507600"
  },
  {
    "text": "the best node groups on the cluster and",
    "start": "1507600",
    "end": "1509760"
  },
  {
    "text": "cluster autoscaler will then scale these",
    "start": "1509760",
    "end": "1511520"
  },
  {
    "text": "node",
    "start": "1511520",
    "end": "1512679"
  },
  {
    "text": "groupoups so we saw this take action in",
    "start": "1512679",
    "end": "1515520"
  },
  {
    "text": "one of our clusters it was previously",
    "start": "1515520",
    "end": "1517600"
  },
  {
    "text": "running m6g.8x large instances uh for",
    "start": "1517600",
    "end": "1521120"
  },
  {
    "text": "quite a while and then you can see a",
    "start": "1521120",
    "end": "1523200"
  },
  {
    "text": "spike in the number of nodes and that's",
    "start": "1523200",
    "end": "1525200"
  },
  {
    "text": "because a new application got deployed",
    "start": "1525200",
    "end": "1527200"
  },
  {
    "text": "to this cluster so originally it was",
    "start": "1527200",
    "end": "1529360"
  },
  {
    "text": "running on M6G.8x 8x large um and then",
    "start": "1529360",
    "end": "1532480"
  },
  {
    "text": "our instance analysis showed us that",
    "start": "1532480",
    "end": "1534679"
  },
  {
    "text": "C6G.4x large was the best and our",
    "start": "1534679",
    "end": "1538480"
  },
  {
    "text": "cluster didn't have this node group so",
    "start": "1538480",
    "end": "1540320"
  },
  {
    "text": "the node group set controller created",
    "start": "1540320",
    "end": "1542159"
  },
  {
    "text": "this node group for us and then cluster",
    "start": "1542159",
    "end": "1544159"
  },
  {
    "text": "autoscaler started scaling it so you can",
    "start": "1544159",
    "end": "1546640"
  },
  {
    "text": "see as we switch to the blue C6G.4x",
    "start": "1546640",
    "end": "1550159"
  },
  {
    "text": "large instance types our potential",
    "start": "1550159",
    "end": "1552240"
  },
  {
    "text": "yearly cost savings went down by $2",
    "start": "1552240",
    "end": "1554480"
  },
  {
    "text": "million so we've saved millions again",
    "start": "1554480",
    "end": "1558159"
  },
  {
    "text": "so we've accomplished both of our goals",
    "start": "1558159",
    "end": "1559919"
  },
  {
    "text": "now we're able to both identify and",
    "start": "1559919",
    "end": "1562480"
  },
  {
    "text": "scale the best instance",
    "start": "1562480",
    "end": "1564600"
  },
  {
    "text": "types so next steps for our platform we",
    "start": "1564600",
    "end": "1567679"
  },
  {
    "text": "want to continue what we're doing we",
    "start": "1567679",
    "end": "1569200"
  },
  {
    "text": "want to continue abstracting these",
    "start": "1569200",
    "end": "1571200"
  },
  {
    "text": "low-level infrastructure details from uh",
    "start": "1571200",
    "end": "1573919"
  },
  {
    "text": "our application teams just like the",
    "start": "1573919",
    "end": "1576080"
  },
  {
    "text": "example I showed this application team",
    "start": "1576080",
    "end": "1578400"
  },
  {
    "text": "didn't really have to worry about the",
    "start": "1578400",
    "end": "1580240"
  },
  {
    "text": "underlying infrastructure like what",
    "start": "1580240",
    "end": "1581919"
  },
  {
    "text": "instance type they're going to get",
    "start": "1581919",
    "end": "1583120"
  },
  {
    "text": "scheduled on they could worry about",
    "start": "1583120",
    "end": "1584799"
  },
  {
    "text": "their application get it deployed and",
    "start": "1584799",
    "end": "1587360"
  },
  {
    "text": "then our cluster would automatically uh",
    "start": "1587360",
    "end": "1590240"
  },
  {
    "text": "adapt itself to be more",
    "start": "1590240",
    "end": "1592600"
  },
  {
    "text": "costefficient another thing we want to",
    "start": "1592600",
    "end": "1594640"
  },
  {
    "text": "expand on is providing different",
    "start": "1594640",
    "end": "1596480"
  },
  {
    "text": "varieties of instance types like Ben",
    "start": "1596480",
    "end": "1598720"
  },
  {
    "text": "mentioned at data dog we have many",
    "start": "1598720",
    "end": "1600480"
  },
  {
    "text": "different products many different types",
    "start": "1600480",
    "end": "1602559"
  },
  {
    "text": "of workloads that each have their own",
    "start": "1602559",
    "end": "1604240"
  },
  {
    "text": "requirements so we need to have instance",
    "start": "1604240",
    "end": "1606400"
  },
  {
    "text": "types to fulfill these requirements",
    "start": "1606400",
    "end": "1608000"
  },
  {
    "text": "things like local discs for example",
    "start": "1608000",
    "end": "1610640"
  },
  {
    "text": "and then another avenue that we want to",
    "start": "1610640",
    "end": "1612480"
  },
  {
    "text": "explore is optimizing the placement of",
    "start": "1612480",
    "end": "1614480"
  },
  {
    "text": "our applications throughout this talk we",
    "start": "1614480",
    "end": "1616960"
  },
  {
    "text": "took existing clusters and their set of",
    "start": "1616960",
    "end": "1619679"
  },
  {
    "text": "applications and adapted to that but",
    "start": "1619679",
    "end": "1621760"
  },
  {
    "text": "what if preemptively we could move an",
    "start": "1621760",
    "end": "1623919"
  },
  {
    "text": "application from one cluster to another",
    "start": "1623919",
    "end": "1625840"
  },
  {
    "text": "or decide where a new application should",
    "start": "1625840",
    "end": "1627679"
  },
  {
    "text": "go because it has better bin packing",
    "start": "1627679",
    "end": "1629919"
  },
  {
    "text": "potential on one cluster over another so",
    "start": "1629919",
    "end": "1632320"
  },
  {
    "text": "these are avenues we're looking to",
    "start": "1632320",
    "end": "1634080"
  },
  {
    "text": "explore to continue improving the",
    "start": "1634080",
    "end": "1635840"
  },
  {
    "text": "platform we provide to our application",
    "start": "1635840",
    "end": "1637919"
  },
  {
    "text": "teams",
    "start": "1637919",
    "end": "1640080"
  },
  {
    "text": "so thank you everyone for listening to",
    "start": "1640080",
    "end": "1641840"
  },
  {
    "text": "our presentation uh we have our",
    "start": "1641840",
    "end": "1643840"
  },
  {
    "text": "LinkedIns on the right side of the",
    "start": "1643840",
    "end": "1645360"
  },
  {
    "text": "screen in case you want to connect with",
    "start": "1645360",
    "end": "1646799"
  },
  {
    "text": "us we have the feedback form on the",
    "start": "1646799",
    "end": "1649039"
  },
  {
    "text": "bottom left um and we're also on the",
    "start": "1649039",
    "end": "1652559"
  },
  {
    "text": "Kubernetes Slack in case you want to",
    "start": "1652559",
    "end": "1654240"
  },
  {
    "text": "message us and meet up while we're all",
    "start": "1654240",
    "end": "1656080"
  },
  {
    "text": "still in London so thank you everyone",
    "start": "1656080",
    "end": "1658000"
  },
  {
    "text": "for attending our talk and we'll take",
    "start": "1658000",
    "end": "1660320"
  },
  {
    "text": "any questions",
    "start": "1660320",
    "end": "1661760"
  },
  {
    "text": "[Applause]",
    "start": "1661760",
    "end": "1669289"
  }
]