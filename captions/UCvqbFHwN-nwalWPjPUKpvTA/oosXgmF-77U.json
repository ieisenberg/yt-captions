[
  {
    "start": "0",
    "end": "24000"
  },
  {
    "text": "hey everybody my phone says it's to my computer just said it was - and the",
    "start": "319",
    "end": "5730"
  },
  {
    "text": "clock in the hall assessed is - so we've achieved distributive consensus I think it's time to start this my name is beau",
    "start": "5730",
    "end": "11759"
  },
  {
    "text": "Ingram I'm here today to talk to you on a talk entitled Plains raft and pods",
    "start": "11759",
    "end": "16770"
  },
  {
    "text": "it's gonna be a tour of distributed systems within kubernetes so a little",
    "start": "16770",
    "end": "21840"
  },
  {
    "text": "bit about me like any millennial I've got a list my Twitter handle first",
    "start": "21840",
    "end": "27180"
  },
  {
    "start": "24000",
    "end": "78000"
  },
  {
    "text": "I am voluptuous on Twitter beau Ingram was taken so I had to get fancy that's my dog Ernie on the right he is",
    "start": "27180",
    "end": "34380"
  },
  {
    "text": "universally recognized as quote the man and I consider taking out all my slides",
    "start": "34380",
    "end": "39870"
  },
  {
    "text": "and just showing pictures of him but I figured you all would leave and they throw me out and it probably wouldn't be good so I am a platform engineer at",
    "start": "39870",
    "end": "47610"
  },
  {
    "text": "Kraft see we're based in Denver Colorado and we're an online education and e-commerce company for makers and",
    "start": "47610",
    "end": "53699"
  },
  {
    "text": "crafters we sell classes in topics like quilting sewing cooking photography and",
    "start": "53699",
    "end": "58829"
  },
  {
    "text": "we've got supplies we've got patterns we've got kids to help you make projects and all sorts of fun stuff so I spend",
    "start": "58829",
    "end": "64739"
  },
  {
    "text": "most of my time working on backend features there but also do some of the operations he tasks sometimes as well as",
    "start": "64739",
    "end": "70650"
  },
  {
    "text": "some of your traditional site reliability is tasks I'm probably somebody's definition of DevOps not mine but somebody is so you know I'm",
    "start": "70650",
    "end": "79619"
  },
  {
    "start": "78000",
    "end": "136000"
  },
  {
    "text": "big on definitions so let's start off with what's a distributed system so in distributed systems for fun and profit",
    "start": "79619",
    "end": "85830"
  },
  {
    "text": "maketo Takada says distributed programming is the art of solving the same problem that you can solve on a",
    "start": "85830",
    "end": "91409"
  },
  {
    "text": "single computer using multiple computers now that's a little bit of a sarcastic kind of tongue-in-cheek definition but a",
    "start": "91409",
    "end": "98180"
  },
  {
    "text": "distributed systems problem is a problem that you have on one computer it needs to go to more computers usually because",
    "start": "98180",
    "end": "103979"
  },
  {
    "text": "it's gotten too big we can say that a distributed system is several nodes components pieces whatever working",
    "start": "103979",
    "end": "111149"
  },
  {
    "text": "together to accomplish the same goal so it's cube con but what's kubernetes you know we heard",
    "start": "111149",
    "end": "117180"
  },
  {
    "text": "a great talk this morning about like what are all the things kubernetes can be well for me I like to go look at the documentation the docs say that it's an",
    "start": "117180",
    "end": "125100"
  },
  {
    "text": "open source platform for automating deployment scaling and operations of application containers around clusters",
    "start": "125100",
    "end": "131370"
  },
  {
    "text": "of hosts providing container centric and structure what I read that four times",
    "start": "131370",
    "end": "139140"
  },
  {
    "start": "136000",
    "end": "480000"
  },
  {
    "text": "and you know every time I practice this talk I had to run through that slide twice just to get the cadence of it",
    "start": "139140",
    "end": "144300"
  },
  {
    "text": "right it's a mouthful but I like to think kind of back to what the number one definition was this morning which is",
    "start": "144300",
    "end": "150090"
  },
  {
    "text": "that kubernetes is a flexible platform for running containerized apps not word-for-word what it was but close",
    "start": "150090",
    "end": "155760"
  },
  {
    "text": "enough but kubernetes you know provides capabilities greater than hi I'm a container please run me it's got this",
    "start": "155760",
    "end": "161640"
  },
  {
    "text": "really awesome feature set right you know can handle rolling deploys do load balancing you got health checking all",
    "start": "161640",
    "end": "166800"
  },
  {
    "text": "sorts of stuff and they're great and I think a lot of talks talk about how kubernetes does these things but I want",
    "start": "166800",
    "end": "172350"
  },
  {
    "text": "to do something a little different tonight I want to peek behind the curtain I want to see how the components",
    "start": "172350",
    "end": "177480"
  },
  {
    "text": "of kubernetes work together to achieve these things my question for us tonight is today is how does kubernetes leverage",
    "start": "177480",
    "end": "184530"
  },
  {
    "text": "distributed systems so start off with kubernetes has containerized apps well",
    "start": "184530",
    "end": "190170"
  },
  {
    "text": "what's a container container could be said as the usage of Linux see groups and namespaces to provide process",
    "start": "190170",
    "end": "195990"
  },
  {
    "text": "isolation for an abstracted filesystem see groups specify limits for how much memory and CPU a process can use and a",
    "start": "195990",
    "end": "203040"
  },
  {
    "text": "namespace can stop process from interfering with other processes but like all definitions that's great and",
    "start": "203040",
    "end": "208260"
  },
  {
    "text": "somebody has set it better undoubtedly a beach in yesterday she said a container is just a baby computer inside another",
    "start": "208260",
    "end": "215580"
  },
  {
    "text": "computer way simpler than anything I ever said a containerized app though",
    "start": "215580",
    "end": "221280"
  },
  {
    "text": "something we met running kubernetes could be something like an engine X server or a little micro service or if you're feeling stateful perhaps a Redis",
    "start": "221280",
    "end": "227730"
  },
  {
    "text": "instance so when you think of kubernetes right you come to kubernetes for the first time you hear containers",
    "start": "227730",
    "end": "233700"
  },
  {
    "text": "containers you know I'm going to join the container team but in kubernetes the base unit of scheduling and the way",
    "start": "233700",
    "end": "238890"
  },
  {
    "text": "kubernetes thinks about things is with pods so iPod is one or more containers that share a unique IP address in the",
    "start": "238890",
    "end": "245250"
  },
  {
    "text": "cluster you're guaranteed to have all containers running on the same host a pod can share disk volumes this is",
    "start": "245250",
    "end": "251160"
  },
  {
    "text": "pretty helpful in many situations for example if you get an app that logs to a file you could have a sidecar container",
    "start": "251160",
    "end": "257190"
  },
  {
    "text": "sitting alongside it in the same pod that handles fording your logs off to a log aggregation service like log stash",
    "start": "257190",
    "end": "262919"
  },
  {
    "text": "or spunk so the way we typically manage these pods is through it a deployment is a declarative way of",
    "start": "262919",
    "end": "270090"
  },
  {
    "text": "managing pods we can specify what containers we want to run and how many instances of those pods we want to have",
    "start": "270090",
    "end": "275460"
  },
  {
    "text": "in our cluster under-the-hood deployments use a thing called replica sets to ensure that the requested number",
    "start": "275460",
    "end": "281250"
  },
  {
    "text": "of pods is equal to the number of pods running in the cluster for that deployment so when we create a",
    "start": "281250",
    "end": "286950"
  },
  {
    "text": "deployment which contains the container images we want to run for our pods and how many pods we want to have kubernetes",
    "start": "286950",
    "end": "292290"
  },
  {
    "text": "is going to handle creating a replica set this replica set one sure we have a desired number of pods it's good replica",
    "start": "292290",
    "end": "298050"
  },
  {
    "text": "set will create our pods and then eventually I created pods will be scheduled by another component and then run by yet another component so going",
    "start": "298050",
    "end": "306840"
  },
  {
    "text": "through this a little fast right now I'm gonna start kind of at the top level we're gonna dive deeper and then dive deeper so we talked kubernetes is made",
    "start": "306840",
    "end": "315390"
  },
  {
    "text": "of distributed components when I first got into kubernetes what I imagined is I would go to my computer right and I'd be",
    "start": "315390",
    "end": "321960"
  },
  {
    "text": "like okay yum install kubernetes and there'd be a binary and then I try to run it and it wouldn't be on my path I'd",
    "start": "321960",
    "end": "328830"
  },
  {
    "text": "put it on my path I would point it at other nodes sit back and relax my monitor would glow angels would sing and",
    "start": "328830",
    "end": "334980"
  },
  {
    "text": "I'd bask in the glow of magical container orchestration except that's not the way it works kubernetes uses",
    "start": "334980",
    "end": "341460"
  },
  {
    "text": "distributed systems to provide the ephra mention magical container orchestration it's made of several components and each",
    "start": "341460",
    "end": "347010"
  },
  {
    "text": "of them is worth their own deep-dive each of them does their own pretty neat stuff and each plays a crucial role in running",
    "start": "347010",
    "end": "353040"
  },
  {
    "text": "and operating applications so let's take a look at some of them now I'm talking more detail about each of them in a",
    "start": "353040",
    "end": "358830"
  },
  {
    "text": "moment but I want to introduce them to you before we get too far into things so the four components on the slide here at",
    "start": "358830",
    "end": "364980"
  },
  {
    "text": "CD the API server the controllers and the scheduler make up the control plane these run on the master nodes of the",
    "start": "364980",
    "end": "370680"
  },
  {
    "text": "cluster so starting off at CD at CD is a distributed key-value store that serves",
    "start": "370680",
    "end": "376470"
  },
  {
    "text": "as the clusters database now having access to EDD CD read and write access is equivalent to having root access in",
    "start": "376470",
    "end": "383340"
  },
  {
    "text": "your kubernetes cluster so to control this we stick in front of it a component called the API server other components",
    "start": "383340",
    "end": "390240"
  },
  {
    "text": "in kubernetes talk to ed CD through this API server via good old rest calls next up the controllers so the",
    "start": "390240",
    "end": "397500"
  },
  {
    "text": "controllers handle routine cluster tasks they operate on what is known a reconciliation loop pattern the",
    "start": "397500",
    "end": "403560"
  },
  {
    "text": "controllers will check your desired state check your current state diff the two and then modify your current state",
    "start": "403560",
    "end": "410430"
  },
  {
    "text": "so that it matches the desired state there's a variety of different controllers and we're gonna explore a couple later on so last up is the",
    "start": "410430",
    "end": "416639"
  },
  {
    "text": "scheduler as you might surmise from something that has things scheduled on it there's a scheduler the scheduler is",
    "start": "416639",
    "end": "422460"
  },
  {
    "text": "in charge of scheduling unscheduled pods on the nodes couplet is the star of the",
    "start": "422460",
    "end": "428099"
  },
  {
    "text": "show couplet watches for notes that have been assigned to its pod and then runs them couplet is constantly checking Etsy D by",
    "start": "428099",
    "end": "435389"
  },
  {
    "text": "extension the API server and as well as its local config for pods to run it's constantly going hey let's see D in the",
    "start": "435389",
    "end": "442680"
  },
  {
    "text": "API so every have any pods me to run for me any pods and he pods great any pods cube proxy on the nodes handles any",
    "start": "442680",
    "end": "449490"
  },
  {
    "text": "networking things needed it's in charge of maintaining port forwarding and managing networking rules and since",
    "start": "449490",
    "end": "455430"
  },
  {
    "text": "we're running containers we need some sort of container runtime whether it's docker rocket cryo what-have-you so",
    "start": "455430",
    "end": "461280"
  },
  {
    "text": "these are the components of kubernetes Etsy D is the data store the API server provides access to it the controllers",
    "start": "461280",
    "end": "467610"
  },
  {
    "text": "handle running routine cluster tasks and the scheduler schedules meanwhile couplet is listening for pods to run on",
    "start": "467610",
    "end": "473460"
  },
  {
    "text": "each node coop proxy is maintaining Network rules and there's a container run time running containers so whenever",
    "start": "473460",
    "end": "480840"
  },
  {
    "text": "you're talking about the distributed components of kubernetes you've got to start with Etsy D Etsy D explains so",
    "start": "480840",
    "end": "485940"
  },
  {
    "text": "much about kubernetes behavior so as I mentioned earlier Etsy D is a distributed key-value store that serves",
    "start": "485940",
    "end": "491520"
  },
  {
    "text": "as the database for our cluster if you're familiar with console or zookeeper it's pretty similar to those as well so distributed implies that",
    "start": "491520",
    "end": "499979"
  },
  {
    "text": "there's multiple nodes running all working together to store data so in order to operate majority of the nodes",
    "start": "499979",
    "end": "505710"
  },
  {
    "text": "and our Etsy D cluster need to be up running and healthy so we refer to this",
    "start": "505710",
    "end": "510810"
  },
  {
    "text": "minimum number of healthy nodes as a quorum so what happens if there isn't a quorum what does that CD do to look at",
    "start": "510810",
    "end": "517320"
  },
  {
    "text": "that we're gonna look at everyone's favorite distributed systems concept the cap theorem alright the cap theorem it",
    "start": "517320",
    "end": "525690"
  },
  {
    "text": "says it's impossible for a web service to provide all three of consistency availability and partition tolerance so",
    "start": "525690",
    "end": "533670"
  },
  {
    "text": "we're gonna define consistency always reading the most recently written value for a given key will define",
    "start": "533670",
    "end": "540100"
  },
  {
    "text": "availability as every request to a non failing node receives a response the",
    "start": "540100",
    "end": "546010"
  },
  {
    "text": "partition tolerance will be defined as a system our system can handle the dropping of messages between two nodes",
    "start": "546010",
    "end": "551100"
  },
  {
    "text": "so in the schema provided by the cap theorem EDD is a consistent partition",
    "start": "551100",
    "end": "556120"
  },
  {
    "text": "tolerant system so imagine a 5 node cluster if three of our nodes take a",
    "start": "556120",
    "end": "561310"
  },
  {
    "text": "dive the other two are going to stop responding to requests until the quorum has been restored and we have more than",
    "start": "561310",
    "end": "567700"
  },
  {
    "text": "the minimum number of healthy nodes so why does kubernetes use EDD well if your",
    "start": "567700",
    "end": "572980"
  },
  {
    "text": "goal is to have a distributed system for running containerized applications you need something that plays nice with clustered systems so Etsy DS Doc's say",
    "start": "572980",
    "end": "581290"
  },
  {
    "text": "that once again big quote Etsy D is designed for large-scale distributed systems that never tolerate split-brain",
    "start": "581290",
    "end": "587470"
  },
  {
    "text": "behavior and are willing to sacrifice availability to achieve it split brain behavior is when you have multiple",
    "start": "587470",
    "end": "592630"
  },
  {
    "text": "masters each one thinks they're in charge and they're each setting their own state so you have the you don't have",
    "start": "592630",
    "end": "598690"
  },
  {
    "text": "distributed consensus and you're disagreeing on what the state is not great for kubernetes so you can see the",
    "start": "598690",
    "end": "604450"
  },
  {
    "text": "cap theorem references in this quote though NC d is going to sacrifice availability to preserve consistency now",
    "start": "604450",
    "end": "611860"
  },
  {
    "text": "I think there's an important clarification to make here looking further ahead right so if EDD CD is running as the",
    "start": "611860",
    "end": "616930"
  },
  {
    "text": "kubernetes data store and drops below the minimum number of healthy nodes and is forced to sacrifice availability that",
    "start": "616930",
    "end": "623620"
  },
  {
    "text": "doesn't mean that all the applications running on your cluster your micro services or nginx service servers your",
    "start": "623620",
    "end": "629440"
  },
  {
    "text": "rightist instances they're not all suddenly unavailable what it means is that kubernetes the platform is",
    "start": "629440",
    "end": "634690"
  },
  {
    "text": "unavailable your ship is dead in the water and it you know not a great state you're in but your ships not sinking",
    "start": "634690",
    "end": "640750"
  },
  {
    "text": "you're still taking traffic but you can't schedule new pods add new deployments or do anything that's going",
    "start": "640750",
    "end": "645910"
  },
  {
    "text": "to require a consensus a net CD your existing applications will still be running and taking traffic so Etsy D",
    "start": "645910",
    "end": "653079"
  },
  {
    "text": "does the typical crud operations and a couple other things on top of that via rest calls or command line tool but this",
    "start": "653079",
    "end": "659260"
  },
  {
    "text": "simple interface hides the answers to some complex problems how do the nodes agree on what the value is for a given",
    "start": "659260",
    "end": "665380"
  },
  {
    "text": "key we've got like three nodes out there I alluded as a distributed consensus earlier saying that my you know all the",
    "start": "665380",
    "end": "670480"
  },
  {
    "text": "clocks said it was two o'clock but how does that happen if not done smartly things can go awry pretty quickly so how",
    "start": "670480",
    "end": "677560"
  },
  {
    "text": "does Etsy D agree on the value for a given key while still upholding its consistency guarantee it's the raft",
    "start": "677560",
    "end": "683920"
  },
  {
    "text": "algorithm that's a really great logo by the way I like that so the raft algorithm is a method of achieving",
    "start": "683920",
    "end": "689290"
  },
  {
    "text": "distributed consensus having multiple distributed servers agree and what the value is for a given key in a",
    "start": "689290",
    "end": "695350"
  },
  {
    "text": "fault-tolerant manner now there's a really trivial way to solve this you can just say you know what the answer is",
    "start": "695350",
    "end": "703000"
  },
  {
    "text": "always three everyone agrees on it you know it doesn't matter the time doesn't",
    "start": "703000",
    "end": "708700"
  },
  {
    "text": "matter the date doesn't matter where you are most formal papers though for distributed systems say you can't do that I really think that it started with",
    "start": "708700",
    "end": "715480"
  },
  {
    "text": "like some exasperated person somewhere arguing with somebody but let's take a look at a not wrapped system to",
    "start": "715480",
    "end": "720910"
  },
  {
    "text": "illustrate the difficulties in distributed consensus this is not raft it's not EDD CD it's been specifically",
    "start": "720910",
    "end": "726250"
  },
  {
    "text": "designed to show how distributed consensus can be difficult don't go away and implement this and run it in",
    "start": "726250",
    "end": "731290"
  },
  {
    "text": "production bad idea so in our example system we're gonna have three nodes a B",
    "start": "731290",
    "end": "736990"
  },
  {
    "text": "and C each one holds a single value our nodes will receive requests from a",
    "start": "736990",
    "end": "743230"
  },
  {
    "text": "client it will write to disk and then broadcast the new value out to all other nodes so let's examine some not great",
    "start": "743230",
    "end": "750130"
  },
  {
    "text": "scenarios what happens if there's multiple updates to the value at the same time so here we go",
    "start": "750130",
    "end": "756430"
  },
  {
    "text": "we've got our three nodes a B and C everyone agrees the value is X everyone's in agreement things are happy",
    "start": "756430",
    "end": "762310"
  },
  {
    "text": "things are great and then oh no two clients come in client one tells node a",
    "start": "762310",
    "end": "767620"
  },
  {
    "text": "hey the new value is y client tutos node B hey the new value is going to be Z so",
    "start": "767620",
    "end": "775120"
  },
  {
    "text": "a and B each write the new value to disk writes Y to disc B write Z to disk",
    "start": "775120",
    "end": "780910"
  },
  {
    "text": "meanwhile you know she doesn't know anything yet season 4 trouble so a broadcast the new value Y and B",
    "start": "780910",
    "end": "788230"
  },
  {
    "text": "broadcast the new value Z so we're going to assume that a is messages arrive before bees just via some arbitrary",
    "start": "788230",
    "end": "793480"
  },
  {
    "text": "ordering you can see here we've got problems a and C think the value is Z",
    "start": "793480",
    "end": "799330"
  },
  {
    "text": "whereas B thinks the value is y we have not achieved distribute consensus not great so let's take a look",
    "start": "799330",
    "end": "806300"
  },
  {
    "text": "at another scenario in this new scenario same baseline a B and C are all in",
    "start": "806300",
    "end": "812390"
  },
  {
    "text": "agreement that the value is X things are going great then Oh No the cluster",
    "start": "812390",
    "end": "819560"
  },
  {
    "text": "undergoes a network partition si is unable to talk to a or b and a or b can't talk to see all the messages to",
    "start": "819560",
    "end": "826100"
  },
  {
    "text": "and from C are being dropped meanwhile a client comes along not aware that the",
    "start": "826100",
    "end": "832520"
  },
  {
    "text": "cluster maybe isn't in great shape and says hey guess what the new value is y so a broadcast this out and says hey",
    "start": "832520",
    "end": "839630"
  },
  {
    "text": "everybody the new value is y B gets it successfully it's like okay gotcha meanwhile the message is to C are being",
    "start": "839630",
    "end": "845900"
  },
  {
    "text": "dropped so C doesn't find out meanwhile time goes by and C recovers let C hasn't",
    "start": "845900",
    "end": "853730"
  },
  {
    "text": "gotten the messages because the messages were lost forever and so a and B think the value is y whereas C thinks the value is X sad so",
    "start": "853730",
    "end": "863510"
  },
  {
    "text": "seemingly simple systems can fail in all sorts of fun ways when exposed to concurrent operations in order for",
    "start": "863510",
    "end": "869690"
  },
  {
    "text": "consensus to be achieved we're going to require greater coordination between our nodes we can't just fire and forget like",
    "start": "869690",
    "end": "875300"
  },
  {
    "text": "our terrible example system this is where Raph comes into play so raft is a",
    "start": "875300",
    "end": "880460"
  },
  {
    "text": "consensus algorithm for managing a replicated blog a replicated log is a",
    "start": "880460",
    "end": "885530"
  },
  {
    "text": "series of commands executed in order by a state machine so we want each log to",
    "start": "885530",
    "end": "890900"
  },
  {
    "text": "have the same commands so that the end of the day our log has the same commands in the same order and each machine will",
    "start": "890900",
    "end": "897530"
  },
  {
    "text": "have the same state so to do this raft a LexA leader the leader is put in charge",
    "start": "897530",
    "end": "903170"
  },
  {
    "start": "899000",
    "end": "915000"
  },
  {
    "text": "of managing the log accepts entries from clients replicates them to the other nodes and then tells the nodes when it's",
    "start": "903170",
    "end": "909470"
  },
  {
    "text": "safe to actually commit them if a leader fails a new one is elected in its place",
    "start": "909470",
    "end": "914980"
  },
  {
    "text": "so my mom always said to me Bo you can be a leader or you can be a follower",
    "start": "914980",
    "end": "920830"
  },
  {
    "start": "915000",
    "end": "958000"
  },
  {
    "text": "generally I was in trouble that often included the statement if all your friends jumped off a bridge would you as",
    "start": "920830",
    "end": "926240"
  },
  {
    "text": "well but in raft you have other options you don't have to be just a leader or a follower you can be a candidate as well",
    "start": "926240",
    "end": "932410"
  },
  {
    "text": "so nodes can be in three states followers just hang out and kind of respond two requests from leaders leaders like",
    "start": "932410",
    "end": "938510"
  },
  {
    "text": "hey replicate this followers like alright gotcha so leaders handle all the rights and handle all the fun coordination between",
    "start": "938510",
    "end": "945290"
  },
  {
    "text": "the followers and our third state candidate candidate is the state used to elect a new leader when a leader goes",
    "start": "945290",
    "end": "951530"
  },
  {
    "text": "away or disappears or whatever candidate is the state used to elect a new leader a node will declare itself as a",
    "start": "951530",
    "end": "957110"
  },
  {
    "text": "candidate and say hey vote for me so we talk about leaders well part of dealing with order time stamps is dealing with",
    "start": "957110",
    "end": "963440"
  },
  {
    "text": "time raft uses an incrementing integer timestamp called a term which is tied to an election terms last until there's a",
    "start": "963440",
    "end": "970430"
  },
  {
    "text": "new leader and there's one leader per term terms serve as a logical clock and aid in detecting obsolete info like",
    "start": "970430",
    "end": "976550"
  },
  {
    "text": "stale leaders if the leader finds out that there's a node with a high another leader out there with a higher term that",
    "start": "976550",
    "end": "982820"
  },
  {
    "text": "leader is the leader of the lower term will stand down okay not in charge anymore so the leader sends heartbeat",
    "start": "982820",
    "end": "990170"
  },
  {
    "start": "988000",
    "end": "998000"
  },
  {
    "text": "messages out to the follower nodes saying hey still alive it happens every fairly frequently okay still alive hey",
    "start": "990170",
    "end": "997250"
  },
  {
    "text": "I'm still alive so what happens if a follower doesn't get a heartbeat well it immediately assumes the leaders dead and that's a",
    "start": "997250",
    "end": "1004540"
  },
  {
    "start": "998000",
    "end": "1018000"
  },
  {
    "text": "dark but effective assumption our follow rule increment its term and declare itself as a candidate that means it's",
    "start": "1004540",
    "end": "1011440"
  },
  {
    "text": "election time our new candidate is going to send an RPC request out for votes to all of the other servers so we heard",
    "start": "1011440",
    "end": "1019360"
  },
  {
    "start": "1018000",
    "end": "1066000"
  },
  {
    "text": "yesterday morning about how HBO uses kubernetes with Game of Thrones well the singing game of Thrones was you",
    "start": "1019360",
    "end": "1025810"
  },
  {
    "text": "know you win or you die in the game of ref leadership elections the stakes are lower you win or you lose servers vote for the first candidate who",
    "start": "1025810",
    "end": "1033130"
  },
  {
    "text": "asks for their votes assuming the candidate is at least as up-to-date as the voter this means if the voter has",
    "start": "1033130",
    "end": "1039280"
  },
  {
    "text": "entries that the candidate doesn't the voter isn't going to vote for that candidate now if a candidate gets a",
    "start": "1039280",
    "end": "1045730"
  },
  {
    "text": "majority of the votes it wins no one gets a majority there's another round of elections and Raph uses",
    "start": "1045730",
    "end": "1051760"
  },
  {
    "text": "randomize timeouts to make sure that eventually somebody will win assume your clusters in an okay state so the after",
    "start": "1051760",
    "end": "1058570"
  },
  {
    "text": "the leader gets a majority of its votes it sends out a new set of heartbeat messages to notify others of its victory and life in the cluster goes on suanne",
    "start": "1058570",
    "end": "1066580"
  },
  {
    "start": "1066000",
    "end": "1125000"
  },
  {
    "text": "draft how to rights work well rights I'll go to the leader the",
    "start": "1066580",
    "end": "1071710"
  },
  {
    "text": "leader gets every right the leader then appends the command to its log locally then tells other servers via RPC to",
    "start": "1071710",
    "end": "1079780"
  },
  {
    "text": "append it to their logs as well now it's the followers behind it's gonna say nope I'm behind can't handle this right now",
    "start": "1079780",
    "end": "1086050"
  },
  {
    "text": "and it's like okay so if way too many nodes in your cluster behind spoiler alert nothing's gonna happen so",
    "start": "1086050",
    "end": "1093010"
  },
  {
    "text": "once the majority have appended things to their nodes locally so the leader hasn't appended and a majority of the",
    "start": "1093010",
    "end": "1098350"
  },
  {
    "text": "followers have it appended the leader commits it to its own log locally now in subsequent messages two nodes leader is",
    "start": "1098350",
    "end": "1105580"
  },
  {
    "text": "gonna the leader is going to inform the other nodes the followers of the index of the last committed entry so nodes are",
    "start": "1105580",
    "end": "1111910"
  },
  {
    "text": "gonna notice this me like okay I you've sent me everything up to twelve and you say twelve was committed and I haven't",
    "start": "1111910",
    "end": "1118300"
  },
  {
    "text": "committed nine ten eleven and twelve well I'm gonna commit nine ten and eleven and twelve so it's going to commit everything up to the last",
    "start": "1118300",
    "end": "1124060"
  },
  {
    "text": "committed index now this solves problems in our bad system from before our",
    "start": "1124060",
    "end": "1129640"
  },
  {
    "start": "1125000",
    "end": "1180000"
  },
  {
    "text": "simultaneous right scenario is solved by the requirement that all rights go to the leader the leader is going to",
    "start": "1129640",
    "end": "1135400"
  },
  {
    "text": "receive each right and each right is a new entry with a new index so if two nodes manage to somehow try to update a",
    "start": "1135400",
    "end": "1141880"
  },
  {
    "text": "value simultaneously each one is going to be a new right with a new index one will occur before the other so that way",
    "start": "1141880",
    "end": "1148330"
  },
  {
    "text": "we don't have the situation where you know two nodes think it's one value in one node thinks it's the other value in",
    "start": "1148330",
    "end": "1154080"
  },
  {
    "text": "our network partition scenario followers will reject requests that they're behind and will eventually be caught up also",
    "start": "1154080",
    "end": "1162130"
  },
  {
    "text": "requests go to the leader and our dead node can't become leader because it's behind the effect is minimized and also",
    "start": "1162130",
    "end": "1169690"
  },
  {
    "text": "if we really care and we don't ever want this value to be stale we can force a consensus read it's slower but it makes",
    "start": "1169690",
    "end": "1176230"
  },
  {
    "text": "sure that the value isn't the majority of nodes before it's returned so we",
    "start": "1176230",
    "end": "1181450"
  },
  {
    "start": "1180000",
    "end": "1205000"
  },
  {
    "text": "talked about the cap theorem a little while ago in how raft and sed is a consistent partition tolerance system",
    "start": "1181450",
    "end": "1188010"
  },
  {
    "text": "well this is achieved through requiring a majority of the nodes to act elections require a majority of the nodes to agree",
    "start": "1188010",
    "end": "1194590"
  },
  {
    "text": "to elect a leader all rights require a majority of the nodes to replicate the transaction in order for the transaction",
    "start": "1194590",
    "end": "1199990"
  },
  {
    "text": "to be successful we lose more than half the nodes they weren't able to do anything so that was",
    "start": "1199990",
    "end": "1206280"
  },
  {
    "start": "1205000",
    "end": "1236000"
  },
  {
    "text": "a brief overview have raft I do semi deeply into it but I didn't cover all the safety guarantees and some",
    "start": "1206280",
    "end": "1212850"
  },
  {
    "text": "of the more formal things I think the raft paper the formal paper is actually really great read I'd highly recommend",
    "start": "1212850",
    "end": "1218160"
  },
  {
    "text": "checking it out also for the visual minded of you and really for everybody because it's this is a really great",
    "start": "1218160",
    "end": "1223590"
  },
  {
    "text": "visualization there's a visualization out there called the secret lives of data which shows the way raft clusters",
    "start": "1223590",
    "end": "1229290"
  },
  {
    "text": "interact in various scenarios it's definitely worth checking out so moving",
    "start": "1229290",
    "end": "1237090"
  },
  {
    "start": "1236000",
    "end": "1250000"
  },
  {
    "text": "on controllers controllers are a loop that watch cluster state and make",
    "start": "1237090",
    "end": "1242250"
  },
  {
    "text": "changes to ensure that we keep the desired state so we're gonna take a look at two controllers right now the replica",
    "start": "1242250",
    "end": "1247290"
  },
  {
    "text": "set controller and the deployment controller the replica set controller is in charge of making sure that for any",
    "start": "1247290",
    "end": "1253770"
  },
  {
    "start": "1250000",
    "end": "1290000"
  },
  {
    "text": "given pod spec or any kind of pod image thing there's a given number of pods",
    "start": "1253770",
    "end": "1258960"
  },
  {
    "text": "running at any time now kubernetes actually suggest you don't interact with replica sets directly so we'll instead",
    "start": "1258960",
    "end": "1265050"
  },
  {
    "text": "use the deployment in the deployment controller to manage them now the replica set controller knows which pods",
    "start": "1265050",
    "end": "1270420"
  },
  {
    "text": "to manage through labels so we'll define labels on the pods we create and tell the replicas set which which labels it",
    "start": "1270420",
    "end": "1277080"
  },
  {
    "text": "should look for so that knows which pods to manage well what's a label labels are just key value pairs and the replica set",
    "start": "1277080",
    "end": "1283980"
  },
  {
    "text": "has a selector another sort of key value pair which tells it which labels to select and manage now the deployment",
    "start": "1283980",
    "end": "1291690"
  },
  {
    "start": "1290000",
    "end": "1345000"
  },
  {
    "text": "controller the deployment controller manages the whole deployment process of your app so the deployment controller",
    "start": "1291690",
    "end": "1296850"
  },
  {
    "text": "provides a declarative way of managing pods and then uses that to roll out your desired changes in a friendly way so",
    "start": "1296850",
    "end": "1303210"
  },
  {
    "text": "deployments will handle like rolling deploys you can roll back your application if it's not going well so we",
    "start": "1303210",
    "end": "1308310"
  },
  {
    "text": "talked about the reconciliation leap a little while ago how the deployment process is managed reconciliation loop",
    "start": "1308310",
    "end": "1313860"
  },
  {
    "text": "is going to cut you know check the current state check the desired state diff the two and then modify the current",
    "start": "1313860",
    "end": "1319740"
  },
  {
    "text": "state so that it matches the desired state so if we're rolling out say a new version of the deployment the deployment",
    "start": "1319740",
    "end": "1325290"
  },
  {
    "text": "controller is reconciliation loop will change the desired state ie the counts of the old and new replicas sets you",
    "start": "1325290",
    "end": "1331590"
  },
  {
    "text": "know the old ones will go down the new ones will go up this will get picked up and the replica set controller is going to",
    "start": "1331590",
    "end": "1336810"
  },
  {
    "text": "handle tearing down old versions in spending up new ones as a semi brief overview of deployment controllers but",
    "start": "1336810",
    "end": "1342900"
  },
  {
    "text": "we're gonna see them in action in a little bit so the scheduler the scheduler watches for unscheduled NIT",
    "start": "1342900",
    "end": "1349230"
  },
  {
    "start": "1345000",
    "end": "1369000"
  },
  {
    "text": "pods watches for unscheduled pods and assigns them to a given node what it's looking for specifically our pods",
    "start": "1349230",
    "end": "1355500"
  },
  {
    "text": "without a node name constantly squaring the API server and by extension EDD looking for these unscheduled pods one",
    "start": "1355500",
    "end": "1363420"
  },
  {
    "text": "thing to note is that the scheduler doesn't isn't actually in charge of running pods it's only in charge of assigning pods to a given node so he",
    "start": "1363420",
    "end": "1370350"
  },
  {
    "start": "1369000",
    "end": "1383000"
  },
  {
    "text": "does this by a scheduling algorithm first off it filters out nodes that aren't desired or just aren't a great",
    "start": "1370350",
    "end": "1376320"
  },
  {
    "text": "fit it then ranks the remaining nodes and picks the top-ranked node after",
    "start": "1376320",
    "end": "1381650"
  },
  {
    "text": "ranking is complete so step one we're filtering its predicates we're filtering",
    "start": "1381650",
    "end": "1387180"
  },
  {
    "start": "1383000",
    "end": "1567000"
  },
  {
    "text": "out nodes that aren't desired or not a great fit I think the simplest scheduling strategy is to try to assign",
    "start": "1387180",
    "end": "1393240"
  },
  {
    "text": "a pod directly to a node so you can do this via a node name specified in the",
    "start": "1393240",
    "end": "1398730"
  },
  {
    "text": "pod spec the host name predicate is in charge of checking this it's going to compare the nodes name to the node name",
    "start": "1398730",
    "end": "1404310"
  },
  {
    "text": "specified in the pods back if any so it's going to exclude every node that doesn't match and it will let us",
    "start": "1404310",
    "end": "1410160"
  },
  {
    "text": "schedule directly to a node assuming it passes the other predacons there's some resource aware predicate Slater that we'll check that will I could prevent it",
    "start": "1410160",
    "end": "1416850"
  },
  {
    "text": "from scheduling so match node selector is kind of a variance of hostname kubernetes lets you provide label",
    "start": "1416850",
    "end": "1422880"
  },
  {
    "text": "selectors to associate resources using labels this predicate checks whether the node selector supplied in the pod spec",
    "start": "1422880",
    "end": "1429720"
  },
  {
    "text": "is valid for a given node for example let's imagine that you only want to schedule on high bandwidth nodes you",
    "start": "1429720",
    "end": "1436260"
  },
  {
    "text": "want the network power you could specify a high bandwidth label put it on your node and then add a selector to your pod",
    "start": "1436260",
    "end": "1442860"
  },
  {
    "text": "spec to only scheduled on nodes with this label so there's also the concept of affinity which is a more expressive",
    "start": "1442860",
    "end": "1449250"
  },
  {
    "text": "and flexible version of the node selector concept you're not just limited to an exact match can you set matching and all sorts of",
    "start": "1449250",
    "end": "1455190"
  },
  {
    "text": "fun things also you can match not only on nodes but pods as well using affinity accordingly there's a predicate that",
    "start": "1455190",
    "end": "1461640"
  },
  {
    "text": "checks whether or not the pod plays nice with the affinity settings of the pods on that node the inner pod affinity",
    "start": "1461640",
    "end": "1467490"
  },
  {
    "text": "matches predicate so taints are kind of the inverse of affinity we can specify a taint that",
    "start": "1467490",
    "end": "1473760"
  },
  {
    "text": "tells pods to avoid certain nodes pods won't schedule two nodes with taints unless the pod explicitly tolerates them",
    "start": "1473760",
    "end": "1481380"
  },
  {
    "text": "so if you didn't want to schedule to a node for example you could add a no schedule taint and nothing will schedule",
    "start": "1481380",
    "end": "1487110"
  },
  {
    "text": "to it unless they specifically add a no schedule Toleration the pod tolerates no",
    "start": "1487110",
    "end": "1492450"
  },
  {
    "text": "taints predicate checks whether or not the pod has opted in to tolerating a given nodes taints so the scheduler like",
    "start": "1492450",
    "end": "1499950"
  },
  {
    "text": "I mentioned also checks the resources of each node the pod fits host port predicate checks whether or not the host",
    "start": "1499950",
    "end": "1506070"
  },
  {
    "text": "port a hard-coded port specified in the pod spec is available for that node if",
    "start": "1506070",
    "end": "1511620"
  },
  {
    "text": "it's not available we're going to filter out that host from being scheduled to for this given pod so pod fits resources",
    "start": "1511620",
    "end": "1518490"
  },
  {
    "text": "is another resource aware check pod suits request a given amount of CPU in memory this predicate checks whether the",
    "start": "1518490",
    "end": "1525210"
  },
  {
    "text": "node is capable of satisfying that request so check node memory pressure and check node disk pressure or at some other",
    "start": "1525210",
    "end": "1531720"
  },
  {
    "text": "predicates they won't schedule on to nodes whose memory usage or disk usage is too high check node condition is a more extreme",
    "start": "1531720",
    "end": "1539220"
  },
  {
    "text": "version of this check node condition prevents pods from being scheduled to nodes who are unavailable or network",
    "start": "1539220",
    "end": "1545070"
  },
  {
    "text": "unavailable who aren't ready according to kubernetes or just plain out of disk space those are conditions that would",
    "start": "1545070",
    "end": "1550919"
  },
  {
    "text": "you know make a nodes suboptimal for scheduling too I didn't list them here but there's also some volume checks so",
    "start": "1550919",
    "end": "1557040"
  },
  {
    "text": "if you're on a cloud provider there's often no limit to how many volumes you can have attached and make sure that we're not using more than the allowed",
    "start": "1557040",
    "end": "1562679"
  },
  {
    "text": "number of volumes or maybe that there's no conflicts on volume claims so after",
    "start": "1562679",
    "end": "1567840"
  },
  {
    "start": "1567000",
    "end": "1611000"
  },
  {
    "text": "we filtered out our nodes we have a set of nodes that are valid ish for",
    "start": "1567840",
    "end": "1573900"
  },
  {
    "text": "scheduling some are probably better than others now you could also be left with no nodes in which case your pot is",
    "start": "1573900",
    "end": "1579150"
  },
  {
    "text": "unschedulable in that scenario you're going to need to go back and modify the pods back or maybe wait for some other",
    "start": "1579150",
    "end": "1584940"
  },
  {
    "text": "nodes to come up or take some sort of action possibly in order to make your pod schedule pool again but if you've got a set of nodes that",
    "start": "1584940",
    "end": "1592140"
  },
  {
    "text": "you can possibly schedule - it's time to rank ranking applies a series of priority functions that return a score a",
    "start": "1592140",
    "end": "1598050"
  },
  {
    "text": "higher score is more desirable so functions are run against each node they're all added up and the node with",
    "start": "1598050",
    "end": "1604740"
  },
  {
    "text": "the highest score is the winner ties are broken randomly by picking a winner so",
    "start": "1604740",
    "end": "1611669"
  },
  {
    "start": "1611000",
    "end": "1723000"
  },
  {
    "text": "here's some of the ranking functions and so when you think of scheduling right you want to put your paadam maybe the",
    "start": "1611669",
    "end": "1617039"
  },
  {
    "text": "least used node so the least requested priority helps out with this it calculates how much CPU and memory",
    "start": "1617039",
    "end": "1623340"
  },
  {
    "text": "equally weighted and added together would be left after scheduling our pod to that node so this kind of helps with",
    "start": "1623340",
    "end": "1629490"
  },
  {
    "text": "balancing resource usage across the cluster now balanced resource allocation is a more focused version of this it's",
    "start": "1629490",
    "end": "1636059"
  },
  {
    "text": "attempting to prevent nodes from being largely weighted towards CPU usage or memory usage it's specifically trying to",
    "start": "1636059",
    "end": "1642270"
  },
  {
    "text": "avoid news with like 95% CPU but only 5% memory so it checks to see how the pod",
    "start": "1642270",
    "end": "1648179"
  },
  {
    "text": "would affect the balance of resource usage on that given node this function is going to favor nodes who would have",
    "start": "1648179",
    "end": "1653730"
  },
  {
    "text": "CPU utilization closer to memory utilization after scheduling so next up",
    "start": "1653730",
    "end": "1658860"
  },
  {
    "text": "is selector spread priority we lose a great chunk of the benefits of having multiple copies of an application",
    "start": "1658860",
    "end": "1664409"
  },
  {
    "text": "they're all sitting out there on the same node that node goes down you'll lose every copy of your app that's",
    "start": "1664409",
    "end": "1669990"
  },
  {
    "text": "running on the node there's a metaphor about eggs and baskets I think that's really helpful here this function",
    "start": "1669990",
    "end": "1675000"
  },
  {
    "text": "minimizes the amount of pods from the same service or replicas set on the same node causing our currently being",
    "start": "1675000",
    "end": "1680400"
  },
  {
    "text": "scheduled pods to favor nodes without it's managed siblings so the image locality priority function while",
    "start": "1680400",
    "end": "1687000"
  },
  {
    "text": "containers have images and oftentimes we have to download them this function prioritizes nodes that have already",
    "start": "1687000",
    "end": "1693059"
  },
  {
    "text": "downloaded more of the pods images if you've got them all you're gonna get the highest score if you've got none of them",
    "start": "1693059",
    "end": "1698130"
  },
  {
    "text": "you're gonna get the lowest score it's a node affinity priority and tank Toleration priority notice any priority",
    "start": "1698130",
    "end": "1703950"
  },
  {
    "text": "favors nodes that have that match the pods nodes affinity preferences taint toleration priority prioritizes nodes",
    "start": "1703950",
    "end": "1709980"
  },
  {
    "text": "that have taints that are specifically tolerated by the pod if you've gone out of your way to request some affinity or",
    "start": "1709980",
    "end": "1716280"
  },
  {
    "text": "explicitly tolerate something kubernetes is gonna notice that and say you know what you're gonna get a little boost to",
    "start": "1716280",
    "end": "1721919"
  },
  {
    "text": "schedule into this node so what happens when we submit a deployment to kubernetes we've talked about all these",
    "start": "1721919",
    "end": "1728309"
  },
  {
    "text": "components but let's see how they work together so I've got this deployment here I toss together a really quick",
    "start": "1728309",
    "end": "1733520"
  },
  {
    "start": "1731000",
    "end": "1746000"
  },
  {
    "text": "hello world go app the it's gonna be listening on port 8080 I've labeled it with the TAT the tag app",
    "start": "1733520",
    "end": "1741440"
  },
  {
    "text": "hello world and I say I want three of you and I've also specified a service a",
    "start": "1741440",
    "end": "1748299"
  },
  {
    "start": "1746000",
    "end": "1770000"
  },
  {
    "text": "service will select pods and provide access to them I've see the spec selector app field I've given it the tag",
    "start": "1748299",
    "end": "1755179"
  },
  {
    "text": "app hello cube Khan and so the service is going to select that pods with that",
    "start": "1755179",
    "end": "1760460"
  },
  {
    "text": "tag we've also said hey I want you to Ford port 80 over to port 8080 and I",
    "start": "1760460",
    "end": "1767420"
  },
  {
    "text": "want you create a load balancer for me so how are we gonna actually submit this good old cube cuddle I'm on Team Q",
    "start": "1767420",
    "end": "1773720"
  },
  {
    "start": "1770000",
    "end": "1795000"
  },
  {
    "text": "cuddle by the way for pronunciation there's differing opinions I believe cube cuddles the best one so we use Q",
    "start": "1773720",
    "end": "1779630"
  },
  {
    "text": "cuttle to get our app running in kubernetes it's how we interact with the cluster in view resources as well as modify them gives us a great view of",
    "start": "1779630",
    "end": "1786290"
  },
  {
    "text": "what's going on you can check the status of our pods our deployments but really importantly for this section use it to",
    "start": "1786290",
    "end": "1792169"
  },
  {
    "text": "construct a timeline of events so what do we expect to happen here well we're",
    "start": "1792169",
    "end": "1798830"
  },
  {
    "text": "gonna create a planet a deployment you know cube cube cuddle create a chef deployment you animal our deployment is",
    "start": "1798830",
    "end": "1804440"
  },
  {
    "text": "gonna create a replica set a replica set will create three pods our scheduler will schedule those three pods and then",
    "start": "1804440",
    "end": "1812030"
  },
  {
    "text": "Kubel it which is listening looking for pods to run on agent its node is going to run the schedule of pods so what",
    "start": "1812030",
    "end": "1819679"
  },
  {
    "start": "1818000",
    "end": "1830000"
  },
  {
    "text": "actually happens now before we go on I just want to warn you these are real events they have been formatted to fit",
    "start": "1819679",
    "end": "1826880"
  },
  {
    "text": "my screen and viewer discretion is advised so starting off with we can see here the",
    "start": "1826880",
    "end": "1833660"
  },
  {
    "text": "involved object kind so our deployment is the involved object the one that I just just created assuming I just gone",
    "start": "1833660",
    "end": "1840230"
  },
  {
    "text": "cue cuddle create a chef and the message is that we've scaled up the replica set okay well I guess that means we created",
    "start": "1840230",
    "end": "1845600"
  },
  {
    "text": "a replica set as well so we created a replica set for our deployment and set it at three you can see also the source",
    "start": "1845600",
    "end": "1851540"
  },
  {
    "text": "of it the deployment controllers done this its noticed via API the API server",
    "start": "1851540",
    "end": "1856940"
  },
  {
    "text": "net CD hey a new deployments been created I should check that out and do things accordingly the reconciliation",
    "start": "1856940",
    "end": "1863000"
  },
  {
    "text": "loop here what's the current state what's the desired state okay the current state is there's not a",
    "start": "1863000",
    "end": "1868800"
  },
  {
    "text": "replicas set for this the desired state is there should be a replica set for this let's create it and then scale it up alright now we can see the source",
    "start": "1868800",
    "end": "1876000"
  },
  {
    "text": "component the replica set controllers kicked in it's created a pod we can see the involved object at the top as well",
    "start": "1876000",
    "end": "1881580"
  },
  {
    "text": "as the replica set so a replica set has created a pod for us now we've got two more to create before we do that the",
    "start": "1881580",
    "end": "1889350"
  },
  {
    "start": "1887000",
    "end": "1977000"
  },
  {
    "text": "schedule is kicked in so we can see the source component is the default scheduler and the involved object is a",
    "start": "1889350",
    "end": "1895140"
  },
  {
    "text": "pod the default scheduler is doing something to the pod it is successfully assigned this pod nwc 7k looking at the",
    "start": "1895140",
    "end": "1902520"
  },
  {
    "text": "end of it to a given node in gke so that's pretty great and then hey look",
    "start": "1902520",
    "end": "1908100"
  },
  {
    "text": "the replica set controllers back it's created a pod ooh three hfh you see the",
    "start": "1908100",
    "end": "1915300"
  },
  {
    "text": "involved object is the replica set its created a pod for us is it gonna be scheduled next it is yay",
    "start": "1915300",
    "end": "1922230"
  },
  {
    "text": "so the scheduler has scheduled our second pod and it's assigned it to 33 jg",
    "start": "1922230",
    "end": "1928080"
  },
  {
    "text": "what's the last mo signs here 33 jg so that's interesting we talked about you",
    "start": "1928080",
    "end": "1934500"
  },
  {
    "text": "know the various functions in ranking things one of them was select or spread",
    "start": "1934500",
    "end": "1939810"
  },
  {
    "text": "priority it's gonna favor spreading things out to other nodes well because they're all added together and they're",
    "start": "1939810",
    "end": "1944940"
  },
  {
    "text": "all weighted selector spreader already actually lost out here I had some other things running in my cluster and said even though we desire to have things in",
    "start": "1944940",
    "end": "1951420"
  },
  {
    "text": "multiple places this node is such a great fit that we want everything to go",
    "start": "1951420",
    "end": "1956610"
  },
  {
    "text": "here luckily for us this is a very very very tiny web app it's you know",
    "start": "1956610",
    "end": "1962040"
  },
  {
    "text": "basically a hello world and then I rub his second choice come back and create our third pod Oh 5 kv 9 and it's",
    "start": "1962040",
    "end": "1971400"
  },
  {
    "text": "successfully been assigned by the scheduler to our node the same node 33 JG so this is something different",
    "start": "1971400",
    "end": "1978990"
  },
  {
    "start": "1977000",
    "end": "2041000"
  },
  {
    "text": "couplets kicked in couplet has been like okay I've got a pot of gonna deal with well first I'm gonna pull my image so",
    "start": "1978990",
    "end": "1986250"
  },
  {
    "text": "it's pulling Boni Ingraham slash hello HTTP latest from docker hub and it's downloading that oh the second pod is",
    "start": "1986250",
    "end": "1993690"
  },
  {
    "text": "being having its images download it's downloading that single image it's",
    "start": "1993690",
    "end": "1999330"
  },
  {
    "text": "downloading the third image why is it downloading at the same time that's a great question I assume it's because I",
    "start": "1999330",
    "end": "2004539"
  },
  {
    "text": "probably shouldn't freestyle on this but I believe it's because it hasn't",
    "start": "2004539",
    "end": "2010039"
  },
  {
    "text": "completed the download on any of them yet or it could be my image settings on the deployment so we successfully pulled",
    "start": "2010039",
    "end": "2017000"
  },
  {
    "text": "the image yay it's downloaded we can see that we downloaded the pod like the",
    "start": "2017000",
    "end": "2022220"
  },
  {
    "text": "involved object is the pod couplet is the source component that's handling this we could also see there's a third",
    "start": "2022220",
    "end": "2028519"
  },
  {
    "text": "field and involved object as well that we haven't seen before filled path so it's saying okay I'm trying to find the",
    "start": "2028519",
    "end": "2034909"
  },
  {
    "text": "containers in this hello coop con deployment and that's how it got the image thing that it's downloading so",
    "start": "2034909",
    "end": "2042049"
  },
  {
    "start": "2041000",
    "end": "2076000"
  },
  {
    "text": "after it's successfully pulled the image it creates a container and then it starts a container so cubelet has",
    "start": "2042049",
    "end": "2047929"
  },
  {
    "text": "downloaded a pulled an image successfully downloaded it created a container and then started a container",
    "start": "2047929",
    "end": "2053270"
  },
  {
    "text": "so I expect to see this cycle two more times because we have three pods we need to start up each with a single container",
    "start": "2053270",
    "end": "2059378"
  },
  {
    "text": "so we successfully pulled the image we've created the container for their next pod and we've started it as well",
    "start": "2059379",
    "end": "2066940"
  },
  {
    "text": "the third pod yep we successfully pulled the image we've created the container",
    "start": "2066940",
    "end": "2072490"
  },
  {
    "text": "and we started the container so we did it yay if you can navigate it to your",
    "start": "2072490",
    "end": "2080599"
  },
  {
    "start": "2076000",
    "end": "2102000"
  },
  {
    "text": "browser you would see Hello coupe con Kelsey did almost the exact same thing",
    "start": "2080599",
    "end": "2086599"
  },
  {
    "text": "yesterday morning I just want to say I am I github out there or my docker hub container out there three days ago",
    "start": "2086599",
    "end": "2093200"
  },
  {
    "text": "but either way great minds think alike I am honored to have the same thought so",
    "start": "2093200",
    "end": "2100880"
  },
  {
    "text": "what have we done today so we've looked at the various components that make up kubernetes we've shown how kubernetes",
    "start": "2100880",
    "end": "2106820"
  },
  {
    "start": "2102000",
    "end": "2126000"
  },
  {
    "text": "handles distributed state we do even how we reconcile and schedule reconcile state and schedule pods and then finally",
    "start": "2106820",
    "end": "2114170"
  },
  {
    "text": "we trace the deployment through the system so we're out of time but I learned a ton putting this talk together",
    "start": "2114170",
    "end": "2119780"
  },
  {
    "text": "and I hope you all learned something today as well thanks [Applause]",
    "start": "2119780",
    "end": "2127859"
  }
]