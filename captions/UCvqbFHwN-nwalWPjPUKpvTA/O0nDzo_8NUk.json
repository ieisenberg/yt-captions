[
  {
    "text": "hello my name is ankur I lead the machine learning as a service mlas for",
    "start": "2700",
    "end": "7859"
  },
  {
    "text": "short team at Capital One my colleagues David Harrington Patrick Hennis Trevor hallak Cruz Hall and",
    "start": "7859",
    "end": "14099"
  },
  {
    "text": "Christian langholm and I are excited to tell you more about our platform and share some of the operational challenges",
    "start": "14099",
    "end": "20160"
  },
  {
    "text": "and Lessons Learned in running the platform on kubernetes I also want to give a shout out to Suman",
    "start": "20160",
    "end": "25680"
  },
  {
    "text": "karapali our architect and Jason Stryker Alan Munn and Michael Andrews from our kubernetes SRE team who have been",
    "start": "25680",
    "end": "32700"
  },
  {
    "text": "incredible Partners on our kubernetes journey before proceeding further let me take a",
    "start": "32700",
    "end": "39000"
  },
  {
    "text": "moment to mention Capital one's commitment to the open source community Capital One made an open source first",
    "start": "39000",
    "end": "44460"
  },
  {
    "text": "declaration in 2014 and that's when we made our first contributions to the open source Community we sponsored Pinos",
    "start": "44460",
    "end": "51360"
  },
  {
    "text": "python continuous delivery and the cloud native Computing foundations to help",
    "start": "51360",
    "end": "56879"
  },
  {
    "text": "keep open source sustainable Capital one's contributions to the open source Community have been significant",
    "start": "56879",
    "end": "62640"
  },
  {
    "text": "and we have released more than 40 of our own software projects we have invested for years to build the",
    "start": "62640",
    "end": "69119"
  },
  {
    "text": "culture and governance required to be open source first in a highly regulated industry",
    "start": "69119",
    "end": "74280"
  },
  {
    "text": "I will now hand it over to my colleague David Harrington who will walk you through a high-level architecture of our",
    "start": "74280",
    "end": "79979"
  },
  {
    "text": "platform David over to you thanks Andrew hi so for today's agenda we're going to",
    "start": "79979",
    "end": "85560"
  },
  {
    "text": "walk through an example architecture of building a simple software as a service platform we're going to talk through how",
    "start": "85560",
    "end": "91740"
  },
  {
    "text": "we use kubernetes and how we organize our teams and software in order to accomplish that many of the concepts",
    "start": "91740",
    "end": "97439"
  },
  {
    "text": "likely aren't going to be new for most folks who have basic understandings of the Primitive kubernetes objects but",
    "start": "97439",
    "end": "103380"
  },
  {
    "text": "rather our intent with this presentation is for those to walk for those to walk away with a better appreciation for the",
    "start": "103380",
    "end": "110340"
  },
  {
    "text": "how we combine the basic concepts of kubernetes into a higher order system",
    "start": "110340",
    "end": "116119"
  },
  {
    "text": "so diving in at the heart of it our platform runs ML and data processing pipelines at scale on kubernetes we seek",
    "start": "116700",
    "end": "124079"
  },
  {
    "text": "to make it easy to run out of the box analytics on the desired data sets in a standardized secure manner in this talk",
    "start": "124079",
    "end": "130500"
  },
  {
    "text": "we're going to cover our journey in using kubernetes cover some of the foundational processes design considerations and pepper in some",
    "start": "130500",
    "end": "136739"
  },
  {
    "text": "examples of incidents to elucidate while getting these patterns right is crucial now before we dive deep on any one area",
    "start": "136739",
    "end": "143760"
  },
  {
    "text": "on the agenda it's important we level set on what are requirements in high level architecture look like",
    "start": "143760",
    "end": "149879"
  },
  {
    "text": "as for requirements we need a one to be able to run batch jobs on demand that",
    "start": "149879",
    "end": "155099"
  },
  {
    "text": "connects to end users data stores two we need to be able to enable non-technical",
    "start": "155099",
    "end": "160440"
  },
  {
    "text": "users to configure and launch these jobs via a UI and three we need to enable",
    "start": "160440",
    "end": "166260"
  },
  {
    "text": "lease privilege flexible data access now as a large organization it's",
    "start": "166260",
    "end": "171720"
  },
  {
    "text": "important that we adopt a multi-tenant architecture in order to help ensure lease privileged data access we wouldn't",
    "start": "171720",
    "end": "177900"
  },
  {
    "text": "want people to be able to access data they shouldn't be able to you know one of those important requirements uh for this platform is",
    "start": "177900",
    "end": "184620"
  },
  {
    "text": "that not not everyone is going to be an engineer with direct system access to our cluster rather we need to be able to",
    "start": "184620",
    "end": "190980"
  },
  {
    "text": "serve users via UI in addition to API Services accomplishing the above uh",
    "start": "190980",
    "end": "196680"
  },
  {
    "text": "provisioning of a job is not as simple as having someone run a coupe cuddle apply regardless of where the user",
    "start": "196680",
    "end": "202860"
  },
  {
    "text": "Journey starts we must ensure the same properties of compute Network and data",
    "start": "202860",
    "end": "208019"
  },
  {
    "text": "isolation now in this diagram we have somewhat of a stripped down basic version of what",
    "start": "208019",
    "end": "213720"
  },
  {
    "text": "our system does as a given to this presentation we're running on a kubernetes cluster more on that later",
    "start": "213720",
    "end": "218760"
  },
  {
    "text": "and like any platform we have apis uis databases and most importantly our",
    "start": "218760",
    "end": "224700"
  },
  {
    "text": "platform does something of hopefully value to the end user in this case running some standardized analytics jobs",
    "start": "224700",
    "end": "231260"
  },
  {
    "text": "uh now what the jobs do for this presentation doesn't matter all that",
    "start": "231260",
    "end": "236640"
  },
  {
    "text": "much um other than the fact they might require a customized networking but these jobs need to be able to",
    "start": "236640",
    "end": "242459"
  },
  {
    "text": "connect to the end user's desired data sets as we see here on the right so a huge part of this platform is",
    "start": "242459",
    "end": "249060"
  },
  {
    "text": "running a reliable kubernetes cluster how do you upgrade across your production well production configuration",
    "start": "249060",
    "end": "254220"
  },
  {
    "text": "meets the requirements for your Enterprise what add-ons are necessary for the operation of your platform these",
    "start": "254220",
    "end": "260459"
  },
  {
    "text": "aren't easy questions we have a central SRE team whose responsibility is to provide platform teams like ourselves",
    "start": "260459",
    "end": "266580"
  },
  {
    "text": "the automation tools to provision and manage a production grade cluster this is the Bedrock on which our platform is",
    "start": "266580",
    "end": "273540"
  },
  {
    "text": "built upon without it we cannot securely or reliably do any of the fun things like running thousands of Pipelines",
    "start": "273540",
    "end": "280560"
  },
  {
    "text": "it is important to mention that a central SRE team does not mean we run all kubernetes workloads at Capital One",
    "start": "280560",
    "end": "286320"
  },
  {
    "text": "on One big old cluster rather for large organizations where you may have many complex platforms that is not advisable",
    "start": "286320",
    "end": "293400"
  },
  {
    "text": "to share clusters across platforms needs May differ much lower in the stack making coordination of releases thorny",
    "start": "293400",
    "end": "300240"
  },
  {
    "text": "to say the least this Hub and spoke model provides the best of both worlds of a central team of",
    "start": "300240",
    "end": "305340"
  },
  {
    "text": "experts and the right size scope limited clusters with more predictable Behavior",
    "start": "305340",
    "end": "310680"
  },
  {
    "text": "now on to the next layer in our architecture this is a big one our platform apis it is conceivably the",
    "start": "310680",
    "end": "317160"
  },
  {
    "text": "entry point for all user interactions on the system an entire talk can be dedicated just to how to build",
    "start": "317160",
    "end": "322560"
  },
  {
    "text": "multi-tenants offers of service applications but that's not the purview of this talk rather what's important",
    "start": "322560",
    "end": "328199"
  },
  {
    "text": "about our platform apis is how they interact with our cluster and the layers below it",
    "start": "328199",
    "end": "333539"
  },
  {
    "text": "when building a multi-tenant system like this you're always faced with the question of whether you want to provision a copy of your stack per",
    "start": "333539",
    "end": "340020"
  },
  {
    "text": "tenant or utilize a shared service model what does that mean in terms of terms of",
    "start": "340020",
    "end": "345060"
  },
  {
    "text": "kubernetes if you have a platform API that needs to create jobs on behalf of users is it",
    "start": "345060",
    "end": "350759"
  },
  {
    "text": "better to have a single service that has permissions to deploy jobs for all users perhaps across many namespaces or is it",
    "start": "350759",
    "end": "358440"
  },
  {
    "text": "preferable to have a deployment of your service for each tenant where the permissions of each API is specific to",
    "start": "358440",
    "end": "364320"
  },
  {
    "text": "each tenant the blast radius May differ depending on your approach and so that's a judgment",
    "start": "364320",
    "end": "369780"
  },
  {
    "text": "call that cannot be made universally for all platforms as both options have their own sets of pros and cons",
    "start": "369780",
    "end": "376080"
  },
  {
    "text": "regardless of whether you have more of a single tenant model for your services or a shared multi-tenant model we still",
    "start": "376080",
    "end": "382080"
  },
  {
    "text": "need to manage resources on behalf of our users which leads us to the next part of our architecture next slide",
    "start": "382080",
    "end": "388259"
  },
  {
    "text": "please the primary function of this layer is to maintain and manage the specifications of resources for your",
    "start": "388259",
    "end": "395100"
  },
  {
    "text": "users the API is not much different in responsibility from your SRE job",
    "start": "395100",
    "end": "400199"
  },
  {
    "text": "function maintaining say Helm charts for deployment the engineer must maintain and upgrade the deployment when",
    "start": "400199",
    "end": "406380"
  },
  {
    "text": "appropriate deprecate it when it's time and have a disaster plan in place for when the service or cluster goes down",
    "start": "406380",
    "end": "412199"
  },
  {
    "text": "only in this instance instead of an engineer committing code to Version Control and kicking off builds all of",
    "start": "412199",
    "end": "419100"
  },
  {
    "text": "this management has to be codified and automated as to be repeatable arbitrarily many times which leads us to",
    "start": "419100",
    "end": "425280"
  },
  {
    "text": "this layer the tenant sandbox for our example platform we're running jobs for our users we want to limit what",
    "start": "425280",
    "end": "431699"
  },
  {
    "text": "these jobs are capable of doing as we laid out in our initial requirements we want a minimal Network surface a limited",
    "start": "431699",
    "end": "438060"
  },
  {
    "text": "set of capabilities permissions and available resources as to avoid any one",
    "start": "438060",
    "end": "443340"
  },
  {
    "text": "tenant causing problems or maybe even snooping where they should one of the primary functions of the",
    "start": "443340",
    "end": "449520"
  },
  {
    "text": "outer platform apis is to automate the provisioning of the sandbox whenever a",
    "start": "449520",
    "end": "455039"
  },
  {
    "text": "new tenant signs up for the platform and if we manage the environment well then the next part should be simple",
    "start": "455039",
    "end": "460380"
  },
  {
    "text": "enough we get to run our meaningful workloads for our users when indirectly opening up what can run on your cluster",
    "start": "460380",
    "end": "466979"
  },
  {
    "text": "to a large number of people of varying skills and backgrounds problems at this layer are bound to arise in this",
    "start": "466979",
    "end": "473280"
  },
  {
    "text": "presentation we're going to cover techniques to catch problems as they arise and avoid classes of errors and",
    "start": "473280",
    "end": "478680"
  },
  {
    "text": "with that I'm going to hand it over to Christian thank you David",
    "start": "478680",
    "end": "483960"
  },
  {
    "text": "all right so I'll talk to you about how to go about updating your cluster",
    "start": "483960",
    "end": "490139"
  },
  {
    "text": "um though it can feel a more mundane and routine part of your uh deployment process than say your platform",
    "start": "490139",
    "end": "496560"
  },
  {
    "text": "deployments uh cluster upgrades require equal presentation preparation and",
    "start": "496560",
    "end": "501720"
  },
  {
    "text": "attention to detail your cluster is the foundation upon which your platform is",
    "start": "501720",
    "end": "506759"
  },
  {
    "text": "built and therefore can have and lead to unintended consequences when that",
    "start": "506759",
    "end": "512099"
  },
  {
    "text": "Foundation changes unexpectedly it's therefore important to have a plan before during and after any up upcoming",
    "start": "512099",
    "end": "519839"
  },
  {
    "text": "cluster upgrades to identify potential regressions test and monitor for those regressions and recover from those",
    "start": "519839",
    "end": "526380"
  },
  {
    "text": "regressions if necessary this plan should be equally robust and uh equally",
    "start": "526380",
    "end": "532380"
  },
  {
    "text": "ready as your platform deployment plan such a plan is equally important whether",
    "start": "532380",
    "end": "538320"
  },
  {
    "text": "you own your cluster scripts or if they come pre-packaged for your use like in our case",
    "start": "538320",
    "end": "543360"
  },
  {
    "text": "and a good cluster plan includes the following steps first uh listen for upcoming cluster",
    "start": "543360",
    "end": "550080"
  },
  {
    "text": "upgrade dates or deadlines and prepare your team to have a resource or resources on standby for potential",
    "start": "550080",
    "end": "556440"
  },
  {
    "text": "failures rollbacks or hotfixes ideally these support resources have been set aside already as part of standard a",
    "start": "556440",
    "end": "563940"
  },
  {
    "text": "standard support rotation and have been prepared with this action plan in advance second make sure you review the change",
    "start": "563940",
    "end": "570899"
  },
  {
    "text": "log and any relevant documentation for the upcoming upgrade and identify known",
    "start": "570899",
    "end": "576420"
  },
  {
    "text": "breaking changes and suspected points of failure third prepare for the upgrade if you",
    "start": "576420",
    "end": "583080"
  },
  {
    "text": "have identified definite breaking changes make a plan to implement the necessary changes before the update",
    "start": "583080",
    "end": "588839"
  },
  {
    "text": "deadline it's important to review upcoming changes well enough in advance to make these preparations second if you",
    "start": "588839",
    "end": "596820"
  },
  {
    "text": "suspect any changes might be problematic or cause failures ensure your tests and",
    "start": "596820",
    "end": "601860"
  },
  {
    "text": "monitors and alerts cover those potential points of failure so that they can be quickly identified post upgrade",
    "start": "601860",
    "end": "608880"
  },
  {
    "text": "four test monitor and alert for regressions during and after the cluster",
    "start": "608880",
    "end": "614399"
  },
  {
    "text": "upgrade ensure that your entire test Suite is run including integration tests end-to-end tests and performance tests",
    "start": "614399",
    "end": "621180"
  },
  {
    "text": "these are powerful Tools in identifying regressions quickly no test Suite is a complete picture",
    "start": "621180",
    "end": "627240"
  },
  {
    "text": "however so it's equally important to monitor your logs performance metrics and application Health wherever possible",
    "start": "627240",
    "end": "634200"
  },
  {
    "text": "using whatever tools are in Your Arsenal five after the upgrade",
    "start": "634200",
    "end": "640680"
  },
  {
    "text": "um conclude there are no regressions introduced into your platform and you can safely sign off on the cluster",
    "start": "640680",
    "end": "647519"
  },
  {
    "text": "upgrade as a success in the current environment however if regressions have been",
    "start": "647519",
    "end": "652800"
  },
  {
    "text": "discovered ensure you communicate your Discovery and determine whether you need to roll back and deploy a fix only after",
    "start": "652800",
    "end": "659459"
  },
  {
    "text": "a successful assessment of your current environment should you elevate your cluster upgrade to your next higher",
    "start": "659459",
    "end": "665220"
  },
  {
    "text": "environment 6. performing the elevation it is crucial that any change to your cluster",
    "start": "665220",
    "end": "671220"
  },
  {
    "text": "or your platform that will ultimately end up in production begins in your lowest possible environment only after",
    "start": "671220",
    "end": "677820"
  },
  {
    "text": "explicit approval in a lower environment should any deployment be elevated to the next level it's therefore critically",
    "start": "677820",
    "end": "683760"
  },
  {
    "text": "important to allow sufficient time before your upgrade deadline to fully test these cluster upgrades in a lower",
    "start": "683760",
    "end": "690120"
  },
  {
    "text": "environment",
    "start": "690120",
    "end": "692660"
  },
  {
    "text": "now um I will explain a situation that happened to us during a previous cluster",
    "start": "696360",
    "end": "703380"
  },
  {
    "text": "upgrade um and I'll explain how we learn the hard way how we needed to implement such",
    "start": "703380",
    "end": "710040"
  },
  {
    "text": "a plan as I laid out in previous slide uh so back in March of 2020 after",
    "start": "710040",
    "end": "716940"
  },
  {
    "text": "several smooth cluster upgrades um our lack of proper planning came to light with a regression that was",
    "start": "716940",
    "end": "723839"
  },
  {
    "text": "introduced uh in our QA environment after a routine cluster uh deployment",
    "start": "723839",
    "end": "730380"
  },
  {
    "text": "after this deployment over the course of a few days some of our users began began",
    "start": "730380",
    "end": "735959"
  },
  {
    "text": "reporting High error rates in the form of 502s and 499s coming from our API",
    "start": "735959",
    "end": "743279"
  },
  {
    "text": "um these higher error rates were not caught by our alerts nor did we have sufficient uh monitoring dashboards in",
    "start": "743279",
    "end": "749339"
  },
  {
    "text": "place to help identify its source of these failures after triaging and assigning a lead to",
    "start": "749339",
    "end": "754920"
  },
  {
    "text": "this issue as well as informing stakeholders and clients about this issue we began investigation",
    "start": "754920",
    "end": "761040"
  },
  {
    "text": "um due to our lack of foresight to plan for issues like this this investigation did in fact have to be done manually and",
    "start": "761040",
    "end": "767760"
  },
  {
    "text": "touched many aspects of our ecosystem that many of us were only tangentially familiar with unfortunately",
    "start": "767760",
    "end": "775500"
  },
  {
    "text": "um after concluding a regression was not introduced within our platform code our team struggled to identify the source of",
    "start": "775500",
    "end": "782279"
  },
  {
    "text": "these errors let alone the root cause or solution and all that we could conclude was that",
    "start": "782279",
    "end": "788760"
  },
  {
    "text": "due to the nature of our HTTP error codes we were seeing Network traffic was",
    "start": "788760",
    "end": "794040"
  },
  {
    "text": "severed somewhere along the way though after some thorough investigation",
    "start": "794040",
    "end": "799860"
  },
  {
    "text": "we discovered logs from our Ingress controller which pointed to it as our source of failure",
    "start": "799860",
    "end": "806399"
  },
  {
    "text": "further Network testing on the Ingress controller confirmed a high rate of",
    "start": "806399",
    "end": "811440"
  },
  {
    "text": "connectivity problems between it and our server pods further investigation along our Network along with our Network",
    "start": "811440",
    "end": "817620"
  },
  {
    "text": "administrators revealed an undocumented regression in our cni plug-in psyllium which was blocking internode traffic for",
    "start": "817620",
    "end": "824700"
  },
  {
    "text": "pods attached to certain outdated Network policy definitions our Ingress controller was detached to",
    "start": "824700",
    "end": "830820"
  },
  {
    "text": "one such policy and after updating the policy Network traffic returned to its former working state",
    "start": "830820",
    "end": "838100"
  },
  {
    "text": "the this trial by fire introduced our team to aspects of our platform ecosystem which were previously",
    "start": "839100",
    "end": "845040"
  },
  {
    "text": "unfamiliar and revealed to us many new potential points of failure worth testing and monitoring for example the",
    "start": "845040",
    "end": "851760"
  },
  {
    "text": "image on the right summarizes all the Hops skips and jumps our Network takes our Network traffic takes to get from",
    "start": "851760",
    "end": "858300"
  },
  {
    "text": "our client to our server and Back Again as we locked a Consolidated view of our",
    "start": "858300",
    "end": "863459"
  },
  {
    "text": "system we had to start at both ends of this flow and test and inspect each stage for network issues manually",
    "start": "863459",
    "end": "870600"
  },
  {
    "text": "this involved running commands like nslookup to confirm DNS resolution and",
    "start": "870600",
    "end": "876060"
  },
  {
    "text": "at points even running simple curl commands from within our Ingress controller pods to Tech to test",
    "start": "876060",
    "end": "881699"
  },
  {
    "text": "connection to our Downstream systems this process was unsustainable to say",
    "start": "881699",
    "end": "887040"
  },
  {
    "text": "the least ultimately we concluded that the network error originated from the connection",
    "start": "887040",
    "end": "893160"
  },
  {
    "text": "between our Ingress controller and our server but we determined it's worth testing",
    "start": "893160",
    "end": "898680"
  },
  {
    "text": "monitoring and alerting on every stage of our Network flow since then we have since made efforts to",
    "start": "898680",
    "end": "905940"
  },
  {
    "text": "consolidate our logs from each stage into a dashboard on Splunk as well as monitor each stage for health and",
    "start": "905940",
    "end": "911399"
  },
  {
    "text": "performance metrics using tools such as New Relic and cloudwatch we've also",
    "start": "911399",
    "end": "916560"
  },
  {
    "text": "revitalized our knowledge transfer sessions to teach our team about the various supporting actors in our overall",
    "start": "916560",
    "end": "922740"
  },
  {
    "text": "platform ecosystem so let's analyze our experience with",
    "start": "922740",
    "end": "929820"
  },
  {
    "text": "this particular cluster upgrade what went right what went wrong how could we have improved our experience with a",
    "start": "929820",
    "end": "936360"
  },
  {
    "text": "proper action plan like we described earlier starting with the positives to ensure",
    "start": "936360",
    "end": "941579"
  },
  {
    "text": "customer questions and issues were promptly addressed we have built a",
    "start": "941579",
    "end": "946920"
  },
  {
    "text": "multi-layered support system and this support system worked as intended uh in",
    "start": "946920",
    "end": "951959"
  },
  {
    "text": "the resolution of the issue we triaged and prioritized the issue assigned a",
    "start": "951959",
    "end": "957000"
  },
  {
    "text": "lead to resolve it and effectively collaborated using slack and zoom to troubleshoot with our team and with our",
    "start": "957000",
    "end": "963540"
  },
  {
    "text": "cluster administrators simultaneously we kept the impacted customers and Leadership informed on our",
    "start": "963540",
    "end": "970500"
  },
  {
    "text": "progress additionally despite our setbacks we were able to identify the root cause of",
    "start": "970500",
    "end": "975660"
  },
  {
    "text": "failures and iterate quickly to deploy the fix and before the cluster hit our production environment we were able to",
    "start": "975660",
    "end": "982560"
  },
  {
    "text": "resolve the issue and continue as normal however despite our quick turnaround on",
    "start": "982560",
    "end": "987660"
  },
  {
    "text": "this particular issue we did expose ourselves as insufficiently prepared for cluster upgrades in general",
    "start": "987660",
    "end": "994440"
  },
  {
    "text": "a few glaring issues can be identified from auditing the upgrade process from the perspective of a platform team",
    "start": "994440",
    "end": "1001759"
  },
  {
    "text": "first while we were reviewing the release notes for perspective this prospective cluster upgrade we failed to",
    "start": "1001759",
    "end": "1008720"
  },
  {
    "text": "identify the psyllium version update as a potential source of failure correctly identifying this risk would have",
    "start": "1008720",
    "end": "1014720"
  },
  {
    "text": "narrowed our investigation significantly and since we have taken care to call out any version upgrades coming down the",
    "start": "1014720",
    "end": "1021380"
  },
  {
    "text": "pipeline to investigate first in case of detected regressions second",
    "start": "1021380",
    "end": "1027140"
  },
  {
    "text": "our testing monitoring and alerting Suites proved insufficient to notify us of any failures outside of the scope of",
    "start": "1027140",
    "end": "1033678"
  },
  {
    "text": "our immediate platform components we only discovered this particular issue after being notified about degraded",
    "start": "1033679",
    "end": "1039740"
  },
  {
    "text": "performance from our clients and when investigating we had to call Cobble together logs and metrics from disparate",
    "start": "1039740",
    "end": "1045678"
  },
  {
    "text": "sources a Consolidated testing strategy and monitoring Suite would have identified this regression quickly and",
    "start": "1045679",
    "end": "1052220"
  },
  {
    "text": "we've since begun consolidating long streams into centralized dashboards",
    "start": "1052220",
    "end": "1057740"
  },
  {
    "text": "third before this instant incident many of our development teams uh members had",
    "start": "1057740",
    "end": "1064340"
  },
  {
    "text": "a tentative grasp at best on some of the systems in our platform ecosystem outside of our immediate platform",
    "start": "1064340",
    "end": "1071360"
  },
  {
    "text": "components a firm understanding of these systems would have resulted in a more confident",
    "start": "1071360",
    "end": "1076520"
  },
  {
    "text": "and robust debug process we've since ensured that our team has several smes",
    "start": "1076520",
    "end": "1082039"
  },
  {
    "text": "of our platform ecosystem and Beyond and we also do regular knowledge transfers",
    "start": "1082039",
    "end": "1087980"
  },
  {
    "text": "to elevate the rest of the team by addressing the above gaps we've conclude we've conducted subsequent",
    "start": "1087980",
    "end": "1095000"
  },
  {
    "text": "cluster upgrades with confidence in our game plan I'll now hand it off to Cruz to discuss",
    "start": "1095000",
    "end": "1100940"
  },
  {
    "text": "observability thanks Christian uh so turning now to",
    "start": "1100940",
    "end": "1107900"
  },
  {
    "text": "observability uh I think it's really helpful to frame any discussion or",
    "start": "1107900",
    "end": "1113120"
  },
  {
    "text": "um any work around observability in terms of the target outcomes and so for",
    "start": "1113120",
    "end": "1119299"
  },
  {
    "text": "us there are really two outcomes that matter and and all of our observability",
    "start": "1119299",
    "end": "1124580"
  },
  {
    "text": "work drives towards enabling these outcomes the first one is probably familiar to you and that is minimizing",
    "start": "1124580",
    "end": "1131600"
  },
  {
    "text": "our time to restore and that time to restore captures uh how long it takes to",
    "start": "1131600",
    "end": "1138440"
  },
  {
    "text": "bring the service back up whenever an incident occurs and it's also helpful to",
    "start": "1138440",
    "end": "1143539"
  },
  {
    "text": "think of that journey in terms of two separate stages first is uh the stage",
    "start": "1143539",
    "end": "1148760"
  },
  {
    "text": "where you're actually waiting for your on-call engineer to detect that there's an issue and so that it can be measured",
    "start": "1148760",
    "end": "1153860"
  },
  {
    "text": "as the time to detect and then once the on-call engineer knows about the issue there's another delay as they figure out",
    "start": "1153860",
    "end": "1160700"
  },
  {
    "text": "how to restore the issue or rather how to fix the issue is it our issue to fix and so that diagnostic process we can",
    "start": "1160700",
    "end": "1168740"
  },
  {
    "text": "kind of capture as the time to repair and so by thinking about time to restore in those two categories it helps direct",
    "start": "1168740",
    "end": "1174980"
  },
  {
    "text": "our investments to um the most valuable work and so in the case of uh alerting alerting is there to",
    "start": "1174980",
    "end": "1181580"
  },
  {
    "text": "help minimize that time to detect it helps us learn about issues before our customers come and report them to us as",
    "start": "1181580",
    "end": "1188480"
  },
  {
    "text": "Christian mentioned earlier and then we have other levers to pull to minimize the time to repair so specifically",
    "start": "1188480",
    "end": "1195320"
  },
  {
    "text": "that's things like having a run book so that some common uh some common",
    "start": "1195320",
    "end": "1200720"
  },
  {
    "text": "remediation steps are easy and apparent to the on-call engineer and also having very fine-grained application",
    "start": "1200720",
    "end": "1207740"
  },
  {
    "text": "performance monitoring or APM and that also looks like having distributed traces and even metrics that can point",
    "start": "1207740",
    "end": "1215299"
  },
  {
    "text": "to very specific uh parts of your stack so that you can easily see where the",
    "start": "1215299",
    "end": "1220340"
  },
  {
    "text": "issue likely is and so by thinking about the time to restore in terms of these two uh components it really helps us to",
    "start": "1220340",
    "end": "1227419"
  },
  {
    "text": "optimize and focus on the most valuable instrumentation efforts and even even efforts that may not involve",
    "start": "1227419",
    "end": "1233539"
  },
  {
    "text": "instrumentation such as writing run books and then another really important outcome is to maximize the legibility of",
    "start": "1233539",
    "end": "1241039"
  },
  {
    "text": "the system and so we can think about legibility as a measure of how readily",
    "start": "1241039",
    "end": "1246440"
  },
  {
    "text": "the system can be read and understood by anyone on the team if you have a situation where only the experts know",
    "start": "1246440",
    "end": "1253220"
  },
  {
    "text": "about this one piece of the ecosystem then if that is where the incident is",
    "start": "1253220",
    "end": "1258799"
  },
  {
    "text": "occurring then you have to get that expert on the phone but if you've done a good job instrumenting the system and centralizing those signals to be viewed",
    "start": "1258799",
    "end": "1266720"
  },
  {
    "text": "in one or a few places then suddenly everyone can kind of be an expert in in that sense we've we've really like",
    "start": "1266720",
    "end": "1273820"
  },
  {
    "text": "improved the team's ability to respond to issues and to understand what's happening in the system and so we can",
    "start": "1273820",
    "end": "1280460"
  },
  {
    "text": "also think about our dashboards as opportunities to sort",
    "start": "1280460",
    "end": "1285500"
  },
  {
    "text": "of to answer some known questions of some frequently asked questions but even",
    "start": "1285500",
    "end": "1290780"
  },
  {
    "text": "that has limitation and ultimately we want to have opportunities to ask any",
    "start": "1290780",
    "end": "1296120"
  },
  {
    "text": "question of the system with logs that are really rich with context and traces",
    "start": "1296120",
    "end": "1301280"
  },
  {
    "text": "which also can show the interdependencies between the system and help us even diagnose",
    "start": "1301280",
    "end": "1307400"
  },
  {
    "text": "um you know very subtle issues with high resolution traces and so in our system",
    "start": "1307400",
    "end": "1312559"
  },
  {
    "text": "as mentioned earlier because we're living in this layered world we have a",
    "start": "1312559",
    "end": "1317780"
  },
  {
    "text": "couple of unique challenges so the first one is our shared responsibility model our",
    "start": "1317780",
    "end": "1324740"
  },
  {
    "text": "mlas team is going to own the compute infrastructure and the platform apis that ultimately help orchestrate our",
    "start": "1324740",
    "end": "1331220"
  },
  {
    "text": "work our workers or whether our workflow jobs that our users are running but then the individual users those tenants they",
    "start": "1331220",
    "end": "1338360"
  },
  {
    "text": "have their own code that they have to attend to their settings certain",
    "start": "1338360",
    "end": "1343580"
  },
  {
    "text": "resource specifications obviously the right adding the logic that could go wrong and so oftentimes it's been",
    "start": "1343580",
    "end": "1349580"
  },
  {
    "text": "difficult to parse through the root cause of an issue and know whether or not it's ours to solve or if it's a",
    "start": "1349580",
    "end": "1356000"
  },
  {
    "text": "user's dissolve or in some cases it's even up to the cluster administrators to resolve certain issues and so keeping",
    "start": "1356000",
    "end": "1362419"
  },
  {
    "text": "that in front of our mind has been really important as we enter in the system and then another unique Challenge",
    "start": "1362419",
    "end": "1368600"
  },
  {
    "text": "and this this is sort of specific to the way our platform is orchestrating our",
    "start": "1368600",
    "end": "1374600"
  },
  {
    "text": "users workloads is we have very short-lived pods and in some cases the pods that are most problematic the ones",
    "start": "1374600",
    "end": "1381559"
  },
  {
    "text": "that our users want to uh debug and dive into further those are the pods that are",
    "start": "1381559",
    "end": "1386840"
  },
  {
    "text": "only living for a few seconds if that long and so it turns out that there are several tools in the observability in",
    "start": "1386840",
    "end": "1393740"
  },
  {
    "text": "the ecosystem that rely on a pool based mechanism and so we'll learn how that has become problematic in some cases",
    "start": "1393740",
    "end": "1402080"
  },
  {
    "text": "um to to sort of outline a few of these tools that are operating at different layers in the stack I think that this uh",
    "start": "1402080",
    "end": "1408620"
  },
  {
    "text": "this diagram is useful um yet again for uh illustrating core",
    "start": "1408620",
    "end": "1414260"
  },
  {
    "text": "observability plays at each level so at the cluster level we really do rely on our cluster to",
    "start": "1414260",
    "end": "1420260"
  },
  {
    "text": "provide some of those foundational capabilities of collecting uh metrics",
    "start": "1420260",
    "end": "1426020"
  },
  {
    "text": "and logs and traces and shipping those to our tools of choice so in our case we're using the fluent D David set and",
    "start": "1426020",
    "end": "1433280"
  },
  {
    "text": "to collect the logs on individual hosts and then those logs are ultimately forwarded to Splunk using the HEC plugin",
    "start": "1433280",
    "end": "1441260"
  },
  {
    "text": "um even even there we have two different projects the the HCC plug-in is this",
    "start": "1441260",
    "end": "1446480"
  },
  {
    "text": "project separate from the fluent d project and so um there have been cases where we have to dive into issues that may originate",
    "start": "1446480",
    "end": "1453679"
  },
  {
    "text": "from the from the agency plugin and then others that seem to be related to the fluent d uh the damage set",
    "start": "1453679",
    "end": "1461539"
  },
  {
    "text": "um pod and then apart from fluent D and Splunk we have a new Relic operator that's collecting infrastructure",
    "start": "1461539",
    "end": "1467659"
  },
  {
    "text": "utilization metrics and it's essentially providing that infrastructure view in",
    "start": "1467659",
    "end": "1473539"
  },
  {
    "text": "New Relic and then in addition to the metrics collected by the New Relic operator we also have Prometheus",
    "start": "1473539",
    "end": "1479960"
  },
  {
    "text": "collecting metrics not only from other applications in the cluster but also a lot of these plugins that we rely on and",
    "start": "1479960",
    "end": "1486679"
  },
  {
    "text": "those metrics are also Consolidated in new relics so that we can see them uh side by side and debug issues as needed",
    "start": "1486679",
    "end": "1493520"
  },
  {
    "text": "uh and I can I can even speak specifically to the uh to the Ingress controller that Christian mentioned",
    "start": "1493520",
    "end": "1499580"
  },
  {
    "text": "earlier we rely on the nginx Ingress metrics that are exposed and collected",
    "start": "1499580",
    "end": "1505520"
  },
  {
    "text": "by Prometheus to see into that pod because otherwise there wouldn't really be a good way for us to understand",
    "start": "1505520",
    "end": "1512000"
  },
  {
    "text": "what's happening inside of that um inside that uh prior to the",
    "start": "1512000",
    "end": "1517520"
  },
  {
    "text": "application because we didn't uh write that and we didn't instrument it and then the layer that we actually have",
    "start": "1517520",
    "end": "1522860"
  },
  {
    "text": "control over is the platform apis and within that layer we're implementing",
    "start": "1522860",
    "end": "1529820"
  },
  {
    "text": "some very fine-grained instrumentation with newer like APM and that allows us to have distributed tracing as well as",
    "start": "1529820",
    "end": "1535640"
  },
  {
    "text": "error reporting and that um that error reporting has been really valuable for detecting",
    "start": "1535640",
    "end": "1541159"
  },
  {
    "text": "um very specific errors with our clients code and we're always leaning into improving the granularity of the errors",
    "start": "1541159",
    "end": "1547760"
  },
  {
    "text": "that we're able to see in New Relic and it's this layer that we're actually pointing all of our monitors for",
    "start": "1547760",
    "end": "1554120"
  },
  {
    "text": "alerting we've learned that alerting on anything that our on-call engineer",
    "start": "1554120",
    "end": "1559159"
  },
  {
    "text": "actually can't resolve has been it's been really frustrating and it's been hard to actually understand what issues",
    "start": "1559159",
    "end": "1565940"
  },
  {
    "text": "are worth interrupting and uh and beginning to triage and so we've really",
    "start": "1565940",
    "end": "1571580"
  },
  {
    "text": "just tried to focus all of our alerting efforts on this platform API layer and that's really helped over reduce toil",
    "start": "1571580",
    "end": "1577520"
  },
  {
    "text": "and reduce churn on the part of the on-call engineer and then moving on to the pla to the tenant sandbox we have",
    "start": "1577520",
    "end": "1584179"
  },
  {
    "text": "several guard rails including the resource quota construct and those",
    "start": "1584179",
    "end": "1590299"
  },
  {
    "text": "guardrails help us to prevent those tenants from exceeding certain resource",
    "start": "1590299",
    "end": "1595880"
  },
  {
    "text": "limits and uh that we that is ultimately the the best approach to preventing",
    "start": "1595880",
    "end": "1601159"
  },
  {
    "text": "those tenants from having too big of an impact on other systems that are running",
    "start": "1601159",
    "end": "1606559"
  },
  {
    "text": "alongside them in the cluster that's a much better approach than simply alerting on high resource utilization",
    "start": "1606559",
    "end": "1613100"
  },
  {
    "text": "and requiring someone to take an action so we're able to leverage that resource quota construct to essentially provide",
    "start": "1613100",
    "end": "1620000"
  },
  {
    "text": "that guard rub by default and then also in addition to the logs and metrics that",
    "start": "1620000",
    "end": "1625279"
  },
  {
    "text": "we capture from our platform apis we're also capturing all the the logs from those 10 namespaces and we're making",
    "start": "1625279",
    "end": "1631400"
  },
  {
    "text": "sure that each tenant has the label that they need to be able to pull the logs and see the logs that are relevant for",
    "start": "1631400",
    "end": "1637700"
  },
  {
    "text": "their workloads and then lastly within the actual jobs themselves we have application logs that our users are",
    "start": "1637700",
    "end": "1645559"
  },
  {
    "text": "going to be writing and they understand best and we're labeling those and surfacing those in tools like Splunk",
    "start": "1645559",
    "end": "1652159"
  },
  {
    "text": "where the users can diagnose issues related to their application code we don't yet have metrics and traces and",
    "start": "1652159",
    "end": "1659179"
  },
  {
    "text": "structured logs throughout those batch jobs that we're pursuing those those opportunities right now as mentioned",
    "start": "1659179",
    "end": "1666320"
  },
  {
    "text": "earlier there are a couple of interesting considerations when you have short-lived pods so for instance with",
    "start": "1666320",
    "end": "1671840"
  },
  {
    "text": "Prometheus you might actually not scrape that pod before the Pod dies and so",
    "start": "1671840",
    "end": "1678080"
  },
  {
    "text": "there is a chance that your pod metrics wouldn't be presented or they would present be presented in an incomplete",
    "start": "1678080",
    "end": "1684559"
  },
  {
    "text": "form and then for the observability and the road the instrumentation of the",
    "start": "1684559",
    "end": "1689779"
  },
  {
    "text": "individual API calls using things like open Telemetry we know that there's a non-zero performance impact to that",
    "start": "1689779",
    "end": "1697120"
  },
  {
    "text": "instrumentation and so we're trying to find ways to measure that impact and quantify it and then ultimately give our",
    "start": "1697120",
    "end": "1703159"
  },
  {
    "text": "users an out opportunity to opt in and maybe even control the granularity or the",
    "start": "1703159",
    "end": "1708799"
  },
  {
    "text": "um the sampling rates so that they can essentially make those trade-offs themselves and so that's sort of the the",
    "start": "1708799",
    "end": "1714620"
  },
  {
    "text": "state of of open Telemetry and and those um those more advanced mechanisms of uh",
    "start": "1714620",
    "end": "1721640"
  },
  {
    "text": "tracing within our individual batch jobs so now I'll hand it over to Trevor who",
    "start": "1721640",
    "end": "1727520"
  },
  {
    "text": "will uh sort of explain a recent issue that we had with logging and how we resolve that in our platform",
    "start": "1727520",
    "end": "1734659"
  },
  {
    "text": "thank you Cruz I would just like to dive into one entertaining case study I will",
    "start": "1734659",
    "end": "1740299"
  },
  {
    "text": "affectionately dub my logs are missing so as earlier mentioned as mentioned",
    "start": "1740299",
    "end": "1745760"
  },
  {
    "text": "earlier our SRE team maintains an Enterprise cops repository providing our",
    "start": "1745760",
    "end": "1751220"
  },
  {
    "text": "clusters with several conveniences such as fluent d this comes with a seemingly handy Cube",
    "start": "1751220",
    "end": "1757820"
  },
  {
    "text": "fluent D operator self-described as a fluent D config manager with batteries",
    "start": "1757820",
    "end": "1764179"
  },
  {
    "text": "included con config validation no needs to restart with sensible defaults and",
    "start": "1764179",
    "end": "1770419"
  },
  {
    "text": "best practices built in based on a theme of verifying defaults",
    "start": "1770419",
    "end": "1775820"
  },
  {
    "text": "as well as the studies foreboding title the question becomes which of the defaults was not so sensible for our",
    "start": "1775820",
    "end": "1782480"
  },
  {
    "text": "environment and I ask that because verifying the defaults has been a cause of several",
    "start": "1782480",
    "end": "1788240"
  },
  {
    "text": "logging pains especially the default resource specifications",
    "start": "1788240",
    "end": "1793700"
  },
  {
    "text": "to answer that question we'll look at how a log message is built using this pattern so the log starts its journey",
    "start": "1793700",
    "end": "1800179"
  },
  {
    "text": "and the data scientist's job hoping to tell the world about the looming issue in prod just as all little logs do",
    "start": "1800179",
    "end": "1807860"
  },
  {
    "text": "the log is written to the file system and Screen scraped by a fluency file",
    "start": "1807860",
    "end": "1813380"
  },
  {
    "text": "source next on a schedule the log Rider queries the cube API for the pods metadata",
    "start": "1813380",
    "end": "1819980"
  },
  {
    "text": "ensuring that the log's author is able to find it finally the enriched log is sent to",
    "start": "1819980",
    "end": "1825799"
  },
  {
    "text": "Splunk to be United with its engineer by default the log router queries the",
    "start": "1825799",
    "end": "1833360"
  },
  {
    "text": "cube API every one minute which is time which is fine for long running pods such",
    "start": "1833360",
    "end": "1839480"
  },
  {
    "text": "as web servers and the like however in an environment with many moving Parts which blogs would you",
    "start": "1839480",
    "end": "1845899"
  },
  {
    "text": "suppose people are most interested to find the ones with errors",
    "start": "1845899",
    "end": "1851840"
  },
  {
    "text": "it is well known that errors can dramatically decrease the pod's lifetime sometimes by so much the pot is long",
    "start": "1851840",
    "end": "1858679"
  },
  {
    "text": "gone by the time the log router asks for its metadata this leaves our lonely log without any",
    "start": "1858679",
    "end": "1864559"
  },
  {
    "text": "labels no logs without labels are now impossible to search for",
    "start": "1864559",
    "end": "1871100"
  },
  {
    "text": "with so many logs in our Splunk index we found that logs without labels provide little more value than no logs at all in",
    "start": "1871100",
    "end": "1877399"
  },
  {
    "text": "fact some of the configurations users add a sleep greater than a fluent D's",
    "start": "1877399",
    "end": "1883520"
  },
  {
    "text": "refreshing interval just to avoid this issue of course another option is to configure",
    "start": "1883520",
    "end": "1889100"
  },
  {
    "text": "the fluent D query to query the cube API more frequently but some jobs have a",
    "start": "1889100",
    "end": "1895520"
  },
  {
    "text": "sub-second lifetime so there's a limit how often you can do it thus we're moving towards a structured",
    "start": "1895520",
    "end": "1903200"
  },
  {
    "text": "logging approach to remedy these missing logs by enabling the kubernetes downward",
    "start": "1903200",
    "end": "1908539"
  },
  {
    "text": "facing API the pods creating the logs can also retrieve the kubernetes metadata required to make them",
    "start": "1908539",
    "end": "1914960"
  },
  {
    "text": "searchable as shown here we can more reliably provide meaningful logs by entirely",
    "start": "1914960",
    "end": "1920720"
  },
  {
    "text": "avoiding the fallible enrichment step the fluent D operator offers in short",
    "start": "1920720",
    "end": "1926600"
  },
  {
    "text": "our logs themselves are created batteries included",
    "start": "1926600",
    "end": "1931659"
  },
  {
    "text": "also as a last note when it comes to logging your logging it is helpful to enable fluent D metrics in our case for",
    "start": "1931659",
    "end": "1939020"
  },
  {
    "text": "me dsn's fluent dmetrics for the number of errors in the Q buffer length to New",
    "start": "1939020",
    "end": "1944120"
  },
  {
    "text": "Relic for alerting this lets you keep tabs of even the logs you missed enough of this use case though I'm",
    "start": "1944120",
    "end": "1950720"
  },
  {
    "text": "handing it back to Christian thank you Trevor I will now uh be",
    "start": "1950720",
    "end": "1957140"
  },
  {
    "text": "discussing how to isolate your tenants uh compute within your cluster and your platform",
    "start": "1957140",
    "end": "1963740"
  },
  {
    "text": "um and I'll discuss how the machine learning as a service platform does so so uh at its core the machine learning",
    "start": "1963740",
    "end": "1970940"
  },
  {
    "text": "as a service platform provides its clients known as tenants the means to author manage and execute their data",
    "start": "1970940",
    "end": "1978500"
  },
  {
    "text": "workflows given the unique and open-ended functional capabilities of a workflow each running instance of said",
    "start": "1978500",
    "end": "1985399"
  },
  {
    "text": "workloads considered its own application thus as The Trusted host of such",
    "start": "1985399",
    "end": "1990740"
  },
  {
    "text": "applications the machine learning as a service platform needs to ensure compute isolation in order to prevent tenant",
    "start": "1990740",
    "end": "1997700"
  },
  {
    "text": "workflows from Gaining access to other tenant workflows and data and if",
    "start": "1997700",
    "end": "2003460"
  },
  {
    "text": "complete isolation is not achieved there is a high risk of data exposure or denial of service",
    "start": "2003460",
    "end": "2009760"
  },
  {
    "text": "therefore uh machine learning as a service platform uses kubernetes namespaces to implement a multi-tenancy",
    "start": "2009760",
    "end": "2016419"
  },
  {
    "text": "model for compute isolation as namespaces provide a means to house or",
    "start": "2016419",
    "end": "2021519"
  },
  {
    "text": "isolate groups of resources within a cluster in this use case namespaces are used to",
    "start": "2021519",
    "end": "2027279"
  },
  {
    "text": "house tenants running workflows every tenant has their own namespace and tenant workloads run in their designated",
    "start": "2027279",
    "end": "2033640"
  },
  {
    "text": "namespace only isolating tenants to their own namespace allows us to administer tenants",
    "start": "2033640",
    "end": "2039940"
  },
  {
    "text": "individually with their own configurations limits and permissions primarily we use namespaces to ensure",
    "start": "2039940",
    "end": "2046659"
  },
  {
    "text": "lease privilege access and to manage resources on a tenant level for example using network policies and",
    "start": "2046659",
    "end": "2053440"
  },
  {
    "text": "role-based access control we can configure least privileged Network and resource permissions respectively",
    "start": "2053440",
    "end": "2058720"
  },
  {
    "text": "allowing us to limit which services our tenants workflows can communicate with and which resources they can modify",
    "start": "2058720",
    "end": "2066040"
  },
  {
    "text": "we can also prevent denial of service and resource hogging by setting resource quotas and metering resource consumption",
    "start": "2066040",
    "end": "2073118"
  },
  {
    "text": "which we'll discuss in more detail later we apply all of these configurations and permissions on a namespace level to",
    "start": "2073119",
    "end": "2079960"
  },
  {
    "text": "apply the tenant workflows across the board now namespaces are a powerful tool for",
    "start": "2079960",
    "end": "2087700"
  },
  {
    "text": "administrating and organizing your tenants in your platform but in practice you're going to run into some overhead",
    "start": "2087700",
    "end": "2094599"
  },
  {
    "text": "when it comes to configuring and deploying said namespaces the primary consideration to tackle when",
    "start": "2094599",
    "end": "2101080"
  },
  {
    "text": "administering name spaces is determining the minimum permissions needed by the fewest entities to meet your namespace",
    "start": "2101080",
    "end": "2108339"
  },
  {
    "text": "Administration requirements as with all cluster scoped resources the cluster-wide nature of namespaces",
    "start": "2108339",
    "end": "2115240"
  },
  {
    "text": "necessitates due diligence when assigning permissions to create and modify them",
    "start": "2115240",
    "end": "2120400"
  },
  {
    "text": "as a misconfiguration could in fact open a security gap which could expose all",
    "start": "2120400",
    "end": "2126099"
  },
  {
    "text": "name spaces in a cluster therefore it can be simpler and safer to",
    "start": "2126099",
    "end": "2131920"
  },
  {
    "text": "offload the work of namespace management to a separate service dedicated for such",
    "start": "2131920",
    "end": "2137500"
  },
  {
    "text": "a purpose a kubernetes operator could potentially fit this use case",
    "start": "2137500",
    "end": "2143680"
  },
  {
    "text": "operators which are extension software extensions to kubernetes that Define the",
    "start": "2143680",
    "end": "2148960"
  },
  {
    "text": "deployment and management of custom resources uh which extended like vanilla",
    "start": "2148960",
    "end": "2154420"
  },
  {
    "text": "kubernetes API can be used for research for namespace management in this case the custom resource would",
    "start": "2154420",
    "end": "2160900"
  },
  {
    "text": "consist of potentially a single definition file which wraps a definition",
    "start": "2160900",
    "end": "2165940"
  },
  {
    "text": "of a namespace and any additional resources which live within that namespace the operator would then manage",
    "start": "2165940",
    "end": "2172960"
  },
  {
    "text": "the deployment and maintenance of this custom resource and therefore the namespace and objects defined within it",
    "start": "2172960",
    "end": "2179140"
  },
  {
    "text": "it's therefore a good idea to investigate if any such kubernetes operators exist which fit your namespace",
    "start": "2179140",
    "end": "2185260"
  },
  {
    "text": "management needs as it is possible that there exists a well-defined operator within your organization or an open",
    "start": "2185260",
    "end": "2191619"
  },
  {
    "text": "source which you can use to create and configure your tenants namespaces and",
    "start": "2191619",
    "end": "2196780"
  },
  {
    "text": "resources by offloading namespace management to such an operator you're ensuring proper",
    "start": "2196780",
    "end": "2203320"
  },
  {
    "text": "segregation of Duties and Lease previous access for your application if you choose to follow this pattern",
    "start": "2203320",
    "end": "2209800"
  },
  {
    "text": "consider the following when choosing a namespace operator first as these",
    "start": "2209800",
    "end": "2215380"
  },
  {
    "text": "operators deal with a critical cluster scoped resource and therefore can have a",
    "start": "2215380",
    "end": "2221140"
  },
  {
    "text": "cluster-wide impact in case of failure discuss the permissions this operator needs and the impact on existing name",
    "start": "2221140",
    "end": "2227320"
  },
  {
    "text": "spaces in your cluster with cluster administrators and any other applications on your cluster before",
    "start": "2227320",
    "end": "2234040"
  },
  {
    "text": "deploying such an operator to your cluster and second research the level of",
    "start": "2234040",
    "end": "2239260"
  },
  {
    "text": "developer support this operator has whether it's open source or in-house you should see how frequently The Operators",
    "start": "2239260",
    "end": "2244960"
  },
  {
    "text": "contributed to and whether it has any critical outstanding issues and is repository and whether any developer",
    "start": "2244960",
    "end": "2251880"
  },
  {
    "text": "team exists to offer integration to drug support we had to consider these items when",
    "start": "2251880",
    "end": "2259240"
  },
  {
    "text": "deciding between two such operators to conduct our namespace uh management for",
    "start": "2259240",
    "end": "2264820"
  },
  {
    "text": "our platform these operators were the hierarchical namespace controller and open source operator and Embark a",
    "start": "2264820",
    "end": "2271780"
  },
  {
    "text": "namespace operator within our organization",
    "start": "2271780",
    "end": "2276000"
  },
  {
    "text": "when we were exploring operators to manage our attendant namespaces and resources to promising options presented",
    "start": "2277420",
    "end": "2283480"
  },
  {
    "text": "themselves first as stated before the hierarchical namespace controller this is an open source operator which",
    "start": "2283480",
    "end": "2290079"
  },
  {
    "text": "establishes a hierarchical relationship between namespaces allowing for the creation of the concept of a child name",
    "start": "2290079",
    "end": "2297040"
  },
  {
    "text": "space to which you can propagate resource or can resources or configuration items from my parent",
    "start": "2297040",
    "end": "2303640"
  },
  {
    "text": "namespace in our case this Paradigm would allow us to create tenant name spaces as children",
    "start": "2303640",
    "end": "2310119"
  },
  {
    "text": "of our application's main namespace to which we would automatically propagate",
    "start": "2310119",
    "end": "2315400"
  },
  {
    "text": "standard Network policies roles role bindings and other resources automatically",
    "start": "2315400",
    "end": "2320859"
  },
  {
    "text": "changes to these resources and configuration items in the application's namespace would then automatically",
    "start": "2320859",
    "end": "2326560"
  },
  {
    "text": "reflect in the child name space our second option Embark is an operator",
    "start": "2326560",
    "end": "2332500"
  },
  {
    "text": "developed within our organization which alternatively defines and operates a",
    "start": "2332500",
    "end": "2338260"
  },
  {
    "text": "custom resource called a super namespace this super namespace object lets you",
    "start": "2338260",
    "end": "2343300"
  },
  {
    "text": "define namespace and any resources within it like Network policies roles",
    "start": "2343300",
    "end": "2348579"
  },
  {
    "text": "and role bindings within a single yaml file deploying or modifying any of those",
    "start": "2348579",
    "end": "2354640"
  },
  {
    "text": "items can be done by simply modifying the super namespace definition itself and deploying that object the operator",
    "start": "2354640",
    "end": "2362020"
  },
  {
    "text": "handles the rest this option would have allowed us to configure all of our attendant",
    "start": "2362020",
    "end": "2368140"
  },
  {
    "text": "namespaces from one single standard template and deploy a super namespace per tenant",
    "start": "2368140",
    "end": "2375520"
  },
  {
    "text": "which would thus deploy their namespace and all of their resources a promising option Embark was only",
    "start": "2375520",
    "end": "2382720"
  },
  {
    "text": "lacking uh one feature and that was the ability to configure uh custom",
    "start": "2382720",
    "end": "2388720"
  },
  {
    "text": "additional resources on a individual super namespace level uh by default you",
    "start": "2388720",
    "end": "2394240"
  },
  {
    "text": "were able to configure objects on an operator level that would apply to all namespaces in the cluster",
    "start": "2394240",
    "end": "2400420"
  },
  {
    "text": "um but was missing the feature to do so on an individual super name Space level",
    "start": "2400420",
    "end": "2405579"
  },
  {
    "text": "if we contributed to Embark to fill this feature Gap both operators from a functional perspective would have",
    "start": "2405579",
    "end": "2411880"
  },
  {
    "text": "provided a viable solution to host our namespace management however after taking into account the",
    "start": "2411880",
    "end": "2418420"
  },
  {
    "text": "considerations mentioned in the previous slide even with that feature Gap Embark",
    "start": "2418420",
    "end": "2423520"
  },
  {
    "text": "stood out to us as our only single viable option for namespace management for the following reasons",
    "start": "2423520",
    "end": "2429940"
  },
  {
    "text": "first the hierarchical namespace controller lacked an official Helm chart to install and maintain it on our",
    "start": "2429940",
    "end": "2437020"
  },
  {
    "text": "cluster instead requiring installation through a tool called crew as crew is not a supported tool in our",
    "start": "2437020",
    "end": "2444820"
  },
  {
    "text": "Enterprise clusters we would have had to take the time to create and maintain our own Helm chart for this operator now in",
    "start": "2444820",
    "end": "2452320"
  },
  {
    "text": "the long run we were considering taking this Challenge on and contributing it back to the open source project but we",
    "start": "2452320",
    "end": "2460060"
  },
  {
    "text": "did hesitate given our internal deadlines and our Relic relative experience at the time with helm and",
    "start": "2460060",
    "end": "2467740"
  },
  {
    "text": "given the following concern which I'll State now we certainly did not want to make any mistakes when creating the helm",
    "start": "2467740",
    "end": "2475119"
  },
  {
    "text": "chart for this operator and that is because of the following reason this was the main issue we had with the",
    "start": "2475119",
    "end": "2481180"
  },
  {
    "text": "project and that was that the hierarchical namespace controller has a",
    "start": "2481180",
    "end": "2486220"
  },
  {
    "text": "very broad impact on the rest of our cluster Beyond just simply the tenant",
    "start": "2486220",
    "end": "2491800"
  },
  {
    "text": "name spaces we wanted to create using it by default the operator placed web books",
    "start": "2491800",
    "end": "2497500"
  },
  {
    "text": "on every single namespace in a given cluster which would therefore trigger",
    "start": "2497500",
    "end": "2502900"
  },
  {
    "text": "admission controllers on upon any namespace modification that would be for",
    "start": "2502900",
    "end": "2508180"
  },
  {
    "text": "tenant namespaces our platform namespace or any additional namespace that exists in our cluster for other purposes",
    "start": "2508180",
    "end": "2515920"
  },
  {
    "text": "therefore in the case of the operator failing or going down this could have",
    "start": "2515920",
    "end": "2521800"
  },
  {
    "text": "had a potentially cluster-wide impact on namespace creation or modification and",
    "start": "2521800",
    "end": "2527980"
  },
  {
    "text": "could have potentially blocked those actions altogether and this would include non-tenant namespaces necessary",
    "start": "2527980",
    "end": "2534160"
  },
  {
    "text": "for other platform operations as a best practice we wanted to minimize the scope and impact of any operator we",
    "start": "2534160",
    "end": "2541540"
  },
  {
    "text": "install in our cluster and this control over non-tenant namespaces proved too big of a risk for us to take on",
    "start": "2541540",
    "end": "2548500"
  },
  {
    "text": "especially without a standard Helm chart for us to install embark on the other hand had no such",
    "start": "2548500",
    "end": "2554680"
  },
  {
    "text": "issues establishing no admission controllers or web Hooks and having no impact on namespaces not managed by the",
    "start": "2554680",
    "end": "2562300"
  },
  {
    "text": "super namespace custom resource in other words the only namespaces that Embark would have touched would be those which",
    "start": "2562300",
    "end": "2569680"
  },
  {
    "text": "we were creating for uh the purposes of our tenants because of these two reasons ultimately",
    "start": "2569680",
    "end": "2576160"
  },
  {
    "text": "our team chose Embark to manage our tenant namespaces and we were confident",
    "start": "2576160",
    "end": "2581500"
  },
  {
    "text": "in that decision having weighed the impact and level of support for each operator",
    "start": "2581500",
    "end": "2586540"
  },
  {
    "text": "so if you are in this situation in which you want to manage 10 namespaces and you",
    "start": "2586540",
    "end": "2592180"
  },
  {
    "text": "want to offload this to an operator uh certainly consider",
    "start": "2592180",
    "end": "2597460"
  },
  {
    "text": "um the same things that we considered um for that purpose I will now hand this",
    "start": "2597460",
    "end": "2602500"
  },
  {
    "text": "off to Patrick to discuss rate limiting and resource management",
    "start": "2602500",
    "end": "2607660"
  },
  {
    "text": "thanks Christian in order to reduce the impact one user can have on the overall system influencing rate limiting per client on",
    "start": "2607660",
    "end": "2614859"
  },
  {
    "text": "our apis and important stuff we can take the goal is to put limits in place so one client cannot push the services over",
    "start": "2614859",
    "end": "2620980"
  },
  {
    "text": "and cause outages for all other users of the platform or cause a response times to fall beneath what is defined in our",
    "start": "2620980",
    "end": "2626920"
  },
  {
    "text": "SLA the biggest consideration from implementing rate limiting is at what layer of the application stack to put",
    "start": "2626920",
    "end": "2632980"
  },
  {
    "text": "the limits in place the API Gateway Ingress controllers and the app layer were all taken into",
    "start": "2632980",
    "end": "2638560"
  },
  {
    "text": "consideration in their pros and cons for weighed not all of our traffic goes through the API Gateway something between at this",
    "start": "2638560",
    "end": "2645280"
  },
  {
    "text": "layer would not catch all traffic possibly leaving us exposed still we're limiting an Ingress controller",
    "start": "2645280",
    "end": "2650859"
  },
  {
    "text": "layer was considered we already used the nginx Ingress controller in our",
    "start": "2650859",
    "end": "2656020"
  },
  {
    "text": "architecture so we explored its rate limiting offering but with its complex setup difficulty to monitor rejected",
    "start": "2656020",
    "end": "2662020"
  },
  {
    "text": "requests and inability to limit by anything besides IP we do not pursue this option",
    "start": "2662020",
    "end": "2668200"
  },
  {
    "text": "other Ingress controllers such as traffic were explored but similar pitfalls and needing to add an",
    "start": "2668200",
    "end": "2673359"
  },
  {
    "text": "additional layer to architecture led to us beginning looking at limiting on the application",
    "start": "2673359",
    "end": "2678940"
  },
  {
    "text": "layer since our API is written in Python we explored the rate limiting packages that",
    "start": "2678940",
    "end": "2684280"
  },
  {
    "text": "exist and found the offerings for to be what we were looking for asgi rate limiting is the package we are",
    "start": "2684280",
    "end": "2691420"
  },
  {
    "text": "currently testing with its simple implementation rule-based rate limiting and custom authorization function we",
    "start": "2691420",
    "end": "2697300"
  },
  {
    "text": "found a solution that is easy to maintain moving forward and does not require major architectural changes",
    "start": "2697300",
    "end": "2704339"
  },
  {
    "text": "with the amount of jobs running in our cluster we have seen some situations that required manual cleanup of PODS and",
    "start": "2708520",
    "end": "2714160"
  },
  {
    "text": "hanging States and other resources that stuck around kubernetes offers a TTL this only",
    "start": "2714160",
    "end": "2719200"
  },
  {
    "text": "applies to pods and jobs in a finish State the kubernetes D scheduler offers",
    "start": "2719200",
    "end": "2724480"
  },
  {
    "text": "a way to configure Max runtime on pods this is just a hard limit and could lead to preemptively terminating workflows",
    "start": "2724480",
    "end": "2730240"
  },
  {
    "text": "that do not meet our criteria to clean up this led to a design of a cleanup Nanny",
    "start": "2730240",
    "end": "2735280"
  },
  {
    "text": "process deployed in our clusters using kubernetes cron jobs we have a job schedule that checks multiple criteria",
    "start": "2735280",
    "end": "2741040"
  },
  {
    "text": "such as logs of a pod to determine if a workflow is still running the job terminates resources in our",
    "start": "2741040",
    "end": "2747040"
  },
  {
    "text": "cluster and then issue status updates to our database if applicable to reduce the load on these cleanup",
    "start": "2747040",
    "end": "2753160"
  },
  {
    "text": "processes we deploy a cleanup Crown job in each of our tenant name spaces this also helps reduce the blast radius if",
    "start": "2753160",
    "end": "2758980"
  },
  {
    "text": "one cleanup job fails rather than one cleanup job handling the entire cluster this also helps tenants stay within the",
    "start": "2758980",
    "end": "2765339"
  },
  {
    "text": "resource quotas this nanny process that handles jobs and pods has been generalized to handle",
    "start": "2765339",
    "end": "2771040"
  },
  {
    "text": "other parts of our system as well we did face unintended consequence of this cleanup job where it was actively",
    "start": "2771040",
    "end": "2777040"
  },
  {
    "text": "hurting our cluster's health the queen of trap identified failed pods to delete however the jaw remained active and it",
    "start": "2777040",
    "end": "2784359"
  },
  {
    "text": "continued to spin up failing pods as this cleanup job deleted pods it was also resetting the failed pod count for",
    "start": "2784359",
    "end": "2790599"
  },
  {
    "text": "that job never letting the job fail we've uh resolved this by not cleaning",
    "start": "2790599",
    "end": "2795819"
  },
  {
    "text": "up the pods directly but by just letting the job fail and cleaning up the job itself the delete on the job propagated to",
    "start": "2795819",
    "end": "2803020"
  },
  {
    "text": "clean up the respective pods cleaning up everything with the implementation of this nanny",
    "start": "2803020",
    "end": "2808660"
  },
  {
    "text": "process to clean up resources in our cluster we saw a saving of over a million dollars in compute spend per",
    "start": "2808660",
    "end": "2814240"
  },
  {
    "text": "year now I'll handle back to David to wrap up awesome thanks Ashley yeah so to wrap up",
    "start": "2814240",
    "end": "2820960"
  },
  {
    "text": "uh We've laid out the requirements for our sample platform and we took a bit of a tour of all of the organizational",
    "start": "2820960",
    "end": "2826920"
  },
  {
    "text": "processes and Technical considerations we found important in building and maintaining this platform",
    "start": "2826920",
    "end": "2832780"
  },
  {
    "text": "for those building higher order systems on top of kubernetes we really implore you to spend time up front toward",
    "start": "2832780",
    "end": "2838300"
  },
  {
    "text": "establishing these types of processes for maintaining a production production grade cluster and its upgrades having",
    "start": "2838300",
    "end": "2845319"
  },
  {
    "text": "full observability throughout your stack ensuring least privileged access and finally when building platforms that",
    "start": "2845319",
    "end": "2851380"
  },
  {
    "text": "pseudo extend the kubernetes control plane be mindful about what your services actually exposed to your end",
    "start": "2851380",
    "end": "2856720"
  },
  {
    "text": "user in part those safeguards to avoid the eventual non-ideal user that we'll call them and with that uh thank you for",
    "start": "2856720",
    "end": "2863500"
  },
  {
    "text": "watching and that's it",
    "start": "2863500",
    "end": "2867540"
  }
]