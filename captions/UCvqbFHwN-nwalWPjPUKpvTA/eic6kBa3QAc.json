[
  {
    "text": "alright let's get started hola good morning everyone the nos Diaz thank you all for coming",
    "start": "30",
    "end": "7080"
  },
  {
    "text": "it's I'm very happy to see so much interest in our work introductions I am",
    "start": "7080",
    "end": "13290"
  },
  {
    "text": "my name is Vinay and I've worked with future way for over a year now in the Seattle Research Center besides",
    "start": "13290",
    "end": "20160"
  },
  {
    "text": "leading this effort here I work with container networking and container runtimes over there and unfortunately my",
    "start": "20160",
    "end": "28500"
  },
  {
    "text": "co-speaker hang do could not be here today because of unforeseen events last",
    "start": "28500",
    "end": "34020"
  },
  {
    "text": "week and but he did a great job helping with this project so what are we going",
    "start": "34020",
    "end": "41730"
  },
  {
    "text": "to talk about today we are gonna go or a customer scenario which motivated this",
    "start": "41730",
    "end": "47100"
  },
  {
    "text": "whole effort for us and we're gonna quickly take a look at the scaling in",
    "start": "47100",
    "end": "54329"
  },
  {
    "text": "communities today and how pod scheduling works because this is needed for the design and we're gonna go a little bit",
    "start": "54329",
    "end": "61289"
  },
  {
    "text": "deeper into our solution or designed for the vertical scaling of community spots we'll look at some of the policy",
    "start": "61289",
    "end": "67439"
  },
  {
    "text": "controls that we implemented as part of this effort and some how we do the failure handling for in our when we do",
    "start": "67439",
    "end": "75210"
  },
  {
    "text": "the vertical scaling then we're gonna look at how we could potentially integrate with vertical pod autoscaler",
    "start": "75210",
    "end": "81630"
  },
  {
    "text": "which is a project that was created to automate the process of scaling your",
    "start": "81630",
    "end": "86909"
  },
  {
    "text": "pods for you nobody's gonna sit there and do this manually so and then we had",
    "start": "86909",
    "end": "93689"
  },
  {
    "text": "another issue we had to look at which is how to handle memory spikes we now go over that and then I'm gonna cross my",
    "start": "93689",
    "end": "100740"
  },
  {
    "text": "fingers and do a demo and get an a after that so this is this is a biotech",
    "start": "100740",
    "end": "111000"
  },
  {
    "text": "industry customer and they are currently using kubernetes jobs for running their",
    "start": "111000",
    "end": "117090"
  },
  {
    "text": "applications this is a very long process what I understand is that the end-to-end",
    "start": "117090",
    "end": "122729"
  },
  {
    "text": "run for analysis takes almost a day and they have some jobs which run for almost",
    "start": "122729",
    "end": "127740"
  },
  {
    "text": "four hours so if your pod were to get killed hurry started because of resources they",
    "start": "127740",
    "end": "135850"
  },
  {
    "text": "lose a lot of work now they have this legacy applications and they could",
    "start": "135850",
    "end": "142600"
  },
  {
    "text": "potentially rewrite it to you know become more restart resilient but that's",
    "start": "142600",
    "end": "147970"
  },
  {
    "text": "not a priority or not feasible for them to do it's don't fix if it ain't broken kind",
    "start": "147970",
    "end": "153190"
  },
  {
    "text": "of moto there and so they were looking to get this feature where they estimate",
    "start": "153190",
    "end": "158830"
  },
  {
    "text": "this elastic part feature would save them about fifty to sixty percent in resource utilization cost savings which",
    "start": "158830",
    "end": "164200"
  },
  {
    "text": "otherwise they're resizing for peak usage and they're not utilizing the full",
    "start": "164200",
    "end": "170380"
  },
  {
    "text": "capacity of the cluster note besides that this has been a very long desired",
    "start": "170380",
    "end": "176140"
  },
  {
    "text": "feature the first issue requesting this feature was raised four years ago so",
    "start": "176140",
    "end": "181209"
  },
  {
    "text": "it's been a while a horizontal scaling",
    "start": "181209",
    "end": "187930"
  },
  {
    "text": "as many of you already know it's the bigger the load more number of pods unique smaller the load you reduce the",
    "start": "187930",
    "end": "193660"
  },
  {
    "text": "pods kubernetes the forces already and vertical scaling this is gonna be the focus of our talk today this is where we",
    "start": "193660",
    "end": "200980"
  },
  {
    "text": "increase or decrease the resources given to a part that's in for now it is CPU and memory for our top and this is",
    "start": "200980",
    "end": "208540"
  },
  {
    "text": "common for stateful applications and the job that I just described with our customers scenario what coal power",
    "start": "208540",
    "end": "215620"
  },
  {
    "text": "autoscaler can do this for you but the way it does it today it ever except trend bar and",
    "start": "215620",
    "end": "221739"
  },
  {
    "text": "writes the new resources which is restarting the pod and that may not work",
    "start": "221739",
    "end": "226780"
  },
  {
    "text": "for our customer",
    "start": "226780",
    "end": "229560"
  },
  {
    "text": "the scheduling this is a very high-level or with how a pod gets scheduled so we",
    "start": "232840",
    "end": "239900"
  },
  {
    "text": "have the core components the API server scheduler couplet and runtime I left out",
    "start": "239900",
    "end": "246440"
  },
  {
    "text": "etcd intentionally for space so it's knots with the pod create we put the part in the API server and the scheduler",
    "start": "246440",
    "end": "254780"
  },
  {
    "text": "which is watching for new parts its job is to find parts that don't have a node and find it at home which is a node and",
    "start": "254780",
    "end": "261100"
  },
  {
    "text": "to do that it runs through a bunch of predicates and priorities it's a",
    "start": "261100",
    "end": "266300"
  },
  {
    "text": "non-trivial process it takes a little bit of work to do that and one",
    "start": "266300",
    "end": "271700"
  },
  {
    "text": "particular predicate that's of interest to us here is called pod Fitz resources",
    "start": "271700",
    "end": "277420"
  },
  {
    "text": "this predicate finds filters out the nodes which can accommodate the pot and",
    "start": "277420",
    "end": "283540"
  },
  {
    "text": "it's once the scheduler has found the",
    "start": "283540",
    "end": "289090"
  },
  {
    "text": "found a node for the pot it executes sabayon in point find operation and the",
    "start": "289090",
    "end": "294770"
  },
  {
    "text": "pot is assigned to the node it essentially sets the node name for the scheduler in the pot spec now the",
    "start": "294770",
    "end": "303170"
  },
  {
    "text": "couplet goes to work the couplet is watching parts that are bound to its node and when it sees the new part come",
    "start": "303170",
    "end": "310400"
  },
  {
    "text": "to its node it does a pod admission control check this it does another check",
    "start": "310400",
    "end": "317420"
  },
  {
    "text": "to see if the pods gonna fit on the node this is necessary because community",
    "start": "317420",
    "end": "323780"
  },
  {
    "text": "supports multiple schedulers as you may know and that inherently brings in potential race condition so your couplet",
    "start": "323780",
    "end": "331340"
  },
  {
    "text": "is the ultimate gate it gets to say yes or no so let's assume everything goes well then the cubelet now moves to setup",
    "start": "331340",
    "end": "339230"
  },
  {
    "text": "the pod which is today's sets of the C groups and then it calls the container",
    "start": "339230",
    "end": "344300"
  },
  {
    "text": "CRI API asking the runtime to run the set of the start the container and the",
    "start": "344300",
    "end": "350570"
  },
  {
    "text": "container runtime does that and reports back status and cubelet then updates the",
    "start": "350570",
    "end": "356270"
  },
  {
    "text": "parts status saying okay the container is ready country is running and it had everything",
    "start": "356270",
    "end": "361759"
  },
  {
    "text": "and let's go to when we started working",
    "start": "361759",
    "end": "368870"
  },
  {
    "text": "on this project we were faced with a couple of potential options the choices",
    "start": "368870",
    "end": "375499"
  },
  {
    "text": "we had what first choice was to directly update their pod resources and let the",
    "start": "375499",
    "end": "381229"
  },
  {
    "text": "cubelet and scheduler update their update they did I in parallel so one",
    "start": "381229",
    "end": "388759"
  },
  {
    "text": "thing I forgot to mention earlier the scheduler it keeps a cache of the node available resources it does this for",
    "start": "388759",
    "end": "396169"
  },
  {
    "text": "performance reasons I suppose and because of that it doesn't have a live",
    "start": "396169",
    "end": "401539"
  },
  {
    "text": "view of the node available resources and if we went with this option let's say",
    "start": "401539",
    "end": "407389"
  },
  {
    "text": "that cubelet were to apply the resize it says ok yeah this this capacity request",
    "start": "407389",
    "end": "413749"
  },
  {
    "text": "for the from the part I can do that I have a capacity but at that very moment the scheduler might be working on",
    "start": "413749",
    "end": "420919"
  },
  {
    "text": "assigning a new part to this node which is going to use that capacity so when that new pot arrives at the node it's",
    "start": "420919",
    "end": "427999"
  },
  {
    "text": "going to get rejected and then the scheduler will have to go back to work all over again from the top so this",
    "start": "427999",
    "end": "433490"
  },
  {
    "text": "would increase the overall scheduling load for the system so the other option",
    "start": "433490",
    "end": "438740"
  },
  {
    "text": "was to put the resize into the workflow bringing the scheduler into the",
    "start": "438740",
    "end": "444259"
  },
  {
    "text": "decision-making just as it would if it were a new part coming in we looked at this as ok this is the current part has",
    "start": "444259",
    "end": "451459"
  },
  {
    "text": "requested X now the it's requesting X plus y so it's as if it's a new Delta",
    "start": "451459",
    "end": "458389"
  },
  {
    "text": "pod of size Y on the same node so we went with this option the scheduler the",
    "start": "458389",
    "end": "464180"
  },
  {
    "text": "way we do this is we set an annotation saying I want my resources are X I want",
    "start": "464180",
    "end": "471080"
  },
  {
    "text": "Y and schedule looks at this and looks at its node available resources cache and says ok looks like you're the node",
    "start": "471080",
    "end": "478789"
  },
  {
    "text": "has capacity I'm going to let this through and once the scheduler lets this through it does so by updating the",
    "start": "478789",
    "end": "485419"
  },
  {
    "text": "resources section the couplet takes over and it does the submission so we are we are focused on updating the",
    "start": "485419",
    "end": "493930"
  },
  {
    "text": "resources section here and as shown in the spec so going a bit more deeper into",
    "start": "493930",
    "end": "501940"
  },
  {
    "text": "this it starts with the API caller which could be you could be using coop CTO a",
    "start": "501940",
    "end": "507610"
  },
  {
    "text": "patch if you were doing it manually in automation it would be a vertical part",
    "start": "507610",
    "end": "512950"
  },
  {
    "text": "autoscaler which would be doing this for you so we in our case the customer wanted jobs so",
    "start": "512950",
    "end": "519700"
  },
  {
    "text": "naturally we started with job controller but this could be done for any kind of controller or it could be done for a pod",
    "start": "519700",
    "end": "525700"
  },
  {
    "text": "standalone pod and so what we do is we patch the job saying okay I have one CPU",
    "start": "525700",
    "end": "532570"
  },
  {
    "text": "I need to and then the API server does some basic validation which is you",
    "start": "532570",
    "end": "538630"
  },
  {
    "text": "should not change the QoS class by doing this and once it sees that the request",
    "start": "538630",
    "end": "544630"
  },
  {
    "text": "is good there is no errors there it updates the etcd and updates the job spec now the job",
    "start": "544630",
    "end": "551920"
  },
  {
    "text": "controller here is washing the job spec it sees there is a difference between the job spec and what the running bar",
    "start": "551920",
    "end": "559480"
  },
  {
    "text": "has and it says ok this is a case of a resize we need to bring the running part",
    "start": "559480",
    "end": "566650"
  },
  {
    "text": "to the desired size that's in the job spec so it sets the annotation and the",
    "start": "566650",
    "end": "572530"
  },
  {
    "text": "scheduler which is watching the annotation goes to work here so what the",
    "start": "572530",
    "end": "579850"
  },
  {
    "text": "scheduler does it interprets that annotation as a resize request and it checks its node resources cache if there",
    "start": "579850",
    "end": "586600"
  },
  {
    "text": "is capacity it will say ok we're going to go to step 6 which is update the pod resources now if the request cannot be",
    "start": "586600",
    "end": "595290"
  },
  {
    "text": "cannot be filled in that node then it looks at the policy if the policy allows",
    "start": "595290",
    "end": "601090"
  },
  {
    "text": "ok if I can't get two CPUs on this node put me on some other node then it will",
    "start": "601090",
    "end": "606550"
  },
  {
    "text": "evict the pod and then we schedule it to you know if the policy says now don't",
    "start": "606550",
    "end": "612160"
  },
  {
    "text": "reschedule me then it will market his faith so in this case we're going to go with the six step six where we had the",
    "start": "612160",
    "end": "620800"
  },
  {
    "text": "capacity and were able to update the pond resources now it's cubelets time to act",
    "start": "620800",
    "end": "626440"
  },
  {
    "text": "so couplet is washing the pot as well so it sees that there is a update to the pots back and it says okay looks like it",
    "start": "626440",
    "end": "634330"
  },
  {
    "text": "needs more resources or less resources and it runs through the same predicates",
    "start": "634330",
    "end": "639430"
  },
  {
    "text": "that the scheduler is looking through same algorithm so it sees if it fits this is again with the multiple",
    "start": "639430",
    "end": "646300"
  },
  {
    "text": "scheduler race condition in mind so assuming that it fits then it will set",
    "start": "646300",
    "end": "652390"
  },
  {
    "text": "the resize pod condition to true if it does not fit it would set the resize pod",
    "start": "652390",
    "end": "657940"
  },
  {
    "text": "condition to false now and again if it's",
    "start": "657940",
    "end": "662950"
  },
  {
    "text": "doesn't fit it will check policy and evict if necessary or Marquez failed now",
    "start": "662950",
    "end": "668500"
  },
  {
    "text": "we go to the next step where if the scheduler is watching the pod status",
    "start": "668500",
    "end": "675460"
  },
  {
    "text": "once the Keyblade has had a chance to update this look at the resize request",
    "start": "675460",
    "end": "680890"
  },
  {
    "text": "and update the pod status if the couplet was successfully was able to",
    "start": "680890",
    "end": "686830"
  },
  {
    "text": "successfully resize support accept the updated request then there is nothing else left to do for the scheduler this",
    "start": "686830",
    "end": "693190"
  },
  {
    "text": "is a completion of a successful resize operation however if the couplet said",
    "start": "693190",
    "end": "698410"
  },
  {
    "text": "now I couldn't take this resize because there's no capacity then the pod resize",
    "start": "698410",
    "end": "704110"
  },
  {
    "text": "condition would be set to false and the scheduler will roll back the resource update that it did in step six and at",
    "start": "704110",
    "end": "712390"
  },
  {
    "text": "this point the job controller will come and take a look at it so you know this is not they don't match I'm gonna try",
    "start": "712390",
    "end": "720459"
  },
  {
    "text": "but this is a failure I'm gonna come back later and try again when I might be supposed successful so we had we got",
    "start": "720459",
    "end": "731980"
  },
  {
    "text": "triggers for us a certain degree of control from the users and we came up",
    "start": "731980",
    "end": "738010"
  },
  {
    "text": "with these policies the in place preferred policy is where you where you say okay if I",
    "start": "738010",
    "end": "746680"
  },
  {
    "text": "cannot be resized to the requested capacity on that note we scheduled to a new node and then I'll be fine",
    "start": "746680",
    "end": "754180"
  },
  {
    "text": "this is in this case you could have this is for then you have deployments this",
    "start": "754180",
    "end": "759970"
  },
  {
    "text": "would be the kind of scenario that you are looking to there is a thing called pot description budget where you specify",
    "start": "759970",
    "end": "767190"
  },
  {
    "text": "okay I have I want five pots but at any given time I want two parts to be online",
    "start": "767190",
    "end": "773880"
  },
  {
    "text": "so in that case the scheduler won't resize all five parts at the same time",
    "start": "773880",
    "end": "779740"
  },
  {
    "text": "it's going to look at oh okay there is a PDB I'm gonna have to I can risk",
    "start": "779740",
    "end": "785290"
  },
  {
    "text": "reschedule three parts but I'm gonna keep two of them for later so it's Maximus fail and the controller comes",
    "start": "785290",
    "end": "790570"
  },
  {
    "text": "back later and says okay this was marked as fail and then it's gonna retry later so eventually the resize request will",
    "start": "790570",
    "end": "799030"
  },
  {
    "text": "get to the desired state and in place only that's the kind of policy that our",
    "start": "799030",
    "end": "805360"
  },
  {
    "text": "customer would potentially use they don't want they if the CPU if you start",
    "start": "805360",
    "end": "811390"
  },
  {
    "text": "for CPU and you can potentially continue running sub-optimally so you'd say okay",
    "start": "811390",
    "end": "817270"
  },
  {
    "text": "I'll take my chances I'm going to continue to run here because rescheduling me is more expensive and",
    "start": "817270",
    "end": "823560"
  },
  {
    "text": "the the customer whose application doesn't wanna be he said very schedule",
    "start": "823560",
    "end": "829120"
  },
  {
    "text": "would choose this policy and then kubernetes system would try to get to",
    "start": "829120",
    "end": "834160"
  },
  {
    "text": "that at a later point through retries restart this is an interesting one where",
    "start": "834160",
    "end": "840130"
  },
  {
    "text": "there are some applications where even if you were to successfully resize the pod all the same node it's not able to",
    "start": "840130",
    "end": "847300"
  },
  {
    "text": "use that this one example that I have for this is Java with the X MX flag",
    "start": "847300",
    "end": "852970"
  },
  {
    "text": "where you specify the max limit even if you increase it you have to restart the application to get that make use of that",
    "start": "852970",
    "end": "859600"
  },
  {
    "text": "increased memory limit and failure",
    "start": "859600",
    "end": "864970"
  },
  {
    "text": "handling the scheme came out actually as we were working through our",
    "start": "864970",
    "end": "870249"
  },
  {
    "text": "we figured that you know kubernetes does it's common for communities to we try it",
    "start": "870249",
    "end": "876369"
  },
  {
    "text": "with a back off okay I'll try now if it doesn't work I'll try it two seconds later four seconds later we figured we",
    "start": "876369",
    "end": "883329"
  },
  {
    "text": "could also do this based on events so if you have a policy of increase preferred",
    "start": "883329",
    "end": "888579"
  },
  {
    "text": "and you failed because of poor desertion budget when you get the notification that your pdb is available you try",
    "start": "888579",
    "end": "895480"
  },
  {
    "text": "that's a good time to try and if it is the case we're in place only and you have you don't have resources on the",
    "start": "895480",
    "end": "903160"
  },
  {
    "text": "node then when other parts of the same priority are hired they leave the node",
    "start": "903160",
    "end": "908980"
  },
  {
    "text": "that's a good time to try so we we would do this based on events besides in",
    "start": "908980",
    "end": "915549"
  },
  {
    "text": "addition to doing based on time vertical",
    "start": "915549",
    "end": "923859"
  },
  {
    "text": "power arts give us this integration with VBA we were looking into how to do this",
    "start": "923859",
    "end": "929790"
  },
  {
    "text": "because eventually we will be using VBA what we did here was this is a very",
    "start": "929790",
    "end": "937749"
  },
  {
    "text": "brief overview of how BPA works here you have the recommender you have the B pod vpa object where the recommendations are",
    "start": "937749",
    "end": "945220"
  },
  {
    "text": "written and you have the updater which does which takes actions so we p8 the",
    "start": "945220",
    "end": "950230"
  },
  {
    "text": "way it comes up with recommendation it reads the current pod usage of resources",
    "start": "950230",
    "end": "955769"
  },
  {
    "text": "from the metrics API I believe API server has this metrics API which pulls the four metrics server",
    "start": "955769",
    "end": "962230"
  },
  {
    "text": "you have to run that component and then you have Prometheus cap a time series database which stores the history",
    "start": "962230",
    "end": "968860"
  },
  {
    "text": "historic utilization of the pod and BPA consumes these two in pieces of",
    "start": "968860",
    "end": "975369"
  },
  {
    "text": "information and the recommender comes up with a recommendation in this case if you look at the example there we have",
    "start": "975369",
    "end": "982269"
  },
  {
    "text": "two CPU and one gig of memory maybe VPN says ok you might benefit with",
    "start": "982269",
    "end": "988059"
  },
  {
    "text": "more memory so I'm gonna write two gig of memory with 1.5 lower bond 2.5 upper",
    "start": "988059",
    "end": "993069"
  },
  {
    "text": "bound this is a very abbreviated example of what a recommendation looks like and",
    "start": "993069",
    "end": "999100"
  },
  {
    "text": "once we piay recomm recommender has written the recommendation to the VP a object for that part",
    "start": "999100",
    "end": "1005970"
  },
  {
    "text": "VP a updater goes to work VP updater is watching the pod the pod VP a object and",
    "start": "1005970",
    "end": "1014540"
  },
  {
    "text": "its job is to figure out how to give this pod the new recommended resources",
    "start": "1014540",
    "end": "1020600"
  },
  {
    "text": "now today it does this by evicting the current pod and then the controller",
    "start": "1020600",
    "end": "1027600"
  },
  {
    "text": "creates a new part to replace that oh I'm missing a part so it's going to create a new part and during admission",
    "start": "1027600",
    "end": "1034230"
  },
  {
    "text": "control it rewrites the resources section with the new recommended resources with our work that is",
    "start": "1034230",
    "end": "1041520"
  },
  {
    "text": "unnecessary we just simply update it and",
    "start": "1041520",
    "end": "1046760"
  },
  {
    "text": "which is essentially we call the patch API to do that",
    "start": "1047630",
    "end": "1053300"
  },
  {
    "text": "coming to memory usage spikes so even with this feature you could have your",
    "start": "1054330",
    "end": "1061080"
  },
  {
    "text": "pods getting out of memory killed this can happen for a few reasons you have",
    "start": "1061080",
    "end": "1067290"
  },
  {
    "text": "there is a certain sampling interval for the map from the metrics API which is I believe 30 seconds and VP a response",
    "start": "1067290",
    "end": "1074190"
  },
  {
    "text": "there is a lag with VP as well so if your memory is really spiking which happens at the start of the application",
    "start": "1074190",
    "end": "1080010"
  },
  {
    "text": "or maybe at the tail end you could potentially have your app killed before the VP has had a chance to resize it so",
    "start": "1080010",
    "end": "1087290"
  },
  {
    "text": "we looked into this and we were looking for a solution we looked at this home",
    "start": "1087290",
    "end": "1093720"
  },
  {
    "text": "kill disable flag that's there the C group and I believe it's kind of",
    "start": "1093720",
    "end": "1099810"
  },
  {
    "text": "supported with different runtimes although it's hit or miss we put an",
    "start": "1099810",
    "end": "1106020"
  },
  {
    "text": "annotation there which the uncle disable annotation which if you would set it true we disabled the home killer for the",
    "start": "1106020",
    "end": "1112920"
  },
  {
    "text": "part and what happens in this case your pod won't get killed if it exceeds the",
    "start": "1112920",
    "end": "1118740"
  },
  {
    "text": "memory limit that you are asked for instead the applications in the pod get suspended there paused and then",
    "start": "1118740",
    "end": "1126170"
  },
  {
    "text": "until VP a or has had a chance to look at the usage and then update and give it",
    "start": "1126170",
    "end": "1132320"
  },
  {
    "text": "more memory for example and once that happens the application becomes runnable",
    "start": "1132320",
    "end": "1137330"
  },
  {
    "text": "again and its resume starts running again this is suitable for our customers",
    "start": "1137330",
    "end": "1144590"
  },
  {
    "text": "long-running jobs where if you're a for our job at 3 or 15 minutes you hit this",
    "start": "1144590",
    "end": "1150050"
  },
  {
    "text": "condition you do not want to be killed for that and with that let's so let's do",
    "start": "1150050",
    "end": "1159350"
  },
  {
    "text": "the demo in this demo what I'm gonna do is I'm gonna run this coop CTL patch",
    "start": "1159350",
    "end": "1165650"
  },
  {
    "text": "command where I'm patching the the container name stress I'm patching its",
    "start": "1165650",
    "end": "1172280"
  },
  {
    "text": "resources so it currently has two CPUs and one gig I'm gonna be patching it to",
    "start": "1172280",
    "end": "1177770"
  },
  {
    "text": "two gig so hopefully it works if you're",
    "start": "1177770",
    "end": "1186080"
  },
  {
    "text": "in the back and you can't really see it clearly please feel free to come from come forward and I apologize the text is",
    "start": "1186080",
    "end": "1197810"
  },
  {
    "text": "gonna be a little small for the people who are at the back",
    "start": "1197810",
    "end": "1201790"
  },
  {
    "text": "so this is the job that have been talking about we're looking at the shop",
    "start": "1216420",
    "end": "1223950"
  },
  {
    "text": "would run a part that contains one container in stress it's your Ubuntu",
    "start": "1223950",
    "end": "1230090"
  },
  {
    "text": "image with stress app installed on it because I want to run some stress tests on this to stress the memory and it's",
    "start": "1230090",
    "end": "1237180"
  },
  {
    "text": "going to ask for three CPUs and one gig of memory so let's create the part so it",
    "start": "1237180",
    "end": "1249030"
  },
  {
    "text": "looks like it's created and we're gonna go in and do a just watch the pod",
    "start": "1249030",
    "end": "1256850"
  },
  {
    "text": "now this is just your standard cube CTL described part output I put some filters",
    "start": "1258110",
    "end": "1264480"
  },
  {
    "text": "and there to filter out information that we really don't care what and care about and focus on what we are interested in",
    "start": "1264480",
    "end": "1270360"
  },
  {
    "text": "here so you can see as you can see the pod has been scheduled it's running on node one it's got this container ID here",
    "start": "1270360",
    "end": "1277950"
  },
  {
    "text": "and it's got three CPUs and one gig of memory now let's run the stress",
    "start": "1277950",
    "end": "1287340"
  },
  {
    "text": "application asking for 500 Meg what this does is it gets into the container execs",
    "start": "1287340",
    "end": "1293130"
  },
  {
    "text": "into the container and runs the stress tool and asks for 500 make of memory and",
    "start": "1293130",
    "end": "1298950"
  },
  {
    "text": "runs for five seconds exist Alice zero which is a good thing we were",
    "start": "1298950",
    "end": "1304350"
  },
  {
    "text": "successfully able to run this dress too now what would happen if we were to go about the limit oops",
    "start": "1304350",
    "end": "1314000"
  },
  {
    "text": "well it's expected you have one gig of memory and you're going for 1.5 and the",
    "start": "1314000",
    "end": "1320040"
  },
  {
    "text": "own killer says no so signal line as you got terminated today you can VP a would",
    "start": "1320040",
    "end": "1328560"
  },
  {
    "text": "potentially do this fix it would take care of this for you by analyzing what your current usage is it's growing and",
    "start": "1328560",
    "end": "1335040"
  },
  {
    "text": "then giving you more memory but as I mentioned earlier you have to reschedule the part you have",
    "start": "1335040",
    "end": "1342270"
  },
  {
    "text": "to evict the current part and then update the resources of the new part let's try doing it without updating the",
    "start": "1342270",
    "end": "1350400"
  },
  {
    "text": "resources so I'm gonna do have a handy little script that does it for me so all",
    "start": "1350400",
    "end": "1360480"
  },
  {
    "text": "this does is runs that cube city old patch command that I described in the earlier slide so we did that and if",
    "start": "1360480",
    "end": "1366660"
  },
  {
    "text": "you're watching the blue window you see that it's gone to do gig so the container the pod is running the",
    "start": "1366660",
    "end": "1373500"
  },
  {
    "text": "container is running it hasn't been restarted let's make sure that's the case so we just run this again and see",
    "start": "1373500",
    "end": "1379490"
  },
  {
    "text": "it's now gonna go and allocate 1.5 gig it seems to be getting farther along",
    "start": "1379490",
    "end": "1384930"
  },
  {
    "text": "down the road and exists out of zero that's a good thing alright that now I",
    "start": "1384930",
    "end": "1396330"
  },
  {
    "text": "can uncross my fingers and let's continue so we had just",
    "start": "1396330",
    "end": "1407960"
  },
  {
    "text": "so last year late last year as we were finishing up the first iteration of this project",
    "start": "1422220",
    "end": "1427759"
  },
  {
    "text": "JD comms modulator comes engineers reached out to us and they said they were interested in trying out our work",
    "start": "1427759",
    "end": "1433980"
  },
  {
    "text": "and we partnered with them they needed this for enable this for deployment",
    "start": "1433980",
    "end": "1440429"
  },
  {
    "text": "parts so we worked with them and got it working for deployment and they're",
    "start": "1440429",
    "end": "1445830"
  },
  {
    "text": "reporting that it's doing fairly well in its in pre-production staging they found",
    "start": "1445830",
    "end": "1451350"
  },
  {
    "text": "another use case for this where if they see parts that are not scheduled they're",
    "start": "1451350",
    "end": "1457379"
  },
  {
    "text": "pending because resources is not available they do a walk-through and see which parts are not using the resources",
    "start": "1457379",
    "end": "1464820"
  },
  {
    "text": "that they have been given and then try to resize them down and that way that they get these pending pods running so",
    "start": "1464820",
    "end": "1472529"
  },
  {
    "text": "they get more work done and the the last",
    "start": "1472529",
    "end": "1479940"
  },
  {
    "text": "slide that I have here these are the resources it's the design doc and implementation these are pretty much",
    "start": "1479940",
    "end": "1485490"
  },
  {
    "text": "like I see at this point what I want to focus on is the latest the third bullet point over there",
    "start": "1485490",
    "end": "1491429"
  },
  {
    "text": "based on there was a another design that was proposal that was preceding us so",
    "start": "1491429",
    "end": "1497370"
  },
  {
    "text": "with Carol and viata from the auto-scaling team they started this new",
    "start": "1497370",
    "end": "1503009"
  },
  {
    "text": "proposal which merges the two design designs and then it's working progress",
    "start": "1503009",
    "end": "1509940"
  },
  {
    "text": "at this point I highly recommend that you guys go to this github kubernetes",
    "start": "1509940",
    "end": "1516330"
  },
  {
    "text": "enhancement enhancements link and take a look at that if this was you before we",
    "start": "1516330",
    "end": "1523080"
  },
  {
    "text": "conclude I have a few asks container run time growers if you're in the audience please ensure good support for update",
    "start": "1523080",
    "end": "1530129"
  },
  {
    "text": "container resources API and consider our own kill disable we kind of would like",
    "start": "1530129",
    "end": "1535409"
  },
  {
    "text": "that feature to be enabled and work well across different runtimes Kartik especially and we have been",
    "start": "1535409",
    "end": "1545679"
  },
  {
    "text": "working closely with signaled on this new cap and one of the things that we",
    "start": "1545679",
    "end": "1551770"
  },
  {
    "text": "could use how but signal could use help with is reviewing code PRS and looking",
    "start": "1551770",
    "end": "1558940"
  },
  {
    "text": "at issues if you have the bandwidth and the interest we'd really like you to do",
    "start": "1558940",
    "end": "1564669"
  },
  {
    "text": "that there is another talk right after this talk back Carol and be at our I it's",
    "start": "1564669",
    "end": "1571169"
  },
  {
    "text": "also it's going to talk about this latest cap and where things are I highly",
    "start": "1571169",
    "end": "1576490"
  },
  {
    "text": "recommend that you attend that so with that I conclude the presentation and",
    "start": "1576490",
    "end": "1582669"
  },
  {
    "text": "open up for questions the mic here",
    "start": "1582669",
    "end": "1589259"
  },
  {
    "text": "let's say I have a long-running job let's say two or three hours and I don't want to deal with defining the resources",
    "start": "1606120",
    "end": "1613000"
  },
  {
    "text": "all the time like my CPU or whatever do you recommend using the VPI or like",
    "start": "1613000",
    "end": "1620170"
  },
  {
    "text": "dealing with it manually like let's say the job fails let's raise the memory",
    "start": "1620170",
    "end": "1627600"
  },
  {
    "text": "maybe not like okay so actually when I'm talking about the new job something I",
    "start": "1627600",
    "end": "1634120"
  },
  {
    "text": "didn't get tested yet okay the question is do you recommend manually updating",
    "start": "1634120",
    "end": "1640570"
  },
  {
    "text": "the resources or using VP a I believe the answer is to use VP a nobody wants to you could do it manually for",
    "start": "1640570",
    "end": "1647350"
  },
  {
    "text": "experimenting to see what is the right size for your resources but VP a has a",
    "start": "1647350",
    "end": "1652840"
  },
  {
    "text": "mode where it lets you do that for more details I think I would recommend I would look recommend looking at the VP a",
    "start": "1652840",
    "end": "1659140"
  },
  {
    "text": "project documentation I would not do this manually I would use the VP I",
    "start": "1659140",
    "end": "1664300"
  },
  {
    "text": "recommended to come up with recommendations and set that as your starting point and let VP a use when",
    "start": "1664300",
    "end": "1671680"
  },
  {
    "text": "this feature comes in and becomes available upstream I would recommend using okay",
    "start": "1671680",
    "end": "1678130"
  },
  {
    "text": "and one more question like when you define the VPI you can choose what update mode like you can do off initial",
    "start": "1678130",
    "end": "1685180"
  },
  {
    "text": "auto whatever like you currently they have those modes of initial in auto the",
    "start": "1685180",
    "end": "1691540"
  },
  {
    "text": "auto mode I believe will promote like it ruins your port and it creates it's like",
    "start": "1691540",
    "end": "1696970"
  },
  {
    "text": "automatically if it sees like it's like the auto mode is the one where it would",
    "start": "1696970",
    "end": "1703480"
  },
  {
    "text": "update it for you today does it by evicting I would say that eventually it",
    "start": "1703480",
    "end": "1711220"
  },
  {
    "text": "should become instead of evicting updated in place but if not there might",
    "start": "1711220",
    "end": "1716620"
  },
  {
    "text": "be a new mode for it I don't know where that will land this feature this proposal is still a work in progress and",
    "start": "1716620",
    "end": "1723130"
  },
  {
    "text": "once it's approved we'll work on implementing it and then a full land it'll take some time for that address in",
    "start": "1723130",
    "end": "1741910"
  },
  {
    "text": "your use case you have this very important job that you could not kill but if there are not resources on the",
    "start": "1741910",
    "end": "1748210"
  },
  {
    "text": "nodes can you also evict other parts to other nodes because I think there are also other initiatives to make the",
    "start": "1748210",
    "end": "1754419"
  },
  {
    "text": "scheduler more dynamic to spread of workloads even and how does it work together with the proposal that you had",
    "start": "1754419",
    "end": "1760900"
  },
  {
    "text": "now yes in the new proposal we are considering that so the scheduler looks",
    "start": "1760900",
    "end": "1766240"
  },
  {
    "text": "at any parts that are of lower priority than the current part and it fixed them",
    "start": "1766240",
    "end": "1772960"
  },
  {
    "text": "as part of giving it more resources that's natural if you were to schedule a new part it would do that and if a new",
    "start": "1772960",
    "end": "1780970"
  },
  {
    "text": "part with the higher priority comes in it does the eviction anyways that feature came in recently I believe",
    "start": "1780970",
    "end": "1786580"
  },
  {
    "text": "I don't know maybe in the last year or so so that feature is there it's gonna leverage that",
    "start": "1786580",
    "end": "1793080"
  },
  {
    "text": "anybody",
    "start": "1798430",
    "end": "1801430"
  },
  {
    "text": "so just one one question how does it handle extended resurface or or does it",
    "start": "1810390",
    "end": "1817090"
  },
  {
    "text": "handle them at all today we we don't have support for extended resources that",
    "start": "1817090",
    "end": "1825000"
  },
  {
    "text": "mainly vp8 is not supported today but that's something that's been asked before if there is a strong desire for",
    "start": "1825000",
    "end": "1832330"
  },
  {
    "text": "doing this for extended resources the support will eventually come in the scope of this cap is kept for the CPU",
    "start": "1832330",
    "end": "1840700"
  },
  {
    "text": "and memory at this point and potentially ephemeral disk space are you considering",
    "start": "1840700",
    "end": "1857050"
  },
  {
    "text": "also scaling down in memory or CPU usage based on for example Prometheus",
    "start": "1857050",
    "end": "1864210"
  },
  {
    "text": "measurements yes yes in the new cap we are gonna be auto scaling down the auto",
    "start": "1864210",
    "end": "1872740"
  },
  {
    "text": "scaling down of memory is a little bit of a challenge because you can't really take away memory if the pages are in use",
    "start": "1872740",
    "end": "1878590"
  },
  {
    "text": "so we're still working through how we could really do that I think the solution there is to set your limit at",
    "start": "1878590",
    "end": "1886740"
  },
  {
    "text": "slightly below what's being currently used and try to force a reclaim that is what it would do at this point but CPU",
    "start": "1886740",
    "end": "1894940"
  },
  {
    "text": "we can take away see giving and taking away CPU is pretty easy that we do that",
    "start": "1894940",
    "end": "1900040"
  },
  {
    "text": "that's I think it works I haven't really tested this end to end but you can see",
    "start": "1900040",
    "end": "1907330"
  },
  {
    "text": "that the CPU count has been reduced and the C group enforces not when you do that down",
    "start": "1907330",
    "end": "1913200"
  },
  {
    "text": "just one question so is this also integrated with horizontal cluster",
    "start": "1923280",
    "end": "1929320"
  },
  {
    "text": "autoscaler if the cluster is run out of resources this vpa also trigger bring up",
    "start": "1929320",
    "end": "1936850"
  },
  {
    "text": "two more notes that the last I looked it",
    "start": "1936850",
    "end": "1942610"
  },
  {
    "text": "was not recommended to use VP a and horizontal scaling together I don't know",
    "start": "1942610",
    "end": "1949690"
  },
  {
    "text": "if that the support for that will come in at some point I might be a little bit",
    "start": "1949690",
    "end": "1954760"
  },
  {
    "text": "out of date on my information but at this point I believe it's not supported",
    "start": "1954760",
    "end": "1960450"
  },
  {
    "text": "anybody",
    "start": "1961950",
    "end": "1964950"
  },
  {
    "text": "alright it looks like we can conclude this talk of rapper",
    "start": "1968200",
    "end": "1974309"
  },
  {
    "text": "[Applause]",
    "start": "1975610",
    "end": "1978250"
  }
]