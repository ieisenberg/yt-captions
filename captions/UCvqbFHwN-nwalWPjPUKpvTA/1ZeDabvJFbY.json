[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "thanks for coming to this talk just before we start I'd like a show of hands",
    "start": "1490",
    "end": "6930"
  },
  {
    "text": "how many of you have plans or are already running kubernetes on public clouds oh great",
    "start": "6930",
    "end": "13950"
  },
  {
    "text": "okay how many of you would like to save on op X costs okay so I think you've",
    "start": "13950",
    "end": "21869"
  },
  {
    "text": "come to the right talk so my name is",
    "start": "21869",
    "end": "27119"
  },
  {
    "text": "piccoli and here's Arun with me were from platform 9 and today we want to",
    "start": "27119",
    "end": "35070"
  },
  {
    "text": "discuss ways to run kubernetes clusters on public clouds more cheaply ok and to",
    "start": "35070",
    "end": "43440"
  },
  {
    "text": "motivate our talk let me just give you a quick introduction to what we do at platform 9 we are in the business of",
    "start": "43440",
    "end": "51030"
  },
  {
    "start": "45000",
    "end": "45000"
  },
  {
    "text": "running open source software as a service for our enterprise customers and we specialize in infrastructure",
    "start": "51030",
    "end": "58320"
  },
  {
    "text": "management and this means we offer kubernetes OpenStack and fish'n as an",
    "start": "58320",
    "end": "66330"
  },
  {
    "text": "integrated product and what makes this unique is we run the control plane separately from the data plane so that",
    "start": "66330",
    "end": "73470"
  },
  {
    "text": "our customers can run the data plane on the infrastructure of their choice",
    "start": "73470",
    "end": "78689"
  },
  {
    "text": "including bare metal or public cloud instances and then we host the control",
    "start": "78689",
    "end": "86040"
  },
  {
    "text": "plane and we run it as a service and we're currently in the process of",
    "start": "86040",
    "end": "91170"
  },
  {
    "text": "migrating all of our control plane services from VMs onto containers",
    "start": "91170",
    "end": "96600"
  },
  {
    "text": "running on kubernetes running on the public cloud and just like any sane",
    "start": "96600",
    "end": "101880"
  },
  {
    "text": "business we are always exploring ways to be super cost efficient without",
    "start": "101880",
    "end": "107119"
  },
  {
    "text": "sacrificing our quality of service so in",
    "start": "107119",
    "end": "112290"
  },
  {
    "text": "our journey what we've learned and what we've been exploring is you know what",
    "start": "112290",
    "end": "118979"
  },
  {
    "start": "113000",
    "end": "113000"
  },
  {
    "text": "we've discovered is one of the best ways to reduce costs is to take advantage of",
    "start": "118979",
    "end": "126560"
  },
  {
    "text": "spa instances on AWS or preemptable instances on Google Cloud right there",
    "start": "126560",
    "end": "133340"
  },
  {
    "text": "kind of equivalent and are most of you",
    "start": "133340",
    "end": "139220"
  },
  {
    "text": "familiar with the concept of spot instances we can do a quick review so spot instances are essentially the same",
    "start": "139220",
    "end": "148390"
  },
  {
    "text": "virtual machine instances that you can get regularly they have the same types",
    "start": "148390",
    "end": "153830"
  },
  {
    "text": "except that they are considerably cheaper from a per hour cost right and",
    "start": "153830",
    "end": "160880"
  },
  {
    "text": "our observations have shown that on average they can be 60 to 80 percent",
    "start": "160880",
    "end": "168040"
  },
  {
    "text": "lower cost than the regular on-demand instances right so the potential savings",
    "start": "168040",
    "end": "173989"
  },
  {
    "text": "are huge however they come with a drawback you know that there's a catch to them and so",
    "start": "173989",
    "end": "182209"
  },
  {
    "text": "the issue is bought instances or preemptable can be terminated at any",
    "start": "182209",
    "end": "188480"
  },
  {
    "text": "time for reasons that are beyond your control right so for example on AWS if",
    "start": "188480",
    "end": "193600"
  },
  {
    "text": "the real time price exceeds your bid price then you can lose the instance on",
    "start": "193600",
    "end": "201200"
  },
  {
    "text": "Google cloud they can get terminated for a variety of reasons but even if nothing",
    "start": "201200",
    "end": "207200"
  },
  {
    "text": "happens after 24 hours you're guaranteed to lose the instance but by policy so",
    "start": "207200",
    "end": "213590"
  },
  {
    "text": "what this means is to really take advantage of spot instances historically",
    "start": "213590",
    "end": "219489"
  },
  {
    "text": "if you're an application developer you really need to understand the trade-off",
    "start": "219489",
    "end": "225470"
  },
  {
    "text": "between availability and price and kind of match your application to this",
    "start": "225470",
    "end": "232220"
  },
  {
    "text": "trade-off and what this means is typically people have been running specialized workloads on spot and have",
    "start": "232220",
    "end": "241040"
  },
  {
    "text": "required special tooling or scripting to handle a node failure right so the good",
    "start": "241040",
    "end": "247250"
  },
  {
    "text": "news is fast forward to today we have kubernetes and we think that kubernetes is going to",
    "start": "247250",
    "end": "257840"
  },
  {
    "text": "make spa instances mainstream right and why is that kubernetes can really hide",
    "start": "257840",
    "end": "265010"
  },
  {
    "text": "the complexity and the details of spot instances because it is really",
    "start": "265010",
    "end": "270240"
  },
  {
    "text": "built to handle note failure right under the scenes it will for example leverage",
    "start": "270240",
    "end": "276660"
  },
  {
    "text": "public cloud resources like auto-scaling groups that will automatically spin up a",
    "start": "276660",
    "end": "282480"
  },
  {
    "text": "new instance if one dies and at the pod level if you use the correct constructs",
    "start": "282480",
    "end": "288390"
  },
  {
    "text": "like deployment or replica set if your pod resides on one of those nodes and",
    "start": "288390",
    "end": "294560"
  },
  {
    "text": "disappears kubernetes will reschedule another pod somewhere else for you and",
    "start": "294560",
    "end": "300260"
  },
  {
    "text": "crouppen ATS has really taught us to design applications for failure right to",
    "start": "300260",
    "end": "306210"
  },
  {
    "text": "be resilient with constructs like replica sets and services which can",
    "start": "306210",
    "end": "313110"
  },
  {
    "text": "distribute requests across a pool of pods so that if you lose a bunch of pods once in a while",
    "start": "313110",
    "end": "318630"
  },
  {
    "text": "no big deal your application continues to run right so we think that crouppen",
    "start": "318630",
    "end": "323940"
  },
  {
    "text": "Eddie's and spa instances are really a marriage made in heaven but the Devils",
    "start": "323940",
    "end": "330270"
  },
  {
    "text": "in the details right and today arune is going to walk us through all the details",
    "start": "330270",
    "end": "336780"
  },
  {
    "text": "about how to understand spa instances and how to best take advantage of them within the context of kubernetes thanks",
    "start": "336780",
    "end": "344430"
  },
  {
    "text": "Rick so let's dig deeper and jump in to",
    "start": "344430",
    "end": "350400"
  },
  {
    "text": "see how to save money on your bills I wanted to look at this blog that Jeff",
    "start": "350400",
    "end": "358590"
  },
  {
    "text": "Barr from AWS wrote in 2015 and he spoke",
    "start": "358590",
    "end": "363750"
  },
  {
    "text": "about how you choose instances or how you group them into pools so he spoke",
    "start": "363750",
    "end": "370020"
  },
  {
    "text": "about a concept of capacity pool so this capacity pool is a logical encapsulation",
    "start": "370020",
    "end": "378120"
  },
  {
    "text": "of instances that are both standard or on-demand instances and preemptable",
    "start": "378120",
    "end": "385290"
  },
  {
    "text": "instances or spot instances and these instances are pretty much of the same",
    "start": "385290",
    "end": "390750"
  },
  {
    "text": "type so t2 medium they are on the same availability zone and they belong to the",
    "start": "390750",
    "end": "397560"
  },
  {
    "text": "same region and these capacity are attributed to be able to create a",
    "start": "397560",
    "end": "403490"
  },
  {
    "start": "399000",
    "end": "399000"
  },
  {
    "text": "instance or launch an instance at any given time for a given price so what we",
    "start": "403490",
    "end": "410690"
  },
  {
    "text": "need to take into account when we choose such capacity pools for our cluster are",
    "start": "410690",
    "end": "416659"
  },
  {
    "text": "some of these best practices so one is when we build our applications we can make them price aware so if our",
    "start": "416659",
    "end": "424009"
  },
  {
    "text": "applications are built in a way that they don't they are not attached to a particular instance type we can choose a",
    "start": "424009",
    "end": "431930"
  },
  {
    "text": "specific instance which is of lower cost for running that application the other",
    "start": "431930",
    "end": "438879"
  },
  {
    "text": "important thing is about checking the history of the instances so this can be either manual or it can be automated so",
    "start": "438879",
    "end": "445180"
  },
  {
    "text": "it's always nice to go back to history figure out what the costs are and then",
    "start": "445180",
    "end": "451490"
  },
  {
    "text": "think about which a type or which capacity pool would be best to use one",
    "start": "451490",
    "end": "459199"
  },
  {
    "text": "main important thing is we need to make sure that when we deploy a cluster and choose capacity pools we we choose",
    "start": "459199",
    "end": "465409"
  },
  {
    "text": "multiple of them we compose them of both preemptable instances and spot instances",
    "start": "465409",
    "end": "471740"
  },
  {
    "text": "so that our application always runs and our service never goes down let's",
    "start": "471740",
    "end": "478159"
  },
  {
    "start": "478000",
    "end": "478000"
  },
  {
    "text": "quickly take a look at what big mentioned with respect to spot instances in Amazon if you look at the graph there",
    "start": "478159",
    "end": "484789"
  },
  {
    "text": "it's an m1 medium over three months I took it out very recently and you can",
    "start": "484789",
    "end": "490430"
  },
  {
    "text": "see that the spot price is always about 680 percent less than the on-demand",
    "start": "490430",
    "end": "495560"
  },
  {
    "text": "price on-demand is at around point zero nine spot price is less than point zero two if I blow that up the disadvantage",
    "start": "495560",
    "end": "504680"
  },
  {
    "text": "though is that one time or another this spot instance price of the bid price",
    "start": "504680",
    "end": "509840"
  },
  {
    "text": "goes above on-demand price and that point in time it doesn't make too much sense to be running things on spot",
    "start": "509840",
    "end": "515510"
  },
  {
    "text": "instances could switch over to undermine instances and and in ec2 you add a bid",
    "start": "515510",
    "end": "522169"
  },
  {
    "text": "price to a spot instance and normally if the price goes above that bit price the instance was terminated and since",
    "start": "522169",
    "end": "528290"
  },
  {
    "text": "kubernetes handles node failure kubernetes kind of works best with such an in Google flower it's a little different",
    "start": "528290",
    "end": "535040"
  },
  {
    "text": "it's it doesn't run on the surplus capacity market they give you a flat-out",
    "start": "535040",
    "end": "540339"
  },
  {
    "text": "discount on the price the only catch is that the instance is ease always",
    "start": "540339",
    "end": "546980"
  },
  {
    "text": "terminated somewhere around twenty four hours or within 24 hours the good thing is if it gets terminated within ten",
    "start": "546980",
    "end": "553130"
  },
  {
    "text": "minutes I think Google does not charge you so that's the other thing let's look",
    "start": "553130",
    "end": "558380"
  },
  {
    "text": "at what type of applications would be best suited for such infrastructure so not all applications work well with",
    "start": "558380",
    "end": "565610"
  },
  {
    "text": "sparked instances so one the most common use case is bursting applications so",
    "start": "565610",
    "end": "571880"
  },
  {
    "text": "depending on season depending on a special occasion you do have a bunch of",
    "start": "571880",
    "end": "577160"
  },
  {
    "text": "traffic so and that traffic can always be offloaded to spot instances because it goes away and it's a phenomenal",
    "start": "577160",
    "end": "584709"
  },
  {
    "text": "another use case is the HPC industry where the most amount of work you do is",
    "start": "584709",
    "end": "591829"
  },
  {
    "text": "number crunching and and if these processor or your applications are more",
    "start": "591829",
    "end": "596930"
  },
  {
    "text": "or less stateless you can always run them on preemptable instances of spart",
    "start": "596930",
    "end": "602510"
  },
  {
    "text": "instances and it's not too much of a pain if an instance goes down to restart from where it was where the part left",
    "start": "602510",
    "end": "609880"
  },
  {
    "text": "another use case is highly available clustered apps so there are still people",
    "start": "609880",
    "end": "615410"
  },
  {
    "text": "writing applications in a way that there is one which runs active and there's another copy of the app running standby",
    "start": "615410",
    "end": "622040"
  },
  {
    "text": "waiting for waiting to take over in case of the active app going down so in those",
    "start": "622040",
    "end": "628579"
  },
  {
    "text": "cases again the standby app would just be syncing and if it's not participating",
    "start": "628579",
    "end": "633589"
  },
  {
    "text": "in an active active configuration it it can be offloaded to a spot instance and pretty much take lesser of our money",
    "start": "633589",
    "end": "641390"
  },
  {
    "text": "than otherwise another is the node auto scaling so the first one is with elastic bursting is",
    "start": "641390",
    "end": "648019"
  },
  {
    "text": "you increase or dynamically expand your app and with expanding app you will",
    "start": "648019",
    "end": "654199"
  },
  {
    "text": "probably need to expand your infrastructure as well and horizontal power order scaling and horizontal nor",
    "start": "654199",
    "end": "660649"
  },
  {
    "text": "order scaling and kubernetes pretty much gives you a very good mix to solve this use case and it's great to work on support",
    "start": "660649",
    "end": "667379"
  },
  {
    "text": "instances having said that let's look at what we can do to deploy clusters with",
    "start": "667379",
    "end": "674129"
  },
  {
    "start": "673000",
    "end": "673000"
  },
  {
    "text": "either sparked instances or preemptable instances on gke we'll look at GK and",
    "start": "674129",
    "end": "680430"
  },
  {
    "text": "we'll also look at AWS with respect to K ops so in gke the instance groups are",
    "start": "680430",
    "end": "687720"
  },
  {
    "text": "are also called node pools so given a node pool you can specify a type of",
    "start": "687720",
    "end": "694139"
  },
  {
    "text": "instance whether that node pool is going to be preemptable or not so a catch is that within a node pool you cannot have",
    "start": "694139",
    "end": "701009"
  },
  {
    "text": "a preemptable instance and fixed instance it's either other rather so the",
    "start": "701009",
    "end": "706829"
  },
  {
    "text": "best way to build your app is to have two node pools one fixed and one preemptable to start off with you can",
    "start": "706829",
    "end": "715470"
  },
  {
    "text": "always have a fixed node pool and then as in when capacity grows you can add node pools dynamically to the cluster",
    "start": "715470",
    "end": "721949"
  },
  {
    "text": "you can also have note pools with zero nodes in it so that auto scaling will kick in when your application is needing",
    "start": "721949",
    "end": "729839"
  },
  {
    "text": "the capacity this was one of the examples of an app that I had on uke",
    "start": "729839",
    "end": "735870"
  },
  {
    "text": "where the the way I deployed the cluster is created two pools one is the fixed",
    "start": "735870",
    "end": "741329"
  },
  {
    "text": "pool with preemptable in nodes disabled and the other one is enabling",
    "start": "741329",
    "end": "746550"
  },
  {
    "text": "preemptable nodes and if you go with k ops k ops has various backends you can",
    "start": "746550",
    "end": "754230"
  },
  {
    "text": "deploy clusters across multiple cloud providers with k ops I choose this since",
    "start": "754230",
    "end": "760410"
  },
  {
    "text": "it's an open source tool let's see how you deploy and a cluster using K ops and",
    "start": "760410",
    "end": "765689"
  },
  {
    "text": "using sparked instances so how chaos does it is 4k ops the instance group is",
    "start": "765689",
    "end": "771779"
  },
  {
    "text": "the binding and instance group pretty much consists of instances of the same",
    "start": "771779",
    "end": "777449"
  },
  {
    "text": "type so an instance group can either be spot or preemptable or the fixed pool",
    "start": "777449",
    "end": "784889"
  },
  {
    "text": "and when you create a cluster with K ops it creates multiple instance groups one",
    "start": "784889",
    "end": "791670"
  },
  {
    "text": "for the master node master components and one for the nodes or the workers and",
    "start": "791670",
    "end": "797220"
  },
  {
    "text": "these are more more or less backed by auto-scaling groups so if a node gets terminated",
    "start": "797220",
    "end": "802650"
  },
  {
    "text": "within that node node pool or an instance group it gets recreated how",
    "start": "802650",
    "end": "808110"
  },
  {
    "text": "many of you used K ops here Johanns okay so and have you used chaos with sparked",
    "start": "808110",
    "end": "815070"
  },
  {
    "text": "instances by any chance okay perfect so we will see how to do that in a bit this",
    "start": "815070",
    "end": "821250"
  },
  {
    "start": "821000",
    "end": "821000"
  },
  {
    "text": "is how it looks when you deploy a cluster with K ops and AWS you you get a",
    "start": "821250",
    "end": "826290"
  },
  {
    "text": "auto scaling group for a master and you get an auto scaling group for the worker",
    "start": "826290",
    "end": "831840"
  },
  {
    "text": "in our example we'll create another instance pool or an instance group that",
    "start": "831840",
    "end": "837720"
  },
  {
    "text": "are is a worker type node and it has spot instances k op ml file defines the",
    "start": "837720",
    "end": "845370"
  },
  {
    "text": "role and it's defined by either a master or a node and the only difference",
    "start": "845370",
    "end": "852660"
  },
  {
    "text": "between an instance group which is supporting spot instances and an instance group that does not support",
    "start": "852660",
    "end": "858390"
  },
  {
    "text": "spart instance is a fixed instance pool is the max price what you can see there",
    "start": "858390",
    "end": "865410"
  },
  {
    "text": "so as soon as you add this max price key the instance pool that is defined by the",
    "start": "865410",
    "end": "872670"
  },
  {
    "text": "samel becomes a spot instance pool with",
    "start": "872670",
    "end": "878310"
  },
  {
    "text": "K ops you can add new instance groups you can also edit instance group so one",
    "start": "878310",
    "end": "885450"
  },
  {
    "text": "difference between google gke and k AWS is DKE if you create a node pool with",
    "start": "885450",
    "end": "892290"
  },
  {
    "text": "preemptable nodes you cannot change it you need to create a new node pool with k ops with AWS you can switch a instance",
    "start": "892290",
    "end": "902160"
  },
  {
    "text": "group from preemptable to fixed or fixed to preemptable and so on so let's",
    "start": "902160",
    "end": "907650"
  },
  {
    "text": "quickly take a look at creating a cluster using K ops and sparked",
    "start": "907650",
    "end": "912840"
  },
  {
    "text": "instances",
    "start": "912840",
    "end": "915380"
  },
  {
    "text": "so what you pretty much do is first you",
    "start": "918830",
    "end": "924810"
  },
  {
    "text": "run AWS configure onto the node and specify the access key secret key the",
    "start": "924810",
    "end": "930690"
  },
  {
    "text": "region name and the format that you want all your commands to output in once",
    "start": "930690",
    "end": "936030"
  },
  {
    "text": "that's done let's you can run kayoppe commands so we just run @k ops get cluster I have already a cluster",
    "start": "936030",
    "end": "943530"
  },
  {
    "text": "deployed because caustic Department takes time we look at that in a bit how",
    "start": "943530",
    "end": "948540"
  },
  {
    "text": "do you create a cluster you run the k ops create cluster command give it a zone and a name it goes through a bunch",
    "start": "948540",
    "end": "955350"
  },
  {
    "text": "of tasks it assigns a CIDR for that cluster and then it creates a cluster a",
    "start": "955350",
    "end": "961440"
  },
  {
    "text": "cluster object with all of the resources locally so you can it's still not created on AWS you can go ahead and look",
    "start": "961440",
    "end": "968310"
  },
  {
    "text": "at it and manage and modify configuration so let's quickly take a",
    "start": "968310",
    "end": "973470"
  },
  {
    "text": "look at the instance groups that the cluster create went through so there is a master instance group and an",
    "start": "973470",
    "end": "979680"
  },
  {
    "text": "organizational group let's quickly change the node instance group to just have one node in it because we will",
    "start": "979680",
    "end": "985770"
  },
  {
    "text": "create a another pool for the sparked instances that's done let's now create a",
    "start": "985770",
    "end": "995100"
  },
  {
    "text": "new instance group using the create ID command and the only difference or to do",
    "start": "995100",
    "end": "1002060"
  },
  {
    "text": "this is to specify a max price let's say something like 0.08 that's pretty high",
    "start": "1002060",
    "end": "1009170"
  },
  {
    "text": "up there for the T to medium instance and that's done so let's now take a look",
    "start": "1009170",
    "end": "1015380"
  },
  {
    "text": "at the instance groups that we've created so there is a spot pool that we created there's a nodes that it created",
    "start": "1015380",
    "end": "1021080"
  },
  {
    "text": "and a master group we update the cluster with this change of configuration it's",
    "start": "1021080",
    "end": "1028040"
  },
  {
    "text": "still all of this is still happening locally so it's just running through a check to see if all of the configuration",
    "start": "1028040",
    "end": "1033890"
  },
  {
    "text": "that you have done is correct and as soon as you specify a - - yes - the update command it starts deploying",
    "start": "1033890",
    "end": "1040040"
  },
  {
    "text": "things into AWS so let's quickly look at the AWS console so this is the cluster",
    "start": "1040040",
    "end": "1050600"
  },
  {
    "text": "that ID earlier as it didn't take a bit and you should see new ones coming up here but",
    "start": "1050600",
    "end": "1056480"
  },
  {
    "text": "if you look at the spot pool for the launch configuration that that is responsible to bring up instances in AWS",
    "start": "1056480",
    "end": "1062090"
  },
  {
    "text": "it has a spot price set and the others don't so that's that's the main",
    "start": "1062090",
    "end": "1067190"
  },
  {
    "text": "difference of creating instances or groups in AWS which supports part pricing I think it should have created",
    "start": "1067190",
    "end": "1075860"
  },
  {
    "text": "the other ones there you go it created another one the point eight that we just ran so it takes about five to six",
    "start": "1075860",
    "end": "1082490"
  },
  {
    "text": "minutes to eight minutes and the cluster is up then you should be able to deploy apps on it will not wait for it we'll go",
    "start": "1082490",
    "end": "1089330"
  },
  {
    "text": "with another demo so this is on GK we",
    "start": "1089330",
    "end": "1094640"
  },
  {
    "text": "saw AWS let's take a look at GK this take this demo pretty much shows you how",
    "start": "1094640",
    "end": "1100340"
  },
  {
    "text": "I have multiple pools a fixed pool and a preemptable pool and I deploy app and I",
    "start": "1100340",
    "end": "1106400"
  },
  {
    "text": "create load on it so it runs a horizontal power order scaler it creates instant more instances or more nodes for",
    "start": "1106400",
    "end": "1113390"
  },
  {
    "text": "to support it but those nodes are created in the preemptable pool and not the fixed pool and then as soon as the",
    "start": "1113390",
    "end": "1119720"
  },
  {
    "text": "load goes down it again shrinks so there's a cluster that I have an GK initially I had three nodes a",
    "start": "1119720",
    "end": "1128320"
  },
  {
    "text": "fixed pool with just one ordinate and preemptable pretty much disabled I have",
    "start": "1128320",
    "end": "1134390"
  },
  {
    "text": "another pool with 300 enabled and it can be auto scaled up to three nodes and I",
    "start": "1134390",
    "end": "1142490"
  },
  {
    "text": "have another one that is just there so",
    "start": "1142490",
    "end": "1148190"
  },
  {
    "text": "let's quickly go and deploy a PHP apache server that that's going to serve our",
    "start": "1148190",
    "end": "1156790"
  },
  {
    "text": "traffic for us and we'll also create a",
    "start": "1156790",
    "end": "1163820"
  },
  {
    "text": "busy box or a normal instance where we can start generating traffic for it",
    "start": "1163820",
    "end": "1169750"
  },
  {
    "text": "before that I'm going to create an auto scaler group and I say that if the load",
    "start": "1169750",
    "end": "1174980"
  },
  {
    "text": "goes above 75% the horizontal power auto scaling should trigger and it can",
    "start": "1174980",
    "end": "1180020"
  },
  {
    "text": "trigger to a maximum replicas of eight so that load is now running if you look the bottom right you can see that the",
    "start": "1180020",
    "end": "1186490"
  },
  {
    "text": "load now is running and the number of parts that are running for this is just",
    "start": "1186490",
    "end": "1192100"
  },
  {
    "text": "one but the load increase to 223 so the horizontal power order scaler created multiple instances but you can see that",
    "start": "1192100",
    "end": "1199090"
  },
  {
    "text": "one of the part is now in pending State so that part is in pending state because it's it has already ran out of resources",
    "start": "1199090",
    "end": "1206220"
  },
  {
    "text": "the cube cut will describe pod logs is going to show us that it it didn't",
    "start": "1206220",
    "end": "1212170"
  },
  {
    "text": "happen because of insufficient CPU the DKE will now automatically trigger a",
    "start": "1212170",
    "end": "1219030"
  },
  {
    "text": "node order scalar and it creates a new node that from the preemptable pool and",
    "start": "1219030",
    "end": "1224350"
  },
  {
    "text": "it's it's getting authorization ares cluster so we had one earlier and it is two now as soon as it gets ready you can",
    "start": "1224350",
    "end": "1232780"
  },
  {
    "text": "you will see that the parts now start creating again and the Lord should slowly reduce on our app the load is",
    "start": "1232780",
    "end": "1243250"
  },
  {
    "text": "still there so it's going to try more creating create more instances till it reaches eight replicas and since we had",
    "start": "1243250",
    "end": "1252580"
  },
  {
    "text": "an order scalar on the node pool set to maximum of three nodes now we have three preemptable nodes there and everything",
    "start": "1252580",
    "end": "1260080"
  },
  {
    "text": "is running the if you look at it the target now is about 63 percent or the the load on the app is 63 percent so",
    "start": "1260080",
    "end": "1267310"
  },
  {
    "text": "it's fine let's quit the load I just quit the load there and killed my load",
    "start": "1267310",
    "end": "1273370"
  },
  {
    "text": "generator you can see that the deployment the autoscaler deployment target will now fall to zero quickly and",
    "start": "1273370",
    "end": "1280510"
  },
  {
    "text": "as soon as it falls to zero you can see that all of the app apps pods are being",
    "start": "1280510",
    "end": "1286660"
  },
  {
    "text": "terminated and once the app pods are gone the nodes from the node pool also",
    "start": "1286660",
    "end": "1292060"
  },
  {
    "text": "we will get cleaned up by gke so you can see that one went away to and away and",
    "start": "1292060",
    "end": "1297850"
  },
  {
    "text": "now we are again back to three nodes in our task in our original faster so we",
    "start": "1297850",
    "end": "1302890"
  },
  {
    "text": "expanded the cluster for our app during bursted load and we brought them back",
    "start": "1302890",
    "end": "1309280"
  },
  {
    "text": "down it takes about five to six minutes for kuben gke to clean up nodes that are",
    "start": "1309280",
    "end": "1315040"
  },
  {
    "text": "not used so that's the reason for the now we saw how you design a cluster - to",
    "start": "1315040",
    "end": "1325110"
  },
  {
    "start": "1321000",
    "end": "1321000"
  },
  {
    "text": "use part instances or orders killing now we look at how you design an application to use part instances of preemptable",
    "start": "1325110",
    "end": "1331800"
  },
  {
    "text": "instance because you don't want the application to die completely and not have your service pretty much going down",
    "start": "1331800",
    "end": "1339330"
  },
  {
    "text": "right so some of the application considerations that are required are easier application stateless or is your",
    "start": "1339330",
    "end": "1345390"
  },
  {
    "text": "application stateful if your application is stateless it is better - it is a very",
    "start": "1345390",
    "end": "1351750"
  },
  {
    "text": "good match for such a use case what about the application replica or distribution of your application so if",
    "start": "1351750",
    "end": "1358200"
  },
  {
    "text": "you have six parts that is print backing your application and all those six parts",
    "start": "1358200",
    "end": "1364410"
  },
  {
    "text": "running on the same preemptable pool may not be a good idea definitely not a good idea because it goes down what happens",
    "start": "1364410",
    "end": "1372000"
  },
  {
    "text": "when a node fails communities automatically reschedule those parts to",
    "start": "1372000",
    "end": "1377070"
  },
  {
    "text": "different nodes or instances and if kubernetes reschedules them to the preemptable pool again again you have a",
    "start": "1377070",
    "end": "1383940"
  },
  {
    "text": "problem that all of the nodes might end up in the preemptable pool which is also a problem so there are also cases where",
    "start": "1383940",
    "end": "1390900"
  },
  {
    "text": "applications require specific GPU processing for example machine learning or big data analysis so there again",
    "start": "1390900",
    "end": "1397620"
  },
  {
    "text": "preemptable pools might might be present in your cluster but you might not want your apps to go and sit in the",
    "start": "1397620",
    "end": "1403350"
  },
  {
    "text": "preemptable pool so to do this kubernetes kind of has a bunch of mechanisms I'm sure most of you would",
    "start": "1403350",
    "end": "1410310"
  },
  {
    "start": "1407000",
    "end": "1407000"
  },
  {
    "text": "have used it if not I think you should use it one of it is the node selector so",
    "start": "1410310",
    "end": "1416610"
  },
  {
    "text": "nodes come with labels or you can apply labels to nodes and you could use that",
    "start": "1416610",
    "end": "1422760"
  },
  {
    "text": "as a node selected when you deploy our application to say if your parts go to",
    "start": "1422760",
    "end": "1428190"
  },
  {
    "text": "that particular node or not now let's say for example you need to deploy an application on this cluster with",
    "start": "1428190",
    "end": "1436050"
  },
  {
    "text": "preemptable instances and fixed instances so one way of deploying this or one example is I have to deployment",
    "start": "1436050",
    "end": "1444360"
  },
  {
    "text": "specs of my same app just an engine X server I deploy one on the preemptive",
    "start": "1444360",
    "end": "1451200"
  },
  {
    "text": "Poole by specifying the DKE preemptable true and I deploy the other the other",
    "start": "1451200",
    "end": "1458669"
  },
  {
    "text": "part of my apps are it's the same app but another deployment on to the fixed pool using the node selector using the",
    "start": "1458669",
    "end": "1466500"
  },
  {
    "text": "name of the node pool so gke when you create node pools it adlai it adds labels to those node pools and",
    "start": "1466500",
    "end": "1472289"
  },
  {
    "text": "for instances that are preemptable they come with a label inbuilt and that's the",
    "start": "1472289",
    "end": "1478200"
  },
  {
    "text": "GK - preemptable equal to true label so",
    "start": "1478200",
    "end": "1484139"
  },
  {
    "text": "what are the supporting mechanisms here so let's look at another example right",
    "start": "1484139",
    "end": "1489899"
  },
  {
    "text": "the H a app that we were talking about so let's say we have to know - note pulls we have a bunch of apps running on",
    "start": "1489899",
    "end": "1496889"
  },
  {
    "text": "them we want to deploy this H a app and we have one copy or one deployment",
    "start": "1496889",
    "end": "1502320"
  },
  {
    "text": "already deployed to the fixed pool and now we want to deploy the deploy the the",
    "start": "1502320",
    "end": "1508500"
  },
  {
    "text": "other one passive one strand by one to the preemptable node pool so the way you can do that is by using a a node",
    "start": "1508500",
    "end": "1516630"
  },
  {
    "text": "affinity and the type of node affinity that you would use is something called the required during scheduling ignored",
    "start": "1516630",
    "end": "1523620"
  },
  {
    "text": "during exception basically what it says is deploy this app only to the",
    "start": "1523620",
    "end": "1528690"
  },
  {
    "text": "preemptable node pool and not the fixed node pool and YC versa so if there's another app that I would like to run on",
    "start": "1528690",
    "end": "1535590"
  },
  {
    "text": "my preemptable node pool but i'm okay if my preemptable node pool is full I would",
    "start": "1535590",
    "end": "1541080"
  },
  {
    "text": "use the preferred during scheduling ignoring during exception type for the",
    "start": "1541080",
    "end": "1546929"
  },
  {
    "text": "node affinity if you want to make sure",
    "start": "1546929",
    "end": "1552389"
  },
  {
    "text": "that your apps are on different AZ's for high availability there is another label that you clear to use which is the",
    "start": "1552389",
    "end": "1558360"
  },
  {
    "text": "failure domain label with the slash zone that tells you which zone your node is",
    "start": "1558360",
    "end": "1564269"
  },
  {
    "text": "running on so you could have affinity or anti affinity for your app for that particular zone let's quickly take a",
    "start": "1564269",
    "end": "1573720"
  },
  {
    "text": "look at the node selector or the application availability it's basically",
    "start": "1573720",
    "end": "1579899"
  },
  {
    "text": "how you deploy an app to these clusters",
    "start": "1579899",
    "end": "1584929"
  },
  {
    "text": "so what I do is I have my application like the way you saw it I have to",
    "start": "1587300",
    "end": "1594840"
  },
  {
    "text": "deployment specs the fixed deployment spec and the preemptable deployment flex it's just going to deploy the nginx",
    "start": "1594840",
    "end": "1601710"
  },
  {
    "text": "server and the main difference that each",
    "start": "1601710",
    "end": "1608460"
  },
  {
    "text": "of this pack have is that one is node selector to the fixed pool and the",
    "start": "1608460",
    "end": "1613500"
  },
  {
    "text": "preemptable one is not selected to the printable pool I just have an init",
    "start": "1613500",
    "end": "1620280"
  },
  {
    "text": "container there which adds this echo statement to the index.html which tells",
    "start": "1620280",
    "end": "1625590"
  },
  {
    "text": "us which part is going to be replying that's for us to figure out in the demo there is a service that's going to Frank",
    "start": "1625590",
    "end": "1632460"
  },
  {
    "text": "both of this so that is one service but two deployments there are two nginx",
    "start": "1632460",
    "end": "1637710"
  },
  {
    "text": "deployments one on preemptable one on fixed but that is just one service that friends all of this so that's how you",
    "start": "1637710",
    "end": "1644490"
  },
  {
    "text": "can make use of the service but not worry about where it's running I also",
    "start": "1644490",
    "end": "1649650"
  },
  {
    "text": "have a small busy box that generates load from where I can generate load or",
    "start": "1649650",
    "end": "1654690"
  },
  {
    "text": "curl so let's quickly run into deploy this so",
    "start": "1654690",
    "end": "1661520"
  },
  {
    "text": "okay there is nothing so let's deploy them all of them",
    "start": "1676000",
    "end": "1682559"
  },
  {
    "text": "typing so all of them are running so I have",
    "start": "1694669",
    "end": "1700680"
  },
  {
    "text": "nginx one nginx server running on the fixed pool I have two of the other",
    "start": "1700680",
    "end": "1707250"
  },
  {
    "text": "instances running on two different printable pools let's look at the",
    "start": "1707250",
    "end": "1712530"
  },
  {
    "text": "service and let's go in and start",
    "start": "1712530",
    "end": "1717860"
  },
  {
    "text": "running some load on it",
    "start": "1717860",
    "end": "1721640"
  },
  {
    "text": "not there here",
    "start": "1732890",
    "end": "1736659"
  },
  {
    "text": "okay and let's do a while true let's do",
    "start": "1738510",
    "end": "1745140"
  },
  {
    "text": "a double you get and the closer I be I",
    "start": "1745140",
    "end": "1753650"
  },
  {
    "text": "won VPN so 1055 248 133 we always need a",
    "start": "1755840",
    "end": "1767250"
  },
  {
    "text": "sleep there oh yeah",
    "start": "1767250",
    "end": "1772500"
  },
  {
    "text": "mr. Lou okay",
    "start": "1772500",
    "end": "1780060"
  },
  {
    "text": "oh that was a quite not a P okay so we",
    "start": "1782140",
    "end": "1791230"
  },
  {
    "text": "are getting our responses from both parts running on preemptable and fixed",
    "start": "1791230",
    "end": "1797650"
  },
  {
    "text": "let's go and terminate our preemptable pool let's let's assume that the",
    "start": "1797650",
    "end": "1804010"
  },
  {
    "text": "preemptable pool kind of dies",
    "start": "1804010",
    "end": "1807330"
  },
  {
    "text": "into that come on okay let's go to the",
    "start": "1819590",
    "end": "1830090"
  },
  {
    "text": "GCE and let's terminate the flexible",
    "start": "1830090",
    "end": "1837539"
  },
  {
    "text": "instances",
    "start": "1837539",
    "end": "1840080"
  },
  {
    "text": "so you can see that the it takes a bunch of time for the service to reconfigure",
    "start": "1848460",
    "end": "1855059"
  },
  {
    "text": "for parts going down but then you have the part on the fixed instance running",
    "start": "1855059",
    "end": "1861659"
  },
  {
    "text": "and responding and I think live demo but",
    "start": "1861659",
    "end": "1869970"
  },
  {
    "text": "yeah so as soon as the Prima table instances come back up we will start getting responses from the preemptable",
    "start": "1869970",
    "end": "1876600"
  },
  {
    "text": "pool as well I think I might have deployed the curler on the preemptable",
    "start": "1876600",
    "end": "1881879"
  },
  {
    "text": "pool as well that might be the reason I might have lost this that's fine so",
    "start": "1881879",
    "end": "1887850"
  },
  {
    "text": "that's that's the demo basically you use note selector and affinity note affinity",
    "start": "1887850",
    "end": "1893399"
  },
  {
    "text": "to deploy apps in the right place we'll quickly go back I had a interesting experiment that we",
    "start": "1893399",
    "end": "1901470"
  },
  {
    "start": "1900000",
    "end": "1900000"
  },
  {
    "text": "ran just had a two node cluster one basically a split 50-50 percent one on",
    "start": "1901470",
    "end": "1907470"
  },
  {
    "text": "preemptive and one on fixed ran on gke for 12 days no act overloads and one in",
    "start": "1907470",
    "end": "1915320"
  },
  {
    "text": "observation that we saw was the preemptable node goes away somewhere",
    "start": "1915320",
    "end": "1921029"
  },
  {
    "text": "around 24 hours in the median time almost sometimes it's a little more than",
    "start": "1921029",
    "end": "1926039"
  },
  {
    "text": "24 hours otherwise it's normally around 24 hours and the most important thing",
    "start": "1926039",
    "end": "1931619"
  },
  {
    "text": "was the reason why you're here is the costs so if you look at the total cost or the bill that I got for the 12 days",
    "start": "1931619",
    "end": "1938460"
  },
  {
    "text": "for the two instances is $16 and if you replace this preemptable custom in",
    "start": "1938460",
    "end": "1944220"
  },
  {
    "text": "instance score with a standard instance the total would be around $24 so cost",
    "start": "1944220",
    "end": "1952139"
  },
  {
    "text": "saving is about $8 for 12 days just for two nodes and no workloads so if you",
    "start": "1952139",
    "end": "1957169"
  },
  {
    "text": "just do a mental experiment and scale it up the cost saving somewhat looks like",
    "start": "1957169",
    "end": "1962309"
  },
  {
    "text": "around $13,000 for a year for 50 nodes on fixed and 15 nodes on preemptable so",
    "start": "1962309",
    "end": "1969659"
  },
  {
    "text": "that that's the interesting figure that we are seeing and it will definitely increase as you increase the number of",
    "start": "1969659",
    "end": "1975419"
  },
  {
    "text": "nodes or reduce your fixed fixed node prices",
    "start": "1975419",
    "end": "1980500"
  },
  {
    "text": "that's about it that I had so hopefully one key takeaway that I think you should",
    "start": "1980500",
    "end": "1986630"
  },
  {
    "text": "you could take away from this is using spot instances is beneficial it would",
    "start": "1986630",
    "end": "1991880"
  },
  {
    "text": "reduce your costs but there are two things that you need to take care of one is to architect the cluster and the",
    "start": "1991880",
    "end": "1998060"
  },
  {
    "text": "second is to architect your app and doing this both of this a good in a good way can get you quite a bit of savings in",
    "start": "1998060",
    "end": "2005290"
  },
  {
    "text": "terms of bill and communities is the best for this approach and that was it",
    "start": "2005290",
    "end": "2012280"
  },
  {
    "text": "for me we can take maybe one or two",
    "start": "2012280",
    "end": "2022030"
  },
  {
    "text": "questions it's a great presentation",
    "start": "2022030",
    "end": "2029830"
  },
  {
    "text": "thank you definitely in introduction into it I agree with you I think kubernetes it",
    "start": "2029830",
    "end": "2035140"
  },
  {
    "text": "really gives us a chance to take care of these spot and interrupt abou instances it gets really complicated on AWS and",
    "start": "2035140",
    "end": "2042390"
  },
  {
    "text": "things keep changing so like they just recently changed their pricing structure",
    "start": "2042390",
    "end": "2047560"
  },
  {
    "text": "they even went from our two per second and every time we have a strategy put together it seems like it changes",
    "start": "2047560",
    "end": "2053740"
  },
  {
    "text": "dynamically right so that's actually what motivated me to try to encapsulate this into a service so that people that",
    "start": "2053740",
    "end": "2060158"
  },
  {
    "text": "don't have a team of 12 people to go off and do that can still leverage and get benefits from these but the key points",
    "start": "2060159",
    "end": "2066908"
  },
  {
    "text": "is to make sure your application can survive and one other point I think you missed is that even though pods recover",
    "start": "2066909",
    "end": "2073658"
  },
  {
    "text": "you still don't want them going down a lot so at least for the spot instances you can actually detect that and tell",
    "start": "2073659",
    "end": "2081310"
  },
  {
    "text": "them to start draining and try to move things off there so that's another important piece yes and the last one is",
    "start": "2081310",
    "end": "2086590"
  },
  {
    "text": "diversify across multiple instance types just like a stock plan yeah but",
    "start": "2086590",
    "end": "2092220"
  },
  {
    "text": "interesting thanks for the comments",
    "start": "2092220",
    "end": "2098429"
  },
  {
    "text": "so I saw you used cops which is cool I'm not using cops so are there any other",
    "start": "2102120",
    "end": "2108300"
  },
  {
    "text": "tools to use spotter preemptable instances with like a service I can run",
    "start": "2108300",
    "end": "2115620"
  },
  {
    "text": "on kubernetes to monitor that kind of stuff or I don't know good question so cops is pretty popular there so I use",
    "start": "2115620",
    "end": "2123660"
  },
  {
    "text": "that and it had sparked instant support platform 9 reduce for instances I would",
    "start": "2123660",
    "end": "2131670"
  },
  {
    "text": "want to say cube ATM desert but I'm not sure that that is yet if anybody with cube adium knowledge",
    "start": "2131670",
    "end": "2138480"
  },
  {
    "text": "could talk about it but yeah cops is one I know that there's Spartans okay that's",
    "start": "2138480",
    "end": "2146160"
  },
  {
    "text": "all we have time for feel free to talk to us on the side thanks",
    "start": "2146160",
    "end": "2152690"
  },
  {
    "text": "[Applause]",
    "start": "2153140",
    "end": "2156059"
  }
]