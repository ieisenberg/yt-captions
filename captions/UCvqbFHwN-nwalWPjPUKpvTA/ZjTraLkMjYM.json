[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "text": "all right everyone let's get this show on the road",
    "start": "2090",
    "end": "7890"
  },
  {
    "text": "so my name's Tom I'm Alex and this is the story of how we moved hundreds of my",
    "start": "7890",
    "end": "13349"
  },
  {
    "text": "sequel databases into kubernetes so what is how spot we're marketing sales and",
    "start": "13349",
    "end": "19650"
  },
  {
    "start": "16000",
    "end": "16000"
  },
  {
    "text": "services software platform based in Cambridge Massachusetts the HubSpot app is actually thousands of different micro",
    "start": "19650",
    "end": "26430"
  },
  {
    "text": "services built and deployed hundreds of times a day by many small autonomous teams our computing footprint is about",
    "start": "26430",
    "end": "34140"
  },
  {
    "text": "3,000 v vm s in both AWS and g CP and before you get to the good stuff i want",
    "start": "34140",
    "end": "41129"
  },
  {
    "text": "to turn the clock back to 2013 for a minute well is a developer at HubSpot it's kind of hard we had a clunky CI pipeline as",
    "start": "41129",
    "end": "48120"
  },
  {
    "text": "basically just one Jenkins's instance that we affectionately referred to as big J it was constantly falling over we",
    "start": "48120",
    "end": "54690"
  },
  {
    "text": "had a flaky ssh based deployment system with no sophistication we have no elasticity and no automation in our",
    "start": "54690",
    "end": "61350"
  },
  {
    "text": "infrastructure which meant it was not uncommon to be woken up at 3:00 a.m. in the morning because someone's nightly job was taking down your web service or",
    "start": "61350",
    "end": "68790"
  },
  {
    "text": "one of the ec2 instances that powered your web app went impaired and you had to do something to replace it we've been",
    "start": "68790",
    "end": "76560"
  },
  {
    "text": "looking at Apache mezzos at the time and we seriously considered evaluating it in the fall of 2013",
    "start": "76560",
    "end": "83060"
  },
  {
    "text": "the thing about mesas is different from kubernetes is that it only provides the",
    "start": "83060",
    "end": "89070"
  },
  {
    "text": "primitives for running your apps on a cluster of resources don't worry I'm gonna get to careen Eddy soon this is I",
    "start": "89070",
    "end": "94740"
  },
  {
    "text": "didn't think I'm out of maces con so we surveyed the landscape there are a bunch of different things out there for",
    "start": "94740",
    "end": "101310"
  },
  {
    "text": "running tasks on meso s-- but we found that they were very generic it wouldn't provide a great developer experience for",
    "start": "101310",
    "end": "108000"
  },
  {
    "text": "people working on the HubSpot product so we did something crazy and we made our own meso framework called singularity",
    "start": "108000",
    "end": "113880"
  },
  {
    "text": "it's still running to this day and it's an opinionated look at running Web",
    "start": "113880",
    "end": "120420"
  },
  {
    "text": "Services workers and cron jobs and meso s-- tailored to the way our developers",
    "start": "120420",
    "end": "125670"
  },
  {
    "text": "work as far as I know at this time kubernetes is just the twinkle in the eye of a few Googlers",
    "start": "125670",
    "end": "131740"
  },
  {
    "text": "and it ended up being a great success we were able to save money by consolidating instances we were able to improve",
    "start": "131740",
    "end": "138550"
  },
  {
    "text": "reliability by automating away all those 3:00 a.m. wake-up calls and we were able to improve productivity by making it",
    "start": "138550",
    "end": "144610"
  },
  {
    "text": "easy for developers to do things that mattered most like tailing logs are taking heap dumps or whatever and as you",
    "start": "144610",
    "end": "151480"
  },
  {
    "text": "can see by a younger less stickered Tom we bragged about it a lot so time goes",
    "start": "151480",
    "end": "160030"
  },
  {
    "text": "on being a product developer is now not so bad you have singularity but what",
    "start": "160030",
    "end": "165520"
  },
  {
    "text": "about the infrastructure the infrastructure at HubSpot was still kind of in that twenty thirteen world of you",
    "start": "165520",
    "end": "172030"
  },
  {
    "text": "know lack of automation life was hard we could have taken the route of adding",
    "start": "172030",
    "end": "178990"
  },
  {
    "text": "stateful support to singularity but we really liked how singularity just did one thing well and we didn't want have",
    "start": "178990",
    "end": "185140"
  },
  {
    "text": "to reinvent a bunch of different wheels so at that time we kicked the tires on kubernetes we liked what we saw and we",
    "start": "185140",
    "end": "192100"
  },
  {
    "text": "decided that we would explicitly try to do our state fall workloads on",
    "start": "192100",
    "end": "197170"
  },
  {
    "text": "kubernetes and worry about consolidating singularity and qumran aid kubernetes down the road so all of our",
    "start": "197170",
    "end": "205600"
  },
  {
    "text": "infrastructure teams data infrastructure was the first to take the plunge and the hottest fire in that world was my sequel",
    "start": "205600",
    "end": "212170"
  },
  {
    "text": "and we got Alex from the sequel team here to tell more yeah so in 2016 if you",
    "start": "212170",
    "end": "219580"
  },
  {
    "text": "were a product team at HubSpot and you were spinning up a new micro service and you had some data you had this decision",
    "start": "219580",
    "end": "225700"
  },
  {
    "text": "tree to look at if your data was streaming or you had to deal with user",
    "start": "225700",
    "end": "231040"
  },
  {
    "text": "input queries you used either Kafka or elasticsearch but otherwise you were",
    "start": "231040",
    "end": "237010"
  },
  {
    "text": "looking at my sequel or HBase and probably just because it was more familiar to you used my sequel and with",
    "start": "237010",
    "end": "244810"
  },
  {
    "text": "so many small autonomous teams building micro services that means we ran a lot of my sequel and just to put numbers on",
    "start": "244810",
    "end": "251890"
  },
  {
    "text": "it I'm talking about roughly 450 my sequel clusters back in 2016 and a",
    "start": "251890",
    "end": "259359"
  },
  {
    "text": "cluster here means generally two machines possibly",
    "start": "259359",
    "end": "265120"
  },
  {
    "text": "or but like a master and some number of slaves and the 450 clusters are 225 in",
    "start": "265120",
    "end": "271120"
  },
  {
    "text": "QA and 225 in prod so if you do the math there we're talking about like at least",
    "start": "271120",
    "end": "277300"
  },
  {
    "text": "900 ec2 instances but actually in reality closer to a thousand and that's",
    "start": "277300",
    "end": "285160"
  },
  {
    "text": "painful right that's expensive and operationally it doesn't scale right",
    "start": "285160",
    "end": "290830"
  },
  {
    "start": "288000",
    "end": "288000"
  },
  {
    "text": "with more services coming up all the time we just have a worse and worse",
    "start": "290830",
    "end": "295930"
  },
  {
    "text": "problem because Amazon has a non zero impairment rate which means somebody on",
    "start": "295930",
    "end": "300940"
  },
  {
    "text": "my team gets paid and you put hands on keyboard run an ansible play or two or",
    "start": "300940",
    "end": "306010"
  },
  {
    "text": "three of them so that sucks and like I said it's getting worse all the time because we're always spinning up new",
    "start": "306010",
    "end": "312220"
  },
  {
    "text": "micro services but even just to do that right your team your product team files",
    "start": "312220",
    "end": "319479"
  },
  {
    "text": "a JIRA and then somebody on my team has to provision some virtual machines with",
    "start": "319479",
    "end": "324729"
  },
  {
    "text": "a puppet class and set some puppet variables and all the while your product",
    "start": "324729",
    "end": "329740"
  },
  {
    "text": "team is just waiting and waiting and waiting the other axis it wasn't scaling on was the actual database itself the",
    "start": "329740",
    "end": "337180"
  },
  {
    "text": "query and data load you know with no real idea how big your service is gonna",
    "start": "337180",
    "end": "343090"
  },
  {
    "text": "become inside the HubSpot app we would generally start you off with a smaller set of ec2 machines and we could again",
    "start": "343090",
    "end": "351280"
  },
  {
    "text": "by some ansible plays sized you up to a larger instant size and size you up again but at some point you know ec2",
    "start": "351280",
    "end": "360070"
  },
  {
    "text": "instance sizes are limited right it doesn't keep on going and you would have to kind of graduate up to HBase which is",
    "start": "360070",
    "end": "367090"
  },
  {
    "text": "terrible right you're putting your relational data into HBase and you have",
    "start": "367090",
    "end": "372490"
  },
  {
    "text": "to like rewrite the whole data layer in your app the other thing you could do is build sharding into your application",
    "start": "372490",
    "end": "379000"
  },
  {
    "text": "which is itself a nightmare and really just a nightmare that you would ever just want to do once not a thousand",
    "start": "379000",
    "end": "386080"
  },
  {
    "text": "times right we have like 1500 micro services so yeah that's also pain yes so",
    "start": "386080",
    "end": "392710"
  },
  {
    "text": "we needed something that would meet us on the operational front and the data",
    "start": "392710",
    "end": "398140"
  },
  {
    "text": "scalability and we looked around so it's end of 2016 at that point maybe like September",
    "start": "398140",
    "end": "403750"
  },
  {
    "text": "October November and this is what we saw right so here's situs and cockroach and RDS and Valera and by Tess that's what",
    "start": "403750",
    "end": "411790"
  },
  {
    "text": "we saw but we don't really want to use like situs or cockroach because we want",
    "start": "411790",
    "end": "418270"
  },
  {
    "text": "to stick with my sequel right it's what we know it's what we have all the operational experience with and we",
    "start": "418270",
    "end": "424180"
  },
  {
    "text": "weren't really comfortable with handing our infrastructure off to a third party like AWS and pay through the nose and",
    "start": "424180",
    "end": "432430"
  },
  {
    "text": "deal with more or less Glacial support turn around and it doesn't actually",
    "start": "432430",
    "end": "438220"
  },
  {
    "text": "scale horizontally right it doesn't shard so that's kind of a non-starter not a not a good fit Rob spot and we",
    "start": "438220",
    "end": "446110"
  },
  {
    "text": "knew like Tom was saying that the rest of our infrastructure will be moving in",
    "start": "446110",
    "end": "451720"
  },
  {
    "text": "the kubernetes direction so we wanted something that would at least rhyme with kubernetes and so Galera extra DB",
    "start": "451720",
    "end": "459310"
  },
  {
    "text": "cluster or whatever that's out and so we landed on by tests and we really liked what we saw its my sequel base it's open",
    "start": "459310",
    "end": "467590"
  },
  {
    "text": "source it's cloud native and we really liked that it wasn't new at that time",
    "start": "467590",
    "end": "472810"
  },
  {
    "text": "back in 2006 it had been deployed in production at YouTube for several years",
    "start": "472810",
    "end": "478440"
  },
  {
    "text": "so what's my test hopefully you got to go to one of the talks earlier this week",
    "start": "478440",
    "end": "485290"
  },
  {
    "start": "480000",
    "end": "480000"
  },
  {
    "text": "we've had a few really great talks but even if you did I want to highlight a few parts of the architecture they're",
    "start": "485290",
    "end": "491910"
  },
  {
    "text": "over on the right you see it's labeled admin it's the control plane there's a",
    "start": "491910",
    "end": "498160"
  },
  {
    "text": "bunch of boxes there but the thing it doesn't say is that this vtc TLD that's the vy test control daemon actually has",
    "start": "498160",
    "end": "504730"
  },
  {
    "text": "an API that you can code to so that's awesome the data plane however is",
    "start": "504730",
    "end": "511320"
  },
  {
    "text": "consists of two parts the VT gate which is a proxy there and VT tablets which",
    "start": "511320",
    "end": "517000"
  },
  {
    "text": "are like sidecars that run next to your my sequel process yeah so that's a lot",
    "start": "517000",
    "end": "522700"
  },
  {
    "text": "of boxes right how do you set it all up well it should run in kubernetes right",
    "start": "522700",
    "end": "529360"
  },
  {
    "text": "well not really it was like designed to run in borg should run kubernetes so we just got started and we got started like anybody",
    "start": "529360",
    "end": "536970"
  },
  {
    "text": "would write with helm but it got a little unwieldy right that's a lot of",
    "start": "536970",
    "end": "542670"
  },
  {
    "text": "different helm charts and it didn't really give us the control that we wanted right I never ever want to roll",
    "start": "542670",
    "end": "550170"
  },
  {
    "text": "out a release and have all five hundred databases see the numbers gonna go up as the time",
    "start": "550170",
    "end": "555420"
  },
  {
    "text": "goes up all five hundred databases get updated right then that's like right I need Canaries I need",
    "start": "555420",
    "end": "562170"
  },
  {
    "text": "all that kind of stuff so around let's say January maybe of 2017 somebody on my",
    "start": "562170",
    "end": "570600"
  },
  {
    "text": "team found this blog post from core OS introducing operators so that I guess was written November 2016 but we didn't",
    "start": "570600",
    "end": "577980"
  },
  {
    "text": "really have a need for it didn't see it until January or so and we fell in love",
    "start": "577980",
    "end": "583380"
  },
  {
    "text": "we scrapped helm immediately and started writing an operator and this you know",
    "start": "583380",
    "end": "588590"
  },
  {
    "text": "this is like well before operator SDK so we didn't have any of that so we're just",
    "start": "588590",
    "end": "593970"
  },
  {
    "text": "writing against client go so hopefully you've seen something about operators it",
    "start": "593970",
    "end": "601680"
  },
  {
    "start": "597000",
    "end": "597000"
  },
  {
    "text": "was even in one of the keynotes this morning but just to put a pin in it here it's an extension of the kubernetes api",
    "start": "601680",
    "end": "609030"
  },
  {
    "text": "with resources that you defined called custom resources and then your operator just translates those things into",
    "start": "609030",
    "end": "615990"
  },
  {
    "text": "objects that kubernetes knows about natively like stateful sets and pod disruption budgets and that was really",
    "start": "615990",
    "end": "622170"
  },
  {
    "text": "important for us to have a translation layer like made out of code that we wrote so that we could in fact like roll",
    "start": "622170",
    "end": "629550"
  },
  {
    "text": "out a change across all of our databases at once but do so in a controlled way so yeah I",
    "start": "629550",
    "end": "636750"
  },
  {
    "start": "636000",
    "end": "636000"
  },
  {
    "text": "want to dive in a little bit here we have a couple of custom resources but the main one that describes databases or",
    "start": "636750",
    "end": "644040"
  },
  {
    "text": "key spaces in the terminology of eye test is right there on the left and it's",
    "start": "644040",
    "end": "649560"
  },
  {
    "text": "this is abbreviated but you can imagine the kind of stuff that's in there you got a name you got a performance class that shows",
    "start": "649560",
    "end": "656570"
  },
  {
    "text": "like the size of resource limits and requests for your pod how many replicas",
    "start": "656570",
    "end": "662910"
  },
  {
    "text": "and that last one they're sensitive just to say like HubSpot s-- public so this one is does",
    "start": "662910",
    "end": "670860"
  },
  {
    "text": "the database contain data which should be considered sensitive for sarbanes-oxley compliance so it's a",
    "start": "670860",
    "end": "677520"
  },
  {
    "text": "mouthful and our operator translates that bit of yeah mole into three different things pod disruption budget",
    "start": "677520",
    "end": "683960"
  },
  {
    "text": "which tom will talk about a little bit later a cron job that we use for backups and a stateful set and yeah the meat of",
    "start": "683960",
    "end": "691680"
  },
  {
    "text": "that is the stateful set and I want to dig into that and the meat of the stateful set is the template for the",
    "start": "691680",
    "end": "699600"
  },
  {
    "start": "695000",
    "end": "695000"
  },
  {
    "text": "containers in a pot and you could see our pods are pretty thick we have an",
    "start": "699600",
    "end": "704910"
  },
  {
    "text": "init container down at the bottom I'll work bottom up it's called init VT root and all that does is like",
    "start": "704910",
    "end": "711390"
  },
  {
    "text": "initialize a fresh persistent volume right set up directories and pull data",
    "start": "711390",
    "end": "716460"
  },
  {
    "text": "in and then we've got like three housekeeping containers Crom fluency for",
    "start": "716460",
    "end": "722400"
  },
  {
    "text": "shipping out logs and collecti for shipping out metrics and then the two kind of heavyweight containers my sequel",
    "start": "722400",
    "end": "729330"
  },
  {
    "text": "and VT tablet and you see those both",
    "start": "729330",
    "end": "735570"
  },
  {
    "text": "have pre stop hook which again Tom will talk about a little bit more later and",
    "start": "735570",
    "end": "740820"
  },
  {
    "text": "the V T tablet pod the BT tablet container has a liveness probe and a readiness probe so I don't have like two",
    "start": "740820",
    "end": "750690"
  },
  {
    "text": "hours to talk to you about all the work my team did but I want to hit on some of the high points we do have another",
    "start": "750690",
    "end": "756150"
  },
  {
    "text": "custom resource that manages pools of Vth describes pools of ET gates we run",
    "start": "756150",
    "end": "763470"
  },
  {
    "text": "several different pools that you know pass different options into the underlying VT gates and we want to be",
    "start": "763470",
    "end": "769080"
  },
  {
    "text": "able to scale those things differently so that's that HubSpot looked at the options for JDBC",
    "start": "769080",
    "end": "778590"
  },
  {
    "start": "774000",
    "end": "774000"
  },
  {
    "text": "driver and while Vai test speaks the my sequel wire protocol we chose to",
    "start": "778590",
    "end": "784050"
  },
  {
    "text": "actually use the VAD BC driver which is really just a thin wrapper over G RPC",
    "start": "784050",
    "end": "790790"
  },
  {
    "text": "mostly because the rest of our inner infrastructure is going to go in the G RPC direction and if you remember all",
    "start": "790790",
    "end": "797490"
  },
  {
    "text": "the little boxes on the architecture diagram all those things talk to each other by a gr PC and so to secure it we had to make",
    "start": "797490",
    "end": "806060"
  },
  {
    "text": "a little infrastructure for mutual TLS and we yeah we built out one of those",
    "start": "806060",
    "end": "811209"
  },
  {
    "text": "backed by Hoshi corpse vault and it includes security all the way into the",
    "start": "811209",
    "end": "816740"
  },
  {
    "text": "micro service where we've got like short-lived TLS certificates another bit",
    "start": "816740",
    "end": "823940"
  },
  {
    "start": "823000",
    "end": "823000"
  },
  {
    "text": "of work we did was to integrate Orchestrator into our installation if you're not familiar with it",
    "start": "823940",
    "end": "829910"
  },
  {
    "text": "Orchestrator is the h a solution for my sequel out of github great piece of",
    "start": "829910",
    "end": "835519"
  },
  {
    "text": "software and finally as you might expect",
    "start": "835519",
    "end": "841820"
  },
  {
    "text": "if I test grew up at YouTube but hub spots corpus of queries is different",
    "start": "841820",
    "end": "848329"
  },
  {
    "text": "from the corpus of queries at YouTube so we actually had to augment the grammar of the sequel dialect of I test and we",
    "start": "848329",
    "end": "856250"
  },
  {
    "text": "did that in a couple of parts first step was to basically pull the parser out of I test into a thing called bt parse and",
    "start": "856250",
    "end": "863449"
  },
  {
    "text": "just shove all the general logs through it just to see what was going to fail and then once we had an idea about that",
    "start": "863449",
    "end": "869240"
  },
  {
    "text": "we could hit yak and actually like change things up so that was pretty",
    "start": "869240",
    "end": "874910"
  },
  {
    "text": "interesting the other thing I'd like to talk about is how we did the migration itself just",
    "start": "874910",
    "end": "881720"
  },
  {
    "start": "877000",
    "end": "877000"
  },
  {
    "text": "because it comes up all the time in the Vitis slack Channel it was really important for us when we were preparing",
    "start": "881720",
    "end": "888170"
  },
  {
    "text": "for it to do some work in our back-end framework library stuff like that so",
    "start": "888170",
    "end": "894649"
  },
  {
    "text": "that we could make transition one database at a time this I agree is pretty simple it's just",
    "start": "894649",
    "end": "902839"
  },
  {
    "text": "got one app - one database in general we've got like a end to em setup but just to make the diagram live with the",
    "start": "902839",
    "end": "911329"
  },
  {
    "text": "bull I'll just show this one to one thing and here so it's an app so that's a micro service which could actually be",
    "start": "911329",
    "end": "918170"
  },
  {
    "text": "like multiple instances of the same micro service and then that's two ec2",
    "start": "918170",
    "end": "924139"
  },
  {
    "text": "hosts on the bottom one replicating from the other so that's the initial State",
    "start": "924139",
    "end": "929890"
  },
  {
    "text": "the next step was to stand up a bt gate and tends to be in kubernetes but that's",
    "start": "929890",
    "end": "935360"
  },
  {
    "text": "neither here nor there for this and at this point we're not using it it's just running and then we start up the VT",
    "start": "935360",
    "end": "944360"
  },
  {
    "text": "tablet that's the sidecar right there on the ec2 host and then we create a key",
    "start": "944360",
    "end": "950240"
  },
  {
    "text": "space object corresponding to that database and then our operator in kubernetes starts up three pods for that",
    "start": "950240",
    "end": "956060"
  },
  {
    "text": "database replicating from the my sequels that are not running in kubernetes and",
    "start": "956060",
    "end": "962560"
  },
  {
    "text": "then we built a little flag that we could flip on and off for the app to",
    "start": "962560",
    "end": "969260"
  },
  {
    "text": "send its traffic through by test or not and once we had figured out that all the",
    "start": "969260",
    "end": "976160"
  },
  {
    "text": "bugs were figured out for this particular apps queries and NPS we could",
    "start": "976160",
    "end": "984290"
  },
  {
    "text": "proceed with what I don't know what you want to call it a switch over master slave swap by test calls at re-parent",
    "start": "984290",
    "end": "991670"
  },
  {
    "text": "where we made one of the my sequels running in kubernetes the master for the replication topology and then finally",
    "start": "991670",
    "end": "1001150"
  },
  {
    "text": "once we were satisfied everything was working properly we could shut down the ec2 hosts and start saving money it's",
    "start": "1001150",
    "end": "1007330"
  },
  {
    "text": "important to note this whole thing has zero downtime right at no point where we",
    "start": "1007330",
    "end": "1012850"
  },
  {
    "text": "read only at no point where we totally down the data was always there and the",
    "start": "1012850",
    "end": "1018730"
  },
  {
    "text": "my sequel so if we needed to switch back to an earlier phase and not use by test",
    "start": "1018730",
    "end": "1024160"
  },
  {
    "text": "for something it was all there it all just worked it's pretty important that we had that we smooth everything out",
    "start": "1024160",
    "end": "1030610"
  },
  {
    "text": "because we had to do this hundreds of times right like hundreds and hundreds of times but while my team was getting",
    "start": "1030610",
    "end": "1039220"
  },
  {
    "text": "by test to work for HubSpot Tom's team was getting kubernetes to work for my tests so here's Tom with some of the",
    "start": "1039220",
    "end": "1044980"
  },
  {
    "start": "1044000",
    "end": "1044000"
  },
  {
    "text": "issues that his team faced thanks Alex so one of the early traps that we fell",
    "start": "1044980",
    "end": "1050590"
  },
  {
    "text": "into was neglecting to account for how node maintenance could affect our",
    "start": "1050590",
    "end": "1055990"
  },
  {
    "text": "services we figured that if a pod could handle a deployment scale maybe a surprise termination we were golden and",
    "start": "1055990",
    "end": "1062230"
  },
  {
    "text": "then we learned the hard way about pod evictions when we replaced too many nodes at once",
    "start": "1062230",
    "end": "1067330"
  },
  {
    "text": "during a kubernetes upgrade and the scheduler had no concept of that three",
    "start": "1067330",
    "end": "1073419"
  },
  {
    "text": "you know all pods and a staple said being evicted is problematic so when Hanna did it and we experienced",
    "start": "1073419",
    "end": "1080470"
  },
  {
    "text": "downtime so the fix here was easy as Alex said earlier we updated the operator to ensure that every stateful",
    "start": "1080470",
    "end": "1086350"
  },
  {
    "text": "set had a corresponding Padma Sri option budget that allows you to define the",
    "start": "1086350",
    "end": "1091450"
  },
  {
    "text": "rate at which pods are evicted if there's one thing I tattoo on my arm",
    "start": "1091450",
    "end": "1096820"
  },
  {
    "text": "though it be that deletion is not eviction this bit us as recently as last",
    "start": "1096820",
    "end": "1101919"
  },
  {
    "text": "week when we ran a cube control delete pod and forgot that that hits the pod",
    "start": "1101919",
    "end": "1108909"
  },
  {
    "text": "deletion API not the pod of fiction API the pod deletion API is not subject to",
    "start": "1108909",
    "end": "1115029"
  },
  {
    "text": "pod disruption budgets so need a little surprise there if it can happen to us it",
    "start": "1115029",
    "end": "1120159"
  },
  {
    "text": "can happen to you kubernetes kubernetes makes it really easy to inject config into your pods",
    "start": "1120159",
    "end": "1126220"
  },
  {
    "start": "1121000",
    "end": "1121000"
  },
  {
    "text": "through secrets and config maps but we run seven different kubernetes clusters",
    "start": "1126220",
    "end": "1131470"
  },
  {
    "text": "across different environments in different regions so a given secret in a kubernetes cluster may not be the single",
    "start": "1131470",
    "end": "1138340"
  },
  {
    "text": "source of truth instead what we do is we store all of our secrets in one centralized vault cluster and then we",
    "start": "1138340",
    "end": "1144549"
  },
  {
    "text": "created an operator that will sync these secrets into kubernetes the secret",
    "start": "1144549",
    "end": "1149740"
  },
  {
    "text": "updater operator defines a custom resource that we call a secret template that lets you define a secret that references values involved",
    "start": "1149740",
    "end": "1157419"
  },
  {
    "text": "and it will make sure that the generated secret is always up-to-date with vault the other benefit here is that if we",
    "start": "1157419",
    "end": "1164289"
  },
  {
    "text": "experience it's any downtime of all it's no problem at all so because we're running things that",
    "start": "1164289",
    "end": "1172330"
  },
  {
    "start": "1168000",
    "end": "1168000"
  },
  {
    "text": "aren't necessarily kubernetes native like my sequel we have to write out a lot of configuration files in our pods",
    "start": "1172330",
    "end": "1179070"
  },
  {
    "text": "and one way you could do that is by having config maps but all of our my sequel pods are slightly different and",
    "start": "1179070",
    "end": "1185710"
  },
  {
    "text": "that would result in a dizzying number of config maps and we don't want to have to manage all that so instead we built a",
    "start": "1185710",
    "end": "1191409"
  },
  {
    "text": "tool that will take care of it for us HS render template is a Python script that we build into our base docker image",
    "start": "1191409",
    "end": "1198850"
  },
  {
    "text": "and all it does is it accepts a ginger template and any combination of downward",
    "start": "1198850",
    "end": "1204130"
  },
  {
    "text": "API values node and metadata the ammo files environment variables and writes",
    "start": "1204130",
    "end": "1209590"
  },
  {
    "text": "out the result to another file this is typically something that we do in the entry point of our docker images so as",
    "start": "1209590",
    "end": "1218380"
  },
  {
    "start": "1216000",
    "end": "1216000"
  },
  {
    "text": "you know staple services in kubernetes means that you have to pay close attention to pod lifecycle as Alex",
    "start": "1218380",
    "end": "1225880"
  },
  {
    "text": "mentioned earlier we used three stop hooks in order to terminate pods gracefully and we do it in two different",
    "start": "1225880",
    "end": "1231159"
  },
  {
    "text": "locations for the VT tablet container we have a pre stop hook that checks to see is this pot a master and if it is before",
    "start": "1231159",
    "end": "1238840"
  },
  {
    "text": "we shut that down do a plan dree parent or the Vitesse's term for a master slave swap to make it a replica",
    "start": "1238840",
    "end": "1246549"
  },
  {
    "text": "once the pau is a replica it's safe to shut down the tablet container what",
    "start": "1246549",
    "end": "1251860"
  },
  {
    "text": "because Tet the tablet is proxying these queries to the my sequel database we",
    "start": "1251860",
    "end": "1257830"
  },
  {
    "text": "also got to make sure that the my sequel container only terminates after the tablet container otherwise there could",
    "start": "1257830",
    "end": "1263830"
  },
  {
    "text": "be a different service so you have another pre stop hook that simply just polls to see is the tablet still running",
    "start": "1263830",
    "end": "1269350"
  },
  {
    "text": "if it's still running let's not shut down my sequel yet does this look familiar to anyone this is",
    "start": "1269350",
    "end": "1277269"
  },
  {
    "start": "1273000",
    "end": "1273000"
  },
  {
    "text": "what happens when you're an idiot and run out of resources in your cluster so",
    "start": "1277269",
    "end": "1283360"
  },
  {
    "text": "we hit this early on you know we would have people testing things and be like",
    "start": "1283360",
    "end": "1288370"
  },
  {
    "text": "hey you know my paw is pending like what does that mean and we'd have to dig through and we'd you know we'd assume",
    "start": "1288370",
    "end": "1293679"
  },
  {
    "text": "like something was broken and then finally just turned out no you know we didn't have enough nodes so outside of",
    "start": "1293679",
    "end": "1300460"
  },
  {
    "text": "kubernetes we have a lot of custom solutions to handle auto scaling for things like meso s-- and initially we",
    "start": "1300460",
    "end": "1306700"
  },
  {
    "text": "were going to do the same but then we found the cluster autoscaler project for kubernetes which does a much better job",
    "start": "1306700",
    "end": "1312159"
  },
  {
    "text": "than we could ever do because it bakes the kubernetes scheduler logic into it one other really cool outcome from this",
    "start": "1312159",
    "end": "1319419"
  },
  {
    "text": "is that we're able to do things like only spin up expensive GPU enabled nodes",
    "start": "1319419",
    "end": "1325179"
  },
  {
    "text": "when only when our our ml team is actually like training models or",
    "start": "1325179",
    "end": "1330309"
  },
  {
    "text": "something so we're able to give them what they need without wasting a lot of money so we format our",
    "start": "1330309",
    "end": "1338110"
  },
  {
    "start": "1334000",
    "end": "1334000"
  },
  {
    "text": "my sequel volumes with XFS instead of extension for for improved performance and kubernetes makes that really easy",
    "start": "1338110",
    "end": "1344410"
  },
  {
    "text": "with storage classes except the version that we were running when we first started did not support FS typed or",
    "start": "1344410",
    "end": "1351340"
  },
  {
    "text": "dynamically provisioned volumes this was later fixed in 1.8 but we had to do",
    "start": "1351340",
    "end": "1356500"
  },
  {
    "text": "something macgyvering to get around that in the meantime and the way we did that is we noticed that when cubit is",
    "start": "1356500",
    "end": "1363610"
  },
  {
    "text": "mounting a volume it's shelling out to make FS and mountain in order to set everything up so we injected a wrapper",
    "start": "1363610",
    "end": "1369820"
  },
  {
    "text": "onto the cubelet path that it's insane when you're mounting a volume it calls",
    "start": "1369820",
    "end": "1378520"
  },
  {
    "text": "out to AWS to see what are the tags on this EBS volume so that we can figure out the name of the PVC once we knew the",
    "start": "1378520",
    "end": "1385000"
  },
  {
    "text": "PVC we call that the kubernetes to figure out what the storage class was and once we knew the storage class we",
    "start": "1385000",
    "end": "1390490"
  },
  {
    "text": "knew what we actually needed the file system to be and we call the write scripts so this one's a mouthful we",
    "start": "1390490",
    "end": "1401440"
  },
  {
    "start": "1396000",
    "end": "1396000"
  },
  {
    "text": "needed native networking performance and the ability for services outside of",
    "start": "1401440",
    "end": "1406450"
  },
  {
    "text": "kubernetes to easily reach pods inside kubernetes so overlay networks like flannel were out instead we opted for",
    "start": "1406450",
    "end": "1413740"
  },
  {
    "text": "using the VP seas network space directly nowadays you can do that pretty easily",
    "start": "1413740",
    "end": "1420460"
  },
  {
    "text": "with eks or lifts IP VLAN plugin but those weren't options at the time what",
    "start": "1420460",
    "end": "1426520"
  },
  {
    "text": "we ended up doing was using the default docker bridge and some fancy routes and",
    "start": "1426520",
    "end": "1433870"
  },
  {
    "text": "some complicated IP tables rules to directly bridge pods to our ec2 network",
    "start": "1433870",
    "end": "1440980"
  },
  {
    "text": "this was really cool but it came out of price because we were beholden to the",
    "start": "1440980",
    "end": "1446020"
  },
  {
    "text": "what the default allocation strategy that docker uses for IP addresses we",
    "start": "1446020",
    "end": "1451810"
  },
  {
    "text": "were noticing that it was going lowest first which is fine except for when",
    "start": "1451810",
    "end": "1457270"
  },
  {
    "text": "you're rolling all the pods so we would see these issues where am i single pod",
    "start": "1457270",
    "end": "1462880"
  },
  {
    "text": "would be torn down a new one would come up and take its IP and there would be a period of time",
    "start": "1462880",
    "end": "1468160"
  },
  {
    "text": "where that wasn't fully propagated throughout the cluster so that you know apps would be trying to reach a a pod",
    "start": "1468160",
    "end": "1474760"
  },
  {
    "text": "thinking it's one thing but it really being the other it wasn't that bad for my sequel but it was truly bad for Redis",
    "start": "1474760",
    "end": "1481450"
  },
  {
    "text": "which at one point we actually had two clusters join each other which was bonkers the way we work around this was",
    "start": "1481450",
    "end": "1490720"
  },
  {
    "text": "by creating a CNI plugin for qubit and an AI pamda plugin for docker that effectively allow allowed us to allocate",
    "start": "1490720",
    "end": "1498640"
  },
  {
    "text": "IP addresses in a first-in first-out order which bought us more time before reuse so our Vitesse backup strategy",
    "start": "1498640",
    "end": "1508570"
  },
  {
    "start": "1505000",
    "end": "1505000"
  },
  {
    "text": "involved stopping my sequel on an up-to-date replica but we didn't want to",
    "start": "1508570",
    "end": "1513730"
  },
  {
    "text": "sacrifice one of the replicas pause that we had for this so instead we used a",
    "start": "1513730",
    "end": "1518800"
  },
  {
    "text": "cron job to create an ephemeral pod whose sole purpose was just for backups so it would restore from the you know",
    "start": "1518800",
    "end": "1526180"
  },
  {
    "text": "the last successful backup catch-up on replication take the backup and then shut down like anything else that needs",
    "start": "1526180",
    "end": "1532780"
  },
  {
    "text": "storage and the easiest solution was to simply create a persistent volume like all the others the problem there is that",
    "start": "1532780",
    "end": "1539890"
  },
  {
    "text": "these pods don't last very long whereas the PV sticks around forever unless you intervene so that's you know the ball",
    "start": "1539890",
    "end": "1547330"
  },
  {
    "text": "the volume is idle and money is wasted you may think that the solution here is",
    "start": "1547330",
    "end": "1552820"
  },
  {
    "text": "empty dirt and you'd be wrong because although that is some improvement you",
    "start": "1552820",
    "end": "1558460"
  },
  {
    "text": "still have to provision your kubernetes nodes for your biggest database in terms of space and i/o bandwidth the solution",
    "start": "1558460",
    "end": "1565480"
  },
  {
    "text": "that we arrived with that is the best of both worlds is using the Flex volume plugin we created our own scratch",
    "start": "1565480",
    "end": "1571900"
  },
  {
    "text": "EBS flex volume plugin that our pack of pods use that creates an EBS volume when",
    "start": "1571900",
    "end": "1577330"
  },
  {
    "text": "the pod starts up and destroys it when it shuts down so when there's no backup running there's no volume no waste",
    "start": "1577330",
    "end": "1585419"
  },
  {
    "start": "1584000",
    "end": "1584000"
  },
  {
    "text": "Staples Center rollouts are kind of a crapshoot because kubernetes had no concept you know that one of these pause",
    "start": "1585600",
    "end": "1591790"
  },
  {
    "text": "is a master or you know a more important you know maybe not more important but you know needed more care than some of",
    "start": "1591790",
    "end": "1598960"
  },
  {
    "text": "the other pod so we could get into situations like this where we're rolling out a change and we incur three different fail overs",
    "start": "1598960",
    "end": "1606789"
  },
  {
    "text": "of course the best case scenario would be terminate the master at last there's",
    "start": "1606789",
    "end": "1614539"
  },
  {
    "text": "one fail over but what can you do well we ended up doing was taking this",
    "start": "1614539",
    "end": "1619910"
  },
  {
    "text": "roll out logic out of kubernetes and putting it into our operator by making changes to our stateful sets with the",
    "start": "1619910",
    "end": "1625580"
  },
  {
    "text": "delete propagation orphan setting on well that allowed us to do is make the stay full set change but not trigger a",
    "start": "1625580",
    "end": "1631700"
  },
  {
    "text": "rolling restart of the pods and then the operator knowing which one is the master",
    "start": "1631700",
    "end": "1636740"
  },
  {
    "text": "and which one are the replicas would restart the replicas first to recap alex",
    "start": "1636740",
    "end": "1643400"
  },
  {
    "text": "is gonna hop back over to talk about lessons learned so I think the main",
    "start": "1643400",
    "end": "1649100"
  },
  {
    "start": "1646000",
    "end": "1646000"
  },
  {
    "text": "theme for lessons learned for us is that this was a transplant operation my test",
    "start": "1649100",
    "end": "1654110"
  },
  {
    "text": "grew up at YouTube HubSpot has different needs different corpus of queries but",
    "start": "1654110",
    "end": "1659960"
  },
  {
    "text": "also a different environment you know we're installing it in kubernetes at YouTube it runs in Borg we",
    "start": "1659960",
    "end": "1666020"
  },
  {
    "text": "run zookeeper YouTube has whatever it is chubby you know and down the line DNS is",
    "start": "1666020",
    "end": "1675200"
  },
  {
    "text": "different from BNs all that stuff so I want to say that we've worked through",
    "start": "1675200",
    "end": "1681140"
  },
  {
    "text": "most of the stuff by now but if you're considering using buy tests in your organization it might be useful to like",
    "start": "1681140",
    "end": "1687140"
  },
  {
    "text": "remember where it came from but with all that hard work from my team and Tom's team I just want to brag a little bit",
    "start": "1687140",
    "end": "1693110"
  },
  {
    "text": "about what we got to do so we have a real database as a service right no more filing a JIRA and waiting any engineer",
    "start": "1693110",
    "end": "1700820"
  },
  {
    "text": "can go to a page fill out a simple form hit create and then like five minutes later their team gets a little slack",
    "start": "1700820",
    "end": "1707360"
  },
  {
    "text": "notification saying like hey your database is ready and I want to be clear this forum is dead simple it posts JSON",
    "start": "1707360",
    "end": "1714860"
  },
  {
    "text": "into kubernetes you know the watch in our operator gets a little tickle and",
    "start": "1714860",
    "end": "1720590"
  },
  {
    "text": "like makes pods and so we we do nothing right we actually we get a slack",
    "start": "1720590",
    "end": "1726679"
  },
  {
    "text": "notification as well it says like if somebody's making a database yeah yeah yeah high-fives all around we",
    "start": "1726679",
    "end": "1734890"
  },
  {
    "start": "1730000",
    "end": "1730000"
  },
  {
    "text": "have trivial read scaling right if you want to add another pod to the database it's a simple Kubb CTL edit and it just",
    "start": "1734890",
    "end": "1742990"
  },
  {
    "text": "happens we take advantage of the kubernetes schedulers bin packing in a",
    "start": "1742990",
    "end": "1748419"
  },
  {
    "start": "1744000",
    "end": "1744000"
  },
  {
    "text": "pretty neat way because we're saving money we actually then run three pods",
    "start": "1748419",
    "end": "1753520"
  },
  {
    "text": "for every database instead of just like two ec2 hosts and that lets us do a neat thing because we're fault-tolerant to",
    "start": "1753520",
    "end": "1762130"
  },
  {
    "text": "like losing a zone in that point we can actually run the my sequel cluster at",
    "start": "1762130",
    "end": "1767890"
  },
  {
    "text": "with semi sync replication and semi sake replication is pretty safe so that lets",
    "start": "1767890",
    "end": "1773620"
  },
  {
    "text": "us turn the write durability down into my sequels and that improves throughput",
    "start": "1773620",
    "end": "1778690"
  },
  {
    "text": "and like is nicer on your EBS volumes and probably cheaper so that's pretty",
    "start": "1778690",
    "end": "1784690"
  },
  {
    "text": "cool we also have application agnostic sharding so that's great and then like",
    "start": "1784690",
    "end": "1791409"
  },
  {
    "text": "zero downtime recharging thanks to by Tess and finally because of Orchestrator",
    "start": "1791409",
    "end": "1797770"
  },
  {
    "start": "1795000",
    "end": "1795000"
  },
  {
    "text": "we have automated impairment handling which we actually had like I don't know",
    "start": "1797770",
    "end": "1802929"
  },
  {
    "text": "a couple months ago at some point we lost the kubernetes node to an impairment and like we did nothing my",
    "start": "1802929",
    "end": "1809950"
  },
  {
    "text": "team did we like saw all the notifications in our slack channel from Orchestrator saying like Oh responding to dead master on blah blah",
    "start": "1809950",
    "end": "1816640"
  },
  {
    "text": "blah and we were just busy you like high-fiving each other it was awesome and that means it can happen at night",
    "start": "1816640",
    "end": "1821890"
  },
  {
    "text": "and I get to sleep through it with no page I should have put like a big red line through that thing that's all we",
    "start": "1821890",
    "end": "1828100"
  },
  {
    "text": "got for you want to have a big thanks to sugu the main author architect of I test",
    "start": "1828100",
    "end": "1834820"
  },
  {
    "text": "from YouTube and thanks to coop con got some links there thanks everybody we don't know the time but we probably",
    "start": "1834820",
    "end": "1841600"
  },
  {
    "text": "have time for questions [Applause] [Music]",
    "start": "1841600",
    "end": "1848250"
  },
  {
    "text": "thirty-five we should have like five minutes for questions yeah yeah nope not",
    "start": "1848250",
    "end": "1855610"
  },
  {
    "text": "gonna happen no it's it's actually pretty tightly coupled with the rest of HubSpot infrastructure yeah we would",
    "start": "1855610",
    "end": "1862659"
  },
  {
    "text": "like to plant that there there is an open source yeah there is an open source my test operator we have been put on",
    "start": "1862659",
    "end": "1868030"
  },
  {
    "text": "this is probably not as good no we actually collaborate a little bit on it",
    "start": "1868030",
    "end": "1873390"
  },
  {
    "text": "but yeah we can't we can't open source the hub spot one sorry yeah I don't know",
    "start": "1873390",
    "end": "1882240"
  },
  {
    "text": "I'm not like cuz I don't use it right",
    "start": "1882240",
    "end": "1887490"
  },
  {
    "text": "little laughs I check so I'm not super familiar and probably other people in the room could make a better answer to",
    "start": "1887669",
    "end": "1893890"
  },
  {
    "text": "that but you know last I knew it didn't have a whole lot of like deploy Canarian",
    "start": "1893890",
    "end": "1899710"
  },
  {
    "text": "right so we we have a kind of at this point sophisticated system where like if",
    "start": "1899710",
    "end": "1904960"
  },
  {
    "text": "we want an update across all of our databases we can designate a few of them it's like alpha QoS and roll the",
    "start": "1904960",
    "end": "1911950"
  },
  {
    "text": "database out their role the change out to that and then be able to like halt the thing and roll back and stuff like",
    "start": "1911950",
    "end": "1918070"
  },
  {
    "text": "that and if it proceeds like keep going and then roll out a little bit faster so we're not you know totalling our thumbs",
    "start": "1918070",
    "end": "1923710"
  },
  {
    "text": "for forever so last I knew it didn't do that but the other thing to keep in mind",
    "start": "1923710",
    "end": "1929470"
  },
  {
    "text": "is that we started our operator I believe before so we've been we've been",
    "start": "1929470",
    "end": "1935350"
  },
  {
    "text": "heads down on that I should say all this stuff is Prague for like a year now yeah",
    "start": "1935350",
    "end": "1941010"
  },
  {
    "text": "yeah very carefully yeah very carefully",
    "start": "1941010",
    "end": "1947700"
  },
  {
    "text": "that's a bit of our infrastructure that we we still need to work on yeah we",
    "start": "1947700",
    "end": "1953710"
  },
  {
    "text": "don't have a smooth like you know spray this DDL out across all the shards kind",
    "start": "1953710",
    "end": "1960520"
  },
  {
    "text": "of thing worked out yet we do we do have some tools that wrap that essentially",
    "start": "1960520",
    "end": "1966100"
  },
  {
    "text": "wrap PT online schema change we're evaluating another github project right",
    "start": "1966100",
    "end": "1971950"
  },
  {
    "text": "now called ghost so the github online schema change and and yeah I think we're just gonna",
    "start": "1971950",
    "end": "1977940"
  },
  {
    "text": "replace the backing in our wrapper like away from PT",
    "start": "1977940",
    "end": "1983750"
  },
  {
    "text": "on let's keep a change on to ghost and then yeah route it so that it like hits all the shards that'll be like a pipette",
    "start": "1983750",
    "end": "1990560"
  },
  {
    "text": "bypassed by tests kind of operation yeah not to my knowledge but that I mean",
    "start": "1990560",
    "end": "2005080"
  },
  {
    "text": "yeah we were we're noobs right so speak for yourself yeah we know I think that's",
    "start": "2005080",
    "end": "2013840"
  },
  {
    "text": "the first I've heard of it so look we might have just missed it the the TLDR is that aside from vault were not a very",
    "start": "2013840",
    "end": "2019540"
  },
  {
    "text": "Hoshi Corp heavy shop not for any specific reason but we're not using console so we didn't we didn't",
    "start": "2019540",
    "end": "2025420"
  },
  {
    "text": "investigate that but thanks for the heads-up yeah",
    "start": "2025420",
    "end": "2030690"
  },
  {
    "text": "so essentially the operator gives us the control of how quickly we roll things",
    "start": "2049610",
    "end": "2055350"
  },
  {
    "text": "out right I as I understand it like you give helm some updated charts or so",
    "start": "2055350",
    "end": "2062580"
  },
  {
    "text": "their new release or something and then it just rolls it out instantly to everything and like we don't wanna do",
    "start": "2062580",
    "end": "2067950"
  },
  {
    "text": "that and so we could have had like oh here's a helm release for like so now for QoS things but yeah in the end we didn't so",
    "start": "2067950",
    "end": "2077909"
  },
  {
    "text": "it's it's a little bit of a change of perspective right helm and helm charts is like you have data in a file and",
    "start": "2077910",
    "end": "2084470"
  },
  {
    "text": "you're managing all your resources by like running commands an operator you",
    "start": "2084470",
    "end": "2090929"
  },
  {
    "text": "can like write a piece of code and deploy that piece of code and then it",
    "start": "2090930",
    "end": "2096720"
  },
  {
    "text": "does stuff so you can you know you have the full yeah yeah yeah so that was you",
    "start": "2096720",
    "end": "2109410"
  },
  {
    "text": "know we wanted to move it out of our hands like in a terminal doing some stuff into something that we can like",
    "start": "2109410",
    "end": "2116130"
  },
  {
    "text": "reason about for its logic yeah yeah no I mean you're right like morally they're",
    "start": "2116130",
    "end": "2122100"
  },
  {
    "text": "pretty similar but yeah the finer points like they're another way to think about it we absolutely could have implemented",
    "start": "2122100",
    "end": "2129060"
  },
  {
    "text": "this with hell it just would have been a layer on top of it talking to helm doing",
    "start": "2129060",
    "end": "2134100"
  },
  {
    "text": "the work and at that point it's like how many moving parts do we want yeah I think that's probably it on time will be",
    "start": "2134100",
    "end": "2141090"
  },
  {
    "text": "like at the front in the hall in the hallway thanks again everybody thank you",
    "start": "2141090",
    "end": "2146180"
  },
  {
    "text": "[Applause]",
    "start": "2146180",
    "end": "2150750"
  }
]