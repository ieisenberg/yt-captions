[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "all right welcome everyone I hope the remaining can find somewhere",
    "start": "0",
    "end": "5430"
  },
  {
    "text": "to sit my name is Michael Aston I'm a software engineer at solando and today I",
    "start": "5430",
    "end": "11880"
  },
  {
    "text": "want to talk about how we in solando continuously deliver our company's infrastructure and hopefully also give",
    "start": "11880",
    "end": "18840"
  },
  {
    "text": "you some ideas of how you can do it I've been working with kubernetes for the past two years almost at solando so not",
    "start": "18840",
    "end": "26279"
  },
  {
    "text": "too much building kubernetes cause anything like this but more operating it and building controllers and things",
    "start": "26279",
    "end": "33120"
  },
  {
    "text": "around it through to be able to operate it in a in a reliable way at a fairly large scale and some of you might not",
    "start": "33120",
    "end": "41399"
  },
  {
    "text": "know cilantro I guess since we're in Europe now more people will know it but you might be wondering why we have shoes",
    "start": "41399",
    "end": "46800"
  },
  {
    "text": "why I have shoes on my slide but to give you some context cilantro is the biggest",
    "start": "46800",
    "end": "52050"
  },
  {
    "start": "50000",
    "end": "50000"
  },
  {
    "text": "fashion online retailer in Europe we have a revenue of 4.5 billion euros as",
    "start": "52050",
    "end": "58559"
  },
  {
    "text": "of 2017 and we have 15,000 employees in",
    "start": "58559",
    "end": "63660"
  },
  {
    "text": "Europe and we are spread over yeah a number of countries in Europe and we",
    "start": "63660",
    "end": "70710"
  },
  {
    "text": "have 2000 million visitors per month and yeah fairly large out of these 15,000",
    "start": "70710",
    "end": "81210"
  },
  {
    "text": "employees around 2009 detect department learned working with tech within solando",
    "start": "81210",
    "end": "87540"
  },
  {
    "text": "and this translates roughly to a little over 200 delivery teams and I work in a",
    "start": "87540",
    "end": "95700"
  },
  {
    "text": "team that is responsible for the kubernetes infrastructure and also the AWS infrastructure and to support all",
    "start": "95700",
    "end": "102720"
  },
  {
    "text": "these teams our current scale looks like this so we have 366 AWS accounts and we",
    "start": "102720",
    "end": "111090"
  },
  {
    "text": "have 84 kubernetes process as of this morning and you might think 366 a Tobias",
    "start": "111090",
    "end": "120509"
  },
  {
    "text": "account seems like quite a lot one of the reasons is that we come from a model",
    "start": "120509",
    "end": "126990"
  },
  {
    "start": "124000",
    "end": "124000"
  },
  {
    "text": "where we gave one AWS account per team or less",
    "start": "126990",
    "end": "132610"
  },
  {
    "text": "maybe some teams even got multiple AWS accounts for staging and testing production so on we have this tool set",
    "start": "132610",
    "end": "140270"
  },
  {
    "text": "around a double yes we call Stoops which is what the infrastructure we are",
    "start": "140270",
    "end": "146150"
  },
  {
    "text": "transitioning from it is just some tools to help making the AWS services a bit",
    "start": "146150",
    "end": "152810"
  },
  {
    "text": "easier to use but it's pretty much but delay AWS you can think of it this way and in this model we have like I said",
    "start": "152810",
    "end": "160400"
  },
  {
    "text": "AWS accounts per team we have some rules that all instances must run with the",
    "start": "160400",
    "end": "167000"
  },
  {
    "text": "same ami for compliance reason so all teams must use the same ami and they their Falls I have to update this ami at",
    "start": "167000",
    "end": "175400"
  },
  {
    "text": "some point everyone gets power user access so like not exactly root but",
    "start": "175400",
    "end": "181520"
  },
  {
    "text": "almost today double use account so they can pretty much do whatever they want which gives them a much power but with",
    "start": "181520",
    "end": "189470"
  },
  {
    "text": "power comes responsibility of course and there's a lot of responsibility if you",
    "start": "189470",
    "end": "194959"
  },
  {
    "text": "have to manage everything for every team that every team has to do this so",
    "start": "194959",
    "end": "200420"
  },
  {
    "text": "basically the rule is you build it and you run everything and everything means also monitoring so we of course have a",
    "start": "200420",
    "end": "208400"
  },
  {
    "text": "central monitoring system a solando but you actually get monitoring for your",
    "start": "208400",
    "end": "213709"
  },
  {
    "text": "services you would have to deploy something into your idea base account like an agent that can can pull data",
    "start": "213709",
    "end": "218750"
  },
  {
    "text": "from from different services that we run and this you have to manage yourself that this 18 this running as same with",
    "start": "218750",
    "end": "225709"
  },
  {
    "text": "CI CT we also provision Jenkins's for each team if they wanted to but they have to kind of manage this themselves",
    "start": "225709",
    "end": "232450"
  },
  {
    "text": "so we're transitioning to a model where we provide kubernetes as the base",
    "start": "232450",
    "end": "238100"
  },
  {
    "text": "infrastructure and then we pulled a bunch of stuff on top of this so we have",
    "start": "238100",
    "end": "243320"
  },
  {
    "text": "instead of having one kubernetes cluster per team something like this we have now",
    "start": "243320",
    "end": "248630"
  },
  {
    "text": "clusters per product and product is multiple teams so three to six teams",
    "start": "248630",
    "end": "254180"
  },
  {
    "text": "depending on how big the product is and then the reason why we don't just",
    "start": "254180",
    "end": "261390"
  },
  {
    "text": "put everything in one cluster only like this is that do you have an organization that is structured in a certain way and",
    "start": "261390",
    "end": "266790"
  },
  {
    "text": "you have to like do it gradually and of course there's also like reliability",
    "start": "266790",
    "end": "271830"
  },
  {
    "text": "issues with putting everything in one cluster but it's a different thing and",
    "start": "271830",
    "end": "276950"
  },
  {
    "text": "one of the benefits here with kubernetes our case is that the teams no longer",
    "start": "276950",
    "end": "282420"
  },
  {
    "text": "have to manage the the infrastructure at the lowest level so the instances are",
    "start": "282420",
    "end": "288720"
  },
  {
    "text": "managed by our team and we update the ami that we use for the kubernetes nodes",
    "start": "288720",
    "end": "296010"
  },
  {
    "text": "and they don't have to to care about this and instead of giving everyone",
    "start": "296010",
    "end": "301680"
  },
  {
    "text": "production like power user access to production we use a hands-off approach",
    "start": "301680",
    "end": "308300"
  },
  {
    "text": "which basically means that in production kubernetes clusters who don't have write",
    "start": "308300",
    "end": "313350"
  },
  {
    "text": "access so you can read everything but you can't just do keep CL apply or Excel or something like this to get into your",
    "start": "313350",
    "end": "319320"
  },
  {
    "text": "container delete something you can do it in in the case of emergencies of course you can request previous taxes but by",
    "start": "319320",
    "end": "325920"
  },
  {
    "text": "default you don't get this and then we also provide a lot of stuff out of the",
    "start": "325920",
    "end": "331530"
  },
  {
    "text": "box so like s was talking about the money trying stuff this we provide automatically in the class when you provision them the C ICT solution is",
    "start": "331530",
    "end": "339360"
  },
  {
    "text": "something we have that is integrated with our company's setup which is more",
    "start": "339360",
    "end": "346920"
  },
  {
    "text": "or less our box when people create a repository for a new application and",
    "start": "346920",
    "end": "353910"
  },
  {
    "text": "just to be clear what I'm talking about what I want to talk to about today is not like CI CD on to kubernetes but",
    "start": "353910",
    "end": "361170"
  },
  {
    "text": "actually continuous delivery of the kubernetes infrastructure and when we",
    "start": "361170",
    "end": "367950"
  },
  {
    "start": "367000",
    "end": "367000"
  },
  {
    "text": "started to set up this whole community's infrastructure about one and a half year",
    "start": "367950",
    "end": "373350"
  },
  {
    "text": "ago or so we had some kind of philosophy I think philosophy is maybe a true big",
    "start": "373350",
    "end": "378690"
  },
  {
    "text": "word but just to have some idea of what do we want with this platform and how",
    "start": "378690",
    "end": "384180"
  },
  {
    "text": "should it kind of work so one thing is that we don't want to have any pet rosters meaning we don't to manage ad clusters individually",
    "start": "384180",
    "end": "392820"
  },
  {
    "text": "because some teams have a very special need in this way and we only deployed the money trying for them and they",
    "start": "392820",
    "end": "397830"
  },
  {
    "text": "others have some other monitoring solution or whatever it should be fairly generic across to all the clusters",
    "start": "397830",
    "end": "404730"
  },
  {
    "text": "because then it's much easier to manage from us of course there will be differences in what instance sizes the",
    "start": "404730",
    "end": "411510"
  },
  {
    "text": "teams need for the different services so these things of what we can tweak on the cluster level but most things we try to",
    "start": "411510",
    "end": "418170"
  },
  {
    "text": "keep as generic as possible at least for the pace cluster setup that we provide",
    "start": "418170",
    "end": "423770"
  },
  {
    "text": "another thing was that we wanted to always provide the latest stable kubernetes version meaning right now we",
    "start": "423770",
    "end": "430920"
  },
  {
    "text": "have one nine instead of one ten but this was just not there yet fast and we",
    "start": "430920",
    "end": "437190"
  },
  {
    "text": "actually is transitioned the older cluster we created the first one which translated from 1.4 all the way to 1.9",
    "start": "437190",
    "end": "443580"
  },
  {
    "text": "so updating all the major versions in between and this is a bit different from",
    "start": "443580",
    "end": "449550"
  },
  {
    "text": "like gke the schewe kubernetes service where you kind of as a user you can",
    "start": "449550",
    "end": "457350"
  },
  {
    "text": "choose to update to which version when when it fits you but in our case we kind",
    "start": "457350",
    "end": "462570"
  },
  {
    "text": "of take the responsibility of updating this for our users and we make sure that",
    "start": "462570",
    "end": "468960"
  },
  {
    "text": "we test it enough before we roll it out and then we kind of just roll it out",
    "start": "468960",
    "end": "474090"
  },
  {
    "text": "when we're sure there's no disruption that can happen almost you all right and",
    "start": "474090",
    "end": "480860"
  },
  {
    "text": "then they don't have to care about it whereas if you ask your users or 8080",
    "start": "480860",
    "end": "486830"
  },
  {
    "text": "products to care about winter update and you will see that they probably won't",
    "start": "486830",
    "end": "491910"
  },
  {
    "text": "update because it's safer to or it's easier to just let it run as it was and",
    "start": "491910",
    "end": "501410"
  },
  {
    "text": "sorry and of course with this being able to provide the latest version always",
    "start": "501860",
    "end": "507630"
  },
  {
    "text": "also means that we want to do it in a continuous and non-disruptive way with with the cluster updates so from the",
    "start": "507630",
    "end": "515849"
  },
  {
    "text": "beginning we didn't want to have any maintenance windows that we only do cluster dates from a knight because then we can",
    "start": "515849",
    "end": "522500"
  },
  {
    "text": "maybe be unavailable for a while and then is fine we wanted this to be something that we can do all the time",
    "start": "522500",
    "end": "529120"
  },
  {
    "text": "also because if you have security issues you want to be able to roll it out right",
    "start": "529120",
    "end": "534769"
  },
  {
    "text": "away you don't want to wait a week because you have a maintenance window in the weekend only so building the",
    "start": "534769",
    "end": "542029"
  },
  {
    "text": "infrastructure from the beginning in a way that it can just be rolled out whenever makes it what much easier when",
    "start": "542029",
    "end": "547790"
  },
  {
    "text": "you really need to do it and we also think about like we do cluster auto",
    "start": "547790",
    "end": "553639"
  },
  {
    "text": "scaling of our clusters and if you think about rolling all the nodes in a cluster or doing scale up and scale down it's",
    "start": "553639",
    "end": "560329"
  },
  {
    "text": "not so much stiff and what's actually happening to the for the things that are running inside the cluster so it should",
    "start": "560329",
    "end": "565850"
  },
  {
    "text": "should work and then another thing was to be fully automated operations again",
    "start": "565850",
    "end": "572420"
  },
  {
    "text": "as in quotes because I think everyone knows who does operation that can't be fully automated this is that the ideal",
    "start": "572420",
    "end": "580639"
  },
  {
    "text": "situation is that we can just make a pull request to our configuration repository with some change and then we",
    "start": "580639",
    "end": "587480"
  },
  {
    "text": "just click merge and then we just see everything roll out and this also works for the most time but of course sometimes you have to do something",
    "start": "587480",
    "end": "593509"
  },
  {
    "text": "manually but this is the philosophy anyway to give you a bit of context",
    "start": "593509",
    "end": "600380"
  },
  {
    "start": "598000",
    "end": "598000"
  },
  {
    "text": "about our setup that I'm talking about doesn't mean that you can't take these",
    "start": "600380",
    "end": "606410"
  },
  {
    "text": "ideas that I'm presenting today and not use it indefinitely I am and just to give you the context for how we're",
    "start": "606410",
    "end": "611930"
  },
  {
    "text": "running it be provisioned our community's infrastructure with we've",
    "start": "611930",
    "end": "618380"
  },
  {
    "text": "run it on AWS and provisional vehicle formation we run its city outside of the",
    "start": "618380",
    "end": "624139"
  },
  {
    "text": "of the Masters which is I mean there's a common approach have it city on the masternodes where we take it outside and",
    "start": "624139",
    "end": "630470"
  },
  {
    "text": "have a separate stack for this and this is just to make it easier to like the masses become stateless in this case and",
    "start": "630470",
    "end": "636980"
  },
  {
    "text": "it's very easy to have a highly available master setup where we don't have to care about HDD when we do our updates of the Masters",
    "start": "636980",
    "end": "644440"
  },
  {
    "text": "we used container linux for the notes and we treated kind of like in a",
    "start": "644440",
    "end": "650129"
  },
  {
    "text": "immutable instance or note so we don't use the feature of like all the updating",
    "start": "650129",
    "end": "656980"
  },
  {
    "text": "of container Linux we just if we want to update the notes we just replace it with",
    "start": "656980",
    "end": "664120"
  },
  {
    "text": "a new one we also deploy our notes in multiple",
    "start": "664120",
    "end": "670720"
  },
  {
    "text": "availability zones so basically an auto scaling group in it obvious if you're",
    "start": "670720",
    "end": "676089"
  },
  {
    "text": "familiar with that that spans multiple availability zones and this is to make",
    "start": "676089",
    "end": "682060"
  },
  {
    "text": "our whatever our users deploy automatically be resilient to",
    "start": "682060",
    "end": "688079"
  },
  {
    "text": "availability zones being down and so on which also of course brings some problems because you have volumes and so",
    "start": "688079",
    "end": "694899"
  },
  {
    "text": "on that are tied to one level ability zone then we have H a high available",
    "start": "694899",
    "end": "704500"
  },
  {
    "text": "control plane which is like I said stateless because we don't have third CD and we have an LP in front where we",
    "start": "704500",
    "end": "710680"
  },
  {
    "text": "terminate as a cell and then we can just yeah roll the roll the master notes",
    "start": "710680",
    "end": "716199"
  },
  {
    "text": "behind this if you do a an upgrade of any kind our cluster configuration is thought in kit and we actually have it",
    "start": "716199",
    "end": "723310"
  },
  {
    "text": "on github.com our production configuration if you want to check this",
    "start": "723310",
    "end": "728470"
  },
  {
    "text": "out I have some links later on and we run instances to be our jingyan set up",
    "start": "728470",
    "end": "734040"
  },
  {
    "text": "in to insist against the clusters that we create and then changes are rolled",
    "start": "734040",
    "end": "739480"
  },
  {
    "text": "out via something we call the cluster lifecycle manager which is like a controller component that knows how to",
    "start": "739480",
    "end": "744970"
  },
  {
    "text": "first of all apply this confirmation in the right way our configuration has some",
    "start": "744970",
    "end": "750100"
  },
  {
    "text": "go templating for a simple substitution of values and this it can it does this",
    "start": "750100",
    "end": "757029"
  },
  {
    "text": "kind of stuff and then it also has it knows how to update the notes in in a",
    "start": "757029",
    "end": "762370"
  },
  {
    "text": "reliable way and this class the lifecycle manager is open source since Friday so you can go",
    "start": "762370",
    "end": "767529"
  },
  {
    "text": "check it up check it out on our github it's still in a state where it's a bit solando specific but we want to work on",
    "start": "767529",
    "end": "774430"
  },
  {
    "text": "it to make it more generic that is useful without too many of",
    "start": "774430",
    "end": "779430"
  },
  {
    "start": "779000",
    "end": "779000"
  },
  {
    "text": "so we have also something that we call the cluster registry which is either a",
    "start": "779670",
    "end": "785590"
  },
  {
    "text": "CP rest API in in our setup or just a yama file but basically the format looks",
    "start": "785590",
    "end": "792220"
  },
  {
    "text": "like this you have a list of clusters with the ID the URL for the API server some configuration items which are",
    "start": "792220",
    "end": "800230"
  },
  {
    "text": "usually unique per cluster and could be some keys in there for secrets for our",
    "start": "800230",
    "end": "806950"
  },
  {
    "text": "locking agent or whatever they're being tripped and stolen air has an environment region life cycle status and",
    "start": "806950",
    "end": "815020"
  },
  {
    "text": "the life cycle status is used by the the lifecycle manager to know if it should create a crust or if it's already",
    "start": "815020",
    "end": "821320"
  },
  {
    "text": "created or if it should decommission a cluster and so on and then we have a list of node pools as we call them which",
    "start": "821320",
    "end": "826840"
  },
  {
    "text": "is I think they also used the term node group in G key for instance but it's",
    "start": "826840",
    "end": "833140"
  },
  {
    "text": "basically in in a davia terms it just translates to an auto scaling group in our case where you just specify I want a",
    "start": "833140",
    "end": "841210"
  },
  {
    "text": "set of nodes that identical that have this instance type that have this name for the node pool there's a min and Max",
    "start": "841210",
    "end": "847210"
  },
  {
    "start": "846000",
    "end": "846000"
  },
  {
    "text": "size and then it can also scale between these sizes and we can have multiple node pools to find our trust our",
    "start": "847210",
    "end": "855820"
  },
  {
    "start": "855000",
    "end": "855000"
  },
  {
    "text": "configuration is this is the one on get up which is roughly looks like this with",
    "start": "855820",
    "end": "862780"
  },
  {
    "text": "a cluster llamó just the confirmation spec basically of the of the cluster set",
    "start": "862780",
    "end": "868570"
  },
  {
    "text": "up and then we have the SD confirmation stack and then we have a folder called",
    "start": "868570",
    "end": "874600"
  },
  {
    "text": "manifest which is all the kubernetes manifests that we deploy as default so all like flannel team and set for our",
    "start": "874600",
    "end": "881860"
  },
  {
    "text": "overlay network and our monitoring system is deployed just by putting a",
    "start": "881860",
    "end": "887230"
  },
  {
    "text": "Yama file company this channel file in this manifest folder and then we have this masked and work without container",
    "start": "887230",
    "end": "895150"
  },
  {
    "text": "Linux configuration which is just the like the user data of the of the ec2",
    "start": "895150",
    "end": "900730"
  },
  {
    "text": "instances also start Cupid and how to bootstrap things on the nodes is what we configure in these files",
    "start": "900730",
    "end": "908700"
  },
  {
    "start": "909000",
    "end": "909000"
  },
  {
    "text": "yeah and then we have there's like at the lifecycle manager so how it works is I try to pinpoint it here so it has two",
    "start": "909090",
    "end": "918640"
  },
  {
    "text": "sources of configuration it has the git repository where we have our cluster configuration and then it has the",
    "start": "918640",
    "end": "924580"
  },
  {
    "text": "cluster registry which has this metadata about the clusters and it basically watches both of these configuration",
    "start": "924580",
    "end": "931990"
  },
  {
    "text": "sources and if there's a change to one of them it will react on this merge the information tricky or not depending on",
    "start": "931990",
    "end": "937270"
  },
  {
    "text": "the cluster and in the beginning if it's a new cluster it will create it by do a",
    "start": "937270",
    "end": "943750"
  },
  {
    "text": "create with confirmation create a new cloud formation stack for it CD and one for for the kubernetes infrastructure",
    "start": "943750",
    "end": "950370"
  },
  {
    "text": "and this will basically set up this where we have the kubernetes api server",
    "start": "950370",
    "end": "955390"
  },
  {
    "text": "usually behind the you'll be with multi multi masters and then we have the",
    "start": "955390",
    "end": "963520"
  },
  {
    "text": "worker pool of nodes deployed and once",
    "start": "963520",
    "end": "970420"
  },
  {
    "text": "this is deployed and when you're creating a cluster it will basically wait for the API server to respond so it",
    "start": "970420",
    "end": "976690"
  },
  {
    "text": "knows this there and once it's available it will start to apply all the manifest that we have to find this manifest",
    "start": "976690",
    "end": "983830"
  },
  {
    "text": "folder so deploying all the things that we run in the in the cube system namespace in our case solder the cluster",
    "start": "983830",
    "end": "990610"
  },
  {
    "text": "components basically and if there if you already have the cluster created and there's just an update then it will just",
    "start": "990610",
    "end": "997750"
  },
  {
    "text": "reapply the confirmation stack this will it obvious will then update whatever a component that was changed there and if",
    "start": "997750",
    "end": "1004530"
  },
  {
    "text": "there's no changes it will not do anything but then it will also apply all the manifest again this will do every",
    "start": "1004530",
    "end": "1010140"
  },
  {
    "text": "time so to synchronize so even if you go and delete something in the cluster on the next update it will be overridden",
    "start": "1010140",
    "end": "1016710"
  },
  {
    "text": "and have the actual configuration that we have stolen yet which is tracked and",
    "start": "1016710",
    "end": "1022310"
  },
  {
    "text": "proceeded to see what what is actually running in this cluster and the way we kind of feed a new",
    "start": "1022310",
    "end": "1033329"
  },
  {
    "text": "configuration into the cluster is we have this kind of upgrade flow where we",
    "start": "1033329",
    "end": "1039288"
  },
  {
    "text": "we will usually create a branch with my new feature then we make a PR we add",
    "start": "1039289",
    "end": "1046709"
  },
  {
    "text": "ready to test label on the PR when it's ready to be pre answer interested and",
    "start": "1046709",
    "end": "1052799"
  },
  {
    "text": "then we when it once is ready we either add a label ready to merge or we just",
    "start": "1052799",
    "end": "1058649"
  },
  {
    "text": "merge it and then we merge it into something we call the dev branch so the reason we have these branches is that we",
    "start": "1058649",
    "end": "1065190"
  },
  {
    "text": "kind of created this idea of channels so our clusters live in different channels",
    "start": "1065190",
    "end": "1071539"
  },
  {
    "text": "so we have dev alpha beta stable and so on we can just create a new branch to",
    "start": "1071539",
    "end": "1077850"
  },
  {
    "text": "create a new like channel level and then it kind of flows from from dev to alpha",
    "start": "1077850",
    "end": "1084390"
  },
  {
    "text": "to beta to stable and so on the changes and ideally we wanted to",
    "start": "1084390",
    "end": "1091350"
  },
  {
    "text": "have it like you just merge two dev and then after some time after some monitoring has happened and there's no",
    "start": "1091350",
    "end": "1096450"
  },
  {
    "text": "issues it automatically can follow it into elf and PSR we're not there yet we are still doing it manually so creating",
    "start": "1096450",
    "end": "1103320"
  },
  {
    "text": "a new PR for the next branch but this is something we want to work on - to",
    "start": "1103320",
    "end": "1109080"
  },
  {
    "text": "actually fully automate this part because it's bit annoying - it's it's one point it's a bit of a safety feeling",
    "start": "1109080",
    "end": "1115740"
  },
  {
    "text": "that you don't do it automatically but at the other point if if we already have it in twin sister and so on we should",
    "start": "1115740",
    "end": "1121350"
  },
  {
    "text": "just be able to to move forward as",
    "start": "1121350",
    "end": "1126570"
  },
  {
    "start": "1125000",
    "end": "1125000"
  },
  {
    "text": "actually we only have three channels right now which is def alpha and beta",
    "start": "1126570",
    "end": "1132450"
  },
  {
    "text": "and the way we have chosen to structure this right now is that we have the dev",
    "start": "1132450",
    "end": "1137580"
  },
  {
    "text": "channel with three clusters which are development clusters meaning that these",
    "start": "1137580",
    "end": "1144659"
  },
  {
    "text": "are used by people to deploy test application and so on but they are not like production critical or anything but",
    "start": "1144659",
    "end": "1151950"
  },
  {
    "text": "there's enough people using and so actually notice if something that we merged in a new configuration change",
    "start": "1151950",
    "end": "1158299"
  },
  {
    "text": "to spot that there might be something wrong that we have to look at next we",
    "start": "1158299",
    "end": "1163710"
  },
  {
    "text": "have the alpha channel which right now we have our main infrastructure cluster there which is basically what my team",
    "start": "1163710",
    "end": "1169139"
  },
  {
    "text": "and surrounding teams are using to apply all the infrastructure that we need to that all our teams need to integrate and",
    "start": "1169139",
    "end": "1175679"
  },
  {
    "text": "CD system and all this is running there and the reason we have it already as the Alpha is that is very important to us so",
    "start": "1175679",
    "end": "1182669"
  },
  {
    "text": "if something happens we will notice it right away or be woken up at night by this and on one side this is good",
    "start": "1182669",
    "end": "1190590"
  },
  {
    "text": "because we can notice things right away and fix it on the other side if we break something we really break something but",
    "start": "1190590",
    "end": "1197700"
  },
  {
    "text": "that's kind of the trade-off your mate right now we might change it at some point but that's at least the reasoning behind and then the last channel is the",
    "start": "1197700",
    "end": "1205320"
  },
  {
    "text": "pizza which baits are stable it could be called doesn't really matter it's just",
    "start": "1205320",
    "end": "1212220"
  },
  {
    "text": "all the remaining clusters for the products that are there so if we see",
    "start": "1212220",
    "end": "1218580"
  },
  {
    "text": "that everything is working in alpha then we are fairly confident that the pea clusters will also work when we roll out",
    "start": "1218580",
    "end": "1224100"
  },
  {
    "text": "and update and this is right now 80 clusters like I said we do into interest",
    "start": "1224100",
    "end": "1232259"
  },
  {
    "text": "on every PR basically looks like this from the github point of view so we have",
    "start": "1232259",
    "end": "1237629"
  },
  {
    "text": "these three orange parts that are running different end-to-end tests and",
    "start": "1237629",
    "end": "1244110"
  },
  {
    "text": "the entrances we run the conformance test suit from directly from upstream",
    "start": "1244110",
    "end": "1249120"
  },
  {
    "text": "kubernetes so there's this whole test /e to E and in the kubernetes repository",
    "start": "1249120",
    "end": "1255799"
  },
  {
    "text": "relah is like almost maybe thousands of different test cases internal test cases",
    "start": "1255799",
    "end": "1262250"
  },
  {
    "text": "and currently it is 144 of the of those are conformance tests as of 1/9 and",
    "start": "1262250",
    "end": "1268710"
  },
  {
    "text": "these we run to like get a general sense of is the cluster performing in like and",
    "start": "1268710",
    "end": "1274740"
  },
  {
    "text": "conformant way this is also the tests that are run by this run oh boy for validating clusters that are these",
    "start": "1274740",
    "end": "1281419"
  },
  {
    "text": "certificate is certified clusters it's the same test food I think we even",
    "start": "1281419",
    "end": "1287450"
  },
  {
    "text": "run a bit more than their next we also pick some stifle set test these are not",
    "start": "1287450",
    "end": "1294620"
  },
  {
    "text": "part of the conformance because they require some default storage class configuration in the cluster which is",
    "start": "1294620",
    "end": "1300980"
  },
  {
    "text": "not required by the conformant part so we picked these out individually and we",
    "start": "1300980",
    "end": "1307940"
  },
  {
    "text": "pick out two and these are really really good because they test stuff like attaching volumes to stateful sets or I",
    "start": "1307940",
    "end": "1314750"
  },
  {
    "text": "think we run the the rate is yes like in Redis deploying a registry note register",
    "start": "1314750",
    "end": "1321350"
  },
  {
    "text": "and then it each node needs volume and in AWS this means an EBS volume so we're",
    "start": "1321350",
    "end": "1327710"
  },
  {
    "text": "actually testing the full entrant of the controller manager being able to schedule of EBS volume adjusting it to",
    "start": "1327710",
    "end": "1334310"
  },
  {
    "text": "the nodes cube loop being able to mount the volume and so on this is mostly this you just work out from kubernetes",
    "start": "1334310",
    "end": "1341090"
  },
  {
    "text": "already but we might miss configure something on our notes that would make it not work so that's why we are testing",
    "start": "1341090",
    "end": "1346130"
  },
  {
    "text": "this and lastly we have some icons alano chests but it's just our chests for our",
    "start": "1346130",
    "end": "1352220"
  },
  {
    "text": "own tooling so we have chests for our interests we have a English controller",
    "start": "1352220",
    "end": "1357470"
  },
  {
    "text": "that's also open source that creates application load balancers for ingress and this retesting we also have this",
    "start": "1357470",
    "end": "1363140"
  },
  {
    "text": "external DNS project which is community singular project and so we're actually",
    "start": "1363140",
    "end": "1368660"
  },
  {
    "text": "testing deploying an application with a part with an increase to we get a TNS name to be able to route all the way",
    "start": "1368660",
    "end": "1375410"
  },
  {
    "text": "through the to the part in the class time and if we also test some part security policy stuff which i think is",
    "start": "1375410",
    "end": "1382760"
  },
  {
    "text": "more or less legacy now in in our case because it's also chest at upstream now but it wasn't when we introduced it last",
    "start": "1382760",
    "end": "1388280"
  },
  {
    "text": "year and the way we sorry",
    "start": "1388280",
    "end": "1397030"
  },
  {
    "text": "the day the way we run the in twenties is that we think that you have to you",
    "start": "1398020",
    "end": "1403660"
  },
  {
    "text": "want to do an upgrade from the dev channel to alpha you want to move all the things that are new in dev into",
    "start": "1403660",
    "end": "1409780"
  },
  {
    "text": "alpha then we basically have a Jenkins pipeline where we first create a cluster",
    "start": "1409780",
    "end": "1415679"
  },
  {
    "text": "based on the alpha branch because it's like the base branch then we do a",
    "start": "1415679",
    "end": "1422860"
  },
  {
    "text": "cluster update using our cluster lifecycle manager from the dev branch so we take all the changes from dev to",
    "start": "1422860",
    "end": "1428620"
  },
  {
    "text": "alpha but if there and apply these and the reason we do this is that if we just",
    "start": "1428620",
    "end": "1434890"
  },
  {
    "text": "create a cluster directly from the dev branch or from the from the merge between alpha and F then we just test",
    "start": "1434890",
    "end": "1441760"
  },
  {
    "text": "creating a new cluster from the new configuration and this might work perfectly fine but what we also want to",
    "start": "1441760",
    "end": "1447160"
  },
  {
    "text": "test is that we are actually testing doing a live migration from one maybe it's one kubernetes version to a second",
    "start": "1447160",
    "end": "1453160"
  },
  {
    "text": "one we actually want to test that this flow also works that they cluster resulting cluster is also running",
    "start": "1453160",
    "end": "1460120"
  },
  {
    "text": "correctly once we have this trust to set up based on the the dev changes then we",
    "start": "1460120",
    "end": "1468850"
  },
  {
    "text": "basically run the interest the conformance test stifles step test and and there's a Lando test and we do we",
    "start": "1468850",
    "end": "1476170"
  },
  {
    "text": "actually run we create one cluster per test case of her these three tests or",
    "start": "1476170",
    "end": "1481420"
  },
  {
    "text": "recreate three trusters in total and this is just to speed it up if one of them fails then you have would have to",
    "start": "1481420",
    "end": "1486910"
  },
  {
    "text": "like wait for for a long time to re run all the tests if they were in one",
    "start": "1486910",
    "end": "1492220"
  },
  {
    "text": "cluster if only one of the tests fails here is only one cluster we have to restart and the other test we don't have",
    "start": "1492220",
    "end": "1497620"
  },
  {
    "text": "to rerun if everything works we just delete the classes again to save cost if",
    "start": "1497620",
    "end": "1504100"
  },
  {
    "text": "everything doesn't work we leave the clusters running def a little while because then we can then we can go and",
    "start": "1504100",
    "end": "1513040"
  },
  {
    "text": "inspect and see if there's something yeah wrong with the cluster or what happened it's easier than looking at the",
    "start": "1513040",
    "end": "1518830"
  },
  {
    "text": "logs to actually to do this from a thing as part of you it just looks like this I think this one",
    "start": "1518830",
    "end": "1526210"
  },
  {
    "text": "it takes like that 20 minutes here to set up cast and update it but it of course depends on how many changes we",
    "start": "1526210",
    "end": "1532539"
  },
  {
    "text": "make and then the integration test here this London one switch takes like 10",
    "start": "1532539",
    "end": "1537909"
  },
  {
    "text": "minutes to run mostly waiting for TNS stuff so it takes like 40 minutes or so total and looking at the the output of",
    "start": "1537909",
    "end": "1547720"
  },
  {
    "text": "the entrances looks something like this this is 144 this the conformance test and it says that it skipped a bunch so",
    "start": "1547720",
    "end": "1554169"
  },
  {
    "text": "638 were skipped and this is because you can you can this is how you run the test",
    "start": "1554169",
    "end": "1561909"
  },
  {
    "text": "basically I created this image where it's just pulled out the entrances from the upstream kubernetes and only pack",
    "start": "1561909",
    "end": "1568149"
  },
  {
    "text": "this into one image so you don't have to have the holconius which is much bigger I just produced four if anyone is",
    "start": "1568149",
    "end": "1574749"
  },
  {
    "text": "interested but basically it just runs the in trenches that are obsolete when it is it has some flag - P means",
    "start": "1574749",
    "end": "1581139"
  },
  {
    "text": "parallel focus means what you want to focus on and skip what you what you",
    "start": "1581139",
    "end": "1586240"
  },
  {
    "text": "don't want so you want to focus on conformance so it's each test has this naming convention that it has this",
    "start": "1586240",
    "end": "1592929"
  },
  {
    "text": "square bracket and then some some value that you can target beta breaks basically and then you just skip the",
    "start": "1592929",
    "end": "1598419"
  },
  {
    "text": "zero which cannot be run in parallel so they have to run one by one so and and these we don't need to take a long time",
    "start": "1598419",
    "end": "1604330"
  },
  {
    "text": "so we skip skip these the stateful set similar you just pick another thing to",
    "start": "1604330",
    "end": "1610419"
  },
  {
    "text": "focus on stateful set basic here and stable set the one with the Redis and",
    "start": "1610419",
    "end": "1615899"
  },
  {
    "text": "pretty much you can if you find some tests that would test something good for you can just pick it out and and and use",
    "start": "1615899",
    "end": "1622600"
  },
  {
    "text": "that yeah this way a few hints for running instances that we found is that",
    "start": "1622600",
    "end": "1629649"
  },
  {
    "start": "1626000",
    "end": "1626000"
  },
  {
    "text": "it's possible to run with flake attempts this is a flex to the chest",
    "start": "1629649",
    "end": "1634960"
  },
  {
    "text": "orchestration thing it's called Kinkos encore and just the framework but flaky",
    "start": "1634960",
    "end": "1640749"
  },
  {
    "text": "at Sims basically means that it will if it's if the test first fails then we will actually try to run it again and this is useful because some of the tests",
    "start": "1640749",
    "end": "1647769"
  },
  {
    "text": "are just creating a pot or creating multiple pots and since our cluster is auto scaling during the test we might",
    "start": "1647769",
    "end": "1653499"
  },
  {
    "text": "lose a part because it's not the tests are not really built in a reliable so having the FADEC attempts to usually",
    "start": "1653499",
    "end": "1659799"
  },
  {
    "text": "it's enough to make sure that that it's working and what we're testing is the",
    "start": "1659799",
    "end": "1665049"
  },
  {
    "text": "law of chastity that works not that the interests are reliable or something so so that's why it's fine and then another idea is to update the",
    "start": "1665049",
    "end": "1673929"
  },
  {
    "text": "interent image for every major release you do you role also going from one a to 1/9 you",
    "start": "1673929",
    "end": "1679900"
  },
  {
    "text": "want to also change the in trend image you use but but going from one nine to one nine one is usually fine that's what",
    "start": "1679900",
    "end": "1687760"
  },
  {
    "text": "we found disabled program test we skip sometimes some tests are just broken in companies release and you want to skip",
    "start": "1687760",
    "end": "1694000"
  },
  {
    "text": "them and then remove all completed paths from the cube system namespace so the",
    "start": "1694000",
    "end": "1699909"
  },
  {
    "text": "entrance has to always check that the cube system namespace is everything is running in there and even if you have",
    "start": "1699909",
    "end": "1705520"
  },
  {
    "text": "like a cron job that creates a completed part then it will fail to run so you also want to make sure these are removed",
    "start": "1705520",
    "end": "1711990"
  },
  {
    "text": "ok so this was the entrance part the the",
    "start": "1711990",
    "end": "1718059"
  },
  {
    "text": "really interesting part is actually upgrading the nose because one thing is just grading faster updating it running",
    "start": "1718059",
    "end": "1723460"
  },
  {
    "text": "in 20s but if you don't have anything running inside you don't really know if the operate went really well in between",
    "start": "1723460",
    "end": "1730230"
  },
  {
    "start": "1730000",
    "end": "1730000"
  },
  {
    "text": "so first we went with a naive approach I call it where we basically just have an",
    "start": "1730230",
    "end": "1736360"
  },
  {
    "start": "1734000",
    "end": "1734000"
  },
  {
    "text": "outer skin group we set a fixed number of which we just fixed the number in the",
    "start": "1736360",
    "end": "1744250"
  },
  {
    "text": "auto skin group when you want to do an update so we kind of lock lock it so it can't auto scale and then we just do the",
    "start": "1744250",
    "end": "1750010"
  },
  {
    "text": "current plus one so we get one extra note and we basically just wait for it",
    "start": "1750010",
    "end": "1755080"
  },
  {
    "text": "obvious to bring up this extra note then we train a random node in the cluster",
    "start": "1755080",
    "end": "1760230"
  },
  {
    "text": "and this will then terminate it and it obvious will bring up a new one because you didn't remove one and the desired is",
    "start": "1760230",
    "end": "1767230"
  },
  {
    "text": "is list and that the current is less than the desired and this you do again for all the notes and this was of course",
    "start": "1767230",
    "end": "1775630"
  },
  {
    "text": "fine there was no issues with this or at all and our users were very happy almost",
    "start": "1775630",
    "end": "1781150"
  },
  {
    "text": "there was a few problems one problem is that we didn't have any",
    "start": "1781150",
    "end": "1787290"
  },
  {
    "start": "1785000",
    "end": "1785000"
  },
  {
    "text": "like time between training notes so we didn't have any specific time interval",
    "start": "1787290",
    "end": "1793090"
  },
  {
    "text": "between when when can we go to the next node so we just drained drained drained and then how long do we even wait",
    "start": "1793090",
    "end": "1800740"
  },
  {
    "text": "it was not clear another problem was that the volumes like I always set up",
    "start": "1800740",
    "end": "1807670"
  },
  {
    "text": "her availability zones so if your pot that needs a volume in one zone it can't",
    "start": "1807670",
    "end": "1814540"
  },
  {
    "text": "get scheduled to a different node so if you don't have enough nodes in a particular zone where you have the volume then you out unlock your part is",
    "start": "1814540",
    "end": "1822040"
  },
  {
    "text": "just pending and you you can't really move forward and since we didn't have any timer we just eventually removed the",
    "start": "1822040",
    "end": "1829540"
  },
  {
    "text": "next the next instance where another pot of the same application might have been",
    "start": "1829540",
    "end": "1835570"
  },
  {
    "text": "running basically killing everything at once another problem was that we fixed the",
    "start": "1835570",
    "end": "1842560"
  },
  {
    "text": "auto scaling group size so we could basically not scale in and out in between the upgrades so we were kind of",
    "start": "1842560",
    "end": "1848920"
  },
  {
    "text": "fixed and we would have like pending notes from the autoscaler which we would",
    "start": "1848920",
    "end": "1854110"
  },
  {
    "text": "have to wait until after the upgrade to actually scale out which is not good if people are deploying a lot and and a",
    "start": "1854110",
    "end": "1861580"
  },
  {
    "text": "fourth problem was that how do we actually handle stateful applications so in our case Proteus is used and if you",
    "start": "1861580",
    "end": "1869710"
  },
  {
    "text": "have the case where you have a master pot and some replicas then how do you know if you do this to drain one node",
    "start": "1869710",
    "end": "1877780"
  },
  {
    "text": "you don't know if you're actually training the master and if you're training the master then your scripts",
    "start": "1877780",
    "end": "1882880"
  },
  {
    "text": "person becomes unavailable so this is not the ideal way to ideal way to go and",
    "start": "1882880",
    "end": "1889660"
  },
  {
    "text": "so what we did was we thought okay we need to design a new upgrade strategy that is more robust to these issues and",
    "start": "1889660",
    "end": "1898230"
  },
  {
    "text": "the first thing that we thought about is okay what does the mean mean for know to be ready which is important because then",
    "start": "1898230",
    "end": "1905650"
  },
  {
    "start": "1899000",
    "end": "1899000"
  },
  {
    "text": "we can know when we can move on to the next one so I think about the notes as the kubernetes node object and the",
    "start": "1905650",
    "end": "1912580"
  },
  {
    "text": "underlying easy to instance in our case or VMO and if you split them up the the couplet",
    "start": "1912580",
    "end": "1918220"
  },
  {
    "text": "has this reporting note ready so this is easy to see if it's ready and easy to",
    "start": "1918220",
    "end": "1924040"
  },
  {
    "text": "instance if you wanted an auto scaling group you're going kind of see if it's ready by is it in service by it does a",
    "start": "1924040",
    "end": "1929440"
  },
  {
    "text": "double yes seed in service in the auto scaling group another thing you can also do is you can check if if there's an",
    "start": "1929440",
    "end": "1935560"
  },
  {
    "text": "applicator elastic load balance I attest to the auto scaling group then you can see if this is if the instance is also",
    "start": "1935560",
    "end": "1942610"
  },
  {
    "text": "in service in this load balancer and this is important for our master setup",
    "start": "1942610",
    "end": "1947710"
  },
  {
    "text": "because if we remove the masters too fast and we might remove those that were in the in the load balancer which we",
    "start": "1947710",
    "end": "1954730"
  },
  {
    "text": "don't want another thing is something that kubernetes actually provides so",
    "start": "1954730",
    "end": "1961270"
  },
  {
    "start": "1956000",
    "end": "1956000"
  },
  {
    "text": "this part disrupting packages and this is a way to say that in this case mean available one meaning that if I have",
    "start": "1961270",
    "end": "1968350"
  },
  {
    "text": "three parts then I can kill two and then I still have one available and and and",
    "start": "1968350",
    "end": "1974140"
  },
  {
    "text": "our infrastructure will not continue deleting more parts if it's disrupt the",
    "start": "1974140",
    "end": "1979990"
  },
  {
    "text": "budget basically so you have a control that evicts their part this is successful it'll fix the next one",
    "start": "1979990",
    "end": "1986920"
  },
  {
    "text": "successful and then it tries to evict the third one and since the the the budget would be disrupted if you were to",
    "start": "1986920",
    "end": "1993310"
  },
  {
    "text": "remove the last one then it fails and then also how to handle the the Postgres",
    "start": "1993310",
    "end": "2001410"
  },
  {
    "text": "stuff basically we have again this setup",
    "start": "2001410",
    "end": "2006750"
  },
  {
    "start": "2005000",
    "end": "2005000"
  },
  {
    "text": "with the master and replica we put this labels on the on the pro square spots and then we have a postage operator",
    "start": "2006750",
    "end": "2015750"
  },
  {
    "text": "which is also an open source project that our database database team are developing it's a basically where you",
    "start": "2015750",
    "end": "2022140"
  },
  {
    "text": "can define as custom resource definition for postage cluster and then it get gives you a stateful set with these",
    "start": "2022140",
    "end": "2028050"
  },
  {
    "text": "properties and manages the whole cluster for you and say here in the case we want",
    "start": "2028050",
    "end": "2034050"
  },
  {
    "text": "to drain the master then the operator will put a disruption party saying we",
    "start": "2034050",
    "end": "2039420"
  },
  {
    "text": "always has to have one master part available so if you try to drain the",
    "start": "2039420",
    "end": "2045570"
  },
  {
    "text": "master a row basically we fail the eviction and then the",
    "start": "2045570",
    "end": "2050610"
  },
  {
    "text": "prosecution operator in the meantime will see that we are doing a class to update so we see the green node is a new node and it will notice this it will",
    "start": "2050610",
    "end": "2058830"
  },
  {
    "text": "evict a random replica of the class time because this is fine just a Republika it",
    "start": "2058830",
    "end": "2064138"
  },
  {
    "text": "will move this by being evicted companies will automatically move it to a new empty pot node and then the",
    "start": "2064139",
    "end": "2071940"
  },
  {
    "text": "Postgres operator will see okay now that is a replica on a new node it makes sense to promote this to master so it",
    "start": "2071940",
    "end": "2078300"
  },
  {
    "text": "promotes this and then we did a control failover and the one that was the master",
    "start": "2078300",
    "end": "2084090"
  },
  {
    "text": "before we can now evict because it's no longer has this disruption part it that",
    "start": "2084090",
    "end": "2089190"
  },
  {
    "text": "you that you can only that you have to have one master this is already satisfied by having moved it there and",
    "start": "2089190",
    "end": "2098910"
  },
  {
    "text": "this would look something like this so you have min available one when you have a label grow master the basic way to do",
    "start": "2098910",
    "end": "2107040"
  },
  {
    "text": "it just to quickly summarize how this rolling upgrade of nodes looks so we",
    "start": "2107040",
    "end": "2113790"
  },
  {
    "start": "2108000",
    "end": "2108000"
  },
  {
    "text": "have an opal or a skin group we have a bunch of availability zones we have some",
    "start": "2113790",
    "end": "2119970"
  },
  {
    "text": "instances there we have of course the parts running and on the instances and we have this volumes persistent volumes",
    "start": "2119970",
    "end": "2126990"
  },
  {
    "text": "attached to each of these part and when we do an update we basically find all",
    "start": "2126990",
    "end": "2133860"
  },
  {
    "text": "the old nodes in the cluster we put a chain prefer no schedule this is also a kubernetes feature where you can say if",
    "start": "2133860",
    "end": "2140580"
  },
  {
    "text": "there are nodes available with without this chain then prefer to schedule the",
    "start": "2140580",
    "end": "2146100"
  },
  {
    "text": "new parts through to these new nodes and then we bring up three new nodes because",
    "start": "2146100",
    "end": "2153870"
  },
  {
    "text": "then we get one in each availability zone and even if we don't get one in each like if it's unbalanced in some way",
    "start": "2153870",
    "end": "2160200"
  },
  {
    "text": "then we actually wait so we have at least one new note in each availability zone because then we if we move parts",
    "start": "2160200",
    "end": "2167910"
  },
  {
    "text": "from an old notes when you want them we always have we will always be able to attach the persistent volume so if we",
    "start": "2167910",
    "end": "2175170"
  },
  {
    "text": "then train a node then we will company we'll move the parts to something else",
    "start": "2175170",
    "end": "2180910"
  },
  {
    "text": "and the ones that have the persistent volume will stay in the same zone but the others without a volume might go to",
    "start": "2180910",
    "end": "2187330"
  },
  {
    "text": "a difference from we could also stay in the same and then kubernetes will",
    "start": "2187330",
    "end": "2192400"
  },
  {
    "text": "basically our AWS will bring up a new instance because we changed the value in",
    "start": "2192400",
    "end": "2197620"
  },
  {
    "text": "the auto scaling group and then yeah it basically continues from there and the",
    "start": "2197620",
    "end": "2204370"
  },
  {
    "text": "way we do it is that we don't we don't fix the auto scaling group size anymore we just always find the old nodes so",
    "start": "2204370",
    "end": "2212200"
  },
  {
    "text": "even if we're in the middle of an upgrade and our controller dies it will just start up again and find all the old nodes and continue from that so our",
    "start": "2212200",
    "end": "2219070"
  },
  {
    "text": "scaling can happen it was just bring up new notes in the meantime and then we actually have less work to do in the",
    "start": "2219070",
    "end": "2224710"
  },
  {
    "text": "controller yeah of course there was a",
    "start": "2224710",
    "end": "2230170"
  },
  {
    "text": "few times where this wasn't fine I just want to mention because you can always talk about how well it goes and and one",
    "start": "2230170",
    "end": "2238270"
  },
  {
    "text": "thing was that we broke our overlay Network in our main infrastructure cluster one time because we wanted to",
    "start": "2238270",
    "end": "2243760"
  },
  {
    "text": "switch from HDD as a back-end for flannel so using the kubernetes objects",
    "start": "2243760",
    "end": "2249250"
  },
  {
    "text": "as a back-end for flannel and we of course wanted to do this live in a cluster that was running so we plan for",
    "start": "2249250",
    "end": "2255220"
  },
  {
    "text": "awhile how to actually migrate between HDD and the kubernetes as a back-end because you have to flannel has to know",
    "start": "2255220",
    "end": "2261160"
  },
  {
    "text": "about the the port network the the IP space for each node so it can actually",
    "start": "2261160",
    "end": "2266500"
  },
  {
    "text": "route between the nodes and if you just switch the configuration in the flannel",
    "start": "2266500",
    "end": "2271720"
  },
  {
    "text": "then you would have some information LCD and some in kubernetes and they wouldn't match so our idea was to migrate this",
    "start": "2271720",
    "end": "2277390"
  },
  {
    "text": "correctly and this really fails to do so basically we had on each node two",
    "start": "2277390",
    "end": "2283450"
  },
  {
    "text": "different networks one from it'sa demon from kubernetes so the paths couldn't route anywhere because they have to miss",
    "start": "2283450",
    "end": "2288910"
  },
  {
    "text": "matching networks and the reason this happened was that we basically didn't test the upgrade path that I was talking",
    "start": "2288910",
    "end": "2294910"
  },
  {
    "text": "about in our intern chest we only created a new cluster from the new configuration and this worked fine or we didn't actually create an old Rasta",
    "start": "2294910",
    "end": "2301660"
  },
  {
    "text": "updated and then just this would have caught this issue another one just not really kubernetes",
    "start": "2301660",
    "end": "2308830"
  },
  {
    "text": "rate or anything it was just that we have our images on our docker registry",
    "start": "2308830",
    "end": "2313930"
  },
  {
    "text": "internal one and when we were confident at some point that we could roll out",
    "start": "2313930",
    "end": "2319510"
  },
  {
    "text": "more it trust us at a time we still had like five at a time we could deal with then we thought that let's just remove",
    "start": "2319510",
    "end": "2325510"
  },
  {
    "text": "this limit and do like forty at a time it doesn't matter and we are confident then we started to pull so many images",
    "start": "2325510",
    "end": "2332020"
  },
  {
    "text": "from this registry that it basically took it down and this was before we have this strategy where we checked if the",
    "start": "2332020",
    "end": "2338650"
  },
  {
    "text": "notes were real ready or not so we just kept rolling out new notes that didn't get any any couplet running or anything",
    "start": "2338650",
    "end": "2345520"
  },
  {
    "text": "so yeah we basically removed everything from the faster kind of it was easy to",
    "start": "2345520",
    "end": "2350890"
  },
  {
    "text": "stop the beating because we could just remove the controller because it's a single instance so we just scale it down to zero and then everything stopped or",
    "start": "2350890",
    "end": "2357730"
  },
  {
    "text": "we could let the the the registry recover yeah this is pretty much it",
    "start": "2357730",
    "end": "2363339"
  },
  {
    "text": "I just believe some links here for us our own source particulated so kubernetes",
    "start": "2363339",
    "end": "2368760"
  },
  {
    "text": "[Music]",
    "start": "2369620",
    "end": "2372699"
  },
  {
    "start": "2370000",
    "end": "2370000"
  },
  {
    "text": "and yeah thank you very much",
    "start": "2377430",
    "end": "2381530"
  },
  {
    "text": "[Applause]",
    "start": "2382590",
    "end": "2384749"
  }
]