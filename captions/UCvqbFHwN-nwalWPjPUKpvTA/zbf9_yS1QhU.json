[
  {
    "text": "hello everyone and um I think I should um I do speak Chinese by the way but I",
    "start": "560",
    "end": "5799"
  },
  {
    "text": "use English for this talk and because I see people who probably can't speak Chinese here so you know but if you uh",
    "start": "5799",
    "end": "12280"
  },
  {
    "text": "if you want to come back and ask me questions feel free to use mry um so um",
    "start": "12280",
    "end": "18480"
  },
  {
    "text": "my name is Michael Y and uh uh this talk is about um portable large language model applications in doer and uh uh I'm",
    "start": "18480",
    "end": "25720"
  },
  {
    "text": "the maintainer of cnf's wasm edge project and I'm also a doer captain which is uh you know sort of like cncf",
    "start": "25720",
    "end": "32758"
  },
  {
    "text": "Ambassador but with ster right um so this is a ongoing collaboration between",
    "start": "32759",
    "end": "38760"
  },
  {
    "text": "uh our two projects was Mage and doer so we gave a first preview of this of this",
    "start": "38760",
    "end": "44160"
  },
  {
    "text": "work at AI Dev Con in uh in Paris about a couple months ago and there are um new",
    "start": "44160",
    "end": "52520"
  },
  {
    "text": "progress that I made which I thought was uh is a really important subject you",
    "start": "52520",
    "end": "57600"
  },
  {
    "text": "know because um as you know cncf was uh and doer was the OG cncf you know some",
    "start": "57600",
    "end": "64838"
  },
  {
    "text": "someone may argue you know that's the whole Cloud native Trend was started with um you know containerization and uh",
    "start": "64839",
    "end": "71560"
  },
  {
    "text": "with stalker tools being the first pioneer of that right so um how do you run large language models in the in the",
    "start": "71560",
    "end": "79240"
  },
  {
    "text": "new era of large language models how do we do this with container and uh there's different ways to do that so we uh we",
    "start": "79240",
    "end": "85439"
  },
  {
    "text": "hope to provide a overview of how how things are going and uh um you know what",
    "start": "85439",
    "end": "90520"
  },
  {
    "text": "are the uh efforts that happens in both communities to make to make it happen right so uh first of all you know that's",
    "start": "90520",
    "end": "99040"
  },
  {
    "text": "um I I think all of you guys use Docker is that correct you know is there anyone who's not using",
    "start": "99040",
    "end": "105240"
  },
  {
    "text": "Docker okay nobody 100% it's [Laughter] okay if you look at the developer tools",
    "start": "105240",
    "end": "113479"
  },
  {
    "text": "you know I I find this quite surprising you know because this is a a stock overflow survey um you know they they",
    "start": "113479",
    "end": "120039"
  },
  {
    "text": "just release this uh this result this year so they release result every year you know one of the consistent result is",
    "start": "120039",
    "end": "126280"
  },
  {
    "text": "rust is always almost most loved programming language and uh in the past",
    "start": "126280",
    "end": "131360"
  },
  {
    "text": "couple years stalker has been pretty high in uh uh the most popular developer tools right you know so it's uh I'm",
    "start": "131360",
    "end": "137280"
  },
  {
    "text": "actually somewhat surprised to find out it's more popular than mpm you know because you'd think a lot of you know",
    "start": "137280",
    "end": "143040"
  },
  {
    "text": "there's every single Java developers use mpm right every single python developers using pip so you know there's a um you",
    "start": "143040",
    "end": "151480"
  },
  {
    "text": "so I mean there's over half of the developers who said that they are already using darker so to have the",
    "start": "151480",
    "end": "158239"
  },
  {
    "text": "modern um applications or modern large language applications available in doer is probably one of the um most impactful",
    "start": "158239",
    "end": "165959"
  },
  {
    "text": "things that that we could do because it's going to affect a large number of developers",
    "start": "165959",
    "end": "171519"
  },
  {
    "text": "right but before we go to the details of how um how we um enable this in Darker I",
    "start": "171519",
    "end": "179440"
  },
  {
    "text": "want to take take a step back and uh um because I have talked about this a lot you know in our Boo and when I when we",
    "start": "179440",
    "end": "185400"
  },
  {
    "text": "talk to people you know is the current state for application development in large language model applications and",
    "start": "185400",
    "end": "191760"
  },
  {
    "text": "today because large language model started almost two years ago with open AI it was the chat gbt was a HTTP based",
    "start": "191760",
    "end": "199480"
  },
  {
    "text": "web interface so this mode of development has persisted throughout today so in the in the clown native",
    "start": "199480",
    "end": "206799"
  },
  {
    "text": "jargon or in the in the uh Enterprise architecture we call this a side card pattern right you know so you have some",
    "start": "206799",
    "end": "212959"
  },
  {
    "text": "kind of applications that has um you know um",
    "start": "212959",
    "end": "220400"
  },
  {
    "text": "uh um you have a large you have API server that is running the large",
    "start": "220400",
    "end": "226400"
  },
  {
    "text": "language model so um it used to be just open ey or chat GPT and now there's",
    "start": "226400",
    "end": "231680"
  },
  {
    "text": "other you know Cloud providers or centralized providers and as the open",
    "start": "231680",
    "end": "237000"
  },
  {
    "text": "source models come along you have uh projects like AMA and then you know even on wasm edge we have project called",
    "start": "237000",
    "end": "243319"
  },
  {
    "text": "llama Edge the um and even llama CBP you know all those open source projects all those SAS providers all those chatbot",
    "start": "243319",
    "end": "250840"
  },
  {
    "text": "providers they all have some kind of HTTP API that allow you to call into the large language model and this piece of",
    "start": "250840",
    "end": "257239"
  },
  {
    "text": "software typically runs on GPU and npu and TPU architecture right so it is a",
    "start": "257239",
    "end": "262720"
  },
  {
    "text": "separate part that's segmented from the rest of the cloud and then in the cloud you have the real application that",
    "start": "262720",
    "end": "269360"
  },
  {
    "text": "interact act with a large language model so um as time goes on I think in the past two years there has been tremendous",
    "start": "269360",
    "end": "275240"
  },
  {
    "text": "amount of development um around those applications so those applications um",
    "start": "275240",
    "end": "280320"
  },
  {
    "text": "you know the representative framework of those applications they have you have land chain you have LV index you have",
    "start": "280320",
    "end": "285960"
  },
  {
    "text": "those um you know there are lots of Frameworks out there you have dii you know which is really popular in China",
    "start": "285960",
    "end": "291199"
  },
  {
    "text": "and you have you know so those applications help you manage the prompts you know the system promt you know how",
    "start": "291199",
    "end": "297199"
  },
  {
    "text": "to instruct the model to do things the work flows when the model returns something how do you handle it you know",
    "start": "297199",
    "end": "302800"
  },
  {
    "text": "the function calls sometimes you don't want the model to respond in natural language but you'd rather the model to",
    "start": "302800",
    "end": "307919"
  },
  {
    "text": "respond in code you know if I want to do something just to send me the say if I tell the model I want to fly this strong",
    "start": "307919",
    "end": "314759"
  },
  {
    "text": "the model should return a function call a piece of python code that can be directly upload to the Drone and perform",
    "start": "314759",
    "end": "320199"
  },
  {
    "text": "the function that I just described right sometimes you want the model to Output wed Json format in in a in a",
    "start": "320199",
    "end": "327080"
  },
  {
    "text": "certain format or something that has a grammar component to it there's um say if you have a large knowledge base you",
    "start": "327080",
    "end": "333600"
  },
  {
    "text": "want to um you know um build a vector database around it and uh you want to",
    "start": "333600",
    "end": "338919"
  },
  {
    "text": "chunk the the the the text or the knowledge into small pieces so that it can be retrieved later and once it",
    "start": "338919",
    "end": "345520"
  },
  {
    "text": "retrieved you can put that into prompt so that to reduce the hallucination of the model and to make the model better",
    "start": "345520",
    "end": "350560"
  },
  {
    "text": "right so there's a um I wouldn't go into detail because that's entirely out of the scope of this stock but there's a",
    "start": "350560",
    "end": "357360"
  },
  {
    "text": "huge amount of work that have been done in in all programming languages including python JavaScript rust and all",
    "start": "357360",
    "end": "363639"
  },
  {
    "text": "that that's designed to build middleware components that interact with a large language model right so those things are",
    "start": "363639",
    "end": "371360"
  },
  {
    "text": "typically put in Docker containers and those things are typically managed by kubernetes in a CPU based cluster okay",
    "start": "371360",
    "end": "378240"
  },
  {
    "text": "so you have this uh two separate um uh components of that made up the entire",
    "start": "378240",
    "end": "385360"
  },
  {
    "text": "application so the here is the business logic or the application logic and there is the large language model and uh um",
    "start": "385360",
    "end": "392280"
  },
  {
    "text": "like I said in kubernetes we call this a side car pattern you know that you have application and something else that",
    "start": "392280",
    "end": "397840"
  },
  {
    "text": "provide service to like the database you know things like that that that's that",
    "start": "397840",
    "end": "403319"
  },
  {
    "text": "provide service to the to the large to the to the application right however this model",
    "start": "403319",
    "end": "410560"
  },
  {
    "text": "is has come under tremendous amount of challenge recently you know this those",
    "start": "410560",
    "end": "416319"
  },
  {
    "text": "tws about I think two years ago right you know that's when you took over Twitter he was very against the whole",
    "start": "416319",
    "end": "422479"
  },
  {
    "text": "notion of R services microservices and all that stuff right you know so he's um",
    "start": "422479",
    "end": "427800"
  },
  {
    "text": "at the time I think a lot of people predicted that you know Twitter would cease to exist the techn the technology",
    "start": "427800",
    "end": "432879"
  },
  {
    "text": "infrastructure would would be completely gone but two years later you know the",
    "start": "432879",
    "end": "437960"
  },
  {
    "text": "company has laid off 80% of their employees and reduced most of the services did what he said you know and",
    "start": "437960",
    "end": "445440"
  },
  {
    "text": "the service still exists right so that gets a lot of people thinking you know is a notion that we need a lot of",
    "start": "445440",
    "end": "451919"
  },
  {
    "text": "containers a lot of services a lot of side cars all connected through HTTP or",
    "start": "451919",
    "end": "457080"
  },
  {
    "text": "RPC connections is that the right architecture right so let's set aside because that's I think that's very",
    "start": "457080",
    "end": "462720"
  },
  {
    "text": "controversial so we set aside this argument you know that's uh so maybe in the in the traditional application we",
    "start": "462720",
    "end": "467840"
  },
  {
    "text": "still need a lot of microservices but the the argument I want to make is that in the large language model applications",
    "start": "467840",
    "end": "475440"
  },
  {
    "text": "this side car RPC pattern is definitely wrong why because that promotes a idea",
    "start": "475440",
    "end": "482039"
  },
  {
    "text": "that was raised from the architecture pattern of loosely coupled components Loosely couple components is great if",
    "start": "482039",
    "end": "489159"
  },
  {
    "text": "your all your components are fairly mature so you have database that speak SQL you know and you have you know",
    "start": "489159",
    "end": "494720"
  },
  {
    "text": "that's um the application has a SQL driver jdbc obbc right and uh so you can",
    "start": "494720",
    "end": "499879"
  },
  {
    "text": "upgrade them independently you can upgrade the application without much consideration of what happened on the on",
    "start": "499879",
    "end": "505840"
  },
  {
    "text": "on the database that's the definition of loose coupling right they're not coupled so can they can each be managed and",
    "start": "505840",
    "end": "511400"
  },
  {
    "text": "scaled and upgraded separately however in the large language model this is the",
    "start": "511400",
    "end": "517080"
  },
  {
    "text": "opposite of the truth you know so because the large language model has so many particulars it's the application",
    "start": "517080",
    "end": "524039"
  },
  {
    "text": "has to be built around it the application cannot be built by the side of it so I listed some you know um uh",
    "start": "524039",
    "end": "531480"
  },
  {
    "text": "important points that um as a application developer for large language models that we have seen in our community that people have to do so for",
    "start": "531480",
    "end": "538360"
  },
  {
    "text": "instance you know the the version of the model must be tightly matched to the WR",
    "start": "538360",
    "end": "543640"
  },
  {
    "text": "time versions let's give you an example so when uh Microsoft released their new",
    "start": "543640",
    "end": "548720"
  },
  {
    "text": "uh open source model called 53 um there's um all the previous version of",
    "start": "548720",
    "end": "553880"
  },
  {
    "text": "the say llama cppp which is a very popular around time would cease to U would cease to work on that particular",
    "start": "553880",
    "end": "559800"
  },
  {
    "text": "model because it use a slightly different encoder so you will need a very specific version of ll. CBP and",
    "start": "559800",
    "end": "566640"
  },
  {
    "text": "going onward in order to use that model ever from that point on there was a bug",
    "start": "566640",
    "end": "572680"
  },
  {
    "text": "that was introduced that would break the Google scamma model okay so there's no single run time that can run both model",
    "start": "572680",
    "end": "579160"
  },
  {
    "text": "anymore so just imagine if you have a rpcc service that runs by AMA say you",
    "start": "579160",
    "end": "584519"
  },
  {
    "text": "run you you upgraded the wrong time and suddenly there's at least one of those popular models that you can't run",
    "start": "584519",
    "end": "590200"
  },
  {
    "text": "anymore so the model has to be the the version of the model and the Quant the quantization um level of the model must",
    "start": "590200",
    "end": "597240"
  },
  {
    "text": "be exactly matched to the WR time Merion the problem is so severe that um project Lama CPP have multiple release per day",
    "start": "597240",
    "end": "605440"
  },
  {
    "text": "we're not talking about multiple release you know we talk about Linux release lus Go on stage and said you know six weeks",
    "start": "605440",
    "end": "611560"
  },
  {
    "text": "per release that was a huge achievement the if you go to ll. CPP GitHub",
    "start": "611560",
    "end": "616760"
  },
  {
    "text": "repository they have at least six releases per day each fix a different bug and break something else okay so",
    "start": "616760",
    "end": "623720"
  },
  {
    "text": "it's a very fast evolving field so you know so the mo model version and longtime version is a huge issue you",
    "start": "623720",
    "end": "628800"
  },
  {
    "text": "know that's it's they have to be perfectly matched you can't say the wrong time is Loosely",
    "start": "628800",
    "end": "634079"
  },
  {
    "text": "coupled with the model you know it's there no such thing they they are very tightly coupled and different models",
    "start": "634079",
    "end": "639880"
  },
  {
    "text": "require very different prompts you know so um that's people have discovered the hard way you know when um when open I",
    "start": "639880",
    "end": "646240"
  },
  {
    "text": "upgraded from the uh GPD 3.5 to gbd4 right you know that's uh the old prom",
    "start": "646240",
    "end": "651839"
  },
  {
    "text": "doesn't work anymore because the mo the new model has a new capability to understand new things so you can do more",
    "start": "651839",
    "end": "657200"
  },
  {
    "text": "things but some of the old things no longer does so you know so each model has its unique promp templates how you",
    "start": "657200",
    "end": "663880"
  },
  {
    "text": "construct the the the conversation history is that in Json or is it in some kind of XML format each model when it's",
    "start": "663880",
    "end": "671200"
  },
  {
    "text": "trained has its own specific format and some models can follow instructions in system prompt better some models you can",
    "start": "671200",
    "end": "677560"
  },
  {
    "text": "you can put a lot of instructions in system prompt so for instance you can tell you are rust programmer so now I'm going to ask you to review rust code",
    "start": "677560",
    "end": "684440"
  },
  {
    "text": "some model can follow this instruction some model can't follow this type of instruction at all right you know so the",
    "start": "684440",
    "end": "689920"
  },
  {
    "text": "the the prom have to be tailored to the model and some models can follow complex instructions and the chain of Reason",
    "start": "689920",
    "end": "695760"
  },
  {
    "text": "chain of thoughts chain of reason some model cannot right so the prompt that you you feed into the model going to be",
    "start": "695760",
    "end": "701600"
  },
  {
    "text": "very different different kind of differ on the model by model basis different models also require",
    "start": "701600",
    "end": "708480"
  },
  {
    "text": "different preparations of the knowledge base and to use the the rack technique is one of the things that's I think the",
    "start": "708480",
    "end": "715200"
  },
  {
    "text": "most significant Innovation advances in in large language model applications in the recent years so basically in order",
    "start": "715200",
    "end": "721320"
  },
  {
    "text": "to reduce the hallucination of the model you g you do not uh rely on the model to record knowledge but you give it you",
    "start": "721320",
    "end": "728480"
  },
  {
    "text": "supplement it with external knowledge the external knowledge I'll give an example very quickly you know that's uh",
    "start": "728480",
    "end": "735320"
  },
  {
    "text": "so the the external knowledge is stored in a vector database and that Vector how do you prepare that Vector database is",
    "start": "735320",
    "end": "741680"
  },
  {
    "text": "highly dependent on the model because what is the text size that you can um you know each chunk of knowledge going",
    "start": "741680",
    "end": "748360"
  },
  {
    "text": "to be is that 100 words 200 words a thousand words can can you put in a whole chapter of books into the",
    "start": "748360",
    "end": "754760"
  },
  {
    "text": "knowledge base so that it can be recalled later you know so different models going to have very different characteristics when you prepare this",
    "start": "754760",
    "end": "761120"
  },
  {
    "text": "knowledge base so this knowledge base is also tightly coupled with model if you say I want a Model A that's you need the",
    "start": "761120",
    "end": "768320"
  },
  {
    "text": "knowledge base to be prepared in a way in a way that the model A would accept right so different models also have very",
    "start": "768320",
    "end": "774320"
  },
  {
    "text": "different post-processing responses you know so when the model is very good at Genera Json when fine tuned for that",
    "start": "774320",
    "end": "780160"
  },
  {
    "text": "it's uh you know it going to perform a lot better for say coding tasks or agent tasks you know things like that so when",
    "start": "780160",
    "end": "787320"
  },
  {
    "text": "I say all this is that you know the point I'm trying to make is really the the model the the development model that",
    "start": "787320",
    "end": "793399"
  },
  {
    "text": "I showed you in the in the previous slide is um I think it's it's good for experimentation you know when you have a",
    "start": "793399",
    "end": "799720"
  },
  {
    "text": "lot of large language models that you want to experiment that you want to try different workflows and you know things like that but it's definitely wrong for",
    "start": "799720",
    "end": "806360"
  },
  {
    "text": "production system because production system you do not build those two side by side you build those you build this",
    "start": "806360",
    "end": "812480"
  },
  {
    "text": "stuff around this stuff means you have to pull this in here to have the entire",
    "start": "812480",
    "end": "818760"
  },
  {
    "text": "application to have an enti application that's with the large language model embedded in it okay so that when you",
    "start": "818760",
    "end": "824560"
  },
  {
    "text": "upgrade you upgrade the whole thing if you don't if there's no such thing as you can upgrade the model without",
    "start": "824560",
    "end": "829880"
  },
  {
    "text": "without upgrading the framework in the the workflow right so that gives a",
    "start": "829880",
    "end": "835199"
  },
  {
    "text": "strong reason to use tools like Docker right so large language models are",
    "start": "835199",
    "end": "841880"
  },
  {
    "text": "fundamentally tily coupled we need to embed the large language model into the application not put it on the side of it",
    "start": "841880",
    "end": "849000"
  },
  {
    "text": "okay so that's if there's one take home points take away points from this talk I want you to remember this okay and um",
    "start": "849000",
    "end": "856920"
  },
  {
    "text": "that's really is the original idea of Docker right you know because it's to",
    "start": "856920",
    "end": "862399"
  },
  {
    "text": "the developer needs to build something and when you ship it you would know the customer can use it can run it you know",
    "start": "862399",
    "end": "868360"
  },
  {
    "text": "you can't um you know build application using L chain and llama index and then have",
    "start": "868360",
    "end": "873800"
  },
  {
    "text": "complex instructions to the to the to the customer to say that you have to set up your vector database this way you",
    "start": "873800",
    "end": "880360"
  },
  {
    "text": "have to have the WR time that b 1724 that the exact WR time that release",
    "start": "880360",
    "end": "885519"
  },
  {
    "text": "August 16th in the afternoon on GitHub repository of llamas. CB you have to download that exactly wrong time and you",
    "start": "885519",
    "end": "892399"
  },
  {
    "text": "have to download that exact model that works with that wrong time right so all those things were developer have to",
    "start": "892399",
    "end": "899360"
  },
  {
    "text": "solve pre- doer right you know with a container you know container is designed to solve that so I think you know um to",
    "start": "899360",
    "end": "905800"
  },
  {
    "text": "have new way of containerization around large language model to have large language model embedded in the container",
    "start": "905800",
    "end": "912079"
  },
  {
    "text": "is one of the um you know I think is one of the most important things that we can do to make those applications truly",
    "start": "912079",
    "end": "918160"
  },
  {
    "text": "Cloud native yeah so now I want to show you a demo because I think there's enough talking you know",
    "start": "918160",
    "end": "924519"
  },
  {
    "text": "that's does it work you know so you talked about using Docker to package deliver a complete large language model",
    "start": "924519",
    "end": "930959"
  },
  {
    "text": "application so let me show you and um okay so the demo is really easy you",
    "start": "930959",
    "end": "939319"
  },
  {
    "text": "know it's just the two commands the first is the docker wrong command because I already pulled the docker",
    "start": "939319",
    "end": "944959"
  },
  {
    "text": "image so I'll explain it a little when I when when it starts up so I just run",
    "start": "944959",
    "end": "950600"
  },
  {
    "text": "this this is on my Mac okay so Docker on the Mac as you know it's a it's a it's a",
    "start": "950600",
    "end": "955839"
  },
  {
    "text": "virtualized environment okay let me",
    "start": "955839",
    "end": "960920"
  },
  {
    "text": "really did I forgot",
    "start": "962880",
    "end": "966680"
  },
  {
    "text": "to did I forgot to stop it yeah can I",
    "start": "974560",
    "end": "979600"
  },
  {
    "text": "oh I need to remove it isn't",
    "start": "979600",
    "end": "983720"
  },
  {
    "text": "it okay I think I should okay so this stalker image",
    "start": "986399",
    "end": "992480"
  },
  {
    "text": "contains a a a a large language model from Alibaba called Chan wi right it's",
    "start": "992480",
    "end": "997920"
  },
  {
    "text": "the smallest version of Chan one it's so if you if you think about Lama 3 the smallest version is 7 billion parameters",
    "start": "997920",
    "end": "1005240"
  },
  {
    "text": "this chair model only have half a billion parameter okay and it comes with a knowledge base what is knowledge what",
    "start": "1005240",
    "end": "1012120"
  },
  {
    "text": "is knowledge base the knowledge base is from the official document documentation",
    "start": "1012120",
    "end": "1017800"
  },
  {
    "text": "of the rust programming language okay so the goal that I want to do is that I want to build a large language model",
    "start": "1017800",
    "end": "1024240"
  },
  {
    "text": "that is understanding of rust because it has the external knowledge based of that and it would be able to answer questions",
    "start": "1024240",
    "end": "1029959"
  },
  {
    "text": "about rust so and we also have you know um we can build an entire application to help it review rust the code actually we",
    "start": "1029959",
    "end": "1036880"
  },
  {
    "text": "are using setup like this in our own GitHub repository if you send a pool request into our own GitHub repository",
    "start": "1036880",
    "end": "1043160"
  },
  {
    "text": "is aot would uh would would just review your code and tell you you know how how",
    "start": "1043160",
    "end": "1048600"
  },
  {
    "text": "it things things are right or wrong right you know so so there's a not so this in this stalker image you have the",
    "start": "1048600",
    "end": "1055039"
  },
  {
    "text": "whole thing about the VOR database the the knowledge base about R you know",
    "start": "1055039",
    "end": "1062760"
  },
  {
    "text": "about knowledge about the Ros programming language and a very small large language model and this this",
    "start": "1062760",
    "end": "1069080"
  },
  {
    "text": "combination was um was chosen on purpose because we want the large language model",
    "start": "1069080",
    "end": "1074799"
  },
  {
    "text": "to have as little knowledge of its own as possible okay so if you have a large",
    "start": "1074799",
    "end": "1081200"
  },
  {
    "text": "language model that has a huge number of parameters like the one that open ey has it has two things one is has a lot of",
    "start": "1081200",
    "end": "1087400"
  },
  {
    "text": "memory it has a it memorized a lot of things from internet the second is that it has strong reasoning capabilities",
    "start": "1087400",
    "end": "1093919"
  },
  {
    "text": "because you know um you just have more nework connections so um I think over",
    "start": "1093919",
    "end": "1099559"
  },
  {
    "text": "years people have discovered when I say over years it's only just 18 months you know something like that over the months",
    "start": "1099559",
    "end": "1105440"
  },
  {
    "text": "past months people have discovered you can distill the large language Model A large model into a smaller model while",
    "start": "1105440",
    "end": "1112320"
  },
  {
    "text": "preserving the reasoning capabilities so the reasoning capabilities does not degrade much but the the the the actual",
    "start": "1112320",
    "end": "1119000"
  },
  {
    "text": "knowledge in the model becomes much less and that is a good thing because if I",
    "start": "1119000",
    "end": "1124120"
  },
  {
    "text": "ask the model about something about uh obscure topic like a rust programming language in all likelihood if you ask",
    "start": "1124120",
    "end": "1130840"
  },
  {
    "text": "open eyes this it going to answer using python syntax because it has so much you",
    "start": "1130840",
    "end": "1136360"
  },
  {
    "text": "know knowledge about Python and how to solve this particular problem in Python it going to give you um highly",
    "start": "1136360",
    "end": "1142919"
  },
  {
    "text": "hallucinated or contaminated results right so by using a very small language",
    "start": "1142919",
    "end": "1148080"
  },
  {
    "text": "model that by itself it it probably doesn't have much knowledge about either python or rust but I supplemented with",
    "start": "1148080",
    "end": "1155120"
  },
  {
    "text": "rust knowledge base to build an entire application around it so the the um if",
    "start": "1155120",
    "end": "1161760"
  },
  {
    "text": "my description show shows that I chose the model and then I build the entire",
    "start": "1161760",
    "end": "1167120"
  },
  {
    "text": "application around the model the the application was built for the purpose of this model right it's not I can't just",
    "start": "1167120",
    "end": "1173559"
  },
  {
    "text": "switch the model to something else it probably wouldn't work because the um because the whole application was built",
    "start": "1173559",
    "end": "1179440"
  },
  {
    "text": "on on on the Cs that I going to have uh supplemented python knowledge that fit into the context window of this",
    "start": "1179440",
    "end": "1185960"
  },
  {
    "text": "particular model and then have this model answer python Rel uh rust related questions so once I so now the container",
    "start": "1185960",
    "end": "1194039"
  },
  {
    "text": "you know as we speak as the container has started a long time ago so now because the container starts a server",
    "start": "1194039",
    "end": "1201159"
  },
  {
    "text": "and map to the Local Host 8080 so what I going to do is I going to do Local Host",
    "start": "1201159",
    "end": "1207559"
  },
  {
    "text": "880 to load it and then I go to the chatot interface you know it's the",
    "start": "1207559",
    "end": "1213000"
  },
  {
    "text": "chatot interface is just for the convenience you know that's um you know people um you know that's",
    "start": "1213000",
    "end": "1220400"
  },
  {
    "text": "um so what I going to do is that I'm going to ask a question how it's do",
    "start": "1220400",
    "end": "1229280"
  },
  {
    "text": "I sorry convert",
    "start": "1229280",
    "end": "1234600"
  },
  {
    "text": "a string to an integer",
    "start": "1234600",
    "end": "1242440"
  },
  {
    "text": "number okay so it goes to the so the the",
    "start": "1242440",
    "end": "1249559"
  },
  {
    "text": "docker application has its own web server and you and and the and the HTML Pages that's associated with that so",
    "start": "1249559",
    "end": "1255919"
  },
  {
    "text": "draun locally and when I ask this question on my own web uh on my own web page what it going to do is that it's",
    "start": "1255919",
    "end": "1261880"
  },
  {
    "text": "going to go to the um you know it's going to go to the docker application with a large language model and",
    "start": "1261880",
    "end": "1267880"
  },
  {
    "text": "knowledge based embedded in it and ask and ask to answer this question right so it happens completely locally and",
    "start": "1267880",
    "end": "1273159"
  },
  {
    "text": "happens you know with Tailor Made for This model and you can see if I if you ask chat GPT this question what it going",
    "start": "1273159",
    "end": "1281360"
  },
  {
    "text": "to tell you it may give you the right answer but definitely going to give you the answer in Python you're not going to",
    "start": "1281360",
    "end": "1286480"
  },
  {
    "text": "give you the answer in Rust but because this model does not have knowledge about",
    "start": "1286480",
    "end": "1292080"
  },
  {
    "text": "or does not have much knowledge about the rest of the world all it knowledge come from the the the roster",
    "start": "1292080",
    "end": "1298400"
  },
  {
    "text": "documentation that I buil into the knowledge base that feed into it right so it automatically knows that how do I",
    "start": "1298400",
    "end": "1304799"
  },
  {
    "text": "convert the string to an integer number that's I'm asking it to generate coding rust so it's generates well formatted",
    "start": "1304799",
    "end": "1311200"
  },
  {
    "text": "and the pretty okay rust code right you know so it's",
    "start": "1311200",
    "end": "1316840"
  },
  {
    "text": "um you know so you may wonder you know that's the the the the the turbo past",
    "start": "1316840",
    "end": "1323600"
  },
  {
    "text": "and the you know the you know where did it get the information okay so here it",
    "start": "1323600",
    "end": "1329520"
  },
  {
    "text": "get the information it gets it us the number five the number 10 right you know",
    "start": "1329520",
    "end": "1335400"
  },
  {
    "text": "that's the rust documentation that's for public available on Rust foundation's website so it takes this piece of",
    "start": "1335400",
    "end": "1342279"
  },
  {
    "text": "knowledge when I ask this question about how to convert a string into integer it does a sematic search in the knowledge",
    "start": "1342279",
    "end": "1348320"
  },
  {
    "text": "base and find this text and then it rephrase the using it language",
    "start": "1348320",
    "end": "1353640"
  },
  {
    "text": "capabilities and rephrase this text and breaks that into two um two parts of the answers right so this is um",
    "start": "1353640",
    "end": "1360960"
  },
  {
    "text": "so by this example I I I was hoping to demonstrate at least two things right",
    "start": "1360960",
    "end": "1366360"
  },
  {
    "text": "you know the the first thing really is that um um you can use a very",
    "start": "1366360",
    "end": "1374240"
  },
  {
    "text": "small you can use a very small model and have it answer something that is uh",
    "start": "1374240",
    "end": "1379279"
  },
  {
    "text": "answer topic that is actually quite complex so let me see if I can give",
    "start": "1379279",
    "end": "1387360"
  },
  {
    "text": "me and example",
    "start": "1387400",
    "end": "1392840"
  },
  {
    "text": "of passing Json",
    "start": "1392840",
    "end": "1398320"
  },
  {
    "text": "into sorry into a struct right you know so",
    "start": "1398320",
    "end": "1407278"
  },
  {
    "text": "it takes some time to start because it's uh because the model needs to be loaded into the memory but once it goes into",
    "start": "1409200",
    "end": "1415120"
  },
  {
    "text": "the memory it's remember it's running on my on my laptop without external um",
    "start": "1415120",
    "end": "1420760"
  },
  {
    "text": "power power so you know the CPU is running at a lower speed and they running inside the doer container inside",
    "start": "1420760",
    "end": "1426520"
  },
  {
    "text": "inside the mac and because doer container is not native to the Mac so it has the the the arm CPU pass through or",
    "start": "1426520",
    "end": "1432880"
  },
  {
    "text": "this or the emulation that is not using the GPU at all and by by choosing a very small model I can see that the the the",
    "start": "1432880",
    "end": "1440559"
  },
  {
    "text": "the speed that is generate text and generated code is actually I think more than reasonable you know it's uh it's",
    "start": "1440559",
    "end": "1446520"
  },
  {
    "text": "much faster than I can talk right so you know so by having a very small model running locally that I can have say that",
    "start": "1446520",
    "end": "1454000"
  },
  {
    "text": "I can have privacy I can have speed I can have you know all those nice things and uh by supplementing it with the with",
    "start": "1454000",
    "end": "1461600"
  },
  {
    "text": "a knowledge base that is tailor made for this particular model that I can you know um have it answer fairly complex",
    "start": "1461600",
    "end": "1469520"
  },
  {
    "text": "questions instead of just uh um instead of just hallucinate some",
    "start": "1469520",
    "end": "1475360"
  },
  {
    "text": "python answers to me right you know so if I don't have that knowledge base then you know that's it won't let me tell you",
    "start": "1475360",
    "end": "1481640"
  },
  {
    "text": "it wouldn't be able to answer those questions certainly not in Rust you know so that's one um that's one of the demos",
    "start": "1481640",
    "end": "1489520"
  },
  {
    "text": "I want to show and uh uh this whole project is done by a um Linux Foundation",
    "start": "1489520",
    "end": "1495919"
  },
  {
    "text": "intern um he's a um um uh last year's uh college student in",
    "start": "1495919",
    "end": "1501559"
  },
  {
    "text": "India and so we have a um so he's working on the wasam project and the",
    "start": "1501559",
    "end": "1506720"
  },
  {
    "text": "Linux Foundation pays him for the for for the internship so we tried many different things if you are interested",
    "start": "1506720",
    "end": "1512799"
  },
  {
    "text": "you can go to this GitHub issue you know so this um particular issue is just to say how do I make larg language models",
    "start": "1512799",
    "end": "1519039"
  },
  {
    "text": "more knowledgeable about rust right you know that's uh you know so do I fine tune it do I supplement it with external",
    "start": "1519039",
    "end": "1525039"
  },
  {
    "text": "knowledge where do I get those external knowledge how do I con structs the the knowledge base that's uh that are",
    "start": "1525039",
    "end": "1531760"
  },
  {
    "text": "particular to the model right you know if have a fine tuned model I may have a different knowledge base different way of constructing a knowledge base if I",
    "start": "1531760",
    "end": "1538080"
  },
  {
    "text": "have a like the Chan 0.5b model I would have another way to give it as much",
    "start": "1538080",
    "end": "1544039"
  },
  {
    "text": "knowledge context as possible right so if you're interested you can um you can go to our GitHub and follow this issue",
    "start": "1544039",
    "end": "1550120"
  },
  {
    "text": "that's there are lots of discussions in terms of you know what he tried and what's what's working what's not working",
    "start": "1550120",
    "end": "1555480"
  },
  {
    "text": "you know so it's uh U it's definitely a good learning opportunity for him but it's also a really good learning",
    "start": "1555480",
    "end": "1560640"
  },
  {
    "text": "opportunity for me as well so the um inside the docker container we have um",
    "start": "1560640",
    "end": "1568520"
  },
  {
    "text": "we sort of achieved somewhat achieved crossplatform um portability by having",
    "start": "1568520",
    "end": "1574720"
  },
  {
    "text": "the wasm edge run time as the intermediary be um you know on top of",
    "start": "1574720",
    "end": "1579919"
  },
  {
    "text": "the the um you know the native drivers so you um I wouldn't go into too much",
    "start": "1579919",
    "end": "1586399"
  },
  {
    "text": "depth because I think I'm I'm a a little bit running short on time but you know so was is a web assembly run time that",
    "start": "1586399",
    "end": "1593399"
  },
  {
    "text": "can run not only across CPUs but across gpus as well so it has so if you write a",
    "start": "1593399",
    "end": "1599399"
  },
  {
    "text": "um uh AI inference application you just write it to the API that provided by was",
    "start": "1599399",
    "end": "1604600"
  },
  {
    "text": "Mage by the way it's a standard API called wnn and it's a it's a w3c standard it's it allows you to interact",
    "start": "1604600",
    "end": "1611600"
  },
  {
    "text": "with um um the AI inference engine at a higher level at a level that above Cuda wasm Ed translates that function cost",
    "start": "1611600",
    "end": "1619240"
  },
  {
    "text": "into Cuda or into the Mac metal framework or into whatever the the hardware you are running out and uh at",
    "start": "1619240",
    "end": "1625840"
  },
  {
    "text": "wrong time so that so so all those instructions runs on GPU but being translated by wasm Edge right so llama",
    "start": "1625840",
    "end": "1632320"
  },
  {
    "text": "Edge is a is a rust application that build on top of wasm edge so so we we write Lama Edge in Rust and then compile",
    "start": "1632320",
    "end": "1639399"
  },
  {
    "text": "that into wasm to have it running on wasm edge and put all those at WR time into darker container right so that's",
    "start": "1639399",
    "end": "1646640"
  },
  {
    "text": "however even with but even with that compatibility layer darker fundamentally",
    "start": "1646640",
    "end": "1651880"
  },
  {
    "text": "is not really portable across gpus in fact it's not portable across even across CPUs but you know the because we",
    "start": "1651880",
    "end": "1658600"
  },
  {
    "text": "only have really two CPUs maybe risk five is the third so it's not may not be a big issue so when you see doer images",
    "start": "1658600",
    "end": "1665120"
  },
  {
    "text": "when they publish doer images you always see there's different versions you know there's ARM version and the um you know",
    "start": "1665120",
    "end": "1671200"
  },
  {
    "text": "the the x86 version so it's not really comp the problem become much more severe",
    "start": "1671200",
    "end": "1678440"
  },
  {
    "text": "uh in the GPU world so those are just the some of the gpus that's available in the marketplace okay and uh if you look",
    "start": "1678440",
    "end": "1684760"
  },
  {
    "text": "at what's happening in in China right now every single cloud provider has their own AI accelerator you know so",
    "start": "1684760",
    "end": "1692200"
  },
  {
    "text": "everyone not you know because I'm not a hardware person now I think this must be easy to do you know because everyone",
    "start": "1692200",
    "end": "1697720"
  },
  {
    "text": "seem to have one you know that's uh but even those you know you can have AWS one",
    "start": "1697720",
    "end": "1702840"
  },
  {
    "text": "you know that's Intel has multiple you know Intel has their GPU standard has their new engine standard has their CD",
    "start": "1702840",
    "end": "1708760"
  },
  {
    "text": "stand out is in your CPU you know there are lots just a lot of stuff so all those things um if the the traditional",
    "start": "1708760",
    "end": "1716240"
  },
  {
    "text": "Docker infrastructure because it's Ron native code you know so it would be um a nightmare to have different versions of",
    "start": "1716240",
    "end": "1724279"
  },
  {
    "text": "image for each and different drivers for each of those Target and GPU platforms right so so you know so as I mentioned",
    "start": "1724279",
    "end": "1733640"
  },
  {
    "text": "Docker is a great tool to to package those applications and make them wrong the platform that you specify but there",
    "start": "1733640",
    "end": "1739799"
  },
  {
    "text": "are just too many platforms so one of the collaborations that we had with doer is to do Docker plus WM meaning that we",
    "start": "1739799",
    "end": "1746159"
  },
  {
    "text": "get rid of the Linux container Al together we use the WM container meaning that's Docker acting as the",
    "start": "1746159",
    "end": "1752200"
  },
  {
    "text": "orchestration or the management tool so when it sees a wasum application it just runs use the wasum runtime to run it",
    "start": "1752200",
    "end": "1759080"
  },
  {
    "text": "instead of using the uh starting a Linux container to run it that completely bypasses the native there you know so",
    "start": "1759080",
    "end": "1766159"
  },
  {
    "text": "that indeed with that approach you can have cross GPU applications because wasm is cross GPU so you can have application",
    "start": "1766159",
    "end": "1773440"
  },
  {
    "text": "that R rust compiled on this Mac and then runs it on the IM media device you know I think we we see that demo in the",
    "start": "1773440",
    "end": "1780760"
  },
  {
    "text": "keynote today with with quasa right you know that's uh so you know you develop on the Mac and then upload the quasa and",
    "start": "1780760",
    "end": "1786399"
  },
  {
    "text": "it's orchestrated and and provision to a to Nvidia machine and runs there right you know because the wasn't provide",
    "start": "1786399",
    "end": "1792480"
  },
  {
    "text": "higher level abstraction than Cuda and metal you know that's um so that's one way to do it but it's uh it's also has",
    "start": "1792480",
    "end": "1799360"
  },
  {
    "text": "some disadvantages because wasm is a um it's not a operating system container so",
    "start": "1799360",
    "end": "1805200"
  },
  {
    "text": "say if you want to run a database in the wasm world you have to have a database that can compile to wasm so that means",
    "start": "1805200",
    "end": "1811799"
  },
  {
    "text": "sqlite you know things like that if you want my SQL mongod DB and you know things like that those those databases",
    "start": "1811799",
    "end": "1817919"
  },
  {
    "text": "don't compile to wasm always take a major effort to compile them to wasm so you would not be able to put them into",
    "start": "1817919",
    "end": "1822960"
  },
  {
    "text": "the same container so we go back to square one is that we have things that",
    "start": "1822960",
    "end": "1828039"
  },
  {
    "text": "are that's running on the side as a side car application that outside of the container which is something that we don't want right you know we want we",
    "start": "1828039",
    "end": "1835080"
  },
  {
    "text": "build around the large language model application have everything um packaged together in a container so we have to",
    "start": "1835080",
    "end": "1840679"
  },
  {
    "text": "look for a new way you know so that's so the container got got us solve some problems the docker plus wasm solves",
    "start": "1840679",
    "end": "1847799"
  },
  {
    "text": "some other problems and you know so they both get us halfway there and uh so then",
    "start": "1847799",
    "end": "1854600"
  },
  {
    "text": "what is um you know um in the doer Community what are people",
    "start": "1854600",
    "end": "1860080"
  },
  {
    "text": "thinking about how to solve this problem and one of the angles to solve this problem you know I think this is still",
    "start": "1860080",
    "end": "1865480"
  },
  {
    "text": "uh early in the in the experimentation phase this is a presentation we did at uh at AI Dev in in Paris right is to",
    "start": "1865480",
    "end": "1872799"
  },
  {
    "text": "have a new layer of abstraction in in Docker it's called Web GPU you know so",
    "start": "1872799",
    "end": "1878559"
  },
  {
    "text": "okay so um you may have heard of web GPU the first time I I heard about it it's also very surprising it's the same thing",
    "start": "1878559",
    "end": "1884880"
  },
  {
    "text": "when people tell me wasn't can r on the server I said what you know that's isn't that running the browser you know that's",
    "start": "1884880",
    "end": "1890399"
  },
  {
    "text": "uh it's also the same thing 20 years ago when people told me Java can run the server it's the same same reaction you know isn't that the applet language",
    "start": "1890399",
    "end": "1897320"
  },
  {
    "text": "right you know so web GPU is designed to run GPU application in the browser so",
    "start": "1897320",
    "end": "1903440"
  },
  {
    "text": "it's has a lot of interface with the JavaScript so you can call GPU uh",
    "start": "1903440",
    "end": "1908559"
  },
  {
    "text": "functions from the JavaScript right um but it's not just for the browser because someone provided a standard C",
    "start": "1908559",
    "end": "1915440"
  },
  {
    "text": "definitions for the for the API so which would allow it to run outside of the",
    "start": "1915440",
    "end": "1921000"
  },
  {
    "text": "browser because now you have not only the JavaScript binding you have the C binding and in fact it's also common to compile c-based web GPU application in",
    "start": "1921000",
    "end": "1928880"
  },
  {
    "text": "Wason you know so those two worlds start match together right you know so on the server or on the on the on the PC",
    "start": "1928880",
    "end": "1936000"
  },
  {
    "text": "outside of the browser you can still use web GPU using their C library and uh um",
    "start": "1936000",
    "end": "1941279"
  },
  {
    "text": "so um one of the angles that Docker wants to do is to provide in a GPU machine instead of having the Nvidia",
    "start": "1941279",
    "end": "1948799"
  },
  {
    "text": "container tool kit you know whatever you know those are those those elaborate tools can't we just build this as a",
    "start": "1948799",
    "end": "1954440"
  },
  {
    "text": "standard feature in doer the same way it going to expose the CPU it going to expose the GPU through a GP web GPU C",
    "start": "1954440",
    "end": "1961840"
  },
  {
    "text": "header inside Docker so every Docker application would be able to access the GPU through a standard way right you",
    "start": "1961840",
    "end": "1967279"
  },
  {
    "text": "know so that's the new abstraction and uh so Docker now have um I think a",
    "start": "1967279",
    "end": "1973159"
  },
  {
    "text": "couple employees that we engaged with and also some community members I think they are really looking for for um",
    "start": "1973159",
    "end": "1978440"
  },
  {
    "text": "people to contribute to this as well to make the web GPU uh API available from out from inside the containers without",
    "start": "1978440",
    "end": "1985200"
  },
  {
    "text": "extra tooling right so if you deploy a Docker container if it's running on the Mac it would have the web GPU interface",
    "start": "1985200",
    "end": "1991360"
  },
  {
    "text": "to the Mac uh the the Apple CTIC GPU if you run on Intel it's going to have the",
    "start": "1991360",
    "end": "1997480"
  },
  {
    "text": "access to the G or the you know the the Intel CD in the CPU so one of the demos that we gave is uh um it's um running on",
    "start": "1997480",
    "end": "2005480"
  },
  {
    "text": "was match it's uh um a whisper demo you know so it's a um",
    "start": "2005480",
    "end": "2011039"
  },
  {
    "text": "voice recognition um model that so basically um you know it's a very simple",
    "start": "2011039",
    "end": "2017279"
  },
  {
    "text": "Docker file because this is Docker plus wasm right you know so there's not no operating system you don't specify",
    "start": "2017279",
    "end": "2022519"
  },
  {
    "text": "operating system by the way I'm talking about since that's the top left okay so you know so there's from scratch the",
    "start": "2022519",
    "end": "2027760"
  },
  {
    "text": "container has nothing and you copy the model which is the CFG and mpk file and the Json file in there and then we have",
    "start": "2027760",
    "end": "2034960"
  },
  {
    "text": "a um uh application WR rust and comp to wasam that read those files and uh um",
    "start": "2034960",
    "end": "2041519"
  },
  {
    "text": "use the wasi interface which the interface I talked about that sits above CA and above now it's also above web GPU",
    "start": "2041519",
    "end": "2049079"
  },
  {
    "text": "to run those um to run those models using the web GPU API and then we we put",
    "start": "2049079",
    "end": "2055000"
  },
  {
    "text": "that into the um into the docker container and start the container it will be able to take the um you know um",
    "start": "2055000",
    "end": "2061118"
  },
  {
    "text": "take the audio file and then uh spit out the the spit out the text so there's a",
    "start": "2061119",
    "end": "2066358"
  },
  {
    "text": "whole explanation of how how to run it with a pre preview release of Docker I",
    "start": "2066359",
    "end": "2071440"
  },
  {
    "text": "think it's really a nightly build um that was web GPU enabled on your own device right you know that's um you can",
    "start": "2071440",
    "end": "2077118"
  },
  {
    "text": "run it on the Mac and see it's using the GPU so that's one of the things and uh yeah that's uh um I think it's from that",
    "start": "2077119",
    "end": "2085118"
  },
  {
    "text": "readme file so you know um um to just get get into a little more detail is",
    "start": "2085119",
    "end": "2090720"
  },
  {
    "text": "that we compiled on burn. RS which is Rost the library for web GPU into WM and",
    "start": "2090720",
    "end": "2096638"
  },
  {
    "text": "um and then have running inside the inside the docker container without the operating system just the wasm wasum",
    "start": "2096639",
    "end": "2103119"
  },
  {
    "text": "runs on the host operating system and then use the web GPU to simulate the the",
    "start": "2103119",
    "end": "2108880"
  },
  {
    "text": "um the the gpus underneath it right so um I think I think my time is up but you",
    "start": "2108880",
    "end": "2114560"
  },
  {
    "text": "know uh hopefully um I gave you a preview of you know what's happening with cross GPU compatibility especially",
    "start": "2114560",
    "end": "2121240"
  },
  {
    "text": "in the doer community and we have many other efforts that going on so for instance we are working with uh the",
    "start": "2121240",
    "end": "2126599"
  },
  {
    "text": "container d um and um we are working with the cron plus u pman group to um to",
    "start": "2126599",
    "end": "2133599"
  },
  {
    "text": "enable GPU support um you know on workloads that running on cron right you know because cron also has direct WM",
    "start": "2133599",
    "end": "2140440"
  },
  {
    "text": "integration so the same setup can can run cron and we you know but when the worst come to worst today even today",
    "start": "2140440",
    "end": "2147079"
  },
  {
    "text": "what you can do is to put WM into a Docker container and at least have the application that's has the correct",
    "start": "2147079",
    "end": "2152599"
  },
  {
    "text": "architecture that has build application around the large language model instead of by the side of it right you know so",
    "start": "2152599",
    "end": "2158480"
  },
  {
    "text": "yeah that's uh I'm happy to take any questions thank you very much and yeah",
    "start": "2158480",
    "end": "2164330"
  },
  {
    "text": "[Applause]",
    "start": "2164330",
    "end": "2169429"
  }
]