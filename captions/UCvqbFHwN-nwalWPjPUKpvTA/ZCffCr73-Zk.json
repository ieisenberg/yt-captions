[
  {
    "start": "0",
    "end": "394000"
  },
  {
    "text": "hi everyone thank you for joining our talk today",
    "start": "0",
    "end": "5439"
  },
  {
    "text": "in this talk you will learn about anal serve and how it run on quick runs on kubernetes",
    "start": "5759",
    "end": "12880"
  },
  {
    "text": "this is me alvira giraiva i'm a technical product manager from fiscal",
    "start": "12880",
    "end": "18080"
  },
  {
    "text": "and we have another speaker today she won juan and he is a technical leader at cisco",
    "start": "18080",
    "end": "26640"
  },
  {
    "text": "so let's get started in today's agenda we will cover",
    "start": "26800",
    "end": "34800"
  },
  {
    "text": "some general information about benchmarking nai some introduction into mlp community",
    "start": "34800",
    "end": "43120"
  },
  {
    "text": "and shen 1 will talk about ml proof in details about trainings and inferencing and",
    "start": "43120",
    "end": "49680"
  },
  {
    "text": "other benchmarking also we'll cover how to run anal perf on",
    "start": "49680",
    "end": "56840"
  },
  {
    "text": "kubernetes in this slide you can see",
    "start": "56840",
    "end": "64720"
  },
  {
    "text": "a huge picture of variety of tools for big data and ai this landscape shows how",
    "start": "64720",
    "end": "71920"
  },
  {
    "text": "much ai was integrated into our life and how many tools we can use nowadays",
    "start": "71920",
    "end": "78799"
  },
  {
    "text": "to support our machine learning applications and given this amount of platforms",
    "start": "78799",
    "end": "86000"
  },
  {
    "text": "and tools and infrastructure and even hardware how do we know that we run our machine",
    "start": "86000",
    "end": "91840"
  },
  {
    "text": "learning workflows in a most efficient way and how can we compare our results and",
    "start": "91840",
    "end": "98000"
  },
  {
    "text": "our efficiency and this diagram was prepared by",
    "start": "98000",
    "end": "104960"
  },
  {
    "text": "matt turk and he always created this kind of infographics each year to showcase the",
    "start": "104960",
    "end": "112840"
  },
  {
    "text": "industry",
    "start": "112840",
    "end": "115840"
  },
  {
    "text": "as i said uh given the amount of tools we have how do we know what to choose and",
    "start": "118000",
    "end": "124320"
  },
  {
    "text": "especially when it comes to performance how do we know which",
    "start": "124320",
    "end": "129920"
  },
  {
    "text": "direction to choose and what uh think you need to look in first in order to",
    "start": "129920",
    "end": "135280"
  },
  {
    "text": "improve it do you need to improve your network do you need to improve your hardware",
    "start": "135280",
    "end": "140840"
  },
  {
    "text": "accelerators or maybe something wrong with all the model code",
    "start": "140840",
    "end": "146000"
  },
  {
    "text": "so which direction to choose in order to achieve the biggest impact",
    "start": "146000",
    "end": "151920"
  },
  {
    "text": "so benchmarking is the answer to that so benchmarking helps to answer",
    "start": "154319",
    "end": "159360"
  },
  {
    "text": "the questions like how efficient are the leading companies in your niche why are they more efficient and what",
    "start": "159360",
    "end": "167360"
  },
  {
    "text": "steps we need to take to improve effectiveness across all areas of our business including",
    "start": "167360",
    "end": "173040"
  },
  {
    "text": "machine learning workloads and as well as how can we use this",
    "start": "173040",
    "end": "178239"
  },
  {
    "text": "benchmark to improve our next year evaluation and our",
    "start": "178239",
    "end": "184000"
  },
  {
    "text": "technology improvements for the next year",
    "start": "184000",
    "end": "188000"
  },
  {
    "text": "so as i said we have so many technologies and because of the complicated nature of",
    "start": "189040",
    "end": "196560"
  },
  {
    "text": "machine learning workload and infrastructure behind it as well as the models are so different",
    "start": "196560",
    "end": "204080"
  },
  {
    "text": "from one another so how can we compare the effectiveness and performance",
    "start": "204080",
    "end": "210080"
  },
  {
    "text": "if these tools are not even uh close to each other and",
    "start": "210080",
    "end": "217360"
  },
  {
    "text": "we use benchmark in ei to achieve this efficiency and effectiveness",
    "start": "217360",
    "end": "222480"
  },
  {
    "text": "improve the performance potential and through that we can even expand our business horizon",
    "start": "222480",
    "end": "229120"
  },
  {
    "text": "and it will help to motivate our staff because they will see the area",
    "start": "229120",
    "end": "235599"
  },
  {
    "text": "where they can improve the current infrastructure and it can boost the innovation",
    "start": "235599",
    "end": "243599"
  },
  {
    "text": "and inspiration in order to achieve the best results and it will give insight into the",
    "start": "243599",
    "end": "250000"
  },
  {
    "text": "present performance and save potentially operation expenses",
    "start": "250000",
    "end": "255840"
  },
  {
    "text": "so last i'm going to introduce an oper it is a fair and useful benchmarks for",
    "start": "257600",
    "end": "263600"
  },
  {
    "text": "measuring training and inference performance of machine learning hardware software and services so",
    "start": "263600",
    "end": "271759"
  },
  {
    "text": "mlport was created by a large community we have more than 60 organizations and",
    "start": "271759",
    "end": "278960"
  },
  {
    "text": "eight plus universities and more than 1 000 members in this community contributing to this",
    "start": "278960",
    "end": "287360"
  },
  {
    "text": "machine learning performance standards you can see like",
    "start": "288720",
    "end": "295120"
  },
  {
    "text": "very huge and big enterprise companies such as google cisco microsoft hp and many others",
    "start": "295120",
    "end": "301840"
  },
  {
    "text": "contributing to it as well as very prominent universities such as harvard stanford university of",
    "start": "301840",
    "end": "309600"
  },
  {
    "text": "toronto that all these communities are heavily working on the one set of",
    "start": "309600",
    "end": "317840"
  },
  {
    "text": "benchmarking that can be used across the industry so anyone can use these benchmarks and",
    "start": "317840",
    "end": "324880"
  },
  {
    "text": "can compare its machine learning performance both for the inferencing side",
    "start": "324880",
    "end": "331199"
  },
  {
    "text": "and the training side and based on this comparison you may you can make the insightful and",
    "start": "331199",
    "end": "337919"
  },
  {
    "text": "insightful and informed decisions based on which direction to go and improve your performance and",
    "start": "337919",
    "end": "346160"
  },
  {
    "text": "effectiveness as well as save the operational and business cost",
    "start": "346160",
    "end": "351520"
  },
  {
    "text": "and now i'd like to hand it over to xinhuan who can talk more about ml4 benchmarks",
    "start": "351520",
    "end": "359360"
  },
  {
    "text": "as well as how ml pro can be used on top of kubernetes because kubernetes currently is the one",
    "start": "359360",
    "end": "366160"
  },
  {
    "text": "of the most leading platform across the industry that can be around",
    "start": "366160",
    "end": "371199"
  },
  {
    "text": "both on premises on and in the cloud so opening mlperv to kubernetes",
    "start": "371199",
    "end": "376880"
  },
  {
    "text": "meaning that you can expand uh your uh benchmarking to the highest and",
    "start": "376880",
    "end": "385759"
  },
  {
    "text": "like standard level so please uh shin one uh okay",
    "start": "385759",
    "end": "394479"
  },
  {
    "start": "394000",
    "end": "671000"
  },
  {
    "text": "uh thank you azura this is jingan and uh um i am going to",
    "start": "394479",
    "end": "401280"
  },
  {
    "text": "start with a brief introduction of another benchmarks",
    "start": "401280",
    "end": "406880"
  },
  {
    "text": "so in the last few years there have been a",
    "start": "406880",
    "end": "413039"
  },
  {
    "text": "lot of efforts put in the benchmark development for machine learning systems",
    "start": "413039",
    "end": "420319"
  },
  {
    "text": "and earlier works in the field starts with",
    "start": "420840",
    "end": "426639"
  },
  {
    "text": "micro benchmarks such like deep bench that isolates and evaluates individual operators",
    "start": "426639",
    "end": "432720"
  },
  {
    "text": "in a machine learning workload well this approach is useful for evaluating",
    "start": "432720",
    "end": "439840"
  },
  {
    "text": "individual components or aspects of a machinery system people quickly realize that micro",
    "start": "440080",
    "end": "445599"
  },
  {
    "text": "benchmarks cannot reflect the complexity of",
    "start": "445599",
    "end": "451120"
  },
  {
    "text": "and challenges in a real world system so in order to address this problem people gradually move to use workloads",
    "start": "451120",
    "end": "458400"
  },
  {
    "text": "that are at real world scale and starts to evaluate a new system from",
    "start": "458400",
    "end": "463599"
  },
  {
    "text": "end to end so for example tbd and dumbbench are",
    "start": "463599",
    "end": "469599"
  },
  {
    "text": "some of the outstanding works that",
    "start": "469599",
    "end": "474400"
  },
  {
    "text": "takes those approaches and mlperv is one of the latest efforts in this field",
    "start": "476160",
    "end": "481440"
  },
  {
    "text": "a notepad is built on strength of its predecessors where it tries to combine a broad set of",
    "start": "481440",
    "end": "487280"
  },
  {
    "text": "real-world benchmark workflows along with end-to-end matrix that evaluate",
    "start": "487280",
    "end": "492639"
  },
  {
    "text": "the software and hardware stacks as a whole now purpose is also inspired by the",
    "start": "492639",
    "end": "499120"
  },
  {
    "text": "industrial consortium such like spec where the benchmarks are developed and maintained by a",
    "start": "499120",
    "end": "505360"
  },
  {
    "text": "broader organizational effort and then embraced by the respective",
    "start": "505360",
    "end": "511599"
  },
  {
    "text": "communities",
    "start": "518839",
    "end": "521839"
  },
  {
    "start": "671000",
    "end": "1131000"
  },
  {
    "text": "um okay so i apologize for the delay and we have uh technical issues here but",
    "start": "672240",
    "end": "678000"
  },
  {
    "text": "um let me review my presentation um so um the most important goal",
    "start": "678000",
    "end": "686480"
  },
  {
    "text": "of mlperv is to enable a fair and useful benchmark that means",
    "start": "686480",
    "end": "692480"
  },
  {
    "text": "it needs to reflect real world challenges in ml system",
    "start": "692480",
    "end": "698079"
  },
  {
    "text": "performance by using models and data at a real-world scale",
    "start": "698079",
    "end": "703839"
  },
  {
    "text": "mlperv also strives to make the benchmark results reliable by putting in a large amount of",
    "start": "703839",
    "end": "710320"
  },
  {
    "text": "effort in rural designs that enforces reproducibility",
    "start": "710320",
    "end": "715760"
  },
  {
    "text": "and finally mlprobe wants to be designed in a way so that it can encourage state-of-the-art",
    "start": "715760",
    "end": "721600"
  },
  {
    "text": "innovation in both research and commercial communities",
    "start": "721600",
    "end": "731839"
  },
  {
    "text": "the knowledge benchmark today consists of two main tracks that are training and",
    "start": "731839",
    "end": "737120"
  },
  {
    "text": "increase these targets the two most important tasks in our lifecycle",
    "start": "737120",
    "end": "742720"
  },
  {
    "text": "and now let's take a brief look into each of them",
    "start": "742720",
    "end": "747839"
  },
  {
    "text": "mlp training track provides eight workloads today that cover a number of popular fields and typical",
    "start": "749360",
    "end": "755760"
  },
  {
    "text": "machine learning tasks which includes image classification object detection",
    "start": "755760",
    "end": "761600"
  },
  {
    "text": "translation language modeling recommendation systems and reinforcement learning systems and",
    "start": "761600",
    "end": "768560"
  },
  {
    "text": "by the way i just want to clarify that the the list of workloads here is by no",
    "start": "768560",
    "end": "774959"
  },
  {
    "text": "means fixed at least and america's community will keep adding more",
    "start": "774959",
    "end": "780079"
  },
  {
    "text": "new workloads as new developments in the machine community become mainstream and",
    "start": "780079",
    "end": "785839"
  },
  {
    "text": "coming to our constitution the main approach we take to measure",
    "start": "785839",
    "end": "792639"
  },
  {
    "text": "performance in machine learning training is to train a model from scratch to a target quality and measure the",
    "start": "792639",
    "end": "799360"
  },
  {
    "text": "total time taken in the time training this is a very computationally expensive approach",
    "start": "799360",
    "end": "806160"
  },
  {
    "text": "and there must be a question that why don't you use simple metrics such like throughput",
    "start": "806160",
    "end": "812839"
  },
  {
    "text": "or the answer is that although more",
    "start": "812839",
    "end": "818000"
  },
  {
    "text": "affordable approaches such like throughput can give you a performance number at a given time point it",
    "start": "818000",
    "end": "826240"
  },
  {
    "text": "doesn't necessarily reflect the overall cost of training in the real world",
    "start": "826240",
    "end": "834319"
  },
  {
    "text": "so let me give you an example one important observation here is that you might be able to easily",
    "start": "837279",
    "end": "844320"
  },
  {
    "text": "increase the throughput of a machine learning system by increasing the mini bench mini batch",
    "start": "844320",
    "end": "849680"
  },
  {
    "text": "sizes in training iterations however there is a side effect which is often",
    "start": "849680",
    "end": "855680"
  },
  {
    "text": "overlooked that in some cases or in most cases actually increasing batch sizes will have a",
    "start": "855680",
    "end": "863279"
  },
  {
    "text": "mathematical impact to your optimization process that will cause your model to convert slower",
    "start": "863279",
    "end": "870560"
  },
  {
    "text": "and you might end up training a model for more epochs to reach the same quality which means that",
    "start": "870560",
    "end": "877920"
  },
  {
    "text": "you might have a higher total cost for your training job when you train at a higher throughput so comparatively",
    "start": "877920",
    "end": "885680"
  },
  {
    "text": "end-to-end training time is a relatively better measurement in a way that it targets the total cost of your whole",
    "start": "885680",
    "end": "892480"
  },
  {
    "text": "training process instead of instant performance and that also encouraged the system developers",
    "start": "892480",
    "end": "899279"
  },
  {
    "text": "to optimize their systems in the right direction",
    "start": "899279",
    "end": "904160"
  },
  {
    "text": "now let's take a look at the influence track as of today the inference track provides",
    "start": "905760",
    "end": "912480"
  },
  {
    "text": "five workloads that includes two image classification tasks two object detection tasks and a",
    "start": "912480",
    "end": "919120"
  },
  {
    "text": "translation task and more workflows are being developed at the moment and they are going to be",
    "start": "919120",
    "end": "925519"
  },
  {
    "text": "added soon in the next release similar as training an output of",
    "start": "925519",
    "end": "931360"
  },
  {
    "text": "inference tries to design a realistic set of metrics for performance measurement",
    "start": "931360",
    "end": "936880"
  },
  {
    "text": "the overall idea here is to use both quality and performance for the measurement",
    "start": "936880",
    "end": "943120"
  },
  {
    "text": "so that you do not you know over overcome compromise quality of service",
    "start": "943120",
    "end": "948320"
  },
  {
    "text": "to achieve a marginal improvement in performance and vice versa",
    "start": "948320",
    "end": "954639"
  },
  {
    "text": "in the inference world there is a wide range of different use cases and different systems that range from",
    "start": "954639",
    "end": "961040"
  },
  {
    "text": "clouds and data centers to edge and mobile devices the requirements for all those systems",
    "start": "961040",
    "end": "967120"
  },
  {
    "text": "and use cases are largely different from each other and hence we need to evaluate them on the different scenarios",
    "start": "967120",
    "end": "975759"
  },
  {
    "text": "in mlps there are four types of scenarios today the single stream scenario",
    "start": "975759",
    "end": "982639"
  },
  {
    "text": "targets those use cases where interest queries come in at a constant rate and a single",
    "start": "982639",
    "end": "989120"
  },
  {
    "text": "sample at a time this is the case very commonly seen in things like video processing",
    "start": "989120",
    "end": "995120"
  },
  {
    "text": "applications especially in edge devices such like phones and cameras",
    "start": "995120",
    "end": "1001360"
  },
  {
    "text": "the performance measurement of this scenario is to simply measure the latency of each queries the multi-stream",
    "start": "1001360",
    "end": "1009360"
  },
  {
    "text": "scenario is similar as the single stream in a way that it also involves inference queries that",
    "start": "1009360",
    "end": "1015839"
  },
  {
    "text": "comes in a streaming manner at constant rate however in this case it involves",
    "start": "1015839",
    "end": "1021279"
  },
  {
    "text": "multiple streams of data at the same time and an example case is that um say in an",
    "start": "1021279",
    "end": "1028240"
  },
  {
    "text": "autonomous driving use case where many sensors data are injected into an inference engine at the same",
    "start": "1028240",
    "end": "1034720"
  },
  {
    "text": "time and they need to be processed altogether in parallel the measurement of",
    "start": "1034720",
    "end": "1039918"
  },
  {
    "text": "multi-stream scenario is to measure the maximum number of streams possible",
    "start": "1039919",
    "end": "1045520"
  },
  {
    "text": "under the condition that the latencies can be guaranteed under a certain",
    "start": "1045520",
    "end": "1050840"
  },
  {
    "text": "threshold now there is the third scenario that is the server scenario which is most",
    "start": "1050840",
    "end": "1056080"
  },
  {
    "text": "mostly suitable for machining applications as an online service",
    "start": "1056080",
    "end": "1061360"
  },
  {
    "text": "for example recommendation services and translation applications are some of the popular use cases in this scenario",
    "start": "1061360",
    "end": "1069919"
  },
  {
    "text": "we issue queries at an arrival rate that follows a poisson distribution",
    "start": "1069919",
    "end": "1076080"
  },
  {
    "text": "which mimics the influence queries in real world then we measure the performance by",
    "start": "1076080",
    "end": "1082160"
  },
  {
    "text": "setting a latency threshold first then measure the highest qps the system can reach",
    "start": "1082160",
    "end": "1087280"
  },
  {
    "text": "on the condition that the predetermined latency threshold can be met",
    "start": "1087280",
    "end": "1092880"
  },
  {
    "text": "this can reflect the real world requirements where you want to optimize your system performance",
    "start": "1092880",
    "end": "1097919"
  },
  {
    "text": "while still meet a quality of service requirement and finally there is an offline scenario",
    "start": "1097919",
    "end": "1105600"
  },
  {
    "text": "which is also mostly suitable for a server-based system it is suitable for applications that do",
    "start": "1105600",
    "end": "1112080"
  },
  {
    "text": "not care about latency and requires to process a large batch of data at the",
    "start": "1112080",
    "end": "1117520"
  },
  {
    "text": "same time so in this scenario data are grouped into a large batch",
    "start": "1117520",
    "end": "1122559"
  },
  {
    "text": "and sent to a target system or at once and the performance is measured using",
    "start": "1122559",
    "end": "1128880"
  },
  {
    "text": "throughput numbers so now we have very briefly walk through",
    "start": "1128880",
    "end": "1135679"
  },
  {
    "start": "1131000",
    "end": "1214000"
  },
  {
    "text": "the basics about another benchmarks we want to talk about kubernetes as you know the",
    "start": "1135679",
    "end": "1143440"
  },
  {
    "text": "number of benchmarks are designed for evaluating the whole software and hardware stack of a machine learning system",
    "start": "1143440",
    "end": "1149280"
  },
  {
    "text": "and our kubernetes can certainly take a part of such a system so we want to see how we can run a",
    "start": "1149280",
    "end": "1155600"
  },
  {
    "text": "number of kubernetes and more importantly how we can leverage ml curve to optimize the",
    "start": "1155600",
    "end": "1161120"
  },
  {
    "text": "machine learning performance on a kubernetes based machine learning system so first of all let's look at training",
    "start": "1161120",
    "end": "1168880"
  },
  {
    "text": "the training workload itself has no difference between running on a bare metal environment and running on a kubernetes cluster",
    "start": "1168880",
    "end": "1176240"
  },
  {
    "text": "the main difference comes in orchestration part where you turn the model training program into a kubernetes",
    "start": "1176240",
    "end": "1183120"
  },
  {
    "text": "jaw and also on the fit of of shared shared hardware resources such as",
    "start": "1183120",
    "end": "1190480"
  },
  {
    "text": "accelerators network devices and storage devices that are now managed and",
    "start": "1190480",
    "end": "1195840"
  },
  {
    "text": "allocated by kubernetes so here are a few things that we want to take into consideration",
    "start": "1195840",
    "end": "1202480"
  },
  {
    "text": "if we want to maximize the performance when adapting to another form adapting another point of training",
    "start": "1202480",
    "end": "1209280"
  },
  {
    "text": "workload on kubernetes",
    "start": "1209280",
    "end": "1220000"
  },
  {
    "start": "1214000",
    "end": "1396000"
  },
  {
    "text": "so first of all to run uh training at real world scale you might need help from some",
    "start": "1220000",
    "end": "1225840"
  },
  {
    "text": "distributed com distributed computing libraries or frameworks such as openmpi",
    "start": "1225840",
    "end": "1231520"
  },
  {
    "text": "or horrorboard or disability tensorflow etc you might want to take some extra care",
    "start": "1231520",
    "end": "1237679"
  },
  {
    "text": "when deploying programs using those frameworks on the kubernetes cluster and luckily the support for those",
    "start": "1237679",
    "end": "1244400"
  },
  {
    "text": "distributed training do exist in kubernetes community for example you can check out powerful tools such like",
    "start": "1244400",
    "end": "1250640"
  },
  {
    "text": "control or other similar works that can help you simplify the deployment of distributed work disability training",
    "start": "1250640",
    "end": "1257440"
  },
  {
    "text": "jobs on kubernetes and talking about distribution",
    "start": "1257440",
    "end": "1262640"
  },
  {
    "text": "distributed training another important problem we want to address is the resource",
    "start": "1262640",
    "end": "1268640"
  },
  {
    "text": "scheduling now that all the compute and network and storage devices are managed by",
    "start": "1268640",
    "end": "1274080"
  },
  {
    "text": "kubernetes to maximize performance you want to make sure that you have the right affinity",
    "start": "1274080",
    "end": "1280240"
  },
  {
    "text": "settings that co-locate your kubernetes resources so here we also want to talk a bit about",
    "start": "1280240",
    "end": "1287440"
  },
  {
    "text": "topology management that can be considered as another kind of scheduling in a more micro scale",
    "start": "1287440",
    "end": "1294799"
  },
  {
    "text": "the temperature management we are talking here is basically managing the alignment of cpus and peripheral devices such as gpus and",
    "start": "1294799",
    "end": "1302799"
  },
  {
    "text": "mix within the newmar system so that you will get the best performance when you correctly align the cpus",
    "start": "1302799",
    "end": "1308799"
  },
  {
    "text": "and peripherals to the same local memory in a given water node",
    "start": "1308799",
    "end": "1314240"
  },
  {
    "text": "and also we do see that there is a better uh there's a better feature for topology",
    "start": "1314240",
    "end": "1321200"
  },
  {
    "text": "management in latest kubernetes release so this is going to be very helpful for us to leverage",
    "start": "1321200",
    "end": "1326640"
  },
  {
    "text": "the pneuma system so in addition to the problems related with distributed training",
    "start": "1326640",
    "end": "1333039"
  },
  {
    "text": "there are also other settings you might want to take care you want to have the proper plugins for",
    "start": "1333039",
    "end": "1338799"
  },
  {
    "text": "computer network devices such like gpus and nics and you might also want to set up",
    "start": "1338799",
    "end": "1344880"
  },
  {
    "text": "corresponding cni plugins for your iob network devices so that you can get support for high",
    "start": "1344880",
    "end": "1351280"
  },
  {
    "text": "performance network communications and this is also very important because",
    "start": "1351280",
    "end": "1356400"
  },
  {
    "text": "the default network setup in the vanilla kubernetes cluster is not going to be an optimal choice for",
    "start": "1356400",
    "end": "1363200"
  },
  {
    "text": "intent intensive workloads like distributing ml training and finally there are a few",
    "start": "1363200",
    "end": "1370799"
  },
  {
    "text": "other aspects which might also make some differences one of them can be the performance of your volume",
    "start": "1370799",
    "end": "1376799"
  },
  {
    "text": "access and others can be some security policy settings that might give you extra",
    "start": "1376799",
    "end": "1382000"
  },
  {
    "text": "privilege to access host resources directly so these are the main factors we",
    "start": "1382000",
    "end": "1387760"
  },
  {
    "text": "consider for running and optimizing uh mlp training workload on kubernetes",
    "start": "1387760",
    "end": "1394159"
  },
  {
    "text": "and now we talk a bit about inference there might be two ways that you can use",
    "start": "1394159",
    "end": "1399679"
  },
  {
    "start": "1396000",
    "end": "1577000"
  },
  {
    "text": "to run an inference performance test on the kubernetes cluster one of the way can be to run it like a",
    "start": "1399679",
    "end": "1407360"
  },
  {
    "text": "local job where you only test the performance of the parts specific to the",
    "start": "1407360",
    "end": "1412880"
  },
  {
    "text": "machinery task and this is the default method also as specified in on the official",
    "start": "1412880",
    "end": "1421200"
  },
  {
    "text": "mlp rules so let's first take a look at how this would work",
    "start": "1421200",
    "end": "1426640"
  },
  {
    "text": "another inference provides a load generator that works like a harness for benchmark experiments during a benchmark",
    "start": "1426640",
    "end": "1434640"
  },
  {
    "text": "experiment the load generator will first query the system of the test",
    "start": "1434640",
    "end": "1439760"
  },
  {
    "text": "or we call it sut to load a portion of test data that fits into its memory the sdut loads",
    "start": "1439760",
    "end": "1447360"
  },
  {
    "text": "the query data with help of a data module and then returns to the load generator",
    "start": "1447360",
    "end": "1453440"
  },
  {
    "text": "with ready signal then the load gen will start issuing influence queries at a",
    "start": "1453440",
    "end": "1458799"
  },
  {
    "text": "given distribution where the sut will be responsible to pass the queries to the backend",
    "start": "1458799",
    "end": "1464960"
  },
  {
    "text": "inference runner where the increased results will begin",
    "start": "1464960",
    "end": "1471039"
  },
  {
    "text": "the location will be able to load the output afterwards and then finally a post processing script will be used to",
    "start": "1471039",
    "end": "1476960"
  },
  {
    "text": "process the outputs and get the summarizer report so as you can see all this process is",
    "start": "1476960",
    "end": "1484080"
  },
  {
    "text": "happening in a local device where the influence queries and outputs are all simply transferred from memory to memory",
    "start": "1484080",
    "end": "1490880"
  },
  {
    "text": "this is an ideal way to test your performance when you want to eliminate all other factors that are now",
    "start": "1490880",
    "end": "1497520"
  },
  {
    "text": "relevant to the influence task itself however now in a kubernetes environment",
    "start": "1497520",
    "end": "1503600"
  },
  {
    "text": "it is very common to run a machine application at the service for example",
    "start": "1503600",
    "end": "1509279"
  },
  {
    "text": "you might be leveraging some inference services such as kf survey or zelda or you might be simply running",
    "start": "1509279",
    "end": "1515360"
  },
  {
    "text": "a tf server to serve your machine learning models on kubernetes and you might also want to be",
    "start": "1515360",
    "end": "1520960"
  },
  {
    "text": "interested to know that what is the end-to-end cost of your inference service",
    "start": "1520960",
    "end": "1526799"
  },
  {
    "text": "from a client point of view and in order to do that we will need to",
    "start": "1526799",
    "end": "1531840"
  },
  {
    "text": "make some modifications to the original mechanism as defining an output",
    "start": "1531840",
    "end": "1536960"
  },
  {
    "text": "so in this case we will load and serve a model in a remote machine learning service",
    "start": "1536960",
    "end": "1542400"
  },
  {
    "text": "running on kubernetes instead of directly on a local",
    "start": "1542400",
    "end": "1548240"
  },
  {
    "text": "specialty then we can implement the sut as a",
    "start": "1548240",
    "end": "1554880"
  },
  {
    "text": "client that is able to query such a community service as its back-end in this way",
    "start": "1554880",
    "end": "1562159"
  },
  {
    "text": "we will be able to get the end-to-end service level performance evaluation from this modified sut while we will",
    "start": "1562159",
    "end": "1570000"
  },
  {
    "text": "still be able to leverage the special launching behavior from ml",
    "start": "1570000",
    "end": "1576080"
  },
  {
    "start": "1577000",
    "end": "1709000"
  },
  {
    "text": "so finally we want to show some example performance results from a very simple very more experiment we did before we go",
    "start": "1577919",
    "end": "1586000"
  },
  {
    "text": "into details i just want to clarify that the experiment settings and the target requirements are all fitted to our",
    "start": "1586000",
    "end": "1592880"
  },
  {
    "text": "specific testing environment and are not directly aligned with mlk's official rules",
    "start": "1592880",
    "end": "1598240"
  },
  {
    "text": "so please just take it as you know just a demonstration only and do not",
    "start": "1598240",
    "end": "1603600"
  },
  {
    "text": "compare it with official amount of results",
    "start": "1603600",
    "end": "1608400"
  },
  {
    "text": "in this experiment we took the ssd mobile mat workload and run it on a vanilla",
    "start": "1608640",
    "end": "1614159"
  },
  {
    "text": "kubernetes cluster that uses default network setup and uses cpus only for the influencers",
    "start": "1614159",
    "end": "1621360"
  },
  {
    "text": "the experiments are done on three scenarios which are single stream server and offline",
    "start": "1621360",
    "end": "1628320"
  },
  {
    "text": "and we compare the results from the two experiment settings we talked about just now",
    "start": "1628320",
    "end": "1634639"
  },
  {
    "text": "one of them being the machining only test which is tensorflow locally and another",
    "start": "1634960",
    "end": "1642240"
  },
  {
    "text": "of them is the enter end service level test that uses tf serving to serve a model in",
    "start": "1642240",
    "end": "1649120"
  },
  {
    "text": "the kubernetes cluster and use the client site to query it",
    "start": "1649120",
    "end": "1654240"
  },
  {
    "text": "from the charts we can see that there is maybe 10 to 15 percent difference between the two",
    "start": "1654240",
    "end": "1660080"
  },
  {
    "text": "settings and both in the single stream latency and in the",
    "start": "1660080",
    "end": "1665760"
  },
  {
    "text": "server's qps numbers and the delta is even a bit larger in",
    "start": "1665760",
    "end": "1671679"
  },
  {
    "text": "offline throughput so after some further investigations we",
    "start": "1671679",
    "end": "1677279"
  },
  {
    "text": "find that a large portion of the extra overhead might come from the network communication",
    "start": "1677279",
    "end": "1683200"
  },
  {
    "text": "and if we use say gpus instead of cpus the interest time might be further",
    "start": "1683200",
    "end": "1689520"
  },
  {
    "text": "reduced which means on the other hand it can make the extra overhead even more significant",
    "start": "1689520",
    "end": "1696559"
  },
  {
    "text": "so in this example case we would want to improve our software stack in the classic bonding part so that we",
    "start": "1696559",
    "end": "1702399"
  },
  {
    "text": "can improve the service level performance especially on the network communication side although",
    "start": "1702399",
    "end": "1710320"
  },
  {
    "start": "1709000",
    "end": "1738000"
  },
  {
    "text": "i would say that it might be controversial whether some of the overhead in the end to end experiments",
    "start": "1710320",
    "end": "1716480"
  },
  {
    "text": "would actually make sense from the perspective of a pure machine learning benchmark",
    "start": "1716480",
    "end": "1722080"
  },
  {
    "text": "but i i think that this would at least help us to get some ideas or point us to",
    "start": "1722080",
    "end": "1727840"
  },
  {
    "text": "some directions where we can know how we can optimize our machining system",
    "start": "1727840",
    "end": "1735840"
  },
  {
    "text": "that is built on top of kubernetes so if you are interested to know a bit more details about this example",
    "start": "1735840",
    "end": "1742240"
  },
  {
    "start": "1738000",
    "end": "1893000"
  },
  {
    "text": "or if you want to try it on yourself you can go to the repository that is shown on the slide where you",
    "start": "1742240",
    "end": "1748640"
  },
  {
    "text": "will find some additional details so that concludes our presentation",
    "start": "1748640",
    "end": "1756240"
  },
  {
    "text": "and i would like to thank you for your attendance and if you are interested to",
    "start": "1756240",
    "end": "1761919"
  },
  {
    "text": "learn more about low perf um you can go to the network.org website",
    "start": "1761919",
    "end": "1767600"
  },
  {
    "text": "to learn more details and please feel free to contact us if you have any questions",
    "start": "1767600",
    "end": "1775200"
  },
  {
    "text": "okay thank you very much thank you",
    "start": "1775200",
    "end": "1781840"
  },
  {
    "text": "so now we have some time for uh questions and answers and if you have any",
    "start": "1793679",
    "end": "1798880"
  },
  {
    "text": "questions please feel free to ask them now and we will try to cover",
    "start": "1798880",
    "end": "1806000"
  },
  {
    "text": "them",
    "start": "1806840",
    "end": "1809840"
  },
  {
    "text": "seems like we don't have any questions so then thank you very much again for coming and",
    "start": "1874640",
    "end": "1880080"
  },
  {
    "text": "hear this talk and if you get some additional things uh please",
    "start": "1880080",
    "end": "1885279"
  },
  {
    "text": "uh still feel free to connect us via email so thank you very much again",
    "start": "1885279",
    "end": "1890880"
  },
  {
    "text": "and thank you bye",
    "start": "1890880",
    "end": "1895600"
  }
]