[
  {
    "text": "welcome everyone in this session we are talking about part power an exciting new",
    "start": "240",
    "end": "6040"
  },
  {
    "text": "feature that will most likely go Alpha and 132 which will help uh simplify the",
    "start": "6040",
    "end": "11799"
  },
  {
    "text": "kubernetes resource management as we are trying to move the resource management",
    "start": "11799",
    "end": "17039"
  },
  {
    "text": "from the container level to the Pod level um my name is dsh and I work for",
    "start": "17039",
    "end": "22960"
  },
  {
    "text": "Google and I'm an active contributor to signote community and with me is my friend hello my name is Peter I'm a uh",
    "start": "22960",
    "end": "31359"
  },
  {
    "text": "senior software engineer at Red Hat I am a cryo maintainer and a signno chair and I'm excited to be here and talking with",
    "start": "31359",
    "end": "37719"
  },
  {
    "text": "you all so thank you for joining um so today we're going to be talk talking about one of the foundational aspects of",
    "start": "37719",
    "end": "44480"
  },
  {
    "text": "multi- tennessy in kubernetes requests and limits um and I presume the majority",
    "start": "44480",
    "end": "50000"
  },
  {
    "text": "of people are familiar but I'm going to go over for those of us that may be new here um a request is a way for a pod to",
    "start": "50000",
    "end": "57199"
  },
  {
    "text": "specify um to the kubernetes uh system I would like to have at least this much of",
    "start": "57199",
    "end": "63160"
  },
  {
    "text": "a resource today the resources we're going to be talking about are CPU and memory there are other ones you can request but those aren't relevant here",
    "start": "63160",
    "end": "70439"
  },
  {
    "text": "and then a limit is saying I'm prepared to be punished if I used more than this amount I say this in this particular way",
    "start": "70439",
    "end": "76600"
  },
  {
    "text": "and I'll explain a little bit later because you're not always punished of use more than it sometimes you can be so the way specifically that this",
    "start": "76600",
    "end": "85040"
  },
  {
    "text": "works for uh memory because they work a little bit different between CPU and memory a memory request is used by the",
    "start": "85040",
    "end": "92000"
  },
  {
    "text": "scheduler basically saying like so when the Pod is created you know the API server will give that to the schedule",
    "start": "92000",
    "end": "98360"
  },
  {
    "text": "and schedule will be like ah I need to find a node with this much memory available the CRI does not use uh memory",
    "start": "98360",
    "end": "106680"
  },
  {
    "text": "that's requested uh there is a memory qos kep which will change this but that's in this Perma Alpha State because",
    "start": "106680",
    "end": "114560"
  },
  {
    "text": "uh it's there are some other issues with it so we're not doing that right now A Memory limit on the other hand scheduler",
    "start": "114560",
    "end": "120759"
  },
  {
    "text": "doesn't use scheduler doesn't really pay attention to limits at all and the container runtime will map the limits uh",
    "start": "120759",
    "end": "127560"
  },
  {
    "text": "specified to memory. Max which is a file in the cgroup hierarchy cgroups are the way that U uh limits work in some",
    "start": "127560",
    "end": "134800"
  },
  {
    "text": "requests as well um and this is sometimes enforced by the kernel by oom killing so if if a container uses more",
    "start": "134800",
    "end": "140680"
  },
  {
    "text": "than its allotted amount of memory then it may be oom killed with the uh kernel is feeling memory pressure we can see a",
    "start": "140680",
    "end": "146519"
  },
  {
    "text": "pod here as an example of uh you know what it looks like to specify memory",
    "start": "146519",
    "end": "152519"
  },
  {
    "text": "requests and limits um so here we have uh two containers one of which has a",
    "start": "152519",
    "end": "158160"
  },
  {
    "text": "limit of 64 megabytes and then one of them is 32 and then you can see this in the coup p c so I created this pod and",
    "start": "158160",
    "end": "164519"
  },
  {
    "text": "this is the Pod itself and this asterisk here is just saying show me all of the um you know subpaths in this pod you can",
    "start": "164519",
    "end": "171720"
  },
  {
    "text": "ignore this Max that's for the infra container it's not relevant here so but you can see these two values so this 67",
    "start": "171720",
    "end": "177760"
  },
  {
    "text": "7,000 whatever I'm not looking at the uh decimals that's the 64 megabytes and then this 33 here is this 32 um so we",
    "start": "177760",
    "end": "186120"
  },
  {
    "text": "can see that the uh memory request directly mapped to the cgroups there similarly CPU uh for the Schuler",
    "start": "186120",
    "end": "195000"
  },
  {
    "text": "culer will see the CPU limit value uh request value and find a node that",
    "start": "195000",
    "end": "200120"
  },
  {
    "text": "there's enough space for that on the CRI the container run time will map the",
    "start": "200120",
    "end": "205319"
  },
  {
    "text": "requested CPU to the cpu. weight field in the cgroups so this is a different from memory where it actually the uh the",
    "start": "205319",
    "end": "212959"
  },
  {
    "text": "colel will actually you know enforce the request um this is done with CPU weight",
    "start": "212959",
    "end": "218439"
  },
  {
    "text": "um and we uh it's uh basically a way of uh specifying you know it's like",
    "start": "218439",
    "end": "226200"
  },
  {
    "text": "choosing a slice of uh the CPU it's like always going to be allocated that amount so it's it when the um container the Pod",
    "start": "226200",
    "end": "234079"
  },
  {
    "text": "request is like I would like at least this much CPU weight is a way of specifying that and you can see be uh",
    "start": "234079",
    "end": "240640"
  },
  {
    "text": "the uh this sort of playing out you know you have a half of the um request here",
    "start": "240640",
    "end": "247079"
  },
  {
    "text": "um you know 250 and 125 and this is mapped to the CP weight of uh 10 and 5",
    "start": "247079",
    "end": "254159"
  },
  {
    "text": "and the total cumulative for the Pod is 15 again ignoring the infra container not relevant CPU limit um scheduler is",
    "start": "254159",
    "end": "262919"
  },
  {
    "text": "again going to ignore CPU limits the container runtime Maps the limits to cpu. Max um which is specified over",
    "start": "262919",
    "end": "269800"
  },
  {
    "text": "period of time so when you say I want 125 um per this period it's going to be",
    "start": "269800",
    "end": "276199"
  },
  {
    "text": "mapped directly to the kernel again ignoring this Max then we have this um 250 for uh this CPU request and 125",
    "start": "276199",
    "end": "285240"
  },
  {
    "text": "roughly and this um and then there added together to cpu. Max so what this is painting here is basically for both CPU",
    "start": "285240",
    "end": "291479"
  },
  {
    "text": "and memory uh the individual container uh requests are summed together and that is you know contributing to what the Pod",
    "start": "291479",
    "end": "298479"
  },
  {
    "text": "overall is going to use um this is all standard this is the way it's worked for 10 years now and it's worked great but",
    "start": "298479",
    "end": "305280"
  },
  {
    "text": "as many of you as probably know this is not a really you know fully comprehensive solution there are some",
    "start": "305280",
    "end": "310759"
  },
  {
    "text": "cases that it doesn't work quite well for um so you know for instance when you're trying to fit cats in boxes",
    "start": "310759",
    "end": "316320"
  },
  {
    "text": "sometimes your cats don't fit perfectly well into the boxes that they've chosen for themselves um and this opens in",
    "start": "316320",
    "end": "322080"
  },
  {
    "text": "containers as well so the first sort of situation that this comes up with is in multicontainer pot so imagine you have",
    "start": "322080",
    "end": "328240"
  },
  {
    "text": "an a development environment pod and you have one container that's running your IDE and one's um that's going to be you",
    "start": "328240",
    "end": "334280"
  },
  {
    "text": "know for debugging uh in and then one of them is just a language server that's going to you know give you your extra",
    "start": "334280",
    "end": "339520"
  },
  {
    "text": "your fun um special highlighting and everything like that um when you create",
    "start": "339520",
    "end": "345319"
  },
  {
    "text": "this pod you have to know exactly how much each of these containers are going to use and then allocate them that amount so that requires a decent amount",
    "start": "345319",
    "end": "351639"
  },
  {
    "text": "of understanding of the workload that you're running and uh it's hard to predict if like maybe these workloads",
    "start": "351639",
    "end": "357759"
  },
  {
    "text": "don't work quite as well like exact ex actly as you want there's a lot of adjustment and this is you know the motivation projects like vertical pod",
    "start": "357759",
    "end": "364319"
  },
  {
    "text": "autoscaler which will change the requests uh and limit or the limits that a um PO is",
    "start": "364319",
    "end": "369840"
  },
  {
    "text": "allocated another sort of similar example but a little bit different um is in high burst applications so like",
    "start": "369840",
    "end": "375120"
  },
  {
    "text": "imagine you have you know this data pre-process this data processing pod and it's got all these different functions",
    "start": "375120",
    "end": "380840"
  },
  {
    "text": "and they're sort of passing information back and forth and at different times they're going to be using different amounts of memory but they're going to",
    "start": "380840",
    "end": "386880"
  },
  {
    "text": "spike at different times cuz you know you're going to be doing formation a different time maybe than you're doing cleaning um and when you allocate out",
    "start": "386880",
    "end": "394520"
  },
  {
    "text": "resources for these specifically with you know with request and limits you're saying like I need this at my max amount",
    "start": "394520",
    "end": "400680"
  },
  {
    "text": "because I don't want to get killed or I don't want to get throttled um but you can't really account for the way that",
    "start": "400680",
    "end": "406720"
  },
  {
    "text": "they idle differently uh or Spike differently so you have to sort of over allocate for each of these containers",
    "start": "406720",
    "end": "412240"
  },
  {
    "text": "and um you know have times where there's downtime and unused resources and this is even to the point",
    "start": "412240",
    "end": "419160"
  },
  {
    "text": "where like you know there's uh some common wisdom um that says don't even use CPU limits because if you put a CPU",
    "start": "419160",
    "end": "426280"
  },
  {
    "text": "limit in a PO a container and it doesn't use all of its CPU that CPU is wasted basically the kernel will not use that",
    "start": "426280",
    "end": "432440"
  },
  {
    "text": "CPU for anything else uh and this is not um very uh good for having our nodes",
    "start": "432440",
    "end": "439360"
  },
  {
    "text": "being fully utilized so you know this problem uh",
    "start": "439360",
    "end": "444479"
  },
  {
    "text": "luckily is we're trying to address and I'll um pass it over to",
    "start": "444479",
    "end": "450080"
  },
  {
    "text": "to show how sure so um like Peter highlighted container level resource",
    "start": "450080",
    "end": "456280"
  },
  {
    "text": "management or trying to assess how much each container would need specifically if you have a lot of containers in a pot",
    "start": "456280",
    "end": "462039"
  },
  {
    "text": "it can be a daunting task so it reminds me of uh Alexis from shitek picture her",
    "start": "462039",
    "end": "467599"
  },
  {
    "text": "arriving at cubec con with a with an adorable cabin bag but once she goes to the Expo and she collects a lot of Swag",
    "start": "467599",
    "end": "474199"
  },
  {
    "text": "it just doesn't fit in so this is the problem with re container level resource management",
    "start": "474199",
    "end": "480479"
  },
  {
    "text": "with that she has only one option she could utilize her limited resource which is money to get a bigger bag just for",
    "start": "480479",
    "end": "487199"
  },
  {
    "text": "this one instance where the uh suitcase is bursting basically but with po level resources",
    "start": "487199",
    "end": "494599"
  },
  {
    "text": "she could redistribute the extra swag and find a friend whose bag is half empty and she's traveling to the same",
    "start": "494599",
    "end": "501120"
  },
  {
    "text": "place as her and she could just put in her stuff there and then get it back once they have arrived at the",
    "start": "501120",
    "end": "506400"
  },
  {
    "text": "destination so Port level Resources with with this feature we are basically enabling the containers to dynamically",
    "start": "506400",
    "end": "512760"
  },
  {
    "text": "share the unused resources within the Pod and the users will just have to specify the requests and limits for the",
    "start": "512760",
    "end": "519518"
  },
  {
    "text": "entire pod and not worry about how much each container would need so it's a more holistic approach to",
    "start": "519519",
    "end": "526839"
  },
  {
    "text": "Resource Management and to enable this holistic approach we have modified the Pod API we have added the support for",
    "start": "526839",
    "end": "534360"
  },
  {
    "text": "resources in here at the spec level where you define the uh res the over",
    "start": "534360",
    "end": "539440"
  },
  {
    "text": "overall resource needs of your spec and then all your containers in it be it regular containers init containers side",
    "start": "539440",
    "end": "545200"
  },
  {
    "text": "car containers or ephemeral containers all of them would have the ceiling the boundaries of what you have specified uh",
    "start": "545200",
    "end": "551240"
  },
  {
    "text": "at the board level so how it how it can be beneficial is it simplifies the resource management",
    "start": "551240",
    "end": "557680"
  },
  {
    "text": "the user just has to worry about one set of requests and limits making the configuration really simple and not",
    "start": "557680",
    "end": "563680"
  },
  {
    "text": "worrying about n sets of requests and limits for each end containers in their pods it also Al gives the greater",
    "start": "563680",
    "end": "570160"
  },
  {
    "text": "flexibility if there are any unused resources that are allocated to the pod which some of the containers are not",
    "start": "570160",
    "end": "576120"
  },
  {
    "text": "using while the others need for the amount of time they're bursting they could just share those resources it also",
    "start": "576120",
    "end": "582240"
  },
  {
    "text": "helps with the better uh resource utilization which leads to some uh cost savings like Alexus was able to save not",
    "start": "582240",
    "end": "588640"
  },
  {
    "text": "buying another suitcase so I want to highlight uh this one example of a pod U which can be now",
    "start": "588640",
    "end": "596560"
  },
  {
    "text": "uh which can now be used with pod level resour sources say you have a pod with three containers um we are just taking",
    "start": "596560",
    "end": "604360"
  },
  {
    "text": "regular containers for this example if you do not specify container level requests uh kubernetes will treat all",
    "start": "604360",
    "end": "610480"
  },
  {
    "text": "three containers of equal priority when it comes to oom killing but if you specify request then the uh the ones",
    "start": "610480",
    "end": "617000"
  },
  {
    "text": "with the higher request specified will have a lower priority so in this in this",
    "start": "617000",
    "end": "622200"
  },
  {
    "text": "uh application you don't have to worry about uh the limits specifying the limits for each container you just",
    "start": "622200",
    "end": "628000"
  },
  {
    "text": "specify a request a top level request or sorry top level limit at the Pod level and then the containers can just burst",
    "start": "628000",
    "end": "634440"
  },
  {
    "text": "into each other this is a very important use case when it comes to the AIML uh",
    "start": "634440",
    "end": "639760"
  },
  {
    "text": "applications where uh it's I mean the containers can burst into each other",
    "start": "639760",
    "end": "645279"
  },
  {
    "text": "like for example the data processing pod which uh Peter was talking about say the data transformation doesn't need as much",
    "start": "645279",
    "end": "651639"
  },
  {
    "text": "resources at a point of time while the actual application while the actual container that's cleaning the data at",
    "start": "651639",
    "end": "657920"
  },
  {
    "text": "that point requires more resources so they could just burst into each other and share those resources let's Deep dive into the",
    "start": "657920",
    "end": "665120"
  },
  {
    "text": "implementation details uh what has essentially changed for all the kubernetes components with this feature",
    "start": "665120",
    "end": "671800"
  },
  {
    "text": "uh from the control plane perspective in the API server we have added some new validation logic which will ensure that",
    "start": "671800",
    "end": "678240"
  },
  {
    "text": "your pod level requests are less than the limits your pod level requests are",
    "start": "678240",
    "end": "683920"
  },
  {
    "text": "uh are greater than or equal to your aggregated container level requests if you specify those but the magic happens",
    "start": "683920",
    "end": "690519"
  },
  {
    "text": "with the Pod level limits if you specify the Pod level limits they don't necessarily have to be greater than or",
    "start": "690519",
    "end": "697519"
  },
  {
    "text": "less than the container level limits they can they can still be less than the container level",
    "start": "697519",
    "end": "702760"
  },
  {
    "text": "limits the next change that has gone in is in the Schuler uh the Schuler was",
    "start": "702760",
    "end": "707959"
  },
  {
    "text": "previously just aggregating the container level requests to find the node that would fit the Pod but now if",
    "start": "707959",
    "end": "713680"
  },
  {
    "text": "you specify the Pod level values it will just use those values directly there are a lot of Chang",
    "start": "713680",
    "end": "719240"
  },
  {
    "text": "changes that went in the cuet but the main being cubet was also doing some complex calculation trying to see if it",
    "start": "719240",
    "end": "725800"
  },
  {
    "text": "is a side car container it has some logic uh for the calculation with this pod level resources it would just use",
    "start": "725800",
    "end": "731560"
  },
  {
    "text": "the value from the Pod spec and pass it on to the container run times and the container run times will ensure that",
    "start": "731560",
    "end": "737760"
  },
  {
    "text": "those values are set as the boundaries in the c groups so that your pod as a whole is using only that much resources",
    "start": "737760",
    "end": "744320"
  },
  {
    "text": "that you have specified in the limits like I said there are a lot of changes that happened in the cuet some",
    "start": "744320",
    "end": "750639"
  },
  {
    "text": "of them we made in Alpha and some of them we'll be making uh in beta so today",
    "start": "750639",
    "end": "756079"
  },
  {
    "text": "uh in cu the cuet determines the Qs classes uh from the container level",
    "start": "756079",
    "end": "761320"
  },
  {
    "text": "resources but with this new feature if you specify pod level request or limit",
    "start": "761320",
    "end": "766519"
  },
  {
    "text": "any of those cuet will consider only those values and it will discard the container level values for the Qs",
    "start": "766519",
    "end": "772560"
  },
  {
    "text": "determination logic so Qs in uh kubernetes are of three types one is",
    "start": "772560",
    "end": "778079"
  },
  {
    "text": "guaranteed the other one is burstable the third one is best effort uh cuet uses the Qs classes to also determine uh",
    "start": "778079",
    "end": "786279"
  },
  {
    "text": "the behavior of the O killer your guaranteed and your best effort pods get sort of a constant uh o score adjustment",
    "start": "786279",
    "end": "794240"
  },
  {
    "text": "but for the best uh for the burstable pods kubernetes does some calculation which we'll talk",
    "start": "794240",
    "end": "800600"
  },
  {
    "text": "about so kubernetes uses this uh Linux kernels mechanism omcore adjust which",
    "start": "800600",
    "end": "807440"
  },
  {
    "text": "influences which which which influences the priority order of the processes",
    "start": "807440",
    "end": "813000"
  },
  {
    "text": "which uh which will be killed by the OM killer so today uh the omcore adjustment",
    "start": "813000",
    "end": "818320"
  },
  {
    "text": "only takes memory request into account so we have made sure that we adjust that",
    "start": "818320",
    "end": "823920"
  },
  {
    "text": "and take uh pod level memory request also into account because if we wouldn't",
    "start": "823920",
    "end": "829120"
  },
  {
    "text": "have done that then your memory request at the container level would be zero which means the value of this formula",
    "start": "829120",
    "end": "835120"
  },
  {
    "text": "would would result a value of thousand which means all your containers in a pod that have only pod level Resources with",
    "start": "835120",
    "end": "842160"
  },
  {
    "text": "this formula would be um killed the first so to change that we have added uh",
    "start": "842160",
    "end": "849279"
  },
  {
    "text": "uh this pod level memory request also in this formula to understand more details uh I have linked the cap in the last",
    "start": "849279",
    "end": "857320"
  },
  {
    "text": "slide so the common questions and misconceptions that people have asked us is why are we doing this like weren the",
    "start": "857320",
    "end": "865240"
  },
  {
    "text": "Pod level resources just enough like why what's the main advantage but like I highlighted in one of the",
    "start": "865240",
    "end": "871120"
  },
  {
    "text": "previous slides what happens is it's very difficult to gauge how much each container would need and within a pod it",
    "start": "871120",
    "end": "878519"
  },
  {
    "text": "happens that the burst the containers might not burst together synchronously there are instances when one container",
    "start": "878519",
    "end": "885199"
  },
  {
    "text": "is using the resources and then the other container is not using the resources and to make sure that these",
    "start": "885199",
    "end": "890240"
  },
  {
    "text": "idle resources are used in an efficient manner we are we we have added this feature so that the idle resources can",
    "start": "890240",
    "end": "896759"
  },
  {
    "text": "be shared dynamically amongst the containers so the second question is uh does the",
    "start": "896759",
    "end": "902680"
  },
  {
    "text": "Pod level container management or the Pod level spec disallow container level",
    "start": "902680",
    "end": "908120"
  },
  {
    "text": "requests and limits but that's not the case these both work in tandem you can specify both or you can specify either",
    "start": "908120",
    "end": "915240"
  },
  {
    "text": "of these and kuet is takes care of making sure that these values are translated to the C group values and",
    "start": "915240",
    "end": "921639"
  },
  {
    "text": "also passed on to the other features that would need these values the third most important question",
    "start": "921639",
    "end": "927800"
  },
  {
    "text": "that people have asked us is does this affect our monitoring tools and what about the high level schedulers like Q",
    "start": "927800",
    "end": "935040"
  },
  {
    "text": "how how will it know uh about this how it's still not aware of the P level request so how will it know about this",
    "start": "935040",
    "end": "940319"
  },
  {
    "text": "new feature so in order to uh help these schedulers or any third party libraries",
    "start": "940319",
    "end": "945560"
  },
  {
    "text": "that would want to know how much the pods are uh pods will need we have added",
    "start": "945560",
    "end": "950639"
  },
  {
    "text": "a helper a comp helper component in an external library that can be used by all these third party tools they can query",
    "start": "950639",
    "end": "957680"
  },
  {
    "text": "this library and this library library takes care of the logic all the if and else if you have container level values",
    "start": "957680",
    "end": "964319"
  },
  {
    "text": "do the aggregation if you have pod level values just spit out those values from the helper so we we have added these uh",
    "start": "964319",
    "end": "970920"
  },
  {
    "text": "helper methods that can be used by the external uh Fe external components the last question is is this",
    "start": "970920",
    "end": "978199"
  },
  {
    "text": "feature supported for all resource types unfortunately this is only supported for memory and CPU right now in future we",
    "start": "978199",
    "end": "985560"
  },
  {
    "text": "might support it for other resources but as of now we are only planning to to support uh memory and CPU",
    "start": "985560",
    "end": "992680"
  },
  {
    "text": "only so we are very excited about this feature and the potential of this feature and all the use cases that this",
    "start": "992759",
    "end": "999079"
  },
  {
    "text": "would unlock this feature is going Alpha in 132 most likely still in the release",
    "start": "999079",
    "end": "1005279"
  },
  {
    "text": "phase and we have added the minimum support like the values from the spec are translated correctly to the c groups",
    "start": "1005279",
    "end": "1012079"
  },
  {
    "text": "that's taken care of the schuer logic is taken care of API server validation is also taken care of we have also taken",
    "start": "1012079",
    "end": "1018560"
  },
  {
    "text": "care of of the admission controller logic uh the limit Ranger and the resource quota but all the complex uh",
    "start": "1018560",
    "end": "1024640"
  },
  {
    "text": "managers topology memory and CPU the support for that will come in 133 and",
    "start": "1024640",
    "end": "1030480"
  },
  {
    "text": "also the support for huge Pages resource type would come in 133 and we also plan to support this",
    "start": "1030480",
    "end": "1036959"
  },
  {
    "text": "exciting uh in place pod resize feature which is going beta in 132 we plan to",
    "start": "1036959",
    "end": "1042520"
  },
  {
    "text": "support that uh in the beta of pod level resources in 133 if we figure it out",
    "start": "1042520",
    "end": "1047760"
  },
  {
    "text": "correctly because it's a very complicated feature and then there is this another interesting cap that is in",
    "start": "1047760",
    "end": "1054200"
  },
  {
    "text": "talk in signode community which is dynamic containers in a pod so this feature is still in works and if the it",
    "start": "1054200",
    "end": "1062480"
  },
  {
    "text": "materializes this will allow adding containers and removing containers from the Pod dynamically again for AIML",
    "start": "1062480",
    "end": "1069000"
  },
  {
    "text": "workloads and pod level resources is the feature that enables Dynamic containers in a",
    "start": "1069000",
    "end": "1075919"
  },
  {
    "text": "pod so last but not the least uh before we go beta in 133 we want to get",
    "start": "1075919",
    "end": "1083080"
  },
  {
    "text": "we are trying to gather as much feedback as possible from the community around so it would be great if you try this",
    "start": "1083080",
    "end": "1088640"
  },
  {
    "text": "feature out with your existing deployments and let us know how it works and if there if there are any use cases",
    "start": "1088640",
    "end": "1095440"
  },
  {
    "text": "that we have missed please reach out to us I have added the link for the cap and",
    "start": "1095440",
    "end": "1100760"
  },
  {
    "text": "also the uh the link for the GitHub uh Community page so please reach out to us",
    "start": "1100760",
    "end": "1106760"
  },
  {
    "text": "we have lot of work that we want to do in the B are like supporting all these uh components so it would be great if",
    "start": "1106760",
    "end": "1112960"
  },
  {
    "text": "you would also help us uh in developing this feature and last but not the least",
    "start": "1112960",
    "end": "1118760"
  },
  {
    "text": "please drop us feedback good or bad using this QR code thank",
    "start": "1118760",
    "end": "1124580"
  },
  {
    "text": "[Applause]",
    "start": "1124580",
    "end": "1130880"
  },
  {
    "text": "you I just had one question um how does it work at the low level about the the",
    "start": "1130880",
    "end": "1136480"
  },
  {
    "text": "cgroup limits like I thought that that the underlying containers wouldn't be",
    "start": "1136480",
    "end": "1141919"
  },
  {
    "text": "allowed to exceed their limits so the cgroup limits will be specified at the Pod level but if you",
    "start": "1141919",
    "end": "1149280"
  },
  {
    "text": "specify cgroup limits at the container level as well those would be set as well and containers would individually not be",
    "start": "1149280",
    "end": "1155440"
  },
  {
    "text": "able to use more than what you have set at the container level but together like",
    "start": "1155440",
    "end": "1161039"
  },
  {
    "text": "if you don't specify the limits at container level together they can burst up to uh the P level limit I'll give an",
    "start": "1161039",
    "end": "1167200"
  },
  {
    "text": "example for example you have have a pod in which you specify pod level limit of",
    "start": "1167200",
    "end": "1172480"
  },
  {
    "text": "100 but container one has limit of 60 container two has limit of 50 the",
    "start": "1172480",
    "end": "1178039"
  },
  {
    "text": "aggregate is more than 100 right so together they will never be able to use more than 100 but individually when one",
    "start": "1178039",
    "end": "1184480"
  },
  {
    "text": "container is not using the resource they would be able to burst until their own individual limits okay thank you",
    "start": "1184480",
    "end": "1192799"
  },
  {
    "text": "welcome question how can you hear me yes uh how is the various Prometheus metrics",
    "start": "1195200",
    "end": "1201640"
  },
  {
    "text": "for uh resource requests and limits going to is that going to be rolled up or is it going to add another metric for",
    "start": "1201640",
    "end": "1208919"
  },
  {
    "text": "the Pod level request so the the metrics uh in Prometheus actually are coming",
    "start": "1208919",
    "end": "1214000"
  },
  {
    "text": "from C adviser and it's scraping I think the majority of the cgroups on the Node and so the because the limits and",
    "start": "1214000",
    "end": "1220520"
  },
  {
    "text": "request implemented the cgroup so so it doesn't matter it'll just be scraped from those cgroups and then reported in the same way cool thank you",
    "start": "1220520",
    "end": "1229480"
  },
  {
    "text": "hi can you hear me yeah I had a question on the U Bo level spec so when you",
    "start": "1229480",
    "end": "1237120"
  },
  {
    "text": "specify only pod level spec uh what guard rail exist for a",
    "start": "1237120",
    "end": "1244799"
  },
  {
    "text": "particular container to not uh consume all the resources there is no guard rail",
    "start": "1244799",
    "end": "1252200"
  },
  {
    "text": "preventing any one container from consuming all of the resources within the Pod the only guardrail is like the",
    "start": "1252200",
    "end": "1258280"
  },
  {
    "text": "boundary of the Pod so the containers do have to do a little bit of coordination to make sure that you know some",
    "start": "1258280",
    "end": "1264919"
  },
  {
    "text": "everyone's getting their fair share and you can use you know limits and requests within the containers like as D",
    "start": "1264919",
    "end": "1271440"
  },
  {
    "text": "mentioned earlier which will you know make sure that everyone gets a little bit but not every uh every container um",
    "start": "1271440",
    "end": "1278200"
  },
  {
    "text": "is able to use uh you know you can still burst within it and like the containers will look over provision but from the",
    "start": "1278200",
    "end": "1284720"
  },
  {
    "text": "perspective of the Pod it'll never go over the pods limit okay thank you",
    "start": "1284720",
    "end": "1290520"
  },
  {
    "text": "thanks for a great talk uh maybe this slide answers it but I'm just curious whether init containers are constrained",
    "start": "1292240",
    "end": "1298320"
  },
  {
    "text": "by or exempt from the Pod level limits uh they are also all your containers will be constrained by the Pod level",
    "start": "1298320",
    "end": "1304000"
  },
  {
    "text": "limits although we are still trying to figure out like for the O score adjust",
    "start": "1304000",
    "end": "1309559"
  },
  {
    "text": "calculation right now we are dividing the Pod level request equally amongst all the containers if",
    "start": "1309559",
    "end": "1317480"
  },
  {
    "text": "the container level request are not specified but maybe we might want to change that with respect to the inet",
    "start": "1317480",
    "end": "1323000"
  },
  {
    "text": "containers But to answer the resource boundaries uh of all the containers will be guarded by the Pod level values thank",
    "start": "1323000",
    "end": "1330720"
  },
  {
    "text": "you welcome uh just a clarification question on the previous one that was asked so if",
    "start": "1330720",
    "end": "1336279"
  },
  {
    "text": "I specify requests at the container containers level and then I specify just the limits at the part level the",
    "start": "1336279",
    "end": "1343440"
  },
  {
    "text": "behavior I would get is each of my containers can is guaranteed the resources are requested but they can",
    "start": "1343440",
    "end": "1349159"
  },
  {
    "text": "burst up to the part limit and aggregate is that correct yes that's correct so",
    "start": "1349159",
    "end": "1354200"
  },
  {
    "text": "the use case where we want to allow the users to specify container level values in this Cas is if they care about the",
    "start": "1354200",
    "end": "1360440"
  },
  {
    "text": "priority of O kill when there is a resource crunch but if you do not care about the priority of your containers",
    "start": "1360440",
    "end": "1366919"
  },
  {
    "text": "just don't specify the requests at the container level but if you want to make sure that one container gets a higher",
    "start": "1366919",
    "end": "1372520"
  },
  {
    "text": "priority over the other specify either higher requests for one and a lower for the other one or specify request for one",
    "start": "1372520",
    "end": "1379240"
  },
  {
    "text": "and no request for the other one the main uh use case in our case would be that we want to ensure our containers",
    "start": "1379240",
    "end": "1385679"
  },
  {
    "text": "get some guaranteed amount of resources so that you don't have side cards using more resources than the app container for example uh because if you don't",
    "start": "1385679",
    "end": "1391960"
  },
  {
    "text": "specify anything at the container level then you have a noising a problem one caveat is that um so that",
    "start": "1391960",
    "end": "1399880"
  },
  {
    "text": "you would be able to get that behavior with CPU because CPU is you know the",
    "start": "1399880",
    "end": "1404919"
  },
  {
    "text": "weight is being reflected in the c groups but actually there's no guarantee with with memory because without the",
    "start": "1404919",
    "end": "1410320"
  },
  {
    "text": "memory Q us because uh there's nothing saying in the cgroups like every like",
    "start": "1410320",
    "end": "1416360"
  },
  {
    "text": "this container must get at least this amount so it's a little bit more but because memory limits are a little bit",
    "start": "1416360",
    "end": "1422440"
  },
  {
    "text": "more um fuzzy anyway if you're enabling overcommit like it it could be okay but",
    "start": "1422440",
    "end": "1429480"
  },
  {
    "text": "yeah you it would actually do that um functionally uh in uh for uh CPU okay so",
    "start": "1429480",
    "end": "1436520"
  },
  {
    "text": "for CPU it will be guaranteed resources but more memory not so much if you specify the part exactly and uh last",
    "start": "1436520",
    "end": "1443080"
  },
  {
    "text": "question the topology awareness meaning like uh CPU pinning or uh Numa boundaries they don't they are not",
    "start": "1443080",
    "end": "1449799"
  },
  {
    "text": "supported yet but there's a plan to do it in the next version right that's right and we'll short circuit all the",
    "start": "1449799",
    "end": "1456000"
  },
  {
    "text": "policies that work only at the container level for p level resources okay thank",
    "start": "1456000",
    "end": "1461039"
  },
  {
    "text": "you welcome the famous less of a question more of a comment this is a really good",
    "start": "1461039",
    "end": "1466960"
  },
  {
    "text": "idea you guys should feel proud thank you thank",
    "start": "1466960",
    "end": "1472440"
  },
  {
    "text": "you question uh the CPU limit bad practice that you shared before is the",
    "start": "1472679",
    "end": "1478480"
  },
  {
    "text": "same at po level as the container limit or this new feature removes that bad",
    "start": "1478480",
    "end": "1485159"
  },
  {
    "text": "practice this this one yeah uh sorry what was the question yeah um the CPU",
    "start": "1485159",
    "end": "1492720"
  },
  {
    "text": "limit bad practice that you share at the beginning that you should not specify",
    "start": "1492720",
    "end": "1498399"
  },
  {
    "text": "CPU limit is the same in this new feature or it removes that yeah that's a",
    "start": "1498399",
    "end": "1504520"
  },
  {
    "text": "good question um and I don't know probably because still there so",
    "start": "1504520",
    "end": "1512600"
  },
  {
    "text": "theoretically this will help mitigate it for inter container within a pod issue",
    "start": "1512600",
    "end": "1518720"
  },
  {
    "text": "so like containers within that pod would still be able to sort of like help reach the limit but there will always be sort",
    "start": "1518720",
    "end": "1525720"
  },
  {
    "text": "of a ceiling that a process will be able to use as CPU wise and so like if you're",
    "start": "1525720",
    "end": "1531039"
  },
  {
    "text": "not careful with the way that you set your limits and making sure that your processes are always going to use them",
    "start": "1531039",
    "end": "1536159"
  },
  {
    "text": "then you're still going to hit that risk but it does sort of mitigate it in the sense that like there is a backup there's a fall back that could be",
    "start": "1536159",
    "end": "1542039"
  },
  {
    "text": "potentially used but um it's not perfect and so I think a lot of people will end up still not using them great thanks M",
    "start": "1542039",
    "end": "1550080"
  },
  {
    "text": "yeah great great talk I think if I understand correctly this is to well",
    "start": "1550080",
    "end": "1555279"
  },
  {
    "text": "going to help uh when a pod has many containers that going to consolidate then you can",
    "start": "1555279",
    "end": "1560520"
  },
  {
    "text": "set po level resource but then you still have the problem how to figure out and",
    "start": "1560520",
    "end": "1567039"
  },
  {
    "text": "set a prop part level CPU and memory request limits that will still be very",
    "start": "1567039",
    "end": "1573440"
  },
  {
    "text": "challenging in practice absolutely yeah you're right that we we haven't solved the whole process but the idea is that",
    "start": "1573440",
    "end": "1581880"
  },
  {
    "text": "uh you know the average of the like you're reducing the risk like because",
    "start": "1581880",
    "end": "1587240"
  },
  {
    "text": "the scope is larger so you're it's easier to sort of it's like a larger Target to hit right like you know you",
    "start": "1587240",
    "end": "1593799"
  },
  {
    "text": "have more sort of a leeway within that um pod because there's multiple processes to like you know use the",
    "start": "1593799",
    "end": "1599919"
  },
  {
    "text": "resources so theoretically it should be a little bit easier to um to you know",
    "start": "1599919",
    "end": "1605320"
  },
  {
    "text": "find a an amount that works yeah I agree it will be a little bit easier I just",
    "start": "1605320",
    "end": "1610919"
  },
  {
    "text": "wondering whether in you see know there's a road map or plan to help the",
    "start": "1610919",
    "end": "1617520"
  },
  {
    "text": "developer well vpa would be the in place resizing would be then the next logical step I",
    "start": "1617520",
    "end": "1623080"
  },
  {
    "text": "mean and that's already that's ahead of this kept as being beta so I mean that I would say then you have a way you know",
    "start": "1623080",
    "end": "1629760"
  },
  {
    "text": "you could use different controllers to actually ask hey this pot is actually not using enough we need to sort of um",
    "start": "1629760",
    "end": "1635600"
  },
  {
    "text": "we need to reduce its limits or vice versa um to help right size so that's",
    "start": "1635600",
    "end": "1640720"
  },
  {
    "text": "that's the sort of that would be this uh Sig's recommendation got it thank you um I had a couple questions can you",
    "start": "1640720",
    "end": "1648640"
  },
  {
    "text": "guys hear me yeah a little bit closer that the mics are a little bit quiet okay I have a couple questions um my",
    "start": "1648640",
    "end": "1654320"
  },
  {
    "text": "first question and maybe I just missed it um if you specify requests at both",
    "start": "1654320",
    "end": "1659559"
  },
  {
    "text": "the Pod level and for every container and the container requests some to",
    "start": "1659559",
    "end": "1665760"
  },
  {
    "text": "higher than the Pod level requests how does the schedule handle that case or is that not even allowed that's not allowed",
    "start": "1665760",
    "end": "1671159"
  },
  {
    "text": "okay the requests at the the aggregate of the requests at the container level have to be less than what's specified at",
    "start": "1671159",
    "end": "1677120"
  },
  {
    "text": "the bo level okay thanks um and I'm assuming we would just take the higher one okay or the the Pod level okay um",
    "start": "1677120",
    "end": "1684320"
  },
  {
    "text": "and then my second question so you guys mentioned that um you know this is particularly effective for like ml",
    "start": "1684320",
    "end": "1690799"
  },
  {
    "text": "workloads um are there any workloads that you see this maybe not being effective for I'm",
    "start": "1690799",
    "end": "1697200"
  },
  {
    "text": "curious single container",
    "start": "1697200",
    "end": "1701120"
  },
  {
    "text": "pods I I don't know um yeah that's a good question I mean if you already right sized your applications then I",
    "start": "1702679",
    "end": "1708440"
  },
  {
    "text": "mean like what's the point you've already you know but um you know your guaranteed pods if you've already like",
    "start": "1708440",
    "end": "1714360"
  },
  {
    "text": "yeah correctly figured it out then keep it up um and also you know a vpa case could maybe lower the uh the um the",
    "start": "1714360",
    "end": "1723039"
  },
  {
    "text": "amount that you can get from it because you know if if uh the autoscaler is Right sizing it then like maybe it's not",
    "start": "1723039",
    "end": "1729760"
  },
  {
    "text": "maybe it'd be better just to keep it right size than having this buffer but um yeah I don't think any particular uh",
    "start": "1729760",
    "end": "1737399"
  },
  {
    "text": "workload hey thanks a lot um",
    "start": "1737399",
    "end": "1743919"
  },
  {
    "text": "I I was thinking one of the weirdness with the current limits is like I",
    "start": "1743919",
    "end": "1749200"
  },
  {
    "text": "understand the oom killer comes out of Linux it's basically killing something to try to free stuff up right um is",
    "start": "1749200",
    "end": "1755960"
  },
  {
    "text": "there any thought because we're putting pod limits of causing the O instead of just killing",
    "start": "1755960",
    "end": "1762159"
  },
  {
    "text": "the one container maybe taking the Pod out right so it can then be rescheduled because that's I've seen system where",
    "start": "1762159",
    "end": "1769440"
  },
  {
    "text": "you know a container keeps crashing and crashing but the Pod gets stuck because everything else around it so thank you",
    "start": "1769440",
    "end": "1775799"
  },
  {
    "text": "yeah that that's a great question um uh we're actually you know scheming on a world where um we integrate PSI metrics",
    "start": "1775799",
    "end": "1782799"
  },
  {
    "text": "um which the colonel will use to sort of like Adit to say like you know these uh",
    "start": "1782799",
    "end": "1788480"
  },
  {
    "text": "processes have been waiting for this specific resource for this amount of time and the idea is like you could have",
    "start": "1788480",
    "end": "1795039"
  },
  {
    "text": "Integrations with something like a deser or something like you know there there's all of these different ways or like smarter eviction things like that to",
    "start": "1795039",
    "end": "1802080"
  },
  {
    "text": "have you know we can see the granularity in the um containers in the Pod levels of like okay these are really waiting",
    "start": "1802080",
    "end": "1808559"
  },
  {
    "text": "for a lot of memory maybe we should free some stuff up and then like other controllers outside of you know maybe",
    "start": "1808559",
    "end": "1813600"
  },
  {
    "text": "inry can um react to",
    "start": "1813600",
    "end": "1817480"
  },
  {
    "text": "that thanks for the talk so uh I see that the C at this point passing down",
    "start": "1822600",
    "end": "1828760"
  },
  {
    "text": "these pot resources now to the coner run time so my question is that in the case",
    "start": "1828760",
    "end": "1835799"
  },
  {
    "text": "that these sport level resources are not defined is there any change in the current communication towards confeder R",
    "start": "1835799",
    "end": "1843279"
  },
  {
    "text": "time or so are you filling filling this up in the case of they are",
    "start": "1843279",
    "end": "1849200"
  },
  {
    "text": "missing yeah um I think the idea is going to be um and this will come in",
    "start": "1849200",
    "end": "1854440"
  },
  {
    "text": "beta um like with there's the kept for the pass down the resource the CRI piece and I think that the idea is there'll be",
    "start": "1854440",
    "end": "1861080"
  },
  {
    "text": "you know a a um basically the Cub will tell the CRI like I chose this for the",
    "start": "1861080",
    "end": "1866399"
  },
  {
    "text": "Pod level um you know request and limits just so you know um and then the CRI",
    "start": "1866399",
    "end": "1871840"
  },
  {
    "text": "could sort of take that into account um and pass it down to you know a Kata or someone else to actually you know",
    "start": "1871840",
    "end": "1877639"
  },
  {
    "text": "rightsize the VM that's being created thanks thank",
    "start": "1877639",
    "end": "1884200"
  },
  {
    "text": "you any other questions oh another",
    "start": "1884799",
    "end": "1889919"
  },
  {
    "text": "this probably wouldn't make sense for an initial thing but uh what do you think uh the impact might be if an inferred",
    "start": "1891720",
    "end": "1899960"
  },
  {
    "text": "value of this was chosen say the sum of all the containers requests or and the sum of all the limits were used",
    "start": "1899960",
    "end": "1908760"
  },
  {
    "text": "automatically um are you suggesting that we do that or are you wondering the consequences I'm wondering the",
    "start": "1909760",
    "end": "1916720"
  },
  {
    "text": "consequences um I mean that's basically like setting the um that's basically like setting the limits with in the",
    "start": "1916720",
    "end": "1922320"
  },
  {
    "text": "container itself you know just like a more automated process this gives us a little bit more flexibility to actually not have you know limits or requests",
    "start": "1922320",
    "end": "1929039"
  },
  {
    "text": "within the contain on the container granularity so I mean it like could do that but you could also pretty easily",
    "start": "1929039",
    "end": "1935320"
  },
  {
    "text": "have like some admission controller to do that for you um and just or like you know a mutating web hook to change like",
    "start": "1935320",
    "end": "1941200"
  },
  {
    "text": "oh I see that it's a pod limit let's just divide it among all the containers but that also wouldn't really help for you know some more Dynamic cases also so",
    "start": "1941200",
    "end": "1947960"
  },
  {
    "text": "so I mean I think the we currently have that just less automated so having it",
    "start": "1947960",
    "end": "1954440"
  },
  {
    "text": "this not work that way would provides like net new Behavior thanks thank",
    "start": "1954440",
    "end": "1961480"
  },
  {
    "text": "you thank you very much thank you happy cucon",
    "start": "1963080",
    "end": "1968919"
  }
]