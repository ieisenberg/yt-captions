[
  {
    "text": "hi uh if you're not asleep already you can join us in the next talk",
    "start": "1040",
    "end": "7480"
  },
  {
    "text": "I am paru and with me you have Krishna uh we are going to talk about how you",
    "start": "7480",
    "end": "13719"
  },
  {
    "text": "can empower the efficiency of your clusters by using power aware kubernetes",
    "start": "13719",
    "end": "19560"
  },
  {
    "text": "scheduling so we've had too many Ai and LM sessions this is a good refresher for",
    "start": "19560",
    "end": "25240"
  },
  {
    "text": "you and stay with us so first who we are we are open-",
    "start": "25240",
    "end": "31480"
  },
  {
    "text": "Source contributors that are working on cloud native sustainability stack and the major contribution comes from IBM",
    "start": "31480",
    "end": "38800"
  },
  {
    "text": "Intel and red hat any intel folks here no we have few ibmers and any Red",
    "start": "38800",
    "end": "48960"
  },
  {
    "text": "Hats okay so at the Cornerstone we have",
    "start": "48960",
    "end": "54079"
  },
  {
    "text": "the project Kepler which gives you energy observability metrics of your CL",
    "start": "54079",
    "end": "59960"
  },
  {
    "text": "cler and around Kepler we have other projects that we are using to consume",
    "start": "59960",
    "end": "68080"
  },
  {
    "text": "those observability metrics to do cluster optimization Kepler is a CNC of sandbox project already and feel free to",
    "start": "68080",
    "end": "75479"
  },
  {
    "text": "drop at a kiosk on Friday 10:30 to 12:30 if you want to know more about Kepler",
    "start": "75479",
    "end": "81799"
  },
  {
    "text": "but today we are talking about weaks which uses Kepler observability data to",
    "start": "81799",
    "end": "88840"
  },
  {
    "text": "do energy and and power aware kuity scheduling so what problem are we trying",
    "start": "88840",
    "end": "96439"
  },
  {
    "text": "to solve well we all know that the I'm not sure if all know like anybody familiar",
    "start": "96439",
    "end": "103439"
  },
  {
    "text": "with kubernetes scheduling framework okay so like kubernetes",
    "start": "103439",
    "end": "108840"
  },
  {
    "text": "scheduling framework gives you the provision to have your inbuilt or like custom schuer plugin that you just hook",
    "start": "108840",
    "end": "115840"
  },
  {
    "text": "up with the uh scheduling framework and you can uh optimize or you can try to",
    "start": "115840",
    "end": "122640"
  },
  {
    "text": "solve whatever problem you're solving in the scheduling or whatever objectives you are uh trying to maximize or",
    "start": "122640",
    "end": "130039"
  },
  {
    "text": "minimize but within the ecosystem we found that there's a lack of schedular",
    "start": "130039",
    "end": "135640"
  },
  {
    "text": "plugins that focuses on uh Power optimization or Energy Efficiency while",
    "start": "135640",
    "end": "142480"
  },
  {
    "text": "also taking care of the other objectives that they are trying to solve so uh we",
    "start": "142480",
    "end": "148200"
  },
  {
    "text": "thought of developing Peaks that can address this Gap and what Peaks does it",
    "start": "148200",
    "end": "153840"
  },
  {
    "text": "it essentially try tries to maximize the Energy Efficiency of your cluster while",
    "start": "153840",
    "end": "159120"
  },
  {
    "text": "also focusing or letting you do other stuffs uh like how to optimize on",
    "start": "159120",
    "end": "164560"
  },
  {
    "text": "topology and how to optimize on network um Network or CPU",
    "start": "164560",
    "end": "171200"
  },
  {
    "text": "utilization so our goal was to come up with a configural scheduling plug-in",
    "start": "171200",
    "end": "177599"
  },
  {
    "text": "that minimize the aggregate power consumption of the entire cluster and we",
    "start": "177599",
    "end": "182640"
  },
  {
    "text": "wanted to implement it as a score plug-in while making sure that we are",
    "start": "182640",
    "end": "188120"
  },
  {
    "text": "not altering the default scoring plugging U of the kubernetes",
    "start": "188120",
    "end": "193480"
  },
  {
    "text": "framework so the solution obviously Peaks been just talked about Peaks for so long so uh paks is a kubernetes",
    "start": "193480",
    "end": "201840"
  },
  {
    "text": "schedul plugging that aims to optimize the aggregate power consumption of your",
    "start": "201840",
    "end": "207440"
  },
  {
    "text": "entire cluster and the important thing to note over here is during scheduling",
    "start": "207440",
    "end": "213040"
  },
  {
    "text": "so it at the moment weaks only do uh this optimization at PL pod",
    "start": "213040",
    "end": "219280"
  },
  {
    "text": "placement and it uses pre-trained machine learning model that correlates",
    "start": "219280",
    "end": "224680"
  },
  {
    "text": "node utilization with power consumption to predict the most suitable Noe for",
    "start": "224680",
    "end": "229840"
  },
  {
    "text": "your workloads and these predictions are based on the resource need of the",
    "start": "229840",
    "end": "236280"
  },
  {
    "text": "incoming workloads and the real time utilization of the",
    "start": "236280",
    "end": "242640"
  },
  {
    "text": "nodes so let's go where peak's workflow and this is quite going to be very",
    "start": "242640",
    "end": "250079"
  },
  {
    "text": "descriptive so stay with me uh we'll start with what happens or the pre-processing we need per scheduling",
    "start": "250079",
    "end": "257519"
  },
  {
    "text": "cycle so we start with taking a note in your cluster and you have all the notes",
    "start": "257519",
    "end": "264040"
  },
  {
    "text": "that you have in your cluster you extract some metrics right now we are using energy consumption metrics that",
    "start": "264040",
    "end": "270919"
  },
  {
    "text": "comes from Kepler and we use usage node usage metrics that comes from uh node",
    "start": "270919",
    "end": "277039"
  },
  {
    "text": "exporter or load Watcher but the thing to note over here is you can bring your",
    "start": "277039",
    "end": "282160"
  },
  {
    "text": "own metric provider it doesn't really matter what metric provider you're using",
    "start": "282160",
    "end": "287479"
  },
  {
    "text": "as long as that metric provider pushed the metrics into Prometheus and you can read from Prometheus uh the energy",
    "start": "287479",
    "end": "294639"
  },
  {
    "text": "consumption and the node utilization metrics so for each of the node you will have these metrics and then you will",
    "start": "294639",
    "end": "302240"
  },
  {
    "text": "create a node power model which is a function of node utilization node energy",
    "start": "302240",
    "end": "309400"
  },
  {
    "text": "consumption and the workload request that your pod has so you train a machine",
    "start": "309400",
    "end": "314919"
  },
  {
    "text": "learning model for each of the node within the cluster the next step is you have an",
    "start": "314919",
    "end": "321400"
  },
  {
    "text": "incoming workload and the Pod has its resource requirement that can be read",
    "start": "321400",
    "end": "326560"
  },
  {
    "text": "from the Manifest so uh now this processing is done per node so what you",
    "start": "326560",
    "end": "333160"
  },
  {
    "text": "do is you consider a cluster node and its power model so you have a node you",
    "start": "333160",
    "end": "338319"
  },
  {
    "text": "have the power model you try to uh predict what is going to be the change",
    "start": "338319",
    "end": "343840"
  },
  {
    "text": "in the nodes instantaneous power if you schedule this part on this node and you",
    "start": "343840",
    "end": "349919"
  },
  {
    "text": "whatever is the change in the node's instantaneous power is what is returned as the plug-in",
    "start": "349919",
    "end": "357319"
  },
  {
    "text": "score so you do this for all the nodes in your cluster if you have more nodes",
    "start": "357319",
    "end": "362520"
  },
  {
    "text": "you just repeat this process if no then you just go towards normalizing the",
    "start": "362520",
    "end": "367599"
  },
  {
    "text": "plugging score so over here we have given a basic normalization function but",
    "start": "367599",
    "end": "373199"
  },
  {
    "text": "essentially what it does that it gives higher score for nodes which has less change in",
    "start": "373199",
    "end": "379479"
  },
  {
    "text": "power and once you have uh the normalized plug-in scoring done then you",
    "start": "380199",
    "end": "387000"
  },
  {
    "text": "can put a suitable weight for the Peaks plugging and weight is essentially",
    "start": "387000",
    "end": "392599"
  },
  {
    "text": "kubernetes let you assign a weight to the plug-in and depending on the weight",
    "start": "392599",
    "end": "398080"
  },
  {
    "text": "uh the the priority or the importance is given to that particular",
    "start": "398080",
    "end": "403759"
  },
  {
    "text": "plugin so uh once you have the weights you have the score you just uh this",
    "start": "403840",
    "end": "410360"
  },
  {
    "text": "whole cycle will give you a node that is best fitted for the Pod uh if it uh and",
    "start": "410360",
    "end": "417280"
  },
  {
    "text": "you decide that by the highest normalized score so if you have more parts to place you",
    "start": "417280",
    "end": "422360"
  },
  {
    "text": "just repeat this process if not then your cluster that's that's really",
    "start": "422360",
    "end": "427599"
  },
  {
    "text": "happens you will always have more workload so let's talk about the power",
    "start": "427599",
    "end": "433400"
  },
  {
    "text": "model training and inferencing so each note will have run",
    "start": "433400",
    "end": "439440"
  },
  {
    "text": "some benchmarks and this is like a screenshot of all we were using stress ngy for this particular experiment so we",
    "start": "439440",
    "end": "446720"
  },
  {
    "text": "ran a lot of Benchmark we uh over utilized 100% utilization of all the node we collected the metrics the",
    "start": "446720",
    "end": "454039"
  },
  {
    "text": "particular metrics we focused on were CP utilization and power",
    "start": "454039",
    "end": "459319"
  },
  {
    "text": "consumption and using those metrics you generate your node power",
    "start": "459879",
    "end": "465319"
  },
  {
    "text": "profile and inside the Pak plug-in you you import the model power parameters",
    "start": "465319",
    "end": "472720"
  },
  {
    "text": "and once you have the Pod you get the resource requirement of the Pod you get",
    "start": "472720",
    "end": "477919"
  },
  {
    "text": "the node uh model parameters from the model and then you predict the best node for that particular",
    "start": "477919",
    "end": "484919"
  },
  {
    "text": "B and the prerequisite for this was a metric provider as I said you can bring",
    "start": "484919",
    "end": "490960"
  },
  {
    "text": "your own metric provider as long as they push the metrics into Prometheus we used Kepler and node exporter you can also",
    "start": "490960",
    "end": "498599"
  },
  {
    "text": "bring your own model like it doesn't really matter how you have trained your model as long as Peak can use that model",
    "start": "498599",
    "end": "505879"
  },
  {
    "text": "so uh the training is outside the scope of Peaks but when we were creating this",
    "start": "505879",
    "end": "511120"
  },
  {
    "text": "we have a training pipeline but it doesn't matter how you train your model Peaks just uses those model to the uh",
    "start": "511120",
    "end": "518080"
  },
  {
    "text": "inferencing so the important thing to note is you you can train your model",
    "start": "518080",
    "end": "523240"
  },
  {
    "text": "depending on your cluster Behavior so if you have like AI specific uh workloads",
    "start": "523240",
    "end": "529240"
  },
  {
    "text": "then you can create the model according to the AI if you have Edge specific so",
    "start": "529240",
    "end": "534519"
  },
  {
    "text": "you will consider those parameters in your model training and you'll have Ed specific uh node model so next we have",
    "start": "534519",
    "end": "542200"
  },
  {
    "text": "experiments and Krishna will give you an overview yeah thanks par so we have gone",
    "start": "542200",
    "end": "549079"
  },
  {
    "text": "through the what Peaks is till now uh I would like to take you through the value",
    "start": "549079",
    "end": "554800"
  },
  {
    "text": "of Peaks how you can realize actually the value of paks so that we all can",
    "start": "554800",
    "end": "560160"
  },
  {
    "text": "appreciate the work right so uh before I jump into the um different use cases uh",
    "start": "560160",
    "end": "567480"
  },
  {
    "text": "uh let me also first explain you briefly about the experimental setup that we",
    "start": "567480",
    "end": "573040"
  },
  {
    "text": "have uh so it's a two note cluster that we have considered for the evaluation of",
    "start": "573040",
    "end": "578640"
  },
  {
    "text": "Peaks performance and uh we considered in particular a bare metal cluster",
    "start": "578640",
    "end": "584600"
  },
  {
    "text": "kubernetes cluster with two nodes uh these nodes being heterogeneous in nature what heterogeneous here refers to",
    "start": "584600",
    "end": "591240"
  },
  {
    "text": "is the resources allocated to these uh uh nodes are U not exactly the same",
    "start": "591240",
    "end": "599720"
  },
  {
    "text": "right so one is a high power machine one is a low power machine you can see the",
    "start": "599720",
    "end": "605519"
  },
  {
    "text": "uh CPU course varies from 8 to 40 and also the memory allocated is very high",
    "start": "605519",
    "end": "611519"
  },
  {
    "text": "in this node and uh and then we uh Run The Benchmark or clotes and fit the try",
    "start": "611519",
    "end": "620320"
  },
  {
    "text": "to capture the uh nod's Power behavior and these caros represent the uh nodes",
    "start": "620320",
    "end": "625720"
  },
  {
    "text": "power Behavior the uh one in blue is for the uh node",
    "start": "625720",
    "end": "631200"
  },
  {
    "text": "one and the one in Orange here is for node two uh you can see that uh the the",
    "start": "631200",
    "end": "638320"
  },
  {
    "text": "power caros actually depict two things one is uh they consume some power even",
    "start": "638320",
    "end": "644800"
  },
  {
    "text": "when the node is uh almost not utilized right even in the idle State they",
    "start": "644800",
    "end": "650440"
  },
  {
    "text": "consume some power and the power consumed the idle power is very high for the high power node and low for the low",
    "start": "650440",
    "end": "658200"
  },
  {
    "text": "power node and also so during the active phase as well the amount of power consumed by the high power node is very",
    "start": "658200",
    "end": "664600"
  },
  {
    "text": "high so this we can uh look at this from this graph and with this you can",
    "start": "664600",
    "end": "670519"
  },
  {
    "text": "actually U uh relatively rank these uh nodes one being the more efficient node",
    "start": "670519",
    "end": "676959"
  },
  {
    "text": "the low power uh node is more efficient the high power node is the less",
    "start": "676959",
    "end": "682680"
  },
  {
    "text": "efficient uh right uh another important um aspect is that uh as I said some of",
    "start": "682680",
    "end": "689560"
  },
  {
    "text": "the use cases we uh go through to realize the value of pak's plug-in um uh",
    "start": "689560",
    "end": "696639"
  },
  {
    "text": "this is the scheme of evaluation right we uh first um run some workloads using the",
    "start": "696639",
    "end": "705680"
  },
  {
    "text": "kuet is default scheduler then we repeat the same experiment uh replacing the",
    "start": "705680",
    "end": "712760"
  },
  {
    "text": "default kuber scheduler with Peaks scheduler uh during these two scenarios",
    "start": "712760",
    "end": "718320"
  },
  {
    "text": "we calculate the the we collect the metrics like node utilization Energy Efficiency metrics and then compare how",
    "start": "718320",
    "end": "725720"
  },
  {
    "text": "much is the uh energy consumption during this experiment and uh see if there is",
    "start": "725720",
    "end": "730800"
  },
  {
    "text": "any uh saving in the energy consumption using Peaks if that is the case then",
    "start": "730800",
    "end": "735920"
  },
  {
    "text": "that is attributed to the value of Peaks plugin so this this this scheme of",
    "start": "735920",
    "end": "741639"
  },
  {
    "text": "evaluation is uh common across all the use cases that uh I will take you through and this is the particular",
    "start": "741639",
    "end": "748560"
  },
  {
    "text": "cluster setup on which we uh demonstrated these use cases but these use cases are generic enough to be able",
    "start": "748560",
    "end": "755279"
  },
  {
    "text": "to uh carry out on uh any cluster node with uh any number of uh nodes and with",
    "start": "755279",
    "end": "762600"
  },
  {
    "text": "any types of homogeneous or hetrogeneous uh only that the uh amount of savings will",
    "start": "762600",
    "end": "769680"
  },
  {
    "text": "differ okay so let me jump into the uh list of use cases that I would like to take you through um so this one is the",
    "start": "769680",
    "end": "778240"
  },
  {
    "text": "most regular conf fation when a part is being deployed with the help of a kubernetes scheduler second one is a",
    "start": "778240",
    "end": "784839"
  },
  {
    "text": "scaling of a part using horizontal part order scaler uh and uh using a cub Cal",
    "start": "784839",
    "end": "790519"
  },
  {
    "text": "scale that is another very popular CLI interface uh to operate on the different",
    "start": "790519",
    "end": "797560"
  },
  {
    "text": "API objects and fourth one is migration of a pod VI a pod explicit eviction and",
    "start": "797560",
    "end": "803519"
  },
  {
    "text": "the fifth one is a much more generic case uh that is cluster autoscaler we will see more details and of the these",
    "start": "803519",
    "end": "809560"
  },
  {
    "text": "uh use cases but this is the gist of the use cases that I would like to spend the next 10 to 15 minutes uh the first uh",
    "start": "809560",
    "end": "816079"
  },
  {
    "text": "use case is deployment of a pod uh using kubernetes scheduler uh under this uh we",
    "start": "816079",
    "end": "822240"
  },
  {
    "text": "considered the cluster uh with the two nodes and we assume there is no other",
    "start": "822240",
    "end": "827320"
  },
  {
    "text": "application or part running on this cluster um and only one part is to be",
    "start": "827320",
    "end": "833480"
  },
  {
    "text": "scheduled um that belongs to our application um and that part when it is",
    "start": "833480",
    "end": "839160"
  },
  {
    "text": "scheduled using the default scheduler because the default scheduler is not cognizant of the nodes efficiencies it",
    "start": "839160",
    "end": "845600"
  },
  {
    "text": "can place this object this part on any of these nodes assume that it places this part on the Node in on the Node",
    "start": "845600",
    "end": "853120"
  },
  {
    "text": "which is energy inefficient so and if it runs for a while about 10 minutes uh you can see that the node utilization of",
    "start": "853120",
    "end": "859680"
  },
  {
    "text": "that energy inefficient node um is uh at some level it is around 12 percentage",
    "start": "859680",
    "end": "865759"
  },
  {
    "text": "you could actually run a bigger workload and the utilization can be much more higher then repeat the same experiment",
    "start": "865759",
    "end": "872759"
  },
  {
    "text": "with replacing the default scheduler with Peak scheduler but this time",
    "start": "872759",
    "end": "878279"
  },
  {
    "text": "because Peak scheduler is aware of the energy efficiencies of these nodes it will choose the node N1 which is most",
    "start": "878279",
    "end": "885399"
  },
  {
    "text": "energy efficient in this case so you can see the part uh executing on this node",
    "start": "885399",
    "end": "891120"
  },
  {
    "text": "again it runs for about the same period 10 minutes right and uh the right side",
    "start": "891120",
    "end": "897480"
  },
  {
    "text": "graph actually uh depicts uh the energy consumption of the cluster across these",
    "start": "897480",
    "end": "904240"
  },
  {
    "text": "two nodes over the time progresses now you can see that using the default scheduler the uh energy is consumed some",
    "start": "904240",
    "end": "912759"
  },
  {
    "text": "uh some amount and with using Peaks the amount of energy consumption is lesser",
    "start": "912759",
    "end": "917839"
  },
  {
    "text": "so there is a saving the gap between these two graphs represents the saving in the uh amount of energy consumed by",
    "start": "917839",
    "end": "925880"
  },
  {
    "text": "the cluster across its all nodes so so uh in this experiment what we observed",
    "start": "925880",
    "end": "931319"
  },
  {
    "text": "is the saving amounts to uh roughly about 12% uh and points to note before I",
    "start": "931319",
    "end": "937120"
  },
  {
    "text": "jump into the uh other use case is that the Energy savings can be more if you",
    "start": "937120",
    "end": "943160"
  },
  {
    "text": "run this same experiment for longer duration because the uh accumulation um",
    "start": "943160",
    "end": "949440"
  },
  {
    "text": "accumulated energy over time uh over time we have some portions of energy",
    "start": "949440",
    "end": "955720"
  },
  {
    "text": "which gets accumulated over time and hence over time if you run the workload for longer duration you see more savings",
    "start": "955720",
    "end": "962560"
  },
  {
    "text": "so that's one thing and second thing is because the active energy for the uh the",
    "start": "962560",
    "end": "968680"
  },
  {
    "text": "high power machine here is very high so the as the utilization of that node goes",
    "start": "968680",
    "end": "975040"
  },
  {
    "text": "up so here we are running these noes at lower utilization if you increase the utilization of these nodes you would",
    "start": "975040",
    "end": "980959"
  },
  {
    "text": "also see higher savings so there are two ways uh for realizing higher savings and",
    "start": "980959",
    "end": "986600"
  },
  {
    "text": "uh I would also like to uh extrapolate the fact that if these parts are running",
    "start": "986600",
    "end": "993680"
  },
  {
    "text": "on these clusters along with other applications as well we can realize such a saving just for the sake of",
    "start": "993680",
    "end": "1000319"
  },
  {
    "text": "understanding the concept more easily we uh we assumed the fact that there is no other application running at this time",
    "start": "1000319",
    "end": "1006360"
  },
  {
    "text": "but this can be collocated or Co executing with other applications",
    "start": "1006360",
    "end": "1012759"
  },
  {
    "text": "Parts the next use case is a scaling of a part using horizontal Part auto scaler",
    "start": "1013040",
    "end": "1018639"
  },
  {
    "text": "so so in this use case actually we have a particular uh deployment and that",
    "start": "1018639",
    "end": "1025079"
  },
  {
    "text": "deployment is configured with the um horizontal part AO scaler the particular",
    "start": "1025079",
    "end": "1032918"
  },
  {
    "text": "uh uh CPU percentage here represents the fact that when the uh resource",
    "start": "1032919",
    "end": "1038400"
  },
  {
    "text": "utilization allocated for this part goes beyond the 50% uh limit uh then a new",
    "start": "1038400",
    "end": "1045798"
  },
  {
    "text": "part is created by the HPA uh and we want to have minimum of one part at any",
    "start": "1045799",
    "end": "1053480"
  },
  {
    "text": "time and a maximum of 30 parts uh to be configured by the HPA controller and",
    "start": "1053480",
    "end": "1060679"
  },
  {
    "text": "when we do this experiment using the default Peaks scheduler so you can see",
    "start": "1060679",
    "end": "1065919"
  },
  {
    "text": "that as the load increases uh then uh the HP controller uh starts",
    "start": "1065919",
    "end": "1074360"
  },
  {
    "text": "creating uh more parts and when the HBA controller wants to create a new pod uh",
    "start": "1074360",
    "end": "1081120"
  },
  {
    "text": "the placement of that pod the decision is made by the scheduler default scheduler and uh uh because it's not the",
    "start": "1081120",
    "end": "1090480"
  },
  {
    "text": "default scheduler does something called spreading of the parts on the nodes right so it equally tries to uh place",
    "start": "1090480",
    "end": "1099280"
  },
  {
    "text": "these parts over the time so during this experiment we created about 15 parts and",
    "start": "1099280",
    "end": "1105880"
  },
  {
    "text": "those parts were alternatively being placed uh between the nodes of this cluster so you can see that the at any",
    "start": "1105880",
    "end": "1112799"
  },
  {
    "text": "point in time almost the utilization across the these two nodes is at the same level all right so and uh",
    "start": "1112799",
    "end": "1121520"
  },
  {
    "text": "now uh come come back to the um the peak scenario when Peaks is uh used for",
    "start": "1121520",
    "end": "1129640"
  },
  {
    "text": "scheduling uh when the load increases the HPA controller horizontal Part auto",
    "start": "1129640",
    "end": "1135320"
  },
  {
    "text": "scalar controller uh does the same kind of scaling that uh that it decides a new",
    "start": "1135320",
    "end": "1140840"
  },
  {
    "text": "part to be created as the load increases but this time the part placement is uh",
    "start": "1140840",
    "end": "1146440"
  },
  {
    "text": "the decision is made by the uh pxs schedular plug-in and uh because PX is",
    "start": "1146440",
    "end": "1152559"
  },
  {
    "text": "aware of the Energy Efficiency of nodes it wants to First exhaust all the resources available on the most energy",
    "start": "1152559",
    "end": "1159840"
  },
  {
    "text": "efficient node so it tries to keep the parts as much as possible on node one",
    "start": "1159840",
    "end": "1164960"
  },
  {
    "text": "first okay so you can see that the utilization of node one goes all the way",
    "start": "1164960",
    "end": "1170320"
  },
  {
    "text": "up to 100% before a pod is placed on the",
    "start": "1170320",
    "end": "1175720"
  },
  {
    "text": "Node two right so initially the Pod um the node one is packed with the parts",
    "start": "1175720",
    "end": "1182720"
  },
  {
    "text": "and then once the node one all the resources are exhausted then note two starts getting packed right so this is",
    "start": "1182720",
    "end": "1190240"
  },
  {
    "text": "the place where note two starts getting packed and you can see that at this level node one uh is at 100% utilization",
    "start": "1190240",
    "end": "1199039"
  },
  {
    "text": "okay now you compare the energy consumption of the cluster across these two nodes under these two scenarios the",
    "start": "1199039",
    "end": "1206760"
  },
  {
    "text": "Orange Line represents the energy consumption using the default schedular",
    "start": "1206760",
    "end": "1212760"
  },
  {
    "text": "plugin across these two nodes whereas the blue one represents the energy consumption across the cluster nodes",
    "start": "1212760",
    "end": "1220679"
  },
  {
    "text": "both the nodes using the Peaks uh schedular plugin so you can see there is a gap between the uh energy uh",
    "start": "1220679",
    "end": "1227799"
  },
  {
    "text": "consumption uh over the period right so as the time progresses the Gap increases",
    "start": "1227799",
    "end": "1233760"
  },
  {
    "text": "um so this is because uh while time progresses actually in this default scenario we are",
    "start": "1233760",
    "end": "1241679"
  },
  {
    "text": "using the more resources of uh node two which is energy inefficient hence the",
    "start": "1241679",
    "end": "1248120"
  },
  {
    "text": "Gap is increasing now there is another observation that you can make is that after a point here the um Gap starts",
    "start": "1248120",
    "end": "1255640"
  },
  {
    "text": "decreasing why the blue curve starts growing up here is because uh this",
    "start": "1255640",
    "end": "1261120"
  },
  {
    "text": "represents the time scale timeline where uh using Peaks we started using the",
    "start": "1261120",
    "end": "1266880"
  },
  {
    "text": "resources of node 2 which is energy inefficient right so as long as this was not used the Gap was increasing uh",
    "start": "1266880",
    "end": "1273799"
  },
  {
    "text": "because we are using under uh the default scenario resources of node 2 in",
    "start": "1273799",
    "end": "1279600"
  },
  {
    "text": "this so hence the there is a increase in the Gap and uh here uh we started uh",
    "start": "1279600",
    "end": "1286760"
  },
  {
    "text": "using the resources of node hence the Gap starts decreasing so this",
    "start": "1286760",
    "end": "1292760"
  },
  {
    "text": "demonstrates the fact that using uh HPA uh horizontal pod autoscaler with Peaks",
    "start": "1292760",
    "end": "1299039"
  },
  {
    "text": "plugin you can still realize Energy savings and horizontal pod Auto scalar using horizontal Part auto scalar along",
    "start": "1299039",
    "end": "1306440"
  },
  {
    "text": "with uh the deployment is a very common use case because HPA enables us uh do",
    "start": "1306440",
    "end": "1313279"
  },
  {
    "text": "scale up and scale down automatically right so let me go to the",
    "start": "1313279",
    "end": "1318799"
  },
  {
    "text": "uh third use case in the third use case we see uh the advantage of peaks with a",
    "start": "1318799",
    "end": "1326120"
  },
  {
    "text": "very famous um CLI that is Cube cutle scale the cube cutle scale actually",
    "start": "1326120",
    "end": "1332360"
  },
  {
    "text": "interface uh is helpful to scale up and scale down the parts of an application",
    "start": "1332360",
    "end": "1338320"
  },
  {
    "text": "where the uh API resources uh are not just being the deployment resources but",
    "start": "1338320",
    "end": "1344279"
  },
  {
    "text": "they can be um the stateful sets or replication controller or replica set",
    "start": "1344279",
    "end": "1349799"
  },
  {
    "text": "and so on so there are a variety of application uh API objects that",
    "start": "1349799",
    "end": "1355559"
  },
  {
    "text": "kubernetes actually allows users uh to work with and under uh these variety of",
    "start": "1355559",
    "end": "1363480"
  },
  {
    "text": "uh API objects uh to configure right to scale up scale down this is the API now",
    "start": "1363480",
    "end": "1370400"
  },
  {
    "text": "what happens let's see with the default uh controller plugin uh assume that we",
    "start": "1370400",
    "end": "1377120"
  },
  {
    "text": "have a deployment uh which has two parts initially assume that these two parts",
    "start": "1377120",
    "end": "1382440"
  },
  {
    "text": "are deployed on node one and because this is the most energy efficient uh",
    "start": "1382440",
    "end": "1388159"
  },
  {
    "text": "assume that due to some scenario uh the nodes the parts are initially placed on these nodes now uh think that the",
    "start": "1388159",
    "end": "1396240"
  },
  {
    "text": "application Warner wants to scale up the number of parts from 2 to five now the",
    "start": "1396240",
    "end": "1401279"
  },
  {
    "text": "new three parts they are scheduled by the default scheduler default kuet",
    "start": "1401279",
    "end": "1407440"
  },
  {
    "text": "scheduler and the node two because default scheduler behaves uh the thumb",
    "start": "1407440",
    "end": "1413799"
  },
  {
    "text": "rule is to spread the nodes across the nodes right so",
    "start": "1413799",
    "end": "1419000"
  },
  {
    "text": "it places this newly created Parts on node two even though there is a space",
    "start": "1419000",
    "end": "1424960"
  },
  {
    "text": "for uh deploying these parts on node one which is more energy efficient because it cannot simply do that it is not aware",
    "start": "1424960",
    "end": "1432039"
  },
  {
    "text": "of the Energy Efficiency of these nodes now U on the other hand if you uh",
    "start": "1432039",
    "end": "1437279"
  },
  {
    "text": "consider the scenario of of Peaks plug-in uh consider that we have",
    "start": "1437279",
    "end": "1443080"
  },
  {
    "text": "initially two nodes and sorry two parts which are deployed on the most energy",
    "start": "1443080",
    "end": "1448480"
  },
  {
    "text": "efficient node uh and now we are doing the same experiment we are scaling up nodes number of parts from 2 to5 but",
    "start": "1448480",
    "end": "1455559"
  },
  {
    "text": "this time the new parts are deployed uh on the Node one uh because this",
    "start": "1455559",
    "end": "1461960"
  },
  {
    "text": "deployment is uh handled by Peak schedular plugin and it is aware of the",
    "start": "1461960",
    "end": "1467520"
  },
  {
    "text": "Energy Efficiency right so please note the fact that um",
    "start": "1467520",
    "end": "1472960"
  },
  {
    "text": "okay so maybe I will uh go to the another use case migration of part so",
    "start": "1472960",
    "end": "1479840"
  },
  {
    "text": "this is the fourth use case where um consider the scenario uh where at any",
    "start": "1479840",
    "end": "1485320"
  },
  {
    "text": "random timeing instance uh certain parts may be running on the uh cluster nodes",
    "start": "1485320",
    "end": "1490960"
  },
  {
    "text": "and U uh assume that at a given point in time we we have an application running",
    "start": "1490960",
    "end": "1496440"
  },
  {
    "text": "and its two parts are running two different nodes but uh you know that uh",
    "start": "1496440",
    "end": "1502080"
  },
  {
    "text": "the node one t 1 is more energy efficient so ideally you have two",
    "start": "1502080",
    "end": "1507240"
  },
  {
    "text": "options here you can let the um application run as is and complete or",
    "start": "1507240",
    "end": "1512520"
  },
  {
    "text": "you can actually try to move this pod from node 2 to node one because node running on node one is more energy",
    "start": "1512520",
    "end": "1518799"
  },
  {
    "text": "efficient so if you try to delete that pod manually and uh this time uh because",
    "start": "1518799",
    "end": "1525279"
  },
  {
    "text": "the API object uh the replica set uh knows that a part of this number of",
    "start": "1525279",
    "end": "1531840"
  },
  {
    "text": "replicas is deleted it needs to create a new part so it will try to create that",
    "start": "1531840",
    "end": "1537320"
  },
  {
    "text": "part to meet the desired state of two uh replicas so but this time U the part is",
    "start": "1537320",
    "end": "1544320"
  },
  {
    "text": "again created on the same node because this uh the underlying scheduler plugin",
    "start": "1544320",
    "end": "1550320"
  },
  {
    "text": "is default on the other hand you repeat the same experiment with",
    "start": "1550320",
    "end": "1555440"
  },
  {
    "text": "PX plugin assume that there is another application running which is already consuming most of the resources of",
    "start": "1555440",
    "end": "1562039"
  },
  {
    "text": "energy efficient node and our particular application PHP apach uh is uh uh",
    "start": "1562039",
    "end": "1567840"
  },
  {
    "text": "running with two parts one on node one and another one on node two um so assume",
    "start": "1567840",
    "end": "1573919"
  },
  {
    "text": "that after a while the other application CPU stress test exits uh so when it",
    "start": "1573919",
    "end": "1579399"
  },
  {
    "text": "terminates uh it releases the resources allocated to this application so now we",
    "start": "1579399",
    "end": "1584840"
  },
  {
    "text": "have the same scenario where uh the either we can let this application run",
    "start": "1584840",
    "end": "1590480"
  },
  {
    "text": "as is or try to see if we can move this part to the energy efficient node so",
    "start": "1590480",
    "end": "1595720"
  },
  {
    "text": "this time try again the same command try to delete the Pod that is running on the uh energy inefficient node uh and this",
    "start": "1595720",
    "end": "1602919"
  },
  {
    "text": "time again replica set knows that one one part is uh exiting so it needs to create a new part it tries to create",
    "start": "1602919",
    "end": "1609640"
  },
  {
    "text": "that new pod and now this time the scheduler plugin will try to allocate",
    "start": "1609640",
    "end": "1614720"
  },
  {
    "text": "that on the energy efficient node so this mimics the Auto migration of parts",
    "start": "1614720",
    "end": "1620679"
  },
  {
    "text": "from energy inefficient nodes to energy efficient nodes uh at random time periods right so this we have done it",
    "start": "1620679",
    "end": "1627480"
  },
  {
    "text": "using a script but it is always possible to um uh exercise the same behavior",
    "start": "1627480",
    "end": "1634039"
  },
  {
    "text": "using um um some uh automated scripts like uh jobs and Crown jobs and so on",
    "start": "1634039",
    "end": "1640640"
  },
  {
    "text": "right which can actually uh be monitoring the energy the resource usage",
    "start": "1640640",
    "end": "1645840"
  },
  {
    "text": "of energy efficient nodes and if they find that energy efficient nodes are anytime underutilized then they try to",
    "start": "1645840",
    "end": "1653120"
  },
  {
    "text": "move um workloads from energy inefficient nodes to energy efficient nodes automatically right so all this is",
    "start": "1653120",
    "end": "1660320"
  },
  {
    "text": "possible to uh automate so the last use case that I want to touch upon is uh",
    "start": "1660320",
    "end": "1668080"
  },
  {
    "text": "using cluster autoscaler in the previous use case we have seen the scenario where",
    "start": "1668080",
    "end": "1673240"
  },
  {
    "text": "uh the parts corresponding to a given uh deployment we are trying to migrate what",
    "start": "1673240",
    "end": "1678279"
  },
  {
    "text": "if we want to do it uh across all the deployments of a of a cluster right why",
    "start": "1678279",
    "end": "1684159"
  },
  {
    "text": "can't we do it so if we try to do that uh across the cluster uh by trying to",
    "start": "1684159",
    "end": "1691720"
  },
  {
    "text": "migrate all the parts running on the most energy inefficient node of the cluster to any other node of that",
    "start": "1691720",
    "end": "1697559"
  },
  {
    "text": "cluster then ensure that the most energy inefficient node of that cluster is um",
    "start": "1697559",
    "end": "1703640"
  },
  {
    "text": "dried up of the parts then the cluster autoscaler will identify that this is",
    "start": "1703640",
    "end": "1708799"
  },
  {
    "text": "underutilized and automatically will delete that part then by doing that you",
    "start": "1708799",
    "end": "1714519"
  },
  {
    "text": "are not only saving the active power so by the way when we do the migration of",
    "start": "1714519",
    "end": "1720320"
  },
  {
    "text": "pod we are actually saving on the active power but by shutting down the nodes we",
    "start": "1720320",
    "end": "1726360"
  },
  {
    "text": "are also saving on the idle power and if you see these two graphs uh the idle",
    "start": "1726360",
    "end": "1731919"
  },
  {
    "text": "when the node is any node you consider any cluster node when it is in idle state it accounts for some energy and",
    "start": "1731919",
    "end": "1739480"
  },
  {
    "text": "which is also known as ideal energy and this is increasing in nature even if you",
    "start": "1739480",
    "end": "1744519"
  },
  {
    "text": "just keep the uh node idle right you you think that okay my node is not doing any",
    "start": "1744519",
    "end": "1750519"
  },
  {
    "text": "work and still it is consuming a lot of power and it is a significant portion of",
    "start": "1750519",
    "end": "1755760"
  },
  {
    "text": "node's total power so shutting down the node is one of the best ways to handle",
    "start": "1755760",
    "end": "1760919"
  },
  {
    "text": "Energy savings so this is the use case that uh that demonstrates uh shutting",
    "start": "1760919",
    "end": "1767720"
  },
  {
    "text": "down so before I uh move on the other aspects let me because of the uh time",
    "start": "1767720",
    "end": "1775320"
  },
  {
    "text": "constraints we are not actually doing a live demo but this is a uh snip of the",
    "start": "1775320",
    "end": "1781120"
  },
  {
    "text": "logs from the um live cluster where this demonstrates the fact that a incoming",
    "start": "1781120",
    "end": "1786960"
  },
  {
    "text": "pod uh is placed on a node which has less jump in energy consumption over a",
    "start": "1786960",
    "end": "1793880"
  },
  {
    "text": "node which has more jump in the energy consumption so that means that we are making the decision where the incoming",
    "start": "1793880",
    "end": "1800440"
  },
  {
    "text": "part is placed on a node such that the change in the Clusters energy consumption is lesser by making this",
    "start": "1800440",
    "end": "1806840"
  },
  {
    "text": "decision which is the Wier decision in our context okay there are some FAQs I will",
    "start": "1806840",
    "end": "1813880"
  },
  {
    "text": "come to this slide later before that let me spend couple of minutes on future work for example the part migration",
    "start": "1813880",
    "end": "1820000"
  },
  {
    "text": "scenario that I explained uh we have demonstrated Energy savings possib",
    "start": "1820000",
    "end": "1825200"
  },
  {
    "text": "possibility with uh part migration but uh one can actually automate the same workflow that I explained using",
    "start": "1825200",
    "end": "1831960"
  },
  {
    "text": "automated scripts very easily so that is one fature work item for us immediate on",
    "start": "1831960",
    "end": "1837640"
  },
  {
    "text": "milestone for us and shutting down the nodes to minimize the power consumption again this one the use case five that I",
    "start": "1837640",
    "end": "1844080"
  },
  {
    "text": "have referred to is done uh using manually uh running scripts but one can",
    "start": "1844080",
    "end": "1851240"
  },
  {
    "text": "write automated scripts to exercise the part migration across the cluster and",
    "start": "1851240",
    "end": "1856880"
  },
  {
    "text": "across the nodes in automated fashion the next enhancement possible is",
    "start": "1856880",
    "end": "1862320"
  },
  {
    "text": "vertical part Auto scalar we have seen that with the use of HPA horizontal prod out scalar there is a benefit uh in",
    "start": "1862320",
    "end": "1869760"
  },
  {
    "text": "terms of Energy savings uh with when it comes to vpa uh when vpa tries to scale",
    "start": "1869760",
    "end": "1875639"
  },
  {
    "text": "up the resources allocated uh on a node for a particular part you could actually",
    "start": "1875639",
    "end": "1882600"
  },
  {
    "text": "um make vpa more energy uh aware by enhancing the",
    "start": "1882600",
    "end": "1888600"
  },
  {
    "text": "the by performing the scale up on those nodes which are energy efficient when",
    "start": "1888600",
    "end": "1894360"
  },
  {
    "text": "you want to uh scale of the resources allocated for a part using vpa uh today",
    "start": "1894360",
    "end": "1900600"
  },
  {
    "text": "it does on any of these nodes without being aware of the Energy Efficiency right you could actually make the vpa",
    "start": "1900600",
    "end": "1907200"
  },
  {
    "text": "aware of the Energy Efficiency and ensure that it scales up those parts running on most energy efficient nodes",
    "start": "1907200",
    "end": "1914799"
  },
  {
    "text": "so the another one is a cube cutle scale okay I will not spend that much time and the here we have listed some of the",
    "start": "1914799",
    "end": "1921399"
  },
  {
    "text": "related work and uh yeah here are some of the reports uh the first repo is",
    "start": "1921399",
    "end": "1926919"
  },
  {
    "text": "where the plugin score uh is implemented so this repo paks repo is more for the",
    "start": "1926919",
    "end": "1932240"
  },
  {
    "text": "project management um and the last repo is the Kepler repository which is one of",
    "start": "1932240",
    "end": "1938279"
  },
  {
    "text": "the um dependencies of our project I'll hand it over to uh uh paru yeah before",
    "start": "1938279",
    "end": "1945120"
  },
  {
    "text": "we get into the questions just wanted to know that we are working on a cap that will be uh creating with the kubernetes",
    "start": "1945120",
    "end": "1951760"
  },
  {
    "text": "uh scheduling Sig and so keep on monitoring our Peaks repository and we also have a community meeting that",
    "start": "1951760",
    "end": "1958360"
  },
  {
    "text": "happens once in a month so uh we are trying to build a community and we want more contributors so if you have any",
    "start": "1958360",
    "end": "1965960"
  },
  {
    "text": "suggestions you want to participate in in any discussions or you want to have present to us like a use case please",
    "start": "1965960",
    "end": "1973320"
  },
  {
    "text": "open a discussion or issue on a repository and now we are open to yeah",
    "start": "1973320",
    "end": "1980440"
  },
  {
    "text": "few acknowledgements like Felix was not able to make it uh to our session but he has actively contributed and we also",
    "start": "1980440",
    "end": "1987320"
  },
  {
    "text": "have a s so thank you to both of them and now we are open for",
    "start": "1987320",
    "end": "1994320"
  },
  {
    "text": "questions any questions yeah so yeah sure please goad yeah she's getting the",
    "start": "1995720",
    "end": "2006000"
  },
  {
    "text": "mic hello thank you for the presentation um I saw the future slide",
    "start": "2006760",
    "end": "2013320"
  },
  {
    "text": "at the end and I'm uh just asking uh could the where the power uh is created",
    "start": "2013320",
    "end": "2022279"
  },
  {
    "text": "you know because it's it's important to know where the electricity come from",
    "start": "2022279",
    "end": "2028360"
  },
  {
    "text": "especially for the carbon um will pick PS take in",
    "start": "2028360",
    "end": "2035440"
  },
  {
    "text": "consideration sorry for my French English oh so if I can rephrase you are",
    "start": "2035440",
    "end": "2041799"
  },
  {
    "text": "yeah asking the fact that during the Energy Efficiency power uh consumption minimization process do we also consider",
    "start": "2041799",
    "end": "2049398"
  },
  {
    "text": "the carbon Efficiency do you consider where the node are running uh",
    "start": "2049399",
    "end": "2057158"
  },
  {
    "text": "J speaking so if I got your question right you are talking about do we also assign",
    "start": "2057159",
    "end": "2064320"
  },
  {
    "text": "a value or like a priety if depending on the source of energy right yeah exactly",
    "start": "2064320",
    "end": "2069480"
  },
  {
    "text": "so we did dive into like we two years in 2022 we were working on the use case",
    "start": "2069480",
    "end": "2074638"
  },
  {
    "text": "where we are trying to consider carbon as well as the source of electricity but",
    "start": "2074639",
    "end": "2080878"
  },
  {
    "text": "uh we like our process to be very transparent and open source based and it was very hard to get those data um in",
    "start": "2080879",
    "end": "2087118"
  },
  {
    "text": "Europe they still possible if you use electricity Maps but in us uh it's just",
    "start": "2087119",
    "end": "2092398"
  },
  {
    "text": "very hard to get by so just because we want our policies to be super transparent that's why we right now we",
    "start": "2092399",
    "end": "2098400"
  },
  {
    "text": "are not working on that we explode but we are not uh and and just to add to what parul said uh optimizing for Energy",
    "start": "2098400",
    "end": "2105079"
  },
  {
    "text": "Efficiency and carbon minimization are not exactly the same right and uh there",
    "start": "2105079",
    "end": "2110920"
  },
  {
    "text": "is a a different talk called caspan uh that is going to come tomorrow from the",
    "start": "2110920",
    "end": "2116480"
  },
  {
    "text": "same group uh that we belong to uh please stay up to that uh uh session if",
    "start": "2116480",
    "end": "2121800"
  },
  {
    "text": "you want to really know about carbon optimization yeah so we have one more question yeah sure",
    "start": "2121800",
    "end": "2128200"
  },
  {
    "text": "hello thank you for the talk um I noticed all the pods got scheduled to a",
    "start": "2128200",
    "end": "2133280"
  },
  {
    "text": "single node uh what happens if I have anti-affinity policy uh does the",
    "start": "2133280",
    "end": "2138480"
  },
  {
    "text": "scheduler respect that sure so because uh the scheduler plugin um follows the",
    "start": "2138480",
    "end": "2144160"
  },
  {
    "text": "scheduler framework so it does respect the part Affinity anti Affinity node labels all these aspects and if there",
    "start": "2144160",
    "end": "2151760"
  },
  {
    "text": "are any such nodes filtered after these uh phases only those notes are",
    "start": "2151760",
    "end": "2158040"
  },
  {
    "text": "considered so this plug-in respects the framework of",
    "start": "2158040",
    "end": "2164520"
  },
  {
    "text": "scheduling at the beginning of your talk you referred to like creating a profile for servers uh would that be would that",
    "start": "2165520",
    "end": "2173599"
  },
  {
    "text": "profile be the same for the same configuration of server like if you create a rack where all the servers are",
    "start": "2173599",
    "end": "2179440"
  },
  {
    "text": "the same not necessarily not ohes why would that be yeah because uh even",
    "start": "2179440",
    "end": "2185240"
  },
  {
    "text": "though the hardware configuration may be same but the applications running may be different and different workloads may",
    "start": "2185240",
    "end": "2192000"
  },
  {
    "text": "use different amounts of resources of the uh servers hence your power modeling",
    "start": "2192000",
    "end": "2197560"
  },
  {
    "text": "needs to be cognizant to the workload type that is going to run and there's also two components to it so either you",
    "start": "2197560",
    "end": "2203920"
  },
  {
    "text": "can have like an offline model that you train once and then you don't revisit again but you can also do like keep an",
    "start": "2203920",
    "end": "2210960"
  },
  {
    "text": "online model where depending on the instantaneous of what is happening at the cluster at that moment you keep on retraining your model so so it's very",
    "start": "2210960",
    "end": "2218520"
  },
  {
    "text": "unlikely that two nodes will have identical node model profile uh next person",
    "start": "2218520",
    "end": "2225960"
  },
  {
    "text": "yeah so this is the last question sorry okay so two question uh you thought",
    "start": "2225960",
    "end": "2232280"
  },
  {
    "text": "about creating model per node per node when does this occur what's the",
    "start": "2232280",
    "end": "2239720"
  },
  {
    "text": "overhead cor creating this so yeah that's what we said that that's a pre-processing uh step let me just",
    "start": "2239720",
    "end": "2246200"
  },
  {
    "text": "quickly go back to the slide yeah so at the start when you are just starting and",
    "start": "2246200",
    "end": "2251359"
  },
  {
    "text": "you uh when you have you haven't enabled the scheduling so what you do is you",
    "start": "2251359",
    "end": "2257200"
  },
  {
    "text": "just pre-process the node model for each of the nodes in the cluster so that is",
    "start": "2257200",
    "end": "2262319"
  },
  {
    "text": "pre-scheduling but again as I mentioned there are two approach you can have online or an offline so for an offline",
    "start": "2262319",
    "end": "2268079"
  },
  {
    "text": "you just do it once uh but for online this process keeps on going on and",
    "start": "2268079",
    "end": "2273119"
  },
  {
    "text": "that's when you get the uh model parameters using for example of web",
    "start": "2273119",
    "end": "2278520"
  },
  {
    "text": "hook okay thanks and second question um the processor have poor",
    "start": "2278520",
    "end": "2284960"
  },
  {
    "text": "efficiency modes and they can switch basically between uh po modes now why do",
    "start": "2284960",
    "end": "2293160"
  },
  {
    "text": "not did you try to leverate this features during your sorry say that",
    "start": "2293160",
    "end": "2300599"
  },
  {
    "text": "again I didn't the PO modes CPU can uh downclock and P yeah yeah that is called",
    "start": "2300599",
    "end": "2307160"
  },
  {
    "text": "the functionality uh Dynamic DVS functionality yeah this can work with",
    "start": "2307160",
    "end": "2313359"
  },
  {
    "text": "dvfs though in the use cases we didn't include any particular use case with",
    "start": "2313359",
    "end": "2319040"
  },
  {
    "text": "dvfs but definitely it should uh work with dvfs too it's cognizant to uh the frequency",
    "start": "2319040",
    "end": "2327480"
  },
  {
    "text": "of the node so yeah that's about it and I'll just bring to the QR codes so yeah drop",
    "start": "2327480",
    "end": "2335960"
  },
  {
    "text": "by in a community meeting and we have a kiosk on Friday as well uh 10:30 to",
    "start": "2335960",
    "end": "2341280"
  },
  {
    "text": "12:30 so feel free to talk if you're interested in exploring the use case or",
    "start": "2341280",
    "end": "2346359"
  },
  {
    "text": "start a discussion or whatever thank you so much and also try to uh provide your feedback by scanning this QR code thank",
    "start": "2346359",
    "end": "2354880"
  },
  {
    "text": "[Applause] you",
    "start": "2354880",
    "end": "2360319"
  }
]