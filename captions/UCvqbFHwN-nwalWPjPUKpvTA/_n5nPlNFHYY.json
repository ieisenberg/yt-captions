[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "hello everybody thanks for coming we can get started I guess so",
    "start": "30",
    "end": "6509"
  },
  {
    "text": "my name is Ron Lukey I'm a senior engineer cloud type person on the",
    "start": "6509",
    "end": "12900"
  },
  {
    "text": "platform as a service team at Gannett I'm gonna talk to you a little bit about our container journey full disclosure",
    "start": "12900",
    "end": "21270"
  },
  {
    "text": "this is my first time ever speaking at a conference so in addition to dying a",
    "start": "21270",
    "end": "26369"
  },
  {
    "text": "little inside right now I'm also a person who stutters or if you're from",
    "start": "26369",
    "end": "31500"
  },
  {
    "text": "the UK its stammer so if it looks like I'm rebooting up here or stalling",
    "start": "31500",
    "end": "37170"
  },
  {
    "text": "that's probably why so some of you might be wondering just who or what gannett is",
    "start": "37170",
    "end": "43710"
  },
  {
    "start": "39000",
    "end": "78000"
  },
  {
    "text": "we are a national news a media company with roots in the traditional print of print business we're probably most",
    "start": "43710",
    "end": "51570"
  },
  {
    "text": "recognized by our national brand at USA Today but we're also we also have about",
    "start": "51570",
    "end": "58170"
  },
  {
    "text": "a hundred and thirty other local news outlets in the United States that make up the DSA today I network and that",
    "start": "58170",
    "end": "63960"
  },
  {
    "text": "brings in about 125 million unique visitors a month these are just all the",
    "start": "63960",
    "end": "70799"
  },
  {
    "text": "brands and USA Today a network maybe you grew up around one or you recognize a logo from where you live now so what",
    "start": "70799",
    "end": "79740"
  },
  {
    "start": "78000",
    "end": "141000"
  },
  {
    "text": "does a platform as-a-service do forget we provide the central the central",
    "start": "79740",
    "end": "86000"
  },
  {
    "text": "location for self-service our provisioning and and tooling for",
    "start": "86000",
    "end": "91430"
  },
  {
    "text": "infrastructure for about 40 internal a dev teams across cadet and that's pretty",
    "start": "91430",
    "end": "98579"
  },
  {
    "text": "much where they can test and deploy and and all the way up to you up there apps",
    "start": "98579",
    "end": "104250"
  },
  {
    "text": "all the way up to production workloads and this is all in the public cloud we",
    "start": "104250",
    "end": "109409"
  },
  {
    "text": "do have some data centers but everything on the platform is in in the public cloud so for us not only are we are we",
    "start": "109409",
    "end": "120119"
  },
  {
    "text": "managing the environment that runs all of all of us a today apps but our",
    "start": "120119",
    "end": "125250"
  },
  {
    "text": "customers dev is our prod from from our point of view",
    "start": "125250",
    "end": "130369"
  },
  {
    "text": "we have a we have a five teams on paths I'm on the integration team and we are",
    "start": "131129",
    "end": "136319"
  },
  {
    "text": "responsible for architecting and maintaining the core features of the platform so the start of our at",
    "start": "136319",
    "end": "143849"
  },
  {
    "text": "container journey goes all the way into the deep annals of history at the 2016",
    "start": "143849",
    "end": "150049"
  },
  {
    "text": "United States a presidential election so basically in the news industry there's a",
    "start": "150049",
    "end": "156480"
  },
  {
    "text": "two types of events that you have to plan for there's breaking news when everyone's phone is going off that ends",
    "start": "156480",
    "end": "164700"
  },
  {
    "text": "up in a very unpredictable yet high volume of traffic that dissipates pretty",
    "start": "164700",
    "end": "170939"
  },
  {
    "text": "quickly and really you're a defense against that is make sure all the things",
    "start": "170939",
    "end": "176639"
  },
  {
    "text": "are scalable and if they're not run over over a provisioned which isn't the best",
    "start": "176639",
    "end": "182930"
  },
  {
    "text": "and cash all the things cash in front cashing back protect your origin at all",
    "start": "182930",
    "end": "189060"
  },
  {
    "text": "costs and you should be able to handle all those new spikes that are seem to be",
    "start": "189060",
    "end": "194430"
  },
  {
    "text": "happening much more frequently then there's events like the Super Bowl the Olympics the presidential election which",
    "start": "194430",
    "end": "203699"
  },
  {
    "text": "have a sustained high volume of traffic pattern and the data footprint is of",
    "start": "203699",
    "end": "209790"
  },
  {
    "text": "it's very different because your users want the most accurate most up-to-date information as possible so you can't",
    "start": "209790",
    "end": "217709"
  },
  {
    "text": "protect your origin behind those really high cache details and the content and",
    "start": "217709",
    "end": "223769"
  },
  {
    "text": "the product teams are probably going to be requesting a lot of but deployments to change how the data is being",
    "start": "223769",
    "end": "229680"
  },
  {
    "text": "presented fix any bugs or anything like that so the faster we can do those up",
    "start": "229680",
    "end": "237030"
  },
  {
    "text": "deployments and safer we can the more satisfied our internal and our external what customers are so this was shaping",
    "start": "237030",
    "end": "245729"
  },
  {
    "text": "up to be the largest night of traffic in USA today history and we figured why not",
    "start": "245729",
    "end": "251849"
  },
  {
    "text": "make that or first time we run a container app up to this point we had not a container on the platform so this",
    "start": "251849",
    "end": "259049"
  },
  {
    "text": "was it was new for us it was something that we had been considering but really hadn't and out yet and then our core product",
    "start": "259049",
    "end": "267870"
  },
  {
    "text": "team came to us and said hey we were interested in running the Elections app in a container we said sure and we",
    "start": "267870",
    "end": "274710"
  },
  {
    "text": "scoped it as kind of a stretch goal if we could run maybe 5 or 10% of election",
    "start": "274710",
    "end": "280979"
  },
  {
    "text": "night election night traffic in a container we would call that a success",
    "start": "280979",
    "end": "287930"
  },
  {
    "text": "so we started out by creating a list of our requirements this is just a short",
    "start": "287930",
    "end": "293639"
  },
  {
    "start": "288000",
    "end": "408000"
  },
  {
    "text": "short list of those we are an old enterprise we have a data centers all",
    "start": "293639",
    "end": "299520"
  },
  {
    "text": "across country so we have a lot of legacy infrastructure that we needed to",
    "start": "299520",
    "end": "304680"
  },
  {
    "text": "just be aware of that there were dependencies here that we couldn't we couldn't engineer out of and make",
    "start": "304680",
    "end": "310020"
  },
  {
    "text": "everything just a perfect cloud native app we really have to take advantage of",
    "start": "310020",
    "end": "315750"
  },
  {
    "text": "our existing bootstrapping our existing up features on the platform to bootstrap",
    "start": "315750",
    "end": "322080"
  },
  {
    "text": "these at clusters we had limited time to even do this so we couldn't learn",
    "start": "322080",
    "end": "329250"
  },
  {
    "text": "something new like stand up up for prometheus a cluster or something we had to use what we were using on the",
    "start": "329250",
    "end": "336030"
  },
  {
    "text": "platform and we're a chef shop so we had to use chef 2 to bootstrap these things",
    "start": "336030",
    "end": "343460"
  },
  {
    "text": "auto-scaling was non-negotiable for nodes in containers because of the",
    "start": "343460",
    "end": "349229"
  },
  {
    "text": "breaking news thing and we really didn't have time to like write our own so it would be better if this was inherent in",
    "start": "349229",
    "end": "355919"
  },
  {
    "text": "whatever we chose to use and pretty much everything on the platform is a self",
    "start": "355919",
    "end": "361770"
  },
  {
    "text": "service so these a coup daddies like these container clusters have to be in",
    "start": "361770",
    "end": "367889"
  },
  {
    "text": "the same vein they have to be a self service with minimal manual steps required for a team to get started and I",
    "start": "367889",
    "end": "375659"
  },
  {
    "text": "went too fast and we really needed to maintain our cost boundaries and",
    "start": "375659",
    "end": "381599"
  },
  {
    "text": "ownership on these clusters so that kind of eliminated a multi-tenancy and at the",
    "start": "381599",
    "end": "387840"
  },
  {
    "text": "time the Federation really wasn't like baked quite well yet so we have to find a middle ground in there",
    "start": "387840",
    "end": "393610"
  },
  {
    "text": "somewhere and then of course we wanted to quickly iterate with the with the",
    "start": "393610",
    "end": "398879"
  },
  {
    "text": "community as they're moving a quite fast and just keep up with requests from our",
    "start": "398879",
    "end": "404620"
  },
  {
    "text": "users as they started to adopt containers more so we started out and we",
    "start": "404620",
    "end": "410430"
  },
  {
    "start": "408000",
    "end": "454000"
  },
  {
    "text": "took a whole bunch of sprint tasks we gave them out to team members and we said hey go take one of these things and",
    "start": "410430",
    "end": "417280"
  },
  {
    "text": "POC it maybe speak with sales team or a solutions engineering team and then come",
    "start": "417280",
    "end": "423550"
  },
  {
    "text": "back demo it and advocate for its use we",
    "start": "423550",
    "end": "428969"
  },
  {
    "text": "all of these are great products there were things that in and of itself they were great but we kept a finding that as",
    "start": "428969",
    "end": "436060"
  },
  {
    "text": "we got a little bit down in the development cycle with one we would hit a deal breaker or just something we",
    "start": "436060",
    "end": "441699"
  },
  {
    "text": "couldn't engineer around in the in a time that we had but this is a cube con",
    "start": "441699",
    "end": "447909"
  },
  {
    "text": "so it's no surprise that the winner was kubernetes that would be really awkward ECS so a good part of the reason why we",
    "start": "447909",
    "end": "457599"
  },
  {
    "start": "454000",
    "end": "492000"
  },
  {
    "text": "chose we chose this is something I like to call the Kelsie Hightower effect we",
    "start": "457599",
    "end": "464379"
  },
  {
    "text": "all know who he is if you don't know who he is I highly recommend checking out his Twitter and github accounts our",
    "start": "464379",
    "end": "472180"
  },
  {
    "text": "Google team-up put us in front of Kelsey after we just kept bombarding them with questions that they were like someone",
    "start": "472180",
    "end": "478300"
  },
  {
    "text": "else needs to answer these and after that meeting we came out like really",
    "start": "478300",
    "end": "483400"
  },
  {
    "text": "confident that cube was was was the choice that that was going to work the",
    "start": "483400",
    "end": "489430"
  },
  {
    "text": "best for us so just to talk about some",
    "start": "489430",
    "end": "495159"
  },
  {
    "start": "492000",
    "end": "610000"
  },
  {
    "text": "of other requirements I was talking about and how they informed our choice of kubernetes we probably spent a lot of",
    "start": "495159",
    "end": "503740"
  },
  {
    "text": "time probably the most time on on networking and I want to give a shout out to my teammate Dane who's doing this",
    "start": "503740",
    "end": "511150"
  },
  {
    "text": "now for our team he was a network engineer in a former life so his",
    "start": "511150",
    "end": "517209"
  },
  {
    "text": "expertise here was super super invaluable and Q has some pretty some",
    "start": "517209",
    "end": "524649"
  },
  {
    "text": "pretty like gnarly a network it's where everything has to be able to see everything else without you know",
    "start": "524649",
    "end": "530950"
  },
  {
    "text": "what the magic of nat and since we're running these in a public cloud we really weren't comfortable with",
    "start": "530950",
    "end": "537209"
  },
  {
    "text": "delegating a cluster the ability to edit",
    "start": "537209",
    "end": "542290"
  },
  {
    "text": "or network routes in like AWS a PCP V PC or in GCE plus there's inherent limits",
    "start": "542290",
    "end": "550630"
  },
  {
    "text": "you know you can only about 50 routes so we went with an overlay network which is pretty common we looked at weave we",
    "start": "550630",
    "end": "558370"
  },
  {
    "text": "looked at flannel we looked at calico weave we eliminated early flannel to be honest we just didn't have time we never",
    "start": "558370",
    "end": "564940"
  },
  {
    "text": "like it was planned that we were gonna look at it but we just ran out of time and that was okay because it turned out",
    "start": "564940",
    "end": "572709"
  },
  {
    "text": "that calico was really good fit for us it's policy management was was a super",
    "start": "572709",
    "end": "579579"
  },
  {
    "text": "helpful with our app deployment strategy which I'll get to in a slide pretty soon",
    "start": "579579",
    "end": "586350"
  },
  {
    "text": "IP type II encapsulation meant we could just drop it into an already existing V",
    "start": "586350",
    "end": "591850"
  },
  {
    "text": "PC we didn't have to stand up anything extra or reconfigure everything and a SS",
    "start": "591850",
    "end": "597730"
  },
  {
    "text": "was probably happy since we're not trying to set up a BGP routes between our clusters and their routers and yeah",
    "start": "597730",
    "end": "604990"
  },
  {
    "text": "so we've we've been doing all really well with picking a calico",
    "start": "604990",
    "end": "611589"
  },
  {
    "start": "610000",
    "end": "677000"
  },
  {
    "text": "so we you chef it doesn't matter if you use puppet ansible a bunch of gross",
    "start": "611589",
    "end": "616870"
  },
  {
    "text": "shell scripts this is what scaler looks like to an end-user on the platform it",
    "start": "616870",
    "end": "624670"
  },
  {
    "text": "has a really great UI it also has a really extensive API and it handles all",
    "start": "624670",
    "end": "630730"
  },
  {
    "text": "of the creating and managing of our cloud user our cloud resources and supports really extensive governance of",
    "start": "630730",
    "end": "638440"
  },
  {
    "text": "policy and roll base access so we can empower our users but we can also fine-tune what they have access to and",
    "start": "638440",
    "end": "645180"
  },
  {
    "text": "scalar runs on a farm paradigm so like one cube cluster would be a farm and",
    "start": "645180",
    "end": "651670"
  },
  {
    "text": "then within that farm on the left you have your farm roles and they map to",
    "start": "651670",
    "end": "657230"
  },
  {
    "text": "cube master the workers at C D your API a load balancer and each of those are",
    "start": "657230",
    "end": "662510"
  },
  {
    "text": "formals has its own separate settings for instance size auto scaling of",
    "start": "662510",
    "end": "668300"
  },
  {
    "text": "settings orchestration scripts and this really worked well with for how we were",
    "start": "668300",
    "end": "674120"
  },
  {
    "text": "envisioning our or clusters and then kind of more on the side of non",
    "start": "674120",
    "end": "681290"
  },
  {
    "start": "677000",
    "end": "779000"
  },
  {
    "text": "technical challenges we really have to architect our Q clusters knowing that",
    "start": "681290",
    "end": "688580"
  },
  {
    "text": "they be owned and a provision by just a single team so that means we kind of have to automate and abstract all of the",
    "start": "688580",
    "end": "695810"
  },
  {
    "text": "little things that an app team just probably does want to deal with like",
    "start": "695810",
    "end": "700880"
  },
  {
    "text": "making sure SCD is is backed up setting",
    "start": "700880",
    "end": "706220"
  },
  {
    "text": "our name spacing of standards upfront because we were going to have a lot of teams with a lot of clusters and that was going to get really annoying soon",
    "start": "706220",
    "end": "712490"
  },
  {
    "text": "and and automating all of the creation",
    "start": "712490",
    "end": "718220"
  },
  {
    "text": "of the secrets needed to stand up a queue cluster but at the same time we",
    "start": "718220",
    "end": "724730"
  },
  {
    "text": "needed to keep in mind that even though these teams have been on platform for a while this is probably the most complex",
    "start": "724730",
    "end": "731090"
  },
  {
    "text": "thing they're gonna be op Singh and at canet we promote like a culture of",
    "start": "731090",
    "end": "738650"
  },
  {
    "text": "shared ownership shared shared responsibility so app teams are getting alerted on their infrastructure like",
    "start": "738650",
    "end": "745880"
  },
  {
    "text": "they're like if their worker nodes are getting crushed on CPU then that alert",
    "start": "745880",
    "end": "751190"
  },
  {
    "text": "goes to them first so we really have to make sure that our documentation was",
    "start": "751190",
    "end": "756650"
  },
  {
    "text": "good we really wanted to incentivize them to care and and part of that is",
    "start": "756650",
    "end": "764180"
  },
  {
    "text": "also eating or eating our own dog food so to speak so that's we were using cube",
    "start": "764180",
    "end": "770690"
  },
  {
    "text": "on our team and running our apps on it and kind of getting a perspective of",
    "start": "770690",
    "end": "776300"
  },
  {
    "text": "what their user experience looks like so that takes us to what our current cube",
    "start": "776300",
    "end": "783760"
  },
  {
    "start": "779000",
    "end": "894000"
  },
  {
    "text": "architecture is like and this is pretty much what happens",
    "start": "783760",
    "end": "791320"
  },
  {
    "text": "when any Atene when when anytime a team comes to us and says hey Paz I want to do some containers and we say hold up",
    "start": "791320",
    "end": "798510"
  },
  {
    "text": "start here and everyone has to a complete our Paz labs as we call them",
    "start": "798510",
    "end": "805510"
  },
  {
    "text": "they basically run through all the features of the platform basically what",
    "start": "805510",
    "end": "812890"
  },
  {
    "text": "it takes to start using them how they can make your team as successful the cube lab is our longest one",
    "start": "812890",
    "end": "819580"
  },
  {
    "text": "it's lab 27 and it walks through the provisioning of an entire entire cube",
    "start": "819580",
    "end": "826870"
  },
  {
    "text": "cluster in scaler from the creation of the farm the farm rolls launching the",
    "start": "826870",
    "end": "833560"
  },
  {
    "text": "farm and then deploying your first app once they do that the second step is a",
    "start": "833560",
    "end": "839410"
  },
  {
    "text": "requirement for every cue clustering that gets stood up after that they submit a JIRA ticket to us that kicks",
    "start": "839410",
    "end": "846400"
  },
  {
    "text": "off a bill job that builds all of the required all of the required",
    "start": "846400",
    "end": "853830"
  },
  {
    "text": "secrets and tokens like cube CTL token the cubelet a bootstrap a token their",
    "start": "853830",
    "end": "859750"
  },
  {
    "text": "teams a token for their team's console ACL policy API a server certs and stuff",
    "start": "859750",
    "end": "866560"
  },
  {
    "text": "like that and then it puts it in our sequence back-end and then the team gets supplied with a namespace of where to",
    "start": "866560",
    "end": "873520"
  },
  {
    "text": "find all those things that then they take that they go they build their farm",
    "start": "873520",
    "end": "879300"
  },
  {
    "text": "if they're a new user or a new team they'll probably be doing that in in the UI which is lots of pointing and",
    "start": "879300",
    "end": "886150"
  },
  {
    "text": "clicking if it's a more advanced team it's definitely being done automated in",
    "start": "886150",
    "end": "891790"
  },
  {
    "text": "there CI pipeline this is a really bad drawing of what our cube kind of looks",
    "start": "891790",
    "end": "898150"
  },
  {
    "start": "894000",
    "end": "1090000"
  },
  {
    "text": "like right now some things to note here are we are masochists and we basically",
    "start": "898150",
    "end": "907930"
  },
  {
    "text": "roll our own cube RPMs using the Google binaries we're not running them as",
    "start": "907930",
    "end": "914490"
  },
  {
    "text": "containers themselves and that's for everything in the Q control plane and",
    "start": "914490",
    "end": "920140"
  },
  {
    "text": "then queue Thank You proxy on the workers we run sed and its own cluster there it's",
    "start": "920140",
    "end": "925960"
  },
  {
    "text": "usually always a 3-node a quorum but for some of our bigger clusters we go up to",
    "start": "925960",
    "end": "931270"
  },
  {
    "text": "five and then we put AJ proxy which is heavily orchestrated in front of all the",
    "start": "931270",
    "end": "939730"
  },
  {
    "text": "worker nodes and then that gets updated with where to find the API server and",
    "start": "939730",
    "end": "945339"
  },
  {
    "text": "anything that needs to talk to the cube API it goes through that HR proxy so we",
    "start": "945339",
    "end": "954070"
  },
  {
    "text": "also have this really awesome API team on on paths they build all these great",
    "start": "954070",
    "end": "960220"
  },
  {
    "text": "tools and go and they had built a deployment API that was initially used",
    "start": "960220",
    "end": "965560"
  },
  {
    "text": "for deploying apps on cloud instances and managing that entire the entire",
    "start": "965560",
    "end": "971740"
  },
  {
    "text": "lifecycle so they built some a new some new functionality into it to handle a",
    "start": "971740",
    "end": "976750"
  },
  {
    "text": "container deployments on a cube cluster basically it's managing like three three",
    "start": "976750",
    "end": "982270"
  },
  {
    "text": "key things it's taking a cube a deployment object so anything you can do",
    "start": "982270",
    "end": "988330"
  },
  {
    "text": "in in a cube deployment you can do in our API it just takes that DC realises",
    "start": "988330",
    "end": "994180"
  },
  {
    "text": "it and sends it to the cube API it creates a service to go with that deployment and it picks a node port from",
    "start": "994180",
    "end": "1001020"
  },
  {
    "text": "this list of node ports assigned to that cluster then it goes and updates the HF",
    "start": "1001020",
    "end": "1009450"
  },
  {
    "text": "proxy config with the IP addresses of all of the worker nodes in the farm and",
    "start": "1009450",
    "end": "1015990"
  },
  {
    "text": "the node ports where you can find the currently a deployed version of your app and in the new version then the user can",
    "start": "1015990",
    "end": "1023580"
  },
  {
    "text": "use the API and start shifting a traffic however they want and however they are comfortable with they could also do a",
    "start": "1023580",
    "end": "1031319"
  },
  {
    "text": "canary deployment they can just shift a little bit of traffic and then do some automated eye testing with a pass/fail",
    "start": "1031319",
    "end": "1038730"
  },
  {
    "text": "mechanism in it once they're okay with the deployment and they say it's it's good to go",
    "start": "1038730",
    "end": "1044370"
  },
  {
    "text": "they send a complete a complete message to the complete endpoint and",
    "start": "1044370",
    "end": "1052370"
  },
  {
    "text": "the API will go in and delete the old appointment and services and update HR",
    "start": "1052740",
    "end": "1058920"
  },
  {
    "text": "proxy that config again or if it didn't go well they could ask for a rollback and it'll just put everything back to",
    "start": "1058920",
    "end": "1064890"
  },
  {
    "text": "the way it was before you started a deployment and this is really where Calico shines for us",
    "start": "1064890",
    "end": "1071190"
  },
  {
    "text": "because the H I proxy has no idea what node is running a pod for the app you",
    "start": "1071190",
    "end": "1077850"
  },
  {
    "text": "are looking for so when it sends traffic to a node that doesn't have that port on",
    "start": "1077850",
    "end": "1083970"
  },
  {
    "text": "it calcio will come in take it and send it to a node that does so a Cubism is is",
    "start": "1083970",
    "end": "1094559"
  },
  {
    "text": "really good at like abstracting all these things from people who just want to run container workloads but I'd say",
    "start": "1094559",
    "end": "1105690"
  },
  {
    "text": "that a benefit comes with a lot of added complexity from operations a standpoint",
    "start": "1105690",
    "end": "1112700"
  },
  {
    "text": "there's a valid argument also that like whether or not taking on and that added",
    "start": "1112730",
    "end": "1118590"
  },
  {
    "text": "a complexity is worth it at this point we're where we're at to run some containers but it's the only way right",
    "start": "1118590",
    "end": "1125220"
  },
  {
    "text": "now on the pass a platform that you can run containers so we kind of have done",
    "start": "1125220",
    "end": "1132360"
  },
  {
    "text": "the best we can to build up a toolset to handle those issues when they come up",
    "start": "1132360",
    "end": "1138120"
  },
  {
    "text": "and dealing with that complexity one of those things is we run a server spec in",
    "start": "1138120",
    "end": "1144030"
  },
  {
    "text": "our CI CI a pipeline against a fully running cube stack we're checking for",
    "start": "1144030",
    "end": "1153710"
  },
  {
    "text": "making sure that the system is configured right and we're also checking actual functionality we're running a",
    "start": "1153710",
    "end": "1161070"
  },
  {
    "text": "commands we're checking in the output of it we're making sure everything in the cube system namespace isn't just saying",
    "start": "1161070",
    "end": "1167490"
  },
  {
    "text": "it's running but it's actually also working then we run some canary clusters",
    "start": "1167490",
    "end": "1174420"
  },
  {
    "start": "1171000",
    "end": "1203000"
  },
  {
    "text": "so any changes that we make or new releases or if we update things in our",
    "start": "1174420",
    "end": "1180000"
  },
  {
    "text": "cookbooks we run them against a cluster that actually has apps running in it to",
    "start": "1180000",
    "end": "1185309"
  },
  {
    "text": "validate those changes check for any regressions it's not like perfect eye catching everything because",
    "start": "1185309",
    "end": "1192690"
  },
  {
    "text": "we can't account for all of the use cases that are our users are inventing",
    "start": "1192690",
    "end": "1199740"
  },
  {
    "text": "in these clusters but it does a pretty good job we maintain a set of run books",
    "start": "1199740",
    "end": "1206940"
  },
  {
    "start": "1203000",
    "end": "1242000"
  },
  {
    "text": "if you're not doing this I highly suggest it and not just for like the you",
    "start": "1206940",
    "end": "1213419"
  },
  {
    "text": "know the new person's first night on call it's it's really good just as a reference for the team we have",
    "start": "1213419",
    "end": "1220380"
  },
  {
    "text": "everything in there from common troubleshooting techniques also links to",
    "start": "1220380",
    "end": "1227190"
  },
  {
    "text": "documentation they are continuously updated any time we encounter something that's just weird",
    "start": "1227190",
    "end": "1232380"
  },
  {
    "text": "it goes in there and they're all in a github repo and then we link those to all of our our New Relic alerts and our",
    "start": "1232380",
    "end": "1239100"
  },
  {
    "text": "Victor ops like our paging services alerts and then these last two things",
    "start": "1239100",
    "end": "1244110"
  },
  {
    "start": "1242000",
    "end": "1334000"
  },
  {
    "text": "are kind of just like standard ops things I think in the keynote yesterday morning we heard observability",
    "start": "1244110",
    "end": "1250380"
  },
  {
    "text": "engineering I kind of like that term and this is kind of like in a cute cluster",
    "start": "1250380",
    "end": "1257880"
  },
  {
    "text": "which is different from our normal stuff it's very dynamic and it's volatile",
    "start": "1257880",
    "end": "1263549"
  },
  {
    "text": "things are being spun up and torn down all the time so we shove all of our cue",
    "start": "1263549",
    "end": "1270870"
  },
  {
    "text": "blogs and our container logs off to our logging vendor this is also where it",
    "start": "1270870",
    "end": "1276000"
  },
  {
    "text": "helps to have some good name spacing up patterns up front and then we've designed and created some pretty nifty",
    "start": "1276000",
    "end": "1283940"
  },
  {
    "text": "dashboards that our success team has even automated as part of the",
    "start": "1283940",
    "end": "1289100"
  },
  {
    "text": "provisioning step when you're when you're building a new acute cluster this",
    "start": "1289100",
    "end": "1295530"
  },
  {
    "text": "is just an example of what one looks like just standard worker stuff you know",
    "start": "1295530",
    "end": "1302120"
  },
  {
    "text": "it's CPU disk memory and then just checking what like all the important",
    "start": "1302120",
    "end": "1309660"
  },
  {
    "text": "services on a worker are still there like docker and queue proxy and cube lid stuff and then we look at things like",
    "start": "1309660",
    "end": "1316710"
  },
  {
    "text": "how many of a container and a version that container are on all the workers we",
    "start": "1316710",
    "end": "1324240"
  },
  {
    "text": "look at we look at the container at distribution that gives us a little bit of insight into how the bin packing is",
    "start": "1324240",
    "end": "1329730"
  },
  {
    "text": "working and then just some random bio stuff over there so we did run into",
    "start": "1329730",
    "end": "1337710"
  },
  {
    "start": "1334000",
    "end": "1484000"
  },
  {
    "text": "somewhat challenges one of them was we",
    "start": "1337710",
    "end": "1342840"
  },
  {
    "text": "had problems with nodes are terminating and terminating cleanly not only",
    "start": "1342840",
    "end": "1348049"
  },
  {
    "text": "draining pods but removing themselves out of a calico so we didn't have routes",
    "start": "1348049",
    "end": "1355200"
  },
  {
    "text": "to knows that it weren't weren't there anymore we handle this by creating an",
    "start": "1355200",
    "end": "1360750"
  },
  {
    "text": "orchestration scripts that scaler will trigger before a worker node is removed",
    "start": "1360750",
    "end": "1367759"
  },
  {
    "text": "and terminated in it's a cloud provider and all that's really doing is it's",
    "start": "1367759",
    "end": "1373740"
  },
  {
    "text": "hitting it it's hitting at the cube API for its own a node name then it drains itself I think we have 120 seconds there",
    "start": "1373740",
    "end": "1381419"
  },
  {
    "text": "and then it will delete itself and then it will stop a calico note on that worker and then delete itself from from",
    "start": "1381419",
    "end": "1390960"
  },
  {
    "text": "calico and that runs every time unless the thing just dies and then you have to",
    "start": "1390960",
    "end": "1397049"
  },
  {
    "text": "go in and manually do that we had some trouble with auto scaling there's like",
    "start": "1397049",
    "end": "1405690"
  },
  {
    "text": "inherent ways I to do this in kubernetes or you can just use your cloud provider",
    "start": "1405690",
    "end": "1410940"
  },
  {
    "text": "you know if you're in AWS you're using auto scaling groups in GCE or using managed instance groups but there are",
    "start": "1410940",
    "end": "1420299"
  },
  {
    "text": "our chances are that you probably want to do things after the node is there and it like post config management post boot",
    "start": "1420299",
    "end": "1427049"
  },
  {
    "text": "steps so for us we didn't want to use",
    "start": "1427049",
    "end": "1432320"
  },
  {
    "text": "user data or user scripts through metadata and GCE so we created our",
    "start": "1432320",
    "end": "1440009"
  },
  {
    "text": "custom scaling scripts in scalar that scalar will run on a worker node at a",
    "start": "1440009",
    "end": "1445620"
  },
  {
    "text": "set interval and then based on the output on that script it'll make a scaling decision initially on our or",
    "start": "1445620",
    "end": "1453850"
  },
  {
    "text": "smaller initial clusters we were taking the average of those metrics so when we",
    "start": "1453850",
    "end": "1461289"
  },
  {
    "text": "got to the larger clusters we started at seeing that there were a few at worker nodes that were just getting crushed",
    "start": "1461289",
    "end": "1466809"
  },
  {
    "text": "even though everything else was fine so we rewrote those scripts to scale off",
    "start": "1466809",
    "end": "1471970"
  },
  {
    "text": "of an individual workers resource at consumption and we made those both",
    "start": "1471970",
    "end": "1478059"
  },
  {
    "text": "available to our users so whichever works better in in their cluster they could think it's use we had some issues",
    "start": "1478059",
    "end": "1486400"
  },
  {
    "start": "1484000",
    "end": "1531000"
  },
  {
    "text": "with contract limits this was kind of a race condition and our testing because",
    "start": "1486400",
    "end": "1492850"
  },
  {
    "text": "we were disabling a firewall D in chef and also in our pre-baked images but we",
    "start": "1492850",
    "end": "1502210"
  },
  {
    "text": "knew that a calico was running its own firewall dia pretty much but it was also",
    "start": "1502210",
    "end": "1507669"
  },
  {
    "text": "reloading the net filter contract kernel module so it was putting back into play",
    "start": "1507669",
    "end": "1513880"
  },
  {
    "text": "those sis ETL contract limits and we started seeing just random nodes in our",
    "start": "1513880",
    "end": "1520000"
  },
  {
    "text": "larger clusters just dropping out TCP connections so we just bumped all those",
    "start": "1520000",
    "end": "1525280"
  },
  {
    "text": "limits and added some pretty detailed alerting and monitoring to our dashboards cloud parity as much as Cube",
    "start": "1525280",
    "end": "1535120"
  },
  {
    "start": "1531000",
    "end": "1664000"
  },
  {
    "text": "wants to be like wants to abstract how resources are provisioned versus how",
    "start": "1535120",
    "end": "1540190"
  },
  {
    "text": "they're consumed by a user like persistent volumes the user should just",
    "start": "1540190",
    "end": "1547630"
  },
  {
    "text": "be able to say I need a 25 gig volume here it shouldn't care if that's an EBS",
    "start": "1547630",
    "end": "1552970"
  },
  {
    "text": "volume or if that's GCE a persistent disk or its manifest but there's",
    "start": "1552970",
    "end": "1558730"
  },
  {
    "text": "currently some issues here or we're getting bit by an open issue where if",
    "start": "1558730",
    "end": "1565299"
  },
  {
    "text": "you set the cloud provider equals GCE cubelet flag which is required to get",
    "start": "1565299",
    "end": "1571960"
  },
  {
    "text": "any of the GC specific functionality like persistent volumes and you're using",
    "start": "1571960",
    "end": "1579159"
  },
  {
    "text": "a CNI plugin it will still try and create routes in GCE and then you'll",
    "start": "1579159",
    "end": "1584679"
  },
  {
    "text": "have nodes come up that will say that ready but there's no routes to them the",
    "start": "1584679",
    "end": "1590389"
  },
  {
    "text": "workaround right now is the not set a cloud provider if you're in GCE which",
    "start": "1590389",
    "end": "1595690"
  },
  {
    "text": "kind of doesn't work because then we can't use precious volumes that we can't use like staple sets so having to tell",
    "start": "1595690",
    "end": "1602210"
  },
  {
    "text": "our customers at this point that like you can use these over here but you can't over here is not good for now but",
    "start": "1602210",
    "end": "1609769"
  },
  {
    "text": "it's supposedly he's supposed to be fixed soon and that would be awesome and",
    "start": "1609769",
    "end": "1616090"
  },
  {
    "text": "this is probably what's been the most challenging up for us our initial cube",
    "start": "1616090",
    "end": "1621649"
  },
  {
    "text": "clusters were one for we released one five skipped one six released one seven",
    "start": "1621649",
    "end": "1628490"
  },
  {
    "text": "planned on doing one eight and now we're looking at one nine probably in q1 for",
    "start": "1628490",
    "end": "1633499"
  },
  {
    "text": "us if we were just like a team working on just this we probably would be able",
    "start": "1633499",
    "end": "1639499"
  },
  {
    "text": "to do it but you know we have a whole other platform to manage and we're also",
    "start": "1639499",
    "end": "1644830"
  },
  {
    "text": "we're also integrating a features into our cube clusters like our our vault",
    "start": "1644830",
    "end": "1651919"
  },
  {
    "text": "integration or our console integration so we're trying to get better at this",
    "start": "1651919",
    "end": "1658039"
  },
  {
    "text": "and we're looking at some alternatives that can help us out with the pace of development here so how do we do well",
    "start": "1658039",
    "end": "1667340"
  },
  {
    "start": "1664000",
    "end": "1740000"
  },
  {
    "text": "I'm still getting paid and they let me come and talked about this so I imagine we did pretty good",
    "start": "1667340",
    "end": "1674768"
  },
  {
    "text": "elections are 2016 was a complete success for us broke all our traffic",
    "start": "1674980",
    "end": "1680059"
  },
  {
    "text": "records about a week before the election we went to our CTO and we said we were",
    "start": "1680059",
    "end": "1686840"
  },
  {
    "text": "we want to run hundred percent on cube and he had the faith and in the",
    "start": "1686840",
    "end": "1693769"
  },
  {
    "text": "confidence in us to do that and I think at one point around midnight one of the",
    "start": "1693769",
    "end": "1702889"
  },
  {
    "text": "API layers behind the application was having some some issues so they just",
    "start": "1702889",
    "end": "1709399"
  },
  {
    "text": "threw it in a container and we deployed it on the cube cluster and had no no",
    "start": "1709399",
    "end": "1714889"
  },
  {
    "text": "problems we managed over a hundred and seventy at deployments that night all success",
    "start": "1714889",
    "end": "1721559"
  },
  {
    "text": "which we couldn't have done if we were running this on the old old vm way so it",
    "start": "1721559",
    "end": "1728159"
  },
  {
    "text": "was a long night for a lot of reasons but it was it was a rewarding a finish",
    "start": "1728159",
    "end": "1733980"
  },
  {
    "text": "to a very challenging project and a very transformative a project for us and I",
    "start": "1733980",
    "end": "1742289"
  },
  {
    "start": "1740000",
    "end": "1815000"
  },
  {
    "text": "just won't talk about this up real quick because this is a a little bit more recent so our success team recently took",
    "start": "1742289",
    "end": "1748529"
  },
  {
    "text": "all of the desktop sites for all of our properties and move them into the",
    "start": "1748529",
    "end": "1754010"
  },
  {
    "text": "current kubernetes cluster and that resulted in reducing our daily operating",
    "start": "1754010",
    "end": "1761100"
  },
  {
    "text": "costs by a several hundred bucks and they're looking at further optimizations",
    "start": "1761100",
    "end": "1766139"
  },
  {
    "text": "there our deployment times went from fully optimized getting all 140",
    "start": "1766139",
    "end": "1773159"
  },
  {
    "text": "sites out took over two hours and now we're down to 25 minutes and we could",
    "start": "1773159",
    "end": "1779130"
  },
  {
    "text": "probably get even faster there and during the recent tragedies of Hurricane",
    "start": "1779130",
    "end": "1786570"
  },
  {
    "text": "Harvey in Houston area and Hurricane Irma in Florida we dropped our pay wall for all of the local sites in the in",
    "start": "1786570",
    "end": "1794669"
  },
  {
    "text": "those areas so all the residents could get you know the most up-to-date and accurate information that they needed to",
    "start": "1794669",
    "end": "1801690"
  },
  {
    "text": "stay safe and informed we ended up making 1500 deployments during that a time period which again we could not",
    "start": "1801690",
    "end": "1808649"
  },
  {
    "text": "have done the the old way so that was a pretty big win for us and real quick",
    "start": "1808649",
    "end": "1817830"
  },
  {
    "start": "1815000",
    "end": "1947000"
  },
  {
    "text": "open what's next for us we're eventually going to finish our move into Etsy d3",
    "start": "1817830",
    "end": "1825320"
  },
  {
    "text": "cube 9 like I talked about we're relooking looking we're we're",
    "start": "1825320",
    "end": "1832889"
  },
  {
    "text": "taking another look at cube ATM when we first started this it wasn't really",
    "start": "1832889",
    "end": "1837899"
  },
  {
    "text": "ready and now it's it looks like it will help streamline our how we handle our a",
    "start": "1837899",
    "end": "1845519"
  },
  {
    "text": "control plane with those growths rpms vault integration is actually live now",
    "start": "1845519",
    "end": "1851220"
  },
  {
    "text": "we're using the the kubernetes vault off back ends in",
    "start": "1851220",
    "end": "1856850"
  },
  {
    "text": "conjunction with a pre-configured role for that team a policy for that team and",
    "start": "1856850",
    "end": "1862830"
  },
  {
    "text": "a role for that cluster if if any deployment needs to access a secrets in",
    "start": "1862830",
    "end": "1869130"
  },
  {
    "text": "vault you just add a vault in a container to that a deployment and then it'll mount a token with a very short a",
    "start": "1869130",
    "end": "1876060"
  },
  {
    "text": "TTL in a shared a secrets volume for any of the containers in that pod to access",
    "start": "1876060",
    "end": "1882920"
  },
  {
    "text": "we started looking at service mesh we think it's really cool we really want to",
    "start": "1882920",
    "end": "1888180"
  },
  {
    "text": "do awesome things with it but no one's really asking for it so it's it's taking",
    "start": "1888180",
    "end": "1894060"
  },
  {
    "text": "a backseat to some higher priority stuff and last but not least the elephant in the room are terrible not terrible but",
    "start": "1894060",
    "end": "1901620"
  },
  {
    "text": "are terribly over orchestrated H a proxy per application ingress load balancer",
    "start": "1901620",
    "end": "1907790"
  },
  {
    "text": "solution that we're looking at some cloud native stuff like using al bees in",
    "start": "1907790",
    "end": "1915690"
  },
  {
    "text": "Amazon or GCE a load balancers or maybe something like envoy but that's where",
    "start": "1915690",
    "end": "1924330"
  },
  {
    "text": "we're at so thank you for listening to me if you have any questions feel free",
    "start": "1924330",
    "end": "1931170"
  },
  {
    "text": "to ask or find me ordained or we're also hiring so if you want to do cool interesting things in a in the news",
    "start": "1931170",
    "end": "1939180"
  },
  {
    "text": "industry come talk to us and that's it thanks",
    "start": "1939180",
    "end": "1944929"
  }
]