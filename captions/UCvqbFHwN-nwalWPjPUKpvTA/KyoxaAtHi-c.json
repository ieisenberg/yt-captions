[
  {
    "text": "welcome and we're very happy and excited to be here today and to present to you",
    "start": "480",
    "end": "5920"
  },
  {
    "text": "our talk dashboards and dragons crafting SLOs's to tame the AI platform chaos at",
    "start": "5920",
    "end": "12679"
  },
  {
    "text": "scale so welcome brave adventures to our quest where we'll craft the SLOs's to",
    "start": "12679",
    "end": "18480"
  },
  {
    "text": "vanquish the chaos haunting our AI realms my name is Alexa Griffith and I am a senior software engineer at",
    "start": "18480",
    "end": "24960"
  },
  {
    "text": "Bloomberg i work on uh building our AI inference platform my name is Ankita and I'm a senior",
    "start": "24960",
    "end": "31519"
  },
  {
    "text": "product manager at Bloomberg um I focus mostly on our computer infrastructure",
    "start": "31519",
    "end": "37079"
  },
  {
    "text": "nice so our quest begins um as you can tell I had a little bit of fun making",
    "start": "37079",
    "end": "42480"
  },
  {
    "text": "these doodles for this presentation so let's check out our treasure map for today's journey we're going to tackle",
    "start": "42480",
    "end": "49280"
  },
  {
    "text": "the Genai Hydra peer into the crystal ball of observability and then venture",
    "start": "49280",
    "end": "54480"
  },
  {
    "text": "deep into the dashboard uh dungeon and defend against the fortress of platform",
    "start": "54480",
    "end": "61079"
  },
  {
    "text": "challenges first a little bit about AI platforms at Bloomberg we service the",
    "start": "61079",
    "end": "66240"
  },
  {
    "text": "whole model development life cycle so from exploration with the data to building your model and experimentation",
    "start": "66240",
    "end": "73680"
  },
  {
    "text": "to deployment serving and then um model maintenance and production for",
    "start": "73680",
    "end": "78880"
  },
  {
    "text": "monitoring updating so what this looks like is various platform teams and services um within our AI within AI",
    "start": "78880",
    "end": "85360"
  },
  {
    "text": "platforms so we offer services like Jupyter notebooks um different ways to train and use HPC manage serving managed",
    "start": "85360",
    "end": "93360"
  },
  {
    "text": "inference and different AI pipelines and tools to our users",
    "start": "93360",
    "end": "98960"
  },
  {
    "text": "so let's get into SLOs's think of SLOs's as ancient runes each holding the magic",
    "start": "98960",
    "end": "104400"
  },
  {
    "text": "to illuminate the health of your platform so decoding these runes can give us clarity in uncertain times so",
    "start": "104400",
    "end": "110159"
  },
  {
    "text": "what makes up an SLO first a service level indicator SLI this is a metric",
    "start": "110159",
    "end": "115600"
  },
  {
    "text": "generated by query so an example is latency throughput or error rates what you want to measure then you have an",
    "start": "115600",
    "end": "121680"
  },
  {
    "text": "objective so that's the desired performance for that SLI so if it's",
    "start": "121680",
    "end": "126799"
  },
  {
    "text": "latency we want our latency to be under 50 milliseconds for example next we'll have a target value so how often must",
    "start": "126799",
    "end": "134160"
  },
  {
    "text": "this objective be met so 99.99 999% of the time for example next how do we",
    "start": "134160",
    "end": "140319"
  },
  {
    "text": "measure it with a time window that's so that's the duration over which the target value is measured for example",
    "start": "140319",
    "end": "145680"
  },
  {
    "text": "over a two-hour window over the last 24 hours so just like any magic potion um SLOs's",
    "start": "145680",
    "end": "154239"
  },
  {
    "text": "combine carefully chosen ingredients so what we need is a common language between our engineers uh operations and",
    "start": "154239",
    "end": "160840"
  },
  {
    "text": "stakeholders we want it to be user focused so we want to highlight real impact and allow it to let it allow us",
    "start": "160840",
    "end": "168000"
  },
  {
    "text": "to prioritize improvements next we need we use these for guidance so this helps",
    "start": "168000",
    "end": "173440"
  },
  {
    "text": "us to clearly navigate trade-offs uh and reliability innovation and velocity",
    "start": "173440",
    "end": "181000"
  },
  {
    "text": "so there's different quests different or different platforms different quests right not every quest has the same",
    "start": "181599",
    "end": "187040"
  },
  {
    "text": "danger similarly not every platform has the same needs they all face different",
    "start": "187040",
    "end": "192480"
  },
  {
    "text": "challenges and different things they need some of them some things can be similar right there are a lot of",
    "start": "192480",
    "end": "197680"
  },
  {
    "text": "similarities we want all of our quests going through but there might be specific needs for specific platforms",
    "start": "197680",
    "end": "203519"
  },
  {
    "text": "for example training and predictive inference and Gen AI today we're going to focus on Gen AI um because that's",
    "start": "203519",
    "end": "209760"
  },
  {
    "text": "what we've been working the most on so for Gen AI our usage patterns could be you know real-time inference we might",
    "start": "209760",
    "end": "216159"
  },
  {
    "text": "have interactive workloads we might have some batch things going on um also what",
    "start": "216159",
    "end": "221200"
  },
  {
    "text": "we care about is you know managing how many tokens we're getting in the concurrency the streaming latency and",
    "start": "221200",
    "end": "226799"
  },
  {
    "text": "the availability so the GI hydra is tricky",
    "start": "226799",
    "end": "232080"
  },
  {
    "text": "you know you chop off one head like inconsistent deployments and two more heads pop up like security or observability issues so we should arm",
    "start": "232080",
    "end": "239280"
  },
  {
    "text": "ourselves wisely what makes these platforms complex where there are multiple things right like fractured networking so we have latency service",
    "start": "239280",
    "end": "246159"
  },
  {
    "text": "discovery crosscluster traffic a lot of times now we're running on a hybrid environment so um observability between",
    "start": "246159",
    "end": "252640"
  },
  {
    "text": "you know something like Amazon AWS uh bedrock and running things on prim and",
    "start": "252640",
    "end": "258079"
  },
  {
    "text": "we have different uh deployments like different infrastructure versions and scaling so all of these things make it a",
    "start": "258079",
    "end": "263919"
  },
  {
    "text": "little bit difficult to uh to manage so for an overview let's look at",
    "start": "263919",
    "end": "269360"
  },
  {
    "text": "what a typical you know gen AI platform will look like from a high level so we might have a load balancer we might you",
    "start": "269360",
    "end": "275840"
  },
  {
    "text": "know hit a gateway for us we use the envoy gateway and we use that to be able to serve you know onrem a manage manage",
    "start": "275840",
    "end": "283280"
  },
  {
    "text": "inference uh kubernetes cluster and we use kserve with that but also we use it",
    "start": "283280",
    "end": "288560"
  },
  {
    "text": "to easily be able to hit any other lm provider that you need so this is what uh a request going through our platform",
    "start": "288560",
    "end": "296400"
  },
  {
    "text": "would hit now let's follow the request quest this is just an example say we",
    "start": "296400",
    "end": "301919"
  },
  {
    "text": "have an a request it's going to hit the gateway it might need to go through some hops in the gateway as well maybe it has",
    "start": "301919",
    "end": "308320"
  },
  {
    "text": "some authorization things to do maybe it has the rate limiter set up and then once all that is uh configured it'll go",
    "start": "308320",
    "end": "314720"
  },
  {
    "text": "back to the gateway and then it'll go to the model provider and let's say that",
    "start": "314720",
    "end": "319840"
  },
  {
    "text": "we're running something self-hosted well then we can also manage uh doing",
    "start": "319840",
    "end": "324960"
  },
  {
    "text": "inference and it might have to do a prefill decode and then hit the all the",
    "start": "324960",
    "end": "330560"
  },
  {
    "text": "way back again the response goes to the client so at each step there can be latencies at each step there can be",
    "start": "330560",
    "end": "336320"
  },
  {
    "text": "failures and we need to as a platform team clearly understand and manage uh what is going on at every one of",
    "start": "336320",
    "end": "344120"
  },
  {
    "text": "these so measuring our DNA success uh we have client-f facing SLOs's that",
    "start": "344120",
    "end": "349759"
  },
  {
    "text": "determine our quality of service so uh these are crucial for our SLAs's so",
    "start": "349759",
    "end": "355600"
  },
  {
    "text": "these metrics define user experience and ensure that our platform delivers on uh promised reliability latency and",
    "start": "355600",
    "end": "362400"
  },
  {
    "text": "performance and we have different levels so you for a platform you might want to have a a high priority level so userf",
    "start": "362400",
    "end": "369120"
  },
  {
    "text": "facing real time medium maybe that's like internal real time and then low so our batch uh batch inference that",
    "start": "369120",
    "end": "375759"
  },
  {
    "text": "doesn't need to run on a specific time um then we have to help us with these we",
    "start": "375759",
    "end": "380800"
  },
  {
    "text": "have platform level metrics so we track our infrastructure health at um at each environment and then we also have uh use",
    "start": "380800",
    "end": "388080"
  },
  {
    "text": "these to guide our scaling and resource planning lastly we have model level metrics which help us so we want to know",
    "start": "388080",
    "end": "394720"
  },
  {
    "text": "the performance of the models um self-hosted or vendor and then we want to use certain metrics for determining",
    "start": "394720",
    "end": "401280"
  },
  {
    "text": "the reliability latency and cost efficiency of our platform let's talk about that our",
    "start": "401280",
    "end": "406720"
  },
  {
    "text": "spellbook of metrics so for example uh a genai platform might want some things",
    "start": "406720",
    "end": "412560"
  },
  {
    "text": "for reliability uptime including the routing layer uh API conversion errors cost effectiveness things like how well",
    "start": "412560",
    "end": "419919"
  },
  {
    "text": "are we doing for um optimizations like automatic routing uh model caching",
    "start": "419919",
    "end": "424960"
  },
  {
    "text": "prompt optimizations also we want to measure our scalability not only do we want to know the number of requests but",
    "start": "424960",
    "end": "430479"
  },
  {
    "text": "you know for JAI we want to know something like the number of tokens or time to first token um also we want to",
    "start": "430479",
    "end": "435520"
  },
  {
    "text": "clearly measure our latency so from every platform component and the request quest that I showed",
    "start": "435520",
    "end": "443039"
  },
  {
    "text": "earlier so um imagine that I mean as platform teams",
    "start": "443479",
    "end": "449440"
  },
  {
    "text": "so we're in a unique p position right we're not just responsible for moni monitoring our service or a few services",
    "start": "449440",
    "end": "455280"
  },
  {
    "text": "but the overall health of the platform itself so imagine a scenario as a platform team you have resources across",
    "start": "455280",
    "end": "462000"
  },
  {
    "text": "your platform you have users resources infrastructure resources and all of a sudden they're wiped out um was it a",
    "start": "462000",
    "end": "468800"
  },
  {
    "text": "scheduled cleanup job like how much was wiped out um was it a mis something that was misconfigured that ran wrong was it",
    "start": "468800",
    "end": "475280"
  },
  {
    "text": "a failure in the underlying infrastructure um or even let's take something maybe a little bit more subtle",
    "start": "475280",
    "end": "480879"
  },
  {
    "text": "so what if latency slowly creeps up across services what if traffic goes completely quiet then the question",
    "start": "480879",
    "end": "487120"
  },
  {
    "text": "becomes you know how long would it take us as a platform team to notice uh in too many cases platform teams can find",
    "start": "487120",
    "end": "494319"
  },
  {
    "text": "out when the users complain which is by that point often too late so the impacts",
    "start": "494319",
    "end": "499840"
  },
  {
    "text": "happen and the frustration from the users already set in you know you're losing trust from your users as well so",
    "start": "499840",
    "end": "506319"
  },
  {
    "text": "without this like clearly defined real time visibility platform teams are kind of stuck in this reactive mode uh but",
    "start": "506319",
    "end": "513200"
  },
  {
    "text": "with the right observability tools in place we can you know detect anonymies earlier respond effectively and most",
    "start": "513200",
    "end": "520479"
  },
  {
    "text": "importantly we can better contain the blast radius before it spreads so this is what modern platform observability is",
    "start": "520479",
    "end": "527760"
  },
  {
    "text": "really about not just like dashboards and alert but giving teams the power to you know stay ahead of problems and",
    "start": "527760",
    "end": "534160"
  },
  {
    "text": "maintain confidence in the health of your systems so without our SLO dashboards we wandered in the darkness",
    "start": "534160",
    "end": "540720"
  },
  {
    "text": "but now our dashboards will eliminate our path clearly",
    "start": "540720",
    "end": "546680"
  },
  {
    "text": "thank you Alexa um a little disclaimer before we go into the next part of the presentation the dashboards that you see",
    "start": "547600",
    "end": "553760"
  },
  {
    "text": "here are just for the purpose of demo they do not represent the health of any application or service in any",
    "start": "553760",
    "end": "559519"
  },
  {
    "text": "organization uh a common question a lot of us face u",
    "start": "559519",
    "end": "564880"
  },
  {
    "text": "even with the best intentions dashboards often fail so why do they fail though first of all there's too much noise and",
    "start": "564880",
    "end": "571839"
  },
  {
    "text": "very little signal take the example here we tracking latencies across nine",
    "start": "571839",
    "end": "576880"
  },
  {
    "text": "different models but with too many things led in we cannot really tell what's wrong there's no connection to",
    "start": "576880",
    "end": "583040"
  },
  {
    "text": "any upstream issues or any downstream impact second these dashboards are",
    "start": "583040",
    "end": "588560"
  },
  {
    "text": "rarely maintained uh as they say dashboarding is done in the set and",
    "start": "588560",
    "end": "593600"
  },
  {
    "text": "forget mode um they get set up once but as your platform your infrastructure",
    "start": "593600",
    "end": "598880"
  },
  {
    "text": "your applications evolve uh they quickly became become stale this leads to",
    "start": "598880",
    "end": "605600"
  },
  {
    "text": "misleading signals and even outdated metrics third uh we run into siloed",
    "start": "605600",
    "end": "611680"
  },
  {
    "text": "views uh you can see that model 2 here seems to have high latency but again",
    "start": "611680",
    "end": "616959"
  },
  {
    "text": "there's no connection with the upstream traffic patterns or any downstream impact or business impact for that",
    "start": "616959",
    "end": "623440"
  },
  {
    "text": "matter and finally observability without any kind of alerts is just pure storage",
    "start": "623440",
    "end": "630399"
  },
  {
    "text": "without defined thresholds or SLO based alerting teams cannot discern which",
    "start": "630399",
    "end": "636000"
  },
  {
    "text": "issues to act on and when to act on",
    "start": "636000",
    "end": "641000"
  },
  {
    "text": "uh quoting one of our in-house observability Salarino here if your",
    "start": "641760",
    "end": "647040"
  },
  {
    "text": "dashboard takes more than 90 seconds to understand it's bad um and let's just",
    "start": "647040",
    "end": "652560"
  },
  {
    "text": "acknowledge SLOs's are hard to get right so where do you start step one is to",
    "start": "652560",
    "end": "658160"
  },
  {
    "text": "start with user what does good look like to them if you're running a back-end job",
    "start": "658160",
    "end": "663920"
  },
  {
    "text": "you need need not need like a 59 uh uptime for your service it is it's well",
    "start": "663920",
    "end": "670480"
  },
  {
    "text": "within accept acceptable latency so one size fit all will not work here um next",
    "start": "670480",
    "end": "677839"
  },
  {
    "text": "choose metrics that actually map to the experience like latency error rates throughput uh but be careful just",
    "start": "677839",
    "end": "685279"
  },
  {
    "text": "because you can measure something does not mean it's valuable pick the ones that actually align with the performance",
    "start": "685279",
    "end": "691959"
  },
  {
    "text": "outcomes um then make sure your infrastructure is instrumented with these metrics um if you cannot reliably",
    "start": "691959",
    "end": "699600"
  },
  {
    "text": "collect the data you're missing visibility into some of these layers uh",
    "start": "699600",
    "end": "704880"
  },
  {
    "text": "then test your assumptions what does that mean uh you use some real world baseline data um in case of existing",
    "start": "704880",
    "end": "711200"
  },
  {
    "text": "services uh it's always good to look backwards to look forward um use that",
    "start": "711200",
    "end": "717600"
  },
  {
    "text": "data to validate your chosen SLIs and reflect the uh and whether they actually reflect the end user experience um and",
    "start": "717600",
    "end": "724959"
  },
  {
    "text": "finally benchmark what's realistically achievable you don't want to aim for 100% if your systems has never even hit",
    "start": "724959",
    "end": "732000"
  },
  {
    "text": "like 99.5% uptime um your SLOs should stretch your teams but it should also",
    "start": "732000",
    "end": "738880"
  },
  {
    "text": "remain attainable uh now what are some",
    "start": "738880",
    "end": "744320"
  },
  {
    "text": "principles of good or great dashboards when designing effective observability",
    "start": "744320",
    "end": "749760"
  },
  {
    "text": "dashboards uh it starts with at the glance clarity the goal is to instantly",
    "start": "749760",
    "end": "755120"
  },
  {
    "text": "answer is everything healthy if not where should we look first simplicity here is the power we need to design our",
    "start": "755120",
    "end": "762880"
  },
  {
    "text": "dashboards with high cardality in mind today's system generate data with many many dimensions dashboard should be",
    "start": "762880",
    "end": "769839"
  },
  {
    "text": "capable of slicing and dicing into those complexities without becoming overwhelming it's important to balance",
    "start": "769839",
    "end": "776800"
  },
  {
    "text": "your aggregated global view with a drill down capability you want a top level",
    "start": "776800",
    "end": "782320"
  },
  {
    "text": "pulse of your application or your platform but also the flexibility to zoom in whether it's uh into specific",
    "start": "782320",
    "end": "788760"
  },
  {
    "text": "services error patterns or um time ranges when a issue arises dashboards",
    "start": "788760",
    "end": "796000"
  },
  {
    "text": "are even more powerful when you supplement them with contextual data such as traces logs uh it's not just",
    "start": "796000",
    "end": "802560"
  },
  {
    "text": "about the metrics but the story behind them right what happened where and why",
    "start": "802560",
    "end": "807839"
  },
  {
    "text": "and finally the focus should always be on actionable insights it's not enough to just surface the data we want to",
    "start": "807839",
    "end": "814480"
  },
  {
    "text": "drive decisions prioritize clear thresholds and meaningful alerts that will help uh the teams to know when to",
    "start": "814480",
    "end": "821519"
  },
  {
    "text": "act and what to act on uh let's look at an example of a",
    "start": "821519",
    "end": "827200"
  },
  {
    "text": "typical dashboard for an LLM application it's well structured into key performance areas such as success rates",
    "start": "827200",
    "end": "834240"
  },
  {
    "text": "burn rates error budgets latency and throughput this dashboard gives us a 360deree view",
    "start": "834240",
    "end": "840160"
  },
  {
    "text": "of model 1's performance with respect to its SLOs's u over a period of 48 hours",
    "start": "840160",
    "end": "846560"
  },
  {
    "text": "starting with the good news uh the model seems to have perfect success rate um even the for the past 7 days u that",
    "start": "846560",
    "end": "854800"
  },
  {
    "text": "means short-term reliability is strong however if we take a look at the 28 day window uh we've hit the success rate of",
    "start": "854800",
    "end": "862680"
  },
  {
    "text": "99.9% which is still at the edge uh the burn rate though we have exceeded it in",
    "start": "862680",
    "end": "869120"
  },
  {
    "text": "our long-term goal what that means is uh the burn and combine that with the burn",
    "start": "869120",
    "end": "874720"
  },
  {
    "text": "rate over time chart right it's not a spike but a steady climb that tells us this is not a one-time outage it's a",
    "start": "874720",
    "end": "881680"
  },
  {
    "text": "pattern of small repeated failures looking at latency we see a lot of fluctuation between time to first",
    "start": "881680",
    "end": "888240"
  },
  {
    "text": "token and even the inter token latency uh there are some spikes in both this",
    "start": "888240",
    "end": "893680"
  },
  {
    "text": "could easily be a sign of scaling inefficiencies or model bottlenecks under load and lastly while the",
    "start": "893680",
    "end": "900800"
  },
  {
    "text": "throughput is relatively low there are some error bars present um this means no",
    "start": "900800",
    "end": "906639"
  },
  {
    "text": "major concern right now but they could be contributing to your budget burn overall the model is performing well in",
    "start": "906639",
    "end": "913839"
  },
  {
    "text": "the short term uh but we have se seen start uh signs of early degradation and",
    "start": "913839",
    "end": "919920"
  },
  {
    "text": "it's a good candidate for preventative uh optimization whether that's uh",
    "start": "919920",
    "end": "925279"
  },
  {
    "text": "fine-tuning your retry logic or latency tuning or even adjusting how we handle edge",
    "start": "925279",
    "end": "931320"
  },
  {
    "text": "cases now we'll break down the uh previous dashboard into smaller",
    "start": "931320",
    "end": "936399"
  },
  {
    "text": "components and just zoom in into what each one of them has to offer um a quick glance here tells me um that are we",
    "start": "936399",
    "end": "943440"
  },
  {
    "text": "meeting our SLOs's is there any ongoing issue u this this particular view or",
    "start": "943440",
    "end": "949199"
  },
  {
    "text": "panel gives me a clear temporal glandity uh because it summarizes success rates",
    "start": "949199",
    "end": "954639"
  },
  {
    "text": "over different period of time and then it's also supplemented by a time series",
    "start": "954639",
    "end": "960279"
  },
  {
    "text": "insight there's a trend that I can look at and you can see there's a clear drop",
    "start": "960279",
    "end": "965360"
  },
  {
    "text": "on one of the days it makes it easier to correlate with incident timelines and",
    "start": "965360",
    "end": "971800"
  },
  {
    "text": "deployments speaking of burn rate this is a sideby-side comparison of why burn",
    "start": "971800",
    "end": "977040"
  },
  {
    "text": "rate is such a powerful SLO signal especially when tracked across multiple",
    "start": "977040",
    "end": "982399"
  },
  {
    "text": "time windows on the left we are looking at a transient incident there's a sharp",
    "start": "982399",
    "end": "988079"
  },
  {
    "text": "spike uh in the one one hour burn rate which signals something seriously went wrong uh but it resolved quickly and the",
    "start": "988079",
    "end": "995519"
  },
  {
    "text": "longerterm windows stayed flat um this suggests there's no long long-term impact to reliability however on the",
    "start": "995519",
    "end": "1003199"
  },
  {
    "text": "right side the burn rate starts low and gradually increases across one day and",
    "start": "1003199",
    "end": "1008880"
  },
  {
    "text": "then continues to increase across 28 days uh time window this is a classical",
    "start": "1008880",
    "end": "1014880"
  },
  {
    "text": "pertinent issue nothing looks critical at the moment but over time the system is quickly eroding the error patch and",
    "start": "1014880",
    "end": "1022399"
  },
  {
    "text": "this slow burn rate often slips into the radar if not monitored monitoring multiple windows helps distinguish a",
    "start": "1022399",
    "end": "1029760"
  },
  {
    "text": "noisy blip from a real structural reliability issue both scenarios result",
    "start": "1029760",
    "end": "1036160"
  },
  {
    "text": "in different kind of risk one is a fire drill and the other one is a slow drift in SLO violation you need different",
    "start": "1036160",
    "end": "1043678"
  },
  {
    "text": "responses for each now before we move on to the next",
    "start": "1043679",
    "end": "1049520"
  },
  {
    "text": "dashboard uh let's take a minute to understand a few critical concepts of latencies in a typical LLM inference",
    "start": "1049520",
    "end": "1056960"
  },
  {
    "text": "time to first token which is TTFT and inter token latency ITL time to first",
    "start": "1056960",
    "end": "1062480"
  },
  {
    "text": "token is the time it takes from when a prompt or an input is provided to a model to when it returns the first token",
    "start": "1062480",
    "end": "1070000"
  },
  {
    "text": "it's absolutely critical for perceived responsiveness as fast time to first",
    "start": "1070000",
    "end": "1075120"
  },
  {
    "text": "token gives user uh confidence that the system is working and it's con in",
    "start": "1075120",
    "end": "1080799"
  },
  {
    "text": "conversational interfaces it defines the first impression inter token latency is",
    "start": "1080799",
    "end": "1085919"
  },
  {
    "text": "the time to take taken to generate each subsequent token after the first token",
    "start": "1085919",
    "end": "1091280"
  },
  {
    "text": "from the user's perspective this governs how responsive or smooth the output is",
    "start": "1091280",
    "end": "1096400"
  },
  {
    "text": "streamed especially for longer use cases such as documents code and summaries itl",
    "start": "1096400",
    "end": "1103039"
  },
  {
    "text": "is if ITL is consistently uh slow or inconsistent the user experience is",
    "start": "1103039",
    "end": "1108799"
  },
  {
    "text": "jerky and delayed even if the first token comes back quickly",
    "start": "1108799",
    "end": "1114880"
  },
  {
    "text": "now taking those two metrics onto our dashboard this focuses on uh streaming",
    "start": "1114880",
    "end": "1120799"
  },
  {
    "text": "latency for one of the models um and as we can see TTFT hovers between 400 to",
    "start": "1120799",
    "end": "1127039"
  },
  {
    "text": "700 millisecond which is reasonable for a production grade inference service uh but there are some dips in spikes this",
    "start": "1127039",
    "end": "1134000"
  },
  {
    "text": "could be linked to cold starts load imbalance or even backend queuing even before the inference actually starts",
    "start": "1134000",
    "end": "1141120"
  },
  {
    "text": "itil is mostly low and consistent but there are periodic spikes even up to",
    "start": "1141120",
    "end": "1146640"
  },
  {
    "text": "1400 milliseconds which could indicate concurrency limits batching inefficiencies or even degraded",
    "start": "1146640",
    "end": "1152880"
  },
  {
    "text": "transformer performance together these two metrics offer us a holistic view of user perceived performance of a LLM",
    "start": "1152880",
    "end": "1160840"
  },
  {
    "text": "application uh and we want them to be low and stable this kind of",
    "start": "1160840",
    "end": "1166000"
  },
  {
    "text": "observability is not only essential for engineers S surres or platform owners but also for product teams because when",
    "start": "1166000",
    "end": "1173200"
  },
  {
    "text": "it comes to gen AI performance is the experience again this is a very",
    "start": "1173200",
    "end": "1179760"
  },
  {
    "text": "simplistic uh panel for throughput um out of the total request every single",
    "start": "1179760",
    "end": "1184960"
  },
  {
    "text": "one returned 200 okay which on the surface signals excellent performance and no userfacing failure we see 0400s",
    "start": "1184960",
    "end": "1193039"
  },
  {
    "text": "meaning no client side misuses like bad input or malform a request also",
    "start": "1193039",
    "end": "1199320"
  },
  {
    "text": "05503s suggesting no infrastructure issues u model timeouts or overload um",
    "start": "1199320",
    "end": "1204840"
  },
  {
    "text": "errors now while this might seem perfect on the surface uh in most production environment a true 100% success rate is",
    "start": "1204840",
    "end": "1212640"
  },
  {
    "text": "often alarming so this is a quick prompt for us to check are we correctly capturing error responses are failures",
    "start": "1212640",
    "end": "1220400"
  },
  {
    "text": "being retrieded or overwritten before reaching the metrics are we filtering out any endpoints or failure types if",
    "start": "1220400",
    "end": "1228720"
  },
  {
    "text": "everything checks out this give us strong confidence that the system is performing reliably but if not we might",
    "start": "1228720",
    "end": "1235039"
  },
  {
    "text": "not need uh need to revisit our instrumentation to ensure we are seeing the full picture",
    "start": "1235039",
    "end": "1242720"
  },
  {
    "text": "talking of a different kind of throughput which is more relevant for an LLM application token throughput uh",
    "start": "1242720",
    "end": "1249120"
  },
  {
    "text": "broken into um prompts and generations first we see there's a large",
    "start": "1249120",
    "end": "1255280"
  },
  {
    "text": "spike in prompt traffic early on that could indicate it's a bad job a burst of",
    "start": "1255280",
    "end": "1260400"
  },
  {
    "text": "inference load or high demand from upstream service following that we observe continued bursts through the",
    "start": "1260400",
    "end": "1267679"
  },
  {
    "text": "throughout the day with our gateway uh traffic gradually tapering off um and",
    "start": "1267679",
    "end": "1273600"
  },
  {
    "text": "right after the activity completely drops off this could mean a schedule shutdown a traffic routing issue or even",
    "start": "1273600",
    "end": "1280559"
  },
  {
    "text": "a scaling event another key insight generation token consistently stay low",
    "start": "1280559",
    "end": "1287039"
  },
  {
    "text": "even during periods of high prompting that could point to very short",
    "start": "1287039",
    "end": "1292200"
  },
  {
    "text": "completions or prompt only operations such as embeddings or validations or",
    "start": "1292200",
    "end": "1297440"
  },
  {
    "text": "possible inference failures or timer post prompt this kind of throughput analysis is important because it helps",
    "start": "1297440",
    "end": "1305120"
  },
  {
    "text": "us correlate our usage pattern with cost error budget",
    "start": "1305120",
    "end": "1311919"
  },
  {
    "text": "latency next we have a dashboard which monitors the end toend performance of an",
    "start": "1313320",
    "end": "1319840"
  },
  {
    "text": "inference service it gives us detailed look into the response time across the predictor stack from the gateway to the",
    "start": "1319840",
    "end": "1326640"
  },
  {
    "text": "Q proxy to the model itself on the left we see percentile breakdowns",
    "start": "1326640",
    "end": "1332080"
  },
  {
    "text": "of latency um and the good news is P50 is under sub millisecond u this is a",
    "start": "1332080",
    "end": "1338559"
  },
  {
    "text": "strong indicator that there are network and routing layers are healthy however",
    "start": "1338559",
    "end": "1343840"
  },
  {
    "text": "when we shift to the actual model inference you'll notice that the response times are higher that's not",
    "start": "1343840",
    "end": "1350320"
  },
  {
    "text": "alarming but it actually tells us bulk of our latency lies within the inference and queueing",
    "start": "1350320",
    "end": "1356559"
  },
  {
    "text": "looking at the middle in the right panel queueing uh delay shows some spiky behavior uh this suggests that there's",
    "start": "1356559",
    "end": "1363919"
  },
  {
    "text": "some intermittent queueing pressure likely due to um bursty load patterns or",
    "start": "1363919",
    "end": "1369280"
  },
  {
    "text": "even uneven request patching um while these spikes do not appear to degrade",
    "start": "1369280",
    "end": "1375600"
  },
  {
    "text": "overall performance significantly um our trail latencies remain within acceptable limits uh overall the system is",
    "start": "1375600",
    "end": "1383039"
  },
  {
    "text": "performing well but these traceable queueing spikes highlights areas for possible",
    "start": "1383039",
    "end": "1388919"
  },
  {
    "text": "optimization and finally uh all response code latencies are tied to 200 responses",
    "start": "1388919",
    "end": "1394799"
  },
  {
    "text": "which means we're not seeing any latency inflation due to uh retries or errors",
    "start": "1394799",
    "end": "1400159"
  },
  {
    "text": "and that's exactly what we want uh moving on to a different type of",
    "start": "1400159",
    "end": "1405919"
  },
  {
    "text": "dashboard uh which I tend to use pretty much every day which is utilization of",
    "start": "1405919",
    "end": "1411600"
  },
  {
    "text": "your infrastructure uh this gives you a focused view of GPU resource demand and",
    "start": "1411600",
    "end": "1416799"
  },
  {
    "text": "usage over a 7-day window uh specifically for a cluster that I'm looking at it combines real-time and",
    "start": "1416799",
    "end": "1423600"
  },
  {
    "text": "historical context to assess how effectively GPU's resources are being requested and utilized uh as you can see",
    "start": "1423600",
    "end": "1431120"
  },
  {
    "text": "there's low and sporadic usage through most of the week even when the requests were high actually utilization remained",
    "start": "1431120",
    "end": "1438320"
  },
  {
    "text": "well below 100% I would say even below 40% for some uh this indicates a",
    "start": "1438320",
    "end": "1444960"
  },
  {
    "text": "potential gap between allocated versus used resources the system may be",
    "start": "1444960",
    "end": "1450159"
  },
  {
    "text": "suffering from in inefficient job scheduling overprovisioning or idle allocations sign um optimization",
    "start": "1450159",
    "end": "1458159"
  },
  {
    "text": "opportunity either at the workload or scheduling level now talking about supplementing",
    "start": "1458159",
    "end": "1465279"
  },
  {
    "text": "your dashboards uh with contextual data this is an example of a distributed trace for an inference call um while all",
    "start": "1465279",
    "end": "1472640"
  },
  {
    "text": "operations are within a single service um it spans multiple internal steps uh",
    "start": "1472640",
    "end": "1478240"
  },
  {
    "text": "what you notice here is inference request actually takes up majority of the time signaling this is the main",
    "start": "1478240",
    "end": "1484440"
  },
  {
    "text": "bottleneck supporting spans for model selection input validation or even HTTP",
    "start": "1484440",
    "end": "1489760"
  },
  {
    "text": "response sending are comparatively lightweight uh you'll notice that",
    "start": "1489760",
    "end": "1495120"
  },
  {
    "text": "there's um a few uh pre preliminary operations like validation model",
    "start": "1495120",
    "end": "1500799"
  },
  {
    "text": "selection um and they are lightweight right so if we look deeper though this",
    "start": "1500799",
    "end": "1505919"
  },
  {
    "text": "trace is not just useful for debugging slow requests but for understanding where to invest optimization effort",
    "start": "1505919",
    "end": "1513279"
  },
  {
    "text": "whether that's reducing model latency caching results or even parallelizing",
    "start": "1513279",
    "end": "1519519"
  },
  {
    "text": "tasks now that we have defined SLI and SLOs's uh the next step is to make them",
    "start": "1520279",
    "end": "1526120"
  },
  {
    "text": "actionable and that's where burn rate based alerting comes into picture a burn",
    "start": "1526120",
    "end": "1531840"
  },
  {
    "text": "rate tells you how quickly you're consuming your error budget when the burn rate is high it's a sign that the",
    "start": "1531840",
    "end": "1537520"
  },
  {
    "text": "system is degrading faster than a SLO allows uh let's break it down right uh",
    "start": "1537520",
    "end": "1543039"
  },
  {
    "text": "with some examples here a 1 hour burn rate above 13.5 means your system is burning the entire uh error budget for a",
    "start": "1543039",
    "end": "1551039"
  },
  {
    "text": "30-day period in 2hour window this is likely a major outage and you want a high priority alert here a one day burn",
    "start": "1551039",
    "end": "1559200"
  },
  {
    "text": "rate over 2.8 still isn't great uh but it could mean a moderate issue and",
    "start": "1559200",
    "end": "1565520"
  },
  {
    "text": "that's not immediately catastrophic uh but could escalate fast and if your",
    "start": "1565520",
    "end": "1570880"
  },
  {
    "text": "7-day or 1 day burn rate exceeds one you're seeing a slow burn less urgent",
    "start": "1570880",
    "end": "1576720"
  },
  {
    "text": "but it might be a sign of ongoing degradation or even user impactive behavior the key is to match these",
    "start": "1576720",
    "end": "1584080"
  },
  {
    "text": "thresholds to your environment don't treat your dev and prod the same you want to surface high severity issues in",
    "start": "1584080",
    "end": "1590880"
  },
  {
    "text": "production faster but not drown in noise elsewhere um also don't just fire alert",
    "start": "1590880",
    "end": "1597279"
  },
  {
    "text": "for any blip focus on meaningful sustained violations and prioritize based on severity and impact and finally",
    "start": "1597279",
    "end": "1605679"
  },
  {
    "text": "remember alerting this is not static you'll need to tune it over time um use data from your incident reports",
    "start": "1605679",
    "end": "1612480"
  },
  {
    "text": "postmortems and SLO reviews to refine your thresholds and",
    "start": "1612480",
    "end": "1618279"
  },
  {
    "text": "logic um taking the the learnings from how you can fine-tune your dashboards um",
    "start": "1618279",
    "end": "1624000"
  },
  {
    "text": "this is an example of what we can call a persistent failure mode this could",
    "start": "1624000",
    "end": "1629520"
  },
  {
    "text": "likely be due to a misbehaving deployment um and upstream dependency failing intermittently or poor handling",
    "start": "1629520",
    "end": "1636720"
  },
  {
    "text": "of edge cases causing request failures uh because burn rate remains high across",
    "start": "1636720",
    "end": "1642159"
  },
  {
    "text": "all the windows this isn't just a spike it suggests that the issue has been ongoing and not fully mitigated uh at",
    "start": "1642159",
    "end": "1650480"
  },
  {
    "text": "10.3 the platform is burning through the entire alliable budget 10 times faster",
    "start": "1650480",
    "end": "1655600"
  },
  {
    "text": "than tolerated this is actually critical and unsustainable um observe scattered dips",
    "start": "1655600",
    "end": "1662880"
  },
  {
    "text": "and uh in success rate here during the burn uh window these dips actually",
    "start": "1662880",
    "end": "1668000"
  },
  {
    "text": "correlate with the periods of elevated errors um and the errors are frequent",
    "start": "1668000",
    "end": "1673279"
  },
  {
    "text": "and sustained rather than isolated anomalies then there's this example um",
    "start": "1673279",
    "end": "1681279"
  },
  {
    "text": "just 100% straight zero burn rate throughout all the time windows looks too good to be true right but it",
    "start": "1681279",
    "end": "1688080"
  },
  {
    "text": "actually suggests either the metrics aren't being collected accurately errors are being swallowed or misclassified or",
    "start": "1688080",
    "end": "1695279"
  },
  {
    "text": "there's a discrepancy in how we have defined success or failure for this particular service um in a production system",
    "start": "1695279",
    "end": "1703679"
  },
  {
    "text": "especially one handling real world traffic this level of perfection across all time windows is highly suspicious",
    "start": "1703679",
    "end": "1710000"
  },
  {
    "text": "and should be prompting a deeper review uh so how do you fine-tune your SLOs's",
    "start": "1710000",
    "end": "1716480"
  },
  {
    "text": "right you start by grounding in reality um you looked at the patterns of how",
    "start": "1716480",
    "end": "1722000"
  },
  {
    "text": "your error budget is being consumed if you're not using uh it at all your SLO",
    "start": "1722000",
    "end": "1727600"
  },
  {
    "text": "might be too lenient uh on the flip side if you're burning through your error uh",
    "start": "1727600",
    "end": "1733240"
  },
  {
    "text": "budget consistently the target might be too aggressive or misaligned patterns in",
    "start": "1733240",
    "end": "1739120"
  },
  {
    "text": "burn rate can actually tell you um what a common type of error is whether it's",
    "start": "1739120",
    "end": "1744799"
  },
  {
    "text": "and which part of u the workload it is u every incident is actually learning",
    "start": "1744799",
    "end": "1751240"
  },
  {
    "text": "opportunity u if you had dine that did not register as a violation that's a",
    "start": "1751240",
    "end": "1756880"
  },
  {
    "text": "sign that your metrics or thresholds might not be measuring the right",
    "start": "1756880",
    "end": "1762039"
  },
  {
    "text": "thing to wrap things up if you're building or running an AI platform observability is an optional is",
    "start": "1762039",
    "end": "1769240"
  },
  {
    "text": "foundational we have seen how robust observability stack built with open source tools such as open telemetry",
    "start": "1769240",
    "end": "1776399"
  },
  {
    "text": "Prometheus Envoy uh give you full visibility across inference uh pipelines",
    "start": "1776399",
    "end": "1781679"
  },
  {
    "text": "networking layers and the GPU resource usage but tooling alone is not enough",
    "start": "1781679",
    "end": "1787279"
  },
  {
    "text": "best practices matter this means correlating your logs and your traces to",
    "start": "1787279",
    "end": "1792640"
  },
  {
    "text": "shorten your mean time to resolution it also means setting smart achievable",
    "start": "1792640",
    "end": "1798159"
  },
  {
    "text": "SLOs's and automating alerts based on burn rates and not just vanity metrics",
    "start": "1798159",
    "end": "1804320"
  },
  {
    "text": "and most importantly observability is never done the most resilient platforms",
    "start": "1804320",
    "end": "1809760"
  },
  {
    "text": "are the ones where feedback from incidents data and your users constantly feed a loop into uh refinement and",
    "start": "1809760",
    "end": "1817360"
  },
  {
    "text": "improvement if you walk away with one idea from today treat observability as a",
    "start": "1817360",
    "end": "1822880"
  },
  {
    "text": "product and not just a tool set build it as you evolve your platform um and how",
    "start": "1822880",
    "end": "1828640"
  },
  {
    "text": "it grows just a small plugin we're also hiring across multiple uh roles within",
    "start": "1828640",
    "end": "1835360"
  },
  {
    "text": "our AI or at Bloomberg thank",
    "start": "1835360",
    "end": "1840559"
  },
  {
    "text": "you amazing",
    "start": "1843399",
    "end": "1846960"
  }
]