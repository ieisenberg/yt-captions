[
  {
    "text": "welcome",
    "start": "6180",
    "end": "7680"
  },
  {
    "text": "um welcome to my talk how to blow up a",
    "start": "7680",
    "end": "9660"
  },
  {
    "text": "kubernetes cluster sorry about the",
    "start": "9660",
    "end": "11280"
  },
  {
    "text": "clickbait but judging by how crowd the",
    "start": "11280",
    "end": "13320"
  },
  {
    "text": "rumors I guess it kind of worked",
    "start": "13320",
    "end": "15480"
  },
  {
    "text": "um",
    "start": "15480",
    "end": "16619"
  },
  {
    "text": "I hope some of you have read the talks",
    "start": "16619",
    "end": "18480"
  },
  {
    "text": "description because if you haven't",
    "start": "18480",
    "end": "19560"
  },
  {
    "text": "you'll be disappointed to learn that",
    "start": "19560",
    "end": "21119"
  },
  {
    "text": "this is actually about resource",
    "start": "21119",
    "end": "22800"
  },
  {
    "text": "management and it's about Resource",
    "start": "22800",
    "end": "24900"
  },
  {
    "text": "Management from the point of view of an",
    "start": "24900",
    "end": "26699"
  },
  {
    "text": "application developer because that's who",
    "start": "26699",
    "end": "28560"
  },
  {
    "text": "I am I'm an application developer my",
    "start": "28560",
    "end": "30300"
  },
  {
    "text": "name is Felix I'm a software engineer at",
    "start": "30300",
    "end": "32578"
  },
  {
    "text": "italatech we are a consultancy slash",
    "start": "32579",
    "end": "35219"
  },
  {
    "text": "agency based in Germany",
    "start": "35219",
    "end": "37260"
  },
  {
    "text": "and the reason why I'm giving this talk",
    "start": "37260",
    "end": "40500"
  },
  {
    "text": "is because last year I was I gained",
    "start": "40500",
    "end": "43559"
  },
  {
    "text": "access to a kubernetes cluster and I",
    "start": "43559",
    "end": "45780"
  },
  {
    "text": "didn't really have a clue on how",
    "start": "45780",
    "end": "47579"
  },
  {
    "text": "kubernetes works and I was told there",
    "start": "47579",
    "end": "51180"
  },
  {
    "text": "were some parts that need limits and",
    "start": "51180",
    "end": "53340"
  },
  {
    "text": "they need requests set and I read about",
    "start": "53340",
    "end": "56520"
  },
  {
    "text": "it in the documentation and",
    "start": "56520",
    "end": "58800"
  },
  {
    "text": "I thought I was",
    "start": "58800",
    "end": "60300"
  },
  {
    "text": "doing good things but it turned out the",
    "start": "60300",
    "end": "63960"
  },
  {
    "text": "entire thing blew up",
    "start": "63960",
    "end": "66720"
  },
  {
    "text": "and this is why I'm giving this talk and",
    "start": "66720",
    "end": "69299"
  },
  {
    "text": "since this is the one-on-one track I'll",
    "start": "69299",
    "end": "71640"
  },
  {
    "text": "start from the top so let's look at what",
    "start": "71640",
    "end": "74400"
  },
  {
    "text": "the documentation has to say about",
    "start": "74400",
    "end": "76020"
  },
  {
    "text": "resources",
    "start": "76020",
    "end": "77880"
  },
  {
    "text": "now when we talk about resources",
    "start": "77880",
    "end": "79380"
  },
  {
    "text": "obviously we're talking about compute",
    "start": "79380",
    "end": "80939"
  },
  {
    "text": "resources and there's CPU there's memory",
    "start": "80939",
    "end": "83759"
  },
  {
    "text": "these are the obvious ones there's tons",
    "start": "83759",
    "end": "86640"
  },
  {
    "text": "more like ephemeral storage there's PID",
    "start": "86640",
    "end": "88740"
  },
  {
    "text": "limiting and many more but for the",
    "start": "88740",
    "end": "91500"
  },
  {
    "text": "purpose of this talk I'll concentrate on",
    "start": "91500",
    "end": "93479"
  },
  {
    "text": "CPU and memory",
    "start": "93479",
    "end": "96920"
  },
  {
    "text": "now the documentation tells us how we",
    "start": "97500",
    "end": "100860"
  },
  {
    "text": "can set requests and limits and it",
    "start": "100860",
    "end": "102840"
  },
  {
    "text": "defines requests as the amounts of",
    "start": "102840",
    "end": "105240"
  },
  {
    "text": "memory and CPU that is guaranteed for",
    "start": "105240",
    "end": "108060"
  },
  {
    "text": "our containers and limits is the amount",
    "start": "108060",
    "end": "110460"
  },
  {
    "text": "of CPU or memory that we cannot exceed",
    "start": "110460",
    "end": "113100"
  },
  {
    "text": "with our containers and there are",
    "start": "113100",
    "end": "115020"
  },
  {
    "text": "different units we can use to set these",
    "start": "115020",
    "end": "120140"
  },
  {
    "text": "um settings so for CPU we have one CPU",
    "start": "120240",
    "end": "123899"
  },
  {
    "text": "unit which is one core either physical",
    "start": "123899",
    "end": "125939"
  },
  {
    "text": "or virtual and we can use different",
    "start": "125939",
    "end": "127979"
  },
  {
    "text": "fractions for this core and we can",
    "start": "127979",
    "end": "130940"
  },
  {
    "text": "Define these fractions using Milli CPU",
    "start": "130940",
    "end": "133680"
  },
  {
    "text": "or milli core whereas 1000 Milli core",
    "start": "133680",
    "end": "136379"
  },
  {
    "text": "equals to one CPU unit",
    "start": "136379",
    "end": "139200"
  },
  {
    "text": "and for memory the base unit is byte and",
    "start": "139200",
    "end": "142319"
  },
  {
    "text": "we can use all kinds of",
    "start": "142319",
    "end": "145140"
  },
  {
    "text": "um kilobyte up to exabyte and the binary",
    "start": "145140",
    "end": "148620"
  },
  {
    "text": "equivalent Cube byte up to XP byte of",
    "start": "148620",
    "end": "151260"
  },
  {
    "text": "course now when we look at a plot",
    "start": "151260",
    "end": "153060"
  },
  {
    "text": "definition file when we Define those",
    "start": "153060",
    "end": "155879"
  },
  {
    "text": "requests and limits they look like this",
    "start": "155879",
    "end": "157739"
  },
  {
    "text": "for this part or for this container",
    "start": "157739",
    "end": "160440"
  },
  {
    "text": "rather they're always on a container",
    "start": "160440",
    "end": "162000"
  },
  {
    "text": "basis we have defined a request of 64",
    "start": "162000",
    "end": "166440"
  },
  {
    "text": "maybe byte and a CPU request of 250",
    "start": "166440",
    "end": "170280"
  },
  {
    "text": "Milli core and the limit is two times",
    "start": "170280",
    "end": "173340"
  },
  {
    "text": "our request so we have 128 megabytes and",
    "start": "173340",
    "end": "177959"
  },
  {
    "text": "500 millicore",
    "start": "177959",
    "end": "180180"
  },
  {
    "text": "now what does happen with our part when",
    "start": "180180",
    "end": "182879"
  },
  {
    "text": "we exceed our memory limit",
    "start": "182879",
    "end": "185280"
  },
  {
    "text": "and that's kind of easy because we just",
    "start": "185280",
    "end": "189060"
  },
  {
    "text": "get to terminated we run into an out of",
    "start": "189060",
    "end": "191400"
  },
  {
    "text": "memory kill and that's simply because",
    "start": "191400",
    "end": "193080"
  },
  {
    "text": "memory is an incompressible resource we",
    "start": "193080",
    "end": "196260"
  },
  {
    "text": "cannot make more of it when once it's",
    "start": "196260",
    "end": "198180"
  },
  {
    "text": "gone it's gone",
    "start": "198180",
    "end": "199680"
  },
  {
    "text": "and we cannot share it",
    "start": "199680",
    "end": "202200"
  },
  {
    "text": "for CPU it looks a bit different because",
    "start": "202200",
    "end": "204360"
  },
  {
    "text": "CPU is a compressible resource we have",
    "start": "204360",
    "end": "207300"
  },
  {
    "text": "the possibility to throttle and we don't",
    "start": "207300",
    "end": "209879"
  },
  {
    "text": "need to terminate",
    "start": "209879",
    "end": "211319"
  },
  {
    "text": "these parts or kubernetes doesn't need",
    "start": "211319",
    "end": "213840"
  },
  {
    "text": "to terminate these parts",
    "start": "213840",
    "end": "215700"
  },
  {
    "text": "now let's take a quick look at how pods",
    "start": "215700",
    "end": "218459"
  },
  {
    "text": "are scheduled in kubernetes",
    "start": "218459",
    "end": "220319"
  },
  {
    "text": "and for this scenario we have two",
    "start": "220319",
    "end": "222299"
  },
  {
    "text": "different nodes both nodes have five",
    "start": "222299",
    "end": "225840"
  },
  {
    "text": "units of memory and five units of CPU",
    "start": "225840",
    "end": "228720"
  },
  {
    "text": "let's just say one of these memory units",
    "start": "228720",
    "end": "230640"
  },
  {
    "text": "is one gigaby byte and one CPU is one",
    "start": "230640",
    "end": "233099"
  },
  {
    "text": "core",
    "start": "233099",
    "end": "234000"
  },
  {
    "text": "and now we want to schedule a part and",
    "start": "234000",
    "end": "238019"
  },
  {
    "text": "the part is requesting three gigaby",
    "start": "238019",
    "end": "240540"
  },
  {
    "text": "bytes and two CPUs",
    "start": "240540",
    "end": "243780"
  },
  {
    "text": "the scheduling is done in a round robin",
    "start": "243780",
    "end": "246120"
  },
  {
    "text": "kind of fashion so the first part is",
    "start": "246120",
    "end": "248819"
  },
  {
    "text": "probably going to be placed on the first",
    "start": "248819",
    "end": "250319"
  },
  {
    "text": "node and before kubernetes is going to",
    "start": "250319",
    "end": "253980"
  },
  {
    "text": "place the part on the first node it's",
    "start": "253980",
    "end": "255480"
  },
  {
    "text": "going to check",
    "start": "255480",
    "end": "256680"
  },
  {
    "text": "are we having enough resources",
    "start": "256680",
    "end": "259560"
  },
  {
    "text": "um",
    "start": "259560",
    "end": "260639"
  },
  {
    "text": "to fulfill this request so are we having",
    "start": "260639",
    "end": "263220"
  },
  {
    "text": "three gigaby bytes and two CPUs on this",
    "start": "263220",
    "end": "266639"
  },
  {
    "text": "node",
    "start": "266639",
    "end": "267540"
  },
  {
    "text": "and the node is completely empty so we",
    "start": "267540",
    "end": "269220"
  },
  {
    "text": "can just place the part there and",
    "start": "269220",
    "end": "272940"
  },
  {
    "text": "this goes on for the next part next part",
    "start": "272940",
    "end": "275639"
  },
  {
    "text": "is probably gonna be scheduled on the",
    "start": "275639",
    "end": "278040"
  },
  {
    "text": "second note then because kubernetes is",
    "start": "278040",
    "end": "280259"
  },
  {
    "text": "going to look are we having enough",
    "start": "280259",
    "end": "282419"
  },
  {
    "text": "resources for this second part on this",
    "start": "282419",
    "end": "285060"
  },
  {
    "text": "node and of course we have this node is",
    "start": "285060",
    "end": "287400"
  },
  {
    "text": "also empty",
    "start": "287400",
    "end": "288900"
  },
  {
    "text": "but for this third part it's looking a",
    "start": "288900",
    "end": "291540"
  },
  {
    "text": "bit different first kubernetes is going",
    "start": "291540",
    "end": "293759"
  },
  {
    "text": "to try to schedule it on the first node",
    "start": "293759",
    "end": "295919"
  },
  {
    "text": "again because Simple Run Robin algorithm",
    "start": "295919",
    "end": "298440"
  },
  {
    "text": "and this part is requesting three DB",
    "start": "298440",
    "end": "302040"
  },
  {
    "text": "bytes and two CPUs we have the CPUs but",
    "start": "302040",
    "end": "304680"
  },
  {
    "text": "we don't have the memory so that doesn't",
    "start": "304680",
    "end": "308100"
  },
  {
    "text": "work but we can place it on the other",
    "start": "308100",
    "end": "310199"
  },
  {
    "text": "node and that actually kind of works",
    "start": "310199",
    "end": "313380"
  },
  {
    "text": "so now we have placed three parts on our",
    "start": "313380",
    "end": "315600"
  },
  {
    "text": "two nodes and we have used all the",
    "start": "315600",
    "end": "317820"
  },
  {
    "text": "memory of the second node and some of",
    "start": "317820",
    "end": "319740"
  },
  {
    "text": "the memory of the first node but if",
    "start": "319740",
    "end": "321900"
  },
  {
    "text": "you've paid attention to the limits",
    "start": "321900",
    "end": "323759"
  },
  {
    "text": "we've chosen here because those limits",
    "start": "323759",
    "end": "325560"
  },
  {
    "text": "don't really matter when it comes to",
    "start": "325560",
    "end": "327240"
  },
  {
    "text": "scheduling",
    "start": "327240",
    "end": "328380"
  },
  {
    "text": "then you",
    "start": "328380",
    "end": "330180"
  },
  {
    "text": "might have seen that these limits are a",
    "start": "330180",
    "end": "332699"
  },
  {
    "text": "bit higher so for the first node we have",
    "start": "332699",
    "end": "334500"
  },
  {
    "text": "a combined limit of five gigabyte memory",
    "start": "334500",
    "end": "337680"
  },
  {
    "text": "and two CPU",
    "start": "337680",
    "end": "340139"
  },
  {
    "text": "um and this is basically all we have so",
    "start": "340139",
    "end": "342539"
  },
  {
    "text": "if these parts start to to use all the",
    "start": "342539",
    "end": "345720"
  },
  {
    "text": "memory that",
    "start": "345720",
    "end": "347160"
  },
  {
    "text": "yeah up to their limit on the the entire",
    "start": "347160",
    "end": "350340"
  },
  {
    "text": "memory is being used which is not a",
    "start": "350340",
    "end": "352259"
  },
  {
    "text": "problem now because there's nothing else",
    "start": "352259",
    "end": "353759"
  },
  {
    "text": "on on this node but we still have space",
    "start": "353759",
    "end": "356220"
  },
  {
    "text": "to schedule something on this node and",
    "start": "356220",
    "end": "358020"
  },
  {
    "text": "it could lead to a problem",
    "start": "358020",
    "end": "359880"
  },
  {
    "text": "and it actually might already lead to a",
    "start": "359880",
    "end": "362820"
  },
  {
    "text": "problem on the second note because the",
    "start": "362820",
    "end": "365400"
  },
  {
    "text": "limits are much higher than our capacity",
    "start": "365400",
    "end": "368160"
  },
  {
    "text": "so once these parts start to go over",
    "start": "368160",
    "end": "372600"
  },
  {
    "text": "their requests",
    "start": "372600",
    "end": "374100"
  },
  {
    "text": "and up to the to the limits then we",
    "start": "374100",
    "end": "377520"
  },
  {
    "text": "might run out of memory",
    "start": "377520",
    "end": "379380"
  },
  {
    "text": "and we have another problem here on the",
    "start": "379380",
    "end": "381240"
  },
  {
    "text": "second note because we have allocated",
    "start": "381240",
    "end": "383280"
  },
  {
    "text": "all our memory but we are just using",
    "start": "383280",
    "end": "386039"
  },
  {
    "text": "three out of the five cores",
    "start": "386039",
    "end": "388319"
  },
  {
    "text": "um when it comes to CPU and these other",
    "start": "388319",
    "end": "391080"
  },
  {
    "text": "two cores are just stranded because",
    "start": "391080",
    "end": "392639"
  },
  {
    "text": "there's nothing that we can schedule you",
    "start": "392639",
    "end": "394199"
  },
  {
    "text": "on this node because it's just",
    "start": "394199",
    "end": "395940"
  },
  {
    "text": "completely packed",
    "start": "395940",
    "end": "398699"
  },
  {
    "text": "okay now we've discussed what happens",
    "start": "398699",
    "end": "400680"
  },
  {
    "text": "when we exceed our limit and",
    "start": "400680",
    "end": "405360"
  },
  {
    "text": "but what this actually happened in this",
    "start": "405360",
    "end": "406979"
  },
  {
    "text": "case when a node runs out of memory",
    "start": "406979",
    "end": "410460"
  },
  {
    "text": "well then",
    "start": "410460",
    "end": "412740"
  },
  {
    "text": "kubernetes also has to terminate some",
    "start": "412740",
    "end": "415259"
  },
  {
    "text": "parts but this time it's not the parts",
    "start": "415259",
    "end": "417300"
  },
  {
    "text": "that exceed their limits because",
    "start": "417300",
    "end": "419400"
  },
  {
    "text": "they're probably none",
    "start": "419400",
    "end": "421100"
  },
  {
    "text": "but this time kubernetes terminates the",
    "start": "421100",
    "end": "423900"
  },
  {
    "text": "parts that exceed their memory requests",
    "start": "423900",
    "end": "426960"
  },
  {
    "text": "so limits don't matter in this case",
    "start": "426960",
    "end": "430080"
  },
  {
    "text": "and this is what brings me to the title",
    "start": "430080",
    "end": "434100"
  },
  {
    "text": "of the talk how to blow up a kubernetes",
    "start": "434100",
    "end": "436080"
  },
  {
    "text": "cluster so",
    "start": "436080",
    "end": "438720"
  },
  {
    "text": "the situation that we had so all the",
    "start": "438720",
    "end": "441000"
  },
  {
    "text": "ingredients to blow up a cluster was we",
    "start": "441000",
    "end": "443400"
  },
  {
    "text": "were running a couple of microservices",
    "start": "443400",
    "end": "444840"
  },
  {
    "text": "these microservices were communicating",
    "start": "444840",
    "end": "447060"
  },
  {
    "text": "using Kafka",
    "start": "447060",
    "end": "449039"
  },
  {
    "text": "um and we had barely enough memory in",
    "start": "449039",
    "end": "450780"
  },
  {
    "text": "our cluster",
    "start": "450780",
    "end": "452099"
  },
  {
    "text": "there was a situation",
    "start": "452099",
    "end": "453780"
  },
  {
    "text": "um some some words about Kafka if you're",
    "start": "453780",
    "end": "455340"
  },
  {
    "text": "not familiar it's for distributed event",
    "start": "455340",
    "end": "457080"
  },
  {
    "text": "streaming",
    "start": "457080",
    "end": "458580"
  },
  {
    "text": "um we used it for asynchronous",
    "start": "458580",
    "end": "459900"
  },
  {
    "text": "communication",
    "start": "459900",
    "end": "461280"
  },
  {
    "text": "but what is important is it uses a ton",
    "start": "461280",
    "end": "463680"
  },
  {
    "text": "of memory and it keeps the entire state",
    "start": "463680",
    "end": "465780"
  },
  {
    "text": "in memory",
    "start": "465780",
    "end": "467580"
  },
  {
    "text": "and our Kafka pots were using about 2.8",
    "start": "467580",
    "end": "471660"
  },
  {
    "text": "GB bytes memory usually but we also saw",
    "start": "471660",
    "end": "475080"
  },
  {
    "text": "some spikes so we went and set the",
    "start": "475080",
    "end": "477240"
  },
  {
    "text": "request for three gigaby bytes and the",
    "start": "477240",
    "end": "479460"
  },
  {
    "text": "limit for 8 GB bytes",
    "start": "479460",
    "end": "482880"
  },
  {
    "text": "now the incident looked kind of like",
    "start": "482880",
    "end": "485340"
  },
  {
    "text": "this we had different nodes with",
    "start": "485340",
    "end": "487919"
  },
  {
    "text": "different Kafka pots running on them",
    "start": "487919",
    "end": "491120"
  },
  {
    "text": "we had three Kafka pods in total running",
    "start": "491120",
    "end": "493860"
  },
  {
    "text": "on three different nodes and on all of",
    "start": "493860",
    "end": "495840"
  },
  {
    "text": "these nodes there were also other",
    "start": "495840",
    "end": "497160"
  },
  {
    "text": "services running",
    "start": "497160",
    "end": "499919"
  },
  {
    "text": "and the utilization was incredibly High",
    "start": "499919",
    "end": "503099"
  },
  {
    "text": "we had a memory utilization of 90",
    "start": "503099",
    "end": "505620"
  },
  {
    "text": "something percent",
    "start": "505620",
    "end": "508280"
  },
  {
    "text": "and of course that went well for a while",
    "start": "508919",
    "end": "511979"
  },
  {
    "text": "but there was a time when we just were",
    "start": "511979",
    "end": "515219"
  },
  {
    "text": "trying to use more memory than we",
    "start": "515219",
    "end": "516959"
  },
  {
    "text": "actually had",
    "start": "516959",
    "end": "518039"
  },
  {
    "text": "and of course then kubernetes needed to",
    "start": "518039",
    "end": "520560"
  },
  {
    "text": "terminate some parts",
    "start": "520560",
    "end": "522240"
  },
  {
    "text": "and the first part that got terminated",
    "start": "522240",
    "end": "523860"
  },
  {
    "text": "what was one of our Kafka parts",
    "start": "523860",
    "end": "527100"
  },
  {
    "text": "so that was gone that's not a problem",
    "start": "527100",
    "end": "530519"
  },
  {
    "text": "um Kafka can handle it we have still two",
    "start": "530519",
    "end": "533040"
  },
  {
    "text": "more parts running so that wasn't really",
    "start": "533040",
    "end": "535740"
  },
  {
    "text": "a problem",
    "start": "535740",
    "end": "536820"
  },
  {
    "text": "but we still are trying to process the",
    "start": "536820",
    "end": "540660"
  },
  {
    "text": "same number of messages that we have",
    "start": "540660",
    "end": "542220"
  },
  {
    "text": "previously processed with three parts",
    "start": "542220",
    "end": "543720"
  },
  {
    "text": "and we just have two parts left now",
    "start": "543720",
    "end": "547620"
  },
  {
    "text": "so that led to an increase of memory",
    "start": "547620",
    "end": "551100"
  },
  {
    "text": "usage for the other Kafka pods and well",
    "start": "551100",
    "end": "555899"
  },
  {
    "text": "we need something to terminate now",
    "start": "555899",
    "end": "558300"
  },
  {
    "text": "and we just have we're left with one",
    "start": "558300",
    "end": "560700"
  },
  {
    "text": "Kafka pot",
    "start": "560700",
    "end": "563240"
  },
  {
    "text": "so",
    "start": "564540",
    "end": "566580"
  },
  {
    "text": "now that these applications that were",
    "start": "566580",
    "end": "569459"
  },
  {
    "text": "using Kafka to communicate",
    "start": "569459",
    "end": "572339"
  },
  {
    "text": "run into more more and more problems",
    "start": "572339",
    "end": "574560"
  },
  {
    "text": "because they they couldn't communicate",
    "start": "574560",
    "end": "576120"
  },
  {
    "text": "anymore they couldn't send out messages",
    "start": "576120",
    "end": "577440"
  },
  {
    "text": "so they had to keep the messages",
    "start": "577440",
    "end": "579240"
  },
  {
    "text": "themselves and they all which also led",
    "start": "579240",
    "end": "581880"
  },
  {
    "text": "to an increased usage of memory",
    "start": "581880",
    "end": "584399"
  },
  {
    "text": "so some of these Services also use more",
    "start": "584399",
    "end": "587880"
  },
  {
    "text": "memory",
    "start": "587880",
    "end": "588779"
  },
  {
    "text": "and of course these Kafka parts weren't",
    "start": "588779",
    "end": "590820"
  },
  {
    "text": "gone for good so when when kubernetes",
    "start": "590820",
    "end": "593399"
  },
  {
    "text": "terminates apart it's going to be",
    "start": "593399",
    "end": "595200"
  },
  {
    "text": "rescheduled right away",
    "start": "595200",
    "end": "596760"
  },
  {
    "text": "but we didn't really have many",
    "start": "596760",
    "end": "599279"
  },
  {
    "text": "opportunities to to reschedule a part",
    "start": "599279",
    "end": "601320"
  },
  {
    "text": "because",
    "start": "601320",
    "end": "602580"
  },
  {
    "text": "um our services that couldn't send the",
    "start": "602580",
    "end": "605399"
  },
  {
    "text": "messages had an increased memory usage",
    "start": "605399",
    "end": "607399"
  },
  {
    "text": "which led to the situation that on node",
    "start": "607399",
    "end": "611459"
  },
  {
    "text": "one we couldn't really place the Kafka",
    "start": "611459",
    "end": "613920"
  },
  {
    "text": "pod anymore because we didn't even have",
    "start": "613920",
    "end": "615839"
  },
  {
    "text": "enough memory to schedule it",
    "start": "615839",
    "end": "618060"
  },
  {
    "text": "so there was one that could be that we",
    "start": "618060",
    "end": "620760"
  },
  {
    "text": "could schedule on node two but that",
    "start": "620760",
    "end": "622680"
  },
  {
    "text": "didn't really help us much because that",
    "start": "622680",
    "end": "625019"
  },
  {
    "text": "was the one that just got terminated on",
    "start": "625019",
    "end": "627839"
  },
  {
    "text": "the same note with the same setup",
    "start": "627839",
    "end": "630540"
  },
  {
    "text": "so that's probably not going to last for",
    "start": "630540",
    "end": "632519"
  },
  {
    "text": "long",
    "start": "632519",
    "end": "633779"
  },
  {
    "text": "and we had another Kafka pod that",
    "start": "633779",
    "end": "637260"
  },
  {
    "text": "was stuck in pending because the Note",
    "start": "637260",
    "end": "639360"
  },
  {
    "text": "One",
    "start": "639360",
    "end": "640260"
  },
  {
    "text": "didn't have any memory left for to",
    "start": "640260",
    "end": "643860"
  },
  {
    "text": "schedule this part",
    "start": "643860",
    "end": "646640"
  },
  {
    "text": "and with these Kafka pods currently",
    "start": "648600",
    "end": "651420"
  },
  {
    "text": "constantly",
    "start": "651420",
    "end": "653040"
  },
  {
    "text": "crashing and being rescheduled and being",
    "start": "653040",
    "end": "655920"
  },
  {
    "text": "terminated",
    "start": "655920",
    "end": "657480"
  },
  {
    "text": "the applications that we're trying to",
    "start": "657480",
    "end": "659519"
  },
  {
    "text": "communicate using Kafka run into more",
    "start": "659519",
    "end": "661680"
  },
  {
    "text": "and more problems not all of these",
    "start": "661680",
    "end": "662940"
  },
  {
    "text": "applications",
    "start": "662940",
    "end": "664079"
  },
  {
    "text": "were very good with failure handling",
    "start": "664079",
    "end": "667380"
  },
  {
    "text": "so some of those just crashed and we had",
    "start": "667380",
    "end": "670260"
  },
  {
    "text": "even more",
    "start": "670260",
    "end": "671459"
  },
  {
    "text": "applications stuck in a crash loop back",
    "start": "671459",
    "end": "674820"
  },
  {
    "text": "off",
    "start": "674820",
    "end": "676680"
  },
  {
    "text": "so yeah I think you get the gist of it",
    "start": "676680",
    "end": "678720"
  },
  {
    "text": "we were in a vicious cycle of Parts",
    "start": "678720",
    "end": "680880"
  },
  {
    "text": "being reschedules Parts being terminated",
    "start": "680880",
    "end": "684720"
  },
  {
    "text": "pots needing more memory than usual",
    "start": "684720",
    "end": "686820"
  },
  {
    "text": "because of our originally High",
    "start": "686820",
    "end": "689220"
  },
  {
    "text": "utilization and we learned some things",
    "start": "689220",
    "end": "692160"
  },
  {
    "text": "so we learned that over committing on",
    "start": "692160",
    "end": "695339"
  },
  {
    "text": "memory isn't always a great idea so it's",
    "start": "695339",
    "end": "698040"
  },
  {
    "text": "probably good and the best in the most",
    "start": "698040",
    "end": "699839"
  },
  {
    "text": "cases to just set the memory request",
    "start": "699839",
    "end": "702120"
  },
  {
    "text": "equal to the memory limit",
    "start": "702120",
    "end": "704579"
  },
  {
    "text": "and we also learned that clusters need",
    "start": "704579",
    "end": "706200"
  },
  {
    "text": "some room to operate utilizing",
    "start": "706200",
    "end": "708540"
  },
  {
    "text": "90-something percent is also not a great",
    "start": "708540",
    "end": "710640"
  },
  {
    "text": "idea simply because memory is an",
    "start": "710640",
    "end": "712680"
  },
  {
    "text": "incompressible resource",
    "start": "712680",
    "end": "715820"
  },
  {
    "text": "now I initially said that I'll be",
    "start": "715980",
    "end": "718019"
  },
  {
    "text": "talking about both memory and CPU and",
    "start": "718019",
    "end": "720959"
  },
  {
    "text": "this was all about memory so far so",
    "start": "720959",
    "end": "723720"
  },
  {
    "text": "let's look about let's look at CPU",
    "start": "723720",
    "end": "726600"
  },
  {
    "text": "CPU other than memory is a compressible",
    "start": "726600",
    "end": "729240"
  },
  {
    "text": "resource we can throttle CPU and thus",
    "start": "729240",
    "end": "732420"
  },
  {
    "text": "memory CPU resource management is",
    "start": "732420",
    "end": "735779"
  },
  {
    "text": "completely different to memory resource",
    "start": "735779",
    "end": "738480"
  },
  {
    "text": "management",
    "start": "738480",
    "end": "740100"
  },
  {
    "text": "and the kubernetes documentation gives",
    "start": "740100",
    "end": "742740"
  },
  {
    "text": "us a fair warning about assetting limits",
    "start": "742740",
    "end": "745440"
  },
  {
    "text": "or rather about not setting limits",
    "start": "745440",
    "end": "747720"
  },
  {
    "text": "because it tells us if we don't set",
    "start": "747720",
    "end": "750360"
  },
  {
    "text": "limits",
    "start": "750360",
    "end": "751560"
  },
  {
    "text": "there might be a container that is using",
    "start": "751560",
    "end": "753660"
  },
  {
    "text": "all the resources that our cluster has",
    "start": "753660",
    "end": "756060"
  },
  {
    "text": "to offer or that the node has to offer",
    "start": "756060",
    "end": "757800"
  },
  {
    "text": "rather",
    "start": "757800",
    "end": "759360"
  },
  {
    "text": "and that is true that can happen",
    "start": "759360",
    "end": "761820"
  },
  {
    "text": "but I came to the conclusion that this",
    "start": "761820",
    "end": "764040"
  },
  {
    "text": "is actually not always a bad thing",
    "start": "764040",
    "end": "766860"
  },
  {
    "text": "because let's let's look at a setup",
    "start": "766860",
    "end": "769800"
  },
  {
    "text": "where we have set a request of 500 Milli",
    "start": "769800",
    "end": "773160"
  },
  {
    "text": "core and a limit of 500 Milli core",
    "start": "773160",
    "end": "775980"
  },
  {
    "text": "so we always get 500 millicore in the",
    "start": "775980",
    "end": "778560"
  },
  {
    "text": "worst case but also in the best case",
    "start": "778560",
    "end": "781200"
  },
  {
    "text": "and with a single threaded application",
    "start": "781200",
    "end": "782940"
  },
  {
    "text": "that can just use one core it looks like",
    "start": "782940",
    "end": "785760"
  },
  {
    "text": "this we we use 500 Milli core of One",
    "start": "785760",
    "end": "788339"
  },
  {
    "text": "Core the other 500 Milli core are either",
    "start": "788339",
    "end": "791339"
  },
  {
    "text": "idling or being used by some other",
    "start": "791339",
    "end": "793500"
  },
  {
    "text": "application",
    "start": "793500",
    "end": "796040"
  },
  {
    "text": "um with a multi-threaded application it",
    "start": "796380",
    "end": "798300"
  },
  {
    "text": "looks",
    "start": "798300",
    "end": "799560"
  },
  {
    "text": "looks a bit differently because we're",
    "start": "799560",
    "end": "801240"
  },
  {
    "text": "using two cores",
    "start": "801240",
    "end": "803220"
  },
  {
    "text": "um we're still getting 500 Milli cores",
    "start": "803220",
    "end": "805139"
  },
  {
    "text": "but now it's not just half of a CPU",
    "start": "805139",
    "end": "808260"
  },
  {
    "text": "that's idling half of the time but it",
    "start": "808260",
    "end": "810899"
  },
  {
    "text": "might be three quarters of the CPU",
    "start": "810899",
    "end": "812760"
  },
  {
    "text": "that's idling three quarters of the time",
    "start": "812760",
    "end": "814740"
  },
  {
    "text": "if there is just no demand for this",
    "start": "814740",
    "end": "818220"
  },
  {
    "text": "extra CPU",
    "start": "818220",
    "end": "820680"
  },
  {
    "text": "now if we remove the limit we're still",
    "start": "820680",
    "end": "823440"
  },
  {
    "text": "left with the same worst case so we are",
    "start": "823440",
    "end": "826139"
  },
  {
    "text": "always guaranteed to get our request of",
    "start": "826139",
    "end": "828180"
  },
  {
    "text": "our 500 millicore so in a single third",
    "start": "828180",
    "end": "830339"
  },
  {
    "text": "case we get half a CPU",
    "start": "830339",
    "end": "833700"
  },
  {
    "text": "but",
    "start": "833700",
    "end": "835320"
  },
  {
    "text": "if there is more CPU available",
    "start": "835320",
    "end": "837899"
  },
  {
    "text": "we might get the entire CPU we can just",
    "start": "837899",
    "end": "840839"
  },
  {
    "text": "claim more resources and in a",
    "start": "840839",
    "end": "843120"
  },
  {
    "text": "multi-threaded application we could",
    "start": "843120",
    "end": "845519"
  },
  {
    "text": "claim all the CPU this is kind of what",
    "start": "845519",
    "end": "848339"
  },
  {
    "text": "the documentation want is about but is",
    "start": "848339",
    "end": "851279"
  },
  {
    "text": "it really a problem",
    "start": "851279",
    "end": "853800"
  },
  {
    "text": "I think it's not if as long as we are",
    "start": "853800",
    "end": "856380"
  },
  {
    "text": "setting requests because requests",
    "start": "856380",
    "end": "858899"
  },
  {
    "text": "actually determine our CPU share",
    "start": "858899",
    "end": "863180"
  },
  {
    "text": "in this example we have a request in",
    "start": "863220",
    "end": "866519"
  },
  {
    "text": "place for the yellow part of 250 Milli",
    "start": "866519",
    "end": "869040"
  },
  {
    "text": "core and a request of 500 Milli core for",
    "start": "869040",
    "end": "872399"
  },
  {
    "text": "the red part",
    "start": "872399",
    "end": "875360"
  },
  {
    "text": "so the red part requests twice as much",
    "start": "875459",
    "end": "879019"
  },
  {
    "text": "CPU time as the yellow part",
    "start": "879019",
    "end": "882480"
  },
  {
    "text": "and in the worst case they're just",
    "start": "882480",
    "end": "884760"
  },
  {
    "text": "getting their request and nothing more",
    "start": "884760",
    "end": "886740"
  },
  {
    "text": "but once they start competing for",
    "start": "886740",
    "end": "889500"
  },
  {
    "text": "resources and they're trying to claim as",
    "start": "889500",
    "end": "891240"
  },
  {
    "text": "much as many resources as they can get",
    "start": "891240",
    "end": "894480"
  },
  {
    "text": "um",
    "start": "894480",
    "end": "895139"
  },
  {
    "text": "the share that they could get will be",
    "start": "895139",
    "end": "897300"
  },
  {
    "text": "determined by the ratio of of the",
    "start": "897300",
    "end": "900720"
  },
  {
    "text": "requests so the red one is asking for",
    "start": "900720",
    "end": "904320"
  },
  {
    "text": "twice as much as the yellow one so it's",
    "start": "904320",
    "end": "906240"
  },
  {
    "text": "also getting twice as much as the other",
    "start": "906240",
    "end": "908399"
  },
  {
    "text": "one when they try to claim all the",
    "start": "908399",
    "end": "910440"
  },
  {
    "text": "resources so in this case two thirds of",
    "start": "910440",
    "end": "913260"
  },
  {
    "text": "a CPU for the red one and one third to",
    "start": "913260",
    "end": "915779"
  },
  {
    "text": "the yellow part",
    "start": "915779",
    "end": "917220"
  },
  {
    "text": "and that's more than they have requested",
    "start": "917220",
    "end": "919260"
  },
  {
    "text": "but there's not really an issue with one",
    "start": "919260",
    "end": "922920"
  },
  {
    "text": "of them being a Noisy Neighbor",
    "start": "922920",
    "end": "925019"
  },
  {
    "text": "and",
    "start": "925019",
    "end": "926339"
  },
  {
    "text": "removing CPU limits can lead to drastic",
    "start": "926339",
    "end": "930000"
  },
  {
    "text": "effects in",
    "start": "930000",
    "end": "932160"
  },
  {
    "text": "in favor of response times so Thomas",
    "start": "932160",
    "end": "936360"
  },
  {
    "text": "here posted this graph on on Twitter",
    "start": "936360",
    "end": "940320"
  },
  {
    "text": "um where they removed limits and they",
    "start": "940320",
    "end": "943560"
  },
  {
    "text": "saw their response times drop from 150",
    "start": "943560",
    "end": "946740"
  },
  {
    "text": "milliseconds to 90 milliseconds this is",
    "start": "946740",
    "end": "949320"
  },
  {
    "text": "75p",
    "start": "949320",
    "end": "951660"
  },
  {
    "text": "so this is a drastic fact",
    "start": "951660",
    "end": "955740"
  },
  {
    "text": "um and now you may say okay but when I",
    "start": "955740",
    "end": "957839"
  },
  {
    "text": "remove my limits I'm not in the best",
    "start": "957839",
    "end": "960360"
  },
  {
    "text": "quality of service anymore so let's look",
    "start": "960360",
    "end": "964320"
  },
  {
    "text": "at the concept of quality of service for",
    "start": "964320",
    "end": "966120"
  },
  {
    "text": "a second",
    "start": "966120",
    "end": "968360"
  },
  {
    "text": "um there are three types of quality of",
    "start": "968940",
    "end": "971040"
  },
  {
    "text": "service",
    "start": "971040",
    "end": "972000"
  },
  {
    "text": "if you don't specify a request and you",
    "start": "972000",
    "end": "974820"
  },
  {
    "text": "don't specify a limit you want this best",
    "start": "974820",
    "end": "976740"
  },
  {
    "text": "effort quality of service class you",
    "start": "976740",
    "end": "978360"
  },
  {
    "text": "don't really want to be there it's not",
    "start": "978360",
    "end": "979620"
  },
  {
    "text": "that great",
    "start": "979620",
    "end": "981300"
  },
  {
    "text": "um",
    "start": "981300",
    "end": "981959"
  },
  {
    "text": "but once you specify a request you get",
    "start": "981959",
    "end": "985079"
  },
  {
    "text": "into the burstable quality of service",
    "start": "985079",
    "end": "987480"
  },
  {
    "text": "class where you're always guaranteed to",
    "start": "987480",
    "end": "990000"
  },
  {
    "text": "get your request",
    "start": "990000",
    "end": "991800"
  },
  {
    "text": "but once you exceed the request",
    "start": "991800",
    "end": "994260"
  },
  {
    "text": "you're subject to being a throttled or",
    "start": "994260",
    "end": "996959"
  },
  {
    "text": "terminated",
    "start": "996959",
    "end": "998880"
  },
  {
    "text": "and there's the guarantees quality of",
    "start": "998880",
    "end": "1000560"
  },
  {
    "text": "service class when your request is equal",
    "start": "1000560",
    "end": "1003259"
  },
  {
    "text": "to your limit you're guaranteed exactly",
    "start": "1003259",
    "end": "1005839"
  },
  {
    "text": "that",
    "start": "1005839",
    "end": "1007040"
  },
  {
    "text": "now guaranteed sounds great I want to be",
    "start": "1007040",
    "end": "1009680"
  },
  {
    "text": "in that third guaranteed quality of",
    "start": "1009680",
    "end": "1011720"
  },
  {
    "text": "service class but actually do I really",
    "start": "1011720",
    "end": "1014360"
  },
  {
    "text": "want to because the request is already",
    "start": "1014360",
    "end": "1016579"
  },
  {
    "text": "guaranteed in the burstable class and if",
    "start": "1016579",
    "end": "1019519"
  },
  {
    "text": "we take the same request for both cases",
    "start": "1019519",
    "end": "1023540"
  },
  {
    "text": "it's kind of I mean I'm getting more in",
    "start": "1023540",
    "end": "1026480"
  },
  {
    "text": "the boostable class I",
    "start": "1026480",
    "end": "1028819"
  },
  {
    "text": "sounds better",
    "start": "1028819",
    "end": "1030740"
  },
  {
    "text": "so in the case of CPU I would advise not",
    "start": "1030740",
    "end": "1033020"
  },
  {
    "text": "to set CPU limits but always set",
    "start": "1033020",
    "end": "1035120"
  },
  {
    "text": "requests so you're not suffering from",
    "start": "1035120",
    "end": "1036918"
  },
  {
    "text": "noisy neighbors",
    "start": "1036919",
    "end": "1039380"
  },
  {
    "text": "there are two more pitfalls that I want",
    "start": "1039380",
    "end": "1042380"
  },
  {
    "text": "to address",
    "start": "1042380",
    "end": "1043319"
  },
  {
    "text": "[Music]",
    "start": "1043319",
    "end": "1043819"
  },
  {
    "text": "um",
    "start": "1043819",
    "end": "1045020"
  },
  {
    "text": "first is you should be aware of your",
    "start": "1045020",
    "end": "1047178"
  },
  {
    "text": "resources if you're planning to",
    "start": "1047179",
    "end": "1051020"
  },
  {
    "text": "um fairly distribute all of your",
    "start": "1051020",
    "end": "1052700"
  },
  {
    "text": "resources among your apps um and you may",
    "start": "1052700",
    "end": "1055940"
  },
  {
    "text": "have ordered a 10 gigabytes node with",
    "start": "1055940",
    "end": "1059780"
  },
  {
    "text": "four CPU cores",
    "start": "1059780",
    "end": "1061760"
  },
  {
    "text": "you might not get all all of what you've",
    "start": "1061760",
    "end": "1064220"
  },
  {
    "text": "ordered because the system demons use a",
    "start": "1064220",
    "end": "1067460"
  },
  {
    "text": "portion of the available resources so",
    "start": "1067460",
    "end": "1069380"
  },
  {
    "text": "you should check the node the status",
    "start": "1069380",
    "end": "1071960"
  },
  {
    "text": "allocatable field to figure out what",
    "start": "1071960",
    "end": "1074780"
  },
  {
    "text": "resources you actually have available",
    "start": "1074780",
    "end": "1078020"
  },
  {
    "text": "the second thing I want you to watch out",
    "start": "1078020",
    "end": "1080900"
  },
  {
    "text": "for are namespace limits so even if you",
    "start": "1080900",
    "end": "1084620"
  },
  {
    "text": "do not set a limit there might be a",
    "start": "1084620",
    "end": "1087020"
  },
  {
    "text": "namespace limit as a default so in this",
    "start": "1087020",
    "end": "1089960"
  },
  {
    "text": "case there is a default namespace limit",
    "start": "1089960",
    "end": "1092360"
  },
  {
    "text": "of 500 Milli core and we have a request",
    "start": "1092360",
    "end": "1095840"
  },
  {
    "text": "in place for this part of a 700",
    "start": "1095840",
    "end": "1097940"
  },
  {
    "text": "millicore Which is higher than our limit",
    "start": "1097940",
    "end": "1099620"
  },
  {
    "text": "that doesn't make sense and this part is",
    "start": "1099620",
    "end": "1102620"
  },
  {
    "text": "never going to be scheduled but if",
    "start": "1102620",
    "end": "1105020"
  },
  {
    "text": "you're not aware of this namespace limit",
    "start": "1105020",
    "end": "1106580"
  },
  {
    "text": "we might not realize why aren't why",
    "start": "1106580",
    "end": "1109820"
  },
  {
    "text": "aren't we being scheduled",
    "start": "1109820",
    "end": "1112779"
  },
  {
    "text": "so that's something you should be aware",
    "start": "1112820",
    "end": "1114679"
  },
  {
    "text": "of and watch out for",
    "start": "1114679",
    "end": "1117980"
  },
  {
    "text": "so in summary clusters need room to",
    "start": "1117980",
    "end": "1120320"
  },
  {
    "text": "operate",
    "start": "1120320",
    "end": "1121840"
  },
  {
    "text": "utilizing 90 something 80 something",
    "start": "1121840",
    "end": "1124580"
  },
  {
    "text": "percent is you usually not a great idea",
    "start": "1124580",
    "end": "1126799"
  },
  {
    "text": "for the most part I would advise to set",
    "start": "1126799",
    "end": "1129620"
  },
  {
    "text": "the memory request equal to the memory",
    "start": "1129620",
    "end": "1131660"
  },
  {
    "text": "limit",
    "start": "1131660",
    "end": "1132980"
  },
  {
    "text": "and",
    "start": "1132980",
    "end": "1134179"
  },
  {
    "text": "I would always think about not setting",
    "start": "1134179",
    "end": "1137240"
  },
  {
    "text": "CPU limits of course there are some",
    "start": "1137240",
    "end": "1139460"
  },
  {
    "text": "exceptions there are always exceptions",
    "start": "1139460",
    "end": "1141559"
  },
  {
    "text": "to those rules",
    "start": "1141559",
    "end": "1144200"
  },
  {
    "text": "um",
    "start": "1144200",
    "end": "1144860"
  },
  {
    "text": "here are two I thought of there are",
    "start": "1144860",
    "end": "1146900"
  },
  {
    "text": "probably more",
    "start": "1146900",
    "end": "1148640"
  },
  {
    "text": "um setting CPU limits is a great thing",
    "start": "1148640",
    "end": "1151460"
  },
  {
    "text": "if you prefer consistent workloads over",
    "start": "1151460",
    "end": "1153559"
  },
  {
    "text": "performant workloads so you just want to",
    "start": "1153559",
    "end": "1155600"
  },
  {
    "text": "have that guaranteed quality of service",
    "start": "1155600",
    "end": "1158419"
  },
  {
    "text": "class always have the same quality of",
    "start": "1158419",
    "end": "1161000"
  },
  {
    "text": "servers",
    "start": "1161000",
    "end": "1162400"
  },
  {
    "text": "but nothing more",
    "start": "1162400",
    "end": "1164780"
  },
  {
    "text": "then of course go and set limits and you",
    "start": "1164780",
    "end": "1168140"
  },
  {
    "text": "may want to over commit a memory um when",
    "start": "1168140",
    "end": "1170780"
  },
  {
    "text": "you don't care about termination of your",
    "start": "1170780",
    "end": "1172760"
  },
  {
    "text": "parts if you have workloads that can be",
    "start": "1172760",
    "end": "1174740"
  },
  {
    "text": "picked up anytime that can be",
    "start": "1174740",
    "end": "1176059"
  },
  {
    "text": "interrupted anytime sure don't don't",
    "start": "1176059",
    "end": "1178520"
  },
  {
    "text": "over commit don't",
    "start": "1178520",
    "end": "1181539"
  },
  {
    "text": "um do over commit then in this case",
    "start": "1181580",
    "end": "1183380"
  },
  {
    "text": "because obviously that's cheaper than",
    "start": "1183380",
    "end": "1185660"
  },
  {
    "text": "blocking all the memory",
    "start": "1185660",
    "end": "1188919"
  },
  {
    "text": "so I initially said that I'm just an",
    "start": "1189200",
    "end": "1191600"
  },
  {
    "text": "application developer and I get it you",
    "start": "1191600",
    "end": "1194000"
  },
  {
    "text": "may not want to take this advice from me",
    "start": "1194000",
    "end": "1195679"
  },
  {
    "text": "because it sounds a bit weird it's not",
    "start": "1195679",
    "end": "1197600"
  },
  {
    "text": "really what what's in the documentation",
    "start": "1197600",
    "end": "1200179"
  },
  {
    "text": "um but the good thing is you don't have",
    "start": "1200179",
    "end": "1201740"
  },
  {
    "text": "to here's someone you can trust more who",
    "start": "1201740",
    "end": "1204380"
  },
  {
    "text": "is giving out the same advice so Tim um",
    "start": "1204380",
    "end": "1206900"
  },
  {
    "text": "the creator of kubernet is one of the",
    "start": "1206900",
    "end": "1209179"
  },
  {
    "text": "creators",
    "start": "1209179",
    "end": "1210620"
  },
  {
    "text": "um posted this actually as a response of",
    "start": "1210620",
    "end": "1212780"
  },
  {
    "text": "the Tweet we saw earlier Thomas tweet",
    "start": "1212780",
    "end": "1215000"
  },
  {
    "text": "where",
    "start": "1215000",
    "end": "1216020"
  },
  {
    "text": "um we saw the latency drop the response",
    "start": "1216020",
    "end": "1218900"
  },
  {
    "text": "times dropped from 150 milliseconds down",
    "start": "1218900",
    "end": "1221299"
  },
  {
    "text": "to 90 milliseconds",
    "start": "1221299",
    "end": "1224840"
  },
  {
    "text": "that's it thank you",
    "start": "1224840",
    "end": "1228100"
  }
]