[
  {
    "text": "thank you very much for attending our session today we are going to talk about strategies for efficient LM deployments",
    "start": "359",
    "end": "6640"
  },
  {
    "text": "in any cluster my name is sel and I'm part of the AI and advanc services team",
    "start": "6640",
    "end": "11840"
  },
  {
    "text": "in the in bracom and today I'm here with Francisco hey my name is Francisco Caba I'm a senior technical program manager",
    "start": "11840",
    "end": "18480"
  },
  {
    "text": "at the Asher agent platform team so just to set a little bit",
    "start": "18480",
    "end": "24320"
  },
  {
    "text": "expectations for this talk the goal is that we will learn different strategies to run operate and improve models while",
    "start": "24320",
    "end": "32680"
  },
  {
    "text": "working on their on um specific use cases setting also the appropriate infrastructure so we can run them in",
    "start": "32680",
    "end": "39399"
  },
  {
    "text": "different environments not only in a big cluster and a huge Cloud but also in a small deployments like on premise inside",
    "start": "39399",
    "end": "46160"
  },
  {
    "text": "stores on anywhere in which you want to run them for that we will talk about local catching oci for models GP usage",
    "start": "46160",
    "end": "54039"
  },
  {
    "text": "and you may be familiar with this terms after like being here at cuom for for one day also so we will focus on a small",
    "start": "54039",
    "end": "61160"
  },
  {
    "text": "and medium kubernetes deployments nothing about like AI service provider or Mega cluster for distributed training",
    "start": "61160",
    "end": "68240"
  },
  {
    "text": "but more something that you can deploy in your own premise clusters but before starting with the",
    "start": "68240",
    "end": "74280"
  },
  {
    "text": "strategies and discussing a little bit more about that the first thing you may ask yourself if is why you would like",
    "start": "74280",
    "end": "80600"
  },
  {
    "text": "why you consider to deploy L lamps that are already a huge offering outside with",
    "start": "80600",
    "end": "86079"
  },
  {
    "text": "a lot of really good models that you can just access them via API so you you don't need to host install and manage",
    "start": "86079",
    "end": "92399"
  },
  {
    "text": "those ones but there are 13 good reasons to to deploy them and manage the first",
    "start": "92399",
    "end": "97680"
  },
  {
    "text": "thing is that you have more control and flexibility if you start using one of those Services most likely you will have",
    "start": "97680",
    "end": "103880"
  },
  {
    "text": "access to the models that the propietary models that they offer but you won't have the ability to experiment with new",
    "start": "103880",
    "end": "110040"
  },
  {
    "text": "models go as small try new things and that may impact also the cost because",
    "start": "110040",
    "end": "115439"
  },
  {
    "text": "now you are tied to the cost of the service that you that you choose",
    "start": "115439",
    "end": "121079"
  },
  {
    "text": "also because any company can actually access to those services and you can access get um and use those models but",
    "start": "121079",
    "end": "128720"
  },
  {
    "text": "the real difference between any of the companies that we are here is the data that you have but maybe certain",
    "start": "128720",
    "end": "136440"
  },
  {
    "text": "regulations like if you work on a government or by other reason because you don't want to move your data to the",
    "start": "136440",
    "end": "142760"
  },
  {
    "text": "services you want to keep it private so deploying a LMS locally may give you access to that data that otherwise you",
    "start": "142760",
    "end": "149599"
  },
  {
    "text": "can not use and based on that you can also find in the models there is a huge",
    "start": "149599",
    "end": "155239"
  },
  {
    "text": "offering on the open source site for machine learning models and you can take advantage of your data combine it with",
    "start": "155239",
    "end": "161000"
  },
  {
    "text": "this models and create new models that fits for your use case and perform better for for your",
    "start": "161000",
    "end": "167800"
  },
  {
    "text": "users and when talking about efficiency what we mean is that first you provision",
    "start": "167800",
    "end": "174159"
  },
  {
    "text": "the minimal resources that you need to complete a task without compromising the stability of the system of course you",
    "start": "174159",
    "end": "179640"
  },
  {
    "text": "don't going like to go to the minimum and then just one Spike breaks the entire system um based on the resources",
    "start": "179640",
    "end": "186519"
  },
  {
    "text": "that you allocated you want uh not to have e uh Hardware you want to use them",
    "start": "186519",
    "end": "192440"
  },
  {
    "text": "properly and not just wasting energy and time like running them but now let's put some numbers",
    "start": "192440",
    "end": "199400"
  },
  {
    "text": "about energy consumption which is one of the measures that you may heard about for green energy and green",
    "start": "199400",
    "end": "206200"
  },
  {
    "text": "code for that I'm going to take as an example the bloom um large language",
    "start": "206200",
    "end": "211799"
  },
  {
    "text": "model Bloom is a research project that in the end end up with a 160 uh sorry 76",
    "start": "211799",
    "end": "219560"
  },
  {
    "text": "billion parameter model that was train on 1.6 terab of data so the the",
    "start": "219560",
    "end": "225200"
  },
  {
    "text": "interesting point of this whoops okay it works the interesting thing of this of this model is that the research team was",
    "start": "225200",
    "end": "232280"
  },
  {
    "text": "collecting all the energy data well will while they were developing and training this this model um",
    "start": "232280",
    "end": "241720"
  },
  {
    "text": "we okay maybe I think we can connect it",
    "start": "242360",
    "end": "247360"
  },
  {
    "text": "directly it's on the other",
    "start": "247879",
    "end": "251519"
  },
  {
    "text": "side yes one",
    "start": "254319",
    "end": "258199"
  },
  {
    "text": "second should work okay here we go so while they were trying this huge model",
    "start": "260720",
    "end": "267720"
  },
  {
    "text": "they started to collecting all the metrics that they so just to give you some numbers they they took to train",
    "start": "267720",
    "end": "273680"
  },
  {
    "text": "this specific model it took like 120 days to try and the total energy that they spent was about",
    "start": "273680",
    "end": "280960"
  },
  {
    "text": "400,000 kilowatt per hour so to give you a little bit a comparison with some data",
    "start": "280960",
    "end": "287639"
  },
  {
    "text": "that may sounds more familiar to you and now that we are in Paris this means the same energy as the average of 61 French",
    "start": "287639",
    "end": "295320"
  },
  {
    "text": "homes or French houses in a year which to be honest at least for me this is a",
    "start": "295320",
    "end": "300360"
  },
  {
    "text": "personal perception is not that big because we are millions of people here in France and this is onetime aort so",
    "start": "300360",
    "end": "307320"
  },
  {
    "text": "you spend the time like training this model and now you have this model ready to be used at inference so you don't",
    "start": "307320",
    "end": "313080"
  },
  {
    "text": "need to ret Trin it over and over so that said the important thing",
    "start": "313080",
    "end": "319240"
  },
  {
    "text": "for all of us because I think like most of the people that are here are going to use Lear language model but not to train",
    "start": "319240",
    "end": "325520"
  },
  {
    "text": "them is that we optimize for Energy Efficiency at inference so the system",
    "start": "325520",
    "end": "330759"
  },
  {
    "text": "that we build to provide value to our customers requires less resources and less energy and in other words it cost",
    "start": "330759",
    "end": "338520"
  },
  {
    "text": "less money for us so now let's talk a little bit about the strategies that we can follow to to",
    "start": "338520",
    "end": "344600"
  },
  {
    "text": "get efficient LM deployments the first thing is about model selection for sure if we go like to the",
    "start": "344600",
    "end": "351720"
  },
  {
    "text": "biggest model that we can find out there like gb4 or Mixr there are huge models",
    "start": "351720",
    "end": "357400"
  },
  {
    "text": "that provide really really good results and accuracy when you are like asking many different kind of things and the",
    "start": "357400",
    "end": "363680"
  },
  {
    "text": "reason is that they are big and they were trained with a lot of data but on the other side the bigger the model is",
    "start": "363680",
    "end": "370880"
  },
  {
    "text": "the most energy it consumes to run it so based on this great paper they started",
    "start": "370880",
    "end": "376120"
  },
  {
    "text": "to test the inference energy that they were consuming based on the Lama family so they were comparing 7 billion 13 and",
    "start": "376120",
    "end": "383759"
  },
  {
    "text": "65 billion and as you can see the biggest is the model it can even triple",
    "start": "383759",
    "end": "388880"
  },
  {
    "text": "the energy that unit so for many tasks you can go to the biggest model and they will perform for sure pretty well but",
    "start": "388880",
    "end": "396440"
  },
  {
    "text": "you can also go into the opposite direction like let's try to find the smallest model that can perform well for",
    "start": "396440",
    "end": "402720"
  },
  {
    "text": "your use case and now that way it's more efficient than actually trying to get",
    "start": "402720",
    "end": "408120"
  },
  {
    "text": "the biggest model for everything and you can combine them so you can use a small model for certain things and big models",
    "start": "408120",
    "end": "414400"
  },
  {
    "text": "for others tasks the other thing is about quantization which is a pre popular",
    "start": "414400",
    "end": "420080"
  },
  {
    "text": "technique for anyone that wants to try a mature learning model but you don't have actually like access to a pole full",
    "start": "420080",
    "end": "427720"
  },
  {
    "text": "Nvidia GPU so quantization is a technique that reduce the model size by reducing the Precision so basically you",
    "start": "427720",
    "end": "434800"
  },
  {
    "text": "have like you can see in the table on the right there are a comparison in the Lama family that shows the perplexity",
    "start": "434800",
    "end": "442400"
  },
  {
    "text": "which is one of the scores that you can use to measure the accuracy of the of a model it's not perfect but it's good",
    "start": "442400",
    "end": "448400"
  },
  {
    "text": "enough um here you see like the comparison between full Precision with 16 bits and then",
    "start": "448400",
    "end": "454919"
  },
  {
    "text": "quantization models that we're reducing that Precision to four bits 4.1 5 bits",
    "start": "454919",
    "end": "460520"
  },
  {
    "text": "Etc so if we compare it and we look at this number the perplexity for the uh 7",
    "start": "460520",
    "end": "467800"
  },
  {
    "text": "billion model at full Precision is 5.9 and just in case the for this specific",
    "start": "467800",
    "end": "474159"
  },
  {
    "text": "score lower is better and if you compare it with the perplexity of the 13 billion",
    "start": "474159",
    "end": "480919"
  },
  {
    "text": "parameter but now quantized to four uh 4.1 bits those always beat the full",
    "start": "480919",
    "end": "487000"
  },
  {
    "text": "Precision of the 7 billion model and if we compare now the sizes you can see they are pretty small they are almost",
    "start": "487000",
    "end": "493000"
  },
  {
    "text": "half of the size of those models and meaning that for running the quantin",
    "start": "493000",
    "end": "498039"
  },
  {
    "text": "model you need less memory and they are faster than others and this is a pretty pretty hot ecosystem Microsoft I think",
    "start": "498039",
    "end": "505639"
  },
  {
    "text": "like two or three weeks ago you announc like a new pap about um a quantization of 1.5 bits",
    "start": "505639",
    "end": "513800"
  },
  {
    "text": "which is kind almost anything um the good thing about this is that it it uh Rises pretty promising um Benchmark like",
    "start": "513800",
    "end": "522159"
  },
  {
    "text": "being 2.7 times faster than others while keeping more or less the accuracy using",
    "start": "522159",
    "end": "528399"
  },
  {
    "text": "three point less uh 3.5 less GPU memory which is for me really really",
    "start": "528399",
    "end": "535120"
  },
  {
    "text": "amazing the other thing that we can consider is that until now we have been talking about large language models but",
    "start": "535120",
    "end": "541480"
  },
  {
    "text": "there is also a new category which is called small uh language models which by the way the chronic is is wrong so this",
    "start": "541480",
    "end": "549399"
  },
  {
    "text": "this is from a posing in LinkedIn from Clen the CE of hugging phase and he was mentioning like not all in link but in",
    "start": "549399",
    "end": "556440"
  },
  {
    "text": "many places that small cheaper faster and customized models could be the",
    "start": "556440",
    "end": "561959"
  },
  {
    "text": "future because it will cover many many use cases for different for different companies um we have since last year I",
    "start": "561959",
    "end": "570120"
  },
  {
    "text": "think we have a pretty good uh number of a small language model available for the for for use so just to show you an",
    "start": "570120",
    "end": "577760"
  },
  {
    "text": "example we have f 2 from Microsoft Gemma toillion from Google and we have W 1.5",
    "start": "577760",
    "end": "583959"
  },
  {
    "text": "from Alibaba and when actually came in different sizes like four 0.4 I think is",
    "start": "583959",
    "end": "589440"
  },
  {
    "text": "the smallest one and then the biggest one is around like like Lama oops like the Lama big models and the cool thing",
    "start": "589440",
    "end": "596720"
  },
  {
    "text": "about this models is that customizing or in other words when you are doing it for machine learning fine tuning those",
    "start": "596720",
    "end": "602800"
  },
  {
    "text": "models is pretty pretty cheap because thanks to the size of this models you don't need huge uh graphic cards and",
    "start": "602800",
    "end": "610160"
  },
  {
    "text": "Powerful ones to do the fine tuning so you can take them as a base model not meaning that it's generic good enough",
    "start": "610160",
    "end": "617000"
  },
  {
    "text": "when they are generic for most of the task for that you need to use to use like bigger models but you can take them",
    "start": "617000",
    "end": "622680"
  },
  {
    "text": "and then you can take your private data retraining them and adding those small layers using different techniques like",
    "start": "622680",
    "end": "629920"
  },
  {
    "text": "for example lurer layers and the cool thing and we will talk a little bit more about lurer layers later is that lower",
    "start": "629920",
    "end": "636240"
  },
  {
    "text": "layers can be independent from the base model so they are really good for catching because you have the base model",
    "start": "636240",
    "end": "641320"
  },
  {
    "text": "which is fi gilion or quem and then you have a set of Laural layers that can be fine-tuned for different uh different",
    "start": "641320",
    "end": "648160"
  },
  {
    "text": "use cases inside your company so it's it's a pretty pretty really good um",
    "start": "648160",
    "end": "653560"
  },
  {
    "text": "approach and the other thing that I wanted to mention here is that there are also advances on",
    "start": "653560",
    "end": "659880"
  },
  {
    "text": "moving the war loads that we currently run in cluster or B on in the cloud to",
    "start": "659880",
    "end": "665519"
  },
  {
    "text": "actually the user devices and this is not something new we have seen like Google for example using mature learning",
    "start": "665519",
    "end": "671800"
  },
  {
    "text": "in Android devices like for long time but the good thing and with why I wanted to mention web assembly here is that it",
    "start": "671800",
    "end": "678399"
  },
  {
    "text": "simplifies a lot the way in which you can do that now you can have mature learning engines that can run even in",
    "start": "678399",
    "end": "684519"
  },
  {
    "text": "the browser in your mobile phone and then you can use the same exact models that you are running in your cluster but",
    "start": "684519",
    "end": "690240"
  },
  {
    "text": "now moving them into the user device and even fine-tuning them for very specific",
    "start": "690240",
    "end": "695680"
  },
  {
    "text": "user because they are small enough so they are easy to train and easy to distribute to your to your",
    "start": "695680",
    "end": "702839"
  },
  {
    "text": "users and that said we're moving about how to make it everything on kubernetes",
    "start": "702839",
    "end": "708360"
  },
  {
    "text": "well thank you I think that was a great introduction to the different type of models right and how to use them uh but",
    "start": "708360",
    "end": "714000"
  },
  {
    "text": "now let's talk a bit about you know how we bring those models to kubernetes and",
    "start": "714000",
    "end": "719639"
  },
  {
    "text": "you know just when you have the model generally the big issue is how you you actually host the model in these",
    "start": "719639",
    "end": "724800"
  },
  {
    "text": "production deployments um you know like you need to get the GPU you need to get your resources right and they cost money",
    "start": "724800",
    "end": "730560"
  },
  {
    "text": "so you need to actually you know find a way to actually bring the models right to this kubernets cluster in a secure",
    "start": "730560",
    "end": "736240"
  },
  {
    "text": "and efficient way um so the first thing you need to do is actually make sure you define your architecture probably you'll",
    "start": "736240",
    "end": "741800"
  },
  {
    "text": "go through you know more like a microservices approach you want to make sure that you know your GPU is being",
    "start": "741800",
    "end": "747079"
  },
  {
    "text": "used when you actually need it right if you moving to the cloud you want to actually make sure that you're using",
    "start": "747079",
    "end": "752360"
  },
  {
    "text": "more like you know serverless right so you're paying what you're using um at the same time we we all use these models",
    "start": "752360",
    "end": "758720"
  },
  {
    "text": "right and generally the way we use it is we just go we ask a question get we get a response we ask another question we're",
    "start": "758720",
    "end": "765800"
  },
  {
    "text": "done with our probably with the code that we don't know uh and that's it so we want to make sure that we scale up we",
    "start": "765800",
    "end": "772160"
  },
  {
    "text": "scale down when we're done right so we we want to make sure that we're actually using that scalability in a kind of a",
    "start": "772160",
    "end": "778320"
  },
  {
    "text": "good way um and the third part is that probably a lot of these models maybe they you being part of critical",
    "start": "778320",
    "end": "784240"
  },
  {
    "text": "applications and we want to make sure that we have a unified deployment between cloud and Edge and we want to move right if needed right from Edge to",
    "start": "784240",
    "end": "791279"
  },
  {
    "text": "cloud or from Cloud to Edge whenever needed so we're going to go over um you kind of a really simple architecture so",
    "start": "791279",
    "end": "798839"
  },
  {
    "text": "U you can have know your Ingress or Gateway you'll have your your outer scaler your proxy that's you again",
    "start": "798839",
    "end": "804320"
  },
  {
    "text": "depending on your architecture your AI service um and then you know the interesting part is around you know the the inferencing PO with the model loader",
    "start": "804320",
    "end": "811639"
  },
  {
    "text": "uh the GPU operator and of course the you know the llm storage so we're going to be talking all the blue kind of pieces here um so let's go ahead and",
    "start": "811639",
    "end": "818920"
  },
  {
    "text": "start from right uh all the way to left so let's start with the models uh in the",
    "start": "818920",
    "end": "824480"
  },
  {
    "text": "end models as you know like anel was saying they're just big b binary you",
    "start": "824480",
    "end": "829800"
  },
  {
    "text": "know files um and because they're just so big right the closer you get them to your kubernetes cluster the better um",
    "start": "829800",
    "end": "836160"
  },
  {
    "text": "you can actually see for example here in examples like well when you using small SML models you're in the 3.7 5 gig kind",
    "start": "836160",
    "end": "843399"
  },
  {
    "text": "of size it's big but still manageable right we we can still manage that uh but",
    "start": "843399",
    "end": "849079"
  },
  {
    "text": "as you grow more right into more complex model you know 7 billion it just goes into 13 14 right and it's going to",
    "start": "849079",
    "end": "855959"
  },
  {
    "text": "really big ones right the Lama 70 billion were already 140 gigs so just pretty pretty complicated and to that",
    "start": "855959",
    "end": "861240"
  },
  {
    "text": "you of course you need to add you know your container your application Cuda drivers you know so it just gets really",
    "start": "861240",
    "end": "866480"
  },
  {
    "text": "really big um so you need to actually think of how you're going to be Distributing this in a secure and",
    "start": "866480",
    "end": "873120"
  },
  {
    "text": "efficient way and the different methods right you could use like blob storage you could use you know short side you",
    "start": "873120",
    "end": "878880"
  },
  {
    "text": "know URLs um in particular what you've been kind of doing a bit of research is actually how you leverage the",
    "start": "878880",
    "end": "885079"
  },
  {
    "text": "infrastructure that you have and there were great talks today about like how you can actually leverage you know the oci infrastructure um so basically what",
    "start": "885079",
    "end": "891560"
  },
  {
    "text": "you can do is you can just use the same kind of container Registries that you're using and actually put all your weights",
    "start": "891560",
    "end": "897079"
  },
  {
    "text": "there um and by doing that you're getting a lot of benefits right you already solve right for example Al and",
    "start": "897079",
    "end": "902680"
  },
  {
    "text": "arback for all your other kind of containers so you just use the same that you're using there um you will get other",
    "start": "902680",
    "end": "908639"
  },
  {
    "text": "other benefits like you know you have the layering you have the hashing for each of the different chunks that you'll be doing um you have the retral",
    "start": "908639",
    "end": "914600"
  },
  {
    "text": "mechanism which is also you know really important when you kind of on these Network constraint devices um and of",
    "start": "914600",
    "end": "921000"
  },
  {
    "text": "course then because of chunking right you'll just you know you'll have you know better performance when you're kind of packing and unpacking these kind of",
    "start": "921000",
    "end": "927240"
  },
  {
    "text": "uh big files so how does it works first of all choose your model format the",
    "start": "927240",
    "end": "933399"
  },
  {
    "text": "different model kind of formats you have you know like the binary you have pytorch you have hdf5 so recently",
    "start": "933399",
    "end": "940360"
  },
  {
    "text": "hagging phase they open source a new kind of format it's called safe tens Source it has a bunch of kind of",
    "start": "940360",
    "end": "945800"
  },
  {
    "text": "advantages over you know the previous formats it's it's open source it's quite new so probably there's some models that",
    "start": "945800",
    "end": "951759"
  },
  {
    "text": "you won't find you know this a specific format but it's it's quite kind of easy to you know move from let's say part P",
    "start": "951759",
    "end": "958279"
  },
  {
    "text": "Tor to safet tensor um as soon as you have that model what you would do is you need to kind of Define what's going to",
    "start": "958279",
    "end": "964199"
  },
  {
    "text": "be your optimal Division and divide into smaller chunks so again these safe tensor files are big so you need to just",
    "start": "964199",
    "end": "970600"
  },
  {
    "text": "put it into small chunks so that you know depending on your network right maybe you want to do 500 MB you want to",
    "start": "970600",
    "end": "976079"
  },
  {
    "text": "do 200 again depends on your scenario um and as soon as you have that then you",
    "start": "976079",
    "end": "981240"
  },
  {
    "text": "just go ahead and upload this kind of uh chunks into uh your container registry",
    "start": "981240",
    "end": "986880"
  },
  {
    "text": "in particular here like we're using auras so or AAS is an open source cncf tool that allows you to use you know oci",
    "start": "986880",
    "end": "994120"
  },
  {
    "text": "kind of reg um container Registries and upload all these model weights as oci",
    "start": "994120",
    "end": "999839"
  },
  {
    "text": "artifacts so it's pretty simple you just use the kind of the same approach as just doing with containers where you",
    "start": "999839",
    "end": "1005800"
  },
  {
    "text": "just do you know like instead of like Docker push just or push right you just put what's your specific kind of uh",
    "start": "1005800",
    "end": "1011360"
  },
  {
    "text": "format you could just create a format let's say do safe tens source and you just push it so here's just an example I got a",
    "start": "1011360",
    "end": "1019519"
  },
  {
    "text": "model which was like I think it was 5gb I just like chunk it right into 500 MB pieces then I just go ahead I upload and",
    "start": "1019519",
    "end": "1026798"
  },
  {
    "text": "I can see there as part of you know Asher container registry um almost all Container Registries right now on the",
    "start": "1026799",
    "end": "1032959"
  },
  {
    "text": "cloud they already support this kind of O artifact so you should be fine um and as you can see there you can see that",
    "start": "1032959",
    "end": "1038400"
  },
  {
    "text": "you know the Manifest Json and it has all the different layers of the different kind of chunks of you know something like 500",
    "start": "1038400",
    "end": "1046039"
  },
  {
    "text": "MBS so once you have that right you have your model there and your kind of registry what you need to do is actually",
    "start": "1046640",
    "end": "1053080"
  },
  {
    "text": "download it and because we chunk it we need to recreate that but again it's it's a real simple task you can use oras",
    "start": "1053080",
    "end": "1058919"
  },
  {
    "text": "kind of SDK you have is for your python for go rust and what you do end up doing is just every time you download a chunk",
    "start": "1058919",
    "end": "1065880"
  },
  {
    "text": "you just copy it right to you know the kind of final model and as soon as you finish you just like delete you know the",
    "start": "1065880",
    "end": "1071200"
  },
  {
    "text": "kind of different chunks um and then once you have that I just put here you a sample code of how easy is to actually",
    "start": "1071200",
    "end": "1078120"
  },
  {
    "text": "load the base model and then you can also load the custom kind of lural layers so for example in our demo what",
    "start": "1078120",
    "end": "1083559"
  },
  {
    "text": "we're going to be doing is we're going to load a base uh F tune uh F tune model",
    "start": "1083559",
    "end": "1089000"
  },
  {
    "text": "and then we're going to put the fine tune lower layers the also the good part of this is that you can actually have",
    "start": "1089000",
    "end": "1095000"
  },
  {
    "text": "multiple Lura layers so let's say you have a specific lurer layer for your marketing kind of team well that's one",
    "start": "1095000",
    "end": "1101120"
  },
  {
    "text": "layer you have another layer for let's say HR team um and you're going to have the same base model which is going to be",
    "start": "1101120",
    "end": "1107760"
  },
  {
    "text": "52 the other kind of good practice that you",
    "start": "1107760",
    "end": "1112960"
  },
  {
    "text": "can do is actually have local caching or your PE your peer-to-peer alternative so yeah there's some really you know good",
    "start": "1112960",
    "end": "1119159"
  },
  {
    "text": "projects out there like Spiegel which in the end what they're doing is this local mirroring of these kind of a artifacts",
    "start": "1119159",
    "end": "1126280"
  },
  {
    "text": "uh so basically what it does is every time somebody looks for a container or any kind of layer let's call any oci",
    "start": "1126280",
    "end": "1132440"
  },
  {
    "text": "artifact it will actually talk in inside the cluster and see if any other node has already that layer if so it will",
    "start": "1132440",
    "end": "1138799"
  },
  {
    "text": "just like serve it directly and if not it will go to The Container registry and get it um recently I think it was",
    "start": "1138799",
    "end": "1144559"
  },
  {
    "text": "yesterday also Asher team announced this a new project open source project that's called PRD that is doing kind of similar",
    "start": "1144559",
    "end": "1150600"
  },
  {
    "text": "this peer-to-peer communication and caching but you know one of the advantages is that also doing for other",
    "start": "1150600",
    "end": "1155760"
  },
  {
    "text": "kind of Technologies not only oci but also for example for blob storage or kind of and also supporting artifact",
    "start": "1155760",
    "end": "1162880"
  },
  {
    "text": "streaming so a bit of numbers here I just did this on you know my local cluster I just download uh the Cuda",
    "start": "1162880",
    "end": "1169280"
  },
  {
    "text": "container which is something like 3.8 gigs I downloaded first on the Node one it took something like 151 seconds and",
    "start": "1169280",
    "end": "1176159"
  },
  {
    "text": "then what I did I change it into node two right but because it was already part of node one it was actually you",
    "start": "1176159",
    "end": "1182240"
  },
  {
    "text": "know like it was served by node one right on my kind of local network and it's just like 5.7 seconds 28 faster",
    "start": "1182240",
    "end": "1188640"
  },
  {
    "text": "this will actually depend on your your network infrastructure your kubernetes cluster but you can see results between",
    "start": "1188640",
    "end": "1194080"
  },
  {
    "text": "10x to 50x generally so know the you know one of",
    "start": "1194080",
    "end": "1200480"
  },
  {
    "text": "the other parts here is just you know get the GPU of course and when you're using gpus what is that you need to take into account and of course first you",
    "start": "1200480",
    "end": "1207039"
  },
  {
    "text": "need to go ahead and set up your Noe there different kind of pieces that you need to take into account you NVIDIA drivers the container toolkit the",
    "start": "1207039",
    "end": "1213240"
  },
  {
    "text": "container runtime um and one of you know the important kind of pieces here is also what how you define your kubernetes",
    "start": "1213240",
    "end": "1219120"
  },
  {
    "text": "device allocation uh if anyone like was today in the keynote you know we the",
    "start": "1219120",
    "end": "1224280"
  },
  {
    "text": "media team actually talked a bit about you know what's it you know the the D driver and all the Ben benefits that you",
    "start": "1224280",
    "end": "1229360"
  },
  {
    "text": "get with that and what are the kind of limitations with the kind of device plugin right now like for example right you cannot assign more than one type of",
    "start": "1229360",
    "end": "1235679"
  },
  {
    "text": "GPU for a specific node um you can there's some limitation when to share in that GPU between containers and all",
    "start": "1235679",
    "end": "1242080"
  },
  {
    "text": "those are already being sold right now with you know KSD driver only issue is just stilling you know kind of an alpha",
    "start": "1242080",
    "end": "1247919"
  },
  {
    "text": "version but probably you know if you know if that's okay with you you can just go ahead and start testing um once",
    "start": "1247919",
    "end": "1253799"
  },
  {
    "text": "that you also have you know defined what's your device allocation you need to make sure what how you're going to access access you know that that kind of",
    "start": "1253799",
    "end": "1260120"
  },
  {
    "text": "a Cuda or your GPU and there different ways right so you have like you know it should be a single process but also",
    "start": "1260120",
    "end": "1266960"
  },
  {
    "text": "right you could be like when it comes to concurrent access you can be MPS it can be time slicing virtual gpus and again",
    "start": "1266960",
    "end": "1272840"
  },
  {
    "text": "you it's going to depend on what's your use cases kind of what you're going to be defining as the axis and then the",
    "start": "1272840",
    "end": "1279880"
  },
  {
    "text": "final thing is around the application logic so you need to take into account okay what are you going to be doing if it's GPU on fallback to CPU if you're",
    "start": "1279880",
    "end": "1286440"
  },
  {
    "text": "going to be doing lower layers well make sure so you could use for example safe tensor for lazy loading and just loading",
    "start": "1286440",
    "end": "1292159"
  },
  {
    "text": "you know the layers that you need um and also how you're going to host that model I are you going to using a framework so",
    "start": "1292159",
    "end": "1298200"
  },
  {
    "text": "there different Frameworks to host the model you have you have theama you have like uh VM you have open open LM so make",
    "start": "1298200",
    "end": "1305400"
  },
  {
    "text": "sure also that you define that logic based on you know these kind of Frameworks that they already",
    "start": "1305400",
    "end": "1311000"
  },
  {
    "text": "available um and then the final thing is around custom scaling um so again because this",
    "start": "1311000",
    "end": "1317640"
  },
  {
    "text": "kind of uh application gener just like you go you type something you finish then you",
    "start": "1317640",
    "end": "1323559"
  },
  {
    "text": "know and then you're not using that anymore probably you need to make sure that you have a good kind of logic",
    "start": "1323559",
    "end": "1329039"
  },
  {
    "text": "around how you scale up and scale down uh so you could use K right uh in order to actually have this event driven kind",
    "start": "1329039",
    "end": "1336200"
  },
  {
    "text": "of um Autos scaling uh you need to make sure also that if you're going to be using GPU you use right the DC hgmm",
    "start": "1336200",
    "end": "1343880"
  },
  {
    "text": "Nvidia exporter so that you can get all the GPU metrics and then finally just make sure you all also using right your",
    "start": "1343880",
    "end": "1349640"
  },
  {
    "text": "own logic and your custom metrics so you could use like metrics like latency tokens per second right what's it GP",
    "start": "1349640",
    "end": "1355159"
  },
  {
    "text": "utilization and getting all these metrics done then you can actually go ahead and create your kind of uh scaler",
    "start": "1355159",
    "end": "1361520"
  },
  {
    "text": "um for your kubernetes cluster so now let's let's go ahead and show you",
    "start": "1361520",
    "end": "1367880"
  },
  {
    "text": "a demo of you know end to end of trying to apply all these things that we've been talking um and to give you a bit of",
    "start": "1367880",
    "end": "1374720"
  },
  {
    "text": "context here so the demo that we end up doing we wanted to do a like simple demo but kind of you know like useful in your",
    "start": "1374720",
    "end": "1380400"
  },
  {
    "text": "real world so imagine you you're working for an HR company that is just actually receiving a lot of you know emails and",
    "start": "1380400",
    "end": "1386559"
  },
  {
    "text": "resumes right they're like from multiple countries multiple language and in the end what you want to do is some",
    "start": "1386559",
    "end": "1391760"
  },
  {
    "text": "filtering you want to make sure that you abstract information based on those emails and you just put it for example",
    "start": "1391760",
    "end": "1397039"
  },
  {
    "text": "in a database so for example you get something hey my name is anel I'm 32",
    "start": "1397039",
    "end": "1402279"
  },
  {
    "text": "years old I live in sevia and I'm working as a software engineer probably right a restroom and what you want to do",
    "start": "1402279",
    "end": "1408039"
  },
  {
    "text": "is hey just get this Json that is extracting the information right and just creating your this new Final Json",
    "start": "1408039",
    "end": "1415480"
  },
  {
    "text": "file so we actually went and say okay based on this kind of application let's go ahead and try the you know online",
    "start": "1415480",
    "end": "1420880"
  },
  {
    "text": "models so we try with mistal 7 billion it work quite good we try with Gemma",
    "start": "1420880",
    "end": "1426760"
  },
  {
    "text": "quite good and then we when we tried with F2 and Gemma 2 billion it was it was okay but had a couple of mistakes",
    "start": "1426760",
    "end": "1434320"
  },
  {
    "text": "right so we say okay let's go ahead and try to see if we can actually fine-tune these and use the base fight to model",
    "start": "1434320",
    "end": "1440039"
  },
  {
    "text": "and create a you know a lower layer and see how it works so what we end up doing there was we created you know a",
    "start": "1440039",
    "end": "1445080"
  },
  {
    "text": "synthetic data set so we went they like online we created I think it was 100,000 lines of you know these examples then we",
    "start": "1445080",
    "end": "1452240"
  },
  {
    "text": "we go ahead and we train we fine-tune these F2 model um and actually try to see what happened right when we running",
    "start": "1452240",
    "end": "1458640"
  },
  {
    "text": "uh the fine tune model in our cluster okay so now anel is going to demonstrate this live how this",
    "start": "1458640",
    "end": "1466320"
  },
  {
    "text": "works so oops okay can you yeah going to do this",
    "start": "1466320",
    "end": "1474559"
  },
  {
    "text": "okay so let me clear this",
    "start": "1474559",
    "end": "1478799"
  },
  {
    "text": "um cool so what we did like with the with the fight to uh find tune in so we",
    "start": "1480039",
    "end": "1486080"
  },
  {
    "text": "train it but in a real environment you may think like this is going to be more of a work for mature learning engineer",
    "start": "1486080",
    "end": "1491760"
  },
  {
    "text": "so at this point what we did is like we' find this model pretty pretty simple and",
    "start": "1491760",
    "end": "1496880"
  },
  {
    "text": "then we have this model available volume or oci registry so you're going to start right away using it so the first thing",
    "start": "1496880",
    "end": "1503000"
  },
  {
    "text": "that we need to do is actually to deploy it and as as Francisco was mentioning there are many different Frameworks and",
    "start": "1503000",
    "end": "1508960"
  },
  {
    "text": "services that you can use for this in this case we use VM so how it looks in",
    "start": "1508960",
    "end": "1514159"
  },
  {
    "text": "reality is if we go to the configuration file and then we see like the inference",
    "start": "1514159",
    "end": "1519200"
  },
  {
    "text": "jaml file here you can see that we are defining a PO that comes with a ining",
    "start": "1519200",
    "end": "1525600"
  },
  {
    "text": "container this is because as as um Francisco was mentioning for the oci uh",
    "start": "1525600",
    "end": "1530840"
  },
  {
    "text": "registry we need to download all the different layers and recompose all the files so this is part of the of the init",
    "start": "1530840",
    "end": "1536840"
  },
  {
    "text": "container that we have that we have here and then we're loading it in a in a volume and then we we are using the VM",
    "start": "1536840",
    "end": "1543840"
  },
  {
    "text": "uh container right away we don't need to perform any modification just to load it from the folder that was downloaded",
    "start": "1543840",
    "end": "1550440"
  },
  {
    "text": "before and in this case we are using a local cluster using kind so this is the way like we are asking for the cluster",
    "start": "1550440",
    "end": "1556120"
  },
  {
    "text": "to allocate one specific GPU to run this this example so we can see like the result of",
    "start": "1556120",
    "end": "1564120"
  },
  {
    "text": "the of the init container so let me like do keep C",
    "start": "1564120",
    "end": "1570520"
  },
  {
    "text": "locks so basically yeah basically the container is just downloading all the all the",
    "start": "1570520",
    "end": "1577039"
  },
  {
    "text": "layers and then just packing back to the original model it's just really simple python",
    "start": "1577039",
    "end": "1583320"
  },
  {
    "text": "code so here as you can see it's a pretty simple uh python code is just like with with Francisco said at the end",
    "start": "1585960",
    "end": "1592880"
  },
  {
    "text": "you can see like the final files that happen after all the Reconstruction this took time so that's why we made it like",
    "start": "1592880",
    "end": "1599120"
  },
  {
    "text": "before the the presentation um after this this is like uh available we can",
    "start": "1599120",
    "end": "1604919"
  },
  {
    "text": "see on the VM service is now like ready to start",
    "start": "1604919",
    "end": "1611960"
  },
  {
    "text": "receiving pring responses and this was based on a fine T model and it was pretty simple we don't have to run any",
    "start": "1611960",
    "end": "1617440"
  },
  {
    "text": "custom code just let use the same uh service that that we have but now we have a fine two model so now let's try",
    "start": "1617440",
    "end": "1623600"
  },
  {
    "text": "to run it so for this I have a pretty simple um infer test.py file here what",
    "start": "1623600",
    "end": "1630960"
  },
  {
    "text": "you can find is that we are using the open AI um library because V exposes an",
    "start": "1630960",
    "end": "1638320"
  },
  {
    "text": "open AI API compatible layer so we don't need to create any custom SDK here just",
    "start": "1638320",
    "end": "1643760"
  },
  {
    "text": "for Simplicity we are adding the text inside so here instead of having like a short description like before we added a",
    "start": "1643760",
    "end": "1649880"
  },
  {
    "text": "little bit longer one with some information around and then we have the prompt in",
    "start": "1649880",
    "end": "1654960"
  },
  {
    "text": "which we are adding the struction about hey stct this information this is the format that we want just skip things",
    "start": "1654960",
    "end": "1660600"
  },
  {
    "text": "before and after and then give me the output with the final Json so let's try it so now if I run",
    "start": "1660600",
    "end": "1669279"
  },
  {
    "text": "like python infinet uh P I need to put like the",
    "start": "1669279",
    "end": "1675840"
  },
  {
    "text": "URL V1 we're good so in the result like we see",
    "start": "1675960",
    "end": "1682840"
  },
  {
    "text": "like we have already the Json that we wanted to stct in the right format and",
    "start": "1682840",
    "end": "1688080"
  },
  {
    "text": "then after performing some tests you can see like it's giving a little bit more accuracy of course this is synthetic",
    "start": "1688080",
    "end": "1693679"
  },
  {
    "text": "data so we cannot expect like a really really good accuracy when using like in real with real data but since you can",
    "start": "1693679",
    "end": "1700440"
  },
  {
    "text": "now access to the private data that you have in your company you can f tune with this data and improve over and over and",
    "start": "1700440",
    "end": "1706559"
  },
  {
    "text": "since fine tuning is so cheap you can do it like iteratively and constantly and just to finalize this",
    "start": "1706559",
    "end": "1713799"
  },
  {
    "text": "demo the cool thing about this this for me is that everything that you saw here from the fine-tuning to the to the",
    "start": "1713799",
    "end": "1720919"
  },
  {
    "text": "running of this entire model is actually running in Nvidia GeForce RTX 3060 which",
    "start": "1720919",
    "end": "1727000"
  },
  {
    "text": "is a 300 bus uh graphic card with 12 GB of memory so you don't need to have like",
    "start": "1727000",
    "end": "1733039"
  },
  {
    "text": "huge graphic cards to run this kind of of models and you can use this maybe for doing some specific task inside the",
    "start": "1733039",
    "end": "1739640"
  },
  {
    "text": "stores or in any H deployment that you want and you cannot put um like a,",
    "start": "1739640",
    "end": "1745720"
  },
  {
    "text": "graphic cards on every store that that your company",
    "start": "1745720",
    "end": "1750320"
  },
  {
    "text": "owns yeah with that thank you so much and yeah I don't know if have any questions thank",
    "start": "1754960",
    "end": "1762559"
  },
  {
    "text": "you I think we have some time for",
    "start": "1767039",
    "end": "1775398"
  },
  {
    "text": "questions okay I think you can go yeah to the",
    "start": "1782440",
    "end": "1787000"
  },
  {
    "text": "microphone cool thank you for talk yes I have a quick question so when we put the model in the oci so uh compatible",
    "start": "1789399",
    "end": "1798600"
  },
  {
    "text": "server uh when we download a model in our code we are existing Library like",
    "start": "1798600",
    "end": "1804679"
  },
  {
    "text": "huging face or other py to code can download from the oi registry directly",
    "start": "1804679",
    "end": "1810440"
  },
  {
    "text": "or you have to use some init container to download it so as far as I know there's no like",
    "start": "1810440",
    "end": "1818000"
  },
  {
    "text": "direct tool inside the hacking face Transformer library to download using the oi layer but I believe like in the",
    "start": "1818000",
    "end": "1823919"
  },
  {
    "text": "future it will be some kind of adapter that you can use right away with the Transformer Library so it download it",
    "start": "1823919",
    "end": "1829279"
  },
  {
    "text": "directly but I think as today there is no l okay cool cool thank you yeah you're",
    "start": "1829279",
    "end": "1836360"
  },
  {
    "text": "welcome okay so cool thank you very much",
    "start": "1841880",
    "end": "1848000"
  }
]