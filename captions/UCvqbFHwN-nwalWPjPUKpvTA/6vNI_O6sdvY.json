[
  {
    "text": "hi everyone good evening uh my name is Alex Meyer I'm Michael dresser and we're",
    "start": "40",
    "end": "5520"
  },
  {
    "text": "software engineers at Cube cost uh we we're the creators of the Cube cost app and also uh open cost and we're super",
    "start": "5520",
    "end": "12480"
  },
  {
    "text": "excited to share uh some interesting information we found with you tonight uh",
    "start": "12480",
    "end": "17720"
  },
  {
    "text": "in the course of you know developing our app right uh surface and cost information for customers and helping",
    "start": "17720",
    "end": "24039"
  },
  {
    "text": "them act on that to you know size their nodes and workloads so we'll do a quick overview",
    "start": "24039",
    "end": "29679"
  },
  {
    "text": "of what we're going to go over tonight we'll start by defining this concept of node overhead or we'll shorten it to",
    "start": "29679",
    "end": "35480"
  },
  {
    "text": "overhead we'll go over what it is how do you you know obtain it right how it's calculated uh we'll go over open cost",
    "start": "35480",
    "end": "42800"
  },
  {
    "text": "very quickly um which is the open source component of Cube cost uh we'll have a quick demo of how to use open cost to",
    "start": "42800",
    "end": "49719"
  },
  {
    "text": "determine these concepts of node overhead and sort of measure your sunk cost if you will uh we'll then use this",
    "start": "49719",
    "end": "56199"
  },
  {
    "text": "tooling to conduct a study and share the results of that study with you and we'll",
    "start": "56199",
    "end": "61399"
  },
  {
    "text": "summarize of our key findings from that um in the interest of you know giving action items based on the study",
    "start": "61399",
    "end": "67640"
  },
  {
    "text": "information we'll go over some node sizing algorithms uh both our existing node sizing algorithm and then we'll",
    "start": "67640",
    "end": "73759"
  },
  {
    "text": "compare and contrast that to the updated node sizing algorithm that sort of incorporates these ideas uh of node",
    "start": "73759",
    "end": "81720"
  },
  {
    "text": "overhead so to start right off uh if we Define node overhead it's basically",
    "start": "81720",
    "end": "87680"
  },
  {
    "text": "compute that is used to run kubernetes itself right the cost of doing business if you will so um nodes typically have",
    "start": "87680",
    "end": "95240"
  },
  {
    "text": "capacity I'm sure we're all familiar with that right that's the sticker price that's what you're built for uh in gcp",
    "start": "95240",
    "end": "101280"
  },
  {
    "text": "or you know AWS or whatever you're using um it's what's in the pricing apis and U",
    "start": "101280",
    "end": "106520"
  },
  {
    "text": "that's what you what you're paying for essentially right now there is some subset of that that is defined as",
    "start": "106520",
    "end": "112320"
  },
  {
    "text": "allocatable now allocatable capacity is something that's passed to the cuet",
    "start": "112320",
    "end": "117479"
  },
  {
    "text": "through the command line arguments when it's booted up so we're going to study a little bit both of a property of",
    "start": "117479",
    "end": "122520"
  },
  {
    "text": "kubernetes itself but also managed kubernetes right um the difference between these is is overheads so",
    "start": "122520",
    "end": "127920"
  },
  {
    "text": "overhead includes things like the cuet right uh any control plane",
    "start": "127920",
    "end": "133200"
  },
  {
    "text": "infrastructure running on that node if it's applicable um the container run time like Docker or container D itself",
    "start": "133200",
    "end": "139920"
  },
  {
    "text": "um in general any software running directly on the Node that isn't in a pot right uh overhead does not include",
    "start": "139920",
    "end": "146959"
  },
  {
    "text": "things like Prometheus um any sort of DNS pods or you know C manager if you're",
    "start": "146959",
    "end": "152120"
  },
  {
    "text": "running it anything in Cube system because again that's all part of kubernetes uh so that is out of scope of",
    "start": "152120",
    "end": "158840"
  },
  {
    "text": "what our definition of overhead is so uh you can see just from QC",
    "start": "158840",
    "end": "164800"
  },
  {
    "text": "describe or any sort of ku's API request that tells you about the nodes in your cluster uh you can actually see this",
    "start": "164800",
    "end": "170120"
  },
  {
    "text": "capacity and the allocatable and the difference immediately um this is a slightly cut up response I'm sure you've",
    "start": "170120",
    "end": "175200"
  },
  {
    "text": "cctl described a node before it's a lot more than this but these are the relevant Fields uh we can see see that I'm running an N1 standard 2 here uh we",
    "start": "175200",
    "end": "182360"
  },
  {
    "text": "have the capacity block and the allocatable block uh we're comparing 2 CPU capacity which is again that sticker",
    "start": "182360",
    "end": "188120"
  },
  {
    "text": "price that Alex just talked about uh versus allocatable 1930 m not too much of a loss there uh but if we go to the",
    "start": "188120",
    "end": "194560"
  },
  {
    "text": "memory I think we'll see a pretty Stark difference here we're looking at I don't know what that is 7 1/2 gigs of capacity",
    "start": "194560",
    "end": "200280"
  },
  {
    "text": "versus uh 5 1 halfish allocatable big loss there obviously this is a small node type but we'll dig more into uh",
    "start": "200280",
    "end": "207440"
  },
  {
    "text": "node types across the providers uh in just a moment to show you that survey information but I just want to highlight",
    "start": "207440",
    "end": "212560"
  },
  {
    "text": "this here like some of this information is accessible to you today in your cluster uh a brief turn to open cost",
    "start": "212560",
    "end": "220080"
  },
  {
    "text": "just so we can set up uh some of the rest of the talk uh open cost is a cncf Sandbox project uh the primary",
    "start": "220080",
    "end": "225640"
  },
  {
    "text": "maintainers uh is our company Cube cost but we have many other excellent contributors that we appreciate",
    "start": "225640",
    "end": "231159"
  },
  {
    "text": "including Microsoft graphon Labs uh and many others uh what I want to highlight today is the rest API um it does uh come",
    "start": "231159",
    "end": "237920"
  },
  {
    "text": "with a bakedin UI uh but for the sake of of the demo we're going to be looking at the rest API and how it combines that",
    "start": "237920",
    "end": "244519"
  },
  {
    "text": "information I just showed you from cctl describe with actual cost information uh about your nodes there's much more you",
    "start": "244519",
    "end": "250599"
  },
  {
    "text": "can do with open cost of course but we only have time for a small slice of it cool so to dive a little deeper into how",
    "start": "250599",
    "end": "258079"
  },
  {
    "text": "we calculate overhead within open cost itself uh we sort of have this flow diagram so it begins at the bottom right",
    "start": "258079",
    "end": "266160"
  },
  {
    "text": "of this uh diagram where basically we surface those allocatable and capacity",
    "start": "266160",
    "end": "272000"
  },
  {
    "text": "metrics that we sort of referred to a few slides ago uh in Prometheus metrics now these are standard uh Cube State",
    "start": "272000",
    "end": "277960"
  },
  {
    "text": "metrics metrics so these would either be emitted from your Cube State metrics pod if you have that installed uh otherwise",
    "start": "277960",
    "end": "283680"
  },
  {
    "text": "open cost itself actually emits these uh if you don't have that installed uh so",
    "start": "283680",
    "end": "288919"
  },
  {
    "text": "then those are of course exposed to Prometheus we're not going to go too far into that because I'm sure many of us here use that are familiar with it but",
    "start": "288919",
    "end": "296160"
  },
  {
    "text": "you know Prometheus will scrape these and provides our critical s sort of aggregation query",
    "start": "296160",
    "end": "301280"
  },
  {
    "text": "functionality uh on the left here we have uh open cost itself and in that we",
    "start": "301280",
    "end": "306919"
  },
  {
    "text": "have the different components of the algorithm broken up so we begin the overhead calculation by of course the",
    "start": "306919",
    "end": "312240"
  },
  {
    "text": "query to Prometheus right and we'll follow just RAM on this side for brevity",
    "start": "312240",
    "end": "317280"
  },
  {
    "text": "but CPU calculations would be analogous right uh so here we have like an average call to the Prometheus server that will",
    "start": "317280",
    "end": "324800"
  },
  {
    "text": "return our historical data that we can use to calculate the overhead uh and then we proceed to The Next Step which",
    "start": "324800",
    "end": "331080"
  },
  {
    "text": "we call pre-processing right so we have basically a calculation here that",
    "start": "331080",
    "end": "336199"
  },
  {
    "text": "calculates the absolute value right we perform our uh classic capacity minus",
    "start": "336199",
    "end": "342120"
  },
  {
    "text": "allocatable to get an absolute amount of bytes being used for overhead and then",
    "start": "342120",
    "end": "348199"
  },
  {
    "text": "to get it to turn that into a proportion we divide that by the overall capacity",
    "start": "348199",
    "end": "353360"
  },
  {
    "text": "right to give you a percentage of your node being used to run kubernetes itself um our final step here is what we call",
    "start": "353360",
    "end": "359960"
  },
  {
    "text": "postprocessing right uh our goal is to consolidate all of these different components into one number that sort of",
    "start": "359960",
    "end": "365759"
  },
  {
    "text": "summarizes uh the total overhead cost so here we're performing a cost weighted",
    "start": "365759",
    "end": "370880"
  },
  {
    "text": "average between the CPU which we brought back in for this last example and the Ram uh multiplying them by their",
    "start": "370880",
    "end": "377319"
  },
  {
    "text": "respective costs so that out comes one proportional number that accurately represents um the proportional the cost",
    "start": "377319",
    "end": "384319"
  },
  {
    "text": "that is being used for overhead so for me this is the critical addition that cost provides and of",
    "start": "384319",
    "end": "390919"
  },
  {
    "text": "course Cube cost on top will provide on top of what the kubernetes API already exposes to you so we're going to pray to",
    "start": "390919",
    "end": "396360"
  },
  {
    "text": "the live live demo Gods here um I have an environment here that is backed by open cost uh we're going to be hitting",
    "start": "396360",
    "end": "401880"
  },
  {
    "text": "the assets API which is what you do to access node information um relevantly we're just filtering down to node types",
    "start": "401880",
    "end": "408880"
  },
  {
    "text": "and I'm only going to show you the overhead information uh because I don't think we need to see a massive Json blob",
    "start": "408880",
    "end": "414080"
  },
  {
    "text": "uh and all you can do with it uh the live demo God's been kind to me uh we can see I have four four nodes uh in",
    "start": "414080",
    "end": "419199"
  },
  {
    "text": "this environment um we have that CPU specific uh fraction on this Noe it's",
    "start": "419199",
    "end": "424639"
  },
  {
    "text": "like 32% not a big deal uh RAM is more like 24% that's pretty substantial um",
    "start": "424639",
    "end": "431039"
  },
  {
    "text": "these are pretty small nodes and we'll see why that matters uh shortly uh and then also we have an overall uh overhead",
    "start": "431039",
    "end": "438000"
  },
  {
    "text": "cost fraction this is the key bit that open cost adds 15% of the cost according",
    "start": "438000",
    "end": "443639"
  },
  {
    "text": "to open cost is uh being spent on the kubernetes overhead and is not usable in",
    "start": "443639",
    "end": "449720"
  },
  {
    "text": "my environment to schedule workloads the only other thing I'll point out here is that I have a couple other nodes here",
    "start": "449720",
    "end": "454960"
  },
  {
    "text": "where the CPU overhead looks like 50% that's really really really bad that's a particular outlier that we're going to",
    "start": "454960",
    "end": "461120"
  },
  {
    "text": "highlight in a couple of slides uh in just a moment so we wanted to use this tooling",
    "start": "461120",
    "end": "468120"
  },
  {
    "text": "to basically conduct a study right and we have this this nice new API",
    "start": "468120",
    "end": "473400"
  },
  {
    "text": "capability and open cost right uh and we wanted to use it to gain a better understanding of overhead so we wanted",
    "start": "473400",
    "end": "479960"
  },
  {
    "text": "to take it in a couple different directions right uh and this is an empirical study right it's worth noting",
    "start": "479960",
    "end": "485159"
  },
  {
    "text": "that some Cloud providers like I know for a fact AKs does this specifically they will publish sort of formulas",
    "start": "485159",
    "end": "491159"
  },
  {
    "text": "guidelines for their overhead but we want an empirical study to determine how",
    "start": "491159",
    "end": "496280"
  },
  {
    "text": "uh overhead behaves as things like the node size increases right the node family changes if there's any difference",
    "start": "496280",
    "end": "502479"
  },
  {
    "text": "there and then of course comparing and contrasting the different providers with the end goal here being that we can",
    "start": "502479",
    "end": "508639"
  },
  {
    "text": "generate data to input into our decision- making process when making the",
    "start": "508639",
    "end": "513760"
  },
  {
    "text": "extraordinarily uh crucial decision to size your cluster so a quick overview of our",
    "start": "513760",
    "end": "520719"
  },
  {
    "text": "methodology right we' call each Cloud provider's APA end points to obtain both the node sizes available in a given",
    "start": "520719",
    "end": "527519"
  },
  {
    "text": "region and their prices we'd then take 10 batches of those or batches of 10 of",
    "start": "527519",
    "end": "533040"
  },
  {
    "text": "those at a time each node type in its own node pool and deploy to the actual",
    "start": "533040",
    "end": "539200"
  },
  {
    "text": "cloud provider and then after that install open cost uh and it's according",
    "start": "539200",
    "end": "545560"
  },
  {
    "text": "Prometheus on those and obtain uh the allocatable metrics capacity right and",
    "start": "545560",
    "end": "551440"
  },
  {
    "text": "those would actually come directly from Cube cuddle itself we would use the open cost installation uh to obtain that",
    "start": "551440",
    "end": "556959"
  },
  {
    "text": "weighted overhead cost metric we went over a little bit earlier uh and then finally any node metadata just for later",
    "start": "556959",
    "end": "563279"
  },
  {
    "text": "analysis we save that in a Json file and then that was the input into our",
    "start": "563279",
    "end": "568360"
  },
  {
    "text": "analysis pipe line that uh will generate the graphs we're about to go over so we",
    "start": "568360",
    "end": "573920"
  },
  {
    "text": "we started by just putting together a couple of exploratory graphs just to look at a couple of the smaller nodes to",
    "start": "573920",
    "end": "579120"
  },
  {
    "text": "have an idea of what kind of data we were looking at um we're going to start here with CPU just because that's where I started and I immediately found",
    "start": "579120",
    "end": "584880"
  },
  {
    "text": "something very odd uh there's a bunch of on the left graph there there's a bunch of dots down towards the bottom but way",
    "start": "584880",
    "end": "591600"
  },
  {
    "text": "up at the top in near 50% CPU overhead is an E2 medium node uh and we can see",
    "start": "591600",
    "end": "597440"
  },
  {
    "text": "that reflected again on the right where we have have uh three nodes I'm sorry if it's quite small but those are E2 small E2 micro and E2 medium that all have the",
    "start": "597440",
    "end": "605760"
  },
  {
    "text": "50% Big Red Bar there um this is a particular outlier that we excluded from",
    "start": "605760",
    "end": "610880"
  },
  {
    "text": "future graphs uh but what we want to highlight it right now is sort of a case study and something interesting we found",
    "start": "610880",
    "end": "616360"
  },
  {
    "text": "uh about this family of notes in particular when used in kubernetes so what's going on here uh we",
    "start": "616360",
    "end": "623360"
  },
  {
    "text": "we looked at these ones specifically we didn't really see this in any other instance types uh so upon further",
    "start": "623360",
    "end": "629240"
  },
  {
    "text": "investigation right the gcp docs tell us everything we need to know right these are special burstable instance types",
    "start": "629240",
    "end": "635160"
  },
  {
    "text": "right and we quote gcp here uh it basically sustains 2 vcpus at 50% of CPU",
    "start": "635160",
    "end": "642200"
  },
  {
    "text": "time effectively giving you one usable vcpu core as a baseline but letting you",
    "start": "642200",
    "end": "648600"
  },
  {
    "text": "burst up to two vcpus right um so okay you know that makes sense right in the",
    "start": "648600",
    "end": "654839"
  },
  {
    "text": "context of what we're seeing for overhead right we're seeing about 50% CPU overhead uh and we have one core",
    "start": "654839",
    "end": "660880"
  },
  {
    "text": "effectively usable so how do the prices stack up right like we are we what's going on we we see that we're about 3.3",
    "start": "660880",
    "end": "667800"
  },
  {
    "text": "cents in this region for the versible instance and you know just using the E2",
    "start": "667800",
    "end": "673480"
  },
  {
    "text": "instance Builder type building an equivalent one vcpu instance with the 4 gigs of RAM for this medium uh is three",
    "start": "673480",
    "end": "680360"
  },
  {
    "text": "and a half cents so you're not getting ripped off here right you're actually getting a slightly lower price for",
    "start": "680360",
    "end": "686959"
  },
  {
    "text": "something that can burst to a higher utilization and maybe gcp is doing some sort of efficiency on their side and",
    "start": "686959",
    "end": "693040"
  },
  {
    "text": "they pass the savings on to you that being said though you aren't quite getting what you think you are right um",
    "start": "693040",
    "end": "700200"
  },
  {
    "text": "and at minimum right this could be a challenge for capacity planning when if you try and schedule a pod that",
    "start": "700200",
    "end": "705399"
  },
  {
    "text": "approaches to vcpu of allocation you're not going to be able to schedule it on these uh so in the subsequent graphs",
    "start": "705399",
    "end": "713000"
  },
  {
    "text": "we're generally going to remove these uh as outliers just because of this sort of",
    "start": "713000",
    "end": "718240"
  },
  {
    "text": "interesting situ situation here so let's jump straight into the metric that I think is the most interesting for most",
    "start": "718240",
    "end": "724760"
  },
  {
    "text": "people which is what percentage of the cost of a node uh is overhead and how",
    "start": "724760",
    "end": "730399"
  },
  {
    "text": "does That Vary as cost increases cost increasing is generally a good proxy for",
    "start": "730399",
    "end": "735880"
  },
  {
    "text": "the capacity of the node increasing we'll break it down by specific resources in just a moment but I just want to highlight the overall trend that",
    "start": "735880",
    "end": "742079"
  },
  {
    "text": "we're going to see repeated multiple times which is there's some range of time for which uh range of price or",
    "start": "742079",
    "end": "747839"
  },
  {
    "text": "capacity uh for which we're paying a pretty steep overhead price and then as that uh capacity of a single node type",
    "start": "747839",
    "end": "755440"
  },
  {
    "text": "increases the overall penalty we're paying to kubernetes to help manage that",
    "start": "755440",
    "end": "760560"
  },
  {
    "text": "node for us uh is decreasing right that that sort of down uh and to the right",
    "start": "760560",
    "end": "765639"
  },
  {
    "text": "trend is going to repeat itself multiple times uh in our graphs um the only other interesting Trend here um is that AKs",
    "start": "765639",
    "end": "772959"
  },
  {
    "text": "has a sort of a marked Step Above uh gke and eks in terms of uh overhead um more",
    "start": "772959",
    "end": "779600"
  },
  {
    "text": "work to do there to understand why I'm not sure if that's simply AKs being more conservative or something else um all",
    "start": "779600",
    "end": "787040"
  },
  {
    "text": "also notice that on the the y- axis here we're seeing a a penalty of up to like 20% uh more commonly it's like 5 to 10%",
    "start": "787040",
    "end": "794360"
  },
  {
    "text": "uh and we're sort of tailing off uh towards not quite to zero but but almost to zero uh as we get to a certain",
    "start": "794360",
    "end": "801120"
  },
  {
    "text": "extreme uh breaking it down now uh by individual resource type uh looking at CPU so now we're away from the cost",
    "start": "801120",
    "end": "808320"
  },
  {
    "text": "dimension and we're looking specifically just at amount of resources paid um this is percentage of CPU capacity uh that's",
    "start": "808320",
    "end": "816160"
  },
  {
    "text": "as the absolute capacity increases um this is pretty low right we're talking single- digigit percentages tailing off",
    "start": "816160",
    "end": "822839"
  },
  {
    "text": "towards you know 0.51 2% not a big deal which points us towards memory as sort",
    "start": "822839",
    "end": "829160"
  },
  {
    "text": "of the main driver uh of overhead cost uh we can see again that AKs is sort of",
    "start": "829160",
    "end": "834759"
  },
  {
    "text": "popping above the other two here again more to more to look into uh as far as memory goes here's where that driver is",
    "start": "834759",
    "end": "841639"
  },
  {
    "text": "coming we can see that the Y AIS is now in tens of percent uh we have again same Trend really high spiking uh towards the",
    "start": "841639",
    "end": "849480"
  },
  {
    "text": "left side and we're just sort of trailing off there's definitely a sweet spot here um we can see in this graph it's probably around like the 48 gigs of",
    "start": "849480",
    "end": "856959"
  },
  {
    "text": "capacity Mark for most for the average provider um varying of course um eks is",
    "start": "856959",
    "end": "864360"
  },
  {
    "text": "uh you know difference from uh AKs and gk's Market here uh again we don't quite",
    "start": "864360",
    "end": "869959"
  },
  {
    "text": "know why uh more work to do but I just want to highlight the same Trend and the fact that memory uh as far as overhead",
    "start": "869959",
    "end": "875440"
  },
  {
    "text": "goes is the main driver uh of the penalty uh that you're paying so we also included some studies",
    "start": "875440",
    "end": "883519"
  },
  {
    "text": "where we look at the different families here right uh so on the the left here we",
    "start": "883519",
    "end": "889199"
  },
  {
    "text": "have the ks instance families and the specific instances aren't terribly",
    "start": "889199",
    "end": "894600"
  },
  {
    "text": "important right just know that as it goes to the right they get larger right so so we can see sort of here the same",
    "start": "894600",
    "end": "900759"
  },
  {
    "text": "Trend we're seeing elsewhere the smaller members of each family have generally higher overheads right moving all the",
    "start": "900759",
    "end": "907720"
  },
  {
    "text": "way to um the right where we have the dot metal instances right which is aws's",
    "start": "907720",
    "end": "913519"
  },
  {
    "text": "abstraction giving you the entire underlying VM essentially uh with generally the lowest overheads which you",
    "start": "913519",
    "end": "918959"
  },
  {
    "text": "know makes sense following from our other charts um on the right we have uh",
    "start": "918959",
    "end": "924279"
  },
  {
    "text": "gke instances uh separated by high CPU High M and standard again the exact",
    "start": "924279",
    "end": "930920"
  },
  {
    "text": "families don't matter our big takeaway was family type doesn't significantly o",
    "start": "930920",
    "end": "936000"
  },
  {
    "text": "affect overhead so going to a high M instance won't significantly reduce or increase your overhead per se versus the",
    "start": "936000",
    "end": "942959"
  },
  {
    "text": "high CPU instance so finally a couple of interesting graphs uh on just the most",
    "start": "942959",
    "end": "949639"
  },
  {
    "text": "expensive in terms of wastefulness relative to overhead as a percentage of cost uh we'll flip in just a moment the",
    "start": "949639",
    "end": "955199"
  },
  {
    "text": "next one is the the least expensive uh in terms of overhead you're paying as a proportion of cost um we've brought the",
    "start": "955199",
    "end": "960680"
  },
  {
    "text": "E2 family back in here you can see that really substantial difference we saw both both in the open cost API example",
    "start": "960680",
    "end": "967560"
  },
  {
    "text": "and then that Alex explained so of course they're dominating the figure here because they're unique uh the",
    "start": "967560",
    "end": "973720"
  },
  {
    "text": "interesting Trend here I'd see that you can see as well is that uh AKs and gke really dominate this field and as we saw",
    "start": "973720",
    "end": "980240"
  },
  {
    "text": "in the other graph these are all small node types right we're talking two three four cores you know four 6 8 gigs of RAM",
    "start": "980240",
    "end": "987199"
  },
  {
    "text": "is what most of these node types are giving you um eks just coming in towards the bottom there uh under the 10% Mark",
    "start": "987199",
    "end": "993800"
  },
  {
    "text": "I'm not sure if that's some Target that they aim for uh or something like that but but it is interesting uh switching",
    "start": "993800",
    "end": "999360"
  },
  {
    "text": "to uh the lowest the most efficient in terms of overhead uh in particular uh",
    "start": "999360",
    "end": "1004759"
  },
  {
    "text": "eks dominates the field here um part of this is because uh eks was the node",
    "start": "1004759",
    "end": "1010000"
  },
  {
    "text": "group uh the provider that we surveyed the most broadly so we may have captured uh the most largest Siz nodes on eks uh",
    "start": "1010000",
    "end": "1018839"
  },
  {
    "text": "but again we see the same kind of idea here where the percentage here we like below 6% this is basically negligible",
    "start": "1018839",
    "end": "1024798"
  },
  {
    "text": "but the thing I'll point you to is simply the names of these instances these are just about the largest",
    "start": "1024799",
    "end": "1029918"
  },
  {
    "text": "instances in their class so that's just matching that Trend we saw in all the previous graphs in terms of uh larger",
    "start": "1029919",
    "end": "1036160"
  },
  {
    "text": "node size you pay less of a penalty uh towards overhead so to just summarize what we're",
    "start": "1036160",
    "end": "1041600"
  },
  {
    "text": "finding right I'd say our Top Line finding we've got here is all the charts I've made sort of painfully clear large",
    "start": "1041600",
    "end": "1048600"
  },
  {
    "text": "to the node the lower the overhead right to summarize it in one sentence uh we converge towards 0% of overhead ASM",
    "start": "1048600",
    "end": "1054760"
  },
  {
    "text": "totically starting rather high for small node types um an interesting finding we",
    "start": "1054760",
    "end": "1060200"
  },
  {
    "text": "had is there are significant number of nodes with 10% or greater overhead right which is a pretty significant number",
    "start": "1060200",
    "end": "1067400"
  },
  {
    "text": "especially as your Cloud spin gets larger uh we find that the total overhead cost again borne out by the",
    "start": "1067400",
    "end": "1073640"
  },
  {
    "text": "charts is overwhelmingly driven by Ram right we didn't really see anything over six % CPU usage being given or reserved",
    "start": "1073640",
    "end": "1082360"
  },
  {
    "text": "as overhead I guess you could say um we find that kubernetes has an interesting way of uh handling the gcp shared uh",
    "start": "1082360",
    "end": "1089480"
  },
  {
    "text": "core instance types right as we saw with those outliers um provider wise we generally see AKs with the highest",
    "start": "1089480",
    "end": "1096080"
  },
  {
    "text": "overhead again uh perhaps conservative decisions or other algorithm algorithmic",
    "start": "1096080",
    "end": "1101799"
  },
  {
    "text": "work uh and then finally we see eks with in general the lowest overhead uh just a",
    "start": "1101799",
    "end": "1108000"
  },
  {
    "text": "quick not on the scope of this study right we didn't include node dis which also has overhead components we also",
    "start": "1108000",
    "end": "1114840"
  },
  {
    "text": "didn't study gpus uh node dis because it's not a very large driver of node",
    "start": "1114840",
    "end": "1121080"
  },
  {
    "text": "cost relative to CPU and RAM and uh GPU we just sort of brought out of scope for",
    "start": "1121080",
    "end": "1127120"
  },
  {
    "text": "for this portion of the study so uh how do we action on this",
    "start": "1127120",
    "end": "1132480"
  },
  {
    "text": "information um I'm going to give you a brief overview I'm not going to read the whole slide I promise uh of how Cube",
    "start": "1132480",
    "end": "1138080"
  },
  {
    "text": "cost recommends nodes to our users um we're going to talk then about some general trade-offs when doing node",
    "start": "1138080",
    "end": "1144200"
  },
  {
    "text": "sizing uh and then Alex will tell you about how we can incorporate uh overhead into this algorithm so the basic idea is",
    "start": "1144200",
    "end": "1149840"
  },
  {
    "text": "that we gather some heuristics about historical behavior of workloads that ran in your cluster um the exact ones",
    "start": "1149840",
    "end": "1156120"
  },
  {
    "text": "are listed here um we then have to look through all of the node types you might have available for you in a given region",
    "start": "1156120",
    "end": "1162880"
  },
  {
    "text": "look at their cost say can this set of nodes schedule the workloads you need to",
    "start": "1162880",
    "end": "1168000"
  },
  {
    "text": "have how many of them do you need and what is the cost and if you do that analysis for all the the note types",
    "start": "1168000",
    "end": "1173200"
  },
  {
    "text": "available to you we pick the lowest cost one and make that recommendation the specific thing I'll point you to is just",
    "start": "1173200",
    "end": "1178520"
  },
  {
    "text": "towards the middle of the slide we're looking at um I don't know if you can see that vcps per node Ram GP per node",
    "start": "1178520",
    "end": "1184480"
  },
  {
    "text": "if you're naive like we were before we did a a careful analysis here uh that's the sticker price that we have just",
    "start": "1184480",
    "end": "1190320"
  },
  {
    "text": "debunked as something that you don't actually have access to allive for scheduling and so you might be able to",
    "start": "1190320",
    "end": "1195480"
  },
  {
    "text": "work around this by simply saying oh I need overhead and you don't really think about it it but in terms of the sophistication of picking nodes for your",
    "start": "1195480",
    "end": "1201640"
  },
  {
    "text": "cluster if you're not taking overhead into account you're not picking the right nodes uh to schedule your",
    "start": "1201640",
    "end": "1207960"
  },
  {
    "text": "workloads so how do we go about adding overhead into this algorithm and making it more aware of reality I guess you",
    "start": "1207960",
    "end": "1215640"
  },
  {
    "text": "could say right uh I guess to summarize the biggest challenge here is we have sort of forces that are in ttention here",
    "start": "1215640",
    "end": "1222039"
  },
  {
    "text": "right uh you naturally don't want to choose the biggest node you can if it's going to be sitting idle so that pulls",
    "start": "1222039",
    "end": "1227880"
  },
  {
    "text": "us toward smaller nodes right you don't want to pay for what you're not using but what's pulling us in the other direction is these smaller nodes you are",
    "start": "1227880",
    "end": "1235520"
  },
  {
    "text": "paying a higher you know cost of doing business essentially right that's not even available to you so you know we we",
    "start": "1235520",
    "end": "1242400"
  },
  {
    "text": "have a few thoughts here of like well you know you can't take this to the extreme and say okay we're going to go",
    "start": "1242400",
    "end": "1247600"
  },
  {
    "text": "to an 800% larger node uh unless you can use it right you're you're losing a lot",
    "start": "1247600",
    "end": "1254480"
  },
  {
    "text": "of money there right just pay 800% more just to save 10% right uh unless you can",
    "start": "1254480",
    "end": "1260360"
  },
  {
    "text": "use that capacity right so to sort of extrapolate this if you are using say dozens or hundreds or thousands of small",
    "start": "1260360",
    "end": "1267799"
  },
  {
    "text": "nodes then you could potentially consolidate those into fewer larger nodes subject to your availability",
    "start": "1267799",
    "end": "1273960"
  },
  {
    "text": "requirements right that's an important consideration here cost aside however if you're able to successfully do that and",
    "start": "1273960",
    "end": "1280440"
  },
  {
    "text": "meet your availability constraints um you can see some significant savings here right um yeah just a note on like a",
    "start": "1280440",
    "end": "1288240"
  },
  {
    "text": "few percent can make a huge impact on the bottom line right node costs are generally the largest driver of",
    "start": "1288240",
    "end": "1294640"
  },
  {
    "text": "kubernetes expenditures we found um and just in general right when picking node",
    "start": "1294640",
    "end": "1300240"
  },
  {
    "text": "sizes it might be better to just bias towards larger nodes because if the reasoning being well you know if I have",
    "start": "1300240",
    "end": "1307400"
  },
  {
    "text": "to pay for this capacity I might as well have it usable and available to me if I need to scale quickly or you know",
    "start": "1307400",
    "end": "1313440"
  },
  {
    "text": "natural growth versus go with smaller nodes and just sort of give that up",
    "start": "1313440",
    "end": "1320039"
  },
  {
    "text": "so here we sort of revised our algorithm to incorporate these these findings",
    "start": "1320559",
    "end": "1325799"
  },
  {
    "text": "right we've highlighted the the key changes that we made from this uh algorithm that Mel reviewed earlier um",
    "start": "1325799",
    "end": "1332080"
  },
  {
    "text": "so what we do now is before we even begin calculating uh our optimal node",
    "start": "1332080",
    "end": "1337720"
  },
  {
    "text": "sizes we use the data that we obtained through this study to fit a trend line",
    "start": "1337720",
    "end": "1342840"
  },
  {
    "text": "to it right in general inputs being number of CPU cores or bytes of r",
    "start": "1342840",
    "end": "1348960"
  },
  {
    "text": "and uh out we get sort of the amount of overhead in in general and a line of",
    "start": "1348960",
    "end": "1354600"
  },
  {
    "text": "best fit we compute the same set of requirements right uh maximum individual",
    "start": "1354600",
    "end": "1360360"
  },
  {
    "text": "pod requirements for node sizing our total workload requirements uh and then",
    "start": "1360360",
    "end": "1365400"
  },
  {
    "text": "any demon sets right that need to that need to run one per node um then for each node that on a given cloud provider",
    "start": "1365400",
    "end": "1373080"
  },
  {
    "text": "uh First We Begin by uh Computing the amount of overhead again using our best",
    "start": "1373080",
    "end": "1378720"
  },
  {
    "text": "fit line right it's a heuristic but we compute uh the amount that we're going to Discount the resources by so we have",
    "start": "1378720",
    "end": "1386080"
  },
  {
    "text": "here our our algorithm for computing that resource so we again pass the",
    "start": "1386080",
    "end": "1392320"
  },
  {
    "text": "sticker the name plate capacity into our function outcomes the amount of expected",
    "start": "1392320",
    "end": "1398600"
  },
  {
    "text": "overhead and then we also have in our algorith built in uh an overhead penalty",
    "start": "1398600",
    "end": "1403760"
  },
  {
    "text": "right and this is configurable we default it to 5% but it's configurable Able by the user to discriminate against",
    "start": "1403760",
    "end": "1411760"
  },
  {
    "text": "higher overhead nodes right and amplify the impact of the overhead um for for",
    "start": "1411760",
    "end": "1419320"
  },
  {
    "text": "efficiency so then we reduce the nodes available resources by that calculated",
    "start": "1419320",
    "end": "1425240"
  },
  {
    "text": "overhead amount so now when we contemplate our largest pod size we can",
    "start": "1425240",
    "end": "1431039"
  },
  {
    "text": "use our actual allocatable capacity to make that recommendation right so we",
    "start": "1431039",
    "end": "1436559"
  },
  {
    "text": "won't attempt to fit a pod requesting exactly two vcpus on an instance with",
    "start": "1436559",
    "end": "1442400"
  },
  {
    "text": "two vcpus of capacity then as before in a prior algorithm we sort of determin the",
    "start": "1442400",
    "end": "1448880"
  },
  {
    "text": "minimum number of nodes required to satisfy your total workload amount um and then again we have inputs for any",
    "start": "1448880",
    "end": "1456120"
  },
  {
    "text": "availability concerns and here generally we rely on the the user to supply those right it's so application dependent and",
    "start": "1456120",
    "end": "1463400"
  },
  {
    "text": "then as before compute the total cost we return that back to the user but this",
    "start": "1463400",
    "end": "1468679"
  },
  {
    "text": "time we have additional information where again we surface to the user the total amount of overhead for this",
    "start": "1468679",
    "end": "1476080"
  },
  {
    "text": "hypothetical cluster if they were to pick this node size uh again just visibility right is sort of the first",
    "start": "1476080",
    "end": "1481480"
  },
  {
    "text": "step and just being aware of it is uh important here and and can help with",
    "start": "1481480",
    "end": "1486520"
  },
  {
    "text": "decision making so finally um to close out the",
    "start": "1486520",
    "end": "1492360"
  },
  {
    "text": "main objective here was first just to raise awareness we know that many very sophisticated uh users of kubernetes at",
    "start": "1492360",
    "end": "1498240"
  },
  {
    "text": "scale totally understand this problem they they have all sorts of other uh configuration problems they run into but",
    "start": "1498240",
    "end": "1503320"
  },
  {
    "text": "they they know about this but a lot of people don't um you'll tell them a cubet",
    "start": "1503320",
    "end": "1508360"
  },
  {
    "text": "needs space and they say yeah that makes sense but they don't really put a dollar figure that to that or a scheduling uh",
    "start": "1508360",
    "end": "1513559"
  },
  {
    "text": "penalty on that so first we wanted to just highlight the problem we were very curious about the results of the empirical study um I I found them very",
    "start": "1513559",
    "end": "1520640"
  },
  {
    "text": "enlightening personally um but the the overall conclusion is that nothing is free and you do have to pay a cost for running kubernetes but there are real",
    "start": "1520640",
    "end": "1527200"
  },
  {
    "text": "ways that you can optimize that um we've seen you know 50 plus% penalty we saw also that was kind of an outlier we",
    "start": "1527200",
    "end": "1533600"
  },
  {
    "text": "often see more like 10 to 20% uh in the smaller node sizes and then as you get larger and larger we're getting to single digit percents of overhead which",
    "start": "1533600",
    "end": "1540159"
  },
  {
    "text": "is mostly negligible though at scale it can matter um obviously uh we want to",
    "start": "1540159",
    "end": "1546080"
  },
  {
    "text": "optimize and reduce this waste so you might say well let's just pick as Alex said one or two very large nodes there",
    "start": "1546080",
    "end": "1551840"
  },
  {
    "text": "are availability trade-offs there that you should be aware of and be careful of um a larger number of nodes can be more",
    "start": "1551840",
    "end": "1558200"
  },
  {
    "text": "resilient to degradation of nodes or simple failure of nodes so it's always important to weigh that in your",
    "start": "1558200",
    "end": "1564760"
  },
  {
    "text": "environment but there's real real cost savings available at larger node sizes um we also briefly went through our our",
    "start": "1564760",
    "end": "1570520"
  },
  {
    "text": "revised node sizing algorithm uh about how we can incorporate overhead and of course I have to plug our product in a",
    "start": "1570520",
    "end": "1576559"
  },
  {
    "text": "single line which is just install Cube cost uh it's free how install you can try it out uh and then uh yeah you use",
    "start": "1576559",
    "end": "1583840"
  },
  {
    "text": "that uh algorithm that Alex and I just talked about um thank for your time uh you can come see us uh at Q cost Booth",
    "start": "1583840",
    "end": "1590919"
  },
  {
    "text": "M10 uh tomorrow I know it's kind of late today uh and there's also an open cost Booth uh at F34 thank you for your time",
    "start": "1590919",
    "end": "1597120"
  },
  {
    "text": "thanks",
    "start": "1597120",
    "end": "1599360"
  },
  {
    "text": "everybody uh we can take questions now and I know there's a microphone or you can come up if you",
    "start": "1602919",
    "end": "1609440"
  },
  {
    "text": "like um hi um I had a question about how exactly do you measure this overhead",
    "start": "1616279",
    "end": "1623279"
  },
  {
    "text": "right for example you mentioned that it could be the host OS overhead but uh if",
    "start": "1623279",
    "end": "1629760"
  },
  {
    "text": "I ask for 1 gig and I'm using only 500 uh mag then the OS is free to use the",
    "start": "1629760",
    "end": "1635440"
  },
  {
    "text": "other 500 megabytes to actually help me in keeping page cash Etc so it may seem like I have 50% overhead but actually",
    "start": "1635440",
    "end": "1642919"
  },
  {
    "text": "there's no overhead so I'm curious about if I have a workload how exactly do I",
    "start": "1642919",
    "end": "1648520"
  },
  {
    "text": "measure the memory overhead so the workload itself which uh",
    "start": "1648520",
    "end": "1654840"
  },
  {
    "text": "in kubernetes land is is a pod uh doesn't have overhead um the the",
    "start": "1654840",
    "end": "1660159"
  },
  {
    "text": "workload itself uses resources on the Node uh they cost you money um open costs can tell you how much money that",
    "start": "1660159",
    "end": "1666360"
  },
  {
    "text": "costs you um overhead is a is a trait of the node itself we can't tell you how much of that is the the host OS uh we",
    "start": "1666360",
    "end": "1673840"
  },
  {
    "text": "can't tell you how much of that is cubet uh that's set as Alex said uh as a runtime parameter to cubet so if you're",
    "start": "1673840",
    "end": "1681039"
  },
  {
    "text": "interested in sort of digging into that uh that that's more of like a your say",
    "start": "1681039",
    "end": "1686559"
  },
  {
    "text": "you have a managed provider I don't know if you use gke or eks or AKs or it doesn't matter what you use they're",
    "start": "1686559",
    "end": "1691799"
  },
  {
    "text": "choosing what that parameter to cubet should be and that's what determines what cubet tells the cluster its",
    "start": "1691799",
    "end": "1698600"
  },
  {
    "text": "available capacity is most people are setting requests above usage I hope for stability purposes unless they're really",
    "start": "1698600",
    "end": "1704480"
  },
  {
    "text": "redlining it and so if you know the that allocatable space is really what matters",
    "start": "1704480",
    "end": "1710159"
  },
  {
    "text": "to the scheduler and thus to things like autoscalers and things like that does that answer the question yeah yeah yeah",
    "start": "1710159",
    "end": "1716240"
  },
  {
    "text": "thank you so uh there's a recommendation by",
    "start": "1716240",
    "end": "1722679"
  },
  {
    "text": "instance family right and would you just hold the mic up yeah so there's a recommendation by instance family right",
    "start": "1722679",
    "end": "1728760"
  },
  {
    "text": "and um so like U when there's like uh some types of instances which where the",
    "start": "1728760",
    "end": "1734120"
  },
  {
    "text": "workloads cannot run let's say maybe like this graviton instances where where uh uh uh the workloads cannot run right",
    "start": "1734120",
    "end": "1741200"
  },
  {
    "text": "like maybe they're not compatible uh with those kind of uh graviton instances is there a way like uh we could just use",
    "start": "1741200",
    "end": "1747720"
  },
  {
    "text": "a subset of instance family types where we could get a recommendation on uh sure",
    "start": "1747720",
    "end": "1753159"
  },
  {
    "text": "so you're talking about uh if I can rephrase is there a way to look at a",
    "start": "1753159",
    "end": "1758559"
  },
  {
    "text": "subset of families when recommending cluster sizes yeah um that is a thing",
    "start": "1758559",
    "end": "1764240"
  },
  {
    "text": "that Cube cost does not currently do um but that's sounds like an excellent idea cuz I know like when when we're looking",
    "start": "1764240",
    "end": "1771000"
  },
  {
    "text": "at um like a graph like this like by family or something like that like this is a very general like hey all CPU cores",
    "start": "1771000",
    "end": "1779000"
  },
  {
    "text": "are equal uh we know this is not true um people PID people want particular",
    "start": "1779000",
    "end": "1784320"
  },
  {
    "text": "instance families either due to you know specific Hardware availability specific CPU architecture whatever it may be uh",
    "start": "1784320",
    "end": "1791880"
  },
  {
    "text": "and and so that that becomes a real Factor but uh we'd like to do that we do not do it today okay thank thank",
    "start": "1791880",
    "end": "1799240"
  },
  {
    "text": "you hey uh great product uh so question is uh we mostly run on Prem but we're",
    "start": "1799240",
    "end": "1806039"
  },
  {
    "text": "making our journey slowly to the cloud is there a model that you'll have to say hey analyze my kubernetes clusters on",
    "start": "1806039",
    "end": "1813799"
  },
  {
    "text": "Prem make a recommendation on hey your nodes would best you know this is the family of nodes you'd best run on if",
    "start": "1813799",
    "end": "1820679"
  },
  {
    "text": "you're e right that's a really interesting idea um I have not heard of",
    "start": "1820679",
    "end": "1825799"
  },
  {
    "text": "that use case before it's not something we support out of the box in the product um I bet we could figure something out",
    "start": "1825799",
    "end": "1831559"
  },
  {
    "text": "for you is all I could say like say hi um afterward yeah all right thanks",
    "start": "1831559",
    "end": "1838080"
  },
  {
    "text": "yeah okay yeah um I wanted to clarify um because on resources on the Node I know",
    "start": "1838080",
    "end": "1845320"
  },
  {
    "text": "like in the deployments you can have like the limits and the requests so I want have to know like during the study",
    "start": "1845320",
    "end": "1850519"
  },
  {
    "text": "which one took um consideration in the um overhead um calcul ations um yeah I",
    "start": "1850519",
    "end": "1860639"
  },
  {
    "text": "think we generally use requests uh in terms of calculating overhead so so",
    "start": "1860639",
    "end": "1866880"
  },
  {
    "text": "overhead is a property of the node itself right workloads have requests nodes don't have requests um but when",
    "start": "1866880",
    "end": "1874080"
  },
  {
    "text": "you talk about the scheduler usually the first thing you think about is is requests because the scheduler says I",
    "start": "1874080",
    "end": "1879679"
  },
  {
    "text": "need to place a workload on a node that has the allocatable capacity uh that can",
    "start": "1879679",
    "end": "1885960"
  },
  {
    "text": "fit this pods request like that's where we look first I'm um a novice at",
    "start": "1885960",
    "end": "1891080"
  },
  {
    "text": "scheduler uh exact Behavior I know there are some things that will take limit into account um but most people when",
    "start": "1891080",
    "end": "1896760"
  },
  {
    "text": "they think about scheduling workloads think about requests and the way the schedule generally looks at it is is",
    "start": "1896760",
    "end": "1901840"
  },
  {
    "text": "there an allocatable space to fit the request of the Pod uh so the overhead is what's reducing the capacity that",
    "start": "1901840",
    "end": "1908720"
  },
  {
    "text": "sticker price uh into what's actually allocatable by kubernetes for your workload since requests are like a",
    "start": "1908720",
    "end": "1914600"
  },
  {
    "text": "minimum right it's like I need a min minimum this this run right whereas a",
    "start": "1914600",
    "end": "1919799"
  },
  {
    "text": "limit is sort of like an aspirational maximum I guess so we sort of went with the lowest common denominator there okay",
    "start": "1919799",
    "end": "1927600"
  },
  {
    "text": "um and uh as a followup because you noted that you also provide node recommendations is that are also based",
    "start": "1927600",
    "end": "1932880"
  },
  {
    "text": "on the requests or the limits um for the scheduling aspect s so that that's based",
    "start": "1932880",
    "end": "1938880"
  },
  {
    "text": "on both request and usage so um if your",
    "start": "1938880",
    "end": "1944159"
  },
  {
    "text": "usage exceeds your request you probably this is not always the case but the general assumption is that you actually",
    "start": "1944159",
    "end": "1950679"
  },
  {
    "text": "need that capacity even if you're not requesting it for example if you exceed memory request if we were to super tight",
    "start": "1950679",
    "end": "1956919"
  },
  {
    "text": "sizee your cluster and you had a bunch of workloads over using memory above the request they would die so we look at both requests and usage pick the larger",
    "start": "1956919",
    "end": "1964039"
  },
  {
    "text": "of the two to make sure that there is not only space to schedule the workloads but also to run them without failure u",
    "start": "1964039",
    "end": "1969880"
  },
  {
    "text": "based on historical Behavior okay thank you you're",
    "start": "1969880",
    "end": "1974880"
  },
  {
    "text": "welcome I'll pref face this by saying I might have missed something but uh do",
    "start": "1975399",
    "end": "1980440"
  },
  {
    "text": "you guys have a way with open cost to kind of factor in Damon sets as like part of the overhead the of a",
    "start": "1980440",
    "end": "1988639"
  },
  {
    "text": "node so that comes in more as this is an very interesting question um this is",
    "start": "1988639",
    "end": "1994919"
  },
  {
    "text": "like if you look at something like Cube system it sort of falls into the same category right um this is a cost of",
    "start": "1994919",
    "end": "2000600"
  },
  {
    "text": "doing business as sort of Alex described it but it doesn't fall under that specifically capacity minus allocatable",
    "start": "2000600",
    "end": "2007120"
  },
  {
    "text": "category um so in both open cost and Cube cost uh the way we treat this is as a shared cost usually um some teams like",
    "start": "2007120",
    "end": "2014279"
  },
  {
    "text": "to allocate all of cube system and then anything under it uh to like a specific",
    "start": "2014279",
    "end": "2019760"
  },
  {
    "text": "uh Team maybe their infer team and some share it among all the tenants of the cluster that's just like a a it's it's",
    "start": "2019760",
    "end": "2025840"
  },
  {
    "text": "up to you kind of um but we don't think of it in terms of specifically overhead",
    "start": "2025840",
    "end": "2031279"
  },
  {
    "text": "as we've defined it in the talk like it is overhead certainly of of doing business but it's not node overhead in",
    "start": "2031279",
    "end": "2036600"
  },
  {
    "text": "terms of capacity minus allocatable so it falls under a different bucket and it's tracked as a pod so you do you know",
    "start": "2036600",
    "end": "2042279"
  },
  {
    "text": "kubernetes native reporting on it uh rather than something a little more complicated with with node overhead you",
    "start": "2042279",
    "end": "2047600"
  },
  {
    "text": "generally have more control over what's running in the cube system Nam space right when you spin up an eks cluster with managed nodes like the the",
    "start": "2047600",
    "end": "2054358"
  },
  {
    "text": "allocatable is set by whatever the platform team you know is using to calculate that right whereas with Cube",
    "start": "2054359",
    "end": "2060520"
  },
  {
    "text": "system you can play with those resource requests at your own risk right but at least it's available to you as an API",
    "start": "2060520",
    "end": "2067878"
  },
  {
    "text": "user at the K API yeah the the I guess the use case here is potentially like if",
    "start": "2067879",
    "end": "2073960"
  },
  {
    "text": "I'm running data dog agents I'm also running uh EFS CSI drivers EBS CSI",
    "start": "2073960",
    "end": "2080040"
  },
  {
    "text": "drivers and a whole bunch of other stuff then all of a sudden like oh well how much of my actual instance is still left",
    "start": "2080040",
    "end": "2086520"
  },
  {
    "text": "for my actual workloads and not just operational overhead oh for sure yeah so",
    "start": "2086520",
    "end": "2091560"
  },
  {
    "text": "when it in when it comes to specifically like recommending nodes when we get away from simp simply just measuring overhead",
    "start": "2091560",
    "end": "2098160"
  },
  {
    "text": "but our algorithm for recommending what node you should be running we're taking out not just overhead now as as Alex",
    "start": "2098160",
    "end": "2104640"
  },
  {
    "text": "described but we have always been taking away that Damon set capacity uh under the assumption that they're running on",
    "start": "2104640",
    "end": "2110440"
  },
  {
    "text": "your nodes because uh that is effectively unallocate capacity we just measure it differently because they're",
    "start": "2110440",
    "end": "2117040"
  },
  {
    "text": "pods not this weird fixed overhead thing uh based on no type thank you you're",
    "start": "2117040",
    "end": "2122480"
  },
  {
    "text": "welcome any other questions hey thank you you um I'd like",
    "start": "2122480",
    "end": "2129040"
  },
  {
    "text": "to do the same kind of study on on premise um would would you like to share",
    "start": "2129040",
    "end": "2135240"
  },
  {
    "text": "the methodology that you use to to do that on on the cloud provider to share the ra data and maybe could plot also",
    "start": "2135240",
    "end": "2142200"
  },
  {
    "text": "the result of un premise analysis and see if the UN premise how it performs compared to the",
    "start": "2142200",
    "end": "2149359"
  },
  {
    "text": "cloud provider so we we do intend to open source uh the scripts and things we",
    "start": "2149359",
    "end": "2155920"
  },
  {
    "text": "use and actually probably the experimental data as well um very soon we had every intention of having it done",
    "start": "2155920",
    "end": "2162160"
  },
  {
    "text": "by this talk but uh you know there's a lot of support for that so um yeah it",
    "start": "2162160",
    "end": "2167319"
  },
  {
    "text": "would be very interesting to see the findings on an on-prem system because largely you're in control at that point",
    "start": "2167319",
    "end": "2173040"
  },
  {
    "text": "right like you're the you're the person passing the arguments into the cuet right and you decide what your overhead",
    "start": "2173040",
    "end": "2178440"
  },
  {
    "text": "is um so it might even be more interesting in that use case to you know validate your assumptions about what",
    "start": "2178440",
    "end": "2184119"
  },
  {
    "text": "your underlying System is using versus you know what you think it needs to run cases okay thank",
    "start": "2184119",
    "end": "2192440"
  },
  {
    "text": "you do you know if the uh overhead uh increase with the amount of PODS running",
    "start": "2193000",
    "end": "2199680"
  },
  {
    "text": "on the nodes is it some sort of fixed Reserve uh capacity and it's",
    "start": "2199680",
    "end": "2207960"
  },
  {
    "text": "going to increase eventually or it's really more uh linear scale it's fixed",
    "start": "2207960",
    "end": "2214440"
  },
  {
    "text": "at startup time so when you uh spin up a node and then you spin up cubet on that node as Alex described uh a little while",
    "start": "2214440",
    "end": "2221800"
  },
  {
    "text": "ago uh I think um you pass it a flag that says this is the uh allocatable",
    "start": "2221800",
    "end": "2227119"
  },
  {
    "text": "capacity that you should report so it's not going to scale with the number of pods on the Node like as the node gets busier um it's not going to uh increase",
    "start": "2227119",
    "end": "2236319"
  },
  {
    "text": "the amount of overhead it has it's just fixed there I I I assumably under the assumption that uh that it's going to",
    "start": "2236319",
    "end": "2243160"
  },
  {
    "text": "reach whatever Max Capacity the cubet is capable of I know like the defaults like 110 pods per node or something like that",
    "start": "2243160",
    "end": "2249880"
  },
  {
    "text": "U I know that's configurable too if you want to have fun um but yeah it's it's fully fixed for the lifetime of of the",
    "start": "2249880",
    "end": "2255599"
  },
  {
    "text": "cuet process okay thanks yeah you're",
    "start": "2255599",
    "end": "2260440"
  },
  {
    "text": "welcome I'm back again so um so when you show the efficiency and the the dollar",
    "start": "2260720",
    "end": "2266680"
  },
  {
    "text": "amount like you know how much it is used right so uh for a specific workload or by your name space uh is it the",
    "start": "2266680",
    "end": "2273119"
  },
  {
    "text": "recommendation right so is it by the average of CPU and memory or like is it like the max whichever is the max of",
    "start": "2273119",
    "end": "2280200"
  },
  {
    "text": "both or like um because often times let's say if I take an instance type like uh CF right so although like the",
    "start": "2280200",
    "end": "2287680"
  },
  {
    "text": "computer is less but I have a lot of more or like like an M5 for example like there's a lot more memory than the CPU",
    "start": "2287680",
    "end": "2293119"
  },
  {
    "text": "consumption right so although there's more memory wastage but uh I'm at Max at",
    "start": "2293119",
    "end": "2298640"
  },
  {
    "text": "CPU so that might not be very accurate if I'm taking a Max or an average right",
    "start": "2298640",
    "end": "2305040"
  },
  {
    "text": "so what this chart shows is it's the cost weighted so if your node has a lot of ram right the ram cost will be",
    "start": "2305040",
    "end": "2312599"
  },
  {
    "text": "significantly higher than the CPU right and so if your RAM percentage of",
    "start": "2312599",
    "end": "2318079"
  },
  {
    "text": "overhead is you know whatever it's Amplified right because Ram is such a big component of the node it's not",
    "start": "2318079",
    "end": "2324359"
  },
  {
    "text": "necessarily an attribute of like the kubernetes name space or the workload it's more of an attribute of the",
    "start": "2324359",
    "end": "2329760"
  },
  {
    "text": "underlying Noe it's sort of one level lower than that but in terms of of",
    "start": "2329760",
    "end": "2335440"
  },
  {
    "text": "profiling the workload loads to recommend node sizes um most of the time we're looking at Max uh information",
    "start": "2335440",
    "end": "2342520"
  },
  {
    "text": "because that's conservative I know it's a little controversial um we like to prioritize availability recommendations",
    "start": "2342520",
    "end": "2348160"
  },
  {
    "text": "until you tell us more um which is why we tend to look at at Max over averages because averages hide um spiky Behavior",
    "start": "2348160",
    "end": "2354960"
  },
  {
    "text": "which is particularly dangerous with memory uh and getting out of memory and dying right yeah yeah I think like uh",
    "start": "2354960",
    "end": "2361280"
  },
  {
    "text": "Max probably makes more sense but sometimes where like you have like U like let's say M uh type of instances",
    "start": "2361280",
    "end": "2368960"
  },
  {
    "text": "right like there's a lot more memory available than CPU but if your applications are maxing on CPU already",
    "start": "2368960",
    "end": "2374599"
  },
  {
    "text": "that's oh yeah so yeah so we look at the resource requirements independently right CPU is one resource memory is one",
    "start": "2374599",
    "end": "2380520"
  },
  {
    "text": "resource so there might be some no types that simply um are a better fit right",
    "start": "2380520",
    "end": "2386400"
  },
  {
    "text": "we'll might give you a node recommendation that has uh less memory and the same or more amount of CPU uh",
    "start": "2386400",
    "end": "2392920"
  },
  {
    "text": "simply because it's it's a more efficient use of resources right like we saw that that in memory in particular",
    "start": "2392920",
    "end": "2398000"
  },
  {
    "text": "right there's like a lot of overhead we're paying for that so perhaps that might be might be an optimization there does the recommendation Today Show like",
    "start": "2398000",
    "end": "2404520"
  },
  {
    "text": "a homogeneous node type or like is it like what sh uh you do two of this type",
    "start": "2404520",
    "end": "2409599"
  },
  {
    "text": "two of this type that kind of uh the the primary recommendation is homogeneous but there is a recommendation that we'll",
    "start": "2409599",
    "end": "2415000"
  },
  {
    "text": "pick two or three different node types just to try to pick a different uh method it's obviously not like super",
    "start": "2415000",
    "end": "2420920"
  },
  {
    "text": "optimal in terms of picking every possible combination because that can get kind of expensive uh in terms of",
    "start": "2420920",
    "end": "2426400"
  },
  {
    "text": "computation but but we do a best effort uh attempt to pick a heterogenous set of",
    "start": "2426400",
    "end": "2431720"
  },
  {
    "text": "nodes but for the for the scope of this presentation we just did our single node sizing algorithm okay yeah we started",
    "start": "2431720",
    "end": "2438599"
  },
  {
    "text": "using Cube cost like uh pretty recently so uh that's where we come for this back yeah we'd love to talk more if if you'd",
    "start": "2438599",
    "end": "2444480"
  },
  {
    "text": "like at the end yeah do that thanks hey great talk I I wanted to",
    "start": "2444480",
    "end": "2451599"
  },
  {
    "text": "follow up on that cost of doing business question that came up a couple of questions ago",
    "start": "2451599",
    "end": "2456920"
  },
  {
    "text": "and so you're you're taking a look at overhead as uh to run the virtual machine I was wondering if you noticed",
    "start": "2456920",
    "end": "2463160"
  },
  {
    "text": "any trends for what's running in CP system Damon sets that were applicable",
    "start": "2463160",
    "end": "2469800"
  },
  {
    "text": "across dros like does eks consume more",
    "start": "2469800",
    "end": "2474960"
  },
  {
    "text": "overhead in running its pods necessary for running the system versus AKs versus",
    "start": "2474960",
    "end": "2480520"
  },
  {
    "text": "gke did did you notice anything when you dug Beyond just the difference between",
    "start": "2480520",
    "end": "2485680"
  },
  {
    "text": "allocat versus uh what was available for the node so we we didn't uh we didn't look",
    "start": "2485680",
    "end": "2493440"
  },
  {
    "text": "at the actual uh contents of the cube system namespace we sort of put that out of scope and made it more of a study of",
    "start": "2493440",
    "end": "2500079"
  },
  {
    "text": "the underlying note itself but that is a really interesting uh insight there and",
    "start": "2500079",
    "end": "2505160"
  },
  {
    "text": "like definitely is worth consideration right because if a certain provider you",
    "start": "2505160",
    "end": "2510560"
  },
  {
    "text": "know gets away with lower overhead as we measure it because they move a lot in",
    "start": "2510560",
    "end": "2516280"
  },
  {
    "text": "into or out of our definition of overhead then yeah you're not getting uh",
    "start": "2516280",
    "end": "2521839"
  },
  {
    "text": "everything but so we made a little bit of an assumption that you know all the cloud providers even though the services are different they sort of you know were",
    "start": "2521839",
    "end": "2529520"
  },
  {
    "text": "were approximately which we know is not the case because everyone's managing their own flavor uh derivative uh I I'm",
    "start": "2529520",
    "end": "2535640"
  },
  {
    "text": "very interested I'll log in under future work for sure and we also have choices to make like well what CSI driver or",
    "start": "2535640",
    "end": "2541599"
  },
  {
    "text": "drivers are reusing because then that Damon set will affect the amount of allocation able space not allocable from",
    "start": "2541599",
    "end": "2549119"
  },
  {
    "text": "kuet from what you're putting up here but what we can put on in terms of workload pods right then the nice thing",
    "start": "2549119",
    "end": "2555040"
  },
  {
    "text": "is that's easier to measure um because it's I mean you use a tool like open cost or cube cost and because it lives",
    "start": "2555040",
    "end": "2560920"
  },
  {
    "text": "in cluster all the native cluster monitoring tools are available to you so it's a little easier to keep track of that but the unified picture of both",
    "start": "2560920",
    "end": "2567720"
  },
  {
    "text": "Cube system and overhead is is very interesting um and what tools you pick but and so in open cost can you go in",
    "start": "2567720",
    "end": "2574440"
  },
  {
    "text": "and say all right include the pods from these name spaces as what we include is",
    "start": "2574440",
    "end": "2579559"
  },
  {
    "text": "kind of overhead or is that yeah so we would call that like a shared cost um and and you could share that cost among",
    "start": "2579559",
    "end": "2587040"
  },
  {
    "text": "whatever aggregation you do so you say I want to share this among share the cube system Nam space among all other name",
    "start": "2587040",
    "end": "2593480"
  },
  {
    "text": "spaces for example because maybe this is a shared cost of doing business which is kind of what I described earlier other",
    "start": "2593480",
    "end": "2599400"
  },
  {
    "text": "teams will just say the infro team is responsible for cube system comes out of their budget um that's you know it's a",
    "start": "2599400",
    "end": "2605400"
  },
  {
    "text": "org by or kind of decision to make and it's it's flexibility of reporting there is what's important okay thank you yeah",
    "start": "2605400",
    "end": "2613880"
  },
  {
    "text": "absolutely how do can you uh switch to the middle graph uh which one the next",
    "start": "2613880",
    "end": "2620520"
  },
  {
    "text": "one I think yeah what why is there a dip there in the green line uh so I think it it went from such",
    "start": "2620520",
    "end": "2628359"
  },
  {
    "text": "a steep curve to such a shallow one it's probably just a shortcoming of our curve",
    "start": "2628359",
    "end": "2634000"
  },
  {
    "text": "fitting algorithm so yes there is no hidden 0% overhead instance type in",
    "start": "2634000",
    "end": "2640359"
  },
  {
    "text": "there I I think it's just trying to like make a polinomial curve okay yeah I I",
    "start": "2640359",
    "end": "2645480"
  },
  {
    "text": "hit I hit the the the fit line but button in the library I chose to graph things with and that's the shape it made",
    "start": "2645480",
    "end": "2652640"
  },
  {
    "text": "okay cool cuz there's a Green Dot on the blue line just above yeah anyway uh does",
    "start": "2652640",
    "end": "2659040"
  },
  {
    "text": "uh Cube Coster open cost report the cost of Cube cost absolutely yeah yeah yeah",
    "start": "2659040",
    "end": "2664200"
  },
  {
    "text": "and open cost report the cost of open cost and Cube cost will report the cost of Cube cost cool cool thank you you're",
    "start": "2664200",
    "end": "2669319"
  },
  {
    "text": "welcome any other",
    "start": "2669319",
    "end": "2672640"
  },
  {
    "text": "questions when you were showing um families for eks I I I wasn't able to",
    "start": "2674760",
    "end": "2683520"
  },
  {
    "text": "see all these it's very dense yeah I was wondering because you for gke you said the E2 family it was the reason the uh",
    "start": "2683520",
    "end": "2691880"
  },
  {
    "text": "overhead was so high is because the way they calculate CPU avail a ability because it's burstable and I was",
    "start": "2691880",
    "end": "2697680"
  },
  {
    "text": "wondering for the burstable types for AWS did you see something similar obviously the numbers weren't as high",
    "start": "2697680",
    "end": "2703720"
  },
  {
    "text": "but was that just an anomaly based on the way that gcp uh computes what's allocat or or or",
    "start": "2703720",
    "end": "2713240"
  },
  {
    "text": "reports that back to the kuet or is that true across all burstable types across Cloud",
    "start": "2713240",
    "end": "2718359"
  },
  {
    "text": "families I I think it's the way that gcp just happened to be passing that through",
    "start": "2718359",
    "end": "2723640"
  },
  {
    "text": "right so the platform team is configuring the allocatable capacity um if aws's burstable instance types like",
    "start": "2723640",
    "end": "2730040"
  },
  {
    "text": "are some multiple right you can burst to some multiple above the you know sort of Baseline number of cores we would expect",
    "start": "2730040",
    "end": "2736880"
  },
  {
    "text": "to see more outliers of 50% plus 75% overhead uh we didn't observe that uh so",
    "start": "2736880",
    "end": "2746319"
  },
  {
    "text": "yeah we we again we we studied a few hundred instance types so we didn't exactly do a direct comparison between",
    "start": "2746319",
    "end": "2752559"
  },
  {
    "text": "the AWS instance burstable instances and the gcp uh but definitely great followon work",
    "start": "2752559",
    "end": "2759559"
  },
  {
    "text": "yeah and and based on the numbers I mean we're seeing even here um my my suspicion is that AWS simply reports",
    "start": "2759559",
    "end": "2767160"
  },
  {
    "text": "capacity differently for those node types and so it just doesn't show up as markedly because like you know gcp shows",
    "start": "2767160",
    "end": "2774240"
  },
  {
    "text": "the sticker price two CPU 4 gigs of RAM obviously like you know the allocatable is one Alex said you're not getting",
    "start": "2774240",
    "end": "2780359"
  },
  {
    "text": "ripped off but like you might feel like you are I suspect that that AWS is just reporting things differently at the OS",
    "start": "2780359",
    "end": "2786640"
  },
  {
    "text": "or cubet level so it doesn't show up here butth follow you say that gcps way",
    "start": "2786640",
    "end": "2792040"
  },
  {
    "text": "is arguably more accurate right because limits will let you over subscribe right so you can over subscribe above your",
    "start": "2792040",
    "end": "2798040"
  },
  {
    "text": "allocatable so you know you can like through this mation right you can",
    "start": "2798040",
    "end": "2804640"
  },
  {
    "text": "leverage that bursting if your workload bursts um but again it it shows up weird",
    "start": "2804640",
    "end": "2809760"
  },
  {
    "text": "and maybe the AWS product team made a different decision thank you",
    "start": "2809760",
    "end": "2817799"
  },
  {
    "text": "all right we'll call it there thank you for coming everybody",
    "start": "2817839",
    "end": "2823280"
  }
]