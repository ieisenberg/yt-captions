[
  {
    "start": "0",
    "end": "95000"
  },
  {
    "text": "hello and welcome I'm Andy Goldstein I'm Steve Chris and we are here to talk to",
    "start": "319",
    "end": "6150"
  },
  {
    "text": "you about disaster recovery for your kubernetes clusters a little bit about ourselves I'm a staff systems engineer",
    "start": "6150",
    "end": "13349"
  },
  {
    "text": "at hep tio I have been programming for a long time I started with Commodore 64 basic and I'm now up using go and I've",
    "start": "13349",
    "end": "21630"
  },
  {
    "text": "been a kubernetes contributor since 2014 and I am the hub do work team lead and",
    "start": "21630",
    "end": "27810"
  },
  {
    "text": "I'm Steeve Chris I'm a Senior Systems Engineer at hep Tia I work on hep do arc with Andy and I've been in the past a",
    "start": "27810",
    "end": "35160"
  },
  {
    "text": "contributor to upstream kubernetes and also a member of the release team and in a past life I was an enterprise IT",
    "start": "35160",
    "end": "42059"
  },
  {
    "text": "engineer so I certainly have some experience with the challenges of designing implementing and testing dr",
    "start": "42059",
    "end": "47760"
  },
  {
    "text": "strategies alright can I get a show of hands how many of you manage clusters in",
    "start": "47760",
    "end": "53399"
  },
  {
    "text": "production alright and how many of you have a disaster recovery strategy",
    "start": "53399",
    "end": "59780"
  },
  {
    "text": "excellent and how many of you have actually used it to recover from",
    "start": "59780",
    "end": "65309"
  },
  {
    "text": "something ok a few of you so let's talk about what is it that you want in your",
    "start": "65309",
    "end": "71549"
  },
  {
    "text": "IT infrastructure what you want is a collection of servers and services and",
    "start": "71549",
    "end": "76830"
  },
  {
    "text": "applications that are all running perfectly well you've got your monitoring in place and all of your",
    "start": "76830",
    "end": "82920"
  },
  {
    "text": "checkmarks are green because everything is running great but before we get to",
    "start": "82920",
    "end": "88829"
  },
  {
    "text": "that so you want to sleep soundly at night because everything is running",
    "start": "88829",
    "end": "94229"
  },
  {
    "text": "correctly but what actually happens well at some point no matter how good your",
    "start": "94229",
    "end": "99900"
  },
  {
    "start": "95000",
    "end": "135000"
  },
  {
    "text": "infrastructure is no matter how excellent your network is something is",
    "start": "99900",
    "end": "105030"
  },
  {
    "text": "gonna go wrong you'll probably get a page or a phone call or a slack",
    "start": "105030",
    "end": "110640"
  },
  {
    "text": "notification and you're gonna have to deal with it and in the short term",
    "start": "110640",
    "end": "116310"
  },
  {
    "text": "you're probably not going to be very happy but that's ok because we're going to give you some ideas around how you",
    "start": "116310",
    "end": "123060"
  },
  {
    "text": "can do disaster recovery for your kubernetes clusters so the first thing",
    "start": "123060",
    "end": "128310"
  },
  {
    "text": "you need to do is probably do some rebuilding and some of the tools that we're going to",
    "start": "128310",
    "end": "133410"
  },
  {
    "text": "today hopefully we'll help you with that so before we get into talking about dr4",
    "start": "133410",
    "end": "139260"
  },
  {
    "start": "135000",
    "end": "200000"
  },
  {
    "text": "kubernetes I want to do a quick review of what dr might look like in a more traditional IT setting so in the old",
    "start": "139260",
    "end": "146520"
  },
  {
    "text": "world we had a pretty strong correspondence between an app and a server and so typically we would deploy",
    "start": "146520",
    "end": "152250"
  },
  {
    "text": "a single app onto a single server now that application might be made up of multiple components and the server might",
    "start": "152250",
    "end": "159180"
  },
  {
    "text": "be virtual it might be physical but regardless there was a very strong correspondence between the application",
    "start": "159180",
    "end": "164910"
  },
  {
    "text": "and the server and so if we ever had a disaster and we needed to recover the",
    "start": "164910",
    "end": "169920"
  },
  {
    "text": "service for the application we need to be able to bring back the server with all of the same software configuration",
    "start": "169920",
    "end": "176760"
  },
  {
    "text": "and data as it had before and so typically the way that we would do this is we would take full backups of our",
    "start": "176760",
    "end": "182160"
  },
  {
    "text": "server on a regular basis a nightly basis typically and if we ever had a",
    "start": "182160",
    "end": "187260"
  },
  {
    "text": "disaster and the server went down we do a full restore from our backup bring up",
    "start": "187260",
    "end": "192270"
  },
  {
    "text": "a new server that was essentially identical to the old one and this would enable us to restore the service for our",
    "start": "192270",
    "end": "198180"
  },
  {
    "text": "application in the new world with kubernetes things look a little bit",
    "start": "198180",
    "end": "204330"
  },
  {
    "start": "200000",
    "end": "367000"
  },
  {
    "text": "different than that and so now when you're running a kubernetes cluster you don't just have one server you have one",
    "start": "204330",
    "end": "211590"
  },
  {
    "text": "or more masters which are running the kubernetes control plane you have many nodes which are running again some",
    "start": "211590",
    "end": "217650"
  },
  {
    "text": "kubernetes components as well as all of your containerized workloads and then you also have a net CD cluster which may",
    "start": "217650",
    "end": "224580"
  },
  {
    "text": "be running in or outside of your kubernetes cluster but this is actually storing all your your kubernetes state",
    "start": "224580",
    "end": "229590"
  },
  {
    "text": "information and so let's take a look at what's inside each of those a little bit more so within the master we have first",
    "start": "229590",
    "end": "238410"
  },
  {
    "text": "of all the kubernetes api server and this is the entry point for creating or",
    "start": "238410",
    "end": "244050"
  },
  {
    "text": "fetching information about kubernetes state we have a scheduler which is responsible for deciding which nodes",
    "start": "244050",
    "end": "251190"
  },
  {
    "text": "pods should run on we have a controller manager which is going to be running the core control loops to constantly push",
    "start": "251190",
    "end": "258180"
  },
  {
    "text": "the state of the kubernetes cluster towards the desired state and then we have NCD which is our persistent store",
    "start": "258180",
    "end": "264600"
  },
  {
    "text": "of state information kubernetes and then we also potentially have some CNI pods and a cube proxy",
    "start": "264600",
    "end": "270509"
  },
  {
    "text": "which are going to help with all the the networking and communication concerns and then on the nodes we also have cube",
    "start": "270509",
    "end": "278280"
  },
  {
    "text": "proxy and the CNI pods for networking beyond that we have all of your containerized workloads in the form of",
    "start": "278280",
    "end": "284639"
  },
  {
    "text": "pods and so as we start to think about how to design a dr strategy for for this",
    "start": "284639",
    "end": "291330"
  },
  {
    "text": "new kubernetes environment we really need to think about where is this state within this environment which components",
    "start": "291330",
    "end": "297660"
  },
  {
    "text": "are stateful and which are stateless and so if we think about where state is it's",
    "start": "297660",
    "end": "304320"
  },
  {
    "text": "really in two places within this system and the first is obviously at CD that CD",
    "start": "304320",
    "end": "309330"
  },
  {
    "text": "is the persistent store of all of the kubernetes state information contains all the specs for your deployments your",
    "start": "309330",
    "end": "315270"
  },
  {
    "text": "services your config maps and your secrets etc and the second is in your",
    "start": "315270",
    "end": "321180"
  },
  {
    "text": "persistent volumes for your applications so if you have workloads that are using volumes to store persistent data we",
    "start": "321180",
    "end": "327570"
  },
  {
    "text": "obviously have a lot of state here and so these are really the key components that we need to focus on and make sure",
    "start": "327570",
    "end": "333060"
  },
  {
    "text": "that we have robust backup strategies so that we can restore this data in the case of a failure",
    "start": "333060",
    "end": "339440"
  },
  {
    "text": "if we think about the masters and the nodes themselves that are running the core kubernetes components they're",
    "start": "339470",
    "end": "345240"
  },
  {
    "text": "really basically stateless and so this means that if we as long as we can quickly bring up new versions of those",
    "start": "345240",
    "end": "351900"
  },
  {
    "text": "in the case of a failure we don't really need to restore from an exact copy of the previous version of them we can spin",
    "start": "351900",
    "end": "358470"
  },
  {
    "text": "up a new cluster and as long as we can restore our @cd data and our persistent volume data we'll be able to restore",
    "start": "358470",
    "end": "364800"
  },
  {
    "text": "service to our applications so let's talk about master and no disaster",
    "start": "364800",
    "end": "371130"
  },
  {
    "start": "367000",
    "end": "511000"
  },
  {
    "text": "recovery like Steve said they're basically stateless you may have some of",
    "start": "371130",
    "end": "376680"
  },
  {
    "text": "these that are unhealthy maybe they're running okay for the most part but you're getting some alerts you've got a",
    "start": "376680",
    "end": "382949"
  },
  {
    "text": "disc that is flaky a network card that's not performing correctly so there's some",
    "start": "382949",
    "end": "388650"
  },
  {
    "text": "tools you can use to take these out of service cube control has a couple of features Gordon and Drain that you'll",
    "start": "388650",
    "end": "395789"
  },
  {
    "text": "probably want to add to your toolbox Gordon allows you to mark a No is unschedulable so any new pods that",
    "start": "395789",
    "end": "401670"
  },
  {
    "text": "are created will not be assigned to whatever node you've cordoned and drain",
    "start": "401670",
    "end": "407070"
  },
  {
    "text": "goes one step further and will actually evict any pods that are running on a node that you're trying to take out of",
    "start": "407070",
    "end": "413280"
  },
  {
    "text": "service and once you've done that as Steve said you want to very quickly be",
    "start": "413280",
    "end": "419460"
  },
  {
    "text": "able to provision a replacement master or node so how do you do that automate",
    "start": "419460",
    "end": "426210"
  },
  {
    "text": "it now unfortunately we are not going to be able to tell you the one and only one",
    "start": "426210",
    "end": "431460"
  },
  {
    "text": "way to automate recovering Andry provisioning a master or a node or a cluster because you all have opinions",
    "start": "431460",
    "end": "438000"
  },
  {
    "text": "you have IT departments who say we're using ansible or we're using chef where",
    "start": "438000",
    "end": "443340"
  },
  {
    "text": "we're using puppet so we can't tell you what to use but we are strongly encouraging you to automate the creation",
    "start": "443340",
    "end": "450660"
  },
  {
    "text": "of masters nodes and clusters one thing to keep in mind is that there is a teeny",
    "start": "450660",
    "end": "457320"
  },
  {
    "text": "tiny amount of state that is necessary to preserve and those are the certificates that are used for the",
    "start": "457320",
    "end": "462690"
  },
  {
    "text": "components in the cluster to talk to each other so when you're cubelets talk to the",
    "start": "462690",
    "end": "467760"
  },
  {
    "text": "master or to the API servers and when the controller manager talks to the API servers there typically are SSL",
    "start": "467760",
    "end": "474000"
  },
  {
    "text": "certificates that are used and these things you want to maintain and retain and incorporate into your automation so",
    "start": "474000",
    "end": "481560"
  },
  {
    "text": "that when you have your ansible or your chef or your puppet and you're using that to automate provisioning all of",
    "start": "481560",
    "end": "488190"
  },
  {
    "text": "these instances you want to bring your certificates with you so that you don't lose them if you do lose them you have",
    "start": "488190",
    "end": "493470"
  },
  {
    "text": "to regenerate them or get new ones and you potentially could have an outage in",
    "start": "493470",
    "end": "499020"
  },
  {
    "text": "that situation and I do want to highlight that recovering the masters and the nodes is not really the crux of",
    "start": "499020",
    "end": "506310"
  },
  {
    "text": "the disaster recovery problem stateful data is what we're really here about so",
    "start": "506310",
    "end": "512159"
  },
  {
    "text": "let's talk about EDD CD first there's a few different ways that you can approach disaster recovery for ED CD the first",
    "start": "512160",
    "end": "519990"
  },
  {
    "text": "two are similar at the block level you could take a backup of the partition or",
    "start": "519990",
    "end": "525780"
  },
  {
    "text": "the disk where the @ CD data directory resides this is where all of your EDD",
    "start": "525780",
    "end": "531660"
  },
  {
    "text": "State is and with something like this if you lose",
    "start": "531660",
    "end": "536810"
  },
  {
    "text": "one of your members if you have a highly available etsy D cluster and you definitely should if you restore from",
    "start": "536810",
    "end": "542930"
  },
  {
    "text": "either the block device or at the file system level you just take a backup of the directory and restore it when your",
    "start": "542930",
    "end": "549709"
  },
  {
    "text": "member comes back online it will get a delta of the data that happened in the",
    "start": "549709",
    "end": "557029"
  },
  {
    "text": "cluster since it was offline so the the surviving members will send a snapshot of what it needs to catch up and so your",
    "start": "557029",
    "end": "564529"
  },
  {
    "text": "cluster can become whole again another option is to use sed control they have a",
    "start": "564529",
    "end": "570620"
  },
  {
    "text": "great feature as part of the exit III API to be able to take a snapshot of your @cd database and at some point in",
    "start": "570620",
    "end": "579110"
  },
  {
    "text": "the future restore it that's someone you've got to be a little bit careful with though because if you do a snapshot and restore",
    "start": "579110",
    "end": "586339"
  },
  {
    "text": "when you restore it ends up creating a brand new at CD cluster so this",
    "start": "586339",
    "end": "592070"
  },
  {
    "text": "effectively means you will have an outage if you go this route but it's a good tool to have in the event that you",
    "start": "592070",
    "end": "598190"
  },
  {
    "text": "have a total total outage you can certainly recover some of your @cd state this way assuming you have backups and",
    "start": "598190",
    "end": "604180"
  },
  {
    "text": "then the fourth option here which is our favorite is using kubernetes itself to",
    "start": "604180",
    "end": "610190"
  },
  {
    "text": "get the information out of the API server about what's running in the cluster so the API machinery special",
    "start": "610190",
    "end": "617839"
  },
  {
    "text": "interest group spent a lot of time building a discovery mechanism so you can go to your queue brandies api server",
    "start": "617839",
    "end": "624110"
  },
  {
    "text": "as a client and you can say what are all of the api groups that exist and within",
    "start": "624110",
    "end": "629390"
  },
  {
    "text": "each API group what are all other resources that exist so you can look at the core API and you can see that there",
    "start": "629390",
    "end": "636829"
  },
  {
    "text": "are pods and deployments and our pods and secrets etc you can go to the apps API and see all the deployments and this",
    "start": "636829",
    "end": "644329"
  },
  {
    "text": "is something it's very easy to write just a loop you can iterate through everything and say tell me all the data",
    "start": "644329",
    "end": "650449"
  },
  {
    "text": "that you have so what about persistent volumes because presumably if you've got",
    "start": "650449",
    "end": "657170"
  },
  {
    "start": "652000",
    "end": "718000"
  },
  {
    "text": "stateful workloads in a kubernetes cluster you probably are using persistent volumes for that",
    "start": "657170",
    "end": "662860"
  },
  {
    "text": "unfortunately I don't have a great answer here at least for a generic one because some of your data",
    "start": "662860",
    "end": "670160"
  },
  {
    "text": "might be in cloud provider specific persistent volumes EBS volumes Azure",
    "start": "670160",
    "end": "676250"
  },
  {
    "text": "managed disks GCE persistent disks etc so there's nothing in kubernetes that",
    "start": "676250",
    "end": "682100"
  },
  {
    "text": "right now allows you to say take a snapshot of my PV there's a proposed a",
    "start": "682100",
    "end": "688070"
  },
  {
    "text": "set of api's to do that but they're not available yet so if you've got kubernetes in production and you've got",
    "start": "688070",
    "end": "694459"
  },
  {
    "text": "persistent volumes maybe you've got some tooling that you've written to do it but unfortunately you can't rely on",
    "start": "694459",
    "end": "700339"
  },
  {
    "text": "kubernetes for that and there's other volume types as well beyond just the",
    "start": "700339",
    "end": "706579"
  },
  {
    "text": "cloud provider ones NFS volumes anything that can come in from a flex volume so",
    "start": "706579",
    "end": "711800"
  },
  {
    "text": "how do you back those up again it tends to be roll-your-own but we have a better",
    "start": "711800",
    "end": "716810"
  },
  {
    "text": "solution we'll get to that in a minute here with Steve yeah so I'd like to talk to you now about an open source tool we",
    "start": "716810",
    "end": "723350"
  },
  {
    "start": "718000",
    "end": "1143000"
  },
  {
    "text": "built called hep do arc and its purpose is to help with backing up some of that",
    "start": "723350",
    "end": "729320"
  },
  {
    "text": "stateful data that we've talked about within your kubernetes cluster so what exactly does hep do arc do so it has two",
    "start": "729320",
    "end": "738140"
  },
  {
    "text": "core features and the first one is that it enables you to backup and restore your kubernetes api objects now Andy",
    "start": "738140",
    "end": "744680"
  },
  {
    "text": "just talked to us about some of the different options for backing those up in terms of what's in at CD and we use",
    "start": "744680",
    "end": "750589"
  },
  {
    "text": "in our kubernetes discovery API for accessing all of that information and creating backups of it as well as",
    "start": "750589",
    "end": "756829"
  },
  {
    "text": "restoring it in the case of a disaster and we do this for a few reasons and Andy started to talk about some of",
    "start": "756829",
    "end": "763160"
  },
  {
    "text": "the pros and cons there but you know one of the reasons we think the discovery API makes a lot of sense is that if",
    "start": "763160",
    "end": "769370"
  },
  {
    "text": "you're running in a managed kubernetes provider you may not have access to the underlying at CD cluster and so using at",
    "start": "769370",
    "end": "776870"
  },
  {
    "text": "CD CTL to take snapshots may not even be a feasible option for you",
    "start": "776870",
    "end": "782029"
  },
  {
    "text": "additionally arc and using the discovery API gives you a lot of fine-grained",
    "start": "782029",
    "end": "788120"
  },
  {
    "text": "control over what types of resources you backup so with that CD backups it's",
    "start": "788120",
    "end": "794000"
  },
  {
    "text": "really kind of all or nothing and if you want to restore your cluster you basically have to restore this state of the entire cluster if",
    "start": "794000",
    "end": "800240"
  },
  {
    "text": "using the API though you have all the controls that it provides you in terms of filtering by namespace filtering by",
    "start": "800240",
    "end": "806240"
  },
  {
    "text": "resource types filtering by label selectors and so we enable this all through arc and additionally if you are",
    "start": "806240",
    "end": "816110"
  },
  {
    "text": "backing up at CD directly you you don't get the benefit of being able to capture",
    "start": "816110",
    "end": "821990"
  },
  {
    "text": "all the information that is stored for extension API mechanisms so if you have",
    "start": "821990",
    "end": "827119"
  },
  {
    "text": "an extension API server as part of your cluster odds are that the data to support that is actually stored in a",
    "start": "827119",
    "end": "832550"
  },
  {
    "text": "separate at CD cluster this is the recommendation for for how to design these extension mechanisms and so if",
    "start": "832550",
    "end": "838369"
  },
  {
    "text": "you're backing up at CD you're not going to capture that information but if you use the discovery API this will actually",
    "start": "838369",
    "end": "843740"
  },
  {
    "text": "aggregate all of that information about extension API servers and so you can just back that information up directly",
    "start": "843740",
    "end": "849619"
  },
  {
    "text": "into your your arc backup or whatever other backup mechanism you have and so",
    "start": "849619",
    "end": "854929"
  },
  {
    "text": "we believe that the the discovery API makes a lot of sense here for accessing that information so our ques is the",
    "start": "854929",
    "end": "861079"
  },
  {
    "text": "discovery API to to pull that all out of your cluster and it creates a tarball that stores all this information and",
    "start": "861079",
    "end": "867949"
  },
  {
    "text": "places the backup in the object storage system of your choice now the second big feature that arc has",
    "start": "867949",
    "end": "874699"
  },
  {
    "text": "is that it will actually backup and restore your persistent volumes for you assuming you're on one of the supported",
    "start": "874699",
    "end": "880970"
  },
  {
    "text": "cloud provider platforms and as Andy mentioned a minute ago we we use the",
    "start": "880970",
    "end": "886670"
  },
  {
    "text": "snapshot API is that the cloud providers offer for taking backups of volumes arq",
    "start": "886670",
    "end": "891860"
  },
  {
    "text": "out-of-the-box supports the three major public clouds but as we'll see in a minute we also have an easy way to",
    "start": "891860",
    "end": "897350"
  },
  {
    "text": "extend the functionality of arc to support the platform of your choice so as long as there's an API for you to",
    "start": "897350",
    "end": "903319"
  },
  {
    "text": "take backups arc can easily integrate with that now beyond those two big",
    "start": "903319",
    "end": "910579"
  },
  {
    "text": "features we have another of a number of other features that make it really easy for you to use so we support scheduled",
    "start": "910579",
    "end": "916999"
  },
  {
    "text": "backups so rather than having to go manually create a backup you can simply configure the information you'd like to",
    "start": "916999",
    "end": "923179"
  },
  {
    "text": "back up through our set of schedule and have those run on an automated basis over time additionally as I mentioned a",
    "start": "923179",
    "end": "931519"
  },
  {
    "text": "minute ago we support complex filtering both when you take a backup of information as well as when",
    "start": "931519",
    "end": "938120"
  },
  {
    "text": "you do a restore of that information back into a cluster so you can filter based on the namespaces you want to",
    "start": "938120",
    "end": "944089"
  },
  {
    "text": "backup based on the resource types and based on label selectors and so often we",
    "start": "944089",
    "end": "949819"
  },
  {
    "text": "see that users will take a backup of their entire cluster so that they have all the information and when they go to",
    "start": "949819",
    "end": "955940"
  },
  {
    "text": "do a restore they may do it on a namespace by namespace basis or they may only restore components that match a",
    "start": "955940",
    "end": "962899"
  },
  {
    "text": "certain label selector and so this gives you a lot of control over how you recover the information into your target",
    "start": "962899",
    "end": "968329"
  },
  {
    "text": "cluster additionally we give you the ability to restore in two different namespaces than",
    "start": "968329",
    "end": "974720"
  },
  {
    "text": "you backed up from and so this is really useful for use cases where maybe you have an existing namespace and you want",
    "start": "974720",
    "end": "981529"
  },
  {
    "text": "to create a clone of it maybe for testing purposes so that you can fiddle with some configuration or maybe you",
    "start": "981529",
    "end": "987769"
  },
  {
    "text": "have other use cases that require you to change the namespace arc makes that really easy to do now we also design",
    "start": "987769",
    "end": "996860"
  },
  {
    "text": "dart to be very extensible we recognize that we can't meet everyone's needs out of the box and so we want to give users",
    "start": "996860",
    "end": "1003010"
  },
  {
    "text": "the ability to extend our to meet their needs and so the first of these mechanisms is what's called hooks and",
    "start": "1003010",
    "end": "1009630"
  },
  {
    "text": "hooks are basically a way for you as the the user of Arc to define scripts that",
    "start": "1009630",
    "end": "1015610"
  },
  {
    "text": "need to be run within your pods immediately before or immediately after or backing up those pods and so a great",
    "start": "1015610",
    "end": "1023139"
  },
  {
    "text": "example is this is if you're if you have a pub that's running that's using a persistent volume and prior to executing",
    "start": "1023139",
    "end": "1029890"
  },
  {
    "text": "a backup of that volume you actually need to freeze the file system to ensure you get a consistent backup arc makes it",
    "start": "1029890",
    "end": "1035558"
  },
  {
    "text": "really easy to plug in an FS freeze command before the backup and similarly and unfreeze command right after the",
    "start": "1035559",
    "end": "1041079"
  },
  {
    "text": "backup the second major way that we allow you to extend arc is through",
    "start": "1041079",
    "end": "1046780"
  },
  {
    "text": "what's called plugins and so there are sort of two major categories of plugins that we support currently the first one",
    "start": "1046780",
    "end": "1053260"
  },
  {
    "text": "has to do with with cloud providers and so there are there are kind of two core cloud provider api's that arc relies on",
    "start": "1053260",
    "end": "1060419"
  },
  {
    "text": "the first one is object storage which is where we actually store the tarball that contains all of your your @cd data",
    "start": "1060419",
    "end": "1067260"
  },
  {
    "text": "and the second one is block storage and this is what allows you to take snapshots of your volumes and restore",
    "start": "1067260",
    "end": "1073530"
  },
  {
    "text": "them later on and so we have a plug-in model which allows you to define your own implementations for both of these",
    "start": "1073530",
    "end": "1080010"
  },
  {
    "text": "and to very easily plug it into the arc server at runtime so that you can extend arc to run on your platform of choice",
    "start": "1080010",
    "end": "1086370"
  },
  {
    "text": "and this doesn't require you to submit PRS to the arc the core our code base doesn't require you to recompile or",
    "start": "1086370",
    "end": "1093180"
  },
  {
    "text": "maintain your own component container images the second major category of",
    "start": "1093180",
    "end": "1098670"
  },
  {
    "text": "plugins is what are called item actions and we support these on both backup and restore and so these are little bits of",
    "start": "1098670",
    "end": "1106050"
  },
  {
    "text": "functionality that run as each item is being backed up or restored they're different from hooks in that they're not",
    "start": "1106050",
    "end": "1112500"
  },
  {
    "text": "actually scripts that are being executed within your pods they're being run by the arcs server and they allow you to",
    "start": "1112500",
    "end": "1119100"
  },
  {
    "text": "potentially call out to external systems to take certain actions or they allow you to actually mutate the item that",
    "start": "1119100",
    "end": "1125460"
  },
  {
    "text": "you're backing up or restoring so if you need to added some annotations to items as you're backing them up add labels or",
    "start": "1125460",
    "end": "1132360"
  },
  {
    "text": "maybe you want to actually modify the spec as you're restoring your backup into a new cluster we make it really",
    "start": "1132360",
    "end": "1138120"
  },
  {
    "text": "easy for you to plug in your own logic to do this all right so we have a demo",
    "start": "1138120",
    "end": "1144020"
  },
  {
    "start": "1143000",
    "end": "1442000"
  },
  {
    "text": "hopefully the demo gods are with us today okay so on our script here so this",
    "start": "1144020",
    "end": "1156180"
  },
  {
    "text": "is all alive so first thing we're gonna do is show you what namespaces we have and we have",
    "start": "1156180",
    "end": "1161520"
  },
  {
    "text": "typical ones you'd see default cube public cube system we also have hep-c Oh arc which is where arc is running and we",
    "start": "1161520",
    "end": "1168480"
  },
  {
    "text": "are using rook for dynamic provisioning for persistent volumes today so we're",
    "start": "1168480",
    "end": "1174810"
  },
  {
    "text": "gonna start by deploying a simple nginx application and you'll see that this",
    "start": "1174810",
    "end": "1180450"
  },
  {
    "text": "creates a namespace a PVC a deployment and a service so if we take a look at",
    "start": "1180450",
    "end": "1185760"
  },
  {
    "text": "what the PVC looks like this is a rook block storage class and it is bound and",
    "start": "1185760",
    "end": "1191580"
  },
  {
    "text": "we are going to be storing the logs for nginx in this persistent volume so here is the",
    "start": "1191580",
    "end": "1197119"
  },
  {
    "text": "PV similarly you'll see that it's bound to nginx logs and if we take a look at",
    "start": "1197119",
    "end": "1203720"
  },
  {
    "text": "the deployment we want one replicas and we have one running and here is the pot",
    "start": "1203720",
    "end": "1209990"
  },
  {
    "text": "so everything deployed great for us here and we're gonna go ahead and take a look",
    "start": "1209990",
    "end": "1215749"
  },
  {
    "text": "at this service so we see it's got a cluster IP so let's go ahead and talk to nginx looks pretty straightforward so",
    "start": "1215749",
    "end": "1224149"
  },
  {
    "text": "the next thing we're gonna do here is hit it 10 more times just to get some extra traffic in the logs and we'll go",
    "start": "1224149",
    "end": "1231470"
  },
  {
    "text": "ahead and exec into the nginx container so that we can see we've got a couple",
    "start": "1231470",
    "end": "1236779"
  },
  {
    "text": "files in here access log bout a kilobyte nothing in the error log yet and now",
    "start": "1236779",
    "end": "1243559"
  },
  {
    "text": "let's actually look at this access log so we're gonna exec into the container and take a look at that file and pretty",
    "start": "1243559",
    "end": "1249529"
  },
  {
    "text": "vanilla access log we've got the initial request that we made and then the 10 after so let's create a backup it's this",
    "start": "1249529",
    "end": "1258440"
  },
  {
    "text": "simple you just say arc backup create give it a name and then whatever filters you want in",
    "start": "1258440",
    "end": "1263480"
  },
  {
    "text": "this case we're only going to select the nginx namespace and it's done so we have",
    "start": "1263480",
    "end": "1270019"
  },
  {
    "text": "an arc backup the data is available in object storage for this demo we're using",
    "start": "1270019",
    "end": "1275509"
  },
  {
    "text": "Mineo deployed into the cluster but in a real world scenario you probably would want to have your data backed up outside",
    "start": "1275509",
    "end": "1281990"
  },
  {
    "text": "of your cluster so it's time for a disaster we're gonna go ahead and delete",
    "start": "1281990",
    "end": "1287419"
  },
  {
    "text": "the nginx namespace this will delete all of the components that we just deployed including the persistent volume that was",
    "start": "1287419",
    "end": "1294409"
  },
  {
    "text": "dynamically provisioned one of the great things about arc is that it can walk from the pod to the persistent volume",
    "start": "1294409",
    "end": "1301070"
  },
  {
    "text": "claim to the persistent volume to figure out that there is a relationship between 3 and make sure that we backup",
    "start": "1301070",
    "end": "1307279"
  },
  {
    "text": "everything that we need to be backing up alrighty so our name space has been",
    "start": "1307279",
    "end": "1312769"
  },
  {
    "text": "deleted here I will prove that to you so you can see we do not have an engine x",
    "start": "1312769",
    "end": "1317960"
  },
  {
    "text": "namespace anymore and just to show you that there's no longer a PV anymore that",
    "start": "1317960",
    "end": "1323450"
  },
  {
    "text": "is gone we can't find it so let's go ahead and use Ark to restore",
    "start": "1323450",
    "end": "1329830"
  },
  {
    "text": "the backup that we just took and while this is happening I will say that when the backup was going what Steve",
    "start": "1329830",
    "end": "1336400"
  },
  {
    "text": "described about doing an FS freeze before and after the snapshot was taken was exactly what we had or do today",
    "start": "1336400",
    "end": "1343530"
  },
  {
    "text": "so our restore is done let's go ahead and take a look we have a PVC it's bound",
    "start": "1343530",
    "end": "1350920"
  },
  {
    "text": "using the rook block storage class again and we have PV similarly so this is just",
    "start": "1350920",
    "end": "1357850"
  },
  {
    "text": "going to show everything that we had before but the the individual names for anything that has a generated name like",
    "start": "1357850",
    "end": "1364060"
  },
  {
    "text": "the pods for example this has a different name than it did before and",
    "start": "1364060",
    "end": "1369780"
  },
  {
    "text": "everything is running fantastic let's go to the service this is a different IP",
    "start": "1369780",
    "end": "1375010"
  },
  {
    "text": "than we had for and we'll go ahead and take a look at that file system and",
    "start": "1375010",
    "end": "1381100"
  },
  {
    "text": "again this is the log file system from the persistent volume that are stored",
    "start": "1381100",
    "end": "1387030"
  },
  {
    "text": "still has about a kilobyte that is wonderful let's go ahead and take a look",
    "start": "1387030",
    "end": "1392230"
  },
  {
    "text": "at that file it's all of our data so we",
    "start": "1392230",
    "end": "1397690"
  },
  {
    "text": "have not lost anything and just to show that we can augment it we'll go ahead",
    "start": "1397690",
    "end": "1403690"
  },
  {
    "text": "and curl it another time and take a look at the file size and the file one more",
    "start": "1403690",
    "end": "1409480"
  },
  {
    "text": "time so you'll see that 1045 has gone up to 1140 and if we take a look at the",
    "start": "1409480",
    "end": "1415180"
  },
  {
    "text": "file one last time you'll see that we",
    "start": "1415180",
    "end": "1420430"
  },
  {
    "text": "had a series of requests from 20 past the hour and then the last one from 23",
    "start": "1420430",
    "end": "1427480"
  },
  {
    "text": "minutes so our backup was successful our restores was successful and we were able",
    "start": "1427480",
    "end": "1433930"
  },
  {
    "text": "to continue using the data that was in the volume that we recovered and that's the end of the demo oh let me get this",
    "start": "1433930",
    "end": "1440470"
  },
  {
    "text": "back there we go great thanks Andy for the demo so I'd like to say please come join us in the",
    "start": "1440470",
    "end": "1446860"
  },
  {
    "start": "1442000",
    "end": "1507000"
  },
  {
    "text": "art community so Andy and I obviously both work at hep do but Ark is completely open-source we have a number",
    "start": "1446860",
    "end": "1452620"
  },
  {
    "text": "of external contributors who have been working on Ark since the initial release and so we'd love to have you come join",
    "start": "1452620",
    "end": "1457690"
  },
  {
    "text": "us whether it's to provide feedback on arc whether it's to provide real-world use",
    "start": "1457690",
    "end": "1462720"
  },
  {
    "text": "cases that you're using it for or whether you'd like to add features yourself please come talk to us so we're easily accessible through",
    "start": "1462720",
    "end": "1469560"
  },
  {
    "text": "github or through slack we have a slack channel in the kubernetes org we have a Google Group if you'd like to subscribe",
    "start": "1469560",
    "end": "1475080"
  },
  {
    "text": "for release notifications and we're on Twitter as well so please come join us and we really are",
    "start": "1475080",
    "end": "1480990"
  },
  {
    "text": "looking for your input we have so many ideas about backup and recovery but I'm",
    "start": "1480990",
    "end": "1488520"
  },
  {
    "text": "sure you have more and specific needs so please do come and find us whether it's",
    "start": "1488520",
    "end": "1493710"
  },
  {
    "text": "today or next week next month we would appreciate the input and at this point if anyone has any questions we would be",
    "start": "1493710",
    "end": "1500310"
  },
  {
    "text": "happy to answer them why don't you come up to the mics I think everyone will be",
    "start": "1500310",
    "end": "1505950"
  },
  {
    "text": "able to hear that'd be great take one",
    "start": "1505950",
    "end": "1512850"
  },
  {
    "start": "1507000",
    "end": "1607000"
  },
  {
    "text": "over girl sure go ahead so in this case if you restore back a copy that doesn't include",
    "start": "1512850",
    "end": "1520500"
  },
  {
    "text": "data about a pod that like happened to survive whatever outage you had what",
    "start": "1520500",
    "end": "1527400"
  },
  {
    "text": "happens to that pod so is the question if the the backup didn't include the pod",
    "start": "1527400",
    "end": "1533100"
  },
  {
    "text": "and then there was a restore yeah exactly um best effort for what happens I mean",
    "start": "1533100",
    "end": "1539100"
  },
  {
    "text": "if you were expecting that pod to be running and it's it's not in your backup and it certainly won't be in your recovery so you just need to be careful",
    "start": "1539100",
    "end": "1547500"
  },
  {
    "text": "with what how you spec out your backups and make sure that you know your label",
    "start": "1547500",
    "end": "1552930"
  },
  {
    "text": "selector is appropriate to match your pods and whatever else you need or you don't use label selectors and you just",
    "start": "1552930",
    "end": "1559020"
  },
  {
    "text": "say I want to backup everything okay cool thanks does that answer your question sort of sort of it's sort of a",
    "start": "1559020",
    "end": "1565950"
  },
  {
    "text": "kubernetes very specific question like if there are pods running on your machines that are kubernetes manages or",
    "start": "1565950",
    "end": "1574890"
  },
  {
    "text": "rather just containers that aren't part of pods that kubernetes knows about will",
    "start": "1574890",
    "end": "1580560"
  },
  {
    "text": "kubernetes kill those or let them continue or like so kubernetes will not",
    "start": "1580560",
    "end": "1585930"
  },
  {
    "text": "touch any containers that it's not responsible for okay and so similarly if",
    "start": "1585930",
    "end": "1591420"
  },
  {
    "text": "you have contain that you are running manually and then you do an arc back up in an arc restore",
    "start": "1591420",
    "end": "1596780"
  },
  {
    "text": "you don't know anything about those containers because kubernetes doesn't okay it doesn't like wipe and recreate",
    "start": "1596780",
    "end": "1602450"
  },
  {
    "text": "okay no the cubelet will leave them untouched cool thanks sure table 1 over here right",
    "start": "1602450",
    "end": "1609440"
  },
  {
    "start": "1607000",
    "end": "1682000"
  },
  {
    "text": "what's the appropriate way to monitor whether a backup was successful or failed really it's a good question",
    "start": "1609440",
    "end": "1616700"
  },
  {
    "text": "so we have logs that are stored per backup so when we start doing a backup",
    "start": "1616700",
    "end": "1623950"
  },
  {
    "text": "you'll see that it's in progress and when the backup has completed or failed you'll be able to retrieve those logs",
    "start": "1623950",
    "end": "1630470"
  },
  {
    "text": "and see what the problems were is there any way you can add like a status hook or something to just say you know just",
    "start": "1630470",
    "end": "1637730"
  },
  {
    "text": "call this service web hoax script whatever to be like this didn't work that's a really good idea and it's",
    "start": "1637730",
    "end": "1644090"
  },
  {
    "text": "actually in line was something we are planning on doing which is in addition to the pod hooks that Steve mentioned we",
    "start": "1644090",
    "end": "1649520"
  },
  {
    "text": "do want to have just overall backup level hooks so when it starts send up a web hook when it finishes what it was",
    "start": "1649520",
    "end": "1656410"
  },
  {
    "text": "successful or a failure send out a notification as well yeah I don't care if it's successful just just just that",
    "start": "1656410",
    "end": "1663080"
  },
  {
    "text": "it's it the other thing to note is that backups restores and and many of the other crew arc concepts are CR DS within",
    "start": "1663080",
    "end": "1670700"
  },
  {
    "text": "kubernetes and so you always have the option to write a watch on the CR DS themselves and look for for failures in",
    "start": "1670700",
    "end": "1677330"
  },
  {
    "text": "the status that's a good pattern for now cool thank you okay so great presentation in the multi",
    "start": "1677330",
    "end": "1685460"
  },
  {
    "text": "classic cig we're looking at disaster recovery use cases and trying to figure out how to do that and it's just curious",
    "start": "1685460",
    "end": "1692300"
  },
  {
    "text": "whether you'd seen anybody do this particular scenario where you have like a primary cluster and a secondary",
    "start": "1692300",
    "end": "1698180"
  },
  {
    "text": "cluster and something that monitors if the primary cluster goes down and then uses you know arc to basically you know",
    "start": "1698180",
    "end": "1707030"
  },
  {
    "text": "launch what was running on the primary on the secondary that's a very good question I don't think we've heard any",
    "start": "1707030",
    "end": "1714230"
  },
  {
    "text": "specific requests around that but one of the things that is on our roadmap is being able to take a backup that was in",
    "start": "1714230",
    "end": "1722360"
  },
  {
    "text": "say one region if you're on a cloud and be able to migrate that over to a",
    "start": "1722360",
    "end": "1727940"
  },
  {
    "text": "different region and restore over there so that potentially could play into what you're looking to do but I think art",
    "start": "1727940",
    "end": "1734990"
  },
  {
    "text": "could definitely fit into that picture in terms of monitoring and automating moving the data from cluster cluster as",
    "start": "1734990",
    "end": "1742700"
  },
  {
    "text": "needed and I would I would also add to that that because because Ark uses CR",
    "start": "1742700",
    "end": "1749690"
  },
  {
    "text": "G's you always have the option to write a layer yourself on top of Ark that's monitoring the health of your primary cluster and if there is a disaster you",
    "start": "1749690",
    "end": "1756770"
  },
  {
    "text": "can write code to basically create a restore CR D in your secondary cluster and automatically restore objects into",
    "start": "1756770",
    "end": "1764179"
  },
  {
    "text": "that so that's that's something you can very easily do around arc school thanks is very helpful building block sure okay",
    "start": "1764179",
    "end": "1772730"
  },
  {
    "start": "1772000",
    "end": "1847000"
  },
  {
    "text": "I'm wondering like what are preconditions that might cause arc to",
    "start": "1772730",
    "end": "1778130"
  },
  {
    "text": "fail like I asked a question before it gave me an idea what if let's say you",
    "start": "1778130",
    "end": "1784039"
  },
  {
    "text": "toured that a namespace or you lost the namespace that you had backed up and you wanted to restore it but for some reason",
    "start": "1784039",
    "end": "1790100"
  },
  {
    "text": "somebody brought that namespace up and maybe had some of the resources created",
    "start": "1790100",
    "end": "1795500"
  },
  {
    "text": "already what would happen if I did a restore sure so what art does right now is it will try to create every single",
    "start": "1795500",
    "end": "1802130"
  },
  {
    "text": "object that you've specified as part of the restore and if it encounters any conflicts it logs it as a warning right",
    "start": "1802130",
    "end": "1808610"
  },
  {
    "text": "or it puts it in the status as a warning right now so it's very visible when you see that the restore has completed it'll",
    "start": "1808610",
    "end": "1815330"
  },
  {
    "text": "tell you if there were any errors which would be catastrophic it'll tell you if there any warnings such as there was a conflict at the present we just record",
    "start": "1815330",
    "end": "1824270"
  },
  {
    "text": "that fact so if there's something that's already pre-existing in the namespace we won't touch it in the future what we'd",
    "start": "1824270",
    "end": "1830600"
  },
  {
    "text": "like to do is make it pluggable so that you can say on a conflict here is custom",
    "start": "1830600",
    "end": "1836000"
  },
  {
    "text": "logic to run to make the decision do i patch what's in there already with what i have do I accept what's already there",
    "start": "1836000",
    "end": "1843500"
  },
  {
    "text": "or do I replace what's there what came from the backup are there any types of resources that don't behave so well than",
    "start": "1843500",
    "end": "1849980"
  },
  {
    "start": "1847000",
    "end": "1907000"
  },
  {
    "text": "when they're restored from a backup as if you know compared to just being created yes the one that I can think of",
    "start": "1849980",
    "end": "1856880"
  },
  {
    "text": "off the top of my head load balancer services those depend on the UID of the service and that's not a",
    "start": "1856880",
    "end": "1864159"
  },
  {
    "text": "field that is something you can mutate or set it's set by the API server itself so if you have a load balancer API",
    "start": "1864159",
    "end": "1870879"
  },
  {
    "text": "service tied to say an amazon ELB if you take a backup and you do a restore you're gonna get a different one",
    "start": "1870879",
    "end": "1877470"
  },
  {
    "text": "unfortunately and hopefully we can work with the community to see if we can solve that how are you typing so fast so I've got",
    "start": "1877470",
    "end": "1888549"
  },
  {
    "text": "to thank Joe Beda for finding his script on github I think it's called demo magic where all",
    "start": "1888549",
    "end": "1895389"
  },
  {
    "text": "of that was a real demo I just was hitting Enter to get it to type for me okay thanks thank you",
    "start": "1895389",
    "end": "1901960"
  },
  {
    "text": "go ahead in the middle every time so",
    "start": "1901960",
    "end": "1915460"
  },
  {
    "start": "1907000",
    "end": "1992000"
  },
  {
    "text": "we'll go as fast as the API server will let us right now we aren't setting the",
    "start": "1915460",
    "end": "1921879"
  },
  {
    "text": "QPS on the client so I believe the default is about five requests a second which certainly could be slow if you",
    "start": "1921879",
    "end": "1928779"
  },
  {
    "text": "have a lot of data we do plan to make that configurable and then as far as the PV snapshots go it's as fast as your",
    "start": "1928779",
    "end": "1937029"
  },
  {
    "text": "cloud provider or whatever you're using can do the snapshots over here so just",
    "start": "1937029",
    "end": "1945399"
  },
  {
    "text": "to tie in to the last two comments it'll be great to see it run faster actually",
    "start": "1945399",
    "end": "1950499"
  },
  {
    "text": "because it takes a couple hours to do a restore something event of a major outage they'll be pretty difficult for",
    "start": "1950499",
    "end": "1956679"
  },
  {
    "text": "us and you know our at CD size about 850 Meg's two to 3,000 pods and a whole ton",
    "start": "1956679",
    "end": "1963369"
  },
  {
    "text": "of config maps that helm leaves behind and the other thing is so what are we",
    "start": "1963369",
    "end": "1968679"
  },
  {
    "text": "doing about load balancers because that's a major impediment to restore I",
    "start": "1968679",
    "end": "1974379"
  },
  {
    "text": "have been involved in a little bit of discussion about that with the community I honestly don't know where it currently",
    "start": "1974379",
    "end": "1981399"
  },
  {
    "text": "stands but we will be following up with that Thanks",
    "start": "1981399",
    "end": "1988440"
  },
  {
    "text": "I so my questions related to previous ones also how well does it interface",
    "start": "1988440",
    "end": "1994620"
  },
  {
    "start": "1992000",
    "end": "2100000"
  },
  {
    "text": "with other things that are managing resources like if I do a home deploy it's kind of keeping track of what",
    "start": "1994620",
    "end": "2000260"
  },
  {
    "text": "resources are part of the chart and something horrible happens everything",
    "start": "2000260",
    "end": "2005720"
  },
  {
    "text": "goes away I restore using arc if I do another helm deploy will it pick up",
    "start": "2005720",
    "end": "2012080"
  },
  {
    "text": "correctly or will try to start a whole new thing I'm not intimately familiar with helm",
    "start": "2012080",
    "end": "2018590"
  },
  {
    "text": "but the way that our backups and restores generally work is we backup the",
    "start": "2018590",
    "end": "2025670"
  },
  {
    "text": "majority of the object we may strip off status for example and then most of our",
    "start": "2025670",
    "end": "2031730"
  },
  {
    "text": "objects we restore as is there's a couple of exceptions here and there so if there are certain pieces of data that",
    "start": "2031730",
    "end": "2039110"
  },
  {
    "text": "you need that we're accidentally stripping off or not on purpose then police file an issue if you find",
    "start": "2039110",
    "end": "2045770"
  },
  {
    "text": "problems and will correct them okay great thanks I think we have time for one more over here so to answer the",
    "start": "2045770",
    "end": "2052370"
  },
  {
    "text": "question earlier for load balancers there's actually an open PR to set the load balancer name which cloud providers",
    "start": "2052370",
    "end": "2057860"
  },
  {
    "text": "use to look up load balancers so then they will be out here they'd be able to restore then and not rely on the UID for",
    "start": "2057860",
    "end": "2063740"
  },
  {
    "text": "a name that's open so if you want to go comment on that PR we're trying to figure out a good way but also for a",
    "start": "2063740",
    "end": "2070340"
  },
  {
    "text": "question for arc what about resources managed outside of kubernetes like DNS to have a hook for if you need to have a",
    "start": "2070340",
    "end": "2077149"
  },
  {
    "text": "new load balancer like right now to have DNS be able to update as well to have",
    "start": "2077150",
    "end": "2082220"
  },
  {
    "text": "like an outside hook I think that's a great idea I would be happy to talk more",
    "start": "2082220",
    "end": "2087860"
  },
  {
    "text": "about exactly where the hook would fit in and feel free to file github issue and then we can talk about it",
    "start": "2087860",
    "end": "2093740"
  },
  {
    "text": "thanks thank you everyone I think we're about out of time thanks everyone",
    "start": "2093740",
    "end": "2099670"
  }
]