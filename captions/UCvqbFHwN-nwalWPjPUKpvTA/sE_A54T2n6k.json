[
  {
    "start": "0",
    "end": "74000"
  },
  {
    "text": "um hi everyone good afternoon uh thank you very much for being here and being interested in k-serve um i also know",
    "start": "320",
    "end": "8080"
  },
  {
    "text": "many of my team team members are watching uh this session virtually and on a thanks for my team members and my",
    "start": "8080",
    "end": "15839"
  },
  {
    "text": "the contributors to k-serve my name is eudry i am the team lead of",
    "start": "15839",
    "end": "21680"
  },
  {
    "text": "data science runtime team of bloomberg our team provides a data science platform with functionalities to support",
    "start": "21680",
    "end": "29920"
  },
  {
    "text": "what bloomberg internal users need for machine learning model development life cycle",
    "start": "29920",
    "end": "36320"
  },
  {
    "text": "those functionalities include how to do data exploring using jupyter notebook",
    "start": "36320",
    "end": "42160"
  },
  {
    "text": "how to train models using popular frameworks such as tensorflow pytorch",
    "start": "42160",
    "end": "48399"
  },
  {
    "text": "escalar extra we also manage experiments and do online inference today i'm going",
    "start": "48399",
    "end": "56079"
  },
  {
    "text": "to talk about the open source project we initiated together with multiple collaborators named kaiser",
    "start": "56079",
    "end": "63680"
  },
  {
    "text": "we also run into recent problems we want to deploy many models at scale and i",
    "start": "63680",
    "end": "70000"
  },
  {
    "text": "want to talk about how we address this scalability problem",
    "start": "70000",
    "end": "75679"
  },
  {
    "start": "74000",
    "end": "100000"
  },
  {
    "text": "and first i want to give you a little bit background about k serve k serve is previously known as cave",
    "start": "76000",
    "end": "83439"
  },
  {
    "text": "serving if you are already familiar with scale serving if you are already deploying cave serving into your",
    "start": "83439",
    "end": "88799"
  },
  {
    "text": "platform caser is the same as cave serving it recently moved to an independent git",
    "start": "88799",
    "end": "95200"
  },
  {
    "text": "organization and it now enjoys more autonomous",
    "start": "95200",
    "end": "100680"
  },
  {
    "start": "100000",
    "end": "166000"
  },
  {
    "text": "k-surf has experienced multiple important milestones in september 2019",
    "start": "102560",
    "end": "109439"
  },
  {
    "text": "we released the first version of k-serve as a sub project under coop flow",
    "start": "109439",
    "end": "115840"
  },
  {
    "text": "it was released under the name kf serving a few months later we introduced it at",
    "start": "115840",
    "end": "121680"
  },
  {
    "text": "coupon us and we spent the next year and a half to develop this project and we released its",
    "start": "121680",
    "end": "129039"
  },
  {
    "text": "stable version v1 beta 1 version into 2021.",
    "start": "129039",
    "end": "134080"
  },
  {
    "text": "finally last month we renamed the project to k-serve it is a sign that it has reached",
    "start": "134080",
    "end": "139920"
  },
  {
    "text": "the next level of maturity here are linked to two important",
    "start": "139920",
    "end": "147120"
  },
  {
    "text": "articles and blog posts you are very welcome to follow the links to learn over announcement together with coop",
    "start": "147120",
    "end": "153599"
  },
  {
    "text": "flow community and also the blog we posted tech at bloomberg that talk about our",
    "start": "153599",
    "end": "160000"
  },
  {
    "text": "journey of building a production grade machine learning model sewing solution",
    "start": "160000",
    "end": "167120"
  },
  {
    "text": "we won't be able to achieve all this without all the awesome contributors",
    "start": "167440",
    "end": "172640"
  },
  {
    "text": "here i want to spend a few seconds to acknowledge everyone who have contributed and collaborated with us",
    "start": "172640",
    "end": "180879"
  },
  {
    "start": "180000",
    "end": "231000"
  },
  {
    "text": "for those of you who are less familiar with inference this diagram demonstrates a typical and",
    "start": "182319",
    "end": "189440"
  },
  {
    "text": "also simplified model development life cycle normally overuser are data scientists",
    "start": "189440",
    "end": "196239"
  },
  {
    "text": "and machine learning engineers they will first prepare data and use a machine learning framework to train a model and",
    "start": "196239",
    "end": "203200"
  },
  {
    "text": "once the model is trained our user wants to deploy this model into a production system and this model",
    "start": "203200",
    "end": "210000"
  },
  {
    "text": "should be able to answer real-time questions such as given two sentences can you can my model",
    "start": "210000",
    "end": "216400"
  },
  {
    "text": "tell me what is the similarities between these two sentences or given a news article",
    "start": "216400",
    "end": "222080"
  },
  {
    "text": "the model should be able to run inference and let us know what are the topic topics related to this news",
    "start": "222080",
    "end": "228640"
  },
  {
    "text": "article so how hard can it be",
    "start": "228640",
    "end": "236879"
  },
  {
    "start": "231000",
    "end": "370000"
  },
  {
    "text": "it turns out building a solution like this is very very difficult",
    "start": "237599",
    "end": "243920"
  },
  {
    "text": "first we want to think about the cost of deploying such a machine learning model how much cpu how much memory how much",
    "start": "244319",
    "end": "251519"
  },
  {
    "text": "gpu resource is needed and when there's no request coming into the service is there a way to automatically scale it up",
    "start": "251519",
    "end": "259280"
  },
  {
    "text": "or scale it down we also want to monitor our machine learning services",
    "start": "259280",
    "end": "264639"
  },
  {
    "text": "we want to think about how to do readiness check how to do liveness check uh we also want to produce permitted",
    "start": "264639",
    "end": "272160"
  },
  {
    "text": "metrics where which we can use to build dashboard and setting alert for",
    "start": "272160",
    "end": "279039"
  },
  {
    "text": "we also care about a secure secure rollout once we build a new version of the",
    "start": "279280",
    "end": "285919"
  },
  {
    "text": "service and that break the production system how can we automatically detect that and stop the rollout",
    "start": "285919",
    "end": "292800"
  },
  {
    "text": "can we cannery a new version of a model and compare the result and be able to",
    "start": "292800",
    "end": "299199"
  },
  {
    "text": "swap traffic between the two different versions of the model",
    "start": "299199",
    "end": "304240"
  },
  {
    "text": "we also want to define a protocol inference protocol there are many",
    "start": "304800",
    "end": "311360"
  },
  {
    "text": "different types of model server in the open source world and how can we allow users to have a consistent layer",
    "start": "311360",
    "end": "318000"
  },
  {
    "text": "of protocol to send requests using grpc http and kafka",
    "start": "318000",
    "end": "325560"
  },
  {
    "text": "there are not all the users want to run inference using real-time requests some",
    "start": "326000",
    "end": "332160"
  },
  {
    "text": "of them only want to run end-of-day batch interest so how can we support this type of users",
    "start": "332160",
    "end": "339840"
  },
  {
    "text": "there are also many different type of machine learning training framework so when we do the serving step we want to",
    "start": "339840",
    "end": "346400"
  },
  {
    "text": "be able to serve various models trained by different frameworks",
    "start": "346400",
    "end": "353240"
  },
  {
    "text": "after model is running inference the next step will be how can i explain",
    "start": "355840",
    "end": "361840"
  },
  {
    "text": "my prediction so we also integrate the explainer component into k-serve",
    "start": "361840",
    "end": "368638"
  },
  {
    "start": "370000",
    "end": "461000"
  },
  {
    "text": "so here comes caser so k-serve is a highly scalable and standard-based",
    "start": "373120",
    "end": "379600"
  },
  {
    "text": "model inference platform on kubernetes for trusted ai at the",
    "start": "379600",
    "end": "385680"
  },
  {
    "text": "lowest level is our computer resource in normally in a kubernetes cluster we have",
    "start": "385680",
    "end": "392319"
  },
  {
    "text": "a collection of cpu gpu and memory sometimes even tpu those",
    "start": "392319",
    "end": "398479"
  },
  {
    "text": "are the resources we have for computing on top of all the computing resources we",
    "start": "398479",
    "end": "404400"
  },
  {
    "text": "run a kubernetes layer kubernetes is used as a way to orchestrate and manage",
    "start": "404400",
    "end": "409840"
  },
  {
    "text": "all the compute resource on top of the kubernetes layer",
    "start": "409840",
    "end": "415360"
  },
  {
    "text": "this is over serverless layer we use k-native and ecl to build the",
    "start": "415360",
    "end": "420479"
  },
  {
    "text": "serverless layer with this layer we are able to automatically scale up and scale",
    "start": "420479",
    "end": "426240"
  },
  {
    "text": "down according to the incoming traffic we are also able to scale down the number of parts",
    "start": "426240",
    "end": "433280"
  },
  {
    "text": "to zero when there is no requests coming in so we can release the compute resource when it's not being used",
    "start": "433280",
    "end": "441199"
  },
  {
    "text": "at the top level is some machine learning integration layer",
    "start": "441199",
    "end": "447120"
  },
  {
    "text": "we integrate with multiple popular model server in the industry so k-serve is able to",
    "start": "447120",
    "end": "455840"
  },
  {
    "text": "serve machine learning models trained by various machine learning frameworks",
    "start": "455840",
    "end": "461720"
  },
  {
    "start": "461000",
    "end": "573000"
  },
  {
    "text": "this is the diagram that explains the major component in case of solution",
    "start": "467199",
    "end": "473599"
  },
  {
    "text": "if you're already familiar with kubernetes resource you must understand that most resource is represented by",
    "start": "473599",
    "end": "481120"
  },
  {
    "text": "yaml file so k-serve is the same we define a resource called inference",
    "start": "481120",
    "end": "488000"
  },
  {
    "text": "the case of user can describe what kind of machine learning model they want to deploy into",
    "start": "488000",
    "end": "494720"
  },
  {
    "text": "into a system and this request will be handled by kubernetes api server",
    "start": "494720",
    "end": "500479"
  },
  {
    "text": "stored into fcd and is eventually being reconciled by inference service controller which is the main controller",
    "start": "500479",
    "end": "507440"
  },
  {
    "text": "of k-serve after inference service controller reconcile the incoming resource it will",
    "start": "507440",
    "end": "514000"
  },
  {
    "text": "create the underlying major component one of the most important component we",
    "start": "514000",
    "end": "519360"
  },
  {
    "text": "create is the predictor which is essentially the model server rerun with the predictor component",
    "start": "519360",
    "end": "526800"
  },
  {
    "text": "the model server can handle incoming requests and around inference result",
    "start": "526800",
    "end": "532800"
  },
  {
    "text": "the second important control component is transformer sometimes users want to implement",
    "start": "532800",
    "end": "539600"
  },
  {
    "text": "customized um pre-process and post-process steps to translate data point",
    "start": "539600",
    "end": "547279"
  },
  {
    "text": "into a format that the predictor can understand and then post process it back to a format that's an application called",
    "start": "547279",
    "end": "554640"
  },
  {
    "text": "understand so that's how the customized implementation can be integrated into k-serve",
    "start": "554640",
    "end": "560800"
  },
  {
    "text": "we also have an explainer component which is which use alibis",
    "start": "560800",
    "end": "566480"
  },
  {
    "text": "explainer that can explain why inference result",
    "start": "566480",
    "end": "571760"
  },
  {
    "text": "is produced",
    "start": "571760",
    "end": "575000"
  },
  {
    "start": "573000",
    "end": "608000"
  },
  {
    "text": "a very critical part of k surf is we define a",
    "start": "577440",
    "end": "582640"
  },
  {
    "text": "standard inference protocol this standard we work very closely with",
    "start": "582640",
    "end": "588959"
  },
  {
    "text": "multiple model server community including triton torch surf and ml server we make sure we",
    "start": "588959",
    "end": "597200"
  },
  {
    "text": "the case of community can come up with a set of consistent",
    "start": "597200",
    "end": "602399"
  },
  {
    "text": "inference protocol to provide a unified user experience",
    "start": "602399",
    "end": "608480"
  },
  {
    "start": "608000",
    "end": "637000"
  },
  {
    "text": "this is a set of http protocol we have defined as you can see we have the standard",
    "start": "611279",
    "end": "616480"
  },
  {
    "text": "protocol for model server to check to do liveness check readiness check and to",
    "start": "616480",
    "end": "622320"
  },
  {
    "text": "check if a model is ready to take incoming requests we can also",
    "start": "622320",
    "end": "628079"
  },
  {
    "text": "use this protocol to check a service metadata a modus metadata and of course",
    "start": "628079",
    "end": "633760"
  },
  {
    "text": "most importantly runner inference similarly we have a",
    "start": "633760",
    "end": "640399"
  },
  {
    "start": "637000",
    "end": "660000"
  },
  {
    "text": "set of grpc protocol we can use to check the health state",
    "start": "640399",
    "end": "645680"
  },
  {
    "text": "server metadata model metadata and inference with the standard protocol",
    "start": "645680",
    "end": "651200"
  },
  {
    "text": "we can easily integrate with multiple model server and the client set can set",
    "start": "651200",
    "end": "656640"
  },
  {
    "text": "requests consistently",
    "start": "656640",
    "end": "660519"
  },
  {
    "start": "660000",
    "end": "677000"
  },
  {
    "text": "so now we have already run case of in our production environment for a while",
    "start": "662959",
    "end": "668240"
  },
  {
    "text": "and now we start to run into new scalability problem how do we deploy a large number of",
    "start": "668240",
    "end": "674720"
  },
  {
    "text": "models in production so let's take a look",
    "start": "674720",
    "end": "680079"
  },
  {
    "start": "677000",
    "end": "733000"
  },
  {
    "text": "how the current approach works so how does how currently k serve deploy a",
    "start": "680079",
    "end": "685440"
  },
  {
    "text": "model a machine learning model uh the gray box here represents inference",
    "start": "685440",
    "end": "691200"
  },
  {
    "text": "platform cluster and in this cluster each of our users on their own name space which is",
    "start": "691200",
    "end": "698160"
  },
  {
    "text": "represented by the blue box the light blue box and in their own name space they can run multiple",
    "start": "698160",
    "end": "705120"
  },
  {
    "text": "inference services and this inference service will fetch a model from external model storage",
    "start": "705120",
    "end": "712320"
  },
  {
    "text": "the model storage can have can be bcs or gcs or even http service",
    "start": "712320",
    "end": "719360"
  },
  {
    "text": "once the model is downloaded into the inference service it will open up a http endpoint",
    "start": "719360",
    "end": "725519"
  },
  {
    "text": "where user can send request data to and get inference result back so what kind of problem this approach",
    "start": "725519",
    "end": "733200"
  },
  {
    "start": "733000",
    "end": "771000"
  },
  {
    "text": "bring us when we want to scale up number of models because when if we want to scale up",
    "start": "733200",
    "end": "739600"
  },
  {
    "text": "number of models we essentially need to scale up the number of services we run in our platform",
    "start": "739600",
    "end": "745200"
  },
  {
    "text": "which doesn't scale very well and i will talk about the scalability",
    "start": "745200",
    "end": "751760"
  },
  {
    "text": "limitations we foresee when we want to if each team want to run hundreds even",
    "start": "751760",
    "end": "757760"
  },
  {
    "text": "thousand models in our inference platform those are the limitations we are already",
    "start": "757760",
    "end": "764480"
  },
  {
    "text": "aware of and of course there may be other limitations we may run into",
    "start": "764480",
    "end": "772120"
  },
  {
    "start": "771000",
    "end": "845000"
  },
  {
    "text": "first i want to talk about compute resource limitations in each inference service",
    "start": "773040",
    "end": "779200"
  },
  {
    "text": "it comes with a certain amount of resource overhead we have a side car that run alongside each model server",
    "start": "779200",
    "end": "786480"
  },
  {
    "text": "that handles the incoming request produce permissive metrics",
    "start": "786480",
    "end": "792399"
  },
  {
    "text": "it can also do certain batching and logging so those sidecar has a needs certain",
    "start": "792399",
    "end": "798880"
  },
  {
    "text": "amount of cpu and memory to run alongside the model server so let's",
    "start": "798880",
    "end": "803920"
  },
  {
    "text": "that's the config the resource the sidecar requires is configurable but let's for example um let's think each",
    "start": "803920",
    "end": "810959"
  },
  {
    "text": "side car takes 0.5 cpu and 0.5 gigabyte memory overhead",
    "start": "810959",
    "end": "816160"
  },
  {
    "text": "based on this configuration if we deploy 10 models let's say each model has two replicas",
    "start": "816160",
    "end": "823519"
  },
  {
    "text": "then each model's resource overhead is around one cpu and one gpu per model",
    "start": "823519",
    "end": "830720"
  },
  {
    "text": "but if we can figure out a way to load 10 models into one inference service then on average each model's",
    "start": "830720",
    "end": "838480"
  },
  {
    "text": "resource overhead is about 0.1 cpu and 0.1 gpu that's a lot of resource",
    "start": "838480",
    "end": "844399"
  },
  {
    "text": "reduction the second limitation we foresee that we are going to run into is",
    "start": "844399",
    "end": "851040"
  },
  {
    "start": "845000",
    "end": "889000"
  },
  {
    "text": "the maximum pod limitations some of you may be already familiar with the kubernetes default setting",
    "start": "851040",
    "end": "858079"
  },
  {
    "text": "on each node by default we can run 110 parts",
    "start": "858079",
    "end": "864160"
  },
  {
    "text": "and based on the official documentation from kubernetes scalability best practice",
    "start": "864160",
    "end": "870880"
  },
  {
    "text": "we shouldn't really run more than 100 parts per node based on this limit on a 50 node cluster",
    "start": "870880",
    "end": "879199"
  },
  {
    "text": "we can deploy around 1000 to 4000 models based on the number of reps because we",
    "start": "879199",
    "end": "885839"
  },
  {
    "text": "want to we we want to configure per model the third limitation",
    "start": "885839",
    "end": "893120"
  },
  {
    "start": "889000",
    "end": "952000"
  },
  {
    "text": "we foresee ourselves will run into is the maximum ip address limitations",
    "start": "893120",
    "end": "899120"
  },
  {
    "text": "i think a lot of you also understand each part has an independent ip address in kubernetes clusters",
    "start": "899120",
    "end": "906800"
  },
  {
    "text": "the ip address are assigned to new models replicas of models uh if you",
    "start": "906800",
    "end": "912880"
  },
  {
    "text": "uh have run transformers in case of um there need to be ip address assigned to",
    "start": "912880",
    "end": "918720"
  },
  {
    "text": "transformers and explainers uh let alone there are also basic kubernetes control",
    "start": "918720",
    "end": "924079"
  },
  {
    "text": "plane parts running in the cluster the number of ip address available in",
    "start": "924079",
    "end": "930160"
  },
  {
    "text": "each kubernetes cluster varies a lot depending on how the admin manages this",
    "start": "930160",
    "end": "935199"
  },
  {
    "text": "cluster but i want to point out that this is a in one of the testing cluster we run a",
    "start": "935199",
    "end": "942320"
  },
  {
    "text": "test we have several thousand um ip address available and based on that uh",
    "start": "942320",
    "end": "948560"
  },
  {
    "text": "limitation we can run like several thousand models",
    "start": "948560",
    "end": "954440"
  },
  {
    "text": "so in order to solve the scalability problem we work very closely with our",
    "start": "956800",
    "end": "962320"
  },
  {
    "text": "collaborator from ibm and we come up with a solution called model mesh",
    "start": "962320",
    "end": "969079"
  },
  {
    "start": "968000",
    "end": "1101000"
  },
  {
    "text": "this is the diagram of model mesh solution so let me walk you through it from the",
    "start": "970079",
    "end": "976320"
  },
  {
    "text": "top to the bottom at the top it is the machine learning application which sends inference",
    "start": "976320",
    "end": "982800"
  },
  {
    "text": "requests into model mesh and one model mesh can contain",
    "start": "982800",
    "end": "988720"
  },
  {
    "text": "multiple serving runtime different serving runtime is essentially",
    "start": "988720",
    "end": "994000"
  },
  {
    "text": "a different type of model server in this diagram we there are two serving runtime",
    "start": "994000",
    "end": "999600"
  },
  {
    "text": "available different serving runtime can produce service solutions for different type of machine learning model",
    "start": "999600",
    "end": "1007600"
  },
  {
    "text": "one critical component in this diagram is the mesh and polar sidecar",
    "start": "1008480",
    "end": "1014240"
  },
  {
    "text": "that run alongside model server so the grid the light green box and light pink box here represent different model",
    "start": "1014240",
    "end": "1020880"
  },
  {
    "text": "server and you can see that there are mesh and polar sidecar running alongside it",
    "start": "1020880",
    "end": "1027678"
  },
  {
    "text": "the sidecar will decide when and where to load and unload models",
    "start": "1027679",
    "end": "1032959"
  },
  {
    "text": "based on the usage and the current request volumes if a particular model is under heavy",
    "start": "1032959",
    "end": "1040640"
  },
  {
    "text": "load it will be scaled across more parts you can see those little circles inside",
    "start": "1040640",
    "end": "1047918"
  },
  {
    "text": "the serving runtime they represent different models if we take a look at model b in the in the blue",
    "start": "1047919",
    "end": "1055280"
  },
  {
    "text": "circle it is scaled to have two replicas so comparing to model a which is in the",
    "start": "1055280",
    "end": "1062480"
  },
  {
    "text": "green circle it can handle more inference requests",
    "start": "1062480",
    "end": "1068559"
  },
  {
    "text": "the mesh sidecar also acts as a router the model mesh store",
    "start": "1072000",
    "end": "1077840"
  },
  {
    "text": "model to part it routing table in the lcd in external fcd so when there's inference",
    "start": "1077840",
    "end": "1085520"
  },
  {
    "text": "requests coming in the sidecar will look up the routing table and it will figure out which model is",
    "start": "1085520",
    "end": "1092080"
  },
  {
    "text": "loaded into which pile id and routes the request to the correct pot according to the routing",
    "start": "1092080",
    "end": "1098400"
  },
  {
    "text": "table",
    "start": "1098400",
    "end": "1100720"
  },
  {
    "text": "and so you now may be curious to learn what kind of service runtime model mesh",
    "start": "1103520",
    "end": "1108640"
  },
  {
    "text": "can provide so out of box integration we have we will provide triton inference",
    "start": "1108640",
    "end": "1115120"
  },
  {
    "text": "server which is developed by nvidia's which is developed by nvidia this model",
    "start": "1115120",
    "end": "1121360"
  },
  {
    "text": "server can serve machine learning framework models trained by emotional frameworks such as tensorflow",
    "start": "1121360",
    "end": "1127360"
  },
  {
    "text": "pytorch tensor rt or onyx we also by default integrate with a",
    "start": "1127360",
    "end": "1133200"
  },
  {
    "text": "sounds ml server this zelda's ml server is a python based server",
    "start": "1133200",
    "end": "1138559"
  },
  {
    "text": "it can serve frameworks such as sklen xgboost or like gbm",
    "start": "1138559",
    "end": "1145520"
  },
  {
    "start": "1146000",
    "end": "1222000"
  },
  {
    "text": "so a lot of you may be very curious to know what kind of performance of model match can provide",
    "start": "1149679",
    "end": "1156240"
  },
  {
    "text": "if we co-locate multiple models into the same part will that have impact on the latency of throughput this solution can",
    "start": "1156240",
    "end": "1163600"
  },
  {
    "text": "provide so we did a performance test this performance test was done on a single node",
    "start": "1163600",
    "end": "1169840"
  },
  {
    "text": "8 cpu 64 gigabyte ram cluster and we deployed a very very simple",
    "start": "1169840",
    "end": "1175360"
  },
  {
    "text": "string model it's all it only has around 700 bytes",
    "start": "1175360",
    "end": "1180720"
  },
  {
    "text": "so if we use the traditional one model one per container deployment approach",
    "start": "1180720",
    "end": "1187360"
  },
  {
    "text": "we are limited by cpu and we can approximately deploy around 40 models in this testing cluster",
    "start": "1187360",
    "end": "1196400"
  },
  {
    "text": "sometimes if we deploy into a larger node will be eventually limited by ip addresses",
    "start": "1196400",
    "end": "1204000"
  },
  {
    "text": "but now if we move on to use model mesh deployment we are able to compact around",
    "start": "1204000",
    "end": "1210240"
  },
  {
    "text": "20k models into this testing cluster and essentially run into memory limit",
    "start": "1210240",
    "end": "1218559"
  },
  {
    "text": "in addition to the density test we also did a latency test",
    "start": "1219120",
    "end": "1224960"
  },
  {
    "start": "1222000",
    "end": "1286000"
  },
  {
    "text": "this latency test is done by running two triton serving run times",
    "start": "1225840",
    "end": "1232159"
  },
  {
    "text": "and we gradually increase the qps from 25 per second all the way to 2800 per",
    "start": "1232159",
    "end": "1238159"
  },
  {
    "text": "second each performance test will run for 60 seconds for each qps",
    "start": "1238159",
    "end": "1245440"
  },
  {
    "text": "we also gradually increase the density of the model mesh from 1000 2000 all the",
    "start": "1245440",
    "end": "1252400"
  },
  {
    "text": "way to 20 20 000. as we increase the density of the models",
    "start": "1252400",
    "end": "1258000"
  },
  {
    "text": "we can we notice that there's a slight increase in latencies but for single digit",
    "start": "1258000",
    "end": "1264559"
  },
  {
    "text": "millisecond latency inference one worker node can support about 20k models for up",
    "start": "1264559",
    "end": "1270880"
  },
  {
    "text": "to 1000 qps i also like to point out that this performance test is down on cpu nodes",
    "start": "1270880",
    "end": "1278640"
  },
  {
    "text": "normally when we run inference on gpu the performance can be increased",
    "start": "1278640",
    "end": "1285440"
  },
  {
    "start": "1286000",
    "end": "1304000"
  },
  {
    "text": "dramatically uh now model mesh is already released um as part of case",
    "start": "1288840",
    "end": "1295200"
  },
  {
    "text": "server case of 0.7 deliverable so you are all welcome",
    "start": "1295200",
    "end": "1301200"
  },
  {
    "text": "to check it out and try a model mesh",
    "start": "1301200",
    "end": "1306559"
  },
  {
    "start": "1304000",
    "end": "1380000"
  },
  {
    "text": "there's still a lot of work we want to continue to work on for the modern match",
    "start": "1309840",
    "end": "1315280"
  },
  {
    "text": "so here's the roadmap we have in mind in q4 2021",
    "start": "1315280",
    "end": "1321200"
  },
  {
    "text": "we want to have better influence and serving runtime integration and currently in order to",
    "start": "1321200",
    "end": "1327440"
  },
  {
    "text": "use model mesh each username space needs to have its own model mesh controller so",
    "start": "1327440",
    "end": "1333120"
  },
  {
    "text": "we would like to enhance the smaller mesh controller to support multiple namespace",
    "start": "1333120",
    "end": "1339600"
  },
  {
    "text": "currently the model mesh only supports downloading model from s3 storage we want to spend time to expand the",
    "start": "1339600",
    "end": "1346799"
  },
  {
    "text": "storage we support including gcs and http service next year q1",
    "start": "1346799",
    "end": "1353520"
  },
  {
    "text": "we want to spend some time to work on inference graph we also start to extend model mesh to",
    "start": "1353520",
    "end": "1360000"
  },
  {
    "text": "support transformer so users can plug in their customized pre-processed and post",
    "start": "1360000",
    "end": "1365760"
  },
  {
    "text": "process implementation we also want to make model mesh start to",
    "start": "1365760",
    "end": "1370799"
  },
  {
    "text": "support canary roller and eventually consolidate that model mesh controller with the case of main",
    "start": "1370799",
    "end": "1377440"
  },
  {
    "text": "controller",
    "start": "1377440",
    "end": "1380080"
  },
  {
    "start": "1380000",
    "end": "1419000"
  },
  {
    "text": "so you're all very welcome to contact us by visiting our website and check out our",
    "start": "1383679",
    "end": "1390320"
  },
  {
    "text": "github or slidecast or just talk to me after this talk",
    "start": "1390320",
    "end": "1396320"
  },
  {
    "text": "okay now i'm open to answer questions",
    "start": "1396320",
    "end": "1402950"
  },
  {
    "text": "[Applause]",
    "start": "1402950",
    "end": "1409679"
  },
  {
    "text": "i have two questions actually you mentioned several types of constraints like cpu",
    "start": "1409679",
    "end": "1416080"
  },
  {
    "text": "and memory constraints for cpu only entrance how does this picture changes when you",
    "start": "1416080",
    "end": "1421440"
  },
  {
    "text": "start using uh gpus difference i would imagine there will be another set of constraint",
    "start": "1421440",
    "end": "1426880"
  },
  {
    "text": "on top of this they were the one we mentioned already oh okay you can repeat the second part",
    "start": "1426880",
    "end": "1432320"
  },
  {
    "text": "how does the picture changes when you start using gpu assisted inference",
    "start": "1432320",
    "end": "1437520"
  },
  {
    "text": "in terms of the constraint in the system i'm sorry can i walk closer",
    "start": "1437520",
    "end": "1445480"
  },
  {
    "text": "so how does the picture changes when you start using gpu assisted inference",
    "start": "1445840",
    "end": "1454799"
  },
  {
    "text": "what kind of constraints or what kind of problem or bottlenecks you start seeing",
    "start": "1454880",
    "end": "1460159"
  },
  {
    "text": "okay so thank you very much for the question so the question is that i mentioned that",
    "start": "1460400",
    "end": "1465679"
  },
  {
    "text": "there's a compute resource overhead come with each inference service the question is that if we start to use gpu what's",
    "start": "1465679",
    "end": "1473360"
  },
  {
    "text": "the change um what kind of change it is about the overhead",
    "start": "1473360",
    "end": "1479200"
  },
  {
    "text": "so there's nothing changed so in this uh for example if we take a look at the diagram the model one will be deployed",
    "start": "1479200",
    "end": "1486000"
  },
  {
    "text": "into a model server so model server is the one that requires cpu or gpu for",
    "start": "1486000",
    "end": "1491279"
  },
  {
    "text": "inference so that's one independent container around inside the inference service",
    "start": "1491279",
    "end": "1497039"
  },
  {
    "text": "and alongside that model server there's another container that's the extra container running as a",
    "start": "1497039",
    "end": "1503520"
  },
  {
    "text": "side car requires extra cpu and memory so changing the model server from using",
    "start": "1503520",
    "end": "1509360"
  },
  {
    "text": "cpu to gpu doesn't really change the side cars requirement for computer",
    "start": "1509360",
    "end": "1515039"
  },
  {
    "text": "resource uh yeah but i mean if you start using inference then the gpu is going to be a",
    "start": "1515039",
    "end": "1520400"
  },
  {
    "text": "bottleneck in this inference right depending on the load so you effectively introduce another",
    "start": "1520400",
    "end": "1526240"
  },
  {
    "text": "level of bottleneck of possible bottlenecks in the system uh i was curious if you can see if you",
    "start": "1526240",
    "end": "1532159"
  },
  {
    "text": "if you saw those bottlenecks in inference and how do you address those bottlenecks",
    "start": "1532159",
    "end": "1537360"
  },
  {
    "text": "uh do you mean how when we start to use uh gpus yeah when you start using gpu yeah",
    "start": "1537360",
    "end": "1543840"
  },
  {
    "text": "so do you know if we start to use gpu as a way to use gpu resource to address those",
    "start": "1544799",
    "end": "1551760"
  },
  {
    "text": "overhead because it's a very specific like resources yeah that's a very good question so if some of you",
    "start": "1551760",
    "end": "1558000"
  },
  {
    "text": "this may lead to some discussion about slicing gpu so currently if we want to request gpu resource in kubernetes",
    "start": "1558000",
    "end": "1565840"
  },
  {
    "text": "there's no very straight straightforward or easy way to request a slice of a gpu",
    "start": "1565840",
    "end": "1571039"
  },
  {
    "text": "so um i know in the managed compute in the mesh kubernetes concept it's very",
    "start": "1571039",
    "end": "1576640"
  },
  {
    "text": "easy to think about well i have a gpu allocate to this inference service can i request a slice of the gpu and just to",
    "start": "1576640",
    "end": "1583679"
  },
  {
    "text": "use that for the sidecar the answer is that there's no very easy way to do that so",
    "start": "1583679",
    "end": "1589760"
  },
  {
    "text": "when we request gpu we request a full gpu actually in real use cases very often we notice that",
    "start": "1589760",
    "end": "1597200"
  },
  {
    "text": "even though a model server requests a full gpu but when the inference happens",
    "start": "1597200",
    "end": "1602960"
  },
  {
    "text": "it won't really use the full capacity of the gpu sometimes it doesn't even use the full capacity of the",
    "start": "1602960",
    "end": "1610000"
  },
  {
    "text": "memory so if we take a look at typical gpu most of them come with 32 gigabyte",
    "start": "1610000",
    "end": "1615679"
  },
  {
    "text": "memory so some models actually really small they are like megabyte level uh some larger ones like a bird model maybe",
    "start": "1615679",
    "end": "1622720"
  },
  {
    "text": "um maybe gigabyte level but that still wouldn't come that only consumes a small",
    "start": "1622720",
    "end": "1628960"
  },
  {
    "text": "fraction of the full gpu memory uh in terms of the computing power sometimes",
    "start": "1628960",
    "end": "1634159"
  },
  {
    "text": "is uh even less the amount of computing power you consume heavily depend on the volume of",
    "start": "1634159",
    "end": "1642399"
  },
  {
    "text": "requests that you send into the model server so when you have a lot of concurrent requests sent to the",
    "start": "1642399",
    "end": "1649120"
  },
  {
    "text": "to the model server that only has a gpu allocated to it you can notice that the gpu consumption world goes up but during",
    "start": "1649120",
    "end": "1656799"
  },
  {
    "text": "the time that there's a very small amount of volume coming to the gpu powered model server uh like you it's",
    "start": "1656799",
    "end": "1663840"
  },
  {
    "text": "very obvious that the consumption of gpu computing power decrease a lot",
    "start": "1663840",
    "end": "1668880"
  },
  {
    "text": "i think that also really depends on what kind of model server requesting that gpu",
    "start": "1668880",
    "end": "1674159"
  },
  {
    "text": "and how optimized that gpu has so that's why we",
    "start": "1674159",
    "end": "1679200"
  },
  {
    "text": "work very closely with triton because triton is developed by nvidia and they have a",
    "start": "1679200",
    "end": "1684240"
  },
  {
    "text": "lot of optimization about gpu in the trial model server all right thank you",
    "start": "1684240",
    "end": "1691278"
  },
  {
    "text": "okay one more question and then we'll have to take our break but as i walk over i'll just remind",
    "start": "1693200",
    "end": "1698399"
  },
  {
    "text": "everybody that we after this we have a break until 2 55 pacific time and then the sessions will start back up",
    "start": "1698399",
    "end": "1704799"
  },
  {
    "text": "again",
    "start": "1704799",
    "end": "1707799"
  },
  {
    "text": "so in this diagram you're talking about single model serving versus multi-modal serving",
    "start": "1711760",
    "end": "1718240"
  },
  {
    "text": "so in multi-modal surveying we're saying that multiple models running in a single",
    "start": "1718240",
    "end": "1724240"
  },
  {
    "text": "kubernetes board is that correct understanding right yes that's that's the correct understanding so what we're",
    "start": "1724240",
    "end": "1730000"
  },
  {
    "text": "trying to do is to collocate multiple models that can be served by the same model serving runtime and collocate them",
    "start": "1730000",
    "end": "1737279"
  },
  {
    "text": "together but isn't it a little bit against the kubernetes model reason i'm",
    "start": "1737279",
    "end": "1742799"
  },
  {
    "text": "saying that one thing is uh all of these model competing for the",
    "start": "1742799",
    "end": "1748960"
  },
  {
    "text": "resources because you don't never don't have isolation of resources so there is no quality of service one",
    "start": "1748960",
    "end": "1755360"
  },
  {
    "text": "model is taking more computer resources it will impact rest of the models second thing if this part goes down or",
    "start": "1755360",
    "end": "1762320"
  },
  {
    "text": "this node goes down all the model goes down together yeah that's a very good question",
    "start": "1762320",
    "end": "1767840"
  },
  {
    "text": "each model can take some different a different amount of computing service actually that's a we internally over",
    "start": "1767840",
    "end": "1774000"
  },
  {
    "text": "working group we had a long discussion so the original design of the solution is that each model of let me go back to",
    "start": "1774000",
    "end": "1781200"
  },
  {
    "text": "the to this diagram one of the original idea we have is that each model will have the",
    "start": "1781200",
    "end": "1786799"
  },
  {
    "start": "1782000",
    "end": "1872000"
  },
  {
    "text": "same amount of replicas that just spread among all the replicas belongs to a",
    "start": "1786799",
    "end": "1792320"
  },
  {
    "text": "model server and then we start to realize let's say we have model a and b and c in the same part and the model a",
    "start": "1792320",
    "end": "1799200"
  },
  {
    "text": "takes most of the request and a b and c only take like one request per hour but",
    "start": "1799200",
    "end": "1804960"
  },
  {
    "text": "because of model a need to handle really high volume so model a will drive the same serving runtime cost up into like",
    "start": "1804960",
    "end": "1812559"
  },
  {
    "text": "many replicas like 10 20 but motor b and c will be forced to scale up together so",
    "start": "1812559",
    "end": "1817679"
  },
  {
    "text": "that's that's the idea like we can we spend a lot of time discussing and that's why we moved to the model mesh",
    "start": "1817679",
    "end": "1823919"
  },
  {
    "text": "idea so we can scale up different model differently so let's think of the",
    "start": "1823919",
    "end": "1829520"
  },
  {
    "text": "diagram again the model b it has more replicas so essentially collectively",
    "start": "1829520",
    "end": "1834720"
  },
  {
    "text": "model b owns more computing resource in this serving runtime and model a only has one",
    "start": "1834720",
    "end": "1841679"
  },
  {
    "text": "replica so like collectively model a only owns like less computing resource",
    "start": "1841679",
    "end": "1847600"
  },
  {
    "text": "that's why we designed the solution in a way that each model can have different number of replicas across all the",
    "start": "1847600",
    "end": "1855600"
  },
  {
    "text": "parts that belongs to one server runtime",
    "start": "1855600",
    "end": "1859840"
  },
  {
    "text": "okay thank you very much and we will meet back here again at 2 55.",
    "start": "1861840",
    "end": "1867840"
  },
  {
    "text": "thank you",
    "start": "1867840",
    "end": "1871000"
  }
]