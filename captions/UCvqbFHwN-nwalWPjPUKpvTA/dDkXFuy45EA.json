[
  {
    "text": "all right um hey everyone welcome to our talk uh in the next 30 minutes we'll walk you through how we are building a",
    "start": "240",
    "end": "6080"
  },
  {
    "text": "scalable compute platform all the way from metal bare metal to apps we are part of the central comput team at",
    "start": "6080",
    "end": "12000"
  },
  {
    "text": "LinkedIn serving thousands of engineers uh we don't use Kubernetes in the traditional way but we extend it",
    "start": "12000",
    "end": "17760"
  },
  {
    "text": "extremely heavily um just a little warning we'll have bunch of slides bunch of text and bunch of pictures feel free",
    "start": "17760",
    "end": "24000"
  },
  {
    "text": "to take snapshots and we'll share slides towards the end as well uh all right",
    "start": "24000",
    "end": "29279"
  },
  {
    "text": "before we get started a little bit about us i'm Ron i'm based out of Toronto this is my third CubeCon first one as a",
    "start": "29279",
    "end": "36320"
  },
  {
    "text": "speaker and 2022 Detroit was my first one outside of Kubernetes I love to play",
    "start": "36320",
    "end": "42079"
  },
  {
    "text": "racket sports specifically badminton if someone wants to play uh and I also host a podcast called Soft Misadventures",
    "start": "42079",
    "end": "49360"
  },
  {
    "text": "yeah I'm one of the top followers of Ron Next podcast uh hello everyone my name is Amit i'm uh coming here from Seattle",
    "start": "49360",
    "end": "55600"
  },
  {
    "text": "i think this is my seventh or eighth CubeCon i actually I couldn't count uh I think this is my third time giving a",
    "start": "55600",
    "end": "60879"
  },
  {
    "text": "talk outside Kubernetes I like gardening in house plans but actually still outside Kubernetes I like to develop",
    "start": "60879",
    "end": "66720"
  },
  {
    "text": "cubecado plugins for you all uh maybe you're using a few of them so I want to I want to walk you",
    "start": "66720",
    "end": "74400"
  },
  {
    "text": "through LinkedIn scale because without putting things in perspective I don't think you'll get a full picture of why",
    "start": "74400",
    "end": "79520"
  },
  {
    "text": "we're building what we're building first of all I assume you're all familiar with LinkedIn uh we have over a billion",
    "start": "79520",
    "end": "85200"
  },
  {
    "text": "members on our site and these members are served by some 3,000 services and some of these are databases stream",
    "start": "85200",
    "end": "91840"
  },
  {
    "text": "processing services or microservices and these systems run on about 500,000 plus",
    "start": "91840",
    "end": "97680"
  },
  {
    "text": "servers and these servers are again a mix of Kubernetes as well as as well as our in-house orchestrator and these",
    "start": "97680",
    "end": "104240"
  },
  {
    "text": "applications run on one and a half million containers uh it's a pretty good number and they all get deployed pretty",
    "start": "104240",
    "end": "110240"
  },
  {
    "text": "frequently we have we're serving thousands of engineers internally at LinkedIn and they're doing deploys",
    "start": "110240",
    "end": "116000"
  },
  {
    "text": "multiple times a day on these applications and one interesting thing about us is that we do not run on a cloud provider we have our own data",
    "start": "116000",
    "end": "122320"
  },
  {
    "text": "centers and we run everything on bare metal we do not do virtualization uh for many reasons one of them being",
    "start": "122320",
    "end": "130000"
  },
  {
    "text": "performance so today we'll walk you through our compute platform uh you here you'll see three layers at the bottom we",
    "start": "130520",
    "end": "136959"
  },
  {
    "text": "have an infrastructure as a service layer you're not going to see any Kubernetes in that box and on top of that we have a Kubernetes cluster",
    "start": "136959",
    "end": "142879"
  },
  {
    "text": "management layer which are the concepts that you're kind of familiar with you know there are clusters pools nodes that kind of stuff and we'll talk a little",
    "start": "142879",
    "end": "149520"
  },
  {
    "text": "bit about uh some of the controllers that we built there and on top uh most of the talk we're going to dedicate this",
    "start": "149520",
    "end": "154720"
  },
  {
    "text": "talk to the workload platform layer but first let's start with the bottom two layers so I'm going to cover",
    "start": "154720",
    "end": "161040"
  },
  {
    "text": "these one by one uh I want to start with the infrastructure as a service layer and then move on to the Kubernetes",
    "start": "161040",
    "end": "166400"
  },
  {
    "text": "cluster management so in this case in the infrastructure as a service layer uh you saw a few boxes and let's go through",
    "start": "166400",
    "end": "172239"
  },
  {
    "text": "them one by one first we have an inventory manager you can think of the inventory manager as a metadata store of",
    "start": "172239",
    "end": "178400"
  },
  {
    "text": "which machines we have in our data centers so for example if a data center techni technician comes into the data",
    "start": "178400",
    "end": "184239"
  },
  {
    "text": "center and racks a machine in well that machine has to show up in a database for us to use and that's the machine name",
    "start": "184239",
    "end": "189760"
  },
  {
    "text": "you know what are the characteristics what is the hardware what's the CPU memory GPU that kind of stuff uh and on",
    "start": "189760",
    "end": "194959"
  },
  {
    "text": "top of that we have this component called compute broker compute broker is basically kind of like the heart and",
    "start": "194959",
    "end": "200720"
  },
  {
    "text": "brain of our uh data center and machine layer uh it's a centralized component that everyone talks to it's not a",
    "start": "200720",
    "end": "206720"
  },
  {
    "text": "kubernetes operator it's actually just a gRPC service it's a pretty simple one it helps you manage the pools and adds",
    "start": "206720",
    "end": "213040"
  },
  {
    "text": "remove capacity to the pools uh if you're using a cloud provider you can think of it as your you know virtual",
    "start": "213040",
    "end": "218319"
  },
  {
    "text": "machine scale set or you know autoscaling uh instance group API and so on so forth uh so the pools that we have",
    "start": "218319",
    "end": "224319"
  },
  {
    "text": "are composed of heterogeneous hardware um we don't really get fixated on like specific machine types and so on but",
    "start": "224319",
    "end": "231120"
  },
  {
    "text": "these machine types are largely interchangeable within a pool and we'll talk a little bit about this shortly so",
    "start": "231120",
    "end": "236400"
  },
  {
    "text": "each pool specifies something called a node profile which is basically what do you want out of a machine and we kind of",
    "start": "236400",
    "end": "241840"
  },
  {
    "text": "capture these minimum requirements as the lowest common denominator to configure a pool and the compute broker",
    "start": "241840",
    "end": "247360"
  },
  {
    "text": "itself is also the source of truth for machine allocation and maintenance operations like if a machine belongs to",
    "start": "247360",
    "end": "252799"
  },
  {
    "text": "a a pool compute broker is basically the database for that and if I let's say want to take a machine out for",
    "start": "252799",
    "end": "258720"
  },
  {
    "text": "maintenance or if I want to you know uh change the hardware on that machine I come to the compute broker to schedule a",
    "start": "258720",
    "end": "265040"
  },
  {
    "text": "maintenance operation and lastly we have u host health remediation well it turns out if you have that many machines some",
    "start": "265040",
    "end": "270960"
  },
  {
    "text": "machines are always going to be failing in your data centers uh so we have built automated systems to take machines out",
    "start": "270960",
    "end": "276240"
  },
  {
    "text": "of rotation um service them do upgrades on them and so on and so forth so the maintenance orchestrator that we build",
    "start": "276240",
    "end": "282800"
  },
  {
    "text": "actually has some decent ideas from Kubernetes itself uh it's an orchestrator that goes through our fleet",
    "start": "282800",
    "end": "288320"
  },
  {
    "text": "and upgrades our fleet because again when you run bare metal you have to worry about kernel upgrades security",
    "start": "288320",
    "end": "293520"
  },
  {
    "text": "upgrades and so on so the first concept that I'll talk about in the maintenance domain is the",
    "start": "293520",
    "end": "299680"
  },
  {
    "text": "concept of maintenance zones these are not like your availability zones they're actually just software update domains",
    "start": "299680",
    "end": "305759"
  },
  {
    "text": "for us so when we roll out a software update like a kernel upgrade through our fleet uh we stripe our data center to 20",
    "start": "305759",
    "end": "312240"
  },
  {
    "text": "different parts let's we call that maintenance zone 1 2 3 all the way to 20 and we roll them out roll these software",
    "start": "312240",
    "end": "318400"
  },
  {
    "text": "updates out one by one by doing so we kind of ensure that we're not taking more than let's say 5% of the capacity",
    "start": "318400",
    "end": "324160"
  },
  {
    "text": "in a particular data center out at a time so if we have a bad kernel that goes out and somehow you know the",
    "start": "324160",
    "end": "329759"
  },
  {
    "text": "machines don't come back online we only uh take down 5% of the fleet that we have uh now the compute pools that we",
    "start": "329759",
    "end": "335840"
  },
  {
    "text": "create that runs our applications the pools themselves have machines participating from each of these",
    "start": "335840",
    "end": "340880"
  },
  {
    "text": "maintenance zones and so when you look at a pool uh it's striped evenly across our data center however that said the",
    "start": "340880",
    "end": "347039"
  },
  {
    "text": "Kubernetes clusters themselves are still very much a fault domain right when you deploy a bad operator or when you mess",
    "start": "347039",
    "end": "352880"
  },
  {
    "text": "up your web hook configuration you're kind of breaking that entire cluster so there's that uh I want to talk a little",
    "start": "352880",
    "end": "359199"
  },
  {
    "text": "bit about the coordinated maintenance operation uh operations that we have so as I mentioned mentioned earlier uh",
    "start": "359199",
    "end": "364800"
  },
  {
    "text": "humans are not in the loop for doing maintenance in our fleets all our systems are rather automated if",
    "start": "364800",
    "end": "370080"
  },
  {
    "text": "something goes wrong again the systems talk to each other to figure out what they need to do so we have this concept",
    "start": "370080",
    "end": "375759"
  },
  {
    "text": "of a disruption disruption basically means you want to take the control from Kubernetes or whoever is using that",
    "start": "375759",
    "end": "382240"
  },
  {
    "text": "machine away from that person and give it to a maintenance actor so we have two types of disruptions in our fleet one of",
    "start": "382240",
    "end": "388240"
  },
  {
    "text": "them is planned maintenance operations this one this is basically you know your regular OS kernel upgrades your you know",
    "start": "388240",
    "end": "394000"
  },
  {
    "text": "switch upgrades that are happening in your racks hardware u decommissioning operations that we have or maybe the",
    "start": "394000",
    "end": "399520"
  },
  {
    "text": "cublet upgrades themselves as well and the unpl unplanned operations as you can imagine there you know when stuff goes",
    "start": "399520",
    "end": "405680"
  },
  {
    "text": "really wrong and we need to get this machine out of the way so these are usually the hostile remediation operations so this picture kind of shows",
    "start": "405680",
    "end": "412080"
  },
  {
    "text": "you um what happens when we receive a disruption so in this case you have if several actors a machine disruptor comes",
    "start": "412080",
    "end": "418639"
  },
  {
    "text": "in creates a disruption and this disruption gets sent to the compute worker so now comput broker knows about",
    "start": "418639",
    "end": "424160"
  },
  {
    "text": "this disruption and our cluster management layer observes that okay somebody wants to take this node away",
    "start": "424160",
    "end": "429360"
  },
  {
    "text": "limit core don't drain the node and we approve the disruption we give a thumbs up the maintenance actor sees that it",
    "start": "429360",
    "end": "434960"
  },
  {
    "text": "does its thing but whatever it needs to do either reboot the machine reimage whatever it needs to do and then they remove the disruption and then we put",
    "start": "434960",
    "end": "441039"
  },
  {
    "text": "the machine back in rotation so all that is automated now I want to talk a little",
    "start": "441039",
    "end": "446160"
  },
  {
    "text": "bit about how we organize our Kubernetes clusters i bet you're all kind of curious about that uh we don't use any",
    "start": "446160",
    "end": "451280"
  },
  {
    "text": "specific Kubernetes DRO we just take the open source bits we configure them in our own way we don't use any cube atom",
    "start": "451280",
    "end": "456880"
  },
  {
    "text": "or cluster API uh a lot of the features in cluster API are actually stuff that we don't uh need uh we use a single",
    "start": "456880",
    "end": "462479"
  },
  {
    "text": "provisioner you know we we don't run on cloud providers and so on so our stack is pretty simplistic in this regard and",
    "start": "462479",
    "end": "467599"
  },
  {
    "text": "we want to keep it simple uh so this works well with our cluster bootstrapping stack we we also tune a",
    "start": "467599",
    "end": "473039"
  },
  {
    "text": "lot of flags in API server and CD um in terms of cluster size which is I assume is a hot topic we'd like to push our",
    "start": "473039",
    "end": "479520"
  },
  {
    "text": "clusters to 5K and we want to actually push them a little further and the main motivator behind that is to reduce the",
    "start": "479520",
    "end": "485680"
  },
  {
    "text": "hardware fragmentation across clusters like if I need certain hardware used by a certain customer I want to kind of",
    "start": "485680",
    "end": "491199"
  },
  {
    "text": "keep it all in the same place and another reason is uh we want to do that is um clusters are multi-tenant and",
    "start": "491199",
    "end": "496800"
  },
  {
    "text": "customers grow in place you deploy an app to a cluster well we want to give it a little bit of a wiggle room to grow",
    "start": "496800",
    "end": "502479"
  },
  {
    "text": "and uh our clusters are pretty mixed up we run stateless stateful uh batch operate batch workloads all in the same",
    "start": "502479",
    "end": "508720"
  },
  {
    "text": "cluster and the cublet upgrades that we have in our fleet again uh as I covered earlier they happen as part of the OS",
    "start": "508720",
    "end": "514800"
  },
  {
    "text": "maintenance we regularly upgrade uh cublet itself and we have a centralized hub cluster to manage all the other",
    "start": "514800",
    "end": "520560"
  },
  {
    "text": "clusters that we have and manage which application goes where which Ronach is going to talk a little bit shortly",
    "start": "520560",
    "end": "526959"
  },
  {
    "text": "so we use the Kubernetes resource management uh concept we use the KRM APIs we've built our in-house",
    "start": "526959",
    "end": "532959"
  },
  {
    "text": "controllers to manage the pools and clusters that we have uh here's a pretty simple example uh we modeled the",
    "start": "532959",
    "end": "538080"
  },
  {
    "text": "Kubernetes pools as a custom resource in our management cluster and these changes portion of them gets converted into a",
    "start": "538080",
    "end": "544320"
  },
  {
    "text": "compute broker custom resource and then the comput broker pool gets synchronized to the gRPC API that comput broker",
    "start": "544320",
    "end": "550800"
  },
  {
    "text": "offers that I mentioned earlier uh however you use this custom resource to specify other like Kubernetes related settings for example what demon sets do",
    "start": "550800",
    "end": "557440"
  },
  {
    "text": "I have what node labels do I have right and um when you model things this way adjusting capacity on a pool becomes",
    "start": "557440",
    "end": "563600"
  },
  {
    "text": "really easy you just change an integer on a YAML file you submit it and then you wait for a minute and suddenly you have bare metal machines showing up in",
    "start": "563600",
    "end": "570000"
  },
  {
    "text": "your cluster pretty magical i want to touch on how we scale Kubernetes itself um as I mentioned we",
    "start": "570000",
    "end": "577279"
  },
  {
    "text": "were running pretty large clusters and the clusters the control planes themselves are shared resources",
    "start": "577279",
    "end": "582640"
  },
  {
    "text": "technically if your control plane goes down it's bad time for basically everybody right um so when our customers",
    "start": "582640",
    "end": "590160"
  },
  {
    "text": "bring their own operators into our ecosystem we heavily review their arbback so we can kind of control what they can and cannot do and on top of",
    "start": "590160",
    "end": "596880"
  },
  {
    "text": "that we use the APF which is API priority and fairness to ensure everyone is kind of staying within their limits",
    "start": "596880",
    "end": "601920"
  },
  {
    "text": "now is a hard topic um again it doesn't have a lot of these cool mechanisms to restrict load u and CD also happens to",
    "start": "601920",
    "end": "609519"
  },
  {
    "text": "be the first bottleneck when you try to grow your clusters um as you submit more events you know there's more pod turnurn",
    "start": "609519",
    "end": "615040"
  },
  {
    "text": "and all that like yourd will get overloaded uh so one thing that we've done is we've increased storage uh limit",
    "start": "615040",
    "end": "621040"
  },
  {
    "text": "from 8 gigs to 16 gigs and we're planning to potentially increase it further uh we rund on SSDs so uh based",
    "start": "621040",
    "end": "628000"
  },
  {
    "text": "on our stress testing this seems to be okay and uh we also built our internal",
    "start": "628000",
    "end": "633040"
  },
  {
    "text": "backup restore system so in case something goes uh catastrophically wrong we are going to resort to that and",
    "start": "633040",
    "end": "638560"
  },
  {
    "text": "controller scalability also remains to be uh an active topic that we're trying to figure out because controllers we run",
    "start": "638560",
    "end": "644399"
  },
  {
    "text": "many of them and you can't infinitely scale controllers actually funny enough right now there's another talk happening",
    "start": "644399",
    "end": "649440"
  },
  {
    "text": "elsewhere about horizontal cu controller scaling and it's not our sole problem right now so uh this bothers us quite a",
    "start": "649440",
    "end": "657440"
  },
  {
    "text": "bit and we're working on it and I'll pass it to Ronak to talk about workload platforms so for the rest of the talk",
    "start": "657440",
    "end": "663360"
  },
  {
    "text": "we'll focus on our workload platform layers so we use Kubernetes to run our stateless stateful workloads as well as",
    "start": "663360",
    "end": "668720"
  },
  {
    "text": "jobs now we don't want our app developers to be Kubernetes experts or worry about how to craft the exact",
    "start": "668720",
    "end": "674399"
  },
  {
    "text": "deployment spec so what we do is we create custom resources which we creatively named as LI deployments and",
    "start": "674399",
    "end": "680000"
  },
  {
    "text": "all stateful sets so users only have access to these two custom resources when they're deploying stateless stateful services uh these are exposed",
    "start": "680000",
    "end": "686880"
  },
  {
    "text": "to the users and they don't have any permissions to any other native Kubernetes resources at the same time we",
    "start": "686880",
    "end": "692000"
  },
  {
    "text": "also want to cater to other platform teams like Spark machine learning infrastructure teams who want to build",
    "start": "692000",
    "end": "697200"
  },
  {
    "text": "their own platform on top of Kubernetes so we also support those teams by providing volcano as a batch scheduler",
    "start": "697200",
    "end": "702720"
  },
  {
    "text": "to help with things like fair scheduling gang scheduling and we have built our own regional job queue as well as kota",
    "start": "702720",
    "end": "708880"
  },
  {
    "text": "systems the reason we built our own is because the systems that exist in volcano today they are cluster scoped whereas we have hundreds of clusters",
    "start": "708880",
    "end": "715040"
  },
  {
    "text": "running within a region and we need kas to be effective across all of those for the rest of the talk we'll focus more on",
    "start": "715040",
    "end": "720880"
  },
  {
    "text": "how we run services on Kubernetes now about a decade ago LinkedIn started building its own",
    "start": "720880",
    "end": "726880"
  },
  {
    "text": "container runtime and auler this is before Docker and Kubernetes days we have been running all of our stateless",
    "start": "726880",
    "end": "732320"
  },
  {
    "text": "and stateful applications on top of that system and it has been serving us pretty well so far over the last few years we",
    "start": "732320",
    "end": "738000"
  },
  {
    "text": "started migration to Kubernetes and we want to do this migration without any downtime to the life site we want to do",
    "start": "738000",
    "end": "743519"
  },
  {
    "text": "this centrally with automation with almost no user involvement and at least for the stateless applications and we",
    "start": "743519",
    "end": "749680"
  },
  {
    "text": "want to challenge legacy requirements as we go ahead so that we can reduce tech debt along the way at this point we have",
    "start": "749680",
    "end": "755279"
  },
  {
    "text": "more than half our stateless fleet running on Kubernetes and we have several stateful systems running in production on Kubernetes as",
    "start": "755279",
    "end": "761800"
  },
  {
    "text": "well now before we go into how we run these services on Kubernetes it's important to talk about our internal",
    "start": "761800",
    "end": "767680"
  },
  {
    "text": "service infrastructure at LinkedIn we don't use several Kubernetes features like services DNS configs and secret",
    "start": "767680",
    "end": "775839"
  },
  {
    "text": "management or network policies and the reasons are twofold one these features work really well within the boundary of",
    "start": "775839",
    "end": "782240"
  },
  {
    "text": "a single cluster but when you span across hundreds of clusters the overhead becomes really large for example making",
    "start": "782240",
    "end": "788240"
  },
  {
    "text": "Kubernetes services work across all of these cluster boundaries becomes complicated really fast the other thing",
    "start": "788240",
    "end": "794240"
  },
  {
    "text": "is we have an equivalent of all of these offerings at LinkedIn which have been scaled to all of our regions and they",
    "start": "794240",
    "end": "800000"
  },
  {
    "text": "have been hardened over the years so we have been developing these offerings independent to Kubernetes and then we integrate Kubernetes with it to run our",
    "start": "800000",
    "end": "806160"
  },
  {
    "text": "applications on top so Kubernetes becomes the primary pod orchestrator for us and we don't use",
    "start": "806160",
    "end": "812880"
  },
  {
    "text": "several of these Kubernetes features that I mentioned but we really appreciate the flexibility that it offers so that we can extend Kubernetes",
    "start": "812880",
    "end": "819040"
  },
  {
    "text": "for our use cases now let's first talk about stateful on Kubernetes uh it's been an ongoing meme",
    "start": "819040",
    "end": "825760"
  },
  {
    "text": "that Kubernetes does not really work for stateful systems especially the ones that store data on local disk now we",
    "start": "825760",
    "end": "831760"
  },
  {
    "text": "have several data systems like Kafka some of you might have heard of that uh Espresso Wen Pino etc all of these",
    "start": "831760",
    "end": "838800"
  },
  {
    "text": "systems store data on local disk and we do local disk because of performance reasons these systems are replicated",
    "start": "838800",
    "end": "844160"
  },
  {
    "text": "they're sharded so any part eviction or update requires coordination simple PDBs",
    "start": "844160",
    "end": "849760"
  },
  {
    "text": "or stateful sets don't quite cut it at the same time we don't want to ask all of our data systems teams running data",
    "start": "849760",
    "end": "856079"
  },
  {
    "text": "systems to run learn Kubernetes and write Kubernetes controllers so what we did instead is build a generic stateful",
    "start": "856079",
    "end": "862480"
  },
  {
    "text": "controller uh that integrates with shard managers that teams rely on now stateful teams are really good at managing data",
    "start": "862480",
    "end": "868959"
  },
  {
    "text": "systems they know how to build these shard managers our controller interacts with these shard managers through our",
    "start": "868959",
    "end": "874160"
  },
  {
    "text": "custom protocol and handles things like pod updates version updates and even eviction and the maintenance aspects",
    "start": "874160",
    "end": "879519"
  },
  {
    "text": "that Ameth mentioned earlier u we gave a talk on how we built uh this operator",
    "start": "879519",
    "end": "884959"
  },
  {
    "text": "last year in cubecon North America and we also published a blog post we highly encourage you to check that out to learn",
    "start": "884959",
    "end": "890440"
  },
  {
    "text": "more u let's talk about stateless on Kubernetes so on as for stateless and",
    "start": "890440",
    "end": "895920"
  },
  {
    "text": "kubernetes a user would typically specify their simple spec in 10 lines for the alli deployment custom resource",
    "start": "895920",
    "end": "902399"
  },
  {
    "text": "that we have we also offer various part orchestration capabilities in it like canary for instance where you can say I",
    "start": "902399",
    "end": "908399"
  },
  {
    "text": "want to ramp a change only up to 10% validate that it looks good and then go forward we use clone set under the hood",
    "start": "908399",
    "end": "915120"
  },
  {
    "text": "instead of deployments because clone set offers volume claim templates so that we can use a PVC pod which you cannot do",
    "start": "915120",
    "end": "921279"
  },
  {
    "text": "with deployments and then we translate this clone set to about 500 plus line of podsp spec we rely on init containers",
    "start": "921279",
    "end": "928240"
  },
  {
    "text": "pretty heavily to integrate with the rest of our infrastructure we create default volume mounts environment variable so that an application can get",
    "start": "928240",
    "end": "935760"
  },
  {
    "text": "everything it needs to run in Kubernetes without having to worry too much about changing it going from our legacy system",
    "start": "935760",
    "end": "941600"
  },
  {
    "text": "onto the new stack let's go through a few user workflows um",
    "start": "941600",
    "end": "946959"
  },
  {
    "text": "the first thing that an app developer does at LinkedIn is they would write their own manifest they would check it into their repo as part of our build",
    "start": "946959",
    "end": "953440"
  },
  {
    "text": "process we also publish these Helen charts in the Helm repo now because we are serving several thousand engineers",
    "start": "953440",
    "end": "959440"
  },
  {
    "text": "we want to focus a lot more on validation and prevent people from making mistakes even before the code is",
    "start": "959440",
    "end": "964639"
  },
  {
    "text": "checked into production so what happens is when you have a PR and let's say you don't specify a field that is mandatory",
    "start": "964639",
    "end": "971680"
  },
  {
    "text": "in the manifest we have a GitHub action that would call out saying hey you're missing memory there for example and",
    "start": "971680",
    "end": "976880"
  },
  {
    "text": "it'll block the PR from getting merged there are several such checks which run and beyond schema checks what we also do",
    "start": "976880",
    "end": "982800"
  },
  {
    "text": "is we have similar checks that we do in our web hooks we run them on the PRs themselves so even before the request",
    "start": "982800",
    "end": "989680"
  },
  {
    "text": "hits the API server you already see in your PR what are all the things that you're going to be blocked by this has",
    "start": "989680",
    "end": "995040"
  },
  {
    "text": "resulted in significant reduction of user support load for us because now users can see what's wrong and they can",
    "start": "995040",
    "end": "1000320"
  },
  {
    "text": "go and fix it um we use a namespace per application",
    "start": "1000320",
    "end": "1006880"
  },
  {
    "text": "the reason we do that is because all of our permissions are based on namespaces and a user only has access to namespaces",
    "start": "1006880",
    "end": "1012560"
  },
  {
    "text": "which has applications that they own if a namespace for an application does not exist what we do is we create for the",
    "start": "1012560",
    "end": "1018639"
  },
  {
    "text": "first time and we route that to a cluster so let's walk through this workflow for a second a user will never be requesting a namespace they don't",
    "start": "1018639",
    "end": "1024959"
  },
  {
    "text": "even know about namespaces they don't need to know about it they go to our deployment orchestration service and they'll say I want to deploy my",
    "start": "1024959",
    "end": "1031120"
  },
  {
    "text": "application if we get to know that the namespace does not exist for this app we get information like what is the app",
    "start": "1031120",
    "end": "1037520"
  },
  {
    "text": "name what tenant meaning is a stateless or stateful application and the app owners get to specify what node profile",
    "start": "1037520",
    "end": "1042959"
  },
  {
    "text": "they want so mentioned earlier a node profile is basically a combination of what is the machine type you want to use",
    "start": "1042959",
    "end": "1049280"
  },
  {
    "text": "and hardware configuration you want to run on so we get that information we have a controller at the hub which also",
    "start": "1049280",
    "end": "1054880"
  },
  {
    "text": "gets information about your authorization rules from our regional authorization service what it'll do next",
    "start": "1054880",
    "end": "1060160"
  },
  {
    "text": "is it will find a cluster which has a pool that matches the node profile that you have and if one doesn't exist it'll",
    "start": "1060160",
    "end": "1065679"
  },
  {
    "text": "create one and it'll propagate the name space as well as role bindings to that particular cluster the next step that happens is",
    "start": "1065679",
    "end": "1072880"
  },
  {
    "text": "the app getting deployed now the deployment orchestration service specifies the application version passes",
    "start": "1072880",
    "end": "1078080"
  },
  {
    "text": "it to Argo CD within the cluster argo CD will sync all the manifests from the Helm repo apply to the API server and",
    "start": "1078080",
    "end": "1084880"
  },
  {
    "text": "then our controllers take over our controllers interact with all of those services mentioned above those are",
    "start": "1084880",
    "end": "1090320"
  },
  {
    "text": "regional services deployed and all controllers running in every cluster talk to them and then we translate these",
    "start": "1090320",
    "end": "1096000"
  },
  {
    "text": "custom resources to the part specs that I mentioned earlier we have several logs and events that get emitted as part of",
    "start": "1096000",
    "end": "1102080"
  },
  {
    "text": "our controllers and applications all of their make all of them make their way through Kafka to Azure data explorer",
    "start": "1102080",
    "end": "1108240"
  },
  {
    "text": "what we do is we create default dashboard for every application so you could simply go to a link type in your",
    "start": "1108240",
    "end": "1113760"
  },
  {
    "text": "app name and you get a bunch of events logs as well as metrics like how much CPU memory your application is using how",
    "start": "1113760",
    "end": "1120000"
  },
  {
    "text": "many times your containers are restarting why they're restarting how many replicas are desired and how many are available so all of this is given to",
    "start": "1120000",
    "end": "1126480"
  },
  {
    "text": "the app owners out of the box uh a quick note on Argo City uh so",
    "start": "1126480",
    "end": "1131679"
  },
  {
    "text": "we run Argo City one per cluster so it manages all applications within that cluster uh it has served us pretty well",
    "start": "1131679",
    "end": "1138080"
  },
  {
    "text": "so far while the scale was small but what we have started to see is that as the number of objects in the cluster",
    "start": "1138080",
    "end": "1143440"
  },
  {
    "text": "grows the application sync times have been going really high the other problem we start seeing is that as the number of",
    "start": "1143440",
    "end": "1149760"
  },
  {
    "text": "replicas within an application grow the health status sync is taking too long and this has resulted into not the ideal",
    "start": "1149760",
    "end": "1156240"
  },
  {
    "text": "user experience that we want and it's becoming more of an operational word for us so what we're doing now is we're writing our own GitOps engine and we'll",
    "start": "1156240",
    "end": "1163120"
  },
  {
    "text": "be replacing Argo CD for majority of the app deployments we'll still use it for some of the infra pieces but not",
    "start": "1163120",
    "end": "1168559"
  },
  {
    "text": "necessarily the end user applications uh let's talk about failures and categorizations for a",
    "start": "1168559",
    "end": "1174559"
  },
  {
    "text": "second now going back to again serving several thousand users u if we don't specify why an application rollout",
    "start": "1174559",
    "end": "1181679"
  },
  {
    "text": "failed we would be getting a lot of support requests so what we do is uh for anything that is a validation error as",
    "start": "1181679",
    "end": "1188240"
  },
  {
    "text": "soon as the manifest is attempted to be applied we fail it right away specifying why it's a bad manifest and why the",
    "start": "1188240",
    "end": "1193840"
  },
  {
    "text": "deployment will fail if it's a terminal error then we fail it on the first reconciliation specifying it's a",
    "start": "1193840",
    "end": "1199039"
  },
  {
    "text": "terminal error and if it's a transient error or an application health issue in that case we use progress deadline",
    "start": "1199039",
    "end": "1204640"
  },
  {
    "text": "seconds we implemented our own progress deadline seconds on top of our custom resources what it does is that if it",
    "start": "1204640",
    "end": "1210240"
  },
  {
    "text": "observes there is no progress made for a given amount of time then it fails that deployment in addition to failing that",
    "start": "1210240",
    "end": "1216000"
  },
  {
    "text": "rollout what we also do is we categorize what the failure is and whether it's an app issue or an infra issue so if you",
    "start": "1216000",
    "end": "1222080"
  },
  {
    "text": "see in the conditions so what we do is all of these app categorization they make their way to our object conditions",
    "start": "1222080",
    "end": "1228080"
  },
  {
    "text": "and we show user uh user readable errors saying this application failed because you have 18 parts which are unhealthy on",
    "start": "1228080",
    "end": "1234240"
  },
  {
    "text": "this version and this is an app category all of these condition we don't expect users to necessarily go and query every",
    "start": "1234240",
    "end": "1240400"
  },
  {
    "text": "single object so what happens is our deployment orchestration service will take all of this data and send this to",
    "start": "1240400",
    "end": "1245840"
  },
  {
    "text": "the UI so a user goes to the UI clicks deploy deploy fail they actually see exactly why it failed if they went and",
    "start": "1245840",
    "end": "1252640"
  },
  {
    "text": "inspected the object in the cluster they see the exact same thing",
    "start": "1252640",
    "end": "1257760"
  },
  {
    "text": "now because we have custom resources and we have application running across several hundred clusters we don't want",
    "start": "1257760",
    "end": "1264240"
  },
  {
    "text": "users to have to figure out where the app is how to query it how many parts to list so we provide a cubectl plugin that",
    "start": "1264240",
    "end": "1270640"
  },
  {
    "text": "application owners rely on the semantics of the plug-in are very similar to cubectl but instead of cubectl a user",
    "start": "1270640",
    "end": "1276720"
  },
  {
    "text": "would type cubectl in and then they would do something like get pods get alli deployments get ally stateful sets",
    "start": "1276720",
    "end": "1282880"
  },
  {
    "text": "um what we do is behind the scenes we figure out where the app is running and we scatter gather all of that data and",
    "start": "1282880",
    "end": "1288320"
  },
  {
    "text": "display it to the user in addition to the default commands that a cubectl plug-in would provide we also have some",
    "start": "1288320",
    "end": "1294240"
  },
  {
    "text": "custom things like status so a user can type cubectl and status my alli deployment the alli deployment name uh",
    "start": "1294240",
    "end": "1300960"
  },
  {
    "text": "you don't see the full picture here but on the right side that is kind of the summary we present to the user we say this is the alli deployment name these",
    "start": "1300960",
    "end": "1307679"
  },
  {
    "text": "are the desired specs you have whether it's healthy or unhealthy and if it's unhealthy here's why and all of this",
    "start": "1307679",
    "end": "1313120"
  },
  {
    "text": "information is propagated to the user we also have a system that runs within each region it's watching all of these custom",
    "start": "1313120",
    "end": "1320240"
  },
  {
    "text": "resources pods and nodes and it collects them in a central database we built a custom UI on top that users go to to",
    "start": "1320240",
    "end": "1326720"
  },
  {
    "text": "debug their applications so instead of having to just use a CLI a user can simply go to a UI and get a bird's eye",
    "start": "1326720",
    "end": "1332880"
  },
  {
    "text": "view of everything at the same time that UI doesn't hit our API servers which allows us to scale API server and",
    "start": "1332880",
    "end": "1338880"
  },
  {
    "text": "control plane it hits that internal API that we've built which is collecting all of this",
    "start": "1338880",
    "end": "1344440"
  },
  {
    "text": "information now we also spent a lot of time on building API guardrails so if",
    "start": "1344440",
    "end": "1349520"
  },
  {
    "text": "cubectl deleting a resource would result in an outage then we have delete protection for it so this includes all",
    "start": "1349520",
    "end": "1355440"
  },
  {
    "text": "of our custom resources that are userfacing every single namespace even for admins and custom resources that we",
    "start": "1355440",
    "end": "1361760"
  },
  {
    "text": "built ourselves we also have other protections in place to prevent users from having an incident by just fat",
    "start": "1361760",
    "end": "1368240"
  },
  {
    "text": "fingering for example i'm sure everyone would have had gone through a time where instead of thousand you press 10 uh and",
    "start": "1368240",
    "end": "1374640"
  },
  {
    "text": "pressed enter and something bad happened so we prevent scale down operations in one shot beyond x% and typically that x%",
    "start": "1374640",
    "end": "1382080"
  },
  {
    "text": "relies on how big your application is we also have an upper bound on how much max",
    "start": "1382080",
    "end": "1387200"
  },
  {
    "text": "surge and canary percentage you can use so that you don't go over your kota",
    "start": "1387200",
    "end": "1392960"
  },
  {
    "text": "so what's next um so so far we have been intentionally running each application within one cluster that's been an",
    "start": "1392960",
    "end": "1399360"
  },
  {
    "text": "intentional choice to keep operational simplicity what we want to do moving forward is federate these workloads",
    "start": "1399360",
    "end": "1405039"
  },
  {
    "text": "across multiple clusters we want to align our clusters with maintenance zones so we get two sets of alignments",
    "start": "1405039",
    "end": "1410880"
  },
  {
    "text": "one we don't have a single cluster failure taking down an entire application the second part is every",
    "start": "1410880",
    "end": "1416400"
  },
  {
    "text": "roll out we do as part of data plane or control plane it also only impacts up to 5% of an application and no more it also",
    "start": "1416400",
    "end": "1423360"
  },
  {
    "text": "allows an application to grow way beyond its cluster size and we have several applications that would grow beyond a cluster size uh it also helps with",
    "start": "1423360",
    "end": "1430480"
  },
  {
    "text": "machine types getting fragmented across clusters so let's say if I need five GPUs three are in one cluster two are",
    "start": "1430480",
    "end": "1436159"
  },
  {
    "text": "the other one if we fra federate a workload we can actually utilize all of those machines too now by default we",
    "start": "1436159",
    "end": "1443679"
  },
  {
    "text": "actually allow applications to burst so they can use more CPU if it's available on the host while this helps with",
    "start": "1443679",
    "end": "1449200"
  },
  {
    "text": "efficiency we have found that applications that care about performance it's not enough it doesn't give them predictability and it sometimes results",
    "start": "1449200",
    "end": "1456000"
  },
  {
    "text": "into noisy neighbor problems so what we doing is we rolling out CPU pinning for these applications and one thing we do",
    "start": "1456000",
    "end": "1461760"
  },
  {
    "text": "is we only pin physical cores we don't pin logical coursees so it gives us more isolation than otherwise we're also",
    "start": "1461760",
    "end": "1468080"
  },
  {
    "text": "building uh part CNI or IPv VLAN based part IPs where every pod gets an IPv6",
    "start": "1468080",
    "end": "1474000"
  },
  {
    "text": "address that is globally routable within our entire production regions so still we don't have to worry about cluster",
    "start": "1474000",
    "end": "1479120"
  },
  {
    "text": "tocluster routing it's all just a flat network and cubeception uh we want to run Kubernetes control plane as pods in",
    "start": "1479120",
    "end": "1485919"
  },
  {
    "text": "a management cluster this makes managing clusters super easy uh especially when",
    "start": "1485919",
    "end": "1491120"
  },
  {
    "text": "we have several of them in a region and allows us to stack components resulting in more efficiency",
    "start": "1491120",
    "end": "1498400"
  },
  {
    "text": "uh let's talk about migration lessons real quick uh so one thing which we have learned so far is always start early and",
    "start": "1498400",
    "end": "1504480"
  },
  {
    "text": "make incremental progress because when you're migrating something from a decade or legacy system to Kubernetes there are",
    "start": "1504480",
    "end": "1511039"
  },
  {
    "text": "several things that you'll find that you didn't expect to u and I'm sure we have we all have fair share of those",
    "start": "1511039",
    "end": "1516240"
  },
  {
    "text": "learnings um one thing which we also realized is migrating the first 50% is the easiest part of the job and the",
    "start": "1516240",
    "end": "1522400"
  },
  {
    "text": "remaining 50% like that's a long tail which takes really long we want to also be pragmatic about the tech that we",
    "start": "1522400",
    "end": "1528720"
  },
  {
    "text": "choose to solve because if we chose to solve everything along the way then we wouldn't necessarily finish the",
    "start": "1528720",
    "end": "1533919"
  },
  {
    "text": "migration at all so one example is we wanted to generate container images for each application instead of having and",
    "start": "1533919",
    "end": "1540159"
  },
  {
    "text": "teaching every app owner about how to write docker files we have an automated system that actually automatically",
    "start": "1540159",
    "end": "1545760"
  },
  {
    "text": "generates a docker file creates container images on the fly and that's what we use",
    "start": "1545760",
    "end": "1550880"
  },
  {
    "text": "we also want to be intentional about what Kubernetes features we use and expose to the users if we expose raw",
    "start": "1550880",
    "end": "1556640"
  },
  {
    "text": "Kubernetes to users it wouldn't necessarily work for our scale invest in guardrails when you're",
    "start": "1556640",
    "end": "1561919"
  },
  {
    "text": "exposing APIs to users because they would use it in every which way beyond what you would expect and I'm sure we",
    "start": "1561919",
    "end": "1567600"
  },
  {
    "text": "all have experienced that uh and develop good user guides and self-s serve troubleshooting it goes a long way and",
    "start": "1567600",
    "end": "1573360"
  },
  {
    "text": "reduces support load significantly lastly uh thank you so much for coming to the talk i think we",
    "start": "1573360",
    "end": "1579279"
  },
  {
    "text": "have some time for questions and there's a mic in the middle and also we have some other resources we encourage you to",
    "start": "1579279",
    "end": "1585679"
  },
  {
    "text": "go and check out thank you so much",
    "start": "1585679",
    "end": "1590039"
  },
  {
    "text": "right yeah go ahead thanks a lot for the talk i have a quick question you mentioned uh um that you have to tune",
    "start": "1594640",
    "end": "1599919"
  },
  {
    "text": "API 7 and flags i wanted to I was wondering if you had considered um",
    "start": "1599919",
    "end": "1605039"
  },
  {
    "text": "storing the events the communities events to another cluster and if you have already done that if you went back",
    "start": "1605039",
    "end": "1610080"
  },
  {
    "text": "from it why yeah I can take that sorry yeah I can take that one um so we we do",
    "start": "1610080",
    "end": "1615679"
  },
  {
    "text": "currently run uh separate CD events clusters for at least one of the clusters that we have um but we're",
    "start": "1615679",
    "end": "1621600"
  },
  {
    "text": "actually planning to merge it back because we are not liking having two CDs um they're you know you can't use the",
    "start": "1621600",
    "end": "1627840"
  },
  {
    "text": "same disc if you're running two CDs side by side you're just going to you're not going get that much performance out of it so instead we're going the route of",
    "start": "1627840",
    "end": "1633840"
  },
  {
    "text": "increasing the database size okay great another question I had was the operating",
    "start": "1633840",
    "end": "1639120"
  },
  {
    "text": "system that you use on your on your bare metal boxes which one uh so it's all Linux so we run just Linux are you are",
    "start": "1639120",
    "end": "1646960"
  },
  {
    "text": "you asking specifically about which distribution of Linux we run yeah exactly yeah uh we run Azure Linux which",
    "start": "1646960",
    "end": "1652799"
  },
  {
    "text": "one sorry azure Linux so Azure has a Linux offering so we learn Azure Linux yeah thanks a lot thank you",
    "start": "1652799",
    "end": "1661480"
  },
  {
    "text": "thanks for the talk uh crazy to see the scale at which Kubernetes can run really",
    "start": "1662799",
    "end": "1669120"
  },
  {
    "text": "cool um so you are using uh open source technology but we also saw a couple of",
    "start": "1669120",
    "end": "1676159"
  },
  {
    "text": "in-house uh developments uh have you considered to",
    "start": "1676159",
    "end": "1681279"
  },
  {
    "text": "make it generic and donate it to the open source community especially for me the uh the cube ADM cluster installation",
    "start": "1681279",
    "end": "1689480"
  },
  {
    "text": "technology sounds really interesting so uh I would like to know more about it but but I can imagine some of it is",
    "start": "1689480",
    "end": "1697039"
  },
  {
    "text": "proprietary but also maybe you would like to donate something make it generic",
    "start": "1697039",
    "end": "1702559"
  },
  {
    "text": "what are your considerations uh in that yeah yeah i would say that like we um in",
    "start": "1702559",
    "end": "1708159"
  },
  {
    "text": "the past we considered like donating parts of the stuff that we have but uh",
    "start": "1708159",
    "end": "1713440"
  },
  {
    "text": "for example we developed an in-house HCD operator but right as we were doing that the an open source HCD operator uh",
    "start": "1713440",
    "end": "1720640"
  },
  {
    "text": "effort just started so we're you know uh trying to participate to that as much as possible but for the most part a lot of",
    "start": "1720640",
    "end": "1726320"
  },
  {
    "text": "the cluster management stack that we have is bespoke for a reason that wouldn't widely apply and there are actually really cool tools out there",
    "start": "1726320",
    "end": "1732080"
  },
  {
    "text": "that let you do stuff like cubeception or you know cluster API is actually pretty good for most use cases",
    "start": "1732080",
    "end": "1739760"
  },
  {
    "text": "thanks and so you mentioned earlier that you're",
    "start": "1739760",
    "end": "1746080"
  },
  {
    "text": "using arbback for your access control you know that works pretty well in a name space code but the moment you need",
    "start": "1746080",
    "end": "1751360"
  },
  {
    "text": "to go beyond that you know somehow you lose that fine grain access and I wanted to ask do you use any other web hooks or",
    "start": "1751360",
    "end": "1757279"
  },
  {
    "text": "anything else so we actually considered writing our own uh web hook for authorization so instead of relying on",
    "start": "1757279",
    "end": "1763039"
  },
  {
    "text": "kumar's arbback at all we basically have this web hook based authorization so far every application",
    "start": "1763039",
    "end": "1769600"
  },
  {
    "text": "lives within its own namespace an application can also have multiple namespaces because you can say I want a pro cluster a test cluster so you get",
    "start": "1769600",
    "end": "1776640"
  },
  {
    "text": "two different namespaces with those different tags u because we have applications bound within that namespace",
    "start": "1776640",
    "end": "1782240"
  },
  {
    "text": "the arbbacks kind of work out for us so that's not been a problem uh what we do though is we don't necessarily treat",
    "start": "1782240",
    "end": "1787840"
  },
  {
    "text": "that as source of truth what we do is we have that regional service so we just sync all the arbback rules on a periodic",
    "start": "1787840",
    "end": "1793360"
  },
  {
    "text": "basis and that's that's how we apply permissions but if you need to go beyond that then yes webbook is the way to go",
    "start": "1793360",
    "end": "1798799"
  },
  {
    "text": "uh we have several other webbooks for different validation checks but not for arbback thank you",
    "start": "1798799",
    "end": "1805360"
  },
  {
    "text": "uh thanks for the talk uh I wonder how you solve the ingress and uh use",
    "start": "1805360",
    "end": "1812240"
  },
  {
    "text": "interactive user access and machine to machine access between services so u in",
    "start": "1812240",
    "end": "1818240"
  },
  {
    "text": "our production fleet we have flat network meaning every machine can talk to each other every application can talk",
    "start": "1818240",
    "end": "1823279"
  },
  {
    "text": "to each other our service discovery system relies on that as a prerequisite so today every application can talk to",
    "start": "1823279",
    "end": "1830240"
  },
  {
    "text": "each other and there's MTLS and authorization in the middle meaning you need to use your certificates or better",
    "start": "1830240",
    "end": "1835360"
  },
  {
    "text": "tokens and then you can only talk to a service if you have the permissions to talk to that API so in today's case it",
    "start": "1835360",
    "end": "1841600"
  },
  {
    "text": "doesn't matter whether your application is running on cluster A or cluster B the network is flat and they can talk to each other the way it works is our data",
    "start": "1841600",
    "end": "1848720"
  },
  {
    "text": "center network is controlled by us and because we have control from all the way through which rack we run the machine on",
    "start": "1848720",
    "end": "1854480"
  },
  {
    "text": "and how we deploy applications we we can manage some of that what about the",
    "start": "1854480",
    "end": "1860200"
  },
  {
    "text": "interactive user access like through the browser to these apps yeah so interactive user apps in terms of",
    "start": "1860200",
    "end": "1866880"
  },
  {
    "text": "production if you're talking to an application in production you can't connect to it directly uh we use jumphost for example but typically users",
    "start": "1866880",
    "end": "1873840"
  },
  {
    "text": "are not talking to applications in production through a web UI all of these applications we deploy they're serving",
    "start": "1873840",
    "end": "1879679"
  },
  {
    "text": "link.com we have several internal applications to expose those we have internal proxies that we run but they",
    "start": "1879679",
    "end": "1885840"
  },
  {
    "text": "are not cumulative controllers uh so we have a traffic team that manages regional proxies any application that",
    "start": "1885840",
    "end": "1891520"
  },
  {
    "text": "needs to be exposed to the browser becomes a backend on this traffic proxy layer and that's how we interact with it",
    "start": "1891520",
    "end": "1898320"
  },
  {
    "text": "thanks thanks hi thanks for the great talk so my",
    "start": "1898320",
    "end": "1905360"
  },
  {
    "text": "question is related to uh one of the initiatives which we are trying to do in",
    "start": "1905360",
    "end": "1910480"
  },
  {
    "text": "our uh company so have you ever tried upgrading the Kubernetes uh components",
    "start": "1910480",
    "end": "1916799"
  },
  {
    "text": "along with the applications which is running on top of it like control plane",
    "start": "1916799",
    "end": "1921919"
  },
  {
    "text": "as well as data plane so all the time you want to take that i don't fully get the question actually you want to take",
    "start": "1921919",
    "end": "1928080"
  },
  {
    "text": "that uh so I just to make sure we get the question right uh you're asking whether we do a Kubernetes control plane",
    "start": "1928080",
    "end": "1934880"
  },
  {
    "text": "and data plane upgrades while the applications are still running yeah and and uh some of the application you can",
    "start": "1934880",
    "end": "1940720"
  },
  {
    "text": "assume that although it's not advisable but application is running on control planes as well uh so none of our",
    "start": "1940720",
    "end": "1947919"
  },
  {
    "text": "applications are on control planes uh but we def we always do in place updates",
    "start": "1947919",
    "end": "1953039"
  },
  {
    "text": "for our control plane and data planes uh so there are let's just say two parts to it for data plane it goes back to what",
    "start": "1953039",
    "end": "1958880"
  },
  {
    "text": "Ameth mentioned earlier the maintenance train would go and deploy new cubit versions across the board while the",
    "start": "1958880",
    "end": "1964399"
  },
  {
    "text": "applications are there so we drain the machine upgrade the machine bring it back into rotation for control plane we",
    "start": "1964399",
    "end": "1970159"
  },
  {
    "text": "have our custom orchestrator right now that'll go and update API server for example in a rolling phase in a rolling",
    "start": "1970159",
    "end": "1975600"
  },
  {
    "text": "fashion so once we check 10% of it looks good it goes forward and the applications continue running it's like",
    "start": "1975600",
    "end": "1982320"
  },
  {
    "text": "inertia objects in motion stay in motion objects at stay stay at rest so control plane or data plane upgrades don't",
    "start": "1982320",
    "end": "1987919"
  },
  {
    "text": "affect the applications at all does it require downtime not at all",
    "start": "1987919",
    "end": "1993279"
  },
  {
    "text": "okay uh it will be very good if we can share some links or documents the way we",
    "start": "1993279",
    "end": "1999039"
  },
  {
    "text": "you're doing it that will be very helpful for us thank you happy to chat thank you",
    "start": "1999039",
    "end": "2004559"
  },
  {
    "text": "there might be a talk for North America maybe oh yeah",
    "start": "2004559",
    "end": "2009760"
  },
  {
    "text": "hi thanks for the talk um I'm curious about the freedom of your teams about",
    "start": "2009760",
    "end": "2015480"
  },
  {
    "text": "scalability so you define uh the name spaces for the teams and how what is the",
    "start": "2015480",
    "end": "2021440"
  },
  {
    "text": "limits for the teams uh limits in terms of autoscalability in resources CPU",
    "start": "2021440",
    "end": "2027120"
  },
  {
    "text": "memory uh so there's no specific limit in terms Okay the limits are defined based on two factors one is how many",
    "start": "2027120",
    "end": "2034320"
  },
  {
    "text": "replicas you need to serve the site and then the app owners choose how much CPU and memory they need u typically what",
    "start": "2034320",
    "end": "2040559"
  },
  {
    "text": "happens is we have another system which is called autoite sizing so it's not reactive autoscaling it's proactive",
    "start": "2040559",
    "end": "2046640"
  },
  {
    "text": "autoscaling so majority of our application owners don't have to worry about how many replicas to use what we",
    "start": "2046640",
    "end": "2052079"
  },
  {
    "text": "typically do is that we observe site traffic over time we develop a model to see how many how much QPS does this",
    "start": "2052079",
    "end": "2059280"
  },
  {
    "text": "specific application need to handle what its CPU memory usage looks like and we have a decent idea of how much QPS we",
    "start": "2059280",
    "end": "2065040"
  },
  {
    "text": "expect to see next week or two weeks from now that system will actually go talk to the LI deployment API here and",
    "start": "2065040",
    "end": "2071440"
  },
  {
    "text": "update the replica count dynamically and the application owners don't need to worry about that okay do you react if",
    "start": "2071440",
    "end": "2077280"
  },
  {
    "text": "there's like a overuse of the resources we have quota systems in place so if let's say for example an application",
    "start": "2077280",
    "end": "2082960"
  },
  {
    "text": "starts using beyond we don't necessarily enforce it in the sense that the application is killed but the",
    "start": "2082960",
    "end": "2088000"
  },
  {
    "text": "application owner receives warnings that you're going above Kota thank you",
    "start": "2088000",
    "end": "2094760"
  },
  {
    "text": "hello thanks for the presentation my question more about bare metal server uh",
    "start": "2094960",
    "end": "2100400"
  },
  {
    "text": "you know the management it was always it is always challenging you know to the the bare metal server you need to get in",
    "start": "2100400",
    "end": "2107440"
  },
  {
    "text": "the right nickart you know you need to know the this the nickname and Linux has",
    "start": "2107440",
    "end": "2113520"
  },
  {
    "text": "a always recommendation persistent nicknaming you know this and also for",
    "start": "2113520",
    "end": "2118960"
  },
  {
    "text": "the disk so I think you didn't use the SDA or SDAB naming instead of maybe WWN",
    "start": "2118960",
    "end": "2125920"
  },
  {
    "text": "I mean it's all you need to also automate right so how do you manage uh that kind of thing yeah I I mean we we",
    "start": "2125920",
    "end": "2133440"
  },
  {
    "text": "do manage them basically like we have to configure the hosts exactly the right way but like stuff like storage for",
    "start": "2133440",
    "end": "2139200"
  },
  {
    "text": "example we develop our own CSI drivers to be able to you know allocate um space for containers to use on the host so",
    "start": "2139200",
    "end": "2146000"
  },
  {
    "text": "some of that is a little bit you know tamed with the help of Kubernetes but yeah we do have a host configuration",
    "start": "2146000",
    "end": "2151119"
  },
  {
    "text": "layer we use um puppet internally and we use puppet extensively to um configure a",
    "start": "2151119",
    "end": "2157040"
  },
  {
    "text": "lot of stuff like you know several kernel settings um I mean yeah are like packages running on the dro so on and so",
    "start": "2157040",
    "end": "2164400"
  },
  {
    "text": "forth but my question uh regarding this nick how do you detect you know the",
    "start": "2164400",
    "end": "2169520"
  },
  {
    "text": "which nick card is actually online you know for example bare metal server has a multiple nick cards and you need to the",
    "start": "2169520",
    "end": "2176640"
  },
  {
    "text": "the pick the right one to you know to add it to the cluster So in this case",
    "start": "2176640",
    "end": "2181920"
  },
  {
    "text": "when a machine is dragged in our data centers for example it has one neck associated with it uh not multiple neck",
    "start": "2181920",
    "end": "2187680"
  },
  {
    "text": "cards and I I would say we have a separate team who takes care of taking care of how machines are not just",
    "start": "2187680",
    "end": "2194400"
  },
  {
    "text": "configured in a data centers but managed over time u they would be a better experts to tell you in detail on how to",
    "start": "2194400",
    "end": "2200400"
  },
  {
    "text": "do it uh but typically when a machine shows up there's one interface attached to one neck and we know exactly what IP",
    "start": "2200400",
    "end": "2206160"
  },
  {
    "text": "address it has okay thanks",
    "start": "2206160",
    "end": "2210920"
  },
  {
    "text": "hello hi um thanks for the great talk and uh especially the scale you",
    "start": "2215359",
    "end": "2220480"
  },
  {
    "text": "mentioned in terms of servers like currently LinkedIn has more than 500,000 servers so is LinkedIn uh managing its",
    "start": "2220480",
    "end": "2228960"
  },
  {
    "text": "own data centers and if yes then so like LinkedIn is used globally like all",
    "start": "2228960",
    "end": "2234320"
  },
  {
    "text": "around the world so how you guys are dealing with latency like you're using some CDN kind of services or is it like",
    "start": "2234320",
    "end": "2241040"
  },
  {
    "text": "you're have data centers like at different locations so this is something I'm curious to know uh so there are",
    "start": "2241040",
    "end": "2246880"
  },
  {
    "text": "multiple data centers hosting all of our applications and then there are many number of pops or edge networks uh that",
    "start": "2246880",
    "end": "2252560"
  },
  {
    "text": "we rely on to reduce that latency to you so if you're let's say talking to linkedin.com it'll go to the nearest",
    "start": "2252560",
    "end": "2258000"
  },
  {
    "text": "edge site and then it gets routed to our data centers cool thanks thanks for your",
    "start": "2258000",
    "end": "2263520"
  },
  {
    "text": "time awesome i guess we'll take the last question and yeah put it up there hi",
    "start": "2263520",
    "end": "2268960"
  },
  {
    "text": "thanks for the talk i do have a question at some point of time you chose to have",
    "start": "2268960",
    "end": "2274400"
  },
  {
    "text": "inhouse solution instead of choosing like vunder solution or also what is yours in",
    "start": "2274400",
    "end": "2279880"
  },
  {
    "text": "kubernetes at what moment do you know if it was a success and how do you collect",
    "start": "2279880",
    "end": "2285359"
  },
  {
    "text": "feedback batch on your own work because you don't have anyone doing uh similar",
    "start": "2285359",
    "end": "2290640"
  },
  {
    "text": "stuff so how do you know it has been the good choice and is there occurs already",
    "start": "2290640",
    "end": "2296079"
  },
  {
    "text": "that you need to change or you come back again on your decision and choose to use",
    "start": "2296079",
    "end": "2301160"
  },
  {
    "text": "Kubernetes stuff yeah I would say like we try to not you know lock ourselves",
    "start": "2301160",
    "end": "2306240"
  },
  {
    "text": "into a corner and we don't we try not to make decisions that we can't back out of so that that's pretty critical to us uh",
    "start": "2306240",
    "end": "2311920"
  },
  {
    "text": "so we try to experiment with other things um for for most intents and purposes anything that comes from the",
    "start": "2311920",
    "end": "2317839"
  },
  {
    "text": "open source into our ecosystem we have to be experts on that like we got bit by a lot of open source components we had",
    "start": "2317839",
    "end": "2324320"
  },
  {
    "text": "incidents because of them and so as a result we started to become really selective and really choose picky and we",
    "start": "2324320",
    "end": "2330480"
  },
  {
    "text": "started to read the source code and we started to understand okay well here's an open source component what are its failure modes what is its scaling story",
    "start": "2330480",
    "end": "2337520"
  },
  {
    "text": "like how um can it accommodate this size of cluster and so on so forth so basically Um I would say there are key",
    "start": "2337520",
    "end": "2345359"
  },
  {
    "text": "areas where we are able to successfully bring in something from the open source it does exactly what we want and we",
    "start": "2345359",
    "end": "2351040"
  },
  {
    "text": "contribute back to it uh clone set operator for example is a good example of that like it works really well for us",
    "start": "2351040",
    "end": "2356400"
  },
  {
    "text": "but there there has been several other components that you know we still kind of hang on to but we're aware of their limitations and we kind of work around",
    "start": "2356400",
    "end": "2362480"
  },
  {
    "text": "them uh so developing in-house has also hasn't been the worst thing i think a company of our size and our you know",
    "start": "2362480",
    "end": "2369440"
  },
  {
    "text": "engineering culture it's kind of reasonable for us to kind of own some of our fate especially in key areas where",
    "start": "2369440",
    "end": "2375680"
  },
  {
    "text": "financially it also makes sense yeah okay thanks all right thank you so much",
    "start": "2375680",
    "end": "2383040"
  },
  {
    "text": "thank you and",
    "start": "2383040",
    "end": "2386680"
  }
]