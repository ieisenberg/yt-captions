[
  {
    "text": "welcome everyone and thank you for",
    "start": "80",
    "end": "1839"
  },
  {
    "text": "coming to our talk this talk is for you",
    "start": "1839",
    "end": "4480"
  },
  {
    "text": "if you're interested in in running",
    "start": "4480",
    "end": "7120"
  },
  {
    "text": "really uh generative AI models on your",
    "start": "7120",
    "end": "9440"
  },
  {
    "text": "own infrastructure in a secure way uh",
    "start": "9440",
    "end": "12000"
  },
  {
    "text": "across the number of companies we're",
    "start": "12000",
    "end": "13759"
  },
  {
    "text": "talking to we're seeing that lots of",
    "start": "13759",
    "end": "15759"
  },
  {
    "text": "businesses are becoming more wary of",
    "start": "15759",
    "end": "18080"
  },
  {
    "text": "putting their data on uh proprietary uh",
    "start": "18080",
    "end": "21520"
  },
  {
    "text": "hosted AI platforms and what we're going",
    "start": "21520",
    "end": "24240"
  },
  {
    "text": "to do today is show you an alternative",
    "start": "24240",
    "end": "27119"
  },
  {
    "text": "solution where you can host your own",
    "start": "27119",
    "end": "29439"
  },
  {
    "text": "LLMs on your own infrastructure and um",
    "start": "29439",
    "end": "33360"
  },
  {
    "text": "every time you you blink and you realize",
    "start": "33360",
    "end": "35920"
  },
  {
    "text": "the AI landscape has changed quite a bit",
    "start": "35920",
    "end": "39040"
  },
  {
    "text": "and uh we're seeing also that lots of",
    "start": "39040",
    "end": "41440"
  },
  {
    "text": "people look at this as this is quite",
    "start": "41440",
    "end": "43960"
  },
  {
    "text": "complex and what we're trying to do",
    "start": "43960",
    "end": "46239"
  },
  {
    "text": "today is peel back that complexity and",
    "start": "46239",
    "end": "49440"
  },
  {
    "text": "start this um this journey from very",
    "start": "49440",
    "end": "53039"
  },
  {
    "text": "very basic building blocks and take you",
    "start": "53039",
    "end": "56399"
  },
  {
    "text": "through how uh sort of our experiences",
    "start": "56399",
    "end": "59199"
  },
  {
    "text": "building AI platforms out of um",
    "start": "59199",
    "end": "62480"
  },
  {
    "text": "Kubernetes and uh open source LLMs we",
    "start": "62480",
    "end": "66000"
  },
  {
    "text": "want to talk about um patterns that",
    "start": "66000",
    "end": "68000"
  },
  {
    "text": "we've used pitfalls that we stumbled",
    "start": "68000",
    "end": "71200"
  },
  {
    "text": "into and really talk about our mistakes",
    "start": "71200",
    "end": "73680"
  },
  {
    "text": "as well so you don't have to do them",
    "start": "73680",
    "end": "75360"
  },
  {
    "text": "yourselves and uh we'll also talk about",
    "start": "75360",
    "end": "78080"
  },
  {
    "text": "the layers that we built on top of um uh",
    "start": "78080",
    "end": "80799"
  },
  {
    "text": "a basic open LLM running on Kubernetes",
    "start": "80799",
    "end": "84560"
  },
  {
    "text": "uh before we move on I'm going to take a",
    "start": "84560",
    "end": "86880"
  },
  {
    "text": "moment to introduce ourselves so my name",
    "start": "86880",
    "end": "90720"
  },
  {
    "text": "is Priya Samuel i'm a technology",
    "start": "90720",
    "end": "92799"
  },
  {
    "text": "architect i've worked in many small and",
    "start": "92799",
    "end": "95600"
  },
  {
    "text": "large businesses and uh most of my",
    "start": "95600",
    "end": "98799"
  },
  {
    "text": "background is in DevOps as well as",
    "start": "98799",
    "end": "100880"
  },
  {
    "text": "consulting and um my current um uh",
    "start": "100880",
    "end": "104400"
  },
  {
    "text": "environment I work in is around building",
    "start": "104400",
    "end": "106560"
  },
  {
    "text": "identity and access layer on top of uh",
    "start": "106560",
    "end": "109600"
  },
  {
    "text": "genai applications and um over the",
    "start": "109600",
    "end": "113119"
  },
  {
    "text": "number of years I've worked um on",
    "start": "113119",
    "end": "114960"
  },
  {
    "text": "building u machine learning pipelines as",
    "start": "114960",
    "end": "117520"
  },
  {
    "text": "well as automating these pipelines and",
    "start": "117520",
    "end": "120079"
  },
  {
    "text": "um so this is quite an exciting area to",
    "start": "120079",
    "end": "122479"
  },
  {
    "text": "be uh working and over to Luke for an",
    "start": "122479",
    "end": "124719"
  },
  {
    "text": "introduction cool hi everyone um yeah",
    "start": "124719",
    "end": "127920"
  },
  {
    "text": "I'm I'm Luke Marsden um I've worked in",
    "start": "127920",
    "end": "130800"
  },
  {
    "text": "DevOps pretty much my whole career uh I",
    "start": "130800",
    "end": "133599"
  },
  {
    "text": "started out um doing a startup that did",
    "start": "133599",
    "end": "136640"
  },
  {
    "text": "storage for Docker and Kubernetes back",
    "start": "136640",
    "end": "138560"
  },
  {
    "text": "in the early Docker and Kubernetes days",
    "start": "138560",
    "end": "140959"
  },
  {
    "text": "um I was involved in SIG cluster life",
    "start": "140959",
    "end": "143680"
  },
  {
    "text": "cycle at the time that we created Kubadm",
    "start": "143680",
    "end": "146560"
  },
  {
    "text": "so if anyone's used Cub Cubeadm here you",
    "start": "146560",
    "end": "148959"
  },
  {
    "text": "can blame me um and then um I've done",
    "start": "148959",
    "end": "152640"
  },
  {
    "text": "yeah like a string of startups we we",
    "start": "152640",
    "end": "154800"
  },
  {
    "text": "then did a an endto-end MLOps platform",
    "start": "154800",
    "end": "157519"
  },
  {
    "text": "uh before uh AI was cool and then I did",
    "start": "157519",
    "end": "160239"
  },
  {
    "text": "consulting for a few years while um",
    "start": "160239",
    "end": "163920"
  },
  {
    "text": "during the time that the chat GPT moment",
    "start": "163920",
    "end": "166239"
  },
  {
    "text": "happened and so I f I saw firsthand",
    "start": "166239",
    "end": "168959"
  },
  {
    "text": "working with those clients um the",
    "start": "168959",
    "end": "171519"
  },
  {
    "text": "opportunity to take open source models",
    "start": "171519",
    "end": "174160"
  },
  {
    "text": "that were getting better really quickly",
    "start": "174160",
    "end": "176480"
  },
  {
    "text": "um and run them locally in Kubernetes",
    "start": "176480",
    "end": "178640"
  },
  {
    "text": "clusters for improved kind of security",
    "start": "178640",
    "end": "180640"
  },
  {
    "text": "and reliability uh so yeah it's a it's",
    "start": "180640",
    "end": "183360"
  },
  {
    "text": "an interesting topic and happy to share",
    "start": "183360",
    "end": "185040"
  },
  {
    "text": "some of our learnings today",
    "start": "185040",
    "end": "188840"
  },
  {
    "text": "cool we're going to start really really",
    "start": "191360",
    "end": "193360"
  },
  {
    "text": "basic so what is an LLM so um if you're",
    "start": "193360",
    "end": "196959"
  },
  {
    "text": "sitting in this room and this is new",
    "start": "196959",
    "end": "198959"
  },
  {
    "text": "language for you probably isn't but then",
    "start": "198959",
    "end": "201840"
  },
  {
    "text": "I'm going to introduce it really quickly",
    "start": "201840",
    "end": "204239"
  },
  {
    "text": "just to lay enough context um to go on",
    "start": "204239",
    "end": "207120"
  },
  {
    "text": "to what the nuances of running these on",
    "start": "207120",
    "end": "209319"
  },
  {
    "text": "Kubernetes so the mathematical",
    "start": "209319",
    "end": "212560"
  },
  {
    "text": "explanation a very simplified of an LLM",
    "start": "212560",
    "end": "215360"
  },
  {
    "text": "is that it's a multi-dimensional",
    "start": "215360",
    "end": "217120"
  },
  {
    "text": "function that maps sentences onto other",
    "start": "217120",
    "end": "220519"
  },
  {
    "text": "sentences and um the input is a phrase",
    "start": "220519",
    "end": "224000"
  },
  {
    "text": "for example what is the capital of",
    "start": "224000",
    "end": "225599"
  },
  {
    "text": "France so that gets turned into a vector",
    "start": "225599",
    "end": "228400"
  },
  {
    "text": "that's basically a string of numbers and",
    "start": "228400",
    "end": "230640"
  },
  {
    "text": "then you're reading the values of this",
    "start": "230640",
    "end": "233440"
  },
  {
    "text": "highdimensional uh coordinates getting",
    "start": "233440",
    "end": "236159"
  },
  {
    "text": "the answer turning that back into a",
    "start": "236159",
    "end": "238159"
  },
  {
    "text": "sentence and returning it back to the",
    "start": "238159",
    "end": "239680"
  },
  {
    "text": "user",
    "start": "239680",
    "end": "240560"
  },
  {
    "text": "this is much more easier when you've got",
    "start": "240560",
    "end": "244319"
  },
  {
    "text": "a GPU and",
    "start": "244319",
    "end": "246519"
  },
  {
    "text": "um I guess um why run LLMs on uh",
    "start": "246519",
    "end": "250560"
  },
  {
    "text": "Kubernetes and what you need the very",
    "start": "250560",
    "end": "252480"
  },
  {
    "text": "basic thing you'd need is access to um",
    "start": "252480",
    "end": "255280"
  },
  {
    "text": "uh is a GPU processor a TPU or GPU and",
    "start": "255280",
    "end": "258720"
  },
  {
    "text": "um you could um get that as a hardware",
    "start": "258720",
    "end": "262320"
  },
  {
    "text": "basically go and buy off a GPU run it in",
    "start": "262320",
    "end": "264960"
  },
  {
    "text": "your office like Luke does has the added",
    "start": "264960",
    "end": "267919"
  },
  {
    "text": "advantage of warming up your uh office",
    "start": "267919",
    "end": "270400"
  },
  {
    "text": "in winter but then you could also might",
    "start": "270400",
    "end": "272479"
  },
  {
    "text": "as well uh get it off um any one of",
    "start": "272479",
    "end": "274960"
  },
  {
    "text": "these um cloud providers so across um",
    "start": "274960",
    "end": "278560"
  },
  {
    "text": "Google or",
    "start": "278560",
    "end": "279960"
  },
  {
    "text": "AWS and or selium the there's that's",
    "start": "279960",
    "end": "285120"
  },
  {
    "text": "just not enough so let's say you want to",
    "start": "285120",
    "end": "287120"
  },
  {
    "text": "do something with that you would need to",
    "start": "287120",
    "end": "288960"
  },
  {
    "text": "connect your GPU cluster to your",
    "start": "288960",
    "end": "291360"
  },
  {
    "text": "Kubernetes cluster and that's done with",
    "start": "291360",
    "end": "293520"
  },
  {
    "text": "a device plug-in that makes your GPU",
    "start": "293520",
    "end": "296240"
  },
  {
    "text": "accessible uh into your into your",
    "start": "296240",
    "end": "298639"
  },
  {
    "text": "Kubernetes",
    "start": "298639",
    "end": "299720"
  },
  {
    "text": "cluster but sort of like let's step back",
    "start": "299720",
    "end": "302560"
  },
  {
    "text": "why bother why go through all this",
    "start": "302560",
    "end": "304960"
  },
  {
    "text": "trouble so you can run these LLMs",
    "start": "304960",
    "end": "307440"
  },
  {
    "text": "locally on your cluster so the main",
    "start": "307440",
    "end": "310080"
  },
  {
    "text": "driver we're seeing is security and",
    "start": "310080",
    "end": "312160"
  },
  {
    "text": "compliance so lots of customers we speak",
    "start": "312160",
    "end": "314720"
  },
  {
    "text": "to who are either in a health care",
    "start": "314720",
    "end": "317600"
  },
  {
    "text": "business or if you're a European telco",
    "start": "317600",
    "end": "321120"
  },
  {
    "text": "where you store your data is is quite",
    "start": "321120",
    "end": "324080"
  },
  {
    "text": "critical and this is not just data",
    "start": "324080",
    "end": "326320"
  },
  {
    "text": "residency but also where your data gets",
    "start": "326320",
    "end": "328880"
  },
  {
    "text": "processed so which means a lot of the",
    "start": "328880",
    "end": "331280"
  },
  {
    "text": "hosted LLM platforms are completely out",
    "start": "331280",
    "end": "334080"
  },
  {
    "text": "of scope and then going on availability",
    "start": "334080",
    "end": "337440"
  },
  {
    "text": "and latency is is a big deal as well the",
    "start": "337440",
    "end": "340639"
  },
  {
    "text": "closer your LLM is to you more likely",
    "start": "340639",
    "end": "343759"
  },
  {
    "text": "that you'll get your answers back",
    "start": "343759",
    "end": "345720"
  },
  {
    "text": "quicker so that's uh we're also seeing",
    "start": "345720",
    "end": "348560"
  },
  {
    "text": "that um hosted AI platforms like um uh",
    "start": "348560",
    "end": "352960"
  },
  {
    "text": "the ones around today uptime and um API",
    "start": "352960",
    "end": "357520"
  },
  {
    "text": "uh response is still quite shaky at the",
    "start": "357520",
    "end": "360240"
  },
  {
    "text": "moment so you're in control of your",
    "start": "360240",
    "end": "361759"
  },
  {
    "text": "infrastructure",
    "start": "361759",
    "end": "363360"
  },
  {
    "text": "we're also we we live in an interesting",
    "start": "363360",
    "end": "365919"
  },
  {
    "text": "era where it looks like open-source LLMs",
    "start": "365919",
    "end": "370240"
  },
  {
    "text": "it looks like they may be overtaking a",
    "start": "370240",
    "end": "372720"
  },
  {
    "text": "lot of the proprietary stuff and uh I'm",
    "start": "372720",
    "end": "375520"
  },
  {
    "text": "I'm sure most of you have heard of",
    "start": "375520",
    "end": "377280"
  },
  {
    "text": "DeepSeek at this time and the last",
    "start": "377280",
    "end": "380479"
  },
  {
    "text": "driver that I'm going to talk about is",
    "start": "380479",
    "end": "382240"
  },
  {
    "text": "the cost of ownership the longer you run",
    "start": "382240",
    "end": "384960"
  },
  {
    "text": "these and the more at scale you're",
    "start": "384960",
    "end": "387440"
  },
  {
    "text": "running these um LLMs the costs start",
    "start": "387440",
    "end": "390560"
  },
  {
    "text": "shifting so if it's a quick P then great",
    "start": "390560",
    "end": "393919"
  },
  {
    "text": "you get an open AI um token and you're",
    "start": "393919",
    "end": "396960"
  },
  {
    "text": "running but then if it's something where",
    "start": "396960",
    "end": "398800"
  },
  {
    "text": "you want your model to think longer so a",
    "start": "398800",
    "end": "401680"
  },
  {
    "text": "lot of these LLMs that pretty much like",
    "start": "401680",
    "end": "404440"
  },
  {
    "text": "deepse are um LLM that are sort of like",
    "start": "404440",
    "end": "408479"
  },
  {
    "text": "a chain of thought model so the longer",
    "start": "408479",
    "end": "410800"
  },
  {
    "text": "they think the better answers you get",
    "start": "410800",
    "end": "413120"
  },
  {
    "text": "and this is where the cost starts",
    "start": "413120",
    "end": "415680"
  },
  {
    "text": "shifting quite a big deal into um your",
    "start": "415680",
    "end": "418880"
  },
  {
    "text": "locally hosted LMS by local I don't",
    "start": "418880",
    "end": "421360"
  },
  {
    "text": "really mean on your laptop but I mean",
    "start": "421360",
    "end": "423599"
  },
  {
    "text": "any kind of environment that is",
    "start": "423599",
    "end": "426319"
  },
  {
    "text": "accessible to you on the cloud or on a",
    "start": "426319",
    "end": "429280"
  },
  {
    "text": "server farm it doesn't matter it's still",
    "start": "429280",
    "end": "432280"
  },
  {
    "text": "local cool and uh lastly I'm just going",
    "start": "432280",
    "end": "436240"
  },
  {
    "text": "to quickly say why Kubernetes it's",
    "start": "436240",
    "end": "438800"
  },
  {
    "text": "really just because Kubernetes is",
    "start": "438800",
    "end": "440479"
  },
  {
    "text": "Kubernetes because of portability",
    "start": "440479",
    "end": "442800"
  },
  {
    "text": "because of consistency and uh",
    "start": "442800",
    "end": "445440"
  },
  {
    "text": "scalability and it's just that you can",
    "start": "445440",
    "end": "447680"
  },
  {
    "text": "apply the exact same podspec on um your",
    "start": "447680",
    "end": "451440"
  },
  {
    "text": "u server farm or you can apply it",
    "start": "451440",
    "end": "453199"
  },
  {
    "text": "locally and you've got that consistent",
    "start": "453199",
    "end": "455599"
  },
  {
    "text": "environment",
    "start": "455599",
    "end": "458160"
  },
  {
    "text": "accessible cool this talk as I said is a",
    "start": "458360",
    "end": "461759"
  },
  {
    "text": "series of demos and uh on every slide",
    "start": "461759",
    "end": "464720"
  },
  {
    "text": "you can see a QR code to a GitHub",
    "start": "464720",
    "end": "467360"
  },
  {
    "text": "repository so this repository has got a",
    "start": "467360",
    "end": "470240"
  },
  {
    "text": "readme file with all of the in",
    "start": "470240",
    "end": "472160"
  },
  {
    "text": "instructions in terms of commands we're",
    "start": "472160",
    "end": "474400"
  },
  {
    "text": "going to show you today as well as um a",
    "start": "474400",
    "end": "477840"
  },
  {
    "text": "walkthrough of what else you could do on",
    "start": "477840",
    "end": "480000"
  },
  {
    "text": "your Kubernetes cluster our first demo",
    "start": "480000",
    "end": "484080"
  },
  {
    "text": "is going to uh pull in a very uh simple",
    "start": "484080",
    "end": "488440"
  },
  {
    "text": "open-source product called Olama and",
    "start": "488440",
    "end": "491199"
  },
  {
    "text": "this is predominantly used to test out",
    "start": "491199",
    "end": "494080"
  },
  {
    "text": "uh LLMs and we're going to put use a",
    "start": "494080",
    "end": "496400"
  },
  {
    "text": "Helm chart to pull in uh Olama along",
    "start": "496400",
    "end": "499280"
  },
  {
    "text": "with Open Web UI over to Luke yeah cool",
    "start": "499280",
    "end": "502160"
  },
  {
    "text": "so I'm just checking these mics work",
    "start": "502160",
    "end": "503840"
  },
  {
    "text": "okay great um so uh yeah so here we've",
    "start": "503840",
    "end": "507360"
  },
  {
    "text": "got um a Kubernetes cluster uh with a",
    "start": "507360",
    "end": "510160"
  },
  {
    "text": "single A100 in it um and what we can see",
    "start": "510160",
    "end": "513440"
  },
  {
    "text": "is uh that we've got um open web UI and",
    "start": "513440",
    "end": "517839"
  },
  {
    "text": "OAMA running in that cluster um and then",
    "start": "517839",
    "end": "520880"
  },
  {
    "text": "if we go into uh open web UI I'm I'm",
    "start": "520880",
    "end": "524320"
  },
  {
    "text": "going to start by showing you kind of",
    "start": "524320",
    "end": "525600"
  },
  {
    "text": "one of the challenges that you get uh",
    "start": "525600",
    "end": "527600"
  },
  {
    "text": "with this setup when you get it kind of",
    "start": "527600",
    "end": "529360"
  },
  {
    "text": "out of the box and so what I'm going to",
    "start": "529360",
    "end": "531200"
  },
  {
    "text": "show here is um a a prompt that we",
    "start": "531200",
    "end": "534800"
  },
  {
    "text": "wanted to have work um when doing an API",
    "start": "534800",
    "end": "539680"
  },
  {
    "text": "integration between um an LLM and an",
    "start": "539680",
    "end": "543279"
  },
  {
    "text": "external system like in this case Jira",
    "start": "543279",
    "end": "545920"
  },
  {
    "text": "um so the prompt is to try and present",
    "start": "545920",
    "end": "548080"
  },
  {
    "text": "the key information uh from a from an",
    "start": "548080",
    "end": "550480"
  },
  {
    "text": "API response body um including the",
    "start": "550480",
    "end": "553920"
  },
  {
    "text": "user's question um so we get the user's",
    "start": "553920",
    "end": "557279"
  },
  {
    "text": "question um uh which is what issues are",
    "start": "557279",
    "end": "560640"
  },
  {
    "text": "there and then we get the API response",
    "start": "560640",
    "end": "562959"
  },
  {
    "text": "which comes back from Jira um and as you",
    "start": "562959",
    "end": "565600"
  },
  {
    "text": "can see uh even though this is just a a",
    "start": "565600",
    "end": "568080"
  },
  {
    "text": "sprint with 10 uh 10 issues in it the",
    "start": "568080",
    "end": "571120"
  },
  {
    "text": "API response body is quite big um and so",
    "start": "571120",
    "end": "574959"
  },
  {
    "text": "this highlights one of the issues that",
    "start": "574959",
    "end": "576800"
  },
  {
    "text": "you get with Olama out of the box um",
    "start": "576800",
    "end": "579839"
  },
  {
    "text": "which is that um it uh it has a",
    "start": "579839",
    "end": "584080"
  },
  {
    "text": "truncated context length window and so",
    "start": "584080",
    "end": "587519"
  },
  {
    "text": "what you can see specifically here is",
    "start": "587519",
    "end": "589760"
  },
  {
    "text": "that even though we gave the LLM uh",
    "start": "589760",
    "end": "592160"
  },
  {
    "text": "which should be capable of doing a good",
    "start": "592160",
    "end": "593760"
  },
  {
    "text": "job on this task where the task is",
    "start": "593760",
    "end": "596399"
  },
  {
    "text": "summarize the issues in the sprint what",
    "start": "596399",
    "end": "598800"
  },
  {
    "text": "it actually came back with uh was a bad",
    "start": "598800",
    "end": "602080"
  },
  {
    "text": "answer um and in particular it started",
    "start": "602080",
    "end": "604480"
  },
  {
    "text": "talking about the JSON response body um",
    "start": "604480",
    "end": "607120"
  },
  {
    "text": "it talked about the root element and in",
    "start": "607120",
    "end": "609279"
  },
  {
    "text": "particular it said the root element is",
    "start": "609279",
    "end": "610720"
  },
  {
    "text": "an array containing one object but it's",
    "start": "610720",
    "end": "613680"
  },
  {
    "text": "not the response body uh had 10 objects",
    "start": "613680",
    "end": "616399"
  },
  {
    "text": "in it it had 10 issues in the sprint um",
    "start": "616399",
    "end": "619040"
  },
  {
    "text": "and so uh I'm going to pass back to",
    "start": "619040",
    "end": "620880"
  },
  {
    "text": "Priya uh to explain why that happened um",
    "start": "620880",
    "end": "624240"
  },
  {
    "text": "and and then we're going to talk about",
    "start": "624240",
    "end": "625680"
  },
  {
    "text": "how we can solve it cool thank you um so",
    "start": "625680",
    "end": "629519"
  },
  {
    "text": "the model that you're running by default",
    "start": "629519",
    "end": "631680"
  },
  {
    "text": "when you're using Olama is uh a highly",
    "start": "631680",
    "end": "634640"
  },
  {
    "text": "compressed model so what it means is",
    "start": "634640",
    "end": "636880"
  },
  {
    "text": "that it's a quantized model and um these",
    "start": "636880",
    "end": "640720"
  },
  {
    "text": "um really meant to run on test",
    "start": "640720",
    "end": "642880"
  },
  {
    "text": "environments like your laptop and uh",
    "start": "642880",
    "end": "645200"
  },
  {
    "text": "it's not it's not really tuned for a",
    "start": "645200",
    "end": "647279"
  },
  {
    "text": "production environment and um especially",
    "start": "647279",
    "end": "649760"
  },
  {
    "text": "the one that Luke just demoed that's um",
    "start": "649760",
    "end": "652560"
  },
  {
    "text": "a a model that's compressed to store",
    "start": "652560",
    "end": "655440"
  },
  {
    "text": "four bits of information per weight in",
    "start": "655440",
    "end": "657920"
  },
  {
    "text": "the neural network and this is pretty uh",
    "start": "657920",
    "end": "660720"
  },
  {
    "text": "it's sort of I wouldn't say it's very",
    "start": "660720",
    "end": "662399"
  },
  {
    "text": "low resolution but it's low enough that",
    "start": "662399",
    "end": "665120"
  },
  {
    "text": "sometimes your answers will come back",
    "start": "665120",
    "end": "667079"
  },
  {
    "text": "garbled sometimes it will just um uh say",
    "start": "667079",
    "end": "670480"
  },
  {
    "text": "some random stuff and um we've seen that",
    "start": "670480",
    "end": "674399"
  },
  {
    "text": "um pushing that quantization to eight",
    "start": "674399",
    "end": "677040"
  },
  {
    "text": "bits it sort of gives you that sweet",
    "start": "677040",
    "end": "679360"
  },
  {
    "text": "spot between something that's practical",
    "start": "679360",
    "end": "682000"
  },
  {
    "text": "and something that's performant enough",
    "start": "682000",
    "end": "684240"
  },
  {
    "text": "so that's u your lama's quantization um",
    "start": "684240",
    "end": "687839"
  },
  {
    "text": "uh trade-offs there and we've also seen",
    "start": "687839",
    "end": "690320"
  },
  {
    "text": "that some really important environment",
    "start": "690320",
    "end": "692560"
  },
  {
    "text": "variables are turned off by default so",
    "start": "692560",
    "end": "695120"
  },
  {
    "text": "one such variable is called flash",
    "start": "695120",
    "end": "697120"
  },
  {
    "text": "attention and uh flash attention is um",
    "start": "697120",
    "end": "700640"
  },
  {
    "text": "uh I think it uh turning it on reduces",
    "start": "700640",
    "end": "705200"
  },
  {
    "text": "the amount of memory uh you use when you",
    "start": "705200",
    "end": "708160"
  },
  {
    "text": "when your LLMs are answering questions",
    "start": "708160",
    "end": "710640"
  },
  {
    "text": "effectively reduces the memory needed to",
    "start": "710640",
    "end": "713360"
  },
  {
    "text": "um return back an answer and uh we've",
    "start": "713360",
    "end": "716640"
  },
  {
    "text": "seen that turning on flash attention",
    "start": "716640",
    "end": "719360"
  },
  {
    "text": "takes that um response memory usage from",
    "start": "719360",
    "end": "722240"
  },
  {
    "text": "quadratic to subquadratic with context",
    "start": "722240",
    "end": "725040"
  },
  {
    "text": "length so why did Olama break context",
    "start": "725040",
    "end": "728480"
  },
  {
    "text": "length why did um why did they give us a",
    "start": "728480",
    "end": "731040"
  },
  {
    "text": "very um highly quantized model why did",
    "start": "731040",
    "end": "733760"
  },
  {
    "text": "they turn off um um variables flash",
    "start": "733760",
    "end": "736480"
  },
  {
    "text": "attention and other variables the reason",
    "start": "736480",
    "end": "739040"
  },
  {
    "text": "being that it's um it's meant to run on",
    "start": "739040",
    "end": "741839"
  },
  {
    "text": "your say MacBook where you're competing",
    "start": "741839",
    "end": "744639"
  },
  {
    "text": "for um memory with Chrome and Slack and",
    "start": "744639",
    "end": "747920"
  },
  {
    "text": "all your other enterprise software so",
    "start": "747920",
    "end": "750160"
  },
  {
    "text": "it's it's not something you can take it",
    "start": "750160",
    "end": "752720"
  },
  {
    "text": "to production with a lot of tweaks but",
    "start": "752720",
    "end": "755200"
  },
  {
    "text": "then you quickly move on from using",
    "start": "755200",
    "end": "757040"
  },
  {
    "text": "Olama to another very popular",
    "start": "757040",
    "end": "761399"
  },
  {
    "text": "project called um WLM and WLLM gives is",
    "start": "761399",
    "end": "766320"
  },
  {
    "text": "an inference engine and inference server",
    "start": "766320",
    "end": "768959"
  },
  {
    "text": "and uh it's uh effectively the base on",
    "start": "768959",
    "end": "771680"
  },
  {
    "text": "which we serve our models so over to",
    "start": "771680",
    "end": "774639"
  },
  {
    "text": "Luke to give you a demo of that",
    "start": "774639",
    "end": "777360"
  },
  {
    "text": "cool so uh let me just",
    "start": "777360",
    "end": "780000"
  },
  {
    "text": "find that",
    "start": "780000",
    "end": "781800"
  },
  {
    "text": "sorry okay so",
    "start": "781800",
    "end": "784680"
  },
  {
    "text": "um yeah here we have",
    "start": "784680",
    "end": "788440"
  },
  {
    "text": "um right sorry just found the right part",
    "start": "788440",
    "end": "791040"
  },
  {
    "text": "of the video um so here we have um",
    "start": "791040",
    "end": "793920"
  },
  {
    "text": "another Kubernetes cluster uh this one",
    "start": "793920",
    "end": "796000"
  },
  {
    "text": "has two A100s in it um because it's",
    "start": "796000",
    "end": "798800"
  },
  {
    "text": "running um a few different models uh",
    "start": "798800",
    "end": "801040"
  },
  {
    "text": "which we'll look at later but in",
    "start": "801040",
    "end": "802880"
  },
  {
    "text": "particular let's focus on this VLM",
    "start": "802880",
    "end": "804639"
  },
  {
    "text": "instance here which is uh uh which is",
    "start": "804639",
    "end": "806720"
  },
  {
    "text": "running FI 4 from Microsoft so um what",
    "start": "806720",
    "end": "810240"
  },
  {
    "text": "we've done is we've run uh the Helix uh",
    "start": "810240",
    "end": "812800"
  },
  {
    "text": "stack on top of this uh VLM instance and",
    "start": "812800",
    "end": "815839"
  },
  {
    "text": "what you can see is um that when we uh",
    "start": "815839",
    "end": "819839"
  },
  {
    "text": "when we said hi to to 54 it responded",
    "start": "819839",
    "end": "822800"
  },
  {
    "text": "very very quickly and so um as Priya was",
    "start": "822800",
    "end": "825760"
  },
  {
    "text": "saying VLM did a really good job of",
    "start": "825760",
    "end": "828320"
  },
  {
    "text": "optimizing the the latency on responses",
    "start": "828320",
    "end": "830959"
  },
  {
    "text": "and and the number of tokens per second",
    "start": "830959",
    "end": "833040"
  },
  {
    "text": "um and you end up with uh with a system",
    "start": "833040",
    "end": "836639"
  },
  {
    "text": "which is very performant um and uh it",
    "start": "836639",
    "end": "839760"
  },
  {
    "text": "also comes with better context length",
    "start": "839760",
    "end": "841519"
  },
  {
    "text": "defaults so it's kind of closer to",
    "start": "841519",
    "end": "843360"
  },
  {
    "text": "something you'd want to run in",
    "start": "843360",
    "end": "844240"
  },
  {
    "text": "production um so what I'm now going to",
    "start": "844240",
    "end": "846240"
  },
  {
    "text": "show you is applying um an AI spec uh",
    "start": "846240",
    "end": "849120"
  },
  {
    "text": "which I'll explain what that is later",
    "start": "849120",
    "end": "851120"
  },
  {
    "text": "but the AI spec corresponds to this app",
    "start": "851120",
    "end": "853920"
  },
  {
    "text": "uh which is uh for Jira issues in this",
    "start": "853920",
    "end": "856160"
  },
  {
    "text": "case and what I can show you here is",
    "start": "856160",
    "end": "858079"
  },
  {
    "text": "that now that we've got VLM running with",
    "start": "858079",
    "end": "860560"
  },
  {
    "text": "a proper context length setup um it does",
    "start": "860560",
    "end": "863279"
  },
  {
    "text": "a much better job of answering this",
    "start": "863279",
    "end": "865279"
  },
  {
    "text": "question uh to summarize all of the",
    "start": "865279",
    "end": "867279"
  },
  {
    "text": "issues in the sprint um and it actually",
    "start": "867279",
    "end": "870480"
  },
  {
    "text": "uh gives you 10 issues back rather than",
    "start": "870480",
    "end": "873279"
  },
  {
    "text": "um as uh did truncating the response so",
    "start": "873279",
    "end": "878320"
  },
  {
    "text": "um yeah that's uh that's VLM uh very",
    "start": "878320",
    "end": "881279"
  },
  {
    "text": "good project recommend it yeah cool on",
    "start": "881279",
    "end": "884079"
  },
  {
    "text": "this uh Kubernetes spec on the on the on",
    "start": "884079",
    "end": "887519"
  },
  {
    "text": "sort of the right side here you can see",
    "start": "887519",
    "end": "890160"
  },
  {
    "text": "a few things here one is that we're",
    "start": "890160",
    "end": "891839"
  },
  {
    "text": "pulling in Mistra 7B and alongside the",
    "start": "891839",
    "end": "895040"
  },
  {
    "text": "fact that we're using WHLM u one of the",
    "start": "895040",
    "end": "898800"
  },
  {
    "text": "gotchas here is that uh WLM downloads",
    "start": "898800",
    "end": "904000"
  },
  {
    "text": "all the weights that are needed for this",
    "start": "904000",
    "end": "906000"
  },
  {
    "text": "model um from huggingface.com so which",
    "start": "906000",
    "end": "910320"
  },
  {
    "text": "and getting those weights is a gated",
    "start": "910320",
    "end": "912160"
  },
  {
    "text": "process involves um of course signing",
    "start": "912160",
    "end": "915120"
  },
  {
    "text": "terms and conditions but also it means",
    "start": "915120",
    "end": "917600"
  },
  {
    "text": "you're constrained in in the sense that",
    "start": "917600",
    "end": "920240"
  },
  {
    "text": "your cluster needs to have access to the",
    "start": "920240",
    "end": "922680"
  },
  {
    "text": "internet and um uh the whole idea of",
    "start": "922680",
    "end": "925920"
  },
  {
    "text": "running things privately is so that we",
    "start": "925920",
    "end": "928000"
  },
  {
    "text": "can have things in your airgapped",
    "start": "928000",
    "end": "929680"
  },
  {
    "text": "environment so in order to do that we",
    "start": "929680",
    "end": "931839"
  },
  {
    "text": "kind of scratched our heads and we",
    "start": "931839",
    "end": "933199"
  },
  {
    "text": "wondered how can we uh do we need to uh",
    "start": "933199",
    "end": "936079"
  },
  {
    "text": "h invest in a model registry and then we",
    "start": "936079",
    "end": "938800"
  },
  {
    "text": "said no we don't uh let's just use our",
    "start": "938800",
    "end": "941440"
  },
  {
    "text": "docker registry and so we uh baked the",
    "start": "941440",
    "end": "944800"
  },
  {
    "text": "weights of the model into the docker",
    "start": "944800",
    "end": "947199"
  },
  {
    "text": "image along with the model and we check",
    "start": "947199",
    "end": "949519"
  },
  {
    "text": "that into um into docker registry we did",
    "start": "949519",
    "end": "953040"
  },
  {
    "text": "run into a few issues here though um one",
    "start": "953040",
    "end": "956000"
  },
  {
    "text": "being that um Docker by default uses um",
    "start": "956000",
    "end": "959600"
  },
  {
    "text": "a gzip level 8 compression model and uh",
    "start": "959600",
    "end": "962880"
  },
  {
    "text": "this means that um it just takes quite",
    "start": "962880",
    "end": "965600"
  },
  {
    "text": "some time for the model to get up and",
    "start": "965600",
    "end": "967199"
  },
  {
    "text": "running and answer questions so we then",
    "start": "967199",
    "end": "969360"
  },
  {
    "text": "had to patch um the machine building the",
    "start": "969360",
    "end": "972160"
  },
  {
    "text": "docu uh images to use a custom",
    "start": "972160",
    "end": "975360"
  },
  {
    "text": "compression algorithm and that and",
    "start": "975360",
    "end": "977839"
  },
  {
    "text": "pretty much everything else runs as",
    "start": "977839",
    "end": "979759"
  },
  {
    "text": "normal but then uh that's sort of where",
    "start": "979759",
    "end": "982079"
  },
  {
    "text": "we got to let's again step back a bit uh",
    "start": "982079",
    "end": "986880"
  },
  {
    "text": "what are we doing here we're serving",
    "start": "986880",
    "end": "988560"
  },
  {
    "text": "Mistra 7B but then uh there are very",
    "start": "988560",
    "end": "992480"
  },
  {
    "text": "many models that do very different",
    "start": "992480",
    "end": "994480"
  },
  {
    "text": "things really well there are image",
    "start": "994480",
    "end": "996639"
  },
  {
    "text": "models there are multilingual models and",
    "start": "996639",
    "end": "999680"
  },
  {
    "text": "uh in this mechanism you're you're sort",
    "start": "999680",
    "end": "1001600"
  },
  {
    "text": "of at this point where you need a",
    "start": "1001600",
    "end": "1003360"
  },
  {
    "text": "completely separate uh node pool for",
    "start": "1003360",
    "end": "1006480"
  },
  {
    "text": "different types of models and that's",
    "start": "1006480",
    "end": "1008480"
  },
  {
    "text": "sort of where you your costs become",
    "start": "1008480",
    "end": "1010399"
  },
  {
    "text": "limiting so we then decided um we've got",
    "start": "1010399",
    "end": "1013279"
  },
  {
    "text": "to find a way to run multiple models but",
    "start": "1013279",
    "end": "1016480"
  },
  {
    "text": "also have GPU um uh u memory uh sharing",
    "start": "1016480",
    "end": "1021600"
  },
  {
    "text": "in place and um that's sort of where we",
    "start": "1021600",
    "end": "1024880"
  },
  {
    "text": "said we're all good cloud engineers",
    "start": "1024880",
    "end": "1026880"
  },
  {
    "text": "let's build our own",
    "start": "1026880",
    "end": "1028600"
  },
  {
    "text": "scheduleuler so that is one thing um but",
    "start": "1028600",
    "end": "1031520"
  },
  {
    "text": "then the other thing is um you've you're",
    "start": "1031520",
    "end": "1035120"
  },
  {
    "text": "you're increasing your memory",
    "start": "1035120",
    "end": "1036640"
  },
  {
    "text": "utilization on your cluster you've got a",
    "start": "1036640",
    "end": "1039038"
  },
  {
    "text": "production uh readyish uh environment",
    "start": "1039039",
    "end": "1042720"
  },
  {
    "text": "where you can serve multiple models you",
    "start": "1042720",
    "end": "1045120"
  },
  {
    "text": "can also mix fine-tuning along with",
    "start": "1045120",
    "end": "1047918"
  },
  {
    "text": "inference jobs which then means you've",
    "start": "1047919",
    "end": "1050000"
  },
  {
    "text": "got a wider set of tasks you can do with",
    "start": "1050000",
    "end": "1052240"
  },
  {
    "text": "your models this is all great but then",
    "start": "1052240",
    "end": "1055840"
  },
  {
    "text": "what's really valuable to the end user",
    "start": "1055840",
    "end": "1058400"
  },
  {
    "text": "is what you build on top why are we",
    "start": "1058400",
    "end": "1060559"
  },
  {
    "text": "serving these models so Luke's going to",
    "start": "1060559",
    "end": "1062640"
  },
  {
    "text": "give us a run through of that",
    "start": "1062640",
    "end": "1066160"
  },
  {
    "text": "cool thank you Priya um yeah so I I want",
    "start": "1066160",
    "end": "1069360"
  },
  {
    "text": "to talk a bit about um the different",
    "start": "1069360",
    "end": "1071200"
  },
  {
    "text": "layers on top um uh and what you might",
    "start": "1071200",
    "end": "1073679"
  },
  {
    "text": "want to build kind of on top of an LLM",
    "start": "1073679",
    "end": "1076400"
  },
  {
    "text": "now that you've got it deployed um so",
    "start": "1076400",
    "end": "1079760"
  },
  {
    "text": "now that you've got an LLM and you've",
    "start": "1079760",
    "end": "1081360"
  },
  {
    "text": "got like a a chat interface to it well",
    "start": "1081360",
    "end": "1083520"
  },
  {
    "text": "what are you going to do with it well",
    "start": "1083520",
    "end": "1085200"
  },
  {
    "text": "you could ask it to count the number of",
    "start": "1085200",
    "end": "1086799"
  },
  {
    "text": "Rs in Strawberry and maybe it will uh",
    "start": "1086799",
    "end": "1089280"
  },
  {
    "text": "you can ask it to tell you a joke um but",
    "start": "1089280",
    "end": "1091520"
  },
  {
    "text": "neither of those things are particularly",
    "start": "1091520",
    "end": "1093520"
  },
  {
    "text": "useful in a business context and in",
    "start": "1093520",
    "end": "1095679"
  },
  {
    "text": "order to really get value um out of an",
    "start": "1095679",
    "end": "1098160"
  },
  {
    "text": "LLM that you're running in a Kubernetes",
    "start": "1098160",
    "end": "1100400"
  },
  {
    "text": "cluster you need to start connecting it",
    "start": "1100400",
    "end": "1103280"
  },
  {
    "text": "to your business data and your business",
    "start": "1103280",
    "end": "1105000"
  },
  {
    "text": "applications and so those are two",
    "start": "1105000",
    "end": "1106880"
  },
  {
    "text": "different types of integrations that",
    "start": "1106880",
    "end": "1108240"
  },
  {
    "text": "I'll go into in in a little bit more",
    "start": "1108240",
    "end": "1110240"
  },
  {
    "text": "detail now about how you can do uh",
    "start": "1110240",
    "end": "1113440"
  },
  {
    "text": "knowledge which is also known as rag um",
    "start": "1113440",
    "end": "1115679"
  },
  {
    "text": "and API integrations and then the other",
    "start": "1115679",
    "end": "1118320"
  },
  {
    "text": "question is like how do you ensure the",
    "start": "1118320",
    "end": "1120080"
  },
  {
    "text": "quality of the LLM product that you're",
    "start": "1120080",
    "end": "1122400"
  },
  {
    "text": "shipping like the LLM system it's not",
    "start": "1122400",
    "end": "1124880"
  },
  {
    "text": "just an LLM it's all of the stuff around",
    "start": "1124880",
    "end": "1126720"
  },
  {
    "text": "it and the way that that you do that is",
    "start": "1126720",
    "end": "1128799"
  },
  {
    "text": "the same way that you do it with",
    "start": "1128799",
    "end": "1130000"
  },
  {
    "text": "software and DevOps which is that you",
    "start": "1130000",
    "end": "1131679"
  },
  {
    "text": "write tests but writing tests is a",
    "start": "1131679",
    "end": "1134160"
  },
  {
    "text": "little bit different in this world of",
    "start": "1134160",
    "end": "1135440"
  },
  {
    "text": "LLM applications um it's known as eval",
    "start": "1135440",
    "end": "1138400"
  },
  {
    "text": "which means evaluations um so what we",
    "start": "1138400",
    "end": "1140960"
  },
  {
    "text": "did was we uh we set up um we we built a",
    "start": "1140960",
    "end": "1144240"
  },
  {
    "text": "little CLI tool uh that that uses an LLM",
    "start": "1144240",
    "end": "1147440"
  },
  {
    "text": "as a judge to judge the output of uh the",
    "start": "1147440",
    "end": "1150880"
  },
  {
    "text": "system and to then um give you a pass or",
    "start": "1150880",
    "end": "1154160"
  },
  {
    "text": "fail grade that you can then use in a",
    "start": "1154160",
    "end": "1155760"
  },
  {
    "text": "CI/CD system um in order to to test the",
    "start": "1155760",
    "end": "1158960"
  },
  {
    "text": "system and we feel like this is super",
    "start": "1158960",
    "end": "1161120"
  },
  {
    "text": "important because um if you can apply",
    "start": "1161120",
    "end": "1163520"
  },
  {
    "text": "the the sort of same CI/CD process you",
    "start": "1163520",
    "end": "1166240"
  },
  {
    "text": "do you use for the rest of your software",
    "start": "1166240",
    "end": "1168160"
  },
  {
    "text": "to genai applications that are defined",
    "start": "1168160",
    "end": "1170880"
  },
  {
    "text": "as configuration uh like a CRD then um",
    "start": "1170880",
    "end": "1174400"
  },
  {
    "text": "you can apply all of the existing kind",
    "start": "1174400",
    "end": "1176240"
  },
  {
    "text": "of best practices and process that you",
    "start": "1176240",
    "end": "1178080"
  },
  {
    "text": "have for for your engineering teams to",
    "start": "1178080",
    "end": "1179919"
  },
  {
    "text": "to build and iterate on these things",
    "start": "1179919",
    "end": "1182840"
  },
  {
    "text": "um",
    "start": "1182840",
    "end": "1184440"
  },
  {
    "text": "so the next uh piece I want to share is",
    "start": "1184440",
    "end": "1188400"
  },
  {
    "text": "is like how the integrations work so um",
    "start": "1188400",
    "end": "1192559"
  },
  {
    "text": "you actually saw a glimpse of the",
    "start": "1192559",
    "end": "1194080"
  },
  {
    "text": "integrations in the demo I just did a",
    "start": "1194080",
    "end": "1196160"
  },
  {
    "text": "second ago uh where we made an API call",
    "start": "1196160",
    "end": "1199679"
  },
  {
    "text": "um into Jira uh for an as an example and",
    "start": "1199679",
    "end": "1202720"
  },
  {
    "text": "so let me explain kind of break down for",
    "start": "1202720",
    "end": "1204480"
  },
  {
    "text": "you like how that worked because that's",
    "start": "1204480",
    "end": "1206000"
  },
  {
    "text": "actually a multi-step process so the",
    "start": "1206000",
    "end": "1208320"
  },
  {
    "text": "first step is that and and each step in",
    "start": "1208320",
    "end": "1210799"
  },
  {
    "text": "this process by the way is a prompt",
    "start": "1210799",
    "end": "1212880"
  },
  {
    "text": "template so it's like a prompt that can",
    "start": "1212880",
    "end": "1215280"
  },
  {
    "text": "kind of get filled in um uh so the first",
    "start": "1215280",
    "end": "1218960"
  },
  {
    "text": "prompt template is a classifier so it",
    "start": "1218960",
    "end": "1221760"
  },
  {
    "text": "classifies whether the request needs an",
    "start": "1221760",
    "end": "1223840"
  },
  {
    "text": "API call or not um because if the model",
    "start": "1223840",
    "end": "1226640"
  },
  {
    "text": "is just answering what is the capital of",
    "start": "1226640",
    "end": "1228320"
  },
  {
    "text": "of France it doesn't need to make an API",
    "start": "1228320",
    "end": "1230799"
  },
  {
    "text": "call to Jira to answer that question but",
    "start": "1230799",
    "end": "1232720"
  },
  {
    "text": "if you're asking how many issues are",
    "start": "1232720",
    "end": "1234000"
  },
  {
    "text": "there in the sprint or um uh what's the",
    "start": "1234000",
    "end": "1237440"
  },
  {
    "text": "current exchange rate then the system is",
    "start": "1237440",
    "end": "1239840"
  },
  {
    "text": "going to need to make an API call uh to",
    "start": "1239840",
    "end": "1241600"
  },
  {
    "text": "an external system so the first step is",
    "start": "1241600",
    "end": "1244000"
  },
  {
    "text": "to just classify yes or no and if yes",
    "start": "1244000",
    "end": "1246320"
  },
  {
    "text": "then which of the available tools um and",
    "start": "1246320",
    "end": "1249440"
  },
  {
    "text": "then if the answer to the uh classifier",
    "start": "1249440",
    "end": "1252400"
  },
  {
    "text": "was yes um then the next step is to",
    "start": "1252400",
    "end": "1254720"
  },
  {
    "text": "actually ask the LLM to construct an API",
    "start": "1254720",
    "end": "1257120"
  },
  {
    "text": "request body based on the user's query",
    "start": "1257120",
    "end": "1260000"
  },
  {
    "text": "and the open API spec for that API and",
    "start": "1260000",
    "end": "1263360"
  },
  {
    "text": "amazingly LLMs can do this they can take",
    "start": "1263360",
    "end": "1265120"
  },
  {
    "text": "an open API spec they can take the",
    "start": "1265120",
    "end": "1266640"
  },
  {
    "text": "user's question and they can have a go",
    "start": "1266640",
    "end": "1268559"
  },
  {
    "text": "at making an AP at constructing an API",
    "start": "1268559",
    "end": "1270880"
  },
  {
    "text": "call um that can then uh be executed um",
    "start": "1270880",
    "end": "1275039"
  },
  {
    "text": "so then um the API response uh so then",
    "start": "1275039",
    "end": "1279840"
  },
  {
    "text": "the system that we built like will make",
    "start": "1279840",
    "end": "1281840"
  },
  {
    "text": "the API request and take the API",
    "start": "1281840",
    "end": "1284640"
  },
  {
    "text": "response and then put that into the",
    "start": "1284640",
    "end": "1286400"
  },
  {
    "text": "third uh template which then summarizes",
    "start": "1286400",
    "end": "1288720"
  },
  {
    "text": "the response back to the user um so",
    "start": "1288720",
    "end": "1290880"
  },
  {
    "text": "that's API",
    "start": "1290880",
    "end": "1292360"
  },
  {
    "text": "integrations and the other major pattern",
    "start": "1292360",
    "end": "1295360"
  },
  {
    "text": "um so API integrations by the way that",
    "start": "1295360",
    "end": "1297440"
  },
  {
    "text": "was kind of like live queries against",
    "start": "1297440",
    "end": "1299360"
  },
  {
    "text": "running systems the other pattern is",
    "start": "1299360",
    "end": "1301760"
  },
  {
    "text": "called knowledge um also known as",
    "start": "1301760",
    "end": "1303760"
  },
  {
    "text": "retrieval augmented generation although",
    "start": "1303760",
    "end": "1305840"
  },
  {
    "text": "I think that rag is an overly",
    "start": "1305840",
    "end": "1308159"
  },
  {
    "text": "complicated term and I I prefer the word",
    "start": "1308159",
    "end": "1310000"
  },
  {
    "text": "knowledge",
    "start": "1310000",
    "end": "1311400"
  },
  {
    "text": "um the idea with knowledge is just to",
    "start": "1311400",
    "end": "1314000"
  },
  {
    "text": "bring appropriate context much like API",
    "start": "1314000",
    "end": "1316640"
  },
  {
    "text": "calls um bring appropriate context to",
    "start": "1316640",
    "end": "1319360"
  },
  {
    "text": "the LLM when it is uh doing inference so",
    "start": "1319360",
    "end": "1323039"
  },
  {
    "text": "this um this also happens in in three",
    "start": "1323039",
    "end": "1325679"
  },
  {
    "text": "steps uh the first step um is that you",
    "start": "1325679",
    "end": "1328559"
  },
  {
    "text": "need a vector database so a vector",
    "start": "1328559",
    "end": "1330960"
  },
  {
    "text": "database uh and there's good uh",
    "start": "1330960",
    "end": "1333760"
  },
  {
    "text": "Postgress extensions for this now by the",
    "start": "1333760",
    "end": "1335840"
  },
  {
    "text": "way um we use uh PG vector and vector",
    "start": "1335840",
    "end": "1338320"
  },
  {
    "text": "cord i I recommend vector cord actually",
    "start": "1338320",
    "end": "1340799"
  },
  {
    "text": "we use it in production with a customer",
    "start": "1340799",
    "end": "1342400"
  },
  {
    "text": "even though it's quite an early project",
    "start": "1342400",
    "end": "1344400"
  },
  {
    "text": "uh it works very well um uh and it",
    "start": "1344400",
    "end": "1347200"
  },
  {
    "text": "scales well and so what you do with uh",
    "start": "1347200",
    "end": "1350400"
  },
  {
    "text": "with with a vector database is you take",
    "start": "1350400",
    "end": "1352400"
  },
  {
    "text": "your source documents and you chunk them",
    "start": "1352400",
    "end": "1354960"
  },
  {
    "text": "up into pieces and then you uh put each",
    "start": "1354960",
    "end": "1357919"
  },
  {
    "text": "piece into the vector database which",
    "start": "1357919",
    "end": "1359520"
  },
  {
    "text": "then kind of maps the text in that",
    "start": "1359520",
    "end": "1362000"
  },
  {
    "text": "document into a vector space which then",
    "start": "1362000",
    "end": "1364480"
  },
  {
    "text": "allows you to do a similarity search",
    "start": "1364480",
    "end": "1366640"
  },
  {
    "text": "with the user's query it's a very long",
    "start": "1366640",
    "end": "1369360"
  },
  {
    "text": "way of saying that if the user asks a",
    "start": "1369360",
    "end": "1371919"
  },
  {
    "text": "question it will find relevant documents",
    "start": "1371919",
    "end": "1373679"
  },
  {
    "text": "in the database and include those",
    "start": "1373679",
    "end": "1375280"
  },
  {
    "text": "documents along with the user's question",
    "start": "1375280",
    "end": "1377520"
  },
  {
    "text": "when sending that to the LLM and that's",
    "start": "1377520",
    "end": "1379520"
  },
  {
    "text": "the step two which is you take the",
    "start": "1379520",
    "end": "1380880"
  },
  {
    "text": "prompt and the response uh you pass it",
    "start": "1380880",
    "end": "1383120"
  },
  {
    "text": "into the model and then the model is",
    "start": "1383120",
    "end": "1384880"
  },
  {
    "text": "kind of grounded in facts and then the",
    "start": "1384880",
    "end": "1386960"
  },
  {
    "text": "third step is that you can just",
    "start": "1386960",
    "end": "1388480"
  },
  {
    "text": "regularly refresh that knowledge kind of",
    "start": "1388480",
    "end": "1390400"
  },
  {
    "text": "as it",
    "start": "1390400",
    "end": "1391799"
  },
  {
    "text": "changes um and then the last piece oh",
    "start": "1391799",
    "end": "1395520"
  },
  {
    "text": "actually no I'll jump to the demo sorry",
    "start": "1395520",
    "end": "1397320"
  },
  {
    "text": "um so pass that back to you so the",
    "start": "1397320",
    "end": "1400720"
  },
  {
    "text": "there's a third demo here um uh of",
    "start": "1400720",
    "end": "1403400"
  },
  {
    "text": "knowledge and um I actually couldn't",
    "start": "1403400",
    "end": "1406799"
  },
  {
    "text": "help myself uh I I had to um uh share",
    "start": "1406799",
    "end": "1411600"
  },
  {
    "text": "the a demo of something that we just got",
    "start": "1411600",
    "end": "1413919"
  },
  {
    "text": "working yesterday um so this is",
    "start": "1413919",
    "end": "1416080"
  },
  {
    "text": "knowledge but it's a a kind of advanced",
    "start": "1416080",
    "end": "1419200"
  },
  {
    "text": "knowledge pipeline um that's called",
    "start": "1419200",
    "end": "1421280"
  },
  {
    "text": "vision rag and Phil's in the audience",
    "start": "1421280",
    "end": "1422880"
  },
  {
    "text": "over there he got this working like uh",
    "start": "1422880",
    "end": "1425039"
  },
  {
    "text": "yesterday morning so so thank you Phil",
    "start": "1425039",
    "end": "1427280"
  },
  {
    "text": "um what we're going to see here",
    "start": "1427280",
    "end": "1430159"
  },
  {
    "text": "um and actually just just before I I",
    "start": "1430159",
    "end": "1432960"
  },
  {
    "text": "jump into that let me let me explain how",
    "start": "1432960",
    "end": "1435600"
  },
  {
    "text": "a vision rag pipeline works so here we",
    "start": "1435600",
    "end": "1438400"
  },
  {
    "text": "talked about putting text into the",
    "start": "1438400",
    "end": "1440080"
  },
  {
    "text": "vector database and getting text back uh",
    "start": "1440080",
    "end": "1443039"
  },
  {
    "text": "another thing you can do is you can use",
    "start": "1443039",
    "end": "1444559"
  },
  {
    "text": "what's called a multimodal uh embedding",
    "start": "1444559",
    "end": "1447120"
  },
  {
    "text": "model that can embed images and text",
    "start": "1447120",
    "end": "1450240"
  },
  {
    "text": "into the same vector space and then you",
    "start": "1450240",
    "end": "1452400"
  },
  {
    "text": "can get images out of the vector",
    "start": "1452400",
    "end": "1453919"
  },
  {
    "text": "database and then include them along",
    "start": "1453919",
    "end": "1455840"
  },
  {
    "text": "with the user's question as images um",
    "start": "1455840",
    "end": "1458480"
  },
  {
    "text": "and then feed that uh into uh into the",
    "start": "1458480",
    "end": "1461520"
  },
  {
    "text": "LLM uh into a vision language model like",
    "start": "1461520",
    "end": "1464320"
  },
  {
    "text": "one that's able to understand images so",
    "start": "1464320",
    "end": "1467279"
  },
  {
    "text": "um let me show you why that's a good",
    "start": "1467279",
    "end": "1468720"
  },
  {
    "text": "idea so here we have um a texton rag",
    "start": "1468720",
    "end": "1473159"
  },
  {
    "text": "pipeline um and the rag pipeline uh has",
    "start": "1473159",
    "end": "1476799"
  },
  {
    "text": "a PDF in it uh and if we look here we",
    "start": "1476799",
    "end": "1480880"
  },
  {
    "text": "can look inside the PDF",
    "start": "1480880",
    "end": "1483799"
  },
  {
    "text": "um so I think if we take this so this is",
    "start": "1483799",
    "end": "1487200"
  },
  {
    "text": "a financial uh paper for example about",
    "start": "1487200",
    "end": "1490000"
  },
  {
    "text": "um stock sentiment um and you can see",
    "start": "1490000",
    "end": "1493200"
  },
  {
    "text": "that the paper has text in it that you",
    "start": "1493200",
    "end": "1495600"
  },
  {
    "text": "can highlight but it also has these",
    "start": "1495600",
    "end": "1497320"
  },
  {
    "text": "tables and it looks like the person who",
    "start": "1497320",
    "end": "1499600"
  },
  {
    "text": "made this document like took screenshots",
    "start": "1499600",
    "end": "1501840"
  },
  {
    "text": "of Excel and pasted them into a word",
    "start": "1501840",
    "end": "1504480"
  },
  {
    "text": "document pasted the screenshots um",
    "start": "1504480",
    "end": "1507039"
  },
  {
    "text": "because those uh those images um that",
    "start": "1507039",
    "end": "1510400"
  },
  {
    "text": "those tables are images so the problem",
    "start": "1510400",
    "end": "1512960"
  },
  {
    "text": "is if you just take the text out of the",
    "start": "1512960",
    "end": "1515039"
  },
  {
    "text": "PDF and put it through this existing",
    "start": "1515039",
    "end": "1517360"
  },
  {
    "text": "kind of uh text rag pipeline uh we're",
    "start": "1517360",
    "end": "1520159"
  },
  {
    "text": "asking a question here about this table",
    "start": "1520159",
    "end": "1522159"
  },
  {
    "text": "in particular the uh the price change",
    "start": "1522159",
    "end": "1524480"
  },
  {
    "text": "correlation uh table um and if we look",
    "start": "1524480",
    "end": "1527919"
  },
  {
    "text": "at the result uh the result from uh from",
    "start": "1527919",
    "end": "1531440"
  },
  {
    "text": "the system is um is a fairly good",
    "start": "1531440",
    "end": "1534880"
  },
  {
    "text": "summary of kind of the relevant topic um",
    "start": "1534880",
    "end": "1538960"
  },
  {
    "text": "but it has but it admits like the exact",
    "start": "1538960",
    "end": "1541919"
  },
  {
    "text": "10day sentiment lag for the energy",
    "start": "1541919",
    "end": "1543360"
  },
  {
    "text": "sector isn't provided in the context",
    "start": "1543360",
    "end": "1545919"
  },
  {
    "text": "even though it was right there in the",
    "start": "1545919",
    "end": "1547120"
  },
  {
    "text": "document as a human you can read you can",
    "start": "1547120",
    "end": "1549279"
  },
  {
    "text": "look at the document and you can see",
    "start": "1549279",
    "end": "1550640"
  },
  {
    "text": "that um but because it was an image in",
    "start": "1550640",
    "end": "1552960"
  },
  {
    "text": "the document it uh it wasn't able to to",
    "start": "1552960",
    "end": "1555440"
  },
  {
    "text": "do it so we've added this new vision",
    "start": "1555440",
    "end": "1557279"
  },
  {
    "text": "toggle um in the system and uh and now",
    "start": "1557279",
    "end": "1561120"
  },
  {
    "text": "this puts it through an image pipeline",
    "start": "1561120",
    "end": "1562720"
  },
  {
    "text": "like I just described and it immediately",
    "start": "1562720",
    "end": "1564799"
  },
  {
    "text": "gives you an answer so it says the",
    "start": "1564799",
    "end": "1566640"
  },
  {
    "text": "sentiment lag for the energy sector is",
    "start": "1566640",
    "end": "1568120"
  },
  {
    "text": "0.35 so if we go and look here we can",
    "start": "1568120",
    "end": "1570559"
  },
  {
    "text": "check did it get the right answer uh the",
    "start": "1570559",
    "end": "1572880"
  },
  {
    "text": "energy sector uh yes it worked so um",
    "start": "1572880",
    "end": "1577200"
  },
  {
    "text": "it's able to pull the exact right answer",
    "start": "1577200",
    "end": "1579200"
  },
  {
    "text": "out of this document even though it's",
    "start": "1579200",
    "end": "1580799"
  },
  {
    "text": "like a complex document layout um so",
    "start": "1580799",
    "end": "1583279"
  },
  {
    "text": "this is like really basically an amazing",
    "start": "1583279",
    "end": "1585679"
  },
  {
    "text": "information processing system you",
    "start": "1585679",
    "end": "1587520"
  },
  {
    "text": "probably got like loads of uh like",
    "start": "1587520",
    "end": "1590880"
  },
  {
    "text": "knowledge locked up in your organization",
    "start": "1590880",
    "end": "1593120"
  },
  {
    "text": "um and uh and you can use this approach",
    "start": "1593120",
    "end": "1596320"
  },
  {
    "text": "to uh to extract it reliably and make",
    "start": "1596320",
    "end": "1599600"
  },
  {
    "text": "people more productive um so yeah uh",
    "start": "1599600",
    "end": "1602720"
  },
  {
    "text": "I'll take the mic um thanks so the last",
    "start": "1602720",
    "end": "1606000"
  },
  {
    "text": "thing I want to mention is that we're",
    "start": "1606000",
    "end": "1608000"
  },
  {
    "text": "working on this effort called the AI",
    "start": "1608000",
    "end": "1609679"
  },
  {
    "text": "spec and the AI spec as you can see here",
    "start": "1609679",
    "end": "1612559"
  },
  {
    "text": "um is an effort to define a new CRD type",
    "start": "1612559",
    "end": "1616400"
  },
  {
    "text": "in Kubernetes um that allows you to",
    "start": "1616400",
    "end": "1619520"
  },
  {
    "text": "describe all of the applications that",
    "start": "1619520",
    "end": "1621600"
  },
  {
    "text": "I've just shown you as configuration",
    "start": "1621600",
    "end": "1623760"
  },
  {
    "text": "rather than code because that allows you",
    "start": "1623760",
    "end": "1626080"
  },
  {
    "text": "to avoid having uh like Genai sprawl in",
    "start": "1626080",
    "end": "1630720"
  },
  {
    "text": "your um across your company where",
    "start": "1630720",
    "end": "1633600"
  },
  {
    "text": "different teams might be building",
    "start": "1633600",
    "end": "1635520"
  },
  {
    "text": "completely separate um implementations",
    "start": "1635520",
    "end": "1638320"
  },
  {
    "text": "of rag pipelines and then you need to",
    "start": "1638320",
    "end": "1639760"
  },
  {
    "text": "figure how to secure them all in if you",
    "start": "1639760",
    "end": "1641919"
  },
  {
    "text": "instead define everything in terms of",
    "start": "1641919",
    "end": "1643360"
  },
  {
    "text": "this configuration uh then everyone can",
    "start": "1643360",
    "end": "1645520"
  },
  {
    "text": "be iterating on the same type of thing",
    "start": "1645520",
    "end": "1648080"
  },
  {
    "text": "and you can use testing um the evals",
    "start": "1648080",
    "end": "1651039"
  },
  {
    "text": "that I described uh and CI/CD uh to um",
    "start": "1651039",
    "end": "1655279"
  },
  {
    "text": "to result in uh less effort and better",
    "start": "1655279",
    "end": "1658799"
  },
  {
    "text": "results so uh that's uh all we've got um",
    "start": "1658799",
    "end": "1663520"
  },
  {
    "text": "thank you for coming to the talk um if",
    "start": "1663520",
    "end": "1665520"
  },
  {
    "text": "you're interested in any of the things",
    "start": "1665520",
    "end": "1668240"
  },
  {
    "text": "um that we talked about today then",
    "start": "1668240",
    "end": "1669919"
  },
  {
    "text": "please connect uh with both of us on",
    "start": "1669919",
    "end": "1671919"
  },
  {
    "text": "LinkedIn um in particular if you're",
    "start": "1671919",
    "end": "1676000"
  },
  {
    "text": "looking at building like a developer",
    "start": "1676000",
    "end": "1678399"
  },
  {
    "text": "like a Genai developer platform of your",
    "start": "1678399",
    "end": "1680799"
  },
  {
    "text": "own and you are interested in",
    "start": "1680799",
    "end": "1682480"
  },
  {
    "text": "collaborating on the AI spec um then",
    "start": "1682480",
    "end": "1684880"
  },
  {
    "text": "please uh reach out to me um and um yeah",
    "start": "1684880",
    "end": "1688880"
  },
  {
    "text": "if uh if you're working on these kinds",
    "start": "1688880",
    "end": "1691440"
  },
  {
    "text": "of things and you'd like some help um",
    "start": "1691440",
    "end": "1694000"
  },
  {
    "text": "then please also reach out to me uh",
    "start": "1694000",
    "end": "1696000"
  },
  {
    "text": "we're happy happy to talk to everyone",
    "start": "1696000",
    "end": "1698159"
  },
  {
    "text": "and uh compare notes um so thank you",
    "start": "1698159",
    "end": "1701039"
  },
  {
    "text": "very much",
    "start": "1701039",
    "end": "1703759"
  }
]