[
  {
    "text": "[Music] all right thank you everybody for coming out at the very very end of the",
    "start": "0",
    "end": "6919"
  },
  {
    "text": "conference",
    "start": "6919",
    "end": "9919"
  },
  {
    "text": "um thank you I appreciate it I'm Hannah tab I'm a senior engineer on the ethos",
    "start": "12320",
    "end": "19520"
  },
  {
    "text": "cost efficiency team at Adobe and this is the node Tetris Rabbit",
    "start": "19520",
    "end": "25000"
  },
  {
    "text": "Hole why your bin packing and ours is underperforming",
    "start": "25000",
    "end": "31720"
  },
  {
    "text": "quick intro to ethos ethos is adobe's internal kubernetes platform as a",
    "start": "32960",
    "end": "38760"
  },
  {
    "text": "service we cover onboarding deployment integration testing monitoring",
    "start": "38760",
    "end": "44039"
  },
  {
    "text": "everything in between you know the drill you've been here all week we have different levels of guard rails for our",
    "start": "44039",
    "end": "49520"
  },
  {
    "text": "different client needs everywhere from just upload your container and we'll take care of the rest to almost",
    "start": "49520",
    "end": "55760"
  },
  {
    "text": "unfettered access to the kubernetes ecosystem we are multi cloud and multi-",
    "start": "55760",
    "end": "60840"
  },
  {
    "text": "region across Azure AWS Data Center and all over the world we offer both shared",
    "start": "60840",
    "end": "68040"
  },
  {
    "text": "and dedicated clusters for our different team needs and all of this is across",
    "start": "68040",
    "end": "73119"
  },
  {
    "text": "hundreds of clusters and thousands of nodes it's",
    "start": "73119",
    "end": "79159"
  },
  {
    "text": "big our background the cost efficiency team on ethos was and it still somewhat is a",
    "start": "80520",
    "end": "89119"
  },
  {
    "text": "tiny team with the focus on cost efficiency start off with pretty much just me and my manager in 2020 and we",
    "start": "89119",
    "end": "96000"
  },
  {
    "text": "slowly started beginning to grow over the past few years and all of that was to deal with a huge problem space we had",
    "start": "96000",
    "end": "104719"
  },
  {
    "text": "to cover unallocated space we had to cover cost monitoring we had to cover client Resource Management we had to",
    "start": "104719",
    "end": "111119"
  },
  {
    "text": "cover everything else that falls under overhead spend reduction and one of the paths we went",
    "start": "111119",
    "end": "118039"
  },
  {
    "text": "down was improving our resource management by improving our bin",
    "start": "118039",
    "end": "125560"
  },
  {
    "text": "packing a few definitions just before I start for some of the specific terms I'm",
    "start": "126360",
    "end": "132440"
  },
  {
    "text": "using a client in this context is an adobe internal namespace owner running",
    "start": "132440",
    "end": "137959"
  },
  {
    "text": "pods in one of our managed kubernetes clusters unallocated space is empty",
    "start": "137959",
    "end": "144040"
  },
  {
    "text": "space on a kubernetes node that's not reserved by any cluster resource whether that's uh client name space or whether",
    "start": "144040",
    "end": "151360"
  },
  {
    "text": "that's a demon set or whether that's just the kernel a pod disruption budget is a",
    "start": "151360",
    "end": "158879"
  },
  {
    "text": "kubernetes object that controls how many pods uh with a certain label are allowed to be taken down by manual action at any",
    "start": "158879",
    "end": "166280"
  },
  {
    "text": "time this can be from cluster upgrades this can be from nodes getting moved around anything like that taints and",
    "start": "166280",
    "end": "173480"
  },
  {
    "text": "tolerations are pod and node properties that make tainted nodes only schedule nodes only schedule pod pods with the",
    "start": "173480",
    "end": "180440"
  },
  {
    "text": "corresponding Toleration and pod anti-affinity and topology spread are two separate Concepts but in this",
    "start": "180440",
    "end": "187560"
  },
  {
    "text": "particular context they are covering a similar area that Co they cover they are",
    "start": "187560",
    "end": "193799"
  },
  {
    "text": "pod specifications that control how many related pods can be scheduled onto the same node at one",
    "start": "193799",
    "end": "200680"
  },
  {
    "text": "time so the problem are the start of the rabbit",
    "start": "200680",
    "end": "208519"
  },
  {
    "text": "hole here here's what kubernetes clusters should look like in an Ideal World we have nodes with a small and",
    "start": "208519",
    "end": "214879"
  },
  {
    "text": "reasonably even amount of empty space packed tightly and and with good",
    "start": "214879",
    "end": "221159"
  },
  {
    "text": "organization there's some variation expected as nodes get upgraded moved",
    "start": "221159",
    "end": "226439"
  },
  {
    "text": "around pods come up pods come down but overall it's expected that unallocated",
    "start": "226439",
    "end": "231840"
  },
  {
    "text": "will stay within an acceptable range maybe one or two nodes with extra empty space as pods are shuffled around",
    "start": "231840",
    "end": "240920"
  },
  {
    "text": "here's what we were working with we had shared cluster chaos we had nodes that",
    "start": "243120",
    "end": "248400"
  },
  {
    "text": "were more than half empty we had an autoscaler that wasn't properly consolidating them we had auler that",
    "start": "248400",
    "end": "253680"
  },
  {
    "text": "spun up new nodes instead of packing pods onto old ones if we were just running one or two tiny clusters maybe",
    "start": "253680",
    "end": "261600"
  },
  {
    "text": "we could have dropped this further down the priority list but as I've said hundreds of clusters thousands of nodes",
    "start": "261600",
    "end": "268479"
  },
  {
    "text": "this was becoming financially unsustainable so we started",
    "start": "268479",
    "end": "277320"
  },
  {
    "text": "investigating here's where we started we knew the symptoms and we knew a few of",
    "start": "277320",
    "end": "282479"
  },
  {
    "text": "the causes we knew that we had low usage nodes we knew that we had nodes that were imbalanced where CPU might have",
    "start": "282479",
    "end": "289639"
  },
  {
    "text": "been very high but memory usage was very low or the other way around and we knew that we had unutilized resources that",
    "start": "289639",
    "end": "296440"
  },
  {
    "text": "were being requested by clients and not getting used we knew that a few of the causes one of them",
    "start": "296440",
    "end": "304759"
  },
  {
    "text": "were one of them was capacity focused autoscaling the cluster autoscaler that ships with kubernetes is focused on",
    "start": "304759",
    "end": "311520"
  },
  {
    "text": "reliability it's focused on capacity it's not really focused on cost uh we also knew that we had some",
    "start": "311520",
    "end": "319520"
  },
  {
    "text": "clients that as I mentioned were over requesting resources",
    "start": "319520",
    "end": "325520"
  },
  {
    "text": "so we started rolling out a few different measures these were platform PL problems so we",
    "start": "325520",
    "end": "332600"
  },
  {
    "text": "needed platform Solutions we started draining and",
    "start": "332600",
    "end": "339280"
  },
  {
    "text": "rebalancing nodes by force we looked for nodes that had less than that had more",
    "start": "339280",
    "end": "344800"
  },
  {
    "text": "than 30% discrepancy between CPU and memory and we forc drained them so that the scheduler could then place these",
    "start": "344800",
    "end": "351080"
  },
  {
    "text": "pods onto better better node types and in a more balanced",
    "start": "351080",
    "end": "357039"
  },
  {
    "text": "Fashion on we also started Ed draining low usage nodes looking for ones that",
    "start": "357319",
    "end": "362919"
  },
  {
    "text": "had less than 30% usage and forced draining them again so that the Schuler could place them onto better nodes or ES",
    "start": "362919",
    "end": "370520"
  },
  {
    "text": "especially existing ones we started rolling out multi-tier node architecture where we had memory",
    "start": "370520",
    "end": "377800"
  },
  {
    "text": "optimized nodes CPU optimized gpus spot instances and we wanted the scheduler to",
    "start": "377800",
    "end": "385120"
  },
  {
    "text": "be able to look at incoming workloads and place them onto the node type that best fits their resource",
    "start": "385120",
    "end": "391800"
  },
  {
    "text": "usage and we rolled out arc Arc stands for automatic resource configuration it",
    "start": "391800",
    "end": "398360"
  },
  {
    "text": "observes an application's resource usage and sets its CPU and memory requests",
    "start": "398360",
    "end": "403800"
  },
  {
    "text": "based on the 95th percentile of usage so as you can see here we've got two pods",
    "start": "403800",
    "end": "409560"
  },
  {
    "text": "with with requests that got decreased and one POD at the bottom that actually got increased because it turned out that",
    "start": "409560",
    "end": "415759"
  },
  {
    "text": "it wasn't requesting enough resources for what it was actually using",
    "start": "415759",
    "end": "421520"
  },
  {
    "text": "well we thought it was going well but the thing is a lot of the",
    "start": "422720",
    "end": "428720"
  },
  {
    "text": "scaled down nodes would just pop right back up again due to topology spreads and anti Infinity anti-affinity",
    "start": "428720",
    "end": "435400"
  },
  {
    "text": "configurations meaning that these pods that we thought could get packed onto the same node couldn't tolerate that so",
    "start": "435400",
    "end": "443680"
  },
  {
    "text": "they would not only pop back up again but they would scale up nearly empty nodes just to accommodate these scale",
    "start": "443680",
    "end": "449560"
  },
  {
    "text": "down pods some of our for strains were getting blocked",
    "start": "449560",
    "end": "455520"
  },
  {
    "text": "altogether we had pod disruption budgets that allowed few to no",
    "start": "455520",
    "end": "460720"
  },
  {
    "text": "disruptions which meant that our Force drains especially when we drained multiple nodes at one time would either",
    "start": "460720",
    "end": "467039"
  },
  {
    "text": "error out or time out sometimes we even had pdbs that should have been healthy",
    "start": "467039",
    "end": "472280"
  },
  {
    "text": "but sum are all of the pods were crashing forcing the the pdb into an",
    "start": "472280",
    "end": "477319"
  },
  {
    "text": "unhealthy State and making it undrainable these blocked drains stopped most of the",
    "start": "477319",
    "end": "482840"
  },
  {
    "text": "way through and created more low usage cording nodes that we call parked nodes",
    "start": "482840",
    "end": "489120"
  },
  {
    "text": "again driving up our empty space not taking it down and some of the node tiers that",
    "start": "489120",
    "end": "496919"
  },
  {
    "text": "were lower usage were just running nearly empty as an example the memory",
    "start": "496919",
    "end": "502240"
  },
  {
    "text": "optimized taint usually only had a couple pods on it at one time and our spot instances which again Were Meant to",
    "start": "502240",
    "end": "509039"
  },
  {
    "text": "save us money also ran nearly empty and brought up nearly empty nodes for one or",
    "start": "509039",
    "end": "514560"
  },
  {
    "text": "two Pods cost us extra instead of saving so",
    "start": "514560",
    "end": "522440"
  },
  {
    "text": "yikes here's where we landed after that we knew that we had underutilized node",
    "start": "523560",
    "end": "529000"
  },
  {
    "text": "types spot instances these certain tained nodes we were also starting to roll out cryo nodes that were",
    "start": "529000",
    "end": "535560"
  },
  {
    "text": "underutilized we we were still dealing with uneven capacity focused autoscaling",
    "start": "535560",
    "end": "541839"
  },
  {
    "text": "all of our manual intervention for that just didn't really do the job we knew",
    "start": "541839",
    "end": "547040"
  },
  {
    "text": "that we had unhealthy clients missing images crash looping pods that were",
    "start": "547040",
    "end": "552440"
  },
  {
    "text": "breaking pdbs and forcing things into an undrainable State and we had what we call Bad Neighbor clients these are",
    "start": "552440",
    "end": "559440"
  },
  {
    "text": "clients where their service was perfectly healthy it passed all its health checks it responded to requests",
    "start": "559440",
    "end": "565680"
  },
  {
    "text": "but it but these Services caused problems for other clients on our shared clusters they were these were clients",
    "start": "565680",
    "end": "573040"
  },
  {
    "text": "who were over requesting resources these were clients with anti-affinity configurations and pod topology spreads",
    "start": "573040",
    "end": "579560"
  },
  {
    "text": "that reduced bin packing efficiency these were blocking pdbs and these were",
    "start": "579560",
    "end": "584880"
  },
  {
    "text": "abandoned Services services that were going weeks months at a time without",
    "start": "584880",
    "end": "590120"
  },
  {
    "text": "getting touched so we knew that we had some platform",
    "start": "590120",
    "end": "596839"
  },
  {
    "text": "problems and also we were starting to discover these client-based bad neighbor issues so we thought you know client",
    "start": "596839",
    "end": "604640"
  },
  {
    "text": "problems need platform Solutions we tried a couple things we",
    "start": "604640",
    "end": "611160"
  },
  {
    "text": "started mutating blocking pod disruption budgets we looked for ones with this disruptions allowed equals zero or pdbs",
    "start": "611160",
    "end": "618720"
  },
  {
    "text": "in an undrainable state if there were no healthy pods so everything is in Crash",
    "start": "618720",
    "end": "624320"
  },
  {
    "text": "loop back off or some other failure State we just scale down the whole thing nobody's using it anyway and we'd mutate",
    "start": "624320",
    "end": "630480"
  },
  {
    "text": "the pdb to allow 10% disruptions for when it came back up if there were some healthy pods and",
    "start": "630480",
    "end": "637399"
  },
  {
    "text": "it was one of our certain platform managed services such as our namespace monitoring service we'd add up to 10%",
    "start": "637399",
    "end": "645079"
  },
  {
    "text": "allowed disruptions we also started scaling down",
    "start": "645079",
    "end": "650440"
  },
  {
    "text": "broken and abandoned Services as I mentioned if all the pods were in Crash loop back off kill it nobody's using it",
    "start": "650440",
    "end": "656480"
  },
  {
    "text": "anyway we also started scaling down stage services that had zero traffic for at least a",
    "start": "656480",
    "end": "663920"
  },
  {
    "text": "month again we thought it was going great not so",
    "start": "664519",
    "end": "671240"
  },
  {
    "text": "much see here's the problem before we were making platform changes that mostly",
    "start": "671240",
    "end": "676480"
  },
  {
    "text": "went unnoticed by our clients we were working in the background being pretty invisible but when we started actually",
    "start": "676480",
    "end": "682920"
  },
  {
    "text": "touching people's Services the volcano started erupting a",
    "start": "682920",
    "end": "687959"
  },
  {
    "text": "little bit we started off with straight up denial you're not allowed to change",
    "start": "687959",
    "end": "693560"
  },
  {
    "text": "this anger we never had to do this before we never would have migrated to your platform if we knew this was a",
    "start": "693560",
    "end": "700240"
  },
  {
    "text": "requirement bargaining we're swamped and we just don't have the CH the Cycles to make and accommodate these changes and",
    "start": "700240",
    "end": "708000"
  },
  {
    "text": "we started realizing we're creating a bad experience for our users that's not what a platform is supposed to do that's",
    "start": "708000",
    "end": "715560"
  },
  {
    "text": "not what we set out to do and so we hit that twice so we had a decision to make",
    "start": "715560",
    "end": "724360"
  },
  {
    "text": "do we force these changes through changing our current users experience out from under them and demand",
    "start": "724360",
    "end": "731480"
  },
  {
    "text": "acceptance or do we pull back reassess try something",
    "start": "731480",
    "end": "738320"
  },
  {
    "text": "else ultimately we decided to reassess and go back to the drawing",
    "start": "738320",
    "end": "744760"
  },
  {
    "text": "board here's where we are today we are rolling out Carpenter which",
    "start": "744760",
    "end": "750600"
  },
  {
    "text": "we are super excited about being able to scale with cost as a node selection",
    "start": "750600",
    "end": "756560"
  },
  {
    "text": "criteria instead of just uh capacity has been amazing we're already",
    "start": "756560",
    "end": "763000"
  },
  {
    "text": "seeing huge savings from it and better bin packing we found an issue not really an",
    "start": "763000",
    "end": "771760"
  },
  {
    "text": "issue more an oversight honestly the default value of Max pods that ships with kubernetes doesn't always cover all",
    "start": "771760",
    "end": "780160"
  },
  {
    "text": "of the space in modern node types we have some clusters that have many many",
    "start": "780160",
    "end": "786240"
  },
  {
    "text": "many tiny pods and these clusters were being weirdly underutilized and we",
    "start": "786240",
    "end": "791440"
  },
  {
    "text": "started realizing that that was because we had left Max Pods at its original value and never reexamined",
    "start": "791440",
    "end": "797519"
  },
  {
    "text": "it so we started slowly testing out increasing it can the control plane",
    "start": "797519",
    "end": "802800"
  },
  {
    "text": "handle all of these extra pods per node so far it's going great and we're seeing huge savings already",
    "start": "802800",
    "end": "811120"
  },
  {
    "text": "and on this on the client side of things we are working on client education",
    "start": "811399",
    "end": "816519"
  },
  {
    "text": "initiatives we want to start teaching our clients how to be good neighbors on",
    "start": "816519",
    "end": "822360"
  },
  {
    "text": "shared clusters we're also looking into ways to Monitor and expose these",
    "start": "822360",
    "end": "827880"
  },
  {
    "text": "undesired application behaviors such as blocking pdbs um and to clients and maintainers",
    "start": "827880",
    "end": "835120"
  },
  {
    "text": "because again we have clients who have intentional built their services around",
    "start": "835120",
    "end": "840279"
  },
  {
    "text": "these behaviors and they we they rely on them but we also want to inform them of",
    "start": "840279",
    "end": "847240"
  },
  {
    "text": "what these behaviors are doing to the rest of the shared clusters because they might just not know and we're discussing the best way",
    "start": "847240",
    "end": "856120"
  },
  {
    "text": "to handle client config policy changes in a way that won't create an experience like that for our users again what's the",
    "start": "856120",
    "end": "863199"
  },
  {
    "text": "best way to roll out these restrictions what do we do with the clients who rely on this bad neighbor Behavior if you",
    "start": "863199",
    "end": "869199"
  },
  {
    "text": "have a workload that is truly undisrupted how do you accommodate that on a shared",
    "start": "869199",
    "end": "875519"
  },
  {
    "text": "cluster and here's where we are today underused node types we've pretty much",
    "start": "877079",
    "end": "883199"
  },
  {
    "text": "got that one taken care of spot instances and these underused tainted",
    "start": "883199",
    "end": "888399"
  },
  {
    "text": "nodes have been mostly um mostly moved out of usage and",
    "start": "888399",
    "end": "894959"
  },
  {
    "text": "everything on our client on our clusters is using cryo now today so those are no longer underutilized they're just",
    "start": "894959",
    "end": "901480"
  },
  {
    "text": "utilized we're rolling out Carpenter and Max pods increases to uh better to",
    "start": "901480",
    "end": "909720"
  },
  {
    "text": "better accommodate our Auto scaling to increase our bin packing",
    "start": "909720",
    "end": "915800"
  },
  {
    "text": "efficiency we're still we're working on the best way to balance client experience with",
    "start": "915800",
    "end": "924320"
  },
  {
    "text": "efficiency when it comes to client Behavior so we're still discussing",
    "start": "924320",
    "end": "930199"
  },
  {
    "text": "client education initiatives uh monitoring all of this behavior all of that and Arc is doing",
    "start": "930199",
    "end": "938519"
  },
  {
    "text": "great with over requested resources obvious there are always changes that we want to make and new things are being",
    "start": "938519",
    "end": "945600"
  },
  {
    "text": "rolled out into the kubernetes ecosystem every day with things like vpa but arc's been doing a great job",
    "start": "945600",
    "end": "953560"
  },
  {
    "text": "we're really pleased with it",
    "start": "953560",
    "end": "957560"
  },
  {
    "text": "so where did we land after the rabbit hole and what did we",
    "start": "959079",
    "end": "967560"
  },
  {
    "text": "learn it's much easier to build an efficient platform from the ground up than to make it efficient",
    "start": "967680",
    "end": "973600"
  },
  {
    "text": "later we F we really wanted ethos to be reliable we wanted stability for our",
    "start": "973600",
    "end": "981560"
  },
  {
    "text": "clients and trying to introduce efficiency Concepts into it after the",
    "start": "981560",
    "end": "986759"
  },
  {
    "text": "fact after our clients had already gotten used to how our platform behaved",
    "start": "986759",
    "end": "992839"
  },
  {
    "text": "meant that we got a lot of push back and we ended up paying for resources that we might not have",
    "start": "992839",
    "end": "999240"
  },
  {
    "text": "needed cluster behaviors are more intertwined than is always apparent fixing one problem May reveal or even",
    "start": "1000040",
    "end": "1007360"
  },
  {
    "text": "cause two others when we tried scaling down underutilized nodes we didn't re we",
    "start": "1007360",
    "end": "1013560"
  },
  {
    "text": "thought that the problem was just the theer we didn't realize that the actual problem was pdbs and topy",
    "start": "1013560",
    "end": "1019560"
  },
  {
    "text": "spreads different tools can run head first into each other and cause these issues also when you get the chance",
    "start": "1019560",
    "end": "1027678"
  },
  {
    "text": "revisit your existing configs and figure out if any old approaches that may have been necessary when you first started",
    "start": "1027679",
    "end": "1033199"
  },
  {
    "text": "using kinetes are now less necessary and finally stability goes",
    "start": "1033199",
    "end": "1040558"
  },
  {
    "text": "beyond single application Health a healthy name space may still cause problems for other applications if they",
    "start": "1040559",
    "end": "1047400"
  },
  {
    "text": "share the same cluster space",
    "start": "1047400",
    "end": "1050840"
  },
  {
    "text": "and scale brings chaos The Wider your scope is the more you're going to have to handle the more freedom you give your",
    "start": "1053400",
    "end": "1060760"
  },
  {
    "text": "clients the more you rely on them to know how your platform Works make sure",
    "start": "1060760",
    "end": "1066160"
  },
  {
    "text": "they do because regardless of what you promise in your contract and what you",
    "start": "1066160",
    "end": "1072720"
  },
  {
    "text": "expect your clients to do all observable behaviors of your system will be depended on by some",
    "start": "1072720",
    "end": "1079559"
  },
  {
    "text": "somebody whether you want them to be or not and finally if you have a big thorny",
    "start": "1079559",
    "end": "1087600"
  },
  {
    "text": "seemingly unsolvable problem don't be afraid to take a few Sprints and go down",
    "start": "1087600",
    "end": "1094039"
  },
  {
    "text": "the rabbit hole thank",
    "start": "1094039",
    "end": "1098200"
  },
  {
    "text": "you uh I didn't leave the QR code on my slides but if anybody has any feedback",
    "start": "1103120",
    "end": "1108960"
  },
  {
    "text": "uh I would love it if you went into the uh schedule and just left me some",
    "start": "1108960",
    "end": "1114720"
  },
  {
    "text": "feedback and if anybody has any questions I would love to answer them I finished a bit early so we have",
    "start": "1114720",
    "end": "1119960"
  },
  {
    "text": "time uh so you spoke about CPU and me but with bin packing like kues doesn't",
    "start": "1119960",
    "end": "1126280"
  },
  {
    "text": "do a great job with network and disk so do you all restrict those",
    "start": "1126280",
    "end": "1131520"
  },
  {
    "text": "resources um sorry could you repeat the you repeat that uh do you like have a",
    "start": "1131520",
    "end": "1137799"
  },
  {
    "text": "way to segregate Network in disk for pods as you're bin packing them uh so no",
    "start": "1137799",
    "end": "1144240"
  },
  {
    "text": "uh not exactly our bin packing efforts mostly focused on CPU and memory um",
    "start": "1144240",
    "end": "1150159"
  },
  {
    "text": "networking uh we have started looking into better Network efficiency",
    "start": "1150159",
    "end": "1158000"
  },
  {
    "text": "but most of our bin packing efforts focused on CPU and",
    "start": "1158000",
    "end": "1163039"
  },
  {
    "text": "memory and I guess your clients might be writing",
    "start": "1163039",
    "end": "1168080"
  },
  {
    "text": "applications in in various programming languages right so with c groups what",
    "start": "1168080",
    "end": "1174960"
  },
  {
    "text": "I've realized is that not all programming languages respect cgroup limits and maybe some do better job than",
    "start": "1174960",
    "end": "1182000"
  },
  {
    "text": "the others so you need to like actually enforce that so do you do something in your configuration to like set",
    "start": "1182000",
    "end": "1187159"
  },
  {
    "text": "environment variables or like how how does that interaction work for example",
    "start": "1187159",
    "end": "1192200"
  },
  {
    "text": "go you need to use go Max procs and go limit and so when I talk about uh",
    "start": "1192200",
    "end": "1199280"
  },
  {
    "text": "Arc setting requests Arc doesn't touch limits um Arc only modifies requests and",
    "start": "1199280",
    "end": "1206400"
  },
  {
    "text": "we don't even set limits for some of our applications because of exactly what",
    "start": "1206400",
    "end": "1212440"
  },
  {
    "text": "you're saying um there are some applications that will burst far beyond the capacity that they're",
    "start": "1212440",
    "end": "1218720"
  },
  {
    "text": "allocated um so we don't handle we don't set uh",
    "start": "1218720",
    "end": "1227919"
  },
  {
    "text": "requests or limit differently based on what programming language an application is written in and just because we kind",
    "start": "1227919",
    "end": "1234640"
  },
  {
    "text": "of want to keep our platform unaware of that we want people",
    "start": "1234640",
    "end": "1241120"
  },
  {
    "text": "to be up able to upload a container and the platform doesn't care what it's written in it just runs",
    "start": "1241120",
    "end": "1248000"
  },
  {
    "text": "it",
    "start": "1248000",
    "end": "1251000"
  },
  {
    "text": "thanks hi hi so you briefly brought up the use of carpenter in your slides I'm",
    "start": "1253039",
    "end": "1258640"
  },
  {
    "text": "wondering if you all uh maybe perhaps have considered like the usage of like node pools and like basically dedicating",
    "start": "1258640",
    "end": "1265799"
  },
  {
    "text": "a node pool per uh I don't know billing team or or whatever you would like to",
    "start": "1265799",
    "end": "1271120"
  },
  {
    "text": "call it and then just like Shifting the cost or the onus of cost on application teams",
    "start": "1271120",
    "end": "1277600"
  },
  {
    "text": "themselves uh so we do have a pretty extensive chargeback system set up",
    "start": "1277600",
    "end": "1283679"
  },
  {
    "text": "already and that's been in place since long before we started rolling out Carpenter um so we do use node pools but",
    "start": "1283679",
    "end": "1290880"
  },
  {
    "text": "they don't necessarily factor into our chargeback okay thank",
    "start": "1290880",
    "end": "1296080"
  },
  {
    "text": "you at the beginning of the talk I think you mentioned um running clusters in a",
    "start": "1296320",
    "end": "1301520"
  },
  {
    "text": "mix of cloud environments as well as some on Prem environments um a lot of the like Auto scaling and and Bin",
    "start": "1301520",
    "end": "1308039"
  },
  {
    "text": "packing applies really really well to Cloud environments how does um how does",
    "start": "1308039",
    "end": "1313360"
  },
  {
    "text": "handling those on-prem environments change your strategy honestly uh most of our work",
    "start": "1313360",
    "end": "1320360"
  },
  {
    "text": "has been for cloud environments just because on Prem is so different and",
    "start": "1320360",
    "end": "1326600"
  },
  {
    "text": "autoscaling a lot of our Autos scaling tools are designed more for the cloud",
    "start": "1326600",
    "end": "1331960"
  },
  {
    "text": "so the answer is just not as much okay",
    "start": "1331960",
    "end": "1337080"
  },
  {
    "text": "thanks hey so we recently also encountered this kind of problems and we",
    "start": "1338279",
    "end": "1343799"
  },
  {
    "text": "also enrolled car Carpenter uh but our workload is mainly consist of BU for",
    "start": "1343799",
    "end": "1348919"
  },
  {
    "text": "thing I'm I'm so sorry I can't hear you could you step a little closer to the mic yeah H now it's better yes thank you so I said recently we have enrolled",
    "start": "1348919",
    "end": "1355799"
  },
  {
    "text": "Carpenter as well because of uh similar issues uh but our workload mainly consists of uh uh batch processing so uh",
    "start": "1355799",
    "end": "1365400"
  },
  {
    "text": "this still presents an issue because we can't disrupt it as much and also uh we",
    "start": "1365400",
    "end": "1370600"
  },
  {
    "text": "can't when you put High node High pod counts in the notes is it make disruption even more destructive so do",
    "start": "1370600",
    "end": "1377720"
  },
  {
    "text": "you have any advice to this kind of workload um honestly that's one of the things we're still working on because",
    "start": "1377720",
    "end": "1385000"
  },
  {
    "text": "although unbreakable pdbs are kind of the biggest face of undestructible",
    "start": "1385000",
    "end": "1390799"
  },
  {
    "text": "workloads they're not the only version of it um",
    "start": "1390799",
    "end": "1397799"
  },
  {
    "text": "and it really depends uh there are some clients where if they have batch processes they do better in uh dedicated",
    "start": "1398080",
    "end": "1407000"
  },
  {
    "text": "clusters um and for shared",
    "start": "1407000",
    "end": "1412039"
  },
  {
    "text": "clusters uh we have something called the shredder where if a node has been parked",
    "start": "1412039",
    "end": "1420679"
  },
  {
    "text": "like I mentioned uh partially drained and cordoned um it's given a timeout so",
    "start": "1420679",
    "end": "1427080"
  },
  {
    "text": "that pod then gets to sit on that node for up to a certain amount of time depending on what kind of cluster it is",
    "start": "1427080",
    "end": "1434039"
  },
  {
    "text": "prod gets more time stage gets less time so it's given a a more graceful",
    "start": "1434039",
    "end": "1440520"
  },
  {
    "text": "termination period um while nothing else is scheduled on it and the and the shredder checks in and takes it down",
    "start": "1440520",
    "end": "1448279"
  },
  {
    "text": "after that time period cool thank",
    "start": "1448279",
    "end": "1454000"
  },
  {
    "text": "you hi uh first of all you know great presentation Style thank you but I also",
    "start": "1454080",
    "end": "1459640"
  },
  {
    "text": "wanted to know a little bit more about um what would you what do you do or what do you suggest doing for uh non EK um",
    "start": "1459640",
    "end": "1466919"
  },
  {
    "text": "clouds right so so I'm not sure if your workloads are split across you know Google or Azure but Carpenters uh as I",
    "start": "1466919",
    "end": "1475440"
  },
  {
    "text": "understand pretty AWS specific so I was just curious what like what other Solutions you would have for other",
    "start": "1475440",
    "end": "1481760"
  },
  {
    "text": "clouds if if you do I believe Carpenter is being rolled out for Azure soon okay",
    "start": "1481760",
    "end": "1489399"
  },
  {
    "text": "okay good um and we don't run in Google unfortunately okay so uh it's really",
    "start": "1489399",
    "end": "1494799"
  },
  {
    "text": "just those two for us that's fair um and in terms of uh like client education um",
    "start": "1494799",
    "end": "1499840"
  },
  {
    "text": "what do you what would you recommend for like in terms of specifics there is it just making better documentation and",
    "start": "1499840",
    "end": "1506520"
  },
  {
    "text": "sort of presenting that to them is it you know hosting um like I said uh we're",
    "start": "1506520",
    "end": "1511960"
  },
  {
    "text": "having a lot of discussions about that we actually talked about that just at lunch earlier today my team and I uh and",
    "start": "1511960",
    "end": "1519200"
  },
  {
    "text": "so we've been throwing out a bunch of ideas of uh making regular blog posts um",
    "start": "1519200",
    "end": "1525440"
  },
  {
    "text": "having a playlist of brown bags where we we dive into various parts of our",
    "start": "1525440",
    "end": "1530840"
  },
  {
    "text": "environment and how it works um maybe coming up with like I said that dashboard that the idea behind the",
    "start": "1530840",
    "end": "1537600"
  },
  {
    "text": "dashboard would that be not just that it exposes these uh Bad Neighbor behaviors",
    "start": "1537600",
    "end": "1543559"
  },
  {
    "text": "but also that it would link to solutions for how for alternatives to them um and",
    "start": "1543559",
    "end": "1550440"
  },
  {
    "text": "also maybe explanations as to this is why your cost is going up gotcha all",
    "start": "1550440",
    "end": "1556360"
  },
  {
    "text": "right thanks hi thank you for the presentation it was",
    "start": "1556360",
    "end": "1561640"
  },
  {
    "text": "great um we have a problem where we run Java and that means that at container",
    "start": "1561640",
    "end": "1567520"
  },
  {
    "text": "startup they start using as much as CPU as they're pretty much allowed to one to",
    "start": "1567520",
    "end": "1572600"
  },
  {
    "text": "two cores but as soon as that stops uh normal traffic will probably use",
    "start": "1572600",
    "end": "1578320"
  },
  {
    "text": "somewhere like half of that let's say 500 M cores um have you run into similar",
    "start": "1578320",
    "end": "1583640"
  },
  {
    "text": "issues and how have you been dealing with it and basically which means unused resources because when you deploy you",
    "start": "1583640",
    "end": "1590679"
  },
  {
    "text": "want them to all start up reasonably quick so the cool thing about Arc is that it doesn't just modify resources on",
    "start": "1590679",
    "end": "1599279"
  },
  {
    "text": "Startup it modifi it is constantly examining uh resource usage for a pod",
    "start": "1599279",
    "end": "1606200"
  },
  {
    "text": "and on a 7-Day rolling basis I believe um I have my co-worker or some of my co-workers who wrote the thing in my",
    "start": "1606200",
    "end": "1612279"
  },
  {
    "text": "front row uh so it's like I said it examines the 95th percentile and is",
    "start": "1612279",
    "end": "1619880"
  },
  {
    "text": "constantly uh monitoring and updating the requests based on that 7-Day rolling",
    "start": "1619880",
    "end": "1625960"
  },
  {
    "text": "window of usage right uh when when you're starting to make the uh request change that",
    "start": "1625960",
    "end": "1632559"
  },
  {
    "text": "reboots the Pod right the requested the change I see what you mean oh that's a good that's a",
    "start": "1632559",
    "end": "1639159"
  },
  {
    "text": "good point C limits won't help because if the pod doesn't get enough CPU it",
    "start": "1639159",
    "end": "1645159"
  },
  {
    "text": "won't no yeah remove them but still over provisioning the node because it needs",
    "start": "1645159",
    "end": "1651039"
  },
  {
    "text": "the the resources to start up so if it doesn't start off fast enough then Readiness probes fail and",
    "start": "1651039",
    "end": "1657600"
  },
  {
    "text": "then our services are down that's our honestly that's a really good",
    "start": "1657600",
    "end": "1664799"
  },
  {
    "text": "question yes",
    "start": "1671360",
    "end": "1675360"
  },
  {
    "text": "exactly yeah that's exactly our problem okay yeah",
    "start": "1682360",
    "end": "1688200"
  },
  {
    "text": "unfortunately uh our",
    "start": "1688200",
    "end": "1694039"
  },
  {
    "text": "yeah yeah uh unfortunately a lot of our clients were able to just get around",
    "start": "1694039",
    "end": "1699960"
  },
  {
    "text": "this by using our previous not as good bin packing to grab all of that extra",
    "start": "1699960",
    "end": "1705480"
  },
  {
    "text": "space so was a bit of an invisible problem up until recent Rec thank you very",
    "start": "1705480",
    "end": "1711158"
  },
  {
    "text": "much hi Scott crimes uh with drw We have basically the exact same problems that",
    "start": "1711200",
    "end": "1717039"
  },
  {
    "text": "you guys do uh we've gone down the same rabbit holes uh we've got dashboards we've done customer education um I'm",
    "start": "1717039",
    "end": "1724000"
  },
  {
    "text": "very curious what your chargeback model is because I found that that is really the only thing that like puts a fire under people's butts to you know",
    "start": "1724000",
    "end": "1731039"
  },
  {
    "text": "actually go and and make changes to uh what those requested limits are or Implement some sort of system where it",
    "start": "1731039",
    "end": "1737720"
  },
  {
    "text": "can SC automatically um do you do like a straight utilization charge back do you charge like uh extra amounts based on",
    "start": "1737720",
    "end": "1745000"
  },
  {
    "text": "like reserved but unconsumed resources talk a little bit about that it's straight up based on resource",
    "start": "1745000",
    "end": "1752200"
  },
  {
    "text": "reservation um not just for CPU and memory but also for Network usage for",
    "start": "1752200",
    "end": "1758360"
  },
  {
    "text": "storage usage um and it's very much based on reservation because that's the",
    "start": "1758360",
    "end": "1764200"
  },
  {
    "text": "space that they take up in the cluster if they want to pay for Less then they need to reserve less and figure out how",
    "start": "1764200",
    "end": "1771000"
  },
  {
    "text": "to deal with that so it's not it's not utilization it's requests so it's sort of like you can pay for the whole",
    "start": "1771000",
    "end": "1776399"
  },
  {
    "text": "hamburger you don't have to eat it but you're still EXA pay for the whole thing okay thank you",
    "start": "1776399",
    "end": "1782960"
  },
  {
    "text": "welcome uh great questions everyone uh any other",
    "start": "1782960",
    "end": "1789640"
  },
  {
    "text": "questions got one hi uh one more from me sure um you said that uh sometimes you",
    "start": "1789640",
    "end": "1797240"
  },
  {
    "text": "decide uh that some workloads need to stay on dedicated clusters right uh how",
    "start": "1797240",
    "end": "1803200"
  },
  {
    "text": "do you decide that how do you move the workloads um so when I say that some",
    "start": "1803200",
    "end": "1809399"
  },
  {
    "text": "workloads are better that's not really a decision that our team makes that's usually a decision that we come to with",
    "start": "1809399",
    "end": "1815279"
  },
  {
    "text": "after discussion between the infrastructure team and the team that wants to run on the cluster because it's",
    "start": "1815279",
    "end": "1821519"
  },
  {
    "text": "not just um there are a lot of factors that go into deciding whether somebody on a",
    "start": "1821519",
    "end": "1828600"
  },
  {
    "text": "dedicated environment or not it's also to do with maintenance it's to do with their own cost profile it's to do with",
    "start": "1828600",
    "end": "1835360"
  },
  {
    "text": "the type of workload um and we have some clients who run massive applications on",
    "start": "1835360",
    "end": "1841919"
  },
  {
    "text": "our shared clusters because that's still the best environment for them and we have clients who run pretty small",
    "start": "1841919",
    "end": "1847760"
  },
  {
    "text": "applications in dedicated clusters because they absolutely need their own space so it really varies whe maybe they",
    "start": "1847760",
    "end": "1856880"
  },
  {
    "text": "have massive networking needs maybe they uh absolutely cannot have any other pods",
    "start": "1856880",
    "end": "1863799"
  },
  {
    "text": "running on their nodes at all maybe it's that's for security reasons uh it's",
    "start": "1863799",
    "end": "1869639"
  },
  {
    "text": "really a decision that we come to with each individual team thank",
    "start": "1869639",
    "end": "1876559"
  },
  {
    "text": "you anyone else oh yes hi um so two actually two",
    "start": "1877519",
    "end": "1883760"
  },
  {
    "text": "questions so I start with first question um in terms of uh Carpenter using spot",
    "start": "1883760",
    "end": "1889360"
  },
  {
    "text": "versus on demand and how do you how do you categorize what work close to be running on spot versus on demand so we",
    "start": "1889360",
    "end": "1896519"
  },
  {
    "text": "basically don't use spot anymore okay that answers the question yeah um we had so few people using it that it was",
    "start": "1896519",
    "end": "1902600"
  },
  {
    "text": "ending up costing us more just to bring up spot instances than to just put the workloads on to Shared nodes anyway got",
    "start": "1902600",
    "end": "1909240"
  },
  {
    "text": "it a second question is on part destruction budgets so I saw in the slide that um you would set if a",
    "start": "1909240",
    "end": "1916000"
  },
  {
    "text": "specific workload is running it doesn't have a part distruction budget set you would set to be like 10% based off your",
    "start": "1916000",
    "end": "1922240"
  },
  {
    "text": "so it's not that we put pdbs onto applications that didn't have them it's that uh for specific managed applica",
    "start": "1922240",
    "end": "1931840"
  },
  {
    "text": "applications that we the platform manage uh we would move them from zero disruptions allowed to 10% okay what",
    "start": "1931840",
    "end": "1938880"
  },
  {
    "text": "about a service team doesn't have a part distruction budget is it's abstraction tier as a platform team that we have to",
    "start": "1938880",
    "end": "1945840"
  },
  {
    "text": "provide to service teams but do you have have something that anal analyzes on the",
    "start": "1945840",
    "end": "1951320"
  },
  {
    "text": "workloads and decides what kind of part restriction budget to be set for this workload um so if a service does not",
    "start": "1951320",
    "end": "1958720"
  },
  {
    "text": "have a pod disruption budget then they are allowed to do that we do not encourage them to do that because pdbs",
    "start": "1958720",
    "end": "1966159"
  },
  {
    "text": "are an extremely valuable tool when used correctly but they are allowed to if they so choose we do not enforce that",
    "start": "1966159",
    "end": "1972480"
  },
  {
    "text": "for them um but in a state cluster how do you manage the reliability of a service running when you have",
    "start": "1972480",
    "end": "1978360"
  },
  {
    "text": "maintenance going on um oh uh I am being told that I have two minutes left so",
    "start": "1978360",
    "end": "1984559"
  },
  {
    "text": "this is the last question uh generally uh we tell them to put a pdb on their application we do not enforce that for",
    "start": "1984559",
    "end": "1990679"
  },
  {
    "text": "them but if they don't have it then that's their choice to make their application more",
    "start": "1990679",
    "end": "1998039"
  },
  {
    "text": "fragile all right uh if it is very quick I think I can do one last question very quick um is Arc open source uh no okay",
    "start": "1998039",
    "end": "2007399"
  },
  {
    "text": "sorry we'll think about",
    "start": "2007399",
    "end": "2012519"
  },
  {
    "text": "it thank you",
    "start": "2012519",
    "end": "2016720"
  }
]