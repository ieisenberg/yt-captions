[
  {
    "text": "good afternoon everybody and thank you so much for joining Rodrigo and myself this late on a Friday uh we realize",
    "start": "160",
    "end": "6400"
  },
  {
    "text": "we're one of the last things between you and your weekend so we really appreciate that you signed up for our three-hour",
    "start": "6400",
    "end": "11599"
  },
  {
    "text": "lecture on websockets i'm I'm kidding it's just it's just 30 minutes first allow me to",
    "start": "11599",
    "end": "18560"
  },
  {
    "text": "start with a little intro of the two people that's daring to address you right now my co-speaker Rodrigo he's the",
    "start": "18560",
    "end": "26000"
  },
  {
    "text": "reason we're here and the visionary behind Miro's evolution into cloudnative technologies our Kubernetes platform and",
    "start": "26000",
    "end": "32558"
  },
  {
    "text": "CNCF adoption aws has published case studies on his work and uh we use them as a",
    "start": "32559",
    "end": "39200"
  },
  {
    "text": "cheat as a cheat code at Miro since he's more effective than most LLMs my myself Andre I'm a newish S sur",
    "start": "39200",
    "end": "48160"
  },
  {
    "text": "compared to this titan next to me um I've joined I've I've dared to venture",
    "start": "48160",
    "end": "53360"
  },
  {
    "text": "into infrastructure and platforming after a decade of being a product engineer and I will still write",
    "start": "53360",
    "end": "58719"
  },
  {
    "text": "front-end code when in danger like any good story we'd like to",
    "start": "58719",
    "end": "64640"
  },
  {
    "text": "offer a little bit of history set the stage so to speak rodrigo is going to",
    "start": "64640",
    "end": "69760"
  },
  {
    "text": "walk us through a brief history of Mero our product our infrastructure where we",
    "start": "69760",
    "end": "75200"
  },
  {
    "text": "came from to where we are today we'll aim to illuminate why long live stateful connections are so important to our",
    "start": "75200",
    "end": "81880"
  },
  {
    "text": "product next we'll take a look at how we used a platform built on Kubernetes to",
    "start": "81880",
    "end": "86960"
  },
  {
    "text": "migrate Miro's websocket manager into Kubernetes from a stateful EC2 service to a stateless service on EKS reshaping",
    "start": "86960",
    "end": "95439"
  },
  {
    "text": "our edge routing for websocket connections reducing operational cost all while embracing cloudnative",
    "start": "95439",
    "end": "101960"
  },
  {
    "text": "technologies we're going to dive a bit on some of the mistakes we made the lessons we got so that nobody in this",
    "start": "101960",
    "end": "108880"
  },
  {
    "text": "room has to repeat them and finally we're going to brag a bit with the results and hopefully do all of this",
    "start": "108880",
    "end": "114880"
  },
  {
    "text": "fast enough so that we're out of here before midnight just kidding it's still 30 minutes uh thank you Andrea for the",
    "start": "114880",
    "end": "122240"
  },
  {
    "text": "sweet introduction and thank you all for making this very last CubeCon U 2025 uh",
    "start": "122240",
    "end": "127520"
  },
  {
    "text": "talk uh I would like to kick things off with a little bit of history about Mir's product architecture and infrastructure",
    "start": "127520",
    "end": "135520"
  },
  {
    "text": "uh these diagrams shows a simplified overview of one of our most important",
    "start": "135520",
    "end": "142080"
  },
  {
    "text": "parts of the product and this is how Miru was born a few years ago uh you can",
    "start": "142080",
    "end": "147280"
  },
  {
    "text": "imagine Miro as a gaming engine for enterprise collaboration uh currently",
    "start": "147280",
    "end": "153280"
  },
  {
    "text": "Andrea and I we are here uh in what we call the mirror board running this presentation but if we were to invite",
    "start": "153280",
    "end": "160400"
  },
  {
    "text": "you all into this boards in in your own devices our back end would be working",
    "start": "160400",
    "end": "166160"
  },
  {
    "text": "tirelessly uh handling real-time collision detection",
    "start": "166160",
    "end": "171200"
  },
  {
    "text": "locking and everything that makes uh real time collaboration and seamless",
    "start": "171200",
    "end": "176319"
  },
  {
    "text": "real-time collaboration possible at the heart of this engine sits our",
    "start": "176319",
    "end": "181680"
  },
  {
    "text": "board server a stateful service crucial for performance and low",
    "start": "181680",
    "end": "188599"
  },
  {
    "text": "latence every use every user on a board needs to be connected to the same board",
    "start": "188599",
    "end": "195360"
  },
  {
    "text": "server in our back end over a web over a websocket connection the stateful nature",
    "start": "195360",
    "end": "203159"
  },
  {
    "text": "of this service or this the stateful nature here introduces a significant",
    "start": "203159",
    "end": "209200"
  },
  {
    "text": "routing challenge how do clients know which server to connect",
    "start": "209200",
    "end": "215480"
  },
  {
    "text": "to historically we rely on Fabio B a opensource HTTP and TCP reverse",
    "start": "215480",
    "end": "223560"
  },
  {
    "text": "proxy integrated with hash corp console for service discovery when a board",
    "start": "223560",
    "end": "229680"
  },
  {
    "text": "server is started it integrate it registered itself with console and Fabio",
    "start": "229680",
    "end": "235360"
  },
  {
    "text": "dynamically update update its routing table with a pathbased",
    "start": "235360",
    "end": "241959"
  },
  {
    "text": "routing from the edge to from the cl from the client edge to a board server",
    "start": "241959",
    "end": "248799"
  },
  {
    "text": "in our uh in our infrastructure as you can see in this diagram the last part of",
    "start": "248799",
    "end": "254560"
  },
  {
    "text": "the URL literally represented the name of EC2",
    "start": "254560",
    "end": "260440"
  },
  {
    "text": "instance at at infrastructure side this pathbased routing while functional and",
    "start": "260440",
    "end": "267680"
  },
  {
    "text": "introduced additional challenge specifically clients would have to ren renegotiate with the backend server if",
    "start": "267680",
    "end": "275199"
  },
  {
    "text": "for example the existing the existing server that was rambling that that's uh",
    "start": "275199",
    "end": "280720"
  },
  {
    "text": "that board went down adding complexity to the connection management there was",
    "start": "280720",
    "end": "286639"
  },
  {
    "text": "also potential split brain issues that might arise on infrastructure side as",
    "start": "286639",
    "end": "293199"
  },
  {
    "text": "our internal board registering the application logic could go out of sync with the console service discovery one",
    "start": "293199",
    "end": "299759"
  },
  {
    "text": "on infrastructure side while we acknowledge that Fabio B and console has brought us a long way it was time for a",
    "start": "299759",
    "end": "307120"
  },
  {
    "text": "change but before we get there let me tell what",
    "start": "307120",
    "end": "313600"
  },
  {
    "text": "was happening in parallel at our organization as you can imagine our legacy architecture was not really cloud",
    "start": "313600",
    "end": "320280"
  },
  {
    "text": "native recognizing this back in 10 2021 we embarked in a transformative journey",
    "start": "320280",
    "end": "328000"
  },
  {
    "text": "rebuilding our compute platform from the ground up we committed to Kubernetes and",
    "start": "328000",
    "end": "333360"
  },
  {
    "text": "Amazon EKS as the next generation applications launching what we called",
    "start": "333360",
    "end": "339039"
  },
  {
    "text": "our compute platform powered by Kubernetes and enhanced by the best-in-class operators and controllers",
    "start": "339039",
    "end": "345440"
  },
  {
    "text": "this platform provided a featurerich environment for our developers to",
    "start": "345440",
    "end": "351600"
  },
  {
    "text": "rapidly build and iterate over new features primarily using a microser",
    "start": "351600",
    "end": "356759"
  },
  {
    "text": "architecture as you can see in this slide we adopted many technologies that we that we unfortunately won't have time",
    "start": "356759",
    "end": "363039"
  },
  {
    "text": "to talk here today for example Carter as our cluster of scaler on AWS and Kyvernu",
    "start": "363039",
    "end": "369199"
  },
  {
    "text": "as our dynamic admission controller sitting as a brain for every request",
    "start": "369199",
    "end": "374880"
  },
  {
    "text": "coming and going through the Kubernetes API server but back to our topic here fast",
    "start": "374880",
    "end": "380800"
  },
  {
    "text": "forward to 2024 it was time to extend the benefit of this compute platform to",
    "start": "380800",
    "end": "386800"
  },
  {
    "text": "our most critical workloads but let's start with an I one",
    "start": "386800",
    "end": "393039"
  },
  {
    "text": "here why if it end broke did we fix it canvas",
    "start": "393039",
    "end": "399000"
  },
  {
    "text": "2024 our mural launch event was coming up we wanted to show some really cool",
    "start": "399000",
    "end": "406319"
  },
  {
    "text": "new features of our product that would make new realtime collaborations",
    "start": "406319",
    "end": "412800"
  },
  {
    "text": "possible for example documents and data tables on top of our",
    "start": "412800",
    "end": "420240"
  },
  {
    "text": "intelligent uh canvas but clients would have variety",
    "start": "420240",
    "end": "427360"
  },
  {
    "text": "hard limits on the amount of simultaneous open connections to the same",
    "start": "427360",
    "end": "432599"
  },
  {
    "text": "domain mostly because we are talking about browsers here",
    "start": "432599",
    "end": "437840"
  },
  {
    "text": "breaking away from a monolithic design means clients should be able to open",
    "start": "437840",
    "end": "443639"
  },
  {
    "text": "websockets to any number of status workloads in our infrastructure or in",
    "start": "443639",
    "end": "448880"
  },
  {
    "text": "our back end and not all one all to one like we had",
    "start": "448880",
    "end": "454599"
  },
  {
    "text": "before this spawned the need for a smarter multiplexed websocket routing",
    "start": "454599",
    "end": "461680"
  },
  {
    "text": "several logic connections at the back end multiplexed over a single one a",
    "start": "461680",
    "end": "468880"
  },
  {
    "text": "single physical connection to the client now we were a collection of engineers from three different teams",
    "start": "468880",
    "end": "475599"
  },
  {
    "text": "that came together to create this piece while uh Rodrigo and I are the ones in front of you we're by no means the only",
    "start": "475599",
    "end": "482240"
  },
  {
    "text": "ones that deserve credit so firstly we had our cloud networking team they were",
    "start": "482240",
    "end": "487680"
  },
  {
    "text": "responsible for existing Fabio LB console and the load balancer configurations then we had the team",
    "start": "487680",
    "end": "493919"
  },
  {
    "text": "Rodrigo and I are part of called compute we were responsible for the EKS clusters the operators that power power our",
    "start": "493919",
    "end": "500960"
  },
  {
    "text": "microser platform and finally the brains behind our new smart websocket proxy and",
    "start": "500960",
    "end": "507680"
  },
  {
    "text": "our board servers our collaboration runtime team now I really hope this context that was driving the need for",
    "start": "507680",
    "end": "513919"
  },
  {
    "text": "change and the actors and the tools that we're wielding made a little bit of sense because it laid the important",
    "start": "513919",
    "end": "519440"
  },
  {
    "text": "groundwork for our descent into the depths of stateful connections in",
    "start": "519440",
    "end": "524680"
  },
  {
    "text": "Kubernetes and the scaling secrets nobody talks about but before we move",
    "start": "524680",
    "end": "529920"
  },
  {
    "text": "into our implementation let me unpack the game you are ch we are playing uh to",
    "start": "529920",
    "end": "535040"
  },
  {
    "text": "understand the challenge we face is important to grasp the fundamentals of stateful connections and to be honest",
    "start": "535040",
    "end": "543120"
  },
  {
    "text": "each one of the topics that I'm going to to cover now could be having could be",
    "start": "543120",
    "end": "548160"
  },
  {
    "text": "having a talk on its own and they were not always unique to stateful workloads",
    "start": "548160",
    "end": "554560"
  },
  {
    "text": "as they can also be important for stateless scenario but they will for",
    "start": "554560",
    "end": "560480"
  },
  {
    "text": "sure beat us much m much early on when long",
    "start": "560480",
    "end": "566480"
  },
  {
    "text": "connections are in place so please bear with me because we are going to get a little bit in the indep",
    "start": "566480",
    "end": "574320"
  },
  {
    "text": "uh the first thing before moving to any stateful uh",
    "start": "574320",
    "end": "579560"
  },
  {
    "text": "implementation uh we should actually understand the nature of the protocol we",
    "start": "579560",
    "end": "584880"
  },
  {
    "text": "are using each choose each choose case might differ and might require different",
    "start": "584880",
    "end": "591360"
  },
  {
    "text": "considerations regarding to the wire protocol use like for example when establishing a secure connection to a",
    "start": "591360",
    "end": "598880"
  },
  {
    "text": "database have you ever considered if the handshake process of this connection is",
    "start": "598880",
    "end": "605200"
  },
  {
    "text": "identical to the one for let's say done by a web browser while both might be",
    "start": "605200",
    "end": "611600"
  },
  {
    "text": "using TLS the underlying protocol and connection management can introduce nuances in our",
    "start": "611600",
    "end": "618880"
  },
  {
    "text": "case since we are using websockets and that's pretty much what we are enforced to use on a browser",
    "start": "618880",
    "end": "626079"
  },
  {
    "text": "uh which in the case of websockets is an extension of the HTTP 1.1 protocol the",
    "start": "626079",
    "end": "632079"
  },
  {
    "text": "communication flow would go first the over the process of establishing an encrypted and secure HTP connection",
    "start": "632079",
    "end": "639279"
  },
  {
    "text": "between the client and the server and then only after that we'll be able to",
    "start": "639279",
    "end": "644640"
  },
  {
    "text": "upgrade this connection to a websocket one meaning that we have two handshakes",
    "start": "644640",
    "end": "650000"
  },
  {
    "text": "in this process one for the TLS part and the other for the HTTP upgrade and as I",
    "start": "650000",
    "end": "656560"
  },
  {
    "text": "said only after that the birectional message that are being exchanged between the clients and the server will be able",
    "start": "656560",
    "end": "663279"
  },
  {
    "text": "to be sent and received which makes at quering new connections even more",
    "start": "663279",
    "end": "668640"
  },
  {
    "text": "expensive in our scen in our scenario increasing resource consumption and tail",
    "start": "668640",
    "end": "674399"
  },
  {
    "text": "latence when overload this leads us to the next topic",
    "start": "674399",
    "end": "680320"
  },
  {
    "text": "related to keep our lives unfortunately keep our lives are a necessary evil that",
    "start": "680320",
    "end": "685760"
  },
  {
    "text": "we must embrace here but while doing so we need to be careful",
    "start": "685760",
    "end": "691519"
  },
  {
    "text": "and proper configure the idle timeouts between each hop in the communication",
    "start": "691519",
    "end": "696600"
  },
  {
    "text": "flow which may might sounds really uh already well known right but so we need",
    "start": "696600",
    "end": "704240"
  },
  {
    "text": "to make sure that each part in this communication flow uh we reuse a",
    "start": "704240",
    "end": "709680"
  },
  {
    "text": "connection that is never actually closed by the other party upstream",
    "start": "709680",
    "end": "715600"
  },
  {
    "text": "finally in the in in the Linux level we encounter infemoral ports these are the temporary ports",
    "start": "715600",
    "end": "723040"
  },
  {
    "text": "assigned by the operational system for client connections while essential for",
    "start": "723040",
    "end": "728920"
  },
  {
    "text": "communication they come with inherited limits for instance the upper limits of",
    "start": "728920",
    "end": "734160"
  },
  {
    "text": "a given connection tle which is composed by the source IP and the destination IP",
    "start": "734160",
    "end": "740000"
  },
  {
    "text": "and port is a bit over 65,000 in Linux the range of available",
    "start": "740000",
    "end": "746639"
  },
  {
    "text": "ephemeral parts is defined by a namespaced CCTL config and when running",
    "start": "746639",
    "end": "752800"
  },
  {
    "text": "it in a Kubernetes pod it's configured by default at around",
    "start": "752800",
    "end": "759720"
  },
  {
    "text": "28K as you can see here you can also increase and turn that out based on your",
    "start": "759720",
    "end": "764800"
  },
  {
    "text": "necessities by changing and configuring it under the security context of your pod spec but certainly back to the",
    "start": "764800",
    "end": "773200"
  },
  {
    "text": "ephemeral port limits if this range is exhausted new connections can't be",
    "start": "773200",
    "end": "778279"
  },
  {
    "text": "established leading to connection failures that can manifest as drop",
    "start": "778279",
    "end": "783920"
  },
  {
    "text": "requests and poor and frustrating user experience therefore we must account for",
    "start": "783920",
    "end": "790160"
  },
  {
    "text": "those port limits and design for scaling our applications proper original",
    "start": "790160",
    "end": "796000"
  },
  {
    "text": "scaling of components is crucial here sometimes deploying smaller instances",
    "start": "796000",
    "end": "801120"
  },
  {
    "text": "can be more effective than relying solely on vertical scaling no matter how",
    "start": "801120",
    "end": "806320"
  },
  {
    "text": "much you invest in vertical scaling the ephemeral ports limits will always be there",
    "start": "806320",
    "end": "812480"
  },
  {
    "text": "now alongside ephemeral ports we also encounter another crucial Linux",
    "start": "812480",
    "end": "818160"
  },
  {
    "text": "component to consider contracts or connection tracking which is a core",
    "start": "818160",
    "end": "823600"
  },
  {
    "text": "kernel Linux kernel feature that maintains a table of all active network",
    "start": "823600",
    "end": "829480"
  },
  {
    "text": "connections this table is essential for stateful fires network adders translation and other network functions",
    "start": "829480",
    "end": "837519"
  },
  {
    "text": "however the contract also has a size when this table becomes full new",
    "start": "837519",
    "end": "843120"
  },
  {
    "text": "connections attempts will be silently dropped impacting application availability and loading and leading to",
    "start": "843120",
    "end": "849680"
  },
  {
    "text": "difficult to diagnose issues and once again it's important to know those",
    "start": "849680",
    "end": "856079"
  },
  {
    "text": "limitations and proper size and proper account for for example for node sharing",
    "start": "856079",
    "end": "862639"
  },
  {
    "text": "and neighborhoods on your nodes which will lead for distributing the proper load as",
    "start": "862639",
    "end": "868519"
  },
  {
    "text": "needed now Rodrigo has drawn circles around the constraints we're operating under with websockets new connections",
    "start": "868519",
    "end": "875519"
  },
  {
    "text": "are expensive the timing between the hops needs to be tuned and there are",
    "start": "875519",
    "end": "880560"
  },
  {
    "text": "hard limits that prevents us from scaling vertically indefinitely given this context let's",
    "start": "880560",
    "end": "887279"
  },
  {
    "text": "drill into how the first nerves were connected we'll be taking a look at our new in-houseuilt websocket manager how",
    "start": "887279",
    "end": "895839"
  },
  {
    "text": "we routed the traffic there how we kept the traffic secure for those sweet enterprise deals",
    "start": "895839",
    "end": "903680"
  },
  {
    "text": "what we have here is an illustration of our system that will help to highlight the components as we talk through them",
    "start": "903680",
    "end": "909440"
  },
  {
    "text": "and the problems they solved for us so let's start with the star of our show the real time collaboration command",
    "start": "909440",
    "end": "916399"
  },
  {
    "text": "gateway or RTC gateway our replacement for Fabio LB and console now the work",
    "start": "916399",
    "end": "922560"
  },
  {
    "text": "the team did deserves a talk all on its own but a couple of highlights about this application it uses threads equal",
    "start": "922560",
    "end": "929440"
  },
  {
    "text": "to the number of vCPUs to reduce context switching additionally all inbound and",
    "start": "929440",
    "end": "935920"
  },
  {
    "text": "outbound connections are handled in the same thread which significantly increases performance they also swapped",
    "start": "935920",
    "end": "942720"
  },
  {
    "text": "out the default memory allocator for J Malik uh which helps prevents memory fragmentation that comes from needing to",
    "start": "942720",
    "end": "949360"
  },
  {
    "text": "handle packet sizes of wildly diffing frequencies",
    "start": "949360",
    "end": "955040"
  },
  {
    "text": "um now that we had our deployment there how do we get traffic from our users devices onto our shiny new RTC gateway",
    "start": "955040",
    "end": "962800"
  },
  {
    "text": "pods the AWS load balancer controller swoops in to take all the glory here it",
    "start": "962800",
    "end": "968959"
  },
  {
    "text": "has native support for Kubernetes APIs like ingress and services and also which",
    "start": "968959",
    "end": "974320"
  },
  {
    "text": "is what we used in the end a custom resource definition called the target group binding that allows us to",
    "start": "974320",
    "end": "979600"
  },
  {
    "text": "configure how the ALB sends traffic to our Kubernetes bods for those unfamiliar",
    "start": "979600",
    "end": "984639"
  },
  {
    "text": "with what a target group is or the AWS concepts it can be summed up as a resource that allows us to configure how",
    "start": "984639",
    "end": "991040"
  },
  {
    "text": "the load balancing and and traffic protocols work to the pods",
    "start": "991040",
    "end": "997440"
  },
  {
    "text": "next now we had our pods deployed we had the traffic flowing in how do we ensure all of that data is encrypted in transit",
    "start": "997440",
    "end": "1005519"
  },
  {
    "text": "while preventing our S sur from exiting the building via windows or the fire escape because they have to rotate",
    "start": "1005519",
    "end": "1012000"
  },
  {
    "text": "thousands of certificates frequently here we have manager that",
    "start": "1012000",
    "end": "1017759"
  },
  {
    "text": "popped up another CNCF controller that is invaluable it allows us to abstract away the majority of the work of",
    "start": "1017759",
    "end": "1025280"
  },
  {
    "text": "maintaining the PKIs for our cluster in this case we could provision certificates automatically that is valid",
    "start": "1025280",
    "end": "1031839"
  },
  {
    "text": "for a year they are rotated monthly and that combined with our uh node lifetime",
    "start": "1031839",
    "end": "1036880"
  },
  {
    "text": "of max node lifetime of 30 days meant every pod had a crisp certificate when it came alive",
    "start": "1036880",
    "end": "1045160"
  },
  {
    "text": "now if you remember from the previous diagram Rodrigo showed we moved from a",
    "start": "1045160",
    "end": "1050480"
  },
  {
    "text": "stateful service in EC2 to a stateless service in EKS and one of the first",
    "start": "1050480",
    "end": "1055520"
  },
  {
    "text": "important things that we need to think about is how are we going to handle our shutdowns how do we gracefully drain",
    "start": "1055520",
    "end": "1062760"
  },
  {
    "text": "connections so we're going to be zooming into the configuration between the ALB",
    "start": "1062760",
    "end": "1068240"
  },
  {
    "text": "target group and our pod shutdown life cycle but first of all what does graceful shutdown look like for our new",
    "start": "1068240",
    "end": "1074720"
  },
  {
    "text": "RTC gateway ports if we try to bring it back to the user experience we don't want users to go into loops with",
    "start": "1074720",
    "end": "1081440"
  },
  {
    "text": "stuttering connections where the connections are opened and closed frequently while we are scaling down or",
    "start": "1081440",
    "end": "1087840"
  },
  {
    "text": "doing rolling deployments and the RTC gateway did exactly this from the application side",
    "start": "1087840",
    "end": "1094960"
  },
  {
    "text": "by implementing a protocol where they could send close events to all connected clients that would then transparently to",
    "start": "1094960",
    "end": "1101840"
  },
  {
    "text": "the user acquire new connections u to existing pots so first we made sure",
    "start": "1101840",
    "end": "1107760"
  },
  {
    "text": "the ALB gave plenty of time for the RTC gateway to do its thing we figured it",
    "start": "1107760",
    "end": "1113600"
  },
  {
    "text": "should be able to drain all existing connections in about 2 minutes so we doubled that as our dregistration delay",
    "start": "1113600",
    "end": "1120000"
  },
  {
    "text": "on the ALB next we didn't want these closed",
    "start": "1120000",
    "end": "1125520"
  },
  {
    "text": "connections to open to an ex to the exact same pod again so using our",
    "start": "1125520",
    "end": "1130960"
  },
  {
    "text": "pre-top hook we ensured the pod was no longer in the load balancing pool before",
    "start": "1130960",
    "end": "1136320"
  },
  {
    "text": "we started our connection draining sequences now that our pods are",
    "start": "1136320",
    "end": "1142400"
  },
  {
    "text": "terminating smoothly we could start preparing for our expected load now Rodrigo has pointed out previously with",
    "start": "1142400",
    "end": "1149120"
  },
  {
    "text": "limitations like the port the ephemeral ports and contracting um how important it is to lean towards horizontal",
    "start": "1149120",
    "end": "1156000"
  },
  {
    "text": "autoscaling instead of vert vertical autoscaling and",
    "start": "1156000",
    "end": "1161240"
  },
  {
    "text": "traditionally scaling based on resource usage works really well for us in about 80% of our cases um however since this",
    "start": "1161240",
    "end": "1170799"
  },
  {
    "text": "RTC gateway handles connections with varying packet sizes and frequencies",
    "start": "1170799",
    "end": "1175919"
  },
  {
    "text": "imagine a workshop like this where everybody is on the same board versus one person designing a spaceship in his",
    "start": "1175919",
    "end": "1182919"
  },
  {
    "text": "house resource-based scaling might alone might not have been",
    "start": "1182919",
    "end": "1188200"
  },
  {
    "text": "sufficient luckily another way to manage saturation effectively for a proxy is to",
    "start": "1188200",
    "end": "1193600"
  },
  {
    "text": "limit the amount of concurrent active connections per port the team then ran extensive performance",
    "start": "1193600",
    "end": "1200640"
  },
  {
    "text": "testing to understand the resource ratio profiles here like think how much CPU",
    "start": "1200640",
    "end": "1205840"
  },
  {
    "text": "and RAM we need for how many open connections they were then optimized in",
    "start": "1205840",
    "end": "1211360"
  },
  {
    "text": "size to handle around 8,000 connections per pod while they could peak well past that uh without seeing any degraded",
    "start": "1211360",
    "end": "1218840"
  },
  {
    "text": "performance enter Kada the operator that allowed us to scale based on the amount",
    "start": "1218840",
    "end": "1224640"
  },
  {
    "text": "of open connections now here we can see an example of what a kada scaled object",
    "start": "1224640",
    "end": "1230159"
  },
  {
    "text": "looks like um also how we configured it we optimized for scaling up quickly",
    "start": "1230159",
    "end": "1236000"
  },
  {
    "text": "notice there's no limit on the up search scaling down slowly at max one pot every",
    "start": "1236000",
    "end": "1241440"
  },
  {
    "text": "5 minutes also a rolling window sample to prevent scaling down prematurely",
    "start": "1241440",
    "end": "1246480"
  },
  {
    "text": "during metric dips and a cool down period of 5 minutes to prevent flapping now all of this you may say comes",
    "start": "1246480",
    "end": "1253200"
  },
  {
    "text": "standard with the Kubernetes HPA what Kada gives us it allows us to scale on",
    "start": "1253200",
    "end": "1258880"
  },
  {
    "text": "custom metrics like we have there under our triggers in our case active connections with which allowed us to",
    "start": "1258880",
    "end": "1264559"
  },
  {
    "text": "keep our deployment well under saturation levels once we had our graceful",
    "start": "1264559",
    "end": "1271480"
  },
  {
    "text": "shutdowns and autoscaling configured the next logical step was was load balancing",
    "start": "1271480",
    "end": "1277760"
  },
  {
    "text": "yes how do we make sure every pod was pulling its weight and not sitting there yawning while all the others were",
    "start": "1277760",
    "end": "1283360"
  },
  {
    "text": "sweating we're considering the interaction and levers available again between our ALB",
    "start": "1283360",
    "end": "1290240"
  },
  {
    "text": "and the RTC gateway and the two viable contending load balancing algorithms for",
    "start": "1290240",
    "end": "1296480"
  },
  {
    "text": "websockets that we considered were round robin the classic and least active",
    "start": "1296480",
    "end": "1301760"
  },
  {
    "text": "connections after doing research going into my cave",
    "start": "1301760",
    "end": "1308400"
  },
  {
    "text": "coming back out and feeling like I had a great understanding of how they'll behave with websockets I proudly",
    "start": "1308400",
    "end": "1314240"
  },
  {
    "text": "suggested to the team why we should use least active connections because they unlike round robin they prevented the",
    "start": "1314240",
    "end": "1320480"
  },
  {
    "text": "old pods from being overloaded can anybody guess what went",
    "start": "1320480",
    "end": "1327200"
  },
  {
    "text": "wrong cold starts angry ones like I made this graph intentionally",
    "start": "1327799",
    "end": "1334799"
  },
  {
    "text": "vague so the people in the back can't see my shame but we were flooding the new pods with a barrage of websocket",
    "start": "1334799",
    "end": "1341640"
  },
  {
    "text": "connections i did not anticipate all the CPUbound work like the TLS handshakes",
    "start": "1341640",
    "end": "1347200"
  },
  {
    "text": "that we needed to do on mass as well as the latency spikes that we'd need to endure the risk to the customer's",
    "start": "1347200",
    "end": "1353280"
  },
  {
    "text": "experience was not acceptable here so back to the lab we went we needed to reanalyze both options",
    "start": "1353280",
    "end": "1360320"
  },
  {
    "text": "roundroin and least active connections to see how we could mitigate the drawbacks of either to mitigate the cold",
    "start": "1360320",
    "end": "1367520"
  },
  {
    "text": "starts of least active connections we could decrease the connection rate but this isn't simple because slow start",
    "start": "1367520",
    "end": "1373919"
  },
  {
    "text": "doesn't work with uh least active connections and another thing we could try was to warm the application but this",
    "start": "1373919",
    "end": "1381200"
  },
  {
    "text": "was also not very simple because it would need us to figure out which code sites are responsible and which",
    "start": "1381200",
    "end": "1386640"
  },
  {
    "text": "connections may need to be pre-opened yeah pretty hard so we turned our gaze back to round",
    "start": "1386640",
    "end": "1394200"
  },
  {
    "text": "robin how do we stop the old pods from being overloaded now if we take a look",
    "start": "1394200",
    "end": "1399840"
  },
  {
    "text": "at these two players we realize that a single readiness endpoint re usually",
    "start": "1399840",
    "end": "1405200"
  },
  {
    "text": "works really well for normal HTTP traffic however the behaviors are quite",
    "start": "1405200",
    "end": "1410799"
  },
  {
    "text": "different and it becomes very visible with websockets if we're focusing on an old",
    "start": "1410799",
    "end": "1416400"
  },
  {
    "text": "pod on an old pod when we fail readiness two things happened in our current",
    "start": "1416400",
    "end": "1423000"
  },
  {
    "text": "state the ALB probe failed which meant no new connections were opened to this",
    "start": "1423000",
    "end": "1429600"
  },
  {
    "text": "pod since it was saturated that was good however if the Kubernetes probe failed",
    "start": "1429600",
    "end": "1437280"
  },
  {
    "text": "the the pod would be removed from the service it would be removed from the endpoint slices the AWS load balancer",
    "start": "1437280",
    "end": "1443440"
  },
  {
    "text": "controller would observe these changes and start the draining sequence on the load balancer the pod would eventually",
    "start": "1443440",
    "end": "1450720"
  },
  {
    "text": "exceed the graceful draining period and all existing uh connections would be",
    "start": "1450720",
    "end": "1456480"
  },
  {
    "text": "terminated and since Rodrigo pointed out establishing new connections are",
    "start": "1456480",
    "end": "1461760"
  },
  {
    "text": "expensive and the user experience here is bad of losing the connections again this was",
    "start": "1461760",
    "end": "1468600"
  },
  {
    "text": "bad so we split out another endpoint specially for the target group so that",
    "start": "1468600",
    "end": "1474960"
  },
  {
    "text": "we could individually signal how we handle new connections and existing ones the team then updated the RTC gateway to",
    "start": "1474960",
    "end": "1482559"
  },
  {
    "text": "report not ready to the to the target group when it went above 10,000 connections and ready again when below",
    "start": "1482559",
    "end": "1489880"
  },
  {
    "text": "9,000 while continuously telling Kubernetes that it is ready so we don't",
    "start": "1489880",
    "end": "1495039"
  },
  {
    "text": "touch those existing connections success no more cold starts load",
    "start": "1495039",
    "end": "1504360"
  },
  {
    "text": "balanced and to finish our journey here related to HPA uh I would like to guide",
    "start": "1504360",
    "end": "1510000"
  },
  {
    "text": "you through uh uh through two interesting uh experiences we have with it the first one is related to the",
    "start": "1510000",
    "end": "1517120"
  },
  {
    "text": "Kubernetes HPA algorithm itself uh the algorithm has a built-in",
    "start": "1517120",
    "end": "1522880"
  },
  {
    "text": "tolerance uh which is by default defined as 10%",
    "start": "1522880",
    "end": "1528000"
  },
  {
    "text": "and in our case as we are using Amazon ES we cannot really control or change that because we don't have you don't",
    "start": "1528000",
    "end": "1534799"
  },
  {
    "text": "have access to the cube controller manager settings uh and perhaps in many cases uh this is",
    "start": "1534799",
    "end": "1542400"
  },
  {
    "text": "not really important but when we are talking about scaling based on open connections and talk about saturation",
    "start": "1542400",
    "end": "1549520"
  },
  {
    "text": "and avoid uh things go wrong in that scenario that becomes a big deal uh so",
    "start": "1549520",
    "end": "1555120"
  },
  {
    "text": "in our case the developers really want to scale based on the defined thresholds or at least have a predictable",
    "start": "1555120",
    "end": "1561440"
  },
  {
    "text": "understanding of where what that threshold will be uh so in this case",
    "start": "1561440",
    "end": "1567200"
  },
  {
    "text": "it's pretty much about understanding uh understanding this tolerance and accounting for that when defining your",
    "start": "1567200",
    "end": "1573520"
  },
  {
    "text": "threshold in this cable objects uh and the second issue here uh shows how",
    "start": "1573520",
    "end": "1578880"
  },
  {
    "text": "sensitive the HPA uh can be regarding to metrics that would sharply flap and why",
    "start": "1578880",
    "end": "1585360"
  },
  {
    "text": "you should actually define scale scale up behaviors on your scale object or in your HPA itself as Andre pointed out in",
    "start": "1585360",
    "end": "1593360"
  },
  {
    "text": "the beginning when he showed uh the uh the scaled object uh definition we",
    "start": "1593360",
    "end": "1599279"
  },
  {
    "text": "didn't have a scale up policy in the beginning uh and interesting enough",
    "start": "1599279",
    "end": "1604880"
  },
  {
    "text": "there was a talk back on Wednesday from one of the maintainers and they were discussing best practics in terms of",
    "start": "1604880",
    "end": "1611919"
  },
  {
    "text": "scaling in in Kubernetes and their first good practice advised there was to use",
    "start": "1611919",
    "end": "1617679"
  },
  {
    "text": "scale uh scale up policies which we didn't do at that time uh but specific",
    "start": "1617679",
    "end": "1624320"
  },
  {
    "text": "about our pro problem here uh after a little bit of B blame gaming between uh",
    "start": "1624320",
    "end": "1631600"
  },
  {
    "text": "our observability uh components and KA itself went through the code of the cube",
    "start": "1631600",
    "end": "1637120"
  },
  {
    "text": "controller manager uh the code for the KDA uh Prometheus uh scaler and also",
    "start": "1637120",
    "end": "1644320"
  },
  {
    "text": "adding a proxy between KDA and um and our Prometheus read end point we uh",
    "start": "1644320",
    "end": "1650720"
  },
  {
    "text": "actually uh find out and were able to narrow down that our monitoring system was actually misbehaving here and giving",
    "start": "1650720",
    "end": "1657360"
  },
  {
    "text": "away wrong data points for the query even though if you were quering that historically in graphs or or in",
    "start": "1657360",
    "end": "1664080"
  },
  {
    "text": "dashboards later on that misbehaving uh misleading information was not there",
    "start": "1664080",
    "end": "1670080"
  },
  {
    "text": "anymore uh but this is just brings even more visibility into how uh the HPA",
    "start": "1670080",
    "end": "1676399"
  },
  {
    "text": "system would blindly and happily follow that misbehaving uh data uh and then the",
    "start": "1676399",
    "end": "1682799"
  },
  {
    "text": "solution is pretty simple here uh it's pretty much about defining uh scale up policies uh which by default it comes",
    "start": "1682799",
    "end": "1689919"
  },
  {
    "text": "with 100% of scaling alone 100% of scaling up that's why uh we got the",
    "start": "1689919",
    "end": "1696559"
  },
  {
    "text": "double of of of of pods in a given uh scale scale event and here we mitigate",
    "start": "1696559",
    "end": "1702960"
  },
  {
    "text": "that by controlling and and and reducing the amount of pods that could be uh",
    "start": "1702960",
    "end": "1708159"
  },
  {
    "text": "could be created in a given minute uh time",
    "start": "1708159",
    "end": "1713240"
  },
  {
    "text": "frame finally now we've rushed through this",
    "start": "1713240",
    "end": "1718559"
  },
  {
    "text": "journey on how we moved our websocket manager into Kubernetes we've highlighted some of the operators that",
    "start": "1718559",
    "end": "1723919"
  },
  {
    "text": "made us possible as well as some of the scaling and balancing errors we I mean the secrets we found let's take a look",
    "start": "1723919",
    "end": "1731600"
  },
  {
    "text": "at our results here we have active connections per pod over a day in one of our regions",
    "start": "1731600",
    "end": "1739600"
  },
  {
    "text": "there are three events I would like to draw your attention to the yellow line marks when reinforcement starts being",
    "start": "1739600",
    "end": "1746320"
  },
  {
    "text": "called in when our averages starts getting around 8.8K the red line marks",
    "start": "1746320",
    "end": "1752240"
  },
  {
    "text": "when the older pod stops reporting ready to the target group the blue line marks",
    "start": "1752240",
    "end": "1757440"
  },
  {
    "text": "when they start reporting ready again when this happens we can see how the",
    "start": "1757440",
    "end": "1762640"
  },
  {
    "text": "rate of new connection sharply increases on the newer pods but by now they're",
    "start": "1762640",
    "end": "1767840"
  },
  {
    "text": "ready warm and basically shouting taunts at the ALB",
    "start": "1767840",
    "end": "1773200"
  },
  {
    "text": "our in initial connection latency went down 10x allowing us to keep it down",
    "start": "1773200",
    "end": "1778640"
  },
  {
    "text": "keep it real real time and also moving from the stateful",
    "start": "1778640",
    "end": "1785440"
  },
  {
    "text": "setup on EC2 to a stateless setup on EKS allowed us to save around 40k per year",
    "start": "1785440",
    "end": "1791200"
  },
  {
    "text": "which made our PHOPS team uh smile and wink at us and now that I've rounded out",
    "start": "1791200",
    "end": "1796240"
  },
  {
    "text": "the results I'd like to hand back to Rodrigo for a sneak peek into Miro's next steps in our cloud platform and the",
    "start": "1796240",
    "end": "1802159"
  },
  {
    "text": "theory goodbye thank you goodbye",
    "start": "1802159",
    "end": "1807170"
  },
  {
    "text": "[Applause]",
    "start": "1807170",
    "end": "1814519"
  },
  {
    "text": "well here uh just to tell you that uh we are happy to share that after a few",
    "start": "1816840",
    "end": "1822000"
  },
  {
    "text": "years running containerized envir containerized workloads in production on top of Kubernetes we are ready as an",
    "start": "1822000",
    "end": "1828399"
  },
  {
    "text": "organization to take one giant step forward and for this specific part of",
    "start": "1828399",
    "end": "1833520"
  },
  {
    "text": "architecture here covered here today it means that we are also moving our board servers to kubernetes uh which is a",
    "start": "1833520",
    "end": "1840799"
  },
  {
    "text": "direction that we are having as an organization in terms of compute and cloud uh consolidation uh and by the way if you",
    "start": "1840799",
    "end": "1847760"
  },
  {
    "text": "are ever uh find yourself wondering by Amsterdam please come by to our office we will be happy to host you there uh",
    "start": "1847760",
    "end": "1854240"
  },
  {
    "text": "and now goodbye thank you yeah thank you",
    "start": "1854240",
    "end": "1859840"
  }
]