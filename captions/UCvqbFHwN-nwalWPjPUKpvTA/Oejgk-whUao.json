[
  {
    "text": "so good afternoon everyone I know this session has been planned just after the lunch hours but",
    "start": "120",
    "end": "6879"
  },
  {
    "text": "we promise we'll try to make it interesting till the end it's truly an honor to be here at cubec con India this",
    "start": "6879",
    "end": "13799"
  },
  {
    "text": "is going to be my first talk at such a huge event and to be in the same room amongst you know such an inspiring",
    "start": "13799",
    "end": "20160"
  },
  {
    "text": "community of innovators technologists and open source enthusiasts is truly overwhelming my name is shiwani and I",
    "start": "20160",
    "end": "27039"
  },
  {
    "text": "welcome you all to this session of extensive scalability testing on Argo for 30,000 plus applications at Expedia",
    "start": "27039",
    "end": "34960"
  },
  {
    "text": "group today we are excited to take you through our journey which is a story of challenges Solutions and of course",
    "start": "34960",
    "end": "42120"
  },
  {
    "text": "Innovations we hope to share some valuable insights with you all some lessons learned and some good practices",
    "start": "42120",
    "end": "48160"
  },
  {
    "text": "that you guys can take back to your own teams and your own projects thank you again for joining us and we look forward",
    "start": "48160",
    "end": "54239"
  },
  {
    "text": "to an engaging session with you before we go into the technical Deep dive about",
    "start": "54239",
    "end": "59559"
  },
  {
    "text": "our scalability testing initiative let's first get ourselves introduced hi again my name is Shani mutra I'm working as a",
    "start": "59559",
    "end": "66880"
  },
  {
    "text": "software development engineer at Expedia group it's been 1 and a half years for me now at uh EG and a total of 5 years",
    "start": "66880",
    "end": "75240"
  },
  {
    "text": "for me in the industry I'm accompanied by an extremely Cod uh very talented",
    "start": "75240",
    "end": "80400"
  },
  {
    "text": "speaker uh moit hello everyone my name is moit and I've been working with coof",
    "start": "80400",
    "end": "86280"
  },
  {
    "text": "Forge for the last 7 years uh and I've been associated with Expedia group for",
    "start": "86280",
    "end": "91600"
  },
  {
    "text": "the last 1.5 years and yes we are very much excited to present a talk on a very",
    "start": "91600",
    "end": "96799"
  },
  {
    "text": "significant initiative of ours before such a huge audience so yeah that's me",
    "start": "96799",
    "end": "101920"
  },
  {
    "text": "over to you Shivani uh that's true moit this is significant not only because our",
    "start": "101920",
    "end": "107159"
  },
  {
    "text": "team worked on this but because it benefits us and it could benefit your organizations as well in many many ways",
    "start": "107159",
    "end": "114159"
  },
  {
    "text": "uh we are excited to walk you through our journey and share some key insights with you all while uh that we gained",
    "start": "114159",
    "end": "119920"
  },
  {
    "text": "while building and scaling our platform so we'll start by discussing the migration Journey from CA to aocd and",
    "start": "119920",
    "end": "126320"
  },
  {
    "text": "the challenges we overcame during this process next we'll delve into the design and implementation of the scalability",
    "start": "126320",
    "end": "133480"
  },
  {
    "text": "testing platform that we created uh which ensures our platform's performance",
    "start": "133480",
    "end": "138599"
  },
  {
    "text": "at scale here we'll also highlight the ways our work benefits the broader ecosystem uh we'll also discuss the",
    "start": "138599",
    "end": "145959"
  },
  {
    "text": "scalability testing strengths uh how this scalability testing strengthens our platform's resilience and helps us",
    "start": "145959",
    "end": "152720"
  },
  {
    "text": "achieve High availability we'll also provide insights into the fine-tuned",
    "start": "152720",
    "end": "157760"
  },
  {
    "text": "configurations that were optimal for our setup finally we'll open the floor to",
    "start": "157760",
    "end": "162920"
  },
  {
    "text": "your questions and we look forward to have an engaging session we hope this session provides some practical insights",
    "start": "162920",
    "end": "169120"
  },
  {
    "text": "and Sparks some great ideas for your own Journey so let's get started with an",
    "start": "169120",
    "end": "174200"
  },
  {
    "text": "overview of our journey from migrating from Cube fair to aosd before going into that that let me",
    "start": "174200",
    "end": "180440"
  },
  {
    "text": "first introduce you to RCP RCP is runtime compute platform runtime is the",
    "start": "180440",
    "end": "186239"
  },
  {
    "text": "name of our team as well and RCP is an in-house built platform at Expedia Group which is a multi-tenant communities",
    "start": "186239",
    "end": "192799"
  },
  {
    "text": "based container orchestration platform RCP operates at an incredible scale ranging from hundreds to thousands of",
    "start": "192799",
    "end": "199799"
  },
  {
    "text": "clusters um this scale highlights rcps ability to serve as a reliable and efficient backbone for our business",
    "start": "199799",
    "end": "206319"
  },
  {
    "text": "critical Services uh as you can see on the slides here's a glimpse of some of the tools that we use uh in our setup so",
    "start": "206319",
    "end": "213519"
  },
  {
    "text": "yes the migration part originally RCP leveraged the cfed architecture to manage the Clusters and their",
    "start": "213519",
    "end": "220159"
  },
  {
    "text": "applications it was the suitable choice at that given point of time but however uh when CF got deprecated we it became",
    "start": "220159",
    "end": "228480"
  },
  {
    "text": "clear for us that we had to adopt a more uh sustainable modern and a feature-rich solution to meet our platforms evolving",
    "start": "228480",
    "end": "235720"
  },
  {
    "text": "demands and that's why uh this led us to AO CD a robust giops tool that allows us",
    "start": "235720",
    "end": "241519"
  },
  {
    "text": "to manage our kuber resources declaratively Argo provides capabilities like M multicluster application",
    "start": "241519",
    "end": "248200"
  },
  {
    "text": "synchronization automated roll backs Version Control integration and many more which actually fit our needs so the",
    "start": "248200",
    "end": "255079"
  },
  {
    "text": "primary motivation behind the scalability testing actually was rooted in this migration because with this",
    "start": "255079",
    "end": "260759"
  },
  {
    "text": "transition it became crucial for us to understand uh our how our platform would",
    "start": "260759",
    "end": "265919"
  },
  {
    "text": "behave under varing workloads and uh certain different configurations so our major Focus was to test workload",
    "start": "265919",
    "end": "272759"
  },
  {
    "text": "distribution across the platform to ensure even and efficient utilization we",
    "start": "272759",
    "end": "278080"
  },
  {
    "text": "also wanted to experiment with certain different Argo CD tunable parameters to identify the most optimal settings for",
    "start": "278080",
    "end": "284400"
  },
  {
    "text": "our setup we wanted to prepare a platform for you know future workload demands ultimately this exercise wasn't",
    "start": "284400",
    "end": "292600"
  },
  {
    "text": "just about migrating to Argo CD it was to understand and proactively validate that Argo CD could meet our uh current",
    "start": "292600",
    "end": "299960"
  },
  {
    "text": "platforms and future requirements let's take an overview of how we have set up our platform for",
    "start": "299960",
    "end": "307039"
  },
  {
    "text": "performing extensive scalability testing so if you look at the slides we created one argd control plane that acted as the",
    "start": "307039",
    "end": "314280"
  },
  {
    "text": "master attached to this target uh attached to this control plane is an artifactory which actually can uh which",
    "start": "314280",
    "end": "322479"
  },
  {
    "text": "actually serves as a central hub for storing our application Helm charts so it became quite easy and it be it",
    "start": "322479",
    "end": "328560"
  },
  {
    "text": "allowed uh seamless pulling off application hel charts during the deployments to also streamline the",
    "start": "328560",
    "end": "333600"
  },
  {
    "text": "deployment process we wrote an Automation in go so uh how this script helped us this script enabled Dynamic",
    "start": "333600",
    "end": "341120"
  },
  {
    "text": "deployment of applications to the Target clusters that were attached to this control plane it also provided the",
    "start": "341120",
    "end": "347080"
  },
  {
    "text": "flexibility to define the number and the types of applications we wanted to deploy on the uh Target clusters this",
    "start": "347080",
    "end": "353440"
  },
  {
    "text": "setup allowed us to test Argo CD's scalability under real world conditions ensuring its reliability and and",
    "start": "353440",
    "end": "359720"
  },
  {
    "text": "maintaining the performance when managing a large number of applications across multiple clusters so you must be",
    "start": "359720",
    "end": "365560"
  },
  {
    "text": "wondering we have a lot of Target clusters we have V clusters as well so yes our control plane was actually",
    "start": "365560",
    "end": "371759"
  },
  {
    "text": "connected to certain number of Target clusters and each Target cluster was hosting a number of virtual clusters to",
    "start": "371759",
    "end": "378520"
  },
  {
    "text": "highlight the previous diagram back again so yeah some key points is that we",
    "start": "378520",
    "end": "383800"
  },
  {
    "text": "created one rocd control plane that was the master we attached 10 Target clusters to this control plane and each",
    "start": "383800",
    "end": "391800"
  },
  {
    "text": "of the target cluster was actually hosting 11 virtual clusters which we spin up using the V cluster Tool uh I'm",
    "start": "391800",
    "end": "398720"
  },
  {
    "text": "sure most of you might be aware of it but still uh the V clusters allows us to mimic real world cluster setups it",
    "start": "398720",
    "end": "405680"
  },
  {
    "text": "enables to analyze scalability across diverse configurations uh we also created a",
    "start": "405680",
    "end": "411039"
  },
  {
    "text": "custom Automation in automation Ino which actually was creating application",
    "start": "411039",
    "end": "416120"
  },
  {
    "text": "sets on the control plane so now this application set was responsible for",
    "start": "416120",
    "end": "421440"
  },
  {
    "text": "creating applications on all the target clusters that were attached to this master and the application sets were",
    "start": "421440",
    "end": "429280"
  },
  {
    "text": "creating nothing but simple Helm based applications so another important thing to highlight so we created three",
    "start": "429280",
    "end": "435919"
  },
  {
    "text": "different size of uh sizes of applications ranging from small mid and large so we wanted to cater some real",
    "start": "435919",
    "end": "443360"
  },
  {
    "text": "world uh you know production applications where maybe some of your applications might be running heavy",
    "start": "443360",
    "end": "448560"
  },
  {
    "text": "workloads and and some might be some simple kues resources I'm sorry so yes",
    "start": "448560",
    "end": "454759"
  },
  {
    "text": "uh small applications were actually creating somewhere around 10 communities resources middle-sized applications were",
    "start": "454759",
    "end": "460199"
  },
  {
    "text": "creating 15 resources and large scale applications were actually creating 25 plus communities resources this could",
    "start": "460199",
    "end": "466720"
  },
  {
    "text": "include communities jobs and crds",
    "start": "466720",
    "end": "471599"
  },
  {
    "text": "Etc quickly let's take a look at how we can benefit our ecosystem so our journey to adopting",
    "start": "472080",
    "end": "478720"
  },
  {
    "text": "gitops at Expedia group uh was not just about switching the tools it was about",
    "start": "478720",
    "end": "483840"
  },
  {
    "text": "making sure that our new system could handle our large scale needs so yes with",
    "start": "483840",
    "end": "489840"
  },
  {
    "text": "this scalability testing it revealed key tunable parameters of the argd controller and several other components",
    "start": "489840",
    "end": "496159"
  },
  {
    "text": "enabling efficient management of a huge number of applications before migrating them to",
    "start": "496159",
    "end": "501360"
  },
  {
    "text": "production we conducted scalability testing on three times the number of applications we currently have in our",
    "start": "501360",
    "end": "507560"
  },
  {
    "text": "production environment so this ured that we are prepared for future load increases utilizing V clusters uh yes we",
    "start": "507560",
    "end": "516279"
  },
  {
    "text": "optimized costs associated with this scalability testing this approach showcased how we can effectively manage",
    "start": "516279",
    "end": "523200"
  },
  {
    "text": "the resources and offer a cost efficient model which other organizations can also",
    "start": "523200",
    "end": "528640"
  },
  {
    "text": "adopt through this talk for an example uh we are sharing our scalability testing Journey which can help the",
    "start": "528640",
    "end": "535640"
  },
  {
    "text": "community understand the Practical aspects of scaling argocd and and this contribution is invaluable for teams who",
    "start": "535640",
    "end": "542160"
  },
  {
    "text": "are seeking to improve their giops practices or to implement the same we conducted these tests using",
    "start": "542160",
    "end": "549160"
  },
  {
    "text": "applications that are identical to those that we have in our production environment so yes uh we tried to",
    "start": "549160",
    "end": "554640"
  },
  {
    "text": "simulate the real world production applications enough with the theories uh here comes the exciting part now where",
    "start": "554640",
    "end": "561120"
  },
  {
    "text": "we'll get started with discussing how we have tested and what kind of test cases we designed so handing over to Mohit",
    "start": "561120",
    "end": "569440"
  },
  {
    "text": "thank you Shivani thank you for the brief introduction so yeah let's get started",
    "start": "569440",
    "end": "574640"
  },
  {
    "text": "with the scalability design so yes now we will go through the whole scalability testing design the",
    "start": "574640",
    "end": "581600"
  },
  {
    "text": "test cases that we has designed that are more towards the Expedia group application distribution we wanted to",
    "start": "581600",
    "end": "587760"
  },
  {
    "text": "design our test case to keep in mind the number of volume of applications according to our production envirment so",
    "start": "587760",
    "end": "594760"
  },
  {
    "text": "we started with some Argo DET table parameters like sharding algorithm shards server controller settings status",
    "start": "594760",
    "end": "601079"
  },
  {
    "text": "processor operation processor and yes with some testable parameters like CPU and memory of application controller",
    "start": "601079",
    "end": "607079"
  },
  {
    "text": "repos server controller and yes we wanted to know the exact value of these parameters which will be suitable for",
    "start": "607079",
    "end": "612399"
  },
  {
    "text": "our environment before migrating our production workload to Argo so we started with our scalability",
    "start": "612399",
    "end": "619519"
  },
  {
    "text": "test cases with the default values of Argo components and we performed several iterations of these testing until",
    "start": "619519",
    "end": "626640"
  },
  {
    "text": "we until we found the optimal settings for Fargo city which was suitable for our",
    "start": "626640",
    "end": "632440"
  },
  {
    "text": "environment further I'll be discussing about four test cases here including deployment of 13,000 applications",
    "start": "632440",
    "end": "639079"
  },
  {
    "text": "progressively all at once and deploying 30,000 applications in a similar Manner",
    "start": "639079",
    "end": "645120"
  },
  {
    "text": "and yes final test case will be more inclined toward",
    "start": "645120",
    "end": "650120"
  },
  {
    "text": "resiliency so yeah this was the first test case which we conducted during our scalability testings we have deployed",
    "start": "650680",
    "end": "657560"
  },
  {
    "text": "approximately 120 mediumsized of applications across 110 virtual Target",
    "start": "657560",
    "end": "664760"
  },
  {
    "text": "clusters these applications were deployed progressively where we have deployed 1100 applications with a 2.5",
    "start": "664760",
    "end": "671600"
  },
  {
    "text": "minute interval spread over 12 iterations as you can see in the right side where we have mentioned about the",
    "start": "671600",
    "end": "678639"
  },
  {
    "text": "distribution of the applications so yes let's take a take a",
    "start": "678639",
    "end": "685480"
  },
  {
    "text": "look at the parameters which we have chosen for the scalability and their relevance on scalability repo server parallelism",
    "start": "685480",
    "end": "692200"
  },
  {
    "text": "limit we started with a parallelism limit of 250 in the repo server that determines how many how many manifest",
    "start": "692200",
    "end": "698920"
  },
  {
    "text": "Generations can run concurrently we have kept sharts since we are testing with the 100 of 10 of",
    "start": "698920",
    "end": "704880"
  },
  {
    "text": "Target cluster so that each charts has a 10 to 11 virtual Target clusters to",
    "start": "704880",
    "end": "711360"
  },
  {
    "text": "manage status processor and operation processor by default Argo CD has a",
    "start": "711360",
    "end": "717040"
  },
  {
    "text": "status processor and operation processor values set to 20 and 10 respectively status processor uses a",
    "start": "717040",
    "end": "724360"
  },
  {
    "text": "queue to manage application status updates the controller status processor settings controls the number of threads",
    "start": "724360",
    "end": "730959"
  },
  {
    "text": "that checks and update the status of applications similarly we have operation processor settings that controls the",
    "start": "730959",
    "end": "737959"
  },
  {
    "text": "number of threat used to execute operation like syncing roll backing and pruning resources this test case has been",
    "start": "737959",
    "end": "744320"
  },
  {
    "text": "performed in three iterations where we have changed status processor and operation processor value in each",
    "start": "744320",
    "end": "749639"
  },
  {
    "text": "iteration where in this in the first iteration we have capped 12200 in the second iteration of this test we have",
    "start": "749639",
    "end": "755440"
  },
  {
    "text": "capped 2300 and in the third iteration we have kept 3420 now the final settings which we",
    "start": "755440",
    "end": "762720"
  },
  {
    "text": "have client QPS and client burst these settings control the rate of API request",
    "start": "762720",
    "end": "768399"
  },
  {
    "text": "that Argo CD sends to the kubernetes API server and the values which we have considered here are 50 sl00 in the first",
    "start": "768399",
    "end": "776839"
  },
  {
    "text": "scalability test case so yeah here the here are the",
    "start": "776839",
    "end": "783560"
  },
  {
    "text": "observations which we have concluded from some of the important metric of Argo CD controller for the first test",
    "start": "783560",
    "end": "789920"
  },
  {
    "text": "case the stateus processor and operation processor was set 200200 in the second 2300 and the third",
    "start": "789920",
    "end": "797880"
  },
  {
    "text": "3420 so the total time to sync 13,000 applications was similar almost similar",
    "start": "797880",
    "end": "804839"
  },
  {
    "text": "all three configurations averaging around 40 minutes when we compared CPU",
    "start": "804839",
    "end": "809880"
  },
  {
    "text": "metrics for all the three iteration of this test there was nothing major observed in the first two iteration but",
    "start": "809880",
    "end": "816320"
  },
  {
    "text": "in the third iteration of the test we some see some spike in CPU for one of",
    "start": "816320",
    "end": "821399"
  },
  {
    "text": "The Shard reaching it CPU limit so that was the oper observation which we have",
    "start": "821399",
    "end": "827160"
  },
  {
    "text": "observed in CPU the another important metrix which we have included in our scalability",
    "start": "827160",
    "end": "833160"
  },
  {
    "text": "testing was app reconcillation q and operation processing Q app reconcillation Q a queue that handles",
    "start": "833160",
    "end": "840519"
  },
  {
    "text": "the periodic reconcillation of application to ensure the life State matches the desired State similarly we",
    "start": "840519",
    "end": "846839"
  },
  {
    "text": "have app operation processing queue a queue that handles user triggered or automated operational applications such",
    "start": "846839",
    "end": "853199"
  },
  {
    "text": "as syncing roll backing or pruning of sources during the application deployment in both the metrics where",
    "start": "853199",
    "end": "859920"
  },
  {
    "text": "some of the sharts piked for a very short span of time but that got cleared without any noticeable impact on the",
    "start": "859920",
    "end": "866079"
  },
  {
    "text": "performance of the application controller so yeah that was the operation observation which we have",
    "start": "866079",
    "end": "872120"
  },
  {
    "text": "observed in app reconation que and operation processing queue so the overall observation which",
    "start": "872120",
    "end": "879720"
  },
  {
    "text": "we have concluded from this test in the third iteration where both the status processor and operation processors were",
    "start": "879720",
    "end": "886680"
  },
  {
    "text": "higher we observed a significant reduction in same time across all to",
    "start": "886680",
    "end": "892839"
  },
  {
    "text": "iterations we also did some resiliency testing in this test as well where we",
    "start": "892839",
    "end": "898680"
  },
  {
    "text": "have restarted application controller and repo server however there were no",
    "start": "898680",
    "end": "904399"
  },
  {
    "text": "significant changes in the sync process all the health status of the application showing that Argo CD is resilient to",
    "start": "904399",
    "end": "911120"
  },
  {
    "text": "these types of disruption so the conclusion which we have observed from this testing that it",
    "start": "911120",
    "end": "916959"
  },
  {
    "text": "help us to identify the optimal configuration of the Argo CD controller we have implemented the settings in",
    "start": "916959",
    "end": "922839"
  },
  {
    "text": "subsequent test cases to further evaluate the performance of Argo scalability so yeah that was the first",
    "start": "922839",
    "end": "929000"
  },
  {
    "text": "test case and we have observed our optimal settings till now moving to the",
    "start": "929000",
    "end": "934399"
  },
  {
    "text": "next test case in this scalability testing we have",
    "start": "934399",
    "end": "939680"
  },
  {
    "text": "deployed mixed set of applications mixed set of applications means that we have included small size of application",
    "start": "939680",
    "end": "945319"
  },
  {
    "text": "medium size of applications and the large size of application which Shani has already told you about that depends",
    "start": "945319",
    "end": "951920"
  },
  {
    "text": "upon the number of resources so the key factor in this test",
    "start": "951920",
    "end": "957519"
  },
  {
    "text": "was the application distribution as in Argo CD each chart manage its own set of",
    "start": "957519",
    "end": "963279"
  },
  {
    "text": "Target clusters the performance and the resource consumption of each sharts are directly impacted by the number and the",
    "start": "963279",
    "end": "968560"
  },
  {
    "text": "type of the applications on its Target clusters the distribution was designed",
    "start": "968560",
    "end": "974279"
  },
  {
    "text": "to observe the behavior of individual sharts under different application",
    "start": "974279",
    "end": "979639"
  },
  {
    "text": "loads so you must be wondering what has been changed because the application distribution in both the two iteration",
    "start": "979639",
    "end": "985519"
  },
  {
    "text": "of this test we have kept same so yeah status processor and operation processor",
    "start": "985519",
    "end": "992440"
  },
  {
    "text": "we have kept at 420 and 300 which has been dried from the last test case where we have seen better performance",
    "start": "992440",
    "end": "998920"
  },
  {
    "text": "reconcillation time we have kept at 2 40 second and now in this test case we have",
    "start": "998920",
    "end": "1004920"
  },
  {
    "text": "change client QPS in the first iteration and then in second iteration we have",
    "start": "1004920",
    "end": "1010199"
  },
  {
    "text": "kept at200 we wanted to check if there has been effect of client QPS and C",
    "start": "1010199",
    "end": "1015279"
  },
  {
    "text": "burst on the performance of the argd controller so",
    "start": "1015279",
    "end": "1021000"
  },
  {
    "text": "yes the observation which we have concluded from this test that the first",
    "start": "1021000",
    "end": "1026240"
  },
  {
    "text": "in the first iteration the total time to sync 12,000 application was around 24 minutes in the second iteration the",
    "start": "1026240",
    "end": "1031600"
  },
  {
    "text": "total time to sync application was 21 minutes which was slightly faster than the first iteration where we have kept",
    "start": "1031600",
    "end": "1038079"
  },
  {
    "text": "uh client QPS and client bu second 100 and 200 When comparing the graph of the",
    "start": "1038079",
    "end": "1043880"
  },
  {
    "text": "CPU metrix in both the iteration of the test we have seen",
    "start": "1043880",
    "end": "1049440"
  },
  {
    "text": "some CPU spikes in a specific shot three in the second iteration however this was due to the",
    "start": "1049440",
    "end": "1056559"
  },
  {
    "text": "fact that the target cluster with the highest number of application were assigned to this chart resulting in The",
    "start": "1056559",
    "end": "1061640"
  },
  {
    "text": "observed CPU spikes and that was expected another important metrix which",
    "start": "1061640",
    "end": "1068360"
  },
  {
    "text": "we have observed app reconcillation Q and A operation processor Q from this test however no significant Behavior was",
    "start": "1068360",
    "end": "1075240"
  },
  {
    "text": "observed in the application reconciliation queue or the operation processing que however a small",
    "start": "1075240",
    "end": "1081679"
  },
  {
    "text": "q did form in the second iteration of app operation processing que but that was cleared without any noticeable",
    "start": "1081679",
    "end": "1087840"
  },
  {
    "text": "impact on the performance of application controller moving to the conclusion for",
    "start": "1087840",
    "end": "1094480"
  },
  {
    "text": "this test the pass sync time achieved during testing was 21 minutes with the client",
    "start": "1094480",
    "end": "1100360"
  },
  {
    "text": "QPS set to 200 and QPS set to 200 we also did some resiliency testing as well",
    "start": "1100360",
    "end": "1106720"
  },
  {
    "text": "in this case by deleting application controller ports however these action",
    "start": "1106720",
    "end": "1111919"
  },
  {
    "text": "did not cause any noticeable impact on the sharding or the cluster resource distribution in this test we have",
    "start": "1111919",
    "end": "1118760"
  },
  {
    "text": "included applications with jobs as part of their resources which were created and recreated during each reconciliation",
    "start": "1118760",
    "end": "1126039"
  },
  {
    "text": "cycle however this behavior is already addressed by the AR CD using hooks so",
    "start": "1126039",
    "end": "1131200"
  },
  {
    "text": "there were no major concerns as part of this testing we did",
    "start": "1131200",
    "end": "1136400"
  },
  {
    "text": "another testing by redeploying all 12,000 application by changing Port labels invalidating the redish cache and",
    "start": "1136400",
    "end": "1143840"
  },
  {
    "text": "recycling of the running ports of the application controller and the repo server we observed that the work depth",
    "start": "1143840",
    "end": "1150280"
  },
  {
    "text": "increased for all sharts and there was a noticeable rise in CPU and memory consumption compared to the when",
    "start": "1150280",
    "end": "1156760"
  },
  {
    "text": "application were initially deployed so yeah that was the observation which we have observed",
    "start": "1156760",
    "end": "1163280"
  },
  {
    "text": "during this testing now here comes a third test where we have deployed mixed set of",
    "start": "1163280",
    "end": "1169840"
  },
  {
    "text": "application by increasing our loads three times this test has been performed",
    "start": "1169840",
    "end": "1175280"
  },
  {
    "text": "in two iterations where distribution of application is different in the first",
    "start": "1175280",
    "end": "1180520"
  },
  {
    "text": "iteration we have deployed 330 mix site mix set of application on each 110",
    "start": "1180520",
    "end": "1187760"
  },
  {
    "text": "virtual clusters resulting in 36,000 applications the sharding algorithm we",
    "start": "1187760",
    "end": "1192880"
  },
  {
    "text": "have used in the first iteration was round robin in the second iteration we have",
    "start": "1192880",
    "end": "1199400"
  },
  {
    "text": "deployed 165 mixed set of application in 100 of the Clusters as you can see in the second",
    "start": "1199400",
    "end": "1206039"
  },
  {
    "text": "diagram uh that has been deployed in each of 100 Target clusters",
    "start": "1206039",
    "end": "1212840"
  },
  {
    "text": "but but for some of the few Target cluster we have deployed some high number of applications where we have",
    "start": "1212840",
    "end": "1220159"
  },
  {
    "text": "deployed very large applications like as you can see in the table we have deployed 1200 2400 2,000 applications on",
    "start": "1220159",
    "end": "1229120"
  },
  {
    "text": "a single virtual Target clusters so we simulated a scenario",
    "start": "1229120",
    "end": "1234600"
  },
  {
    "text": "similar to our production workload where some Target clusters are running a high number of applications so here in the",
    "start": "1234600",
    "end": "1241520"
  },
  {
    "text": "second iterations we have deployed around 32,000 applications we used a combination of",
    "start": "1241520",
    "end": "1247559"
  },
  {
    "text": "manual sharding and Round Robin sharding the nine Target cluster with a high number of applications count were",
    "start": "1247559",
    "end": "1253720"
  },
  {
    "text": "manually assigned to specific Shard ensuring that not all high volume clusters were are attached to the same",
    "start": "1253720",
    "end": "1261559"
  },
  {
    "text": "Shard these were the resources which we have kept for controller like Rao server and application controller status",
    "start": "1262960",
    "end": "1268960"
  },
  {
    "text": "processor and the operation processor we kept at 3420 which has been taken from the",
    "start": "1268960",
    "end": "1274200"
  },
  {
    "text": "previous test where we observe the better performance the client QPS and client burst has been kept",
    "start": "1274200",
    "end": "1280480"
  },
  {
    "text": "at200 which is also been taken from the previous test",
    "start": "1280480",
    "end": "1285600"
  },
  {
    "text": "cases now come the observation part from this test in the first iteration all application got synced in 72 minutes but",
    "start": "1286520",
    "end": "1294360"
  },
  {
    "text": "in the second iteration syncing of all application took 100 minutes which is longer compared to the first",
    "start": "1294360",
    "end": "1301440"
  },
  {
    "text": "iteration if I talk about the CPU performance there was no significant difference in the CPU performance of the",
    "start": "1301440",
    "end": "1308200"
  },
  {
    "text": "Argo CD application controller between the two test iteration this suggest that",
    "start": "1308200",
    "end": "1313400"
  },
  {
    "text": "the distribution of application did not have a notable impact on CPU performance when we have in the second iteration",
    "start": "1313400",
    "end": "1321440"
  },
  {
    "text": "when we have manual plus round",
    "start": "1321440",
    "end": "1325759"
  },
  {
    "text": "robin app pre constellation que and app operation processing queue so yeah by",
    "start": "1327520",
    "end": "1334640"
  },
  {
    "text": "testing this round robin and manual plus round robin in if I if I talk about the app",
    "start": "1334640",
    "end": "1342799"
  },
  {
    "text": "canellation q a small q also formed in the application processing queue during the second iteration but and also this",
    "start": "1342799",
    "end": "1349080"
  },
  {
    "text": "time this affected the syn time as well during this iteration we noticed a significant syn time difference of 28",
    "start": "1349080",
    "end": "1355039"
  },
  {
    "text": "minutes this delay was linked to a specific Shard one which was handling Target cluster with a high application",
    "start": "1355039",
    "end": "1360880"
  },
  {
    "text": "load the CU formation on this Shard contributed to the increasing time which we have",
    "start": "1360880",
    "end": "1367519"
  },
  {
    "text": "observed we have also included metrics from kubernetes API server especially from the Argo City control plane",
    "start": "1368240",
    "end": "1374760"
  },
  {
    "text": "kubernetes server we aim to check if they were increase in the kubernetes API",
    "start": "1374760",
    "end": "1380120"
  },
  {
    "text": "server latency however the latency remained within the acceptable range with no significant concern observed we",
    "start": "1380120",
    "end": "1387120"
  },
  {
    "text": "also monitored the cube server latency for all target clusters and once again",
    "start": "1387120",
    "end": "1392400"
  },
  {
    "text": "no major issues were observed so yeah the",
    "start": "1392400",
    "end": "1398720"
  },
  {
    "text": "conclusion that the combination of manual sharding and the round robing sharing work well for our setup where",
    "start": "1398720",
    "end": "1404840"
  },
  {
    "text": "certain clusters handle a heavy load While others manager normal",
    "start": "1404840",
    "end": "1409919"
  },
  {
    "text": "load we were observed an increase in sync time when the application distribution varied across clusters",
    "start": "1409919",
    "end": "1415919"
  },
  {
    "text": "particularly when clusters were manually",
    "start": "1415919",
    "end": "1419600"
  },
  {
    "text": "sharded now the scalability test four which is something not related to",
    "start": "1421400",
    "end": "1427120"
  },
  {
    "text": "scalability but more toward resiliency the final test which we have conducted",
    "start": "1427120",
    "end": "1433159"
  },
  {
    "text": "here we have removed some of Target cluster after deploying applications in",
    "start": "1433159",
    "end": "1438799"
  },
  {
    "text": "this test case we have deployed 16,000 applications across all 110 virtual Target clusters consisting a mix set of",
    "start": "1438799",
    "end": "1447480"
  },
  {
    "text": "applications when we deleted four Target clusters where a total of four five 40",
    "start": "1448600",
    "end": "1454000"
  },
  {
    "text": "applications were deployed after deleting the cluster virtual Target clusters from the argd control plane all",
    "start": "1454000",
    "end": "1460000"
  },
  {
    "text": "application deployed on this cluster short a syn status of unknown and a health status of",
    "start": "1460000",
    "end": "1465600"
  },
  {
    "text": "unknown upon deletion of this cluster arus begin reshuffling the remaining cluster across all chart that was",
    "start": "1465600",
    "end": "1471360"
  },
  {
    "text": "expected in round robin we then redeployed the application",
    "start": "1471360",
    "end": "1476760"
  },
  {
    "text": "sets targeting 20 virtual Target clusters which included the four cluster that had been deleted",
    "start": "1476760",
    "end": "1483559"
  },
  {
    "text": "previously the observation which we have noted from this test that the new applications were only created after",
    "start": "1483559",
    "end": "1490640"
  },
  {
    "text": "Argo CD had cleared the unknown status of the previously deployed applications so this was the observation which we",
    "start": "1490640",
    "end": "1496880"
  },
  {
    "text": "have observed during this",
    "start": "1496880",
    "end": "1500320"
  },
  {
    "text": "test so that would be all from my side for the testing which we have performed",
    "start": "1502679",
    "end": "1508159"
  },
  {
    "text": "now handing over to Shivani for taking the talk forward thank you so much moit",
    "start": "1508159",
    "end": "1516080"
  },
  {
    "text": "uh I I hope this technical Deep dive of you know the test cases that we designed",
    "start": "1516080",
    "end": "1521960"
  },
  {
    "text": "uh I know there was a time crant so we discussed just a few of the test cases but um please be posted we'll try to",
    "start": "1521960",
    "end": "1529120"
  },
  {
    "text": "come up with a Blog soon including a lot of other test cases that we did so I hope this knowledge could help you uh",
    "start": "1529120",
    "end": "1536120"
  },
  {
    "text": "guys to understand the key aspects of the test cases design and to how to understand their results",
    "start": "1536120",
    "end": "1542679"
  },
  {
    "text": "Etc so uh let me take you through some optimal settings that we observed that",
    "start": "1542679",
    "end": "1547919"
  },
  {
    "text": "actually worked for our setup at EG you guys can take a reference uh when in future or maybe currently if you're",
    "start": "1547919",
    "end": "1554640"
  },
  {
    "text": "trying to perform such scalability testing uh I would like like to highlight that these values work well",
    "start": "1554640",
    "end": "1560320"
  },
  {
    "text": "for our setup uh the number of clusters that we created at you know uh such a huge scale so these settings might vary",
    "start": "1560320",
    "end": "1567679"
  },
  {
    "text": "from organization to organization infra to infra so yes I won't be repeating the same values again I think you guys have",
    "start": "1567679",
    "end": "1574039"
  },
  {
    "text": "heard it a lot of times in the previous test cases but uh yeah so these were some of the application controller",
    "start": "1574039",
    "end": "1579840"
  },
  {
    "text": "optimal settings that worked well for us and these are some of the argocd repo server settings that worked well for us",
    "start": "1579840",
    "end": "1586679"
  },
  {
    "text": "so also please take a reference but yeah these settings might vary for your organizations and your",
    "start": "1586679",
    "end": "1593159"
  },
  {
    "text": "setups thank you so much uh the floor is now open for questions please connect with us on link uh LinkedIn as well",
    "start": "1593159",
    "end": "1599840"
  },
  {
    "text": "thank [Applause]",
    "start": "1599840",
    "end": "1607240"
  },
  {
    "text": "you",
    "start": "1607240",
    "end": "1610240"
  },
  {
    "text": "okay yeah yeah yeah uh you mentioned something regarding",
    "start": "1615880",
    "end": "1620960"
  },
  {
    "text": "that the application States would come back up but each time it would take a bit more time more time and you said",
    "start": "1620960",
    "end": "1628679"
  },
  {
    "text": "that it's not a big deal like what kind of",
    "start": "1628679",
    "end": "1634799"
  },
  {
    "text": "time problem actually when we started we started with some 13,000 applications so",
    "start": "1634799",
    "end": "1642559"
  },
  {
    "text": "that time we have deployed a similar kind of application on each and every Target clusters and we all know that uh",
    "start": "1642559",
    "end": "1648320"
  },
  {
    "text": "uh distribution of applications might differ on each and every Target clusters and Argo CD performs differently because",
    "start": "1648320",
    "end": "1656480"
  },
  {
    "text": "if a Target cluster with a high number of Alum applications has been attached to some charts and some of the charts",
    "start": "1656480",
    "end": "1663640"
  },
  {
    "text": "where low number of applications are running Target cluster attached to other shards so there might be difference in",
    "start": "1663640",
    "end": "1669399"
  },
  {
    "text": "sync time and there might be difference in the resource consumption as well so yeah that is something which we",
    "start": "1669399",
    "end": "1676640"
  },
  {
    "text": "wanted to test because because in our organization application Distribution on every cluster is different where some of",
    "start": "1676640",
    "end": "1683880"
  },
  {
    "text": "the Clusters are running High number of applications and some of the Clusters are running very low number of",
    "start": "1683880",
    "end": "1689159"
  },
  {
    "text": "applications so that's why we came up with this solution like manual sharding and round robing where we have fixed uh",
    "start": "1689159",
    "end": "1697039"
  },
  {
    "text": "some Target clusters with manual Shard we have assigned manual Shard to those cluster so that high number of",
    "start": "1697039",
    "end": "1703840"
  },
  {
    "text": "application clusters did not land up in the same Shard because because that will be a problem of resource consumption",
    "start": "1703840",
    "end": "1710320"
  },
  {
    "text": "because if suppose Shard zero has been all the target cluster where High number",
    "start": "1710320",
    "end": "1715559"
  },
  {
    "text": "of application got attached to Shard zero if we did not use manual Shard then there will be a resource consumption",
    "start": "1715559",
    "end": "1721600"
  },
  {
    "text": "problem in Argo CD controller right so",
    "start": "1721600",
    "end": "1727559"
  },
  {
    "text": "yeah and same time vary as per the application as well",
    "start": "1727559",
    "end": "1735398"
  },
  {
    "text": "yeah so my question was how do you tackle the problem of shared resource warning because basically if sometimes",
    "start": "1738600",
    "end": "1744880"
  },
  {
    "text": "developers misconfigure uh like pdbs and uh or other as your keyword Secrets uh",
    "start": "1744880",
    "end": "1751720"
  },
  {
    "text": "they put the same name to two dashboard so uh the Argo CID is stuck in the loop",
    "start": "1751720",
    "end": "1757640"
  },
  {
    "text": "of uh for example one application syncs the resource uh changes some labels and",
    "start": "1757640",
    "end": "1763320"
  },
  {
    "text": "then second application becomes out of sync so it starts to sync then it becomes uh out of things so if there are",
    "start": "1763320",
    "end": "1769919"
  },
  {
    "text": "two to three dashboards like Dash uh like that it floods our refresh queue so do you have some validation in place or",
    "start": "1769919",
    "end": "1777399"
  },
  {
    "text": "what can you suggest for that shared resource uh warning should be prevented shared resource",
    "start": "1777399",
    "end": "1783399"
  },
  {
    "text": "warning I didn't get Yeah so basically if uh two resources like uh for example",
    "start": "1783399",
    "end": "1789880"
  },
  {
    "text": "say there is a Azure keyword secret with some name so yeah we have like a ignore",
    "start": "1789880",
    "end": "1795279"
  },
  {
    "text": "differences in Argo CD yeah ignore differences okay ignore differences also",
    "start": "1795279",
    "end": "1801919"
  },
  {
    "text": "we can set up some K rules and policies to you know avoid that kind of a situation we do are using K Noir okay",
    "start": "1801919",
    "end": "1808600"
  },
  {
    "text": "okay thank",
    "start": "1808600",
    "end": "1811240"
  },
  {
    "text": "you any other questions then thank you so much everyone for being a great audience",
    "start": "1813799",
    "end": "1822039"
  },
  {
    "text": "thank you",
    "start": "1822039",
    "end": "1825200"
  }
]