[
  {
    "text": "hello everyone our talk is about pushing the limits of Prometheus at Etsy today",
    "start": "160",
    "end": "5680"
  },
  {
    "text": "we'll share some of the lessons from operating one of the industry's largest Prometheus servers but before we get",
    "start": "5680",
    "end": "11320"
  },
  {
    "text": "into it let us introduce ourselves my name is Chris Levoy and I'm",
    "start": "11320",
    "end": "16760"
  },
  {
    "text": "an observability engineer at Etsy I'm based in waterl Canada and I've been with Etsy for the past four years Etsy",
    "start": "16760",
    "end": "24320"
  },
  {
    "text": "is a global e-commerce Marketplace for unique and creative Goods it was founded in 2005 and is headquartered in Brooklyn",
    "start": "24320",
    "end": "31080"
  },
  {
    "text": "New York now I'll invite my co-speaker Brian hello uh my name is Brian boram I",
    "start": "31080",
    "end": "38559"
  },
  {
    "text": "am a distinguished engineer at grafana Labs grafana is the leading open-source",
    "start": "38559",
    "end": "43960"
  },
  {
    "text": "software for visualizing operational data I had to read that um what I what I",
    "start": "43960",
    "end": "50480"
  },
  {
    "text": "do on my day job is is I work on the uh scaling the massively scalable storage",
    "start": "50480",
    "end": "56160"
  },
  {
    "text": "that we have for metrics logs and traces we store trillions of metric points and",
    "start": "56160",
    "end": "61800"
  },
  {
    "text": "petabytes of logs I the reason I'm on this stage is I'm a Prometheus maintainer uh I've worked on that code",
    "start": "61800",
    "end": "68720"
  },
  {
    "text": "for about seven years now uh so who knows",
    "start": "68720",
    "end": "75280"
  },
  {
    "text": "Prometheus okay like 60% or something like that well good good um I just want",
    "start": "75280",
    "end": "81680"
  },
  {
    "text": "to talk about the uh architecture um",
    "start": "81680",
    "end": "86840"
  },
  {
    "text": "because we'll we'll return to this picture a number of times uh so basically uh data data starts off",
    "start": "86840",
    "end": "93840"
  },
  {
    "text": "on the left uh what we call exporters so that could be data coming from the node",
    "start": "93840",
    "end": "99159"
  },
  {
    "text": "or from containers or application specific metrics anything where the data",
    "start": "99159",
    "end": "104960"
  },
  {
    "text": "is coming from that we call that an exporter and we pull the data in that's a process called",
    "start": "104960",
    "end": "110439"
  },
  {
    "text": "scraping we put it in the time series database uh tsdb first of all in memory",
    "start": "110439",
    "end": "115880"
  },
  {
    "text": "and then on disk and then uh to get data out there's the prom query language um",
    "start": "115880",
    "end": "122439"
  },
  {
    "text": "and then usually visualization is done by something on on the outside like maybe grafana",
    "start": "122439",
    "end": "128920"
  },
  {
    "text": "um the key thing that's important for this talk is everything in that box in",
    "start": "128920",
    "end": "135040"
  },
  {
    "text": "the middle is one process uh that's what's really cool about Prometheus it's really easy to",
    "start": "135040",
    "end": "140840"
  },
  {
    "text": "deploy one process and you're you're off um but the way it scales is get a bigger",
    "start": "140840",
    "end": "147280"
  },
  {
    "text": "machine so this talk is about the biggest",
    "start": "147280",
    "end": "154680"
  },
  {
    "text": "Prometheus I ever saw so uh what is this giant Prometheus",
    "start": "154680",
    "end": "163040"
  },
  {
    "text": "server for well if you're looking for that perfect gift to Mark a special occasion you can try searching on",
    "start": "163040",
    "end": "171159"
  },
  {
    "text": "etsy.com the server we'll be discussing monitors millions of metrics about searches on Etsy it's been around for",
    "start": "171200",
    "end": "177480"
  },
  {
    "text": "about 8 years and it's just chalk full of alerts and recording rules but this server is just one of 30",
    "start": "177480",
    "end": "184480"
  },
  {
    "text": "Prometheus Stacks that we operate at Etsy each stack is isolated around system boundaries in total we Peak at",
    "start": "184480",
    "end": "191680"
  },
  {
    "text": "around 600 million series or 5 million samples per second during busy",
    "start": "191680",
    "end": "196959"
  },
  {
    "text": "periods each each stack writes a copy of the data into a central location in graphon",
    "start": "196959",
    "end": "202560"
  },
  {
    "text": "Cloud uh there's a pair of servers in each stack providing High availability",
    "start": "202560",
    "end": "207640"
  },
  {
    "text": "and this story is about how our largest stack you know couldn't scale vertically",
    "start": "207640",
    "end": "213920"
  },
  {
    "text": "anymore so why use vertical scaling instead of another approach well the",
    "start": "213920",
    "end": "220239"
  },
  {
    "text": "honest answer is that pushing the boundaries of what a single Prometheus server could handle was a fun task um",
    "start": "220239",
    "end": "227200"
  },
  {
    "text": "but also the the tech culture at at Etsy has been heavily influenced by the idea of choosing boring technology and just",
    "start": "227200",
    "end": "234159"
  },
  {
    "text": "keeping it simple we should exhaust all possibilities with the base architecture",
    "start": "234159",
    "end": "239239"
  },
  {
    "text": "before taking more drastic measures because things like sharding and Federation they they they introduce a",
    "start": "239239",
    "end": "245840"
  },
  {
    "text": "lot of complexity and so we set out to delay the inevitable as long as possible",
    "start": "245840",
    "end": "251400"
  },
  {
    "text": "and see how far we could push the limits of a single Prometheus",
    "start": "251400",
    "end": "256560"
  },
  {
    "text": "instance and I'm sure many are anticipating this talk will be about how we crashed our server by adding too much",
    "start": "256560",
    "end": "262880"
  },
  {
    "text": "data and you'd be right we blew it up we blew it up a lot quite often actually",
    "start": "262880",
    "end": "268600"
  },
  {
    "text": "with a show of hands how many people have overloaded some database with too much",
    "start": "268600",
    "end": "273759"
  },
  {
    "text": "data yeah yeah I thought so we've all been there um but the thing is there's",
    "start": "273759",
    "end": "280400"
  },
  {
    "text": "never a simple answer of how much data is too much it really just always",
    "start": "280400",
    "end": "285919"
  },
  {
    "text": "depends now these graphs uh that I'm showing here uh show our server running at Peak",
    "start": "285919",
    "end": "293199"
  },
  {
    "text": "albe it somewhat unsuccessfully the the top graph shows step increases of about 50 million in",
    "start": "293199",
    "end": "299840"
  },
  {
    "text": "head uh time series and the bottom graph shows gaps where we expect a kind of",
    "start": "299840",
    "end": "305240"
  },
  {
    "text": "smooth line at around the 1 million samples per second Mark these dips are signs that the server is overloaded and",
    "start": "305240",
    "end": "312000"
  },
  {
    "text": "dropping data Prometheus happily was chugging along in this degraded state but",
    "start": "312000",
    "end": "318479"
  },
  {
    "text": "recording rules and scrapes were being skipped in order to shed load these orange bars I've added to the",
    "start": "318479",
    "end": "325880"
  },
  {
    "text": "top graph um these aren't metric explosions and this isn't really a high",
    "start": "325880",
    "end": "331440"
  },
  {
    "text": "cardinality problem as it's often described these step increases are from deployment and not somebody adding new",
    "start": "331440",
    "end": "338479"
  },
  {
    "text": "metrics to the system so to explain these increases I'll touch on how Etsy does",
    "start": "338479",
    "end": "345039"
  },
  {
    "text": "deployments at Etsy we do auto scaling and blue green deployment on kubernetes",
    "start": "345039",
    "end": "350479"
  },
  {
    "text": "this results in super fast deploys but it also generates a considerable amount of churn and this churn is a big",
    "start": "350479",
    "end": "357680"
  },
  {
    "text": "challenge when operating Prometheus at this scale flipping between the active and dark sides causes 50 million metrics",
    "start": "357680",
    "end": "364720"
  },
  {
    "text": "to stop reporting and another 50 million to come online and there's an overlapping period where both sides are",
    "start": "364720",
    "end": "371120"
  },
  {
    "text": "reporting at the same time so if we did four deploys in an hour we would need a",
    "start": "371120",
    "end": "376479"
  },
  {
    "text": "much bigger server however after reviewing the",
    "start": "376479",
    "end": "382360"
  },
  {
    "text": "options we realized that Google only offered one final upgrade in This Server family it struck us that we might not be",
    "start": "382360",
    "end": "389199"
  },
  {
    "text": "able to find a larger server next time and even if we did there were signs that Prometheus was already in a state that",
    "start": "389199",
    "end": "395000"
  },
  {
    "text": "prevented us from fully taking advantage of the hardware we need to we needed to act swiftly because the week of Black",
    "start": "395000",
    "end": "401919"
  },
  {
    "text": "Friday and sabber Monday was approaching it's etsy's busiest season of the year so to buy us some time we opted to",
    "start": "401919",
    "end": "409639"
  },
  {
    "text": "deploy the biggest server available but more importantly we committed to taking a closer",
    "start": "409639",
    "end": "416199"
  },
  {
    "text": "look in this middle column we're showing uh at an intermediate State at this time",
    "start": "416479",
    "end": "423039"
  },
  {
    "text": "the the issues seemed to be Memory related so we went from a mega M to an ultr instance type which had twice the",
    "start": "423039",
    "end": "429199"
  },
  {
    "text": "amount of ram going from 2 to 4 terabytes this gave us much needed be",
    "start": "429199",
    "end": "434560"
  },
  {
    "text": "breathing room but clarified that throwing more Hardware at the problem wasn't helping we were bottlenecked on",
    "start": "434560",
    "end": "442280"
  },
  {
    "text": "something and none of our other environments were experiencing issues and we also couldn't easily produce this",
    "start": "443240",
    "end": "450199"
  },
  {
    "text": "in a test environment it's not easy to get approval for a 4 TB server so we",
    "start": "450199",
    "end": "455800"
  },
  {
    "text": "decided to deploy a third replica that allowed us to do side by-side benchmarks in",
    "start": "455800",
    "end": "461400"
  },
  {
    "text": "production this enabled us to quickly iterate through several permutations without jeopardizing service and this",
    "start": "461400",
    "end": "468560"
  },
  {
    "text": "third column here that I'm showing gives you a sneak peek into the specs we eventually landed on but spoiler alert",
    "start": "468560",
    "end": "476159"
  },
  {
    "text": "uh it ran much better at 1 terab of ram than it did at four but even with the faster server we could",
    "start": "476159",
    "end": "483800"
  },
  {
    "text": "not still we could still not determine why the Prometheus remote right function was lagging",
    "start": "483800",
    "end": "488919"
  },
  {
    "text": "behind so that's about when we looked to grafana labs for help and that's about when Brian got",
    "start": "488919",
    "end": "496280"
  },
  {
    "text": "involved yeah so um so as Chris says they they were running uh open source",
    "start": "496280",
    "end": "502360"
  },
  {
    "text": "Prometheus uh in in their infrastructure and pushing the data to our cloud",
    "start": "502360",
    "end": "508080"
  },
  {
    "text": "service and um and this is this is what came in on on the report on the the",
    "start": "508080",
    "end": "514200"
  },
  {
    "text": "ticket uh that I was asked to look at um so the way remote right works when",
    "start": "514200",
    "end": "520760"
  },
  {
    "text": "Prometheus gets hold of the data it it pushes it off uh to a central service",
    "start": "520760",
    "end": "526000"
  },
  {
    "text": "and it's supposed to do that inside like one second there's not supposed to be",
    "start": "526000",
    "end": "531240"
  },
  {
    "text": "any lag at all um what I was uh called",
    "start": "531240",
    "end": "536680"
  },
  {
    "text": "to look at the the lag was was getting up to um five six minutes and and 10",
    "start": "536680",
    "end": "542040"
  },
  {
    "text": "minutes uh and also it it would last for hours I mean it's it's it's possible",
    "start": "542040",
    "end": "547399"
  },
  {
    "text": "that something could glitch and then it should recover quickly uh so the other symptom was was that it would stay high",
    "start": "547399",
    "end": "554279"
  },
  {
    "text": "for several hours um so this was really bad um there was another uh thing we'll",
    "start": "554279",
    "end": "563200"
  },
  {
    "text": "talk about uh but just to mention uh desired shards um this is a uh uh Shard",
    "start": "563200",
    "end": "573920"
  },
  {
    "text": "is a by Shard we mean mean splitting up the work so we can do things in parallel",
    "start": "573920",
    "end": "578959"
  },
  {
    "text": "and um uh Prometheus runs a very very simple model of what how many shards it",
    "start": "578959",
    "end": "585560"
  },
  {
    "text": "might need to send if there's a backlog it it divides the backlog by uh how many",
    "start": "585560",
    "end": "591480"
  },
  {
    "text": "shards it would take to send the data in one minute to clear the whole backlog and um that's a very simple",
    "start": "591480",
    "end": "598760"
  },
  {
    "text": "model it it works if you can send infinitely fast and if the far end can",
    "start": "598760",
    "end": "603920"
  },
  {
    "text": "receive infinitely fast so who's got an infinitely fast service No Hands okay yeah so there's a",
    "start": "603920",
    "end": "611360"
  },
  {
    "text": "problem so yeah I just want to mention uh this number uh desired shards is is",
    "start": "611360",
    "end": "616519"
  },
  {
    "text": "kind of a fantasy number and and uh it's probably something we should have guard rails on in Prometheus because because",
    "start": "616519",
    "end": "622560"
  },
  {
    "text": "people read this and and uh think that they should configure for that but but not",
    "start": "622560",
    "end": "627800"
  },
  {
    "text": "really and that did get us in trouble um but we eventually we eventually",
    "start": "627800",
    "end": "633320"
  },
  {
    "text": "understood that at least in our situation letting Prometheus dynamically adjust the number of remote right shards",
    "start": "633320",
    "end": "639360"
  },
  {
    "text": "was unnecessary because resharding is an expensive operation we opted to hardcode",
    "start": "639360",
    "end": "645079"
  },
  {
    "text": "the Min and Max shards to the same value the right hand table I'm showing here are the settings we landed on but I'll",
    "start": "645079",
    "end": "651680"
  },
  {
    "text": "explain some of the theory behind them we settled on 100 shards as a nice round number far less than the number of",
    "start": "651680",
    "end": "659600"
  },
  {
    "text": "that was suggested by the desired shards metric we recommend picking A Shard count relative to the CPU count and if",
    "start": "659600",
    "end": "667440"
  },
  {
    "text": "latency to the destination is high you can consider doubling it but also don't forget to increase the batch and buffer",
    "start": "667440",
    "end": "673360"
  },
  {
    "text": "sizes to match and calculating your maximum theoretical throughput under a worst case latency scenario is a",
    "start": "673360",
    "end": "680639"
  },
  {
    "text": "shortcut to figuring out what these settings should be for example let's say you have 30 million series scraped every",
    "start": "680639",
    "end": "688040"
  },
  {
    "text": "15 seconds as in R setup this comes out to 2 million samples per second so with",
    "start": "688040",
    "end": "693680"
  },
  {
    "text": "a worst case roundtrip latency of 500 milliseconds each Shard can send two",
    "start": "693680",
    "end": "699120"
  },
  {
    "text": "batches per second so with 20,000 samples per batch this results in a theoretical maximum throughput of 4",
    "start": "699120",
    "end": "705800"
  },
  {
    "text": "million samples per second so if we double the worst case",
    "start": "705800",
    "end": "711680"
  },
  {
    "text": "latency from 500 milliseconds to 1 second we reduce the maximum throughput in half from 4 to 2 million this means",
    "start": "711680",
    "end": "719279"
  },
  {
    "text": "means we have plenty of headro at 500 milliseconds of latency but we can't really tolerate more than 1 second of",
    "start": "719279",
    "end": "725279"
  },
  {
    "text": "latency so after ironing out these settings for remote right we were still puzzled by what was causing the remote",
    "start": "725279",
    "end": "732440"
  },
  {
    "text": "right lag and like many good stories uh we had",
    "start": "732440",
    "end": "739079"
  },
  {
    "text": "several side quests and one of them was triggered when we looked at this chart of network bandwidth we occasionally saw",
    "start": "739079",
    "end": "746000"
  },
  {
    "text": "timeouts and that kind of made it seem like there was an underlying Network issue that was plaguing us here and we",
    "start": "746000",
    "end": "751920"
  },
  {
    "text": "noticed bandwidth peaked at 360 Megs per second even during playback periods when",
    "start": "751920",
    "end": "758120"
  },
  {
    "text": "it should have pushed much much higher it turns out that Google actually documents one flow in their system",
    "start": "758120",
    "end": "764760"
  },
  {
    "text": "cannot exceed this limit when egressing from a compute instance to a destination outside of the VPC I hopped on one of",
    "start": "764760",
    "end": "772160"
  },
  {
    "text": "the servers and ran netstat a few times to discover that Prometheus was indeed",
    "start": "772160",
    "end": "777399"
  },
  {
    "text": "sending data over a single socket which kind of caught us by surprise we ended up disabling",
    "start": "777399",
    "end": "784000"
  },
  {
    "text": "http2 um as one of the settings so that it was send data over multiple connections so we wouldn't have to worry",
    "start": "784000",
    "end": "789800"
  },
  {
    "text": "about this limit anymore but there was more to the story because the next day we got alerts about Prometheus remote",
    "start": "789800",
    "end": "796399"
  },
  {
    "text": "right falling behind again all right so getting back to our main quest I'll turn it back to Brian to",
    "start": "796399",
    "end": "802600"
  },
  {
    "text": "discuss the right ahead log in more detail yeah you always always blame the",
    "start": "802600",
    "end": "807959"
  },
  {
    "text": "network right yeah it's never the network um so uh yeah so there a little",
    "start": "807959",
    "end": "816440"
  },
  {
    "text": "bit more detail what's going on inside Prometheus um and and focusing on the",
    "start": "816440",
    "end": "822199"
  },
  {
    "text": "parallelism so Etsy had uh like 10,000 exporters on the left um those are uh",
    "start": "822199",
    "end": "831720"
  },
  {
    "text": "run in in the code of Prometheus it's written in go each one of those gets a go routine they can if they want to they",
    "start": "831720",
    "end": "837959"
  },
  {
    "text": "can all run in parallel there's a tremendous amount of parallelism there",
    "start": "837959",
    "end": "843120"
  },
  {
    "text": "on the sending side we we had a thousand shards we we reduced it to 100 but even so tremendous amount of parallelism on",
    "start": "843120",
    "end": "850360"
  },
  {
    "text": "the right hand side so why have I drawn the picture this way there's a bit in",
    "start": "850360",
    "end": "855560"
  },
  {
    "text": "the middle um Prometheus like many databases uh when",
    "start": "855560",
    "end": "864440"
  },
  {
    "text": "it gets hold of some data it commits it to disk immediately this is a a a design",
    "start": "864440",
    "end": "870199"
  },
  {
    "text": "called a write ahead log um and it's a very good design for resiliency if if",
    "start": "870199",
    "end": "875880"
  },
  {
    "text": "something happens if the process has to restart it can recover its state by reading the right ahead log the",
    "start": "875880",
    "end": "882519"
  },
  {
    "text": "W um when doing remote right we want to",
    "start": "882519",
    "end": "889320"
  },
  {
    "text": "um balance the receiving and sending there might be some glitch in sending uh the rate the data's coming in",
    "start": "889320",
    "end": "896360"
  },
  {
    "text": "at you you could get gigabytes of data queued up and we don't want that in memory so the it's already on disk",
    "start": "896360",
    "end": "902839"
  },
  {
    "text": "because we wrote it to the wow so this is um this is the design of Prometheus that we use the wow as a q for remote",
    "start": "902839",
    "end": "912320"
  },
  {
    "text": "right and it's it's a really nice design because we don't have to implement",
    "start": "912320",
    "end": "918399"
  },
  {
    "text": "another queue um unfortunately for Etsy with this massive massive",
    "start": "918399",
    "end": "924880"
  },
  {
    "text": "server there's exactly one right ahead log and the whole thing is bottlenecked",
    "start": "924880",
    "end": "931000"
  },
  {
    "text": "on this one operation um how did we figure this",
    "start": "931000",
    "end": "937160"
  },
  {
    "text": "out well you might think uh something like profile you have a performance",
    "start": "937160",
    "end": "942360"
  },
  {
    "text": "problem profile it um but profiling gives you an average over the whole program and um so actually this is",
    "start": "942360",
    "end": "949720"
  },
  {
    "text": "different tool this is the go uh execution tracing",
    "start": "949720",
    "end": "954759"
  },
  {
    "text": "View and uh I'm sure it's too small for everyone to see but but the what I want you to kind of get from this is the um",
    "start": "954759",
    "end": "962440"
  },
  {
    "text": "the yellow line at the top is solid that's the one uh reading the wow um and uh all the other lines the",
    "start": "962440",
    "end": "971720"
  },
  {
    "text": "kind of swim Lanes they're the ones uh that write the data out on remote right",
    "start": "971720",
    "end": "977440"
  },
  {
    "text": "and um and they spend most of their time with with whites space there there's a little bit of the the greens and the the",
    "start": "977440",
    "end": "985279"
  },
  {
    "text": "pinks uh when they get some data they Marshall it compress it send it but they",
    "start": "985279",
    "end": "991519"
  },
  {
    "text": "spend most of the time waiting to get data so this was kind of The Smoking Gun",
    "start": "991519",
    "end": "998240"
  },
  {
    "text": "For What was really wrong once we'd been on all the side",
    "start": "998240",
    "end": "1004120"
  },
  {
    "text": "quests so with with Black Friday the Black Friday deadline looming we didn't",
    "start": "1005839",
    "end": "1011199"
  },
  {
    "text": "have time to rewrite the queuing mechanism we just needed to make this single thread go faster we so we asked",
    "start": "1011199",
    "end": "1017399"
  },
  {
    "text": "ourselves what could we change to the pressure on the bald neck without rewriting the code there are really only",
    "start": "1017399",
    "end": "1023000"
  },
  {
    "text": "a couple ways of doing that one do less work three two make the CPU faster and",
    "start": "1023000",
    "end": "1029520"
  },
  {
    "text": "three just have fewer interrupts we ended up trying all three so for the first item another expensive",
    "start": "1029520",
    "end": "1036959"
  },
  {
    "text": "mechanism in Prometheus is called compaction we disabled it but before we get into why I'll invite Brian back to",
    "start": "1036959",
    "end": "1043678"
  },
  {
    "text": "discuss how the compactor Works in Prometheus yeah the um",
    "start": "1043679",
    "end": "1049919"
  },
  {
    "text": "so switching Tac uh this is this is a bit about how tsdb Works um when the",
    "start": "1049919",
    "end": "1056280"
  },
  {
    "text": "when the data is uh put into the time series database in memory um we don't",
    "start": "1056280",
    "end": "1062880"
  },
  {
    "text": "want to just build up and build up and build up in in data in memory so every two hours we run a process called",
    "start": "1062880",
    "end": "1069480"
  },
  {
    "text": "compaction um and and build a a two-hour block of data on",
    "start": "1069480",
    "end": "1074600"
  },
  {
    "text": "disk um after another two hours we build another two-hour block",
    "start": "1074600",
    "end": "1079679"
  },
  {
    "text": "and after another two hours we build another two-hour block and um and this is great because",
    "start": "1079679",
    "end": "1085400"
  },
  {
    "text": "it's no longer we don't have all that data in memory uh it does um pull the",
    "start": "1085400",
    "end": "1091039"
  },
  {
    "text": "data back if you run queries uh but it only reads the data that you actually need for that query that and that's done",
    "start": "1091039",
    "end": "1097600"
  },
  {
    "text": "through memory maap IO um so uh normally Prometheus then uh",
    "start": "1097600",
    "end": "1105400"
  },
  {
    "text": "starts to build these two our blocks into bigger blocks so um and it's done in threes this is",
    "start": "1105400",
    "end": "1112120"
  },
  {
    "text": "this is default settings for Prometheus so uh so we take three 2hour blocks and",
    "start": "1112120",
    "end": "1118320"
  },
  {
    "text": "make a 6-hour block we take three 6our blocks and make an 18h hour block and it carries on like",
    "start": "1118320",
    "end": "1126880"
  },
  {
    "text": "that 54h hour blocks 162 hour blocks um basically however long your uh storage",
    "start": "1126880",
    "end": "1134480"
  },
  {
    "text": "retention is configured for uh by default Prometheus will will continue trying to make bigger and bigger blocks",
    "start": "1134480",
    "end": "1140559"
  },
  {
    "text": "um which are more efficient and that's the the the normal idea but we realized that that these",
    "start": "1140559",
    "end": "1149480"
  },
  {
    "text": "periodic compactions were uh incredibly intensive operations and again it's all",
    "start": "1149480",
    "end": "1156000"
  },
  {
    "text": "running in one process um so uh inside etsy's very",
    "start": "1156000",
    "end": "1162440"
  },
  {
    "text": "large Prometheus these compactions were taking up a tremendous amount of resources",
    "start": "1162440",
    "end": "1169240"
  },
  {
    "text": "uh oh yeah a little side note it's a little confusing there's two operations called compaction with this one is head",
    "start": "1169919",
    "end": "1177039"
  },
  {
    "text": "compaction where we go from memory to disk and um uh the the other compaction",
    "start": "1177039",
    "end": "1182760"
  },
  {
    "text": "where we're doing historic blocks but but that's um sorry about",
    "start": "1182760",
    "end": "1188039"
  },
  {
    "text": "that Chris um so using smaller 1hour blocks was",
    "start": "1188039",
    "end": "1195120"
  },
  {
    "text": "critical to us for several reasons and first the the default of 2-hour blocks",
    "start": "1195120",
    "end": "1201880"
  },
  {
    "text": "meant that Prometheus could take a really long time to restart because it had to rebuild the last two hours of",
    "start": "1201880",
    "end": "1208720"
  },
  {
    "text": "data in fact we had a maximum startup duration of 1 hour before kubernetes would give up so if we restarted the",
    "start": "1208720",
    "end": "1215960"
  },
  {
    "text": "server between 4: and 6:00 p.m. when the wall was at its peak it would never come back online unless we intervened by",
    "start": "1215960",
    "end": "1222120"
  },
  {
    "text": "deleting the wall and losing all that historical data that hadn't yet been compacted into a block",
    "start": "1222120",
    "end": "1229320"
  },
  {
    "text": "the second reason and perhaps most more important is the compactor would constantly fail a blue green deployment",
    "start": "1229320",
    "end": "1236240"
  },
  {
    "text": "during Peak would produce so much churn in our metrics that even a 2-hour block would exceed an internal index size",
    "start": "1236240",
    "end": "1243280"
  },
  {
    "text": "limit this meant that Prometheus would get stuck with an infinitely growing wall that could not be compacted into",
    "start": "1243280",
    "end": "1249640"
  },
  {
    "text": "smaller blocks again this meant that someone needed to intervene by deleting that wall and giving up historical data",
    "start": "1249640",
    "end": "1256360"
  },
  {
    "text": "so we cut the duration of our initial blocks in half to ensure they never exceeded this size of 64",
    "start": "1256360",
    "end": "1264679"
  },
  {
    "text": "gigs so here is a diagram of Prometheus block layouts on top we have the",
    "start": "1264679",
    "end": "1269919"
  },
  {
    "text": "Prometheus default and on the bottom is the block layout we used at at Etsy we",
    "start": "1269919",
    "end": "1275360"
  },
  {
    "text": "made two changes the setting the minimum and maximum durations both to one to one",
    "start": "1275360",
    "end": "1281159"
  },
  {
    "text": "hour setting the Min let the series clear out of the head block faster and setting the max disables the larger",
    "start": "1281159",
    "end": "1287720"
  },
  {
    "text": "compactions instead of turning the wall into two-hour blocks and then turning those",
    "start": "1287720",
    "end": "1293480"
  },
  {
    "text": "two-hour blocks into progressively larger blocks we configured promethus to write out a continuous stream of 1hour",
    "start": "1293480",
    "end": "1299080"
  },
  {
    "text": "blocks without ever compacting them but before you try this at home you should understand the trade-offs that we",
    "start": "1299080",
    "end": "1305760"
  },
  {
    "text": "made many smaller blocks means queries over historical periods are more",
    "start": "1305760",
    "end": "1310919"
  },
  {
    "text": "expensive we got away with this strategy because our Prometheus server only retains 9 days worth of data or 216 hour",
    "start": "1310919",
    "end": "1318840"
  },
  {
    "text": "blocks where the default is 15 days as well using remote right means we",
    "start": "1318840",
    "end": "1325559"
  },
  {
    "text": "can offload expensive queries from the server and set aggressive query limits so as to not overload the server we set",
    "start": "1325559",
    "end": "1332520"
  },
  {
    "text": "query Max samples at 50 million and my my last point about compaction is to reinforce that for us",
    "start": "1332520",
    "end": "1340679"
  },
  {
    "text": "there was just too much turn in in the metrics from our blue to green deployments and auto scaling for the defaults to just work well so after",
    "start": "1340679",
    "end": "1348600"
  },
  {
    "text": "getting this distraction of the compactor out of the way we had one last side",
    "start": "1348600",
    "end": "1354600"
  },
  {
    "text": "quest we realized that 4 terab of ram was pretty Overkill now that we had uh",
    "start": "1354600",
    "end": "1360679"
  },
  {
    "text": "compaction disabled so we no longer needed that memory optimized machine type and would benefit from the faster",
    "start": "1360679",
    "end": "1366960"
  },
  {
    "text": "clock speeds that a compute optimized machine type would offer so we made a list of things to try and made side",
    "start": "1366960",
    "end": "1373880"
  },
  {
    "text": "by-side comparisons in production over a few days we ended up making some drastic changes",
    "start": "1373880",
    "end": "1379120"
  },
  {
    "text": "here I'm highlighting two config changes I'll let Brian talk about these in a minute but the surprising thing was that",
    "start": "1379120",
    "end": "1385320"
  },
  {
    "text": "Prometheus got Faster by constraining resources in almost half we went from",
    "start": "1385320",
    "end": "1390400"
  },
  {
    "text": "128 cores down to 50 cores and from 4 terabytes down to one terab of",
    "start": "1390400",
    "end": "1396039"
  },
  {
    "text": "ram also note is how we applied those constraints instead of kubernetes defining the upper bounds we set the",
    "start": "1396039",
    "end": "1403000"
  },
  {
    "text": "more restrictive limits at the Go runtime level because it turned out that garbage",
    "start": "1403000",
    "end": "1409039"
  },
  {
    "text": "collection was a baldl neck across the whole program each dip you see in this throughput graph every two minutes or so",
    "start": "1409039",
    "end": "1417640"
  },
  {
    "text": "was was linked to a garbage collection interval that briefly starved the remote right so I'll kick it back to Brian to",
    "start": "1417640",
    "end": "1423880"
  },
  {
    "text": "help us unpack how garbage collection was interfering with throughput it's always memory",
    "start": "1423880",
    "end": "1431279"
  },
  {
    "text": "management I don't know uh I've done a number of talks uh you can you can find",
    "start": "1431279",
    "end": "1436880"
  },
  {
    "text": "them on YouTube uh just you know search for my name uh I've done a number of talks about about memory management how",
    "start": "1436880",
    "end": "1444760"
  },
  {
    "text": "to speed up goal programs uh by tweaking the uh memory management um so let's",
    "start": "1444760",
    "end": "1450960"
  },
  {
    "text": "let's uh how we doing for time yeah pretty good pretty good uh yeah let's get into that um so I uh I love to draw",
    "start": "1450960",
    "end": "1460480"
  },
  {
    "text": "this Sawtooth diagram just just to try and help you understand um the uh the",
    "start": "1460480",
    "end": "1466200"
  },
  {
    "text": "Heap is where all the big memory stuff lives in a go program um and over time",
    "start": "1466200",
    "end": "1472080"
  },
  {
    "text": "it it goes like a like a saw too it it it builds up uh as you use memory and",
    "start": "1472080",
    "end": "1477720"
  },
  {
    "text": "discard it and then the garbage collector runs and it it that number drops again it builds up and it drops",
    "start": "1477720",
    "end": "1483399"
  },
  {
    "text": "and it builds up and it drops and that was the uh the previous picture that that Chris showed there's a there's a",
    "start": "1483399",
    "end": "1489200"
  },
  {
    "text": "default uh 2minute cycle where it where it will run the uh garbage collection",
    "start": "1489200",
    "end": "1495320"
  },
  {
    "text": "and it's it's another very very intensive operation that that drags down the whole program when it",
    "start": "1495320",
    "end": "1501279"
  },
  {
    "text": "runs um but there's two effects uh the the other one is how much bigger uh the",
    "start": "1501279",
    "end": "1508960"
  },
  {
    "text": "Heap will get before it garbage collects and and in Prometheus we configure that",
    "start": "1508960",
    "end": "1514159"
  },
  {
    "text": "to 75% um that is the go GC setting so um",
    "start": "1514159",
    "end": "1521159"
  },
  {
    "text": "so in a steady state is doing this Sawtooth it's it's has a certain amount of memory that it really needs it climbs",
    "start": "1521159",
    "end": "1527880"
  },
  {
    "text": "up to that plus 75% and drops down to that amount and climbs up and drops down climbs up and drops",
    "start": "1527880",
    "end": "1534840"
  },
  {
    "text": "down um when we had those compaction I haven't finished",
    "start": "1534840",
    "end": "1541480"
  },
  {
    "text": "yet uh when we had those uh compaction operations running uh every 6 hours",
    "start": "1541480",
    "end": "1547760"
  },
  {
    "text": "every 18 hours and so on uh the heat would get bigger and it would then grow another",
    "start": "1547760",
    "end": "1554000"
  },
  {
    "text": "75% so so this is one thing uh I I don't have fancy animations or AI or anything",
    "start": "1554000",
    "end": "1560880"
  },
  {
    "text": "but I'm going to wave my hands around um so uh so we were we were really",
    "start": "1560880",
    "end": "1566760"
  },
  {
    "text": "suffering from um uh the memory would grow bigger because it needed more memory to do compaction and then it",
    "start": "1566760",
    "end": "1572720"
  },
  {
    "text": "would grow bigger again because of this 75% Factor um so this setting of go M",
    "start": "1572720",
    "end": "1581039"
  },
  {
    "text": "limit um is is uh basically right just what you need um it caps the memory",
    "start": "1581039",
    "end": "1589480"
  },
  {
    "text": "usage uh and it will run go it will run garbage collection faster as you",
    "start": "1589480",
    "end": "1594799"
  },
  {
    "text": "approach that number you do have to pick that number very very carefully if you go too low if",
    "start": "1594799",
    "end": "1603200"
  },
  {
    "text": "it really needs more memory than that it will run garbage collection infinitely fast and you",
    "start": "1603200",
    "end": "1608679"
  },
  {
    "text": "will be sad",
    "start": "1608679",
    "end": "1614000"
  },
  {
    "text": "um so uh and then another thing once you've set go m go M limit you can turn",
    "start": "1614000",
    "end": "1619960"
  },
  {
    "text": "off um the regular Cadence that was the two-minute thing that we were seeing um",
    "start": "1619960",
    "end": "1626320"
  },
  {
    "text": "so go GC equals off and a go m in this case one terabyte because we um because",
    "start": "1626320",
    "end": "1633600"
  },
  {
    "text": "we limited queries because we turned off compaction we we did not expect any oscillation in the memory uh so this",
    "start": "1633600",
    "end": "1640799"
  },
  {
    "text": "gave us two things it it gave us um a much more constrained process it was",
    "start": "1640799",
    "end": "1646480"
  },
  {
    "text": "running in one tab instead of 4 terabytes uh and the garbage collection slowed down to like every 10 minutes",
    "start": "1646480",
    "end": "1653600"
  },
  {
    "text": "instead of every 2 minutes so the uh the remote right got a chance to catch up a lot better so that's that's a bit",
    "start": "1653600",
    "end": "1660640"
  },
  {
    "text": "complicated um like I say I've I've made some other talks you you can research this but uh but that that was what we",
    "start": "1660640",
    "end": "1666720"
  },
  {
    "text": "did and it it worked really well um so uh turning away from memory",
    "start": "1666720",
    "end": "1674360"
  },
  {
    "text": "management um there were uh quite number of changes um while we went through this",
    "start": "1674360",
    "end": "1682279"
  },
  {
    "text": "investigation uh things that were improved Upstream in Prometheus um so I",
    "start": "1682279",
    "end": "1687360"
  },
  {
    "text": "just put a few of them uh up on the screen there they they um improving",
    "start": "1687360",
    "end": "1692640"
  },
  {
    "text": "caching improving uh efficiency lots of of changes so you I just want to note",
    "start": "1692640",
    "end": "1698960"
  },
  {
    "text": "that this this was kind of a uh our customer and trying to help them out but then the the changes went Upstream in",
    "start": "1698960",
    "end": "1705080"
  },
  {
    "text": "open in open source Prometheus and benefited the whole community so very happy about",
    "start": "1705080",
    "end": "1711880"
  },
  {
    "text": "that so to wrap up our story we're happy to report that after tuning garbage collection we finally alleviated enough",
    "start": "1712399",
    "end": "1719440"
  },
  {
    "text": "pressure on the bottleneck and we made it through black fridy without any crashes or leg so a big thank you to",
    "start": "1719440",
    "end": "1725279"
  },
  {
    "text": "Brian for letting us nerd snipe him on this and another big thank you to the community building and maintaining",
    "start": "1725279",
    "end": "1732559"
  },
  {
    "text": "Prometheus so if you want to operate Prometheus at a similar scale here's a checklist of less learned number one",
    "start": "1732559",
    "end": "1740399"
  },
  {
    "text": "automate catching code changes that may cause metric explosions before they land in prod but sometimes changes slip",
    "start": "1740399",
    "end": "1748399"
  },
  {
    "text": "through the cracks so it's a good idea to set scrape limits as number two how many metrics Prometheus will accept is",
    "start": "1748399",
    "end": "1755240"
  },
  {
    "text": "unbounded in the stock configuration and it's almost certainly going to crash if you try jamming too many metrics in",
    "start": "1755240",
    "end": "1762240"
  },
  {
    "text": "Prometheus does a good job of setting defaults most of the time but not having an upper bound on this one is a foot gun",
    "start": "1762240",
    "end": "1768159"
  },
  {
    "text": "waiting to happen three consider using smaller block sizes and disabling uh compaction",
    "start": "1768159",
    "end": "1775320"
  },
  {
    "text": "if your data retention goals allow it at four tune the remote right for required",
    "start": "1775320",
    "end": "1781000"
  },
  {
    "text": "throughput using worst case latency as a guide and ignore that desired shards metric if it's giving you a silly",
    "start": "1781000",
    "end": "1787039"
  },
  {
    "text": "suggestion and instead Pick A Shard count relative to the CPU cores your server",
    "start": "1787039",
    "end": "1792080"
  },
  {
    "text": "has five right size the server and be on the lookout for bottlenecks causing",
    "start": "1792080",
    "end": "1797600"
  },
  {
    "text": "diminishing returns more isn't always better and six don't be afraid of tuning",
    "start": "1797600",
    "end": "1804480"
  },
  {
    "text": "garbage collection as it may help but make sure you've covered the basics before going down that rabbit hole and",
    "start": "1804480",
    "end": "1811240"
  },
  {
    "text": "as seven my final advice is to avoid the too big to fail moments by thinking",
    "start": "1811240",
    "end": "1816279"
  },
  {
    "text": "early about how you'll eventually split large clusters into smaller ones and lastly remember to have fun we certainly",
    "start": "1816279",
    "end": "1824279"
  },
  {
    "text": "appreciated this opportunity to nerd out and we plan to continue pushing the limits of Prometheus as time",
    "start": "1824279",
    "end": "1831519"
  },
  {
    "text": "permits and with that I hope you've enjoyed our",
    "start": "1831519",
    "end": "1836080"
  },
  {
    "text": "talk we have a a few minutes for questions so there's a microphone",
    "start": "1841880",
    "end": "1847320"
  },
  {
    "text": "Center yeah go for it um hey thanks great talk thank you you said you had",
    "start": "1847320",
    "end": "1852399"
  },
  {
    "text": "data retention set to 9 days did you consider lowering it",
    "start": "1852399",
    "end": "1858080"
  },
  {
    "text": "why do you need nine days um the storage",
    "start": "1858080",
    "end": "1863120"
  },
  {
    "text": "is actually pretty cheap so um it wasn't after we disabled compaction historical",
    "start": "1863120",
    "end": "1868600"
  },
  {
    "text": "queries um um it didn't really impact the performance or it wasn't really part",
    "start": "1868600",
    "end": "1873960"
  },
  {
    "text": "of the bottleneck is the short answer I guess okay cool thanks no problem that",
    "start": "1873960",
    "end": "1879080"
  },
  {
    "text": "uh you limit the qu query scrapes like query data samples did you increase it",
    "start": "1879080",
    "end": "1885880"
  },
  {
    "text": "afterwards did your developers like hey we need more uh samples because our",
    "start": "1885880",
    "end": "1891600"
  },
  {
    "text": "query just doesn't work without them um yeah that does happen so uh we",
    "start": "1891600",
    "end": "1898320"
  },
  {
    "text": "tend to offload those expensive queries to um uh to graphon Cloud where it's",
    "start": "1898320",
    "end": "1903880"
  },
  {
    "text": "more it's a distributed system so it can handle the those heavy expensive queries okay",
    "start": "1903880",
    "end": "1910960"
  },
  {
    "text": "thanks did you did you consider getting rid of meus as a normal way and just",
    "start": "1910960",
    "end": "1916159"
  },
  {
    "text": "running as an agent since you were using Cloud yeah we are considering that um",
    "start": "1916159",
    "end": "1921639"
  },
  {
    "text": "for sure um I think the the main thing is that we run all of our recording rules and alerts and we do a lot of",
    "start": "1921639",
    "end": "1927639"
  },
  {
    "text": "control systems as well for autoscaling and a lot of those systems and we just like keeping it simple of knowing how",
    "start": "1927639",
    "end": "1934399"
  },
  {
    "text": "Prometheus will scale and and be reliable and making Auto scaling and all those complicated operations dependent",
    "start": "1934399",
    "end": "1941399"
  },
  {
    "text": "on an external vendor with an external system it it just it's a lot of lot of things to reason about about how it's a",
    "start": "1941399",
    "end": "1948279"
  },
  {
    "text": "trade-off yeah for sure so we we just like to keep it simple um but we are at",
    "start": "1948279",
    "end": "1954200"
  },
  {
    "text": "that point where we're we have to take um horizontal scaling a lot more seriously",
    "start": "1954200",
    "end": "1961159"
  },
  {
    "text": "so I think we have time for a couple more yeah um so you mentioned you were",
    "start": "1964440",
    "end": "1970279"
  },
  {
    "text": "setting go Max procs to 50 despite having like 87 CPUs and you going",
    "start": "1970279",
    "end": "1976159"
  },
  {
    "text": "understand why that was providing better performance I I should probably take that one",
    "start": "1976159",
    "end": "1981960"
  },
  {
    "text": "uh um so it's complicated is the is the",
    "start": "1981960",
    "end": "1987120"
  },
  {
    "text": "full answer but um one of the things that happens in go is it uh it sets up",
    "start": "1987120",
    "end": "1993799"
  },
  {
    "text": "um 25% of that number uh for for running background garbage",
    "start": "1993799",
    "end": "1998880"
  },
  {
    "text": "collection and um so uh so by constraining that number you uh you",
    "start": "1998880",
    "end": "2005200"
  },
  {
    "text": "lower the amount of background garbage collection and uh I actually have a talk",
    "start": "2005200",
    "end": "2010279"
  },
  {
    "text": "at the next cubon in Japan oh anyway cubon Japan I'm giving a talk about Numa",
    "start": "2010279",
    "end": "2016120"
  },
  {
    "text": "uh which is another reason why you might want to do the why you might want to lower that number um yeah it's it's it's",
    "start": "2016120",
    "end": "2021880"
  },
  {
    "text": "a very complicated tuning mechanism but but as a general rule um uh cutting go",
    "start": "2021880",
    "end": "2029480"
  },
  {
    "text": "Max procs down to what you really need uh will make your program go",
    "start": "2029480",
    "end": "2035679"
  },
  {
    "text": "faster so you mentioned the sharting and yeah that can be complex but why not use",
    "start": "2035679",
    "end": "2041080"
  },
  {
    "text": "multiple pruse instances with different scrape configs um we do actually we we have 30",
    "start": "2041080",
    "end": "2048839"
  },
  {
    "text": "UH 60 servers uh in total um and so this is just the the largest tenant that just",
    "start": "2048839",
    "end": "2055398"
  },
  {
    "text": "kind of grew organically this way um and the alerts and uh dashboards they're all",
    "start": "2055399",
    "end": "2062118"
  },
  {
    "text": "kind of in one tenant and so without having to refactor all the alerts and dashboards that's essentially why this",
    "start": "2062119",
    "end": "2068960"
  },
  {
    "text": "one grew to such a certain large size and we we also operate um um some fairly",
    "start": "2068960",
    "end": "2075240"
  },
  {
    "text": "large you know kind of Monolithic services that uh kind of jam a lot into one one",
    "start": "2075240",
    "end": "2081079"
  },
  {
    "text": "service thank you well I think we'll call it there thank you very much for coming",
    "start": "2081079",
    "end": "2088040"
  }
]