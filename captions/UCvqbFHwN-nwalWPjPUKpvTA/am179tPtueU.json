[
  {
    "start": "0",
    "end": "65000"
  },
  {
    "text": "so how you guys is it so this is the last section presentation before the",
    "start": "60",
    "end": "5490"
  },
  {
    "text": "lunchtime so like I think we will be like quick enough to not be late that's",
    "start": "5490",
    "end": "13320"
  },
  {
    "text": "a lunch time so my name is giorgia a cracker yourself my",
    "start": "13320",
    "end": "20340"
  },
  {
    "text": "short name or nickname is Gosha which is much easier and we have Alex traversing cough as",
    "start": "20340",
    "end": "27090"
  },
  {
    "text": "well and so here's a gender for today so we",
    "start": "27090",
    "end": "32130"
  },
  {
    "text": "will just walk through what we are doing who we are so what is our test setup and",
    "start": "32130",
    "end": "37920"
  },
  {
    "text": "then we will explain what we have as a result and what we",
    "start": "37920",
    "end": "43980"
  },
  {
    "text": "were able to achieve in our labs and they're like do a quick summary so what",
    "start": "43980",
    "end": "50940"
  },
  {
    "text": "we should expect from cubensis at scale especially when we are running",
    "start": "50940",
    "end": "57140"
  },
  {
    "text": "applications and not just running some artificial or synthetic tests",
    "start": "57140",
    "end": "63860"
  },
  {
    "text": "so who we are actually we are a members of performance",
    "start": "63860",
    "end": "70920"
  },
  {
    "start": "65000",
    "end": "65000"
  },
  {
    "text": "teams which is a team of 12 engineers at Moran chest so me righteous is a company",
    "start": "70920",
    "end": "77790"
  },
  {
    "text": "who is who was primarily focused on OpenStack",
    "start": "77790",
    "end": "82820"
  },
  {
    "text": "but year ago we started actually looking into author technologists specifically",
    "start": "82820",
    "end": "90210"
  },
  {
    "text": "in soap containers and kubernetes was one of their like",
    "start": "90210",
    "end": "95939"
  },
  {
    "text": "interesting applications which we started to use and the first work with",
    "start": "95939",
    "end": "102780"
  },
  {
    "text": "kubernetes was actually related to running kubernetes on top of OpenStack",
    "start": "102780",
    "end": "109439"
  },
  {
    "text": "so that was our first experience with kubernetes and we were mostly focused on on OpenStack",
    "start": "109439",
    "end": "118290"
  },
  {
    "text": "and we are doing a lot of stuff in OpenStack",
    "start": "118290",
    "end": "123299"
  },
  {
    "text": "community so for example we have performance working group who is focused on the testing of OpenStack scale and",
    "start": "123299",
    "end": "131640"
  },
  {
    "text": "before and we plan to extend this activity on so kubernetes and containers",
    "start": "131640",
    "end": "138620"
  },
  {
    "text": "as well so we are working closely in partnership with Intel and Intel actually provided",
    "start": "138620",
    "end": "146239"
  },
  {
    "text": "us a physical infrastructure for running those tests so all this like scale",
    "start": "146239",
    "end": "151310"
  },
  {
    "text": "testing requires a lot of hardware and it's pretty expensive exercise so thanks",
    "start": "151310",
    "end": "157819"
  },
  {
    "text": "Intel we have like two different data centers so one is rented a truck space",
    "start": "157819",
    "end": "164540"
  },
  {
    "text": "where we have like dedicated physical 250 nodes and we have Intel data center",
    "start": "164540",
    "end": "171560"
  },
  {
    "text": "where we have 500 node level",
    "start": "171560",
    "end": "176440"
  },
  {
    "text": "so Alex can walk your you through the hardware so what we have in this lab and",
    "start": "176709",
    "end": "183590"
  },
  {
    "text": "as you remember so this is insole hardware and it intellect so yeah our",
    "start": "183590",
    "end": "189620"
  },
  {
    "text": "lab based mostly on Adele air 630 servers which are two sockets have",
    "start": "189620",
    "end": "196099"
  },
  {
    "text": "almost of much Xeon v CPUs which are 12 cores physical ones with 12 future ones",
    "start": "196099",
    "end": "203930"
  },
  {
    "start": "203000",
    "end": "203000"
  },
  {
    "text": "have one quarter of terabyte of RAM to",
    "start": "203930",
    "end": "209590"
  },
  {
    "text": "800 SSDs and which is quite more important to one of the one of the",
    "start": "209590",
    "end": "215959"
  },
  {
    "text": "latest in tow it's 710 Nix which kind of giving us a hard time to basically to",
    "start": "215959",
    "end": "222590"
  },
  {
    "text": "figure out the best in the nation of the operation systems of care now new drivers and they operate you kind of",
    "start": "222590",
    "end": "228829"
  },
  {
    "text": "nothing better but they're still kind of not always disabled and you know kind of optimization which we kind of applauding",
    "start": "228829",
    "end": "235220"
  },
  {
    "text": "and stuff ramen which we kind of playing Tony on turning off and working on the",
    "start": "235220",
    "end": "240820"
  },
  {
    "text": "performance so another lab consists of the none of us which are also having kind of",
    "start": "240820",
    "end": "246370"
  },
  {
    "text": "top-notch CPUs raised the same one set up the same Nicks and oh it's person the",
    "start": "246370",
    "end": "253060"
  },
  {
    "text": "same one so here's how like our network looks",
    "start": "253060",
    "end": "260200"
  },
  {
    "start": "257000",
    "end": "257000"
  },
  {
    "text": "like so we have our lives Brian technology used between the racks and using those three and BGP for this",
    "start": "260200",
    "end": "267520"
  },
  {
    "text": "despite the external barrel a the future for doing some other stuff for different",
    "start": "267520",
    "end": "272860"
  },
  {
    "text": "networking solutions vendors but right now using calico so we have such a five",
    "start": "272860",
    "end": "278590"
  },
  {
    "text": "flags that is the biggest for one lap so we're talking about 500 nodes and so that's it we have each and every",
    "start": "278590",
    "end": "286300"
  },
  {
    "text": "server have to do all had Nicks so each",
    "start": "286300",
    "end": "291820"
  },
  {
    "text": "and every cell have for 10g ports which are connected to different switches switches between themselves carbon blood",
    "start": "291820",
    "end": "297250"
  },
  {
    "text": "and that aggregated on the spine since routers we have busy people who",
    "start": "297250",
    "end": "304720"
  },
  {
    "text": "later everywhere kind of I already matchsticks lands but",
    "start": "304720",
    "end": "310750"
  },
  {
    "text": "fix nice right now know you've not used on the streets",
    "start": "310750",
    "end": "315150"
  },
  {
    "start": "317000",
    "end": "317000"
  },
  {
    "text": "right so another important part of performance and scale testing is to be",
    "start": "317310",
    "end": "323890"
  },
  {
    "text": "able to reproduce all those tests and for that we had to create automation and",
    "start": "323890",
    "end": "332320"
  },
  {
    "text": "we heavily rely on the automation in this lab and",
    "start": "332320",
    "end": "337500"
  },
  {
    "text": "the most actually painful part is to do automatic provisioning because we have",
    "start": "337500",
    "end": "343630"
  },
  {
    "text": "to start from bare metal so we have automation which automatically provision all those like",
    "start": "343630",
    "end": "350680"
  },
  {
    "text": "vermin bare metal nodes in different configurations and we can dynamically",
    "start": "350680",
    "end": "356080"
  },
  {
    "text": "split those 500 nose for like difference smaller environments if we need to run",
    "start": "356080",
    "end": "363130"
  },
  {
    "text": "multiple different tests or do some experimental work before we run this at",
    "start": "363130",
    "end": "368979"
  },
  {
    "text": "the larger scale and for provisioning for very metal part we use having it Kobler",
    "start": "368979",
    "end": "376899"
  },
  {
    "text": "we recently started to play with Ubuntu mas which came with Ubuntu 16 so that's",
    "start": "376899",
    "end": "384369"
  },
  {
    "text": "version which we use and 4q burnishers provisioning so once we",
    "start": "384369",
    "end": "390369"
  },
  {
    "text": "have operational system and it's usually Ubuntu 1604 long-term support darshan so for",
    "start": "390369",
    "end": "399309"
  },
  {
    "text": "kubernetes provisioning together with Callie we use",
    "start": "399309",
    "end": "405689"
  },
  {
    "text": "carga which is one of the related projects in cuber nature's ecosystem",
    "start": "405689",
    "end": "413339"
  },
  {
    "text": "so once we have our environment deployed we have like Jenkins which used for",
    "start": "413339",
    "end": "419649"
  },
  {
    "text": "running specific tasks and then we run all those",
    "start": "419649",
    "end": "424800"
  },
  {
    "text": "tests which we we should have and all the results will be collected in several",
    "start": "424800",
    "end": "430599"
  },
  {
    "text": "databases so we have was great databases just like store some aggregated",
    "start": "430599",
    "end": "437709"
  },
  {
    "text": "statistics but for time series we use influx DBM and",
    "start": "437709",
    "end": "443550"
  },
  {
    "text": "once we have all these data we have special tools which will do some analysis so we use our language to do",
    "start": "443550",
    "end": "451719"
  },
  {
    "text": "some statistical research and all those results are automatically published and",
    "start": "451719",
    "end": "459329"
  },
  {
    "text": "generate like aresty documents which we can then upload to the internet almost",
    "start": "459329",
    "end": "466389"
  },
  {
    "text": "automatically so this system also used in OpenStack and that was a community",
    "start": "466389",
    "end": "474969"
  },
  {
    "text": "standard at those time when we started all those activities and",
    "start": "474969",
    "end": "480599"
  },
  {
    "text": "what we are looking for is to extending the this approach for queue branches",
    "start": "480599",
    "end": "486479"
  },
  {
    "text": "community as well so just like high level understanding",
    "start": "486479",
    "end": "491769"
  },
  {
    "text": "what we have in more like graphical formats or in Jenkins system we wished us or all like",
    "start": "491769",
    "end": "498539"
  },
  {
    "text": "jobs and components which I mentioned like Kobler was SM in Flex dB",
    "start": "498539",
    "end": "505880"
  },
  {
    "text": "so when we deploy cuber images we actually rely on the reference",
    "start": "505880",
    "end": "513120"
  },
  {
    "text": "architecture which is implemented in cargo installer and",
    "start": "513120",
    "end": "519709"
  },
  {
    "text": "cargo deploys highly available kubernetes configuration so we have",
    "start": "520280",
    "end": "526140"
  },
  {
    "text": "three nodes which are running each CD cluster and on the on the same nose we",
    "start": "526140",
    "end": "532680"
  },
  {
    "text": "are running cuban writers masters so we have like API services running there we",
    "start": "532680",
    "end": "539580"
  },
  {
    "text": "have like Cuba's scheduler and cargo itself uses",
    "start": "539580",
    "end": "545930"
  },
  {
    "text": "hypercube distribution of kubernetes so as a",
    "start": "545930",
    "end": "551280"
  },
  {
    "text": "recent version specifically what we used in our lab I mentioned here on this",
    "start": "551280",
    "end": "557220"
  },
  {
    "text": "slide and then we have like minions nodes and based on the Ektron the",
    "start": "557220",
    "end": "564050"
  },
  {
    "text": "specific environments it could be like front one sets or 500 physical nodes we",
    "start": "564050",
    "end": "570480"
  },
  {
    "text": "also use virtual machines so we can create virtual environments when we",
    "start": "570480",
    "end": "575940"
  },
  {
    "text": "spawn like up to four or ten VMs their",
    "start": "575940",
    "end": "580980"
  },
  {
    "text": "physical node and then we use them as a minions so that's another option so when",
    "start": "580980",
    "end": "588120"
  },
  {
    "text": "we need to test a large number of cubed",
    "start": "588120",
    "end": "593430"
  },
  {
    "text": "or inches minions and we do not have enough like physical hardware for that we switch to the virtual machines so",
    "start": "593430",
    "end": "602510"
  },
  {
    "text": "testing approach so we have two tools which we heavily using",
    "start": "602930",
    "end": "608870"
  },
  {
    "text": "so we have for OpenStack rally project so this is a tool which was specifically",
    "start": "608870",
    "end": "615150"
  },
  {
    "text": "designed to test OpenStack API so OpenStack project was all about like",
    "start": "615150",
    "end": "621300"
  },
  {
    "text": "api's and we have a tool which allows you to create pretty complex",
    "start": "621300",
    "end": "627410"
  },
  {
    "text": "scenarios and like execute varios API",
    "start": "627410",
    "end": "632790"
  },
  {
    "text": "commands and collect different API related statistics",
    "start": "632790",
    "end": "639290"
  },
  {
    "text": "but it looks like that when you're heavily rely on API it does not",
    "start": "639590",
    "end": "646980"
  },
  {
    "text": "necessarily and it represents the actual state of the running applications so the",
    "start": "646980",
    "end": "653490"
  },
  {
    "text": "next step like beyond API testing is to be able to deploy a special application",
    "start": "653490",
    "end": "661590"
  },
  {
    "text": "which allow you to collect application specific methods so",
    "start": "661590",
    "end": "668820"
  },
  {
    "text": "for example we have a tool which France so called like master or collector which",
    "start": "668820",
    "end": "676260"
  },
  {
    "text": "sits on top of my sequel database and then we run a lot of ports and in each",
    "start": "676260",
    "end": "682860"
  },
  {
    "text": "port were on container which has a simple agent we just reports back to",
    "start": "682860",
    "end": "689610"
  },
  {
    "text": "that collector service I'm up and running and here's my",
    "start": "689610",
    "end": "695070"
  },
  {
    "text": "time stamp when I send this message so based on this tool",
    "start": "695070",
    "end": "700650"
  },
  {
    "text": "we can actually figure out what actual was going on on top of kubernetes",
    "start": "700650",
    "end": "706950"
  },
  {
    "text": "cluster and actually it will show pretty interesting results",
    "start": "706950",
    "end": "713360"
  },
  {
    "text": "further on so just like briefly what we usually",
    "start": "713360",
    "end": "719280"
  },
  {
    "text": "measure so we can measure API response time and there are several blogs on",
    "start": "719280",
    "end": "728390"
  },
  {
    "text": "humanity's dot I you are related to Cuba nature's performance and",
    "start": "728390",
    "end": "733700"
  },
  {
    "text": "they test almost usually queue branches API response",
    "start": "733700",
    "end": "740370"
  },
  {
    "text": "times but also with our like collector tool we",
    "start": "740370",
    "end": "746240"
  },
  {
    "text": "measure the actual time of response from specific containers and we can actually",
    "start": "746240",
    "end": "753360"
  },
  {
    "text": "figure out is there a difference or there is no difference between those like apiary responses about support",
    "start": "753360",
    "end": "761300"
  },
  {
    "text": "originates and actual running application rigid",
    "start": "761300",
    "end": "767390"
  },
  {
    "text": "so as our title says so we are running",
    "start": "767440",
    "end": "772670"
  },
  {
    "text": "this for specific application so during the last year we made a pretty",
    "start": "772670",
    "end": "779810"
  },
  {
    "start": "776000",
    "end": "776000"
  },
  {
    "text": "big effort on moving OpenStack from bare metal to the",
    "start": "779810",
    "end": "787209"
  },
  {
    "text": "micro service application which can be hosted on top of some",
    "start": "787209",
    "end": "793570"
  },
  {
    "text": "platform like system like kubernetes Overland container management system",
    "start": "793570",
    "end": "798740"
  },
  {
    "text": "like kubernetes so OpenStack II itself was created as a serious oriented",
    "start": "798740",
    "end": "805640"
  },
  {
    "text": "architecture application which actually allows us to move it from bare metal and",
    "start": "805640",
    "end": "813140"
  },
  {
    "text": "standard deployment approach to micro services model so there are still some",
    "start": "813140",
    "end": "819940"
  },
  {
    "text": "stateful components in OpenStack it's mostly related to storage and data bases",
    "start": "819940",
    "end": "827089"
  },
  {
    "text": "which OpenStack uses but overall so each API components of OpenStack is like",
    "start": "827089",
    "end": "837370"
  },
  {
    "text": "stateless application and we almost have one-to-one mapping",
    "start": "837370",
    "end": "843440"
  },
  {
    "text": "between service in kubernetes and OpenStack CRS so so far we achieved this configuration",
    "start": "843440",
    "end": "852620"
  },
  {
    "text": "so we took like most frequently-used OpenStack services and",
    "start": "852620",
    "end": "860260"
  },
  {
    "text": "continued eyes them and we have like 19 queue branches services which represents",
    "start": "860260",
    "end": "867829"
  },
  {
    "text": "OpenStack components and each service use replicas stairs and",
    "start": "867829",
    "end": "874420"
  },
  {
    "text": "as we use self orchestrating micro service approach we rely on config Maps",
    "start": "874420",
    "end": "882500"
  },
  {
    "text": "so that's each component which of this application when it starts and running",
    "start": "882500",
    "end": "889040"
  },
  {
    "text": "it will be able to find its own configuration and it will be able to",
    "start": "889040",
    "end": "894920"
  },
  {
    "text": "test our other components which it depends on so",
    "start": "894920",
    "end": "901730"
  },
  {
    "text": "we can achieve this with basic configuration so in the details it looks",
    "start": "901730",
    "end": "906770"
  },
  {
    "text": "like that so this is a specific OpenStack service we just called opens",
    "start": "906770",
    "end": "913160"
  },
  {
    "start": "909000",
    "end": "909000"
  },
  {
    "text": "like Keystone and this is like identity management system for the OpenStack and",
    "start": "913160",
    "end": "920330"
  },
  {
    "text": "as you see we have like deployment we which will create a service service relies on the replica set and",
    "start": "920330",
    "end": "928250"
  },
  {
    "text": "each OpenStack instance I'm sorry OpenStack keys and Keystone instance",
    "start": "928250",
    "end": "934880"
  },
  {
    "text": "will be running as a port and this allows us not only to easily deploy",
    "start": "934880",
    "end": "942650"
  },
  {
    "text": "OpenStack battle also it allows us to do like day two iterations so if we need to",
    "start": "942650",
    "end": "950090"
  },
  {
    "text": "upgrade OpenStack service we just like produce a new image for the Keystone",
    "start": "950090",
    "end": "956680"
  },
  {
    "text": "service and then we ask queue branches to do rolling upgrade and as this",
    "start": "956680",
    "end": "966010"
  },
  {
    "text": "service is stateless it's pretty straightforward to use queue branches",
    "start": "966010",
    "end": "971600"
  },
  {
    "text": "rolling upgrades to perform service upgrade for the open",
    "start": "971600",
    "end": "978100"
  },
  {
    "text": "so we as OpenStack API should be like externally",
    "start": "978100",
    "end": "984260"
  },
  {
    "text": "available we use ingress our external load balancers so distributed like",
    "start": "984260",
    "end": "989960"
  },
  {
    "text": "incoming external traffic ranks among like port so you can see how it looks",
    "start": "989960",
    "end": "998090"
  },
  {
    "text": "like from the queue branches perspective and when we go to the big picture so that's",
    "start": "998090",
    "end": "1005850"
  },
  {
    "start": "1003000",
    "end": "1003000"
  },
  {
    "text": "application like OpenStack which is a massive application and this is how it",
    "start": "1005850",
    "end": "1011950"
  },
  {
    "text": "looks like when it's deployed at large scale so we have",
    "start": "1011950",
    "end": "1017250"
  },
  {
    "text": "three nodes with kubernetes master and we never deploy any other like",
    "start": "1017250",
    "end": "1024720"
  },
  {
    "text": "applications on top of master nodes so cube managers have those physical nodes",
    "start": "1024720",
    "end": "1030579"
  },
  {
    "text": "like dedicated to queue branches poignant we also say that okay less all",
    "start": "1030580",
    "end": "1038780"
  },
  {
    "text": "OpenStack services deploy on their own dedicated knows just because each",
    "start": "1038780",
    "end": "1045550"
  },
  {
    "text": "OpenStack service actually generates pretty huge laws on node so it uses a",
    "start": "1045550",
    "end": "1053330"
  },
  {
    "text": "lot of CPU and memory so it's better to physically figure out what will be the",
    "start": "1053330",
    "end": "1060440"
  },
  {
    "text": "placement rules and we use like no selectors to specify which like",
    "start": "1060440",
    "end": "1067730"
  },
  {
    "text": "deployment topology will use and on each queue branches minyan we run",
    "start": "1067730",
    "end": "1074750"
  },
  {
    "text": "exactly one instance of OpenStack compute because this OpenStack compute",
    "start": "1074750",
    "end": "1082270"
  },
  {
    "text": "component will manage Lib the earth and KVM hypervisor on top of all this",
    "start": "1082270",
    "end": "1090370"
  },
  {
    "text": "physical machine so we need to have exactly one instance of OpenStack",
    "start": "1090370",
    "end": "1096650"
  },
  {
    "text": "compute on on each physical node because that's the process we which will manage",
    "start": "1096650",
    "end": "1102170"
  },
  {
    "text": "underlying hypervisor on the operational system well",
    "start": "1102170",
    "end": "1109480"
  },
  {
    "text": "very good so let's go to the interesting part results",
    "start": "1109630",
    "end": "1117790"
  },
  {
    "text": "so alex is a original author of this",
    "start": "1117790",
    "end": "1124360"
  },
  {
    "start": "1118000",
    "end": "1118000"
  },
  {
    "text": "collector system so that's the results which we collected by this tool so Alex",
    "start": "1124360",
    "end": "1131870"
  },
  {
    "text": "if you can walk through what we did yeah okay so here's the dependency of the",
    "start": "1131870",
    "end": "1137630"
  },
  {
    "text": "replica replica set creation time versus replica set size so as you can probably",
    "start": "1137630",
    "end": "1142850"
  },
  {
    "text": "see it's almost invisible and there is a kind of replica set aside it's only one replica set it to mention so if a burkha",
    "start": "1142850",
    "end": "1150050"
  },
  {
    "text": "set size is 100 volts so it's kind of quite quite fast 500 volts also kind of",
    "start": "1150050",
    "end": "1156170"
  },
  {
    "text": "rigid quite fast so it's kind of the time is seconds for their 1,000 2,000 5,000 10,000 as",
    "start": "1156170",
    "end": "1163940"
  },
  {
    "text": "you can probably see it's pretty linear so scheduling of the ports in in in terms of one replica set is pretty",
    "start": "1163940",
    "end": "1169730"
  },
  {
    "text": "linear it's pretty fun thing to take a look on their percentile it's kind of a",
    "start": "1169730",
    "end": "1174980"
  },
  {
    "text": "problem you see that this person percentile is kind of pretty interesting it's kind of quite like like it should",
    "start": "1174980",
    "end": "1180410"
  },
  {
    "text": "be so which is kind of very very unusual so I'm gonna probably try to explain what's happening yeah so before this let",
    "start": "1180410",
    "end": "1188030"
  },
  {
    "text": "me so those results are pretty consistent",
    "start": "1188030",
    "end": "1194350"
  },
  {
    "text": "with what you may see on the queue branches dot I our research so and",
    "start": "1194350",
    "end": "1201230"
  },
  {
    "text": "that's pretty much one-to-one mapping between the test type so on cue branches",
    "start": "1201230",
    "end": "1209150"
  },
  {
    "text": "blog they were running like 10 replica sets and they were like measuring time",
    "start": "1209150",
    "end": "1215360"
  },
  {
    "text": "off for replica set deployments there so number",
    "start": "1215360",
    "end": "1221270"
  },
  {
    "text": "of odd so we replicated this",
    "start": "1221270",
    "end": "1226690"
  },
  {
    "text": "scenario and we pretty much confirms as the the linear dependency between",
    "start": "1226690",
    "end": "1234430"
  },
  {
    "text": "replicas size and deployment time is like our",
    "start": "1234430",
    "end": "1239590"
  },
  {
    "text": "concurrent e in our experiments on physical nodes so in this test we were",
    "start": "1239590",
    "end": "1246200"
  },
  {
    "text": "running Q branches on physical hardware so this is like five hundred nodes Q branches cluster it's not a virtual",
    "start": "1246200",
    "end": "1253040"
  },
  {
    "text": "machines and the interesting difference as Alex",
    "start": "1253040",
    "end": "1258320"
  },
  {
    "text": "mentioned we have in percentiles so this 50 percentile is not exactly the",
    "start": "1258320",
    "end": "1268060"
  },
  {
    "text": "median of this big 99 and new percentile",
    "start": "1268060",
    "end": "1273230"
  },
  {
    "text": "bar and that's pretty interesting effect when you start looking into details and",
    "start": "1273230",
    "end": "1282500"
  },
  {
    "start": "1282000",
    "end": "1282000"
  },
  {
    "text": "start looking into dynamic so how those ports were created and so is our like collector tool we",
    "start": "1282500",
    "end": "1290630"
  },
  {
    "text": "have and ability to figure out which sport reported at what time so you will",
    "start": "1290630",
    "end": "1296550"
  },
  {
    "text": "see some nice plotters and by accident or not but those plotters",
    "start": "1296550",
    "end": "1304760"
  },
  {
    "text": "specifically related so number of 500 which is our number of physical",
    "start": "1304760",
    "end": "1311460"
  },
  {
    "text": "nodes or number of minions I don't know is there any relation in",
    "start": "1311460",
    "end": "1319970"
  },
  {
    "text": "scheduling or but that's like actual natural experiment which we did and we",
    "start": "1319970",
    "end": "1326940"
  },
  {
    "text": "saw those dependencies so the the reason what why like 58th percentile is not",
    "start": "1326940",
    "end": "1332760"
  },
  {
    "text": "exactly the median of the 99% whole bar just because",
    "start": "1332760",
    "end": "1339660"
  },
  {
    "text": "of those plateaus so as you see we were trying to create 1,000 nodes",
    "start": "1339660",
    "end": "1346970"
  },
  {
    "text": "1000 ports and at 500 ports",
    "start": "1346970",
    "end": "1352010"
  },
  {
    "text": "scheduling was stopped almost for 10 seconds and then it start working",
    "start": "1352010",
    "end": "1360900"
  },
  {
    "text": "again and then it stopped again for specific amount of seconds and then it's",
    "start": "1360900",
    "end": "1366090"
  },
  {
    "text": "like proceeding it looks like this there is something in scattering which",
    "start": "1366090",
    "end": "1372480"
  },
  {
    "text": "actually creates those gaps and we will see those effects on other tests",
    "start": "1372480",
    "end": "1380640"
  },
  {
    "text": "but when we say linear yes it lie near when you have a bigger scale so when",
    "start": "1380640",
    "end": "1387330"
  },
  {
    "text": "you're at like 10,000 nodes all those gaps will will stay like closer and closer so a bigger scale you will see a",
    "start": "1387330",
    "end": "1395400"
  },
  {
    "text": "nice like line with some small like deviations from the linear law but a",
    "start": "1395400",
    "end": "1403800"
  },
  {
    "text": "smaller scale those effects are pretty not small",
    "start": "1403800",
    "end": "1410510"
  },
  {
    "text": "and when we go to the bigger scale so that's our another experiment when we're",
    "start": "1410800",
    "end": "1418820"
  },
  {
    "text": "we were performing rolling upgrades so on intial phase we",
    "start": "1418820",
    "end": "1427580"
  },
  {
    "text": "deployed an application set with 500 pods and then we created new image",
    "start": "1427580",
    "end": "1439340"
  },
  {
    "text": "uploaded to register and then we asked cube images to perform rolling upgrades",
    "start": "1439340",
    "end": "1447170"
  },
  {
    "start": "1444000",
    "end": "1444000"
  },
  {
    "text": "as a result of parameters like update periods and falling interval we have",
    "start": "1447170",
    "end": "1454610"
  },
  {
    "text": "this nice straight line which actually says as this rolling upgrades like works as like",
    "start": "1454610",
    "end": "1463460"
  },
  {
    "text": "pioneer algorithm by one it was like racing those sports and there is no any",
    "start": "1463460",
    "end": "1469160"
  },
  {
    "text": "like anomalies seen in this process and as you see it when we like increase the",
    "start": "1469160",
    "end": "1475970"
  },
  {
    "text": "scale all those problems with steps and are not noticeable so like a few words",
    "start": "1475970",
    "end": "1482600"
  },
  {
    "text": "about it so we exactly mention it what the parameters we did we do used because the default ones is a little bit",
    "start": "1482600",
    "end": "1489050"
  },
  {
    "text": "different so a big period will be three seconds and the POE interval as far as the remember is one second so because of",
    "start": "1489050",
    "end": "1494780"
  },
  {
    "text": "the pole interval is kind of slightly increased by default its upgrades will",
    "start": "1494780",
    "end": "1500030"
  },
  {
    "text": "be happens a little bit kind of slower so that parameters will allow you to put",
    "start": "1500030",
    "end": "1505040"
  },
  {
    "text": "all these updates I mean probably somewhere around 20 minutes so for for 500",
    "start": "1505040",
    "end": "1513310"
  },
  {
    "text": "size of the replica set but if you go with default ones it would be kind of",
    "start": "1513310",
    "end": "1518540"
  },
  {
    "text": "either maybe 5 maybe 10 times so it's just two parameters because the polling gets real as far as I see as container",
    "start": "1518540",
    "end": "1525050"
  },
  {
    "text": "is up and running the new portals are open right it decided to go and great another one so that's parameters very",
    "start": "1525050",
    "end": "1531350"
  },
  {
    "text": "very important if you plan to do that faster it depends on use cases yeah",
    "start": "1531350",
    "end": "1537970"
  },
  {
    "text": "okay so the next step for us was to figure out",
    "start": "1538330",
    "end": "1544250"
  },
  {
    "text": "if they're in your difference if we use node selector because from like our",
    "start": "1544250",
    "end": "1551380"
  },
  {
    "start": "1546000",
    "end": "1546000"
  },
  {
    "text": "intuition we were like thinking that probably this",
    "start": "1551380",
    "end": "1556730"
  },
  {
    "text": "will somehow affect the the process of scheduling",
    "start": "1556730",
    "end": "1562900"
  },
  {
    "text": "and as our application heavily relies on the north selector that was pretty",
    "start": "1562900",
    "end": "1570440"
  },
  {
    "text": "important for us so we're on those tests and actually within like the error of",
    "start": "1570440",
    "end": "1578420"
  },
  {
    "text": "measurements there is no difference so there are still some like plateaus during the scheduling process but from",
    "start": "1578420",
    "end": "1588170"
  },
  {
    "text": "like time perspective there is no any effect of the using of nodes selector on",
    "start": "1588170",
    "end": "1594410"
  },
  {
    "text": "the scheduling process but when we go to more complex scenarios",
    "start": "1594410",
    "end": "1602330"
  },
  {
    "text": "which you might see actually in applications so here we have multiple",
    "start": "1602330",
    "end": "1609110"
  },
  {
    "start": "1607000",
    "end": "1607000"
  },
  {
    "text": "replicas says with 100 ports in each one",
    "start": "1609110",
    "end": "1614300"
  },
  {
    "text": "and those replicas sets were actually",
    "start": "1614300",
    "end": "1620960"
  },
  {
    "text": "different in a single llamo file so this is a single cube CTL commands to create",
    "start": "1620960",
    "end": "1628160"
  },
  {
    "text": "this configuration and we were expecting that those",
    "start": "1628160",
    "end": "1634370"
  },
  {
    "text": "replicas sets will be created more or less in parallel mode well that's",
    "start": "1634370",
    "end": "1639650"
  },
  {
    "text": "actually not the case so it's not a parallel process so initially it looks",
    "start": "1639650",
    "end": "1648020"
  },
  {
    "text": "like a sequential process so the first replica",
    "start": "1648020",
    "end": "1653900"
  },
  {
    "text": "set was created then the next one started so there is a little bit overlap",
    "start": "1653900",
    "end": "1659570"
  },
  {
    "text": "so it's not exactly one by another but this overlap is pretty small and the",
    "start": "1659570",
    "end": "1667160"
  },
  {
    "text": "time gap between replicas set creation time is actually significant so another",
    "start": "1667160",
    "end": "1675080"
  },
  {
    "text": "interesting effect which we can see on this picture I'm sorry is",
    "start": "1675080",
    "end": "1683230"
  },
  {
    "text": "is grouping so it looks like that the there is some",
    "start": "1683230",
    "end": "1690169"
  },
  {
    "text": "scheduling algorithm or somewhat processing algorithm in queue branches",
    "start": "1690169",
    "end": "1695320"
  },
  {
    "text": "which actually process several a few replicas sets at once so we see a",
    "start": "1695320",
    "end": "1704149"
  },
  {
    "text": "grouping like three or four replicas sets which are created almost",
    "start": "1704149",
    "end": "1709690"
  },
  {
    "text": "simultaneously or at least almost together but between groups there",
    "start": "1709690",
    "end": "1717500"
  },
  {
    "text": "are still gaps in time and this actually may lead to",
    "start": "1717500",
    "end": "1724510"
  },
  {
    "text": "potential effects when you use a lot of replica sets in your application so in",
    "start": "1724600",
    "end": "1732470"
  },
  {
    "text": "in our case when we have nineteen replicas sets which we create",
    "start": "1732470",
    "end": "1738070"
  },
  {
    "text": "simultaneously for open we actually expect that it will be created",
    "start": "1738070",
    "end": "1743620"
  },
  {
    "text": "simultaneously and then our components will figure out they are like dependency",
    "start": "1743620",
    "end": "1749450"
  },
  {
    "text": "and orchestrates himself through the EGCG which we use to figure out who is",
    "start": "1749450",
    "end": "1755960"
  },
  {
    "text": "already configured so where is allocation so on and so forth and it",
    "start": "1755960",
    "end": "1761899"
  },
  {
    "text": "should take some specific amount of time but what we see in the result is quite",
    "start": "1761899",
    "end": "1769490"
  },
  {
    "text": "interesting so because of this grouping effect which is not actually deterministic so we don't know which",
    "start": "1769490",
    "end": "1776889"
  },
  {
    "start": "1770000",
    "end": "1770000"
  },
  {
    "text": "replicas yes will be grouping together so we have an interesting problem and",
    "start": "1776889",
    "end": "1784090"
  },
  {
    "text": "which actually creates an effect of prolonged",
    "start": "1784090",
    "end": "1789850"
  },
  {
    "text": "deployment so within first minutes like specific almost random sets were created",
    "start": "1789850",
    "end": "1800210"
  },
  {
    "text": "and so all those components have actually dependencies on the show",
    "start": "1800210",
    "end": "1805940"
  },
  {
    "text": "offices like RabbitMQ which is used as a message buzz between all those",
    "start": "1805940",
    "end": "1811039"
  },
  {
    "text": "components so all those services which were created on first miners' they were",
    "start": "1811039",
    "end": "1816860"
  },
  {
    "text": "waiting for the availability of the ribbet MQ service and components which were",
    "start": "1816860",
    "end": "1824990"
  },
  {
    "text": "created only on 3rd minute just like because of this grouping effect and",
    "start": "1824990",
    "end": "1831019"
  },
  {
    "text": "because it's not actually created in parallel mode so in ideal situation and that's what we",
    "start": "1831019",
    "end": "1839539"
  },
  {
    "text": "expected initially that when we create all those replicas set in at least one",
    "start": "1839539",
    "end": "1846169"
  },
  {
    "text": "port will be created almost martini ously and then we will have processes running services will actually detect",
    "start": "1846169",
    "end": "1853669"
  },
  {
    "text": "this replica set in house situation and and all this like self ARCIC",
    "start": "1853669",
    "end": "1860799"
  },
  {
    "text": "orchestration will happen but actually as a result of those delays in the",
    "start": "1860799",
    "end": "1867019"
  },
  {
    "text": "replica set creation all the all the orchestration",
    "start": "1867019",
    "end": "1872980"
  },
  {
    "text": "started almost at the like 4th minute when all those replica controls were",
    "start": "1872980",
    "end": "1880570"
  },
  {
    "text": "created so that actually might be interesting effect on the application",
    "start": "1880570",
    "end": "1888200"
  },
  {
    "text": "deployment and this was caused by this scheduling mechanism or like a",
    "start": "1888200",
    "end": "1894289"
  },
  {
    "text": "replication set mechanism right we almost there yep so for the",
    "start": "1894289",
    "end": "1902720"
  },
  {
    "start": "1898000",
    "end": "1898000"
  },
  {
    "text": "mint working that's for its expectable results so we just did a few tests for the figure out how the whisky de we so",
    "start": "1902720",
    "end": "1909799"
  },
  {
    "text": "just to mention that the using Calico right now with the kubernetes so for the",
    "start": "1909799",
    "end": "1916549"
  },
  {
    "text": "networking assumes that kind of you almost get the wire speed v is kind of precious moment to use or standard MTU",
    "start": "1916549",
    "end": "1922720"
  },
  {
    "text": "1500 and it's kind of limited by kind of you see",
    "start": "1922720",
    "end": "1928879"
  },
  {
    "text": "speed of two links because kind of fusing commitments in this fallings warning some using kind of two bonds",
    "start": "1928879",
    "end": "1935210"
  },
  {
    "text": "from all of that and it's kind of perfect try to use for we have to go and maintain the different ports so create",
    "start": "1935210",
    "end": "1943400"
  },
  {
    "text": "based adjust multi-threaded log between the different parts so we tried that we see the expectable that sorts will be",
    "start": "1943400",
    "end": "1949070"
  },
  {
    "text": "about statistics oh you give it's kind of close to our speed so you just kind of 90 percents fine",
    "start": "1949070",
    "end": "1955990"
  },
  {
    "text": "so let's go to a summary yes so summary is based on",
    "start": "1955990",
    "end": "1962980"
  },
  {
    "text": "what we saw but still you need to remember that statistics might actually",
    "start": "1962980",
    "end": "1969470"
  },
  {
    "start": "1963000",
    "end": "1963000"
  },
  {
    "text": "show different results may have like different interpretations but",
    "start": "1969470",
    "end": "1975910"
  },
  {
    "text": "first of all when you look on to basic like performance test like we have",
    "start": "1975910",
    "end": "1981590"
  },
  {
    "text": "published on cue branches dot io that says okay we can easily create like a",
    "start": "1981590",
    "end": "1987410"
  },
  {
    "text": "thousand of ports in cuban it says that's all true except so there are",
    "start": "1987410",
    "end": "1994310"
  },
  {
    "text": "other like effects which are not covered by those tests and you cannot actually",
    "start": "1994310",
    "end": "2000970"
  },
  {
    "text": "rely solely on those tests who predict the time of deployment of your",
    "start": "2000970",
    "end": "2008310"
  },
  {
    "text": "application so as you saw there are some another effects based on the replica set",
    "start": "2008310",
    "end": "2015160"
  },
  {
    "text": "numbers or any other like object numbers which cuban adjust might have and your",
    "start": "2015160",
    "end": "2022020"
  },
  {
    "text": "projections might not be very accurate so from networking sides",
    "start": "2022020",
    "end": "2029470"
  },
  {
    "text": "we didn't see much effects but there are different there are some",
    "start": "2029470",
    "end": "2036420"
  },
  {
    "text": "specifically we saw problems with service discovery so when we were",
    "start": "2036420",
    "end": "2043210"
  },
  {
    "text": "running large amounts of workloads which generated traffic obviously cube dns subsystem was",
    "start": "2043210",
    "end": "2051250"
  },
  {
    "text": "overloaded and we saw like delay in the cuban artists service discovery",
    "start": "2051250",
    "end": "2060030"
  },
  {
    "text": "scheduling is linear but still as usual there are other effects wishing to take",
    "start": "2061169",
    "end": "2068398"
  },
  {
    "text": "into account when you plan your application deployment",
    "start": "2068399",
    "end": "2074299"
  },
  {
    "text": "and because of like replica set behaviour when when you can rate",
    "start": "2074299",
    "end": "2080429"
  },
  {
    "text": "yourself for orchestrating application you might need to take into",
    "start": "2080429",
    "end": "2086849"
  },
  {
    "text": "account those effects",
    "start": "2086849",
    "end": "2090230"
  },
  {
    "text": "and there were few issues which we mentions",
    "start": "2091909",
    "end": "2097108"
  },
  {
    "text": "when we were running those tests and those tests were running on the bare",
    "start": "2097109",
    "end": "2102180"
  },
  {
    "text": "metal so first of all docker running in production might be tricky so we saw",
    "start": "2102180",
    "end": "2110460"
  },
  {
    "text": "multiple instances of problems when docker process was hanging so and the",
    "start": "2110460",
    "end": "2117420"
  },
  {
    "text": "typical symptoms you see you're like cuba in",
    "start": "2117420",
    "end": "2122430"
  },
  {
    "text": "sports in status benzene or like creating but nothing actually happens",
    "start": "2122430",
    "end": "2127950"
  },
  {
    "text": "and then you like login to the system to the docker PS and it just like things and",
    "start": "2127950",
    "end": "2135619"
  },
  {
    "text": "the only solution for that is to restart the cure process which might",
    "start": "2135619",
    "end": "2142200"
  },
  {
    "text": "affect your running or all region not running containers on this node and",
    "start": "2142200",
    "end": "2148290"
  },
  {
    "text": "queue branches itself does not handle this situation very well so just like",
    "start": "2148290",
    "end": "2155369"
  },
  {
    "text": "way and waits and there is no like communication between docker and you",
    "start": "2155369",
    "end": "2161190"
  },
  {
    "text": "know queue branches but a cube crisis will never reschedule on such force and",
    "start": "2161190",
    "end": "2166710"
  },
  {
    "text": "that might be a huge problem at large scale when you have like production",
    "start": "2166710",
    "end": "2172319"
  },
  {
    "text": "environment so when you use calico or any other",
    "start": "2172319",
    "end": "2180500"
  },
  {
    "text": "networking solutions troubleshooting is a nightmare so they use a lot of like",
    "start": "2180500",
    "end": "2186450"
  },
  {
    "text": "underlying technologies bgp IP tables and other side and is",
    "start": "2186450",
    "end": "2192250"
  },
  {
    "text": "without like proper tools it's almost impossible to properly like trace your",
    "start": "2192250",
    "end": "2198099"
  },
  {
    "text": "networking stuff especially when guys went like developers created their",
    "start": "2198099",
    "end": "2203150"
  },
  {
    "text": "contingency is trying to reduce the image size so they remove all like networking tools like",
    "start": "2203150",
    "end": "2209859"
  },
  {
    "text": "yesterday we were trying to troubles with one of those problems and we had no like pink commands in the container so",
    "start": "2209859",
    "end": "2216380"
  },
  {
    "text": "we were unable to do anything so that might be a big problem so don't",
    "start": "2216380",
    "end": "2222260"
  },
  {
    "text": "forget about like troubleshooting tools because it might save you a lot of time",
    "start": "2222260",
    "end": "2227540"
  },
  {
    "text": "and cube analysis itself is not very good with diagnosis and",
    "start": "2227540",
    "end": "2233920"
  },
  {
    "text": "collecting all those like problematic areas so we saw several instances of",
    "start": "2233920",
    "end": "2239390"
  },
  {
    "text": "problems where TNS we know we're not updated and cue",
    "start": "2239390",
    "end": "2246829"
  },
  {
    "text": "branches were unable to do anything or we saw an instance where when",
    "start": "2246829",
    "end": "2251950"
  },
  {
    "text": "cube richest node was actually like cubensis me on",
    "start": "2251950",
    "end": "2257650"
  },
  {
    "text": "was not reporting back to the queue branches master and cue branches was",
    "start": "2257650",
    "end": "2263630"
  },
  {
    "text": "unable to remove ports from this minion and we were trying to remove this",
    "start": "2263630",
    "end": "2270650"
  },
  {
    "text": "namespace and this never happened because extermination process was waiting for those sports and kubernetes",
    "start": "2270650",
    "end": "2277670"
  },
  {
    "text": "was unable to do anything and it's hard to diagnose those problem so it will",
    "start": "2277670",
    "end": "2284390"
  },
  {
    "text": "require a lot of efforts before you figure out what's the root cause so the last part like as we were engaged",
    "start": "2284390",
    "end": "2293960"
  },
  {
    "text": "with insole stuff so I have to pose like Lego not just about like performance",
    "start": "2293960",
    "end": "2300740"
  },
  {
    "text": "results in their dependency on the hardware so it's right there",
    "start": "2300740",
    "end": "2307630"
  },
  {
    "text": "so here's the links so here's a tool it's publicly available on the github",
    "start": "2307630",
    "end": "2313490"
  },
  {
    "text": "you can use it so and we have a lot of queue branches and",
    "start": "2313490",
    "end": "2320030"
  },
  {
    "text": "OpenStack related star on the OpenStack performance documentation page so you",
    "start": "2320030",
    "end": "2328510"
  },
  {
    "text": "are welcome to go and see what we have there so there are interesting like",
    "start": "2328510",
    "end": "2333869"
  },
  {
    "text": "campers and so two branches OpenStack components like environmental provisioning system so it's not",
    "start": "2333869",
    "end": "2340359"
  },
  {
    "text": "specifically related to a mobile sector okay that is thank you",
    "start": "2340359",
    "end": "2347580"
  },
  {
    "text": "questions [Applause]",
    "start": "2347580",
    "end": "2354420"
  },
  {
    "text": "we end up with creating Pigsy server like with cobbler which would strap all",
    "start": "2405010",
    "end": "2412030"
  },
  {
    "text": "those machines we are eBay my interface with a small initial image and then",
    "start": "2412030",
    "end": "2418839"
  },
  {
    "text": "inside this image we are running torrent based system which actually",
    "start": "2418839",
    "end": "2424660"
  },
  {
    "text": "pulls actual aggression all system image and this actually allows us to",
    "start": "2424660",
    "end": "2429700"
  },
  {
    "text": "distribute the networking traffic",
    "start": "2429700",
    "end": "2435088"
  },
  {
    "text": "yes",
    "start": "2436890",
    "end": "2439890"
  },
  {
    "text": "because we were limited to specific like hardware configuration which we hear",
    "start": "2442530",
    "end": "2447540"
  },
  {
    "text": "there's a torrent distribution kind of allows us to basically distribute traffic between the nodes between as",
    "start": "2447540",
    "end": "2453700"
  },
  {
    "text": "soon as the node downloads the image is starting to distributed",
    "start": "2453700",
    "end": "2458640"
  },
  {
    "text": "so when we run tests we actually were on",
    "start": "2464070",
    "end": "2469599"
  },
  {
    "text": "the initial like warming route we round which downloads all the images to the",
    "start": "2469599",
    "end": "2475690"
  },
  {
    "text": "minions and then we rerun those tests so it's it's pure time of kubernetes and",
    "start": "2475690",
    "end": "2481619"
  },
  {
    "text": "dijkers startup so it's not specifically like bounce to the image pool yeah so we",
    "start": "2481619",
    "end": "2491250"
  },
  {
    "text": "ah I don't think so no no right now no it's",
    "start": "2497829",
    "end": "2504729"
  },
  {
    "text": "not tuned in this way but as I know it's a roadmap we use load balancer in front",
    "start": "2504729",
    "end": "2509799"
  },
  {
    "text": "of register obviously yeah but not in front of queue",
    "start": "2509799",
    "end": "2515829"
  },
  {
    "text": "branches so we have pretty like powerful hardware so at 500 node we didn't see",
    "start": "2515829",
    "end": "2522489"
  },
  {
    "text": "any problems with running Cuba in Cuba native master or on a node and so like",
    "start": "2522489",
    "end": "2529289"
  },
  {
    "text": "redirecting all the traffic to this no it's powerful machines like 12",
    "start": "2529289",
    "end": "2534749"
  },
  {
    "text": "cores and a lot of memories so Cuban it is pretty capable to handle all this lot",
    "start": "2534749",
    "end": "2541809"
  },
  {
    "text": "and the only thing that's really kind of thinking about that we have a limited resource on and the scheduling so there",
    "start": "2541809",
    "end": "2548109"
  },
  {
    "text": "is no reason there is only development going on the motor scheduling can kubernetes so we kind of waiting for you",
    "start": "2548109",
    "end": "2553390"
  },
  {
    "text": "because it should change the situation dramatically boss mm-hmm",
    "start": "2553390",
    "end": "2558269"
  },
  {
    "text": "yes",
    "start": "2564380",
    "end": "2567380"
  },
  {
    "text": "rally might help but it's not specifically a diagnostic tool right so there should be a set of tools",
    "start": "2578539",
    "end": "2586490"
  },
  {
    "text": "probably as a part of clearances distribution which are specific to cue branches problems right as well as for",
    "start": "2586490",
    "end": "2593700"
  },
  {
    "text": "networking so it will so if your run like things in production for",
    "start": "2593700",
    "end": "2600420"
  },
  {
    "text": "troubleshooting you'll probably like have to have a special image which you can like quickly start",
    "start": "2600420",
    "end": "2607099"
  },
  {
    "text": "continuous and this image will have all like necessary tools for networking",
    "start": "2607099",
    "end": "2612390"
  },
  {
    "text": "promotion because like in typical situation in standard container there is",
    "start": "2612390",
    "end": "2617940"
  },
  {
    "text": "nothing at you you actually can do anything it's for the kubernetes itself",
    "start": "2617940",
    "end": "2623430"
  },
  {
    "text": "I mean like refusing service so using H a kind of not easy but all bones including KP tables for that is some",
    "start": "2623430",
    "end": "2630299"
  },
  {
    "text": "kind of chance of sending there's some particular kind of session to some budgetary server and it's kind of like",
    "start": "2630299",
    "end": "2636660"
  },
  {
    "text": "be playing this 1000 know some of them are populated with some sometimes just 1000 service at the same time so we have",
    "start": "2636660",
    "end": "2642779"
  },
  {
    "text": "1000 records of the IP tables for the service itself and there's a lot of probabilities they different ones and",
    "start": "2642779",
    "end": "2649440"
  },
  {
    "text": "it's kind of funny I mean if one of these services are not working and if",
    "start": "2649440",
    "end": "2654509"
  },
  {
    "text": "the IP tables decided to scatter our name to this particular kind of basic in translate this particular request this",
    "start": "2654509",
    "end": "2659519"
  },
  {
    "text": "particular session to the series you don't ever know about it because next one will go to the next one and that is kind of well very very hard to figure",
    "start": "2659519",
    "end": "2667079"
  },
  {
    "text": "out but what's going on because the tokete which was written it was written without kind of this is not just",
    "start": "2667079",
    "end": "2673950"
  },
  {
    "text": "kind of trying to figure out how it works but it's written in a way that is real frail and any unpredictable thing",
    "start": "2673950",
    "end": "2679799"
  },
  {
    "text": "if something is wrong so in this case one of the DNS services just sometimes is lock blocks to region I mean using",
    "start": "2679799",
    "end": "2687720"
  },
  {
    "text": "consumer producer/consumer way so it's kind of fit segregate the requests and put them to the MySQL to decrease the cost of the",
    "start": "2687720",
    "end": "2693490"
  },
  {
    "text": "doing that and increase the responsiveness so in this case there is each and every consumer or try to",
    "start": "2693490",
    "end": "2698610"
  },
  {
    "text": "create connection to my skills also be on our case it's 96 consumers because of",
    "start": "2698610",
    "end": "2704410"
  },
  {
    "text": "the twice of the number of virtual CPUs and one of them will fail which one tools is just going exist so I mean in",
    "start": "2704410",
    "end": "2710650"
  },
  {
    "text": "case of the application we all done with some in this particular rate so just die and if you never know what was going on",
    "start": "2710650",
    "end": "2718440"
  },
  {
    "text": "any questions",
    "start": "2719490",
    "end": "2722880"
  },
  {
    "text": "no so the test was actually between multiple between minions so the traffic",
    "start": "2739319",
    "end": "2745329"
  },
  {
    "text": "was flowing between means so there is no any workload which is scattered on the",
    "start": "2745329",
    "end": "2751720"
  },
  {
    "text": "queue branches master and we specifically placed those containers on the nose",
    "start": "2751720",
    "end": "2758079"
  },
  {
    "text": "which are in different tracks because then it will flow through the spine instead of like being bounded by the top",
    "start": "2758079",
    "end": "2766089"
  },
  {
    "text": "four of the rocks wish",
    "start": "2766089",
    "end": "2769140"
  },
  {
    "text": "we did so first of all obviously it's not like very extensive test so it's",
    "start": "2774240",
    "end": "2779950"
  },
  {
    "text": "just like basic one so we didn't see a huge amount of progress but when we were",
    "start": "2779950",
    "end": "2788020"
  },
  {
    "text": "running multiple composable containers so we saw an even distribution of the",
    "start": "2788020",
    "end": "2795089"
  },
  {
    "text": "throughput between containers so you might see the situation when overall you",
    "start": "2795089",
    "end": "2800650"
  },
  {
    "text": "have 18 gigabits per second but one container will have like only 30",
    "start": "2800650",
    "end": "2807280"
  },
  {
    "text": "megabits and another one will have like 4 gigabit and another one like one",
    "start": "2807280",
    "end": "2812680"
  },
  {
    "text": "gigabit so that's more advanced scenarios and requires a lot of like",
    "start": "2812680",
    "end": "2818589"
  },
  {
    "text": "specific testing for the networking but also solve those problems because yeah",
    "start": "2818589",
    "end": "2825069"
  },
  {
    "text": "because you go into the container work you kind of you usually assume that you almost out even kind of much more containers because a bullet overhead",
    "start": "2825069",
    "end": "2831609"
  },
  {
    "text": "much modern VMs but in this particular case that each of them create each and",
    "start": "2831609",
    "end": "2837369"
  },
  {
    "text": "every application will create different footprint on the networking so I probably have to come up with some cure",
    "start": "2837369",
    "end": "2842710"
  },
  {
    "text": "stuff in networking things I mean on an important provider for the kubernetes so in calico there is no such thing and the",
    "start": "2842710",
    "end": "2849550"
  },
  {
    "text": "rest have to I mean not much I don't know extra kind of solution which kind of fully implements or requirements which I'm kind of very very will be used",
    "start": "2849550",
    "end": "2855700"
  },
  {
    "text": "from continue based kind of deployments ok cool thank you guys thank you for",
    "start": "2855700",
    "end": "2863890"
  },
  {
    "text": "coming that's lunch time my let's not miss our lunch thank you",
    "start": "2863890",
    "end": "2871950"
  }
]