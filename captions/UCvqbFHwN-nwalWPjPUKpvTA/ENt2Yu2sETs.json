[
  {
    "text": "hello everybody Welcome to uh another maintenance track session this time",
    "start": "659",
    "end": "6299"
  },
  {
    "text": "working through batch what's new and what's next my name is Aldo cookie konder I'm a TL at six scheduling uh I'm",
    "start": "6299",
    "end": "14820"
  },
  {
    "text": "a active member of the working group batch hi everyone I'm Swati Sagar I'm a",
    "start": "14820",
    "end": "20039"
  },
  {
    "text": "principal software engineer working for red hat and I've been involved in the resource management space for a few",
    "start": "20039",
    "end": "25740"
  },
  {
    "text": "the focus has been on resource Focus has been on Numa where scheduling as well as",
    "start": "25740",
    "end": "31019"
  },
  {
    "text": "resource managers and I'm happy to be here and happy to engage with you guys",
    "start": "31019",
    "end": "37460"
  },
  {
    "text": "um so first first of all I want to want to start by explaining what a working group is within kubernetes for those of",
    "start": "39300",
    "end": "46440"
  },
  {
    "text": "you that might not be familiar hopefully you are familiar with with the six six are groups within special interest",
    "start": "46440",
    "end": "53160"
  },
  {
    "text": "groups within kubernetes as well that have each responsibility to maintain a",
    "start": "53160",
    "end": "60059"
  },
  {
    "text": "set of components for the kubernetes project a working group is a bit different in",
    "start": "60059",
    "end": "65760"
  },
  {
    "text": "the sense that for one it doesn't own any components until it has a temporary",
    "start": "65760",
    "end": "71340"
  },
  {
    "text": "nature so we get together basically to solve a specific problem and then we decide do we do we dissolve",
    "start": "71340",
    "end": "78720"
  },
  {
    "text": "or maybe we evolve into a Sig uh in this case uh I'm here to talk about the",
    "start": "78720",
    "end": "84780"
  },
  {
    "text": "working group batch which as a working group is a forum to uh discuss enhancements uh to better",
    "start": "84780",
    "end": "92040"
  },
  {
    "text": "support batch workloads um batch might mean different things for different people so just to give some",
    "start": "92040",
    "end": "99420"
  },
  {
    "text": "examples uh we deal with HPC AIML data analytics or even cicd",
    "start": "99420",
    "end": "105900"
  },
  {
    "text": "applications things in general jobs that work to completion",
    "start": "105900",
    "end": "113520"
  },
  {
    "text": "um one of the primary goals of the working group is to reduce fragmentation in the",
    "start": "113520",
    "end": "119939"
  },
  {
    "text": "ecosystem if you're already in this room you might have already seen a lot of talks about batch everybody doing",
    "start": "119939",
    "end": "125579"
  },
  {
    "text": "different things and even years before people are already doing many different things we want to bring some uh some",
    "start": "125579",
    "end": "133319"
  },
  {
    "text": "cohesion in the in the batch uh in in the kubernetes project",
    "start": "133319",
    "end": "138379"
  },
  {
    "text": "and to do this we gather a set of stakeholders from the community so six",
    "start": "138379",
    "end": "144480"
  },
  {
    "text": "scheduling is one of them uh c-gobs primarily because of the the",
    "start": "144480",
    "end": "151739"
  },
  {
    "text": "job API and the Chrome job apis uh Sig node we still need uh our resources to",
    "start": "151739",
    "end": "158220"
  },
  {
    "text": "run on nodes and we have accelerators and whatnot so we needed them and auto",
    "start": "158220",
    "end": "163800"
  },
  {
    "text": "scaling to obviously scale up your clusters uh when you need more resources",
    "start": "163800",
    "end": "170760"
  },
  {
    "text": "but we are not just limited to the kubernetes uh developers or maintainers",
    "start": "170760",
    "end": "176879"
  },
  {
    "text": "we welcome a huge diversity of ecosystem developers regular attendees to the meetings are",
    "start": "176879",
    "end": "184680"
  },
  {
    "text": "folks from qflow from Armada and even from let's say competitor schedulers",
    "start": "184680",
    "end": "191220"
  },
  {
    "text": "such as unicorn so we we welcome all the communities to to come and and bring",
    "start": "191220",
    "end": "197099"
  },
  {
    "text": "their ideas and and feature requests or code so what's in scope",
    "start": "197099",
    "end": "203819"
  },
  {
    "text": "um we're going to go through through all these topics in this in this session uh first additions to the job API the the",
    "start": "203819",
    "end": "211800"
  },
  {
    "text": "kubernetes project has had a job API for a while but uh it needed some some uh",
    "start": "211800",
    "end": "217019"
  },
  {
    "text": "features some love in it some love so we we brought those uh to the job on Chrome",
    "start": "217019",
    "end": "222540"
  },
  {
    "text": "job uh and then we we can start talking about Job queuing maximizing utilization",
    "start": "222540",
    "end": "228420"
  },
  {
    "text": "of your clusters and uh last we also discussing topics",
    "start": "228420",
    "end": "234180"
  },
  {
    "text": "around specialized Hardware gpus gpus you name it so we'll go through all of",
    "start": "234180",
    "end": "240060"
  },
  {
    "text": "this uh starting with the job API um so let's start with uh the feature I'm",
    "start": "240060",
    "end": "247560"
  },
  {
    "text": "most excited for because it's the enablement for the rest of the features uh",
    "start": "247560",
    "end": "253260"
  },
  {
    "text": "basically we reach a general availability of job tracking with finalizers in the 126 release so the",
    "start": "253260",
    "end": "261600"
  },
  {
    "text": "last the second last and so what's what's important about this",
    "start": "261600",
    "end": "267000"
  },
  {
    "text": "feature is that before this feature existed the job controller could actually lose progress of of the status",
    "start": "267000",
    "end": "273479"
  },
  {
    "text": "of the of number of completions number of failures Etc and that just happened when you deleted",
    "start": "273479",
    "end": "280020"
  },
  {
    "text": "the pots um so that meant that the job controller",
    "start": "280020",
    "end": "285120"
  },
  {
    "text": "was actually incompatible with the Pod garbage collector which is ironic because both are",
    "start": "285120",
    "end": "291300"
  },
  {
    "text": "uh kubernetes components so they didn't really work with each other so what do",
    "start": "291300",
    "end": "296759"
  },
  {
    "text": "we do we introduce a finalizer to control when pods are deleted so uh",
    "start": "296759",
    "end": "303600"
  },
  {
    "text": "we use this finalizer to keep the correct tracking of of pots and",
    "start": "303600",
    "end": "310020"
  },
  {
    "text": "make sure they are already tracked in the job status before we delete them and we have done some uh some testing in",
    "start": "310020",
    "end": "318479"
  },
  {
    "text": "uh in different environments and we've we've observed uh we can process uh one",
    "start": "318479",
    "end": "324780"
  },
  {
    "text": "jobs of 100 000 Parts uh in minutes and particularly for index jobs",
    "start": "324780",
    "end": "332220"
  },
  {
    "text": "um and this is really not um a visible feature uh",
    "start": "332220",
    "end": "338940"
  },
  {
    "text": "you don't you don't obtain or opt out it's uh it's by default now starting",
    "start": "338940",
    "end": "345419"
  },
  {
    "text": "from uh when 126 is always on and the result that you see is that",
    "start": "345419",
    "end": "352199"
  },
  {
    "text": "simply your jobs are tracked correctly um so hopefully uh more more folks more",
    "start": "352199",
    "end": "359639"
  },
  {
    "text": "more uh Downstream applications or operators can use the job API uh trusted",
    "start": "359639",
    "end": "366900"
  },
  {
    "text": "job API to handle pod management uh so you know they don't have to re-implement it",
    "start": "366900",
    "end": "372479"
  },
  {
    "text": "uh if you want to learn more about how the feature was specifically implemented uh you can refer to the blog post that",
    "start": "372479",
    "end": "379139"
  },
  {
    "text": "we wrote for the 126 release um and yeah it was actually uh an",
    "start": "379139",
    "end": "385020"
  },
  {
    "text": "interesting Journey with uh some pitfalls and whatnot so I encourage you to read if you're interested",
    "start": "385020",
    "end": "393539"
  },
  {
    "text": "um the next teacher I wanted to highlight uh is um uh portfolio policies so as as you may",
    "start": "393539",
    "end": "401580"
  },
  {
    "text": "know in a kubernetes cluster pods are pretty much ephemeral they can go down",
    "start": "401580",
    "end": "406680"
  },
  {
    "text": "at any point and we need some control uh about uh what to do when when there is a",
    "start": "406680",
    "end": "413160"
  },
  {
    "text": "failure in a job um and the the failures can come from so many places right uh can come from the",
    "start": "413160",
    "end": "420780"
  },
  {
    "text": "scheduler because of preemption it can come from the API server because of the",
    "start": "420780",
    "end": "427500"
  },
  {
    "text": "eviction API or it could come from different controllers that watch different events and decide",
    "start": "427500",
    "end": "434580"
  },
  {
    "text": "to evict a pod it can come from the cubelet because the node is reaching um",
    "start": "434580",
    "end": "440940"
  },
  {
    "text": "it's a it's it's getting hammered so the the node needs to free up some some resources it can also evict the pod",
    "start": "440940",
    "end": "448680"
  },
  {
    "text": "and ultimately you you might have your own user Supply controller let's put an",
    "start": "448680",
    "end": "453900"
  },
  {
    "text": "example the Des scheduler could be one of them uh that could disrupt your pot and then suddenly uh you you want to",
    "start": "453900",
    "end": "461520"
  },
  {
    "text": "decide you want to have full control of what to do uh and we had an API to have",
    "start": "461520",
    "end": "467699"
  },
  {
    "text": "some control right the back of limit but it's not",
    "start": "467699",
    "end": "472860"
  },
  {
    "text": "enough right there is only a number you can control if you put a high number",
    "start": "472860",
    "end": "478440"
  },
  {
    "text": "then too many uh you could have too many Recreations you put a low number you have basically no reliability so we",
    "start": "478440",
    "end": "486180"
  },
  {
    "text": "introduced this uh this API well here this is a very",
    "start": "486180",
    "end": "491340"
  },
  {
    "text": "complex configuration it could be much simpler uh but you can exactly decide what to do in each specific failure",
    "start": "491340",
    "end": "498060"
  },
  {
    "text": "there are two types of uh rules or conditions um much matching rules you can use one",
    "start": "498060",
    "end": "505680"
  },
  {
    "text": "in conditions for example this Russian Target or this one config issue or you",
    "start": "505680",
    "end": "510840"
  },
  {
    "text": "can react to exit codes and you can fail you can ignore there are a few more options",
    "start": "510840",
    "end": "517159"
  },
  {
    "text": "and this uh disruption Target condition comes from the cubelet sorry from all",
    "start": "517159",
    "end": "524219"
  },
  {
    "text": "the kubernetes controllers including the cubelet um and this this one is a sample one you",
    "start": "524219",
    "end": "530220"
  },
  {
    "text": "can actually introduce your own pod conditions and you can use you can react",
    "start": "530220",
    "end": "535380"
  },
  {
    "text": "to those failures uh accordingly uh so what's new in 127",
    "start": "535380",
    "end": "540720"
  },
  {
    "text": "um we introduced a a new uh we introduced some guarantees in the cubelet so that",
    "start": "540720",
    "end": "547080"
  },
  {
    "text": "uh all Bots reach a terminal phase so we can reliably apply the the portfolio policies but this is still a beta",
    "start": "547080",
    "end": "553440"
  },
  {
    "text": "feature so we are still learning uh we're still improving it uh if you want",
    "start": "553440",
    "end": "558959"
  },
  {
    "text": "to learn more uh mikho here who is sitting over there is gonna have a presentation on Thursday about",
    "start": "558959",
    "end": "566279"
  },
  {
    "text": "um portfolio policies and some other some other features uh the job of the job API",
    "start": "566279",
    "end": "571560"
  },
  {
    "text": "so if you want to learn more you can join this session uh the next feature we've worked on is",
    "start": "571560",
    "end": "577560"
  },
  {
    "text": "rather simple simple um so we made completion the completions",
    "start": "577560",
    "end": "583860"
  },
  {
    "text": "feel mutable uh uh just just that it's a validation",
    "start": "583860",
    "end": "589080"
  },
  {
    "text": "relaxation uh and there is only one requirement the parallelism and number of completions",
    "start": "589080",
    "end": "595740"
  },
  {
    "text": "need to match and that's all and now why because well multiple uh",
    "start": "595740",
    "end": "602880"
  },
  {
    "text": "jobs can actually scale up and down and one example of those is pytorch so um we",
    "start": "602880",
    "end": "611100"
  },
  {
    "text": "just wanted to accommodate the job API to be useful in a wider set of applications",
    "start": "611100",
    "end": "619080"
  },
  {
    "text": "so um simple you have a a configure",
    "start": "619080",
    "end": "627480"
  },
  {
    "text": "parallelism and completions and you just mutate it to uh",
    "start": "627480",
    "end": "633000"
  },
  {
    "text": "to have a different scale so what's next uh in the job API",
    "start": "633000",
    "end": "639660"
  },
  {
    "text": "um we are investing in a number of of different features one of them is actually a new project it's called job",
    "start": "639660",
    "end": "646320"
  },
  {
    "text": "set job set as its name is implies it's a set of jobs that work together to to",
    "start": "646320",
    "end": "654480"
  },
  {
    "text": "do one computation one example is a driver working relationship right like",
    "start": "654480",
    "end": "660420"
  },
  {
    "text": "spark for example you have a driver and you have the workers or prey so many",
    "start": "660420",
    "end": "665579"
  },
  {
    "text": "applications that can use this this API the idea is that uh",
    "start": "665579",
    "end": "671880"
  },
  {
    "text": "we have a centralized let's say let's call it pod management but in reality we are implementing it as",
    "start": "671880",
    "end": "678300"
  },
  {
    "text": "a set of index jobs so a job site is a set of index jobs each each index job is",
    "start": "678300",
    "end": "685079"
  },
  {
    "text": "a role and you can have number of workers or number of replicas for each uh",
    "start": "685079",
    "end": "690839"
  },
  {
    "text": "job and we also want to automate a pot to pot communication and authorization keys",
    "start": "690839",
    "end": "697140"
  },
  {
    "text": "if the application requires it like MPI um so there is not a as a corollary to get",
    "start": "697140",
    "end": "706680"
  },
  {
    "text": "this implemented we are proposing a change in a signal in the cubelet to",
    "start": "706680",
    "end": "712160"
  },
  {
    "text": "craft environment variables from a file so we can more easily support more apis",
    "start": "712160",
    "end": "720000"
  },
  {
    "text": "um another cap that is kind of still in progress we don't we didn't finish the design yet is uh back of limit per index",
    "start": "720420",
    "end": "727880"
  },
  {
    "text": "naturally as we release index jobs some years ago already people have been asking about having",
    "start": "727880",
    "end": "733980"
  },
  {
    "text": "control per index so that's that's the work we want to do and that's this is useful for parallel applications where",
    "start": "733980",
    "end": "740700"
  },
  {
    "text": "there are a lot of independent uh workers working on Independent pieces of data a",
    "start": "740700",
    "end": "747060"
  },
  {
    "text": "corollary of this is the ability to retry specific in specific indexes instead of retrained entire job",
    "start": "747060",
    "end": "755779"
  },
  {
    "text": "in the next uh idea or work in progress is a terminating pots as active",
    "start": "756060",
    "end": "762540"
  },
  {
    "text": "this is a necessary uh feature for a tiny couple applications that don't",
    "start": "762540",
    "end": "768120"
  },
  {
    "text": "actually support more than one worker per index so we want to accommodate",
    "start": "768120",
    "end": "773160"
  },
  {
    "text": "those features too um so a lot of uh discussions uh I",
    "start": "773160",
    "end": "778680"
  },
  {
    "text": "wanted to bring to your attention so you can uh join the meetings or just the",
    "start": "778680",
    "end": "784260"
  },
  {
    "text": "GitHub issues and talk about your use cases um we also have some open discussions more",
    "start": "784260",
    "end": "791040"
  },
  {
    "text": "more like abstract or less uh less thoughts let's say let's call them for",
    "start": "791040",
    "end": "798360"
  },
  {
    "text": "example we've been talking about a mutable scaling directives for the jobs when suspended uh or we've been talking",
    "start": "798360",
    "end": "805920"
  },
  {
    "text": "about uh terminating Imports uh uh terminal Imports that are pending uh",
    "start": "805920",
    "end": "812519"
  },
  {
    "text": "there is a discussion there Kevin here is working on that um",
    "start": "812519",
    "end": "818579"
  },
  {
    "text": "demon job is also another proposal that we received recently about the ability to execute a port",
    "start": "818579",
    "end": "826139"
  },
  {
    "text": "node but one time as opposed to demon sets that are you know continuously running",
    "start": "826139",
    "end": "832019"
  },
  {
    "text": "these are all open discussions we still we're still collecting feedback and thinking through the implications so uh",
    "start": "832019",
    "end": "839040"
  },
  {
    "text": "don't take anything as granted let's say but we of course welcome your your input",
    "start": "839040",
    "end": "845220"
  },
  {
    "text": "um the next one is stateful index jobs to kind of have PVCs per index",
    "start": "845220",
    "end": "852420"
  },
  {
    "text": "um the next set of things we're working on are Primitives around job queuing and",
    "start": "852420",
    "end": "858839"
  },
  {
    "text": "maximizing utilization of clusters and the primary investment here is Q",
    "start": "858839",
    "end": "864839"
  },
  {
    "text": "um Q is a a controller uh kubernetes Native controller that implements job",
    "start": "864839",
    "end": "870959"
  },
  {
    "text": "queuing uh it offers a resource quota Management in and it has borrowing and",
    "start": "870959",
    "end": "877620"
  },
  {
    "text": "preemption semantics so you can maximize the utilization of your cluster um it also supports resource fungibility",
    "start": "877620",
    "end": "884779"
  },
  {
    "text": "which means you can you can establish quotas for each kind of resource you have and you know",
    "start": "884779",
    "end": "892620"
  },
  {
    "text": "failover to the next resource if there is not enough capacity for an entire job",
    "start": "892620",
    "end": "898560"
  },
  {
    "text": "instead of perpod uh we also we naturally have support for",
    "start": "898560",
    "end": "905040"
  },
  {
    "text": "the job API that's what we've we've we've been working on so that's the",
    "start": "905040",
    "end": "910320"
  },
  {
    "text": "first class Citizen and but we've been adding we added support for qflow MPI",
    "start": "910320",
    "end": "915660"
  },
  {
    "text": "job and we want to also add more uh that's why we provide we're providing a",
    "start": "915660",
    "end": "920760"
  },
  {
    "text": "library so people so um implementers can integrate with q",
    "start": "920760",
    "end": "926699"
  },
  {
    "text": "and we are talking with different communities to do more Integrations",
    "start": "926699",
    "end": "931800"
  },
  {
    "text": "the latest version is 0.3 released a couple of weeks ago and just to give you an overview one of",
    "start": "931800",
    "end": "940500"
  },
  {
    "text": "the primary design patterns primary design of decisions we made with Q is that we don't want to re-implement",
    "start": "940500",
    "end": "946560"
  },
  {
    "text": "anything we are we are fully compatible with Cube scheduler with the controller manager",
    "start": "946560",
    "end": "952800"
  },
  {
    "text": "and cluster Auto scaler because we take a decision at a different level basically a queue here you can see it",
    "start": "952800",
    "end": "960600"
  },
  {
    "text": "injected and it the only decision that Q takes is",
    "start": "960600",
    "end": "967680"
  },
  {
    "text": "whether a job should be started or should be suspended as a whole and then",
    "start": "967680",
    "end": "973139"
  },
  {
    "text": "the rest of the operations are handled by the existing components such as scheduler to actually assign pods to",
    "start": "973139",
    "end": "980279"
  },
  {
    "text": "to notes any cluster Auto scalar to scale up your cluster",
    "start": "980279",
    "end": "986300"
  },
  {
    "text": "um there you go Oh and next uh the roadmap for 0.4 just uh some improvements to to",
    "start": "986339",
    "end": "994440"
  },
  {
    "text": "some of the features that we already have and we want to we have a PR opening the rate pre-operator to integrate and",
    "start": "994440",
    "end": "1003320"
  },
  {
    "text": "we are working with the qflow community to um support more more apis and as I was",
    "start": "1003320",
    "end": "1011240"
  },
  {
    "text": "saying the the design principle in queue is that if something doesn't work in the rest of",
    "start": "1011240",
    "end": "1018199"
  },
  {
    "text": "the ecosystem in the scheduler or the job controller we propose uh we propose",
    "start": "1018199",
    "end": "1023600"
  },
  {
    "text": "fixes or we propose features on those components so that we can Implement higher level decisions in",
    "start": "1023600",
    "end": "1030918"
  },
  {
    "text": "queue and these are a couple of discussions that we are we are having",
    "start": "1030919",
    "end": "1037880"
  },
  {
    "text": "um and if you want to learn more about Q specifically uh you can watch our",
    "start": "1037880",
    "end": "1045558"
  },
  {
    "text": "presentation on Tuesday uh for the for the batch in HPC they",
    "start": "1045559",
    "end": "1052100"
  },
  {
    "text": "um next I'll hand it over to Swati",
    "start": "1052100",
    "end": "1056440"
  },
  {
    "text": "hello yeah okay so for the next part of the talk I'm going to be focusing on support for",
    "start": "1063559",
    "end": "1069080"
  },
  {
    "text": "specialized Hardware I'm going to cover what we've been doing as part of topology aware scheduling and",
    "start": "1069080",
    "end": "1075559"
  },
  {
    "text": "start with a brief overview and then we'll dive into the developments that have happened since we discussed the",
    "start": "1075559",
    "end": "1080840"
  },
  {
    "text": "topic class at cubecon so as you see on this slide we have a kubernetes cluster with two worker nodes",
    "start": "1080840",
    "end": "1087980"
  },
  {
    "text": "they look very similar to each other each node has four instances of device a",
    "start": "1087980",
    "end": "1094700"
  },
  {
    "text": "for instances of device B and eight CPUs so as I said from scheduler's point of",
    "start": "1094700",
    "end": "1100700"
  },
  {
    "text": "view both these nodes are completely identical and and it seems to appear that resource wise they are exactly the",
    "start": "1100700",
    "end": "1107960"
  },
  {
    "text": "same so if we were to schedule a part that is requesting both device a and device B we",
    "start": "1107960",
    "end": "1114799"
  },
  {
    "text": "should end up in a scenario that the part should succeed so let's see what happens if we were to place this part",
    "start": "1114799",
    "end": "1120740"
  },
  {
    "text": "so if this pod was placed on worker 2 the Pod succeeds and it goes into running state",
    "start": "1120740",
    "end": "1127340"
  },
  {
    "text": "but in scenario that the Pod ends up on worker one it ends up in an unexpected",
    "start": "1127340",
    "end": "1132559"
  },
  {
    "text": "behavior and gets rejected and what we see if we inspect the Pod is that it it has topology Affinity error",
    "start": "1132559",
    "end": "1140120"
  },
  {
    "text": "so let's try to understand why we've we've run into this unexpected behavior and what exactly is topology ethnic",
    "start": "1140120",
    "end": "1146660"
  },
  {
    "text": "error in order to do that we're going to zoom into this particular cluster and get a better",
    "start": "1146660",
    "end": "1153020"
  },
  {
    "text": "understanding of how the system has been configured and how those resources have been distributed",
    "start": "1153020",
    "end": "1159260"
  },
  {
    "text": "so as we see on the top topology manager has been configured with single lumenode policy and resources are distributed",
    "start": "1159260",
    "end": "1166220"
  },
  {
    "text": "across pneuma nodes in a way that on one note the resources are interleaved",
    "start": "1166220",
    "end": "1172220"
  },
  {
    "text": "between the rumor nodes whereas on the other one nomenode 0 has all instances of device a",
    "start": "1172220",
    "end": "1178100"
  },
  {
    "text": "and all instances of device P are on pneumonode one and this kind of clearly shows why we",
    "start": "1178100",
    "end": "1185720"
  },
  {
    "text": "are getting the behavior that we were getting so the topology manager is responsive for taking care of resource alignment at",
    "start": "1185720",
    "end": "1193760"
  },
  {
    "text": "a node level however the scheduler is topology unaware and that is what's leading to sub-optimal scheduling",
    "start": "1193760",
    "end": "1200299"
  },
  {
    "text": "decisions so in order to optimize the scheduling Behavior performance of clusters and the",
    "start": "1200299",
    "end": "1207919"
  },
  {
    "text": "resource utilization in general we need to ensure that the scheduler is aware of",
    "start": "1207919",
    "end": "1213140"
  },
  {
    "text": "how those resources are distributed and has the granular information of those resources",
    "start": "1213140",
    "end": "1218539"
  },
  {
    "text": "and in order to do that we came up with topology aware scheduling",
    "start": "1218539",
    "end": "1223820"
  },
  {
    "text": "this is this slide shows the phase one implementation of topology aware",
    "start": "1223820",
    "end": "1228980"
  },
  {
    "text": "scheduling which was done as an out-of-free solution we started back in 2020 that was alongside kubernetes 122.",
    "start": "1228980",
    "end": "1236480"
  },
  {
    "text": "so it has we've come a long way and in order to do that we have a few components so the first one that you see",
    "start": "1236480",
    "end": "1243799"
  },
  {
    "text": "there is the topology aware scheduler plugin kubernetes Sig repo it houses a",
    "start": "1243799",
    "end": "1250400"
  },
  {
    "text": "bunch of repositories of out of three scheduler plugins that are based on scheduling framework and we contributed",
    "start": "1250400",
    "end": "1256520"
  },
  {
    "text": "node resource topology scheduler plug into that and this particular scheduler",
    "start": "1256520",
    "end": "1261620"
  },
  {
    "text": "plugin ensures that we have pneuma where scheduler decisions this scheduler plugins runs a very",
    "start": "1261620",
    "end": "1268640"
  },
  {
    "text": "simplified version of topology manager algorithm and it helps us determine if a",
    "start": "1268640",
    "end": "1274580"
  },
  {
    "text": "particular node is suitable to run apart based on its based on the resources that it's requesting",
    "start": "1274580",
    "end": "1280460"
  },
  {
    "text": "in order for the scheduler to have enough information we have to provided that information so we do that through",
    "start": "1280460",
    "end": "1286760"
  },
  {
    "text": "node resource API which often we call it as nrt API and it is a crd based API and",
    "start": "1286760",
    "end": "1293360"
  },
  {
    "text": "with this API we are able to decipher between how resources are placed on",
    "start": "1293360",
    "end": "1299480"
  },
  {
    "text": "different pneuma nodes or it could be different hierarchical structures in general",
    "start": "1299480",
    "end": "1304700"
  },
  {
    "text": "and the third component is the topology of data agent in addition to the two",
    "start": "1304700",
    "end": "1310580"
  },
  {
    "text": "components we need a component that runs on all the nodes exposes the hardware information and and subsequently the",
    "start": "1310580",
    "end": "1318380"
  },
  {
    "text": "scheduler plugin uses that information that is exposed to make the topology aware placement decision we have two",
    "start": "1318380",
    "end": "1324440"
  },
  {
    "text": "software components that can serve as topology of data agents NFD and resource",
    "start": "1324440",
    "end": "1330080"
  },
  {
    "text": "topology exporter the diagram on the right that you see it shows the interaction between different",
    "start": "1330080",
    "end": "1336080"
  },
  {
    "text": "components we have the control plane where we have no resource topology API Cube API server the topology where",
    "start": "1336080",
    "end": "1342559"
  },
  {
    "text": "scheduler plugin I should point it and then we have the worker node where we have the different components you see",
    "start": "1342559",
    "end": "1347960"
  },
  {
    "text": "the topology of data agents on all the nodes they are interacting with cubelet via pod resource API and then we have",
    "start": "1347960",
    "end": "1354799"
  },
  {
    "text": "the worker workflow that gets placed based on the topology website",
    "start": "1354799",
    "end": "1361720"
  },
  {
    "text": "so as part of the phase one we were able to come up with an end-to-end solution with all the different software",
    "start": "1363380",
    "end": "1369500"
  },
  {
    "text": "components but there was still a lot to be done we had to make several optimizations",
    "start": "1369500",
    "end": "1375440"
  },
  {
    "text": "across the stack as well as in all the different components that we just discussed about",
    "start": "1375440",
    "end": "1381760"
  },
  {
    "text": "so the first one of them was that um this particular slide shows the",
    "start": "1381860",
    "end": "1388700"
  },
  {
    "text": "interaction between the different components so there could be scenarios that that some of these operations being",
    "start": "1388700",
    "end": "1396500"
  },
  {
    "text": "times is time sensitive can lead to race conditions so we need to understand how",
    "start": "1396500",
    "end": "1402980"
  },
  {
    "text": "can we overcome those scenarios and have an information where the scheduler has an up-to-date information and it's",
    "start": "1402980",
    "end": "1409700"
  },
  {
    "text": "making to the right scheduling decision so this particular slide gives an",
    "start": "1409700",
    "end": "1415159"
  },
  {
    "text": "example of a race condition between the scheduler and the updater the blue arrows on the top indicate the",
    "start": "1415159",
    "end": "1422240"
  },
  {
    "text": "pods that are being scheduled over time and the green arrows at the bottom are indicative of the nrt object updates",
    "start": "1422240",
    "end": "1429140"
  },
  {
    "text": "that happen subsequent to the Pod creation as I said the accuracy of the scheduler",
    "start": "1429140",
    "end": "1437179"
  },
  {
    "text": "plugin and its decision relies on the fact that you have accurate data as well",
    "start": "1437179",
    "end": "1442460"
  },
  {
    "text": "as fresh data so the scheduler can end up with stale information as you see in",
    "start": "1442460",
    "end": "1449299"
  },
  {
    "text": "that in that corner because there there could be scenarios where the Pod has",
    "start": "1449299",
    "end": "1456380"
  },
  {
    "text": "come up it has occupied resources but the resource information has not been updated yet",
    "start": "1456380",
    "end": "1461480"
  },
  {
    "text": "there are several ways of overcoming this you can increase the updated interval or you can have watchable for",
    "start": "1461480",
    "end": "1468919"
  },
  {
    "text": "resource endpoints or cubelet endpoints for that matter but the problem is that the updater is",
    "start": "1468919",
    "end": "1476240"
  },
  {
    "text": "always going to be slower than the scheduler plugin and and and and the main reason for that is that after the",
    "start": "1476240",
    "end": "1483320"
  },
  {
    "text": "report comes in contact with Epi server scheduler is this the next component that serves the part and processes the",
    "start": "1483320",
    "end": "1489080"
  },
  {
    "text": "pod so it made sense for us to make changes in the scheduler itself",
    "start": "1489080",
    "end": "1494720"
  },
  {
    "text": "so we optimize the scheduler to take advantage of a local cache with a helpful Reserve plugin so we essentially",
    "start": "1494720",
    "end": "1501140"
  },
  {
    "text": "use the reserve extension point that the scheduler framework provides us",
    "start": "1501140",
    "end": "1506799"
  },
  {
    "text": "so how did we enable the reserve plugin and implement the local cache",
    "start": "1507260",
    "end": "1513559"
  },
  {
    "text": "so the first step for us was to keep track of all the resources that that",
    "start": "1513559",
    "end": "1518960"
  },
  {
    "text": "were allocated but not yet reported back we call those as unreported resources",
    "start": "1518960",
    "end": "1524059"
  },
  {
    "text": "for the rest of the slides and this this cache update happened every time the Pod",
    "start": "1524059",
    "end": "1530480"
  },
  {
    "text": "was scheduled online scheduler doesn't have visibility on",
    "start": "1530480",
    "end": "1536840"
  },
  {
    "text": "what pneumonode the resources are going to be eventually allocated from that's the responsibility of topology manager",
    "start": "1536840",
    "end": "1543440"
  },
  {
    "text": "so the scheduler takes a pessimistic approach and deducts those unreported",
    "start": "1543440",
    "end": "1548900"
  },
  {
    "text": "resources from both the numer nodes and the third step is to invalidate the",
    "start": "1548900",
    "end": "1554900"
  },
  {
    "text": "cash in order to do that the scheduler looks into it constantly ensures that after",
    "start": "1554900",
    "end": "1563059"
  },
  {
    "text": "the nrd updates have happened it has an object called a pod fingerprint that it",
    "start": "1563059",
    "end": "1568640"
  },
  {
    "text": "refers to to determine if the node State corresponds to that of the one that has",
    "start": "1568640",
    "end": "1573740"
  },
  {
    "text": "been updated by nrt updater so I have a small demo but I'm going to",
    "start": "1573740",
    "end": "1580220"
  },
  {
    "text": "go through a few slides to explain what I'm going to demo and then we can we are",
    "start": "1580220",
    "end": "1585919"
  },
  {
    "text": "going to see what the expected behavior is going to be like and then I'm going to show it through",
    "start": "1585919",
    "end": "1591200"
  },
  {
    "text": "um the command line so as you see here we have this particular cluster which",
    "start": "1591200",
    "end": "1596720"
  },
  {
    "text": "was deployed through mini Cube and these are three VMS the first one being the",
    "start": "1596720",
    "end": "1602120"
  },
  {
    "text": "control plane component which is running API server topology about scheduler plugin the other two are worker nodes",
    "start": "1602120",
    "end": "1608840"
  },
  {
    "text": "running cubelet with topology manager configured with single numer node policy",
    "start": "1608840",
    "end": "1614440"
  },
  {
    "text": "resource wise we are just looking into CPUs for this particular demo we have 16",
    "start": "1614440",
    "end": "1620059"
  },
  {
    "text": "CPUs on each node distributed across two Numa nodes and a CPU per number reserved",
    "start": "1620059",
    "end": "1626960"
  },
  {
    "text": "so that means we have a total of 14 allocatable on each node and we have",
    "start": "1626960",
    "end": "1632840"
  },
  {
    "text": "seven per numer node so just to have a high level information of the the environment that we are going to be",
    "start": "1632840",
    "end": "1639140"
  },
  {
    "text": "talking about so what you're going to see in the demo is that we are going to deploy a burst of Parts each requesting six CPUs",
    "start": "1639140",
    "end": "1646700"
  },
  {
    "text": "and this this cluster in general is able to accommodate this much resource but is",
    "start": "1646700",
    "end": "1652820"
  },
  {
    "text": "that what we are going to see especially given that we already spoke about over reservation and the scheduler adopting",
    "start": "1652820",
    "end": "1659299"
  },
  {
    "text": "the pessimistic approach of allocating resources from both the Numa nodes so when the first part comes in it",
    "start": "1659299",
    "end": "1666260"
  },
  {
    "text": "occupies resources from pneumonode 0 and 1. and this this particular like update",
    "start": "1666260",
    "end": "1673159"
  },
  {
    "text": "that you see is a reflection of the local cash and how it's updated",
    "start": "1673159",
    "end": "1678799"
  },
  {
    "text": "once the second part comes in again on the other node it occupies both the numer nodes",
    "start": "1678799",
    "end": "1684559"
  },
  {
    "text": "then what happens to the third part it essentially because there's not enough resources left in this particular",
    "start": "1684559",
    "end": "1692179"
  },
  {
    "text": "cache the scheduler keeps it in pending state",
    "start": "1692179",
    "end": "1697220"
  },
  {
    "text": "but after after the system has reconciled and the state information that has been updated through nrt",
    "start": "1697220",
    "end": "1704240"
  },
  {
    "text": "updater matches with the that of the state captured by the scheduler plugin",
    "start": "1704240",
    "end": "1711140"
  },
  {
    "text": "we see that the Pod goes into a running state so I have a recording here but I'm going",
    "start": "1711140",
    "end": "1716779"
  },
  {
    "text": "to walk you through a demo yeah oops bad idea",
    "start": "1716779",
    "end": "1726700"
  },
  {
    "text": "okay",
    "start": "1729559",
    "end": "1732100"
  },
  {
    "text": "okay so in this demo as I said it's deployed with a mini Cube based cluster the nodes",
    "start": "1736039",
    "end": "1743240"
  },
  {
    "text": "are under they are VMS and we have the kubernetes cluster deployed with 126",
    "start": "1743240",
    "end": "1749900"
  },
  {
    "text": "version of kubernetes you can see in this um in this demonstration that we have three",
    "start": "1749900",
    "end": "1757039"
  },
  {
    "text": "three nodes on the cluster one control plane and two worker nodes",
    "start": "1757039",
    "end": "1762340"
  },
  {
    "text": "so this is the environment that we're using for demonstrating pneuma wear",
    "start": "1763820",
    "end": "1769220"
  },
  {
    "text": "scheduler they have two Numa nodes as we as I already explained we have 14",
    "start": "1769220",
    "end": "1774860"
  },
  {
    "text": "allocatable per node so you can see over here",
    "start": "1774860",
    "end": "1782299"
  },
  {
    "text": "um that this is corresponding to the pneuma nodes we have seven seven CPUs per pneuma node and and this kind of",
    "start": "1782299",
    "end": "1790399"
  },
  {
    "text": "showcases that the resources are we have a system that has number nodes and we",
    "start": "1790399",
    "end": "1796220"
  },
  {
    "text": "resources are split across numer nodes",
    "start": "1796220",
    "end": "1799898"
  },
  {
    "text": "configuration wise cubelet has been configured with with topology manager",
    "start": "1801440",
    "end": "1806659"
  },
  {
    "text": "policy of single numanoor and if we look at the resource allocation This Is Us showing the how nrt API looks like so we",
    "start": "1806659",
    "end": "1815539"
  },
  {
    "text": "here we are showing here Numa 0 numa1 uh one node 0 node one",
    "start": "1815539",
    "end": "1822200"
  },
  {
    "text": "and we are showing here allocatable available and capacity actually I'd like",
    "start": "1822200",
    "end": "1827360"
  },
  {
    "text": "to show one more thing here so we have in addition to in addition to",
    "start": "1827360",
    "end": "1833960"
  },
  {
    "text": "all these things we have something called pod fingerprint and this had to be enabled",
    "start": "1833960",
    "end": "1839179"
  },
  {
    "text": "for us to do the cache invalidation",
    "start": "1839179",
    "end": "1843820"
  },
  {
    "text": "next we are going to see our test workload and as we already saw each workload is",
    "start": "1847820",
    "end": "1855500"
  },
  {
    "text": "going to request six CPU and we have three pods here",
    "start": "1855500",
    "end": "1860559"
  },
  {
    "text": "so these are these this test group is kind of to simulate that we have a burst of odds and essentially to to replicate",
    "start": "1864740",
    "end": "1871940"
  },
  {
    "text": "that race condition that I was talking about um we have eight CPUs per pneumonole",
    "start": "1871940",
    "end": "1877100"
  },
  {
    "text": "which I already explained and six CPUs are to saturate you know the cluster",
    "start": "1877100",
    "end": "1884799"
  },
  {
    "text": "so once we create the test group we want to see if the pods were created",
    "start": "1888440",
    "end": "1894559"
  },
  {
    "text": "so as I explained earlier we have the two pods go creators but the third part",
    "start": "1894559",
    "end": "1900080"
  },
  {
    "text": "stays in pending state so now we have to wait for a little bit for the system to reconcile and and we",
    "start": "1900080",
    "end": "1908059"
  },
  {
    "text": "are constantly pulling to check if uh if the state has reconciled and after a little bit what we'd see is",
    "start": "1908059",
    "end": "1916039"
  },
  {
    "text": "that the third part goes into running more and the Pod gets scheduled so let's wait for a few more seconds there we",
    "start": "1916039",
    "end": "1922340"
  },
  {
    "text": "have so we we see here that the con the task container 3 of part three has gone into",
    "start": "1922340",
    "end": "1928940"
  },
  {
    "text": "running State and now we can look at the resource allocation yeah that's the part running and now we look at the resource",
    "start": "1928940",
    "end": "1935960"
  },
  {
    "text": "allocation and and that's what we see so resources have",
    "start": "1935960",
    "end": "1942020"
  },
  {
    "text": "been updated so that was corresponding to the second node and we're going to take a quick",
    "start": "1942020",
    "end": "1950179"
  },
  {
    "text": "look at the pods and we can see here that the the port has gone into",
    "start": "1950179",
    "end": "1956260"
  },
  {
    "text": "successfully running State and prior to that actually let's look at this",
    "start": "1956260",
    "end": "1963440"
  },
  {
    "text": "it was trying to it was attempting to schedule the part and",
    "start": "1963440",
    "end": "1969140"
  },
  {
    "text": "and it was basically showing it wasn't able to schedule the port let me see",
    "start": "1969140",
    "end": "1974919"
  },
  {
    "text": "it's just me feeling",
    "start": "1974919",
    "end": "1978519"
  },
  {
    "text": "yeah so this is where you see that you know it it it didn't admit the pause at",
    "start": "1980240",
    "end": "1986840"
  },
  {
    "text": "that stage and that completes the demo I'll move back to the slides",
    "start": "1986840",
    "end": "1993460"
  },
  {
    "text": "so in general for the phase two developments of the scheduler plugin we have we have the reserve plugin",
    "start": "1994880",
    "end": "2001840"
  },
  {
    "text": "implementation and we have the PRS corresponding to that we have different caching strategies which is over Reserve",
    "start": "2001840",
    "end": "2009279"
  },
  {
    "text": "discard reserved for scenarios where where we are trying",
    "start": "2009279",
    "end": "2014799"
  },
  {
    "text": "to allocate resources from multiple pneuma nodes because you know there are there are cases where you can't allocate",
    "start": "2014799",
    "end": "2021100"
  },
  {
    "text": "it from a single Luma node you might need scenarios that you want to reduce as less use as less possible so that's",
    "start": "2021100",
    "end": "2029140"
  },
  {
    "text": "the scoring strategy for that and then we have to make a few changes for uh for",
    "start": "2029140",
    "end": "2035980"
  },
  {
    "text": "catching up with the changes to the nrt API um so this as I said we had to make a",
    "start": "2035980",
    "end": "2042519"
  },
  {
    "text": "few changes to the nrt API and this was in collaboration with other community members who gave feedback on how there",
    "start": "2042519",
    "end": "2049419"
  },
  {
    "text": "are other case use cases as well that can use uh nrt API NRI plugins is",
    "start": "2049419",
    "end": "2055480"
  },
  {
    "text": "another component that's using this API in addition to topology aware scheduling so we introduced top level up uh",
    "start": "2055480",
    "end": "2062500"
  },
  {
    "text": "attributes for exposing topology policy information the next step for us is to gather more",
    "start": "2062500",
    "end": "2068260"
  },
  {
    "text": "feedback and move towards V1 beta we even better one",
    "start": "2068260",
    "end": "2074020"
  },
  {
    "text": "similar to this we had to make this is the third component of topology aware scheduling uh the node feature Discovery",
    "start": "2074020",
    "end": "2081040"
  },
  {
    "text": "as I said we have two components RTE and nft and we've been we were previously focusing a lot on RTE",
    "start": "2081040",
    "end": "2088540"
  },
  {
    "text": "development as part of red hat and we're currently working really hard to get those these two software components up",
    "start": "2088540",
    "end": "2095858"
  },
  {
    "text": "to speed and feature parity with each other so we made a bunch of changes to make",
    "start": "2095859",
    "end": "2101680"
  },
  {
    "text": "sure that nrt catches up with NFD and this work is still in progress",
    "start": "2101680",
    "end": "2108820"
  },
  {
    "text": "um like as part of the demo I showed the ball fingerprinting uh so we in order to",
    "start": "2108820",
    "end": "2114099"
  },
  {
    "text": "enable the reserve plugin we had to make changes uh to the nft itself so it has",
    "start": "2114099",
    "end": "2119440"
  },
  {
    "text": "the for fingerprinting so this change corresponds to that and finally we have we have to adapt to the V1 Alpha 2",
    "start": "2119440",
    "end": "2126640"
  },
  {
    "text": "changes and this work is supposed to continue so watch out for that",
    "start": "2126640",
    "end": "2132460"
  },
  {
    "text": "and our future plans are scalability testing we would like to test this solution at scale",
    "start": "2132460",
    "end": "2138960"
  },
  {
    "text": "and evaluate how we can improve the solution and integrate it",
    "start": "2138960",
    "end": "2144400"
  },
  {
    "text": "with Dynamic resource allocation as well as vpa in the long term we would like to see",
    "start": "2144400",
    "end": "2150160"
  },
  {
    "text": "this solution moving in Tree in kubernetes and that would naturally have integration with Dr and vpa",
    "start": "2150160",
    "end": "2158020"
  },
  {
    "text": "I just want to call out all the people who've been involved in this effort this",
    "start": "2158020",
    "end": "2163480"
  },
  {
    "text": "has been a huge effort that has been spanning multiple years so all the contributors reviewers who've been",
    "start": "2163480",
    "end": "2170380"
  },
  {
    "text": "working tirelessly in on this I want to especially thank Francesco Romani who's here for creating the demo that I just",
    "start": "2170380",
    "end": "2177940"
  },
  {
    "text": "showed here so thanks for that and last but not the least I just want to mention that we are looking for input",
    "start": "2177940",
    "end": "2186220"
  },
  {
    "text": "from the community members if there are use cases if you have thoughts if you have any any questions feel free to",
    "start": "2186220",
    "end": "2194260"
  },
  {
    "text": "reach out on the batch working group via slack or mailing list we meet every",
    "start": "2194260",
    "end": "2200440"
  },
  {
    "text": "other Thursday at 2 pm UTC so it'll be nice for us for you to join us there",
    "start": "2200440",
    "end": "2206859"
  },
  {
    "text": "thank you very much I'm happy to take any questions [Applause]",
    "start": "2206859",
    "end": "2218039"
  },
  {
    "text": "hello thank you for the great talk quick question you got the portfolio policy so",
    "start": "2228640",
    "end": "2233920"
  },
  {
    "text": "you mentioned two actions right now failure and ignoring is there any way to",
    "start": "2233920",
    "end": "2239440"
  },
  {
    "text": "like Implement custom actions uh no no I I would ask you",
    "start": "2239440",
    "end": "2247599"
  },
  {
    "text": "to come to open an issue uh so that we can discuss new additions",
    "start": "2247599",
    "end": "2253180"
  },
  {
    "text": "because it it has to it has to be implemented in a job controller uh if I",
    "start": "2253180",
    "end": "2260859"
  },
  {
    "text": "think correctly but yeah we can discuss we are looking for we're looking to",
    "start": "2260859",
    "end": "2267520"
  },
  {
    "text": "implement uh per index for example uh failures or rules but uh that's",
    "start": "2267520",
    "end": "2274540"
  },
  {
    "text": "that's still being designed thank you and can I ask one more question okay the colorations which you mentioned that in",
    "start": "2274540",
    "end": "2282400"
  },
  {
    "text": "this scheduler that what happens if coloration step will fail how can I",
    "start": "2282400",
    "end": "2287740"
  },
  {
    "text": "handle it israelic specific place where I look for the logs for that",
    "start": "2287740",
    "end": "2292839"
  },
  {
    "text": "I like pod resources not the pottery worker or close resources",
    "start": "2292839",
    "end": "2298920"
  },
  {
    "text": "yeah there was a step when there's a correlation step which like uh resources",
    "start": "2299320",
    "end": "2305380"
  },
  {
    "text": "colorations if you can go back to the slight tolerations you may no no correlations",
    "start": "2305380",
    "end": "2311680"
  },
  {
    "text": "oh no corollaries yeah probably yeah no I mean like as a corollary in this",
    "start": "2311680",
    "end": "2319240"
  },
  {
    "text": "context is simply a an issue as as a result of the other issues that we are",
    "start": "2319240",
    "end": "2324640"
  },
  {
    "text": "no no there is a step when um like you manage to find the proper resources like",
    "start": "2324640",
    "end": "2330640"
  },
  {
    "text": "there you have some predictions and then you realize we have another like resources which are available right and",
    "start": "2330640",
    "end": "2336820"
  },
  {
    "text": "there is another step when you update the scheduler mr8 so no the way it works is that",
    "start": "2336820",
    "end": "2343960"
  },
  {
    "text": "um on a regular interval nrt updater that the component that we have that",
    "start": "2343960",
    "end": "2349420"
  },
  {
    "text": "that populates the nrt API it it gathers information from a component within",
    "start": "2349420",
    "end": "2354640"
  },
  {
    "text": "cubelet and populates data so the scheduler is maintaining its own local cash",
    "start": "2354640",
    "end": "2360280"
  },
  {
    "text": "and it's it's keeping track of the resources internally because the update",
    "start": "2360280",
    "end": "2365440"
  },
  {
    "text": "that had happened can happen later you know from updated standpoint so it keeps",
    "start": "2365440",
    "end": "2371020"
  },
  {
    "text": "its own cache and makes the scheduling decisions based on that I I'm I'm not sure I what okay",
    "start": "2371020",
    "end": "2378579"
  },
  {
    "text": "okay so the whole the whole idea of implementing the local cash is to make sure we prevent that failure in the",
    "start": "2378579",
    "end": "2385060"
  },
  {
    "text": "first place you you saw that we could have ended up in a topology Affinity error what we are trying to do is avoid",
    "start": "2385060",
    "end": "2391839"
  },
  {
    "text": "that by actually keeping the part pending so we feel that keeping the pot pending is better than it failing",
    "start": "2391839",
    "end": "2398619"
  },
  {
    "text": "because these are important parts that we want you know they're they're for important use cases so it's better to",
    "start": "2398619",
    "end": "2405520"
  },
  {
    "text": "keep them pending than having them failed I hope I answer your question",
    "start": "2405520",
    "end": "2411839"
  },
  {
    "text": "for other questions thank you very much have a good day",
    "start": "2413460",
    "end": "2419700"
  }
]