[
  {
    "start": "0",
    "end": "87000"
  },
  {
    "text": "cool well may as well get started I think it's 11 40s so good morning everybody hope everybody's been having a",
    "start": "1100",
    "end": "7620"
  },
  {
    "text": "great conference so far my name is Dan Norris I am a senior cloud native",
    "start": "7620",
    "end": "13230"
  },
  {
    "text": "engineer which totally a title we made up over at NetApp working on our net up",
    "start": "13230",
    "end": "18869"
  },
  {
    "text": "kubernetes service on today i'm here to talk to you about big data operations using kubernetes and local storage so",
    "start": "18869",
    "end": "26449"
  },
  {
    "text": "kind of a point of orders to go through the what we're gonna talk about today",
    "start": "26449",
    "end": "31520"
  },
  {
    "text": "give everybody a little bit of a background kind of describe what i'm hoping you take away from this talk give",
    "start": "31520",
    "end": "38309"
  },
  {
    "text": "you just enough background on Cassandra specifically just enough so you can know",
    "start": "38309",
    "end": "43340"
  },
  {
    "text": "know enough to be dangerous and then go through the basics of local storage kind",
    "start": "43340",
    "end": "49920"
  },
  {
    "text": "of give an overview of how it works how you consume it then talk about what deploying Cassandra and communities",
    "start": "49920",
    "end": "56160"
  },
  {
    "text": "looks like while using local storage and also go through the operations that this",
    "start": "56160",
    "end": "61260"
  },
  {
    "text": "setup allows us to automate in a code so quick background um again what I'm",
    "start": "61260",
    "end": "67890"
  },
  {
    "text": "hoping that you take away from this session is basically a high level description of running Cassandra on",
    "start": "67890",
    "end": "75060"
  },
  {
    "text": "kubernetes what it looks like what concerns you may have to kind of address while going through it and also an",
    "start": "75060",
    "end": "80759"
  },
  {
    "text": "example of the operations that you're able to kind of just automate a way while following this process and so in",
    "start": "80759",
    "end": "88650"
  },
  {
    "start": "87000",
    "end": "87000"
  },
  {
    "text": "terms of operations the things that we are able to do is really kind of use the",
    "start": "88650",
    "end": "93900"
  },
  {
    "text": "kubernetes building blocks that we get as part of running applications on this",
    "start": "93900",
    "end": "99329"
  },
  {
    "text": "platform so using things like stateful sets persistent volumes jobs to really",
    "start": "99329",
    "end": "105570"
  },
  {
    "text": "help automate portions of this stack that normally you'd have to have either",
    "start": "105570",
    "end": "110670"
  },
  {
    "text": "individual operators or piles and piles of like chef or ansible or whatever in",
    "start": "110670",
    "end": "117810"
  },
  {
    "text": "order to manage any of this stuff sensibly so with that in mind I'll talk",
    "start": "117810",
    "end": "123600"
  },
  {
    "text": "a little bit about Cassandra real quick and specifically I want to talk to you about the Dana bottle and kind of what",
    "start": "123600",
    "end": "129599"
  },
  {
    "start": "126000",
    "end": "126000"
  },
  {
    "text": "Cassandra is so Cassandra I'm kind of on like the cap theorem transport more the AP side so it's",
    "start": "129599",
    "end": "136450"
  },
  {
    "text": "eventually consistent by design the goal here is that it's every node or it",
    "start": "136450",
    "end": "142840"
  },
  {
    "text": "should be tolerant to individual node failures so data ends up being distributed across many many nodes and",
    "start": "142840",
    "end": "149380"
  },
  {
    "text": "one of the interesting parts is that replication is configurable per data center or region if you happen to be",
    "start": "149380",
    "end": "154420"
  },
  {
    "text": "working in the cloud and so if it is an example of that we whenever we do a data",
    "start": "154420",
    "end": "160210"
  },
  {
    "text": "right we can place two copies of that data in u.s. East one if we have me running a table us and three copies of",
    "start": "160210",
    "end": "167080"
  },
  {
    "text": "that data in u.s. west two gives us all sorts of interesting things especially with like GDP are now to actually go",
    "start": "167080",
    "end": "173709"
  },
  {
    "text": "ahead and really control where our data lives in the world or within these clusters an interesting caveat to all",
    "start": "173709",
    "end": "180340"
  },
  {
    "text": "that is that read and write consistency is tunable so you even on a per read or",
    "start": "180340",
    "end": "186640"
  },
  {
    "text": "a per write basis individually you can actually decide whether or not you want to get cluster poram to respond back to",
    "start": "186640",
    "end": "192130"
  },
  {
    "text": "you local quorum so within your data center right you just care about one node if you really want to get a fast read or",
    "start": "192130",
    "end": "198400"
  },
  {
    "text": "write out of that and so to put this kind of into more of a picture this is",
    "start": "198400",
    "end": "205360"
  },
  {
    "text": "an example of a ten node Cassandra cluster running in two different regions us East one USS two and this is sort of",
    "start": "205360",
    "end": "213130"
  },
  {
    "text": "just what the data path might look like on a right so when a client goes ahead and issues a right it'll hit an",
    "start": "213130",
    "end": "219519"
  },
  {
    "text": "individual node which may or may not be the actual destination for that data and",
    "start": "219519",
    "end": "225130"
  },
  {
    "text": "if it's not the destination where that data that node acts as a coordinator so that node goes ahead and actually issues",
    "start": "225130",
    "end": "233440"
  },
  {
    "text": "rights to the nodes where that data is supposed to live in this case on the diagram node five is acting as that",
    "start": "233440",
    "end": "240760"
  },
  {
    "text": "coordinator writing out two nodes three and four that's on the left hand side of this diagram and concurrently so once it",
    "start": "240760",
    "end": "249010"
  },
  {
    "text": "actually goes through that process that's when generally speaking when that write is considered to be successful in the background it goes ahead and",
    "start": "249010",
    "end": "256090"
  },
  {
    "text": "replicates that data to the other cluster since if it's configured for two",
    "start": "256090",
    "end": "263410"
  },
  {
    "text": "writes into US East one and three into US West to it goes ahead and issues that",
    "start": "263410",
    "end": "269199"
  },
  {
    "text": "replication request on in this case note five on that right hand side is acting as the coordinator replicating three",
    "start": "269199",
    "end": "275650"
  },
  {
    "text": "copies out to the individual nodes where that data is supposed to live so that's a very high-level overview of sort of",
    "start": "275650",
    "end": "282819"
  },
  {
    "start": "279000",
    "end": "279000"
  },
  {
    "text": "that write process so the problems with Cassandra sort of as from an operational",
    "start": "282819",
    "end": "288400"
  },
  {
    "text": "or sort of as while using the system right is that it is actually very difficult to operate it requires quite a",
    "start": "288400",
    "end": "293979"
  },
  {
    "text": "bit of knowledge in terms of tuning quite a bit of knowledge in terms of knowing where those rough edges is it's",
    "start": "293979",
    "end": "299770"
  },
  {
    "text": "no different than running some other big data systems like Kafka Hadoop spark you",
    "start": "299770",
    "end": "304900"
  },
  {
    "text": "kind of you always have to sort of have an expert in play or a team of experts to really get this to scale and part of",
    "start": "304900",
    "end": "311650"
  },
  {
    "text": "that at least from the perspective or looking through the lens of kubernetes is that some of the stuff was actually",
    "start": "311650",
    "end": "317590"
  },
  {
    "text": "built for a pre container world Cassandra originated at Facebook I think it was 2008 so a very different tech",
    "start": "317590",
    "end": "326050"
  },
  {
    "text": "ecosystem and landscape so there are many many commands that you sometimes have to run manually for to get",
    "start": "326050",
    "end": "332199"
  },
  {
    "text": "Cassandra to behave or to change settings within the cluster",
    "start": "332199",
    "end": "338110"
  },
  {
    "text": "one of the more interesting parts that we ended up discovering while going through and building out a very like",
    "start": "338110",
    "end": "344560"
  },
  {
    "text": "production-ready deployed of this is that no discovery is by IP Cassandra identity sort of matters by IP address",
    "start": "344560",
    "end": "351479"
  },
  {
    "text": "and usually that's okay but for example if you want you to run a service mesh",
    "start": "351479",
    "end": "357069"
  },
  {
    "text": "and have something terminate connections in front of Cassandra that's actually really relatively difficult",
    "start": "357069",
    "end": "362949"
  },
  {
    "text": "Cassandra assumes that when you configure it if it's reaching out to talk to nodes on a particular port that",
    "start": "362949",
    "end": "370180"
  },
  {
    "text": "it is also listening on that port so it's not like at CD where if you've ever gone ahead and set up a cluster manually",
    "start": "370180",
    "end": "376120"
  },
  {
    "text": "at CD nodes are actually identified by IP and port which makes it really easy for us kind of in a cloud native",
    "start": "376120",
    "end": "381909"
  },
  {
    "text": "landscape to operate that a little difficult with some with Cassandra in this case so that other way let's talk a",
    "start": "381909",
    "end": "389469"
  },
  {
    "text": "little bit about local storage so local storage what I mean by that is actually",
    "start": "389469",
    "end": "396330"
  },
  {
    "start": "391000",
    "end": "391000"
  },
  {
    "text": "local persistent volumes and this is a feature that's been entry for a little while it's actually",
    "start": "396330",
    "end": "402849"
  },
  {
    "text": "just went beta in 112 so it's you know in kubernetes speak that means it's basically good to go and the api's",
    "start": "402849",
    "end": "408339"
  },
  {
    "text": "aren't going to change so what it allows you to do is expose directories and",
    "start": "408339",
    "end": "415119"
  },
  {
    "text": "paths on your individual worker nodes as persistent volumes you always could do",
    "start": "415119",
    "end": "422529"
  },
  {
    "text": "that at least on the pod level you could use what's called a host path which",
    "start": "422529",
    "end": "427809"
  },
  {
    "text": "would actually allow you to mount in directories on a host but that was in the pod or deployment or staples that a",
    "start": "427809",
    "end": "435879"
  },
  {
    "text": "stateful set spec which is not a great abstraction right it kind of it really",
    "start": "435879",
    "end": "441909"
  },
  {
    "text": "ties you in to your config for that particular workload so a better way to do this is what local",
    "start": "441909",
    "end": "448029"
  },
  {
    "text": "storage allows you to do which is to clue the scheduler in to tell the scheduler and let it worry about how to",
    "start": "448029",
    "end": "454499"
  },
  {
    "text": "put your pod where its storage is rather than have the pod trying to figure out and slot itself in and it also lets you",
    "start": "454499",
    "end": "462749"
  },
  {
    "text": "hide local paths from the pods the pod doesn't need to know anything about how its configured all it's doing is",
    "start": "462749",
    "end": "468309"
  },
  {
    "text": "consuming storage just like it would if you're using like an EBS volume ACEF all um-- something on any other cloud it",
    "start": "468309",
    "end": "475689"
  },
  {
    "text": "doesn't care it just uses that abstraction so that's sort of the the rationale for why this exists on and in",
    "start": "475689",
    "end": "483009"
  },
  {
    "start": "479000",
    "end": "479000"
  },
  {
    "text": "terms of may why you might want to use local storage a lot of it comes down to you make honestly you probably running",
    "start": "483009",
    "end": "490180"
  },
  {
    "text": "on bare metal if you're thinking about it or you may have particular types of nodes that are pre-configured for like",
    "start": "490180",
    "end": "497800"
  },
  {
    "text": "different disks classes that you may want to take advantage of and that lets you kind of map that out in a consistent",
    "start": "497800",
    "end": "504189"
  },
  {
    "text": "way you may have custom Hardware you may have be in a situation where network",
    "start": "504189",
    "end": "510039"
  },
  {
    "text": "storage may may not be an option or you really care about latency right and you want to keep everything on your node and",
    "start": "510039",
    "end": "517209"
  },
  {
    "text": "really speed your your profit your rip loads up that way and not have to worry",
    "start": "517209",
    "end": "523360"
  },
  {
    "text": "about potential latency using network storage so sort of why you might when",
    "start": "523360",
    "end": "528699"
  },
  {
    "text": "actually going down this direction but before we go too much further just an FYI this turns all of your nodes",
    "start": "528699",
    "end": "537220"
  },
  {
    "text": "where you have workloads that are now being pinned to nodes into snowflakes and generally that's not something we",
    "start": "537220",
    "end": "543250"
  },
  {
    "text": "want to do with kubernetes we've all been in that situation as developers and as operators where we've built systems",
    "start": "543250",
    "end": "550900"
  },
  {
    "start": "545000",
    "end": "545000"
  },
  {
    "text": "that are relatively brittle and the whole point here is cattle not pets right and so local storage by its very",
    "start": "550900",
    "end": "558220"
  },
  {
    "text": "nature inherently starts to break that so if you are going to start using local",
    "start": "558220",
    "end": "563230"
  },
  {
    "text": "storage just be very cognizant of that fact it's just something that you want to keep in mind it's not a bad thing you",
    "start": "563230",
    "end": "569800"
  },
  {
    "text": "know you just need to sort of prepare for it but it does really make data and node locality come into play so just as",
    "start": "569800",
    "end": "577360"
  },
  {
    "text": "an FYI and that's not usually a approach you really want to take unless you have to so really cease network storage if",
    "start": "577360",
    "end": "584890"
  },
  {
    "text": "you can that's pretty much all I wanted to get out of that so in terms of using",
    "start": "584890",
    "end": "590830"
  },
  {
    "text": "local storage with thus ends up looking like is you end up creating a storage",
    "start": "590830",
    "end": "596770"
  },
  {
    "start": "592000",
    "end": "592000"
  },
  {
    "text": "class is everybody familiar with a lot of these abstractions see some nods okay cool I'm gonna just kind of go through",
    "start": "596770",
    "end": "603779"
  },
  {
    "text": "what this looks like through storage classes persistent volume just kind of show you what some of the gambol ends up",
    "start": "603779",
    "end": "610390"
  },
  {
    "text": "looking like to give you an idea so this is an example storage class it's all very dependent on what you want to name",
    "start": "610390",
    "end": "617440"
  },
  {
    "text": "it in this case we have a storage class which is calling it local Cassandra and",
    "start": "617440",
    "end": "623320"
  },
  {
    "text": "the real big takeaway here is the provisioner so with most storage classes",
    "start": "623320",
    "end": "629020"
  },
  {
    "text": "you give it you tell it that you wanted to use like kubernetes do like EBS right or something that clues the scheduler",
    "start": "629020",
    "end": "636339"
  },
  {
    "text": "and to know that you were going to dynamically create this volume and for",
    "start": "636339",
    "end": "641950"
  },
  {
    "text": "local storage you tell it to use this special provision which is the null provision or the no provisioner so it",
    "start": "641950",
    "end": "647470"
  },
  {
    "text": "doesn't do that all it does is just create the object and then it's good to go other an interesting thing to keep in",
    "start": "647470",
    "end": "654100"
  },
  {
    "text": "mind with local storage is the there's a option that you always want to turn on while using it it's the volume binding",
    "start": "654100",
    "end": "661990"
  },
  {
    "text": "mode and it really only has one option it's wait for a first consumer or you don't set it at all",
    "start": "661990",
    "end": "667250"
  },
  {
    "text": "and what that does is it prevents pause from binding to volumes and told they're",
    "start": "667250",
    "end": "673430"
  },
  {
    "text": "ready to be scheduled so what prevents from happening is for",
    "start": "673430",
    "end": "679070"
  },
  {
    "text": "example if you have a very busy cluster and you're trying to schedule a new pod to a node and that node for example",
    "start": "679070",
    "end": "685760"
  },
  {
    "text": "doesn't have enough memory RAM or random your RAM or same thing memory or CPU or",
    "start": "685760",
    "end": "690880"
  },
  {
    "text": "any of the other resources that it's trying to actually consume there is a situation where if you don't specify",
    "start": "690880",
    "end": "696500"
  },
  {
    "text": "this option it will actually bind itself to that node and then you'll never be",
    "start": "696500",
    "end": "701900"
  },
  {
    "text": "able to schedule anything there again it's not great so make sure to always set this if you're using local storage",
    "start": "701900",
    "end": "707650"
  },
  {
    "start": "707000",
    "end": "707000"
  },
  {
    "text": "so here's an example of what actually consuming this as a local volume might",
    "start": "707650",
    "end": "714980"
  },
  {
    "text": "look like or a persistent volumes so this is just a very basic persistent",
    "start": "714980",
    "end": "720140"
  },
  {
    "text": "volume spec we're just calling it example PV because I'm super creative naming the capacity kind of doesn't",
    "start": "720140",
    "end": "726890"
  },
  {
    "text": "matter just because it's going to be whatever your disk capacity actually is the important takeaway here is that we",
    "start": "726890",
    "end": "733640"
  },
  {
    "text": "set the storage class name to local Cassandra which is the storage class that we're looking at before because it",
    "start": "733640",
    "end": "740810"
  },
  {
    "text": "is a local volume we we get this option for to set the local key in the Amla",
    "start": "740810",
    "end": "746210"
  },
  {
    "text": "file and so that path is the directory that we actually want to consume from",
    "start": "746210",
    "end": "752360"
  },
  {
    "text": "the host so in in this case we're just using opt local storage Cassandra that's",
    "start": "752360",
    "end": "757820"
  },
  {
    "text": "the directory that this persistent volume represents and then the kind of",
    "start": "757820",
    "end": "764240"
  },
  {
    "text": "the magic that Clues in the scheduler to make all of this kind of consumable from",
    "start": "764240",
    "end": "770540"
  },
  {
    "text": "its perspective is no affinity typically this is sort of generated for you but",
    "start": "770540",
    "end": "775550"
  },
  {
    "text": "what what this block of Hamel is saying is that it requires the pot and we're if",
    "start": "775550",
    "end": "783530"
  },
  {
    "text": "it's going to consume this storage it has to be on the particular node known",
    "start": "783530",
    "end": "788780"
  },
  {
    "text": "as example node right that's just the name of the node in this case so that's what actually locks it to that node and",
    "start": "788780",
    "end": "795709"
  },
  {
    "text": "the scheduler knows how to consume that and it's pretty much good to go so note affinity is how that takes care",
    "start": "795709",
    "end": "803090"
  },
  {
    "start": "798000",
    "end": "798000"
  },
  {
    "text": "of it you can also use anti affinity by the way this is a trick that we kind of",
    "start": "803090",
    "end": "809420"
  },
  {
    "text": "figured out after running say Prometheus and Cassandra on the same node which was a terrible idea because if you've run",
    "start": "809420",
    "end": "816890"
  },
  {
    "text": "Prometheus at scale you know that it tends to thrash the disk as much as Cassandra might so that was causing all",
    "start": "816890",
    "end": "823040"
  },
  {
    "text": "sorts of interesting issues so what you can do there because Cassandra or if you're using Cassandra local storage",
    "start": "823040",
    "end": "829190"
  },
  {
    "text": "right and you want to bind it to a particular node you can use anti affinity in the specification for your",
    "start": "829190",
    "end": "836210"
  },
  {
    "text": "stateful set and we're going to account what that looks like a little bit to prevent that from happening so you can",
    "start": "836210",
    "end": "841820"
  },
  {
    "text": "tell it that actually no make sure that when we when you start scheduling these workloads don't actually try and",
    "start": "841820",
    "end": "848210"
  },
  {
    "text": "schedule to anything that's running Prometheus to avoid the whole situation entirely my recommendation is especially for",
    "start": "848210",
    "end": "854270"
  },
  {
    "text": "these type kind of workloads if you are going through and provisioning these",
    "start": "854270",
    "end": "860780"
  },
  {
    "text": "kind of really heavyweight systems that pretty much are consuming the whole note anyway you may as well just kind of bite",
    "start": "860780",
    "end": "867320"
  },
  {
    "text": "the bullet and make sure that you're only really scheduling like a Cassandra node or consider pod to that node and it",
    "start": "867320",
    "end": "874280"
  },
  {
    "text": "consumes the resources on the node it's just much easier that way so with that",
    "start": "874280",
    "end": "880610"
  },
  {
    "text": "in mind typically when you're provisioning storage you tend not to",
    "start": "880610",
    "end": "885890"
  },
  {
    "text": "create volumes per se at least as objects you tend to embed them into the",
    "start": "885890",
    "end": "891230"
  },
  {
    "text": "specification of your pod or your stateful set using persistent volume claims so this is an example of a",
    "start": "891230",
    "end": "897890"
  },
  {
    "text": "persistent volume claim it's kind as persistent volume claim we're just calling it Cassandra or example and",
    "start": "897890",
    "end": "903830"
  },
  {
    "text": "reclaim because again super creative from the naming and the important part is just to specify the storage class and",
    "start": "903830",
    "end": "910550"
  },
  {
    "text": "then that clue is the scheduler and that it knows to not provision anything and",
    "start": "910550",
    "end": "917720"
  },
  {
    "text": "some of the magic that happens behind the scenes with using some of the stuff is the external volume provisioner this",
    "start": "917720",
    "end": "924560"
  },
  {
    "text": "is a out of tree project it's a career Denny's incubator project I believe it started out of like a NFS sort of",
    "start": "924560",
    "end": "931850"
  },
  {
    "text": "automated system but what this project is is it contains most of the volume code that is entry",
    "start": "931850",
    "end": "939410"
  },
  {
    "text": "and it's basically to split out eventually I suspect they'll end up",
    "start": "939410",
    "end": "944610"
  },
  {
    "text": "that'll just be a plug-in that you install but what this is is that it's a daemon set that you can have",
    "start": "944610",
    "end": "951930"
  },
  {
    "text": "automatically create persistent volumes from these directories or mount pads and",
    "start": "951930",
    "end": "957060"
  },
  {
    "text": "you what you do is you map storage classes to local directories and it",
    "start": "957060",
    "end": "962760"
  },
  {
    "text": "automatically creates and deletes those persistent volumes for you it's pretty slick how they recommend checking it out",
    "start": "962760",
    "end": "969330"
  },
  {
    "text": "it's pretty easy you install it just on github examples are in gke right so",
    "start": "969330",
    "end": "976920"
  },
  {
    "text": "what's ends up looking like from the object point of view are the things that we're actually going to build to run",
    "start": "976920",
    "end": "983520"
  },
  {
    "text": "Cassandra we're gonna use two stateful sets one for seeds and one for nodes and",
    "start": "983520",
    "end": "989130"
  },
  {
    "text": "I'll go into a little bit of what that actually means later but Cassandra does have two types of nodes in its mind and",
    "start": "989130",
    "end": "996390"
  },
  {
    "text": "then of course all of these services that we want to run to expose these",
    "start": "996390",
    "end": "1001540"
  },
  {
    "text": "these workloads right on the internet or make them actually accessible so this is",
    "start": "1001540",
    "end": "1008240"
  },
  {
    "text": "an example of what this might look like from a very high level we have just a pure Benes cluster which is kind of a",
    "start": "1008240",
    "end": "1013880"
  },
  {
    "text": "big box and then two different stateful sets one for nodes one for seeds in this",
    "start": "1013880",
    "end": "1019310"
  },
  {
    "text": "case the scale just set that to two so we see that there are two regular",
    "start": "1019310",
    "end": "1025010"
  },
  {
    "text": "Cassandra pods right and one seed pod what's really really nice about this being able to split those out it's a",
    "start": "1025010",
    "end": "1031910"
  },
  {
    "text": "little bit of overhead in terms of API you have to go through and actually manage these things you have to build",
    "start": "1031910",
    "end": "1040100"
  },
  {
    "text": "tooling to do this right but it allows you to scale up and down and be very very intentional about the way you do",
    "start": "1040100",
    "end": "1047750"
  },
  {
    "text": "that so for example if you need to update or you need to make configuration changes right if you're using stateful",
    "start": "1047750",
    "end": "1053510"
  },
  {
    "text": "sets you can partition and it also lets you be very careful about your seeds so",
    "start": "1053510",
    "end": "1060800"
  },
  {
    "text": "it gives you a lot more options that way to kind of split it out and that's something that we found was very very useful",
    "start": "1060800",
    "end": "1066520"
  },
  {
    "text": "of course the building block in this scenario is local storage and the one of the other caveats",
    "start": "1066520",
    "end": "1075110"
  },
  {
    "text": "of local storage is you have to go through and allocate that store or those directories on every node before you",
    "start": "1075110",
    "end": "1081200"
  },
  {
    "text": "actually try and schedule anything right something has to create directories otherwise it's not going to work so you",
    "start": "1081200",
    "end": "1088970"
  },
  {
    "text": "can use daemon set to do it if you're so inclined you could use your config management you can SSH in as a normal",
    "start": "1088970",
    "end": "1095270"
  },
  {
    "text": "user and do this which I'm totally guilty of but you can go through and so",
    "start": "1095270",
    "end": "1100990"
  },
  {
    "text": "the provisioner by default expects",
    "start": "1100990",
    "end": "1106150"
  },
  {
    "text": "entries and FS tab basically so it expects amounts so you need to have your local directories as a mount for it to",
    "start": "1106150",
    "end": "1113660"
  },
  {
    "text": "actually understand that exists so it's fun because you have to go through and actually bind mount for what feels like",
    "start": "1113660",
    "end": "1120560"
  },
  {
    "text": "kind of no reason but like it makes sense so in this example since we've",
    "start": "1120560",
    "end": "1126050"
  },
  {
    "text": "been using optical storage Cassandra that's actually the bind mount so which",
    "start": "1126050",
    "end": "1131450"
  },
  {
    "text": "you would go ahead and do would be to create the Optus Android directory as the main directory all that data is",
    "start": "1131450",
    "end": "1137810"
  },
  {
    "text": "going to be stored you go ahead and you create a second directory opt local storage Cassandra which is actually what",
    "start": "1137810",
    "end": "1144440"
  },
  {
    "text": "the pods are gonna write to and then you buy it and bound to the original directory to the local storage directory",
    "start": "1144440",
    "end": "1150290"
  },
  {
    "text": "and you're done that actually makes it available for the",
    "start": "1150290",
    "end": "1156250"
  },
  {
    "text": "local volume provisioner to actually be able to consume it and in terms of what",
    "start": "1156250",
    "end": "1163490"
  },
  {
    "start": "1160000",
    "end": "1160000"
  },
  {
    "text": "the external volume provisioner expects you just give it a config map on when it",
    "start": "1163490",
    "end": "1168920"
  },
  {
    "text": "starts up and it's able to have that mapping between storage classes and",
    "start": "1168920",
    "end": "1174080"
  },
  {
    "text": "local directories so we can create all those volumes so this is an example it just really stripped down one of what",
    "start": "1174080",
    "end": "1180230"
  },
  {
    "text": "this might look like which just a single storage class so it's the bolded term is",
    "start": "1180230",
    "end": "1187700"
  },
  {
    "text": "the storage class that we've kind of been working with in this example which is local Cassandra and in it we specify",
    "start": "1187700",
    "end": "1195740"
  },
  {
    "text": "the host directory that we want to create local volumes for so opt local Cassandra",
    "start": "1195740",
    "end": "1201100"
  },
  {
    "text": "same exact directory from the commands I showed you before yup specify mount directory because the",
    "start": "1201100",
    "end": "1207970"
  },
  {
    "text": "provisioner is also going to mount that path to make sure that it exists so can",
    "start": "1207970",
    "end": "1213160"
  },
  {
    "text": "actually make it so if that goes away it'll automatically clean up and delete volumes things like that so typically",
    "start": "1213160",
    "end": "1218530"
  },
  {
    "text": "they're the same thing unless you want to confuse yourself which I don't recommend and then the last part I want",
    "start": "1218530",
    "end": "1225340"
  },
  {
    "text": "to talk about in this config which is really interesting is that entry the container that ships for the provisioner",
    "start": "1225340",
    "end": "1233350"
  },
  {
    "text": "actually has a bunch of really useful scripts in it so it has this hook for a",
    "start": "1233350",
    "end": "1239020"
  },
  {
    "text": "block cleaner command which you can run shred whenever you actually delete the volume so it'll run it to a level of to",
    "start": "1239020",
    "end": "1246100"
  },
  {
    "text": "automatically behind-the-scenes when you effectively do the equivalent of like a cube CTL delete for that persistent",
    "start": "1246100",
    "end": "1253990"
  },
  {
    "text": "volume or that claim which is pretty convenient it gives you some nice options there for automatically kind of",
    "start": "1253990",
    "end": "1260380"
  },
  {
    "text": "cleaning up after itself because this isn't a dynamic provisioner right like if things go away that data is gonna",
    "start": "1260380",
    "end": "1267250"
  },
  {
    "text": "still stick around so you as an operator would then have to clean it up but this helps automate that process and so one",
    "start": "1267250",
    "end": "1275230"
  },
  {
    "text": "of the other building blocks that kind of goes into this would be the actual",
    "start": "1275230",
    "end": "1280659"
  },
  {
    "text": "configuration of Cassandra itself kind of glossing over there's a lot of stuff to configure you're right but typically",
    "start": "1280659",
    "end": "1287590"
  },
  {
    "text": "that is all files some environment variables of course but you probably want to config map for all that because",
    "start": "1287590",
    "end": "1293470"
  },
  {
    "text": "it just makes it much easier so things like Cassandra Gamal which is the base config for Cassandra JVM got options",
    "start": "1293470",
    "end": "1301900"
  },
  {
    "text": "which is to configure the JVM because you're going to want to tune that the other config files that you may need",
    "start": "1301900",
    "end": "1308350"
  },
  {
    "text": "there's a whole slew of them which I'm not gonna get into typically what I tend to do for handling this is just to jam",
    "start": "1308350",
    "end": "1316330"
  },
  {
    "text": "it into one giant config map and use like sub pads to mount the individual files into the pod but it's really up to",
    "start": "1316330",
    "end": "1322720"
  },
  {
    "text": "you all of these files could be individual it's kind of depends on what you want to do with your templating and",
    "start": "1322720",
    "end": "1328320"
  },
  {
    "text": "your tooling another interesting caveat is that you probably want TLS",
    "start": "1328320",
    "end": "1335440"
  },
  {
    "text": "generally speaking if you're running something like Cassandra you probably aren't just playing around with it it's",
    "start": "1335440",
    "end": "1341769"
  },
  {
    "text": "probably either business critical time series data or analytics or who knows",
    "start": "1341769",
    "end": "1347080"
  },
  {
    "text": "what right so you don't people to actually just be able to consume or to connect to that and have that",
    "start": "1347080",
    "end": "1352720"
  },
  {
    "text": "unencrypted so you're gonna need TLS certs typically one per node or you can",
    "start": "1352720",
    "end": "1358870"
  },
  {
    "text": "use one big one depends what you want to do but you do have to end up having to convert that and I'll go into what that",
    "start": "1358870",
    "end": "1364120"
  },
  {
    "text": "looks like in a second so in terms of kind of the way that we would actually",
    "start": "1364120",
    "end": "1370539"
  },
  {
    "start": "1365000",
    "end": "1365000"
  },
  {
    "text": "build out the pods and our stateful set you have obviously a bunch of different",
    "start": "1370539",
    "end": "1376690"
  },
  {
    "text": "options but what I've done is used a couple of init containers and then a",
    "start": "1376690",
    "end": "1382299"
  },
  {
    "text": "couple of other side cars to really surface some interesting functionality that along with Cassandra so for example",
    "start": "1382299",
    "end": "1391740"
  },
  {
    "text": "what I've done for the setup is use assist control container it's like a",
    "start": "1391740",
    "end": "1397720"
  },
  {
    "text": "busy box to go through and make sure that the nodes themselves are ready to go",
    "start": "1397720",
    "end": "1403090"
  },
  {
    "text": "there's increment ideas there's a couple of different options that you can kind of set if you use the give it the right",
    "start": "1403090",
    "end": "1409809"
  },
  {
    "text": "security context things like network bandwidth making sure you're not constrained things like max open files",
    "start": "1409809",
    "end": "1416970"
  },
  {
    "text": "the stuff that you always kind of have to do if you're running a big database and actually hitting or meeting more",
    "start": "1416970",
    "end": "1422789"
  },
  {
    "text": "options that the default kernel settings typically give you and then the other init container that I used which is",
    "start": "1422789",
    "end": "1430750"
  },
  {
    "text": "getting back to the TLS cert is a initializer for the java key store java",
    "start": "1430750",
    "end": "1436630"
  },
  {
    "text": "if you've had to deal with it actually has a lot of its own bespoke tooling for",
    "start": "1436630",
    "end": "1442570"
  },
  {
    "text": "doing really common things like working with TLS certificates so typically it's",
    "start": "1442570",
    "end": "1448899"
  },
  {
    "text": "much easier to take all of that and not have to like check in like a binary set",
    "start": "1448899",
    "end": "1453940"
  },
  {
    "text": "of data you can just take your TLS start as like a PEM file turn that into a key",
    "start": "1453940",
    "end": "1459820"
  },
  {
    "text": "store and mount it and I'll show you what it looks like in a second so the other containers in this pod or the",
    "start": "1459820",
    "end": "1466029"
  },
  {
    "text": "actual running containers that is part of the application you need a container for Cassandra which",
    "start": "1466029",
    "end": "1471489"
  },
  {
    "text": "is the one on the left you'll probably want something for metrics and what I've done typically is if you're using",
    "start": "1471489",
    "end": "1477999"
  },
  {
    "text": "Prometheus there is a JMX exporter out there that's active that's pretty good all it really it's you can give it a",
    "start": "1477999",
    "end": "1485559"
  },
  {
    "text": "config file and you can kind of filter out metrics that way and it'll just expose that through Prometheus scraping",
    "start": "1485559",
    "end": "1491349"
  },
  {
    "text": "so pretty easy and then also there's a backup sidecar so the sidecar process that I call it",
    "start": "1491349",
    "end": "1499089"
  },
  {
    "text": "that but there's more you can do with that but it's a sidecar process that just responds to network requests and",
    "start": "1499089",
    "end": "1504359"
  },
  {
    "text": "actually will take backups and we'll go into kind of what that process ends up looking like in a little bit so dive",
    "start": "1504359",
    "end": "1513009"
  },
  {
    "text": "back to the secret itself as I mentioned the java key store what that ends up",
    "start": "1513009",
    "end": "1519309"
  },
  {
    "text": "looking like is it pulls the TLS secret reads it converts it to the jks format",
    "start": "1519309",
    "end": "1526419"
  },
  {
    "text": "using java tooling saves that in an empty directory which is just in RAM right and you can mount that into",
    "start": "1526419",
    "end": "1532149"
  },
  {
    "text": "Cassandra map that using docker and I'm not the persistent volume you're good to",
    "start": "1532149",
    "end": "1537549"
  },
  {
    "text": "go so what this kind of putting it all together right if we had a multi region",
    "start": "1537549",
    "end": "1544539"
  },
  {
    "text": "Cassandra cluster running into different AWS regions us least one us to in this",
    "start": "1544539",
    "end": "1551379"
  },
  {
    "text": "example that I have up here it's a tended cluster five nodes in each region",
    "start": "1551379",
    "end": "1556450"
  },
  {
    "text": "and as long as you get kind of the networking bits right you just need to make sure that they know how to",
    "start": "1556450",
    "end": "1562989"
  },
  {
    "text": "communicate with one another and you're basically good to go I am hand waving here because it's very depending or dependent on how you set up your cluster",
    "start": "1562989",
    "end": "1570789"
  },
  {
    "text": "if you're running on bare metal if you're running in public cloud that's pretty much a whole talk into itself I'm perfectly happy to kind of dive into",
    "start": "1570789",
    "end": "1577059"
  },
  {
    "text": "that after the talk so now that that's all out of the way let's talk operations",
    "start": "1577059",
    "end": "1584190"
  },
  {
    "start": "1584000",
    "end": "1584000"
  },
  {
    "text": "so I entered at this before with stateful sets but the by having two",
    "start": "1584190",
    "end": "1590559"
  },
  {
    "text": "different stateful sets the way you can actually rule out updates the way you can increase the size of your cluster",
    "start": "1590559",
    "end": "1595959"
  },
  {
    "text": "right all you have to do is make changes to the state of all set itself so if you increase the replicas that gets you more Cassandra nodes you",
    "start": "1595959",
    "end": "1602799"
  },
  {
    "text": "process more data you have to sometimes be careful with that but it's a pretty easy process generally the other nice",
    "start": "1602799",
    "end": "1609340"
  },
  {
    "text": "part about using stateful sets is it's anybody work with partitions in stateful",
    "start": "1609340",
    "end": "1616720"
  },
  {
    "text": "sets I know you have okay so there's a feature that not too many people use",
    "start": "1616720",
    "end": "1622769"
  },
  {
    "text": "which is a partition it's actually filled in a stateful set and what it allows you to do is to actually control",
    "start": "1622769",
    "end": "1631029"
  },
  {
    "text": "and draw a line sort of through doing rolling updates so you can be very very",
    "start": "1631029",
    "end": "1637059"
  },
  {
    "text": "intentional about when you make changes to the configuration of your stateful",
    "start": "1637059",
    "end": "1642220"
  },
  {
    "text": "set when you want to roll out new versions so you can incrementally do that it's a little bit easier than if",
    "start": "1642220",
    "end": "1650139"
  },
  {
    "text": "you had for example a pod for each Cassandra node because that would be a lot of overhead right but that's all",
    "start": "1650139",
    "end": "1656349"
  },
  {
    "text": "built into kubernetes and that's another advantage of having multiple stateful sets in play and so the other bit that",
    "start": "1656349",
    "end": "1666070"
  },
  {
    "start": "1662000",
    "end": "1662000"
  },
  {
    "text": "comes into play here is to use jobs for automation and that's kind of a lot of the focus on the rest of the talk is in",
    "start": "1666070",
    "end": "1672759"
  },
  {
    "text": "places where we would normally use config management you know make changes",
    "start": "1672759",
    "end": "1678070"
  },
  {
    "text": "to the way that clusters laid out or to force backups you can replace all that with gruber names jobs just super nice",
    "start": "1678070",
    "end": "1684279"
  },
  {
    "text": "and we'll show you an example of that right now so of course when running any",
    "start": "1684279",
    "end": "1690340"
  },
  {
    "text": "database you probably want to have to deal with taking backups at some point whether or not you actually restore them",
    "start": "1690340",
    "end": "1696009"
  },
  {
    "text": "is a totally different story but you're gonna want to have them somewhere so",
    "start": "1696009",
    "end": "1701859"
  },
  {
    "text": "what I've done for this is actually set up like a backup cron job since we have",
    "start": "1701859",
    "end": "1709119"
  },
  {
    "text": "that ability to run Rob jobs in kubernetes and having that backup",
    "start": "1709119",
    "end": "1715239"
  },
  {
    "text": "sidecar that responds to network requests which you can do is set up a job that runs incrementally like once",
    "start": "1715239",
    "end": "1721899"
  },
  {
    "text": "per hour or once per day once per week depends how way you feel like and issues a request to all of the backup sidecars",
    "start": "1721899",
    "end": "1730239"
  },
  {
    "text": "and each of these pods so in this example I have the prettiest cluster just three nodes running on a",
    "start": "1730239",
    "end": "1737999"
  },
  {
    "text": "kubernetes cluster with the backup job going so we issue some requests and then you",
    "start": "1737999",
    "end": "1743440"
  },
  {
    "text": "can back that up to any remote storage that you want s3 cloud storage they're the ones who had nice icons by the way",
    "start": "1743440",
    "end": "1748509"
  },
  {
    "text": "there are others and so in terms of the pot itself if we kind of do a little bit",
    "start": "1748509",
    "end": "1754509"
  },
  {
    "text": "of a deep dive from that particular high-level view when the back up side",
    "start": "1754509",
    "end": "1761139"
  },
  {
    "text": "car receives a request what that ends up doing is typically you should not expose",
    "start": "1761139",
    "end": "1768850"
  },
  {
    "text": "certain ports on Cassandra but because they're basically authenticated it's",
    "start": "1768850",
    "end": "1774039"
  },
  {
    "text": "kind of like a management API control plane you can add authentication but it's a pain and you may as well to sort",
    "start": "1774039",
    "end": "1780730"
  },
  {
    "text": "of expose a very specific service to do some of these things so that's what the back up side card does it responds to",
    "start": "1780730",
    "end": "1786460"
  },
  {
    "text": "requests it effectively wraps a Cassandra binary called node tool which",
    "start": "1786460",
    "end": "1792549"
  },
  {
    "text": "is the command line that you typically use to make any changes to the cluster and so what it does is called a node",
    "start": "1792549",
    "end": "1798519"
  },
  {
    "text": "tool snapshot that takes a snapshot saves it or persistent volume locally",
    "start": "1798519",
    "end": "1803860"
  },
  {
    "text": "right so it's it's ready to go and since it's that volume is mapped in the pod and the sidecars in the pod",
    "start": "1803860",
    "end": "1811509"
  },
  {
    "text": "what it does is tore up that snapshot and then syncs that up to s3 whenever it gets around to it or Google Cloud",
    "start": "1811509",
    "end": "1817899"
  },
  {
    "text": "Storage or digital ocean spaces really doesn't matter you can put it wherever you feel like and so the last",
    "start": "1817899",
    "end": "1825210"
  },
  {
    "start": "1823000",
    "end": "1823000"
  },
  {
    "text": "operational bit that I kind of wanted to talk to you about and this is very Cassandra specific as well is syncing",
    "start": "1825210",
    "end": "1831669"
  },
  {
    "text": "seeds so in Cassandra there as I mentioned before there are two different types of notes there are seed nodes and",
    "start": "1831669",
    "end": "1838629"
  },
  {
    "text": "their regular nodes and a seed node is effectively responsible for helping to",
    "start": "1838629",
    "end": "1845559"
  },
  {
    "text": "boot grab the cluster so they kind of serve as kind of communication gateway",
    "start": "1845559",
    "end": "1851019"
  },
  {
    "text": "slightly to tell new nodes the topology the cluster they for all intents and",
    "start": "1851019",
    "end": "1857350"
  },
  {
    "text": "purposes that's their only special option so you just want to make sure you have enough of them around just to",
    "start": "1857350",
    "end": "1863070"
  },
  {
    "text": "survive node failures and case things restart right",
    "start": "1863070",
    "end": "1868590"
  },
  {
    "text": "but if you think about the way that kubernetes handles things right when you",
    "start": "1868590",
    "end": "1875200"
  },
  {
    "text": "restart a pod that IP is going to change so at some point you need to have a new",
    "start": "1875200",
    "end": "1880210"
  },
  {
    "text": "synced list of IPs so one way to do that is of course to run a job on some predefined interval to sync all of that",
    "start": "1880210",
    "end": "1886780"
  },
  {
    "text": "up as an FYI dynamic reloading where you don't have to actually restart Cassander",
    "start": "1886780",
    "end": "1892960"
  },
  {
    "text": "to pick that up is coming in Cassandra for ATO apparently so this is what this",
    "start": "1892960",
    "end": "1899410"
  },
  {
    "text": "looks like from kind of again our high-level architecture we're still working with our two clusters one in US",
    "start": "1899410",
    "end": "1905650"
  },
  {
    "text": "east one one in u.s. West two we have store ten node cluster and so the box on",
    "start": "1905650",
    "end": "1912490"
  },
  {
    "text": "in the left hand cluster is effectively our seed job and what that does is or",
    "start": "1912490",
    "end": "1920320"
  },
  {
    "text": "what it can do is it can query both clusters for the current set of seed IP",
    "start": "1920320",
    "end": "1925510"
  },
  {
    "text": "addresses collect all of those stick it in a config map that new Cassandra pods",
    "start": "1925510",
    "end": "1932680"
  },
  {
    "text": "would load when they restart and then sync that back out to the other cluster you could run multiple of these you can",
    "start": "1932680",
    "end": "1938770"
  },
  {
    "text": "run one of them it's kind of the the tolerance there is sort of more just on what you feel comfortable with the only",
    "start": "1938770",
    "end": "1945340"
  },
  {
    "text": "caveat here is that that job needs to have the ability to communicate with",
    "start": "1945340",
    "end": "1950440"
  },
  {
    "text": "both clusters which means you had need to have some way to either share service",
    "start": "1950440",
    "end": "1955840"
  },
  {
    "text": "account tokens or what I had done in the past for this is use vault to issue certificates that the clusters would",
    "start": "1955840",
    "end": "1964600"
  },
  {
    "text": "actually map that back to like our back roles so I was able to actually just do",
    "start": "1964600",
    "end": "1970000"
  },
  {
    "text": "those reads it was pretty slick so it's pretty much what I had of course I have",
    "start": "1970000",
    "end": "1977680"
  },
  {
    "text": "to say the obligatory we're also hiring if you if anything's sounds pretty cool if you want to work on multi region who",
    "start": "1977680",
    "end": "1984400"
  },
  {
    "text": "are in Eddy's as a service that's effectively what my team does at NetApp so if you want to drop by our booth more",
    "start": "1984400",
    "end": "1989980"
  },
  {
    "text": "than happy to talk about it and I think we have a little bit of time for questions a couple minutes so if you got",
    "start": "1989980",
    "end": "1996880"
  },
  {
    "text": "any questions have at if not I'm gonna stick around after the talk [Applause]",
    "start": "1996880",
    "end": "2004160"
  },
  {
    "text": "yes",
    "start": "2007290",
    "end": "2010290"
  },
  {
    "text": "yes the question was have I done any performance benchmarking and the answer is no not in this case it it will be",
    "start": "2016090",
    "end": "2023480"
  },
  {
    "text": "quicker but it depends really dependent in the provider like if you're doing your latency will be effectively EBS and",
    "start": "2023480",
    "end": "2029900"
  },
  {
    "text": "that's not going to be any different for any other workload typically yeah if",
    "start": "2029900",
    "end": "2036410"
  },
  {
    "text": "your key exactly your local disk like if you're using SSDs this is going to be fast if you're using spinning rust not",
    "start": "2036410",
    "end": "2041929"
  },
  {
    "text": "so much yeah",
    "start": "2041929",
    "end": "2047440"
  },
  {
    "text": "yeah mm-hmm so the question was have I experimented with the local L family",
    "start": "2051650",
    "end": "2057659"
  },
  {
    "text": "controller no but I really want to because that is much much easier especially because you get a little bit",
    "start": "2057660",
    "end": "2063000"
  },
  {
    "text": "of isolation the problem with local Valley or there are many interesting caveats to local volumes some one of which is that you have no isolation and",
    "start": "2063000",
    "end": "2070200"
  },
  {
    "text": "you really can't even specify a size so LVM helps you with that right because",
    "start": "2070200",
    "end": "2076260"
  },
  {
    "text": "you can say I just want a local 50 gig volume and once it exceeds 50 gigs that's actually a partition so yeah I'm",
    "start": "2076260",
    "end": "2082560"
  },
  {
    "text": "super interested in seeing it I'm just having out a chance to play with it there as I think Ben talked a little bit in in cig storage about making that",
    "start": "2082560",
    "end": "2091139"
  },
  {
    "text": "actually the default local and volume provisioner I think they're kind of trying to figure out what the future of",
    "start": "2091140",
    "end": "2096360"
  },
  {
    "text": "that looks like yeah no problem yes",
    "start": "2096360",
    "end": "2103010"
  },
  {
    "text": "so I'm going to try and summarize what that was so I think the question is",
    "start": "2137770",
    "end": "2142880"
  },
  {
    "text": "roughly do we see a way to actually make this kind of a scheme we're using things",
    "start": "2142880",
    "end": "2149000"
  },
  {
    "text": "like Africa that really require like low latency storage locally easier and kind of by default in kubernetes is that a",
    "start": "2149000",
    "end": "2155450"
  },
  {
    "text": "good summary yeah and so I don't know in some ways a lot of this talk is sort of",
    "start": "2155450",
    "end": "2161690"
  },
  {
    "text": "about like man it'd be really nice that there's an operator view this right and there's an alpha one but doesn't do half",
    "start": "2161690",
    "end": "2168260"
  },
  {
    "text": "the stuff that I kind of talked about I think in some ways that ends up being the answer but it would be nice if local",
    "start": "2168260",
    "end": "2175940"
  },
  {
    "text": "storage at least for this tech class of an application right was a little bit easier to use and kind of less cobbled",
    "start": "2175940",
    "end": "2181610"
  },
  {
    "text": "together in some ways I think it may just be a matter of just kind of talking with sig's storage and seeing sort of",
    "start": "2181610",
    "end": "2187460"
  },
  {
    "text": "some of the use case that there that's kind of coming across their radar because it is very very application and",
    "start": "2187460",
    "end": "2193490"
  },
  {
    "text": "really deployment specifics to how any of this stuff ends up working I think we got time for like one more question for",
    "start": "2193490",
    "end": "2204470"
  },
  {
    "text": "depending on what sis control settings you're setting yes yeah",
    "start": "2204470",
    "end": "2214780"
  },
  {
    "text": "sure okay so let me restate the question is so it's on record the question was to your pods when using local storage",
    "start": "2214809",
    "end": "2220700"
  },
  {
    "text": "require privileged access and yet the answer to that is no because it is a",
    "start": "2220700",
    "end": "2225920"
  },
  {
    "text": "persistent volume so it's being kind of consumed as one right it's not actually",
    "start": "2225920",
    "end": "2231440"
  },
  {
    "text": "mounting a host path it's kind of happening behind the scenes kubernetes is doing that for you yes how much we",
    "start": "2231440",
    "end": "2241460"
  },
  {
    "text": "got on time or do I need a disconnect okay yeah yes so question was for the",
    "start": "2241460",
    "end": "2252079"
  },
  {
    "text": "SIS control Inuk container genie its privileges for that depends on what you're running right if you're running",
    "start": "2252079",
    "end": "2258559"
  },
  {
    "text": "the namespace sets of sis controls you can either actually use what's built into",
    "start": "2258559",
    "end": "2263599"
  },
  {
    "text": "kubernetes to do that instead of using that container or you can grant it the proper security context do that but if",
    "start": "2263599",
    "end": "2270589"
  },
  {
    "text": "you're running any of the non namespaced ones which is typically most of the interesting ones it's going to have to be privileged which effectively makes",
    "start": "2270589",
    "end": "2276829"
  },
  {
    "text": "the rest of the pod privileged yeah",
    "start": "2276829",
    "end": "2280960"
  },
  {
    "text": "so kubernetes has an abstraction for doing some of that like there is a security context that you can provide in",
    "start": "2290960",
    "end": "2296359"
  },
  {
    "text": "a pod spec right so that's what you can do that's what you can set to make that happen all right cool I'll be around if",
    "start": "2296359",
    "end": "2303170"
  },
  {
    "text": "anybody has follow-up questions I really appreciate everybody coming out here",
    "start": "2303170",
    "end": "2309369"
  }
]