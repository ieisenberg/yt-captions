[
  {
    "start": "0",
    "end": "423000"
  },
  {
    "text": "hello thank you for joining our talk on kubernetes topology manager my name is victor pickard i work at red",
    "start": "80",
    "end": "5920"
  },
  {
    "text": "hat my colleague connor nolan from intel will be will be presenting with me today",
    "start": "5920",
    "end": "11679"
  },
  {
    "text": "please type your questions in the q a box and we will address these at the end of the presentation",
    "start": "11679",
    "end": "18320"
  },
  {
    "text": "here's an outline of our discussion today we will introduce topology manager why it is desirable and useful for",
    "start": "19119",
    "end": "24640"
  },
  {
    "text": "certain scenarios now we can't talk about topology manager without first giving a brief overview of",
    "start": "24640",
    "end": "31199"
  },
  {
    "text": "two essential components cpu manager and device manager so we will talk about these components a bit",
    "start": "31199",
    "end": "38079"
  },
  {
    "text": "then we can dive into topology manager and show how this kubernetes component works with both cpu manager and device",
    "start": "38079",
    "end": "45039"
  },
  {
    "text": "manager and health topology manager can be leveraged to maximize performance",
    "start": "45039",
    "end": "50239"
  },
  {
    "text": "why use topology manager what benefit does it provide conor will share some testing results from our friends and colleagues at intel",
    "start": "50239",
    "end": "56800"
  },
  {
    "text": "to help answer these questions are we done is topology manager finished does it complete there's tremendous interest in",
    "start": "56800",
    "end": "64000"
  },
  {
    "text": "topology manager with several enhancements being actively worked and investigated",
    "start": "64000",
    "end": "69439"
  },
  {
    "text": "we'll go over and give some highlights of these activities are you interested in learning more want to contribute we'll have a few words on",
    "start": "69439",
    "end": "76159"
  },
  {
    "text": "this topic as well why numa a growing number of",
    "start": "76159",
    "end": "82720"
  },
  {
    "text": "applications and workloads especially in telco 5g machine learning artificial intelligence",
    "start": "82720",
    "end": "88799"
  },
  {
    "text": "and other type of workloads often say we must be numa aligned a common example of such a workload",
    "start": "88799",
    "end": "94799"
  },
  {
    "text": "would be a dvd based application here the application will desire to have a certain number of",
    "start": "94799",
    "end": "101119"
  },
  {
    "text": "dedicated cpus some chunk of huge page memory a high-speed network interface such as sr",
    "start": "101119",
    "end": "107360"
  },
  {
    "text": "iov virtual functions with perhaps two or more of these one for english one for ingress",
    "start": "107360",
    "end": "112399"
  },
  {
    "text": "all of these resources would need to be on the same numa node in order to get the best performance",
    "start": "112399",
    "end": "119920"
  },
  {
    "text": "from a broader context there's a desire to get the best performance possible out of the hardware",
    "start": "120159",
    "end": "126560"
  },
  {
    "text": "how do we achieve optimal for performance with the lowest latency and high throughput",
    "start": "126560",
    "end": "132080"
  },
  {
    "text": "well we have to align the resources on the same pneuma node these resources include cpus devices",
    "start": "132080",
    "end": "138879"
  },
  {
    "text": "such as network gpu devices memory and possibly more to meet these",
    "start": "138879",
    "end": "143920"
  },
  {
    "text": "objectives what is pneuma so we've mentioned numa",
    "start": "143920",
    "end": "150319"
  },
  {
    "text": "several times some of you may already be familiar with pneuma know what it is and why we need it for those that aren't",
    "start": "150319",
    "end": "156560"
  },
  {
    "text": "familiar with pneuma let's briefly review pneuma is non-uniform memory access",
    "start": "156560",
    "end": "162959"
  },
  {
    "text": "looking at the diagram on the right we see a two pneuma node system this system has two cpu sockets each cpu",
    "start": "162959",
    "end": "170879"
  },
  {
    "text": "has local access to the directly connected memory and devices and",
    "start": "170879",
    "end": "176640"
  },
  {
    "text": "each cpu can certainly access all of the memory on the system but as you can see for example for cpu 0",
    "start": "176640",
    "end": "183519"
  },
  {
    "text": "on socket 0 to access memory on numa node 1 it must go over the inter socket",
    "start": "183519",
    "end": "189519"
  },
  {
    "text": "connection accessing memory or devices over the interconnect adds delays so to get the best",
    "start": "189519",
    "end": "196400"
  },
  {
    "text": "performance pod resources such as cpus and devices should be on the same pneuma node and",
    "start": "196400",
    "end": "202480"
  },
  {
    "text": "avoid the inter-socket connections",
    "start": "202480",
    "end": "206720"
  },
  {
    "text": "how do we get exclusive cpus assigned to a container we use cpu manager cpu manager is a",
    "start": "208319",
    "end": "215200"
  },
  {
    "text": "kubernetes component that runs in the kubelet it allocates and assigns exclusive cpus",
    "start": "215200",
    "end": "220239"
  },
  {
    "text": "to a container that specify a guaranteed quality of service meaning that both requests and limits are",
    "start": "220239",
    "end": "227120"
  },
  {
    "text": "entered your values as shown in the sample pod spec now for cpu manager",
    "start": "227120",
    "end": "232799"
  },
  {
    "text": "to allocate guaranteed exclusive cpus to a container the cpu manager policy must be set to",
    "start": "232799",
    "end": "240319"
  },
  {
    "text": "static for more information about the cpu manager we've included a link to the blog post below",
    "start": "240319",
    "end": "245680"
  },
  {
    "text": "at the bottom of this slide",
    "start": "245680",
    "end": "248719"
  },
  {
    "text": "so we have guaranteed exclusive cpus assigned to a container how are devices assigned and allocated",
    "start": "250879",
    "end": "257759"
  },
  {
    "text": "well kubernetes has a device plug-in framework devices can use the device plug-in framework to",
    "start": "257759",
    "end": "263840"
  },
  {
    "text": "implement plug-ins for gpus nics fpgas and other device resources",
    "start": "263840",
    "end": "269919"
  },
  {
    "text": "that require vendor-specific setup and config with the device plug-in you can advertise system hardware resources to",
    "start": "269919",
    "end": "276639"
  },
  {
    "text": "the cubelet that will be used to assign resources to the container",
    "start": "276639",
    "end": "283280"
  },
  {
    "text": "so we've introduced cpu manager device plugins and how they can be utilized to allocate resources",
    "start": "283280",
    "end": "288960"
  },
  {
    "text": "to a pod and or container but these allocations are done independently no",
    "start": "288960",
    "end": "294160"
  },
  {
    "text": "coordination meaning the allocations could come from different pneuma nodes",
    "start": "294160",
    "end": "300080"
  },
  {
    "text": "how do we get these resources aligned we use topology manager",
    "start": "300080",
    "end": "305199"
  },
  {
    "text": "topology manager is a component that runs inside the cubelet on each node topology manager was promoted to beta as",
    "start": "305199",
    "end": "311440"
  },
  {
    "text": "of kubernetes 1.18 it provides an interface to allow cpus and devices",
    "start": "311440",
    "end": "317280"
  },
  {
    "text": "to coordinate resource assignment to pods and containers at the node level both cpu manager and device manager",
    "start": "317280",
    "end": "324000"
  },
  {
    "text": "support the topology manager interface with topology manager we now have the ability to assign",
    "start": "324000",
    "end": "329919"
  },
  {
    "text": "and allocate resources such as cpus gpus network interfaces like sriv",
    "start": "329919",
    "end": "336720"
  },
  {
    "text": "virtual functions to a container from the same pneuma node",
    "start": "336720",
    "end": "342720"
  },
  {
    "text": "let's take a look at an example on the left is a simple pod spec with one container",
    "start": "342800",
    "end": "348800"
  },
  {
    "text": "the container in this pod spec is requesting two cpus some memory and one instance of device a",
    "start": "348800",
    "end": "356240"
  },
  {
    "text": "on the right you see we have a two pneuma node system four cpus each with two devices on each pneuma node now",
    "start": "356240",
    "end": "364319"
  },
  {
    "text": "using topology manager in coordination with cpu management device manager the cubelet will assign and allocate the",
    "start": "364319",
    "end": "371199"
  },
  {
    "text": "resource for this container from the same pneuma node in this example",
    "start": "371199",
    "end": "376479"
  },
  {
    "text": "using topology manager you can see that we have an aligned pod with two cpus assigned from numa node",
    "start": "376479",
    "end": "382639"
  },
  {
    "text": "zero and one device a also from the pneuma node zero",
    "start": "382639",
    "end": "388080"
  },
  {
    "text": "the resources for the pod are numa aligned eliminating the overhead of the socket interconnect",
    "start": "388080",
    "end": "394240"
  },
  {
    "text": "here we have optimized resource allocation now conor will take you through some of",
    "start": "394240",
    "end": "399600"
  },
  {
    "text": "the details of topology manager including policies in a workings along",
    "start": "399600",
    "end": "405039"
  },
  {
    "text": "with some detailed examples connor over to you",
    "start": "405039",
    "end": "409840"
  },
  {
    "text": "thanks victor so um we've seen what topology manager is and why it's needed so",
    "start": "411039",
    "end": "416080"
  },
  {
    "text": "i'm going to go into some more detail on how it actually works starting with the topology manager policies",
    "start": "416080",
    "end": "421520"
  },
  {
    "text": "so the chosen policy is set at node level as a cubelet flag and there are a few different options with varying degrees of strictness",
    "start": "421520",
    "end": "428960"
  },
  {
    "start": "423000",
    "end": "561000"
  },
  {
    "text": "the first is none which is set by default it doesn't do any kind of resource alignment the next is the best",
    "start": "428960",
    "end": "435440"
  },
  {
    "text": "effort policy um this will attempt to perform some resource alignment",
    "start": "435440",
    "end": "440880"
  },
  {
    "text": "but the pod will always be admitted regardless of whether that can be achieved or not then the restricted policy is the same",
    "start": "440880",
    "end": "447280"
  },
  {
    "text": "as the best effort policy in terms of how it actually works under the hood but the difference being that pod",
    "start": "447280",
    "end": "453759"
  },
  {
    "text": "admission can be failed if an optimal alignment cannot be achieved and we've got some examples",
    "start": "453759",
    "end": "458880"
  },
  {
    "text": "coming up to show to show you how that works then finally the single luminol policy as the name would suggest will",
    "start": "458880",
    "end": "465599"
  },
  {
    "text": "attempt alignment of resources for a container on a single luma node and will fail pot admission if that",
    "start": "465599",
    "end": "472080"
  },
  {
    "text": "cannot be achieved so how topology manager actually works",
    "start": "472080",
    "end": "477280"
  },
  {
    "text": "under the hood so at pod admission time the topology manager pod admit handler will",
    "start": "477280",
    "end": "483039"
  },
  {
    "text": "loop over all containers in the pod then for each container it will call out to the individual hint",
    "start": "483039",
    "end": "488879"
  },
  {
    "text": "providers which are currently the cpu manager and the device manager as victor has spoken about",
    "start": "488879",
    "end": "494639"
  },
  {
    "text": "and from those gather what we've termed as topology hints for each topology aware resource type",
    "start": "494639",
    "end": "501120"
  },
  {
    "text": "these topology hints are essentially all the possible allocations for a given",
    "start": "501120",
    "end": "506240"
  },
  {
    "text": "resource and again these are covered in more detail in the next slide so then once all of these hints have",
    "start": "506240",
    "end": "512800"
  },
  {
    "text": "been accumulated the topology manager using the selected policy will merge the gathered hints and then",
    "start": "512800",
    "end": "519760"
  },
  {
    "text": "select a best hint that will align all all requested resources",
    "start": "519760",
    "end": "524959"
  },
  {
    "text": "so then we loop back over those hint providers instructing them to allocate their respective re respective",
    "start": "524959",
    "end": "530959"
  },
  {
    "text": "resources uh using that best hint as a guide for which pneuma node or pneuma nodes they",
    "start": "530959",
    "end": "537360"
  },
  {
    "text": "should allocate from then finally as we mentioned this loop runs at pot admission time",
    "start": "537360",
    "end": "542959"
  },
  {
    "text": "so if any of these steps fail or the alignment cannot be satisfied according to the policy for any of the containers",
    "start": "542959",
    "end": "549680"
  },
  {
    "text": "in the pod the pot admission will fail with a topology affinity error and any of those allocations that",
    "start": "549680",
    "end": "555760"
  },
  {
    "text": "occurred prior to the failure are cleaned up accordingly so mentioned",
    "start": "555760",
    "end": "562560"
  },
  {
    "start": "561000",
    "end": "1047000"
  },
  {
    "text": "in the previous slide a topology hint uh a tv hint is a construct that's used that we use to",
    "start": "562560",
    "end": "568080"
  },
  {
    "text": "describe how a resource request can be satisfied so it currently consists of two fields",
    "start": "568080",
    "end": "573600"
  },
  {
    "text": "the pneuma node affinity this is a bitmask of pneuma nodes where a resource request can be",
    "start": "573600",
    "end": "579440"
  },
  {
    "text": "satisfied i.e which pneuma node are pneuma nodes then the preferred field contains a",
    "start": "579440",
    "end": "584880"
  },
  {
    "text": "boolean that encodes whether that given hint is preferred or not and there are some examples",
    "start": "584880",
    "end": "591040"
  },
  {
    "text": "we have to explain what we mean by preferred and how that can influence pot admission",
    "start": "591040",
    "end": "596240"
  },
  {
    "text": "with respect to the given policy so if we take this example",
    "start": "596240",
    "end": "602079"
  },
  {
    "text": "similar to the diagram picture showed you earlier so if this is the underlying hardware of our of our node uh we see over on the left",
    "start": "602079",
    "end": "608800"
  },
  {
    "text": "we've got numenol zero and connected to that we have socket zero with four cpus and also",
    "start": "608800",
    "end": "614560"
  },
  {
    "text": "connected to numeral zero are three devices so one gpu and two nics then over on the right across that",
    "start": "614560",
    "end": "620480"
  },
  {
    "text": "intersocket connection we've got socket one with the gain four cpus and three three connected devices so two",
    "start": "620480",
    "end": "628000"
  },
  {
    "text": "gpus and one nic and these are all connected to new node one so all of these resources are available",
    "start": "628000",
    "end": "634560"
  },
  {
    "text": "for allocation and then the following part is scheduled to the node",
    "start": "634560",
    "end": "640399"
  },
  {
    "text": "and this pilot is requesting two exclusive cpus one nic and one gpu so now we can",
    "start": "640399",
    "end": "646720"
  },
  {
    "text": "actually take a look at the topology hints that are returned from the individual hemp providers both",
    "start": "646720",
    "end": "652320"
  },
  {
    "text": "the cpu manager and the device manager so for the cpu resource type we can see from the first cpu hint",
    "start": "652320",
    "end": "658959"
  },
  {
    "text": "the one with the zero one true semantics that a request for two cpus can be",
    "start": "658959",
    "end": "664640"
  },
  {
    "text": "satisfied entirely on numerous zero and this is indicated by the bitmask which is encoded with numeral zero",
    "start": "664640",
    "end": "670800"
  },
  {
    "text": "bit zero and we also see that this allocation is preferred because it's the the narrowest possible placement and",
    "start": "670800",
    "end": "677600"
  },
  {
    "text": "narrowness in this instance refers to the the least number of bit set",
    "start": "677600",
    "end": "682640"
  },
  {
    "text": "in that bitmask so then the second cpu into the one zero true this tells us that the request for two",
    "start": "682640",
    "end": "689360"
  },
  {
    "text": "cpus can also be satisfied uh on pneuma node one and this allocation is also preferred",
    "start": "689360",
    "end": "695920"
  },
  {
    "text": "because again it is the narrowest possible placement or equal to the narrowest placement the final third cpu hint tells us that",
    "start": "695920",
    "end": "702880"
  },
  {
    "text": "the request can also be satisfied across both numa note 0 and numa node 1 however",
    "start": "702880",
    "end": "709040"
  },
  {
    "text": "this allocation would not be preferred as there are narrower possible placements as we can see from the",
    "start": "709040",
    "end": "714320"
  },
  {
    "text": "the first two hints so the same hints are also returned from",
    "start": "714320",
    "end": "719839"
  },
  {
    "text": "the other resource types in this particular scenario and for example we can see that a",
    "start": "719839",
    "end": "725200"
  },
  {
    "text": "request for one nic can be satisfied on either numero node alone which would be preferred are with a bit mask",
    "start": "725200",
    "end": "731279"
  },
  {
    "text": "for both pneuma nodes which would be unpreferred and the same applies for the request of one gpu",
    "start": "731279",
    "end": "738399"
  },
  {
    "text": "so now the topology manager has gathered all hints from all providers the next step is to merge these hints",
    "start": "738399",
    "end": "744959"
  },
  {
    "text": "so this is ultimately done by taking a cross product of hints across all resource types",
    "start": "744959",
    "end": "750720"
  },
  {
    "text": "and performing a bitwise and on the numerator affinities of those hints and this gives us a new bitmask for a",
    "start": "750720",
    "end": "756880"
  },
  {
    "text": "merged hinge so for the preferred field in any of the in any of the hints in the cross product",
    "start": "756880",
    "end": "763760"
  },
  {
    "text": "if any of the hints in that cross product contain a non-preferred hint then the merge end is also number",
    "start": "763760",
    "end": "769920"
  },
  {
    "text": "otherwise it's set to true um also if the affinity of the merged hint",
    "start": "769920",
    "end": "775760"
  },
  {
    "text": "is all zeros we set the preferred field to false so then it's just a matter of iterating",
    "start": "775760",
    "end": "781440"
  },
  {
    "text": "over all the possible permutations in those hints and this continues right through until we have compiled a full list of our",
    "start": "781440",
    "end": "788800"
  },
  {
    "text": "merged hints so finally once we have all those",
    "start": "788800",
    "end": "794079"
  },
  {
    "text": "merchants it's up to the policy to then choose a best hint so",
    "start": "794079",
    "end": "799360"
  },
  {
    "text": "for the uh for the best effort policy in this scenario the best hint chosen will be",
    "start": "799360",
    "end": "804800"
  },
  {
    "text": "uh the one with the zero one true semantics and this is true for all okay all three policies uh in other words the",
    "start": "804800",
    "end": "811680"
  },
  {
    "text": "best hint is to allocate from pneumonia zero and this is a preferred allocation so this means that the best effort",
    "start": "811680",
    "end": "817600"
  },
  {
    "text": "restricted and single numeral policies are all satisfied by this placement and the pod will be",
    "start": "817600",
    "end": "824399"
  },
  {
    "text": "admitted successfully under all three policies with the allocations shown so all cpus and all devices are allocated from the",
    "start": "824399",
    "end": "832000"
  },
  {
    "text": "one luma node numeral zero okay so a slightly more complex example",
    "start": "832000",
    "end": "837360"
  },
  {
    "text": "this time again our node has the same underlying hardware topology however on this occasion gpu one on pneuma node one",
    "start": "837360",
    "end": "844160"
  },
  {
    "text": "has already been allocated and is no longer available to us so now the following part is scheduled",
    "start": "844160",
    "end": "850639"
  },
  {
    "text": "and this part is one container requesting two exclusive cpus one nic and two gpus so again we look at",
    "start": "850639",
    "end": "858480"
  },
  {
    "text": "what kind of hints are returned by the hint providers so for the cpu and nick resource types",
    "start": "858480",
    "end": "863680"
  },
  {
    "text": "we see the same result each request can be satisfied on either pneuma node alone or across",
    "start": "863680",
    "end": "868880"
  },
  {
    "text": "both pneuma nodes for the gpu resource type we see a change",
    "start": "868880",
    "end": "874639"
  },
  {
    "text": "so now the request for two gpus cannot be satisfied on numer node zero or pneuma node one alone the only hint",
    "start": "874639",
    "end": "881519"
  },
  {
    "text": "we see tells us that the request can only be satisfied across both numerals and this allocation is not",
    "start": "881519",
    "end": "887360"
  },
  {
    "text": "preferred the reason it's not preferred is that although it is the narrowest possible placement for this",
    "start": "887360",
    "end": "893279"
  },
  {
    "text": "resource request at this time it's still possible to satisfy this request for two gpus on a single luma node",
    "start": "893279",
    "end": "899760"
  },
  {
    "text": "but just not right now because as we've seen gpu one has already been allocated",
    "start": "899760",
    "end": "905360"
  },
  {
    "text": "so then if we look at the results per policy for the best effort policy although the final best change is not",
    "start": "905360",
    "end": "910880"
  },
  {
    "text": "preferred this policy is all this policy always allows pod admission regardless",
    "start": "910880",
    "end": "916800"
  },
  {
    "text": "and then uses the mer the best hint to allocate resources on the best effort basis as shown here",
    "start": "916800",
    "end": "923600"
  },
  {
    "text": "the restricted policy will always produce the same best hint as the best effort policy",
    "start": "923600",
    "end": "929199"
  },
  {
    "text": "however it will fail pot admission when the merged hint is not preferred as we see here the logic being that it's better to fail",
    "start": "929199",
    "end": "936959"
  },
  {
    "text": "and try to reschedule than to schedule with a non-preferred alignment and this exact scenario we have here is a good example of",
    "start": "936959",
    "end": "943759"
  },
  {
    "text": "how the restricted policy differs from the best effort policy then finally the single luminal policy's",
    "start": "943759",
    "end": "949680"
  },
  {
    "text": "best hint tells us that the requested resources cannot be satisfied on a single luma node",
    "start": "949680",
    "end": "955120"
  },
  {
    "text": "so therefore it will also fail pot admission so our final example again to know is",
    "start": "955120",
    "end": "961920"
  },
  {
    "text": "the same underlying hardware this time all resources are available to us and if we look at the pods this time",
    "start": "961920",
    "end": "967839"
  },
  {
    "text": "the podge gets scheduled has one container requesting two exclusive cpus uh three nicks and",
    "start": "967839",
    "end": "973040"
  },
  {
    "text": "three gpus so the hints that are returned for the cpu resource type we see the same result",
    "start": "973040",
    "end": "979759"
  },
  {
    "text": "again the request can be satisfied in either numeral alone are across both newer nodes",
    "start": "979759",
    "end": "985199"
  },
  {
    "text": "for the gpu and nick resource types we now see that they both return a single hint with a bitmask of both",
    "start": "985199",
    "end": "992000"
  },
  {
    "text": "pneuma nodes and this allocation is preferred now the reason this is now preferred is because it is actually the narrowest",
    "start": "992000",
    "end": "999040"
  },
  {
    "text": "possible placement that could ever be achieved for that request for for instance a request for three",
    "start": "999040",
    "end": "1004639"
  },
  {
    "text": "gpus as you can see can only ever be satisfied across both pneuma nodes and likewise",
    "start": "1004639",
    "end": "1010000"
  },
  {
    "text": "the request for three nicks so then for each policy we can see the result again the best effort policy",
    "start": "1010000",
    "end": "1017199"
  },
  {
    "text": "will always allow part admission and use that best hint to allocate on a best effort basis as",
    "start": "1017199",
    "end": "1022320"
  },
  {
    "text": "shown here the restricted policy um the best hint is now preferred so the preferred flag",
    "start": "1022320",
    "end": "1028160"
  },
  {
    "text": "is set to true and for that reason the pod is admitted with what what is now the narrowest alignment that could possibly be",
    "start": "1028160",
    "end": "1034079"
  },
  {
    "text": "achieved for that request and again finally we see the single luminol policy again will fail part",
    "start": "1034079",
    "end": "1040079"
  },
  {
    "text": "admission because its best hint tells us that the requested resources cannot be satisfied on a single luminor",
    "start": "1040079",
    "end": "1048640"
  },
  {
    "start": "1047000",
    "end": "1110000"
  },
  {
    "text": "so what does all this mean for performance well at intel we released some updated collateral for",
    "start": "1048640",
    "end": "1053760"
  },
  {
    "text": "our container experience kits earlier this year and included in that was a technology guide on topology manager",
    "start": "1053760",
    "end": "1059520"
  },
  {
    "text": "with some information on performance benchmarking and the link to that is included if you'd like to check it out in a bit more",
    "start": "1059520",
    "end": "1066000"
  },
  {
    "text": "detail but the key takeaway here is that workloads that have a new new alignment uh of their resources by",
    "start": "1066000",
    "end": "1072960"
  },
  {
    "text": "topology manager can achieve a performance increase of over 2x versus workloads with that sub optimal",
    "start": "1072960",
    "end": "1079440"
  },
  {
    "text": "allocation that victor spoke about earlier and this particular test we did see a flat line on the performance",
    "start": "1079440",
    "end": "1085120"
  },
  {
    "text": "improvements for the larger packet size once you went over 1024 bytes so but this was just down to the the test setup",
    "start": "1085120",
    "end": "1092160"
  },
  {
    "text": "and the the line rate essentially was maxed out at around 200 gigs so we didn't get to see the",
    "start": "1092160",
    "end": "1097919"
  },
  {
    "text": "performance beyond that but again as i said the link to the technology guidance is included in the slides",
    "start": "1097919",
    "end": "1103679"
  },
  {
    "text": "if you'd like to check that out and see what was actually done in detail for anyone who's interested in that",
    "start": "1103679",
    "end": "1110399"
  },
  {
    "start": "1110000",
    "end": "1245000"
  },
  {
    "text": "so what the future looks like for topology managers so first thing their support for device",
    "start": "1111280",
    "end": "1116320"
  },
  {
    "text": "specific constraints uh this will be part of the 1.19 release so this is an enhancement to the device",
    "start": "1116320",
    "end": "1122320"
  },
  {
    "text": "plug-in api that allows allows device plugins to indicate a list of preferred allocations for its device",
    "start": "1122320",
    "end": "1129039"
  },
  {
    "text": "so the plugin can take into account any internal topology constraints on the device",
    "start": "1129039",
    "end": "1134480"
  },
  {
    "text": "when returning this list then this information can be incorporated into the ultimate allocation decision",
    "start": "1134480",
    "end": "1140480"
  },
  {
    "text": "so it extends it beyond uh simply just pneuma node affinity support for pod",
    "start": "1140480",
    "end": "1147200"
  },
  {
    "text": "level resource alignment so the topology manager currently attempts to align all resources",
    "start": "1147200",
    "end": "1153760"
  },
  {
    "text": "for a single container this enhancement would extend the scope to allow alignment of all resources for all",
    "start": "1153760",
    "end": "1160720"
  },
  {
    "text": "containers in a single pod and there's been a cap approved and merged and that's linked",
    "start": "1160720",
    "end": "1166160"
  },
  {
    "text": "here in the in the slides pneuma alignment for huge pages uh as",
    "start": "1166160",
    "end": "1171280"
  },
  {
    "text": "mentioned earlier the current hint providers for topology manager are the cpu manager and the device manager",
    "start": "1171280",
    "end": "1176480"
  },
  {
    "text": "which provide hints for cpu and device resources naturally but this proposal is for a new component",
    "start": "1176480",
    "end": "1182720"
  },
  {
    "text": "called the memory manager which would provide hints and ultimately new alignment for uh memory and huge pages and there's a cap",
    "start": "1182720",
    "end": "1189760"
  },
  {
    "text": "under review on this uh topology aware scheduling uh there's been lots of discussion and talk on this topic",
    "start": "1189760",
    "end": "1195840"
  },
  {
    "text": "um you know should the scheduler be pneuma aware should this be an entry solution or an",
    "start": "1195840",
    "end": "1200960"
  },
  {
    "text": "out of tree solution etc etc and there's a lot of ongoing work in this area across red hat",
    "start": "1200960",
    "end": "1206640"
  },
  {
    "text": "intel samsung huawei and others and there's a couple of kept included in the slides if you again if you'd like to",
    "start": "1206640",
    "end": "1212400"
  },
  {
    "text": "learn more on that uh per pod alignment policy so as mentioned in the",
    "start": "1212400",
    "end": "1217919"
  },
  {
    "text": "on the topology manager policies the policy is statically configured at node level at cubelet startup time",
    "start": "1217919",
    "end": "1224799"
  },
  {
    "text": "and so this enhancement would allow the user to specify a policy in the pod spec for a particular part now this change is",
    "start": "1224799",
    "end": "1231679"
  },
  {
    "text": "something that would require an api change and that that in itself is challenging but it's something that's been discussed briefly",
    "start": "1231679",
    "end": "1238159"
  },
  {
    "text": "as of now there's no official plans are kept but it's something that we've spoken about briefly",
    "start": "1238159",
    "end": "1245039"
  },
  {
    "start": "1245000",
    "end": "1292000"
  },
  {
    "text": "so finally if you would like to find out more about topology manager you can visit kubernetes.io to read up on it or",
    "start": "1245919",
    "end": "1252480"
  },
  {
    "text": "you can read through the blog post that was released earlier this year to go inside with the 1.18 release and the",
    "start": "1252480",
    "end": "1257760"
  },
  {
    "text": "graduation of the feature to beta and again all of this work has been done under the stewardship of signature",
    "start": "1257760",
    "end": "1264400"
  },
  {
    "text": "i just like to give a special mention to uh derek carr of red hat for all his help and also a huge thanks to kevin kluse of",
    "start": "1264400",
    "end": "1271520"
  },
  {
    "text": "nvidia who's done a huge amount of work in this and has been instrumental in taking topology manager to where it is today so",
    "start": "1271520",
    "end": "1278240"
  },
  {
    "text": "if you'd like to get involved or hear more about this or any related projects the link for the",
    "start": "1278240",
    "end": "1284320"
  },
  {
    "text": "weekly signal meeting is also included so i hope you found this informative and thanks for listening and myself and",
    "start": "1284320",
    "end": "1290400"
  },
  {
    "text": "victor are happy to take any questions",
    "start": "1290400",
    "end": "1294159"
  },
  {
    "start": "1292000",
    "end": "1357000"
  },
  {
    "text": "all right i see we have some questions on the q and a board",
    "start": "1301600",
    "end": "1306880"
  },
  {
    "text": "we're working to answer those as we can are there any questions that folks would like to ask now",
    "start": "1306880",
    "end": "1319840"
  },
  {
    "text": "okay so we've got a list of questions uh we'll start um let's start with",
    "start": "1359440",
    "end": "1367280"
  },
  {
    "text": "uh there was a question are there memory allocation hits and so the answer to that is currently",
    "start": "1367280",
    "end": "1374720"
  },
  {
    "text": "there are no memory allocation hints but as connor mentioned in the future",
    "start": "1374720",
    "end": "1381039"
  },
  {
    "text": "enhancements there is a memory manager cap that will allow for hints for both conventional",
    "start": "1381039",
    "end": "1387840"
  },
  {
    "text": "memory and a huge page of memory",
    "start": "1387840",
    "end": "1392080"
  },
  {
    "text": "yeah i see another one victor there um can topology manager reschedule already running pods to achieve best alignment",
    "start": "1394080",
    "end": "1400400"
  },
  {
    "text": "with the request of new pod placement uh so topology manager is a part of the",
    "start": "1400400",
    "end": "1406400"
  },
  {
    "text": "cubeless it's not part of the scheduling decision making um or you know if",
    "start": "1406400",
    "end": "1412799"
  },
  {
    "text": "a pod can in fact be scheduled but then fail and so if you have a deployment",
    "start": "1412799",
    "end": "1418080"
  },
  {
    "text": "you know if you're if you don't get your optimal placement it can be failed and then rescheduled um but as regards to this i think from",
    "start": "1418080",
    "end": "1425919"
  },
  {
    "text": "what i can understand from this specific question and no that's not something that's",
    "start": "1425919",
    "end": "1431120"
  },
  {
    "text": "doable in the current quality manner yes i i agree with that connor can't uh",
    "start": "1431120",
    "end": "1437039"
  },
  {
    "text": "currently no way to reschedule currently running pods they'll either be admitted and be numered or they will",
    "start": "1437039",
    "end": "1443760"
  },
  {
    "text": "fail to be admitted based on the topology management policy",
    "start": "1443760",
    "end": "1448960"
  },
  {
    "text": "um next question is uh there is a question which version of openshift will",
    "start": "1449360",
    "end": "1455440"
  },
  {
    "text": "have topology manager at ga and so the answer to that is",
    "start": "1455440",
    "end": "1460559"
  },
  {
    "text": "topology manager is ga and open shift 4.6",
    "start": "1460559",
    "end": "1467279"
  },
  {
    "text": "another question was uh is it possible to have the slots for this presentation",
    "start": "1470720",
    "end": "1475760"
  },
  {
    "text": "so if you go to the schedule uh on the agenda the slots are there",
    "start": "1475760",
    "end": "1482080"
  },
  {
    "text": "so you should be able to find them",
    "start": "1482080",
    "end": "1487679"
  },
  {
    "text": "okay uh connor you want to take the question about uh how to check the let's see what's the question how do",
    "start": "1487679",
    "end": "1494559"
  },
  {
    "text": "you check the uh um to see if the",
    "start": "1494559",
    "end": "1501360"
  },
  {
    "text": "how can we check in a running cluster if the requested resources for each pod are on the same number node",
    "start": "1501360",
    "end": "1507360"
  },
  {
    "text": "so there there are two answers one is you can for example on your system",
    "start": "1507360",
    "end": "1513760"
  },
  {
    "text": "to check for the cpus you can do ls cpu and you can on the multi-socket system you'll see",
    "start": "1513760",
    "end": "1519440"
  },
  {
    "text": "the cpus and the pneuma node assigned and then you can for example ssh or connect to the pod and",
    "start": "1519440",
    "end": "1526880"
  },
  {
    "text": "look at the cpus that are signed and connor you also had another way to check this",
    "start": "1526880",
    "end": "1532640"
  },
  {
    "text": "yeah so you can also check through the um the kubrick checkpoint file um so that",
    "start": "1532640",
    "end": "1538400"
  },
  {
    "text": "will list any exclusive cpu allocations to containers",
    "start": "1538400",
    "end": "1543600"
  },
  {
    "text": "or devices and it won't tell you the pneuma node but you can you it will tell you which cpu id it is",
    "start": "1543600",
    "end": "1550159"
  },
  {
    "text": "and then you can cross-reference that with something like lscpu and there's some other things popped into my head there",
    "start": "1550159",
    "end": "1555760"
  },
  {
    "text": "interestingly i've seen um a kef uh in the last week or two to",
    "start": "1555760",
    "end": "1561840"
  },
  {
    "text": "uh extend the pod resources uh cube endpoints to be able to show you",
    "start": "1561840",
    "end": "1568320"
  },
  {
    "text": "information such as um numerous information on cpus allocated",
    "start": "1568320",
    "end": "1574400"
  },
  {
    "text": "to containers and pods and i think it's very early days obviously if there's only kept involved",
    "start": "1574400",
    "end": "1579760"
  },
  {
    "text": "but um that would be something that would be very useful i think going forward",
    "start": "1579760",
    "end": "1585840"
  },
  {
    "text": "okay next question would it be possible for you guys to develop a",
    "start": "1586799",
    "end": "1592559"
  },
  {
    "text": "scanning feature to check how much we could improve performance if we use topology",
    "start": "1592559",
    "end": "1598080"
  },
  {
    "text": "manager i'm not quite sure what you mean by scanning feature but you know what what has been done is",
    "start": "1598080",
    "end": "1605679"
  },
  {
    "text": "connor and his team at intel have have done some testing and the results are",
    "start": "1605679",
    "end": "1612960"
  },
  {
    "text": "uh posted and you know shared in some of the improvements that were that were there",
    "start": "1612960",
    "end": "1618559"
  },
  {
    "text": "i'm not quite sure what the what the scanning feature would be um so i think uh i don't know if you can",
    "start": "1618559",
    "end": "1626720"
  },
  {
    "text": "rephrase the question maybe that would help connor do you have any input on that one um no just that and again there's a link",
    "start": "1626720",
    "end": "1634640"
  },
  {
    "text": "to the white paper on that included in the slides and you'll want to check it out in a bit more detail probably all for a bit more than what",
    "start": "1634640",
    "end": "1640240"
  },
  {
    "text": "was mentioned in the in the presentation um i'm just looking there's one really long question in there",
    "start": "1640240",
    "end": "1646320"
  },
  {
    "text": "um describe the setup uh take me a few minutes to get through it and whoever",
    "start": "1646320",
    "end": "1653039"
  },
  {
    "text": "asked that maybe you could sync with us afterwards um i i will be heading over to the",
    "start": "1653039",
    "end": "1660240"
  },
  {
    "text": "the intel booth i think there's a zoom room set up for any q and a's people",
    "start": "1660240",
    "end": "1666080"
  },
  {
    "text": "have afterwards and also the slack channel will be open if you want to carry on that discussion",
    "start": "1666080",
    "end": "1672880"
  },
  {
    "text": "okay next question if i am using host network",
    "start": "1678559",
    "end": "1685360"
  },
  {
    "text": "if i am using host network with node with multiple interfaces can i use topology manager to allocate",
    "start": "1685360",
    "end": "1691600"
  },
  {
    "text": "interfaces to a pod so um you know again topology manager",
    "start": "1691600",
    "end": "1697360"
  },
  {
    "text": "requires a hint provider for things like cpus and devices so",
    "start": "1697360",
    "end": "1703919"
  },
  {
    "text": "right now to use multiple interfaces in kubernetes we have it requires um",
    "start": "1703919",
    "end": "1712559"
  },
  {
    "text": "multis and so with multus it does support having multiple srov interfaces",
    "start": "1712559",
    "end": "1719600"
  },
  {
    "text": "and we can align sr for example srv virtual functions so if you have",
    "start": "1719600",
    "end": "1727200"
  },
  {
    "text": "if you want to use multiple interfaces one way that is supported is to use",
    "start": "1727200",
    "end": "1732559"
  },
  {
    "text": "the srav device plugin and you can get high speed interfaces with that now the question about foundation host",
    "start": "1732559",
    "end": "1738880"
  },
  {
    "text": "network uh i'm not quite sure about that one",
    "start": "1738880",
    "end": "1744159"
  },
  {
    "text": "um i'd have to follow up with you offline maybe or in slack to get more info on that",
    "start": "1744159",
    "end": "1751120"
  },
  {
    "text": "um tom do you have any additional input on that one uh no sorry i was just reading through",
    "start": "1751120",
    "end": "1757600"
  },
  {
    "text": "another one um this one there is the support for private and public clouds",
    "start": "1757600",
    "end": "1764399"
  },
  {
    "text": "um so financial manager was alpha for kubernetes 1.16",
    "start": "1764399",
    "end": "1770880"
  },
  {
    "text": "and base as of 1.18 um and it's it's integrated into the",
    "start": "1770880",
    "end": "1778399"
  },
  {
    "text": "cubelet okay are there any questions that we",
    "start": "1778840",
    "end": "1784399"
  },
  {
    "text": "haven't covered i'm looking to see",
    "start": "1784399",
    "end": "1788320"
  },
  {
    "text": "i think have we addressed them all okay uh",
    "start": "1789440",
    "end": "1796480"
  },
  {
    "text": "i think that's all the questions that um",
    "start": "1798320",
    "end": "1803440"
  },
  {
    "text": "i'm just going through reading these okay so if you want to uh you know",
    "start": "1803440",
    "end": "1810640"
  },
  {
    "text": "continue the conversation you have more questions take a look at the um channel number two",
    "start": "1810640",
    "end": "1816880"
  },
  {
    "text": "kubecon custom extend k8 on your slack workspace after the sessions ends",
    "start": "1816880",
    "end": "1822480"
  },
  {
    "text": "we'll be there uh to answer any questions and you know chat with you and thank you for you know attending",
    "start": "1822480",
    "end": "1829919"
  },
  {
    "text": "feel free to uh you know reach us over there and as connor mentioned uh connor i think you said there was a",
    "start": "1829919",
    "end": "1836960"
  },
  {
    "text": "intel room you want to share some more on that again yeah i'd be heading over to the intel booth now to join",
    "start": "1836960",
    "end": "1843279"
  },
  {
    "text": "um i think there's a zoom room set up for any q a people might have after the talk so if anyone has any follow-ups",
    "start": "1843279",
    "end": "1849440"
  },
  {
    "text": "feel free to head over there and yeah thanks for joining hope you found it informative",
    "start": "1849440",
    "end": "1856960"
  }
]