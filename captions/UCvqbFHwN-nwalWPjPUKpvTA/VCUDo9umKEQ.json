[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "text": "uh good morning good evening good afternoon depending on where you are in the world uh thank you for joining us",
    "start": "1120",
    "end": "6799"
  },
  {
    "text": "today for today's cncf webinar mlo ops operation with git based",
    "start": "6799",
    "end": "12480"
  },
  {
    "text": "ci cd for ml i'm kristy tan i'll be moderating today's webinar we would like to welcome our presenter",
    "start": "12480",
    "end": "19359"
  },
  {
    "text": "today narone javi co-founder and cto of iguazio",
    "start": "19359",
    "end": "24960"
  },
  {
    "text": "a few housekeeping items before we get started during the webinar you are not able to talk as an attendee",
    "start": "24960",
    "end": "30960"
  },
  {
    "text": "there is a q a box at the bottom of your screen please feel free to drop your questions in there",
    "start": "30960",
    "end": "36239"
  },
  {
    "text": "and we'll get to as many as we can at the end this is an official webinar of the cncf",
    "start": "36239",
    "end": "41680"
  },
  {
    "text": "and as such is subject to the cncf code of conduct please do not add anything to the chat",
    "start": "41680",
    "end": "47200"
  },
  {
    "text": "or questions that would be in violation of that code of conduct basically please be respectful of all",
    "start": "47200",
    "end": "52640"
  },
  {
    "text": "your fellow participants and presenters please also note that the recording and slides will be posted later today to the",
    "start": "52640",
    "end": "59039"
  },
  {
    "text": "cncf webinars page at cncf dot io webinars with that i'll hand it over to",
    "start": "59039",
    "end": "65680"
  },
  {
    "text": "your own to kick off today's presentation take it away thank you and uh hi everyone i'm iron",
    "start": "65680",
    "end": "73280"
  },
  {
    "text": "i'm the co-founder and ceo of iguazio iguazio is a very early member of the",
    "start": "73280",
    "end": "79040"
  },
  {
    "text": "cncf and we've been active in a variety of areas what we're going to speak about",
    "start": "79040",
    "end": "84880"
  },
  {
    "text": "today is sort of cicd and devops for machine learning what are the unique characteristics uh what you can",
    "start": "84880",
    "end": "91680"
  },
  {
    "text": "do in order to again bring agility of devops and envelopes into your machine learning",
    "start": "91680",
    "end": "97439"
  },
  {
    "text": "initiatives and obviously leveraging cncf tools like kubernetes and others and with that i'll i'll just start with",
    "start": "97439",
    "end": "104560"
  },
  {
    "text": "the presentation and again the slides will be available later and you can",
    "start": "104560",
    "end": "110320"
  },
  {
    "text": "ping me in linkedin or slack kubernetes or cncf slack i mean both places",
    "start": "110320",
    "end": "119920"
  },
  {
    "text": "so first before we we dive into the solution and what we're trying to solve let's talk about the problem the",
    "start": "119920",
    "end": "126079"
  },
  {
    "start": "120000",
    "end": "120000"
  },
  {
    "text": "main problem today in machine learning is that once you've finalized the research",
    "start": "126079",
    "end": "131840"
  },
  {
    "text": "and you develop something in small scale you extracted some data you ran some exploratory data analysis machine",
    "start": "131840",
    "end": "140000"
  },
  {
    "text": "learning again in small scale in your notebook and evaluation you feel you're ready and now you need",
    "start": "140000",
    "end": "146080"
  },
  {
    "text": "to create something that runs in production the production implementation usually is a",
    "start": "146080",
    "end": "151680"
  },
  {
    "text": "re-implementation it uses different frameworks it's designed for a scale it's designed to",
    "start": "151680",
    "end": "156800"
  },
  {
    "text": "to be always on it needs to be observed and all those things that signify a sort of",
    "start": "156800",
    "end": "162560"
  },
  {
    "text": "a production environment if you look at the diagram on the on the right you see that",
    "start": "162560",
    "end": "168800"
  },
  {
    "text": "we need to feed data from real data sources from production data sources from streaming data we end up with very",
    "start": "168800",
    "end": "175599"
  },
  {
    "text": "large scale data so you have to analyze it in a distributed fashion not within our notebook",
    "start": "175599",
    "end": "181920"
  },
  {
    "text": "later on we need to run training and this training may run in sort of a ci manner in an automate",
    "start": "181920",
    "end": "187840"
  },
  {
    "text": "automated fashion every time a data changes or every time we change code or logic or models and finally we need",
    "start": "187840",
    "end": "195840"
  },
  {
    "text": "to serve the models as well as monitor them and serving a model requires also having real-time data that is",
    "start": "195840",
    "end": "203599"
  },
  {
    "text": "exactly the same like the training data just runs faster in a more interactive way so",
    "start": "203599",
    "end": "210480"
  },
  {
    "text": "usually when people build those solutions they use they re-implement in some cases they use hadoop and spark",
    "start": "210480",
    "end": "217120"
  },
  {
    "text": "and streaming technologies in some cases they leverage kubernetes uh sometimes they change the code from",
    "start": "217120",
    "end": "223280"
  },
  {
    "text": "python to java or rewrite the code to make it more hardened take into consideration security and",
    "start": "223280",
    "end": "230640"
  },
  {
    "text": "other aspects one of the main challenges in that transition is that when data scientists",
    "start": "230640",
    "end": "238560"
  },
  {
    "start": "233000",
    "end": "233000"
  },
  {
    "text": "implement code they write it usually in sort of notebooks and data scientists focuses not on",
    "start": "238560",
    "end": "243840"
  },
  {
    "text": "coding they're not developers they're data scientists so their focus are is on the best mathematics the best",
    "start": "243840",
    "end": "250000"
  },
  {
    "text": "statistics algorithms the best deep learning or neural networks and not necessarily on making their code",
    "start": "250000",
    "end": "256560"
  },
  {
    "text": "uh robust making their code deployable scalable and all those things that is a software",
    "start": "256560",
    "end": "263040"
  },
  {
    "text": "developer or a devil person you usually care about so the main challenge is how do you take code that wasn't",
    "start": "263040",
    "end": "269919"
  },
  {
    "text": "designed for anything operational usually if you look at the notebook it's not even divided into functional building",
    "start": "269919",
    "end": "276400"
  },
  {
    "text": "blocks you have the same notebook with training with data preparation with charting with the",
    "start": "276400",
    "end": "282400"
  },
  {
    "text": "you know sort of inferencing everything in one jumbo notebook and at the end in order to run",
    "start": "282400",
    "end": "287840"
  },
  {
    "text": "it in production we have to turn it into a set of micro services that will run",
    "start": "287840",
    "end": "293440"
  },
  {
    "text": "on a cluster in a real time or batch pipeline so that's the main challenge that we",
    "start": "293440",
    "end": "299600"
  },
  {
    "text": "need to solve before thinking about how to operationalize or automate our data science pipelines",
    "start": "299600",
    "end": "307840"
  },
  {
    "start": "307000",
    "end": "307000"
  },
  {
    "text": "so just looking into the various steps in productizing any artifact in",
    "start": "307840",
    "end": "314080"
  },
  {
    "text": "engineering data science or even traditional software but when you're dealing with data there are",
    "start": "314080",
    "end": "319840"
  },
  {
    "text": "bigger challenges especially around data parallelization and performance and",
    "start": "319840",
    "end": "324960"
  },
  {
    "text": "others so if we take our code the first step what we need to do is package it in in docker and have uh",
    "start": "324960",
    "end": "332320"
  },
  {
    "text": "scripts and make and and command lines and all of that and you think it's trivial but for a data",
    "start": "332320",
    "end": "338560"
  },
  {
    "text": "scientist it's probably not trivial so we need to sometimes someone else will take the code and convert it into a dockerized",
    "start": "338560",
    "end": "345600"
  },
  {
    "text": "uh containerized workload and then you think you need to think about scaling out and scaling out when you're dealing",
    "start": "345600",
    "end": "352000"
  },
  {
    "text": "with data requires various aspects not just load balancing or service matches sometimes you have to",
    "start": "352000",
    "end": "358479"
  },
  {
    "text": "partition the data you have to partition your model distributed uh maybe run automl or",
    "start": "358479",
    "end": "364160"
  },
  {
    "text": "hyperparameter tuning which is running various permutations with the same",
    "start": "364160",
    "end": "369280"
  },
  {
    "text": "logic and different parameters finally when you've done that you need to think about performance data",
    "start": "369280",
    "end": "375919"
  },
  {
    "text": "code that was written for research usually doesn't have low latency it doesn't",
    "start": "375919",
    "end": "381360"
  },
  {
    "text": "have high throughput and someone needs to start thinking about you know what maybe i need to cache",
    "start": "381360",
    "end": "386400"
  },
  {
    "text": "queries maybe i need to introduce gpus add partialism make it asynchronous",
    "start": "386400",
    "end": "393120"
  },
  {
    "text": "instead of synchronous um etc and all those things require engineering efforts required developers",
    "start": "393120",
    "end": "399759"
  },
  {
    "text": "they do this engineering uh they could be developers they could be data engineers depending on the organization sometimes",
    "start": "399759",
    "end": "407039"
  },
  {
    "text": "they are they're called ml engineers um once we have the performance sorted and uh",
    "start": "407039",
    "end": "412479"
  },
  {
    "text": "ready then we need to think about all the instrumentation aspects uh monitoring uh logging instrument",
    "start": "412479",
    "end": "419360"
  },
  {
    "text": "observability you know versioning security now this goes beyond the traditional micro services and cloud native",
    "start": "419360",
    "end": "426000"
  },
  {
    "text": "architecture because we're not just monitoring the container we're we have to log and monitor",
    "start": "426000",
    "end": "431199"
  },
  {
    "text": "the data we have to monitor the versioning of the data we have to apply security also for data",
    "start": "431199",
    "end": "438319"
  },
  {
    "text": "access so this is more complicated especially when there is liability associated with machine",
    "start": "438319",
    "end": "445120"
  },
  {
    "text": "learning if if our model doesn't predict things properly uh we may lose money we may",
    "start": "445120",
    "end": "451199"
  },
  {
    "text": "um annoy someone and he wants to sue us because we made some gender bias so",
    "start": "451199",
    "end": "456319"
  },
  {
    "text": "everything we do in the pipeline our code our data our models our algorithms everything has",
    "start": "456319",
    "end": "462160"
  },
  {
    "text": "to be a and controlled and finally we don't want to repeat the same",
    "start": "462160",
    "end": "467440"
  },
  {
    "text": "thing over and over again so we we have to apply automation pretty much like we're doing with other",
    "start": "467440",
    "end": "472960"
  },
  {
    "text": "cloud architectures we need to apply ci cd and automated workflows and rolling",
    "start": "472960",
    "end": "478960"
  },
  {
    "text": "upgrades and canaries and a b testing the main challenge here is that once you",
    "start": "478960",
    "end": "484240"
  },
  {
    "text": "have the prototype up and running it may take a few weeks because today machine learning is well",
    "start": "484240",
    "end": "489360"
  },
  {
    "text": "understood you just take an extra boost model and you feed it with some data and you get a model",
    "start": "489360",
    "end": "494560"
  },
  {
    "text": "um the main challenge today is that you take productizing the the entire thing would take about 12 to 18 months from",
    "start": "494560",
    "end": "501759"
  },
  {
    "text": "many organizations that i work with so the main challenge is how to",
    "start": "501759",
    "end": "507759"
  },
  {
    "text": "automate that and also break the silos between developers data scientists data engineers and let them work on one",
    "start": "507759",
    "end": "515120"
  },
  {
    "text": "ecosystem so first what is mlab so we understand what is devops devops road is",
    "start": "515120",
    "end": "521120"
  },
  {
    "text": "essentially uh to combine the development and operations and think together how they can",
    "start": "521120",
    "end": "526399"
  },
  {
    "text": "accelerate the business or how they can allow the business to deploy software faster",
    "start": "526399",
    "end": "532080"
  },
  {
    "text": "in an operationalized manner so amalops is essentially similar just extends it with a lot of data and",
    "start": "532080",
    "end": "538320"
  },
  {
    "text": "machine learning uh practices uh of data engineering again building models liability observability",
    "start": "538320",
    "end": "546000"
  },
  {
    "text": "all those aspects so let's take into account a use case a simple use case would be",
    "start": "546000",
    "end": "552720"
  },
  {
    "start": "548000",
    "end": "548000"
  },
  {
    "text": "predictive maintenance pipeline so we can see that even in this pipeline",
    "start": "552720",
    "end": "558880"
  },
  {
    "text": "we have many variety we have many different steps the first steps is to essentially bring the",
    "start": "558880",
    "end": "564640"
  },
  {
    "text": "data in the data could come in different forms it could come in streaming data or just come from a",
    "start": "564640",
    "end": "572399"
  },
  {
    "text": "variety of databases operational databases and we need to run some etl we take all this that data and throw it",
    "start": "572399",
    "end": "579040"
  },
  {
    "text": "into sort of a file usually sort of uh files in a file system",
    "start": "579040",
    "end": "584880"
  },
  {
    "text": "later we we need to run some processing some analysis to combine different data sets to",
    "start": "584880",
    "end": "590000"
  },
  {
    "text": "denormalize to join to aggregate and forum what we usually call a feature",
    "start": "590000",
    "end": "595120"
  },
  {
    "text": "vector or a batch feature vector that feature vector essentially have a denormalized state of all the",
    "start": "595120",
    "end": "601519"
  },
  {
    "text": "features that we're going to train our model with once we've we have that feature set we",
    "start": "601519",
    "end": "607040"
  },
  {
    "text": "don't necessarily want all the features so we have to do some feature analysis and feature detection",
    "start": "607040",
    "end": "612720"
  },
  {
    "text": "and detect which features are the more meaningful maybe they're some of them should be dropped maybe some",
    "start": "612720",
    "end": "618399"
  },
  {
    "text": "some of them should be emphasized or changed or modified so we have to have another step of",
    "start": "618399",
    "end": "624640"
  },
  {
    "text": "feature uh detection and processing and then we have some data that we want to feed into our",
    "start": "624640",
    "end": "631120"
  },
  {
    "text": "training logic usually we split the data into a training set and validation set smaller validation set and we run our",
    "start": "631120",
    "end": "638320"
  },
  {
    "text": "logic and we we measure the accuracy of our our model and finally we generate the model along",
    "start": "638320",
    "end": "644880"
  },
  {
    "text": "with some sort of reports tell us how good is our model or maybe other aspects of our model",
    "start": "644880",
    "end": "652160"
  },
  {
    "text": "this is all just creating the model what about running this in a more production",
    "start": "652160",
    "end": "657600"
  },
  {
    "text": "environment you have to deploy it in an online system so you have to take this model",
    "start": "657600",
    "end": "662720"
  },
  {
    "text": "and wrap it in a container it's not just the model and the container you also have to provide the",
    "start": "662720",
    "end": "668560"
  },
  {
    "text": "apis that for example if i'm providing you a user id and maybe an ad number or something like that you",
    "start": "668560",
    "end": "675839"
  },
  {
    "text": "have to enrich that data into a full blown feature vector similar to the one that you've been",
    "start": "675839",
    "end": "682320"
  },
  {
    "text": "using in the training and only then pass it to the model so this is",
    "start": "682320",
    "end": "687760"
  },
  {
    "text": "real-time feature engineering aspect that needs to take data from the real",
    "start": "687760",
    "end": "693200"
  },
  {
    "text": "data sources after you've done your model serving you have to track all the behavior of your model so you",
    "start": "693200",
    "end": "700240"
  },
  {
    "text": "can identify if the model is misbehaving if the accuracy is getting lost all models eventually lose accuracy and",
    "start": "700240",
    "end": "707839"
  },
  {
    "text": "sometimes also due to the challenge of having the same data for training and serving the models are in day one",
    "start": "707839",
    "end": "715279"
  },
  {
    "text": "already inaccurate because you're if you're not giving the model it's essentially a math equation you won't give it the same data",
    "start": "715279",
    "end": "721920"
  },
  {
    "text": "it won't produce the same results so anyway we have to have a tracking system that monitors all the data all the",
    "start": "721920",
    "end": "728639"
  },
  {
    "text": "requests all the responses of our model and detect the behavior the performance",
    "start": "728639",
    "end": "733839"
  },
  {
    "text": "adrift and this can also be combined later on with real data after",
    "start": "733839",
    "end": "740320"
  },
  {
    "text": "we've made some prediction after a while we know if our prediction was correct or not and we can analyze that with reference",
    "start": "740320",
    "end": "747519"
  },
  {
    "text": "data and measure how well behaving is our model in many cases we actually don't serve a single",
    "start": "747519",
    "end": "753760"
  },
  {
    "text": "model we serve things called ensembles or other more complicated scenarios and we can essentially say you",
    "start": "753760",
    "end": "761040"
  },
  {
    "text": "know what this one has better performance let's prioritize that let's give it a bigger weight",
    "start": "761040",
    "end": "766079"
  },
  {
    "text": "etc or we may need to retrain the model and feed it with fresh data",
    "start": "766079",
    "end": "773279"
  },
  {
    "text": "that increases our util sorry our accuracy so what you see here is a very",
    "start": "773279",
    "end": "779360"
  },
  {
    "text": "complicated flow of delivering machine learning into production the way that we see the",
    "start": "779360",
    "end": "786160"
  },
  {
    "text": "one of the ways to accelerate that is to essentially take those micro services and use the concept of serverless",
    "start": "786160",
    "end": "792639"
  },
  {
    "text": "functions not necessarily the serverless functions that you guys think of of sort of event driven but serverless",
    "start": "792639",
    "end": "798639"
  },
  {
    "text": "functions that also know how to process data and could be stateful uh etc and why",
    "start": "798639",
    "end": "804560"
  },
  {
    "text": "because we can use those functions we can even put them in a library and reuse functions and scale them out because",
    "start": "804560",
    "end": "812320"
  },
  {
    "text": "usually serverless engines know how to scale out workloads etc",
    "start": "812320",
    "end": "818160"
  },
  {
    "start": "818000",
    "end": "818000"
  },
  {
    "text": "one of the challenges with integrating such a big solution is that you have to use many",
    "start": "818800",
    "end": "824639"
  },
  {
    "text": "different tools for offline for online for data engineering for machine learning and you end up",
    "start": "824639",
    "end": "830399"
  },
  {
    "text": "having some cloud provider tools and maybe something in hadoop and something in kubernetes and something on your desktop",
    "start": "830399",
    "end": "838160"
  },
  {
    "text": "and it's a big nightmare it's a big nightmare to develop the security and it's a big",
    "start": "838160",
    "end": "843680"
  },
  {
    "text": "nightmare to make sure everything works and maintain multiple platforms so we believe that kubernetes is should",
    "start": "843680",
    "end": "850639"
  },
  {
    "text": "be a standard for all of those tools so instead of having like a silo hadoop and some silos of of",
    "start": "850639",
    "end": "858079"
  },
  {
    "text": "sas services etc if we can put everything on kubernetes and build an ecosystem",
    "start": "858079",
    "end": "863600"
  },
  {
    "text": "around it that could ease the deployment hassle we can use the same security paradigms the",
    "start": "863600",
    "end": "869600"
  },
  {
    "text": "same api concepts etc so essentially what do we want to do we",
    "start": "869600",
    "end": "874800"
  },
  {
    "text": "want to build a system where we can feed data from a variety of sources we",
    "start": "874800",
    "end": "880320"
  },
  {
    "text": "can run data preparation at scale on the same system run accelerated training at scale again",
    "start": "880320",
    "end": "887519"
  },
  {
    "text": "on the same system and finally deploy the models as a micro set of micro services",
    "start": "887519",
    "end": "893760"
  },
  {
    "text": "with apis and future engineering to go along with the deployment and even monitoring",
    "start": "893760",
    "end": "899120"
  },
  {
    "text": "services all of that on the same platform and also what we another thing that we need to take care",
    "start": "899120",
    "end": "904240"
  },
  {
    "text": "of is sharing data across those the variety of steps in a pipeline so if we",
    "start": "904240",
    "end": "910480"
  },
  {
    "start": "910000",
    "end": "910000"
  },
  {
    "text": "open up this pipeline and look into what is really built into that pipeline so we have to",
    "start": "910480",
    "end": "916560"
  },
  {
    "text": "have three layers in the solution the first underlying layer is data so we need we need the data to store all this",
    "start": "916560",
    "end": "924480"
  },
  {
    "text": "information intermediate information between one step and another sometimes this data is uh",
    "start": "924480",
    "end": "931440"
  },
  {
    "text": "structured sometimes this data is unstructured sometimes it's stable sometimes it's",
    "start": "931440",
    "end": "937199"
  },
  {
    "text": "files but the first step would essentially ingest data from a variety of sources",
    "start": "937199",
    "end": "942720"
  },
  {
    "text": "again etl streaming log scraping then we are going to run microservices",
    "start": "942720",
    "end": "948399"
  },
  {
    "text": "for data preparation they will generate a subset of data finally uh later on we run training when you're",
    "start": "948399",
    "end": "956240"
  },
  {
    "text": "running training we may want to run different micro services that run different algorithms in parallel and then choose the best",
    "start": "956240",
    "end": "963279"
  },
  {
    "text": "model that has the best algorithm sometimes it's referred to as automl",
    "start": "963279",
    "end": "968800"
  },
  {
    "text": "or there is also a notion of hyperparameter tuning where you can run many different permutations of the same model you you",
    "start": "968800",
    "end": "976240"
  },
  {
    "text": "generate the model you generate some results then you do validation and finally deployment now",
    "start": "976240",
    "end": "982959"
  },
  {
    "text": "one of the key things here is that if we can leverage the concept of serverless functions we can very easily go and",
    "start": "982959",
    "end": "989120"
  },
  {
    "text": "deploy some code have fully automated flow of delivering the code fully automated for",
    "start": "989120",
    "end": "994880"
  },
  {
    "text": "flow for instrumenting the code and scaling the code etc and on top we need",
    "start": "994880",
    "end": "1001759"
  },
  {
    "text": "some orchestration framework for tracking and managing this workflow and there are some solutions",
    "start": "1001759",
    "end": "1007920"
  },
  {
    "text": "for that in the kubernetes ecosystem as i'm going to demonstrate",
    "start": "1007920",
    "end": "1025760"
  },
  {
    "text": "okay so uh let's look into the architecture how we",
    "start": "1025760",
    "end": "1032160"
  },
  {
    "start": "1029000",
    "end": "1029000"
  },
  {
    "text": "can deliver such a solution so the first thing that we need to have is sort of a data layer",
    "start": "1032160",
    "end": "1037839"
  },
  {
    "text": "data layer usually will comprise of two types of data one is sort of structured high performance data in the form of",
    "start": "1037839",
    "end": "1044000"
  },
  {
    "text": "server database or a high speed shared file system the second",
    "start": "1044000",
    "end": "1049600"
  },
  {
    "text": "thing will be sort of where you put your blobs and your large-scale storage data warehouses",
    "start": "1049600",
    "end": "1055679"
  },
  {
    "text": "different cloud services that serve objects or files or databases on top of it we need to have a",
    "start": "1055679",
    "end": "1062880"
  },
  {
    "text": "cluster with all those shared resources so kubernetes is the ideal environment for that on top",
    "start": "1062880",
    "end": "1069039"
  },
  {
    "text": "of it there are many different services that comprise this ecosystem or this pipeline that i just showed",
    "start": "1069039",
    "end": "1075360"
  },
  {
    "text": "require all those separate tools things for deep learning like tensorflow and pi torch",
    "start": "1075360",
    "end": "1081440"
  },
  {
    "text": "services for data analysis like spark or presto or task uh services for um",
    "start": "1081440",
    "end": "1088960"
  },
  {
    "text": "training like corvo et cetera we need to put all of those in one framework and we could also use",
    "start": "1088960",
    "end": "1094720"
  },
  {
    "text": "other serverless frameworks like nucleo in order to do things like service and real-time",
    "start": "1094720",
    "end": "1099760"
  },
  {
    "text": "serving real-time ingestion stream processing etc on top of it we need to have a set of",
    "start": "1099760",
    "end": "1106559"
  },
  {
    "text": "higher level tools for rml for example to automate this workflow of training to automate this workflow of",
    "start": "1106559",
    "end": "1114000"
  },
  {
    "text": "feature selection also we need to have sort of experiment and run execution tracking",
    "start": "1114000",
    "end": "1119840"
  },
  {
    "text": "so everything that we do has to be tracked automatically we don't want to have manual work around",
    "start": "1119840",
    "end": "1125280"
  },
  {
    "text": "it we want to have a feature store feature store is a concept where all those features that we're going to",
    "start": "1125280",
    "end": "1131120"
  },
  {
    "text": "produce and consume are managed in one server place including all the automation and recipes",
    "start": "1131120",
    "end": "1137120"
  },
  {
    "text": "of how to produce those and finally we need a workflow engine one of the best workflow engines on top",
    "start": "1137120",
    "end": "1143679"
  },
  {
    "text": "of kubernetes is something called argo and that has a flavor",
    "start": "1143679",
    "end": "1148720"
  },
  {
    "text": "this suited for machine learning and data science which is called cube flow or kubeflow pipelines and",
    "start": "1148720",
    "end": "1155039"
  },
  {
    "text": "obviously we want to integrate all of that with source control like git",
    "start": "1155039",
    "end": "1164510"
  },
  {
    "start": "1161000",
    "end": "1161000"
  },
  {
    "text": "[Music] okay so uh looking at my sorry",
    "start": "1164510",
    "end": "1172280"
  },
  {
    "text": "[Music] another uh thing that we we discussed is a use of serverless",
    "start": "1172280",
    "end": "1178000"
  },
  {
    "text": "functions in order to accelerate all this all this flow of developing model of developing",
    "start": "1178000",
    "end": "1184080"
  },
  {
    "text": "the code making it a container running the scalability accelerating the performance doing the",
    "start": "1184080",
    "end": "1189840"
  },
  {
    "text": "instrumentation and also all the ci aspects of that so what we want to do is",
    "start": "1189840",
    "end": "1195760"
  },
  {
    "text": "again automate the code to production flow the other things that we mentioned that many of those use cases require",
    "start": "1195760",
    "end": "1202480"
  },
  {
    "text": "large scale processing so we need elastic scaling of those uh processing and sometimes maybe scale",
    "start": "1202480",
    "end": "1208799"
  },
  {
    "text": "to zero because we don't want to consume gpus and cpus if our workloads are not really working",
    "start": "1208799",
    "end": "1215440"
  },
  {
    "text": "we we mentioned having to to do logging and monitoring if we can leverage that freely uh why not",
    "start": "1215440",
    "end": "1222480"
  },
  {
    "text": "and there is another point that usually if people think of serverless as something which is slower uh the solution that we developed in our",
    "start": "1222480",
    "end": "1229520"
  },
  {
    "text": "open source like nucleo and mlran and others are essentially designed to accelerate your",
    "start": "1229520",
    "end": "1234799"
  },
  {
    "text": "performance because they bring in a lot of techniques from high-performance computing and real-time",
    "start": "1234799",
    "end": "1240080"
  },
  {
    "text": "processing and for example with nuclear you see your example of a benchmark running the same data processing",
    "start": "1240080",
    "end": "1246720"
  },
  {
    "text": "function there is a blog on that in towards data science the same data processing function running",
    "start": "1246720",
    "end": "1252480"
  },
  {
    "text": "as a simple container with python runs about 20 megabytes per second",
    "start": "1252480",
    "end": "1257520"
  },
  {
    "text": "and running the same code exactly without modifying a single line would run about 500 megabytes per second",
    "start": "1257520",
    "end": "1264080"
  },
  {
    "text": "on a nuclear function controller and that's because it knows how to create parallelism for python it knows how to",
    "start": "1264080",
    "end": "1271440"
  },
  {
    "text": "handle memory and gpus in a much more efficient way than the traditional",
    "start": "1271440",
    "end": "1277120"
  },
  {
    "text": "things that wrap your code another thing that once you've done that we have a building blocks that people",
    "start": "1277120",
    "end": "1283120"
  },
  {
    "text": "can publish and other people can consume and build a workflow out of it and you can also form server marketplace",
    "start": "1283120",
    "end": "1289600"
  },
  {
    "text": "so people can just drag functions place them in a pipeline run this entire",
    "start": "1289600",
    "end": "1294720"
  },
  {
    "text": "pipeline and maybe even use functions that are pre-built like automl functions and",
    "start": "1294720",
    "end": "1299760"
  },
  {
    "text": "deployment functions that i'm going to demonstrate",
    "start": "1299760",
    "end": "1305840"
  },
  {
    "start": "1306000",
    "end": "1306000"
  },
  {
    "text": "one of the things that when we think about serverless we usually think about event driven processing like amazon",
    "start": "1306480",
    "end": "1311919"
  },
  {
    "text": "lambda or k native or things things like that now the point is that if you're running",
    "start": "1311919",
    "end": "1318080"
  },
  {
    "text": "uh machine learning or data engineering uh there are different requirements in serverless",
    "start": "1318080",
    "end": "1323360"
  },
  {
    "text": "typically it's event-driven short-lived uh stateless workloads but when we're processing data we need",
    "start": "1323360",
    "end": "1328799"
  },
  {
    "text": "other things we need jobs that may run for half an hour we need to distribute the workload",
    "start": "1328799",
    "end": "1334320"
  },
  {
    "text": "instead of just using an ingress or a load balancer or service mesh we have to think about the data so we need to apply things like",
    "start": "1334320",
    "end": "1341200"
  },
  {
    "text": "the partitioning data shuffling reduction barriers hyperparameter exchange other techniques",
    "start": "1341200",
    "end": "1348320"
  },
  {
    "text": "that allow us to create partialism for more sophisticated workloads and we have to state",
    "start": "1348320",
    "end": "1354159"
  },
  {
    "text": "workloads with data obviously are stateful and the way that we pass data along the pipeline or the way we pass",
    "start": "1354159",
    "end": "1361280"
  },
  {
    "text": "data between one step and another requires passing not json events but rather a complete data",
    "start": "1361280",
    "end": "1367039"
  },
  {
    "text": "set or a set of parameters so we have to build a new thing that is like serverless in the notion that it's",
    "start": "1367039",
    "end": "1373919"
  },
  {
    "text": "ris has the built-in elasticity it has all the automation around deployment and operations but works with",
    "start": "1373919",
    "end": "1381280"
  },
  {
    "text": "other frameworks so for that one of the things that we've developed is in a package is part of the package",
    "start": "1381280",
    "end": "1387760"
  },
  {
    "start": "1382000",
    "end": "1382000"
  },
  {
    "text": "called mlrung which is a full end to an amalops orchestration layer there is also a portion that",
    "start": "1387760",
    "end": "1395440"
  },
  {
    "text": "automates the creation of crds and treats variety of crds in kubernetes as serverless",
    "start": "1395440",
    "end": "1401200"
  },
  {
    "text": "functions so essentially knows how to orchestrate not only uh you know event driven workloads but",
    "start": "1401200",
    "end": "1407120"
  },
  {
    "text": "things that handle deep learning like horvath or it knows how to orchestrate spark",
    "start": "1407120",
    "end": "1413440"
  },
  {
    "text": "or presto as serverless functions though i can essentially run a spark job as if it was just a function",
    "start": "1413440",
    "end": "1420159"
  },
  {
    "text": "or run a distributed deep learning logic as if it was a function but the trick how the way that it works",
    "start": "1420159",
    "end": "1426880"
  },
  {
    "text": "is essentially allowing you to scale out the containers but also provision",
    "start": "1426880",
    "end": "1432320"
  },
  {
    "text": "low latency messaging across those containers and data exchange across those containers using a low latency",
    "start": "1432320",
    "end": "1439520"
  },
  {
    "text": "data fabric and this way you can scale dynamically the workload even for very intensive",
    "start": "1439520",
    "end": "1445760"
  },
  {
    "text": "jobs it also sandbox all your workload and monitors everything that goes in and",
    "start": "1445760",
    "end": "1451279"
  },
  {
    "text": "out so you automatically get all the experiment tracking all the job tracking as i'm going to show in a",
    "start": "1451279",
    "end": "1457279"
  },
  {
    "text": "demo in a minute so what you gain is you know scale and performance without the hassle",
    "start": "1457279",
    "end": "1463360"
  },
  {
    "text": "and also because it knows how to create a lot of parallelism instead of having a separate system for",
    "start": "1463360",
    "end": "1469039"
  },
  {
    "text": "rml or hyper parameters like katib or others that you need to integrate manually essentially ml run also does automl and",
    "start": "1469039",
    "end": "1476480"
  },
  {
    "text": "hyperparameters so it knows how to distribute workloads and distribute parameters and also exchange algorithms in order to",
    "start": "1476480",
    "end": "1483600"
  },
  {
    "text": "create parallelism for the entire training and analysis phases",
    "start": "1483600",
    "end": "1488880"
  },
  {
    "text": "and again it has a variety of crds that they control and this list is going to grow bigger",
    "start": "1488880",
    "end": "1495039"
  },
  {
    "text": "because it's just essentially through plug-in mechanisms the other very uh critical solution part",
    "start": "1495039",
    "end": "1502320"
  },
  {
    "start": "1499000",
    "end": "1499000"
  },
  {
    "text": "in the solution is a workflow processing engine so this is where argo",
    "start": "1502320",
    "end": "1507520"
  },
  {
    "text": "comes in it's an engine in uh on top of kubernetes that knows how to enforce a state machine",
    "start": "1507520",
    "end": "1513520"
  },
  {
    "text": "and there is a project called kubeflow and qflo pipeline that essentially builds a machine",
    "start": "1513520",
    "end": "1519200"
  },
  {
    "text": "learning pipeline on top of this uh argo uh so what is q flow q flow is a",
    "start": "1519200",
    "end": "1525360"
  },
  {
    "text": "project that has several sub projects within it pipeline is this",
    "start": "1525360",
    "end": "1530960"
  },
  {
    "text": "machine learning orchestration it also has a managed notebooks and it has",
    "start": "1530960",
    "end": "1536480"
  },
  {
    "text": "crds specific to machine learning workloads like distributed tensorflow this distributed",
    "start": "1536480",
    "end": "1542559"
  },
  {
    "text": "xg boost and and uh pythorac etc uh what we've done we've taken those components from",
    "start": "1542559",
    "end": "1548960"
  },
  {
    "text": "kubeflow and we wrapped them in sort of more of a serverless and fully automated frameworks",
    "start": "1548960",
    "end": "1554320"
  },
  {
    "text": "like mlran and nucleo and others and so what it allows you again to publish code into",
    "start": "1554320",
    "end": "1560640"
  },
  {
    "text": "microservices with without doing almost anything it provides a glueless data access and",
    "start": "1560640",
    "end": "1566159"
  },
  {
    "text": "parallelism that i i mentioned it allows you to run steps within cube flow which are distributed",
    "start": "1566159",
    "end": "1572320"
  },
  {
    "text": "usually in kubeflow you'll run it one step at a time every step is a container we now extended it to a notion that",
    "start": "1572320",
    "end": "1578400"
  },
  {
    "text": "every step in a pipeline could actually span 20 containers and also have gpu",
    "start": "1578400",
    "end": "1583600"
  },
  {
    "text": "acceleration built into it and everything is tracked automatically code execution data versioning",
    "start": "1583600",
    "end": "1589520"
  },
  {
    "text": "everything is auto tracked through uh some sandboxing technology built into it",
    "start": "1589520",
    "end": "1595679"
  },
  {
    "start": "1595000",
    "end": "1595000"
  },
  {
    "text": "now let's think about the entire flow and start seeing how we we involve git and and more automated workflows one of",
    "start": "1595679",
    "end": "1603120"
  },
  {
    "text": "the challenges of something like kubeflow is that when you run you have to run everything on kubernetes on a distributed pipeline",
    "start": "1603120",
    "end": "1610640"
  },
  {
    "text": "but that's not really how data scientists work data scientists want to work off their notebook and start doing some experiments and",
    "start": "1610640",
    "end": "1617279"
  },
  {
    "text": "then they want to run a single micro service that for example does the training or does some data preparation",
    "start": "1617279",
    "end": "1623520"
  },
  {
    "text": "and finally you want to build a bigger workflow that combines all the individual steps so we have to",
    "start": "1623520",
    "end": "1629600"
  },
  {
    "text": "think also about the data scientists as first class citizens usually the the concept around",
    "start": "1629600",
    "end": "1635200"
  },
  {
    "text": "kubernetes and kubeflow and those tools is that devops or developers are the first",
    "start": "1635200",
    "end": "1640720"
  },
  {
    "text": "class citizens so we have to sort of flip this around and provide something which is very intuitive very familiar to data",
    "start": "1640720",
    "end": "1647520"
  },
  {
    "text": "scientists so one of the things that we have here is an sdk that they can use inside their",
    "start": "1647520",
    "end": "1653520"
  },
  {
    "text": "ide jupiter pycharm vs code etc and and then they run everything within",
    "start": "1653520",
    "end": "1659520"
  },
  {
    "text": "their native environment as if it's just uh you know they can track every experiment they could crack every run",
    "start": "1659520",
    "end": "1666000"
  },
  {
    "text": "etc at a certain point they want to have bigger amount of resources run on real",
    "start": "1666000",
    "end": "1671919"
  },
  {
    "text": "data that's part of a cluster so what they can do from within their notebooks within their environment they",
    "start": "1671919",
    "end": "1677600"
  },
  {
    "text": "just push one command or one api call and essentially what it does it wraps their entire code",
    "start": "1677600",
    "end": "1683600"
  },
  {
    "text": "it ships it into the kubernetes cluster for an api gateway it builds the containers based",
    "start": "1683600",
    "end": "1690000"
  },
  {
    "text": "on the requirements and dependencies and all of that and runs it as a distributed job on the",
    "start": "1690000",
    "end": "1695279"
  },
  {
    "text": "cluster and tracks all the behavior as if it's running in the native environment i'll show you in a minute",
    "start": "1695279",
    "end": "1700720"
  },
  {
    "text": "you actually see the console of the kubernetes job in the jupyter notebook of the client so",
    "start": "1700720",
    "end": "1706960"
  },
  {
    "text": "again gives them the familiar environment they don't need to understand what kubernetes is they just know that their code now",
    "start": "1706960",
    "end": "1713279"
  },
  {
    "text": "runs distributed on a massive cluster and when job is going to end all the",
    "start": "1713279",
    "end": "1718799"
  },
  {
    "text": "resources are going to get decommissioned including gpus cpus memory so that means",
    "start": "1718799",
    "end": "1724559"
  },
  {
    "text": "that we can have a shared cluster for all of those resources now we're cozy we've built a function we see that",
    "start": "1724559",
    "end": "1731520"
  },
  {
    "text": "it works whether it's a training function or data preparation function and we now want to run it as a part of a",
    "start": "1731520",
    "end": "1737760"
  },
  {
    "text": "complete workflow so we have individual functions as we've seen before one for data gathering one for training",
    "start": "1737760",
    "end": "1745360"
  },
  {
    "text": "one for validation deployment functions etc now someone else i'm an engineer or a",
    "start": "1745360",
    "end": "1752640"
  },
  {
    "text": "devop engineer may just grab those individual functions stitch them into a pipeline and run this",
    "start": "1752640",
    "end": "1759760"
  },
  {
    "text": "entire pipeline as a complete workflow whether it's a production pipeline or training pipeline",
    "start": "1759760",
    "end": "1766080"
  },
  {
    "text": "or development pipeline etc now instead of running it interactively",
    "start": "1766080",
    "end": "1771520"
  },
  {
    "text": "we can actually have the exactly the same flow run in a git pull request so when",
    "start": "1771520",
    "end": "1776640"
  },
  {
    "text": "someone is pushing code we can make this pipeline grab all the code all the components the data",
    "start": "1776640",
    "end": "1783120"
  },
  {
    "text": "the functions the different metadata and convert it into a kubeflow",
    "start": "1783120",
    "end": "1789600"
  },
  {
    "text": "pipeline run it distributed on the entire cluster track everything as if it was in a bunch",
    "start": "1789600",
    "end": "1795440"
  },
  {
    "text": "of individual runs and eventually if we want it can actually throw in a slack notification",
    "start": "1795440",
    "end": "1800799"
  },
  {
    "text": "with a summary and also right back into our pull request write back the",
    "start": "1800799",
    "end": "1805840"
  },
  {
    "text": "results of the training and someone can just look at the pull requests and say you know what",
    "start": "1805840",
    "end": "1811039"
  },
  {
    "text": "i feel good about this model let's promote it in into production with a single command",
    "start": "1811039",
    "end": "1816320"
  },
  {
    "text": "line in inside the pull request so this is the flow again it's very advanced flow i'm sure not everyone is",
    "start": "1816320",
    "end": "1822559"
  },
  {
    "text": "going to jump into it immediately the customers that we have that have jumped into this sort of",
    "start": "1822559",
    "end": "1828240"
  },
  {
    "text": "workflow see much higher productivity they can they can deploy new models they can do updates",
    "start": "1828240",
    "end": "1834640"
  },
  {
    "text": "frequently because everything is sort of very cloud native and very automated",
    "start": "1834640",
    "end": "1840799"
  },
  {
    "start": "1840000",
    "end": "1840000"
  },
  {
    "text": "so again looking into this flow of development ci and cd",
    "start": "1840799",
    "end": "1845919"
  },
  {
    "text": "let's look at it again from a more of a complete workflow let's assume we have a git repository",
    "start": "1845919",
    "end": "1852080"
  },
  {
    "text": "this git repository will just take a branch we want to build something let's say a training function or a",
    "start": "1852080",
    "end": "1858320"
  },
  {
    "text": "data preparation or analysis function we can write it inside our notebook and test it",
    "start": "1858320",
    "end": "1865200"
  },
  {
    "text": "we see that it's working great we just push one command convert it into a micro service um",
    "start": "1865200",
    "end": "1872080"
  },
  {
    "text": "and then one once it's a micro service we want to see that it's actually working on the cluster we haven't missed",
    "start": "1872080",
    "end": "1877120"
  },
  {
    "text": "some dependencies or there's no issue with the you know scale out frameworks or whatever",
    "start": "1877120",
    "end": "1883600"
  },
  {
    "text": "if it breaks we go back to development once we feel it's ready we move to a ci phase",
    "start": "1883600",
    "end": "1889440"
  },
  {
    "text": "so we create a pull request there is an automation frameworks that examine our",
    "start": "1889440",
    "end": "1894720"
  },
  {
    "text": "python code for you know code inspection maybe visual review when we feel this is",
    "start": "1894720",
    "end": "1900720"
  },
  {
    "text": "ready we want to run the pipeline we don't necessarily want to run the pipeline immediately because",
    "start": "1900720",
    "end": "1906320"
  },
  {
    "text": "machine learning pipelines are expensive you know they made you use uh gpus and other things so",
    "start": "1906320",
    "end": "1912799"
  },
  {
    "text": "we want to run initially some unit testing and then run the entire pipeline we can do that from a pull request as",
    "start": "1912799",
    "end": "1919120"
  },
  {
    "text": "i'm going to demonstrate and finally if it's all passed and everything is great we we merge that pull request we we may",
    "start": "1919120",
    "end": "1926880"
  },
  {
    "text": "have additional tests on our sort of development branch before we release a version",
    "start": "1926880",
    "end": "1933279"
  },
  {
    "text": "at the certain point we feel cozy about that and we want to create a new version we we promote our code into some",
    "start": "1933279",
    "end": "1941120"
  },
  {
    "text": "master branch you know depending on your development paradigm we tag the version with some label we",
    "start": "1941120",
    "end": "1948159"
  },
  {
    "text": "deploy that in a canary and we could even put while we're when we're tagging we can even",
    "start": "1948159",
    "end": "1953200"
  },
  {
    "text": "automate the deployment as the minute that we're doing attack uh once we're running the canary and we",
    "start": "1953200",
    "end": "1959760"
  },
  {
    "text": "see that the canary is working properly for some while canary is essentially putting some percentage of the workload",
    "start": "1959760",
    "end": "1966320"
  },
  {
    "text": "on some microservices or the new ones and most of it will go to the old microservices once",
    "start": "1966320",
    "end": "1972480"
  },
  {
    "text": "it's ready we promote it and then we need continuous development and continuous monitoring for concept",
    "start": "1972480",
    "end": "1979279"
  },
  {
    "text": "risk for accuracy disruptions etc and if something breaks we can roll back",
    "start": "1979279",
    "end": "1985039"
  },
  {
    "text": "to older models or we can go and retrain our model and run this flow",
    "start": "1985039",
    "end": "1990480"
  },
  {
    "text": "again so this is a complete uh workflow of automated machine learning",
    "start": "1990480",
    "end": "1997360"
  },
  {
    "start": "1997000",
    "end": "1997000"
  },
  {
    "text": "so it's one example of um one of the customers that we have there is a nice",
    "start": "1997360",
    "end": "2002720"
  },
  {
    "text": "use case around it is working with traditionally with hadoop running a sql server and doing",
    "start": "2002720",
    "end": "2009360"
  },
  {
    "text": "etls periodically and then sort of using some r server to",
    "start": "2009360",
    "end": "2014480"
  },
  {
    "text": "do the prediction because everything is very batch oriented and very hadoop so it has two",
    "start": "2014480",
    "end": "2019679"
  },
  {
    "text": "downsides one is extremely complicated a lot of engineering efforts to make sure that",
    "start": "2019679",
    "end": "2025360"
  },
  {
    "text": "everything works the second is extremely slow because it's batch processing",
    "start": "2025360",
    "end": "2030720"
  },
  {
    "text": "so what they they moved into more of a real-time workflow uh or continuous workflow based on micro",
    "start": "2030720",
    "end": "2037519"
  },
  {
    "start": "2032000",
    "end": "2032000"
  },
  {
    "text": "services where uh they ingest the data directly from the database on every",
    "start": "2037519",
    "end": "2043039"
  },
  {
    "text": "database transaction change then they run the analysis of those features in real",
    "start": "2043039",
    "end": "2049280"
  },
  {
    "text": "time using serverless functions they throw in the analyze data into",
    "start": "2049280",
    "end": "2054638"
  },
  {
    "text": "an online offline feature store they may run additional batch analysis periodically to look at the bigger",
    "start": "2054639",
    "end": "2062079"
  },
  {
    "text": "view a longer time view every a certain amount of time they would run",
    "start": "2062079",
    "end": "2068398"
  },
  {
    "text": "model training as a microservice using psychic learn that essentially generates a new model",
    "start": "2068399",
    "end": "2073839"
  },
  {
    "text": "takes extracts a feature set generates a new model and there's a model inferencing function",
    "start": "2073839",
    "end": "2079839"
  },
  {
    "text": "that essentially get trigger events from a stream analyze the results the feature vector",
    "start": "2079839",
    "end": "2085760"
  },
  {
    "text": "and immediately block the transaction on the fraudulent account so it pushes back into the trend into",
    "start": "2085760",
    "end": "2093040"
  },
  {
    "text": "the user database and update on the user if it's a fraud transaction to block the account",
    "start": "2093040",
    "end": "2101119"
  },
  {
    "text": "and that workflow instead of having like 40 minutes because it's batch is now a continuous workflow that 12",
    "start": "2101119",
    "end": "2107440"
  },
  {
    "text": "seconds from the first indication of fraud it already blocked the account everything is fresh data everything is",
    "start": "2107440",
    "end": "2113520"
  },
  {
    "text": "online everything is micro services another side effect of that thing is",
    "start": "2113520",
    "end": "2118640"
  },
  {
    "text": "that you can actually roll new versions for each individual components you know micro services is",
    "start": "2118640",
    "end": "2124240"
  },
  {
    "text": "about breaking the monolith to individual small components so every time we want to change",
    "start": "2124240",
    "end": "2130240"
  },
  {
    "text": "something change our model change our analysis etc we just do a rolling upgrade",
    "start": "2130240",
    "end": "2136480"
  },
  {
    "text": "and a week from now we can release a new version so moving from a cadence of three four",
    "start": "2136480",
    "end": "2142400"
  },
  {
    "text": "months for release we move into a weekly cadence of versions worst case something doesn't work well",
    "start": "2142400",
    "end": "2148400"
  },
  {
    "text": "we just uh roll back to the previous version nothing happened",
    "start": "2148400",
    "end": "2153839"
  },
  {
    "text": "so uh with that i'm i'm going to show you a demo and to show that all this",
    "start": "2153839",
    "end": "2162079"
  },
  {
    "text": "a nice vision is also working in practice so yeah",
    "start": "2162079",
    "end": "2169520"
  },
  {
    "text": "okay so let's uh let's look at some some notebooks and let's start with a simple notebook",
    "start": "2169520",
    "end": "2176240"
  },
  {
    "text": "just to explain what is a function and how to work with those functions and later on we'll show how to apply",
    "start": "2176240",
    "end": "2182240"
  },
  {
    "text": "automation and full and full ci into the same thing so what we said is that we want to be",
    "start": "2182240",
    "end": "2188880"
  },
  {
    "text": "uh sort of friendly with the data scientists so we want to let them develop in their native environment so",
    "start": "2188880",
    "end": "2194800"
  },
  {
    "text": "their native environment would be like a python you know notebook so let's assume i want to build a simple",
    "start": "2194800",
    "end": "2201599"
  },
  {
    "text": "function that essentially opens some archives some you know s3 bucket and",
    "start": "2201599",
    "end": "2207119"
  },
  {
    "text": "load some zip with images and then i'm going to do a variety of things with those images",
    "start": "2207119",
    "end": "2214240"
  },
  {
    "text": "so i want to write a simple python function get some archived url generates",
    "start": "2214240",
    "end": "2220880"
  },
  {
    "text": "results you know unzip it into a target location it's a good practice to document my",
    "start": "2220880",
    "end": "2227520"
  },
  {
    "text": "function this documentation by the way is going to be auto learn and i'm showing i'm going to show you in",
    "start": "2227520",
    "end": "2233359"
  },
  {
    "text": "a minute and i'm just going to write my code another thing that we want to pass to this function is context context is",
    "start": "2233359",
    "end": "2240560"
  },
  {
    "text": "something that allows us to log artifacts to do instrumentation monitoring metering",
    "start": "2240560",
    "end": "2246640"
  },
  {
    "text": "pass secrets into our function in a service a very secure way so this",
    "start": "2246640",
    "end": "2253119"
  },
  {
    "text": "function is relatively simple and in order to make it a micro service we only",
    "start": "2253119",
    "end": "2258240"
  },
  {
    "text": "do a single operation we just say take our code make it a function we can",
    "start": "2258240",
    "end": "2263760"
  },
  {
    "text": "even document our function you see give it categories and description and other things because",
    "start": "2263760",
    "end": "2269119"
  },
  {
    "text": "this function is also going to be catalogued so what's its catalog we can actually drag it and run it without even opening",
    "start": "2269119",
    "end": "2276079"
  },
  {
    "text": "an editor and it's it's actually also resource you know if you're going to look into",
    "start": "2276079",
    "end": "2282240"
  },
  {
    "text": "that function as a yaml you'll see that it's sort of uh like a kubernetes resource",
    "start": "2282240",
    "end": "2288880"
  },
  {
    "text": "um and then i can just export it or save it in a database upload it to a marketplace a certain",
    "start": "2288880",
    "end": "2295520"
  },
  {
    "text": "point i want to read this function from a repository or from a marketplace",
    "start": "2295520",
    "end": "2301119"
  },
  {
    "text": "i want to examine its documentation so you see it suddenly has documentation",
    "start": "2301119",
    "end": "2306160"
  },
  {
    "text": "and this documentation arrived from my own documentation so i didn't even have to do any special",
    "start": "2306160",
    "end": "2312079"
  },
  {
    "text": "anything special it parsed my python code extracted my documentation and then i just want to",
    "start": "2312079",
    "end": "2319119"
  },
  {
    "text": "run this function i'm giving it sort of a task is a set of parameters i need good",
    "start": "2319119",
    "end": "2325040"
  },
  {
    "text": "data and i want to run it initially in my notebook i don't want to run it on the cluster so",
    "start": "2325040",
    "end": "2330880"
  },
  {
    "text": "i can just say run local it will run it on my notebook and i can see the results i can",
    "start": "2330880",
    "end": "2336320"
  },
  {
    "text": "see the logs everything is great at a certain point i want to run it on the cluster so i just",
    "start": "2336320",
    "end": "2344160"
  },
  {
    "text": "choose a different command and you'll see that essentially it's allocating a pod or multiple pods if it's a distributed",
    "start": "2344160",
    "end": "2350640"
  },
  {
    "text": "job and i see this exact same logs as if it was running in my own notebook just this time it was",
    "start": "2350640",
    "end": "2358000"
  },
  {
    "text": "running on the cluster distributed and i could do other things with it by the way i can even go back into the",
    "start": "2358000",
    "end": "2366000"
  },
  {
    "text": "mlran ui so everything that i i did is also logged into a database so i can",
    "start": "2366000",
    "end": "2371599"
  },
  {
    "text": "see this function that i was running uh what it generated you know the results the logs see the",
    "start": "2371599",
    "end": "2378640"
  },
  {
    "text": "same logs that i've seen here and i maybe just want to run a job",
    "start": "2378640",
    "end": "2383760"
  },
  {
    "text": "from here you know um and you see all the parameters that i documented in my code are actually just",
    "start": "2383760",
    "end": "2391599"
  },
  {
    "text": "parameters that i can just click and type you know maybe i want to give other resources to that function your gpu",
    "start": "2391599",
    "end": "2397839"
  },
  {
    "text": "cpus etc and run it now or in a schedule so someone can develop in his notebook",
    "start": "2397839",
    "end": "2404079"
  },
  {
    "text": "and just pass it into something that someone else can operationalize extremely easy now the next thing that",
    "start": "2404079",
    "end": "2410319"
  },
  {
    "text": "we want to do is build a complete pipeline and full automation so let's take another project uh this",
    "start": "2410319",
    "end": "2417119"
  },
  {
    "text": "project is also documented in uh here uh i'll",
    "start": "2417119",
    "end": "2422560"
  },
  {
    "text": "send the links in a minute so people can uh look into that",
    "start": "2422560",
    "end": "2428590"
  },
  {
    "text": "[Music] this is for a demo amaran repo and there",
    "start": "2428590",
    "end": "2434560"
  },
  {
    "text": "are other repos there that you'll see with this full demo of how to do fully automated ci cd",
    "start": "2434560",
    "end": "2442720"
  },
  {
    "text": "so let's go back into our notebook this time is a more of a real use case",
    "start": "2442800",
    "end": "2449200"
  },
  {
    "text": "so we can start with with a single function the simple function that that generates data",
    "start": "2449359",
    "end": "2455839"
  },
  {
    "text": "this is generating the iris data set it's a common data set in machine learning uh this function of ours also wants to",
    "start": "2455839",
    "end": "2462800"
  },
  {
    "text": "log the data set into the artifact repository so what it",
    "start": "2462800",
    "end": "2467920"
  },
  {
    "text": "will do it will essentially log it into sort of a metadata service that stores features and other things",
    "start": "2467920",
    "end": "2474240"
  },
  {
    "text": "by the way by logging it it also already analyzes it and you'll see all the statistics and all the behavior of that",
    "start": "2474240",
    "end": "2480400"
  },
  {
    "text": "data set in the ui automatically and we can do other things with that so we just",
    "start": "2480400",
    "end": "2485520"
  },
  {
    "text": "generated the data and we later on we want to run it we can forms a notion of a project",
    "start": "2485520",
    "end": "2492000"
  },
  {
    "text": "because sometimes we want to take all those functions and to put them in one project let's say our",
    "start": "2492000",
    "end": "2497920"
  },
  {
    "text": "you know hr analysis our fraud detection project whatever so we're just going to run this",
    "start": "2497920",
    "end": "2506720"
  },
  {
    "text": "data analysis or this data ingestion function and it will analyze our data it will show",
    "start": "2506720",
    "end": "2512400"
  },
  {
    "text": "all the results you get a hyperlink into the ui if you want to see it in a more visual way",
    "start": "2512400",
    "end": "2519119"
  },
  {
    "text": "the next thing that we want to do is run analysis on our data so this time i'm going to take a library",
    "start": "2519119",
    "end": "2525920"
  },
  {
    "text": "function which is a feature analysis function is essentially takes the data set that i just generated",
    "start": "2525920",
    "end": "2531599"
  },
  {
    "text": "and it will analyze it to verify to generate plots around it so as a data scientist i don't need to",
    "start": "2531599",
    "end": "2537839"
  },
  {
    "text": "work hard on packaging containers and distributed workloads i'm just going to run a distributed data analysis on my",
    "start": "2537839",
    "end": "2544880"
  },
  {
    "text": "code so i'm just going to grab a function from a marketplace see the documentation you know it has a",
    "start": "2544880",
    "end": "2551119"
  },
  {
    "text": "bunch of interesting parameters i'm just going to run it on",
    "start": "2551119",
    "end": "2556400"
  },
  {
    "text": "my cluster one of the things that we can do is put modifiers it's a concept of kubeflow",
    "start": "2556400",
    "end": "2562800"
  },
  {
    "text": "of essentially adding things like resources like volumes and other things so here i",
    "start": "2562800",
    "end": "2568160"
  },
  {
    "text": "want to attach because i want this analysis function to see the data on my",
    "start": "2568160",
    "end": "2573280"
  },
  {
    "text": "notebook i need both of them to sit on a shared fast shared file system",
    "start": "2573280",
    "end": "2579520"
  },
  {
    "text": "so i'm modifying i'm essentially mounting both containers on the same",
    "start": "2579520",
    "end": "2585040"
  },
  {
    "text": "file system my jupyter notebook and my analysis function and this way you can see that i'm actually",
    "start": "2585040",
    "end": "2591359"
  },
  {
    "text": "passing the output of my local notebook into that function in reality i'm not really moving any",
    "start": "2591359",
    "end": "2598079"
  },
  {
    "text": "data i'm just passing a pointer because they're both running on a high-speed",
    "start": "2598079",
    "end": "2603440"
  },
  {
    "text": "file system and that could also be done with nfs by the way",
    "start": "2603440",
    "end": "2608640"
  },
  {
    "text": "so i'm just running this analysis function and you see that it's running as if it's running locally",
    "start": "2608640",
    "end": "2613920"
  },
  {
    "text": "but this job again is running on a separate pod on the same cluster just sharing the data with my",
    "start": "2613920",
    "end": "2620240"
  },
  {
    "text": "jupyter notebook and it generates a lot of data now nice thing because it's a shared file",
    "start": "2620240",
    "end": "2626000"
  },
  {
    "text": "system i can just see that in my notebook so let's see what we generated",
    "start": "2626000",
    "end": "2631920"
  },
  {
    "text": "we've generated all sorts of feature analysis diagrams biling charts if you're familiar with",
    "start": "2631920",
    "end": "2638319"
  },
  {
    "text": "that and again a variety of analysis and charts for for my data set",
    "start": "2638319",
    "end": "2644960"
  },
  {
    "text": "at the certain point i want to build the full pipeline not just analysis now i feel the data is ready i chose the",
    "start": "2644960",
    "end": "2650079"
  },
  {
    "text": "right features so the next thing i'm going to do is",
    "start": "2650079",
    "end": "2655680"
  },
  {
    "text": "sorry [Music] so the next thing i want to do is build",
    "start": "2656160",
    "end": "2662400"
  },
  {
    "text": "the pipeline so building the pipeline again i don't want to work too hard i want to take automl training function",
    "start": "2662400",
    "end": "2669760"
  },
  {
    "text": "i want to run testing and validation function i need to deploy my model so i need a",
    "start": "2669760",
    "end": "2675359"
  },
  {
    "text": "model serving function and also after deploying my model i want to verify that it's actually performing",
    "start": "2675359",
    "end": "2681920"
  },
  {
    "text": "well so i need a model serving testing function that actually bombards",
    "start": "2681920",
    "end": "2687040"
  },
  {
    "text": "my model serving http endpoint with a set of data and verify the latency the",
    "start": "2687040",
    "end": "2693040"
  },
  {
    "text": "performance the behavior of my model so again a full pipeline",
    "start": "2693040",
    "end": "2698319"
  },
  {
    "text": "so in order to run this pipeline i use kubeflow pipeline which is again a",
    "start": "2698319",
    "end": "2704240"
  },
  {
    "text": "standard on top of kubernetes using argo and in that pipeline i'm going to have a",
    "start": "2704240",
    "end": "2710880"
  },
  {
    "text": "bunch of steps the first step is ingesting data running this function that i just defined in my notebook",
    "start": "2710880",
    "end": "2717359"
  },
  {
    "text": "again i take my notebook and i'm running it in a pipeline without building any container that",
    "start": "2717359",
    "end": "2723040"
  },
  {
    "text": "amaran did all the magic for me then i'm running an analysis function that takes the",
    "start": "2723040",
    "end": "2729040"
  },
  {
    "text": "output of my ingestion function i'm running automl function on my",
    "start": "2729040",
    "end": "2736640"
  },
  {
    "text": "dataset on my dataset using a set of algorithms that are more suitable for this type of",
    "start": "2736640",
    "end": "2743680"
  },
  {
    "text": "data set i'm asking it to automatically choose the best accuracy across all those permutations finally",
    "start": "2743680",
    "end": "2751599"
  },
  {
    "text": "i'm i'm running validation against the results of my model and when i have a model in my hand i'm",
    "start": "2751599",
    "end": "2757760"
  },
  {
    "text": "going to deploy that into the cluster i'm just going to pass the model from the",
    "start": "2757760",
    "end": "2763440"
  },
  {
    "text": "training into a new function that does deployment setting giving it some parameters and",
    "start": "2763440",
    "end": "2768480"
  },
  {
    "text": "that will deploy a real-time micro service so essentially what we've done here we've taken q flow which is usually a",
    "start": "2768480",
    "end": "2774960"
  },
  {
    "text": "machine learning pipeline we've extended it to do data analysis pipeline data preparation pipelines",
    "start": "2774960",
    "end": "2780720"
  },
  {
    "text": "things that you usually do with like uh airflow and then we run machine learning pipeline for training and",
    "start": "2780720",
    "end": "2787200"
  },
  {
    "text": "validation and finally we're running a deployment pipeline something that you would usually do with jenkins",
    "start": "2787200",
    "end": "2793680"
  },
  {
    "text": "or other ci tools so we have a single pipeline that does data engineering machine learning nci in a single",
    "start": "2793680",
    "end": "2801599"
  },
  {
    "text": "flow in order to run this thing the only thing i need to do is just ask for run and you know i can watch the",
    "start": "2801599",
    "end": "2808960"
  },
  {
    "text": "results locally so you can see that if i'm watching the results i'm going to get",
    "start": "2808960",
    "end": "2814400"
  },
  {
    "text": "all the results back into my notebook i don't have to open the kubeflow pipeline ui but if we want to look into",
    "start": "2814400",
    "end": "2821760"
  },
  {
    "text": "kubeflow pipeline this is integrated as a managed service in our",
    "start": "2821760",
    "end": "2827119"
  },
  {
    "text": "iguazio platform then you could just go and see this pipeline and then just defined",
    "start": "2827119",
    "end": "2833119"
  },
  {
    "text": "you see the first step was getting the data here's a snapshot of my data a second step was training the data",
    "start": "2833119",
    "end": "2841680"
  },
  {
    "text": "you know here are all the rc seekers you see all the visualization which is very hard usually to do in kubeflow all the",
    "start": "2841680",
    "end": "2847520"
  },
  {
    "text": "visualization is automatically generated inside kubeflow without doing any work",
    "start": "2847520",
    "end": "2853040"
  },
  {
    "text": "on on my container i can run testing um i can run the",
    "start": "2853040",
    "end": "2860079"
  },
  {
    "text": "analysis as i've shown of all those nice charts i run the deployment which",
    "start": "2860079",
    "end": "2866880"
  },
  {
    "text": "is essentially generating a nucleo real-time serverless function and generating a real-time endpoint",
    "start": "2866880",
    "end": "2873200"
  },
  {
    "text": "and finally i'm running a testing a utility where i can see the latency",
    "start": "2873200",
    "end": "2879839"
  },
  {
    "text": "some roughly like five millisecond latency which is relatively good for that model",
    "start": "2879839",
    "end": "2886000"
  },
  {
    "text": "so i know that my model behaves well the point is that i don't want to run this manually just as i've",
    "start": "2886000",
    "end": "2892160"
  },
  {
    "text": "done right now i want to run this entire thing in git so the way to do it if you clone this",
    "start": "2892160",
    "end": "2898960"
  },
  {
    "text": "repository and you install mlran and kubeflow on your kubernetes cluster you can",
    "start": "2898960",
    "end": "2904480"
  },
  {
    "text": "follow all the documentation on how to do that you can just uh write a github",
    "start": "2904480",
    "end": "2911359"
  },
  {
    "text": "actions workflow or just throw in this yaml if you clone the repo it's already there",
    "start": "2911359",
    "end": "2917680"
  },
  {
    "text": "which is uh essentially going to remote launch email run and kubeflow every time there",
    "start": "2917680",
    "end": "2924800"
  },
  {
    "text": "is a change or there is a commit in in my repository so what i can do i",
    "start": "2924800",
    "end": "2930000"
  },
  {
    "text": "can edit the code that is here in the repository uh it's essentially the same project",
    "start": "2930000",
    "end": "2935680"
  },
  {
    "text": "that i've just shown you uh part of every project there's like a project yml file which is uh",
    "start": "2935680",
    "end": "2942160"
  },
  {
    "text": "you know tells us all the information about the linked functions and data and parameters",
    "start": "2942160",
    "end": "2947280"
  },
  {
    "text": "etc very simple and intuitive yaml file that is generated for you you",
    "start": "2947280",
    "end": "2952960"
  },
  {
    "text": "don't generate it when you save a project it generates satiamo",
    "start": "2952960",
    "end": "2958559"
  },
  {
    "text": "and then the the way that it works is that you can just uh cool request your code when you pull",
    "start": "2958640",
    "end": "2965440"
  },
  {
    "text": "request you do in the pull request i could just do slash run this will automatically",
    "start": "2965440",
    "end": "2971200"
  },
  {
    "text": "trigger a workflow remote workflow uh in kubernetes so in kubeflow so you see",
    "start": "2971200",
    "end": "2976800"
  },
  {
    "text": "it generates a pipeline uh once the pipeline finishes it generates a result back into our",
    "start": "2976800",
    "end": "2984240"
  },
  {
    "text": "pull request so we can crack the behavior of our model we have historical information about",
    "start": "2984240",
    "end": "2989599"
  },
  {
    "text": "every pull request and then we can from here deploy also the model",
    "start": "2989599",
    "end": "2994960"
  },
  {
    "text": "um if we love slack most of us do then we can get the same notification",
    "start": "2994960",
    "end": "3000319"
  },
  {
    "text": "into email or into a slack message so as we develop we can just see what happened",
    "start": "3000319",
    "end": "3005839"
  },
  {
    "text": "in our training model directly in slack and everything is a hyperlink so",
    "start": "3005839",
    "end": "3011359"
  },
  {
    "text": "those hyperlinks will go back into the emerald run ui",
    "start": "3011359",
    "end": "3017359"
  },
  {
    "text": "where we can see all the different jobs and their behavior so let's go to my",
    "start": "3018240",
    "end": "3025040"
  },
  {
    "text": "githubs project here you can see the all the executions of each step we can",
    "start": "3025040",
    "end": "3031040"
  },
  {
    "text": "organize it in by pipeline by by function by job",
    "start": "3031040",
    "end": "3036880"
  },
  {
    "text": "and we can see all the variations all the runs of of every uh thing let's go into the training",
    "start": "3036880",
    "end": "3043440"
  },
  {
    "text": "function for an example you see that it automatically logged all the information about our training",
    "start": "3043440",
    "end": "3049119"
  },
  {
    "text": "it logged all the metadata which workflow which container which pod was used on",
    "start": "3049119",
    "end": "3055920"
  },
  {
    "text": "that for that workflow all the matrix even the code that was used for",
    "start": "3055920",
    "end": "3061680"
  },
  {
    "text": "running this this training function could be viewed i could just see this is the exact version of the code",
    "start": "3061680",
    "end": "3068000"
  },
  {
    "text": "that i was running for this training function [Music]",
    "start": "3068000",
    "end": "3073520"
  },
  {
    "text": "i can see all the artifacts that were generated you know each each one of those i could",
    "start": "3073520",
    "end": "3079920"
  },
  {
    "text": "see the results because it's multiple experiments and i can actually compare",
    "start": "3079920",
    "end": "3084960"
  },
  {
    "text": "it shows this one because it has the best accuracy but i can also go into individual",
    "start": "3084960",
    "end": "3090079"
  },
  {
    "text": "results i can't even see the logs of my run and that is interactive if my job keeps on running i will see that",
    "start": "3090079",
    "end": "3096880"
  },
  {
    "text": "progressing it also monitors all the all the information about the the",
    "start": "3096880",
    "end": "3101920"
  },
  {
    "text": "artifacts so let's go into artifacts let's take a data set",
    "start": "3101920",
    "end": "3108319"
  },
  {
    "text": "i have a preview of my data set i have a full analysis of my entire data set i haven't written a",
    "start": "3108319",
    "end": "3113920"
  },
  {
    "text": "single line of code in order to get all this full automation everything was done for me",
    "start": "3113920",
    "end": "3119119"
  },
  {
    "text": "using wrappers and automation code the other thing that we we mentioned is that",
    "start": "3119119",
    "end": "3125359"
  },
  {
    "text": "it also generated real-time functions so this is using a nucleo which is a very high speed high",
    "start": "3125359",
    "end": "3132000"
  },
  {
    "text": "performance serverless engine very popular so in nucleo which is also open source and",
    "start": "3132000",
    "end": "3138720"
  },
  {
    "text": "runs over kubernetes as a crd you can generate all those functions it's",
    "start": "3138720",
    "end": "3144960"
  },
  {
    "text": "looking into a function this is a serving function so i can see that this",
    "start": "3144960",
    "end": "3150160"
  },
  {
    "text": "relatively simple function it looks like a kf serving",
    "start": "3150160",
    "end": "3155599"
  },
  {
    "text": "but it's much faster because it's using the nuclear runtime and it's much simpler because of all the",
    "start": "3155599",
    "end": "3160880"
  },
  {
    "text": "unique capabilities of of nucleo but it looks if someone some of you have used the kf serving the",
    "start": "3160880",
    "end": "3167359"
  },
  {
    "text": "code itself inside the function is similar to kf serving so very simple classes",
    "start": "3167359",
    "end": "3173839"
  },
  {
    "text": "advantages of nuclear again controls resources dynamically cpus gpus et cetera it's",
    "start": "3173839",
    "end": "3180480"
  },
  {
    "text": "stateful it has many different capabilities you can use triggering for http but also for like a dozen",
    "start": "3180480",
    "end": "3188559"
  },
  {
    "text": "different streaming protocols it does auto checkpointing auto rebalancing is very again very",
    "start": "3188559",
    "end": "3194960"
  },
  {
    "text": "advanced very high performance serverless engine so now here we had",
    "start": "3194960",
    "end": "3200079"
  },
  {
    "text": "in our project we had two different functions v1 and v2 one thing that we need is a canary so",
    "start": "3200079",
    "end": "3206880"
  },
  {
    "text": "that again in nuclear not too difficult you just go into the api gateway",
    "start": "3206880",
    "end": "3213200"
  },
  {
    "text": "definition you can set the like authentication logic you can set the routing logic let's just",
    "start": "3213200",
    "end": "3220240"
  },
  {
    "text": "create a canary and give it [Music]",
    "start": "3220240",
    "end": "3226720"
  },
  {
    "text": "okay and i here i can just set the canary and make it split the the data between the two",
    "start": "3226720",
    "end": "3234240"
  },
  {
    "text": "models that i that i generated and i mean from now it's just going to be provision",
    "start": "3234240",
    "end": "3239599"
  },
  {
    "text": "if i want to monitor everything that nuclear also generates all the telemetry nuclear enamel",
    "start": "3239599",
    "end": "3244800"
  },
  {
    "text": "generates all the telemetry into uh prometheus and you can also monitor it",
    "start": "3244800",
    "end": "3250800"
  },
  {
    "text": "in grafana so again you don't have to do anything for instrumentation all the instrumentation all the observability is",
    "start": "3250800",
    "end": "3257119"
  },
  {
    "text": "built in for you okay so with that we've seen the demo and i know there are a bunch of",
    "start": "3257119",
    "end": "3263440"
  },
  {
    "text": "questions so uh thank you everyone for listening and i'm going to",
    "start": "3263440",
    "end": "3268800"
  },
  {
    "text": "start going over the the questions um [Music]",
    "start": "3268800",
    "end": "3273920"
  },
  {
    "text": "okay um",
    "start": "3273920",
    "end": "3279838"
  },
  {
    "text": "so there is a question on setima setting up local development",
    "start": "3281040",
    "end": "3287440"
  },
  {
    "text": "for the data scientists so again what usually you don't use argo",
    "start": "3287440",
    "end": "3293280"
  },
  {
    "text": "you you just install kubeflow which is a set of tools it's not a trivial task but there are a",
    "start": "3293280",
    "end": "3299520"
  },
  {
    "text": "bunch of documentation of how to set up queue flow once you've installed qclo",
    "start": "3299520",
    "end": "3305119"
  },
  {
    "text": "installing mrran is also relatively easy there are many different full demos",
    "start": "3305119",
    "end": "3311520"
  },
  {
    "text": "enamel runoff of you know everything from churn prediction",
    "start": "3311520",
    "end": "3317119"
  },
  {
    "text": "you know predictive maintenance image recognition face recognition etc that you could just clone and use for",
    "start": "3317119",
    "end": "3323920"
  },
  {
    "text": "your stuff and with demo renders also a lot of documentation",
    "start": "3323920",
    "end": "3329359"
  },
  {
    "text": "including also the aspects of installing the ecosystem around it so like nfs for",
    "start": "3329359",
    "end": "3334880"
  },
  {
    "text": "shared storage and uh you know kubeflow and and other things that you uh you may need so if you go into demo",
    "start": "3334880",
    "end": "3342720"
  },
  {
    "text": "run documentation for the you'll find it through the git you can just go and see all the all the",
    "start": "3342720",
    "end": "3349839"
  },
  {
    "text": "details there are also pre-baked images uh with like jupiter and everything that you could use",
    "start": "3349839",
    "end": "3355440"
  },
  {
    "text": "and just you know run a few docker commands and start working so that's on that answer that question",
    "start": "3355440",
    "end": "3363119"
  },
  {
    "text": "um okay um",
    "start": "3363119",
    "end": "3369520"
  },
  {
    "text": "yeah i haven't seen on other anyone wants to ask another question",
    "start": "3373040",
    "end": "3378230"
  },
  {
    "text": "[Music] yeah i think that might be it uh your own um is there a place",
    "start": "3378230",
    "end": "3384640"
  },
  {
    "text": "um where people can contact you after this webinar like maybe a slack link or i think you might have that in your",
    "start": "3384640",
    "end": "3390559"
  },
  {
    "text": "slide deck if you want to go back to that we can show that for folks",
    "start": "3390559",
    "end": "3395680"
  },
  {
    "text": "the easier way in uh for cncf folks is to just find me in the cncf",
    "start": "3396960",
    "end": "3403280"
  },
  {
    "text": "slack you can also just connect through linkedin to me and i'm",
    "start": "3403280",
    "end": "3409520"
  },
  {
    "text": "i'm pretty open i'll i'll approve you and we can have a nice chat and again we i'm willing to help you",
    "start": "3409520",
    "end": "3416960"
  },
  {
    "text": "know any of you that is uh working on this sort of trying to operationalize their their their effort you know sort of uh",
    "start": "3416960",
    "end": "3423599"
  },
  {
    "text": "evangelism you don't have to use our platform uh you don't have to use our open source projects",
    "start": "3423599",
    "end": "3429040"
  },
  {
    "text": "um but i think uh we are we do have this agenda of promoting amalops and all this automation and",
    "start": "3429040",
    "end": "3436079"
  },
  {
    "text": "kubernetes as the mainstream platform for data engineering and data science awesome",
    "start": "3436079",
    "end": "3443680"
  },
  {
    "text": "well we're getting a lot of comments in the chat that people really liked your presentation i'd agree thanks again for a great",
    "start": "3443680",
    "end": "3448880"
  },
  {
    "text": "presentation today uh thanks folks for joining us for today's cncf webinar a reminder that the recording slides are",
    "start": "3448880",
    "end": "3455280"
  },
  {
    "text": "going to be posted later today on the cncf webinars page that's cncf.io",
    "start": "3455280",
    "end": "3460559"
  },
  {
    "text": "webinars thanks again from all of us i hope everyone has a great day and stay",
    "start": "3460559",
    "end": "3466319"
  },
  {
    "text": "safe thanks all thank you everyone",
    "start": "3466319",
    "end": "3473519"
  }
]