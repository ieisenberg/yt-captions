[
  {
    "text": "very good morning everyone Thank you so",
    "start": "240",
    "end": "1760"
  },
  {
    "text": "much for coming to my talk I am Selvi",
    "start": "1760",
    "end": "3919"
  },
  {
    "text": "Kaderville I'm the engineering lead at a",
    "start": "3919",
    "end": "5839"
  },
  {
    "text": "startup called Elottle Uh a quick",
    "start": "5839",
    "end": "8639"
  },
  {
    "text": "project background This is an open",
    "start": "8639",
    "end": "10000"
  },
  {
    "text": "source uh genai",
    "start": "10000",
    "end": "12280"
  },
  {
    "text": "infrastruct Uh we are a seat stage",
    "start": "12280",
    "end": "14799"
  },
  {
    "text": "startup We build tools for AI and",
    "start": "14799",
    "end": "16800"
  },
  {
    "text": "generic workloads on uh Kubernetes Uh we",
    "start": "16800",
    "end": "19680"
  },
  {
    "text": "build out Luna which is an intelligent",
    "start": "19680",
    "end": "21520"
  },
  {
    "text": "cluster autoscaler and Nova which is a",
    "start": "21520",
    "end": "23359"
  },
  {
    "text": "multicluster fleet manager Uh a bit",
    "start": "23359",
    "end": "25680"
  },
  {
    "text": "about my background Uh Prior to Elottal",
    "start": "25680",
    "end": "27519"
  },
  {
    "text": "I was a founding engineer and tech lead",
    "start": "27519",
    "end": "29439"
  },
  {
    "text": "at container X We built out uh container",
    "start": "29439",
    "end": "31679"
  },
  {
    "text": "management platforms We were acquired by",
    "start": "31679",
    "end": "33600"
  },
  {
    "text": "Cisco and this continued to be uh built",
    "start": "33600",
    "end": "36239"
  },
  {
    "text": "out as CCP which is Cisco's container",
    "start": "36239",
    "end": "38559"
  },
  {
    "text": "platform uh and fronts uh all of UCS",
    "start": "38559",
    "end": "41760"
  },
  {
    "text": "servers as well as Cisco's",
    "start": "41760",
    "end": "42879"
  },
  {
    "text": "hyperconverged infrastructure Uh prior",
    "start": "42879",
    "end": "45120"
  },
  {
    "text": "to this I was in a field called AI ops",
    "start": "45120",
    "end": "47360"
  },
  {
    "text": "which is using a IML techniques for",
    "start": "47360",
    "end": "50160"
  },
  {
    "text": "distributed systems uh performance",
    "start": "50160",
    "end": "51840"
  },
  {
    "text": "management Uh so I worked as a software",
    "start": "51840",
    "end": "54079"
  },
  {
    "text": "engineer to autoscale virtual machines",
    "start": "54079",
    "end": "56559"
  },
  {
    "text": "uh for performance uh at VMware and also",
    "start": "56559",
    "end": "59760"
  },
  {
    "text": "uh worked on performance prediction and",
    "start": "59760",
    "end": "61760"
  },
  {
    "text": "anomaly detection as part of my PhD",
    "start": "61760",
    "end": "63520"
  },
  {
    "text": "thesis I've been excited to work on this",
    "start": "63520",
    "end": "65600"
  },
  {
    "text": "project because it got together my",
    "start": "65600",
    "end": "67119"
  },
  {
    "text": "infrastructure as well as my a IML",
    "start": "67119",
    "end": "69040"
  },
  {
    "text": "expertise",
    "start": "69040",
    "end": "70200"
  },
  {
    "text": "together Okay I'll start with describing",
    "start": "70200",
    "end": "73280"
  },
  {
    "text": "uh the various characteristics of the AI",
    "start": "73280",
    "end": "75880"
  },
  {
    "text": "infrastructed and the specific choices",
    "start": "75880",
    "end": "78000"
  },
  {
    "text": "that made these uh characteristics",
    "start": "78000",
    "end": "79759"
  },
  {
    "text": "possible",
    "start": "79759",
    "end": "81680"
  },
  {
    "text": "Uh we'll start with the lowest level Uh",
    "start": "81680",
    "end": "84400"
  },
  {
    "text": "GPUs are a critical resource Uh we were",
    "start": "84400",
    "end": "86720"
  },
  {
    "text": "interested in uh AI inferencing and not",
    "start": "86720",
    "end": "89360"
  },
  {
    "text": "training We wanted to use pre-trained uh",
    "start": "89360",
    "end": "91680"
  },
  {
    "text": "existing large language models Uh GPUs",
    "start": "91680",
    "end": "94479"
  },
  {
    "text": "are expensive So we needed",
    "start": "94479",
    "end": "96320"
  },
  {
    "text": "infrastructure software that could",
    "start": "96320",
    "end": "98560"
  },
  {
    "text": "choose uh cost-effective instance types",
    "start": "98560",
    "end": "101360"
  },
  {
    "text": "Uh furthermore even the more affordable",
    "start": "101360",
    "end": "103520"
  },
  {
    "text": "GPU types on various major cloud",
    "start": "103520",
    "end": "106159"
  },
  {
    "text": "providers are always in limited supply",
    "start": "106159",
    "end": "108560"
  },
  {
    "text": "So in addition to making cost-ffective",
    "start": "108560",
    "end": "110320"
  },
  {
    "text": "choices we needed to choose instance",
    "start": "110320",
    "end": "112320"
  },
  {
    "text": "types that were available at different",
    "start": "112320",
    "end": "115119"
  },
  {
    "text": "uh points in a day or week Furthermore",
    "start": "115119",
    "end": "118320"
  },
  {
    "text": "we wanted traditional autoscaling which",
    "start": "118320",
    "end": "120000"
  },
  {
    "text": "meant that as and when incoming",
    "start": "120000",
    "end": "122079"
  },
  {
    "text": "inference workloads increased or",
    "start": "122079",
    "end": "123600"
  },
  {
    "text": "decreased we want to be able to",
    "start": "123600",
    "end": "124960"
  },
  {
    "text": "autoscale our uh worker nodes on a",
    "start": "124960",
    "end": "126960"
  },
  {
    "text": "Kubernetes cluster Uh we then move on to",
    "start": "126960",
    "end": "129759"
  },
  {
    "text": "the data requirements Uh we wanted to",
    "start": "129759",
    "end": "131599"
  },
  {
    "text": "build out a question answer pipeline for",
    "start": "131599",
    "end": "133599"
  },
  {
    "text": "users private or specialized data that",
    "start": "133599",
    "end": "136160"
  },
  {
    "text": "could not be sent out to a thirdparty AI",
    "start": "136160",
    "end": "138879"
  },
  {
    "text": "service provider like OpenAI or",
    "start": "138879",
    "end": "140400"
  },
  {
    "text": "Anthropic Uh this meant that our AI",
    "start": "140400",
    "end": "142800"
  },
  {
    "text": "software had to be able to self-host",
    "start": "142800",
    "end": "145520"
  },
  {
    "text": "open-source large language models Uh",
    "start": "145520",
    "end": "148000"
  },
  {
    "text": "furthermore in order to leverage uh uh",
    "start": "148000",
    "end": "151280"
  },
  {
    "text": "infrastructure level scaling the",
    "start": "151280",
    "end": "153599"
  },
  {
    "text": "inference engine also had to be scalable",
    "start": "153599",
    "end": "156480"
  },
  {
    "text": "Uh I'll then go over to the specific",
    "start": "156480",
    "end": "158640"
  },
  {
    "text": "choices we made to achieve these",
    "start": "158640",
    "end": "160440"
  },
  {
    "text": "characteristics Uh the bottom two layers",
    "start": "160440",
    "end": "162640"
  },
  {
    "text": "are quite obvious Uh GPU and CPU instant",
    "start": "162640",
    "end": "165280"
  },
  {
    "text": "types from cloud providers within the",
    "start": "165280",
    "end": "167760"
  },
  {
    "text": "customer's cloud account is what we",
    "start": "167760",
    "end": "169200"
  },
  {
    "text": "chose Kubernetes worked really well for",
    "start": "169200",
    "end": "171840"
  },
  {
    "text": "uh scalable uh infrastructure uh cluster",
    "start": "171840",
    "end": "175680"
  },
  {
    "text": "autoscaler like a lot luna or open-",
    "start": "175680",
    "end": "177680"
  },
  {
    "text": "source uh uh autoscalers like carpenter",
    "start": "177680",
    "end": "181040"
  },
  {
    "text": "work really well to make uh right sizing",
    "start": "181040",
    "end": "183560"
  },
  {
    "text": "instance size choices as well as",
    "start": "183560",
    "end": "186159"
  },
  {
    "text": "autoscaling during day two and day n",
    "start": "186159",
    "end": "188519"
  },
  {
    "text": "operations In the green uh blocks are",
    "start": "188519",
    "end": "191599"
  },
  {
    "text": "the choices that allowed us to uh scale",
    "start": "191599",
    "end": "194319"
  },
  {
    "text": "the inference engine We use a project",
    "start": "194319",
    "end": "196400"
  },
  {
    "text": "called VLLM This is a project open",
    "start": "196400",
    "end": "198879"
  },
  {
    "text": "source project from University of",
    "start": "198879",
    "end": "200159"
  },
  {
    "text": "Berkeley uh their main key highlights",
    "start": "200159",
    "end": "202959"
  },
  {
    "text": "are um fast inference as well as memory",
    "start": "202959",
    "end": "206800"
  },
  {
    "text": "efficient inference and they have use a",
    "start": "206800",
    "end": "209360"
  },
  {
    "text": "distributed inference uh framework they",
    "start": "209360",
    "end": "211599"
  },
  {
    "text": "use the ray open source computing",
    "start": "211599",
    "end": "213120"
  },
  {
    "text": "framework which is also a project from",
    "start": "213120",
    "end": "214720"
  },
  {
    "text": "University of Berkeley it's uh",
    "start": "214720",
    "end": "216640"
  },
  {
    "text": "enterprise version is provided by any",
    "start": "216640",
    "end": "218480"
  },
  {
    "text": "scale uh ray provides both uh training",
    "start": "218480",
    "end": "222000"
  },
  {
    "text": "and inference uh libraries VLM",
    "start": "222000",
    "end": "224959"
  },
  {
    "text": "specifically uses the ray serve",
    "start": "224959",
    "end": "226480"
  },
  {
    "text": "component",
    "start": "226480",
    "end": "228159"
  },
  {
    "text": "uh above this in yellow are the specific",
    "start": "228159",
    "end": "230640"
  },
  {
    "text": "big open source large language models we",
    "start": "230640",
    "end": "232159"
  },
  {
    "text": "worked with We use Microsoft's 53 model",
    "start": "232159",
    "end": "234640"
  },
  {
    "text": "Even though it's a few billion parameter",
    "start": "234640",
    "end": "236319"
  },
  {
    "text": "around three billion parameters it",
    "start": "236319",
    "end": "238080"
  },
  {
    "text": "worked really well for our uh data sets",
    "start": "238080",
    "end": "240400"
  },
  {
    "text": "We also use uh Rubra AI which is from",
    "start": "240400",
    "end": "242879"
  },
  {
    "text": "AON labs Thank you They uh uh open",
    "start": "242879",
    "end": "246159"
  },
  {
    "text": "sourced fine-tuned versions of the 53",
    "start": "246159",
    "end": "248799"
  },
  {
    "text": "model Uh this allowed for function",
    "start": "248799",
    "end": "250959"
  },
  {
    "text": "calling or tool calling from these LLMs",
    "start": "250959",
    "end": "254480"
  },
  {
    "text": "Uh the purple blocks refer to the",
    "start": "254480",
    "end": "256079"
  },
  {
    "text": "specific AI pipeline which we will jump",
    "start": "256079",
    "end": "258160"
  },
  {
    "text": "into",
    "start": "258160",
    "end": "260560"
  },
  {
    "text": "Okay So uh as soon as we talk about",
    "start": "260560",
    "end": "263280"
  },
  {
    "text": "private data the def factor solution is",
    "start": "263280",
    "end": "266320"
  },
  {
    "text": "retrieval augmented generation which is",
    "start": "266320",
    "end": "268960"
  },
  {
    "text": "where we zero in on a small subset of",
    "start": "268960",
    "end": "272560"
  },
  {
    "text": "the private data that has relevant",
    "start": "272560",
    "end": "275199"
  },
  {
    "text": "information to the user's question Uh",
    "start": "275199",
    "end": "277919"
  },
  {
    "text": "and it works really well for pointed",
    "start": "277919",
    "end": "280080"
  },
  {
    "text": "questions However we noticed that uh",
    "start": "280080",
    "end": "282400"
  },
  {
    "text": "from our practical experience we",
    "start": "282400",
    "end": "284080"
  },
  {
    "text": "couldn't restrict our end users to only",
    "start": "284080",
    "end": "287040"
  },
  {
    "text": "certain types of questions uh a number",
    "start": "287040",
    "end": "289600"
  },
  {
    "text": "of the questions were of the knowledge",
    "start": "289600",
    "end": "291520"
  },
  {
    "text": "aggregation type which meant that rather",
    "start": "291520",
    "end": "293520"
  },
  {
    "text": "than a small sliver of data there's uh a",
    "start": "293520",
    "end": "296400"
  },
  {
    "text": "wide subset of data that needs to be uh",
    "start": "296400",
    "end": "298880"
  },
  {
    "text": "used before asking the LLM This is where",
    "start": "298880",
    "end": "301840"
  },
  {
    "text": "text to SQL agents come in In this the",
    "start": "301840",
    "end": "304720"
  },
  {
    "text": "user's natural language question is",
    "start": "304720",
    "end": "306560"
  },
  {
    "text": "converted into a semantically correct",
    "start": "306560",
    "end": "308960"
  },
  {
    "text": "and syntactically correct SQL query This",
    "start": "308960",
    "end": "311840"
  },
  {
    "text": "is then sent to a SQL database of course",
    "start": "311840",
    "end": "314080"
  },
  {
    "text": "with restricted readonly permissions The",
    "start": "314080",
    "end": "316479"
  },
  {
    "text": "SQL results are then sent to the LLM to",
    "start": "316479",
    "end": "319039"
  },
  {
    "text": "be converted into natural language So",
    "start": "319039",
    "end": "321280"
  },
  {
    "text": "with a combination of rag agents and",
    "start": "321280",
    "end": "323280"
  },
  {
    "text": "texttosql agents we were able to satisfy",
    "start": "323280",
    "end": "326240"
  },
  {
    "text": "all of our end users uh question",
    "start": "326240",
    "end": "328039"
  },
  {
    "text": "categories However the key missing link",
    "start": "328039",
    "end": "330880"
  },
  {
    "text": "was a question router How do we know",
    "start": "330880",
    "end": "333199"
  },
  {
    "text": "whether an incoming question is to be",
    "start": "333199",
    "end": "335039"
  },
  {
    "text": "answered by the rag agent or the SQL",
    "start": "335039",
    "end": "336720"
  },
  {
    "text": "agent uh this is where traditional ML",
    "start": "336720",
    "end": "339440"
  },
  {
    "text": "worked really well We used a text",
    "start": "339440",
    "end": "341280"
  },
  {
    "text": "classification model We played around",
    "start": "341280",
    "end": "343199"
  },
  {
    "text": "with a few models and random forest",
    "start": "343199",
    "end": "345199"
  },
  {
    "text": "classifiers worked really well Uh and",
    "start": "345199",
    "end": "347680"
  },
  {
    "text": "that is what was able to uh kind of link",
    "start": "347680",
    "end": "350400"
  },
  {
    "text": "together the two code paths in our",
    "start": "350400",
    "end": "352680"
  },
  {
    "text": "pipeline The data sets that are of",
    "start": "352680",
    "end": "354960"
  },
  {
    "text": "interest to us was uh project management",
    "start": "354960",
    "end": "357600"
  },
  {
    "text": "Jira data sets of different internal",
    "start": "357600",
    "end": "360240"
  },
  {
    "text": "company databases as well as uh customer",
    "start": "360240",
    "end": "362639"
  },
  {
    "text": "support uh zendes data set type of uh",
    "start": "362639",
    "end": "365360"
  },
  {
    "text": "data sets So",
    "start": "365360",
    "end": "368199"
  },
  {
    "text": "uh uh so the end user question comes in",
    "start": "368199",
    "end": "371440"
  },
  {
    "text": "the question router determines which AI",
    "start": "371440",
    "end": "373199"
  },
  {
    "text": "path uh it's uh the data is stored",
    "start": "373199",
    "end": "376160"
  },
  {
    "text": "either in a vector database for",
    "start": "376160",
    "end": "377600"
  },
  {
    "text": "unstructured data and a SQL database for",
    "start": "377600",
    "end": "379440"
  },
  {
    "text": "structured data The blue box is Phoenix",
    "start": "379440",
    "end": "382560"
  },
  {
    "text": "Phoenix is an open source project from",
    "start": "382560",
    "end": "384160"
  },
  {
    "text": "Arise AI which worked really well out of",
    "start": "384160",
    "end": "386400"
  },
  {
    "text": "the box for auto instrumentation Uh this",
    "start": "386400",
    "end": "389840"
  },
  {
    "text": "meant that everything from input tokens",
    "start": "389840",
    "end": "392240"
  },
  {
    "text": "output tokens inference timings the",
    "start": "392240",
    "end": "394479"
  },
  {
    "text": "specific chunks chosen as part of rag",
    "start": "394479",
    "end": "396880"
  },
  {
    "text": "were all out of the box uh shown in a",
    "start": "396880",
    "end": "399360"
  },
  {
    "text": "nice GI environment and that worked",
    "start": "399360",
    "end": "400960"
  },
  {
    "text": "really well for",
    "start": "400960",
    "end": "403479"
  },
  {
    "text": "us Okay so uh this is the high level of",
    "start": "403479",
    "end": "407120"
  },
  {
    "text": "the pipeline We wanted to conclude with",
    "start": "407120",
    "end": "409600"
  },
  {
    "text": "uh some of the lessons learned in this",
    "start": "409600",
    "end": "412360"
  },
  {
    "text": "process Uh data data real data uh AI",
    "start": "412360",
    "end": "416240"
  },
  {
    "text": "pipelines are great They give us you a",
    "start": "416240",
    "end": "418160"
  },
  {
    "text": "starting structural framework but",
    "start": "418160",
    "end": "419919"
  },
  {
    "text": "without real data the final solution is",
    "start": "419919",
    "end": "422479"
  },
  {
    "text": "completely unusable not just battle",
    "start": "422479",
    "end": "424479"
  },
  {
    "text": "tested Uh so if you are planning for a",
    "start": "424479",
    "end": "428080"
  },
  {
    "text": "geni app delivery uh make sure you have",
    "start": "428080",
    "end": "431280"
  },
  {
    "text": "a sufficient part of your uh delivery",
    "start": "431280",
    "end": "433440"
  },
  {
    "text": "pipeline allocated to real data sets Uh",
    "start": "433440",
    "end": "436639"
  },
  {
    "text": "data prep-processing just like a",
    "start": "436639",
    "end": "438080"
  },
  {
    "text": "previous speaker talked about takes a",
    "start": "438080",
    "end": "439680"
  },
  {
    "text": "significant amount of time and affects",
    "start": "439680",
    "end": "441440"
  },
  {
    "text": "data uh answer accuracy significantly Uh",
    "start": "441440",
    "end": "445520"
  },
  {
    "text": "luckily for us uh ML engineers and data",
    "start": "445520",
    "end": "447599"
  },
  {
    "text": "engineers have been working on this for",
    "start": "447599",
    "end": "448800"
  },
  {
    "text": "decades There's a lot we can learn from",
    "start": "448800",
    "end": "450400"
  },
  {
    "text": "and leverage from that field Uh in our",
    "start": "450400",
    "end": "453039"
  },
  {
    "text": "case a lot of time was spent on field",
    "start": "453039",
    "end": "454560"
  },
  {
    "text": "cleanup uh choosing metadata for the rag",
    "start": "454560",
    "end": "457360"
  },
  {
    "text": "pipeline as well as uh SQL field",
    "start": "457360",
    "end": "459680"
  },
  {
    "text": "importance uh and cleanup Uh next comes",
    "start": "459680",
    "end": "462960"
  },
  {
    "text": "postprocessing of generated responses Uh",
    "start": "462960",
    "end": "466639"
  },
  {
    "text": "uh there's a lot of uh chat ML stands",
    "start": "466639",
    "end": "468960"
  },
  {
    "text": "for chat markup language tokens which",
    "start": "468960",
    "end": "470639"
  },
  {
    "text": "get generated as part of uh different",
    "start": "470639",
    "end": "472160"
  },
  {
    "text": "LLMs These needed uh quite a bit of",
    "start": "472160",
    "end": "474240"
  },
  {
    "text": "clean up and only after we started",
    "start": "474240",
    "end": "477440"
  },
  {
    "text": "testing with real data set we were able",
    "start": "477440",
    "end": "479360"
  },
  {
    "text": "to identify a large number of",
    "start": "479360",
    "end": "481280"
  },
  {
    "text": "hallucination patterns and once we were",
    "start": "481280",
    "end": "483360"
  },
  {
    "text": "able to observe patterns we were able to",
    "start": "483360",
    "end": "484879"
  },
  {
    "text": "clean them up uh pretty",
    "start": "484879",
    "end": "486680"
  },
  {
    "text": "well Uh although I'm focusing on real",
    "start": "486680",
    "end": "489360"
  },
  {
    "text": "data sets there's one situation where",
    "start": "489360",
    "end": "491280"
  },
  {
    "text": "synthetic data worked really well It is",
    "start": "491280",
    "end": "493360"
  },
  {
    "text": "for question generation If you talk to",
    "start": "493360",
    "end": "495440"
  },
  {
    "text": "your end users and say hey give me a",
    "start": "495440",
    "end": "497280"
  },
  {
    "text": "list of all types of questions that",
    "start": "497280",
    "end": "498800"
  },
  {
    "text": "you'd like the system to answer They're",
    "start": "498800",
    "end": "500479"
  },
  {
    "text": "not going to be able to give you an",
    "start": "500479",
    "end": "501520"
  },
  {
    "text": "answer they'll probably provide you with",
    "start": "501520",
    "end": "503039"
  },
  {
    "text": "a small subset slightly representative",
    "start": "503039",
    "end": "505280"
  },
  {
    "text": "of what they'll eventually use Uh this",
    "start": "505280",
    "end": "507759"
  },
  {
    "text": "is the case where synthetic data set",
    "start": "507759",
    "end": "509199"
  },
  {
    "text": "worked really well This was what we used",
    "start": "509199",
    "end": "511199"
  },
  {
    "text": "as a training data for our Q&A system",
    "start": "511199",
    "end": "514320"
  },
  {
    "text": "with a pretty good about 97% accuracy",
    "start": "514320",
    "end": "517200"
  },
  {
    "text": "for the question",
    "start": "517200",
    "end": "518518"
  },
  {
    "text": "router Uh finally uh every one of the",
    "start": "518519",
    "end": "521599"
  },
  {
    "text": "block pieces that we showed in the AI",
    "start": "521599",
    "end": "523440"
  },
  {
    "text": "pipeline has dozens of tuning parameters",
    "start": "523440",
    "end": "526800"
  },
  {
    "text": "This is a small sample of what was",
    "start": "526800",
    "end": "528640"
  },
  {
    "text": "needed Uh one of the things that we",
    "start": "528640",
    "end": "531120"
  },
  {
    "text": "learned was testing on local models such",
    "start": "531120",
    "end": "532640"
  },
  {
    "text": "as Olaml models on your local laptop is",
    "start": "532640",
    "end": "534880"
  },
  {
    "text": "great for ironing out uh Python",
    "start": "534880",
    "end": "537200"
  },
  {
    "text": "workflows but you have to use your",
    "start": "537200",
    "end": "540240"
  },
  {
    "text": "actual LM uh as soon as possible within",
    "start": "540240",
    "end": "543360"
  },
  {
    "text": "your delivery uh pipeline Uh we we uh",
    "start": "543360",
    "end": "546640"
  },
  {
    "text": "realize that each LLM is a unique beast",
    "start": "546640",
    "end": "549360"
  },
  {
    "text": "and it requires quite a bit of tender",
    "start": "549360",
    "end": "551040"
  },
  {
    "text": "loving care for to make it perform So",
    "start": "551040",
    "end": "553040"
  },
  {
    "text": "get on to using your uh final LLMs uh",
    "start": "553040",
    "end": "555519"
  },
  {
    "text": "before well before your app delivery uh",
    "start": "555519",
    "end": "558080"
  },
  {
    "text": "deadlines Finally don't do not expect to",
    "start": "558080",
    "end": "560959"
  },
  {
    "text": "use your LLM and your inference engine",
    "start": "560959",
    "end": "562560"
  },
  {
    "text": "as a black box There's a lot of uh",
    "start": "562560",
    "end": "565279"
  },
  {
    "text": "parameter tuning that is needed",
    "start": "565279",
    "end": "567120"
  },
  {
    "text": "Everything from your KV cache your model",
    "start": "567120",
    "end": "569120"
  },
  {
    "text": "context length the maximum number of",
    "start": "569120",
    "end": "571120"
  },
  {
    "text": "tokens uh you know tensor pipeline",
    "start": "571120",
    "end": "573120"
  },
  {
    "text": "parallelism that is part of your",
    "start": "573120",
    "end": "574720"
  },
  {
    "text": "inference engine that become really",
    "start": "574720",
    "end": "576240"
  },
  {
    "text": "important Uh an example of say model",
    "start": "576240",
    "end": "578560"
  },
  {
    "text": "context length in theory the 53 model",
    "start": "578560",
    "end": "580480"
  },
  {
    "text": "can take for example 128k context length",
    "start": "580480",
    "end": "583279"
  },
  {
    "text": "We started with it 4k context length and",
    "start": "583279",
    "end": "585120"
  },
  {
    "text": "we said hey this is working really well",
    "start": "585120",
    "end": "586880"
  },
  {
    "text": "If we were able to put in more context",
    "start": "586880",
    "end": "588480"
  },
  {
    "text": "maybe our answers will be more accurate",
    "start": "588480",
    "end": "590640"
  },
  {
    "text": "But we're uh kind of uh hit back by",
    "start": "590640",
    "end": "593440"
  },
  {
    "text": "delayed inference latencies uh poor",
    "start": "593440",
    "end": "595519"
  },
  {
    "text": "performance and uh inference latencies",
    "start": "595519",
    "end": "597920"
  },
  {
    "text": "as up as high as a few dozens of seconds",
    "start": "597920",
    "end": "600399"
  },
  {
    "text": "which makes your system completely",
    "start": "600399",
    "end": "601600"
  },
  {
    "text": "unusable So we had to do parameter",
    "start": "601600",
    "end": "603680"
  },
  {
    "text": "scanning to figure out that uh optimal",
    "start": "603680",
    "end": "606480"
  },
  {
    "text": "golden context length that works well",
    "start": "606480",
    "end": "608399"
  },
  {
    "text": "and uh uh good latencies as well as",
    "start": "608399",
    "end": "611600"
  },
  {
    "text": "reasonable answer accuracy",
    "start": "611600",
    "end": "614560"
  },
  {
    "text": "uh rag systems once again a lot of",
    "start": "614560",
    "end": "616160"
  },
  {
    "text": "parameter tuning available there",
    "start": "616160",
    "end": "617920"
  },
  {
    "text": "everything from chunk size if you were",
    "start": "617920",
    "end": "619440"
  },
  {
    "text": "to use uh uh your uh you know vector",
    "start": "619440",
    "end": "622640"
  },
  {
    "text": "db's API or your lang chain lang graph",
    "start": "622640",
    "end": "625600"
  },
  {
    "text": "APIs you'll be provided a lot of out of",
    "start": "625600",
    "end": "627519"
  },
  {
    "text": "the box parameters which will most",
    "start": "627519",
    "end": "629680"
  },
  {
    "text": "probably not work so you'll have to",
    "start": "629680",
    "end": "631440"
  },
  {
    "text": "spend quite a bit of time to zero in for",
    "start": "631440",
    "end": "633839"
  },
  {
    "text": "your specific data sets so uh these are",
    "start": "633839",
    "end": "638079"
  },
  {
    "text": "some of our lessons learned uh that's",
    "start": "638079",
    "end": "640240"
  },
  {
    "text": "all I have for today our infra stack is",
    "start": "640240",
    "end": "642720"
  },
  {
    "text": "fresh out of the oven it's It's",
    "start": "642720",
    "end": "644079"
  },
  {
    "text": "available at our GitHub repo Please uh",
    "start": "644079",
    "end": "646320"
  },
  {
    "text": "give it a try for self-hosted large",
    "start": "646320",
    "end": "648399"
  },
  {
    "text": "language model question answer systems",
    "start": "648399",
    "end": "651040"
  },
  {
    "text": "Uh send me email connect with me on",
    "start": "651040",
    "end": "652800"
  },
  {
    "text": "LinkedIn Happy to help you make it work",
    "start": "652800",
    "end": "654560"
  },
  {
    "text": "for your uh environment Uh and thank you",
    "start": "654560",
    "end": "658240"
  },
  {
    "text": "I'm happy to take any questions if you",
    "start": "658240",
    "end": "659839"
  },
  {
    "text": "have any",
    "start": "659839",
    "end": "661760"
  },
  {
    "text": "Thank you Thank you very",
    "start": "661760",
    "end": "664839"
  },
  {
    "text": "much So don't if you have a question",
    "start": "664839",
    "end": "668560"
  },
  {
    "text": "just go to the microphone I see one",
    "start": "668560",
    "end": "671880"
  },
  {
    "text": "there Yeah Go ahead Hello Uh great talk",
    "start": "671880",
    "end": "676640"
  },
  {
    "text": "Thank you very much Uh very inspiring Uh",
    "start": "676640",
    "end": "679519"
  },
  {
    "text": "I would have two questions Uh the first",
    "start": "679519",
    "end": "681839"
  },
  {
    "text": "one being a little bit controversive",
    "start": "681839",
    "end": "684000"
  },
  {
    "text": "being here at CubeCon Why Kubernetes uh",
    "start": "684000",
    "end": "687279"
  },
  {
    "text": "why not in a different um um and the",
    "start": "687279",
    "end": "691200"
  },
  {
    "text": "second question uh you mentioned about",
    "start": "691200",
    "end": "693279"
  },
  {
    "text": "tool calling and I forgot Ruby",
    "start": "693279",
    "end": "697440"
  },
  {
    "text": "Yeah Um my question is are you using",
    "start": "697440",
    "end": "700240"
  },
  {
    "text": "model contracts protocol at all and does",
    "start": "700240",
    "end": "702480"
  },
  {
    "text": "it does it work uh sure First question",
    "start": "702480",
    "end": "705680"
  },
  {
    "text": "Uh Kubernetes uh scalability was an",
    "start": "705680",
    "end": "709040"
  },
  {
    "text": "important uh requirement for us and uh",
    "start": "709040",
    "end": "711760"
  },
  {
    "text": "Kubernetes provides that out of the box",
    "start": "711760",
    "end": "713200"
  },
  {
    "text": "for us Uh we built cluster autoscalers",
    "start": "713200",
    "end": "715839"
  },
  {
    "text": "So it was a match made in heaven The",
    "start": "715839",
    "end": "717839"
  },
  {
    "text": "distributed inference engines worked",
    "start": "717839",
    "end": "719760"
  },
  {
    "text": "great For example we would start with",
    "start": "719760",
    "end": "721600"
  },
  {
    "text": "the cheapest AWS EKS instances We could",
    "start": "721600",
    "end": "724480"
  },
  {
    "text": "quickly uh kind of scale it up to say",
    "start": "724480",
    "end": "726880"
  },
  {
    "text": "you know four GPUs We had improvement in",
    "start": "726880",
    "end": "729040"
  },
  {
    "text": "inference times from a few dozens of",
    "start": "729040",
    "end": "731040"
  },
  {
    "text": "seconds to a few seconds So the",
    "start": "731040",
    "end": "732480"
  },
  {
    "text": "scalability of all of the pieces work",
    "start": "732480",
    "end": "734000"
  },
  {
    "text": "really well together which is why we",
    "start": "734000",
    "end": "735519"
  },
  {
    "text": "went with Kubernetes Uh with respect to",
    "start": "735519",
    "end": "738560"
  },
  {
    "text": "uh tool calling no we do not use a model",
    "start": "738560",
    "end": "740880"
  },
  {
    "text": "control uh protocol Uh these uh",
    "start": "740880",
    "end": "744399"
  },
  {
    "text": "fine-tuned and the foundational models",
    "start": "744399",
    "end": "746639"
  },
  {
    "text": "worked really well for us Thank you very",
    "start": "746639",
    "end": "749200"
  },
  {
    "text": "much Thank you for the question",
    "start": "749200",
    "end": "752240"
  },
  {
    "text": "Thank",
    "start": "752240",
    "end": "752920"
  },
  {
    "text": "you There's one more question and that's",
    "start": "752920",
    "end": "756079"
  },
  {
    "text": "it after Yeah Okay Thank you very much",
    "start": "756079",
    "end": "758480"
  },
  {
    "text": "for the talk I really did enjoy it I",
    "start": "758480",
    "end": "760320"
  },
  {
    "text": "have a question on the data processing",
    "start": "760320",
    "end": "762560"
  },
  {
    "text": "Um what was your tech stack how did you",
    "start": "762560",
    "end": "764560"
  },
  {
    "text": "actually process it did you use Go and",
    "start": "764560",
    "end": "766480"
  },
  {
    "text": "if so was it like Go routines give us a",
    "start": "766480",
    "end": "769120"
  },
  {
    "text": "little insight Uh sure It was a lot of",
    "start": "769120",
    "end": "771600"
  },
  {
    "text": "Python processing Everything was Python",
    "start": "771600",
    "end": "774000"
  },
  {
    "text": "We were a Go uh tech stack but as soon",
    "start": "774000",
    "end": "776240"
  },
  {
    "text": "as we moved into the you know going up",
    "start": "776240",
    "end": "778079"
  },
  {
    "text": "the geni stack uh Python was what",
    "start": "778079",
    "end": "780560"
  },
  {
    "text": "allowed us to provide us with a lot of",
    "start": "780560",
    "end": "782079"
  },
  {
    "text": "the modules we needed for cleanup uh uh",
    "start": "782079",
    "end": "784800"
  },
  {
    "text": "pre-processing and cleanup Got it Thank",
    "start": "784800",
    "end": "786880"
  },
  {
    "text": "you Thank you Thank you very much Let's",
    "start": "786880",
    "end": "789600"
  },
  {
    "text": "thank Sylvia again Thank you",
    "start": "789600",
    "end": "794199"
  }
]