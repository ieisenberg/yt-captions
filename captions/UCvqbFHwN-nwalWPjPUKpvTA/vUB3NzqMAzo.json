[
  {
    "text": "okay good afternoon everybody thanks for being here my name is jpe I work for for",
    "start": "120",
    "end": "5600"
  },
  {
    "text": "I work for CERN and in particular for the data Cris of Atlas which is one of the big experiments at CERN so let's",
    "start": "5600",
    "end": "12160"
  },
  {
    "text": "start with the with the talk now um I'm I would like to to to tell you a story",
    "start": "12160",
    "end": "17400"
  },
  {
    "text": "that is the story of our kubernetes journey in the that acquisition system of the atel experiment at CERN and I",
    "start": "17400",
    "end": "23640"
  },
  {
    "text": "have divided this journey into three parts the departure the adventure and the reward but before going deep into",
    "start": "23640",
    "end": "29720"
  },
  {
    "text": "that I would like to spend a few words about Atlas and large Adon collider which is the accelerator providing the",
    "start": "29720",
    "end": "37239"
  },
  {
    "text": "physics that we like and additionally let me also give some motivations and the challenges that we are going to face",
    "start": "37239",
    "end": "43760"
  },
  {
    "text": "introducing kubernetes in our in our system so let's start with Atlas and LHC",
    "start": "43760",
    "end": "49039"
  },
  {
    "text": "and and this is atlas so this is a picture a real picture in scale of Atlas it's actually a big detector as you can",
    "start": "49039",
    "end": "55920"
  },
  {
    "text": "see and at is one of the main of the four main particle detectors are the large Adon collider and when I say huge",
    "start": "55920",
    "end": "62760"
  },
  {
    "text": "I say I say that because atas is 44 M long and 25 M high so really really",
    "start": "62760",
    "end": "69439"
  },
  {
    "text": "really a little bit and you see it has this Barrel shape with two end caps and",
    "start": "69439",
    "end": "74600"
  },
  {
    "text": "consider that that collisions are actually happening at the heart of Atlas so atas is really built around one of",
    "start": "74600",
    "end": "80920"
  },
  {
    "text": "the places where the collisions are happening but what is the goal of attles so obviously the goal of attles is to",
    "start": "80920",
    "end": "87240"
  },
  {
    "text": "collect data from the proton collisions but what does that really mean that Tas has to collect this data I will try to",
    "start": "87240",
    "end": "94280"
  },
  {
    "text": "explain here in a in a in a kind of few word so collisions at LHC are happening",
    "start": "94280",
    "end": "100399"
  },
  {
    "text": "at a rate of 40 MHz so this means that every 25 NCS there there are protons colliding into the accelerator as I said",
    "start": "100399",
    "end": "107439"
  },
  {
    "text": "before the protons are colliding actually at the in the middle of Atlas really at the Earth of Atlas and the",
    "start": "107439",
    "end": "112960"
  },
  {
    "text": "product of proton collisions are several other particles so you have a huge amount of particles that are traversing",
    "start": "112960",
    "end": "118799"
  },
  {
    "text": "the atlas detector all the active parts of the atas detector and all the electric signals then are digitized and",
    "start": "118799",
    "end": "125520"
  },
  {
    "text": "we have data out of collisions and this is what we call an event okay so this is the event is the representation of the",
    "start": "125520",
    "end": "132400"
  },
  {
    "text": "collisions for for atas so we can say I will mention this word several times event so atas the input rate for the",
    "start": "132400",
    "end": "139280"
  },
  {
    "text": "events of atas is 40 Megs matching the the Collision rate of the of the LHC and",
    "start": "139280",
    "end": "144360"
  },
  {
    "text": "what is the final goal of atas as I said before we want to collect this data this physics event and we want to save them",
    "start": "144360",
    "end": "150440"
  },
  {
    "text": "to storage uh for physics analysis for fter physics analysis the point is that",
    "start": "150440",
    "end": "155840"
  },
  {
    "text": "we want to collect the interesting event the the interesting events we cannot collect everything because we cannot",
    "start": "155840",
    "end": "161239"
  },
  {
    "text": "afford writing for a very sustained in a very sustained way for years for months for years uh all the amount of data that",
    "start": "161239",
    "end": "168400"
  },
  {
    "text": "we produce so that's why we have a way to filter event online and this filtering is done in two stages the",
    "start": "168400",
    "end": "175400"
  },
  {
    "text": "first stage is Hardware based so a decision is taken in few milliseconds and the second stage is software based",
    "start": "175400",
    "end": "181280"
  },
  {
    "text": "so we have a kind of big form uh where our applications are running and are",
    "start": "181280",
    "end": "186480"
  },
  {
    "text": "able to filter these event and this is what I call the filtering farm and let's try to look also a little bit of how",
    "start": "186480",
    "end": "193200"
  },
  {
    "text": "much data we are we are managing so uh as I say the first level of filtering is",
    "start": "193200",
    "end": "198560"
  },
  {
    "text": "Hardware based and event rate goes from 40 MHz down to 100 khz corresponding to",
    "start": "198560",
    "end": "204400"
  },
  {
    "text": "amount of data of more or less 300 more or less 300 gabt per second so this is",
    "start": "204400",
    "end": "209519"
  },
  {
    "text": "the of data that our even filtering Farm orline even filtering Farm has to sustain and then the second the second",
    "start": "209519",
    "end": "216439"
  },
  {
    "text": "phase of the filtering as I I said before done by the EV filtering Farm reduces this amount of data down to 3",
    "start": "216439",
    "end": "223080"
  },
  {
    "text": "Kilz and this is the amount of data that we write to dis for I mean for years okay so for years we write 8 gbes per",
    "start": "223080",
    "end": "230760"
  },
  {
    "text": "second this is not the end of the story because we are working to upgrade the accelerator in what we call the high",
    "start": "230760",
    "end": "237040"
  },
  {
    "text": "Luminosity LHC and also the uh the detector itself Atlas will be will be",
    "start": "237040",
    "end": "242480"
  },
  {
    "text": "upgraded and as you can see we are going to have higher rate up to 1 MHz inside",
    "start": "242480",
    "end": "248000"
  },
  {
    "text": "input as input for our H and filtering Farm corresponding to about 5 terabytes",
    "start": "248000",
    "end": "253360"
  },
  {
    "text": "of data per second and we are going to write to dis something like 50 gbt per second again for years and where is",
    "start": "253360",
    "end": "261160"
  },
  {
    "text": "kubernetes in all of that we want actually our goal is to use kubernetes to orchestrate our online even filtering",
    "start": "261160",
    "end": "269000"
  },
  {
    "text": "even filtering farm so we are working on that this is something that is going to happen in a couple of years from now but",
    "start": "269000",
    "end": "274440"
  },
  {
    "text": "we are doing already a lot of work to be sure that kubernetes can do the job for us and what are the the challenges of uh",
    "start": "274440",
    "end": "282120"
  },
  {
    "text": "of using kubernetes in our online filtering farm so let me point out we are work we are talking about something",
    "start": "282120",
    "end": "287840"
  },
  {
    "text": "that happens online that is at the very beginning of our of our of our data taking so if anything is lost if our",
    "start": "287840",
    "end": "294600"
  },
  {
    "text": "infrastructure doesn't work properly if our applications do not work properly what is lost cannot be recovered so we",
    "start": "294600",
    "end": "300160"
  },
  {
    "text": "lose time we lose money and we lose knowledge we lose physics I've tried to to summarize the challenges in into four",
    "start": "300160",
    "end": "307000"
  },
  {
    "text": "main main items so the first one is sides so we are going to have a kind of a big farm today we have something like",
    "start": "307000",
    "end": "313320"
  },
  {
    "text": "2,000 servers corresponding to about 60,000 CPU cores and in the upgrade",
    "start": "313320",
    "end": "318759"
  },
  {
    "text": "scenarios for the high Luminosity LHC we are going to have something like 5,000 this is the let's say the conservative",
    "start": "318759",
    "end": "325759"
  },
  {
    "text": "scenario with about 500k CPU cores",
    "start": "325759",
    "end": "331360"
  },
  {
    "text": "this corresponds also to high data volume and low latency as I said before we are going to move a lot of terabytes",
    "start": "331360",
    "end": "336520"
  },
  {
    "text": "of data and also the time budget that we have this happens in quasy real time so",
    "start": "336520",
    "end": "341800"
  },
  {
    "text": "the the single event has to be selected or eventually discarded in less than one second so we have to be somehow fast",
    "start": "341800",
    "end": "349319"
  },
  {
    "text": "number three we have a high number of uh processing applications so together with 5,000 servers we the last estimates that",
    "start": "349319",
    "end": "356800"
  },
  {
    "text": "we have in the worst case scenario we need to start something like 65,000 of filtering application in the case of the",
    "start": "356800",
    "end": "363440"
  },
  {
    "text": "LHC Ade last but not least Readiness so we need to be ready in a prompt way why",
    "start": "363440",
    "end": "370800"
  },
  {
    "text": "because it's not a start and forget scenario so the LHC provides collisions in in batches if you want so the LHC",
    "start": "370800",
    "end": "377400"
  },
  {
    "text": "provides collisions for 12 24 hours and then the accelerator needs to be refilled again with protons okay and we",
    "start": "377400",
    "end": "384479"
  },
  {
    "text": "in Atlas we have to be ready to start a data taking session as soon as the collisions are available at the same",
    "start": "384479",
    "end": "390240"
  },
  {
    "text": "time so if something weird happens in our Computing farm and we need to restart this application we need to be",
    "start": "390240",
    "end": "395880"
  },
  {
    "text": "fast because otherwise as I said I said before whatever is lost at this at this stage is lost forever there is there is",
    "start": "395880",
    "end": "401800"
  },
  {
    "text": "no other way so to summarize this first part so we what what do we want to do",
    "start": "401800",
    "end": "407880"
  },
  {
    "text": "with kubernetes today of course we are able to manage our our farm but we do it with the in a custom way with custom",
    "start": "407880",
    "end": "414039"
  },
  {
    "text": "software all developed by us so we have a custom process control Custer schedule custom scheduling we have also",
    "start": "414039",
    "end": "420800"
  },
  {
    "text": "intelligent pieces with for for eror detection and management at the same time it's all based on be process we do",
    "start": "420800",
    "end": "427080"
  },
  {
    "text": "not support containers so I'm not going to tell you how nice is kubernetes but let just me highlight few few points",
    "start": "427080",
    "end": "434479"
  },
  {
    "text": "introducing kubernetes we would like to have a simplified operations and maintenance of course we would like to have a more Dynamic scheduling that in",
    "start": "434479",
    "end": "441199"
  },
  {
    "text": "our case is pretty static at the moment we would like to enhance High have ability and full tolerance and of course",
    "start": "441199",
    "end": "446639"
  },
  {
    "text": "we want to exploit containerization let's start start with the journey as say as said before the",
    "start": "446639",
    "end": "452360"
  },
  {
    "text": "first the first part of this journey is the departure and we started with that in 2018 so a lot of time ago and at the",
    "start": "452360",
    "end": "460039"
  },
  {
    "text": "time we were just performing some preliminary studies to check to verify that kubernetes could do the job for us",
    "start": "460039",
    "end": "466479"
  },
  {
    "text": "and what was our main goal well we actually had two two main goals first we wanted to understand how fast we could",
    "start": "466479",
    "end": "473199"
  },
  {
    "text": "start the pods and also we would like to understand okay but how this goes if we have if we change the the number of",
    "start": "473199",
    "end": "479960"
  },
  {
    "text": "working nodes so how this scales with the with the number of working nodes at the time we had a very a very simple a",
    "start": "479960",
    "end": "485639"
  },
  {
    "text": "very simple system so we were running kubernetes version5 okay it looks like ages maybe it is yes uh we had a simple",
    "start": "485639",
    "end": "492960"
  },
  {
    "text": "one single control play node and we were we were able to put together up to 20",
    "start": "492960",
    "end": "498599"
  },
  {
    "text": "240 working nodes and we were starting something like for more or less for four",
    "start": "498599",
    "end": "504199"
  },
  {
    "text": "pods per node in a very simple way single pose container so we just want to measure how fast kubernetes without any",
    "start": "504199",
    "end": "510639"
  },
  {
    "text": "any other kind of of other load the container was preloaded and everything",
    "start": "510639",
    "end": "516919"
  },
  {
    "text": "was running in Virtual machines and the plot that you see on the right is is actually showing the time needed to",
    "start": "516919",
    "end": "523200"
  },
  {
    "text": "start all the containers as a function of the work of the number of worker nodes and what what could at the time",
    "start": "523200",
    "end": "530000"
  },
  {
    "text": "what we were able to extract from this information so from sure there was some good linear scaling this is always good",
    "start": "530000",
    "end": "535040"
  },
  {
    "text": "that your system is predictable so you can understand pretty well what happen at the same time",
    "start": "535040",
    "end": "540160"
  },
  {
    "text": "it was taking something like 70 seconds just to start 1,00 pods which was I say",
    "start": "540160",
    "end": "546000"
  },
  {
    "text": "not not not really what we will wanted to to have and then something happened so I I",
    "start": "546000",
    "end": "552519"
  },
  {
    "text": "I like to say that QPS went to rescue so we were dinging a little at the time a little bit into the documentation and",
    "start": "552519",
    "end": "558880"
  },
  {
    "text": "all the options that you could pass to the various kubernetes components and we noticed these uh QPS values that you",
    "start": "558880",
    "end": "565360"
  },
  {
    "text": "could pass to the controller manager and the cube schul and we said okay but let's see what happens if we increase this and we played a simple game so we",
    "start": "565360",
    "end": "572560"
  },
  {
    "text": "say okay we take the default values of this and we just multiply for a fixed multiplier and let's see how the",
    "start": "572560",
    "end": "578360"
  },
  {
    "text": "performance goes better or worse and the result is on the left on the plot on the left again the same plot as before the",
    "start": "578360",
    "end": "585440"
  },
  {
    "text": "time needed to start all the containers as a function the pods as a function of the working nodes for different values",
    "start": "585440",
    "end": "590880"
  },
  {
    "text": "of this QPS multiplier up to four times and you can see that actually the time is greatly reduced and if you look at",
    "start": "590880",
    "end": "597160"
  },
  {
    "text": "the other plot on the right that shows the average starting the average pod starting rate",
    "start": "597160",
    "end": "602240"
  },
  {
    "text": "you can see that we were going from less than 20 for the default configuration",
    "start": "602240",
    "end": "607360"
  },
  {
    "text": "with almost 50 for the for the for a QPS multiplier of four so what what we could",
    "start": "607360",
    "end": "614959"
  },
  {
    "text": "say about that what was the executive summary of this very preliminary test so generally the system was behaving in a",
    "start": "614959",
    "end": "621880"
  },
  {
    "text": "good way so linear scaling all the perform all the results would be reproduced pretty easily and there was a",
    "start": "621880",
    "end": "627360"
  },
  {
    "text": "nice performance Improvement increasing the the QPS with this with this tunit at the same time what was not really let's",
    "start": "627360",
    "end": "633600"
  },
  {
    "text": "say at the spot the Pod startup rate was still a little bit too low but anyway results were reassuring and we were",
    "start": "633600",
    "end": "639959"
  },
  {
    "text": "confident that we could do something better to to use kubernetes in our system and that is where the real",
    "start": "639959",
    "end": "645399"
  },
  {
    "text": "Adventure Starts so the the previous tests were actually pretty nice I would",
    "start": "645399",
    "end": "651440"
  },
  {
    "text": "say Okay reassuring but there were a lot of questions without any answer so which pod startup rate we we could be",
    "start": "651440",
    "end": "659360"
  },
  {
    "text": "able to reach to to achieve what about the QPS limits so we could increase this whatever value so we are just free store",
    "start": "659360",
    "end": "666200"
  },
  {
    "text": "and performance increase what happens if we going to have a much larger cluster because we just tested for 240 nodes and",
    "start": "666200",
    "end": "674120"
  },
  {
    "text": "what if we put in place some scheduling strategy so what if we want to to put the the pods exactly where we want and",
    "start": "674120",
    "end": "680800"
  },
  {
    "text": "the way we want and last the most important question what happens when we increase massively the number of PS um",
    "start": "680800",
    "end": "689320"
  },
  {
    "text": "and the answer to this question was well our way to have uh to to get answers to this question was actually to do some",
    "start": "689320",
    "end": "695600"
  },
  {
    "text": "systematic test using the current available Computing farm at the atas experimental site that you can imagine",
    "start": "695600",
    "end": "702160"
  },
  {
    "text": "it's not easy because we are even today we are taking data we take data 24/7 and we don't have two Farms or two",
    "start": "702160",
    "end": "708079"
  },
  {
    "text": "accelerators or two or two two detectors so this was anyway some challenging",
    "start": "708079",
    "end": "713880"
  },
  {
    "text": "situation but okay we managed to to put a to to build a test set up at the experimental side",
    "start": "713880",
    "end": "719760"
  },
  {
    "text": "so we prepared our cluster with four contol plane contol plane servers four dedicated atcd servers so three in a",
    "start": "719760",
    "end": "727000"
  },
  {
    "text": "cluster and one just used to send the events the special events to to it and we were able to put together about 2600",
    "start": "727000",
    "end": "735040"
  },
  {
    "text": "worker worker nodes and everything was in bare metal okay no no VMS something",
    "start": "735040",
    "end": "740519"
  },
  {
    "text": "like that and what was the testing plan so what we wanted to do first of all we wanted to study the Pod startup startup",
    "start": "740519",
    "end": "747959"
  },
  {
    "text": "and stop rate to see how these these scales the usual the usual strategy post",
    "start": "747959",
    "end": "753079"
  },
  {
    "text": "container simple post container already pre-loaded on the machine we also decided to to to put in place realistic",
    "start": "753079",
    "end": "760639"
  },
  {
    "text": "scheduling as I was saying before so I want to put the pods in the worker no the way I want not just the way",
    "start": "760639",
    "end": "765880"
  },
  {
    "text": "kubernetes wants and we decide to exploit the cluster loader two and we just had the very little modification so",
    "start": "765880",
    "end": "772120"
  },
  {
    "text": "let's just follow the full life cycle of every single code and let's just print",
    "start": "772120",
    "end": "777480"
  },
  {
    "text": "this data to a comma separated for fter for fasther analysis and",
    "start": "777480",
    "end": "782920"
  },
  {
    "text": "then as I said with the Custer loader two we can we can look at the single startup phases so from the creation of",
    "start": "782920",
    "end": "789880"
  },
  {
    "text": "the Pod up to the watching phase and the first thing that we did we",
    "start": "789880",
    "end": "795920"
  },
  {
    "text": "went back to the QPS and say okay let's see what we can really do at scale with QPS so we prepared a um a cluster of",
    "start": "795920",
    "end": "803279"
  },
  {
    "text": "2,000 worker nodes we started 10,000 10,000 podes and in the configuration",
    "start": "803279",
    "end": "808600"
  },
  {
    "text": "that you can read on the slide so we used OST Network I will come later why we use OST Network we have some good reasons for that and we used a very",
    "start": "808600",
    "end": "815920"
  },
  {
    "text": "simple deployment and it was at the time in kubernetes version 1 Point",
    "start": "815920",
    "end": "821880"
  },
  {
    "text": "12221 and then we set the API QPS equal to the API burst which is a little bit",
    "start": "821880",
    "end": "827160"
  },
  {
    "text": "strange but we made a lot of test and in our use case in our case there were no real changes putting a higher value for",
    "start": "827160",
    "end": "833600"
  },
  {
    "text": "the burst and the two and the two the two tables that you can see there are showing actually the startup stop times",
    "start": "833600",
    "end": "841079"
  },
  {
    "text": "for the different configuration of QPS values both for the scheduler and and the controller managers so without going",
    "start": "841079",
    "end": "846959"
  },
  {
    "text": "too much into the details of the of the numbers because they may be actually relative to our setup but there is there",
    "start": "846959",
    "end": "852759"
  },
  {
    "text": "there is something that we can extract from this from this data so let's look at at the startup time first of all any",
    "start": "852759",
    "end": "859199"
  },
  {
    "text": "changes in the schedular QPS are only relevant for a controller manager QPS higher than 400 second point for a SC of",
    "start": "859199",
    "end": "866199"
  },
  {
    "text": "QPS of of of 200 actually there is is no gain in changing fter the controller",
    "start": "866199",
    "end": "872480"
  },
  {
    "text": "manager QPS what about the stop time much easier so the stop time was completely independent of the schedular",
    "start": "872480",
    "end": "878480"
  },
  {
    "text": "QPS and the stop time were the stop times were actually better with higher",
    "start": "878480",
    "end": "883519"
  },
  {
    "text": "QPS for the controller manager so given these observations we decided to set our working points and we said okay we use",
    "start": "883519",
    "end": "890399"
  },
  {
    "text": "800 for both for the QPS of both the controller manager and uh and uh and the",
    "start": "890399",
    "end": "895839"
  },
  {
    "text": "schul so all the results that are going to that I'm going to to show show in the next moments are referring to this",
    "start": "895839",
    "end": "903440"
  },
  {
    "text": "configuration but okay we see we see these results but why so we want to understand why if we increase this or",
    "start": "903440",
    "end": "910160"
  },
  {
    "text": "that the results change and at that moment we started looking as I was saying before to the startup phes to all",
    "start": "910160",
    "end": "916800"
  },
  {
    "text": "the faces for each single p and the graphs that you can see here are actually referring to the same configuration the only thing that",
    "start": "916800",
    "end": "922839"
  },
  {
    "text": "changes is the value of the controller manager QPS 400 on the 400 on the left",
    "start": "922839",
    "end": "928319"
  },
  {
    "text": "and 800 on the right and the profile shows the number this plot shows the number of pods for each single phase as",
    "start": "928319",
    "end": "935240"
  },
  {
    "text": "a function of time so what you can see is that what we can learn again from that so clearly if we increase the",
    "start": "935240",
    "end": "941160"
  },
  {
    "text": "controller manager QPS from 400 to 800 we see a nice increase in the creation rate so the pods take much shorter time",
    "start": "941160",
    "end": "949600"
  },
  {
    "text": "to be created while there is actually no no no difference for the other for the other for the other faces and let me",
    "start": "949600",
    "end": "956600"
  },
  {
    "text": "also point out that the configuration is exactly the same as before so 10,000 pods over to 2,000 2,000 nodes and this",
    "start": "956600",
    "end": "963759"
  },
  {
    "text": "explains what we have seen before right so we we can clearly see why if the controller manager if the controller",
    "start": "963759",
    "end": "970319"
  },
  {
    "text": "manager QPS was not high enough any increase in the schedular QPS could not led to any result because the limiting",
    "start": "970319",
    "end": "977279"
  },
  {
    "text": "factor was actually the controller manager so this was giving at least an explanation to what we were observing",
    "start": "977279",
    "end": "983800"
  },
  {
    "text": "and what we learn so let's summarize what we learned up to now so lesson number one some PS tuning is needed in",
    "start": "983800",
    "end": "990360"
  },
  {
    "text": "large clusters to improve P start up and stop times lesson number two scheduling",
    "start": "990360",
    "end": "995680"
  },
  {
    "text": "time may actually be a bottleneck for large deployments this was the first stage",
    "start": "995680",
    "end": "1002600"
  },
  {
    "text": "okay we have understood what happens we have a clue of what happens with the with the QPS but then what if we want to",
    "start": "1002600",
    "end": "1008639"
  },
  {
    "text": "schedule the pods the way we want let me first say what is our main goal our main",
    "start": "1008639",
    "end": "1013680"
  },
  {
    "text": "goal is to try to demonstrate that we can do what we do today with our system with our c system with kubernetes and",
    "start": "1013680",
    "end": "1020880"
  },
  {
    "text": "what we do today we fill the farm that we have as much as we can in a kind of uniform way okay and this is what we we",
    "start": "1020880",
    "end": "1027880"
  },
  {
    "text": "want to achieve with kubernetes as well we want to demonstrate that we can do what we do today even with kubernetes",
    "start": "1027880",
    "end": "1033678"
  },
  {
    "text": "and we we attack the problem with in two different from two different perspectives so the first one is the",
    "start": "1033679",
    "end": "1039400"
  },
  {
    "text": "strategy to deploy and we used three different strategy the first one a simple deployment and we were using",
    "start": "1039400",
    "end": "1045199"
  },
  {
    "text": "resource description to properly distribute the the pods over the the cluster the second one is a deployment",
    "start": "1045199",
    "end": "1051200"
  },
  {
    "text": "with a topological spread where the topological zone is the single node so we could say okay let's start five five",
    "start": "1051200",
    "end": "1056880"
  },
  {
    "text": "pods per node using the topological spread and the third one a little bit less Orthodox if you want multiple demon",
    "start": "1056880",
    "end": "1063000"
  },
  {
    "text": "sets so I want five pods per node I start five five demon sets it's not really easy I mean but it's it was worth",
    "start": "1063000",
    "end": "1070919"
  },
  {
    "text": "uh testing at least from a performance point of view to understand better and the second perspective to attack the problem was configuration so let's try",
    "start": "1070919",
    "end": "1077640"
  },
  {
    "text": "to see what we can do at the scheduling level we can create for instance a custom scheduler profile or we can also",
    "start": "1077640",
    "end": "1083679"
  },
  {
    "text": "tune the number of the percentage of nodes to score because again we want to populate the full F so we don't really",
    "start": "1083679",
    "end": "1088880"
  },
  {
    "text": "need maybe to score too many nodes and let's look at the results for the first",
    "start": "1088880",
    "end": "1094559"
  },
  {
    "text": "part so the scheduling strategies the PS the the plots that you can see refer to the three different strategies that I've",
    "start": "1094559",
    "end": "1101720"
  },
  {
    "text": "shown before and again is is the time needed to start all the pods as a",
    "start": "1101720",
    "end": "1107120"
  },
  {
    "text": "function of the number of working notes worker nodes in the cluster and for a different number of pods in pods per",
    "start": "1107120",
    "end": "1114240"
  },
  {
    "text": "node from one up to 10 which means that we reached the maximum of 26,000 pods",
    "start": "1114240",
    "end": "1119919"
  },
  {
    "text": "over 2600 2600 uh uh servers worker nodes and we were using for this",
    "start": "1119919",
    "end": "1126559"
  },
  {
    "text": "kubernetes kubernetes 1.8 one 128 which is actually what we did last last year",
    "start": "1126559",
    "end": "1132840"
  },
  {
    "text": "so there is clearly a dependency on the start time upon the upon the strategies",
    "start": "1132840",
    "end": "1138799"
  },
  {
    "text": "with the topological spread being the most expensive one so you can see that this stops to 200 seconds the simple",
    "start": "1138799",
    "end": "1145159"
  },
  {
    "text": "deployment with the resource description on average and the fastest one the the the the multiple the multiple demon sets",
    "start": "1145159",
    "end": "1152640"
  },
  {
    "text": "and again nice to see this but why and uh to find a question we again we",
    "start": "1152640",
    "end": "1158960"
  },
  {
    "text": "started looking at the single phases of the Pod startup rate so the first thing that you we can observe is exactly the",
    "start": "1158960",
    "end": "1166200"
  },
  {
    "text": "same PL as before a profile of the number of Po as a function of time for each faces and we can clearly see that",
    "start": "1166200",
    "end": "1173000"
  },
  {
    "text": "there is there is no change more or less in the time needed to create the the pods so all the pods reach the cre the",
    "start": "1173000",
    "end": "1180039"
  },
  {
    "text": "created phase pretty fast and if you look at the rate if you look at this plot at the rate on which pods are",
    "start": "1180039",
    "end": "1185080"
  },
  {
    "text": "created so this value is pretty close to the 800 QPS value that we have used to",
    "start": "1185080",
    "end": "1190159"
  },
  {
    "text": "configure the controller manager the other thing that you can see on the plot is also the scheduling rate for each",
    "start": "1190159",
    "end": "1196080"
  },
  {
    "text": "case so on the on the right y y axis and in the other cases what is improving",
    "start": "1196080",
    "end": "1202000"
  },
  {
    "text": "is the scheduling rate so the scheduling rate is going from 135 for the simple deployment up to more than 700 with the",
    "start": "1202000",
    "end": "1209520"
  },
  {
    "text": "using the the multiple demon sets and it's also interesting to look at the at this this pattern that sometimes happens",
    "start": "1209520",
    "end": "1216039"
  },
  {
    "text": "with the with the scheduling rate for the for the topological spread this goes a little bit up and down for the simple",
    "start": "1216039",
    "end": "1221960"
  },
  {
    "text": "deployment it's a little bit constant decreasing towards the last five 50 seconds while for the demon sets is more",
    "start": "1221960",
    "end": "1228159"
  },
  {
    "text": "or less Conant and again what we can extract more from this",
    "start": "1228159",
    "end": "1234360"
  },
  {
    "text": "data so it's clear it's clear that there is some kind of relationship of scaling low correlating the time needed to start",
    "start": "1234360",
    "end": "1241480"
  },
  {
    "text": "the to start the pods with the number of working nodes and the number of PODS per working nodes but can we for instance",
    "start": "1241480",
    "end": "1248240"
  },
  {
    "text": "try to see if there is a more generic more General scaling law which correlates just the number of the time",
    "start": "1248240",
    "end": "1254760"
  },
  {
    "text": "with the total number of PS so we convolute the the the the the number of working nodes with the number of PODS",
    "start": "1254760",
    "end": "1260559"
  },
  {
    "text": "per node and the result of this is on the two plots that you see and actually yes I would say that yes there is a",
    "start": "1260559",
    "end": "1266280"
  },
  {
    "text": "there is a nice linear scaling correlating the total time with just the total time of of the total number of PS",
    "start": "1266280",
    "end": "1272080"
  },
  {
    "text": "that we are going to start and the two plots that you see are actually the same they just use different colors on the",
    "start": "1272080",
    "end": "1277200"
  },
  {
    "text": "left to to group The the data referring to the same number of working nodes and on the right to group the data the",
    "start": "1277200",
    "end": "1284520"
  },
  {
    "text": "referring to same number of PODS per work per worker nodes and if we use this",
    "start": "1284520",
    "end": "1289720"
  },
  {
    "text": "uh if we assume that this linear correlation is is correct then we can use all the data that we took to to",
    "start": "1289720",
    "end": "1295240"
  },
  {
    "text": "calculate the the to to compute the average both startup rate so just fitting with a straight line the data",
    "start": "1295240",
    "end": "1302360"
  },
  {
    "text": "and you can see clearly that the most demanding uh the most demanding deployment of the topological spread is",
    "start": "1302360",
    "end": "1308279"
  },
  {
    "text": "around 13 33 HZ and we reach almost 700 with the with the with the demon sets so",
    "start": "1308279",
    "end": "1315480"
  },
  {
    "text": "again summarizing what we have learned from this let's l number three scheduling strategies do impact the Pod",
    "start": "1315480",
    "end": "1321640"
  },
  {
    "text": "startup times in a way and lesson number four Global pod startup times seem to",
    "start": "1321640",
    "end": "1327120"
  },
  {
    "text": "depend on the total number of PODS only it's not the end of the story as I",
    "start": "1327120",
    "end": "1333080"
  },
  {
    "text": "said before the other thing that we tried let's try to we tried was to configure the schedule in such a way",
    "start": "1333080",
    "end": "1338360"
  },
  {
    "text": "that we can improve the scheduling throughput what we did we defined a custom profile that we call no scoring",
    "start": "1338360",
    "end": "1344520"
  },
  {
    "text": "schul because as the word say we were just disabling all the plugins in the Press score and score phase and we also",
    "start": "1344520",
    "end": "1351279"
  },
  {
    "text": "say okay let's try to tune the percentage of notes to score and we set this percentage to five and let's see",
    "start": "1351279",
    "end": "1357400"
  },
  {
    "text": "what happens so the plot on the left uh is H the plot that was described before so with this uh scaling low that we",
    "start": "1357400",
    "end": "1364720"
  },
  {
    "text": "found so the the time as a function of the total number of PODS to be started",
    "start": "1364720",
    "end": "1369799"
  },
  {
    "text": "started into the cluster and the three data sets refer to the to the standard",
    "start": "1369799",
    "end": "1374840"
  },
  {
    "text": "situation okay to the theault scheduler as it comes then we have the default scheduler plus",
    "start": "1374840",
    "end": "1381200"
  },
  {
    "text": "the 5% scoring percentage of not to SC to to score and then at the bottom the",
    "start": "1381200",
    "end": "1386760"
  },
  {
    "text": "best results that we have is using a custom sched profile plus the 5% scoring",
    "start": "1386760",
    "end": "1392600"
  },
  {
    "text": "and the data in this case in this plot refer just to to give you an example to the to the simple deployment with the",
    "start": "1392600",
    "end": "1398200"
  },
  {
    "text": "OST Network and the tables give give uh give you the result for all for all our",
    "start": "1398200",
    "end": "1404480"
  },
  {
    "text": "for all our cases so clearly there is a a nice Improvement in in the time to start 26,000 PS across the cluster and",
    "start": "1404480",
    "end": "1412760"
  },
  {
    "text": "now we go from 122 for the topological spread down to again 40 44 for the",
    "start": "1412760",
    "end": "1418919"
  },
  {
    "text": "multiple demon sets as you can see there is no impact on the demon set case and this was somehow expected and the the",
    "start": "1418919",
    "end": "1425640"
  },
  {
    "text": "Improvement is even clearer if we look at the average board startup rate and the on the table on the at the bottom so",
    "start": "1425640",
    "end": "1432880"
  },
  {
    "text": "we we we had a nice increase of about 70% for the topological for the in case",
    "start": "1432880",
    "end": "1438480"
  },
  {
    "text": "where the topological spread was used and for the standard deployment with just resource resource description we",
    "start": "1438480",
    "end": "1443600"
  },
  {
    "text": "had a wonderful increase in startup rate of of",
    "start": "1443600",
    "end": "1449480"
  },
  {
    "text": "280% I didn't say anything I didn't mention up to now anything about the hardware that we were using for the",
    "start": "1450600",
    "end": "1456240"
  },
  {
    "text": "contol plane because obviously we can say okay you were using some very old PC and you get some crappy results and this",
    "start": "1456240",
    "end": "1462400"
  },
  {
    "text": "is okay we all the results that have shown before was with some kind of modest Hardware yes but that gave us the",
    "start": "1462400",
    "end": "1468840"
  },
  {
    "text": "the possibility to also study how this how performance changes changing changing the hard generation so we were",
    "start": "1468840",
    "end": "1475159"
  },
  {
    "text": "running up to now all the results that have shown with the four servers for the control plane using a kind of old CPUs",
    "start": "1475159",
    "end": "1482360"
  },
  {
    "text": "then recently we increased the power of our our contol plane always four nodes",
    "start": "1482360",
    "end": "1487880"
  },
  {
    "text": "and again dual sockets but this time with a much modern uh CPU and with 10 gigabit technology we didn't touch the",
    "start": "1487880",
    "end": "1494320"
  },
  {
    "text": "tcd cluster because the tcd cluster was already let's say state of the art with the very fast and be discs and all and",
    "start": "1494320",
    "end": "1501720"
  },
  {
    "text": "all the stuff and the plot here in the middle of the slide shows the refers to",
    "start": "1501720",
    "end": "1507039"
  },
  {
    "text": "a topological to a deployment with a topological spread in the best situation so we use already the custom profile for",
    "start": "1507039",
    "end": "1512919"
  },
  {
    "text": "the schedul we use the 5% of scoring and you can see how there is how the new",
    "start": "1512919",
    "end": "1518000"
  },
  {
    "text": "hardw actually had an impact so the time to start all the pods again 26,000 PS went down considerably while the two",
    "start": "1518000",
    "end": "1525399"
  },
  {
    "text": "little plots on the right refer to the other two configuration where the gain there is still some gain but not as much",
    "start": "1525399",
    "end": "1530600"
  },
  {
    "text": "as big as in the case of the topological spread and the table shows actually put puts in Number the increase that we had",
    "start": "1530600",
    "end": "1538039"
  },
  {
    "text": "the Improvement so you can see that for the deployment using topological spread we had an increase of the 50% so we were",
    "start": "1538039",
    "end": "1545159"
  },
  {
    "text": "reaching 340 Hertz and for the deployment for the multiple demon sets",
    "start": "1545159",
    "end": "1550799"
  },
  {
    "text": "we had a more modest increase of performance of about 16% but still so we",
    "start": "1550799",
    "end": "1556159"
  },
  {
    "text": "now we were over 700 codes per second which is much better than what we were",
    "start": "1556159",
    "end": "1561480"
  },
  {
    "text": "observing in the past but again so we increase the power of our control plane",
    "start": "1561480",
    "end": "1567039"
  },
  {
    "text": "what is actually changing and as before again the profiling for all the faces and simple",
    "start": "1567039",
    "end": "1575760"
  },
  {
    "text": "case of the deployment in our best case scenario the custom schedul profile plus the 5% scoring so if you look at the",
    "start": "1575760",
    "end": "1582480"
  },
  {
    "text": "creation time for the new hardware and the old Hardware this is exactly the same so the creation time did not change",
    "start": "1582480",
    "end": "1587960"
  },
  {
    "text": "at all so this means that the new hardware the much more powerful CPU that we were using did not increase the",
    "start": "1587960",
    "end": "1593760"
  },
  {
    "text": "performance of the controller manager we also tried that this time since we were having a lot of good CPU power to",
    "start": "1593760",
    "end": "1599880"
  },
  {
    "text": "increase even fter the QPS of the controller manager sorry we pushed up",
    "start": "1599880",
    "end": "1605279"
  },
  {
    "text": "to, 1600 from 800 but nothing changed at the same time again we observe some nice",
    "start": "1605279",
    "end": "1611799"
  },
  {
    "text": "improvements in the in the running phase to reach the running phase to reach the watching phase which is a hint that the",
    "start": "1611799",
    "end": "1617520"
  },
  {
    "text": "scheduling was actually improving so again the scheduling was uh was uh the scheder was able to profit much more of",
    "start": "1617520",
    "end": "1624120"
  },
  {
    "text": "the new hardware and this also explains why we were observing some much better increase in performance",
    "start": "1624120",
    "end": "1631159"
  },
  {
    "text": "in the most demanding case for the scheduling which is when we use the the the topological spread another thing",
    "start": "1631159",
    "end": "1638559"
  },
  {
    "text": "that I I mentioned before we were using OST Network for all our tests but what about the cn9 so first let me say why we",
    "start": "1638559",
    "end": "1645840"
  },
  {
    "text": "were using the the OST Network we were using Network because the interprocess communication system that our applications are using currently it's it",
    "start": "1645840",
    "end": "1653880"
  },
  {
    "text": "works much better with the host network not using the cni at the same time we want to explore we want to be ready for",
    "start": "1653880",
    "end": "1659679"
  },
  {
    "text": "everything okay whatever whatever the final applications will be that are still under development we want to be",
    "start": "1659679",
    "end": "1664840"
  },
  {
    "text": "sure that we can cover all all the cases so that's why we performed all the tests that I've shown before even using asni",
    "start": "1664840",
    "end": "1671720"
  },
  {
    "text": "in this case in this case helium in version 11 141 with the with the default configuration basic so we didn't we",
    "start": "1671720",
    "end": "1678519"
  },
  {
    "text": "didn't dig deep into the into the celum case and the plot on the left shows the",
    "start": "1678519",
    "end": "1684559"
  },
  {
    "text": "Pod startup time for both the new and the old Hardware with both the OST",
    "start": "1684559",
    "end": "1689600"
  },
  {
    "text": "Network and and and the case the cni while on the right you have basically the same plot but just for the Stop time",
    "start": "1689600",
    "end": "1696240"
  },
  {
    "text": "I didn't talk too much actually with I didn't I didn't show too much about the stop time because it's a much easier",
    "start": "1696240",
    "end": "1701279"
  },
  {
    "text": "case so it just depends on the on the controller manager and the stop times were much more predictable so that's why",
    "start": "1701279",
    "end": "1706880"
  },
  {
    "text": "I didn't go deep into that and what we again what we can learn from these two plots so General observation",
    "start": "1706880",
    "end": "1713880"
  },
  {
    "text": "it looks like startup and stop times are higher than when the cni is used with",
    "start": "1713880",
    "end": "1719399"
  },
  {
    "text": "respect to the case where the host network is used at the same time as soon as we improved our Hardware this",
    "start": "1719399",
    "end": "1725880"
  },
  {
    "text": "difference actually went down so this seems related to the capability of the nodes but again why this and we again we",
    "start": "1725880",
    "end": "1734600"
  },
  {
    "text": "look at the profiling of the number of pods in different faces so on the left these two plots are actually are",
    "start": "1734600",
    "end": "1741159"
  },
  {
    "text": "actually the same the only difference is one the one on the left refers to the holdo hardware and the one on the right",
    "start": "1741159",
    "end": "1746480"
  },
  {
    "text": "refers to the to the new hardware and on the left you can see that the profile of the controller manag is not really a",
    "start": "1746480",
    "end": "1751600"
  },
  {
    "text": "straight line as we were used to see so maybe this was the first evidence the first case where the controller manager",
    "start": "1751600",
    "end": "1757240"
  },
  {
    "text": "was a little bit under stress and as soon as we in we we had a much more powerful Hardware the profile again",
    "start": "1757240",
    "end": "1764360"
  },
  {
    "text": "start to be a very nice straight line the total time to create the the pods went down and yeah so again in this case",
    "start": "1764360",
    "end": "1772960"
  },
  {
    "text": "this the in this specific case when the cni is used the controller manager was actually able to profit of the higher",
    "start": "1772960",
    "end": "1780399"
  },
  {
    "text": "power of the control of the control play nodes so what are the lessons that we learned from these last test so lesson",
    "start": "1780399",
    "end": "1787880"
  },
  {
    "text": "number five custom schedu profiles May greatly increase the its throughput so",
    "start": "1787880",
    "end": "1793320"
  },
  {
    "text": "we can increase the throughput of the schedular using some custom profiles SCH uh lesson number six better hardware for",
    "start": "1793320",
    "end": "1799760"
  },
  {
    "text": "the control plane seems to improve the more scheduling demanding scenarios and",
    "start": "1799760",
    "end": "1805080"
  },
  {
    "text": "we are actually at the last phase of our journey the reward and what was what is",
    "start": "1805080",
    "end": "1811799"
  },
  {
    "text": "our reward so if we go back in time at the beginning of this talk I say we started in 2018 and we were able to",
    "start": "1811799",
    "end": "1818880"
  },
  {
    "text": "start just 20 pods per seconds today in 2024 for our use case for our",
    "start": "1818880",
    "end": "1824679"
  },
  {
    "text": "configuration we are able to go even a little bit beyond 700 pods per second",
    "start": "1824679",
    "end": "1830120"
  },
  {
    "text": "and now we were able to achieve that with QPS tuning with with custom schedular profiles with different",
    "start": "1830120",
    "end": "1837679"
  },
  {
    "text": "deployment and scheduling strategies reducing the number of nodes to be scored and last but not least also",
    "start": "1837679",
    "end": "1843679"
  },
  {
    "text": "improving the hardware that we were using for the control play it's not the end of the story so this is the end of",
    "start": "1843679",
    "end": "1849039"
  },
  {
    "text": "this journey several other Journeys I think are coming on but today what we can say today we can say that we gained",
    "start": "1849039",
    "end": "1855320"
  },
  {
    "text": "a lot of experience in operating a large kubernetes cluster we improved greatly",
    "start": "1855320",
    "end": "1860639"
  },
  {
    "text": "the performance in terms of pod startup times we profited a lot of the very flexible way kubernetes offer us offers",
    "start": "1860639",
    "end": "1868279"
  },
  {
    "text": "us to schedule our pods and we also profit of enhan monitoring and operability of of",
    "start": "1868279",
    "end": "1875000"
  },
  {
    "text": "kubernetes last but not least very important all the results across all these years across all the kubernetes",
    "start": "1875000",
    "end": "1881399"
  },
  {
    "text": "releases very very predictable that in our case is very very important what is coming tomorrow well we want to SC scale",
    "start": "1881399",
    "end": "1888279"
  },
  {
    "text": "even to more pods as I said at the beginning we in the final system we may need to start something like 65,000 pods",
    "start": "1888279",
    "end": "1896039"
  },
  {
    "text": "and up to now we tested our system up to 26,000 we want to evaluate not extended",
    "start": "1896039",
    "end": "1901559"
  },
  {
    "text": "resources for simplified scheduling and we also in case the startup times get",
    "start": "1901559",
    "end": "1907039"
  },
  {
    "text": "too high I think that's some moment we should evaluate whether we want to run more containers per POD at the same time",
    "start": "1907039",
    "end": "1914720"
  },
  {
    "text": "we lik a lot what kubernetes offers and the way it works and now we want to try to exploit kubernetes also for a",
    "start": "1914720",
    "end": "1921320"
  },
  {
    "text": "different workload so I said we want to run our online filtering applications",
    "start": "1921320",
    "end": "1926600"
  },
  {
    "text": "but what about having a dynamic mix of both online and offline simulation jobs",
    "start": "1926600",
    "end": "1933399"
  },
  {
    "text": "in our in our farm why not and what about about kwalk so the as as said our",
    "start": "1933399",
    "end": "1941120"
  },
  {
    "text": "farm is not available all the time well it's available for a very short time for doing our tests and in a little bit more",
    "start": "1941120",
    "end": "1947559"
  },
  {
    "text": "than one year there will be no farm at all because the experiment will stop the accelerator will stop for the upgrades",
    "start": "1947559",
    "end": "1954240"
  },
  {
    "text": "so using something like that allows us to simulate the schedular behavior will",
    "start": "1954240",
    "end": "1959559"
  },
  {
    "text": "be actually pretty pretty useful and that's the end of the Journey of the",
    "start": "1959559",
    "end": "1964720"
  },
  {
    "text": "story so thanks for listening and if you have any feedback do not hesitate",
    "start": "1964720",
    "end": "1970450"
  },
  {
    "text": "[Applause]",
    "start": "1970450",
    "end": "1979349"
  },
  {
    "text": "okay hello uh I'd like to ask a question sure um specifically so why is it so",
    "start": "1988159",
    "end": "1994919"
  },
  {
    "text": "important the startup time of the pods so because like I just want to understand the life cycle of the",
    "start": "1994919",
    "end": "2001000"
  },
  {
    "text": "processes that you have do you want them like to start up finish certain given task and exit immediately or can they",
    "start": "2001000",
    "end": "2007200"
  },
  {
    "text": "leave like through it for a longer time uh so the question is about why why we are so sensible so sensitive to the PO",
    "start": "2007200",
    "end": "2014200"
  },
  {
    "text": "startup Times Yes uh the fact is that as I was saying at the beginning the um the",
    "start": "2014200",
    "end": "2019840"
  },
  {
    "text": "LHC has some Cycles okay it's not a start and forget so it's not we are going to start our processes and we stay",
    "start": "2019840",
    "end": "2025760"
  },
  {
    "text": "there for one month two months uh it may happen that usually a data taking",
    "start": "2025760",
    "end": "2031000"
  },
  {
    "text": "session is between let's say 12 12 hours 24 hours then the data the data taking",
    "start": "2031000",
    "end": "2036559"
  },
  {
    "text": "session stops cause LC as to refill so we need to inject again protons into the",
    "start": "2036559",
    "end": "2041720"
  },
  {
    "text": "accelerator as soon as the LHC is ready we need to restart basically all the infrastructure all the applications even",
    "start": "2041720",
    "end": "2048398"
  },
  {
    "text": "why from one data taking session to another one the configuration of these applications may be completely different",
    "start": "2048399",
    "end": "2054240"
  },
  {
    "text": "so they do not let's say survive because they need to be probably reconfigured and we cannot afford to take I don't",
    "start": "2054240",
    "end": "2060398"
  },
  {
    "text": "know 10 minutes to do that if there is also another use case something may go wrong during the run there may be a bug",
    "start": "2060399",
    "end": "2066960"
  },
  {
    "text": "unexpected in the in the in the pods in the application that we are going to run and we may need to restart all of them",
    "start": "2066960",
    "end": "2073520"
  },
  {
    "text": "and again we need to be fast otherwise we lose beam because LC will not stop just because we have troubles with our",
    "start": "2073520",
    "end": "2079358"
  },
  {
    "text": "applications I hope this answer to your question yes this does and then just the technical one so based on your final",
    "start": "2079359",
    "end": "2086280"
  },
  {
    "text": "findings I assume that you are going with like deploy resource based deployments as your main schingo not the",
    "start": "2086280",
    "end": "2092960"
  },
  {
    "text": "demon sets probably not well the last res may be the the demon set",
    "start": "2092960",
    "end": "2098480"
  },
  {
    "text": "and actually we are planning to to do all this deployment with an operator because we need to link the rest of the",
    "start": "2098480",
    "end": "2105119"
  },
  {
    "text": "system with the kubernetes cluster and this operator application will actually breed a bridge and at that point since",
    "start": "2105119",
    "end": "2111079"
  },
  {
    "text": "it will not be anywhere a human operation we are free to use the even the most complicated scheduling scenario",
    "start": "2111079",
    "end": "2117240"
  },
  {
    "text": "that's that's also why I say that we want to try to evaluate these no extended resources because then I don't",
    "start": "2117240",
    "end": "2122800"
  },
  {
    "text": "know we may use a kind of how many applications of this kind we want to start of this node and instead of",
    "start": "2122800",
    "end": "2128280"
  },
  {
    "text": "playing with the memory and CPU that is a little bit tricky we may just use this extended resource but we have to try",
    "start": "2128280",
    "end": "2134160"
  },
  {
    "text": "first okay thank you welcome uh hi uh great talk by the way I",
    "start": "2134160",
    "end": "2141079"
  },
  {
    "text": "think this was this was a really good lesson on how one should do p test so uh not just the configurations but we also",
    "start": "2141079",
    "end": "2146800"
  },
  {
    "text": "sorry can you can you speak louder because I uh I was going this was a really good uh demonstration of how one should run performance tests when you're",
    "start": "2146800",
    "end": "2153280"
  },
  {
    "text": "trying to Benchmark something and the way you ploted the results of kuros on that um couple quick questions so uh one",
    "start": "2153280",
    "end": "2159319"
  },
  {
    "text": "how are you measuring part startup time exactly in this case I miss that yes we we are measuring this this is something",
    "start": "2159319",
    "end": "2164720"
  },
  {
    "text": "that comes with the with the cluster loader too as I as I said we just added a few print out and because cluster",
    "start": "2164720",
    "end": "2171760"
  },
  {
    "text": "loader is able to track the single faces for each single pod and we actually are extracting this time from the result of",
    "start": "2171760",
    "end": "2178920"
  },
  {
    "text": "the cluster loader to I see so you're basically paring all those logs to see how Okay uh and the second question I",
    "start": "2178920",
    "end": "2184280"
  },
  {
    "text": "had was around uh the final configuration you had on scheduler specifically so are you only ranking",
    "start": "2184280",
    "end": "2190280"
  },
  {
    "text": "like 5% of notes to achieve 700 parts per second right now yes I see got it and you said the QPS to like 800 for on",
    "start": "2190280",
    "end": "2197400"
  },
  {
    "text": "both because because in our specific case since we want to fill anyway the the the cluster as much as possible we",
    "start": "2197400",
    "end": "2203400"
  },
  {
    "text": "don't need to score because anyway the pods have to go everywhere so for you packing matters less you just want it to",
    "start": "2203400",
    "end": "2208560"
  },
  {
    "text": "be scheduled as fast as possible exactly correct thank you so much thank you we are over time we have time just",
    "start": "2208560",
    "end": "2216079"
  },
  {
    "text": "for this question for the last one uh yeah uh thank you for the great talk um I'm curious if you like if you noticed",
    "start": "2216079",
    "end": "2224480"
  },
  {
    "text": "anything related to the time it takes to like volume attach for the images that you're going to run as part uh no we are",
    "start": "2224480",
    "end": "2231119"
  },
  {
    "text": "not as I say we were just preloading everything so this is something that will come at the moment we don't even",
    "start": "2231119",
    "end": "2237359"
  },
  {
    "text": "have the final version of the applications that we are going to run so we don't even know how big this images will be and in these studies we just",
    "start": "2237359",
    "end": "2244200"
  },
  {
    "text": "wanted to evaluate the Pure Performance of the schedule all the rest so we were not taking into",
    "start": "2244200",
    "end": "2250119"
  },
  {
    "text": "account the time needed to pull the images got it got it thanks okay thank you",
    "start": "2250119",
    "end": "2257560"
  }
]