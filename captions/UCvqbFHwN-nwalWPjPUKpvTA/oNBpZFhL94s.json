[
  {
    "text": "thank you for joining us so my name is Zack Hassan and I'm here with Diane",
    "start": "60",
    "end": "6150"
  },
  {
    "text": "Fatima we work at Red Hat in the Emergency Technology Group on data analytics I've been at Red Hat for the",
    "start": "6150",
    "end": "13500"
  },
  {
    "text": "past four years our project is rad analytics which is a platform where",
    "start": "13500",
    "end": "19380"
  },
  {
    "text": "developers build data science applications on our kubernetes offering which is OpenShift",
    "start": "19380",
    "end": "25140"
  },
  {
    "text": "along with building the platform I also develop intelligent applications do things like image recognition and other",
    "start": "25140",
    "end": "32398"
  },
  {
    "text": "machine learning AI type of workloads and I'm Diane Fatima I've been doing big",
    "start": "32399",
    "end": "38520"
  },
  {
    "text": "data work for about the last five years at Red Hat I started out working with Hadoop initially for a couple years and",
    "start": "38520",
    "end": "44789"
  },
  {
    "text": "I've moved into using spark and I primarily analyze performance data",
    "start": "44789",
    "end": "50760"
  },
  {
    "text": "I run experiments collect that data and then use the machine learning algorithms in spark to analyze them so today we're",
    "start": "50760",
    "end": "61379"
  },
  {
    "text": "gonna spend the next half an hour to talk about the story of how we set up",
    "start": "61379",
    "end": "67290"
  },
  {
    "text": "Prometheus the monitors our spark cluster and run and our running spark",
    "start": "67290",
    "end": "72420"
  },
  {
    "text": "jobs live in kubernetes pods will dive into high-level concepts we'll look at",
    "start": "72420",
    "end": "79770"
  },
  {
    "text": "code will actually show you a live demo with Griffin prometheus all running in",
    "start": "79770",
    "end": "86840"
  },
  {
    "text": "kubernetes are open shipped offering so",
    "start": "86840",
    "end": "93590"
  },
  {
    "text": "we're also gonna do some slight code changes and we'll show you performance improvements of code and then we'll run",
    "start": "93590",
    "end": "100950"
  },
  {
    "text": "a job with the the not being tuned and then we'll run a job that's tuned and we'll show you some performance",
    "start": "100950",
    "end": "106890"
  },
  {
    "text": "improvements and quicker runtime so our",
    "start": "106890",
    "end": "112380"
  },
  {
    "text": "goal for building the the platform was actually when we built the platform our goal was to try to improve the runtime",
    "start": "112380",
    "end": "119820"
  },
  {
    "text": "of applications to allow data scientists to do more rapid experimentation to",
    "start": "119820",
    "end": "126390"
  },
  {
    "text": "provide you know if you prove if you write a program and you get quick feedback rapid feedback then you",
    "start": "126390",
    "end": "133380"
  },
  {
    "text": "do more experiments and also when you're doing integration testing you can do",
    "start": "133380",
    "end": "138480"
  },
  {
    "text": "more ambitious testing when you know exactly how much memory is CPU and whatever resources that are being",
    "start": "138480",
    "end": "143910"
  },
  {
    "text": "consumed in kubernetes and one of the compelling reasons for even using spark",
    "start": "143910",
    "end": "150090"
  },
  {
    "text": "for developers is actually because of its parallel processing it's in memory pipelining and data intent optimization",
    "start": "150090",
    "end": "158360"
  },
  {
    "text": "so our story basically this whole experiment I started looking at this",
    "start": "158360",
    "end": "167130"
  },
  {
    "text": "experiment last year like around mid June I looked at since we were running",
    "start": "167130",
    "end": "172800"
  },
  {
    "text": "spark in containers I looked at what if we put an agent in there and then expose the metrics and then at first we were",
    "start": "172800",
    "end": "181230"
  },
  {
    "text": "using Jolokia and then later we decided to use Prometheus endpoints later as the",
    "start": "181230",
    "end": "188340"
  },
  {
    "text": "experiment continued we we created a CRD custom resource definition with a spark",
    "start": "188340",
    "end": "193590"
  },
  {
    "text": "operator that's able to deploy spark cluster discover the the the workers in",
    "start": "193590",
    "end": "199890"
  },
  {
    "text": "the panda master and then dynamically configure the targets to be consumed by",
    "start": "199890",
    "end": "207090"
  },
  {
    "text": "Prometheus we set up alert manager and you know and and you know what that",
    "start": "207090",
    "end": "213990"
  },
  {
    "text": "wasn't enough actually we we try to even do further optimization where I got in",
    "start": "213990",
    "end": "219480"
  },
  {
    "text": "contact with Diane and we set up Diane help actually set up the CRO final integration with Prometheus and we're",
    "start": "219480",
    "end": "226860"
  },
  {
    "text": "off to experiment on doing actual spark jobs so this is not going to be an",
    "start": "226860",
    "end": "233850"
  },
  {
    "text": "introduction to Prometheus this is going to be more of actually using Prometheus or if this is the if that's a talk this",
    "start": "233850",
    "end": "240690"
  },
  {
    "text": "is the talk for you so I'm gonna briefly",
    "start": "240690",
    "end": "246270"
  },
  {
    "text": "go over what prometheus is who does not",
    "start": "246270",
    "end": "251340"
  },
  {
    "text": "know Prometheus okay so Prometheus is a",
    "start": "251340",
    "end": "257280"
  },
  {
    "text": "modern solution and it is a times is it's a time series database so it",
    "start": "257280",
    "end": "262979"
  },
  {
    "text": "collects the metrics running in your application and you could either instrument your",
    "start": "262979",
    "end": "271440"
  },
  {
    "text": "application or you can use an exporter that would export out like if you have",
    "start": "271440",
    "end": "279509"
  },
  {
    "text": "JMX metrics being exposed then you can use JMX exporter or there's different",
    "start": "279509",
    "end": "285990"
  },
  {
    "text": "exporters for different tools in your infrastructure luckily if you're using kubernetes that comes out of the box",
    "start": "285990",
    "end": "293300"
  },
  {
    "text": "already pre instrumented so what's SPARC",
    "start": "293300",
    "end": "301590"
  },
  {
    "text": "anyway so SPARC is actually a data processing engine it's parallel",
    "start": "301590",
    "end": "309000"
  },
  {
    "text": "processing it lets you spread your data across many machines you can do in memory processing and it has",
    "start": "309000",
    "end": "315389"
  },
  {
    "text": "optimizations to form queries machine learning stream processing workloads and more you can choose to run on yarn Mei",
    "start": "315389",
    "end": "323070"
  },
  {
    "text": "sauce or standalone so for our setup we're using stand-alone there's actually some interesting work that's happening",
    "start": "323070",
    "end": "329639"
  },
  {
    "text": "at Apache foundation our group and other companies actually are working together",
    "start": "329639",
    "end": "335159"
  },
  {
    "text": "on using kubernetes as the cluster manager talk to me offline if you want",
    "start": "335159",
    "end": "341400"
  },
  {
    "text": "to know a little bit more about that so this is actually the building blocks of",
    "start": "341400",
    "end": "347610"
  },
  {
    "text": "SPARC itself so you have the core spark core then you have different api's and",
    "start": "347610",
    "end": "353039"
  },
  {
    "text": "libraries that you would use for whatever particular problem you were trying to solve right so you can use",
    "start": "353039",
    "end": "359639"
  },
  {
    "text": "sequel if you want to query the data spread across your cluster you can use a machine learning you could use graph you",
    "start": "359639",
    "end": "367409"
  },
  {
    "text": "can use streaming if you have data that's being streamed into a Kafka topic then that's that's one option as well",
    "start": "367409",
    "end": "375169"
  },
  {
    "text": "and you can actually access your data from different data sources so you can access from HDFS Amazon s3 SEF s3 if you",
    "start": "375169",
    "end": "383550"
  },
  {
    "text": "want and you can write jobs in Java Python and Scala I think some people",
    "start": "383550",
    "end": "391979"
  },
  {
    "text": "also use our how many people here use our okay",
    "start": "391979",
    "end": "398839"
  },
  {
    "text": "so rdd's is one concept to understand because that's the foundational thing",
    "start": "399310",
    "end": "405250"
  },
  {
    "text": "with it with inspark so rdd's are resilient distributed datasets and",
    "start": "405250",
    "end": "413230"
  },
  {
    "text": "basically your data is split into partitions and it's spread across a cluster and then you it's it's like",
    "start": "413230",
    "end": "420340"
  },
  {
    "text": "basically you have a collection of rows that sit on a physical physical machines and you have this this concept of",
    "start": "420340",
    "end": "427120"
  },
  {
    "text": "executors that execute your work and these executors are actually in the",
    "start": "427120",
    "end": "432910"
  },
  {
    "text": "spark worker so you might think okay why",
    "start": "432910",
    "end": "439270"
  },
  {
    "text": "do I even need partitions interesting thing is you need partitions because",
    "start": "439270",
    "end": "445330"
  },
  {
    "text": "when you execute work if you have only one partition and you have a thousand executors your parallelism is only one",
    "start": "445330",
    "end": "451770"
  },
  {
    "text": "if you have a thousand partitions but only one executors same thing parallelism is only one so you want to",
    "start": "451770",
    "end": "458229"
  },
  {
    "text": "have more partitions and more executors and we're gonna go into a high level of",
    "start": "458229",
    "end": "465040"
  },
  {
    "text": "what a spark cluster is made of so spark",
    "start": "465040",
    "end": "471160"
  },
  {
    "text": "a plication as I mentioned before a spark application is basically you get",
    "start": "471160",
    "end": "477310"
  },
  {
    "text": "data from a source right let's say HDFS or let's say s3 and then you have a",
    "start": "477310",
    "end": "485260"
  },
  {
    "text": "spark job you write a job so you have to decide if your job is gonna be streaming or if it's gonna be a job that's that's",
    "start": "485260",
    "end": "492580"
  },
  {
    "text": "just a batch job either option you could you could use the libraries that that",
    "start": "492580",
    "end": "499020"
  },
  {
    "text": "that come with spark and then you want to store your data somewhere if you want",
    "start": "499020",
    "end": "504940"
  },
  {
    "text": "to train a model you might want to store that model somewhere and one thing to",
    "start": "504940",
    "end": "510419"
  },
  {
    "text": "that I mentioned earlier remember that point when I said Y spark is popular it's because of its",
    "start": "510419",
    "end": "517210"
  },
  {
    "text": "parallelism right it's lazy by default",
    "start": "517210",
    "end": "522419"
  },
  {
    "text": "its core data structure is immutable let me actually explain that so what it does",
    "start": "522419",
    "end": "529660"
  },
  {
    "text": "is it performs a transfer but only until the very end when an action is actually called on it so",
    "start": "529660",
    "end": "536750"
  },
  {
    "text": "internally spark builds up a dag and it makes a plan for the transformation and",
    "start": "536750",
    "end": "542300"
  },
  {
    "text": "then in a polite applies that transformation to the source data and generates a result so under the hood it",
    "start": "542300",
    "end": "550310"
  },
  {
    "text": "actually uses rdd's remember that term that I mentioned resilient distributed data set but what you can do this",
    "start": "550310",
    "end": "558110"
  },
  {
    "text": "library on top of that called data frames that makes it super like a little bit more simple to use we'll talk about",
    "start": "558110",
    "end": "565610"
  },
  {
    "text": "that more so this is our setup within kubernetes we have positive familiar",
    "start": "565610",
    "end": "572270"
  },
  {
    "text": "with we have containers that are running inside those pots and these containers",
    "start": "572270",
    "end": "578350"
  },
  {
    "text": "have a spark master a worker and you can",
    "start": "578350",
    "end": "583460"
  },
  {
    "text": "have more than one worker and then the application that's running it's running",
    "start": "583460",
    "end": "588470"
  },
  {
    "text": "as a driver application so I want you to remember that term the driver the driver",
    "start": "588470",
    "end": "594500"
  },
  {
    "text": "is basically running actually doing the job right so we use source to image to",
    "start": "594500",
    "end": "601040"
  },
  {
    "text": "when you have good source code you point to the gate source code we we pull down",
    "start": "601040",
    "end": "606770"
  },
  {
    "text": "the image it builds an image and then it deploys a job you're gonna see all that stuff in the live demo so some",
    "start": "606770",
    "end": "614930"
  },
  {
    "text": "interesting metrics that we got back from the driver is things like block manager things like the tag scheduler",
    "start": "614930",
    "end": "623830"
  },
  {
    "text": "things like JVM and some of the internals of what's happening within the JVM inside the container that's running",
    "start": "623830",
    "end": "630770"
  },
  {
    "text": "and we found some interesting details so",
    "start": "630770",
    "end": "636530"
  },
  {
    "text": "this is just a high-level view of the the cluster so Prometheus is gonna go",
    "start": "636530",
    "end": "643310"
  },
  {
    "text": "and scrape these endpoints so we have Java agents running in the spark master",
    "start": "643310",
    "end": "650900"
  },
  {
    "text": "and worker and also on the actual driver application that's running it's all",
    "start": "650900",
    "end": "655910"
  },
  {
    "text": "running in kubernetes so we get information about the executor how much",
    "start": "655910",
    "end": "661790"
  },
  {
    "text": "memory is being used we can get more information about you know how much memory is actually being",
    "start": "661790",
    "end": "668950"
  },
  {
    "text": "used by the whole kubernetes cluster and other different details as well and then",
    "start": "668950",
    "end": "674260"
  },
  {
    "text": "but the thing is you said how often you want it to scrape so I'll explain that",
    "start": "674260",
    "end": "681370"
  },
  {
    "text": "part in a later slide so this is",
    "start": "681370",
    "end": "690220"
  },
  {
    "text": "basically just the when we're building docker image this is the Java agent who",
    "start": "690220",
    "end": "696100"
  },
  {
    "text": "is familiar with Java agents who has used Java agent before okay",
    "start": "696100",
    "end": "701910"
  },
  {
    "text": "so 10% of the room okay so Java agent",
    "start": "701910",
    "end": "706960"
  },
  {
    "text": "you pass a - Java agent at runtime when you're running the the worker or the",
    "start": "706960",
    "end": "712540"
  },
  {
    "text": "master and you can pass it a jar and that jar is gonna put for this",
    "start": "712540",
    "end": "718840"
  },
  {
    "text": "particular example that jar is gonna go and look for JMX metrics and expose it over rest in a format that permittees",
    "start": "718840",
    "end": "724420"
  },
  {
    "text": "understands so some prerequisites you have to have JMX enabled so this is a view of we can",
    "start": "724420",
    "end": "733750"
  },
  {
    "text": "see the network CPU memory of kubernetes we can see application metrics and we",
    "start": "733750",
    "end": "739210"
  },
  {
    "text": "actually interesting thing is we actually ran OpenShift ansible to set this up which is on github and then we",
    "start": "739210",
    "end": "750250"
  },
  {
    "text": "have to do a little bit more experimentation for the spark side and we'll talk about that more this is the",
    "start": "750250",
    "end": "759160"
  },
  {
    "text": "configurations for an example of configuring something to scrape an",
    "start": "759160",
    "end": "764500"
  },
  {
    "text": "endpoint in prometheus this is an example of a simple very very simple",
    "start": "764500",
    "end": "770370"
  },
  {
    "text": "alert rule you can set different rules and then you would use prompt QL so if",
    "start": "770370",
    "end": "776170"
  },
  {
    "text": "you see the up equal equals zero that means if any of these nodes that are being monitored if they go down I want",
    "start": "776170",
    "end": "782680"
  },
  {
    "text": "to get notified an email slack notification or whatnot from qo prompt Q",
    "start": "782680",
    "end": "791830"
  },
  {
    "text": "is what we use when we want to do visualizations and Dianne has some visualizations that she's going to cover",
    "start": "791830",
    "end": "798629"
  },
  {
    "text": "so just remember gauges are like when you're driving a car gauges tell you how",
    "start": "798629",
    "end": "804550"
  },
  {
    "text": "fast you're going right so that's a gauge Prometheus you have gauges and then there's something called counters",
    "start": "804550",
    "end": "810910"
  },
  {
    "text": "think of that as an odometer how many miles you've traveled so far so without",
    "start": "810910",
    "end": "818230"
  },
  {
    "text": "further ado I'm going to pass it on to my teammate Diane furthermore thanks",
    "start": "818230",
    "end": "824050"
  },
  {
    "text": "Zack okay so for the second part of the presentation we're going to talk about how Prometheus and Ravana can be used",
    "start": "824050",
    "end": "831309"
  },
  {
    "text": "when you're doing performance analysis which is what I do in my job so there are a number of things that you do get",
    "start": "831309",
    "end": "837550"
  },
  {
    "text": "with spark out of the box that tell you what's happening with your spark job as",
    "start": "837550",
    "end": "842649"
  },
  {
    "text": "it's running there are many important things that you want to know that the spark web UI does not tell you and",
    "start": "842649",
    "end": "849160"
  },
  {
    "text": "that's what we're gonna get from Prometheus and Griffon a-- really important things that help you not get",
    "start": "849160",
    "end": "855069"
  },
  {
    "text": "out of memory errors when you run your job it help you tune your job it just sort of turns the lights on and you",
    "start": "855069",
    "end": "860680"
  },
  {
    "text": "suddenly know all the their resources that are being used for your job so we",
    "start": "860680",
    "end": "866589"
  },
  {
    "text": "want to know things like how much memory is are my workers and my master using",
    "start": "866589",
    "end": "872189"
  },
  {
    "text": "how much memory is available on the entire system if someone else might be running at the same time as you is there",
    "start": "872189",
    "end": "879370"
  },
  {
    "text": "is the network being saturated by my job and what might I do to eliminate that bottleneck",
    "start": "879370",
    "end": "884649"
  },
  {
    "text": "what's the CPU usage on the container level in the pod level and on the node",
    "start": "884649",
    "end": "890110"
  },
  {
    "text": "level you can with Prometheus you can look at a Pernod level of four all these metrics you can also look at a per",
    "start": "890110",
    "end": "895689"
  },
  {
    "text": "application level for all of these metrics so we as Zach said added these",
    "start": "895689",
    "end": "904410"
  },
  {
    "text": "endpoints in spark to scrape and we're also scraping for career Nettie's to get",
    "start": "904410",
    "end": "910389"
  },
  {
    "text": "all this information to help us do our job and there are just hundreds of metrics that are being saved in this",
    "start": "910389",
    "end": "917470"
  },
  {
    "text": "time series manner and we can't really go into all of them in this format in this presentation so we are going to",
    "start": "917470",
    "end": "924360"
  },
  {
    "text": "concentrate on memory usage and the duration of the job so we're going to",
    "start": "924360",
    "end": "929500"
  },
  {
    "text": "just show some example code we're going do some tuning and we're going to improve the memory usage and the timing",
    "start": "929500",
    "end": "936529"
  },
  {
    "text": "for the job so efficient memory use is",
    "start": "936529",
    "end": "944180"
  },
  {
    "text": "important for SPARC but it's also important for any data intensive system so impossible we want to keep us met as",
    "start": "944180",
    "end": "951800"
  },
  {
    "text": "much information as we can near the processors and we don't want to spill to disk we want to avoid spilling to disk",
    "start": "951800",
    "end": "958420"
  },
  {
    "text": "with SPARC jobs whenever possible and we",
    "start": "958420",
    "end": "963589"
  },
  {
    "text": "want to figure out what we want to cache these are Dedes that Zak talked about",
    "start": "963589",
    "end": "969200"
  },
  {
    "text": "earlier they're often costly to compute and so we want to cache them instead of",
    "start": "969200",
    "end": "975019"
  },
  {
    "text": "having to recompute them so if you have multiple stages in our job you want to cache the rdd's between the stages so",
    "start": "975019",
    "end": "982430"
  },
  {
    "text": "we're going to create Prometheus and graph on the dashboards what we already did and we're going to demonstrate them to you and show you how that helps us to",
    "start": "982430",
    "end": "990860"
  },
  {
    "text": "know our spark job and it's actually really fun to create these dashboards I",
    "start": "990860",
    "end": "996110"
  },
  {
    "text": "mean sometimes I forget that I'm actually working when I'm doing this it's I mean it's just a lot of fun to",
    "start": "996110",
    "end": "1002290"
  },
  {
    "text": "create these graph on a dashboards with an input from Prometheus so SPARC is an",
    "start": "1002290",
    "end": "1009730"
  },
  {
    "text": "in-memory compute engine it very aggressively uses memory and even though these nodes that we're gonna do our demo",
    "start": "1009730",
    "end": "1016630"
  },
  {
    "text": "on have 256 gigabytes of memory each so a cluster can easily have a terabyte of",
    "start": "1016630",
    "end": "1022720"
  },
  {
    "text": "memory there's still contention for memory and it's still worth tuning your memory because these large analytics",
    "start": "1022720",
    "end": "1028839"
  },
  {
    "text": "jobs very aggressively use memory and in",
    "start": "1028839",
    "end": "1035740"
  },
  {
    "text": "a spark job you can easily have a hundred gigabyte heap spaces so it's",
    "start": "1035740",
    "end": "1042069"
  },
  {
    "text": "worth it to tune that and we'll talk about how to optimize that for overall performance improvement so now I'm going",
    "start": "1042069",
    "end": "1049870"
  },
  {
    "text": "to talk a bit about Sparx memory management model just so you'll",
    "start": "1049870",
    "end": "1054880"
  },
  {
    "text": "understand why our tuning helps once we employ the tuning we're going to go with here there are two memory usage types",
    "start": "1054880",
    "end": "1062830"
  },
  {
    "text": "and SPARC execution and and they share the heap space the",
    "start": "1062830",
    "end": "1069130"
  },
  {
    "text": "execution memory on the left here oops here we go no animation didn't start yet",
    "start": "1069130",
    "end": "1076950"
  },
  {
    "text": "execution memory on the left is used for short live results",
    "start": "1076950",
    "end": "1082950"
  },
  {
    "text": "it buffers intermediate results that are your computation is performing so for",
    "start": "1082950",
    "end": "1089110"
  },
  {
    "text": "instance in our example code we're going to do a group by in order to do a group by you have to create a hash map and you",
    "start": "1089110",
    "end": "1097600"
  },
  {
    "text": "have to build that hash map somewhere it's going to get built here and the execution side of the heap and after we",
    "start": "1097600",
    "end": "1104169"
  },
  {
    "text": "have performed this group by it's good the the result is going to be an RDD this resilient resilient distributed",
    "start": "1104169",
    "end": "1110679"
  },
  {
    "text": "data set as I was talking about and it is going to be stored over in the member the storage side of the heap",
    "start": "1110679",
    "end": "1119039"
  },
  {
    "text": "so here's a little example of your code running and the sharing of the open",
    "start": "1119909",
    "end": "1129039"
  },
  {
    "text": "space here in the middle is a space on the heap that's available so the storage",
    "start": "1129039",
    "end": "1135059"
  },
  {
    "text": "area say your caching you put dot cache on a bunch at the end of a bunch of your your a Scala or Python lines of spark",
    "start": "1135059",
    "end": "1143139"
  },
  {
    "text": "code and you want to you want to cache a little bit more of your data so do you",
    "start": "1143139",
    "end": "1148629"
  },
  {
    "text": "suppose this what do you think do you think that you're gonna be able to put this blog here yes you know there's open space we're gonna get to do that once we",
    "start": "1148629",
    "end": "1154840"
  },
  {
    "text": "have contention we have to decide which block is going to be evicted and what",
    "start": "1154840",
    "end": "1160600"
  },
  {
    "text": "happens is the execution side always takes precedence over the storage side because the processes that are relying",
    "start": "1160600",
    "end": "1167799"
  },
  {
    "text": "on this intermediate results that are in the execution side would fail if you were to evict their blocks so that just",
    "start": "1167799",
    "end": "1176860"
  },
  {
    "text": "forced some storage memory to spill to disk which is ok sometimes people over",
    "start": "1176860",
    "end": "1182679"
  },
  {
    "text": "cache anyway and it's no harm you can just reload what was in the cache earlier again the execution side is",
    "start": "1182679",
    "end": "1189820"
  },
  {
    "text": "going to take precedence it's going to cause this to spill to disk but there is a limit that you can set and we can tune",
    "start": "1189820",
    "end": "1195519"
  },
  {
    "text": "it and this is something else we can view with Prometheus and and play with this value you user can set this spark memory",
    "start": "1195519",
    "end": "1203230"
  },
  {
    "text": "storage fraction and that is the unev ICTA below part of the storage memory so",
    "start": "1203230",
    "end": "1209430"
  },
  {
    "text": "again execution takes precedence over storage",
    "start": "1209430",
    "end": "1215260"
  },
  {
    "text": "up to a point where you have any victim blocks so one more thing I want you to",
    "start": "1215260",
    "end": "1223930"
  },
  {
    "text": "understand is some api's we're going to use so when you look at this example code it'll make more sense spark sequel",
    "start": "1223930",
    "end": "1231040"
  },
  {
    "text": "is essentially a library that sits on top of spark core and it gives us three",
    "start": "1231040",
    "end": "1238300"
  },
  {
    "text": "additional api's it gives us spark sequel syntax so if you're accustomed to using relational databases you can make",
    "start": "1238300",
    "end": "1246090"
  },
  {
    "text": "sequel calls to and interact with spark that way it also gives you data frames",
    "start": "1246090",
    "end": "1252280"
  },
  {
    "text": "which we're going to use in our example and data frames are similar to a relational database table with named",
    "start": "1252280",
    "end": "1259440"
  },
  {
    "text": "columns it also gives you a data sets which are similar to data frames but the",
    "start": "1259440",
    "end": "1264610"
  },
  {
    "text": "columns are not named",
    "start": "1264610",
    "end": "1268230"
  },
  {
    "text": "additionally so we're going to use the data frames additionally you get okay",
    "start": "1269850",
    "end": "1276130"
  },
  {
    "text": "you get two back-end optimizations as well and this is where spark sequel sits in the stack and in your user code you",
    "start": "1276130",
    "end": "1283870"
  },
  {
    "text": "can intermix the use of spark API and spark sequel API so we are going to do a",
    "start": "1283870",
    "end": "1294100"
  },
  {
    "text": "performance optimization we're going to show a non-cash version and a cache version in our live demo you need to",
    "start": "1294100",
    "end": "1299710"
  },
  {
    "text": "figure out how much space your RTD takes so this is something they spark you I",
    "start": "1299710",
    "end": "1306220"
  },
  {
    "text": "will tell you you put dot cash in the end of the line that creates your data",
    "start": "1306220",
    "end": "1311740"
  },
  {
    "text": "frame then you run your job you bring up the spark web UI you go to the storage",
    "start": "1311740",
    "end": "1317230"
  },
  {
    "text": "tab and you see the size in memory that your RDD is going to take so then you're",
    "start": "1317230",
    "end": "1322270"
  },
  {
    "text": "going to set that that storage on a victim storage region to be at least as",
    "start": "1322270",
    "end": "1327280"
  },
  {
    "text": "big as all the rdd's that you want to cache",
    "start": "1327280",
    "end": "1332480"
  },
  {
    "text": "so this is our code this is the before we create these data frames we do a",
    "start": "1332480",
    "end": "1341340"
  },
  {
    "text": "cross join which is a Cartesian product and then we do a group by operation and because we reuse an RDD and that group",
    "start": "1341340",
    "end": "1348390"
  },
  {
    "text": "by we want to cache it so in our improved version you just put",
    "start": "1348390",
    "end": "1353549"
  },
  {
    "text": "dot cache at the end of the line that's going to cache those are Dedes and we're gonna see what that looks like",
    "start": "1353549",
    "end": "1359520"
  },
  {
    "text": "if we ran after and our timings and in our memory usage so this is the query",
    "start": "1359520",
    "end": "1364890"
  },
  {
    "text": "plan on the left for the non-cash version you see that that's creating",
    "start": "1364890",
    "end": "1370230"
  },
  {
    "text": "those rdd's but they're not in memory on the right it shows that those are in memory tables because they've been cached so this results you're gonna see",
    "start": "1370230",
    "end": "1377730"
  },
  {
    "text": "in the demo as well but just as a preview we looked at all this running",
    "start": "1377730",
    "end": "1383250"
  },
  {
    "text": "and spark we look at how much memory is being used with viewing it through Prometheus and Griffin ax and the result",
    "start": "1383250",
    "end": "1389970"
  },
  {
    "text": "is that after our optimization we went from about 5.3 gigabytes per worker to",
    "start": "1389970",
    "end": "1397590"
  },
  {
    "text": "about around 2 gigabytes per worker after we cashed that RDD so this is my",
    "start": "1397590",
    "end": "1407640"
  },
  {
    "text": "relative index of performance that I used in my performance metrics but here it's the same value as a percentage",
    "start": "1407640",
    "end": "1413130"
  },
  {
    "text": "change so I'll just explain it as a percentage change we got a 76 percent improvement in",
    "start": "1413130",
    "end": "1420740"
  },
  {
    "text": "memory usage so that's memory reduction and we had a 10% decrease in runtime by",
    "start": "1420740",
    "end": "1428970"
  },
  {
    "text": "adding that optimization ok so demo time",
    "start": "1428970",
    "end": "1437630"
  },
  {
    "text": "so this is Griffin oh and Diane's gonna show us running a spark job and okay see",
    "start": "1466519",
    "end": "1475970"
  },
  {
    "text": "quite you'll see you'll see how performant it is so so first we see here",
    "start": "1475970",
    "end": "1485580"
  },
  {
    "text": "that there is no spark job running and I'm gonna go to my B deployment configs",
    "start": "1485580",
    "end": "1493620"
  },
  {
    "text": "am I gonna go to my nan cached version",
    "start": "1493620",
    "end": "1498529"
  },
  {
    "text": "I'm gonna deploy this non cash job then",
    "start": "1499429",
    "end": "1507210"
  },
  {
    "text": "I'm going to look at the spark master we should see this job showing up here take",
    "start": "1507210",
    "end": "1515309"
  },
  {
    "text": "a second there it is there's our non cash spark job we can see that it's",
    "start": "1515309",
    "end": "1524309"
  },
  {
    "text": "running we can see her under the storage tab that in fact there is nothing",
    "start": "1524309",
    "end": "1529409"
  },
  {
    "text": "because we haven't cashed anything now we can go over to Griffin ax and we see",
    "start": "1529409",
    "end": "1534600"
  },
  {
    "text": "that it's going to start the job is going to start showing up here soon here",
    "start": "1534600",
    "end": "1540149"
  },
  {
    "text": "it is so we see our CPU usage going up to start with we just have like the see",
    "start": "1540149",
    "end": "1545760"
  },
  {
    "text": "that the met the network is quiet the whole system is pretty quiet but as this",
    "start": "1545760",
    "end": "1550769"
  },
  {
    "text": "job most in here we're gonna see our memory usage going up let's just go and",
    "start": "1550769",
    "end": "1559350"
  },
  {
    "text": "make sure that that job was still running",
    "start": "1559350",
    "end": "1564049"
  },
  {
    "text": "yeah it's running okay that's good it's all good all right so",
    "start": "1566980",
    "end": "1574899"
  },
  {
    "text": "yes so it's not looks like it's starting so you see that the cluster CPU usage is",
    "start": "1578850",
    "end": "1584700"
  },
  {
    "text": "going up you see the memory usage is going up pod CP usage okay so now what",
    "start": "1584700",
    "end": "1595169"
  },
  {
    "text": "we're interested in here is you see that let's see can they read the numbers you",
    "start": "1595169",
    "end": "1602190"
  },
  {
    "text": "think okay so um so you can see that we've already we've got five point two",
    "start": "1602190",
    "end": "1608610"
  },
  {
    "text": "gigabytes for these four top entries here those are our spark workers this is",
    "start": "1608610",
    "end": "1613620"
  },
  {
    "text": "the non-cash version okay and there is only one pod per container so the the",
    "start": "1613620",
    "end": "1620000"
  },
  {
    "text": "pod metrics are the same and down here at the bottom we have our JVM pools we",
    "start": "1620000",
    "end": "1627419"
  },
  {
    "text": "have the Eden space the survivor space and the old gen space which is the Eden",
    "start": "1627419",
    "end": "1632519"
  },
  {
    "text": "spaces where our new Java objects are being created after the first garbage collection things get moved over to the",
    "start": "1632519",
    "end": "1639809"
  },
  {
    "text": "survivor space and then our cache items and things that make it through another",
    "start": "1639809",
    "end": "1646500"
  },
  {
    "text": "garbage collection are going to end up in the old gen space so we can use these Griffin Oh dashboards at the bottom here",
    "start": "1646500",
    "end": "1653549"
  },
  {
    "text": "to optimize how we're going to use the JVM for this part for these SPARC workers in the spark master now we see",
    "start": "1653549",
    "end": "1664080"
  },
  {
    "text": "we're up at 40% of CPU usage for the cluster and that's because that Cartesian product is happening right now",
    "start": "1664080",
    "end": "1670740"
  },
  {
    "text": "it's very CPU intensive operation and you can see our two nodes here that are",
    "start": "1670740",
    "end": "1677279"
  },
  {
    "text": "sitting in Boston doing this work as we watch it and so here we have the memory",
    "start": "1677279",
    "end": "1686460"
  },
  {
    "text": "high-water mark for the workers at five gigabytes five and a half gigabytes each",
    "start": "1686460",
    "end": "1693350"
  },
  {
    "text": "so now we'll go back and see that spark job is finished I happen to know that it's finishes in about three minutes",
    "start": "1693350",
    "end": "1700230"
  },
  {
    "text": "there are two point nine minutes it finished okay so now we're gonna go back",
    "start": "1700230",
    "end": "1705809"
  },
  {
    "text": "and deploy the cached version",
    "start": "1705809",
    "end": "1711950"
  },
  {
    "text": "so if you notice there Dyan pressed deploy so this is a deployment config and when Diane was was",
    "start": "1715470",
    "end": "1724320"
  },
  {
    "text": "building her application she created a she's sourced to image to",
    "start": "1724320",
    "end": "1729550"
  },
  {
    "text": "build an image and then once Diane had her image we used the deployment effect",
    "start": "1729550",
    "end": "1735400"
  },
  {
    "text": "to roll out the the spark job and then when she went iron wanted to rerun that same spark job she just had to just",
    "start": "1735400",
    "end": "1741850"
  },
  {
    "text": "press deploy and I really ran the same job yeah one of the challenges and",
    "start": "1741850",
    "end": "1747580"
  },
  {
    "text": "making something that you can do in a five minute demo is you have to cut an actual spark job into something that",
    "start": "1747580",
    "end": "1753010"
  },
  {
    "text": "will finish in the amount of time that we have up here so I've just submitted the cache version now we're going to",
    "start": "1753010",
    "end": "1760210"
  },
  {
    "text": "look at storage see we have the the two rdd's there that we've cached that's",
    "start": "1760210",
    "end": "1765730"
  },
  {
    "text": "running live right now and now we can look over at go fauna and you can see that that last job that",
    "start": "1765730",
    "end": "1776260"
  },
  {
    "text": "drop-off right there is the last job that finished we're gonna see the next job coming through so there's a little",
    "start": "1776260",
    "end": "1783220"
  },
  {
    "text": "bit of a lag because we set the intervals a little bit with a little bit",
    "start": "1783220",
    "end": "1788260"
  },
  {
    "text": "more time in between so if you wanted it",
    "start": "1788260",
    "end": "1794230"
  },
  {
    "text": "to see you know more more you know metrics a little bit quicker than you",
    "start": "1794230",
    "end": "1799630"
  },
  {
    "text": "would configure those things instead of scraping every 10 seconds like we you",
    "start": "1799630",
    "end": "1805660"
  },
  {
    "text": "know so you could set to scrape every 10 seconds or you can set lower but so now",
    "start": "1805660",
    "end": "1812620"
  },
  {
    "text": "the second job is running we've already hit the high-water mark for the containers that are holding the workers",
    "start": "1812620",
    "end": "1818650"
  },
  {
    "text": "it's showing right here saying for the pods about two gigabytes each again we",
    "start": "1818650",
    "end": "1826090"
  },
  {
    "text": "can go down and look at our JVM pools we also I noticed that the network was very",
    "start": "1826090",
    "end": "1832870"
  },
  {
    "text": "quiet network is not an issue with this example at all and it's there's no bottleneck there but the important thing",
    "start": "1832870",
    "end": "1839500"
  },
  {
    "text": "here is that these values were not known to us previously and it's a guessing game if you don't have these",
    "start": "1839500",
    "end": "1846390"
  },
  {
    "text": "metrics to look at so this is incredibly helpful when you're tuning a spark job and I'm sure applications in general",
    "start": "1846390",
    "end": "1853550"
  },
  {
    "text": "just the fact that you can look at how its load balanced across the nodes then you can dive down deeper and see at the",
    "start": "1853550",
    "end": "1861480"
  },
  {
    "text": "pod level in container level exactly what's happening so that's it so we'll",
    "start": "1861480",
    "end": "1877620"
  },
  {
    "text": "take any questions if anybody has any we also have office hours for about an hour",
    "start": "1877620",
    "end": "1882660"
  },
  {
    "text": "we're gonna be at the Red Hat booth so if you want to stop by ask any questions",
    "start": "1882660",
    "end": "1888120"
  },
  {
    "text": "of anything that we showed up here because everything that we did was all open source and reproducible and",
    "start": "1888120",
    "end": "1894480"
  },
  {
    "text": "reproduce do it yourself thank you",
    "start": "1894480",
    "end": "1898850"
  }
]