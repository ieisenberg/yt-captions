[
  {
    "start": "0",
    "end": "463000"
  },
  {
    "text": "welcome everyone to uh our maintain our talk about with us",
    "start": "480",
    "end": "6160"
  },
  {
    "text": "this is the end of the day so thank you for showing up and uh welcome also to",
    "start": "6160",
    "end": "11599"
  },
  {
    "text": "our online audience",
    "start": "11599",
    "end": "15320"
  },
  {
    "text": "just sit next the other way just next ah okay thank you",
    "start": "17840",
    "end": "24880"
  },
  {
    "text": "my name is deepti sigredi i'm a software engineer at planetscale and i'm the tech",
    "start": "24880",
    "end": "30160"
  },
  {
    "text": "lead for wittes i will start off with a brief",
    "start": "30160",
    "end": "35680"
  },
  {
    "text": "introduction to witus and afterwards my co-speakers will talk about our",
    "start": "35680",
    "end": "41920"
  },
  {
    "text": "kubernetes operator vt admin which is a replacement for our existing control",
    "start": "41920",
    "end": "48640"
  },
  {
    "text": "plane and there are some demos",
    "start": "48640",
    "end": "53120"
  },
  {
    "text": "with is a clustering system for horizontal scaling of mysql or mariadb",
    "start": "53760",
    "end": "59520"
  },
  {
    "text": "it's effectively a distributed database it is a cncf graduated project the first",
    "start": "59520",
    "end": "64960"
  },
  {
    "text": "storage project to graduate from cncf it's open source apache 2.0",
    "start": "64960",
    "end": "70560"
  },
  {
    "text": "we have contributors from many different companies and a vibrant community in our",
    "start": "70560",
    "end": "76240"
  },
  {
    "text": "slack workspace and it's mostly written in golang",
    "start": "76240",
    "end": "82399"
  },
  {
    "text": "fittus is a cloud native database it runs in kubernetes it was started at youtube where it used",
    "start": "82560",
    "end": "88560"
  },
  {
    "text": "to run in borg so it was a natural evolution for bitters to be able to run in kubernetes",
    "start": "88560",
    "end": "94799"
  },
  {
    "text": "it is scalable highly available all of those good things with provides certain durability",
    "start": "94799",
    "end": "100640"
  },
  {
    "text": "guarantees that you don't get with vanilla mysql it also gives you the illusion of a",
    "start": "100640",
    "end": "106720"
  },
  {
    "text": "single database so behind the scenes you might be running hundreds or thousands of individual database servers",
    "start": "106720",
    "end": "113600"
  },
  {
    "text": "in order to achieve the scalability and the high availability but to an application it looks like a single",
    "start": "113600",
    "end": "120560"
  },
  {
    "text": "database applications can get single dedicated connections to with us which behind the",
    "start": "120560",
    "end": "126799"
  },
  {
    "text": "scenes translate to many individual mysql connections and vitus can present as mysql 5.7 or 8.0 it",
    "start": "126799",
    "end": "135440"
  },
  {
    "text": "can run with mysql 5.7 or 8.0 and it's compatible with many frameworks and orms",
    "start": "135440",
    "end": "143360"
  },
  {
    "text": "bitter serves millions of qps in production there are many adopters of with us",
    "start": "143360",
    "end": "149040"
  },
  {
    "text": "running it in production uh over the last six years so vitas started about 11 years ago at youtube but over",
    "start": "149040",
    "end": "155760"
  },
  {
    "text": "the last six years a lot of outside companies have adopted it",
    "start": "155760",
    "end": "163440"
  },
  {
    "text": "notably slack runs all of their data on bitters square cache runs everything on",
    "start": "163440",
    "end": "169599"
  },
  {
    "text": "with this jd.com has a huge kubernetes deployment of with us with thousands of nodes and",
    "start": "169599",
    "end": "176239"
  },
  {
    "text": "tens of thousands of widgets components and planet scale runs the database service onbittens with onbiters with",
    "start": "176239",
    "end": "183040"
  },
  {
    "text": "thousands of individual witness clusters a few key concepts and then we will do",
    "start": "183040",
    "end": "189599"
  },
  {
    "text": "an architecture overview before we move on to the rest of the talk",
    "start": "189599",
    "end": "194959"
  },
  {
    "text": "the concepts that are important to know to understand bitters are key space shard",
    "start": "194959",
    "end": "200319"
  },
  {
    "text": "cell a key space is just a logical database there may be thousands of physical databases but it",
    "start": "200319",
    "end": "207120"
  },
  {
    "text": "it can present as a single logical database and a shard is a subset of the logical database the union of all the",
    "start": "207120",
    "end": "214640"
  },
  {
    "text": "shards comprises your database as far as the application is concerned and a cell is a failure domain",
    "start": "214640",
    "end": "221840"
  },
  {
    "text": "so depending on the availability guarantees you want you will deploy",
    "start": "221840",
    "end": "227760"
  },
  {
    "text": "witness components in different cells and a cell could be a data center or it",
    "start": "227760",
    "end": "233040"
  },
  {
    "text": "could be an individual server rack or it could be an availability zone or a region or something like that",
    "start": "233040",
    "end": "239920"
  },
  {
    "text": "in a cloud provider it is very common to run",
    "start": "239920",
    "end": "245519"
  },
  {
    "text": "databases today in a replicated configuration with a primary and replicas",
    "start": "245519",
    "end": "251920"
  },
  {
    "text": "when you run bitters in front of such a replicated configuration of mysql there is a witus component that lives",
    "start": "251920",
    "end": "258720"
  },
  {
    "text": "with each of those mysqls called a vt tablet",
    "start": "258720",
    "end": "263759"
  },
  {
    "text": "in production you typically run multiple clusters they might be shards of the",
    "start": "263919",
    "end": "269120"
  },
  {
    "text": "same key space or they may be multiple key spaces that has multiple logical databases",
    "start": "269120",
    "end": "276479"
  },
  {
    "text": "all traffic goes through a component called vtgate this is a proxy that pretends to be my sql it speaks to my",
    "start": "276479",
    "end": "283600"
  },
  {
    "text": "sql protocol it looks like a single mysql server to an application and it",
    "start": "283600",
    "end": "288800"
  },
  {
    "text": "figures out which actual mysql the query needs to go to",
    "start": "288800",
    "end": "295040"
  },
  {
    "text": "and in order to scale out depending on the application traffic vt gates can be scaled up",
    "start": "295040",
    "end": "300720"
  },
  {
    "text": "as the traffic grows and each vt gate will be able to route to any of the key spaces and shards",
    "start": "300720",
    "end": "309120"
  },
  {
    "text": "vtgate has to do this in a transparent way as as far as the application is concerned there is only a database",
    "start": "309120",
    "end": "316560"
  },
  {
    "text": "and vtgate has to route them to the right databases the right shards the right",
    "start": "316560",
    "end": "322400"
  },
  {
    "text": "clusters how does it do this it does this by looking at the schema and the sharding scheme sharding scheme",
    "start": "322400",
    "end": "329280"
  },
  {
    "text": "is what is also known as v-schema or vita schema this tells vtgate",
    "start": "329280",
    "end": "334880"
  },
  {
    "text": "the necessary metadata in order for vtgate to figure out where to route the queries",
    "start": "334880",
    "end": "342560"
  },
  {
    "text": "the other component that is of interest in windows deployment is the topo server",
    "start": "345360",
    "end": "350639"
  },
  {
    "text": "this is a distributed key value store people typically use hcd or zookeeper or even kubernetes as the topo server and",
    "start": "350639",
    "end": "358400"
  },
  {
    "text": "this topo server stores the configuration information that witness requires in order for the components to",
    "start": "358400",
    "end": "365039"
  },
  {
    "text": "discover each other and in order for the query routing to work and this is a pretty small data set it",
    "start": "365039",
    "end": "371440"
  },
  {
    "text": "is cached by vtgate and one of the principles in witus is that the topo server does not need to be up for",
    "start": "371440",
    "end": "378240"
  },
  {
    "text": "applications to be able to query the database the last thing i want to talk about is",
    "start": "378240",
    "end": "384960"
  },
  {
    "text": "the control daemon there is a component called vtctld which allows you to",
    "start": "384960",
    "end": "390000"
  },
  {
    "text": "perform management operations on the cluster it works with the toppo it gives you a view of the topo and it allows",
    "start": "390000",
    "end": "398479"
  },
  {
    "text": "the operators to perform manual failovers and things like that all sorts",
    "start": "398479",
    "end": "404080"
  },
  {
    "text": "of manual operations just briefly about uh what is new in",
    "start": "404080",
    "end": "411280"
  },
  {
    "text": "with us recently and what's coming up we are doing our 12-0 release ga in about",
    "start": "411280",
    "end": "417039"
  },
  {
    "text": "two weeks we did a release candidate last week we have been improving the query support",
    "start": "417039",
    "end": "423440"
  },
  {
    "text": "we have there is still a small set of sub queries that are not supported especially in sharded mode but that set",
    "start": "423440",
    "end": "430000"
  },
  {
    "text": "keeps getting smaller as we keep working there have been a lot of performance",
    "start": "430000",
    "end": "435039"
  },
  {
    "text": "improvements recently and we also publish continuous benchmarks on benchmark.witness.io",
    "start": "435039",
    "end": "442840"
  },
  {
    "text": "coming up in the future we have vt admin that you will hear a little more about",
    "start": "443039",
    "end": "448960"
  },
  {
    "text": "in a few minutes and we are also going to complete support for multi-column",
    "start": "448960",
    "end": "454880"
  },
  {
    "text": "indexes automatic failure detection also known as vtr collisions and distributed transactions",
    "start": "454880",
    "end": "463120"
  },
  {
    "start": "463000",
    "end": "633000"
  },
  {
    "text": "with that i'll hand it off to alken all right thank you dt for the intro and the",
    "start": "463120",
    "end": "469039"
  },
  {
    "text": "new features and what's coming up my name is alken and i work with deept at planetscale as part of the",
    "start": "469039",
    "end": "476240"
  },
  {
    "text": "tess open source team and i'm one of the maintainers of the test project and",
    "start": "476240",
    "end": "483199"
  },
  {
    "text": "many other things so we'll talk about the operators as you know we heard from the other",
    "start": "483199",
    "end": "489280"
  },
  {
    "text": "talks the operator and the kubernetes are recommended way of running stateful applications for especially for",
    "start": "489280",
    "end": "495120"
  },
  {
    "text": "databases and there is also with operator for kubernetes which is open",
    "start": "495120",
    "end": "500319"
  },
  {
    "text": "source and what does it do beyond the regular operators it does uh",
    "start": "500319",
    "end": "507599"
  },
  {
    "text": "automation of vtest functions and basically it helps with the the cluster management",
    "start": "507599",
    "end": "513680"
  },
  {
    "text": "and all that other flags and and the command line tools that you would have to run",
    "start": "513680",
    "end": "518800"
  },
  {
    "text": "and also keeps the consistency and and the",
    "start": "518800",
    "end": "524080"
  },
  {
    "text": "failovers automated and helps a lot so",
    "start": "524080",
    "end": "529839"
  },
  {
    "text": "so i want to show you a little bit of the operator how it does it and",
    "start": "529839",
    "end": "536080"
  },
  {
    "text": "because this time consuming and and it won't fit in this part of the the talk",
    "start": "536080",
    "end": "541360"
  },
  {
    "text": "we need to deploy a test cluster in this case it's going to be deployed in gke",
    "start": "541360",
    "end": "546480"
  },
  {
    "text": "and and then we deployed operator which is a very low uh footprint and",
    "start": "546480",
    "end": "552959"
  },
  {
    "text": "it's basically fast um it's customizable it's open source and",
    "start": "552959",
    "end": "558480"
  },
  {
    "text": "you could actually have to set up your own security users and and the other settings that you would do normally and",
    "start": "558480",
    "end": "564480"
  },
  {
    "text": "then we will create a database um and um",
    "start": "564480",
    "end": "569600"
  },
  {
    "text": "with the sharding workflow which is um these links will be provided in in the slides",
    "start": "569600",
    "end": "575120"
  },
  {
    "text": "we have operator examples that we'll run through a scenario we'll shard a",
    "start": "575120",
    "end": "582320"
  },
  {
    "text": "commerce schema like an e-commerce database to a customer database which is charted so i'm going to talk about that",
    "start": "582320",
    "end": "588320"
  },
  {
    "text": "a little bit and and then we're going to create some load on it this is not for a benchmarking it's for just to",
    "start": "588320",
    "end": "594959"
  },
  {
    "text": "demonstrate that there's some activity in the database and as dt said we run continuous",
    "start": "594959",
    "end": "600399"
  },
  {
    "text": "benchmarks on our old releases and and deploys that's separate from this talk",
    "start": "600399",
    "end": "606880"
  },
  {
    "text": "and also we do online ddl on on this environment and then running the",
    "start": "606880",
    "end": "614480"
  },
  {
    "text": "charted field charted cluster and i want to demonstrate a failover while running the",
    "start": "614480",
    "end": "620000"
  },
  {
    "text": "load and the online ddl and everything else so this basically a debate between",
    "start": "620000",
    "end": "625440"
  },
  {
    "text": "the the stateful databases in kubernetes running with operator under the test so",
    "start": "625440",
    "end": "632560"
  },
  {
    "text": "okay there's going to be a workflow i'll talk about it and and i have the github link for the",
    "start": "632560",
    "end": "638640"
  },
  {
    "start": "633000",
    "end": "970000"
  },
  {
    "text": "the demo so first of all",
    "start": "638640",
    "end": "643680"
  },
  {
    "text": "that's not correct okay this one hopefully you can see this i'm gonna start this so we have um",
    "start": "643680",
    "end": "650640"
  },
  {
    "text": "kubernetes cluster that's already deployed and um and then running with the with",
    "start": "650640",
    "end": "656959"
  },
  {
    "text": "the test operator and basically operator has one pod there's three parts for xcd and",
    "start": "656959",
    "end": "663600"
  },
  {
    "text": "and then we have um since we already ran the sharded cluster we have a commerce and",
    "start": "663600",
    "end": "670959"
  },
  {
    "text": "and a customer key spaces which they have vt tablets as a pod so",
    "start": "670959",
    "end": "676160"
  },
  {
    "text": "also we have some some",
    "start": "676160",
    "end": "681600"
  },
  {
    "text": "vtc tld and vtgate pods in this cluster and already running along with the operator",
    "start": "681600",
    "end": "689120"
  },
  {
    "text": "and what i wanted to do is um to deploy some load and two parts it's",
    "start": "689120",
    "end": "696560"
  },
  {
    "text": "going to deploy uh select and and insert recursive on",
    "start": "696560",
    "end": "701680"
  },
  {
    "text": "this database in key space customer and it will create some load on it",
    "start": "701680",
    "end": "709440"
  },
  {
    "text": "and we have the customer table and we have the order table it's called c",
    "start": "709440",
    "end": "714959"
  },
  {
    "text": "order and then as we run these uh db load pods which are actually docker",
    "start": "714959",
    "end": "723040"
  },
  {
    "text": "images that's pre-created we have all documented all the on the link that i mentioned",
    "start": "723040",
    "end": "728320"
  },
  {
    "text": "um if you want to try this yourself it's very easy to do this and um and then we generate some load on",
    "start": "728320",
    "end": "735040"
  },
  {
    "text": "the sharded cluster and and then the traffic starts coming in",
    "start": "735040",
    "end": "740800"
  },
  {
    "text": "uh for the sake of this demo and",
    "start": "740800",
    "end": "746959"
  },
  {
    "text": "so the the inserts are increasing that the number of rows are increasing and then",
    "start": "747040",
    "end": "754160"
  },
  {
    "text": "the cluster is healthy running with the operator an operator actually manages everything so if if in in this case of",
    "start": "754160",
    "end": "761360"
  },
  {
    "text": "of kubernetes if a pod was killed it will reinstate and and then",
    "start": "761360",
    "end": "768079"
  },
  {
    "text": "recover the pod from the existing image and then if it's a replica it will fail over to the primary it would do all of",
    "start": "768079",
    "end": "775440"
  },
  {
    "text": "that so what we're going to do in this example over here we have a table called",
    "start": "775440",
    "end": "780560"
  },
  {
    "text": "c order and it has multiple columns skew is the one of them it's var binary 128",
    "start": "780560",
    "end": "787279"
  },
  {
    "text": "default now i set the ddl strategy which you mentioned we do",
    "start": "787279",
    "end": "792480"
  },
  {
    "text": "online ddl and in the test world and then to online and then invoke this",
    "start": "792480",
    "end": "799760"
  },
  {
    "text": "ddl with altar table and then it becomes a migration the migration gets executed",
    "start": "799760",
    "end": "805360"
  },
  {
    "text": "behind the scenes without impacting the load or the um there's no like locking or anything it's",
    "start": "805360",
    "end": "811680"
  },
  {
    "text": "it's it's online and and then we can see the the migration status that's running and",
    "start": "811680",
    "end": "818880"
  },
  {
    "text": "running on the customer key space on a charted there are two two shards in this example",
    "start": "818880",
    "end": "826720"
  },
  {
    "text": "and each chart has one primary and one replica running under this uh kubernetes cluster",
    "start": "826720",
    "end": "833279"
  },
  {
    "text": "and and then we can see it's actually um the tablets are are healthy uh whether",
    "start": "833279",
    "end": "839440"
  },
  {
    "text": "from the command line example over here from mysql or from the test commands that you can do it look at it also",
    "start": "839440",
    "end": "847120"
  },
  {
    "text": "there's an option to see from the kubernetes end there are multiple points of access to this whole cluster",
    "start": "847120",
    "end": "853519"
  },
  {
    "text": "where you want to manage but the demonstration over here is using the vtctl client that we test command line",
    "start": "853519",
    "end": "860399"
  },
  {
    "text": "and then there's the mysql client that's actually accessing the cluster",
    "start": "860399",
    "end": "865600"
  },
  {
    "text": "and and also we have uh",
    "start": "865600",
    "end": "871680"
  },
  {
    "text": "the the primary and replicas set up for this uh customer key space you can see in the in the customer uh charted",
    "start": "871680",
    "end": "877920"
  },
  {
    "text": "replicas we have um two primaries what i wanted to show over",
    "start": "877920",
    "end": "883120"
  },
  {
    "text": "here while the load is running uh there is a there's a replica uh ended with four two two three and i want to",
    "start": "883120",
    "end": "890079"
  },
  {
    "text": "actually say i want to fail over to this just i think that the the primary on this",
    "start": "890079",
    "end": "897360"
  },
  {
    "text": "shard is not doing well just a a scenario over here um",
    "start": "897360",
    "end": "902800"
  },
  {
    "text": "and i want to fail over to um to the replica while the the migration the altar is",
    "start": "902800",
    "end": "909920"
  },
  {
    "text": "running on that cluster on the sharder cluster i can actually",
    "start": "909920",
    "end": "915120"
  },
  {
    "text": "tell with this okay fail over to this shard this is called a planned repair",
    "start": "915120",
    "end": "921120"
  },
  {
    "text": "and shard because i know this is not an emergency i'm planning this um",
    "start": "921120",
    "end": "927600"
  },
  {
    "text": "and then and i invoke and um because um it's a small cluster it",
    "start": "927600",
    "end": "932880"
  },
  {
    "text": "becomes available and then it flips over so when when we test uh when you tell vitesse",
    "start": "932880",
    "end": "939440"
  },
  {
    "text": "that i want to get this this replica become a primary and it will actually take the other one",
    "start": "939440",
    "end": "945680"
  },
  {
    "text": "if the if the uh the old one is is available it will actually detach and and attach as a",
    "start": "945680",
    "end": "951120"
  },
  {
    "text": "replica so always have one replica available um of course in production you would have",
    "start": "951120",
    "end": "957839"
  },
  {
    "text": "multiple replicas not just one at least two and and then you could actually set them",
    "start": "957839",
    "end": "962880"
  },
  {
    "text": "in different regions so so that's it and we'll go back to the",
    "start": "962880",
    "end": "970160"
  },
  {
    "start": "970000",
    "end": "1077000"
  },
  {
    "text": "workflow so what we did is we ran the 101 example it's in sio and then we created a load",
    "start": "970160",
    "end": "976639"
  },
  {
    "text": "with an example docker image did the recursive select and insert and we graph there's a load coming in we",
    "start": "976639",
    "end": "982720"
  },
  {
    "text": "execute an online ddl while all this running and then we failed over uh in",
    "start": "982720",
    "end": "988000"
  },
  {
    "text": "one of the shards from a replica to a master uh to primary and then and",
    "start": "988000",
    "end": "993519"
  },
  {
    "text": "then we were able to continue the load thank you all right my my friend andrew",
    "start": "993519",
    "end": "999519"
  },
  {
    "text": "will come over and uh do this okay we're gonna switch over laptops",
    "start": "999519",
    "end": "1005440"
  },
  {
    "text": "real quick just doing a planned uh repair plan repair laptop okay",
    "start": "1005440",
    "end": "1012959"
  },
  {
    "text": "give you a little taste of the humor you can expect okay all right",
    "start": "1013040",
    "end": "1021160"
  },
  {
    "text": "okay my demo is done um hello uh my name is andrew mason i am a",
    "start": "1024480",
    "end": "1030319"
  },
  {
    "text": "senior oh yeah i can take my mask off you can probably hear me a lot better um hi my name is andrew mason um i'm a",
    "start": "1030319",
    "end": "1036959"
  },
  {
    "text": "senior software engineer at slack and i am a maintainer on the test project and i'm here to talk to you about something",
    "start": "1036959",
    "end": "1042959"
  },
  {
    "text": "that i and a couple other people have been working on called vt admin uh as deepti was saying earlier uh vitamin is",
    "start": "1042959",
    "end": "1050720"
  },
  {
    "text": "the next generation of cluster management and uh web ui tooling for for cluster",
    "start": "1050720",
    "end": "1057039"
  },
  {
    "text": "management um it will eventually replace the vtc tld which we just saw in in alken's demo",
    "start": "1057039",
    "end": "1064240"
  },
  {
    "text": "so i'm gonna kind of do a demo and talk at the same time so we'll see how that goes um",
    "start": "1064240",
    "end": "1070320"
  },
  {
    "text": "in in the original case uh you have a single vitesse cluster which is like a",
    "start": "1070320",
    "end": "1076000"
  },
  {
    "text": "uh let me jump through here so different version of what dp showed you",
    "start": "1076000",
    "end": "1081360"
  },
  {
    "text": "earlier um basically everything that talks to the topo i consider to be a single of a test cluster so we have the",
    "start": "1081360",
    "end": "1087679"
  },
  {
    "text": "application going to the vt gate going to the tablets and then on the side we have the topo and the btc dld just like",
    "start": "1087679",
    "end": "1093600"
  },
  {
    "text": "we saw before and what vt admin is going to let us do and that you know oh boy",
    "start": "1093600",
    "end": "1100480"
  },
  {
    "text": "bigger bigger bigger bigger",
    "start": "1100480",
    "end": "1104880"
  },
  {
    "text": "and that you know looks like this so i have some shards i have some not serving don't worry about that we'll come back",
    "start": "1106400",
    "end": "1111600"
  },
  {
    "text": "to that later um and what vt admin does is vtm is going",
    "start": "1111600",
    "end": "1116960"
  },
  {
    "text": "to sit in front of these vtctlds and allow you to manage multiple",
    "start": "1116960",
    "end": "1122880"
  },
  {
    "text": "completely isolated deployments at the same time so that looks like this",
    "start": "1122880",
    "end": "1128720"
  },
  {
    "text": "so we took that one chart and shrunk it down and stamped out two more of them and we",
    "start": "1128720",
    "end": "1134559"
  },
  {
    "text": "have a single vt admin api and v2 ad and web that sits in front of",
    "start": "1134559",
    "end": "1139760"
  },
  {
    "text": "all three clusters where this is useful just as a trivial example is if you have prod dev and qa you would like them to",
    "start": "1139760",
    "end": "1147360"
  },
  {
    "text": "not overlap and not collaborate each other so you can have them completely isolated from each other and then stick",
    "start": "1147360",
    "end": "1153200"
  },
  {
    "text": "a vt admin and have traffic admin traffic flow only one way application traffic being completely",
    "start": "1153200",
    "end": "1158960"
  },
  {
    "text": "separate so in words it's a single control plane for",
    "start": "1158960",
    "end": "1165840"
  },
  {
    "start": "1160000",
    "end": "1605000"
  },
  {
    "text": "multiple the test deployments which uh are we use the very overloaded but useful term of cluster",
    "start": "1165840",
    "end": "1172000"
  },
  {
    "text": "uh it provides both grpc and http api endpoints for you to program against if",
    "start": "1172000",
    "end": "1178160"
  },
  {
    "text": "you want to build any automation on top of vt admin api you can do that there's also ui for",
    "start": "1178160",
    "end": "1183919"
  },
  {
    "text": "human friendly use of the tool um the back end is written in go along with the rest of the",
    "start": "1183919",
    "end": "1189600"
  },
  {
    "text": "test project and we're using react on the front end uh and the goal here being to eventually",
    "start": "1189600",
    "end": "1195440"
  },
  {
    "text": "replace the existing btc dlv ui which is for a single cluster and vtm it works just as well if you only have one",
    "start": "1195440",
    "end": "1201840"
  },
  {
    "text": "cluster but you get you know more power out of it from having a many to",
    "start": "1201840",
    "end": "1207200"
  },
  {
    "text": "one relationship and so",
    "start": "1207200",
    "end": "1212480"
  },
  {
    "text": "there's a branch which you can get from the slides so here i have a local deployment",
    "start": "1212480",
    "end": "1218559"
  },
  {
    "text": "running on my laptop so to be a little small and use our imaginations a little bit",
    "start": "1218559",
    "end": "1223679"
  },
  {
    "text": "because my laptop is not very large so i have two uh two clusters",
    "start": "1223679",
    "end": "1229039"
  },
  {
    "text": "one having a single key space called media which has two shards zero to 80 and 80 to zero and then four non-serving",
    "start": "1229039",
    "end": "1235840"
  },
  {
    "text": "shards which i will get to in a moment and then i have a second cluster over here which has a single uncharted",
    "start": "1235840",
    "end": "1242080"
  },
  {
    "text": "commerce key space and from the vt admin view i get to see everything in one place so i can go",
    "start": "1242080",
    "end": "1249360"
  },
  {
    "text": "through i see i have two clusters each cluster has a vt gate they're called local one and local two",
    "start": "1249360",
    "end": "1256080"
  },
  {
    "text": "uh key spaces as before schemas with sizes i can look at the",
    "start": "1256080",
    "end": "1262720"
  },
  {
    "text": "schema i can see how they are sharded",
    "start": "1262720",
    "end": "1267600"
  },
  {
    "text": "and one of the other useful things to do in the tests which has not been talked about i think is a thing called vt",
    "start": "1268000",
    "end": "1274720"
  },
  {
    "text": "explain which is basically my sql explain plan but the test style",
    "start": "1274720",
    "end": "1280000"
  },
  {
    "text": "so you can sort of see how the test will route a query based on the sharding scheme so i can go",
    "start": "1280000",
    "end": "1285600"
  },
  {
    "text": "into vt admin and i can go here select star from books and",
    "start": "1285600",
    "end": "1292159"
  },
  {
    "text": "unsurprisingly this will go to every shard in the key space because in order to collect everything we have to go everywhere",
    "start": "1292159",
    "end": "1297919"
  },
  {
    "text": "but if i'm going to pin it down based on a single sharding key and this is actually",
    "start": "1297919",
    "end": "1303280"
  },
  {
    "text": "my most popular my personal most common use of vt explain is to see",
    "start": "1303280",
    "end": "1308400"
  },
  {
    "text": "where a particular row lives so where in this logical database does this row actually live so i can go to the tablets",
    "start": "1308400",
    "end": "1314000"
  },
  {
    "text": "in that shard and do some diagnostics so i can see that id10 gets mapped to",
    "start": "1314000",
    "end": "1320240"
  },
  {
    "text": "shard media-80 and that is pretty much vt admin so now",
    "start": "1320240",
    "end": "1327679"
  },
  {
    "text": "as you grow over time your sharding may end up",
    "start": "1327679",
    "end": "1333280"
  },
  {
    "text": "not fitting in the space that you allowed anymore so right now i have two shards but i have a whole bunch more",
    "start": "1333280",
    "end": "1338400"
  },
  {
    "text": "users and so a whole bunch more data and now my data doesn't fit onto two charts anymore so i'm going to use a feature of",
    "start": "1338400",
    "end": "1344159"
  },
  {
    "text": "a test called reshard to uh live without taking down time take my",
    "start": "1344159",
    "end": "1350000"
  },
  {
    "text": "data that's on two shards and split it across four shards and we're going to see what that looks like",
    "start": "1350000",
    "end": "1356000"
  },
  {
    "text": "inside of et admin um so that is what these other four shards are for",
    "start": "1356000",
    "end": "1362159"
  },
  {
    "text": "uh so i'm going to go over to workflows which is where things are going to show up and the ap has made my",
    "start": "1362159",
    "end": "1369520"
  },
  {
    "text": "thing tiny that looks sufficiently large",
    "start": "1369520",
    "end": "1375440"
  },
  {
    "text": "um okay so the first thing that i'm going to do is i've done a bunch of prep work i've",
    "start": "1375440",
    "end": "1381600"
  },
  {
    "text": "inserted some test data i've spun everything up i've created all the shards because it takes forever",
    "start": "1381600",
    "end": "1387360"
  },
  {
    "text": "so i'm going to go right to it and i'm going to create two workflows one for",
    "start": "1387360",
    "end": "1392559"
  },
  {
    "text": "going from dash 80 to 40 uh boy",
    "start": "1392559",
    "end": "1397679"
  },
  {
    "text": "40 and 40 80 and then a second workflow going from 80",
    "start": "1397679",
    "end": "1403120"
  },
  {
    "text": "to 80 00 to zero to the end of the key range",
    "start": "1403120",
    "end": "1408559"
  },
  {
    "text": "um you can reshard the entire key space with a single workflow we have found in our",
    "start": "1408559",
    "end": "1414720"
  },
  {
    "text": "personal experience that when you are operating on sufficiently large key spaces and you have",
    "start": "1414720",
    "end": "1420159"
  },
  {
    "text": "dozens to hundreds of shards it becomes trickier to manage with like retrying and restarting and",
    "start": "1420159",
    "end": "1426400"
  },
  {
    "text": "doing cut overs and piecemeal so we like to create one workflow per source to",
    "start": "1426400",
    "end": "1431840"
  },
  {
    "text": "destination which is why i'm doing this way",
    "start": "1431840",
    "end": "1437559"
  },
  {
    "text": "there we go so that's the first workflow created and",
    "start": "1445039",
    "end": "1450159"
  },
  {
    "text": "i can hop over to vtmn and",
    "start": "1450159",
    "end": "1454799"
  },
  {
    "text": "reload here's the one that's up and running and here's the one that is coming up",
    "start": "1455200",
    "end": "1461520"
  },
  {
    "text": "so if i click in here i can see that the stream is lagged a little bit because we just finished copying data over",
    "start": "1461520",
    "end": "1469279"
  },
  {
    "text": "so this workflow is actually composed of two streams one going from",
    "start": "1469279",
    "end": "1474880"
  },
  {
    "text": "the source to the left-hand side and the other going from the source to the right-hand side",
    "start": "1474880",
    "end": "1481039"
  },
  {
    "text": "and if i look at one of these streams",
    "start": "1481039",
    "end": "1486720"
  },
  {
    "text": "oh that's the json view uh we don't want that i can go to a tablet",
    "start": "1486720",
    "end": "1493039"
  },
  {
    "text": "and we can see that there's no real qps but there is v replication qps which is to be expected",
    "start": "1493039",
    "end": "1498480"
  },
  {
    "text": "and now if i throw some traffic",
    "start": "1498480",
    "end": "1505360"
  },
  {
    "text": "there we go oh no am i in the wrong directory",
    "start": "1506000",
    "end": "1511559"
  },
  {
    "text": "so we'll send some rows over here and now we can see qps skyrockets as expected i'm going to kill that before",
    "start": "1519679",
    "end": "1525840"
  },
  {
    "text": "it ruins my demo and so at this point we now have two streams",
    "start": "1525840",
    "end": "1532240"
  },
  {
    "text": "running or two workflows running four streams total copying data from our sources to our destinations and now we",
    "start": "1532240",
    "end": "1538159"
  },
  {
    "text": "are going to wait for things to catch up and while we do that malcolm is going to fill us in on some of the work that's",
    "start": "1538159",
    "end": "1543440"
  },
  {
    "text": "going to tie in here",
    "start": "1543440",
    "end": "1546320"
  },
  {
    "text": "thank you for that wonderful introduction and demo of vt admin andrew hi everybody my name is malcolm mckinsey",
    "start": "1550320",
    "end": "1557279"
  },
  {
    "text": "and i'm a software engineer at slack on the data stores team and a recent graduate of tufts university class of",
    "start": "1557279",
    "end": "1562960"
  },
  {
    "text": "2020 go jumbos this past year i've had the fortunate opportunity of being able to contribute",
    "start": "1562960",
    "end": "1569039"
  },
  {
    "text": "to the tests mainly focusing on the resharding workflows and the entire resharding experience most recently i've",
    "start": "1569039",
    "end": "1575279"
  },
  {
    "text": "been focusing on the vdif step of that resharding experience vdif",
    "start": "1575279",
    "end": "1580480"
  },
  {
    "text": "is the only optional step of a resharding experience but is immensely critical when wanting to validate that",
    "start": "1580480",
    "end": "1586880"
  },
  {
    "text": "the prior steps of your resharding workflow have ran to completion and ran as intended what vdif does is that it",
    "start": "1586880",
    "end": "1594320"
  },
  {
    "text": "takes a diff between your target and source charts to actually validate that the data that you've copied over is",
    "start": "1594320",
    "end": "1600799"
  },
  {
    "text": "there and it validates if that the data thank you",
    "start": "1600799",
    "end": "1606320"
  },
  {
    "start": "1605000",
    "end": "1639000"
  },
  {
    "text": "and it validates that the there is no extra data that has been copied over any data that you did not intend to be",
    "start": "1606320",
    "end": "1613679"
  },
  {
    "text": "in your destination charts at slac we currently",
    "start": "1613679",
    "end": "1619679"
  },
  {
    "text": "manage one of the largest for test clusters in production and because of that due to our scale we do end up in",
    "start": "1619679",
    "end": "1627520"
  },
  {
    "text": "certain situations where we run into certain bottlenecks or unintended situations that is only due to our scale",
    "start": "1627520",
    "end": "1634400"
  },
  {
    "text": "most recently that has been noticed in vdif",
    "start": "1634400",
    "end": "1639440"
  },
  {
    "start": "1639000",
    "end": "1813000"
  },
  {
    "text": "if we step into v diff specifically uh what happens is that v diff mainly uh",
    "start": "1639440",
    "end": "1646399"
  },
  {
    "text": "all of the work happens in the vtc tld all of the most of the load is generated on the vtc tld and also you need to run",
    "start": "1646399",
    "end": "1654799"
  },
  {
    "text": "one v diff per source shard so if you're splitting one shard to two or two to",
    "start": "1654799",
    "end": "1660399"
  },
  {
    "text": "four you'll need uh one v diff or two and two v diffs respectively",
    "start": "1660399",
    "end": "1665919"
  },
  {
    "text": "and at slack when we run a vdif in production it normally ends up around",
    "start": "1665919",
    "end": "1671039"
  },
  {
    "text": "averaging seven hours so if you're running one vdif things are okay you can run it and",
    "start": "1671039",
    "end": "1676480"
  },
  {
    "text": "finish it by the end of the workday but as that number increases you need to",
    "start": "1676480",
    "end": "1681600"
  },
  {
    "text": "run these v-diffs either sequentially so back-to-back or you can run them manually in parallel running them",
    "start": "1681600",
    "end": "1689039"
  },
  {
    "text": "sequentially leads you into the issue of you run one v-diff you come back seven hours later hopefully it's done and then",
    "start": "1689039",
    "end": "1695919"
  },
  {
    "text": "you run the you run the next one that obviously becomes a",
    "start": "1695919",
    "end": "1701440"
  },
  {
    "text": "more uh that be obviously becomes more unmanageable as the amount of vidifs you need to run increases",
    "start": "1701440",
    "end": "1707440"
  },
  {
    "text": "and then you step into the world of okay let me try to run these v diffs uh in parallel",
    "start": "1707440",
    "end": "1714240"
  },
  {
    "text": "when you want to run these vdifs in parallel since they're blocking calls what normally happens is that you end up",
    "start": "1714240",
    "end": "1719600"
  },
  {
    "text": "running multiple screens or multiple t-muxes and a common scenario at slack is that we'll",
    "start": "1719600",
    "end": "1726000"
  },
  {
    "text": "be splitting more than one or two shards and in that case you'll be juggling",
    "start": "1726000",
    "end": "1732880"
  },
  {
    "text": "maybe six plus t-muxes and screen sessions at one time but you're also now doing this in",
    "start": "1732880",
    "end": "1738399"
  },
  {
    "text": "parallel and one v diff is already generating load on a vtc tld so what",
    "start": "1738399",
    "end": "1743840"
  },
  {
    "text": "happens when you want to run two three or four you can generate so much load on a vtc tld that you could be in risk of",
    "start": "1743840",
    "end": "1750720"
  },
  {
    "text": "actually bringing that entire service down which is what you don't want",
    "start": "1750720",
    "end": "1756320"
  },
  {
    "text": "so what will then happen after that is okay maybe you're running more than one vtc tld you're now juggling multiple",
    "start": "1756320",
    "end": "1763440"
  },
  {
    "text": "screens or multiple team x's on multiple different hosts and that entire process",
    "start": "1763440",
    "end": "1768799"
  },
  {
    "text": "is cumbersome at times so what ended up happening over the past six months",
    "start": "1768799",
    "end": "1774799"
  },
  {
    "text": "uh while i've been at slack as i have worked on an in-house tool uh",
    "start": "1774799",
    "end": "1780960"
  },
  {
    "text": "built on top of our existing vtops uh binary adslock which allows us to",
    "start": "1780960",
    "end": "1787919"
  },
  {
    "text": "interact with our topology with our cluster and it will orchestrate vdifs and it",
    "start": "1787919",
    "end": "1794880"
  },
  {
    "text": "will work with our concurrency limit and it will allow us to",
    "start": "1794880",
    "end": "1800080"
  },
  {
    "text": "spread out all of the load on all of our vtc tlds and also allow vdiffs to run",
    "start": "1800080",
    "end": "1805200"
  },
  {
    "text": "back to back so even if vita finishes at 3 a.m in the middle of the night it'll",
    "start": "1805200",
    "end": "1810559"
  },
  {
    "text": "kick off the next one on the list so this is sort of what the output will look like uh you'll end up enqueuing",
    "start": "1810559",
    "end": "1818080"
  },
  {
    "start": "1813000",
    "end": "1917000"
  },
  {
    "text": "your vdif you specify your source cell your target cell what tablet types do you want to use for",
    "start": "1818080",
    "end": "1824480"
  },
  {
    "text": "this vdif and the workflows you actually want to run we have our",
    "start": "1824480",
    "end": "1829840"
  },
  {
    "text": "service discovery which will end up finding these vtc tlds and from there you start your vdif workers and the vdif",
    "start": "1829840",
    "end": "1836880"
  },
  {
    "text": "workers are tied to a specific vtc tld and all they do is grab one workflow and",
    "start": "1836880",
    "end": "1843360"
  },
  {
    "text": "then send that request to a vtctld and when it's done it will grab another one",
    "start": "1843360",
    "end": "1848640"
  },
  {
    "text": "you can specify the number of vt uh v diff workers you want and with that",
    "start": "1848640",
    "end": "1854960"
  },
  {
    "text": "you can also increase the number yeah you can increase the number of vdif workers and each vdif worker will take",
    "start": "1854960",
    "end": "1860159"
  },
  {
    "text": "one workflow um yep and so i will actually spin up",
    "start": "1860159",
    "end": "1867760"
  },
  {
    "text": "a couple vdifs right now in order to show you how the tool works",
    "start": "1867760",
    "end": "1873519"
  },
  {
    "text": "so right here as andrew said previously we have done some of the",
    "start": "1873519",
    "end": "1879039"
  },
  {
    "text": "work before so these commands are already here but in this example we have vtops vdif",
    "start": "1879039",
    "end": "1886159"
  },
  {
    "text": "workflow i'll copy this whole line",
    "start": "1886159",
    "end": "1890679"
  },
  {
    "text": "and we should be able to run this right here",
    "start": "1893279",
    "end": "1898880"
  },
  {
    "text": "and so right now the vdif has started on these vtc tlds that we have spun up for",
    "start": "1900480",
    "end": "1906399"
  },
  {
    "text": "this demo and currently it's running and so while this is running we can talk about what we are looking",
    "start": "1906399",
    "end": "1913679"
  },
  {
    "text": "forward to for the future of vdif uh the future goals of v diff is",
    "start": "1913679",
    "end": "1920080"
  },
  {
    "text": "actually to move v diff from the vtc tld layer down to the tablet layer in order",
    "start": "1920080",
    "end": "1925919"
  },
  {
    "text": "to uh relieve some of the pressure from those vtc tlds and allow vdif to be a lot more",
    "start": "1925919",
    "end": "1932720"
  },
  {
    "text": "scalable than it currently is which will then make the entire resharding",
    "start": "1932720",
    "end": "1938000"
  },
  {
    "text": "experience a lot better and a lot more streamlined",
    "start": "1938000",
    "end": "1942720"
  },
  {
    "text": "and if we look back here we can see",
    "start": "1944159",
    "end": "1950159"
  },
  {
    "text": "is that in the output of this command v diff for media 80 dash is completed uh",
    "start": "1950159",
    "end": "1956320"
  },
  {
    "text": "v diff for media dash 80 has completed and in this disfound false shows you",
    "start": "1956320",
    "end": "1962559"
  },
  {
    "text": "what vtc tld was used and it also shows you the output so the v diff will tell",
    "start": "1962559",
    "end": "1968000"
  },
  {
    "text": "you how many rows were processed how many rows were matching if there were extra rows as i mentioned before and",
    "start": "1968000",
    "end": "1973760"
  },
  {
    "text": "this is grouped by workflow and then by table",
    "start": "1973760",
    "end": "1978080"
  },
  {
    "text": "and so now since vdf is run and vdiff has validated that our copying of data",
    "start": "1979600",
    "end": "1984960"
  },
  {
    "text": "has gone successfully and that there are no extra rows and that everything is matching as uh expected we can now cut",
    "start": "1984960",
    "end": "1991360"
  },
  {
    "text": "over and so if i step right here",
    "start": "1991360",
    "end": "1998720"
  },
  {
    "text": "the first command will be to switch traffic on read-onlys and replicas",
    "start": "2000640",
    "end": "2007200"
  },
  {
    "text": "traffic switch traffic was successful so now if we take a look back in vt admin",
    "start": "2015039",
    "end": "2020559"
  },
  {
    "text": "we should see a difference inside of our",
    "start": "2020559",
    "end": "2025760"
  },
  {
    "text": "step into the key space and we step into media you should see",
    "start": "2025760",
    "end": "2033840"
  },
  {
    "text": "okay so now after that's successful we will switch the primaries",
    "start": "2034320",
    "end": "2041480"
  },
  {
    "text": "switch traffic was successful and now i'll read your switch uh right's",
    "start": "2051359",
    "end": "2056720"
  },
  {
    "text": "not switched that was the start state and the current state now is all reads of switch and rights are switched so the",
    "start": "2056720",
    "end": "2061839"
  },
  {
    "text": "destination shards should be taking uh they would be taking traffic right now so now if we head here",
    "start": "2061839",
    "end": "2068960"
  },
  {
    "text": "and refresh the page we can see now that the original uh",
    "start": "2068960",
    "end": "2074878"
  },
  {
    "text": "source shards dash 80 and yeah",
    "start": "2074879",
    "end": "2080320"
  },
  {
    "text": "the the original source shards are no longer serving and the destination shards are now the serving",
    "start": "2080320",
    "end": "2087118"
  },
  {
    "text": "charts here and so with all of that said",
    "start": "2087119",
    "end": "2093040"
  },
  {
    "start": "2092000",
    "end": "2391000"
  },
  {
    "text": "thank you for your time i appreciate everyone who was able to make it here i appreciate everyone who was able to meet",
    "start": "2093040",
    "end": "2099520"
  },
  {
    "text": "with us online and watch our talk uh feel free to check out the test docs dot",
    "start": "2099520",
    "end": "2104880"
  },
  {
    "text": "dot io docs uh the github repository and the link in the slides as well as if you",
    "start": "2104880",
    "end": "2110480"
  },
  {
    "text": "have any uh questions or after this talk feel free to message us in slack at",
    "start": "2110480",
    "end": "2116320"
  },
  {
    "text": "thetest.slock.com thank you",
    "start": "2116320",
    "end": "2120960"
  },
  {
    "text": "i guess we are out of time or do we have time for questions",
    "start": "2125119",
    "end": "2130760"
  },
  {
    "text": "before we take that question we also do have office hours tomorrow at 11 30 pacific uh for people that",
    "start": "2141599",
    "end": "2147920"
  },
  {
    "text": "think of questions later",
    "start": "2147920",
    "end": "2151640"
  },
  {
    "text": "does the test require a storage manager like rook or fort worts or does it manage the local storage on each node",
    "start": "2156800",
    "end": "2164640"
  },
  {
    "text": "um i'll take that one yeah with us does not require a storage sorry",
    "start": "2164640",
    "end": "2170400"
  },
  {
    "text": "so the question was does witness require a storage manager like rook or portworx",
    "start": "2170400",
    "end": "2176240"
  },
  {
    "text": "or uh does it manage local storage on its own with us does not require a storage",
    "start": "2176240",
    "end": "2182480"
  },
  {
    "text": "manager like seph or like rook or portworx it can work with cloud storage or",
    "start": "2182480",
    "end": "2189280"
  },
  {
    "text": "network attached storage or local storage in",
    "start": "2189280",
    "end": "2194400"
  },
  {
    "text": "when when running with the kubernetes operator most people use cloud storage",
    "start": "2194400",
    "end": "2199920"
  },
  {
    "text": "like amazon i'm blanking on the ebs",
    "start": "2199920",
    "end": "2207200"
  },
  {
    "text": "or gcs google cloud storage but people have also run with us with",
    "start": "2207200",
    "end": "2212839"
  },
  {
    "text": "ceph so uh storage managers are optional",
    "start": "2212839",
    "end": "2218320"
  },
  {
    "text": "any questions from the room what was the other question you said there are two questions",
    "start": "2219839",
    "end": "2226400"
  },
  {
    "text": "so the question was uh for vt admin does it talk to the vtc tlb or does it ever talk to the topo",
    "start": "2240640",
    "end": "2247119"
  },
  {
    "text": "directly and uh it only ever talks to the vtc tlv there's a couple of things that go",
    "start": "2247119",
    "end": "2252320"
  },
  {
    "text": "through the tablets or sorry for through the vt gates um because vtgates have a",
    "start": "2252320",
    "end": "2257680"
  },
  {
    "text": "different view of the health of the tablet so that's what's powering those those tablets serving not serving",
    "start": "2257680",
    "end": "2263359"
  },
  {
    "text": "um if i go like this this uh one two three fourth",
    "start": "2263359",
    "end": "2269040"
  },
  {
    "text": "column um that's coming from the gates because we are interested in what the gates think about tablet health because",
    "start": "2269040",
    "end": "2275680"
  },
  {
    "text": "it affects query serving um versus what's in the topo which is delayed basically",
    "start": "2275680",
    "end": "2281920"
  },
  {
    "text": "but other than that everything goes directly to the vtc dld which then proxies the topo",
    "start": "2281920",
    "end": "2289559"
  },
  {
    "text": "right now",
    "start": "2296480",
    "end": "2299640"
  },
  {
    "text": "so commands have to be run from the cli uh vtm is mostly read-only is the plan",
    "start": "2303920",
    "end": "2310640"
  },
  {
    "text": "to to make it more powerful than just just a read-only a very good read-only",
    "start": "2310640",
    "end": "2315760"
  },
  {
    "text": "ui and yes that that's the plan that's where we're going the vtc that's part of the vtct ld parity is the vtc dld ui",
    "start": "2315760",
    "end": "2322880"
  },
  {
    "text": "lets you do all of those uh like destructive administrative operations and we're we're getting there",
    "start": "2322880",
    "end": "2330880"
  },
  {
    "text": "okay buy stream",
    "start": "2335359",
    "end": "2340838"
  },
  {
    "text": "we support 5.7 and 8.0 and the minor versions of those two major versions we used to",
    "start": "2352240",
    "end": "2359280"
  },
  {
    "text": "support 5.6 we don't really support it anymore because it's been and end of life",
    "start": "2359280",
    "end": "2365839"
  },
  {
    "text": "with us can work with the percona distribution of mysql oracle community edition enterprise",
    "start": "2365839",
    "end": "2372960"
  },
  {
    "text": "edition mariadb almost all the variants",
    "start": "2372960",
    "end": "2379640"
  },
  {
    "text": "okay i think that's it thank you everyone",
    "start": "2386480",
    "end": "2392279"
  }
]