[
  {
    "text": "hi everyone Uh welcome to this uh CubeCon talk This is going to be a working group update So uh we hope this",
    "start": "80",
    "end": "7040"
  },
  {
    "text": "is going to be rather a short talk and just an introduction for those that doesn't know the working group exist and",
    "start": "7040",
    "end": "13840"
  },
  {
    "text": "hopefully we will get question for you mostly on like if you require new features or you if you have any pain",
    "start": "13840",
    "end": "19920"
  },
  {
    "text": "points that you would like the working group to work on So uh for starters uh",
    "start": "19920",
    "end": "24960"
  },
  {
    "text": "who are we Hi everyone Uh thank you for being here My name is Yan I'm a senior",
    "start": "24960",
    "end": "30480"
  },
  {
    "text": "principal software engineer at Red Hat Uh working on our hybrid cloud AI platform I'm one of the co-chairs for",
    "start": "30480",
    "end": "37760"
  },
  {
    "text": "working group serving and also a project lead for Argo and Coupflow and also",
    "start": "37760",
    "end": "43360"
  },
  {
    "text": "maintain a couple of projects Uh one of the most active project we are working on is a llama stack project So feel free",
    "start": "43360",
    "end": "50000"
  },
  {
    "text": "to take a look at the project if you are interested and I also uh authored a",
    "start": "50000",
    "end": "55280"
  },
  {
    "text": "couple of books in case you are interested in reading as well Yeah",
    "start": "55280",
    "end": "61520"
  },
  {
    "text": "thank you Joan and uh myself uh Eduardo Arango also a working group chair for",
    "start": "61520",
    "end": "67600"
  },
  {
    "text": "the working group Serbin and I've been working in distributed uh systems using",
    "start": "67600",
    "end": "73119"
  },
  {
    "text": "containers for a long time now and um I work mostly on the low-level container",
    "start": "73119",
    "end": "78400"
  },
  {
    "text": "runtime bits and now I'm working on projects related to CDI and the array",
    "start": "78400",
    "end": "84000"
  },
  {
    "text": "which are also of big importance for working group serving which we will talk in a",
    "start": "84000",
    "end": "90119"
  },
  {
    "text": "Right So what is working group serbing So uh actually just a minute ago I was",
    "start": "90119",
    "end": "95680"
  },
  {
    "text": "thinking that we forgot one slide and is uh is one year now So working group",
    "start": "95680",
    "end": "101439"
  },
  {
    "text": "serbing was born in uh CubeCon Europe last year Paris and it basically was",
    "start": "101439",
    "end": "107920"
  },
  {
    "text": "born out of the necessity where everyone wanted to uh collaborate on Serbian",
    "start": "107920",
    "end": "114720"
  },
  {
    "text": "projects and Serbian needs for Kubernetes there are some gaps in Kubernetes and during one of the",
    "start": "114720",
    "end": "120640"
  },
  {
    "text": "unconference sessions at the at that time contributor summit now maintainer",
    "start": "120640",
    "end": "125840"
  },
  {
    "text": "summit the name changings that we're going on uh we decided we needed two",
    "start": "125840",
    "end": "131360"
  },
  {
    "text": "working groups so last year the working group device management and working group serving uh was born and now it's a",
    "start": "131360",
    "end": "138400"
  },
  {
    "text": "full one year of the working group serving so we forgot the uh one year celebration is slide yeah but yeah happy",
    "start": "138400",
    "end": "144720"
  },
  {
    "text": "birthday to working group serving So the working group observing is basically uh it has three main goals Uh",
    "start": "144720",
    "end": "152080"
  },
  {
    "text": "one goal is to enhance Kubernetes controllers meaning we are proposing",
    "start": "152080",
    "end": "157120"
  },
  {
    "text": "APIs we are proposing uh changes to main Kubernetes controllers and also",
    "start": "157120",
    "end": "162720"
  },
  {
    "text": "low-level components as we are going to talk about things like DRA So we can make it easier for everyone to run uh",
    "start": "162720",
    "end": "170959"
  },
  {
    "text": "sering workloads on Kubernetes The second thing that the working group focus on and it's a a project that Joan",
    "start": "170959",
    "end": "177920"
  },
  {
    "text": "is going to deep dive later is uh investigate or research uh orchestration",
    "start": "177920",
    "end": "183519"
  },
  {
    "text": "and scalability and why the investigate work Uh we are working on a project that",
    "start": "183519",
    "end": "188560"
  },
  {
    "text": "is called the inference perf or is basically a benchmark tool So we can",
    "start": "188560",
    "end": "194159"
  },
  {
    "text": "really understand what is happening when we run uh these large language models because uh last year when we started the",
    "start": "194159",
    "end": "200800"
  },
  {
    "text": "working group serin we had a couple of very long meetings discussing what should we monitor when knowing how to",
    "start": "200800",
    "end": "207920"
  },
  {
    "text": "autoscale a cluster that is running uh large language models and really we",
    "start": "207920",
    "end": "213840"
  },
  {
    "text": "couldn't agree on one topic and then we knew okay if we cannot agree on a topic we really need to deep dive and do",
    "start": "213840",
    "end": "220720"
  },
  {
    "text": "benchmarks to know what are the key components that we need to monitor in a cluster so we can do",
    "start": "220720",
    "end": "227400"
  },
  {
    "text": "autoscaling and also for this optimize resource sharing for serving workloads",
    "start": "227400",
    "end": "232640"
  },
  {
    "text": "Uh here is where uh we are working on the low-level container runtimes uh DRA",
    "start": "232640",
    "end": "238480"
  },
  {
    "text": "and very exposing ways for pots to share uh resources like two pot sharing a GPU",
    "start": "238480",
    "end": "245040"
  },
  {
    "text": "or pot uh communicating across multiple hosts for things like MPI communication",
    "start": "245040",
    "end": "251519"
  },
  {
    "text": "So these are the three main goals for the working group serving",
    "start": "251519",
    "end": "256560"
  },
  {
    "text": "Yeah And working group serving is led by multiple different companies Uh we have",
    "start": "256560",
    "end": "261680"
  },
  {
    "text": "four co-chairs from Google Cloud Red Hat Nvidia and Bite Dance And uh besides us",
    "start": "261680",
    "end": "267600"
  },
  {
    "text": "we have more than 330 people uh on Slack So you're welcome to join us If you ever",
    "start": "267600",
    "end": "274160"
  },
  {
    "text": "have any ideas to contributing to uh the improvement to serving workloads feel free to join us and send us feedback",
    "start": "274160",
    "end": "282320"
  },
  {
    "text": "Yeah And this slide uh has a list of um uh",
    "start": "282320",
    "end": "290000"
  },
  {
    "text": "talks that we had uh from the community members last year Um the the very top one is the Kubernetes podcast Eduardo",
    "start": "290000",
    "end": "297440"
  },
  {
    "text": "and I did uh a while ago and the the rest of them are basically from KubeCon",
    "start": "297440",
    "end": "303360"
  },
  {
    "text": "last year and there were a lot of talks related to some of the initiatives from",
    "start": "303360",
    "end": "309199"
  },
  {
    "text": "these working groups and talking about different projects uh in more details uh from this working group So feel free to",
    "start": "309199",
    "end": "316400"
  },
  {
    "text": "check them out as well And oh it's okay as as Joan was saying",
    "start": "316400",
    "end": "324400"
  },
  {
    "text": "uh the leadership has four uh working group chairs but really since we have",
    "start": "324400",
    "end": "330320"
  },
  {
    "text": "hundreds of people joining the calls we ended up deciding for work streams So we",
    "start": "330320",
    "end": "335680"
  },
  {
    "text": "have four main work streams uh because it was we were running into hours longs",
    "start": "335680",
    "end": "341680"
  },
  {
    "text": "meeting every week and to this day we are still running weekly meetings where we run the whole hour discussing about",
    "start": "341680",
    "end": "348560"
  },
  {
    "text": "these topics So we we ended up going for work streams updates Uh the 2024 report",
    "start": "348560",
    "end": "356000"
  },
  {
    "text": "uh the for working group report it's out So if if you want to check it out there is like a a whole abstract of what we",
    "start": "356000",
    "end": "363199"
  },
  {
    "text": "did last year all the initiatives that we're working on the projects that we are pushing uh the repos that have been",
    "start": "363199",
    "end": "369919"
  },
  {
    "text": "created by uh the contributors to the working group So uh just going to take a",
    "start": "369919",
    "end": "375280"
  },
  {
    "text": "moment for everyone to take a picture there and basically if you read the",
    "start": "375280",
    "end": "380400"
  },
  {
    "text": "report you don't need to be in this talk So you're welcome Uh but yeah this talk is basically a a live presentation of",
    "start": "380400",
    "end": "387680"
  },
  {
    "text": "what the report was and it's nice because the report was review and we got",
    "start": "387680",
    "end": "392800"
  },
  {
    "text": "a lot of contributions from everyone that participates in the working group not just the chairs but we got uh",
    "start": "392800",
    "end": "399039"
  },
  {
    "text": "participation from everyone on like how to structure the report So it's a it's a very active working group Yeah I also",
    "start": "399039",
    "end": "404319"
  },
  {
    "text": "want to mention like it's a collective effort from everyone in the working group including different sub project",
    "start": "404319",
    "end": "410880"
  },
  {
    "text": "leads and work uh work stream leads So thank you everyone for helping out with the report",
    "start": "410880",
    "end": "418400"
  },
  {
    "text": "So the first work stream is called autoscaling and here this slide can be",
    "start": "418400",
    "end": "423599"
  },
  {
    "text": "split in two So the first part is uh what I was pointing on benchmarking So",
    "start": "423599",
    "end": "429039"
  },
  {
    "text": "for autoscaling we are uh deep diving on researching which metrics we should",
    "start": "429039",
    "end": "435199"
  },
  {
    "text": "really monitor when autoscaling So uh Joan is going to deep dive into what is the benchmarking tool that we are",
    "start": "435199",
    "end": "441840"
  },
  {
    "text": "working on at the the work group And the second is uh a very interesting topic",
    "start": "441840",
    "end": "447039"
  },
  {
    "text": "that really takes me back years even almost before Kubernetes and is the",
    "start": "447039",
    "end": "452400"
  },
  {
    "text": "challenges with OCI images So for those who know uh some models are very big",
    "start": "452400",
    "end": "458560"
  },
  {
    "text": "images right like we're talking about even hundreds of gigabytes that we have to pull to run a model So right now at",
    "start": "458560",
    "end": "465840"
  },
  {
    "text": "the working group we are discussing again what years ago was being called uh",
    "start": "465840",
    "end": "471199"
  },
  {
    "text": "volume containers and is basically containers that are not uh a running",
    "start": "471199",
    "end": "476560"
  },
  {
    "text": "system but are mostly uh for data and now we have a image volume source ke to",
    "start": "476560",
    "end": "482319"
  },
  {
    "text": "address this right like we want to use containers as a way to move data and we want to just treat the these models as",
    "start": "482319",
    "end": "489120"
  },
  {
    "text": "data because they are getting very very big So these are two of the topics that the autoscaling workstream is working",
    "start": "489120",
    "end": "495720"
  },
  {
    "text": "on Uh the second workstream is the multi-host and multi-node uh workstream",
    "start": "495720",
    "end": "502240"
  },
  {
    "text": "where as I said we uh focus on how to better run distributed workloads uh",
    "start": "502240",
    "end": "507759"
  },
  {
    "text": "things like MPI or things like we are addressing with the leader working set and BLM and KSERF Uh so three of the key",
    "start": "507759",
    "end": "517440"
  },
  {
    "text": "milestones that we reached last year was uh not the full like it's not the full",
    "start": "517440",
    "end": "524159"
  },
  {
    "text": "release of leader working set uh 0.3 or 0.5 was done by the working group",
    "start": "524159",
    "end": "529519"
  },
  {
    "text": "serving but it's more like we participated with feature requests and reviewing some PRs and proposing things",
    "start": "529519",
    "end": "536480"
  },
  {
    "text": "to the leader working set uh team or or the contributors to the leader working set saying like hey please can you",
    "start": "536480",
    "end": "542320"
  },
  {
    "text": "accommodate this and that for us it will make the life of the serbing community better So it's that's what I'm trying to",
    "start": "542320",
    "end": "549519"
  },
  {
    "text": "say here also uh working group serbing and the BLM uh community join it for",
    "start": "549519",
    "end": "555440"
  },
  {
    "text": "testing uh and now we are also working for disagregating orchestration so we can do like multi-pnd testing right and",
    "start": "555440",
    "end": "564000"
  },
  {
    "text": "for ker we have Jan that is one of the leaders of we have been also proposing",
    "start": "564000",
    "end": "569360"
  },
  {
    "text": "features there so we can uh merge as a community and not just have like disagregated components but KERF is also",
    "start": "569360",
    "end": "576000"
  },
  {
    "text": "watching the discussions questions that happen at the working group Yeah So Kerf supports multihost serving right now but",
    "start": "576000",
    "end": "582480"
  },
  {
    "text": "we are also like trying to improve it going forward like we are proposing some new set of CRDs to better support",
    "start": "582480",
    "end": "590240"
  },
  {
    "text": "multiode serving in term in case users want to customize the behavior",
    "start": "590240",
    "end": "596880"
  },
  {
    "text": "Uh the third work stream it's uh the dynamic resource allocation So it's weird to be called like that So it's",
    "start": "596880",
    "end": "603600"
  },
  {
    "text": "basically a work stream to monitor developments on the new feature that the",
    "start": "603600",
    "end": "608720"
  },
  {
    "text": "exciting feature of Kubernetes that that is DRA and here we basically are just",
    "start": "608720",
    "end": "613760"
  },
  {
    "text": "sending letters to the working group uh device management as you can see it there uh and we have uh John uh Bellamic",
    "start": "613760",
    "end": "622720"
  },
  {
    "text": "and u Sergey joining the working group serbing meetings so we can have a a",
    "start": "622720",
    "end": "628000"
  },
  {
    "text": "weekly communication on what do we need from the serbing workloads so they can",
    "start": "628000",
    "end": "633120"
  },
  {
    "text": "better accommodate the RA as the RA right now is under change right Like it's still a beta feature So it is the",
    "start": "633120",
    "end": "640720"
  },
  {
    "text": "right moment to push features before it goes ZA because once it's ZA it's going to be very very hard to make",
    "start": "640720",
    "end": "647279"
  },
  {
    "text": "modifications to the RA So our focus during 2024 and during 2025 is pushing",
    "start": "647279",
    "end": "653279"
  },
  {
    "text": "as much features and requests as we can into the DRRA uh the working group",
    "start": "653279",
    "end": "658640"
  },
  {
    "text": "device management So once the RA goes uh GA hopefully in 1.34 in December this",
    "start": "658640",
    "end": "666160"
  },
  {
    "text": "year uh it will accommodate as much of features that we need from the working",
    "start": "666160",
    "end": "671680"
  },
  {
    "text": "group uh serving community and uh something that we are also working on very heavily and I hope",
    "start": "671680",
    "end": "678560"
  },
  {
    "text": "to participate more this year is in uh device failure handling and resilient workload management So uh we still in",
    "start": "678560",
    "end": "686519"
  },
  {
    "text": "Kubernetes uh mostly from the hardware part we don't communicate very well to",
    "start": "686519",
    "end": "691920"
  },
  {
    "text": "the scheduleuler when a node is not healthy So uh this uh work stream from",
    "start": "691920",
    "end": "697519"
  },
  {
    "text": "the working group is trying to push uh new features and new APIs so we can",
    "start": "697519",
    "end": "702880"
  },
  {
    "text": "better report to the Kubernetes when a GPU is unhealthy or misbehaving So it",
    "start": "702880",
    "end": "708320"
  },
  {
    "text": "can better reroute a workload and we can save some time and not have a workload on a GPU that is not healthy and just",
    "start": "708320",
    "end": "715040"
  },
  {
    "text": "trying to run and and hang in there And the last work stream is orchestration on",
    "start": "715040",
    "end": "721519"
  },
  {
    "text": "this I'm just basically going to go very fast because uh Jan is going to deep dive on it Uh one initiative uh coming",
    "start": "721519",
    "end": "728399"
  },
  {
    "text": "out of the working group is the gateway API inference extension uh for short gee",
    "start": "728399",
    "end": "734639"
  },
  {
    "text": "uh and it already has uh a first release so a 0.1 release and uh it is already",
    "start": "734639",
    "end": "740560"
  },
  {
    "text": "showing improvements So it's it shows that this working group uh and everyone",
    "start": "740560",
    "end": "745600"
  },
  {
    "text": "that has been contributing to it are already providing value to the community by joining and as as uh Jan mentioned is",
    "start": "745600",
    "end": "752959"
  },
  {
    "text": "multiple companies that are contributing to this So in just one year we managed to create a a a product that is helping",
    "start": "752959",
    "end": "759839"
  },
  {
    "text": "everyone and making things uh more performant Yeah Next I'll talk about are some of",
    "start": "759839",
    "end": "766720"
  },
  {
    "text": "the initiatives that already mentioned One of them is the inference proof project This is a collaboration between",
    "start": "766720",
    "end": "774000"
  },
  {
    "text": "um among different companies including IBM Red Hat Google Cloud and Nvidia And",
    "start": "774000",
    "end": "779760"
  },
  {
    "text": "we anticipate more uh people uh to contribute and welcome you to contribute as well It's a project that we uh that's",
    "start": "779760",
    "end": "787839"
  },
  {
    "text": "purposed to um be a standardized tool for benchmarking uh as a library uh you",
    "start": "787839",
    "end": "793519"
  },
  {
    "text": "can run it uh on anywhere like on Kubernetes or independently and it's",
    "start": "793519",
    "end": "799839"
  },
  {
    "text": "aiming to solve different uh use cases for benchmarking such as autoscaling uh",
    "start": "799839",
    "end": "804880"
  },
  {
    "text": "Laura use cases with the u gateway API inference extension um etc and it",
    "start": "804880",
    "end": "812880"
  },
  {
    "text": "provides uh the current status is um it provides a Python library for benchmarking workloads and it supports",
    "start": "812880",
    "end": "820240"
  },
  {
    "text": "the VRM model server and support multiple distributions uh with specified",
    "start": "820240",
    "end": "826760"
  },
  {
    "text": "QPS and uh also supports like um CR GPT data set to resemble real world",
    "start": "826760",
    "end": "833920"
  },
  {
    "text": "conversational workloads and if you have any additional use cases any uh other",
    "start": "833920",
    "end": "839839"
  },
  {
    "text": "data set that you can think of feel free to pose it uh propose it in GitHub issue as well and we uh I think the project",
    "start": "839839",
    "end": "848959"
  },
  {
    "text": "also supports report generation with all the metrics are valuable uh and uh aim",
    "start": "848959",
    "end": "855120"
  },
  {
    "text": "to be extensible to add support for different model servers not just for VM but multiple other ones as well from the",
    "start": "855120",
    "end": "862800"
  },
  {
    "text": "community and data sets and different load generators and on the right hand side is",
    "start": "862800",
    "end": "868880"
  },
  {
    "text": "our architecture diagram uh that I'm not going to talk in more details uh but",
    "start": "868880",
    "end": "874639"
  },
  {
    "text": "there's a dedicated um weekly meeting weekly contributors meeting for",
    "start": "874639",
    "end": "879680"
  },
  {
    "text": "inference perf in case you want to join uh the design",
    "start": "879680",
    "end": "885160"
  },
  {
    "text": "discussions and the road map for inference perf um u we're planning to",
    "start": "885160",
    "end": "890800"
  },
  {
    "text": "add support for multiple model servers right like triton tgi and u also like",
    "start": "890800",
    "end": "898079"
  },
  {
    "text": "support integration with different orchestration projects including some of the new ones from the vm community uh",
    "start": "898079",
    "end": "905760"
  },
  {
    "text": "like the production stack and a bricks and also the uh integration with gateway",
    "start": "905760",
    "end": "911199"
  },
  {
    "text": "API inference extension and uh um we also want to add support for different",
    "start": "911199",
    "end": "916800"
  },
  {
    "text": "use cases and data set Um as I mentioned earlier like feel free to propose propose anything that's missing Um we",
    "start": "916800",
    "end": "924959"
  },
  {
    "text": "are also aiming to support multiodel use cases and uh different traffic",
    "start": "924959",
    "end": "930360"
  },
  {
    "text": "distributions and so on And the second initiative and sub",
    "start": "930360",
    "end": "936399"
  },
  {
    "text": "project is the gateway API inference extension u said gee in short uh it's",
    "start": "936399",
    "end": "944320"
  },
  {
    "text": "always a challenge to come up with a new name for a project uh but that's our best effort there uh there's a link to",
    "start": "944320",
    "end": "951440"
  },
  {
    "text": "the pro uh repository and so this project basically are trying to improve",
    "start": "951440",
    "end": "957120"
  },
  {
    "text": "like resource sharing across multiple use cases on a shared foundation model",
    "start": "957120",
    "end": "963199"
  },
  {
    "text": "Uh like for example if you are using Laura adapters this is going to be something that you might want to try And",
    "start": "963199",
    "end": "969839"
  },
  {
    "text": "it improves the tail latency and throughput of RM completion requests",
    "start": "969839",
    "end": "975199"
  },
  {
    "text": "against um uh Kubernetes hosted model servers um using a custom scheduling",
    "start": "975199",
    "end": "983040"
  },
  {
    "text": "algorithm that you can extend um there is a whole design doc for that uh in",
    "start": "983040",
    "end": "989040"
  },
  {
    "text": "case you want to look that's I think that's available in the project repository as well Um it provides a set",
    "start": "989040",
    "end": "996079"
  },
  {
    "text": "of uh declar declarative uh APIs for to route client model names to different",
    "start": "996079",
    "end": "1002320"
  },
  {
    "text": "use cases uh different Laura specific adapter use cases uh so that they can",
    "start": "1002320",
    "end": "1008320"
  },
  {
    "text": "share resources uh more efficiently and there are also end to end",
    "start": "1008320",
    "end": "1014600"
  },
  {
    "text": "observabilities built in uh with different service objective attainment Some of these objectives are custom",
    "start": "1014600",
    "end": "1021759"
  },
  {
    "text": "defined Um that's also part of the design doc as well And",
    "start": "1021759",
    "end": "1028038"
  },
  {
    "text": "um yeah and this also this project also ensures operational guards between",
    "start": "1028039",
    "end": "1033918"
  },
  {
    "text": "different client model names allowing like platform teams to safely serve different uh workloads on the pool of",
    "start": "1033919",
    "end": "1041760"
  },
  {
    "text": "shared resources running on the same foundation model And there's a detailed status report uh that's available in",
    "start": "1041760",
    "end": "1049360"
  },
  {
    "text": "this link Uh we'll be attaching the slides uh in our talk as well in case",
    "start": "1049360",
    "end": "1055360"
  },
  {
    "text": "you want to look into the details And that's uh this is a a",
    "start": "1055360",
    "end": "1061840"
  },
  {
    "text": "diagram that uh this project um basically wants to integrate with many",
    "start": "1061840",
    "end": "1067440"
  },
  {
    "text": "different projects in the ecosystem So everything in the uh blue boxes are",
    "start": "1067440",
    "end": "1073200"
  },
  {
    "text": "already implemented Uh and other components in these diagrams are rather",
    "start": "1073200",
    "end": "1078880"
  },
  {
    "text": "external components and uh they are not implemented yet but we are aiming to do",
    "start": "1078880",
    "end": "1084240"
  },
  {
    "text": "so with together with the community Um there's a link to the road map as",
    "start": "1084240",
    "end": "1090440"
  },
  {
    "text": "well and uh the next initiative from this serving working group is the",
    "start": "1090440",
    "end": "1096080"
  },
  {
    "text": "serving catalog So basically this project aims to provide uh working examples for different model servers",
    "start": "1096080",
    "end": "1104480"
  },
  {
    "text": "different models different deployment patterns uh for example single or multihost inference and as well as",
    "start": "1104480",
    "end": "1111679"
  },
  {
    "text": "different primitives or orchestration frameworks For example if you are just using Kubernetes u deployment you can",
    "start": "1111679",
    "end": "1119600"
  },
  {
    "text": "find examples there Um if you are interested in leader worker set there",
    "start": "1119600",
    "end": "1124640"
  },
  {
    "text": "are also examples here and case of inference inference surveys are still in",
    "start": "1124640",
    "end": "1130360"
  },
  {
    "text": "progress So we are working with the case of community on that one and the this",
    "start": "1130360",
    "end": "1136000"
  },
  {
    "text": "project aims to help uh community of users u to explore different",
    "start": "1136000",
    "end": "1141120"
  },
  {
    "text": "configurations uh recommended configurations uh not that they are not like best practices or best performing",
    "start": "1141120",
    "end": "1148720"
  },
  {
    "text": "but rather our best effort to recommend to the community for running inference",
    "start": "1148720",
    "end": "1154080"
  },
  {
    "text": "workloads So there you can find different patterns that you can tweak and uh and change and as a uh good it",
    "start": "1154080",
    "end": "1163120"
  },
  {
    "text": "could be served as a good reference implementation Um and the implementations are meant to be like uh",
    "start": "1163120",
    "end": "1170799"
  },
  {
    "text": "cloud uh uh provider specific So you there you can also find different",
    "start": "1170799",
    "end": "1176000"
  },
  {
    "text": "configurations uh for different hardware accelerators and different cloud providers uh in the",
    "start": "1176000",
    "end": "1181760"
  },
  {
    "text": "examples as well Um so and a status update you uh the the",
    "start": "1181760",
    "end": "1188720"
  },
  {
    "text": "single host inference using just Kubernetes deployment is available uh",
    "start": "1188720",
    "end": "1194480"
  },
  {
    "text": "for VRM and jet stream serving uh model servers and the multihost inference",
    "start": "1194480",
    "end": "1200160"
  },
  {
    "text": "using leader worker set for VRM is also available and there are also components",
    "start": "1200160",
    "end": "1205919"
  },
  {
    "text": "and examples for HPA stops uh for token latency",
    "start": "1205919",
    "end": "1212320"
  },
  {
    "text": "Next we'll talk about the community Yeah So uh this week we already had uh two",
    "start": "1212320",
    "end": "1218320"
  },
  {
    "text": "events where topics around the working group serving were mentioned So you can look for the recordings afterwards is",
    "start": "1218320",
    "end": "1225120"
  },
  {
    "text": "the cloud native uh cubernetes AI day which was yesterday and also the Qflow",
    "start": "1225120",
    "end": "1230720"
  },
  {
    "text": "summit So uh it's hard to recommend things Yeah there are also talks Yeah",
    "start": "1230720",
    "end": "1237360"
  },
  {
    "text": "they're going to be recorded on YouTube So and moving forward So we have 11",
    "start": "1237360",
    "end": "1242640"
  },
  {
    "text": "talks that are somehow related to running LLMs on Kubernetes this week So",
    "start": "1242640",
    "end": "1248159"
  },
  {
    "text": "the list was too long to put it in a single slide and it would be too small So it's like uh please look for uh LLM",
    "start": "1248159",
    "end": "1255280"
  },
  {
    "text": "related talks uh in the schedule And tomorrow we also have the working group device management update uh where we",
    "start": "1255280",
    "end": "1262559"
  },
  {
    "text": "will be uh showing the progress of DRA and hopefully uh dreaming to have DRA GA",
    "start": "1262559",
    "end": "1269760"
  },
  {
    "text": "by December Yeah and uh we we haven't really checked in details but there are many talks",
    "start": "1269760",
    "end": "1276480"
  },
  {
    "text": "related to some of the sub projects and initiatives that we talked about uh from this working group So if you search by",
    "start": "1276480",
    "end": "1284400"
  },
  {
    "text": "the project name you can probably find some there There were a couple of them like from the AI day yesterday So you",
    "start": "1284400",
    "end": "1290799"
  },
  {
    "text": "might want to check that out as well And this I think this is the last slide",
    "start": "1290799",
    "end": "1296559"
  },
  {
    "text": "and we just want to welcome everyone here to uh or who's watching the",
    "start": "1296559",
    "end": "1302000"
  },
  {
    "text": "recording to participate in our working group activities So as we mentioned we have different work streams but we",
    "start": "1302000",
    "end": "1308799"
  },
  {
    "text": "somehow combine them to a weekly um weekly community meeting but there are",
    "start": "1308799",
    "end": "1314720"
  },
  {
    "text": "also like separate individual um separate contributors meetings for",
    "start": "1314720",
    "end": "1320000"
  },
  {
    "text": "different sub projects and initiatives and we may expand um um the the the the",
    "start": "1320000",
    "end": "1327360"
  },
  {
    "text": "meetings when needed So as we expand to more projects there are likely more",
    "start": "1327360",
    "end": "1333280"
  },
  {
    "text": "separate meetings available and uh we are on the Kubernetes Slack uh with the",
    "start": "1333280",
    "end": "1340320"
  },
  {
    "text": "name WG-erving and feel free to reach out to",
    "start": "1340320",
    "end": "1345840"
  },
  {
    "text": "the community and reach out to any one of us the co-chairs and sub projects leads as well if you have any questions",
    "start": "1345840",
    "end": "1354320"
  },
  {
    "text": "Yeah as uh John mentioned the main meeting happens every Wednesday at uh at",
    "start": "1354320",
    "end": "1360120"
  },
  {
    "text": "noon Well noon Eastern time Noon Eastern time but we have multiple meetings",
    "start": "1360120",
    "end": "1366080"
  },
  {
    "text": "across the week to talk about specific uh topics of the working group So yeah it's if you want to go to the main one",
    "start": "1366080",
    "end": "1372559"
  },
  {
    "text": "it's on Wednesday but uh it's all over the week My calendar is just beeping about working group Yeah we may change",
    "start": "1372559",
    "end": "1379919"
  },
  {
    "text": "the frequency going forward like as as as we mentioned like I think we we want",
    "start": "1379919",
    "end": "1385360"
  },
  {
    "text": "to listen actively from the feedback from the communities So if there's any improvement and feedback uh don't",
    "start": "1385360",
    "end": "1392000"
  },
  {
    "text": "hesitate to reach out and but eastern time since this is this is a group Europe eastern time means eastern US",
    "start": "1392000",
    "end": "1399840"
  },
  {
    "text": "time So just want to avoid confusion there And thank you Uh if there is any",
    "start": "1399840",
    "end": "1405840"
  },
  {
    "text": "questions uh there are there's one microphone there",
    "start": "1405840",
    "end": "1411960"
  },
  {
    "text": "Yeah Any questions",
    "start": "1411960",
    "end": "1416440"
  },
  {
    "text": "Hey can you hear me Yeah Uh I'm Yen from Nvidia Less known Yen than Yan Tong over",
    "start": "1420080",
    "end": "1426159"
  },
  {
    "text": "there different So my question is yeah I I didn't follow up and uh yeah the",
    "start": "1426159",
    "end": "1431760"
  },
  {
    "text": "discussions just wondering like something like a KV caching and for the performance",
    "start": "1431760",
    "end": "1437720"
  },
  {
    "text": "optimization and uh even for tolerance is that the topic or any plan or ongoing",
    "start": "1437720",
    "end": "1444000"
  },
  {
    "text": "work about that yeah key value caching and uh you know what I'm talking about right Yeah I think that's more like a",
    "start": "1444000",
    "end": "1450400"
  },
  {
    "text": "focus topic for the VM and model server community I know VRM has community has",
    "start": "1450400",
    "end": "1456240"
  },
  {
    "text": "been working with uh the M cache project Uh we have one of the VRM maintainers",
    "start": "1456240",
    "end": "1462159"
  },
  {
    "text": "here Michael Gan Um he's over there So in case you have more specific questions",
    "start": "1462159",
    "end": "1468799"
  },
  {
    "text": "talk to him Yeah So I'm asking is particularly to my best knowledge there are no universally acceptable and",
    "start": "1468799",
    "end": "1476640"
  },
  {
    "text": "standard right for the KV cache or KV caching even model caching right as a",
    "start": "1476640",
    "end": "1482159"
  },
  {
    "text": "working group serving I was wondering right moving forward right are we going to have some kind of a standard for yeah",
    "start": "1482159",
    "end": "1489279"
  },
  {
    "text": "Kubernetes and to support the the serving workload right what's the way or",
    "start": "1489279",
    "end": "1494520"
  },
  {
    "text": "recommendation to optimize the performance use the advanced and caching and or other yeah mechanism course topic",
    "start": "1494520",
    "end": "1503200"
  },
  {
    "text": "uh the caching part it's uh what triggered the whole um conversation",
    "start": "1503200",
    "end": "1508240"
  },
  {
    "text": "about uh image volume source kept and is uh we want to find a standardized way of",
    "start": "1508240",
    "end": "1513679"
  },
  {
    "text": "moving around data and if we standardize that we can start talking about how do we standardize caching for everyone So",
    "start": "1513679",
    "end": "1520559"
  },
  {
    "text": "it's uh it's related to this conversation Yeah that's a good question Usually whenever it comes to some kind",
    "start": "1520559",
    "end": "1526640"
  },
  {
    "text": "of standard it's very hard to define something as a small community So we",
    "start": "1526640",
    "end": "1532320"
  },
  {
    "text": "really hope uh more vendors and more end users can can participate in our",
    "start": "1532320",
    "end": "1538720"
  },
  {
    "text": "community discussions so can so that we can uh perhaps in in uh in the long run",
    "start": "1538720",
    "end": "1544159"
  },
  {
    "text": "we can drive uh like lead some initiative towards some kind of standard on this Okay thank you Way to go",
    "start": "1544159",
    "end": "1555240"
  },
  {
    "text": "First of all thanks for the talk It was awesome Do I think it's kind of for you",
    "start": "1555520",
    "end": "1562240"
  },
  {
    "text": "uh when you are talking about how we are the group discussion about how to what",
    "start": "1562240",
    "end": "1568279"
  },
  {
    "text": "to watch to scale L&M Uh actually I was trying with some some benchmarks also So",
    "start": "1568279",
    "end": "1576320"
  },
  {
    "text": "it's kind of similar of what it's doing uh and mostly of the VLM and other ones",
    "start": "1576320",
    "end": "1582559"
  },
  {
    "text": "they use like the GPU uh cache utilization of the KV cache uh but",
    "start": "1582559",
    "end": "1588000"
  },
  {
    "text": "sometimes it kind of doesn't have like a real performance or completion time or",
    "start": "1588000",
    "end": "1595360"
  },
  {
    "text": "end to end time it does like token wise it would thinks like it's good but uh",
    "start": "1595360",
    "end": "1601600"
  },
  {
    "text": "end to end wise it doesn't work well Do you have any like other metrics that you are watching that you could share Yeah",
    "start": "1601600",
    "end": "1608600"
  },
  {
    "text": "Uh one example is uh prompt long like how long is your",
    "start": "1608600",
    "end": "1614480"
  },
  {
    "text": "prompt sometimes affects the performance of your process right so that's why uh",
    "start": "1614480",
    "end": "1619760"
  },
  {
    "text": "we kick it out the the initiative for for the inference perf because it's like",
    "start": "1619760",
    "end": "1625279"
  },
  {
    "text": "we really really need to deep dive into the behavior of each language model",
    "start": "1625279",
    "end": "1630400"
  },
  {
    "text": "because we are still at the early stages and we don't truly understand what",
    "start": "1630400",
    "end": "1636320"
  },
  {
    "text": "really impacts the performance So during the meetings discussions have been around like even the the the length of",
    "start": "1636320",
    "end": "1643279"
  },
  {
    "text": "the prompt can affect things So we need to monitor things from the the prompt itself all the way down to memory and",
    "start": "1643279",
    "end": "1650799"
  },
  {
    "text": "GPU utilization Right So it's like every layer impacts a model in a different way",
    "start": "1650799",
    "end": "1657440"
  },
  {
    "text": "So it really goes into the model So we are trying to like uh create categories and kind of like it's it's slice and",
    "start": "1657440",
    "end": "1664240"
  },
  {
    "text": "dice So we can then uh take that to the autoscaling workstream and then have",
    "start": "1664240",
    "end": "1670080"
  },
  {
    "text": "some standards on like okay model type A behaves like this model type B behaves",
    "start": "1670080",
    "end": "1675120"
  },
  {
    "text": "like this but we are still like we are still working on the tool to do the benchmark So we are on early stages Yeah",
    "start": "1675120",
    "end": "1682159"
  },
  {
    "text": "I'm I'm sorry but I will just have one more question and I think this is more for you uh I have been working using the",
    "start": "1682159",
    "end": "1689039"
  },
  {
    "text": "VLM benchmark and also the AI bricks benchmark They have kind of similar to",
    "start": "1689039",
    "end": "1694480"
  },
  {
    "text": "the inference PF that you're using Uh do you think it's viable to her run my",
    "start": "1694480",
    "end": "1700640"
  },
  {
    "text": "benchmarks using the inference PF or they are quite similar",
    "start": "1700640",
    "end": "1706080"
  },
  {
    "text": "Uh I think we are just working with trying to partner with different communities there Right So I think the",
    "start": "1706080",
    "end": "1713279"
  },
  {
    "text": "gateway API inference extension uh requires some additional metrics and inference perf also wants some",
    "start": "1713279",
    "end": "1719840"
  },
  {
    "text": "additional metrics as well from the VRM model server So that's something we are",
    "start": "1719840",
    "end": "1724960"
  },
  {
    "text": "working closely with the VRM community to make sure like we can have the",
    "start": "1724960",
    "end": "1730640"
  },
  {
    "text": "additional metrics available that can better represent the workload uh especially when running large language",
    "start": "1730640",
    "end": "1737440"
  },
  {
    "text": "models where the standardized standard traditional metrics are not like that useful anymore So yeah that's something",
    "start": "1737440",
    "end": "1744799"
  },
  {
    "text": "we are actively working on So I'd really suggest like joining our community",
    "start": "1744799",
    "end": "1750559"
  },
  {
    "text": "course to raise your specific requirements and use cases uh because we'd love to hear from more users Yeah",
    "start": "1750559",
    "end": "1758159"
  },
  {
    "text": "Yeah Thank you very much Yeah no problem",
    "start": "1758159",
    "end": "1762919"
  },
  {
    "text": "So when you look at the um developments on the model server side uh in terms of",
    "start": "1766000",
    "end": "1771039"
  },
  {
    "text": "what they're doing in terms of like autoscaling or dynamic resource allocation how does that relate to like",
    "start": "1771039",
    "end": "1776399"
  },
  {
    "text": "you know what's being done at the cluster orchestrator level versus with something like Dynamo right like things",
    "start": "1776399",
    "end": "1782320"
  },
  {
    "text": "that are uh similarly been done at the model server layer and early thoughts on how they could",
    "start": "1782320",
    "end": "1788279"
  },
  {
    "text": "compose if that's been sort of a area of thought uh the Dynamo word uh yeah it's",
    "start": "1788279",
    "end": "1796120"
  },
  {
    "text": "uh at the working group we try to be bender neutral and and we we are trying",
    "start": "1796120",
    "end": "1801360"
  },
  {
    "text": "to accommodate and make curities better for everyone right that's why we ask for contributions so yes at uh the working",
    "start": "1801360",
    "end": "1808960"
  },
  {
    "text": "group meetings we have presented from Nvidia things like the Nemo operator and u hopefully soon now that Dynamo is open",
    "start": "1808960",
    "end": "1816720"
  },
  {
    "text": "source we will be presenting it there and what we from Nvidia do during that meeting is say like hey we We are doing",
    "start": "1816720",
    "end": "1823200"
  },
  {
    "text": "it like this Something that is hurt hurting us or like it's a painoint is this and we will just like point it out",
    "start": "1823200",
    "end": "1829360"
  },
  {
    "text": "to the working group and from there we will take it to a kept or we will take it to to something that we can take to",
    "start": "1829360",
    "end": "1835840"
  },
  {
    "text": "make your better right But it's uh that's kind of like the process Okay For it Yeah Got it Thank you",
    "start": "1835840",
    "end": "1843840"
  },
  {
    "text": "How are we in time Do we have time Do we have time Is no No",
    "start": "1843840",
    "end": "1850880"
  },
  {
    "text": "I think that's enough All right Thanks No I'll just talk to you We can take questions offline",
    "start": "1850880",
    "end": "1855919"
  },
  {
    "text": "Thank Thank you everyone",
    "start": "1855919",
    "end": "1859679"
  }
]