[
  {
    "text": "so welcoming you all again to this talk um we are from Nvidia I'm Smita and my",
    "start": "799",
    "end": "7279"
  },
  {
    "text": "colleague vinod we'll be today talking about uh how we can achieve inferencing",
    "start": "7279",
    "end": "13360"
  },
  {
    "text": "at scale and uh how the kuus platform is uh is really an ideal platform for us to",
    "start": "13360",
    "end": "20039"
  },
  {
    "text": "achieve inferencing at scale uh let's move",
    "start": "20039",
    "end": "25800"
  },
  {
    "text": "on so uh today's Contex is uh pretty much uh about you know ml inferencing so",
    "start": "27480",
    "end": "35559"
  },
  {
    "text": "from morning we've been we've heard a lot about uh different aspects of uh",
    "start": "35559",
    "end": "40920"
  },
  {
    "text": "deploying uh you know inferencing microservices some in terms of uh you know we saw we we just had a talk on",
    "start": "40920",
    "end": "47640"
  },
  {
    "text": "performance a little earlier we had a talk on how to deploy whether it should be a canary deployment or a blue green",
    "start": "47640",
    "end": "53879"
  },
  {
    "text": "deployment I think morning we had another talk uh similarly but uh you",
    "start": "53879",
    "end": "59079"
  },
  {
    "text": "know what we are going to cover is is slightly different so uh bear with me we're going to take an example of a rag",
    "start": "59079",
    "end": "65760"
  },
  {
    "text": "pipeline implementation and show how that can actually scale on kubernetes",
    "start": "65760",
    "end": "71479"
  },
  {
    "text": "using uh gpus so that will be achieved",
    "start": "71479",
    "end": "76640"
  },
  {
    "text": "through our uh Nvidia GPU ecosystem on kubernetes we'll introduce you to uh",
    "start": "76640",
    "end": "82680"
  },
  {
    "text": "Nvidia inferencing microservices and a little bit about you know how they are",
    "start": "82680",
    "end": "87759"
  },
  {
    "text": "different from uh what is available in open source like Alama 3 or uh mistl or",
    "start": "87759",
    "end": "94040"
  },
  {
    "text": "whatever we already have how the uh what is the difference between uh those",
    "start": "94040",
    "end": "100920"
  },
  {
    "text": "models and what we say as uh Nvidia inference optimized inferencing",
    "start": "100920",
    "end": "107119"
  },
  {
    "text": "microservices uh we'll talk about the pipeline uh basically it's not just one",
    "start": "107119",
    "end": "112680"
  },
  {
    "text": "inferencing engine that will help you achieve any complex business use case we'll have to uh basic deploy a set of",
    "start": "112680",
    "end": "122079"
  },
  {
    "text": "uh inferencing services to achieve what we want to do so how do we pipeline that how do we deploy them together and make",
    "start": "122079",
    "end": "128399"
  },
  {
    "text": "it work and uh building a rag a rag is essentially a retrieval augmented uh",
    "start": "128399",
    "end": "135440"
  },
  {
    "text": "generator it's uh it's an example of a generative AI where uh you know at the",
    "start": "135440",
    "end": "140640"
  },
  {
    "text": "end of it you'll really be seeing how you can uh quickly and efficiently",
    "start": "140640",
    "end": "146000"
  },
  {
    "text": "deploy a uh simple chatbot uh for your business",
    "start": "146000",
    "end": "151160"
  },
  {
    "text": "needs uh of course making it production ready is another story right uh me uh",
    "start": "151160",
    "end": "158599"
  },
  {
    "text": "downloading an open source model and running it on agpu and making it work is",
    "start": "158599",
    "end": "163879"
  },
  {
    "text": "is Trivial but uh when we want to host it on production what are the challenges",
    "start": "163879",
    "end": "169440"
  },
  {
    "text": "we face so there we look at metrics autoscaling monitoring and a short demo",
    "start": "169440",
    "end": "175680"
  },
  {
    "text": "of uh you know whatever we've spoken about so far so uh ml right last few years it was",
    "start": "175680",
    "end": "184879"
  },
  {
    "text": "pretty much in a POC stage where uh there was lot of work and emphasis on",
    "start": "184879",
    "end": "190599"
  },
  {
    "text": "training that happened so training is sort of what happens at the back end or what you call as a development phas in",
    "start": "190599",
    "end": "196560"
  },
  {
    "text": "an uh ml life cycle right you keep training a model you need lot of",
    "start": "196560",
    "end": "201680"
  },
  {
    "text": "resources you need lot of time to train a model and so on and so forth cost and everything but what happens at the end",
    "start": "201680",
    "end": "208360"
  },
  {
    "text": "of that right you are ready to deploy your trained model into production for",
    "start": "208360",
    "end": "214159"
  },
  {
    "text": "actual use so that's where we are today at the cusp of you know 2024 or five",
    "start": "214159",
    "end": "220239"
  },
  {
    "text": "that we are entering we actually want to deploy our trained models on production for real life use cases so we are there",
    "start": "220239",
    "end": "228239"
  },
  {
    "text": "and how we want to achieve this is what we'll see today okay there are two ways",
    "start": "228239",
    "end": "233760"
  },
  {
    "text": "in which we can deploy these inferencing Services uh to achieve our production",
    "start": "233760",
    "end": "240599"
  },
  {
    "text": "grade or Enterprise grade uh uh metrics so one is we can scale them up so you",
    "start": "240599",
    "end": "248239"
  },
  {
    "text": "can have multiple inferencing services that uh cater to uh you know one set of uh queries or",
    "start": "248239",
    "end": "256560"
  },
  {
    "text": "prompts or you will have to pipeline different set of microservices to achieve what we call as a pipeline so uh",
    "start": "256560",
    "end": "265000"
  },
  {
    "text": "we'll look at both of these things today so coming to the production phase",
    "start": "265000",
    "end": "270919"
  },
  {
    "text": "right what what exactly are the challenges so uh how do we actually",
    "start": "270919",
    "end": "277000"
  },
  {
    "text": "deploy these generative AI models what does it involve in in it right so you",
    "start": "277000",
    "end": "282800"
  },
  {
    "text": "can see uh you know let me let me go in a clockwise manner over here so just",
    "start": "282800",
    "end": "288280"
  },
  {
    "text": "like any other microservice that has to get into production first thing is about",
    "start": "288280",
    "end": "294039"
  },
  {
    "text": "uh safety of user data let's say I'm an Enterprise and I want to uh host a",
    "start": "294039",
    "end": "300840"
  },
  {
    "text": "chatbot uh but it it which serves my own custom business needs and I have my own",
    "start": "300840",
    "end": "307080"
  },
  {
    "text": "data that I want to train the chat bot on now I don't want to go take it to a",
    "start": "307080",
    "end": "312680"
  },
  {
    "text": "generative AI or a Google Gemini and say okay this is my data please train it for my use case so that defeats the purpose",
    "start": "312680",
    "end": "320520"
  },
  {
    "text": "of my own proprietary in the data right so one is security of data second is how",
    "start": "320520",
    "end": "327759"
  },
  {
    "text": "relevant the uh bot responses are for my um for my queries so that's what we call",
    "start": "327759",
    "end": "335759"
  },
  {
    "text": "as llm hallucinations right we want to minimize llm hallucinations as much as possible and then uh it's about",
    "start": "335759",
    "end": "343199"
  },
  {
    "text": "performance and latency throughput and latency how many queries per uh second",
    "start": "343199",
    "end": "348720"
  },
  {
    "text": "or minute can my bot handle or uh how many uh images can my uh can I do",
    "start": "348720",
    "end": "357000"
  },
  {
    "text": "inferencing on per second or per minute so so there are these uh there are these",
    "start": "357000",
    "end": "362400"
  },
  {
    "text": "Enterprise level metrics that I want to achieve so if I have such goals how do I",
    "start": "362400",
    "end": "367840"
  },
  {
    "text": "do it it's like working backwards right and of course finally it's all about uh",
    "start": "367840",
    "end": "373319"
  },
  {
    "text": "effective utilization of resources so I think through the day we've been seeing how how well to utilize AG GPU or how",
    "start": "373319",
    "end": "380440"
  },
  {
    "text": "well to utilize a fabric bandwidth Etc so to do this right we can uh sort of do",
    "start": "380440",
    "end": "386759"
  },
  {
    "text": "it in two ways one is we we can um we can take our use case and go to a",
    "start": "386759",
    "end": "394000"
  },
  {
    "text": "managed uh hosted uh inferencing service like a",
    "start": "394000",
    "end": "399280"
  },
  {
    "text": "like I was saying right we can we can use chat gp2 to train the data we want to do or influence the data but that's",
    "start": "399280",
    "end": "406479"
  },
  {
    "text": "not what we want so what we want to do is we want to tell you how you can deploy your own microservice in a cloud",
    "start": "406479",
    "end": "413599"
  },
  {
    "text": "or in your on-prem itself and still achieve Enterprise level uh in",
    "start": "413599",
    "end": "419599"
  },
  {
    "text": "inferencing so I'll hand it over to vno to sort of walk you through what we have",
    "start": "419599",
    "end": "424919"
  },
  {
    "text": "at Nvidia in this good evening everybody and um",
    "start": "424919",
    "end": "430879"
  },
  {
    "text": "welcome again thanks maida for this context uh am I audible okay and how many of you are",
    "start": "430879",
    "end": "438919"
  },
  {
    "text": "here ml practitioners okay good I think that then I think most of this will be",
    "start": "438919",
    "end": "444879"
  },
  {
    "text": "relevant and we will not go deep into the nuts and bolts of what is being done but but we will try to cover it so that",
    "start": "444879",
    "end": "451960"
  },
  {
    "text": "the main idea that we want to convey here is how to build an ml inferencing",
    "start": "451960",
    "end": "457520"
  },
  {
    "text": "service using kubernetes and also leveraging some of the Nvidia specific",
    "start": "457520",
    "end": "463080"
  },
  {
    "text": "software stack of course to run on Nvidia GPU as well as other gpus okay so",
    "start": "463080",
    "end": "468479"
  },
  {
    "text": "now as SM said we will try to take llm as an example and llm also that the text",
    "start": "468479",
    "end": "474520"
  },
  {
    "text": "part of llm not the speech and vision that is just for the description and the problem statement right so if you look",
    "start": "474520",
    "end": "480759"
  },
  {
    "text": "at llm right large language model are um by and large Auto regressive models",
    "start": "480759",
    "end": "486560"
  },
  {
    "text": "right because the what you predict next is going to be a lot LLY depending on",
    "start": "486560",
    "end": "492520"
  },
  {
    "text": "what has been the input prompt and the prediction so far so that's why these models are mostly autor regressive and",
    "start": "492520",
    "end": "499240"
  },
  {
    "text": "there's lot of computation required in the prediction and when you want to deploy it at scale you have to",
    "start": "499240",
    "end": "505520"
  },
  {
    "text": "understand the Dynamics of the internal inferencing engine so that you can actually see what do I need is I is my",
    "start": "505520",
    "end": "513599"
  },
  {
    "text": "inferencing model working good if my inferencing system sufficient to cater",
    "start": "513599",
    "end": "518760"
  },
  {
    "text": "to the needs that I have and how do I look at the observability standpoint right both from performance as well as",
    "start": "518760",
    "end": "524800"
  },
  {
    "text": "utilization so that's what we look at so this this slide is just to give an example now before you go into the",
    "start": "524800",
    "end": "530480"
  },
  {
    "text": "details of kubernetes and how ml inferencing is related I'm sure this is",
    "start": "530480",
    "end": "535519"
  },
  {
    "text": "mostly known to everyone but people who are not ml practitioners we we just thought we'll cover it um as as we said",
    "start": "535519",
    "end": "542600"
  },
  {
    "text": "AIML needs lot of resources and the resources have to come from a cluster and that has to be well orchestrated and",
    "start": "542600",
    "end": "549600"
  },
  {
    "text": "by now we all know kubernetes is a good orchestration engine for a cluster right be in terms of CPU memory and",
    "start": "549600",
    "end": "556720"
  },
  {
    "text": "specialized resources like Network and GPU right now that is where GPU comes in",
    "start": "556720",
    "end": "563240"
  },
  {
    "text": "because um kubernetes has a good management capability for all the native resources right be C be it memory be it",
    "start": "563240",
    "end": "571200"
  },
  {
    "text": "Network right you have lot of um software stack around it does it have a good software stack mechanism to",
    "start": "571200",
    "end": "578519"
  },
  {
    "text": "identify the non native resources what we call gpus right or specialized uh",
    "start": "578519",
    "end": "583959"
  },
  {
    "text": "infinite band controllers no that is where the software stack from companies like Nvidia come where they build",
    "start": "583959",
    "end": "590480"
  },
  {
    "text": "software stack to make the GPU as a first class citizen kubernetes right through the operators through the device",
    "start": "590480",
    "end": "596880"
  },
  {
    "text": "plugins through the feature discovery demons Etc right so we'll see that how",
    "start": "596880",
    "end": "602000"
  },
  {
    "text": "we are developing that and how we are building up the ecosystem and last is as I said AIML and kubernetes goes Long Way",
    "start": "602000",
    "end": "609800"
  },
  {
    "text": "Machine learning and GPU goes long way that means GPU and kubernetes goes long way right that is the context of our",
    "start": "609800",
    "end": "616040"
  },
  {
    "text": "discussion now as I talked about how do we make the gpus the the non-native",
    "start": "616040",
    "end": "622000"
  },
  {
    "text": "resources right the specialized resources and important and first class citizen in kubernetes ecosystem and that",
    "start": "622000",
    "end": "627839"
  },
  {
    "text": "is mostly developed using this uh operator framework and the operator framework is important because",
    "start": "627839",
    "end": "635120"
  },
  {
    "text": "I'm sure how many of you have used gpus or specialized resource in kubernetes right you have to ensure that you",
    "start": "635120",
    "end": "641200"
  },
  {
    "text": "install the right drivers and the entire life cycle management of the driver and",
    "start": "641200",
    "end": "646360"
  },
  {
    "text": "the resources is not easy right so that's where the operator comes in right you build operator so that you have a",
    "start": "646360",
    "end": "652560"
  },
  {
    "text": "GPU operator which will T which takes care of what driver to be installed",
    "start": "652560",
    "end": "657720"
  },
  {
    "text": "right what is the life cycle of the driver how to upgrade it how to find if the GPU health is bad how to propagate",
    "start": "657720",
    "end": "663880"
  },
  {
    "text": "that your application and your uh um Telemetry data Etc so that is where this",
    "start": "663880",
    "end": "670000"
  },
  {
    "text": "uh GPU operator comes in which has a GPU driver pod a container runtime device",
    "start": "670000",
    "end": "675040"
  },
  {
    "text": "plugin device plugin is responsible for advertising the GPU to the kubernetes cluster your uh cubet and also your",
    "start": "675040",
    "end": "682440"
  },
  {
    "text": "scheduler so that he can know okay this node has the specialized gpus I can allocate job to it I can schedule job",
    "start": "682440",
    "end": "688839"
  },
  {
    "text": "Etc right so that's where the GPU operator comes in by and large similarly as we know right when training",
    "start": "688839",
    "end": "695760"
  },
  {
    "text": "and inferencing one GPU is not going to the job or one node is not going to job you need a cluster of them right so how",
    "start": "695760",
    "end": "701880"
  },
  {
    "text": "do you connect the gpus in a way which can scale I think slightly after lunch there was a talk from um I think it rat",
    "start": "701880",
    "end": "710000"
  },
  {
    "text": "right they talked about how they build multi-gpu training pipeline right and",
    "start": "710000",
    "end": "715480"
  },
  {
    "text": "they used about rooc Rocky uh infin band now how does that communication work",
    "start": "715480",
    "end": "722000"
  },
  {
    "text": "right how do you ensure that the GPU can transfer data at scale right without",
    "start": "722000",
    "end": "727480"
  },
  {
    "text": "adding latency that is where we bring the network operator how will you give a secondary Network to your kubernetes",
    "start": "727480",
    "end": "733760"
  },
  {
    "text": "stack and the secondary Network either using Rocky or infin band or any specialized Network right how do you",
    "start": "733760",
    "end": "740120"
  },
  {
    "text": "ensure that the GPU can transfer data through that in a faster way so these two operator these are the two engines I",
    "start": "740120",
    "end": "746480"
  },
  {
    "text": "would call of your any ml B uh training or inferencing engine",
    "start": "746480",
    "end": "751639"
  },
  {
    "text": "examples now having said that now we said the inferencing engines are going",
    "start": "751639",
    "end": "757440"
  },
  {
    "text": "to be built and run as microservices right now do we want developers to",
    "start": "757440",
    "end": "762720"
  },
  {
    "text": "create their own microservices and put time and effort no that is where the nvidia's inferencing micros servic",
    "start": "762720",
    "end": "770000"
  },
  {
    "text": "short-term Nim comes in where as you know right if you want to really use a GPU as I said you need a stack of uh GPU",
    "start": "770000",
    "end": "776680"
  },
  {
    "text": "operator Network operator and you Al need runtime right which actually gives the GPU to your container right because",
    "start": "776680",
    "end": "784360"
  },
  {
    "text": "be it container D Docker D they don't uh look at GPU and say okay how do I pass in the GPU as a visible resource to The",
    "start": "784360",
    "end": "791040"
  },
  {
    "text": "Container right that's not done you need a specialized software stack called container run time now and then after",
    "start": "791040",
    "end": "797639"
  },
  {
    "text": "that you need to ensure you have the right model you have the right inferencing engine and you need the right model for the right GPU you have",
    "start": "797639",
    "end": "804480"
  },
  {
    "text": "that's a complexity right that complexity is what being avoided by building building Nims which is NVIDIA",
    "start": "804480",
    "end": "811160"
  },
  {
    "text": "inferencing microservices which allows you to just take the Nim put it on your Hardware the Nim will automatically",
    "start": "811160",
    "end": "817880"
  },
  {
    "text": "detect what kind of Hardware you have what is the capability of the hardware in terms of compute and memory and what",
    "start": "817880",
    "end": "825360"
  },
  {
    "text": "um driver required what model will be run there Etc right so that's what Nim is about and basically it is to give you",
    "start": "825360",
    "end": "832759"
  },
  {
    "text": "improv TCO Day Zero support and also simplify your development right that's what Nim is about now",
    "start": "832759",
    "end": "840240"
  },
  {
    "text": "by looking by looking inside right what is Nim just like talk about Nim can run on either cloud or it can run on a data",
    "start": "840240",
    "end": "848399"
  },
  {
    "text": "center or it can run on your um data center server or you can run on your um",
    "start": "848399",
    "end": "853519"
  },
  {
    "text": "laptop or a workstation right that is what the fundamentally we want the N the micros Serv run on any infrastructure on",
    "start": "853519",
    "end": "861360"
  },
  {
    "text": "top of that what you need I'm sure who are familiar with uh the GPU acceleration Library which is Cuda for",
    "start": "861360",
    "end": "866720"
  },
  {
    "text": "us or if for that matter any Accel engine and on top of that you have kubernetes as as I said all the GPU",
    "start": "866720",
    "end": "873480"
  },
  {
    "text": "operator and the cloud native stack that actually exposes the GPU to the operating system or the container and on",
    "start": "873480",
    "end": "880120"
  },
  {
    "text": "top of that you have the inferencing engine it can be the standard apis which",
    "start": "880120",
    "end": "885160"
  },
  {
    "text": "is cering to your uh text speech or Vision right which we call as open API",
    "start": "885160",
    "end": "890519"
  },
  {
    "text": "and on the left side you have the the secret sauce which comes in right which is about how do you specify which model",
    "start": "890519",
    "end": "897639"
  },
  {
    "text": "to use because each model model is depending on okay this model is faster on this GPU and for this model I need",
    "start": "897639",
    "end": "904160"
  },
  {
    "text": "this inferencing engine maybe tensor RT or VM so that for this Hardware it is faster so that complexity is all hidden",
    "start": "904160",
    "end": "912279"
  },
  {
    "text": "and abstracted in NS so that developers don't have to worry about oh I need to take this model and I I'm going to run",
    "start": "912279",
    "end": "918480"
  },
  {
    "text": "it on this Hardware so what combination I need that is what's being avoided right that's why it is a prepackaged",
    "start": "918480",
    "end": "925800"
  },
  {
    "text": "microservice the theory of operation basically how n works workers when you when you download an N uh run it on your",
    "start": "925800",
    "end": "932680"
  },
  {
    "text": "system run it on your system as a container actually it detects the hardware which is running on which I told told already mounts the required",
    "start": "932680",
    "end": "939920"
  },
  {
    "text": "cash model right I mean the pulling the C pulling the model always from a repos is going to be hard so you have a n",
    "start": "939920",
    "end": "945800"
  },
  {
    "text": "cache so that you can always load your model that gives you a good time about your U microservice start time or what",
    "start": "945800",
    "end": "952480"
  },
  {
    "text": "we call as the um the time to First token we will we'll see that in in an",
    "start": "952480",
    "end": "958279"
  },
  {
    "text": "example and also select the most optimal model given the hardware and downloads the model from our cloud or any uh cloud",
    "start": "958279",
    "end": "965839"
  },
  {
    "text": "or repo that you can point it to and loads the model and start serving so basically how to ensure that the micros",
    "start": "965839",
    "end": "972360"
  },
  {
    "text": "service starts serving model what you want now one problem which I talked about about the optimization we have two",
    "start": "972360",
    "end": "979480"
  },
  {
    "text": "things one is how do you optimize your Hardware second is how speed is your AI",
    "start": "979480",
    "end": "984880"
  },
  {
    "text": "system right in terms of inferencing how many services it can it can uh how many requests it can do how many tokens it",
    "start": "984880",
    "end": "991440"
  },
  {
    "text": "can generate per second and also what is the throughput right and latency so that",
    "start": "991440",
    "end": "996600"
  },
  {
    "text": "is what we are trying to avoid by saying it's a combinatorial problem it's a combinatorial optimization problem based",
    "start": "996600",
    "end": "1003120"
  },
  {
    "text": "on these parameters what kind of gpus you have right how many gpus you have per node what is the throughput that you",
    "start": "1003120",
    "end": "1009560"
  },
  {
    "text": "want from a latency and um um sorry what's the performance indication that you have from a latency and throughput",
    "start": "1009560",
    "end": "1015519"
  },
  {
    "text": "point of view and what is the Precision will you want right so based on all these you can select which model you",
    "start": "1015519",
    "end": "1022199"
  },
  {
    "text": "want run and that is where the optimization overhead is removed from a",
    "start": "1022199",
    "end": "1027438"
  },
  {
    "text": "developer or a system system admin who is deploying this to ensure that the N will automatically detect it",
    "start": "1027439",
    "end": "1035678"
  },
  {
    "text": "it uses this uh the the intelligence that it has and decides which model is",
    "start": "1035679",
    "end": "1040880"
  },
  {
    "text": "required for you to run right so that's about Nims now we talked about kubernetes we",
    "start": "1040880",
    "end": "1048280"
  },
  {
    "text": "talked about how do this gpus are exposed to Containers right using this Cloud native software stack which we may",
    "start": "1048280",
    "end": "1055039"
  },
  {
    "text": "say and we also told what Nims is Right Nims is a optimized micr service",
    "start": "1055039",
    "end": "1060799"
  },
  {
    "text": "inferencing engine now our whole discussion today now next",
    "start": "1060799",
    "end": "1066240"
  },
  {
    "text": "uh 10 15 minutes is going to be how do you build a working inferencing AI system right using these uh small bits",
    "start": "1066240",
    "end": "1073799"
  },
  {
    "text": "and pieces now to start that to set a stage for that your idea is okay you want to have",
    "start": "1073799",
    "end": "1080360"
  },
  {
    "text": "a Genera system right and you want to use n llm maybe let's say text will be our our sample for today now the key",
    "start": "1080360",
    "end": "1089159"
  },
  {
    "text": "capability of generative AI doesn't depend on only one Nim service right you need multiple of them and why do you",
    "start": "1089159",
    "end": "1095000"
  },
  {
    "text": "need multiple of them because you need one for transforming your data one for do doing the actual generation and then",
    "start": "1095000",
    "end": "1102640"
  },
  {
    "text": "you need something for reranking because you need to ensure the data is appropriate for your context and you",
    "start": "1102640",
    "end": "1109039"
  },
  {
    "text": "also need security guard rails right so that these data which is or the inferencing token that is going out is",
    "start": "1109039",
    "end": "1115679"
  },
  {
    "text": "not uh uh inappropriate for your content it also doesn't leak any of the operations so you need a lot of",
    "start": "1115679",
    "end": "1121640"
  },
  {
    "text": "guardrails in the system so and each of them is another AI model another AI engine another AI microservice so that",
    "start": "1121640",
    "end": "1129000"
  },
  {
    "text": "is where we said one n is not enough you need a a army of nim to ensure that you",
    "start": "1129000",
    "end": "1135840"
  },
  {
    "text": "are AI system is in place right so to illustrate that and then also to see how",
    "start": "1135840",
    "end": "1140960"
  },
  {
    "text": "you can quickly develop and deploy that in kubernetes uh we'll have a demo also now we will take a text based rag as an",
    "start": "1140960",
    "end": "1148919"
  },
  {
    "text": "example right rag stand for retriever augmentation generation right so rag is",
    "start": "1148919",
    "end": "1154640"
  },
  {
    "text": "retrieve augment and generate so what do you mean retrieve right so these are the two aspect of rack right or any text",
    "start": "1154640",
    "end": "1161360"
  },
  {
    "text": "based llm engine right one is you have a lot of external data you you put that to",
    "start": "1161360",
    "end": "1166720"
  },
  {
    "text": "an embedding model then you have an llm right right which is what is we call as the large language model from either",
    "start": "1166720",
    "end": "1172760"
  },
  {
    "text": "meta right or from um uh mistal right so you have an llm now to that llm you give",
    "start": "1172760",
    "end": "1180520"
  },
  {
    "text": "any input token or any query that's going to generate something right and like SM said if that data is not",
    "start": "1180520",
    "end": "1187360"
  },
  {
    "text": "relevant for your domain or your uh company's need llm will generate",
    "start": "1187360",
    "end": "1192440"
  },
  {
    "text": "something which is he thinks is Right which we called as hallucination right to avoid that always you need to give your private data source",
    "start": "1192440",
    "end": "1199320"
  },
  {
    "text": "and the private data also goes in the same model it goes to an embedding model it goes to Vector DB and based on the",
    "start": "1199320",
    "end": "1205640"
  },
  {
    "text": "query the corresponding Vector DB content will be retrieved and that is what is passed into llm to give you the",
    "start": "1205640",
    "end": "1212840"
  },
  {
    "text": "context based inferencing tokens so that's how we build context into the large language models next slide please",
    "start": "1212840",
    "end": "1219880"
  },
  {
    "text": "now this is another I would say the theory of operation what we call the the",
    "start": "1219880",
    "end": "1225200"
  },
  {
    "text": "flow model of how the data right how the data pass through this right so you have uh llama index or maybe Lang chain right",
    "start": "1225200",
    "end": "1232080"
  },
  {
    "text": "that is the orchestration engine and that sends the do documents to your embedding model embedding model sens to",
    "start": "1232080",
    "end": "1238559"
  },
  {
    "text": "Vector DB after creating the embeds and then the vector DB content is what is",
    "start": "1238559",
    "end": "1243919"
  },
  {
    "text": "going to the llm right your LM and based on user query what happens is the queries also are embedded and goes into",
    "start": "1243919",
    "end": "1249720"
  },
  {
    "text": "the vector DB so that as I said the auto regressiveness right because the query also depends on what exactly going to be",
    "start": "1249720",
    "end": "1255320"
  },
  {
    "text": "the token I'm going to produce right so that's what this model is now what I said is we need multiple",
    "start": "1255320",
    "end": "1261360"
  },
  {
    "text": "microservices be it llm model be it an embedding engine be it a retriever engine all these are microservices and",
    "start": "1261360",
    "end": "1267400"
  },
  {
    "text": "you need all of them to work together all of them to scale together to ensure your AI system is appropriate now that",
    "start": "1267400",
    "end": "1274360"
  },
  {
    "text": "problem is not very simple because as I said there is another way um for each of that right what you call the meta or the",
    "start": "1274360",
    "end": "1281559"
  },
  {
    "text": "mral model we call them as Foundation model right and the foundation model itself you have multiple of them to",
    "start": "1281559",
    "end": "1286840"
  },
  {
    "text": "decide right which one to use similarly for text embedding you have multiple of them for Vector dat database there are",
    "start": "1286840",
    "end": "1293640"
  },
  {
    "text": "multiple open source or Enterprise Vector dat database today and for the data framework for llms you have again",
    "start": "1293640",
    "end": "1300400"
  },
  {
    "text": "Lang chain or um Lama Index right one more one more click yeah so this is the",
    "start": "1300400",
    "end": "1306200"
  },
  {
    "text": "problem space what we are trying to solve right because as I said any generative AI system needs multiple",
    "start": "1306200",
    "end": "1312640"
  },
  {
    "text": "microservices and each of them you have thousands of options now how do you ensure that you can can quickly deploy",
    "start": "1312640",
    "end": "1320360"
  },
  {
    "text": "this and generate a and create a AI system by yourself I think that's going to be our next phase of discussion and",
    "start": "1320360",
    "end": "1329240"
  },
  {
    "text": "demo yeah thanks venod uh so whatever theory that we heard from venod right",
    "start": "1334640",
    "end": "1341159"
  },
  {
    "text": "today we we we'll now look at uh sort of how do you deploy them in practice so",
    "start": "1341159",
    "end": "1346559"
  },
  {
    "text": "the next 10 minutes is probably we're going to cover that before the demo so uh we we'll uh you know you can",
    "start": "1346559",
    "end": "1354080"
  },
  {
    "text": "correlate each of these layers to whatever we have discussed so far so basically the lowest layer is the",
    "start": "1354080",
    "end": "1359880"
  },
  {
    "text": "infrastructure which is the GPU and the various operators that are required to",
    "start": "1359880",
    "end": "1364960"
  },
  {
    "text": "make GPU available to the Nim pods or the inferencing pods then we uh have",
    "start": "1364960",
    "end": "1371600"
  },
  {
    "text": "this PVC or a storage class simply to Cache the model data we don't want to keep fetching the model data every time",
    "start": "1371600",
    "end": "1378679"
  },
  {
    "text": "uh a prompt is received so we require a PVC for that the Nim itself is packaged",
    "start": "1378679",
    "end": "1385600"
  },
  {
    "text": "in a very neat way and it can it's just pull and deploy and then uh the the Nim exposes",
    "start": "1385600",
    "end": "1392640"
  },
  {
    "text": "uh itself through a service called as a llm service and that of course you can",
    "start": "1392640",
    "end": "1397679"
  },
  {
    "text": "use an Ingress or not use an Ingress depending so this is a very simplistic uh picture of how a single inferencing",
    "start": "1397679",
    "end": "1406159"
  },
  {
    "text": "engine can be deployed now let's look at how we can chain them because one is not",
    "start": "1406159",
    "end": "1412159"
  },
  {
    "text": "enough just like he was saying we need one for embedding we need one for ranking probably we need multiple such",
    "start": "1412159",
    "end": "1419039"
  },
  {
    "text": "so the the beauty of this kubernetes based deployment is you can add any",
    "start": "1419039",
    "end": "1424240"
  },
  {
    "text": "number of verticals like this for example your front end and your chain service and then that uh you know all of",
    "start": "1424240",
    "end": "1431799"
  },
  {
    "text": "these independent inferencing services are chained in this manner using the chaining",
    "start": "1431799",
    "end": "1437200"
  },
  {
    "text": "server how how do we do that right through uh Helm charts of course but",
    "start": "1437200",
    "end": "1442799"
  },
  {
    "text": "there are two ways to do it one is a simple Helm uh Helm deploy and then you can deploy each of those vertical blocks",
    "start": "1442799",
    "end": "1450120"
  },
  {
    "text": "that we saw separately uh we can pull the helm chart and deploy them and ensure that they are",
    "start": "1450120",
    "end": "1455720"
  },
  {
    "text": "all working together or we can uh use a Nim operator so this is really the",
    "start": "1455720",
    "end": "1463600"
  },
  {
    "text": "oneclick way of uh deploying a rag pipeline because this uh uh make sure",
    "start": "1463600",
    "end": "1471360"
  },
  {
    "text": "that all of those components are there the dependencies are taken care of they",
    "start": "1471360",
    "end": "1476399"
  },
  {
    "text": "are talking to each other their health is good they send out the right metrics they scale in the right way etc etc so",
    "start": "1476399",
    "end": "1484120"
  },
  {
    "text": "Nim operator is uh is a very it makes our life really easy to deploy a",
    "start": "1484120",
    "end": "1489840"
  },
  {
    "text": "pipeline so you can see there are three kinds of custom resources that uh it the",
    "start": "1489840",
    "end": "1494960"
  },
  {
    "text": "operator watches for that is the Nim cache that is nothing but the PVC that gets generated for that gets created for",
    "start": "1494960",
    "end": "1502440"
  },
  {
    "text": "a inferencing engine the Nim service itself because the Nim open API server",
    "start": "1502440",
    "end": "1508600"
  },
  {
    "text": "is exposed through the Nim service and uh the Nim pipeline that is a",
    "start": "1508600",
    "end": "1513840"
  },
  {
    "text": "combination of these Services is called as a pipeline so you can manage the pipeline as a whole you don't have to",
    "start": "1513840",
    "end": "1519720"
  },
  {
    "text": "worry about scaling uh each and every of one of those Services individually you",
    "start": "1519720",
    "end": "1524880"
  },
  {
    "text": "can manage them as a unit uh as the number of input queries",
    "start": "1524880",
    "end": "1531240"
  },
  {
    "text": "increase so you know this sort of gives you a picture of what happens when you have a heterogeneous uh clust set of",
    "start": "1531240",
    "end": "1538960"
  },
  {
    "text": "nodes in the cluster so uh let's say I have you know when when I say",
    "start": "1538960",
    "end": "1544279"
  },
  {
    "text": "hetrogeneous it means I may have a node with two gpus a node with h100 type of",
    "start": "1544279",
    "end": "1550440"
  },
  {
    "text": "Hopper gpus or Amper GPS or l4s so the Nim automatically by virtue of detecting",
    "start": "1550440",
    "end": "1557919"
  },
  {
    "text": "the Hardware knows which model to download knows uh you know what is the",
    "start": "1557919",
    "end": "1563120"
  },
  {
    "text": "capability of the underlying hardware and uh everything is automatically handled the uh admin doesn't have to",
    "start": "1563120",
    "end": "1570279"
  },
  {
    "text": "worry about it other other thing is the access control is very much governed by",
    "start": "1570279",
    "end": "1575480"
  },
  {
    "text": "the kubet is RB back infrastructure so there is uh really no security issue",
    "start": "1575480",
    "end": "1580919"
  },
  {
    "text": "there as far as deployment of the uh inferencing engine is concerned so one",
    "start": "1580919",
    "end": "1586399"
  },
  {
    "text": "of the most important aspects of productizing is uh monitoring and",
    "start": "1586399",
    "end": "1591559"
  },
  {
    "text": "autoscaling and uh Prometheus is of course our uh uh favorite you know",
    "start": "1591559",
    "end": "1597960"
  },
  {
    "text": "monitoring solution it's an open-source server uh this is really nothing to do with Nvidia but this is just like you",
    "start": "1597960",
    "end": "1604760"
  },
  {
    "text": "know I thought it brings in the concept of what Prometheus is and how uh it can",
    "start": "1604760",
    "end": "1610320"
  },
  {
    "text": "monitor the various services this is a very important slide so monitoring comes in two aspects when",
    "start": "1610320",
    "end": "1617120"
  },
  {
    "text": "you think of llm right one is uh monitoring of the hardware itself I mean utilization of the hardware because we",
    "start": "1617120",
    "end": "1623840"
  },
  {
    "text": "know right gpus are very uh expensive resources so how well am I utilizing",
    "start": "1623840",
    "end": "1628919"
  },
  {
    "text": "that is very important and the Nim itself how many requests per minute am I able to handle what is my uh time to",
    "start": "1628919",
    "end": "1636919"
  },
  {
    "text": "generate the first token what is my inter token latency Etc so there are two uh aspects to monitoring the first one",
    "start": "1636919",
    "end": "1643480"
  },
  {
    "text": "is our data center GPU manager that is deployed as part of the G GPU operator",
    "start": "1643480",
    "end": "1648880"
  },
  {
    "text": "itself so you don't have to do anything extra the GPU operator itself will deploy this exporter and the export",
    "start": "1648880",
    "end": "1655520"
  },
  {
    "text": "Prometheus can scrape from that exporter uh automatically now we we we'll keep",
    "start": "1655520",
    "end": "1661880"
  },
  {
    "text": "coming back to this deployment diagram you know we'll keep adding pillars in into this to finally build our chain now",
    "start": "1661880",
    "end": "1668840"
  },
  {
    "text": "we have actually added our Promethea stack or the monitoring stack as well into this let's move on so the second",
    "start": "1668840",
    "end": "1677240"
  },
  {
    "text": "part of monitoring I said right how is the Nim service itself monitored because",
    "start": "1677240",
    "end": "1683320"
  },
  {
    "text": "you know based on what how well the uh Nim is performing you'll know uh whether",
    "start": "1683320",
    "end": "1690120"
  },
  {
    "text": "you're uh reaching your business goal or not for example right like let's say I want to develop a chatbot which",
    "start": "1690120",
    "end": "1697080"
  },
  {
    "text": "serves um let's say about 15 requests per second uh and uh I want the",
    "start": "1697080",
    "end": "1706039"
  },
  {
    "text": "uh I I want the first uh token to be generated in no less than 3 seconds so I",
    "start": "1706039",
    "end": "1712159"
  },
  {
    "text": "can have slas like this when I want to size up my rag uh application because",
    "start": "1712159",
    "end": "1718159"
  },
  {
    "text": "eventually user experience is what counts and we can't have it take forever to produce the result if I say okay like",
    "start": "1718159",
    "end": "1725640"
  },
  {
    "text": "how I'll be showing in My Demo what is the weather in Delhi if I say I really need to see it immediately if I have to",
    "start": "1725640",
    "end": "1731640"
  },
  {
    "text": "wait for one minute then probably then the whole point is lost so uh really to",
    "start": "1731640",
    "end": "1738279"
  },
  {
    "text": "achieve that level of quickness and uh agility we have to monitor the uh",
    "start": "1738279",
    "end": "1744000"
  },
  {
    "text": "service the uh Nim service monitor helps there so the Nim service monitor as you",
    "start": "1744000",
    "end": "1749799"
  },
  {
    "text": "can see uh basically uh says which uh label name I have to match in order for",
    "start": "1749799",
    "end": "1756799"
  },
  {
    "text": "Prometheus to start scraping that uh metrics from there so ultimately Prometheus is the one who scrapes the",
    "start": "1756799",
    "end": "1762919"
  },
  {
    "text": "metrics from the Nim service but then how does it know what to do is what what",
    "start": "1762919",
    "end": "1768399"
  },
  {
    "text": "the Nim uh monitoring service helps in uh this is a very important slide so",
    "start": "1768399",
    "end": "1774919"
  },
  {
    "text": "Nims really provides um many metrics some few of these are uh listed here so",
    "start": "1774919",
    "end": "1782480"
  },
  {
    "text": "in terms of KV cash is basically uh GPU memory utilization okay and then count count is",
    "start": "1782480",
    "end": "1789880"
  },
  {
    "text": "of course how many am I currently concurrently running uh what is the waiting count what is the max max",
    "start": "1789880",
    "end": "1796679"
  },
  {
    "text": "request Etc then latency which is which is really very important because uh time",
    "start": "1796679",
    "end": "1802399"
  },
  {
    "text": "to First token generation is what is like TT uh L ttft is the one that really",
    "start": "1802399",
    "end": "1809440"
  },
  {
    "text": "determines the how optimized your uh inferencing engine",
    "start": "1809440",
    "end": "1814480"
  },
  {
    "text": "is uh time total time to generation is another one because it measures end to",
    "start": "1814480",
    "end": "1819679"
  },
  {
    "text": "end the total time taken to process my input prompt to generate the output",
    "start": "1819679",
    "end": "1825240"
  },
  {
    "text": "tokens so that also must be in the order of uh seconds uh that to less than 10",
    "start": "1825240",
    "end": "1832640"
  },
  {
    "text": "seconds and then inter token latency is is another thing so because we are Auto",
    "start": "1832640",
    "end": "1838279"
  },
  {
    "text": "regressive right every time I generate a token that token gets fed back into the",
    "start": "1838279",
    "end": "1843320"
  },
  {
    "text": "system and the next token gets generated so what is the time taken for that auto",
    "start": "1843320",
    "end": "1848720"
  },
  {
    "text": "regressive mechanism to work is what is determines the inter toen latency of",
    "start": "1848720",
    "end": "1854279"
  },
  {
    "text": "course uh since we are exposing metrics through Prometheus graph is uh a very uh",
    "start": "1854279",
    "end": "1860279"
  },
  {
    "text": "good tool for you to visualize and create an observability chart uh with",
    "start": "1860279",
    "end": "1866080"
  },
  {
    "text": "each of those uh you know as you can see the metrics that uh we just talked",
    "start": "1866080",
    "end": "1871600"
  },
  {
    "text": "about uh lastly we'll talk about Auto scaling a bit because uh ultimately when",
    "start": "1871600",
    "end": "1878399"
  },
  {
    "text": "you have to run something in production it has to be uh scaled up or down uh not",
    "start": "1878399",
    "end": "1884279"
  },
  {
    "text": "only for uh user experience but also to make sure our resources are efficiently",
    "start": "1884279",
    "end": "1889559"
  },
  {
    "text": "utilized uh in yeah basically here we have a we can do a horizontal pod",
    "start": "1889559",
    "end": "1896880"
  },
  {
    "text": "autoscale for the Nim micros service so that uh basically the HPA will uh",
    "start": "1896880",
    "end": "1903799"
  },
  {
    "text": "increase or decrease the replica set count based on the custom metrics that it fetches from the Prometheus adapter",
    "start": "1903799",
    "end": "1911240"
  },
  {
    "text": "Prometheus adapter itself fetches it from the operator so it's like a feedback loop we can uh create an HPA",
    "start": "1911240",
    "end": "1919279"
  },
  {
    "text": "maybe the next the next slide yeah so this is an example of a",
    "start": "1919279",
    "end": "1925880"
  },
  {
    "text": "horizontal pod autoscaler where uh you know we we are saying uh we want Min",
    "start": "1925880",
    "end": "1931120"
  },
  {
    "text": "replicas As One Max as two and for this deployment watch for the metric called",
    "start": "1931120",
    "end": "1937000"
  },
  {
    "text": "GPU average utilization and if the average utilization exceeds 30% uh go ahead and scale it to two",
    "start": "1937000",
    "end": "1944760"
  },
  {
    "text": "replicas so this is the directive that is given to the h HP HPA will uh watch",
    "start": "1944760",
    "end": "1951600"
  },
  {
    "text": "on the Prometheus adapter and implement the replica upscale or",
    "start": "1951600",
    "end": "1957919"
  },
  {
    "text": "downscale here let me pause we'll we'll show you uh exactly uh whatever we spoke",
    "start": "1957919",
    "end": "1964960"
  },
  {
    "text": "spoke about in uh so far the first one minute of this video the video itself is",
    "start": "1964960",
    "end": "1970320"
  },
  {
    "text": "about 4 minutes so maybe the first one and one one and a half minutes talks about uh the operator how the the",
    "start": "1970320",
    "end": "1978440"
  },
  {
    "text": "operator creates the various custom resources for example this is a three node cluster and we have an NFS as the",
    "start": "1978440",
    "end": "1985320"
  },
  {
    "text": "storage client because for my pvc that uh the inferencing caching we needed a PVC so we just install the operator the",
    "start": "1985320",
    "end": "1993720"
  },
  {
    "text": "operator created the crds and now we'll create custom resources for Nim cach",
    "start": "1993720",
    "end": "2001320"
  },
  {
    "text": "okay first and then the Nim service itself you see now the Nim service is uh",
    "start": "2001320",
    "end": "2009360"
  },
  {
    "text": "is is getting created and it is now ready so uh if you can pause here right",
    "start": "2009360",
    "end": "2014919"
  },
  {
    "text": "so you see there are two um inferencing microservices that we are trying to",
    "start": "2014919",
    "end": "2020360"
  },
  {
    "text": "chain and pipeline here so we need the front end and the chaining server which we will come to so this is first half of",
    "start": "2020360",
    "end": "2027159"
  },
  {
    "text": "the infrastructure that is already ready now we uh see that the uh yeah the Nim",
    "start": "2027159",
    "end": "2035039"
  },
  {
    "text": "service is created we'll take the service IP of the the Nim",
    "start": "2035039",
    "end": "2040159"
  },
  {
    "text": "service yeah so it is uh it is now ready we can annotate it so that promethus",
    "start": "2040840",
    "end": "2046760"
  },
  {
    "text": "scrapes from it and then uh we are uh ready to actually deploy the front end",
    "start": "2046760",
    "end": "2054638"
  },
  {
    "text": "or the uh chain server after this so yes so this is this is how an IM",
    "start": "2054639",
    "end": "2062560"
  },
  {
    "text": "service looks like we have a cluster IP and 8,000 is the port where the open API",
    "start": "2062560",
    "end": "2068040"
  },
  {
    "text": "server listens on we'll have to uh this is part of the",
    "start": "2068040",
    "end": "2073720"
  },
  {
    "text": "application deployment we need a milus database so we have created a milis service with a cluster IP and uh 19530",
    "start": "2073720",
    "end": "2081200"
  },
  {
    "text": "is the default port for milis so uh we'll put forward the milis service and",
    "start": "2081200",
    "end": "2088839"
  },
  {
    "text": "now we'll uh deploy a rag sample",
    "start": "2088839",
    "end": "2093878"
  },
  {
    "text": "application that rag sample application itself is a ser service because that's what is the front-end service that will",
    "start": "2093879",
    "end": "2102040"
  },
  {
    "text": "uh be exposed on the notep on this uh particular Port so we can go to a",
    "start": "2102040",
    "end": "2109040"
  },
  {
    "text": "browser if you see that's the not port address and uh this is what comes up so",
    "start": "2109040",
    "end": "2116000"
  },
  {
    "text": "this is really quick right uh in a matter of minutes you can actually deploy your own sample chatbot like this",
    "start": "2116000",
    "end": "2125160"
  },
  {
    "text": "which uh which will actually not only give you uh the uh the the the query but",
    "start": "2125160",
    "end": "2132000"
  },
  {
    "text": "it'll also use embedding and retrieval to make your query more contextual so uh",
    "start": "2132000",
    "end": "2138839"
  },
  {
    "text": "we can see now that we we have as we'll ask a we've asked a sample question you",
    "start": "2138839",
    "end": "2144320"
  },
  {
    "text": "know what's the weather in Delhi I thought that's the most relevant thing people will care about in today's",
    "start": "2144320",
    "end": "2149720"
  },
  {
    "text": "session so uh it just gave me uh uh you know a generic um response saying okay",
    "start": "2149720",
    "end": "2156560"
  },
  {
    "text": "this is what the temperature in Delhi is and this is how it is and stuff like that but that's not uh you know that's",
    "start": "2156560",
    "end": "2163560"
  },
  {
    "text": "not enough for me I want it to be more contextual so we we just uploaded a",
    "start": "2163560",
    "end": "2169800"
  },
  {
    "text": "document uh saying okay this is the Delhi weather in June is so and so so",
    "start": "2169800",
    "end": "2176640"
  },
  {
    "text": "this is that other domain specific information that you will be providing saying that weather is a generic",
    "start": "2176640",
    "end": "2183880"
  },
  {
    "text": "information wether in June is my domain specific information and eventually the response if I ask",
    "start": "2183880",
    "end": "2190200"
  },
  {
    "text": "again what is the weather in Delhi you're going to get a better answer a more fine-tuned answer to the same",
    "start": "2190200",
    "end": "2197680"
  },
  {
    "text": "question so that's the power of creating it as a pipeline versus just downloading",
    "start": "2197680",
    "end": "2203400"
  },
  {
    "text": "a Lama and then saying that I have created an llm so for it to be contextual and for it to be more",
    "start": "2203400",
    "end": "2210560"
  },
  {
    "text": "accurate uh you need to supplement it with some additional information and",
    "start": "2210560",
    "end": "2215599"
  },
  {
    "text": "deploy it as a pipeline so so uh yeah this we can walk through",
    "start": "2215599",
    "end": "2223240"
  },
  {
    "text": "quickly this is uh this is how the uh grafana CH uh chart looks like for my",
    "start": "2223240",
    "end": "2229880"
  },
  {
    "text": "sample application and uh we'll now try to scale this okay we",
    "start": "2229880",
    "end": "2237920"
  },
  {
    "text": "had we had a single uh replica right the Pod we had a single pod basically that",
    "start": "2237920",
    "end": "2243440"
  },
  {
    "text": "was running llama now we will look at uh uh uh the see the replicas are one and",
    "start": "2243440",
    "end": "2251160"
  },
  {
    "text": "Max are two so this is the HPA uh advice that I've said that okay look at the",
    "start": "2251160",
    "end": "2257240"
  },
  {
    "text": "number of requests that are coming in if the number of requests exceeds five go ahead and do the auto scaling so you'll",
    "start": "2257240",
    "end": "2264240"
  },
  {
    "text": "see that uh we wrote up a script which will keep uh sending multiple queries at",
    "start": "2264240",
    "end": "2269440"
  },
  {
    "text": "the same time and uh if you see the number of PODS will eventually",
    "start": "2269440",
    "end": "2276280"
  },
  {
    "text": "increase from uh one to two and so on So You see there are three pods now and uh we are now",
    "start": "2276280",
    "end": "2285760"
  },
  {
    "text": "sending more requests okay instead of the UI this is",
    "start": "2285760",
    "end": "2291160"
  },
  {
    "text": "just doing it through the shell that's the only difference and now you can see the max request",
    "start": "2291160",
    "end": "2296599"
  },
  {
    "text": "has gone to five and",
    "start": "2296599",
    "end": "2302440"
  },
  {
    "text": "10 so so number of PODS became four so this is how simple it is to actually",
    "start": "2306160",
    "end": "2313599"
  },
  {
    "text": "scale and uh Monitor and do the scaling for a rag",
    "start": "2313599",
    "end": "2320240"
  },
  {
    "text": "pipeline that was pretty much the gist of our talk I think VOD will conclude",
    "start": "2320240",
    "end": "2325480"
  },
  {
    "text": "and uh then we'll take up any questions if you have take a one more minute I think U",
    "start": "2325480",
    "end": "2332160"
  },
  {
    "text": "just to conclude this is what we tried to achieve today one is um to talk about inferencing today it's the era of",
    "start": "2332160",
    "end": "2338839"
  },
  {
    "text": "inferencing right to 24 25 2 is going to be and what is Nim how is it going to help you in creating your inferencing uh",
    "start": "2338839",
    "end": "2346440"
  },
  {
    "text": "um AI system uh how do you optimize right along with the Kus and the",
    "start": "2346440",
    "end": "2352079"
  },
  {
    "text": "features of kubernetes how do you make it scalable and robust with all the monitoring facility the the the",
    "start": "2352079",
    "end": "2359359"
  },
  {
    "text": "microservices uh scaling HPA that entire uh collection of um tools that we",
    "start": "2359359",
    "end": "2366520"
  },
  {
    "text": "have and then how easily to integrate this into your mlops right so that's what we wanted to cover thank you thank",
    "start": "2366520",
    "end": "2373400"
  },
  {
    "text": "you and any any questions we can take",
    "start": "2373400",
    "end": "2377119"
  },
  {
    "text": "now yes",
    "start": "2381920",
    "end": "2385078"
  },
  {
    "text": "please uhhuh inp",
    "start": "2388480",
    "end": "2393599"
  },
  {
    "text": "so today Nims is going to have a frend right which is a standard right the fast",
    "start": "2398760",
    "end": "2404560"
  },
  {
    "text": "API based friend end but back end is talking to only Nvidia GPU",
    "start": "2404560",
    "end": "2410880"
  },
  {
    "text": "so yeah southb part is today really tied to the hardware but but but but that's",
    "start": "2411280",
    "end": "2416319"
  },
  {
    "text": "the best of both world right the the the front part anybody can use",
    "start": "2416319",
    "end": "2421960"
  },
  {
    "text": "yeah right all so this is about how do you",
    "start": "2436680",
    "end": "2441920"
  },
  {
    "text": "update the Nim right Nim micro service that's all is bagged into the Nim operator so if you use Nim operator to",
    "start": "2441920",
    "end": "2447560"
  },
  {
    "text": "install Nim you can just make a change into the I mean update the Nim uh",
    "start": "2447560",
    "end": "2452599"
  },
  {
    "text": "operator yaml file and you say okay what Nim version I want is next it will automatically pull and uh upgrade it so",
    "start": "2452599",
    "end": "2460079"
  },
  {
    "text": "you not to worry about it it's just a matter of changing the operator yl file so the operator will take care of",
    "start": "2460079",
    "end": "2466200"
  },
  {
    "text": "upgrading your names no it",
    "start": "2466200",
    "end": "2471880"
  },
  {
    "text": "won't just like you do any upgrade using operator sorry",
    "start": "2471880",
    "end": "2478040"
  },
  {
    "text": "uh you did mention that Nim can choose the model automatically and uh how does",
    "start": "2489040",
    "end": "2494280"
  },
  {
    "text": "that happen is it or does it work only with the model repository of Nvidia or can it work with any model repository",
    "start": "2494280",
    "end": "2501119"
  },
  {
    "text": "yeah the question was the the name when it chooses the model uh can does it work",
    "start": "2501119",
    "end": "2506280"
  },
  {
    "text": "only with the model repository of nvdia or any model so the answer is yes uh it will work with model repes",
    "start": "2506280",
    "end": "2512680"
  },
  {
    "text": "representative Nvidia but we also have the option of building bringing your own model and and you have to just give a yl",
    "start": "2512680",
    "end": "2518599"
  },
  {
    "text": "file which says what model suited for what and automatically so both uh your own",
    "start": "2518599",
    "end": "2524480"
  },
  {
    "text": "model yes yes",
    "start": "2524480",
    "end": "2529440"
  },
  {
    "text": "sorry so we were talking about the monitoring of how much GPU vram is being",
    "start": "2529839",
    "end": "2536119"
  },
  {
    "text": "utilized right so I have seen constraints about how many cores are being utilized whether it's Cuda core or",
    "start": "2536119",
    "end": "2542319"
  },
  {
    "text": "the tensor course right so what is the way to monitor that I was not able to find it out",
    "start": "2542319",
    "end": "2547960"
  },
  {
    "text": "the question is how to uh you look at the GP utilization right now you are looking at the Cuda core or tensor core",
    "start": "2547960",
    "end": "2555680"
  },
  {
    "text": "so that depends on which metrics are you using like so we talked about dcgm exporter here so dcgm exporter gives you",
    "start": "2555680",
    "end": "2562880"
  },
  {
    "text": "counters at every level so the GPU utilization is one counter which is going to give you the gross utilization",
    "start": "2562880",
    "end": "2570200"
  },
  {
    "text": "if you want to look at either a Cuda core or a tensor core then you have sub counters for it so I would say you",
    "start": "2570200",
    "end": "2577200"
  },
  {
    "text": "should better use the GPU util As the metric then that will give you the overall utilization if you're using",
    "start": "2577200",
    "end": "2582920"
  },
  {
    "text": "cudak cor then you may be missing on the encoder decoder Etc right so better to use GP utilization as the one GP",
    "start": "2582920",
    "end": "2588880"
  },
  {
    "text": "utilization and uh memory right the frame buffer U those are the two um",
    "start": "2588880",
    "end": "2594280"
  },
  {
    "text": "counters that you should use so that you get a good view of what the GPU utilization is right not the tenser",
    "start": "2594280",
    "end": "2600800"
  },
  {
    "text": "counters or code account that is to optimize your application as such right am I utilizing my course correctly but",
    "start": "2600800",
    "end": "2607440"
  },
  {
    "text": "when you want to put in production those counters are not required or not important for you what is important is",
    "start": "2607440",
    "end": "2613040"
  },
  {
    "text": "the overall utilization let me so is it what kind of Hardware it",
    "start": "2613040",
    "end": "2619920"
  },
  {
    "text": "support like only Nvidia GPU or other accelerators like there are hug from",
    "start": "2619920",
    "end": "2626079"
  },
  {
    "text": "hugging face which support multiple uh so they have also a similar",
    "start": "2626079",
    "end": "2632880"
  },
  {
    "text": "kind of framework n hugs",
    "start": "2632880",
    "end": "2637200"
  },
  {
    "text": "yeah uh so they support multiple kind of accelerator",
    "start": "2638440",
    "end": "2643920"
  },
  {
    "text": "platforms you can download the hugging face model and uh Nim can work with the hugging face model as well as a hugs",
    "start": "2645559",
    "end": "2652680"
  },
  {
    "text": "will work with the hugging face",
    "start": "2652680",
    "end": "2655920"
  },
  {
    "text": "model uh you said uh Nim will be able to identify which GPU your node have does",
    "start": "2659520",
    "end": "2667160"
  },
  {
    "text": "it work with on Prim or bar metal so the deployment doesn't matter",
    "start": "2667160",
    "end": "2674000"
  },
  {
    "text": "this is more uh you can run it on bare metal you can run it through Docker you",
    "start": "2674000",
    "end": "2679040"
  },
  {
    "text": "can run it through yes D GPU detection as well so for a Nim uh if you run it on",
    "start": "2679040",
    "end": "2684480"
  },
  {
    "text": "bare metal it'll detect the bare metal if you run it as a pod like what we showed then it depends on the GPU",
    "start": "2684480",
    "end": "2689839"
  },
  {
    "text": "operator how it presents the gpus to the part so it",
    "start": "2689839",
    "end": "2696440"
  }
]