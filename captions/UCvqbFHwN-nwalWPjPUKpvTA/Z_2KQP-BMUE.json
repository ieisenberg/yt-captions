[
  {
    "start": "0",
    "end": "12000"
  },
  {
    "text": "good morning everyone thanks a lot for joining in my name is penny and today",
    "start": "60",
    "end": "6180"
  },
  {
    "text": "I'll be talking about how to handle the risky business of doing your cluster your cluster upgrades a bit of",
    "start": "6180",
    "end": "13440"
  },
  {
    "start": "12000",
    "end": "32000"
  },
  {
    "text": "introduction about myself my name is Pune I'm the engineering manager of ARCA station team at lift for the last two",
    "start": "13440",
    "end": "20460"
  },
  {
    "text": "years I have been tinkering with the orchestration and infrastructure things at lyft so if you didn't get a ride on",
    "start": "20460",
    "end": "27900"
  },
  {
    "text": "time yesterday it was probably me not the rain so just a structure around how",
    "start": "27900",
    "end": "34649"
  },
  {
    "start": "32000",
    "end": "57000"
  },
  {
    "text": "we're gonna do things today you're gonna help define the problem that we're trying to solve we're of doing",
    "start": "34649",
    "end": "40170"
  },
  {
    "text": "kubernetes cluster upgrades sort of putting the puzzles of the pieces of the puzzles together how we want to solve",
    "start": "40170",
    "end": "47010"
  },
  {
    "text": "this problem at lyft gonna do a deep dive on the solution that we bought",
    "start": "47010",
    "end": "52020"
  },
  {
    "text": "about at left around this and the state of things as they are for this tool today just an overview of kubernetes at",
    "start": "52020",
    "end": "60629"
  },
  {
    "start": "57000",
    "end": "99000"
  },
  {
    "text": "lyft we basically are running our kubernetes clusters from scratch running",
    "start": "60629",
    "end": "65640"
  },
  {
    "text": "them on AWS we don't use eks but are building our own packer am eyes and are a firm",
    "start": "65640",
    "end": "71760"
  },
  {
    "text": "believer in immutable infrastructure these nodes have inbuilt kubernetes",
    "start": "71760",
    "end": "77100"
  },
  {
    "text": "tooling in them that way when the bootstrap themselves they can do self discovery for the cluster as well as we",
    "start": "77100",
    "end": "83369"
  },
  {
    "text": "become a part of the cluster themselves this helps us get a good amount of grasp",
    "start": "83369",
    "end": "89400"
  },
  {
    "text": "over the control and the data plane and we are essentially using the AWS auto scaling groups to help us vertically",
    "start": "89400",
    "end": "96210"
  },
  {
    "text": "scale our cluster sizes in times of need how that paves in into our uh predation",
    "start": "96210",
    "end": "101970"
  },
  {
    "start": "99000",
    "end": "133000"
  },
  {
    "text": "model is the fact that if ever we have to upgrade our kubernetes clusters we basically do our bug fixes our feature",
    "start": "101970",
    "end": "109320"
  },
  {
    "text": "set ruling build new patter a.m. eyes and then update the launch configuration that goes along with the auto scaling",
    "start": "109320",
    "end": "115649"
  },
  {
    "text": "groups we then terminated the nodes one by one making sure that the auto scaling",
    "start": "115649",
    "end": "120899"
  },
  {
    "text": "group boots the next set of updated nodes they are brought up with the new and updated launch configuration they",
    "start": "120899",
    "end": "127740"
  },
  {
    "text": "become a part of the cluster the state of the cluster is reconciled and we to the next node with that sort of",
    "start": "127740",
    "end": "134500"
  },
  {
    "text": "upgrade model in mind the particular workflow started becoming a pain point as we grew with our adoption of",
    "start": "134500",
    "end": "141670"
  },
  {
    "text": "kubernetes at lift so back in December of 2017 the first kubernetes clusters were born",
    "start": "141670",
    "end": "147489"
  },
  {
    "text": "and we started playing around with it and we were essentially not serving any production traffic we were trying to",
    "start": "147489",
    "end": "153220"
  },
  {
    "text": "understand the technology trying to trying to develop tooling that could get",
    "start": "153220",
    "end": "158349"
  },
  {
    "text": "us at parity with our legacy infrastructure and so upgrading the cluster wasn't really an operational",
    "start": "158349",
    "end": "164380"
  },
  {
    "text": "load on us we could upgrade it at our own time clustered health could be bad and nobody cared at all fast forward a",
    "start": "164380",
    "end": "172480"
  },
  {
    "text": "year from then on we had built upon a huge amount of instrumentation to get our clusters at parity with our legacy",
    "start": "172480",
    "end": "178959"
  },
  {
    "text": "infrastructure and so we are running redundant multi AZ clusters we had a ton of our observability start running in",
    "start": "178959",
    "end": "184989"
  },
  {
    "text": "and we wanted to now maintain both good service health and at the same time always have a cluster health being",
    "start": "184989",
    "end": "192459"
  },
  {
    "text": "maintained throughout the cluster upgrade cycle and with the work flow that we were doing this essentially",
    "start": "192459",
    "end": "198069"
  },
  {
    "text": "became a huge amount of load and burden on the person who was doing the operations for the cluster upgrades and",
    "start": "198069",
    "end": "204970"
  },
  {
    "text": "we started up improving this by using like we began with this doing just the",
    "start": "204970",
    "end": "210280"
  },
  {
    "text": "cluster upgrades on an AWS console but that only takes you so far as soon as",
    "start": "210280",
    "end": "215290"
  },
  {
    "text": "the cluster sizes they blow up you have a lot of things to make sure that the",
    "start": "215290",
    "end": "221349"
  },
  {
    "text": "cluster is fine during the entire course of an upgrade cycle you need to make sure what what defines your cluster",
    "start": "221349",
    "end": "227230"
  },
  {
    "text": "health what defines your service health and all of those are maintained throughout the entire process we started",
    "start": "227230",
    "end": "232870"
  },
  {
    "text": "working with a Python tooling that helped us get rid of the AWS console but that still involved a lot of human",
    "start": "232870",
    "end": "238810"
  },
  {
    "text": "interaction when you're doing the cluster upgrades basically that tooling wasn't really intelligent it was just a",
    "start": "238810",
    "end": "244750"
  },
  {
    "text": "way for you to circumvent going through the AWS console so the beginning of this",
    "start": "244750",
    "end": "249760"
  },
  {
    "start": "248000",
    "end": "312000"
  },
  {
    "text": "year when we had a hyper growth of the migrations from our legacy stack we was trying to think of a solution of",
    "start": "249760",
    "end": "255940"
  },
  {
    "text": "upgradation why do we want to upgrade in the first place there are numerous reasons why one person right why why one",
    "start": "255940",
    "end": "264130"
  },
  {
    "text": "might want to upgrade the cluster's the basic one just being you want to upgrade the kubernetes version that you're running they could be",
    "start": "264130",
    "end": "270430"
  },
  {
    "text": "operating system security patches that you want to throw on the rest of a placer making sure it's not vulnerable to any of those and with the pace of",
    "start": "270430",
    "end": "277900"
  },
  {
    "text": "anyone who's running their own custom clusters it's basically a way to draw out your infrastructure changes any bug",
    "start": "277900",
    "end": "284169"
  },
  {
    "text": "fixes and even things around data retention where you don't want your nodes to be lying around for that long",
    "start": "284169",
    "end": "290639"
  },
  {
    "text": "after we've decided why we need to upgrade depends on what you need to upgrade could be the entire cluster in",
    "start": "290639",
    "end": "297430"
  },
  {
    "text": "the case of like versioning updating your version for Goo minute kubernetes in other cases it could be only related",
    "start": "297430",
    "end": "303970"
  },
  {
    "text": "to the nodes which are either buggy or they are specific to a particular AZ and so it varies with the use case what you",
    "start": "303970",
    "end": "310630"
  },
  {
    "text": "want to upgrade but that in mind we said to define a set of rules for us to set a",
    "start": "310630",
    "end": "318039"
  },
  {
    "text": "golden standard for what an ideal tooling would look like we would essentially a want always the fact that",
    "start": "318039",
    "end": "324400"
  },
  {
    "text": "we can update our clusters in any time you want it could be during business hours it could be otherwise but essentially we",
    "start": "324400",
    "end": "330490"
  },
  {
    "text": "didn't want the operational load to go on someone to do this at 1:00 a.m. in the night on a Friday evening and",
    "start": "330490",
    "end": "336160"
  },
  {
    "text": "whatnot we should be able to update our clusters whenever we want we should always be able to maintain the P 95 and",
    "start": "336160",
    "end": "342789"
  },
  {
    "text": "99 latencies for our services that's priority numero uno we should also be",
    "start": "342789",
    "end": "348130"
  },
  {
    "text": "always able to maintain the cluster health throughout the cluster grade process we wanted the tooling to be",
    "start": "348130",
    "end": "354760"
  },
  {
    "text": "server less because we we didn't want a constant interaction with the operation of with the operator who is trying to",
    "start": "354760",
    "end": "361000"
  },
  {
    "text": "update the cluster all the time making sure either the cluster is updated or if he's going through the right set of",
    "start": "361000",
    "end": "366010"
  },
  {
    "text": "nodes while that was the case we also wanted to make sure that there is a way",
    "start": "366010",
    "end": "371380"
  },
  {
    "text": "for the operator to override that so we wanted to automate much of the process that you would want to do during an",
    "start": "371380",
    "end": "376810"
  },
  {
    "text": "upgrade however still have a way to override the entire functioning of the tooling given we are running into an",
    "start": "376810",
    "end": "383800"
  },
  {
    "text": "incident or things are going wonky so when we set out to find about what kind",
    "start": "383800",
    "end": "389800"
  },
  {
    "start": "387000",
    "end": "413000"
  },
  {
    "text": "of solutions do you want we have to look at the landscape around the two links that existed because we are not running",
    "start": "389800",
    "end": "395289"
  },
  {
    "text": "eks or gke those two links don't really make cents and for things like cube ATM and",
    "start": "395289",
    "end": "401019"
  },
  {
    "text": "cops it's more so ingrained into how you're building your clusters as well so anyone who is doing custom kubernetes",
    "start": "401019",
    "end": "407589"
  },
  {
    "text": "clusters set up either of these tools don't really work for your use case and",
    "start": "407589",
    "end": "412989"
  },
  {
    "text": "so keeping the ideals of what an ideal - sorry keeping the keeping the goals of",
    "start": "412989",
    "end": "418419"
  },
  {
    "start": "413000",
    "end": "441000"
  },
  {
    "text": "the ideal tooling what it would look like we started up with building this tooling in-house and we wanted to follow",
    "start": "418419",
    "end": "424419"
  },
  {
    "text": "the principles of keeping it simple and kept keepin it hydrated because we knew our clusters would grow in complexity",
    "start": "424419",
    "end": "430929"
  },
  {
    "text": "over time how the tooling within our infrastructure when what define cluster health would always be increasing in",
    "start": "430929",
    "end": "438550"
  },
  {
    "text": "complexity as the time went by so the heart of it what does it look like it's",
    "start": "438550",
    "end": "444579"
  },
  {
    "start": "441000",
    "end": "473000"
  },
  {
    "text": "basically it's not as complex of an algorithm that we wanted to implement",
    "start": "444579",
    "end": "449709"
  },
  {
    "text": "but the way it is implemented gets a lot more customized and according to the use",
    "start": "449709",
    "end": "454839"
  },
  {
    "text": "case that you want so essentially what we want to do is we want to ensure that this tooling is going to update your",
    "start": "454839",
    "end": "460419"
  },
  {
    "text": "cluster to the desired state that you want always making sure that the cluster",
    "start": "460419",
    "end": "466449"
  },
  {
    "text": "is healthy and we're going to do it one node at a time again focusing back on keeping the tool as simple as",
    "start": "466449",
    "end": "472119"
  },
  {
    "text": "possible what's interesting about the algorithm is the fact that both of these",
    "start": "472119",
    "end": "477429"
  },
  {
    "start": "473000",
    "end": "504000"
  },
  {
    "text": "features are specific to your use case to your business use case how you want your cluster workloads to be drained",
    "start": "477429",
    "end": "484209"
  },
  {
    "text": "what does it mean for the club for the workloads to be drain at the stateless are they stateful and the fact what is",
    "start": "484209",
    "end": "490899"
  },
  {
    "text": "defining the cluster health this is just the control plane is it just a set of nodes are they system D services that",
    "start": "490899",
    "end": "498159"
  },
  {
    "text": "you want to track and so both of these things are very custom to how you are running your kubernetes clusters so with",
    "start": "498159",
    "end": "505360"
  },
  {
    "start": "504000",
    "end": "559000"
  },
  {
    "text": "that in mind we started with the iteration zero for our tooling and implemented the tooling what we call",
    "start": "505360",
    "end": "510909"
  },
  {
    "text": "gates rotator it's implementing just as a simple custom controller in golang it",
    "start": "510909",
    "end": "516669"
  },
  {
    "text": "runs as a single pod deployment on your target cluster because you wanted the",
    "start": "516669",
    "end": "521709"
  },
  {
    "text": "tooling to be automated but having a way to override its its behavior we differ we design this as a finite state machine",
    "start": "521709",
    "end": "528639"
  },
  {
    "text": "and which could act on a couple of external triggers which were essentially just spawn trick map objects they weren't as complex so",
    "start": "528639",
    "end": "536470"
  },
  {
    "text": "we didn't really want to go and do the stretch of doing CR DS with the version",
    "start": "536470",
    "end": "541720"
  },
  {
    "text": "0 we also want to make sure that we are maintaining the cluster health in the most basic way possible and so we want",
    "start": "541720",
    "end": "548199"
  },
  {
    "text": "to make sure a the control plane is always seen and happy and the fact that we can always draw stats out of your",
    "start": "548199",
    "end": "553870"
  },
  {
    "text": "running cluster because if you can't like there's no way that you can either tell the cluster is healthy at all so",
    "start": "553870",
    "end": "560649"
  },
  {
    "start": "559000",
    "end": "629000"
  },
  {
    "text": "this is what the finite state machine for gates rotator looks like we just have binary States going through a freeze or a rotate free state is",
    "start": "560649",
    "end": "568360"
  },
  {
    "text": "basically the dormant state of the root of the rotator it's the default state that it begins with and it's not doing",
    "start": "568360",
    "end": "575110"
  },
  {
    "text": "anything exciting it's just waiting waiting waiting for an external trigger as soon as it gets a rotate trigger it",
    "start": "575110",
    "end": "581680"
  },
  {
    "text": "then starts it then puts the tooling into the rotates rotate state and which is essentially now tooling always",
    "start": "581680",
    "end": "588459"
  },
  {
    "text": "constantly trying to reconcile your cluster to the state which is defined by the rotate trigger we're going to talk",
    "start": "588459",
    "end": "594819"
  },
  {
    "text": "about that a little later once when you're in the rotate state it is possible that you get another rotate",
    "start": "594819",
    "end": "600610"
  },
  {
    "text": "trigger based upon a next iteration of aggradation or what have you may and that just",
    "start": "600610",
    "end": "606399"
  },
  {
    "text": "cancels the current rotation and gets the cluster to the next state that you want when your cluster rotation",
    "start": "606399",
    "end": "612189"
  },
  {
    "text": "completes or it undergoes a failure scenario the tooling automatically kicks",
    "start": "612189",
    "end": "617649"
  },
  {
    "text": "itself back into the free state stops the cluster rotation in case of a failure scenario it informs the admin or",
    "start": "617649",
    "end": "623949"
  },
  {
    "text": "the operator such as something is wrong and I'm not gonna proceed up ahead this",
    "start": "623949",
    "end": "630519"
  },
  {
    "start": "629000",
    "end": "668000"
  },
  {
    "text": "is what a freeze trigger basically looks like it's a simple config map object which doesn't require much of the",
    "start": "630519",
    "end": "637509"
  },
  {
    "text": "metadata however there are extensions available in case you want to define what a freed state looks looks like you",
    "start": "637509",
    "end": "643839"
  },
  {
    "text": "can define that in the metadata but essentially we see with justifying their as a command we define that as a cluster",
    "start": "643839",
    "end": "649899"
  },
  {
    "text": "name that way the trigger goes to the right cluster that you are directing it to this trigger basically pauses or",
    "start": "649899",
    "end": "656740"
  },
  {
    "text": "halts and ongoing rotation and it's a way for overriding the behavior it gets",
    "start": "656740",
    "end": "662170"
  },
  {
    "text": "auto applied in case of a failure sonar and stops the rotation in steps the",
    "start": "662170",
    "end": "668960"
  },
  {
    "start": "668000",
    "end": "722000"
  },
  {
    "text": "rotate trigger is a bit more descriptive than the freeze trigger because it is plain trying to define your end goal or",
    "start": "668960",
    "end": "674540"
  },
  {
    "text": "your final state of the cluster that you would want it to be in this particular example we're showing that we are taking",
    "start": "674540",
    "end": "680150"
  },
  {
    "text": "our cluster to a defined ami version that we wanted to be could be a tag could be could be could the shark could",
    "start": "680150",
    "end": "687800"
  },
  {
    "text": "be anything that you want your cluster am I to be running your Maxis defines how many nodes you want to delete at a",
    "start": "687800",
    "end": "694790"
  },
  {
    "text": "at a time and being the version zero we wanted to just do one node at a time it's slow but it's gonna help us get",
    "start": "694790",
    "end": "701270"
  },
  {
    "text": "there the drain time essentially defines how long would you want your workloads to",
    "start": "701270",
    "end": "706810"
  },
  {
    "text": "drain off themselves before they're timed out and then kicked off forcefully and there's another set of metadata that",
    "start": "706810",
    "end": "714260"
  },
  {
    "text": "you can define to limit the rotation where it is going how many nodes is it defining at the time",
    "start": "714260",
    "end": "720230"
  },
  {
    "text": "what nodes is it deleting so now if we define the node lifecycle that each node",
    "start": "720230",
    "end": "726080"
  },
  {
    "start": "722000",
    "end": "815000"
  },
  {
    "text": "goes through as the rotator proceeds essentially when we begin gates rotator",
    "start": "726080",
    "end": "731960"
  },
  {
    "text": "is in a free state it doesn't do anything and it classifies all of the nodes as just uninitialized there's",
    "start": "731960",
    "end": "737870"
  },
  {
    "text": "nothing exciting going on let's say we now kick the rotator into a rotate state",
    "start": "737870",
    "end": "743030"
  },
  {
    "text": "and your new and particular node that you're looking at is just running the",
    "start": "743030",
    "end": "748070"
  },
  {
    "text": "latest AMI it goes into directly an updated state alright not that exciting",
    "start": "748070",
    "end": "753770"
  },
  {
    "text": "now comes the case when you actually have an older node that is running in the cluster and it goes through three",
    "start": "753770",
    "end": "759440"
  },
  {
    "text": "set of states gates rotator marks the node as an old node which is meaning the",
    "start": "759440",
    "end": "765140"
  },
  {
    "text": "workloads have now got to come off it it starts draining the node so the workloads get off of that node a new",
    "start": "765140",
    "end": "772040"
  },
  {
    "text": "node boots up the workloads migrate and at the the old node is then deemed as drain once the entire set of workloads",
    "start": "772040",
    "end": "779839"
  },
  {
    "text": "have been drained off your node it then starts it then marks the node as ready",
    "start": "779839",
    "end": "785600"
  },
  {
    "text": "for being terminated and it kicks it off the cluster these are just some",
    "start": "785600",
    "end": "791930"
  },
  {
    "text": "illustrations to show you how the cluster upgradation is going to go so this looks like a generate cluster",
    "start": "791930",
    "end": "798440"
  },
  {
    "text": "for everyone has a net CD as a master node and has a few set of cubelets running in let's say we now spin up a",
    "start": "798440",
    "end": "805880"
  },
  {
    "text": "gauge rotator in this particular cluster as soon as it spins up it's in a default free state and it marks all the nodes as",
    "start": "805880",
    "end": "813590"
  },
  {
    "text": "just uninitialized now if you want to rotate this or if you want to upgrade",
    "start": "813590",
    "end": "818810"
  },
  {
    "start": "815000",
    "end": "855000"
  },
  {
    "text": "this particular cluster we apply this particular trigger which is defining the",
    "start": "818810",
    "end": "823910"
  },
  {
    "text": "cluster name as my cluster takes us to a new ami tag that we built with our bug",
    "start": "823910",
    "end": "829070"
  },
  {
    "text": "fixes that went in you want to do one node at a time and we want to make sure that the nodes that workloads are given",
    "start": "829070",
    "end": "836030"
  },
  {
    "text": "at least five minutes before they're being forcefully kicked off in this particular in this particular case we",
    "start": "836030",
    "end": "842060"
  },
  {
    "text": "are defining the node label to be cubelets and so we only want to upgrade",
    "start": "842060",
    "end": "847310"
  },
  {
    "text": "the two blade nodes we don't care about the control plane at this particular time because the bug fix in our particular case affects only the work",
    "start": "847310",
    "end": "853550"
  },
  {
    "text": "that the worker nodes so once when we apply the Kaito reader starts excuse me",
    "start": "853550",
    "end": "859820"
  },
  {
    "text": "checking for the cluster health it verifies if the control plane is all happy if you are still getting our stats",
    "start": "859820",
    "end": "865340"
  },
  {
    "text": "if that is the case it picks up a particular node marks it as an old node",
    "start": "865340",
    "end": "870710"
  },
  {
    "text": "as soon as we do that the workloads in in the node are beginning to be drained",
    "start": "870710",
    "end": "877280"
  },
  {
    "text": "off workloads go away master node sees we're at loss of nodes the cluster",
    "start": "877280",
    "end": "883700"
  },
  {
    "text": "autoscaler then proceeds to spin up a new node the new node is connected to",
    "start": "883700",
    "end": "889070"
  },
  {
    "text": "the auto scaling group for which the lankan for which the launch configuration has already been updated so the new node comes up it has a latest",
    "start": "889070",
    "end": "896000"
  },
  {
    "text": "AMI it has bootstrapping becomes a part of the cluster and it is marked as",
    "start": "896000",
    "end": "901220"
  },
  {
    "text": "updated and it's ready to now accept workloads the workloads that were",
    "start": "901220",
    "end": "906590"
  },
  {
    "text": "drained off from the older node start appearing on this new node the rest of",
    "start": "906590",
    "end": "912380"
  },
  {
    "text": "the workloads take a little bit take a little bit of more time but end up actually migrating on the new node that",
    "start": "912380",
    "end": "917660"
  },
  {
    "text": "we just spun up now the old node is ready to be booted off the cluster it is marked up as drained so we are not",
    "start": "917660",
    "end": "924830"
  },
  {
    "text": "running any of the workloads on it it gets deleted from the cluster and the next call we make is to the",
    "start": "924830",
    "end": "931830"
  },
  {
    "text": "cloud provider to make the node go poof we have the cluster with a new node that",
    "start": "931830",
    "end": "937230"
  },
  {
    "text": "has brought up and as workloads running all fine provided the cluster is all",
    "start": "937230",
    "end": "942270"
  },
  {
    "text": "still healthy the control train is still up case rotator now proceeds to terminate the next node and so on and so",
    "start": "942270",
    "end": "949920"
  },
  {
    "text": "forth so this is how gates rotator is doing a cluster of rotation one node at",
    "start": "949920",
    "end": "956160"
  },
  {
    "text": "a time so now we are trait to the version 1 the version 0 worked really",
    "start": "956160",
    "end": "963270"
  },
  {
    "start": "957000",
    "end": "1036000"
  },
  {
    "text": "well for us we were able to improve the class the productivity for an operator who is upgrading the cluster we were",
    "start": "963270",
    "end": "970110"
  },
  {
    "text": "also able to make the tooling server less and as automated as possible what",
    "start": "970110",
    "end": "975270"
  },
  {
    "text": "you wanted to do still improve things we wanted to make sure that we can extend the cluster health and have a stricter",
    "start": "975270",
    "end": "981780"
  },
  {
    "text": "control over what a control plane liveness means we also wanted to add a couple of more things as our stack",
    "start": "981780",
    "end": "988730"
  },
  {
    "text": "progressed and be improved on it to have things like log aggregation and having",
    "start": "988730",
    "end": "994260"
  },
  {
    "text": "additional set of pods that the that that we as a team are running in to make sure that the cluster is up and running",
    "start": "994260",
    "end": "1000370"
  },
  {
    "text": "we also wanted to ensure that we can filter a specific type of nodes what we want to upgrade be it just the control",
    "start": "1000370",
    "end": "1007220"
  },
  {
    "text": "plane the API servers the worker nodes even so doing nodes selection by a ZZZ",
    "start": "1007220",
    "end": "1013130"
  },
  {
    "text": "or their placement or even using node labels we also came up with situations",
    "start": "1013130",
    "end": "1019310"
  },
  {
    "text": "where we had unique site type of clusters going in where different type of workloads require a different set of",
    "start": "1019310",
    "end": "1025579"
  },
  {
    "text": "restrictions around control plane and the type of nodes they were running however our tooling wasn't really meant",
    "start": "1025580",
    "end": "1031310"
  },
  {
    "text": "to do that and so then the next iteration we started improving all of these features some of the failure",
    "start": "1031310",
    "end": "1039319"
  },
  {
    "text": "scenarios that the gates rotator is is bound to it's bound to be self-healing",
    "start": "1039320",
    "end": "1046339"
  },
  {
    "text": "with is the fact that it's either waiting enough for a cluster to be in a sane state or it makes sure that if it's",
    "start": "1046339",
    "end": "1053660"
  },
  {
    "text": "not it can completely halt the cluster upgrade rather than you lose this entire state so things like h-series losing",
    "start": "1053660",
    "end": "1060170"
  },
  {
    "text": "quorum or if your API server or in gray is overloaded and you can't make API",
    "start": "1060170",
    "end": "1065240"
  },
  {
    "text": "calls things around AWS losing its capacity limits in the availability zones that you're running",
    "start": "1065240",
    "end": "1071029"
  },
  {
    "text": "in all of these cases like there are ton of situations where a cluster is not able to self heal itself and the cluster",
    "start": "1071029",
    "end": "1077900"
  },
  {
    "text": "rotation or upgradation should be stopped in its tracks then there are things like having an",
    "start": "1077900",
    "end": "1083630"
  },
  {
    "text": "incompatible in place of version upgrade and things around the API rate limits changing for AWS in either of these",
    "start": "1083630",
    "end": "1090620"
  },
  {
    "text": "situations we should be able to a make sure that the cluster is never losing its own state and we are also able to",
    "start": "1090620",
    "end": "1097909"
  },
  {
    "text": "report that back to the operator however",
    "start": "1097909",
    "end": "1104539"
  },
  {
    "start": "1101000",
    "end": "1192000"
  },
  {
    "text": "it's very important for the operator to make sure that we can recover from our failures and so the categories of",
    "start": "1104539",
    "end": "1111860"
  },
  {
    "text": "failures that case rotator can it go through in its lifecycle are between a soft and a hard limit so the soft",
    "start": "1111860",
    "end": "1118190"
  },
  {
    "text": "failures are things around if your API call to the to the master or to the ear bluest console ready field you can retry",
    "start": "1118190",
    "end": "1125539"
  },
  {
    "text": "that and it's not concerned it's not considered as as a hard and read failure while it's is upgrading your cluster it",
    "start": "1125539",
    "end": "1133039"
  },
  {
    "text": "makes sure that you always have a timeout for the cluster to be in a healthy state and once that timeout reaches that's considered as a heart",
    "start": "1133039",
    "end": "1139490"
  },
  {
    "text": "failure and something that the cluster cannot sell feel with once we reach that state gates rotator goes into a damage",
    "start": "1139490",
    "end": "1146299"
  },
  {
    "text": "control mode and applies a freeze trigger implicitly that makes sure that",
    "start": "1146299",
    "end": "1151429"
  },
  {
    "text": "your cluster rotation doesn't proceed ahead you're not losing any state and it also alerts the user the admin or the",
    "start": "1151429",
    "end": "1158029"
  },
  {
    "text": "team who's responsible for upgrading once you get such a notification through",
    "start": "1158029",
    "end": "1163039"
  },
  {
    "text": "pager beauty or through a slack board you can now apply certain things and see",
    "start": "1163039",
    "end": "1169039"
  },
  {
    "text": "the state of the rotation through logs through stats that you've been getting and you can once once you know that",
    "start": "1169039",
    "end": "1175070"
  },
  {
    "text": "things have been seen again things like AWS has got its capacity",
    "start": "1175070",
    "end": "1180140"
  },
  {
    "text": "back or things of that nature you can now remove that freeze trigger and override the behavior once you remove",
    "start": "1180140",
    "end": "1186110"
  },
  {
    "text": "the trigger the cluster rotation goes back into a rotate state and continues its process on so where we are at with",
    "start": "1186110",
    "end": "1195590"
  },
  {
    "start": "1192000",
    "end": "1224000"
  },
  {
    "text": "this tooling today is we have standardized this tooling to upgrade all of our cluster set lifts the",
    "start": "1195590",
    "end": "1202100"
  },
  {
    "text": "operational load has shrunk a ton we are able to do the business our upgrades as we wanted add as an ideal goal for us",
    "start": "1202100",
    "end": "1209060"
  },
  {
    "text": "making sure that we are not waking up someone in the middle of the night however this is still a work in progress",
    "start": "1209060",
    "end": "1215030"
  },
  {
    "text": "and we are learning as we are improving our cluster complexity we are improving the tooling that we are running and what",
    "start": "1215030",
    "end": "1221690"
  },
  {
    "text": "defines a cluster health some of the future work that we've been thinking",
    "start": "1221690",
    "end": "1227360"
  },
  {
    "start": "1224000",
    "end": "1332000"
  },
  {
    "text": "around this tooling is making sure we can we can cater to the long-running",
    "start": "1227360",
    "end": "1233180"
  },
  {
    "text": "workloads so things around machine learning jobs which are running for hours and days the tooling is not yet",
    "start": "1233180",
    "end": "1239600"
  },
  {
    "text": "equipped to handle those type of workloads we would want to make sure that we can upgrade to those specific those specific",
    "start": "1239600",
    "end": "1246260"
  },
  {
    "text": "type of workloads and cater to upgrading them the tooling has minimal amount of",
    "start": "1246260",
    "end": "1252950"
  },
  {
    "text": "interaction now required from the operator however we would ideally want it to be at a place where we can update",
    "start": "1252950",
    "end": "1259340"
  },
  {
    "text": "the state once and then be done with it the rotation tool should automatically pick up where we need the cluster to be",
    "start": "1259340",
    "end": "1266480"
  },
  {
    "text": "in a state and automatically you do so instead of applying the trigger manually as we progress ahead and enter into a",
    "start": "1266480",
    "end": "1275090"
  },
  {
    "text": "state of having multi clusters and having the services spread across all of them we would want to make sure that we",
    "start": "1275090",
    "end": "1281210"
  },
  {
    "text": "can maintain the service health we can maintain the entire class we can maintain the cluster health aggregated",
    "start": "1281210",
    "end": "1288050"
  },
  {
    "text": "lis for all of them and also make sure that we are in sync with the PAS disruption budget and make sure like we",
    "start": "1288050",
    "end": "1293510"
  },
  {
    "text": "don't fall below that we would also want to make sure we can extend this tooling",
    "start": "1293510",
    "end": "1298670"
  },
  {
    "text": "out for others to use and make sure we can we can change our cluster health",
    "start": "1298670",
    "end": "1304880"
  },
  {
    "text": "reporting that way it becomes more sort of a plug in and is available for anyone to use and also to do things around like",
    "start": "1304880",
    "end": "1311720"
  },
  {
    "text": "have a specified way how you want to terminate the nodes do you want to do the control plane first do you want to",
    "start": "1311720",
    "end": "1317900"
  },
  {
    "text": "do the cubelets first or even the increasing tier of your services like you want to do a specific ordering of",
    "start": "1317900",
    "end": "1323300"
  },
  {
    "text": "the upgrade you should be able to define all of that so that's where the tooling",
    "start": "1323300",
    "end": "1329240"
  },
  {
    "text": "is today happy to chat more and with that thank you everyone up for questions so the",
    "start": "1329240",
    "end": "1350240"
  },
  {
    "start": "1332000",
    "end": "1452000"
  },
  {
    "text": "rotator doesn't save any of the state on itself it's basically applying the state on the",
    "start": "1350240",
    "end": "1355760"
  },
  {
    "text": "objects themselves so the node as it goes through the transitions we apply that specific labels on the nodes so",
    "start": "1355760",
    "end": "1361730"
  },
  {
    "text": "when the node on which the gates rotator itself is running and that needs an upgrade the workload gets thrown off the",
    "start": "1361730",
    "end": "1367940"
  },
  {
    "text": "reader the case rotator spins up on a new node and just picks up from where it left anyone has a question you can come",
    "start": "1367940",
    "end": "1375080"
  },
  {
    "text": "this microphone over there",
    "start": "1375080",
    "end": "1378130"
  },
  {
    "text": "hi great talk by the way it's very painful first question what time are you",
    "start": "1390190",
    "end": "1398360"
  },
  {
    "text": "releasing this on github and second question how do you how do you do this",
    "start": "1398360",
    "end": "1404990"
  },
  {
    "text": "for masters so for masters we basically just make sure that our cluster state is",
    "start": "1404990",
    "end": "1411470"
  },
  {
    "text": "seen you've got redundant masters master nodes running in and so we make sure we always have a CN number of them up and",
    "start": "1411470",
    "end": "1417710"
  },
  {
    "text": "running we define the max search for them to be a limited number that way we",
    "start": "1417710",
    "end": "1422809"
  },
  {
    "text": "make sure that the availability for the master nodes is still up and running so we can get rid of one API masters but we",
    "start": "1422809",
    "end": "1428059"
  },
  {
    "text": "still got a redundant set of them still running in but what about if you're upgrading and you have older version",
    "start": "1428059",
    "end": "1434179"
  },
  {
    "text": "masters and newer version masters at the same time so that depends mostly our upgrades are generally with the",
    "start": "1434179",
    "end": "1440120"
  },
  {
    "text": "compatible version upgrades we don't really fall that far behind the upstream versions that way the API servers are",
    "start": "1440120",
    "end": "1446690"
  },
  {
    "text": "compatible with their upgrades does that help answer that Yeah right thanks I",
    "start": "1446690",
    "end": "1452860"
  },
  {
    "start": "1452000",
    "end": "1512000"
  },
  {
    "text": "guess building off his question is do you have any other extra logic around handling stateful applications and",
    "start": "1452860",
    "end": "1459919"
  },
  {
    "text": "services and making sure you don't cause any service disruption not at the moment but as I said like we we are still",
    "start": "1459919",
    "end": "1466129"
  },
  {
    "text": "working with this with this project right now making sure we can attest to all of all types of workloads that are",
    "start": "1466129",
    "end": "1471889"
  },
  {
    "text": "possible but that's one of the things that we have that on our roadmap we'll make sure like we can preserve the state of stateful workloads and make sure they",
    "start": "1471889",
    "end": "1478759"
  },
  {
    "text": "are always in a good health when we are upgrading but thanks thank you just",
    "start": "1478759",
    "end": "1485899"
  },
  {
    "text": "curious this is interesting because I did a similar thing when I was at Capital One what are you or are you kind of",
    "start": "1485899",
    "end": "1493899"
  },
  {
    "text": "allocating things off to the side and saying yes this needs to be rotated or we need to rotate our instances because",
    "start": "1493899",
    "end": "1499850"
  },
  {
    "text": "of this but we're not gonna do it through the Cates rotator for some specific reason so it depends in case",
    "start": "1499850",
    "end": "1507440"
  },
  {
    "text": "you've got a breaking change for the API master node and it cannot be just",
    "start": "1507440",
    "end": "1512809"
  },
  {
    "text": "rotated one node at a time you need to take down everything and then put them up it's on those cases that you then",
    "start": "1512809",
    "end": "1518379"
  },
  {
    "text": "thought none of those nodes or basically don't add them to the set of nodes that you wanted to do",
    "start": "1518379",
    "end": "1523400"
  },
  {
    "text": "so like do the work and do the STDs to the cubelets but don't do the API service and then you can hand roll those",
    "start": "1523400",
    "end": "1529520"
  },
  {
    "text": "in when you were describing your rolling",
    "start": "1529520",
    "end": "1534740"
  },
  {
    "text": "update pattern you kind of sorry you illustrated that pods would move from one to the other",
    "start": "1534740",
    "end": "1540140"
  },
  {
    "text": "have you done anything special with the scheduler to ensure that kind of behavior to minimize service disruption or is it more the default behavior where",
    "start": "1540140",
    "end": "1546920"
  },
  {
    "text": "the pods will just reschedule where available so in the version zero we started doing a cordon of all of the",
    "start": "1546920",
    "end": "1554809"
  },
  {
    "text": "older nodes to help make sure that we are not trashing the Welkin the workloads from going back to the older",
    "start": "1554809",
    "end": "1560990"
  },
  {
    "text": "nodes that was one of the problems that I already forced so like saying you you've got ten words running in you term",
    "start": "1560990",
    "end": "1567290"
  },
  {
    "text": "one of the nodes and that particular workloads get thrown off to the older nine nodes so we did a blanket cordon of",
    "start": "1567290",
    "end": "1572480"
  },
  {
    "start": "1572000",
    "end": "1632000"
  },
  {
    "text": "all of them that sometimes gets into a bit of a trouble where because of the",
    "start": "1572480",
    "end": "1577580"
  },
  {
    "text": "capacity limits or like the way you're scaling up or if there's our deploy going on you might enter a case where",
    "start": "1577580",
    "end": "1582980"
  },
  {
    "text": "the service SSL is just drop and so in those cases we started now doing the batch processing of of the of the nodes",
    "start": "1582980",
    "end": "1590809"
  },
  {
    "text": "so basically just doing five at a time or like X number at a time that way use you ensure that the SSL is are preserved",
    "start": "1590809",
    "end": "1596480"
  },
  {
    "text": "but it goes a little slower but because it's automated it kind of like really works",
    "start": "1596480",
    "end": "1602110"
  },
  {
    "text": "you're basically incentivizing the fact that you can preserve the essays in in",
    "start": "1602110",
    "end": "1607250"
  },
  {
    "text": "loss of getting the cluster rotation done in the amount of time that thank you a nice work this has been a",
    "start": "1607250",
    "end": "1617000"
  },
  {
    "text": "long-standing problem that I see of upgrading nodes in kubernetes I have a",
    "start": "1617000",
    "end": "1622520"
  },
  {
    "text": "specific question so one of the guiding principle that you said was immutable",
    "start": "1622520",
    "end": "1628450"
  },
  {
    "text": "infrastructure have you thought about a pattern where instead of updating launch",
    "start": "1628450",
    "end": "1634160"
  },
  {
    "start": "1632000",
    "end": "1692000"
  },
  {
    "text": "configurations of the EHT spinning up a new ast with the new configuration so",
    "start": "1634160",
    "end": "1641360"
  },
  {
    "text": "that you still maintain that immutable infrastructure choosing one over the",
    "start": "1641360",
    "end": "1647720"
  },
  {
    "text": "other just wanted to know more about it interesting take so the kind of",
    "start": "1647720",
    "end": "1655570"
  },
  {
    "text": "a kind of things that are involved in creating a new ASG become more involved when you are doing an upgrade we tend to",
    "start": "1655570",
    "end": "1662440"
  },
  {
    "text": "reduce the amount of operational node that you are trying to do when you are upgrading and which is why we sort of like stuck to the principle of just",
    "start": "1662440",
    "end": "1668290"
  },
  {
    "text": "sitting with the same ASG and then updating X nodes at a time that way like it we don't have to handle another ASG",
    "start": "1668290",
    "end": "1675010"
  },
  {
    "text": "get drain all of the nodes out and then delete that node because like delete that particular ASE my bad because then",
    "start": "1675010",
    "end": "1680530"
  },
  {
    "text": "it's it's a lot more involved process for you to upgrade rather than just being like a click bait for you right to",
    "start": "1680530",
    "end": "1686290"
  },
  {
    "text": "do that thank you does anything bad happen if",
    "start": "1686290",
    "end": "1694960"
  },
  {
    "text": "the rotator attempts to rotate the node that it's running on are there any special considerations to take around",
    "start": "1694960",
    "end": "1701380"
  },
  {
    "text": "that not specifically so like we specifically wanted to keep the state of",
    "start": "1701380",
    "end": "1707710"
  },
  {
    "text": "gates rotator on the objects themselves rather than have it in sort of like a",
    "start": "1707710",
    "end": "1712900"
  },
  {
    "text": "persistent database because that always begs the question when it bootstraps up where does it pick up from so in that",
    "start": "1712900",
    "end": "1719680"
  },
  {
    "text": "nature we sort of gave that dependency completely up where you want to upgrade when you want to upgrade the node that",
    "start": "1719680",
    "end": "1725860"
  },
  {
    "text": "case rotator is running on just to ease things you generally tend to keep the node that it is running on as the last",
    "start": "1725860",
    "end": "1732160"
  },
  {
    "text": "one in the queue that way you can run through every one of them but it doesn't make a difference if you terminate the",
    "start": "1732160",
    "end": "1737440"
  },
  {
    "text": "node that the rotator itself is running on it just picks up from where it left goes to the set of nodes because the",
    "start": "1737440",
    "end": "1743080"
  },
  {
    "text": "state is nowhere saved in in a persistent manner or a database awesome thanks more questions so have you had a",
    "start": "1743080",
    "end": "1753070"
  },
  {
    "start": "1752000",
    "end": "1812000"
  },
  {
    "text": "situation we had to roll back to a previous version I would believe so I",
    "start": "1753070",
    "end": "1761700"
  },
  {
    "text": "would believe so that is the case but in terms of the way you want their cluster",
    "start": "1761700",
    "end": "1767440"
  },
  {
    "text": "to roll back it doesn't make a difference in the way you're operating so basically you just specify the ami",
    "start": "1767440",
    "end": "1772960"
  },
  {
    "text": "version that you want it to be so be it like when your version at 4.1 and you are rolled up to version or r2 as soon",
    "start": "1772960",
    "end": "1779650"
  },
  {
    "text": "as you see that there's a problem going and you want to roll back to or dot one you just circle it back saying make the",
    "start": "1779650",
    "end": "1784810"
  },
  {
    "text": "state of this state make the state of the cluster back to what it was in so the order of the operations doesn't",
    "start": "1784810",
    "end": "1790480"
  },
  {
    "text": "really change in terms of like III thinking what kind of problems are you",
    "start": "1790480",
    "end": "1795670"
  },
  {
    "text": "thinking that it comes to like upgrading from a TD to sed three and the state of",
    "start": "1795670",
    "end": "1801910"
  },
  {
    "text": "sed changes and you might have to have a hard time rolling back not specifically",
    "start": "1801910",
    "end": "1808030"
  },
  {
    "text": "know whenever it has encountered any such issue like Etsy DS luring losing quorum we've just stopped it in its",
    "start": "1808030",
    "end": "1813610"
  },
  {
    "start": "1812000",
    "end": "1872000"
  },
  {
    "text": "tracks and then reverted back to it so it's never been a problem that I've seen but it's it's thanks thanks for",
    "start": "1813610",
    "end": "1819640"
  },
  {
    "text": "calling that out nice talk so this is a very specific question so",
    "start": "1819640",
    "end": "1827770"
  },
  {
    "text": "did you experience the upgrade from a seed v-22s CD v3 how and if you are",
    "start": "1827770",
    "end": "1836770"
  },
  {
    "text": "using calico as your C&I how did you keep the catacomb work during the SCD",
    "start": "1836770",
    "end": "1844330"
  },
  {
    "text": "upgrade we don't use the catechol plugin for our networking we use our in-house",
    "start": "1844330",
    "end": "1850679"
  },
  {
    "text": "CNI and i adapter that we've that we've open sourced so we don't have that",
    "start": "1850679",
    "end": "1857679"
  },
  {
    "text": "specific problem in terms of upgrading the eight CDs we always make sure that we are always important when we are",
    "start": "1857679",
    "end": "1864429"
  },
  {
    "text": "upgrading so we don't take all of them out and even so if we are taking one at CD at a time we always make sure that they are in",
    "start": "1864429",
    "end": "1870309"
  },
  {
    "text": "quorum and like that's the part that we wanted to make more stricter way that we",
    "start": "1870309",
    "end": "1875470"
  },
  {
    "start": "1872000",
    "end": "1932000"
  },
  {
    "text": "can ensure the control plane is is happy okay so one more question so for what",
    "start": "1875470",
    "end": "1881620"
  },
  {
    "text": "reason you choose the eight Abra's open sourced CNI instead of calico I'm sorry",
    "start": "1881620",
    "end": "1888250"
  },
  {
    "text": "so for what reason you choose to use the a table as open sourced signal I instead",
    "start": "1888250",
    "end": "1894130"
  },
  {
    "text": "of the calico so we're actually not using the Amazon CNI it's very similar to what we have in source sorry in house",
    "start": "1894130",
    "end": "1901750"
  },
  {
    "text": "I jest you do to listen to conversation from Paul we were basically talking",
    "start": "1901750",
    "end": "1907870"
  },
  {
    "text": "about the en I see and I adapted that we worked on so at the time when we were",
    "start": "1907870",
    "end": "1913390"
  },
  {
    "text": "building our clusters Amazon hadn't gotten that out and it just so happened that the release cycle was almost at the",
    "start": "1913390",
    "end": "1918520"
  },
  {
    "text": "same time but we just picked up things that work really well for us I see thank you problem hi hi when you support when you",
    "start": "1918520",
    "end": "1927940"
  },
  {
    "text": "upgrade Carini's master components do you support updating API objects from one version to next let's say from v1",
    "start": "1927940",
    "end": "1934269"
  },
  {
    "start": "1932000",
    "end": "1992000"
  },
  {
    "text": "beta 2 V 1 or you know as Cooney's versions upgrade Brigade some API",
    "start": "1934269",
    "end": "1941409"
  },
  {
    "text": "objects versions could be more specific for example like deployments for a while",
    "start": "1941409",
    "end": "1946570"
  },
  {
    "text": "we want better type objects now they became v1 do you handle transition from",
    "start": "1946570",
    "end": "1951879"
  },
  {
    "text": "older versions to new versions as you upgrade kubernetes versions I don't",
    "start": "1951879",
    "end": "1958029"
  },
  {
    "text": "believe so we encountered that case you meaning even like the release cycle did",
    "start": "1958029",
    "end": "1963519"
  },
  {
    "text": "the change from alpha to beta and from v1 to v2 so for many object types they're backwards compatible or they",
    "start": "1963519",
    "end": "1970299"
  },
  {
    "text": "support multiple versions at the same time but sometimes they deprecated them I don't believe you encountered that",
    "start": "1970299",
    "end": "1976719"
  },
  {
    "text": "problem I'm not sure ok don't think so sorry thank you we've got two minutes",
    "start": "1976719",
    "end": "1983529"
  },
  {
    "text": "left for questions two minutes Thanks so at any point during your design phase",
    "start": "1983529",
    "end": "1990159"
  },
  {
    "text": "for case rotator did you consider also",
    "start": "1990159",
    "end": "1995409"
  },
  {
    "start": "1992000",
    "end": "2052000"
  },
  {
    "text": "adding an option of having blue/green rollouts for clusters as opposed to in",
    "start": "1995409",
    "end": "2001019"
  },
  {
    "text": "place example several of people who ask questions talked about incompatible",
    "start": "2001019",
    "end": "2007049"
  },
  {
    "text": "upgrades like at City v2 to HCD v3 we have gone through that phase we had",
    "start": "2007049",
    "end": "2012479"
  },
  {
    "text": "something similar to this and we had blue green in place so just curious if",
    "start": "2012479",
    "end": "2019169"
  },
  {
    "text": "you ever had to consider that as an option as opposed to in place that's",
    "start": "2019169",
    "end": "2024599"
  },
  {
    "text": "also one of the future works that we wanted to do for handling the updates that can't happen in place but we",
    "start": "2024599",
    "end": "2030809"
  },
  {
    "text": "haven't really thought about that fully yet but thanks thanks for this to call out for that I didn't catch whether you",
    "start": "2030809",
    "end": "2037619"
  },
  {
    "text": "disable the cluster order scalar or if you've still got it running how do you avoid those conflicts where it wants to",
    "start": "2037619",
    "end": "2042809"
  },
  {
    "text": "bring things up and down why training if",
    "start": "2042809",
    "end": "2049470"
  },
  {
    "text": "a node doesn't drain fast enough scalable bring new nodes up faster than you can right",
    "start": "2049470",
    "end": "2055720"
  },
  {
    "start": "2052000",
    "end": "2105000"
  },
  {
    "text": "for us like we keep on we've added the cluster autoscaler health as well as one",
    "start": "2055720",
    "end": "2060970"
  },
  {
    "text": "of the metrics for us to define so if it's not running then you pause the cluster relation right there but like maybe this the entire process definitely",
    "start": "2060970",
    "end": "2068919"
  },
  {
    "text": "depends a lot on the cluster autoscaler behavior does it terminate the nodes after you've made them not ready or do",
    "start": "2068919",
    "end": "2076240"
  },
  {
    "text": "you manual it you manually terminate those Ryan say it again please so when the nodes go to not ready do you wait",
    "start": "2076240",
    "end": "2081849"
  },
  {
    "text": "for the cluster autoscaler to get rid of them or do you manual it or you terminate there was a separate step in there so that takes care of the node",
    "start": "2081849",
    "end": "2088960"
  },
  {
    "text": "bootstrapping they're running on specific nodes and when the node bootstraps the cluster autoscaler kicks end back in thank you I'm sorry ladies",
    "start": "2088960",
    "end": "2095888"
  },
  {
    "text": "and gentlemen that's all the time we have for questions thanks everyone if you have any questions feel free to",
    "start": "2095889",
    "end": "2102549"
  },
  {
    "text": "reach out to me during - in the rest of the cube Tron",
    "start": "2102549",
    "end": "2106710"
  }
]