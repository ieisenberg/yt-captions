[
  {
    "text": "all right hello everyone thank you for attending the Sig autoscaling feature",
    "start": "880",
    "end": "6440"
  },
  {
    "text": "and just general update talk I know it's a little bit late in a Thursday but glad everyone's here uh my name is Jonathan",
    "start": "6440",
    "end": "12200"
  },
  {
    "text": "Enis I am a uh OSS maintainer of a Autos Skilling project called Carpenter at AWS",
    "start": "12200",
    "end": "18760"
  },
  {
    "text": "and a software engineer there and this is ma Patel and he's a software engineer of",
    "start": "18760",
    "end": "25240"
  },
  {
    "text": "Google so just walk through a little bit of an agenda that we're going to do today um we're going to give a little bit of an",
    "start": "26240",
    "end": "32078"
  },
  {
    "text": "overview between cluster autoscaler and Carpenter and the point of that being we're going to talk a little bit about",
    "start": "32079",
    "end": "37480"
  },
  {
    "text": "some work that we want to do um between cluster autoscaler and Carpenter to gain",
    "start": "37480",
    "end": "42680"
  },
  {
    "text": "more alignment across some of our features that we have that look quite similar to each other as well as some",
    "start": "42680",
    "end": "49000"
  },
  {
    "text": "API that would make probably migrating between the two forms of the autoscaling",
    "start": "49000",
    "end": "54160"
  },
  {
    "text": "much easier um for anyone writing application specs um and then we're going to talk a little bit about project",
    "start": "54160",
    "end": "60079"
  },
  {
    "text": "specific updates so um horizontal pod Auto scalar updates Carpenter updates and then cluster Auto scalar updates",
    "start": "60079",
    "end": "68040"
  },
  {
    "text": "finally so just giving a little bit of an overview between the two Auto scalers or the two node Auto scalers that exist",
    "start": "68040",
    "end": "74880"
  },
  {
    "text": "within uh Sig autoscaling today um cluster autoscaler and Carpenter they",
    "start": "74880",
    "end": "80799"
  },
  {
    "text": "solve very much the same problem but in in in kind of in a similar way um there",
    "start": "80799",
    "end": "85920"
  },
  {
    "text": "are some differences between the way they actually do the implementation under the hood um and they do have some",
    "start": "85920",
    "end": "91079"
  },
  {
    "text": "difference in terms of the I guess the scope of what they manage but they solve",
    "start": "91079",
    "end": "96200"
  },
  {
    "text": "a similar problem at least for auto scaling and downscaling so they provide nodes um so that all pods can schedule",
    "start": "96200",
    "end": "102680"
  },
  {
    "text": "so if you have a pod scale up and you have pending pods and the cube schedule Mark those pods is is failed to schedule",
    "start": "102680",
    "end": "107960"
  },
  {
    "text": "because they don't schedule on your existing capacity um both cluster autoscaler and Carpenter will react to",
    "start": "107960",
    "end": "113119"
  },
  {
    "text": "that um and they also remove unnecessary nodes to minimize your cost so we both",
    "start": "113119",
    "end": "118840"
  },
  {
    "text": "have features um that we call different names cluster autoscaler calls it I believe downscaling we call it",
    "start": "118840",
    "end": "125600"
  },
  {
    "text": "consolidation but both both of our systems will remove unnecessary nodes if",
    "start": "125600",
    "end": "131560"
  },
  {
    "text": "um we think that we can minimize the cost of your cluster and we can tear down capacity and those pots could",
    "start": "131560",
    "end": "136879"
  },
  {
    "text": "schedule elsewhere um and that basically covers the implementation piece there there are some major differences",
    "start": "136879",
    "end": "144680"
  },
  {
    "text": "um cluster autoscaler kind of works within the construct of cloud provider based API",
    "start": "144680",
    "end": "150599"
  },
  {
    "text": "so all of your configuration for cluster Auto scaler will generally sit within the cloud provider um typically within",
    "start": "150599",
    "end": "156720"
  },
  {
    "text": "node groups so asgs migs um vmss the life cycle of those node groups",
    "start": "156720",
    "end": "162640"
  },
  {
    "text": "is fully managed by the cloud provider so um if you have a new image that you want to roll out to those node groups",
    "start": "162640",
    "end": "169120"
  },
  {
    "text": "cluster Auto scaler won't manage that for you that's the responsibility of the cloud provider um if you need to like",
    "start": "169120",
    "end": "175360"
  },
  {
    "text": "Carpenter specifically has a feature for um rolling nodes if they're past a certain age cluster Auto scaler doesn't",
    "start": "175360",
    "end": "181040"
  },
  {
    "text": "manage that side of things it only handles the auto scaling down scaling of the existing node groups that you have and so all the configuration will sit in",
    "start": "181040",
    "end": "187959"
  },
  {
    "text": "those node groups in the quad provider and then it will just basically increment or decrement the number",
    "start": "187959",
    "end": "194640"
  },
  {
    "text": "assigned to that node group um across the quad provider apis whereas Carpenter uh picks a slightly different approach",
    "start": "194640",
    "end": "202040"
  },
  {
    "text": "um it handles the entire node life cycle like I mentioned we handle node upgrades",
    "start": "202040",
    "end": "208080"
  },
  {
    "text": "effectively so if you have a new image new work gamy that you're trying to roll out then the node will roll out with",
    "start": "208080",
    "end": "213840"
  },
  {
    "text": "Carpenter um through a feature we call drift um and likewise all the configuration for Carpenter sits inside",
    "start": "213840",
    "end": "221599"
  },
  {
    "text": "of the kubernetes cluster through custom resource definition or custom resources that are based on custom resource definitions that Carpenter provides",
    "start": "221599",
    "end": "229480"
  },
  {
    "text": "so those are at least the configuration differences um between the two um there is a slight difference as well in terms",
    "start": "229480",
    "end": "236319"
  },
  {
    "text": "of implementation if you're looking at implementing a a cloud provider between the two different types of things so um",
    "start": "236319",
    "end": "243439"
  },
  {
    "text": "cluster autoscaler effectively has pretty much all the information it needs to make decisions um so your node groups",
    "start": "243439",
    "end": "250439"
  },
  {
    "text": "I think traditionally you'll say here's the instance type here's the a that's assigned to a node group and then",
    "start": "250439",
    "end": "256160"
  },
  {
    "text": "cluster autoscaler takes all that information is able to make the decision with all of that given to it Carpenter's",
    "start": "256160",
    "end": "262800"
  },
  {
    "text": "approach generally is here's all of my options I'm going to send those options",
    "start": "262800",
    "end": "268560"
  },
  {
    "text": "to the cloud provider and and then the cloud provider is going to make the best decision based on those options that it has um and this this is kind of",
    "start": "268560",
    "end": "275080"
  },
  {
    "text": "particularly beneficial for for spot scenarios um where spot capacity is not necessarily publicly available and Cloud",
    "start": "275080",
    "end": "280800"
  },
  {
    "text": "providers generally have more knowledge about what capacity pools are useful to",
    "start": "280800",
    "end": "286639"
  },
  {
    "text": "launch inside of and so giving that information to the cloud provider that has that information um helps with",
    "start": "286639",
    "end": "292000"
  },
  {
    "text": "provisioning spot capacity that doesn't get reclaimed back immediately after you launch the",
    "start": "292000",
    "end": "298280"
  },
  {
    "text": "node and then we get into some smaller differences so the the first things are kind of ideological and maybe more",
    "start": "298280",
    "end": "303880"
  },
  {
    "text": "philosophical in terms of the differences between the two things and and they probably will never change um",
    "start": "303880",
    "end": "310320"
  },
  {
    "text": "these are the smaller differences and this is kind to kind of dovetail into the conversation that Ma is going to",
    "start": "310320",
    "end": "315520"
  },
  {
    "text": "going to lead in a second around how we can fix some of these things and how we can better align some of these things",
    "start": "315520",
    "end": "320560"
  },
  {
    "text": "across the two projects so very simply we have different wording which sounds",
    "start": "320560",
    "end": "326319"
  },
  {
    "text": "silly but when it comes to documentation I do think it is a little bit confusing and one thing that he's going to talk about is we're kind of looking to align",
    "start": "326319",
    "end": "333360"
  },
  {
    "text": "documentation around some of these conceptual things that are are very similar across the two projects and so",
    "start": "333360",
    "end": "338520"
  },
  {
    "text": "just we call it like Carpenter calls it provisioning cluster scaler calls it scale up Carpenter calls it",
    "start": "338520",
    "end": "345000"
  },
  {
    "text": "consolidation clust scale or C scale down sounds silly but those differences do make I think conceptual confusion",
    "start": "345000",
    "end": "351319"
  },
  {
    "text": "when it comes to figuring out what means what across the different projects um there's also some things like pod annotations are different that's",
    "start": "351319",
    "end": "357840"
  },
  {
    "text": "annoying if you're trying to move app from one project to the other um and",
    "start": "357840",
    "end": "363520"
  },
  {
    "text": "there's some differences in choices around I guess what you call actuation",
    "start": "363520",
    "end": "369400"
  },
  {
    "text": "um when it comes to node drain implementation in particular um we have",
    "start": "369400",
    "end": "374720"
  },
  {
    "text": "different taints between the two projects and all this is kind of developed over time as a result of the two projects kind of developing in",
    "start": "374720",
    "end": "381280"
  },
  {
    "text": "isolation um and then likewise there's there's slightly different Behavior with how the pods get evicted which is also",
    "start": "381280",
    "end": "387520"
  },
  {
    "text": "confusing if you're coming between the two projects um and likely the Divergence will",
    "start": "387520",
    "end": "393000"
  },
  {
    "text": "continue to increase if we don't start collaborating on a lot of these things earlier um so with that I'll let ma talk",
    "start": "393000",
    "end": "400919"
  },
  {
    "text": "a little bit about how that's going to look thanks let's see okay those work um",
    "start": "400919",
    "end": "405960"
  },
  {
    "text": "so I'm going to use those so um as as uh Jonathan already told us right um the",
    "start": "405960",
    "end": "414240"
  },
  {
    "text": "there is there shouldn't be that much difference between running workloads on",
    "start": "414240",
    "end": "420120"
  },
  {
    "text": "cluster managed by cluster autoscale or and Carpenter or at least there shouldn't be that much effort needed to",
    "start": "420120",
    "end": "426039"
  },
  {
    "text": "migrate them um ideally you shouldn't be you shouldn't have to change your pod spec at all and you shouldn't have to",
    "start": "426039",
    "end": "433240"
  },
  {
    "text": "learn any new Concepts like the naming thing we mentioned or or any other really differences which which don't",
    "start": "433240",
    "end": "439520"
  },
  {
    "text": "come back to some major changes in behavior um and I I think we're not",
    "start": "439520",
    "end": "445560"
  },
  {
    "text": "there definitely there are all the minor differences that that Jonathan listed but I don't think we're actually that",
    "start": "445560",
    "end": "450840"
  },
  {
    "text": "far from that point um the main API uh that triggers Autos scaning is sports",
    "start": "450840",
    "end": "456879"
  },
  {
    "text": "and and really the main way of specifying uh requirements is through po",
    "start": "456879",
    "end": "462120"
  },
  {
    "text": "scheduling constraints and those are already standardized so um I think if we",
    "start": "462120",
    "end": "467840"
  },
  {
    "text": "can fix the the minor differences um Jonathan described and if we can",
    "start": "467840",
    "end": "473159"
  },
  {
    "text": "obviously avoid adding any new ones we're actually going to be in a pretty good place um especially if perspective",
    "start": "473159",
    "end": "480159"
  },
  {
    "text": "of of just moving your workloads between clusters um now there are those more uh",
    "start": "480159",
    "end": "487960"
  },
  {
    "text": "significant differences and they show up much more on the configuration Style on the configuration side how you set up",
    "start": "487960",
    "end": "494800"
  },
  {
    "text": "your cluster in the first place is quite different between cluster autoscaler and",
    "start": "494800",
    "end": "500199"
  },
  {
    "text": "Carpenter and I don't think we see a way to um address it so as as Jan already",
    "start": "500199",
    "end": "507240"
  },
  {
    "text": "mentioned we started a unification effort uh where our goal is really uh on the",
    "start": "507240",
    "end": "512320"
  },
  {
    "text": "Pod level or workload level portability uh however we're not trying at least not",
    "start": "512320",
    "end": "517599"
  },
  {
    "text": "at this time to unify the way that um autoscalers are set up or configured in",
    "start": "517599",
    "end": "523479"
  },
  {
    "text": "particular we don't have any plan to adopt Carpenter nopol API in cluster",
    "start": "523479",
    "end": "528519"
  },
  {
    "text": "autoscaler or or or build any other API of that sort that would be shared",
    "start": "528519",
    "end": "533680"
  },
  {
    "text": "between the two projects um and the plan we have right now to",
    "start": "533680",
    "end": "539120"
  },
  {
    "text": "achieve uh our goals uh is basically start with with really it's just basically the list",
    "start": "539120",
    "end": "545920"
  },
  {
    "text": "of the minor differences right and and how we're going to address them so the first step is to build um share",
    "start": "545920",
    "end": "552560"
  },
  {
    "text": "documentation describing no AOS scaling Concepts and use this as an opportunity to unify the naming between cluster",
    "start": "552560",
    "end": "560399"
  },
  {
    "text": "autoscale and Carpenter and also uh hopefully if if we can put this as a",
    "start": "560399",
    "end": "566880"
  },
  {
    "text": "concept at kubernetes level between any other future implementations of autoscalers um the next step is to unify",
    "start": "566880",
    "end": "573880"
  },
  {
    "text": "any pod level API so mostly annotations um like the annotations I think we had",
    "start": "573880",
    "end": "579279"
  },
  {
    "text": "an example on a a few slides back um we want to basically introduce uh some new",
    "start": "579279",
    "end": "586279"
  },
  {
    "text": "shared replacements for those and and deprecate the project specific ones probably uh still keeping them for",
    "start": "586279",
    "end": "592640"
  },
  {
    "text": "backward compatibility and uh we want to go over um annotations and and similar apis on P",
    "start": "592640",
    "end": "600760"
  },
  {
    "text": "or notes that uh exist in one project and uh make sure or validate if if if",
    "start": "600760",
    "end": "607160"
  },
  {
    "text": "it's something we want to add to other project as well um or if it's something that actually makes more sense just in",
    "start": "607160",
    "end": "613160"
  },
  {
    "text": "one of those projects um a necessary part of this going forward as as as also",
    "start": "613160",
    "end": "618360"
  },
  {
    "text": "Jonathan mentioned is is going to be a regular communication between the projects we're already",
    "start": "618360",
    "end": "623720"
  },
  {
    "text": "collaborating um to an extent on on things like da which I'm going to talk about in a second uh but I think we need",
    "start": "623720",
    "end": "630640"
  },
  {
    "text": "more of this and and hopefully we'll be able to uh keep going and and and really",
    "start": "630640",
    "end": "636480"
  },
  {
    "text": "increase our cooperation and uh those those really are the three shortterm goals uh I would",
    "start": "636480",
    "end": "643760"
  },
  {
    "text": "say all of those are in progress to some extent none of this is uh done yet but",
    "start": "643760",
    "end": "649200"
  },
  {
    "text": "but we have um kind of some pull requests um in Flight some um General",
    "start": "649200",
    "end": "656639"
  },
  {
    "text": "agreement on how to do those and then we have some fut ideas so um again",
    "start": "656639",
    "end": "662079"
  },
  {
    "text": "addressing more of the minor differences described like things like U how the drain logic Works uh maybe something",
    "start": "662079",
    "end": "668600"
  },
  {
    "text": "around how we handle this options um building some new shared apis maybe uh",
    "start": "668600",
    "end": "675519"
  },
  {
    "text": "there is no specific plan right now but that's that would be really nice if we can achieve this and finally and this is",
    "start": "675519",
    "end": "681680"
  },
  {
    "text": "this is I think most future looking goal uh would be evaluating if you can take",
    "start": "681680",
    "end": "686760"
  },
  {
    "text": "this Beyond autoscaling and start looking into some level of unification",
    "start": "686760",
    "end": "692760"
  },
  {
    "text": "on the Node management level um but this is this is uh more future looking um and",
    "start": "692760",
    "end": "700519"
  },
  {
    "text": "this is all I had on the uh all we had on the uh unification effort one other",
    "start": "700519",
    "end": "706160"
  },
  {
    "text": "shared effort I wanted to quickly uh mention is theay of dynamic resource allocation this is something that uh has",
    "start": "706160",
    "end": "713560"
  },
  {
    "text": "been already talked about in the cubec con so I'm not going in this cubec Con so I'm not going to go into much detail",
    "start": "713560",
    "end": "720360"
  },
  {
    "text": "but the short summary is that Dr is a new API for requesting resources that",
    "start": "720360",
    "end": "726120"
  },
  {
    "text": "can support more advanced uh use cases than existing resource requests it was",
    "start": "726120",
    "end": "732120"
  },
  {
    "text": "first int introduced in Alpha in kubernetes 126 and the original design made it very hard to integrate it with",
    "start": "732120",
    "end": "739279"
  },
  {
    "text": "either cluster autoscaler or Carpenter and so there is a new cap that that is a",
    "start": "739279",
    "end": "745000"
  },
  {
    "text": "collaboration between um Sig no Sig Jing SE outo scaling uh hope I'm not missing",
    "start": "745000",
    "end": "752519"
  },
  {
    "text": "anyone but I very well maybe uh and this new cap addresses um those issues it",
    "start": "752519",
    "end": "757560"
  },
  {
    "text": "have been submitted to 130 and there is implementation work ongoing in kubernetes 130 and uh our goal is to",
    "start": "757560",
    "end": "765519"
  },
  {
    "text": "support Dr in cluster autoscaler 131 I think there is no uh specific timeline",
    "start": "765519",
    "end": "771120"
  },
  {
    "text": "for Carpenter but there is interest right yeah there's definitely definitely interest a lot of momentum from the skon",
    "start": "771120",
    "end": "776680"
  },
  {
    "text": "so it's definitely something we're evaluating I think the extended res problem has traditionally been a fun problem for us to solve would be a good",
    "start": "776680",
    "end": "783399"
  },
  {
    "text": "way to say it so but there's increasingly more support for that kind of thing obviously so we're definitely",
    "start": "783399",
    "end": "788480"
  },
  {
    "text": "looking at it really hard okay do you want to take it now to HPA updates yeah so now we're going to go into project",
    "start": "788480",
    "end": "795160"
  },
  {
    "text": "updates um we'll quickly cover the HPA project updates the carpenter project updates and the cluster autoscaler",
    "start": "795160",
    "end": "800680"
  },
  {
    "text": "project updates um so on HPA um the container resource metric",
    "start": "800680",
    "end": "806120"
  },
  {
    "text": "type uh is hitting GA and 130 um which is super exciting so um previously if",
    "start": "806120",
    "end": "811800"
  },
  {
    "text": "you were configuring HPA and you were configuring metrics um to watch on to do scale-ups you previously had to um deal",
    "start": "811800",
    "end": "818440"
  },
  {
    "text": "with the fact that it was basically performing it on a summation of the Pod resource requests across all your containers which isn't ideal if you have",
    "start": "818440",
    "end": "825000"
  },
  {
    "text": "containers with highly different uh utilization levels for CPU and memory",
    "start": "825000",
    "end": "830480"
  },
  {
    "text": "and so now you can do it on the container resource um type which allows you to basically scale up your your pods",
    "start": "830480",
    "end": "836399"
  },
  {
    "text": "based on the utilization of a single container um rather than having to do it across the entire pod um so that was or",
    "start": "836399",
    "end": "843079"
  },
  {
    "text": "it is currently beta uh and it was beta starting in 127 and will hit GA in 130",
    "start": "843079",
    "end": "850079"
  },
  {
    "text": "so we're super excited about that for the carpenter project updates",
    "start": "850079",
    "end": "855759"
  },
  {
    "text": "um V1 beta 1 graduation everyone loves this topic um so V1 beta 1 uh we were",
    "start": "855759",
    "end": "862519"
  },
  {
    "text": "really excited to announce B1 beta 1 effectively last cucon cucon and a um we",
    "start": "862519",
    "end": "869120"
  },
  {
    "text": "introduced V1 beta 1 November of 2023 and this was kind of the natural progression from our V1 Alpha 5 apis",
    "start": "869120",
    "end": "875399"
  },
  {
    "text": "which had been around for quite a while and we'd gone through various iterations of the alpha apis um and had uh kind of",
    "start": "875399",
    "end": "883120"
  },
  {
    "text": "repositioned our resource naming and just the general API to remove a lot of technical debt and to also align it with",
    "start": "883120",
    "end": "890199"
  },
  {
    "text": "a lot of kubernetes more Upstream Concepts um kind of duve tailing into",
    "start": "890199",
    "end": "895560"
  },
  {
    "text": "the the acceptance of EST toig Auto scaling back in November as as well so um we originally had resources that were",
    "start": "895560",
    "end": "902320"
  },
  {
    "text": "called provisioner machine and no template we kind of realigned these resources around node pool node claim",
    "start": "902320",
    "end": "909639"
  },
  {
    "text": "and node class and these took inspiration from deployment and also from Storage Concepts um deployment",
    "start": "909639",
    "end": "916519"
  },
  {
    "text": "because node pools effectively if you look at their their actual manifest spec um they have a section called template",
    "start": "916519",
    "end": "922920"
  },
  {
    "text": "and the reason they have a section called template is because no pools templa something called node claims and not claims are responsible for basically",
    "start": "922920",
    "end": "929720"
  },
  {
    "text": "creating a request for a node resource um which is then provisioned by whatever",
    "start": "929720",
    "end": "935279"
  },
  {
    "text": "clad provider you're running with Carpenter um and likewise node classes allow you to Define like a flavor of a",
    "start": "935279",
    "end": "942160"
  },
  {
    "text": "node claim that you're wanting to launch so node classes you describe and effectively today it is the cloud",
    "start": "942160",
    "end": "948040"
  },
  {
    "text": "provider specific API so not classes allow you to describe what's the image I want which subnets do I launch in what's",
    "start": "948040",
    "end": "954160"
  },
  {
    "text": "my security groups um these kinds of things that are more cloud provider specific and some ERS Cloud providers",
    "start": "954160",
    "end": "960000"
  },
  {
    "text": "are opinionated about them and others are not um we collapsed a lot of the",
    "start": "960000",
    "end": "965839"
  },
  {
    "text": "disruption detail so there was a lot of I would say a lot of technical debt around the disruption sections um that",
    "start": "965839",
    "end": "973120"
  },
  {
    "text": "we had in in provisioner and V1 off of five um we introduced consolidation I",
    "start": "973120",
    "end": "979000"
  },
  {
    "text": "think it quite a while ago now but we had consolidation and this was mutually exclusive to a concept called emptiness",
    "start": "979000",
    "end": "985240"
  },
  {
    "text": "and you couldn't set both at the same time and that was kind of annoying and we generally thought that conceptually",
    "start": "985240",
    "end": "991199"
  },
  {
    "text": "consolidation is emptiness like consolidation already reasons about empty nodes it reasons about under wise",
    "start": "991199",
    "end": "997160"
  },
  {
    "text": "nodes it kind of is a suet of the existing like emptiness behavior and so we collapse on consolidation into a",
    "start": "997160",
    "end": "1003839"
  },
  {
    "text": "single thing and now we call it consolidation policy and you can configure the aggressiveness of that",
    "start": "1003839",
    "end": "1010120"
  },
  {
    "text": "consolidation policy and consolidation in general with this field called consolidate after consolidation policy",
    "start": "1010120",
    "end": "1015319"
  },
  {
    "text": "also allows you to to configure how aggressive you want to be um you can either say consolidate my nodes when",
    "start": "1015319",
    "end": "1020920"
  },
  {
    "text": "they're empty or consolidate my nodes when they're slightly underutilized or underutilized in general because when",
    "start": "1020920",
    "end": "1026079"
  },
  {
    "text": "Carpenter sees your pods can reschedule elsewhere it will scale you down or launch Replacements that are",
    "start": "1026079",
    "end": "1031640"
  },
  {
    "text": "cheaper um and that all fell under this section called disruption and it was kind of teeing up additional work around",
    "start": "1031640",
    "end": "1037558"
  },
  {
    "text": "disruption controls which we've done a lot of work over disruption controls in the last six months and we're also",
    "start": "1037559",
    "end": "1043038"
  },
  {
    "text": "planning on doing a lot more work on disruption controls as we head towards B1 stability um we removed web hooks",
    "start": "1043039",
    "end": "1050799"
  },
  {
    "text": "which was another big paino for a lot of users and replaced it with Cal and uh we",
    "start": "1050799",
    "end": "1057120"
  },
  {
    "text": "graduated our drift feature to Beta which means it's enabled by default starting in",
    "start": "1057120",
    "end": "1062879"
  },
  {
    "text": "b033 so yeah disruption got better with budgets so in",
    "start": "1063480",
    "end": "1069840"
  },
  {
    "text": "v34 we introduced this concept called disruption budgets which are effectively no disruption budgets um similar to pod",
    "start": "1069840",
    "end": "1077240"
  },
  {
    "text": "disruption budget Concepts um you can tell Carpenter how aggressive you want",
    "start": "1077240",
    "end": "1083240"
  },
  {
    "text": "disruption to be um and you can also effectively Define maintenance windows",
    "start": "1083240",
    "end": "1088720"
  },
  {
    "text": "on your disruption Behavior which was a heavily asked for thing um so if you",
    "start": "1088720",
    "end": "1094120"
  },
  {
    "text": "just look at the spec here um this disruption budget so disruption budgets",
    "start": "1094120",
    "end": "1100280"
  },
  {
    "text": "effectively Define like the most restrictive one is the one that's going to apply so this budget here says okay",
    "start": "1100280",
    "end": "1106919"
  },
  {
    "text": "you can't you can't deprovision you can't disrupt nodes during non-working hours so there's a schedule component",
    "start": "1106919",
    "end": "1112880"
  },
  {
    "text": "there's a duration component says Monday through Friday from 5:00 p.m. to 8:00 a.m. don't disrupt my nodes from",
    "start": "1112880",
    "end": "1118360"
  },
  {
    "text": "Saturday to Sunday never disrupt my nodes uh and then otherwise you can set a percentage or a numerical value on the",
    "start": "1118360",
    "end": "1125120"
  },
  {
    "text": "disruption budget you don't need to have a duration or a schedule attached to it um to say either disrupt so if I have over 50",
    "start": "1125120",
    "end": "1132840"
  },
  {
    "text": "nodes um don't disrupt more than five because again the most restrictive one applies I have under 50 noes",
    "start": "1132840",
    "end": "1139480"
  },
  {
    "text": "then consider 10% because that would obviously be more restrictive under 50 um and it's it's calculating that based",
    "start": "1139480",
    "end": "1145480"
  },
  {
    "text": "off of the number of nodes the node pool manages so that was a huge win what this",
    "start": "1145480",
    "end": "1151440"
  },
  {
    "text": "also allowed us to do was it allowed us because now disruption is user configurable how aggressive it is the",
    "start": "1151440",
    "end": "1156840"
  },
  {
    "text": "parallelism attached to it is user configurable it meant we could effectively be as aggressive as we",
    "start": "1156840",
    "end": "1162480"
  },
  {
    "text": "wanted to be on the back end respecting user configured limits prior to the introduction of this feature we were",
    "start": "1162480",
    "end": "1168760"
  },
  {
    "text": "doing one replacement and on expired nodes and one replacement on drifted nodes at a time and that was causing a",
    "start": "1168760",
    "end": "1174799"
  },
  {
    "text": "lot of pain for a lot of people who had like thousand node clusters and they wanted them all to roll and they're like",
    "start": "1174799",
    "end": "1180640"
  },
  {
    "text": "can you please do more than one at a time so this allowed us to basically say you know if user wants do 10 they want to do 100% you can go and do it um don't",
    "start": "1180640",
    "end": "1188440"
  },
  {
    "text": "do that in production though that's not a good idea um so just this behavior is",
    "start": "1188440",
    "end": "1194280"
  },
  {
    "text": "kind of interesting and we definitely need to do look a little bit more into why this behavior is but is um some of",
    "start": "1194280",
    "end": "1199559"
  },
  {
    "text": "it has to do with our safety mechanisms around how we reschedule pods to make sure that the pods are still schedulable",
    "start": "1199559",
    "end": "1206799"
  },
  {
    "text": "after we do subsequent disruption operations but effectively what we saw is you know you look here it's like",
    "start": "1206799",
    "end": "1212600"
  },
  {
    "text": "we're doing one at a time so effectively 1.5 per minute um and if you look now",
    "start": "1212600",
    "end": "1219200"
  },
  {
    "text": "after we basically made a wide open disruption budget we can do at maximally 20 a minute which very fast um or at",
    "start": "1219200",
    "end": "1227919"
  },
  {
    "text": "least quite a bit faster faster than it was before and scheduling got quite quite a bit better over the last six months as",
    "start": "1227919",
    "end": "1234240"
  },
  {
    "text": "well um we did a lot of work on our scheduling performance by CPU profiling",
    "start": "1234240",
    "end": "1239960"
  },
  {
    "text": "it and so if you look at 28 which we're on 35.2 as the latest version right now",
    "start": "1239960",
    "end": "1246480"
  },
  {
    "text": "so if you look at 28 um we do scheduling benchmarking on the Upstream repo and so",
    "start": "1246480",
    "end": "1253000"
  },
  {
    "text": "this benchmarking I think now it goes up to 5,000 but at the time it only went to 3500 and it made like divers first sets",
    "start": "1253000",
    "end": "1258880"
  },
  {
    "text": "of PODS and then scheduled them and saw how long that scheduling took and at 3500 pods we were looking at about 30",
    "start": "1258880",
    "end": "1266080"
  },
  {
    "text": "seconds back on V 028 that got cut way down to about 1 second on v0",
    "start": "1266080",
    "end": "1274000"
  },
  {
    "text": "35.2 which is the latest version and then on head we did some more improvements over the last like 20",
    "start": "1274000",
    "end": "1279480"
  },
  {
    "text": "commits and so now it's like 10 milliseconds at least at that that scale",
    "start": "1279480",
    "end": "1284919"
  },
  {
    "text": "for that kind of scheduling simulation now not all those pods are like anti Infinity so there's ways that they could be that could be more expensive and more",
    "start": "1284919",
    "end": "1291480"
  },
  {
    "text": "complex but we effectively cut our scheduling down like 300 times so that's",
    "start": "1291480",
    "end": "1297679"
  },
  {
    "text": "pretty good um and the last one I guess maybe the second to last one um we",
    "start": "1297679",
    "end": "1303720"
  },
  {
    "text": "improved our quad provider support So AWS kind of shephered on and built Carpenter um so AWS is clad provider",
    "start": "1303720",
    "end": "1311159"
  },
  {
    "text": "support and kind of it did from the beginning um Azure supported Carpenter starting at last cucon they announced it",
    "start": "1311159",
    "end": "1317760"
  },
  {
    "text": "I think right during cuon so Azure has support um we launched qua provider support uh which is uh kubernetes",
    "start": "1317760",
    "end": "1324840"
  },
  {
    "text": "without cuet and that that one's more of like a toy cloud provider um if you're interested in just messing with",
    "start": "1324840",
    "end": "1330000"
  },
  {
    "text": "Carpenter without having to run any capacity this exists within our repo it's linked in the slides once you get the slides after this is done you can",
    "start": "1330000",
    "end": "1336279"
  },
  {
    "text": "mess around with Carpenter without having to pay for it um except for the capacity that Carpenter runs on and then",
    "start": "1336279",
    "end": "1341840"
  },
  {
    "text": "cppy is also working on a carpenter provider that would enable it to provision copy resources which is kind",
    "start": "1341840",
    "end": "1347400"
  },
  {
    "text": "of a cool concept so so that one's coming soon that one doesn't exist right now um but there's been a bunch of work",
    "start": "1347400",
    "end": "1352720"
  },
  {
    "text": "in the community and a working group that's happening around that as well I'm going to quickly run through",
    "start": "1352720",
    "end": "1357799"
  },
  {
    "text": "this um looking forward what what are we kind of looking at over the next six to",
    "start": "1357799",
    "end": "1363440"
  },
  {
    "text": "eight months um V1 release is kind of our North Star",
    "start": "1363440",
    "end": "1368919"
  },
  {
    "text": "right now we're looking towards stability and so um we're doing a lot of",
    "start": "1368919",
    "end": "1374760"
  },
  {
    "text": "work to figure out what's on our V1 road map and what does the V1 AP look like so",
    "start": "1374760",
    "end": "1379880"
  },
  {
    "text": "um if you're interested in involved in the community be on the lookout for rfc's that will describe what we think",
    "start": "1379880",
    "end": "1385880"
  },
  {
    "text": "these things should look like and obviously give feedback around that um a lot of this will include things like improved observability we're going to",
    "start": "1385880",
    "end": "1391919"
  },
  {
    "text": "improve our kind of our entire metric story right now where it's a little bit I would say sporadic across the repo and",
    "start": "1391919",
    "end": "1397919"
  },
  {
    "text": "not holistically well defined um we're going to do a lot of work there to better Define that improve our",
    "start": "1397919",
    "end": "1404919"
  },
  {
    "text": "observability around status conditions and Native kubernetes objects and inventing and all that there's going to be a ton of effort that's put into that",
    "start": "1404919",
    "end": "1411760"
  },
  {
    "text": "um we're talking about doing more realistic benchmarking with the quock clad provider that I mentioned so",
    "start": "1411760",
    "end": "1418720"
  },
  {
    "text": "um we currently do the scheduling benchmarking simulations today which are useful but maybe not as accurate in the",
    "start": "1418720",
    "end": "1426039"
  },
  {
    "text": "sense that they don't mock real nodes um so we can go a step for further and do",
    "start": "1426039",
    "end": "1431159"
  },
  {
    "text": "like Live Test scheduling simulations we can also measure our disruption performance through that as well which we don't quite have benchmarking on that",
    "start": "1431159",
    "end": "1437600"
  },
  {
    "text": "today and then um a lot more work on disruption controls so we're talking",
    "start": "1437600",
    "end": "1444400"
  },
  {
    "text": "about a bunch of different features not all these may make it in but these were mingly put up there for example um I",
    "start": "1444400",
    "end": "1449679"
  },
  {
    "text": "think a lot of these will most likely make it in in some form but ton disruption condition this is one people",
    "start": "1449679",
    "end": "1456200"
  },
  {
    "text": "want so that we stop scheduling or that Cube schedulers stop scheduling pods to nodes that we're going to take away soon",
    "start": "1456200",
    "end": "1462279"
  },
  {
    "text": "um because that's caused different issues with different ways that you can control Carpenters disruption mechanisms",
    "start": "1462279",
    "end": "1469159"
  },
  {
    "text": "um a concept called disruption grace period which means that if for whatever reason like there's a pdb blocking my",
    "start": "1469159",
    "end": "1474240"
  },
  {
    "text": "disruption and I like Carpenter hasn't been able to disrupt this node for Way",
    "start": "1474240",
    "end": "1479399"
  },
  {
    "text": "Beyond when it was initially like considered for disruption you can say actually Force like proceed on maybe not",
    "start": "1479399",
    "end": "1486600"
  },
  {
    "text": "force kill it but like proceed on and and ignore my uh protections past a",
    "start": "1486600",
    "end": "1492480"
  },
  {
    "text": "certain time frame like you have a cve you need to patch it that's kind of an example like a day past your expiration",
    "start": "1492480",
    "end": "1497880"
  },
  {
    "text": "period please proceed on um yeah and then things like forceful",
    "start": "1497880",
    "end": "1503880"
  },
  {
    "text": "non- graceful termination which is more of our drain behavior and support for a",
    "start": "1503880",
    "end": "1509080"
  },
  {
    "text": "future or support for consolidate after with consolidation policy under utiliz",
    "start": "1509080",
    "end": "1514159"
  },
  {
    "text": "which isn't supported right now and a lot of people want so these are all things we're thinking about moving forward uh with that I'll hand it off to",
    "start": "1514159",
    "end": "1521240"
  },
  {
    "text": "ma to talk about cast updates um thanks so uh coming to class",
    "start": "1521240",
    "end": "1526960"
  },
  {
    "text": "scaler uh we've been been focusing on performance recently and uh performance in few different dimensions so um one",
    "start": "1526960",
    "end": "1534480"
  },
  {
    "text": "simple update is that we've done a lot of work to optimize CPU and memory usage of of autoscaler so between 127 and 129",
    "start": "1534480",
    "end": "1544440"
  },
  {
    "text": "we've seen more than 30% Improvement in in both in most clusters we've been",
    "start": "1544440",
    "end": "1549799"
  },
  {
    "text": "testing and the improvements actually are bigger in larger clusters so in in",
    "start": "1549799",
    "end": "1555399"
  },
  {
    "text": "5K node tests that we run we've seen more than 2x Improvement this is split",
    "start": "1555399",
    "end": "1560960"
  },
  {
    "text": "between 128 and 129 so probably best to measure between",
    "start": "1560960",
    "end": "1566039"
  },
  {
    "text": "127 um another thing we've done um is we have enabled by default the parallel",
    "start": "1566039",
    "end": "1572120"
  },
  {
    "text": "drain logic which we first introduced in 126 this is essentially a complete",
    "start": "1572120",
    "end": "1577720"
  },
  {
    "text": "reimplementation of the scal down logic we had before and um it makes it both uh",
    "start": "1577720",
    "end": "1585360"
  },
  {
    "text": "safer and um more importantly or more significantly faster uh depending on the",
    "start": "1585360",
    "end": "1591640"
  },
  {
    "text": "configuration and the parallelism level you can uh you you set it can be many",
    "start": "1591640",
    "end": "1597480"
  },
  {
    "text": "many times faster previously we we would only drain one note at a time so with parallelism of 10 which I think is the",
    "start": "1597480",
    "end": "1603880"
  },
  {
    "text": "default one you get essentially 10 times faster scaled down um and you can play",
    "start": "1603880",
    "end": "1609240"
  },
  {
    "text": "with the settings see how much it uh uh how how far you can go in a specific",
    "start": "1609240",
    "end": "1614679"
  },
  {
    "text": "environment um the improvements generally are especially visible in in",
    "start": "1614679",
    "end": "1620080"
  },
  {
    "text": "clusters where ports use long graceful termin termination periods because those make for very long drain um and another",
    "start": "1620080",
    "end": "1628080"
  },
  {
    "text": "effort which we are now working on uh we're not quite done but we've already made some progress is optimizing scale",
    "start": "1628080",
    "end": "1634159"
  },
  {
    "text": "up by essentially reducing the amount of u One controller synchronously waiting",
    "start": "1634159",
    "end": "1640640"
  },
  {
    "text": "for another controller uh let me go into a bit more detail uh on this one so and",
    "start": "1640640",
    "end": "1646559"
  },
  {
    "text": "this is this is on honly a bit of a simplification but um broadly speaking",
    "start": "1646559",
    "end": "1652480"
  },
  {
    "text": "if you create a deployment let's say just one po deployment for for Simplicity for now what's going to",
    "start": "1652480",
    "end": "1658000"
  },
  {
    "text": "happen is that Cube controller manager is going to create a pot this this",
    "start": "1658000",
    "end": "1663600"
  },
  {
    "text": "actually has a few steps but uh let's just say it's creation of a pot um then scheduler is going to observe that pot",
    "start": "1663600",
    "end": "1670399"
  },
  {
    "text": "mark it as um unschedulable and uh it's only this unschedulable pod that triggers",
    "start": "1670399",
    "end": "1676200"
  },
  {
    "text": "autoscaler so only at this point do autoscaler notice it and and basically it goes on and requests more VMS those",
    "start": "1676200",
    "end": "1683000"
  },
  {
    "text": "VMS obviously need time to boot up and finally when the schedule notices the noes are there it's able to schedule the",
    "start": "1683000",
    "end": "1689960"
  },
  {
    "text": "pot U now what we've already done is we have uh manage to essentially cut out",
    "start": "1689960",
    "end": "1696320"
  },
  {
    "text": "this um uh one round trip to Schuler so autoscaler can now react to pods before",
    "start": "1696320",
    "end": "1703600"
  },
  {
    "text": "they are marked um as unschedulable by the scheduler um and this already provides",
    "start": "1703600",
    "end": "1709840"
  },
  {
    "text": "some benefit in um uh latency but our our goal hopefully in",
    "start": "1709840",
    "end": "1716240"
  },
  {
    "text": "131 we'll see about that but our our end state that we'd like to get to is essentially this model so what we'd like",
    "start": "1716240",
    "end": "1723640"
  },
  {
    "text": "to do is have autoscaler look directly at deployment job and and similar controllers look at replica account look",
    "start": "1723640",
    "end": "1730640"
  },
  {
    "text": "at pots speec essentially all the information we need is already there we don't really need to wait for ports to",
    "start": "1730640",
    "end": "1736360"
  },
  {
    "text": "be created um and so essentially the logic in autoscaler and node startup can",
    "start": "1736360",
    "end": "1742559"
  },
  {
    "text": "potentially happen in parallel with Port creation and any schedu action um as I",
    "start": "1742559",
    "end": "1748679"
  },
  {
    "text": "said this this one is still uh being implemented um the motivation for these",
    "start": "1748679",
    "end": "1753720"
  },
  {
    "text": "changes is obviously um latency it's it's just going to be faster if you don't have to go through so many steps",
    "start": "1753720",
    "end": "1760880"
  },
  {
    "text": "and some of them can actually be significant in um large scale scenarios so we've seen um Port Q up in schuer",
    "start": "1760880",
    "end": "1768679"
  },
  {
    "text": "quite a bit um another thing is it actually also improves the quality of",
    "start": "1768679",
    "end": "1773799"
  },
  {
    "text": "Autos scaling decisions so if you if you create a thousand pods today you're going to see essentially cluster",
    "start": "1773799",
    "end": "1780880"
  },
  {
    "text": "autoscaler do multiple smaller scale UPS sequentially this is because as ports are created autoscaler already triggers",
    "start": "1780880",
    "end": "1787640"
  },
  {
    "text": "small scale UPS so for each of those scale UPS it only has a very partial vision of what's going on and so it",
    "start": "1787640",
    "end": "1794120"
  },
  {
    "text": "makes suboptimal decisions if we are aware of all the pods that are are coming we can actually do the bin",
    "start": "1794120",
    "end": "1800600"
  },
  {
    "text": "packing simulation with all of those poorts and use that knowledge to select best um shape of machine that that we",
    "start": "1800600",
    "end": "1808480"
  },
  {
    "text": "want to use um ah and one thing to mention is the the change in the skipping the",
    "start": "1808480",
    "end": "1815000"
  },
  {
    "text": "waiting for scheduler is available in 128 129 but it is opt in for now so if",
    "start": "1815000",
    "end": "1821519"
  },
  {
    "text": "you want to feel free to test it maybe not in production day one but it should",
    "start": "1821519",
    "end": "1827480"
  },
  {
    "text": "be working and now I have two very small changes that I wanted to quickly mention just because",
    "start": "1827480",
    "end": "1833640"
  },
  {
    "text": "of I think um potential impacting some people so one thing we've done is we're",
    "start": "1833640",
    "end": "1839960"
  },
  {
    "text": "finally removing ignore Tain this is something I I've talked uh actually in the past on cubes on Cube con it was a",
    "start": "1839960",
    "end": "1846640"
  },
  {
    "text": "mechanism that caused a lot of confusion basically ignore Tain uh was a way to or you could Mark",
    "start": "1846640",
    "end": "1854080"
  },
  {
    "text": "taint to be ignored by autoscaler in scaleup logic which would um allow autoscaler to create new nodes",
    "start": "1854080",
    "end": "1861120"
  },
  {
    "text": "as if the taint wasn't there so in simulations it would know the pods will be able to schedule uh and this really",
    "start": "1861120",
    "end": "1868519"
  },
  {
    "text": "was designed only for the case of um taints that are used for custom node initialization like installing um custom",
    "start": "1868519",
    "end": "1876360"
  },
  {
    "text": "device plugins is is the usual one um and it was used in many other cases uh",
    "start": "1876360",
    "end": "1882200"
  },
  {
    "text": "leading to very difficult to debug issues so we split this logic into",
    "start": "1882200",
    "end": "1887919"
  },
  {
    "text": "startup taint and Status taint startup taint is what ignore Tain was before",
    "start": "1887919",
    "end": "1893080"
  },
  {
    "text": "status taint is I think what a lot of people wanted uh ignore Tain to be so",
    "start": "1893080",
    "end": "1899000"
  },
  {
    "text": "it's just a more General um any Tain that that should not be taken into",
    "start": "1899000",
    "end": "1904320"
  },
  {
    "text": "account in uh scaleup logic and um final small announcement is we are changing",
    "start": "1904320",
    "end": "1910720"
  },
  {
    "text": "the format of our status config map um it's technically a backward incompatible change since it used to be just humanly",
    "start": "1910720",
    "end": "1918320"
  },
  {
    "text": "impossible to pass format and now it's going to be Y and we're also putting more information there especially about",
    "start": "1918320",
    "end": "1925080"
  },
  {
    "text": "backoffs which uh have been a challenge to deback in the past so we hope this is",
    "start": "1925080",
    "end": "1930440"
  },
  {
    "text": "this is going to help um so thank you and do you have any",
    "start": "1930440",
    "end": "1937559"
  },
  {
    "text": "[Applause]",
    "start": "1938860",
    "end": "1944170"
  },
  {
    "text": "questions hello thanks for your presentation it is for you John I'm using Carpenter for a",
    "start": "1946840",
    "end": "1954200"
  },
  {
    "text": "lot of months and actually we have a a little problem or maybe a a misunderstanding how it works we have a",
    "start": "1954200",
    "end": "1961559"
  },
  {
    "text": "lot of pod in bending States Carpenter spinup VMS but are start starting and",
    "start": "1961559",
    "end": "1970039"
  },
  {
    "text": "few minutes later some them are killed and new nodes are creating a little",
    "start": "1970039",
    "end": "1975360"
  },
  {
    "text": "smaller so boards are are just pod with are starting arived and are going to the",
    "start": "1975360",
    "end": "1983519"
  },
  {
    "text": "new VM do you know this issue or is is something yeah that's interesting okay",
    "start": "1983519",
    "end": "1990760"
  },
  {
    "text": "um so sorry just so I understand you're saying that you have nodes that are launching pods are trying to schedule to",
    "start": "1990760",
    "end": "1996840"
  },
  {
    "text": "those new nodes and then we I assume we kill those nodes and then restart",
    "start": "1996840",
    "end": "2002159"
  },
  {
    "text": "smaller versions of those nodes yes that's correct yeah I'm not I unfortunately am not familiar with that so I can talk after and also um if there",
    "start": "2002159",
    "end": "2009919"
  },
  {
    "text": "is a problem there we should definitely open an issue um yeah I've haven't heard that unfortunately so yeah okay thank",
    "start": "2009919",
    "end": "2015320"
  },
  {
    "text": "you",
    "start": "2015320",
    "end": "2017679"
  },
  {
    "text": "yeah I have a question h on cluster AOS scaler uh there is a long issue about",
    "start": "2020559",
    "end": "2027880"
  },
  {
    "text": "scaling down and cluster Auto scaler not consider the balance between multiple",
    "start": "2027880",
    "end": "2033799"
  },
  {
    "text": "availability zones in case the node group spans multiple of them and if you look at the documentation of",
    "start": "2033799",
    "end": "2040360"
  },
  {
    "text": "all the cloud providers um JY Microsoft or Amazon they all say yeah create one",
    "start": "2040360",
    "end": "2047559"
  },
  {
    "text": "node pool per Cloud per availability Zone and use the balance node groups but",
    "start": "2047559",
    "end": "2053878"
  },
  {
    "text": "this like complicat stuff right so I wanted to understand if this issue is",
    "start": "2053879",
    "end": "2060200"
  },
  {
    "text": "never going to be solved because is spending since 2020 uh and also I have no idea if",
    "start": "2060200",
    "end": "2066079"
  },
  {
    "text": "Carpenter does better because I don't don't know Carpenter at all so I'll take the clust out to scaler",
    "start": "2066079",
    "end": "2072720"
  },
  {
    "text": "side and then maybe Jonathan can do the carpenter uh so we don't as you",
    "start": "2072720",
    "end": "2077839"
  },
  {
    "text": "mentioned we don't have um any mechanism right now for for balancing between availability zones on on scale down we",
    "start": "2077839",
    "end": "2085000"
  },
  {
    "text": "only have the balance similar node groups which only triggers in scale up there is no current ongoing effort to",
    "start": "2085000",
    "end": "2092280"
  },
  {
    "text": "fix it which I don't think means that it's never going to be fixed uh we're definitely open to contributions and uh",
    "start": "2092280",
    "end": "2100839"
  },
  {
    "text": "um hopefully at some point we'll just get uh to fixing it on on the side of of",
    "start": "2100839",
    "end": "2106280"
  },
  {
    "text": "maintainers of class AOS scale we're definitely aware of this issue yeah on the and on the carpenter",
    "start": "2106280",
    "end": "2113520"
  },
  {
    "text": "side um I kind of I guess I'll answer the the",
    "start": "2113520",
    "end": "2119359"
  },
  {
    "text": "like which do I use kind of style of question I guess because that's that's a question that I think we we get a lot",
    "start": "2119359",
    "end": "2125240"
  },
  {
    "text": "generally I'll let not check out any detail if he wants um just kind of like what we said at the beginning like they",
    "start": "2125240",
    "end": "2130800"
  },
  {
    "text": "each kind of have their different trade-offs in terms of where you want your configuration to live and what you want managing your life cycle and",
    "start": "2130800",
    "end": "2136320"
  },
  {
    "text": "there's also a difference and that wasn't covered that in terms of like CL I mean it kind of makes sense there's only four Cloud providers within",
    "start": "2136320",
    "end": "2141560"
  },
  {
    "text": "Carpenter so there's a difference in cloud provider support right now as well um so you have to kind of use all those",
    "start": "2141560",
    "end": "2146640"
  },
  {
    "text": "things to evaluate which which project is is right for you and maybe the answer is both um in terms of Carpenters",
    "start": "2146640",
    "end": "2154160"
  },
  {
    "text": "handling of like multi- a scenarios you because all of the configuration exists",
    "start": "2154160",
    "end": "2160760"
  },
  {
    "text": "inside of the cluster and it's not necessarily tied to like like I said we send off all that information to the",
    "start": "2160760",
    "end": "2166079"
  },
  {
    "text": "quad provider and it makes its decision and all that configuration is not tied to the clad provider API",
    "start": "2166079",
    "end": "2172119"
  },
  {
    "text": "multi-az uh it requires one basically one configuration surface in Carpenter versus having to to create multiple node",
    "start": "2172119",
    "end": "2179000"
  },
  {
    "text": "groups like you would in in Cass traditionally I hope that answers your question",
    "start": "2179000",
    "end": "2185880"
  },
  {
    "text": "yeah um yeah I'm I'm curious about the because now you have two projects that",
    "start": "2186560",
    "end": "2191920"
  },
  {
    "text": "are kind of doing the same thing and maybe you can talk a little bit about what are like the strongest motivations",
    "start": "2191920",
    "end": "2197400"
  },
  {
    "text": "for not ending up with just one project doing the same thing there's probably",
    "start": "2197400",
    "end": "2203319"
  },
  {
    "text": "some reason I I can start and then we'll see uh I think I think uh the major",
    "start": "2203319",
    "end": "2209359"
  },
  {
    "text": "motivation is um the major difference",
    "start": "2209359",
    "end": "2214400"
  },
  {
    "text": "section we had before so they do the same job",
    "start": "2214400",
    "end": "2219680"
  },
  {
    "text": "um and and sort of externally as a user when you run PS there may not be much difference but how they work behind the",
    "start": "2219680",
    "end": "2226800"
  },
  {
    "text": "scenes and in particular how they integrate with the cloud provider is is very different and so the I I think",
    "start": "2226800",
    "end": "2233240"
  },
  {
    "text": "large part of the motivation is um it may depending on the underlying apis and",
    "start": "2233240",
    "end": "2240119"
  },
  {
    "text": "and behaviors of cloud providers those are quite different it may be that one or the other approach just works better",
    "start": "2240119",
    "end": "2246440"
  },
  {
    "text": "and uh I think think we are far too Divergent in in our code base to be able",
    "start": "2246440",
    "end": "2252119"
  },
  {
    "text": "to just say it's one project with two different ways of of integrating cloud provider yeah I think maybe one example",
    "start": "2252119",
    "end": "2259319"
  },
  {
    "text": "is if you um there's specifics on node upgrade behavior that you like about how",
    "start": "2259319",
    "end": "2264720"
  },
  {
    "text": "your clad provider handles it like it does like a a maybe a starting kind of",
    "start": "2264720",
    "end": "2269760"
  },
  {
    "text": "like an exponential roll out where it would start slow and then move forward very quickly um Carpenter doesn't support that kind of thing today so um",
    "start": "2269760",
    "end": "2278000"
  },
  {
    "text": "potentially there's differences in how the life cycle management is handled on the quad provider side that you might like more and therefore you'd want to",
    "start": "2278000",
    "end": "2284800"
  },
  {
    "text": "use Cass versus Carpenter and then there's also the detail about like do you want to manage the the configuration",
    "start": "2284800",
    "end": "2289839"
  },
  {
    "text": "surface through the I mean and that one's more I guess minor but do you want to manage the configuration surface through the quad provider or do you want",
    "start": "2289839",
    "end": "2296040"
  },
  {
    "text": "to manage it directly in kubernetes um yeah and I think they're so",
    "start": "2296040",
    "end": "2301119"
  },
  {
    "text": "philosophically different at this point that it's it's really hard to say like oh I could merge them because they do solve similar problems but they're are",
    "start": "2301119",
    "end": "2307880"
  },
  {
    "text": "like much said they are so like philosophically different around how they actuate everything that it's it's really hard to say oh should we should",
    "start": "2307880",
    "end": "2313920"
  },
  {
    "text": "just converge them yeah thanks I uh I just learned about",
    "start": "2313920",
    "end": "2318960"
  },
  {
    "text": "the Do Not disrupt annotations so good but I was wondering what the difference in implementation um",
    "start": "2318960",
    "end": "2325200"
  },
  {
    "text": "between uh cast and Carpenter are especially when it comes to does if",
    "start": "2325200",
    "end": "2330319"
  },
  {
    "text": "there is The annotation existing there does it especially for cast does it just",
    "start": "2330319",
    "end": "2335480"
  },
  {
    "text": "not scale because the cloud provider handles that and you can't specify hey",
    "start": "2335480",
    "end": "2341160"
  },
  {
    "text": "you can scale but just not this node um and I assume that there's better support in Carpenter for scaling a different",
    "start": "2341160",
    "end": "2348680"
  },
  {
    "text": "node uh but if you could elaborate on that I'd appreciate it um I'm I'm not",
    "start": "2348680",
    "end": "2353839"
  },
  {
    "text": "sure I fully understand the question you mean scaling down noes or right during the scale down or the consolidation um",
    "start": "2353839",
    "end": "2360880"
  },
  {
    "text": "if there is the Do Not disrupt annotation or do not evict annotation what happens uh in in Cass oh I see so I",
    "start": "2360880",
    "end": "2370119"
  },
  {
    "text": "think one one kind of um misconception or one easy assumption to make about C",
    "start": "2370119",
    "end": "2376560"
  },
  {
    "text": "is it just changes the replica count of of the underlying node group but that's not the case it it picks a specific node",
    "start": "2376560",
    "end": "2384160"
  },
  {
    "text": "so it generally expects the underlying node group to have a function that",
    "start": "2384160",
    "end": "2389200"
  },
  {
    "text": "allows reducing size by deleting a specific instance and so it's always a specific instance that's selected for",
    "start": "2389200",
    "end": "2396079"
  },
  {
    "text": "deletion that makes so much sense thank you is it same for Carpenter yes it",
    "start": "2396079",
    "end": "2401119"
  },
  {
    "text": "picks a specific instance yeah thank you hey thanks for the talk uh there's",
    "start": "2401119",
    "end": "2407839"
  },
  {
    "text": "at least one other uh Autos Skilling provider or implementation I'm aware of that's uh escalator uh which I think was",
    "start": "2407839",
    "end": "2414640"
  },
  {
    "text": "published by Cassian um do you know if there's any integration or if there's any uh initiative to also have them join",
    "start": "2414640",
    "end": "2422040"
  },
  {
    "text": "uh the Sig I'm not aware of any we also have Sig here but seems like K said no yeah",
    "start": "2422040",
    "end": "2430040"
  },
  {
    "text": "that seemed like a no so I think uh not so much I think for Carpenter we were really as a SI approach by Carpenter",
    "start": "2430040",
    "end": "2436760"
  },
  {
    "text": "right so I think that was the um kind of Direction it happened last time but",
    "start": "2436760",
    "end": "2442920"
  },
  {
    "text": "we'll see yeah I think if there was some more effort from their side we'd obviously evaluate them yeah thank you",
    "start": "2442920",
    "end": "2451560"
  },
  {
    "text": "y uh hello I use both projects and they are great thank you for the talk uh I my",
    "start": "2453520",
    "end": "2459800"
  },
  {
    "text": "question is about Carpenter I can use the disruption but the disruption annotations to turn down uh non",
    "start": "2459800",
    "end": "2468079"
  },
  {
    "text": "production uh environment uh nodes to turn down the nonproduction envir this was what it was",
    "start": "2468079",
    "end": "2475000"
  },
  {
    "text": "mean for or disruption budget was created for some other reasons so let me",
    "start": "2475000",
    "end": "2480440"
  },
  {
    "text": "see if I so you're saying you I want to turn down a non production environment",
    "start": "2480440",
    "end": "2486839"
  },
  {
    "text": "in non working hours for example oh you want to spin down nodes during working I",
    "start": "2486839",
    "end": "2492319"
  },
  {
    "text": "see um no do not disrupt was invented for most most often for like jobs for",
    "start": "2492319",
    "end": "2497839"
  },
  {
    "text": "AIML use cases where you don't want like your job getting disrupted in the middle um obviously for stateless applications",
    "start": "2497839",
    "end": "2503760"
  },
  {
    "text": "it matters less like I mean ideally you don't want your your stateless application getting disrupted every five",
    "start": "2503760",
    "end": "2508920"
  },
  {
    "text": "minutes but if it's disrupted every now and then you should be able to recover um yeah it was more invented to say okay",
    "start": "2508920",
    "end": "2515200"
  },
  {
    "text": "let this thing finish and then once it finishes Carpenter can then act on it um",
    "start": "2515200",
    "end": "2522560"
  },
  {
    "text": "it it's less for like orchestrating the scale down because realistically what Carpenter does today is if there's a",
    "start": "2522560",
    "end": "2528520"
  },
  {
    "text": "scale down during non-w working hours of your pods then Carpenter will just naturally like with consolidation it'll",
    "start": "2528520",
    "end": "2534240"
  },
  {
    "text": "just naturally scale down for you like it'll remove the nodes because they're no longer used and and you'll save money",
    "start": "2534240",
    "end": "2540119"
  },
  {
    "text": "during non-w working hours okay thank",
    "start": "2540119",
    "end": "2544359"
  },
  {
    "text": "you thank you for all these updates very nice one got my attention the one that",
    "start": "2546119",
    "end": "2552359"
  },
  {
    "text": "you mentioned that is like um the not disruption budget that you put there uh at dat doog we are using the cluster",
    "start": "2552359",
    "end": "2558800"
  },
  {
    "text": "Auto scaler and we have a dedicated layer to to tear down the node especially when you want to roll out a",
    "start": "2558800",
    "end": "2563880"
  },
  {
    "text": "new image and so on we we proactively um uh drain the noes let's say and one of",
    "start": "2563880",
    "end": "2570119"
  },
  {
    "text": "the thing that uh we do is we have a big note pool so think about thousands of nodes and the Restriction that you have",
    "start": "2570119",
    "end": "2577200"
  },
  {
    "text": "put in this not disruption budget um that you show applies to the entire not",
    "start": "2577200",
    "end": "2582520"
  },
  {
    "text": "pool whereas the constraint that you may have may come from all the different constraint may come from different",
    "start": "2582520",
    "end": "2588559"
  },
  {
    "text": "application all running on that same not pool so you may have different",
    "start": "2588559",
    "end": "2594200"
  },
  {
    "text": "application coming with different needs in terms of pace or um you know working",
    "start": "2594200",
    "end": "2599400"
  },
  {
    "text": "on on different days or these kind of things and if you have like 10 nodes on",
    "start": "2599400",
    "end": "2604720"
  },
  {
    "text": "top of a thousand that says like I can be this rup on Monday it's going to be",
    "start": "2604720",
    "end": "2609920"
  },
  {
    "text": "applicable to everyone in your case if the setup is at not pool level I see so you're saying a scenario where you might",
    "start": "2609920",
    "end": "2616359"
  },
  {
    "text": "want to like tweak your pod disruption budgets During certain hours to stop them from being disrupted potentially is",
    "start": "2616359",
    "end": "2622160"
  },
  {
    "text": "that kind of the yeah the thing is that multiple application are running on that not pool right and some may be very",
    "start": "2622160",
    "end": "2629160"
  },
  {
    "text": "restrictive but represent on 10 nodes on top of a thousand I see so you want to say like",
    "start": "2629160",
    "end": "2635880"
  },
  {
    "text": "this application shouldn't shouldn't be disrupted during this time but everything else can be is that exactly this we tackled this this Pro this kind",
    "start": "2635880",
    "end": "2642440"
  },
  {
    "text": "of problem and the thing is that we have this setting at application Level not at not pool level I you're saying you have",
    "start": "2642440",
    "end": "2648760"
  },
  {
    "text": "it set at the application Level how do you orchestrate it today but we have a dedicated controller to do this part oh",
    "start": "2648760",
    "end": "2654280"
  },
  {
    "text": "I see out of Auto scal the cluster autoscala finally tear down the nodes when they're empty but all the we are",
    "start": "2654280",
    "end": "2660280"
  },
  {
    "text": "piloting the drain and everything we've dedicated layer yeah I think I think one interesting thing that we talked about",
    "start": "2660280",
    "end": "2665400"
  },
  {
    "text": "when we were considering no disruption budgets was like why do pod disruption budgets not have like a similar kind of",
    "start": "2665400",
    "end": "2670839"
  },
  {
    "text": "semantic because I think that would probably solve the case you're talking about to some degree this is what we created yeah I see okay yeah so so just",
    "start": "2670839",
    "end": "2678119"
  },
  {
    "text": "saying discuss I think yeah we may have it like at application Level maybe not at notp level if notp is shared by",
    "start": "2678119",
    "end": "2683440"
  },
  {
    "text": "multiple application that you may have yeah no I I I totally I think there I mean we kind of I you can tell by the",
    "start": "2683440",
    "end": "2688880"
  },
  {
    "text": "way that Cass and Carpenter think about configuration there's like there's the two layers obviously so I think the the cluster admin node pool layer is",
    "start": "2688880",
    "end": "2696280"
  },
  {
    "text": "important because admins generally need to be able to configure these things to to protect their application developers to some degree but the application layer",
    "start": "2696280",
    "end": "2702839"
  },
  {
    "text": "is obviously we we typically see like similar configuration surface at both layers because the use cases are very",
    "start": "2702839",
    "end": "2708839"
  },
  {
    "text": "similar so I think I think it's worth a discussion I completely agree and U yeah limitations of pdbs are also something",
    "start": "2708839",
    "end": "2715720"
  },
  {
    "text": "we've been talking about forever in class Al scale community so maybe that's one more thing we can work together",
    "start": "2715720",
    "end": "2721440"
  },
  {
    "text": "on okay I I think we're at this point we're like 10 11 minutes over time so I",
    "start": "2721440",
    "end": "2727359"
  },
  {
    "text": "I think in the interest of time and wrap up if anyone has a question um you can feel free to come and talk to us afterwards but thank",
    "start": "2727359",
    "end": "2734880"
  },
  {
    "text": "you",
    "start": "2735280",
    "end": "2738280"
  }
]