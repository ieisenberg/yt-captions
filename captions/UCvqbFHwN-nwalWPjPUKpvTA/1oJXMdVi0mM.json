[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "okay I guess it's time afternoon thank you all for coming it's the last session of the day you can get two beers after",
    "start": "589",
    "end": "7649"
  },
  {
    "text": "this so I won't run I won't run over so",
    "start": "7649",
    "end": "12809"
  },
  {
    "text": "today we're here to talk about all the metrics that you can pull out of your kubernetes clusters there is a wealth of",
    "start": "12809",
    "end": "19380"
  },
  {
    "text": "metrics to to look at and how do you know which ones are the right ones is a",
    "start": "19380",
    "end": "25769"
  },
  {
    "text": "great question so we'll take a look at a little bit a little bit about that today I'll be here after the talk to answer",
    "start": "25769",
    "end": "32520"
  },
  {
    "start": "28000",
    "end": "28000"
  },
  {
    "text": "any questions that you have about metrics and kubernetes I love talking about metrics and all the things and I",
    "start": "32520",
    "end": "38129"
  },
  {
    "text": "really like talking about brewing beers if you are into beer I like to brew beer so do I so that's that's that's super",
    "start": "38129",
    "end": "44579"
  },
  {
    "text": "fun so so here's here's what we're gonna go over today there are many many much",
    "start": "44579",
    "end": "51480"
  },
  {
    "text": "metrics that come out of a kubernetes systems and trying to figure out which one of those which ones of those metrics",
    "start": "51480",
    "end": "56820"
  },
  {
    "text": "are important is particularly difficult when you're getting started so we're gonna look at a couple of methods that",
    "start": "56820",
    "end": "62149"
  },
  {
    "text": "allow you to sort of winnow down the number of metrics to look at initially",
    "start": "62149",
    "end": "67340"
  },
  {
    "text": "I'm sure some of you have seen these before full for golden signals the use method the read method and will apply",
    "start": "67340",
    "end": "73799"
  },
  {
    "text": "those methods to some of the metrics that we can pull from kubernetes just to give us a place to start by no means",
    "start": "73799",
    "end": "79530"
  },
  {
    "text": "comprehensive but it is a great place to begin and we'll look at individually all",
    "start": "79530",
    "end": "84750"
  },
  {
    "text": "of the all of the metrics sources that we can get from kubernetes itself some of those come out of the box",
    "start": "84750",
    "end": "90630"
  },
  {
    "text": "installing the couplet and installing the API server others are things that you want to install inside of your",
    "start": "90630",
    "end": "96479"
  },
  {
    "text": "cluster in order to be able to gather those metrics and all this is going to be taking place in the context I'm assuming you're going to gather metrics",
    "start": "96479",
    "end": "102390"
  },
  {
    "text": "using Prometheus or some other Prometheus capable scraping target thing",
    "start": "102390",
    "end": "107670"
  },
  {
    "text": "they'll pull those metrics in and at the end we'll look at some interesting things that you can do with recording",
    "start": "107670",
    "end": "113369"
  },
  {
    "text": "rules in in Prometheus and do some metric aggregation up through your cluster so what are the important",
    "start": "113369",
    "end": "121680"
  },
  {
    "text": "metrics in your cluster we have a modest production cluster that we run at fresh",
    "start": "121680",
    "end": "127409"
  },
  {
    "text": "tracks and there are 270 3,000 more than 270 3,000 unique series",
    "start": "127409",
    "end": "133319"
  },
  {
    "text": "come off of this cluster it's an amazing amount of metrics that come out the",
    "start": "133319",
    "end": "138629"
  },
  {
    "text": "amazing the level of visibility that you get from running kubernetes is fantastic but when you install Prometheus and you",
    "start": "138629",
    "end": "145109"
  },
  {
    "text": "install Gras fauna and you open up graph on it for the first time what are you what are you greeted with nothing",
    "start": "145109",
    "end": "150870"
  },
  {
    "text": "blank dashboards nothing and then you have to start assembling all the things together to be able to bring that",
    "start": "150870",
    "end": "156840"
  },
  {
    "text": "together to say what are the important things that we're gonna do so some methods to figure out how it is that",
    "start": "156840",
    "end": "162780"
  },
  {
    "start": "161000",
    "end": "161000"
  },
  {
    "text": "we're gonna look at these these metrics there are a number of methods out there if you've read the Google SR ebook the",
    "start": "162780",
    "end": "169109"
  },
  {
    "text": "four golden signals talked about if you're only going to look at four metrics in your cluster what are the",
    "start": "169109",
    "end": "174180"
  },
  {
    "text": "four metrics you could look at and these are the format metrics latency errors",
    "start": "174180",
    "end": "179489"
  },
  {
    "text": "traffic saturation so I first sat down and I read that are like perfect that's",
    "start": "179489",
    "end": "185159"
  },
  {
    "text": "exactly what I need I can now take these and go apply them to my all of my things but what is saturation of a kafka",
    "start": "185159",
    "end": "192959"
  },
  {
    "text": "cluster mean these these metrics aren't universally applicable for all things",
    "start": "192959",
    "end": "199290"
  },
  {
    "text": "inside of your cluster because really inside of a kubernetes cluster you've got two bits that you're working them that you're thinking about you have",
    "start": "199290",
    "end": "205680"
  },
  {
    "text": "resources that are provided by the hardware and then you have applications or services and those two things are",
    "start": "205680",
    "end": "210900"
  },
  {
    "text": "fundamentally different and there's an overlap for these four for these four metrics that that we want to look at so",
    "start": "210900",
    "end": "218340"
  },
  {
    "text": "some smart people spend a lot of time thinking about this Brendon Greg came up with the use method utilization",
    "start": "218340",
    "end": "226189"
  },
  {
    "start": "220000",
    "end": "220000"
  },
  {
    "text": "saturation and errors these three a subset of the four golden from from",
    "start": "226189",
    "end": "232079"
  },
  {
    "text": "Google allow you to reason about the resources that you have running on your",
    "start": "232079",
    "end": "237419"
  },
  {
    "text": "machines so we're a resource is defined as all physical functional components so",
    "start": "237419",
    "end": "245489"
  },
  {
    "text": "in the scope of kubernetes we have four basic resources that were primarily concerned with we have CPU we have",
    "start": "245489",
    "end": "253289"
  },
  {
    "text": "memory we have disk and disk comes in two flavors both i/o and throughput and",
    "start": "253289",
    "end": "258680"
  },
  {
    "text": "capacity and we have network saturation talks about how full is this",
    "start": "258680",
    "end": "265420"
  },
  {
    "text": "thing some of these things can be described as saturation so I can say my",
    "start": "265420",
    "end": "270460"
  },
  {
    "text": "CPU is saturated in such a way that I've got extra work to do that can't be accomplished therefore I'm going to",
    "start": "270460",
    "end": "275770"
  },
  {
    "text": "queue this stuff and do it later that's the definition burnin gives to saturation and unfortunately some of the",
    "start": "275770",
    "end": "282370"
  },
  {
    "text": "air bits that we want to get out from our hardware just not available to us at the end-user especially in a kubernetes",
    "start": "282370",
    "end": "287590"
  },
  {
    "text": "environment so that's great for covering your hardware resources and it applies",
    "start": "287590",
    "end": "293620"
  },
  {
    "text": "also equally to your to the containers themselves and we'll look deep at that",
    "start": "293620",
    "end": "300000"
  },
  {
    "text": "so the next thing is the read method so Tom Wilkie from from causal now calls",
    "start": "300000",
    "end": "306310"
  },
  {
    "text": "him now Griffin alabs coined this and he it's a spin on the use method and",
    "start": "306310",
    "end": "311680"
  },
  {
    "text": "everybody why do we need another method and he's like well that's because there's the other three other metrics of",
    "start": "311680",
    "end": "317020"
  },
  {
    "text": "the four golden signals so we have rate errors in duration so apply these things to your services so this is the software",
    "start": "317020",
    "end": "324280"
  },
  {
    "text": "that runs on your cluster so it's I don't need to talk about how saturated is my service because my service doesn't",
    "start": "324280",
    "end": "332320"
  },
  {
    "text": "get saturated my hardware gets saturated but I do want to know what the incoming request rate for my services and the",
    "start": "332320",
    "end": "338290"
  },
  {
    "text": "duration that it's going to take that service to request to respond to that rate kubernetes has both of these and",
    "start": "338290",
    "end": "345910"
  },
  {
    "text": "we're gonna look at some of the metrics I can't cover them all I had to slash a bunch of slides out of the stack in",
    "start": "345910",
    "end": "350920"
  },
  {
    "text": "order to get it into 35 minutes but kubernetes has both and we'll look at and we'll apply these methods to some of",
    "start": "350920",
    "end": "356320"
  },
  {
    "text": "them so let's talk a bit about the source of metrics and where we get them",
    "start": "356320",
    "end": "361660"
  },
  {
    "text": "and then we'll apply these individually so when running a kubernetes cluster in",
    "start": "361660",
    "end": "367150"
  },
  {
    "start": "364000",
    "end": "364000"
  },
  {
    "text": "the context of collecting metrics with Prometheus the Prometheus pattern for getting metrics out of something that",
    "start": "367150",
    "end": "372580"
  },
  {
    "text": "doesn't expose in the in the native Prometheus metrics exposition format is to run an exporter and the most common",
    "start": "372580",
    "end": "379570"
  },
  {
    "text": "exporter that you're going to run in your kubernetes cluster is the node exporter you run this as a daemon set it",
    "start": "379570",
    "end": "384610"
  },
  {
    "text": "runs on every node in your cluster and it basically scrapes everything out of proc and reformats it as as a Prometheus",
    "start": "384610",
    "end": "391419"
  },
  {
    "text": "metric endpoint so now Prometheus can come up and scrape all of that information out of it in your cluster and now you have more",
    "start": "391419",
    "end": "398560"
  },
  {
    "text": "information than you could ever hope to want to know about inside of about your notes inside of prometheus running in",
    "start": "398560",
    "end": "405370"
  },
  {
    "text": "GCP a thousand unique series comes off of a typical node there's a ton of",
    "start": "405370",
    "end": "410680"
  },
  {
    "text": "information in here again we're going to use the use method in this case in order to break this down and try to figure out",
    "start": "410680",
    "end": "417370"
  },
  {
    "text": "what are the ones that we want to look at so if we look at we're going to apply",
    "start": "417370",
    "end": "423550"
  },
  {
    "start": "422000",
    "end": "422000"
  },
  {
    "text": "use us e to the CPU resource on your node so the series that comes out from",
    "start": "423550",
    "end": "430810"
  },
  {
    "text": "the node exporter is called node CPU and it is break broken down by 15 different",
    "start": "430810",
    "end": "436660"
  },
  {
    "text": "dimensions about CPU usage on your system all but about three or four of",
    "start": "436660",
    "end": "442690"
  },
  {
    "text": "those actually correspond with actual use of the CPU the rest are idle time",
    "start": "442690",
    "end": "448600"
  },
  {
    "text": "which is the rest of the stuff that's not being used I await the time that's been waiting on disk and and guest I'm not exactly sure",
    "start": "448600",
    "end": "456430"
  },
  {
    "text": "what guest time is I might be when in a virtualized environment somebody else is using this so you can run this query and",
    "start": "456430",
    "end": "462520"
  },
  {
    "text": "you'll actually get a account so this will sum up to because node CPU is a",
    "start": "462520",
    "end": "468910"
  },
  {
    "text": "counter so it's incrementing for every CPU second that's being that's being reused part and pun and so you'll get",
    "start": "468910",
    "end": "478570"
  },
  {
    "text": "basically a count of cores that are being used across your cluster by instance and instances in this case is",
    "start": "478570",
    "end": "485920"
  },
  {
    "text": "the node so now we want to know how saturated my my CPU is and so the",
    "start": "485920",
    "end": "492630"
  },
  {
    "text": "probably the easiest way to talk about CPU saturation is to look at the the load average on a system so the load",
    "start": "492630",
    "end": "500350"
  },
  {
    "text": "average is loosely defined as the number of jobs that are in the run queue that",
    "start": "500350",
    "end": "505600"
  },
  {
    "text": "are running or our queue to get onto the CPU by the scheduler again Brendon greg",
    "start": "505600",
    "end": "510700"
  },
  {
    "text": "has an excellent treatise on the history of the load average in the Linux kernel I encourage you go dig that up and read",
    "start": "510700",
    "end": "517330"
  },
  {
    "text": "it it's fascinating reading to you know if you're if you're digging into this stuff but the load average because it's",
    "start": "517330",
    "end": "524140"
  },
  {
    "text": "already being averaged by the system that it's coming from by itself is not enough because",
    "start": "524140",
    "end": "529480"
  },
  {
    "text": "one note may have 10 CPUs in it and another may have 20 and the load average",
    "start": "529480",
    "end": "535299"
  },
  {
    "text": "is relative to the number of CPUs that are on the machine so we want to normalize the load average against the",
    "start": "535299",
    "end": "541689"
  },
  {
    "text": "number of cores that are actually on that box so this query here the denominator is counting is using the",
    "start": "541689",
    "end": "549459"
  },
  {
    "text": "node CPU so we just pick one of the metrics out of new CPU and we're gonna count and that's going to give me a CPU",
    "start": "549459",
    "end": "554679"
  },
  {
    "text": "count so I'm going to normalize the load by by the number of CPUs times a by 100",
    "start": "554679",
    "end": "560860"
  },
  {
    "text": "and not now I have a representative percentile saturation for my CPU and if",
    "start": "560860",
    "end": "567910"
  },
  {
    "text": "you're having errors on your CPU chances are you're not actually yet getting any metrics at all because you're probably",
    "start": "567910",
    "end": "573699"
  },
  {
    "text": "kernel panicked and you're down so so none of that is actually exposed so",
    "start": "573699",
    "end": "580299"
  },
  {
    "text": "we'll apply these same things for memory",
    "start": "580299",
    "end": "583949"
  },
  {
    "start": "585000",
    "end": "585000"
  },
  {
    "text": "again so the node exporter gives us memory available and memory total memory",
    "start": "586019",
    "end": "592029"
  },
  {
    "text": "available is actually a relatively new metric that came up in the Linux kernel about 318 3.18 if you've ever run free",
    "start": "592029",
    "end": "600429"
  },
  {
    "text": "you know at the command line trying to figure out how much free memory you have on a linux box it's always like well that kind of depends cuz you've got you",
    "start": "600429",
    "end": "607480"
  },
  {
    "text": "know you got you've got file system cache using stuff up you've got buffers over here you've got all these like how",
    "start": "607480",
    "end": "612879"
  },
  {
    "text": "much free memory do I have and they added a thing in the kernel that actually says this is the amount of free",
    "start": "612879",
    "end": "618220"
  },
  {
    "text": "memory that you can get if you were to try to exhaust memory so that's fantastic so if you divide those two",
    "start": "618220",
    "end": "624279"
  },
  {
    "text": "things then you can get a percentage of memory used but that's not the complete",
    "start": "624279",
    "end": "630429"
  },
  {
    "text": "picture when running in a kubernetes environment because kubernetes actually reserves amount of a certain amount of",
    "start": "630429",
    "end": "636429"
  },
  {
    "text": "the system CPU in order to run the system itself it doesn't give it all to the cube lid and so there's another",
    "start": "636429",
    "end": "642970"
  },
  {
    "text": "project that we'll talk in depth about later later it's a cube the cube node",
    "start": "642970",
    "end": "648929"
  },
  {
    "text": "sorry it's the cube status plug-in sorry that's wrong anyway so it actually tells",
    "start": "648929",
    "end": "655419"
  },
  {
    "text": "how much memory is available to kubernetes in order to run so cube status capacity cube status allocatable",
    "start": "655419",
    "end": "663030"
  },
  {
    "text": "so that gives you the amount of memory that's actually available for kubernetes to take advantage of saturation in",
    "start": "663030",
    "end": "671370"
  },
  {
    "text": "memory in a memory term on a machine is kind of weird because the definition of",
    "start": "671370",
    "end": "678360"
  },
  {
    "text": "saturation by Brendon Greg says I've got work to do that needs to be cued somehow needs work that needs to be done later",
    "start": "678360",
    "end": "684990"
  },
  {
    "text": "but what does memory allocation that needs to happen that couldn't happen now in the context of a Linux system I have",
    "start": "684990",
    "end": "693240"
  },
  {
    "text": "virtual memory sure I have unlimited amount of virtual memory but from an operational standpoint you'd never want",
    "start": "693240",
    "end": "698280"
  },
  {
    "text": "to go into swap so the the notion of saturation is is really sort of nebulous because you don't want to go into swap",
    "start": "698280",
    "end": "704550"
  },
  {
    "text": "and in a kubernetes context if you start exceeding the limits on your containers you'll actually get killed whom killed",
    "start": "704550",
    "end": "710730"
  },
  {
    "text": "gone so saturation so you could look at utilization as the amount of memory in",
    "start": "710730",
    "end": "717030"
  },
  {
    "text": "use and maybe saturation as the amount of memory free so you just flip the flip",
    "start": "717030",
    "end": "722250"
  },
  {
    "text": "the numerator denominator and then we're going to call that saturation it's kind of an odd thing to look at from from a",
    "start": "722250",
    "end": "728310"
  },
  {
    "text": "node perspective when we get to the containers something we get some more data that tells us something of interest and if you're running if you have",
    "start": "728310",
    "end": "735270"
  },
  {
    "text": "correctable memory in your system some systems will expose the correction",
    "start": "735270",
    "end": "740339"
  },
  {
    "text": "counts that are happening on your note on your your memory modules GCP doesn't give this to us I was not able to get",
    "start": "740339",
    "end": "747210"
  },
  {
    "text": "any of that out but the node exporter will give it if it has it so that's all",
    "start": "747210",
    "end": "753270"
  },
  {
    "start": "751000",
    "end": "751000"
  },
  {
    "text": "the metrics well that's the application of the use method as applied to a few a handful of the metrics in on your node",
    "start": "753270",
    "end": "760680"
  },
  {
    "text": "on your node exporter metrics we get a wealth of information about the",
    "start": "760680",
    "end": "766650"
  },
  {
    "text": "container runtime itself see advisor is a project from Google that as a standalone project it intersects the",
    "start": "766650",
    "end": "774570"
  },
  {
    "text": "container runtime typically docker or rocket whatever it is that you have running on your system and then exposes in a Prometheus metrics endpoint format",
    "start": "774570",
    "end": "781410"
  },
  {
    "text": "all of the runtime container level metrics about every container that's on your system the cubelet that runs inside of all the",
    "start": "781410",
    "end": "788460"
  },
  {
    "text": "nodes on your cluster vendors that project in and just runs it internally and exposes all the metrics natively",
    "start": "788460",
    "end": "794400"
  },
  {
    "text": "through the cubelet so this is something that you get out of the box for free sits on a-slash metric",
    "start": "794400",
    "end": "799560"
  },
  {
    "text": "a 10-point prometheus will scrape it out of the box the configuration is already there so we get some interesting metrics",
    "start": "799560",
    "end": "806040"
  },
  {
    "text": "about how our containers are running so I get CPU usage both user and system time of the CPU usage of the container",
    "start": "806040",
    "end": "812880"
  },
  {
    "text": "everything that's running in that container and the amount of time that's throttled so if you're running with CPU",
    "start": "812880",
    "end": "818670"
  },
  {
    "text": "limits on your containers your containers can't run past that and they will be throttled and that gets reported",
    "start": "818670",
    "end": "824640"
  },
  {
    "text": "I get filesystem reads and writes IO that's going in and out of the filesystem comes out of the out of C",
    "start": "824640",
    "end": "830910"
  },
  {
    "text": "advisor memory usage for each of those containers and all the network bits so",
    "start": "830910",
    "end": "836190"
  },
  {
    "text": "if we take a couple of these metrics again we're going to look at CPU and memory you se applied to those things in",
    "start": "836190",
    "end": "841830"
  },
  {
    "start": "837000",
    "end": "837000"
  },
  {
    "text": "the context of the containers so per container I get CPUs usage seconds total",
    "start": "841830",
    "end": "847710"
  },
  {
    "text": "again a counter incremented for every CPU second that is used by the container I take a rate of this over some period",
    "start": "847710",
    "end": "854970"
  },
  {
    "text": "of time and now I can get a per core usage of a container wherever my containers running on my cluster so this",
    "start": "854970",
    "end": "861570"
  },
  {
    "text": "will give me a saturation or utilization of my CPU by any individual container",
    "start": "861570",
    "end": "867410"
  },
  {
    "text": "how many people are running with constraints resource requests and constraints it's it's a best practice to",
    "start": "867410",
    "end": "875490"
  },
  {
    "text": "do because if you don't run with them you're basically giving each and every container on the system unlimited access",
    "start": "875490",
    "end": "880650"
  },
  {
    "text": "to all the resources on the box and then you come along and say why why is my machine behaving poorly because I've got",
    "start": "880650",
    "end": "887010"
  },
  {
    "text": "a runaway container that's that's consuming all the CPU or is consuming all the memory it's a best best practice",
    "start": "887010",
    "end": "894000"
  },
  {
    "text": "just at least from a security perspective to do that so you know you're not denial of servicing yourself",
    "start": "894000",
    "end": "899070"
  },
  {
    "text": "your own cluster if you don't have these limits set by setting limits on CPU this",
    "start": "899070",
    "end": "904770"
  },
  {
    "text": "interacts with the completely fair scheduler and containers on the on the machine and the container will only be",
    "start": "904770",
    "end": "912570"
  },
  {
    "text": "allowed to run the number of CPU seconds that you've allocated for that time in any given time period the rest of it",
    "start": "912570",
    "end": "918390"
  },
  {
    "text": "gets recorded as the amount of time for that period of time that the container was trying to run but couldn't and was",
    "start": "918390",
    "end": "924840"
  },
  {
    "text": "throttled so you get a measure of how throttled your system is you were running in limits so if you set",
    "start": "924840",
    "end": "930480"
  },
  {
    "text": "your limits too low now you need to be able to say well did I deploy a bug that something's changed or I need to",
    "start": "930480",
    "end": "936300"
  },
  {
    "text": "increase the limits on my container again these are both counters that come out and again no no errors at the CPU",
    "start": "936300",
    "end": "943680"
  },
  {
    "text": "level at the container level applying use2 memory for your containers so I get",
    "start": "943680",
    "end": "951180"
  },
  {
    "start": "944000",
    "end": "944000"
  },
  {
    "text": "container memory usage bytes seems like the first thing you'd want to reach for when you're trying to figure out how much memory utilization you have but",
    "start": "951180",
    "end": "958830"
  },
  {
    "text": "that met that metric includes all of your filesystem page cache that your process might be using so that's not",
    "start": "958830",
    "end": "964950"
  },
  {
    "text": "it's an incomplete picture because that memory could be freed if you close the file system the operating system can",
    "start": "964950",
    "end": "971670"
  },
  {
    "text": "take it back if it needs to so container memory working bytes is actually the more realistic",
    "start": "971670",
    "end": "977690"
  },
  {
    "text": "approximation of the memory that your container is using and I believe it's the metric that is used that the killer",
    "start": "977690",
    "end": "983550"
  },
  {
    "text": "uses to decide whether or not to kill your container should you exceed the memory limit that you've applied because",
    "start": "983550",
    "end": "989580"
  },
  {
    "text": "you've all you've all applied memory limits saturation again we talked",
    "start": "989580",
    "end": "996000"
  },
  {
    "text": "earlier with no memory saturation where I okay well I don't really have a saturation metric for my node because I",
    "start": "996000",
    "end": "1002570"
  },
  {
    "text": "don't want to use more memory than I have but if you've applied limits to your containers now I can start to see",
    "start": "1002570",
    "end": "1008750"
  },
  {
    "text": "what percentage of that limit have I have I gone into as part of my container",
    "start": "1008750",
    "end": "1013910"
  },
  {
    "text": "so my container is using five Meg's out of a 10 10 Meg limit and I can tell how close I am to that limit or not we had",
    "start": "1013910",
    "end": "1021470"
  },
  {
    "text": "to do a little bit of a little bit of juggling here with the labels because the labels that are coming out from C advisor and the labels that are coming",
    "start": "1021470",
    "end": "1027680"
  },
  {
    "text": "out from cube State metrics are slightly different so I had to align those labels in order to get this but that's the",
    "start": "1027680",
    "end": "1033439"
  },
  {
    "text": "query that will actually give you back how much memory is a ratio how much remember you're using against your limit",
    "start": "1033440",
    "end": "1041290"
  },
  {
    "text": "errors in the context of USC for containers so container memory fail",
    "start": "1041290",
    "end": "1046880"
  },
  {
    "text": "count it's unclear I have not seen this in the wild yet I haven't tested it",
    "start": "1046880",
    "end": "1052340"
  },
  {
    "text": "thoroughly but the number of times that you went to go allocate memory and couldn't so you hit the resource limit",
    "start": "1052340",
    "end": "1059650"
  },
  {
    "text": "chances are you got cout you got him killed so container memory failures total is",
    "start": "1059650",
    "end": "1065630"
  },
  {
    "text": "interesting because if you look at the the labels that are on that secondary that other that other metric there are",
    "start": "1065630",
    "end": "1073190"
  },
  {
    "text": "major faults and page faults so page faults a page fault is where a page of",
    "start": "1073190",
    "end": "1079070"
  },
  {
    "text": "something has to come in off of disk into memory and if you're doing a lot of i/o and the file system and reading a",
    "start": "1079070",
    "end": "1085309"
  },
  {
    "text": "bunch of files page faults are normal so that's just normal paging activity that comes in and out major faults on the",
    "start": "1085309",
    "end": "1091940"
  },
  {
    "text": "other hand are memory pages that have to go to disk so I don't have enough room and in order to be able to fit this into",
    "start": "1091940",
    "end": "1097460"
  },
  {
    "text": "memory so major faults are something that you want to pay attention to so",
    "start": "1097460",
    "end": "1103100"
  },
  {
    "text": "quick sampling of a couple of container level metrics that come out of C advisor and applying the use metrics to it",
    "start": "1103100",
    "end": "1109549"
  },
  {
    "text": "requires in order to show those things those are some of the most important ones so let's talk a little bit about",
    "start": "1109549",
    "end": "1115520"
  },
  {
    "text": "some of the services that are available inside of kubernetes that we want to we want to look at so we're going to switch",
    "start": "1115520",
    "end": "1120740"
  },
  {
    "text": "over now to talking about the read method as applied to the kubernetes api server itself again prometheus metrics",
    "start": "1120740",
    "end": "1129470"
  },
  {
    "text": "exposition format export exported on a-slash metrics endpoint available for Prometheus in order to scrape that data",
    "start": "1129470",
    "end": "1136070"
  },
  {
    "text": "I get performance of all of the controllers that are inside of my kubernetes api and all the work queues",
    "start": "1136070",
    "end": "1142340"
  },
  {
    "text": "that are associated with those controllers when you cube control apply a file you basically lob the file into the into",
    "start": "1142340",
    "end": "1149330"
  },
  {
    "text": "the controller and it queues up the work that has to be done on very large systems there's a backlog of stuff that",
    "start": "1149330",
    "end": "1155299"
  },
  {
    "text": "needs to be worked in order for the controller to get the work done that it wants and if those queues are backing up you may want to know that and do",
    "start": "1155299",
    "end": "1161240"
  },
  {
    "text": "something about it because something's not working well I get requests rates and Layton sees about the API server the",
    "start": "1161240",
    "end": "1167480"
  },
  {
    "text": "G RPC or HTTP calls that are coming into the sea into the the metric server itself I've got a helper cache that runs",
    "start": "1167480",
    "end": "1175549"
  },
  {
    "text": "inside of inside of the API server that caches data from the from Etsy D and I",
    "start": "1175549",
    "end": "1182390"
  },
  {
    "text": "can get hit and miss ratios on that particular cache and then out of the box",
    "start": "1182390",
    "end": "1189230"
  },
  {
    "text": "because I'm using this as go and the go line print Prometheus client exposes the",
    "start": "1189230",
    "end": "1194510"
  },
  {
    "text": "general process information about this process he'll file descriptors memory amount of CPU that's being used and I also get all of the",
    "start": "1194510",
    "end": "1201340"
  },
  {
    "text": "golang instrumentation so all that comes out of the box so we're going to try to apply the read method to some of these",
    "start": "1201340",
    "end": "1208030"
  },
  {
    "text": "metrics the API server request count is",
    "start": "1208030",
    "end": "1216010"
  },
  {
    "text": "just what it says it is it's a counter it goes up and it has a bunch of dimensions on that particular that",
    "start": "1216010",
    "end": "1222520"
  },
  {
    "text": "particular metric one of which is by HTTP verb so I can blow this out and get",
    "start": "1222520",
    "end": "1227710"
  },
  {
    "text": "how many posts how many deletes I'm going to get so many watches that are going on per endpoint if I want to watch",
    "start": "1227710",
    "end": "1235120"
  },
  {
    "text": "errors that are happening on my on my API server I can take a ratio of the",
    "start": "1235120",
    "end": "1240190"
  },
  {
    "text": "failing API server requests against the number of total requests that are that",
    "start": "1240190",
    "end": "1245320"
  },
  {
    "text": "are running so this gives me a ratio of how many things are failing over time and I can put an alert on that and then",
    "start": "1245320",
    "end": "1251860"
  },
  {
    "text": "I get a latency bucket that comes out as well so I can draw a distribution using using the histogram quantiles for from",
    "start": "1251860",
    "end": "1259480"
  },
  {
    "text": "Prometheus to say what is the 90th percentile of that particular thing well as playing around and putting these things together I stumbled across a",
    "start": "1259480",
    "end": "1266760"
  },
  {
    "text": "query that I thought was that was interesting one of the one of the",
    "start": "1266760",
    "end": "1271870"
  },
  {
    "text": "dimensions on API server request count is client which is basically the the",
    "start": "1271870",
    "end": "1278710"
  },
  {
    "text": "client string of all of the the requesting entity to the API server and",
    "start": "1278710",
    "end": "1284230"
  },
  {
    "text": "I thought it was fascinating just looking at all of the different the client strings that were coming into my",
    "start": "1284230",
    "end": "1289960"
  },
  {
    "text": "cluster it's like oh I've got two different versions of Prometheus running in my cluster that's interesting didn't",
    "start": "1289960",
    "end": "1296230"
  },
  {
    "text": "know that I know why now that I think about it but I think that was just a",
    "start": "1296230",
    "end": "1301710"
  },
  {
    "text": "serendipitous find that I found messing around in there so",
    "start": "1301710",
    "end": "1307860"
  },
  {
    "start": "1310000",
    "end": "1310000"
  },
  {
    "text": "okay so I've talked so that's a read method as applied to the kubernetes api",
    "start": "1312710",
    "end": "1318090"
  },
  {
    "text": "server a couple of those metrics cube state metrics this is another project",
    "start": "1318090",
    "end": "1324780"
  },
  {
    "text": "that you can download doesn't come out of the box with kubernetes but pretty much if you want to monitor interesting things in your kubernetes cluster this",
    "start": "1324780",
    "end": "1330929"
  },
  {
    "text": "is one you're going to want to run there's only one instance of it running in your cluster it's not a daemon set",
    "start": "1330929",
    "end": "1335940"
  },
  {
    "text": "you only need one it doesn't need to be high availability it talks to the kubernetes api server and subscribes to",
    "start": "1335940",
    "end": "1342000"
  },
  {
    "text": "a bunch of channel as a bunch of events that's going on and keeps interesting statistics about all the things that are",
    "start": "1342000",
    "end": "1347220"
  },
  {
    "text": "going on inside of your kubernetes cluster you get counts of all of the things how many containers how many pods",
    "start": "1347220",
    "end": "1352559"
  },
  {
    "text": "how many namespaces do I have you get your resource limits because you're",
    "start": "1352559",
    "end": "1357900"
  },
  {
    "text": "setting requests and limits right so then you can actually pull that out we saw those are in earlier earlier",
    "start": "1357900",
    "end": "1362940"
  },
  {
    "text": "examples so that I can use those to actually derive the use metrics that I need you get container States so I want",
    "start": "1362940",
    "end": "1369690"
  },
  {
    "text": "to know how many containers that are in a in a in a restart loop how many containers are ready how many are",
    "start": "1369690",
    "end": "1375420"
  },
  {
    "text": "terminating so I can actually show the container States over time to know whether or not I've got containers that",
    "start": "1375420",
    "end": "1381390"
  },
  {
    "text": "are misbehaving when they shouldn't be a great thing to alert on and then another",
    "start": "1381390",
    "end": "1386880"
  },
  {
    "text": "really interesting thing that comes out of cube state metrics are these the the so called labels series so there's the",
    "start": "1386880",
    "end": "1394110"
  },
  {
    "text": "container underbar label series and that particular series has a constant value",
    "start": "1394110",
    "end": "1400440"
  },
  {
    "text": "of 1 and then a set of label-value pairs that correspond to all the labels that",
    "start": "1400440",
    "end": "1406230"
  },
  {
    "text": "are on the container or the namespace or the whatever it is for all of the node for all the nouns that are in there why",
    "start": "1406230",
    "end": "1412920"
  },
  {
    "text": "would I need a time series series whose value is 1 all the time what do I do",
    "start": "1412920",
    "end": "1417960"
  },
  {
    "text": "with that so you can do interesting things with it if you've got a similar label on another series that's coming",
    "start": "1417960",
    "end": "1424679"
  },
  {
    "text": "out of your application and you want to be able to join those two series together in order to decorate that series with more metadata based on the",
    "start": "1424679",
    "end": "1431670"
  },
  {
    "text": "labels that are on that particular nouns well I've got a container level metric and I have a container set of series I",
    "start": "1431670",
    "end": "1437100"
  },
  {
    "text": "can join this two series together at runtime and then pick off other labels that are on that series and be able to",
    "start": "1437100",
    "end": "1442710"
  },
  {
    "text": "annotate that write that back in to Prometheus video-recording rule so a super powerful technique for being able",
    "start": "1442710",
    "end": "1448540"
  },
  {
    "text": "to merge these set of metadata that's coming out so at CD if you're running in",
    "start": "1448540",
    "end": "1458800"
  },
  {
    "text": "so we're running in Google Cloud so I don't think we have access to the SCDS and the masters but if you're running",
    "start": "1458800",
    "end": "1464710"
  },
  {
    "text": "your own sed server there are some very important metrics within that CD that you want to be able to pay attention to",
    "start": "1464710",
    "end": "1469960"
  },
  {
    "text": "and most likely alert on so Etsy D as a service and it actually has a couple of",
    "start": "1469960",
    "end": "1476650"
  },
  {
    "text": "different ways you want to look at the read method so leader existence do I",
    "start": "1476650",
    "end": "1482320"
  },
  {
    "text": "have a leader in my in my Etsy d cluster if you don't you're having a bad day and",
    "start": "1482320",
    "end": "1488350"
  },
  {
    "text": "that needs to be fixed if your leader if your leader election is changing frequently then you've probably got some",
    "start": "1488350",
    "end": "1494140"
  },
  {
    "text": "sort of network partition that's going on or you've got a machine that's flapping so the rate at which your",
    "start": "1494140",
    "end": "1500620"
  },
  {
    "text": "leader election is reoccurring is the sign of a problem something within your at TD cluster proposals are the things",
    "start": "1500620",
    "end": "1508420"
  },
  {
    "text": "that are coming into SCD as far as the writing of state change to Etsy D and there are rates on the committed the",
    "start": "1508420",
    "end": "1514420"
  },
  {
    "text": "applied the pending and failed proposals and because at C D is a is a quorum based system and nothing gets committed",
    "start": "1514420",
    "end": "1521950"
  },
  {
    "text": "to the database until all of the servers in the cluster have written that",
    "start": "1521950",
    "end": "1527160"
  },
  {
    "text": "information to disk your disk write performance is super important about at",
    "start": "1527160",
    "end": "1532600"
  },
  {
    "text": "CD performance so you've got slow i/o on your disk you're gonna have slow IO slow i/o and you're at C D and you're",
    "start": "1532600",
    "end": "1539200"
  },
  {
    "text": "gonna have slowed i/o on you're kubernetes controllers trying to get work done because they just can't get stuff pushed through so then if we look",
    "start": "1539200",
    "end": "1547150"
  },
  {
    "text": "at if we try to apply the read method well we've got so rate errors in duration at C D actually has two",
    "start": "1547150",
    "end": "1554800"
  },
  {
    "text": "different types of rate error duration that you want to look at one is the inbound so all of the inbound traffic that's coming in so these are the metric",
    "start": "1554800",
    "end": "1562120"
  },
  {
    "text": "names that are the series names that are going to give you the ability to derive your read methods for inbound traffic but we also have inter node",
    "start": "1562120",
    "end": "1570010"
  },
  {
    "text": "communication that goes on so all the RPC traffic there's two going on in between each of the nodes again I've got",
    "start": "1570010",
    "end": "1575470"
  },
  {
    "text": "rate air and duration information I wanted that I want to derive from those as well so it's not so read is not",
    "start": "1575470",
    "end": "1581429"
  },
  {
    "text": "just a single re D for a single service there may be different facets of that service that you want to look at based",
    "start": "1581429",
    "end": "1588119"
  },
  {
    "text": "on how these things are rolling up so",
    "start": "1588119",
    "end": "1594739"
  },
  {
    "start": "1592000",
    "end": "1592000"
  },
  {
    "text": "that's a tour of all of the places that I know of where we can get metrics out of the kubernetes the kubernetes system",
    "start": "1594739",
    "end": "1602759"
  },
  {
    "text": "there may be others but those are definitely the ones that you're going to want to start with install Prometheus",
    "start": "1602759",
    "end": "1607860"
  },
  {
    "text": "and start exploring this information an interesting thing that you can do though in the context of container level",
    "start": "1607860",
    "end": "1614340"
  },
  {
    "text": "metrics as that we were looking as they came out of sea advisor so I've got container level metrics at every",
    "start": "1614340",
    "end": "1620460"
  },
  {
    "text": "container in my system and kubernetes naturally forms a hierarchy based on the",
    "start": "1620460",
    "end": "1625649"
  },
  {
    "text": "container to the pod to the deployment to the namespace so using recording rules inside of firma theose I can",
    "start": "1625649",
    "end": "1632190"
  },
  {
    "text": "author things that take all of this container level metrics and aggregate them up at the pod level and then",
    "start": "1632190",
    "end": "1637590"
  },
  {
    "text": "aggregate them up at the deployment level and then aggregate them up at the namespace and then finally at the cluster level so that I can have on a",
    "start": "1637590",
    "end": "1643590"
  },
  {
    "text": "single series a single metric that represents all CPUs seconds for my",
    "start": "1643590",
    "end": "1649049"
  },
  {
    "text": "entire cluster or my entire namespace or my entire deployment depending on where it is that I want to look at that so",
    "start": "1649049",
    "end": "1655259"
  },
  {
    "text": "that's kind of interesting so then you get to you get to do some some interesting aggregations and",
    "start": "1655259",
    "end": "1662480"
  },
  {
    "text": "visualizations so for example this is a view of all of the deployments or all of",
    "start": "1662480",
    "end": "1669570"
  },
  {
    "text": "the workloads as we call them for the for our production cluster and I've got",
    "start": "1669570",
    "end": "1675239"
  },
  {
    "text": "a aggregate CPU saturation metric that's showing we are using 25 cores on average",
    "start": "1675239",
    "end": "1680669"
  },
  {
    "text": "over over the amount of time for all of these things and then the CPU saturation",
    "start": "1680669",
    "end": "1686070"
  },
  {
    "text": "metric that we decided to come up here with was you can't just sum those things to the bottom so it's sort of a weighted",
    "start": "1686070",
    "end": "1691799"
  },
  {
    "text": "average based on the amount of CPU time and the amount of things that are being throttled overall and I can do",
    "start": "1691799",
    "end": "1697769"
  },
  {
    "text": "interesting visualizations where I've got so the query or workload so here are",
    "start": "1697769",
    "end": "1703289"
  },
  {
    "text": "the so we're using about 3/4 of a core overall on the query and we've got some",
    "start": "1703289",
    "end": "1708330"
  },
  {
    "text": "saturation going on so I can drill into this node and actually get all of the show all the",
    "start": "1708330",
    "end": "1713520"
  },
  {
    "text": "pods that are running inside of that deployment and then I've got aggregate metrics that are coming out to that pod",
    "start": "1713520",
    "end": "1719070"
  },
  {
    "text": "and I can drill into the individual container and then show okay these are the two so I've got the query or pod and",
    "start": "1719070",
    "end": "1725730"
  },
  {
    "text": "I've got Jaeger agent that are running on that as well so Jaeger agent doesn't have any No Limits set on this guy and",
    "start": "1725730",
    "end": "1734760"
  },
  {
    "text": "there are limits set on this guy and I've got about a 10 percent saturation rate going on at the container level itself so that's just an example of some",
    "start": "1734760",
    "end": "1741809"
  },
  {
    "text": "interesting things that you can do with recording rules and the metrics that you're getting out of out of kubernetes",
    "start": "1741809",
    "end": "1749700"
  },
  {
    "text": "I have about 50 more slides that I could",
    "start": "1749700",
    "end": "1754770"
  },
  {
    "text": "go into 35 minutes is not enough time to really talk about all the things I",
    "start": "1754770",
    "end": "1760380"
  },
  {
    "text": "wanted to talk about but I'll leave five minutes at the end for some questions",
    "start": "1760380",
    "end": "1765500"
  },
  {
    "text": "all right thank you all for coming have a great night question buoy",
    "start": "1770480",
    "end": "1778520"
  },
  {
    "text": "[Applause]",
    "start": "1778520",
    "end": "1786279"
  },
  {
    "text": "so the question is when is it okay or not to use the recording rules so typically recording rules are used in",
    "start": "1787250",
    "end": "1795690"
  },
  {
    "text": "place of expensive expensive queries that you would put on a dashboard so if",
    "start": "1795690",
    "end": "1800970"
  },
  {
    "text": "you're gonna run a very complex query that's going to take a lot of time in order to evaluate and and you were to",
    "start": "1800970",
    "end": "1807299"
  },
  {
    "text": "place that on the dashboard then you you would be executing that query over and over and over again so you may consider",
    "start": "1807299",
    "end": "1812820"
  },
  {
    "text": "taking that rule putting it in a recording rule that way it's evaluated by the system internally and then",
    "start": "1812820",
    "end": "1818340"
  },
  {
    "text": "written down and then your query just says give me the values that are as a result of that so the other the other",
    "start": "1818340",
    "end": "1824970"
  },
  {
    "text": "thing that we use it for is that we have long over time metrics that we want to be able to do so an aggregation for",
    "start": "1824970",
    "end": "1831419"
  },
  {
    "text": "example an aggregation at every level here I don't want to have to go back in time to roll up to the cluster level all",
    "start": "1831419",
    "end": "1836940"
  },
  {
    "text": "of the containers that are in my cluster so an example of a of a of an expensive",
    "start": "1836940",
    "end": "1842580"
  },
  {
    "text": "could not otherwise expensive query and then I think if it hasn't come out",
    "start": "1842580",
    "end": "1847650"
  },
  {
    "text": "already Prometheus has the ability to change the the dirt or the frequency of",
    "start": "1847650",
    "end": "1854880"
  },
  {
    "text": "a recording rule by recording rule block and I believe there's also a feature coming that will allow you to change the",
    "start": "1854880",
    "end": "1862549"
  },
  {
    "text": "how long a particular metric is stored so the so the retention policy can be",
    "start": "1862549",
    "end": "1868919"
  },
  {
    "text": "down to the series level so then you can do interesting things like roll up",
    "start": "1868919",
    "end": "1874230"
  },
  {
    "text": "summaries so I can down sample my my metric because Prometheus out of the box gives you metrics that are at full full",
    "start": "1874230",
    "end": "1880919"
  },
  {
    "text": "resolution of the scrape now I can down sample that that metric over a minute",
    "start": "1880919",
    "end": "1886679"
  },
  {
    "text": "over an hour over a day over a year and then leave those series in my database for a longer period of time so that would be another example as to why Tripp",
    "start": "1886679",
    "end": "1894350"
  },
  {
    "text": "what are the questions",
    "start": "1894350",
    "end": "1897799"
  },
  {
    "text": "I don't know let's take a look so the question is you can monitor restarts on for cube state metrics so let's see",
    "start": "1910330",
    "end": "1925119"
  },
  {
    "text": "that's if we just find that guy in real-time status ready says running",
    "start": "1925119",
    "end": "1941340"
  },
  {
    "text": "restarts I'm not seeing it there we go and then if we get a console",
    "start": "1943619",
    "end": "1950859"
  },
  {
    "text": "on that guy and the labels that are",
    "start": "1950859",
    "end": "1955899"
  },
  {
    "text": "coming out of here so I get an app I get",
    "start": "1955899",
    "end": "1963489"
  },
  {
    "text": "a container so yes so you can you can narrow that to the container what are",
    "start": "1963489",
    "end": "1972849"
  },
  {
    "text": "the questions in fact yes so the this is",
    "start": "1972849",
    "end": "1983919"
  },
  {
    "text": "Griffin oh the the thing at the top the is a custom plugin that we've written not just yet",
    "start": "1983919",
    "end": "1998729"
  },
  {
    "text": "we're still we're still developing our open-source strategy but we are kin we are we are contributing back to the",
    "start": "2001300",
    "end": "2007180"
  },
  {
    "text": "things that we're using so yes what other questions oh so I the slides that I published that",
    "start": "2007180",
    "end": "2016870"
  },
  {
    "text": "where can we find the remaining slide so all the remaining slides are in the deck that I published on the talk so",
    "start": "2016870",
    "end": "2022000"
  },
  {
    "text": "everything that I didn't talk about I left all the slides in there so that could there's some information in there",
    "start": "2022000",
    "end": "2027280"
  },
  {
    "text": "about what is the what is the new metric server that was released in kubernetes",
    "start": "2027280",
    "end": "2032830"
  },
  {
    "text": "1.9 so there's some information there I've got a blog post on our blog that talks about that as well oh and so",
    "start": "2032830",
    "end": "2038410"
  },
  {
    "text": "there's a link in the back that so I've got a blog series that's actually going much much deeper into all of these",
    "start": "2038410",
    "end": "2043690"
  },
  {
    "text": "metrics from from an o-level metric a container level metric so each of these things i've touched on today will be",
    "start": "2043690",
    "end": "2048970"
  },
  {
    "text": "part of that series and i go way way deep into that and then another series that or another blog post that's on deck",
    "start": "2048970",
    "end": "2055149"
  },
  {
    "text": "is talking about how your your requests and limits interact with the linux",
    "start": "2055150",
    "end": "2060790"
  },
  {
    "text": "system so what is the interaction between the CPU limit and and the",
    "start": "2060790",
    "end": "2065830"
  },
  {
    "text": "completely fair scheduler which is a which is a fascinating topic so so",
    "start": "2065830",
    "end": "2076720"
  },
  {
    "text": "custom application metrics so that is really up to the application developer so the the application the metrics that",
    "start": "2076720",
    "end": "2082210"
  },
  {
    "text": "were coming out of the API server and SCD are an example of application level metrics in the context of kubernetes",
    "start": "2082210",
    "end": "2088000"
  },
  {
    "text": "your application metrics are in the context of your application and I wouldn't know anything about that for",
    "start": "2088000",
    "end": "2094659"
  },
  {
    "text": "the product that we're putting together we're trying to figure out easy ways that you can build Auto build dashboards",
    "start": "2094660",
    "end": "2100030"
  },
  {
    "text": "based on just here's a list of the metrics that I'm interested in and will give you an aggregate metric from all of",
    "start": "2100030",
    "end": "2105520"
  },
  {
    "text": "the containers up to the to the service that I can see how my service is behaving as a whole because that's",
    "start": "2105520",
    "end": "2111610"
  },
  {
    "text": "really what you're interested in I'm not interested about the individual metric down below",
    "start": "2111610",
    "end": "2117119"
  },
  {
    "text": "I'm sorry is the endpoint server",
    "start": "2121130",
    "end": "2128900"
  },
  {
    "text": "reachable by default I believe it is you",
    "start": "2128900",
    "end": "2134100"
  },
  {
    "text": "may have to I think it's just on by default for for scraping from kubernetes",
    "start": "2134100",
    "end": "2141170"
  },
  {
    "text": "already enjoy the enjoy the festivities this evening and thank you all for coming [Applause]",
    "start": "2144290",
    "end": "2154260"
  }
]