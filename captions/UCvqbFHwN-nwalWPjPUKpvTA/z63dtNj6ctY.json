[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "all right good afternoon so my name is",
    "start": "30",
    "end": "5130"
  },
  {
    "text": "su gu I am the co-creator of Vitesse and CTO at planet-scale",
    "start": "5130",
    "end": "10320"
  },
  {
    "text": "and I thought I should talk about some thing contradictory stateless storage",
    "start": "10320",
    "end": "16170"
  },
  {
    "text": "but hopefully I hope that I can defend this title hopefully I can talk about",
    "start": "16170",
    "end": "22769"
  },
  {
    "text": "why I call this the way it is so how many of you have not been to the keynote",
    "start": "22769",
    "end": "30900"
  },
  {
    "text": "or did not see do not know what Vitesse's yeah there are a few so I'll do a quick",
    "start": "30900",
    "end": "37530"
  },
  {
    "text": "rundown of what we Tessa's and then we'll get into what this stateless storage is all about so with us is what",
    "start": "37530",
    "end": "44460"
  },
  {
    "text": "I call is a cloud native database which means that it runs very comfortably in kubernetes without losing your data it",
    "start": "44460",
    "end": "52140"
  },
  {
    "text": "scales massively actually that's how it was originally born we when we started developing it in 2010 at YouTube it was",
    "start": "52140",
    "end": "59399"
  },
  {
    "text": "mostly meant to just scale massively and later it became cloud native and highly",
    "start": "59399",
    "end": "66420"
  },
  {
    "text": "available you can configure with tests to have almost no downtime or practically no downtime something like",
    "start": "66420",
    "end": "74790"
  },
  {
    "text": "five nines of availability and it's the based on my sequel means that it uses my",
    "start": "74790",
    "end": "79890"
  },
  {
    "text": "sequel underneath it also speaks the my sequel protocol so if you are already on my sequel or if you are familiar with my",
    "start": "79890",
    "end": "86490"
  },
  {
    "text": "sequel migrating with us is not a heavy lift obviously we test is graduated I assume",
    "start": "86490",
    "end": "94200"
  },
  {
    "start": "91000",
    "end": "154000"
  },
  {
    "text": "many of you know mmm and here are some of the stats so more",
    "start": "94200",
    "end": "102750"
  },
  {
    "text": "than even the numbers they think that we are most proud about in with us is who",
    "start": "102750",
    "end": "109619"
  },
  {
    "text": "is using with us who adopted with us right and this here's a more extensive",
    "start": "109619",
    "end": "114899"
  },
  {
    "text": "list of adopters that I didn't share about in the keynote which is so if you",
    "start": "114899",
    "end": "120570"
  },
  {
    "text": "ask me the the coolest part of these adopters is these are what I consider technology leaders in the industry so",
    "start": "120570",
    "end": "128489"
  },
  {
    "text": "when these people say that they are using a technology they are kind of setting the example",
    "start": "128489",
    "end": "133769"
  },
  {
    "text": "that a lot of others tend to follow so it's a pretty good stamp of approval if",
    "start": "133769",
    "end": "139590"
  },
  {
    "text": "you look at some of the names here like slack github New Relic",
    "start": "139590",
    "end": "145680"
  },
  {
    "text": "Pinterest so these are companies that are looked upon - when you think about how do you scale infrastructure so and",
    "start": "145680",
    "end": "154400"
  },
  {
    "start": "154000",
    "end": "205000"
  },
  {
    "text": "some quotes from many of these adapter adopters so slack basically is saying",
    "start": "154400",
    "end": "162299"
  },
  {
    "text": "that there is nothing like we tests out there the things that we wanted to do with our storage system only we test",
    "start": "162299",
    "end": "168480"
  },
  {
    "text": "could could do it and here's one from square we're saying that somehow you can",
    "start": "168480",
    "end": "176099"
  },
  {
    "text": "just keep on scaling with tests and just keep scaling with you so that is his",
    "start": "176099",
    "end": "181169"
  },
  {
    "text": "testimony and here's another one from Pinterest it says that the other cool",
    "start": "181169",
    "end": "186599"
  },
  {
    "text": "thing about Vitas is that yes you can do all this leash sharding and scaling but you don't really have to rewrite your",
    "start": "186599",
    "end": "191700"
  },
  {
    "text": "app to be able to achieve this scale so those are some of the things that I collected there's actually lots and lots",
    "start": "191700",
    "end": "198959"
  },
  {
    "text": "of quotes that we can collect but these are some highlights that I collected by looking around so so that is all",
    "start": "198959",
    "end": "207000"
  },
  {
    "text": "overview about Vitas now I'm going to jump into what makes Vitas cloud native",
    "start": "207000",
    "end": "212579"
  },
  {
    "text": "and what this stateless storage means at all so for that there's two reasons",
    "start": "212579",
    "end": "218010"
  },
  {
    "text": "there's a historical reason why we test is cloud native and there's also a technical reason so I'm going to talk",
    "start": "218010",
    "end": "223290"
  },
  {
    "text": "about the historical reason and we've the technical details into this story so it all starts in 2010 we were at YouTube",
    "start": "223290",
    "end": "230900"
  },
  {
    "text": "on in 2010 we were on Prem we had our own data centers even though we were",
    "start": "230900",
    "end": "236819"
  },
  {
    "text": "acquired by Google we were not part of Google infrastructure and at that time",
    "start": "236819",
    "end": "242069"
  },
  {
    "text": "our infrastructure was awesome was perfect flawless everything was fine it",
    "start": "242069",
    "end": "252479"
  },
  {
    "text": "was not it was like hell was breaking loose things were falling all over the place and my colleague Mike and I",
    "start": "252479",
    "end": "261239"
  },
  {
    "text": "decided to pretty much take ourselves out of this firefighting mode",
    "start": "261239",
    "end": "267000"
  },
  {
    "text": "figure out what we need to do about this thing because it was not going in the right direction it was just getting",
    "start": "267000",
    "end": "272940"
  },
  {
    "text": "worse by the day so we actually built a huge spreadsheet of all the problems we",
    "start": "272940",
    "end": "280500"
  },
  {
    "text": "have ever seen at YouTube and all the problems we are possibly going to face",
    "start": "280500",
    "end": "285630"
  },
  {
    "text": "and then possible solutions it's a good exercise right like that gives good",
    "start": "285630",
    "end": "290850"
  },
  {
    "text": "clarity and when you looked at all that it was obvious we had to build something separate that comes between the",
    "start": "290850",
    "end": "298590"
  },
  {
    "text": "application and the database so if we call it a proxy layer whatever so that's",
    "start": "298590",
    "end": "303600"
  },
  {
    "text": "what we initially built and from that with this was born and it was originally called Voltron by the way because it was",
    "start": "303600",
    "end": "310680"
  },
  {
    "text": "different pieces coming together but later I know you can't use Voltron as a project name which is why and we used",
    "start": "310680",
    "end": "318240"
  },
  {
    "text": "word Ron and we named all our files VT VT VT right so you had to come up with",
    "start": "318240",
    "end": "324330"
  },
  {
    "text": "the name that's not Voltron that had VT in it so that's how it s was born oh and",
    "start": "324330",
    "end": "331710"
  },
  {
    "text": "the others father story is I don't know how many of you have heard of strong bad yeah yeah so he has a he has a thing",
    "start": "331710",
    "end": "339780"
  },
  {
    "text": "where it says that the best way to create a cool name is to actually find a",
    "start": "339780",
    "end": "345390"
  },
  {
    "text": "normal name and completely misspell it I know if you know the Limousin video so",
    "start": "345390",
    "end": "352620"
  },
  {
    "text": "we said okay we test mean speed in French so you're going to drop the e and then that's what that's why the name is",
    "start": "352620",
    "end": "359790"
  },
  {
    "text": "the way it is so it was all fine but in",
    "start": "359790",
    "end": "367320"
  },
  {
    "start": "362000",
    "end": "595000"
  },
  {
    "text": "2013 I don't know you remember the signature the significance of this year",
    "start": "367320",
    "end": "372780"
  },
  {
    "text": "Snowden happened and there was a directive that no data shall be outside",
    "start": "372780",
    "end": "378270"
  },
  {
    "text": "Google data centers so we had to migrate into Google and until then none of us at",
    "start": "378270",
    "end": "386280"
  },
  {
    "text": "YouTube knew anything about Google infrastructure we knew this Borg thing existed and it does did magical things",
    "start": "386280",
    "end": "393479"
  },
  {
    "text": "and stuff but we actually went looked inside it it had nothing that we could",
    "start": "393479",
    "end": "400260"
  },
  {
    "text": "have used needed for running with tests so Google Google's Borg is mostly a an environment",
    "start": "400260",
    "end": "407520"
  },
  {
    "text": "that lets you deploy stateless applications at humongous scale and if",
    "start": "407520",
    "end": "413310"
  },
  {
    "text": "you wanted storage it gave you api's like there's an API for BigTable there is an API for",
    "start": "413310",
    "end": "418580"
  },
  {
    "text": "Colossus there is an API for mega stores like all the storage the SS tables they",
    "start": "418580",
    "end": "423600"
  },
  {
    "text": "had a bunch of storages and as by the way there is no block storage at Google there is no API for storing something in",
    "start": "423600",
    "end": "431040"
  },
  {
    "text": "a file it doesn't exist it's not allowed and Colossus is the only one and it's",
    "start": "431040",
    "end": "436740"
  },
  {
    "text": "not a file API that was very intentional design so we look at all that and he says how do you run my sequel on on that",
    "start": "436740",
    "end": "445980"
  },
  {
    "text": "right and the only other storage was you could write your local disk if you are running as an application but as soon as",
    "start": "445980",
    "end": "454170"
  },
  {
    "text": "your pod gets rescheduled we will wipe",
    "start": "454170",
    "end": "459210"
  },
  {
    "text": "your disk so that was the deal so that's when one limitation and the other",
    "start": "459210",
    "end": "465270"
  },
  {
    "text": "problem is moving with tests into Google's ecosystem they have all these",
    "start": "465270",
    "end": "474300"
  },
  {
    "text": "custom api's they have chubby they have stubby they have Borgman they have log",
    "start": "474300",
    "end": "479370"
  },
  {
    "text": "infrastructure they have throttling infrastructure what do all these things have they all have api's so with this is",
    "start": "479370",
    "end": "485820"
  },
  {
    "text": "an open source project meant to run in bare metal how do you put this into",
    "start": "485820",
    "end": "492390"
  },
  {
    "text": "Google so the immediate answer was or you should close source it there is no way you can run this in Google assets",
    "start": "492390",
    "end": "498870"
  },
  {
    "text": "because Google's API is too custom so we kind of took that as challenge since we",
    "start": "498870",
    "end": "504390"
  },
  {
    "text": "are not going to do that we are going to see what we can do to keep it as open-source so what we built what we did",
    "start": "504390",
    "end": "512430"
  },
  {
    "text": "was just build abstraction layer after abstraction layer and I will come back to what that means what that meant later",
    "start": "512430",
    "end": "519539"
  },
  {
    "text": "in the future so we built an abstraction layer for stubby we'd build an abstraction layer for chubby so we did",
    "start": "519540",
    "end": "525090"
  },
  {
    "text": "all that and manage to get something deployed but what about storage right",
    "start": "525090",
    "end": "530340"
  },
  {
    "text": "that is there is no storage API of Google that we could use so what we did was we just used local",
    "start": "530340",
    "end": "538370"
  },
  {
    "text": "storage but I don't know how many of you were in the previous talk jatin actually",
    "start": "538370",
    "end": "543910"
  },
  {
    "text": "explained the entire thing in like two minutes but I'm going to take longer to explain what what we did so we ran my",
    "start": "543910",
    "end": "550370"
  },
  {
    "text": "sequel is in this semi synchronous mode which means that we had one master and lots of replicas and YouTube requires",
    "start": "550370",
    "end": "557240"
  },
  {
    "text": "huge amount of read traffic so we had like AD 200 replicas we the semi sync",
    "start": "557240",
    "end": "563930"
  },
  {
    "text": "mode allows you to guarantee that a commit is not complete at least until at",
    "start": "563930",
    "end": "569210"
  },
  {
    "text": "least one other replica had the data so what that means is that if a master goes",
    "start": "569210",
    "end": "574550"
  },
  {
    "text": "down it's guaranteed that another replica had it so we used we relied on",
    "start": "574550",
    "end": "580490"
  },
  {
    "text": "that and we said that okay if that happens which means that it is okay if Bob likes the data of the master because",
    "start": "580490",
    "end": "588080"
  },
  {
    "text": "there is always one other replica that has it so that is essentially how we ran",
    "start": "588080",
    "end": "593300"
  },
  {
    "text": "with us on Borg so basically we ran with test as far as Borg was concerned with",
    "start": "593300",
    "end": "600530"
  },
  {
    "start": "595000",
    "end": "620000"
  },
  {
    "text": "this was a stateless application so what do you call a stateless application that runs a storage I call it stateless",
    "start": "600530",
    "end": "608510"
  },
  {
    "text": "storage and it's a it's a better name than serverless right all right okay",
    "start": "608510",
    "end": "620260"
  },
  {
    "start": "620000",
    "end": "662000"
  },
  {
    "text": "so guess what 2015 kubernetes gets announced we didn't even know we all",
    "start": "620260",
    "end": "627440"
  },
  {
    "text": "this was happening and then we look at kubernetes says what kubernetes has",
    "start": "627440",
    "end": "632930"
  },
  {
    "text": "ephemeral storage it does everything isn't that what you've been doing on borg so we go talk to the kubernetes",
    "start": "632930",
    "end": "639200"
  },
  {
    "text": "teams they say hey I think we think with us can run in kubernetes already it says yeah it looks like it can so let's",
    "start": "639200",
    "end": "646100"
  },
  {
    "text": "announce it so we announced before kubernetes 1.0 was released that we test is ready for kubernetes so March 20th is",
    "start": "646100",
    "end": "655100"
  },
  {
    "text": "the date we blocked July 21 is when kubernetes 1.0 was announced so funny",
    "start": "655100",
    "end": "662270"
  },
  {
    "start": "662000",
    "end": "860000"
  },
  {
    "text": "story people believed us and they said so speech labs are the first company",
    "start": "662270",
    "end": "669329"
  },
  {
    "text": "that came and said okay you think it can work say yeah yeah totally so they they",
    "start": "669329",
    "end": "678569"
  },
  {
    "text": "went into production that would they are still in production so they are the",
    "start": "678569",
    "end": "685019"
  },
  {
    "text": "oldest living with us instance they've been running in kubernetes since 2016 so",
    "start": "685019",
    "end": "691410"
  },
  {
    "text": "then came HubSpot so stitch labs was actually a sharded environment they were",
    "start": "691410",
    "end": "696750"
  },
  {
    "text": "previously on scale arc which went out of business so they said oh we test can do what scaler does so therefore we",
    "start": "696750",
    "end": "703470"
  },
  {
    "text": "migrated to a test so hops what was the first company that actually used with test because it ran well on kubernetes",
    "start": "703470",
    "end": "710129"
  },
  {
    "text": "they said oh you have all this orchestration layer you have you know how to survive all these part takedowns",
    "start": "710129",
    "end": "716519"
  },
  {
    "text": "I think we like it and the cool thing they did was they gathered like 10",
    "start": "716519",
    "end": "721769"
  },
  {
    "text": "terabytes of queries and they dumped it on with us and a whole bunch of them",
    "start": "721769",
    "end": "727379"
  },
  {
    "text": "didn't work they worked through each and every one of those and God all of them working so they actually narrowed the",
    "start": "727379",
    "end": "733769"
  },
  {
    "text": "gap of the query the query said that we test could handle too they pushed it",
    "start": "733769",
    "end": "739980"
  },
  {
    "text": "pretty far it's still not completely out there but a large chunk of the queries that working with tests that should work",
    "start": "739980",
    "end": "746879"
  },
  {
    "text": "in my sequel house part actually got it working and then after a while some",
    "start": "746879",
    "end": "753809"
  },
  {
    "text": "Chinese people started showing up in the Vitesse slack they never identified themselves but they were asking",
    "start": "753809",
    "end": "760050"
  },
  {
    "text": "extremely sharp and intelligent questions who are these people asking",
    "start": "760050",
    "end": "765930"
  },
  {
    "text": "about query plans they're asking about like optimization here and there and then after a long time they reveal all",
    "start": "765930",
    "end": "772949"
  },
  {
    "text": "we are from JD comm it's the you know at that time it was the second largest online retailer in China now it looks",
    "start": "772949",
    "end": "781290"
  },
  {
    "text": "like they are the largest now so and this thirty five million QPS information",
    "start": "781290",
    "end": "788009"
  },
  {
    "text": "I found out only last week I had to really prod them no you had your singles",
    "start": "788009",
    "end": "793230"
  },
  {
    "text": "day like can you give me some good number and finally he told me third I",
    "start": "793230",
    "end": "798870"
  },
  {
    "text": "didn't believe it no way say why should I just say like a few like JD does just a few million QPS",
    "start": "798870",
    "end": "805350"
  },
  {
    "text": "say oh no no no this comes straight out of our graphs you can take it to the bank",
    "start": "805350",
    "end": "810389"
  },
  {
    "text": "okay I'm going to announce it in the keynote so that's that's the story behind JD and finally all of you know",
    "start": "810389",
    "end": "818189"
  },
  {
    "text": "about nozzle where they they were the",
    "start": "818189",
    "end": "824189"
  },
  {
    "text": "first ones to prove that you can actually move from cloud to cloud if you are fully on kubernetes and running on",
    "start": "824189",
    "end": "830790"
  },
  {
    "text": "with tests the one part that I was not clear about is how they actually did the pipeline and probably Thursday when",
    "start": "830790",
    "end": "837839"
  },
  {
    "text": "direct talks about it he I'm sure he will cover how he did that so the test",
    "start": "837839",
    "end": "844019"
  },
  {
    "text": "has been running for three years right the thing is that we did not know that kubernetes you are not supposed to run",
    "start": "844019",
    "end": "851999"
  },
  {
    "text": "storage on kubernetes this is all like our own recommendations based on the fact that we ran on bore and that we",
    "start": "851999",
    "end": "857639"
  },
  {
    "text": "know how to run this on kubernetes right so Kelsey comes up and says oh don't put storage on kubernetes it's big danger",
    "start": "857639",
    "end": "864980"
  },
  {
    "start": "860000",
    "end": "889000"
  },
  {
    "text": "I'm like Kelsey we've been doing this for three years now but he has a point",
    "start": "864980",
    "end": "873319"
  },
  {
    "text": "this is his point it says you cannot just take my sequel and put it in",
    "start": "873319",
    "end": "878490"
  },
  {
    "text": "kubernetes you will lose your data you're guaranteed to lose your data or",
    "start": "878490",
    "end": "883550"
  },
  {
    "text": "you are going to have other problems probably bigger than losing your data so",
    "start": "883550",
    "end": "889129"
  },
  {
    "start": "889000",
    "end": "923000"
  },
  {
    "text": "I'm going to go through some of the scenarios some of the details involved",
    "start": "889129",
    "end": "894420"
  },
  {
    "text": "in why he's giving this advice is his advice is sound and there is a reason for it so here's the thing right",
    "start": "894420",
    "end": "902910"
  },
  {
    "text": "I'm the naive person I'm going to say okay I'm going to pick my sequel and put",
    "start": "902910",
    "end": "909660"
  },
  {
    "text": "it in a pod and use local storage boom right the first thing is what happens is",
    "start": "909660",
    "end": "916740"
  },
  {
    "text": "when your pod gets rescheduled is going to wipe all your data so obviously nobody should be running it this way",
    "start": "916740",
    "end": "922699"
  },
  {
    "text": "this is probably a more sane deployment where you say okay I'll have mounted",
    "start": "922699",
    "end": "930029"
  },
  {
    "start": "923000",
    "end": "1014000"
  },
  {
    "text": "volume and then use PD or EBA whatever and just run my sequel as a",
    "start": "930029",
    "end": "936660"
  },
  {
    "text": "stateless application this will work and this can work there are some people who use this configuration the problem here",
    "start": "936660",
    "end": "943920"
  },
  {
    "text": "is if your part gets rescheduled my sequel when it comes back up has to",
    "start": "943920",
    "end": "950190"
  },
  {
    "text": "perform crash recovery and that can take a long time I don't know minutes hours sometimes",
    "start": "950190",
    "end": "955860"
  },
  {
    "text": "depending on how big your data is so it's definitely not good for high uptime",
    "start": "955860",
    "end": "963420"
  },
  {
    "text": "so if you don't care about uptime yes you can configure it this way but there is a problem my sequel is tuned for low",
    "start": "963420",
    "end": "970589"
  },
  {
    "text": "latency I ops and going through a network boundary is not something it enjoys so you're going to pay some price",
    "start": "970589",
    "end": "976410"
  },
  {
    "text": "on throughput strangely it's not many people care about it there are people",
    "start": "976410",
    "end": "982230"
  },
  {
    "text": "who can tolerate this and those who can tolerate it it's fine the people who cannot tolerate this are typically",
    "start": "982230",
    "end": "988380"
  },
  {
    "text": "people who are already on Prem already used to this latency and they wrap has",
    "start": "988380",
    "end": "994050"
  },
  {
    "text": "been tuned to do that latency and suddenly if you increase the latency",
    "start": "994050",
    "end": "999660"
  },
  {
    "text": "between the my sequel and disk it's something that most laps don't tolerate so that's we had that problem at YouTube",
    "start": "999660",
    "end": "1006649"
  },
  {
    "text": "YouTube every millisecond counted and so you couldn't put a network between the database and and it's disk so here's",
    "start": "1006649",
    "end": "1015140"
  },
  {
    "start": "1014000",
    "end": "1207000"
  },
  {
    "text": "another scenario right so how do you how",
    "start": "1015140",
    "end": "1021199"
  },
  {
    "text": "do you put multiple so there yeah the other was the other scenario is to say I'll do the master replica scenario",
    "start": "1021199",
    "end": "1026870"
  },
  {
    "text": "right where I have one master and one replica and each in its their own parts",
    "start": "1026870",
    "end": "1032120"
  },
  {
    "text": "you could do this either with local storage or with mounted storage but the problems are similar so let's say you",
    "start": "1032120",
    "end": "1040100"
  },
  {
    "text": "are going to deploy a stateful set right stateful set was created for stateful",
    "start": "1040100",
    "end": "1046308"
  },
  {
    "text": "things that's a lie stateful state has no state you know that the only state in stateful set is",
    "start": "1046309",
    "end": "1054559"
  },
  {
    "text": "outside of stateful sets so the the only way a stateful the main property of a",
    "start": "1054559",
    "end": "1061549"
  },
  {
    "text": "stateful set is that you can identify each part individually and give give it a specific",
    "start": "1061549",
    "end": "1067130"
  },
  {
    "text": "traffic so that is the property of a stateful set so you can say part zero is",
    "start": "1067130",
    "end": "1072140"
  },
  {
    "text": "my master and part one is my replica and you can ask the app server talked to a",
    "start": "1072140",
    "end": "1078530"
  },
  {
    "text": "part zero because it is the master so we can make that happen you can you cannot",
    "start": "1078530",
    "end": "1083870"
  },
  {
    "text": "use kubernetes --is automatic load balancing because kubernetes will not differentiate between a master",
    "start": "1083870",
    "end": "1090860"
  },
  {
    "text": "and replica so you have to talk to individual parts so the main problem is",
    "start": "1090860",
    "end": "1097100"
  },
  {
    "text": "you cannot designate the master if you if you addressed it the state will set as an entire entity which means that you",
    "start": "1097100",
    "end": "1103820"
  },
  {
    "text": "have to address each part but if a pod gets rescheduled and I say okay I'm",
    "start": "1103820",
    "end": "1110090"
  },
  {
    "text": "going to failover my master - part 1 as replicas you have to tell the app and",
    "start": "1110090",
    "end": "1117190"
  },
  {
    "text": "that doesn't happen in automatically for you you have to write some code some",
    "start": "1117190",
    "end": "1122270"
  },
  {
    "text": "orchestration tool that tells the app either you have to do a DNS update or",
    "start": "1122270",
    "end": "1127490"
  },
  {
    "text": "you have to do some config push something you have to write just taking",
    "start": "1127490",
    "end": "1132590"
  },
  {
    "text": "my sequel and putting it in this configuration is not going to work for you in so how does this work on Prem",
    "start": "1132590",
    "end": "1139610"
  },
  {
    "text": "on-prem masters don't go down the up time for masters is like 2 months 3",
    "start": "1139610",
    "end": "1145070"
  },
  {
    "text": "months 6 months here they go down like once a week at least probably more often",
    "start": "1145070",
    "end": "1152390"
  },
  {
    "text": "depending on how much entropy that exists around here kubernetes may want to shift things around so those are the",
    "start": "1152390",
    "end": "1160940"
  },
  {
    "text": "three problems with using my sequel in this scenario and if you bring up a new",
    "start": "1160940",
    "end": "1168290"
  },
  {
    "text": "replica how do you initialize it that's another problem it cannot in Coober in kubernetes",
    "start": "1168290",
    "end": "1175400"
  },
  {
    "text": "just bringing up my sequel how do you initialize the data saying that ok the data from the master has to be copied it",
    "start": "1175400",
    "end": "1182300"
  },
  {
    "text": "has to be pointed to that master so all that orchestration is something that you have to write extra so which is the",
    "start": "1182300",
    "end": "1188510"
  },
  {
    "text": "reason why Kelsey has been saying that you cannot just take my sequel things that are storage that is supposed to run",
    "start": "1188510",
    "end": "1196010"
  },
  {
    "text": "on Prem directly into kubernetes there is lot of blue code that needs to be written which is essentially",
    "start": "1196010",
    "end": "1202520"
  },
  {
    "text": "what we test does for you which is what we had to do for ourselves all right and",
    "start": "1202520",
    "end": "1208100"
  },
  {
    "start": "1207000",
    "end": "1303000"
  },
  {
    "text": "then there are cultural issues and trust me these are actually really really horrible because if you have configured",
    "start": "1208100",
    "end": "1215480"
  },
  {
    "text": "your system with these long-running instances taking into a cloud environment is extremely different like",
    "start": "1215480",
    "end": "1221450"
  },
  {
    "text": "the lifecycle like you expect things to change at certain rate but in the cloud",
    "start": "1221450",
    "end": "1226700"
  },
  {
    "text": "they change at a different rate like how long does a process live the data size so kubernetes likes smaller data size in",
    "start": "1226700",
    "end": "1234820"
  },
  {
    "text": "on-prem you may have a 14 terabyte database and kubernetes likes to move",
    "start": "1234820",
    "end": "1241010"
  },
  {
    "text": "things around you cannot move terabytes around with such ease so the culture of",
    "start": "1241010",
    "end": "1246650"
  },
  {
    "text": "Vitas is to build smaller chunks and so those we can quickly move around even if",
    "start": "1246650",
    "end": "1252620"
  },
  {
    "text": "kubernetes asked reschedule two kubernetes likes to reuse IPs that's a",
    "start": "1252620",
    "end": "1258080"
  },
  {
    "text": "no-no in on-prem you IPS are fixed for life and and topology overload is",
    "start": "1258080",
    "end": "1266810"
  },
  {
    "text": "another big concern many people think that systems like HDD zookeeper can be",
    "start": "1266810",
    "end": "1273260"
  },
  {
    "text": "used like a regular data store no they are used they are supposed to be used extremely sparingly and when you scale",
    "start": "1273260",
    "end": "1279980"
  },
  {
    "text": "things out you accidentally spam your topo and bring it down so all these things we had to change in with us so",
    "start": "1279980",
    "end": "1287320"
  },
  {
    "text": "yeah it's not very easy to convert a non cloud ready software to be cloud ready",
    "start": "1287320",
    "end": "1292430"
  },
  {
    "text": "because it is not sensitive to these cultural differences that exist between",
    "start": "1292430",
    "end": "1297650"
  },
  {
    "text": "the two and may you may end up spamming things that you should not be spamming all right so far so good",
    "start": "1297650",
    "end": "1304120"
  },
  {
    "start": "1303000",
    "end": "1323000"
  },
  {
    "text": "okay good doing our time so Kelsey did later reiterate his statement and he",
    "start": "1304120",
    "end": "1310850"
  },
  {
    "text": "says yes you cannot but you can use systems like quick tests because they actually glue provide the glue between",
    "start": "1310850",
    "end": "1319520"
  },
  {
    "text": "kubernetes and the storage so here is a witness architecture the app server as",
    "start": "1319520",
    "end": "1329210"
  },
  {
    "start": "1323000",
    "end": "1823000"
  },
  {
    "text": "far as the app server is concerned so essentially if you look at all the problems in the back",
    "start": "1329210",
    "end": "1335210"
  },
  {
    "text": "the proxy layer that we introduced at YouTube for solving a different problem is also coming in handy because that is",
    "start": "1335210",
    "end": "1342289"
  },
  {
    "text": "essentially you cannot run a storage system without a proxy layer because all those problems can only be solved with",
    "start": "1342289",
    "end": "1349070"
  },
  {
    "text": "the proxy layers standing in the middle so we had this VT gate so app server",
    "start": "1349070",
    "end": "1354379"
  },
  {
    "text": "talks to VT gate mitigate is completely stateless you can scale it up and down depending on your growing needs and as",
    "start": "1354379",
    "end": "1361490"
  },
  {
    "text": "far as the app is concerned once it's connected to VT gate it thinks that it is connected to it to a humungous",
    "start": "1361490",
    "end": "1366769"
  },
  {
    "text": "database but under the covers it's actually a whole bunch of mice equals all working in their own tandem like the",
    "start": "1366769",
    "end": "1375499"
  },
  {
    "text": "the way I would describe the philosophy behind this architecture is one is simplicity we don't have any moving",
    "start": "1375499",
    "end": "1381379"
  },
  {
    "text": "parts that are not necessary they are the bare minimum of what we need loose coupling so they are the advantage of",
    "start": "1381379",
    "end": "1389059"
  },
  {
    "text": "loose coupling is indefinite scalability so that's the reason why we test can",
    "start": "1389059",
    "end": "1394309"
  },
  {
    "text": "scale all these parts are meant to operate on their own with very little",
    "start": "1394309",
    "end": "1400240"
  },
  {
    "text": "interdependence II between them and somebody who are saying a loose coupling is actually one of the hardest problems",
    "start": "1400240",
    "end": "1405740"
  },
  {
    "text": "to solve in computer science because things tend to want to come together and",
    "start": "1405740",
    "end": "1411529"
  },
  {
    "text": "how do you draw the correct boundaries and keep the responsibility separate is where we Tessa's innovation lies and the",
    "start": "1411529",
    "end": "1420169"
  },
  {
    "text": "last but not the least is what I would call survivability which means that no single component here is essential at a",
    "start": "1420169",
    "end": "1427999"
  },
  {
    "text": "given point of time so anytime we expect to lose components here and as they lose",
    "start": "1427999",
    "end": "1433240"
  },
  {
    "text": "every other part of the system knows how to handle the fact that that component went away so those are the three",
    "start": "1433240",
    "end": "1438470"
  },
  {
    "text": "principles on which this architecture was built and I'm going to go through the scenarios that I talked about in the",
    "start": "1438470",
    "end": "1445909"
  },
  {
    "text": "previous slides about the problems that existed and how they get addressed in",
    "start": "1445909",
    "end": "1451279"
  },
  {
    "text": "with test right for example working backwards when I bring up a new VT",
    "start": "1451279",
    "end": "1456679"
  },
  {
    "text": "tablet as a new replica so what we test will do is it'll say oh this VT tablet",
    "start": "1456679",
    "end": "1464419"
  },
  {
    "text": "my identity is I am keyspace KS shard 0 let me go see if there is a backup so",
    "start": "1464419",
    "end": "1471260"
  },
  {
    "text": "which means that there exists a way for you to perform a backup so you could go to VD tablet and say please do a backup",
    "start": "1471260",
    "end": "1477710"
  },
  {
    "text": "of this and MIDI tablet will perform the backup and store the identity and necessary positions needed inside the",
    "start": "1477710",
    "end": "1484640"
  },
  {
    "text": "back up when the new VD tablet comes up it'll find that backup and restore it so it has a starting point and then it'll",
    "start": "1484640",
    "end": "1491539"
  },
  {
    "text": "point itself to the master and says based on the bin logs give me all the",
    "start": "1491539",
    "end": "1496820"
  },
  {
    "text": "events that have taken place and it'll catch up as soon as it is caught up what it does is it advertises to the topology",
    "start": "1496820",
    "end": "1503150"
  },
  {
    "text": "saying that hey I am ready to serve traffic and who is watching the topology the VT gates and they discover that oh",
    "start": "1503150",
    "end": "1510620"
  },
  {
    "text": "there's a new VT tablet here I'm going to start start sending traffic to this VT tablet so discovery is handled by",
    "start": "1510620",
    "end": "1517940"
  },
  {
    "text": "this fact and what is initialization is",
    "start": "1517940",
    "end": "1523970"
  },
  {
    "text": "handled what are the topic did we talk about repairing so repenting is the next",
    "start": "1523970",
    "end": "1528980"
  },
  {
    "text": "story like what if a master crashes like if a master the one thing that we found",
    "start": "1528980",
    "end": "1534260"
  },
  {
    "text": "out is in our original design I talked about cultural changes right in our original design when a master went down",
    "start": "1534260",
    "end": "1539690"
  },
  {
    "text": "you said oh all you have to do is save it in the topology and the mitigates we'll all get it but topologists are not",
    "start": "1539690",
    "end": "1548030"
  },
  {
    "text": "meant to react that fast they can they you can be like 15 minutes down even in",
    "start": "1548030",
    "end": "1555919"
  },
  {
    "text": "Google they they're the SLA is like 15 minutes before you can get an update",
    "start": "1555919",
    "end": "1561260"
  },
  {
    "text": "from a tow from a topology change you can't go 15 minutes without serving",
    "start": "1561260",
    "end": "1566450"
  },
  {
    "text": "traffic right master went done I need to react quickly so what did we do is every",
    "start": "1566450",
    "end": "1571700"
  },
  {
    "text": "VT gate that's connected to VD tablet has a back channel where the VD tablets are continuously feeding health",
    "start": "1571700",
    "end": "1577370"
  },
  {
    "text": "information both about how healthy they are and also about who is the master so",
    "start": "1577370",
    "end": "1583039"
  },
  {
    "text": "if a reparent happens as soon as a VT tablet realizes that it's a master it'll tell the vita gate I'm the master now",
    "start": "1583039",
    "end": "1589429"
  },
  {
    "text": "and we had a situation where sometimes network partition right there's a network partition the old cui T tablet",
    "start": "1589429",
    "end": "1596120"
  },
  {
    "text": "see they know I'm the master there's two people claiming they are the master so this is where packs",
    "start": "1596120",
    "end": "1601820"
  },
  {
    "text": "comes to our help where we say what's your master timestamp what's your master times time I'm going to trust you",
    "start": "1601820",
    "end": "1608270"
  },
  {
    "text": "because you have a newer timestamp so that way the system desire greats and safely continues forward so that's the",
    "start": "1608270",
    "end": "1615950"
  },
  {
    "text": "repenting story so finally as far as storage is concerned okay",
    "start": "1615950",
    "end": "1622510"
  },
  {
    "text": "I'm good in time so as far as storage is concerned you can run with us both using",
    "start": "1622510",
    "end": "1629930"
  },
  {
    "text": "mounted volume or local storage mounted was so people don't trust kubernetes",
    "start": "1629930",
    "end": "1637760"
  },
  {
    "text": "enough they still fear that there may be this Black Swan incident where the",
    "start": "1637760",
    "end": "1644660"
  },
  {
    "text": "entire kubernetes crashes so they still have that fear so they still have the fear that they may lose all the nodes at",
    "start": "1644660",
    "end": "1651200"
  },
  {
    "text": "the same time so because of that most people that run with tests still put their storage outside they they use",
    "start": "1651200",
    "end": "1660040"
  },
  {
    "text": "mounted volumes because they trust the mounted volumes for of course that trust",
    "start": "1660040",
    "end": "1665330"
  },
  {
    "text": "was recently broken by certain cloud providers I'm not going to go into that but at the end of the day it is software",
    "start": "1665330",
    "end": "1673580"
  },
  {
    "text": "that is running both whether it's outside or inside so you can run with tests with a mounted volume as a matter",
    "start": "1673580",
    "end": "1680270"
  },
  {
    "text": "of fact you are it's better to run a mounted volume using with tests because",
    "start": "1680270",
    "end": "1685910"
  },
  {
    "text": "you are so widely sharded that your latency sensitivity comes down whereas if you just a thing you have a single",
    "start": "1685910",
    "end": "1692210"
  },
  {
    "text": "monolithic my sequel instance there the latency becomes even more significant so",
    "start": "1692210",
    "end": "1697300"
  },
  {
    "text": "and in that case many of these stories happen but the case where there are",
    "start": "1697300",
    "end": "1703640"
  },
  {
    "text": "there is a way to run with us which is how we ran it at YouTube where we just put it on ephemera storage local storage",
    "start": "1703640",
    "end": "1709910"
  },
  {
    "text": "and semi sync lytic like I explained before and just play the game of",
    "start": "1709910",
    "end": "1716320"
  },
  {
    "text": "survival through numbers you know there is no way Borg is going to bring down all my twenty replicas at the same time",
    "start": "1716320",
    "end": "1723290"
  },
  {
    "text": "so in that case then there is no need to consider data as sacred can say data is",
    "start": "1723290",
    "end": "1733100"
  },
  {
    "text": "disposable because I can rebuild it I because there are the shards are small 200 300 gigs the shards as soon as a pod",
    "start": "1733100",
    "end": "1740330"
  },
  {
    "text": "goes down I can rebuild it in like 10 minutes so which means that my exposure to catastrophic events is highly reduced",
    "start": "1740330",
    "end": "1747320"
  },
  {
    "text": "and there is one more point I wanted to make about about this yes so I I it's",
    "start": "1747320",
    "end": "1755389"
  },
  {
    "text": "possible that this was covered in the slack talk so what what is the advantage",
    "start": "1755389",
    "end": "1760880"
  },
  {
    "text": "of treating your local data as ephemeral the advantage is that you're not going",
    "start": "1760880",
    "end": "1767330"
  },
  {
    "text": "to do crash recovery we have already made the decision so we can run my sequel oh okay so we can run my sequel",
    "start": "1767330",
    "end": "1778010"
  },
  {
    "text": "in crash unsafe mode because we never we are never going to recover so why make",
    "start": "1778010",
    "end": "1783139"
  },
  {
    "text": "it crash safe so crash unsafe mode means that you can use buffered i/o turn off",
    "start": "1783139",
    "end": "1789350"
  },
  {
    "text": "all crash safety and with buffered i/o every commit is in memory operation so",
    "start": "1789350",
    "end": "1794720"
  },
  {
    "text": "it's now going to scream faster than even a on train my sequel and we had",
    "start": "1794720",
    "end": "1800690"
  },
  {
    "text": "those kinds of numbers when you were running at YouTube like we had brought the latency down to like really really",
    "start": "1800690",
    "end": "1806480"
  },
  {
    "text": "tiny numbers like Maya my sequel would react in like microseconds which is like",
    "start": "1806480",
    "end": "1812000"
  },
  {
    "text": "memory right and I am ET tablet was on the same machine locals unique socket",
    "start": "1812000",
    "end": "1817190"
  },
  {
    "text": "issue command instantly get back screaming fast this is a subliminal",
    "start": "1817190",
    "end": "1823760"
  },
  {
    "start": "1823000",
    "end": "2300000"
  },
  {
    "text": "message nobody saw that message alright",
    "start": "1823760",
    "end": "1830779"
  },
  {
    "text": "so I'm finished with my talk do go to be tested IO there is a nice tutorial there",
    "start": "1830779",
    "end": "1837110"
  },
  {
    "text": "join the Vita's slack and try bringing up a cluster you can also go to console",
    "start": "1837110",
    "end": "1843980"
  },
  {
    "text": "dot planet scale calm if you are lazy about bringing up your own Vitas cluster so I am now open to questions there's a",
    "start": "1843980",
    "end": "1852860"
  },
  {
    "text": "question there",
    "start": "1852860",
    "end": "1855370"
  },
  {
    "text": "good question the question is if you have multiple shots how do you take point in time consistent backup the",
    "start": "1865639",
    "end": "1872329"
  },
  {
    "text": "simple answer is we don't we don't ever need to because every shard is on its own it survives on its own so different",
    "start": "1872329",
    "end": "1880279"
  },
  {
    "text": "shards get taken get backups taken at different points of time and the only time we use the backup is to actually",
    "start": "1880279",
    "end": "1886549"
  },
  {
    "text": "start a new video tablet right so it just picks up the snapshot as of",
    "start": "1886549",
    "end": "1891679"
  },
  {
    "text": "whatever time it's found it points it to the master and it catches up but there is a use case where you would want",
    "start": "1891679",
    "end": "1899269"
  },
  {
    "text": "something like point in time it's called when software engineers fat-finger things and accidentally wipe up an",
    "start": "1899269",
    "end": "1906259"
  },
  {
    "text": "entire column never happened before so",
    "start": "1906259",
    "end": "1911539"
  },
  {
    "text": "at that time you want to go back to your old data to restore it so actually Deepti who's going to be",
    "start": "1911539",
    "end": "1917419"
  },
  {
    "text": "talking tomorrow has done a feature where she she implemented a point in",
    "start": "1917419",
    "end": "1924349"
  },
  {
    "text": "time recovery where you say I want to create a snapshot of this key space this",
    "start": "1924349",
    "end": "1931369"
  },
  {
    "text": "is my cutoff time even though the backups were taken at different points in time they are approximately at the",
    "start": "1931369",
    "end": "1936829"
  },
  {
    "text": "same time and that is usually sufficient for a recovery so on top of this we are",
    "start": "1936829",
    "end": "1942440"
  },
  {
    "text": "going to add a feature where it'll actually phosphoryl up to the time that you wanted the snapshot to be to happen",
    "start": "1942440",
    "end": "1949279"
  },
  {
    "text": "so that part we have not got done yet but you can actually restore a key space as of asada as a fern approximate point",
    "start": "1949279",
    "end": "1956899"
  },
  {
    "text": "in time they accurate yes okay cool yes",
    "start": "1956899",
    "end": "1963669"
  },
  {
    "text": "do you have use case where we have multi region deployments absolutely that was what Vitas was built for it in YouTube",
    "start": "1970570",
    "end": "1978700"
  },
  {
    "text": "there are tens of regions pretty much spans YouTube the data centers plan we",
    "start": "1978700",
    "end": "1985540"
  },
  {
    "text": "test deployment spans the entire planet yes yes would you consider adding",
    "start": "1985540",
    "end": "1999640"
  },
  {
    "text": "relational databases absolutely we are very very seriously thinking about Postgres there has been many people",
    "start": "1999640",
    "end": "2006150"
  },
  {
    "text": "who've been asking about it we currently have our hands full but at some point",
    "start": "2006150",
    "end": "2011340"
  },
  {
    "text": "there will come a time when we are going to have spare engineering bad with' and we will definitely look at that yes the",
    "start": "2011340",
    "end": "2021240"
  },
  {
    "text": "question is where do you start the backups where do they store the backups the where does the backups can be stored",
    "start": "2021240",
    "end": "2028260"
  },
  {
    "text": "oh that's the other that's one of the things that I did I forgot to cover which is when we moved from when we",
    "start": "2028260",
    "end": "2036030"
  },
  {
    "text": "moved into Borg and had to build all these abstraction layers we built an abstraction layer for example for the",
    "start": "2036030",
    "end": "2043230"
  },
  {
    "text": "topo server which means that we support zookeeper we support console we supported CD so",
    "start": "2043230",
    "end": "2048570"
  },
  {
    "text": "that we got for free because we had to build that abstraction layer in the same way for backups we built some",
    "start": "2048570",
    "end": "2053940"
  },
  {
    "text": "abstraction layers which means that you can back it up to a mounted filesystem or you can back it up to s3 there's one",
    "start": "2053940",
    "end": "2060659"
  },
  {
    "text": "more storage Google Cloud Storage is the other one and also there's different",
    "start": "2060660",
    "end": "2066870"
  },
  {
    "text": "backup methods there's the regular backup method which is stop my sequel and run it and there's the extra backup",
    "start": "2066870",
    "end": "2072179"
  },
  {
    "text": "which can actually do the backup while the server is continuing to serve queries yes",
    "start": "2072179",
    "end": "2081889"
  },
  {
    "text": "good question so you are saying that with semi sink replication only one",
    "start": "2109000",
    "end": "2114160"
  },
  {
    "text": "other replica is guaranteed to have up-to-date data and how do you make sure",
    "start": "2114160",
    "end": "2119290"
  },
  {
    "text": "that you don't lose any data in terms of even if you have 80 replicas the",
    "start": "2119290",
    "end": "2125350"
  },
  {
    "text": "transaction that has committed just now is is likely is possibly they're only in",
    "start": "2125350",
    "end": "2131770"
  },
  {
    "text": "in one other replica so this goes back to Google's philosophy which is",
    "start": "2131770",
    "end": "2141390"
  },
  {
    "text": "typically Google says this so this has worked well in Borg Google says that",
    "start": "2141990",
    "end": "2147940"
  },
  {
    "text": "things will fail all the time but the likelihood of two things exactly failing",
    "start": "2147940",
    "end": "2153190"
  },
  {
    "text": "at the same time even in at Google scale is extremely low so at YouTube we never lost data due to",
    "start": "2153190",
    "end": "2159940"
  },
  {
    "text": "this and in kubernetes I don't know if",
    "start": "2159940",
    "end": "2165160"
  },
  {
    "text": "anybody has has had the confidence to run in MML mode but if they do so I",
    "start": "2165160",
    "end": "2173230"
  },
  {
    "text": "would encourage like I would not encourage somebody to run it in MML mode with if you had only two replicas but if",
    "start": "2173230",
    "end": "2179350"
  },
  {
    "text": "you had like more than n replicas like 5 or above I would feel fairly confident",
    "start": "2179350",
    "end": "2186820"
  },
  {
    "text": "to say that you can run it in ephemeral mode and at the end of the day it is",
    "start": "2186820",
    "end": "2191920"
  },
  {
    "text": "always a trade-off that you have to make between complete safety versus performance there is you cannot get both",
    "start": "2191920",
    "end": "2199270"
  },
  {
    "text": "it is it is the reality of life and in",
    "start": "2199270",
    "end": "2204820"
  },
  {
    "text": "YouTube we said that if we lost an entire data center we are ok",
    "start": "2204820",
    "end": "2211000"
  },
  {
    "text": "giving up the last few transactions that went so that was our our trade-off thing",
    "start": "2211000",
    "end": "2217930"
  },
  {
    "text": "and everybody was ok with that it just happened that we have no it has never happened losing an entire data center Google",
    "start": "2217930",
    "end": "2227020"
  },
  {
    "text": "actually has the freedom to come and take down a full data center but you have notice so we get notice and then we",
    "start": "2227020",
    "end": "2233440"
  },
  {
    "text": "fail over to another cell and eventually the they call these PCRs they come in",
    "start": "2233440",
    "end": "2240520"
  },
  {
    "text": "and take our our self but during my years at YouTube",
    "start": "2240520",
    "end": "2246709"
  },
  {
    "text": "they an entire cell never went down without whatever without expectation yes",
    "start": "2246709",
    "end": "2255160"
  },
  {
    "text": "right yes so that's an excellent question can you run with us in hybrid mode I've been seriously thinking about that where",
    "start": "2259900",
    "end": "2268099"
  },
  {
    "text": "you run ephemeral everywhere but have one replica that is writing to a mounted",
    "start": "2268099",
    "end": "2275359"
  },
  {
    "text": "volume that's completely viable you need a slightly more complicated scripts to",
    "start": "2275359",
    "end": "2280430"
  },
  {
    "text": "manage that but I think it's an excellent idea yeah yes",
    "start": "2280430",
    "end": "2289449"
  },
  {
    "text": "yes sorry because I wanted to cover the cloud aspect I skipped the rest of the parts so the way this works is the",
    "start": "2299340",
    "end": "2307080"
  },
  {
    "text": "topology has information about key spaces and shards and these VT tablets register themselves and VTA mitigates",
    "start": "2307080",
    "end": "2314070"
  },
  {
    "text": "eventually discover them and send traffic to them the the topology can be as I say read CD zookeeper or console",
    "start": "2314070",
    "end": "2321830"
  },
  {
    "text": "veetc TLD is basically a dashboard it reads the topology and displays what's in there for you to browse eventually",
    "start": "2321830",
    "end": "2329010"
  },
  {
    "text": "you can discover all the VT tablets that exist and all the VT not the VT gates and I can explain why the VT gates are",
    "start": "2329010",
    "end": "2335280"
  },
  {
    "text": "not part of the topology and you can also initiate workflows you can go tell",
    "start": "2335280",
    "end": "2341190"
  },
  {
    "text": "VT c TLD i want to do a recharging i want to create new shards i want to",
    "start": "2341190",
    "end": "2347100"
  },
  {
    "text": "split these two in things you can do you can through VT c @e c TLD you can say",
    "start": "2347100",
    "end": "2352440"
  },
  {
    "text": "create a backup so all those commands happen through VT c TLD there are a few",
    "start": "2352440",
    "end": "2358080"
  },
  {
    "start": "2357000",
    "end": "2540000"
  },
  {
    "text": "more components like VD worker and stuff they essentially are mini VT c TL DS that perform very specific tasks oh",
    "start": "2358080",
    "end": "2365930"
  },
  {
    "text": "that's that's what this this architecture is about and the only",
    "start": "2365930",
    "end": "2372270"
  },
  {
    "text": "reason VT tablet exists is for housekeeping like it does the backup restore and and those things that is its",
    "start": "2372270",
    "end": "2378390"
  },
  {
    "text": "main purpose question here",
    "start": "2378390",
    "end": "2385099"
  },
  {
    "text": "does Vitas have the ability to migrate entire data through teardown of an",
    "start": "2394960",
    "end": "2400300"
  },
  {
    "text": "existing cluster as in is is this an",
    "start": "2400300",
    "end": "2405490"
  },
  {
    "text": "external data type or internal is it like a separate my sequel instance or is it itself Oh within itself it has the",
    "start": "2405490",
    "end": "2413619"
  },
  {
    "text": "concept of cells and all cells are keeping up with the master wherever the master is you can failover a master from",
    "start": "2413619",
    "end": "2420760"
  },
  {
    "text": "one cell to another and completely tear down that that cell and that happens all the time at at YouTube we take down",
    "start": "2420760",
    "end": "2428710"
  },
  {
    "text": "cells sometimes we decommission them we don't want them anymore and then bring up new cells so that happens all the",
    "start": "2428710",
    "end": "2435730"
  },
  {
    "text": "time and with s is built to handle those workflows yes pardon oh how do you do",
    "start": "2435730",
    "end": "2447670"
  },
  {
    "text": "how do you do multiple cloud o query",
    "start": "2447670",
    "end": "2453250"
  },
  {
    "text": "across multiple shots so that is a challenge that is I would say the curse",
    "start": "2453250",
    "end": "2458950"
  },
  {
    "text": "of a Charlotte system the curse of a Charlotte system is that when you make a decision to say that data a lives here",
    "start": "2458950",
    "end": "2465760"
  },
  {
    "text": "and data be lives there that means that if you have to join the two you are going to go across charts so we test",
    "start": "2465760",
    "end": "2473440"
  },
  {
    "text": "will do a best-effort of saying that if you are going to join table a with table B and the rows are not together it will",
    "start": "2473440",
    "end": "2480040"
  },
  {
    "text": "actually do the back and forth for you and still give you the results it will not perform as well but what you can do",
    "start": "2480040",
    "end": "2487180"
  },
  {
    "text": "is there is a new feature of it tests that is called V replication where you can say even though this data is here",
    "start": "2487180",
    "end": "2494650"
  },
  {
    "text": "and even though that data is there I want this data to be here because it is related to stuff here but I want the",
    "start": "2494650",
    "end": "2501549"
  },
  {
    "text": "data to be there also because it is related to stuff there you can say materialized this table from here to",
    "start": "2501549",
    "end": "2507760"
  },
  {
    "text": "there and keep it up-to-date real-time you just give it a select query and say",
    "start": "2507760",
    "end": "2513430"
  },
  {
    "text": "do this and with this will materialize that entire table here and what it will do is it works just like replication it",
    "start": "2513430",
    "end": "2519940"
  },
  {
    "text": "subscribes to the bin locks and watches the events that are coming and immediately updates the target table so",
    "start": "2519940",
    "end": "2526180"
  },
  {
    "text": "you can do that for filtering you can do that for recharging you can do that for aggregations you can",
    "start": "2526180",
    "end": "2533410"
  },
  {
    "text": "actually do real-time roll-ups and it's extremely efficient at keeping the",
    "start": "2533410",
    "end": "2539230"
  },
  {
    "text": "target table up-to-date if you have a network partition you may incur some delays but if your network is healthy",
    "start": "2539230",
    "end": "2545920"
  },
  {
    "start": "2540000",
    "end": "2728000"
  },
  {
    "text": "and fine this table will should keep up within milliseconds of the source I saw hand oh yes",
    "start": "2545920",
    "end": "2554039"
  },
  {
    "text": "first question is are there is there any recommendation on the number of shards no the recommendation is about your",
    "start": "2570800",
    "end": "2577550"
  },
  {
    "text": "shard size so typically we say about two to three hundred gigs is the shard size",
    "start": "2577550",
    "end": "2582560"
  },
  {
    "text": "and depending on how big your data is you have as many shards as you need people have the largest instances that",
    "start": "2582560",
    "end": "2590630"
  },
  {
    "text": "I've heard of have hundreds of shards and we test has served comfortably there are a few people who are going into the",
    "start": "2590630",
    "end": "2596630"
  },
  {
    "text": "thousands of shards range we will see how that goes but I'm I don't know of any architectural limitation that'll",
    "start": "2596630",
    "end": "2603200"
  },
  {
    "text": "cause we test to struggle if there are too many shards I think Raphael fixed one bug in one",
    "start": "2603200",
    "end": "2609950"
  },
  {
    "text": "place about parallelism but what single shard so if there is any issues there",
    "start": "2609950",
    "end": "2615410"
  },
  {
    "text": "will be minor fixes that we can probably make your second question I didn't get Oh correct the question is are there any",
    "start": "2615410",
    "end": "2626050"
  },
  {
    "text": "operational concerns no you the number of shots doesn't cause any operational",
    "start": "2626050",
    "end": "2634070"
  },
  {
    "text": "overheads is because the effective resource used is the same amount of resource used is the same there's very",
    "start": "2634070",
    "end": "2640430"
  },
  {
    "text": "little static cost added by the fact of the existence of this components this is",
    "start": "2640430",
    "end": "2645500"
  },
  {
    "text": "because my sequel is capable of scaling itself down really really well the place",
    "start": "2645500",
    "end": "2651320"
  },
  {
    "text": "where we found where the static cost of my sickle was beginning to show up was when your data size was below 70 gigs so",
    "start": "2651320",
    "end": "2658910"
  },
  {
    "text": "- Oh money wise money wise some people",
    "start": "2658910",
    "end": "2667010"
  },
  {
    "text": "have complained that the TAS does burn a little more CPU than usual you have",
    "start": "2667010",
    "end": "2673670"
  },
  {
    "text": "identified the root cause as gr PC and we have to figure out what in gr PC is",
    "start": "2673670",
    "end": "2680240"
  },
  {
    "text": "causing that that burn so we'll get to the bottom of it very soon but it's only been a very minor complaint nobody has",
    "start": "2680240",
    "end": "2686870"
  },
  {
    "text": "complained like oh this is a showstopper I can't go with this level of CPU so we",
    "start": "2686870",
    "end": "2692510"
  },
  {
    "text": "haven't seen that but people have groaned about it saying that could you yes",
    "start": "2692510",
    "end": "2700180"
  },
  {
    "text": "any kind of any bad example of of workloads so I would say yeah who we are",
    "start": "2710970",
    "end": "2720300"
  },
  {
    "text": "about you have 15 minutes over so all right so I will get out and then you can",
    "start": "2720300",
    "end": "2725819"
  },
  {
    "text": "ask me questions",
    "start": "2725819",
    "end": "2728390"
  }
]