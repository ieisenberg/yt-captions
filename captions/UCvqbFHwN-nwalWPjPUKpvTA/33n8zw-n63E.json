[
  {
    "text": "hello everyone my name is Adrien and together with Victor we are going to present you",
    "start": "599",
    "end": "7399"
  },
  {
    "text": "our five-e journey of how we leverage name spaces in a multi-talent fashion in",
    "start": "7399",
    "end": "12519"
  },
  {
    "text": "order to scale the adoption of kubernetes at Adobe and also how we build a foundation for the Adobes",
    "start": "12519",
    "end": "19039"
  },
  {
    "text": "developer platform few words about me I'm currently Elite CL softer engineer at",
    "start": "19039",
    "end": "25320"
  },
  {
    "text": "Adobe and I'm part of the EOS team which is the team that powers the kubernetes",
    "start": "25320",
    "end": "31000"
  },
  {
    "text": "platform at Adobe I'm also a member of the kubernetes GitHub organization and",
    "start": "31000",
    "end": "38200"
  },
  {
    "text": "currently I'm focusing on contributing as much as I can to the cluster API",
    "start": "38200",
    "end": "44800"
  },
  {
    "text": "ecosystem and when I'm not not breaking clusters I like seeking for big bikes as",
    "start": "44800",
    "end": "50600"
  },
  {
    "text": "you can see on the slide Victor do you want to introduce yourself and kick off",
    "start": "50600",
    "end": "55680"
  },
  {
    "text": "the presentation yeah sure uh thank you Adrian",
    "start": "55680",
    "end": "61160"
  },
  {
    "text": "hello everyone uh my name is Victor V I'm technical lead Adobe um I'm also passionate about open source",
    "start": "61160",
    "end": "67680"
  },
  {
    "text": "contributions I am one of the organizers of kues community days or kcd in Romania",
    "start": "67680",
    "end": "73439"
  },
  {
    "text": "which will be the first kcd event in the southeast of Europe and it will be organized next year in April together",
    "start": "73439",
    "end": "81479"
  },
  {
    "text": "with Adrian we are the authors of adobe SK Shredder and Costa registry two open",
    "start": "81479",
    "end": "88200"
  },
  {
    "text": "source projects that we successfully integrated to our platform and about each we are going to talk",
    "start": "88200",
    "end": "94920"
  },
  {
    "text": "today in the first part of the presentation I'm going to talk about project Tios kubernetes name spaces and",
    "start": "94920",
    "end": "101799"
  },
  {
    "text": "capacity management and Adrian will continue with uh governance policies",
    "start": "101799",
    "end": "106880"
  },
  {
    "text": "multi tenacy at scale and non-disruptive kubernetes upgrades plus a live demo so",
    "start": "106880",
    "end": "112479"
  },
  {
    "text": "stay tuned before we dive in I would like to share with you a nice quote by uh Martin",
    "start": "112479",
    "end": "120039"
  },
  {
    "text": "Kean which I found in his recent book title inspired he says it doesn't matter how",
    "start": "120039",
    "end": "126840"
  },
  {
    "text": "good your engineering team is if they are not giv something worldwide to build in other words it is important",
    "start": "126840",
    "end": "134280"
  },
  {
    "text": "what your engineering teams are building but also the target audience at Adobe",
    "start": "134280",
    "end": "139720"
  },
  {
    "text": "our kubernetes platform called EOS it's used by amazing internal engineering",
    "start": "139720",
    "end": "145200"
  },
  {
    "text": "teams which are working at Adobe products such as Adobe Photoshop Adobe",
    "start": "145200",
    "end": "151000"
  },
  {
    "text": "analytics Adobe Firefly Adobe experience manager Adobe sign and so",
    "start": "151000",
    "end": "158680"
  },
  {
    "text": "on Project itos it's a cross cloud multi-tenant kubernetes based platform",
    "start": "158800",
    "end": "165080"
  },
  {
    "text": "built through the collaboration between the Adobe infrastructure teams and product development teams the initial",
    "start": "165080",
    "end": "172879"
  },
  {
    "text": "version of ethos has its roots in 20 2015 and it was built with Docker and",
    "start": "172879",
    "end": "179239"
  },
  {
    "text": "dco and it was first in production in 2016 it was a good decision at that",
    "start": "179239",
    "end": "185400"
  },
  {
    "text": "moment to start with dcos because we gain experience with containers",
    "start": "185400",
    "end": "190480"
  },
  {
    "text": "microservice architectures multi- tenacy before kubernetes became",
    "start": "190480",
    "end": "195560"
  },
  {
    "text": "matured we also build necessary abstractions so a developer can",
    "start": "195560",
    "end": "200840"
  },
  {
    "text": "seamlessly deploy his application in production this obstruction is called EOS C or containers as a",
    "start": "200840",
    "end": "208680"
  },
  {
    "text": "service in 2018 we started a development of of the Next Generation runtime platform",
    "start": "208680",
    "end": "216080"
  },
  {
    "text": "based on kubernetes we identify an opportunity within kubernetes name spaces and add",
    "start": "216080",
    "end": "222799"
  },
  {
    "text": "them as a new option for our developers this offering is called itos",
    "start": "222799",
    "end": "228360"
  },
  {
    "text": "pass which stands for platform as a service with itos pass developers take",
    "start": "228360",
    "end": "235040"
  },
  {
    "text": "ownership of the kubernetes name space and they can deploy their applications inside",
    "start": "235040",
    "end": "241680"
  },
  {
    "text": "of course using their preferred cicd tool and uh this approach gives flexibility to",
    "start": "242000",
    "end": "248079"
  },
  {
    "text": "developers and it is particularly valuable when your application serves as the core cicd tool and can deploy other",
    "start": "248079",
    "end": "255879"
  },
  {
    "text": "applications in kubernetes it is also valuable when your company is involved in Acquisitions of",
    "start": "255879",
    "end": "262440"
  },
  {
    "text": "other companies so uh making M migration of the applications to your company's",
    "start": "262440",
    "end": "268919"
  },
  {
    "text": "platform it's a straightforward process using kubernetes name spaces in 2019 we started the full",
    "start": "268919",
    "end": "277320"
  },
  {
    "text": "migration of the Legacy uh cast users from dcos to",
    "start": "277320",
    "end": "282720"
  },
  {
    "text": "kubernetes and in 2022 based on the experience that we got with",
    "start": "282720",
    "end": "288600"
  },
  {
    "text": "OS cast and itos pass we introduced a new flavor which is itos flex itos Flex",
    "start": "288600",
    "end": "295720"
  },
  {
    "text": "is running on top of itos pass and it is based on gitops and Argo so it provides",
    "start": "295720",
    "end": "301600"
  },
  {
    "text": "a path way to deploy your application in production but also the flexibility of kubernetes name",
    "start": "301600",
    "end": "308800"
  },
  {
    "text": "spaces another big milestone was this year when we adopted",
    "start": "308800",
    "end": "315520"
  },
  {
    "text": "cluster API and Argo for the infrastructure side for building and managing kubernetes",
    "start": "315520",
    "end": "323240"
  },
  {
    "text": "clusters this is uh itos kubernetes platform from 10,000 FTS and how he",
    "start": "323360",
    "end": "329080"
  },
  {
    "text": "stands it Adobe on the top of the slide we have the three main Adobe Cloud Creative",
    "start": "329080",
    "end": "334840"
  },
  {
    "text": "Cloud experience cloud and document Cloud these clouds are powered by Adobe",
    "start": "334840",
    "end": "340560"
  },
  {
    "text": "software products such as Adobe Photoshop Adobe Firefly Adobe analytics",
    "start": "340560",
    "end": "346520"
  },
  {
    "text": "Adobe experience manager Adobe sign and so on and together with the platform that they are using such as Sensei",
    "start": "346520",
    "end": "353240"
  },
  {
    "text": "machine learning content platform experience platform all together are running on top of EOS and EOS is",
    "start": "353240",
    "end": "359560"
  },
  {
    "text": "basically the Adobe run time for containerized applications it also operates on three",
    "start": "359560",
    "end": "365960"
  },
  {
    "text": "main Cloud providers Adobe private Cloud AWS and",
    "start": "365960",
    "end": "372080"
  },
  {
    "text": "Azure to better understand the platform scalability let's talk about the pretty impressive numbers which are growing",
    "start": "372080",
    "end": "379319"
  },
  {
    "text": "every month itos holds more than 2 million containers encapsulated in 1",
    "start": "379319",
    "end": "384560"
  },
  {
    "text": "million pods and these pods are running in uh",
    "start": "384560",
    "end": "389800"
  },
  {
    "text": "41,000 10 name spaces Nam spaces which are own owned by the um application",
    "start": "389800",
    "end": "396199"
  },
  {
    "text": "development teams we are managing more than 300 clusters deployed on 28 different Cloud",
    "start": "396199",
    "end": "402560"
  },
  {
    "text": "regions in AWS Azure and Adobe private cloud in terms of computing power this",
    "start": "402560",
    "end": "408919"
  },
  {
    "text": "workloads use around 35,000 compute nodes consuming",
    "start": "408919",
    "end": "414080"
  },
  {
    "text": "approximately 2.9 petabytes of RAM memory and and 800,000 virtual",
    "start": "414080",
    "end": "422199"
  },
  {
    "text": "CPUs the AI applications which are more and more present to our platform utilize",
    "start": "422199",
    "end": "428160"
  },
  {
    "text": "almost 8,000 gpus let's talk about multi- tency in",
    "start": "428160",
    "end": "434840"
  },
  {
    "text": "kubernetes how many of you heard about multi- tency",
    "start": "434840",
    "end": "440199"
  },
  {
    "text": "architecture okay who and how many of you are using kubernetes in a multi-end",
    "start": "440199",
    "end": "447400"
  },
  {
    "text": "architecture okay we have a pretty good number I can I can count them um there are many definitions for",
    "start": "447400",
    "end": "455240"
  },
  {
    "text": "multi-tenancy at Adobe we are using multi-tenancy architecture as a way to share multiple",
    "start": "455240",
    "end": "462360"
  },
  {
    "text": "physical clusters with multiple teams from different organizations and different projects and we have two types",
    "start": "462360",
    "end": "469800"
  },
  {
    "text": "of clusters shared clusters and dedicated clusters also known as multi-tenant clusters and single tenant",
    "start": "469800",
    "end": "477039"
  },
  {
    "text": "clusters share clusters are available able for any uh internal engineering",
    "start": "477039",
    "end": "482240"
  },
  {
    "text": "team in Adobe and are highly valuable for optimizing the cost and enhancing",
    "start": "482240",
    "end": "488159"
  },
  {
    "text": "the overall platform reliability dedicated clusters on the",
    "start": "488159",
    "end": "493759"
  },
  {
    "text": "other hand are used for two main purposes uh when high security isolation",
    "start": "493759",
    "end": "499479"
  },
  {
    "text": "is required such as for applications that can run untrusted software for instance Adobe experience manager which",
    "start": "499479",
    "end": "506759"
  },
  {
    "text": "is a Content management system solution can run software written by Adobe",
    "start": "506759",
    "end": "513518"
  },
  {
    "text": "customers another scenario is uh when a specific team requires High resource",
    "start": "513519",
    "end": "518800"
  },
  {
    "text": "demand for their application which need the entire cluster resources an example of this is Adobe",
    "start": "518800",
    "end": "526480"
  },
  {
    "text": "Firefly which is a generative AI content creation solution that requires a high resource",
    "start": "526480",
    "end": "533120"
  },
  {
    "text": "demand for the available CPU and gpus inside of a",
    "start": "533120",
    "end": "538200"
  },
  {
    "text": "cluster in order to implement multi-tenancy we rely on kubernetes name spaces and",
    "start": "538200",
    "end": "545480"
  },
  {
    "text": "developers love Nam spaces because they provide flexibility more control and easy troubleshoot their",
    "start": "545480",
    "end": "551839"
  },
  {
    "text": "applications in EOS we use an unique npace Name Across the entire",
    "start": "551839",
    "end": "557399"
  },
  {
    "text": "fleet and we deploy a namespace profile template on the",
    "start": "557399",
    "end": "563839"
  },
  {
    "text": "Clusters an aspace profile template is made by few kubernetes objects in order to Prov provide a minimum isolation",
    "start": "563839",
    "end": "570839"
  },
  {
    "text": "within a cluster first of all we need a kubernetes n space object to group the objects for a single team within",
    "start": "570839",
    "end": "577560"
  },
  {
    "text": "kubernetes API to ensure that only a specific team has access to a particular name space we",
    "start": "577560",
    "end": "585959"
  },
  {
    "text": "are using role Bindings that link the default kubernetes cluster roles admin",
    "start": "585959",
    "end": "591720"
  },
  {
    "text": "edit and View and C limit range play a crucial",
    "start": "591720",
    "end": "596839"
  },
  {
    "text": "role in limiting um and controlling resource consumption ensuring Fair",
    "start": "596839",
    "end": "602760"
  },
  {
    "text": "resource distribution of the resources inside of a cluster for networ isolation we are",
    "start": "602760",
    "end": "609440"
  },
  {
    "text": "using both kubernetes native Network policies and celum network policies and",
    "start": "609440",
    "end": "616320"
  },
  {
    "text": "here C netor policies are useful for implementing DNS based policies and",
    "start": "616320",
    "end": "621360"
  },
  {
    "text": "other ler 7 policies after the npace profile is",
    "start": "621360",
    "end": "627320"
  },
  {
    "text": "deployed on a cluster the tenant can deploy his application inside and the Tenant application objects will be",
    "start": "627320",
    "end": "633959"
  },
  {
    "text": "restricted only to a specific team and the PS will be isolated by the default Network",
    "start": "633959",
    "end": "640920"
  },
  {
    "text": "policies in a multi-tenant environment capacity management is a key consideration because capacity issues",
    "start": "642000",
    "end": "649120"
  },
  {
    "text": "may result in higher cost by the way who doesn't have cost concerns today when",
    "start": "649120",
    "end": "654240"
  },
  {
    "text": "running an application in the cloud we tend to take actions at three",
    "start": "654240",
    "end": "659560"
  },
  {
    "text": "uh three level at the Pod level so in addition to horizontal pod Auto scaling",
    "start": "659560",
    "end": "665880"
  },
  {
    "text": "and vertical pod Auto scaling we are using a solution named automatic resource",
    "start": "665880",
    "end": "672440"
  },
  {
    "text": "configuration at the Nam Space level we simplify Kota management using the concept of Baseline Kota unit and at the",
    "start": "672440",
    "end": "680959"
  },
  {
    "text": "cluster level we added capacity alerts let's go through automatic res",
    "start": "680959",
    "end": "688480"
  },
  {
    "text": "configuration we know that in kubernetes PS are scheduled on the worker nodes based on",
    "start": "688480",
    "end": "694440"
  },
  {
    "text": "their container resour requests and they can burst up to the specified limits so",
    "start": "694440",
    "end": "700320"
  },
  {
    "text": "if the resource request are lower then smaller allocations are reserved for that pod this allows for more pods to be",
    "start": "700320",
    "end": "708240"
  },
  {
    "text": "scheduled on the Node which result in cost savings and to achieve this we rely",
    "start": "708240",
    "end": "714079"
  },
  {
    "text": "on promit metrics to ga uh historical utilization data for the the deployment",
    "start": "714079",
    "end": "719639"
  },
  {
    "text": "pods then an Opa policy it's applied to adjust the right size of the CPU and",
    "start": "719639",
    "end": "726920"
  },
  {
    "text": "memory request for that specific Pods at the Nam Space level in order to",
    "start": "726920",
    "end": "735320"
  },
  {
    "text": "simplify Kota management operations we introduce the concept of Baseline Kota",
    "start": "735320",
    "end": "740360"
  },
  {
    "text": "unit or BQ a BQ it's actually a quota definition",
    "start": "740360",
    "end": "746040"
  },
  {
    "text": "and every namespace quota increase it's a chiev by multiplying each of the BQ",
    "start": "746040",
    "end": "751600"
  },
  {
    "text": "items and for example we have a BQ definition here and if you want to",
    "start": "751600",
    "end": "757199"
  },
  {
    "text": "allocate let's say 32 virtual CPUs for our name space instead of 16 vcpus as we",
    "start": "757199",
    "end": "763240"
  },
  {
    "text": "have right now we just simply increase the namespace quota from one to two bqs",
    "start": "763240",
    "end": "769079"
  },
  {
    "text": "and the other BQ items will be multiply as well so we also have available for",
    "start": "769079",
    "end": "774199"
  },
  {
    "text": "our Nam space 60 uh ports to run and",
    "start": "774199",
    "end": "779440"
  },
  {
    "text": "this approach simplifies uh the operations for both uh",
    "start": "779440",
    "end": "784720"
  },
  {
    "text": "tenant owners of the Nam space and the cluster",
    "start": "784720",
    "end": "790600"
  },
  {
    "text": "administrators at the cluster level we measure if a cluster reach the capacity",
    "start": "790600",
    "end": "796279"
  },
  {
    "text": "using promus alerts and how we are doing this in itos the source of record for",
    "start": "796279",
    "end": "802800"
  },
  {
    "text": "cluster information is stor in an application named cluster registry which by the way it's open source and it is",
    "start": "802800",
    "end": "809320"
  },
  {
    "text": "available under adobe's GitHub organization and there is a cust regist",
    "start": "809320",
    "end": "815519"
  },
  {
    "text": "client that runs in every cluster and accepts signals from alert manager and",
    "start": "815519",
    "end": "823079"
  },
  {
    "text": "in pritus we have multiple capacity sub alerts That Fire based on some specific",
    "start": "823079",
    "end": "828440"
  },
  {
    "text": "metric thrs and uh yeah for example number of",
    "start": "828440",
    "end": "834480"
  },
  {
    "text": "nodes or number of available IPS that can be assigned to a nod or number of name spaces and and so on and one of the",
    "start": "834480",
    "end": "841079"
  },
  {
    "text": "subalert fires the main capacity alert notifies cluster regist client so",
    "start": "841079",
    "end": "848759"
  },
  {
    "text": "cluster information is updated and for example npace on boarding is disabled or",
    "start": "848759",
    "end": "854759"
  },
  {
    "text": "even more namespace quota increase is Frozen for all of the existing Nam spaces in the",
    "start": "854759",
    "end": "861320"
  },
  {
    "text": "cluster uh now I'm going to pass itan so we can talk more about governance policies multi scale and non disruptive",
    "start": "861320",
    "end": "869160"
  },
  {
    "text": "kubernetes upgrades thank you Victor I would like to continue our talk",
    "start": "869160",
    "end": "876399"
  },
  {
    "text": "about multi tency but tackle it from an infrastructure perspective and I prepared three topics today in order to",
    "start": "876399",
    "end": "883519"
  },
  {
    "text": "cover the reliability and efficiency on one hand and scalability and security on",
    "start": "883519",
    "end": "889320"
  },
  {
    "text": "the other hand and I will start with the governance policies as any company or business is governed by a set of rules",
    "start": "889320",
    "end": "896399"
  },
  {
    "text": "so does a multi-tenant kubernetes cluster why are these policies mandatory and",
    "start": "896399",
    "end": "902040"
  },
  {
    "text": "what benefits do they bring into the kubernetes ecosystem from our perspective along with the security",
    "start": "902040",
    "end": "908120"
  },
  {
    "text": "aspect there are two main advantages when defining a set of rules inside the kubernetes Clusters first it is for",
    "start": "908120",
    "end": "915680"
  },
  {
    "text": "safeguarding teams against inter team collisions and second for protecting the cluster stability so that a single",
    "start": "915680",
    "end": "923440"
  },
  {
    "text": "development team cannot jeopardize the entire cluster few years ago",
    "start": "923440",
    "end": "929199"
  },
  {
    "text": "when we initially started building our platform we had a pretty interesting outage one day two distinct teams",
    "start": "929199",
    "end": "936199"
  },
  {
    "text": "created two different Ingress objects in two distinct name spaces but pointing to",
    "start": "936199",
    "end": "941720"
  },
  {
    "text": "the same fqdn and it took us a while until we realized that these two objects were",
    "start": "941720",
    "end": "948800"
  },
  {
    "text": "conflicting with each other because there were no validating web hooks",
    "start": "948800",
    "end": "953839"
  },
  {
    "text": "implemented by the engress controller at that time except the crd schema",
    "start": "953839",
    "end": "959279"
  },
  {
    "text": "and this was the moment when we decided that a set of governance policies were",
    "start": "959279",
    "end": "964880"
  },
  {
    "text": "mandatory and in order to implement these policies across our cluster Fleet we picked the OPA gatekeeper framework",
    "start": "964880",
    "end": "972040"
  },
  {
    "text": "for those of you who are not familiar with Opa gatekeeper is an extensible admission controller which is already",
    "start": "972040",
    "end": "978600"
  },
  {
    "text": "configured with all the uh with all of the necessary kubernetes API plumbing and cluster",
    "start": "978600",
    "end": "985120"
  },
  {
    "text": "operators can change the business logic of GateKeeper by simply writing policies",
    "start": "985120",
    "end": "990839"
  },
  {
    "text": "which are regular queries as short as a few lines and we will see in a bit such an example but getting back to our",
    "start": "990839",
    "end": "997839"
  },
  {
    "text": "outage after that event we created the the validating Ingress policy which",
    "start": "997839",
    "end": "1004920"
  },
  {
    "text": "denies the creation or update of Ingress objects which attempt to use an fqdn",
    "start": "1004920",
    "end": "1010160"
  },
  {
    "text": "which is already in use by any other existing Ingress",
    "start": "1010160",
    "end": "1015360"
  },
  {
    "text": "objects some other example policies we are currently deploying across our cluster Fleet if I have to name a few I",
    "start": "1015360",
    "end": "1021920"
  },
  {
    "text": "will stop to the control plane Toleration policy which is a policy used to restrict the workloads that can run",
    "start": "1021920",
    "end": "1028400"
  },
  {
    "text": "inside the control play nodes Crown job history another policy which is used to",
    "start": "1028400",
    "end": "1033959"
  },
  {
    "text": "restrict the history of a crown job so that we are not putting unnecessary load",
    "start": "1033959",
    "end": "1039199"
  },
  {
    "text": "on the ATD side default Ingress class another interesting policy which is used",
    "start": "1039199",
    "end": "1045400"
  },
  {
    "text": "to add an Ingress class on all objects that are not explicitly specifying the",
    "start": "1045400",
    "end": "1051799"
  },
  {
    "text": "Ingress class they want to use npace limit another policy which is used to",
    "start": "1051799",
    "end": "1056840"
  },
  {
    "text": "limit the total number of Nam spaces that can be created inside the cluster and external IP Services another policy",
    "start": "1056840",
    "end": "1063760"
  },
  {
    "text": "which is used to deny the creation of external IP services and many others as you can see on the slide we",
    "start": "1063760",
    "end": "1070960"
  },
  {
    "text": "have a Rego sliet that's implementing the external IP services and as you can",
    "start": "1070960",
    "end": "1077240"
  },
  {
    "text": "notice with all only a few lines of code we were able to define a pretty powerful",
    "start": "1077240",
    "end": "1082559"
  },
  {
    "text": "policy which is denying the creation of any external IP services and what is it",
    "start": "1082559",
    "end": "1088240"
  },
  {
    "text": "doing first it will check if the object from the request of type service then it will check if the",
    "start": "1088240",
    "end": "1095200"
  },
  {
    "text": "operation is create or update and in the end we'll check if the object spec has",
    "start": "1095200",
    "end": "1100640"
  },
  {
    "text": "any external IPS defined and if all these three conditions are met we are rejecting the request and sending back a",
    "start": "1100640",
    "end": "1107440"
  },
  {
    "text": "message to the user stating that external IP services are not permitted because there is a pretty",
    "start": "1107440",
    "end": "1113760"
  },
  {
    "text": "high vulnerability found inside the kubernetes code base one thing to keep in mind here is",
    "start": "1113760",
    "end": "1120440"
  },
  {
    "text": "that gatekeeper as any other validation or mutation web hook adds latency to any",
    "start": "1120440",
    "end": "1126520"
  },
  {
    "text": "API request it mutates or validates why simply because of the extra process in",
    "start": "1126520",
    "end": "1132760"
  },
  {
    "text": "time needed to mutate or validate the request so the more policies you define",
    "start": "1132760",
    "end": "1137840"
  },
  {
    "text": "the higher higher API response latency might be another story is about multi-tenancy",
    "start": "1137840",
    "end": "1145159"
  },
  {
    "text": "at scale as you saw the numbers at the beginning of the presentation you can imagine we are running at a pretty high",
    "start": "1145159",
    "end": "1152000"
  },
  {
    "text": "scale and challenges for such a big platform are diverse recently we switch",
    "start": "1152000",
    "end": "1157720"
  },
  {
    "text": "from our internally developed cic CD tool to the Argo ecosystem I guess",
    "start": "1157720",
    "end": "1163440"
  },
  {
    "text": "everyone is already familiar with Argo we just had a argoon a few days ago",
    "start": "1163440",
    "end": "1170240"
  },
  {
    "text": "but during our argocd evaluation process one of the first challenges we encountered was the fact that a single",
    "start": "1170240",
    "end": "1176200"
  },
  {
    "text": "Aro CD instance couldn't handle the reconciliation volume needed for our",
    "start": "1176200",
    "end": "1181400"
  },
  {
    "text": "Fleet and to give you an idea we are deploying between 70 to 90 admin",
    "start": "1181400",
    "end": "1186919"
  },
  {
    "text": "components per kubernetes cluster and with a fleet of more than 300 clusters",
    "start": "1186919",
    "end": "1192440"
  },
  {
    "text": "you can imagine that the total number of applications needed to be synced is over",
    "start": "1192440",
    "end": "1197760"
  },
  {
    "text": "24 4,000 way higher than a single Argo CD instance can handle and so in order to be able to",
    "start": "1197760",
    "end": "1205919"
  },
  {
    "text": "scale the roll out of all the admin components across the fleet we've come up with a pretty interesting pattern",
    "start": "1205919",
    "end": "1212480"
  },
  {
    "text": "which we called Argo of Argos as you can see on the slide we are running a",
    "start": "1212480",
    "end": "1218039"
  },
  {
    "text": "multi-tier Argo CD setup where tier zero is used to reconcile the tier one Argo",
    "start": "1218039",
    "end": "1223600"
  },
  {
    "text": "CD instances and Argo CD instance and tier one Argo CD instances are to",
    "start": "1223600",
    "end": "1229360"
  },
  {
    "text": "reconcile the or to sync the cluster admin components",
    "start": "1229360",
    "end": "1234799"
  },
  {
    "text": "fleetwide sorry moreover H ti1 Argo CD instance is handling only a subset of",
    "start": "1236240",
    "end": "1243360"
  },
  {
    "text": "the kubernetes Clusters that are part of the fleet also all tier one argocd",
    "start": "1243360",
    "end": "1248840"
  },
  {
    "text": "instances have the same config and have registered the same set of application",
    "start": "1248840",
    "end": "1253960"
  },
  {
    "text": "sets so that we have consistency across the entire tier one",
    "start": "1253960",
    "end": "1259559"
  },
  {
    "text": "argd instances and in this way we accomplished the flexibility when it",
    "start": "1259559",
    "end": "1265720"
  },
  {
    "text": "comes to scalability of the continuous delivery system we can always add more tier one Aro CD instances or remove them",
    "start": "1265720",
    "end": "1273240"
  },
  {
    "text": "based on our platform need and the last story for today is",
    "start": "1273240",
    "end": "1280520"
  },
  {
    "text": "about non-d disrupting cluster upgrades as our platform evolved and started",
    "start": "1280520",
    "end": "1286360"
  },
  {
    "text": "onboarding more and more teams the diversity of the workloads running on top was also increasing some teams",
    "start": "1286360",
    "end": "1294039"
  },
  {
    "text": "started running State flaps like databases or distributed event streaming apps that H as we know are pretty",
    "start": "1294039",
    "end": "1301360"
  },
  {
    "text": "sensitive to disruptions and after we had few outages caused by cluster upgrades it was clear",
    "start": "1301360",
    "end": "1309039"
  },
  {
    "text": "that we needed to develop a new strategy while doing cluster upgrades because the",
    "start": "1309039",
    "end": "1314400"
  },
  {
    "text": "PO disruption budgets alone were not enough and we were looking to have a high",
    "start": "1314400",
    "end": "1321279"
  },
  {
    "text": "enough velocity while rotating the worker nodes but still maintain the client's apps availability and",
    "start": "1321279",
    "end": "1327200"
  },
  {
    "text": "infrastructure cost at some reasonable thresholds and so we came up with what",
    "start": "1327200",
    "end": "1332640"
  },
  {
    "text": "we call the Park nodes upgrade strategy and this strategy is implemented around K Shredder which is a",
    "start": "1332640",
    "end": "1339679"
  },
  {
    "text": "kuberi controller developed in House at Adobe and then open sourc it is available under the Adobe GitHub org and",
    "start": "1339679",
    "end": "1346559"
  },
  {
    "text": "you can scan the QR code from the slide in order to get access to it how does our non-disruptive cluster",
    "start": "1346559",
    "end": "1353600"
  },
  {
    "text": "upgrade procedure work from a high level perspective during a full cluster",
    "start": "1353600",
    "end": "1358960"
  },
  {
    "text": "upgrade we are draining in batches a percentage of the total worker node at a time while adding new worker nodes also",
    "start": "1358960",
    "end": "1368039"
  },
  {
    "text": "we are cordoning all the existing worker nodes so that no new pods can be",
    "start": "1368039",
    "end": "1373080"
  },
  {
    "text": "scheduled on them and for the sake of the example let's assume we have a cluster with two",
    "start": "1373080",
    "end": "1379480"
  },
  {
    "text": "worker nodes which we are going to upgrade to a newer kubernetes version",
    "start": "1379480",
    "end": "1384919"
  },
  {
    "text": "once the upgrade process begin as I mentioned earlier we add a",
    "start": "1384919",
    "end": "1390240"
  },
  {
    "text": "new worker node running a newer version of kubernetes and then start draining the old",
    "start": "1390240",
    "end": "1396080"
  },
  {
    "text": "noge evicted pods will be moved to the new worker node since the old ones were",
    "start": "1396080",
    "end": "1402840"
  },
  {
    "text": "already cordoned at the beginning of the upgrade process",
    "start": "1402840",
    "end": "1408840"
  },
  {
    "text": "as the upgrade is progressing more pods are moved to the new node until there is no capacity on it and if you are running",
    "start": "1408840",
    "end": "1416440"
  },
  {
    "text": "out of resources we are simply just pinning up new worker nodes to accommodate all the pods that are",
    "start": "1416440",
    "end": "1423039"
  },
  {
    "text": "evicted by the draining process okay if during the configured R timeout",
    "start": "1423039",
    "end": "1430440"
  },
  {
    "text": "not all the pods are evicted the upgrade process will label the worker as parked and add a l or time to",
    "start": "1430440",
    "end": "1438880"
  },
  {
    "text": "leave for it also all the old ones that were successfully drained and which",
    "start": "1438880",
    "end": "1444559"
  },
  {
    "text": "don't have any running pods on them will be recycled by the cluster autoscaler or by the upgrade process eventually and",
    "start": "1444559",
    "end": "1452200"
  },
  {
    "text": "with that this is the moment we consider the cluster upgrade as finished and once",
    "start": "1452200",
    "end": "1458120"
  },
  {
    "text": "the upgrade is finished development teams that are still running pods on Park nodes are getting notified so that",
    "start": "1458120",
    "end": "1465399"
  },
  {
    "text": "they can take all the necessary measures to move their pods out of these Park nodes before the TTL",
    "start": "1465399",
    "end": "1474080"
  },
  {
    "text": "expires and once the training process is finished on all worker nodes then K Shredder is taking over the process what",
    "start": "1474080",
    "end": "1481000"
  },
  {
    "text": "is it doing behind the scene first it will identified all the park nodes and",
    "start": "1481000",
    "end": "1486200"
  },
  {
    "text": "then for each of them we'll grab all the running pods and for each of these pods we run a set of eviction",
    "start": "1486200",
    "end": "1493679"
  },
  {
    "text": "Loops initially it will periodically try to sof of evict all the running pods on",
    "start": "1493679",
    "end": "1499960"
  },
  {
    "text": "the park nodes while respecting the pdbs and most of the pods will be",
    "start": "1499960",
    "end": "1505159"
  },
  {
    "text": "successfully soft evicted by K Shredder after the uh after few eviction",
    "start": "1505159",
    "end": "1511039"
  },
  {
    "text": "Loops but some of them won't be able to but still K shadder will periodically",
    "start": "1511039",
    "end": "1516440"
  },
  {
    "text": "monitor the TTL of the park node and if after he configured Park node TTL there",
    "start": "1516440",
    "end": "1521799"
  },
  {
    "text": "are still running pods that couldn't be soft evicted then Cas Rader is taking a",
    "start": "1521799",
    "end": "1526840"
  },
  {
    "text": "pretty aggressive measure and will just force evict all those running pods and",
    "start": "1526840",
    "end": "1532240"
  },
  {
    "text": "once there are no more pods running on the park node cluster autoscaler will just uh recycle this park node and with",
    "start": "1532240",
    "end": "1541720"
  },
  {
    "text": "that all worker nodes from the cluster are running the new version of",
    "start": "1541720",
    "end": "1546919"
  },
  {
    "text": "kubernetes putting all the steps together you can notice that the process is pretty smooth and eventually all",
    "start": "1546919",
    "end": "1553440"
  },
  {
    "text": "worker nodes will be running a newer version of kubernetes",
    "start": "1553440",
    "end": "1559679"
  },
  {
    "text": "okay let's see it in action we prepared a live demo for",
    "start": "1564039",
    "end": "1570240"
  },
  {
    "text": "today where we are going to simulate a full cluster upgrade in order to see how",
    "start": "1571840",
    "end": "1579679"
  },
  {
    "text": "Cas shadder can help us clean up uh the running pods from a park node we have a",
    "start": "1579679",
    "end": "1588440"
  },
  {
    "text": "cluster uh running with one control plane and two worker nodes and we are",
    "start": "1588440",
    "end": "1593600"
  },
  {
    "text": "going to park one of these uh worker nodes in the upper left uh terminal we",
    "start": "1593600",
    "end": "1600919"
  },
  {
    "text": "are going to uh start labeling and cordoning a worker",
    "start": "1600919",
    "end": "1607679"
  },
  {
    "text": "node and on the upper right terminal we are going to watch the pods that are",
    "start": "1607679",
    "end": "1613840"
  },
  {
    "text": "running on the Node we are going to park using Cub C get pods and the watch",
    "start": "1613840",
    "end": "1619159"
  },
  {
    "text": "command and on the bottom terminal we are going to watch the case FEA log so",
    "start": "1619159",
    "end": "1624960"
  },
  {
    "text": "that we can get a feel about what is doing behind the scene let me restart the K Shredder so",
    "start": "1624960",
    "end": "1633919"
  },
  {
    "text": "that we can have some clear logs okay K Shredder started let's see",
    "start": "1633919",
    "end": "1642120"
  },
  {
    "text": "the running pods yeah so we have a bunch of PODS running on the Node we are going to park pods from different a",
    "start": "1642120",
    "end": "1649600"
  },
  {
    "text": "spaces coming from a stateful set pods with bad pdbs pod with allow eviction",
    "start": "1649600",
    "end": "1655760"
  },
  {
    "text": "that allow eviction po that doesn't allow eviction and so",
    "start": "1655760",
    "end": "1660960"
  },
  {
    "text": "on let's park this worker",
    "start": "1660960",
    "end": "1666360"
  },
  {
    "text": "nde we added a TTL for of just one minute so that we can see a fast",
    "start": "1666360",
    "end": "1672559"
  },
  {
    "text": "iteration of what shedder is doing behind the scene as we can see sh",
    "start": "1672559",
    "end": "1677840"
  },
  {
    "text": "already reacted and noticed that there is a park node in the cluster and started the eviction Loops for all the",
    "start": "1677840",
    "end": "1684919"
  },
  {
    "text": "running pods and as you notice many pods were successfully soft evicted by the",
    "start": "1684919",
    "end": "1691279"
  },
  {
    "text": "shredder during the first iterations but some of them won't be able to but still",
    "start": "1691279",
    "end": "1696840"
  },
  {
    "text": "Shredder will periodically try to soft AIC them until the TTL of the node will",
    "start": "1696840",
    "end": "1703279"
  },
  {
    "text": "expire these pods couldn't be soft evicted because they have bad pdbs configured or because the tenant",
    "start": "1703279",
    "end": "1708720"
  },
  {
    "text": "explicitly disallowed eviction and after 1 minute we should",
    "start": "1708720",
    "end": "1715480"
  },
  {
    "text": "see yeah uh K Shredder is also able to perform roll out restart for a",
    "start": "1715480",
    "end": "1722360"
  },
  {
    "text": "deployment or the stateful set that are that are behind the running pods and once the TTL expire on this",
    "start": "1722360",
    "end": "1731080"
  },
  {
    "text": "park note we will see that Cas rer is taking that aggressive U action and will",
    "start": "1731080",
    "end": "1736600"
  },
  {
    "text": "just force evict all the running",
    "start": "1736600",
    "end": "1741158"
  },
  {
    "text": "pods from the uh Park node and with that we can see that we don't have any",
    "start": "1741720",
    "end": "1747760"
  },
  {
    "text": "running pods on the park node and cluster autoscaler can Chim in and",
    "start": "1747760",
    "end": "1752960"
  },
  {
    "text": "safely recycle the the worker node okay this was the",
    "start": "1752960",
    "end": "1760398"
  },
  {
    "text": "demo and with that I'm going to pass it back to Victor for the",
    "start": "1763519",
    "end": "1768919"
  },
  {
    "text": "conclusions thank you Adrian uh very good demo and this time didn't",
    "start": "1768919",
    "end": "1774240"
  },
  {
    "text": "fail um let's wrap up our um few takeaways of",
    "start": "1774240",
    "end": "1780200"
  },
  {
    "text": "Five Years Journey of running kubernetes in a multi-tenant architecture uh there is no Silver",
    "start": "1780200",
    "end": "1785519"
  },
  {
    "text": "Bullet while building a multi-tenant developer platform you should always",
    "start": "1785519",
    "end": "1790799"
  },
  {
    "text": "align with your product development teams in this process every company is",
    "start": "1790799",
    "end": "1796320"
  },
  {
    "text": "different and it has its own needs and vision regarding to the multi- teny architecture and here kubernetes name",
    "start": "1796320",
    "end": "1803720"
  },
  {
    "text": "spaces um are um feasible to to to build the",
    "start": "1803720",
    "end": "1811320"
  },
  {
    "text": "boundaries um and the last uh thing and also the not the um yeah last but not",
    "start": "1811840",
    "end": "1819000"
  },
  {
    "text": "the least uh challenges while working at scale are um different comparing with um",
    "start": "1819000",
    "end": "1825480"
  },
  {
    "text": "small or medium size platform thank you foration uh I think",
    "start": "1825480",
    "end": "1832240"
  },
  {
    "text": "we have time for questions uh anyway we will be also available for the next 10",
    "start": "1832240",
    "end": "1837399"
  },
  {
    "text": "to 15 minutes for offline questions if you have also please scan this uh QR",
    "start": "1837399",
    "end": "1842480"
  },
  {
    "text": "code so we can provide us some feedback um thank you and yeah you have any",
    "start": "1842480",
    "end": "1848919"
  },
  {
    "text": "questions I have one question on the charge bag model uh you have various teams using uh the shared uh kubernetes",
    "start": "1849000",
    "end": "1856360"
  },
  {
    "text": "clusters right so how does a chargeback model uh to these departments work like the Photoshop team or em team or various",
    "start": "1856360",
    "end": "1863799"
  },
  {
    "text": "yeah uh do you have anything in Pro U process or the second question is how do",
    "start": "1863799",
    "end": "1869120"
  },
  {
    "text": "you focus on optimization some teams just over provision right like don't set",
    "start": "1869120",
    "end": "1874320"
  },
  {
    "text": "the right uh right resource limits how do you how do you optimize that okay yeah so so for for the first question",
    "start": "1874320",
    "end": "1881200"
  },
  {
    "text": "how are you charging back uh our users right um where's a solution called Cube",
    "start": "1881200",
    "end": "1888440"
  },
  {
    "text": "cost for this and with some algorithms we provide them what is the actually",
    "start": "1888440",
    "end": "1893960"
  },
  {
    "text": "cost for running um their application in our namespace or in our clusters because",
    "start": "1893960",
    "end": "1899440"
  },
  {
    "text": "we have also uh users that use dedicated clusters they they basically use the",
    "start": "1899440",
    "end": "1905279"
  },
  {
    "text": "entire cluster and also please if you have and also we are adding labels on",
    "start": "1905279",
    "end": "1911200"
  },
  {
    "text": "the name spaces like service ID the team that it's running inside that namespace and",
    "start": "1911200",
    "end": "1916880"
  },
  {
    "text": "we can can easily correlate the pods running in that name space with a specific team and so that we can easily",
    "start": "1916880",
    "end": "1923519"
  },
  {
    "text": "charge back them yeah uh thank you uh we we tried that uh so more going to I I'll",
    "start": "1923519",
    "end": "1930279"
  },
  {
    "text": "definitely explore that uh the second uh question around the optimization uh the resource limits",
    "start": "1930279",
    "end": "1935960"
  },
  {
    "text": "mhm yeah so for the resource limits as I shown in the presentation we have um a",
    "start": "1935960",
    "end": "1941600"
  },
  {
    "text": "project called automatic resource configuration it's an internal one but we are thinking to to poor",
    "start": "1941600",
    "end": "1947799"
  },
  {
    "text": "so basically based on the pritus metrics you calculate how um what is the right size of the CPU and memory request that",
    "start": "1947799",
    "end": "1954200"
  },
  {
    "text": "the P should have then we label the deployments then an Opa policy will hook when the PS are created on the cluster",
    "start": "1954200",
    "end": "1961159"
  },
  {
    "text": "and just at that moment we um optimize the right CPU request and memory request",
    "start": "1961159",
    "end": "1968000"
  },
  {
    "text": "for that P yeah one mention to your question is that we are not changing the resource limits only the resource",
    "start": "1968000",
    "end": "1974320"
  },
  {
    "text": "request because the scheduling is done based on the resource request not on the limit that's why we can overcommit uh on",
    "start": "1974320",
    "end": "1982039"
  },
  {
    "text": "worker nodes hi um I had two questions also um",
    "start": "1982039",
    "end": "1988840"
  },
  {
    "text": "one question is you showed the sort of the namespace profile and all the Nam spaced objects that get set to control",
    "start": "1988840",
    "end": "1995440"
  },
  {
    "text": "things like the quota and the limit range and Ro bindings um I'm I'm assuming the developer teams don't set",
    "start": "1995440",
    "end": "2001919"
  },
  {
    "text": "those because they kind of constrain what the team can do so how do those get there in the first place and how do they",
    "start": "2001919",
    "end": "2008120"
  },
  {
    "text": "get updated when there's like a new standard for that mhm yeah very good",
    "start": "2008120",
    "end": "2013200"
  },
  {
    "text": "question so actually we have an automation that five years ago it was a script then we provision name spaces",
    "start": "2013200",
    "end": "2019760"
  },
  {
    "text": "using jaria tickets you know and then we add it into an automation an API and uh",
    "start": "2019760",
    "end": "2025399"
  },
  {
    "text": "those profiles are static uh are controlled by us and they are deployed",
    "start": "2025399",
    "end": "2031000"
  },
  {
    "text": "on the cluster by the by the user so it's a self service mechanism to deploy the",
    "start": "2031000",
    "end": "2037480"
  },
  {
    "text": "the namespace profile on the cluster but they don't control the profile actually",
    "start": "2037480",
    "end": "2042919"
  },
  {
    "text": "we uh we are controlling the Prof and depending on what we are changing on the profile even us update the profile on",
    "start": "2042919",
    "end": "2050358"
  },
  {
    "text": "the cluster but if it's something that can impact an application like Network policies uh we delegate to to the end",
    "start": "2050359",
    "end": "2057839"
  },
  {
    "text": "user to to do the update yeah and one addition here is that tenants are not",
    "start": "2057839",
    "end": "2063040"
  },
  {
    "text": "directly talking with the API server when they want to create npace they are talking with our itos kubernetes on",
    "start": "2063040",
    "end": "2070480"
  },
  {
    "text": "Border it's an the application and that application is talking with the API",
    "start": "2070480",
    "end": "2076000"
  },
  {
    "text": "server when creating a new namespace and that tool is also adding this namespace",
    "start": "2076000",
    "end": "2082118"
  },
  {
    "text": "profile all the network policies and all those stuffs y okay and the other question is",
    "start": "2082119",
    "end": "2089240"
  },
  {
    "text": "um you know you mentioned cluster API at the beginning but where does cluster API fit into this whole",
    "start": "2089240",
    "end": "2095520"
  },
  {
    "text": "picture uh it doesn't fit in our presentation but uh I just mentioned that it's a milestone that we should",
    "start": "2095520",
    "end": "2101800"
  },
  {
    "text": "mention that we uh for the infrastructure are actually adopted the",
    "start": "2101800",
    "end": "2106880"
  },
  {
    "text": "cluster cluster API and also Argo to building and managing kubernetes",
    "start": "2106880",
    "end": "2112079"
  },
  {
    "text": "clusters yeah it's just a mention to is is one of the Argos in that two-tier picture a part of that infrastructure",
    "start": "2112079",
    "end": "2118480"
  },
  {
    "text": "piece yeah okay thanks uh",
    "start": "2118480",
    "end": "2124480"
  },
  {
    "text": "yeah yeah we can take offline the yes question because we finished the the time thank you very much",
    "start": "2124480",
    "end": "2132599"
  },
  {
    "text": "again",
    "start": "2132599",
    "end": "2135599"
  }
]