[
  {
    "start": "0",
    "end": "51000"
  },
  {
    "text": "[Music] hello I'm your track host for tonight this afternoon booyah from Giants warm and the next",
    "start": "0",
    "end": "7830"
  },
  {
    "text": "talk is gonna be about scaling service endpoints it's gonna be by were checked",
    "start": "7830",
    "end": "12960"
  },
  {
    "text": "in ski right and min hansha from Google thank you thank you for introduction so",
    "start": "12960",
    "end": "24529"
  },
  {
    "text": "just to add to this like Meehan is working focusing mostly on networking I'm mostly focusing on performance",
    "start": "24529",
    "end": "31260"
  },
  {
    "text": "scalability and the effort that we are going to describe today is a joint",
    "start": "31260",
    "end": "37320"
  },
  {
    "text": "effort between those those two special interest group and those two teams but",
    "start": "37320",
    "end": "42540"
  },
  {
    "text": "before we describe the effort itself and what we are doing like let's try to understand what the problem actually is",
    "start": "42540",
    "end": "48780"
  },
  {
    "text": "and what we are trying to solve so there are basically two main problems that",
    "start": "48780",
    "end": "55140"
  },
  {
    "start": "51000",
    "end": "236000"
  },
  {
    "text": "user are facing currently in this area the first one is that there is a",
    "start": "55140",
    "end": "61530"
  },
  {
    "text": "limitation for the number of endpoints that you can have in a single service I will talk a little bit more about both",
    "start": "61530",
    "end": "68400"
  },
  {
    "text": "of these like in the next slides and the second second problem that users are",
    "start": "68400",
    "end": "73979"
  },
  {
    "text": "observing is that they may observe like significant performance degradation",
    "start": "73979",
    "end": "79880"
  },
  {
    "text": "especially in very large clusters when using large services so to give you an",
    "start": "79880",
    "end": "85830"
  },
  {
    "text": "example changing even a single endpoint in five thousand nodes cluster if a",
    "start": "85830",
    "end": "92790"
  },
  {
    "text": "service has like five thousand endpoints may freeze the control plane for a",
    "start": "92790",
    "end": "98009"
  },
  {
    "text": "couple seconds completely so where are",
    "start": "98009",
    "end": "103200"
  },
  {
    "text": "those coming from what is the reason for those problems the root cause of both of",
    "start": "103200",
    "end": "109110"
  },
  {
    "text": "them is actually lying in the endpoints API the endpoints API is like being has",
    "start": "109110",
    "end": "116369"
  },
  {
    "text": "been designed in very early days of kubernetes when scalability wasn't really a concern like people weren't",
    "start": "116369",
    "end": "122180"
  },
  {
    "text": "thinking much about scalability implications and to be more specific",
    "start": "122180",
    "end": "128550"
  },
  {
    "text": "like the actual reason for although the actions are different in those",
    "start": "128550",
    "end": "133590"
  },
  {
    "text": "two problems that the root cause for both of them is the problem that the",
    "start": "133590",
    "end": "139830"
  },
  {
    "text": "endpoint object contains a list of all endpoints inside of that service so",
    "start": "139830",
    "end": "145260"
  },
  {
    "text": "let's try to understand like where are those coming from and what is causing them so let's say that we have feedback",
    "start": "145260",
    "end": "151500"
  },
  {
    "text": "and pots in the service then as I mentioned in the previous slide the size",
    "start": "151500",
    "end": "156660"
  },
  {
    "text": "of the endpoints object is proportional to that P that means that with roughly",
    "start": "156660",
    "end": "163170"
  },
  {
    "text": "5,000 endpoints it depends on like how long the length of the name of those",
    "start": "163170",
    "end": "169530"
  },
  {
    "text": "pots are and how long is the name of the name span and stuff like that but with roughly 5,000 endpoints we are we are",
    "start": "169530",
    "end": "177000"
  },
  {
    "text": "getting we are getting to the point where endpoints object has like 1.5 megabytes of size and what does it mean",
    "start": "177000",
    "end": "186380"
  },
  {
    "text": "there is also like a limitation and at CD level for the maximum length of the",
    "start": "186380",
    "end": "192090"
  },
  {
    "text": "value of in the object store the net CD and it's exactly one point megabytes",
    "start": "192090",
    "end": "197850"
  },
  {
    "text": "this value is configurable but increasing that actually changes",
    "start": "197850",
    "end": "203280"
  },
  {
    "text": "peripheral performance characteristic of at CD makes it a little bit worse we",
    "start": "203280",
    "end": "209280"
  },
  {
    "text": "it's not something that we really want to do so basically like to summarize",
    "start": "209280",
    "end": "215850"
  },
  {
    "text": "like the dis limitation of for the number of endpoints and a single service is a consequence of a limitation at the",
    "start": "215850",
    "end": "221910"
  },
  {
    "text": "at CD level for the length of the value that we started in in at CD so let's",
    "start": "221910",
    "end": "233069"
  },
  {
    "text": "take a look into the second problem that I mentioned couple minutes ago but before we get there like let's try to",
    "start": "233069",
    "end": "239160"
  },
  {
    "start": "236000",
    "end": "380000"
  },
  {
    "text": "understand like how the service works lo actually looks like so I will focus on",
    "start": "239160",
    "end": "245819"
  },
  {
    "text": "on the case where we add a pod to already existing service but all the",
    "start": "245819",
    "end": "251250"
  },
  {
    "text": "other cases like deleting a photo or anything like that is pretty analogous so so we will you will understand it so",
    "start": "251250",
    "end": "259470"
  },
  {
    "text": "let's say that there is already a service created and we are creating a pod so what's what's happening now",
    "start": "259470",
    "end": "266240"
  },
  {
    "text": "but is being picked by scheduler and scheduled what is happening then is that",
    "start": "266240",
    "end": "272270"
  },
  {
    "text": "cubelet is watching all the parts that are assigned to do it then it's doing",
    "start": "272270",
    "end": "281330"
  },
  {
    "text": "all the necessary work to start the pot and when it started like the the green",
    "start": "281330",
    "end": "287630"
  },
  {
    "text": "the green lines here means that it's within this this dimension it's already",
    "start": "287630",
    "end": "294110"
  },
  {
    "text": "done so this means that here that the pot is already started in the container",
    "start": "294110",
    "end": "299120"
  },
  {
    "text": "run thing the pot is then one should start that cubelet is reporting it as",
    "start": "299120",
    "end": "304370"
  },
  {
    "text": "ready and as around us running via kubernetes api what is happening there",
    "start": "304370",
    "end": "312320"
  },
  {
    "text": "is that endpoint controller is watching all the endpoints in the system watching",
    "start": "312320",
    "end": "318650"
  },
  {
    "text": "all the pots in the system and based on based on the fact that this pot just became ready it's updating the endpoint",
    "start": "318650",
    "end": "326330"
  },
  {
    "text": "object corresponding to our service and all this happening then is that there's",
    "start": "326330",
    "end": "333320"
  },
  {
    "text": "like an agent called keep proxy that is running on every single node that is basically responsible for implementing",
    "start": "333320",
    "end": "340370"
  },
  {
    "text": "as implementing a service concept in kubernetes cluster and what it does it",
    "start": "340370",
    "end": "345409"
  },
  {
    "text": "basically watches all endpoints in the system and based on that information its",
    "start": "345409",
    "end": "350659"
  },
  {
    "text": "programming IP tables or IP vs or whatever other mechanics you are using",
    "start": "350659",
    "end": "356380"
  },
  {
    "text": "and that basically finishes this flow of a being a pot to a service that like",
    "start": "356380",
    "end": "362090"
  },
  {
    "text": "every all the all the lines are green so everything is this program so where is",
    "start": "362090",
    "end": "367370"
  },
  {
    "text": "our problem actually coming from what is causing it so as I mentioned like there is like a cube proxy running on every",
    "start": "367370",
    "end": "373880"
  },
  {
    "text": "single single node and all of those are watching all the endpoints in the system",
    "start": "373880",
    "end": "379479"
  },
  {
    "text": "so what does it mean it means that if there are n nodes there's also end",
    "start": "379479",
    "end": "385099"
  },
  {
    "start": "380000",
    "end": "518000"
  },
  {
    "text": "Watchers watching for every single endpoint in the system in the current implementation of API machinery every",
    "start": "385099",
    "end": "392599"
  },
  {
    "text": "water is treated independently so that means that to to send them up the sender",
    "start": "392599",
    "end": "397610"
  },
  {
    "text": "watch event to to to torture you need to like serialize it",
    "start": "397610",
    "end": "402640"
  },
  {
    "text": "independently for every single water copy it and stuff like that we are also",
    "start": "402640",
    "end": "407740"
  },
  {
    "text": "trying to address that independently and and this is something that is already happening but even if we fix that that",
    "start": "407740",
    "end": "415210"
  },
  {
    "text": "the number the amount of data that we need to wait to send to all the waters like doesn't change so let's focus on",
    "start": "415210",
    "end": "421660"
  },
  {
    "text": "the on the aspect of like how much data we need to transmit tortures so as we",
    "start": "421660",
    "end": "427600"
  },
  {
    "text": "already discussed like the size of the endpoint object is like proportional to number of back-end pods there are in water so the total number",
    "start": "427600",
    "end": "435610"
  },
  {
    "text": "of total bytes transmitted per update is proportional to the duplication of those",
    "start": "435610",
    "end": "441430"
  },
  {
    "text": "two so let's take some example so let's say that we have five thousand nodes the",
    "start": "441430",
    "end": "448060"
  },
  {
    "text": "size of the endpoints object is like one megabytes let's assume that this is like four four or five thousand endpoints in",
    "start": "448060",
    "end": "453760"
  },
  {
    "text": "the service so that means that we basically need to transmit like five",
    "start": "453760",
    "end": "460540"
  },
  {
    "text": "gigabytes of data which is roughly and this is just for a update of a single",
    "start": "460540",
    "end": "467200"
  },
  {
    "text": "pot in that service so whatever we are rolling update we do we are doing",
    "start": "467200",
    "end": "473680"
  },
  {
    "text": "rolling update of a service that means that since we assumed like five thousand",
    "start": "473680",
    "end": "480240"
  },
  {
    "text": "endpoints it means that we will send roughly five thousand updates to every water time times like five gigabytes and",
    "start": "480240",
    "end": "490330"
  },
  {
    "text": "gives us like twenty five terabytes of data to just do running upgrade of the service so I think it's pretty clear now",
    "start": "490330",
    "end": "497320"
  },
  {
    "text": "that like what scale we are talking about and what problems we are facing and it's it's pretty clear that we in",
    "start": "497320",
    "end": "504400"
  },
  {
    "text": "order to get to higher scale which which Lincoln will be talking about now we",
    "start": "504400",
    "end": "510460"
  },
  {
    "text": "need to do something here so yeah so I'm coming often you have no right thank you",
    "start": "510460",
    "end": "516460"
  },
  {
    "text": "Wojtek for laying out the the problem space so let's focus a little bit on the",
    "start": "516460",
    "end": "523419"
  },
  {
    "text": "user side so what that's the user want the user on to run large services with",
    "start": "523420",
    "end": "529420"
  },
  {
    "text": "thousands of endpoints and it were on to run it in the large cluster with thousands of nodes and then",
    "start": "529420",
    "end": "536760"
  },
  {
    "text": "user want to be agile he wants to deploy wrote out their services frequently or they wanted to",
    "start": "536760",
    "end": "543000"
  },
  {
    "text": "canary or whatever whatever use case so that translates to high turn within the",
    "start": "543000",
    "end": "548190"
  },
  {
    "text": "service so all these should work with our extra caveats this should just work",
    "start": "548190",
    "end": "555260"
  },
  {
    "start": "555000",
    "end": "956000"
  },
  {
    "text": "well when we start looking for the solution for the scalability problem",
    "start": "555260",
    "end": "561570"
  },
  {
    "text": "trying to address during the brainstorming we identify several like",
    "start": "561570",
    "end": "566790"
  },
  {
    "text": "directions we can take right first we say what if we just enhance the API",
    "start": "566790",
    "end": "572130"
  },
  {
    "text": "machinery and to optimize for certain cases or we add some kind of dynamic",
    "start": "572130",
    "end": "578640"
  },
  {
    "text": "batching or a limiting mechanism the service endpoint update pipeline and what we quickly realize that these",
    "start": "578640",
    "end": "586410"
  },
  {
    "text": "alternative approach can get complex very quick and it without downsides so",
    "start": "586410",
    "end": "593850"
  },
  {
    "text": "we decide basically tactical changes will only help a little bit",
    "start": "593850",
    "end": "600420"
  },
  {
    "text": "you only push this scalability threshold a little and what we really need is",
    "start": "600420",
    "end": "606480"
  },
  {
    "text": "something fundamentally better that is to redesign the API so our goal of the",
    "start": "606480",
    "end": "613920"
  },
  {
    "text": "redesign is to support tens of thousands of endpoints in a cluster with thousands",
    "start": "613920",
    "end": "620520"
  },
  {
    "text": "of nodes and at the same time we wanted to like enable future extensions in",
    "start": "620520",
    "end": "628830"
  },
  {
    "text": "comparison the current recommended scalability threshold for service back end is like 250 endpoints behind the",
    "start": "628830",
    "end": "637530"
  },
  {
    "text": "service before you get into performance degradation problems and our goal is to",
    "start": "637530",
    "end": "643790"
  },
  {
    "text": "100x that scalability threshold so we",
    "start": "643790",
    "end": "650280"
  },
  {
    "text": "have some a few guiding principles too for this redesign API so scalability has",
    "start": "650280",
    "end": "657720"
  },
  {
    "text": "many dimensions it's mostly trade-offs and we can even drill down to like",
    "start": "657720",
    "end": "665390"
  },
  {
    "text": "deeper into one dimension like for instance CPU you can say we wanted to optimize CPU usage on the master or on",
    "start": "665390",
    "end": "672440"
  },
  {
    "text": "the nodes based on whatever algorithm you use or the pipeline you choose we",
    "start": "672440",
    "end": "678920"
  },
  {
    "text": "can shift the load between master and the nodes so there are many trade-offs we can we can do and but the goal is",
    "start": "678920",
    "end": "686930"
  },
  {
    "text": "that if this system scale grows the number of nodes grows and the number of",
    "start": "686930",
    "end": "693020"
  },
  {
    "text": "services grow the number of endpoints grow all the dimensions should scale",
    "start": "693020",
    "end": "700070"
  },
  {
    "text": "with bounded increase and a counter example is to over optimize on one",
    "start": "700070",
    "end": "707360"
  },
  {
    "text": "dimension so the current endpoints API sort of is the counter example it",
    "start": "707360",
    "end": "714010"
  },
  {
    "text": "basically optimize on one dimension that is the number of objects so each service",
    "start": "714010",
    "end": "720440"
  },
  {
    "text": "only correspond to one endpoint object and the endpoint object has to contain",
    "start": "720440",
    "end": "725780"
  },
  {
    "text": "all the endpoints so the high level idea of the redesign is to slice up the giant",
    "start": "725780",
    "end": "733490"
  },
  {
    "text": "endpoint object into multiple objects call em point slice it's very similar to",
    "start": "733490",
    "end": "739220"
  },
  {
    "text": "like database sharding but with some kubernetes specific flavors so first of",
    "start": "739220",
    "end": "747170"
  },
  {
    "text": "all the end points lies contains a hundred endpoints we're gonna talk about why we pick 100 and it's gonna be",
    "start": "747170",
    "end": "755840"
  },
  {
    "text": "configurable a vir controller flag and a new controller endpoint slice controller will be",
    "start": "755840",
    "end": "762350"
  },
  {
    "text": "implemented to automatically manage the life cycles of the endpoint slice object",
    "start": "762350",
    "end": "767950"
  },
  {
    "text": "since one service correspond to multiple endpoints wise object so the naming",
    "start": "767950",
    "end": "774380"
  },
  {
    "text": "scheme will we will not reuse the existing endpoints naming scheme the",
    "start": "774380",
    "end": "779570"
  },
  {
    "text": "existing endpoints just share the same name with service and instead the",
    "start": "779570",
    "end": "784940"
  },
  {
    "text": "endpoint slice will use a generator naming scheme similar to the parts",
    "start": "784940",
    "end": "790280"
  },
  {
    "text": "behind replica set and and also he maintained endpoint slice",
    "start": "790280",
    "end": "797220"
  },
  {
    "text": "mapping using a label on each endpoint slice object with the key pointing to a",
    "start": "797220",
    "end": "804779"
  },
  {
    "text": "service and yet the value pointing to a service so over the endpoint update in",
    "start": "804779",
    "end": "811589"
  },
  {
    "text": "this new API so let's say obviously all endpoint slice has to be sent to all",
    "start": "811589",
    "end": "817529"
  },
  {
    "text": "queue proxies at first if there's no update nothing happens let's say there's",
    "start": "817529",
    "end": "822899"
  },
  {
    "text": "one update in one endpoint update in the service so only one endpoint slice will",
    "start": "822899",
    "end": "830790"
  },
  {
    "text": "be updated and that slice will be distributed to all the queue proxies and",
    "start": "830790",
    "end": "838199"
  },
  {
    "text": "there are more details so this is the future extension we want to enable in",
    "start": "838199",
    "end": "844139"
  },
  {
    "text": "the API is that each endpoint can actually contain multiple IPS so in the",
    "start": "844139",
    "end": "850170"
  },
  {
    "text": "old endpoint API each endpoint can only hold one IP that correspond to the part",
    "start": "850170",
    "end": "855449"
  },
  {
    "text": "I P so as part of the ipv6 effort part IPS are getting flaw rised so each part",
    "start": "855449",
    "end": "863009"
  },
  {
    "text": "can have multiple IPS and endpoint slice api can express that so that gives us",
    "start": "863009",
    "end": "868860"
  },
  {
    "text": "duo stat support on day one and the",
    "start": "868860",
    "end": "874050"
  },
  {
    "text": "second point is that each endpoint have extensible status so the current",
    "start": "874050",
    "end": "880740"
  },
  {
    "text": "endpoints API only has a binary status per endpoint that is ready or not ready so as the queue proxy at the ready are",
    "start": "880740",
    "end": "890130"
  },
  {
    "text": "not ready correspond to the pot readiness so based on the current API",
    "start": "890130",
    "end": "895439"
  },
  {
    "text": "definition it's very hard to extend beyond this binary status so in the new",
    "start": "895439",
    "end": "902850"
  },
  {
    "text": "API we treat it as a struct and make it",
    "start": "902850",
    "end": "908459"
  },
  {
    "text": "more easy to extend extend the endpoint status so as the consumer of the",
    "start": "908459",
    "end": "914910"
  },
  {
    "text": "endpoint API endpoint slice API which is particularly queue proxy as the queue process it gets more sophisticated we",
    "start": "914910",
    "end": "922050"
  },
  {
    "text": "can feed the queue proxy with more reach information like for instance connection",
    "start": "922050",
    "end": "928070"
  },
  {
    "text": "drainings day or something else that the queue proxy can handle and at the same",
    "start": "928070",
    "end": "933560"
  },
  {
    "text": "time this ap this status extensible status structure can also maintain the",
    "start": "933560",
    "end": "940850"
  },
  {
    "text": "backward compatibility so let's say a coder IP table based queue proxy only",
    "start": "940850",
    "end": "946190"
  },
  {
    "text": "handles ready then you just need to care about readiness right you don't need to care about whether it's like connection",
    "start": "946190",
    "end": "952520"
  },
  {
    "text": "draining or not all right let me hand back to avoid tack so now let's talk",
    "start": "952520",
    "end": "960350"
  },
  {
    "start": "956000",
    "end": "1416000"
  },
  {
    "text": "like how we were or how we are evaluating the design to ensure that it will really meet our expectations so",
    "start": "960350",
    "end": "967540"
  },
  {
    "text": "first like let's talk about what scenarios we are we are talking about so the first one is update of a single",
    "start": "967540",
    "end": "974660"
  },
  {
    "text": "endpoint in the service draw link upgrade of the whole service all endpoints in the service and creation or",
    "start": "974660",
    "end": "980930"
  },
  {
    "text": "deletion of a service they are pretty much the same assuming that all the pods underneath still ik or already exist so",
    "start": "980930",
    "end": "988720"
  },
  {
    "text": "those are the scenarios what about like metrics that we are we are talking about so there are basically two groups of",
    "start": "988720",
    "end": "994910"
  },
  {
    "text": "metrics that we are focusing on the first one is originating in HCV and the",
    "start": "994910",
    "end": "1001030"
  },
  {
    "text": "specific metrics we are looking at is like number of mutating API calls so number of writes in that CD in our case",
    "start": "1001030",
    "end": "1007000"
  },
  {
    "text": "it's like basically number of endpoint status an endpoint slice or endpoints in",
    "start": "1007000",
    "end": "1013210"
  },
  {
    "text": "the current implementation update and the second thing is like size of the",
    "start": "1013210",
    "end": "1019570"
  },
  {
    "text": "individual object the second group of metrics oriented around watch we already",
    "start": "1019570",
    "end": "1025360"
  },
  {
    "text": "seen why what is so crucial and the specific metrics are a number of watch events per water a number of total",
    "start": "1025360",
    "end": "1033160"
  },
  {
    "text": "number of watch events across all Watchers and total number of data that we need to transmit via via watches the",
    "start": "1033160",
    "end": "1040660"
  },
  {
    "text": "the last two can be actually computed on the first free but like we are showing",
    "start": "1040660",
    "end": "1045819"
  },
  {
    "text": "them just just to show what scaling we are talking about and like they are",
    "start": "1045820",
    "end": "1050920"
  },
  {
    "text": "they're really open eye opening to see like what what numbers we are talking about",
    "start": "1050920",
    "end": "1056110"
  },
  {
    "text": "so as an evaluation case let's let's take a look into some large example that we would like to support",
    "start": "1056110",
    "end": "1062380"
  },
  {
    "text": "like the case is like twenty thousand endpoints and five thousand node cluster and let's say that B is the number of",
    "start": "1062380",
    "end": "1071350"
  },
  {
    "text": "end points per endpoint slice that end point slice that I was talking about the",
    "start": "1071350",
    "end": "1076720"
  },
  {
    "text": "new object that we are going to introduce and the scenarios that we are",
    "start": "1076720",
    "end": "1083380"
  },
  {
    "text": "going to evaluate is the current current design with singular single endpoints object the scenario with like 100",
    "start": "1083380",
    "end": "1091210"
  },
  {
    "text": "endpoints per endpoint slice and the scenario with like one endpoint per",
    "start": "1091210",
    "end": "1096760"
  },
  {
    "text": "endpoint slice so in the first scenario like of update of a single update update",
    "start": "1096760",
    "end": "1103870"
  },
  {
    "text": "of a single endpoint the number of write actually doesn't change there's there's",
    "start": "1103870",
    "end": "1109240"
  },
  {
    "text": "always one right or roughly one right but the size of the object obviously",
    "start": "1109240",
    "end": "1114309"
  },
  {
    "text": "changes significantly currently it's it would be roughly like two megabytes with hundred ten points per endpoint slice it",
    "start": "1114309",
    "end": "1121270"
  },
  {
    "text": "would be like 10 kilobytes and here it will be like less than kilobyte here",
    "start": "1121270",
    "end": "1127870"
  },
  {
    "text": "doesn't nothing like really change it but as a consequence of like much smaller object with endpoint slides then",
    "start": "1127870",
    "end": "1135789"
  },
  {
    "text": "total number of data that needs to be transmitted like changes from 10 geeks to via 50 max to like less than 5 mix so",
    "start": "1135789",
    "end": "1145210"
  },
  {
    "text": "that's a single endpoint update what about like running upgrade over over hard service here the number of writes",
    "start": "1145210",
    "end": "1151750"
  },
  {
    "text": "changes significantly because we need to do roughly one write per per event so we",
    "start": "1151750",
    "end": "1160720"
  },
  {
    "text": "get like 20,000 writes basically size of the API object changes the same",
    "start": "1160720",
    "end": "1169330"
  },
  {
    "text": "way as in the previous case obviously so as a result what we get is that the",
    "start": "1169330",
    "end": "1176830"
  },
  {
    "text": "total number of bytes transmitted is changing from like 200 terabytes via one",
    "start": "1176830",
    "end": "1185980"
  },
  {
    "text": "terabyte to like less than 100 gigabytes it's a bit funny that we are talking",
    "start": "1185980",
    "end": "1191830"
  },
  {
    "text": "about less than 100 gigabytes we are not really caring whether it's like 15 gigabytes or like 70 mega or gigabytes",
    "start": "1191830",
    "end": "1199500"
  },
  {
    "text": "but it's it's only shells like the scale we are talking about here we only care about like order of magnitudes of like",
    "start": "1199500",
    "end": "1206580"
  },
  {
    "text": "hundreds of gigabytes really here so the first scenario that which is like",
    "start": "1206580",
    "end": "1214640"
  },
  {
    "text": "service creation that I mentioned before is slightly different than the previous to the number of writes in the current",
    "start": "1214640",
    "end": "1222960"
  },
  {
    "text": "implementation this will be roughly one but with the endpoints light object it",
    "start": "1222960",
    "end": "1228720"
  },
  {
    "text": "will be significantly more because we need to on service creation we need to write all the audio all the necessary",
    "start": "1228720",
    "end": "1235110"
  },
  {
    "text": "and point slice objects at once so we've single our endpoint object it",
    "start": "1235110",
    "end": "1240990"
  },
  {
    "text": "will be like 20k right with hundred and points per slice it will be like 200",
    "start": "1240990",
    "end": "1250130"
  },
  {
    "text": "size of the object changes the same way as previously so as a consequence of",
    "start": "1250190",
    "end": "1256200"
  },
  {
    "text": "those two and the fact that number of Waters doesn't really change in any of these scenarios the total number of",
    "start": "1256200",
    "end": "1262230"
  },
  {
    "text": "watch events changes from like five thousand to via me Liam to like ten",
    "start": "1262230",
    "end": "1267890"
  },
  {
    "text": "hundred millions but the total number of data that needs to be transmitted remains roughly the same across all",
    "start": "1267890",
    "end": "1275040"
  },
  {
    "text": "those like three cases so we are just really saying that or recommending to",
    "start": "1275040",
    "end": "1283440"
  },
  {
    "text": "create a service first before you create bots in that so that's that scenario",
    "start": "1283440",
    "end": "1290870"
  },
  {
    "text": "seems to be like the least important one so in any in the previous one as I felt",
    "start": "1290870",
    "end": "1297360"
  },
  {
    "text": "like the smaller the endpoint slice object is the better it looks like so why we are actually proceeding with like",
    "start": "1297360",
    "end": "1304710"
  },
  {
    "text": "endpoints slice slice Fink so there are a couple of reasons for that the first one is that for small services",
    "start": "1304710",
    "end": "1311760"
  },
  {
    "text": "let's say smaller than 100 endpoints there is no conceptual change for users",
    "start": "1311760",
    "end": "1317790"
  },
  {
    "text": "so they will there would still be like one endpoint slice objects so there is still this correspondence of one object",
    "start": "1317790",
    "end": "1324390"
  },
  {
    "text": "to and point slice object to a service which means that users don't really need",
    "start": "1324390",
    "end": "1329409"
  },
  {
    "text": "to think about or don't you don't want really see that that the change here the",
    "start": "1329409",
    "end": "1335559"
  },
  {
    "text": "second thing is that with singular and point object like there will now be there will there won't be a way to like",
    "start": "1335559",
    "end": "1342700"
  },
  {
    "text": "batch the updates if many things are changing at the same time which is also a significant performance improvement",
    "start": "1342700",
    "end": "1349710"
  },
  {
    "text": "and the third one is like the last scenario that I that I mentioned which is like creation of a service were we",
    "start": "1349710",
    "end": "1356169"
  },
  {
    "text": "have much higher amplification of mutating API calls which is one of the most important limiting factors for",
    "start": "1356169",
    "end": "1363429"
  },
  {
    "text": "scalability of the system so one thing that I would like to mention here is the",
    "start": "1363429",
    "end": "1370870"
  },
  {
    "text": "API here is very very generic so if at some point like Cal ability",
    "start": "1370870",
    "end": "1377350"
  },
  {
    "text": "characteristic uber Nettie's will change and it will be more efficient to arm more optimal - sweet - to keep all the",
    "start": "1377350",
    "end": "1386289"
  },
  {
    "text": "end points in a single object or just have a singular and point object we can actually change the logic of the",
    "start": "1386289",
    "end": "1392169"
  },
  {
    "text": "controller without any API changes which is like very very powerful or and very",
    "start": "1392169",
    "end": "1397559"
  },
  {
    "text": "important argument for this API yeah",
    "start": "1397559",
    "end": "1403149"
  },
  {
    "text": "that's not all so with that I'm handing over back to between how so so with all",
    "start": "1403149",
    "end": "1409029"
  },
  {
    "text": "that is that the best we can do so what else can we achieve with the this",
    "start": "1409029",
    "end": "1414190"
  },
  {
    "text": "endpoint slice API so like with the current proposed solution the",
    "start": "1414190",
    "end": "1422919"
  },
  {
    "text": "incremental updates per endpoint becomes much more efficient so it's efficiently",
    "start": "1422919",
    "end": "1429340"
  },
  {
    "text": "calculated stored and transmitted to all the nodes but the bottom line is that we",
    "start": "1429340",
    "end": "1436570"
  },
  {
    "text": "are still sending all the endpoints to all the nodes and each node will still",
    "start": "1436570",
    "end": "1443350"
  },
  {
    "text": "have a global view of the service endpoints so this will cause the",
    "start": "1443350",
    "end": "1449200"
  },
  {
    "text": "scalability problem like on the next step so how can we avoid sending all the",
    "start": "1449200",
    "end": "1455500"
  },
  {
    "text": "endpoints to all the nodes so here we have next step of the evolution which is the",
    "start": "1455500",
    "end": "1461799"
  },
  {
    "text": "dynamic m point subset so by subset by only sending a subset of the endpoints",
    "start": "1461799",
    "end": "1468370"
  },
  {
    "text": "to a group of nodes we effectively reduce the per no overhead and also",
    "start": "1468370",
    "end": "1474429"
  },
  {
    "text": "further reduce the transmission overhead per M point updates and the new",
    "start": "1474429",
    "end": "1480850"
  },
  {
    "text": "endpoints API is fully capable of expressing this schema but however since",
    "start": "1480850",
    "end": "1489340"
  },
  {
    "text": "the goal is to not to enable like a million M point behind the service the",
    "start": "1489340",
    "end": "1494559"
  },
  {
    "text": "goal is to enable tens of thousands of the endpoints behind the service so it's",
    "start": "1494559",
    "end": "1499750"
  },
  {
    "text": "not urgent to to implement this kind of like mechanism and and it soon we soon",
    "start": "1499750",
    "end": "1508270"
  },
  {
    "text": "realize that it's actually very hard to implement this so the key of the problem",
    "start": "1508270",
    "end": "1514480"
  },
  {
    "text": "is that we do not know the client source so like today all the endpoints and all",
    "start": "1514480",
    "end": "1519909"
  },
  {
    "text": "the services are distributed to all the nodes and Q proxy a program all the nodes",
    "start": "1519909",
    "end": "1525070"
  },
  {
    "text": "- for traffic to the services so wherever the client is they can reach",
    "start": "1525070",
    "end": "1531789"
  },
  {
    "text": "the back end however if we do subsetting only let's say the clung if the clients",
    "start": "1531789",
    "end": "1539080"
  },
  {
    "text": "are only clustered like in a portion of the nodes then that will cause like the",
    "start": "1539080",
    "end": "1546130"
  },
  {
    "text": "back ends to receive imbalanced traffic and and even for the worst case some",
    "start": "1546130",
    "end": "1552760"
  },
  {
    "text": "backends will not even receive traffic because the the subset is not like their",
    "start": "1552760",
    "end": "1558190"
  },
  {
    "text": "their endpoint is not exist on the certain nodes right so to solve to avoid",
    "start": "1558190",
    "end": "1564220"
  },
  {
    "text": "like this imbalance problem we have to introduce some kind of overlapping so",
    "start": "1564220",
    "end": "1569500"
  },
  {
    "text": "here in the endpoint slice you can see we actually have one endpoint in",
    "start": "1569500",
    "end": "1575190"
  },
  {
    "text": "multiple slice and endpoint in multiple subsets and then there are program on",
    "start": "1575190",
    "end": "1580990"
  },
  {
    "text": "different nodes and this also brings some downside of right amplification",
    "start": "1580990",
    "end": "1587470"
  },
  {
    "text": "that means one endpoint update instead of only updating em points lies object we have to update",
    "start": "1587470",
    "end": "1594370"
  },
  {
    "text": "multiple endpoints light object and it gets very gets like complex very very",
    "start": "1594370",
    "end": "1599590"
  },
  {
    "text": "quickly so at this point the complexity doesn't justify to gain so with this I",
    "start": "1599590",
    "end": "1606040"
  },
  {
    "text": "like not to do it at this point but if the time is right the requirements there",
    "start": "1606040",
    "end": "1612130"
  },
  {
    "text": "the API is ready to like meet the need and go even further alright that",
    "start": "1612130",
    "end": "1622030"
  },
  {
    "start": "1621000",
    "end": "1661000"
  },
  {
    "text": "concludes our presentation so we can take questions now",
    "start": "1622030",
    "end": "1630150"
  },
  {
    "text": "[Applause] just lift your hand if you have a",
    "start": "1634900",
    "end": "1641710"
  },
  {
    "text": "question",
    "start": "1641710",
    "end": "1644130"
  },
  {
    "text": "hello and thanks for your joke first of all my question would be regarding the",
    "start": "1659750",
    "end": "1665460"
  },
  {
    "start": "1661000",
    "end": "1811000"
  },
  {
    "text": "updates from endpoint updates with iptables we're seeing with huge amounts",
    "start": "1665460",
    "end": "1671880"
  },
  {
    "text": "of services and endpoints that are a lot of updates to the IP table rules as also",
    "start": "1671880",
    "end": "1677850"
  },
  {
    "text": "some kind of structural plans to lighten the load on the systems regarding the",
    "start": "1677850",
    "end": "1685110"
  },
  {
    "text": "IPT routes my sister can you repeat the second part of the question I didn't",
    "start": "1685110",
    "end": "1691140"
  },
  {
    "text": "hear the last words other also suggesting suggestions on how to solve",
    "start": "1691140",
    "end": "1698970"
  },
  {
    "text": "the amount of IP table rebuilds that we are getting when you have huge amounts",
    "start": "1698970",
    "end": "1704250"
  },
  {
    "text": "of endpoints so so this is kind of",
    "start": "1704250",
    "end": "1712429"
  },
  {
    "text": "separate problem like we automate it it is a problem but it's also like a",
    "start": "1712429",
    "end": "1717960"
  },
  {
    "text": "slightly different problem we don't want to mix those two together like we can solve them completely separately there",
    "start": "1717960",
    "end": "1725550"
  },
  {
    "text": "are some ideas like what we can do here like me and probably can talk a little bit more about it but like IP vs or",
    "start": "1725550",
    "end": "1731610"
  },
  {
    "text": "other using other things instead of IP tables are something that we are",
    "start": "1731610",
    "end": "1736890"
  },
  {
    "text": "considering ipbs actually is GA already yes support so it",
    "start": "1736890",
    "end": "1743820"
  },
  {
    "text": "may mitigate those we are also looking into other other solutions here I don't",
    "start": "1743820",
    "end": "1751470"
  },
  {
    "text": "think we we might our mind here like what is the desired long term solution",
    "start": "1751470",
    "end": "1759630"
  },
  {
    "text": "unless I'm missing something but I think we didn't try yeah so so the the short answer is the IP",
    "start": "1759630",
    "end": "1767130"
  },
  {
    "text": "tables it's very dependent on the kernel right so some kernel performs better",
    "start": "1767130",
    "end": "1772770"
  },
  {
    "text": "some versions of the netfilter to conform performance worse so from what",
    "start": "1772770",
    "end": "1777870"
  },
  {
    "text": "my impression is that the like a good while performing kernel can handles",
    "start": "1777870",
    "end": "1783660"
  },
  {
    "text": "millions of IP tables rules like easily so it's it still hasn't we haven't like",
    "start": "1783660",
    "end": "1790110"
  },
  {
    "text": "reached the scalability limit of IP tables yeah so if when we get there so there",
    "start": "1790110",
    "end": "1796259"
  },
  {
    "text": "are always other options IPV as a BPF like other implementation",
    "start": "1796259",
    "end": "1801600"
  },
  {
    "text": "of queue proxies",
    "start": "1801600",
    "end": "1804350"
  },
  {
    "text": "I first thank you that was a really nice talk some some of the services we run",
    "start": "1809870",
    "end": "1815630"
  },
  {
    "start": "1811000",
    "end": "1928000"
  },
  {
    "text": "have currently have between like 500 to a thousand endpoints we haven't seen any",
    "start": "1815630",
    "end": "1821600"
  },
  {
    "text": "of the performance limitation yet but probably weren't looking in the right is so I'm definitely going to check that",
    "start": "1821600",
    "end": "1826970"
  },
  {
    "text": "what would be you current recommend it for people reaching this scale to slice",
    "start": "1826970",
    "end": "1832880"
  },
  {
    "text": "things so that they while you are waiting away we are working on making this implementation what can we do to",
    "start": "1832880",
    "end": "1840850"
  },
  {
    "text": "not reach those problems so I think that",
    "start": "1840850",
    "end": "1846350"
  },
  {
    "text": "it also depends like on this but very significantly depends on the large of your cluster so if you have if your",
    "start": "1846350",
    "end": "1853070"
  },
  {
    "text": "clusters say has hundreds of nodes instead of thousands like the problem is",
    "start": "1853070",
    "end": "1858169"
  },
  {
    "text": "much much less visible so one potential mitigation if it's possible would be to",
    "start": "1858169",
    "end": "1865909"
  },
  {
    "text": "like somehow split your clusters and ensure that those large services are",
    "start": "1865909",
    "end": "1871309"
  },
  {
    "text": "running in smaller clusters if the control plane will be provisioned with",
    "start": "1871309",
    "end": "1877010"
  },
  {
    "text": "enough resources then that would like hopefully will be sufficient yeah in",
    "start": "1877010",
    "end": "1885740"
  },
  {
    "text": "terms are some other tricks like storing your endpoints object in a separate ICD",
    "start": "1885740",
    "end": "1891500"
  },
  {
    "text": "so that they have a dedicated Exedy to like which is more scalable and better",
    "start": "1891500",
    "end": "1896649"
  },
  {
    "text": "like monitored yeah your provision and",
    "start": "1896649",
    "end": "1902179"
  },
  {
    "text": "provisioning more API servers that's also one of the mitigation because like",
    "start": "1902179",
    "end": "1907279"
  },
  {
    "text": "one of the limitations like how much data API server has to process so like",
    "start": "1907279",
    "end": "1912620"
  },
  {
    "text": "having more of them means kind of charting the work between those",
    "start": "1912620",
    "end": "1919330"
  },
  {
    "text": "Hey so you basically seem to have shouted the collection of all these end",
    "start": "1926660",
    "end": "1931700"
  },
  {
    "start": "1928000",
    "end": "1954000"
  },
  {
    "text": "points and to end point sets per se and the other problem you're facing is distributing the data itself so why not",
    "start": "1931700",
    "end": "1939950"
  },
  {
    "text": "use something like say multicast or some sort of reflectors that are popular in PGP to actually shut out transmitting",
    "start": "1939950",
    "end": "1946670"
  },
  {
    "text": "this data over to each of the slave nodes like if you could instead of",
    "start": "1946670",
    "end": "1954320"
  },
  {
    "start": "1954000",
    "end": "2241000"
  },
  {
    "text": "retransmitting all that data to each one of the nodes through the API server you have some sort of proxy just to maintain",
    "start": "1954320",
    "end": "1960590"
  },
  {
    "text": "endpoints and you shot that center of the data and maybe use multicast where you can allow that with some sort of",
    "start": "1960590",
    "end": "1967610"
  },
  {
    "text": "thing that might help reduce that problem some sort of dedicated proxy it",
    "start": "1967610",
    "end": "1981530"
  },
  {
    "text": "might mean changes to the API layer but just this distribution problem that you have to solve that bailed so yeah that",
    "start": "1981530",
    "end": "1993170"
  },
  {
    "text": "that's one of the potential mitigations but it sounds more like mitigation right",
    "start": "1993170",
    "end": "1999200"
  },
  {
    "text": "so like it's probably worth trying if someone needs to do it urgently but we",
    "start": "1999200",
    "end": "2005350"
  },
  {
    "text": "wanted to solve it like properly in the API - and also like we were we were",
    "start": "2005350",
    "end": "2014200"
  },
  {
    "text": "starting this effort with hope that we will do things like dynamic sub setting",
    "start": "2014200",
    "end": "2019900"
  },
  {
    "text": "from the early day which will help for other problems like for example how big",
    "start": "2019900",
    "end": "2025390"
  },
  {
    "text": "the IP tables actually are but it appeared that there are a bunch of hard",
    "start": "2025390",
    "end": "2031170"
  },
  {
    "text": "problems that we'd need to be solved that would make this probably this solution like taking 5x more time or",
    "start": "2031170",
    "end": "2038860"
  },
  {
    "text": "something like that and like we decided to let let's not do everything it one step but",
    "start": "2038860",
    "end": "2046020"
  },
  {
    "text": "hi so it seems like the conclusion to draw from your like examples right with",
    "start": "2046740",
    "end": "2058020"
  },
  {
    "text": "I forget what the I forget what the difference between one and the other was but in the end it was that choosing a",
    "start": "2058020",
    "end": "2064929"
  },
  {
    "text": "hundred didn't have optimal bandwidth usage or let's say transit usage but it",
    "start": "2064929",
    "end": "2072040"
  },
  {
    "text": "had better number of events right it had",
    "start": "2072040",
    "end": "2077169"
  },
  {
    "text": "a better number of events than using a single endpoint so what is the metric",
    "start": "2077169",
    "end": "2082780"
  },
  {
    "text": "that is affected by number of events and even when it's 1 million it doesn't it",
    "start": "2082780",
    "end": "2089500"
  },
  {
    "text": "add like a latency problem or is it concurrent enough that like what's the",
    "start": "2089500",
    "end": "2096850"
  },
  {
    "text": "yeah what's my question what what metric gets worse when you have a lot of events",
    "start": "2096850",
    "end": "2103770"
  },
  {
    "text": "so basically there is some overhead per",
    "start": "2103770",
    "end": "2109960"
  },
  {
    "text": "event so in particular and even you have you are sending the kubernetes object",
    "start": "2109960",
    "end": "2116470"
  },
  {
    "text": "with which has like metadata and stuff like that so there is like some overhead per event so the more you send like even",
    "start": "2116470",
    "end": "2123940"
  },
  {
    "text": "though that we are actually saying that like this because here we can see a hundred endpoints per slice it doesn't",
    "start": "2123940",
    "end": "2129160"
  },
  {
    "text": "have optimal transit but in the next scenario it has better total events yes",
    "start": "2129160",
    "end": "2136510"
  },
  {
    "text": "right slightly better but it's still worse than existing endpoints but",
    "start": "2136510",
    "end": "2141940"
  },
  {
    "text": "obviously the bandwidth is intractable with existing endpoints so",
    "start": "2141940",
    "end": "2146700"
  },
  {
    "text": "like let's say because all these controls are very limiting right so like",
    "start": "2151040",
    "end": "2157010"
  },
  {
    "text": "imagine you create a service and then selects a large number of endpoints and your whole like controller because it's",
    "start": "2157010",
    "end": "2163820"
  },
  {
    "text": "running in the controller manager it's gonna share the rate limiting and the whole control manager we're gonna just",
    "start": "2163820",
    "end": "2169040"
  },
  {
    "text": "hose for several minutes to creating these endpoint slices if you only have",
    "start": "2169040",
    "end": "2174920"
  },
  {
    "text": "one endpoint per slice right so so that's something that basically what we",
    "start": "2174920",
    "end": "2180230"
  },
  {
    "text": "lose is that we lose the capability of batching let's say if you're a rolling update and you have like you change 10%",
    "start": "2180230",
    "end": "2187190"
  },
  {
    "text": "of your pods instead of one by one then the controller can actually have some",
    "start": "2187190",
    "end": "2192320"
  },
  {
    "text": "tricks to batch it but if you only put one per slice then yeah there's no way",
    "start": "2192320",
    "end": "2199070"
  },
  {
    "text": "we can optimize on it yeah so there's also like on API server level",
    "start": "2199070",
    "end": "2204590"
  },
  {
    "text": "we are processing like there's at some point we are doing a little bit of",
    "start": "2204590",
    "end": "2211310"
  },
  {
    "text": "processing one of bite serializing events like one even at the time so the more events you have this the more",
    "start": "2211310",
    "end": "2218360"
  },
  {
    "text": "contention you are getting in this small point so there is like there are a bunch",
    "start": "2218360",
    "end": "2223700"
  },
  {
    "text": "of things that we can do there to improve that so but yes we are far from",
    "start": "2223700",
    "end": "2230420"
  },
  {
    "text": "being there also",
    "start": "2230420",
    "end": "2233710"
  },
  {
    "text": "hello silk for the talk a simple and how I can follow up just after the",
    "start": "2240140",
    "end": "2245750"
  },
  {
    "text": "conference with your solution I haven't seen any links to get help or any issues caps with your solution and what you're",
    "start": "2245750",
    "end": "2254900"
  },
  {
    "text": "going to do after so like you described the issue you have some proposals yeah",
    "start": "2254900",
    "end": "2261020"
  },
  {
    "text": "and you say in the buzzer steps what to improve so yeah another month or two just where I can see the information so",
    "start": "2261020",
    "end": "2268700"
  },
  {
    "text": "so what is happening now is that we shared this and discussed with in scalability and networking six there is",
    "start": "2268700",
    "end": "2275090"
  },
  {
    "text": "a dock describing that it's almost in the form of a cap but it wasn't yet has",
    "start": "2275090",
    "end": "2282260"
  },
  {
    "text": "been opened as a formal cabin in the repository but it will happen probably within days or so from now",
    "start": "2282260",
    "end": "2288560"
  },
  {
    "text": "yeah the dogs just easier to come and you don't want it to have a PR and",
    "start": "2288560",
    "end": "2293800"
  },
  {
    "text": "comment on it okay but but in general we will be targeting like 116 for alpha",
    "start": "2293800",
    "end": "2301480"
  },
  {
    "text": "implementation for that",
    "start": "2301480",
    "end": "2304960"
  },
  {
    "text": "if if a service scales up and down during the day you might have many",
    "start": "2311850",
    "end": "2318010"
  },
  {
    "text": "slices that are getting empty and and others words are full is fast is it a",
    "start": "2318010",
    "end": "2324010"
  },
  {
    "text": "problem and second is where is the direction to start doing rebalancing",
    "start": "2324010",
    "end": "2329290"
  },
  {
    "text": "between slices - to rebalance with first numbers yes so there will be rebalancing",
    "start": "2329290",
    "end": "2339670"
  },
  {
    "text": "so like let's say the existing sizes can handle a new endpoint obviously which is",
    "start": "2339670",
    "end": "2344760"
  },
  {
    "text": "the create new slices and then if you like the number of endpoints just shrink and then like the percentage of slice",
    "start": "2344760",
    "end": "2352440"
  },
  {
    "text": "has less than let's say 20% m points then we trigger the rebalancing but like",
    "start": "2352440",
    "end": "2358900"
  },
  {
    "text": "it's really depending on what algorithm we wanted to use this type of algorithm",
    "start": "2358900",
    "end": "2364750"
  },
  {
    "text": "is pretty mature so we can just get it off the top this shelf also like the",
    "start": "2364750",
    "end": "2371410"
  },
  {
    "text": "advantage of like this rebalancing is that we can really do that when there is a bit less work to do other work in the",
    "start": "2371410",
    "end": "2378190"
  },
  {
    "text": "system so we can we can upload it for later if it really need it if we have",
    "start": "2378190",
    "end": "2383650"
  },
  {
    "text": "much mm I love higher priority workers",
    "start": "2383650",
    "end": "2389760"
  },
  {
    "text": "right so we're doing something like we have services which have no selectors",
    "start": "2391260",
    "end": "2396850"
  },
  {
    "text": "that we manually populating the endpoints how it's going to fit in this model I'm not sure so we have a service",
    "start": "2396850",
    "end": "2408040"
  },
  {
    "text": "and service has no selector so we manually populate the end points for the kubernetes service so will it work with",
    "start": "2408040",
    "end": "2415660"
  },
  {
    "text": "this model or not and if so will manually have to manage the slices and",
    "start": "2415660",
    "end": "2422440"
  },
  {
    "text": "so on so it's sorry to clarify like is",
    "start": "2422440",
    "end": "2428680"
  },
  {
    "text": "the question what to do with manually manage services what's going to change",
    "start": "2428680",
    "end": "2434890"
  },
  {
    "text": "yes in this case now there is just one slice i just populate the endpoints",
    "start": "2434890",
    "end": "2441460"
  },
  {
    "text": "right then I have a kubernetes service which has no selector so the service community service itself",
    "start": "2441460",
    "end": "2450670"
  },
  {
    "text": "so so we are not changing the community service Specht with this new api so this",
    "start": "2450670",
    "end": "2457340"
  },
  {
    "text": "is mostly for recognize internals which is q proxy in particular so the other",
    "start": "2457340",
    "end": "2462350"
  },
  {
    "text": "consumers of the existing endpoints object they we're gonna keep running the endpoints controller and whoever is",
    "start": "2462350",
    "end": "2468440"
  },
  {
    "text": "consuming it can keep consuming it but we're gonna apply some kind of limitation on the old API and say hey",
    "start": "2468440",
    "end": "2475100"
  },
  {
    "text": "Pico if you grow beyond this amount of endpoints or you need to use the new API so from the service itself there's no",
    "start": "2475100",
    "end": "2482900"
  },
  {
    "text": "change so there's no user like user like facing change on the server so and",
    "start": "2482900",
    "end": "2488570"
  },
  {
    "text": "that's one of the goals we wanted to achieve in this redesign is that like it",
    "start": "2488570",
    "end": "2493970"
  },
  {
    "text": "should be independent of whether we wanted to like change this or respect and an ideally service v2 the next",
    "start": "2493970",
    "end": "2502280"
  },
  {
    "text": "version of a service API should just work with the new API in the last",
    "start": "2502280",
    "end": "2509900"
  },
  {
    "text": "question otherwise I would ask you to take it to the hallway track I have one",
    "start": "2509900",
    "end": "2520460"
  },
  {
    "start": "2518000",
    "end": "2588000"
  },
  {
    "text": "last question so it mentions you know changes on a service definition well so",
    "start": "2520460",
    "end": "2526580"
  },
  {
    "text": "how you gonna define our determined size",
    "start": "2526580",
    "end": "2532040"
  },
  {
    "text": "of needed slice for endpoint so who decide how to configure all of you",
    "start": "2532040",
    "end": "2537440"
  },
  {
    "text": "because you know some other places so you want to split and point to a",
    "start": "2537440",
    "end": "2543470"
  },
  {
    "text": "different slice so what about sides of slice who will decide and so we will be recommending like probably we will end",
    "start": "2543470",
    "end": "2550670"
  },
  {
    "text": "up with like 100 or something around that in general I wouldn't expect anyone",
    "start": "2550670",
    "end": "2557420"
  },
  {
    "text": "to like play with that unless they have a very good reason and very specific use case and and stuff like that but but",
    "start": "2557420",
    "end": "2566300"
  },
  {
    "text": "yeah I I wouldn't expect like 99.9 percent of users touching it at all",
    "start": "2566300",
    "end": "2573730"
  },
  {
    "text": "okay thank you if you have more questions I",
    "start": "2575300",
    "end": "2581670"
  },
  {
    "text": "guess against Iran will be available in the hallway Thanks",
    "start": "2581670",
    "end": "2589369"
  }
]