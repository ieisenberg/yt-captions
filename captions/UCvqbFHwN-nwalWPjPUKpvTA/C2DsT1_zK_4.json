[
  {
    "text": "hello everyone um we're here to give you an introduction of the test and present to",
    "start": "60",
    "end": "7200"
  },
  {
    "text": "you a real world usage of vitess so I'm here with Arthur who's a software",
    "start": "7200",
    "end": "14759"
  },
  {
    "text": "engineer at GitHub and also a maintainer of the Vitas project and my name is Florent I'm a software",
    "start": "14759",
    "end": "20820"
  },
  {
    "text": "engineer at Planet skill the company behind vitess and I'm also a maintainer of",
    "start": "20820",
    "end": "26100"
  },
  {
    "text": "vitess So today we're going to give you a brief",
    "start": "26100",
    "end": "31199"
  },
  {
    "text": "overview of what is vitess try to explain how does it work why we have the test in the first place",
    "start": "31199",
    "end": "36960"
  },
  {
    "text": "and then Arthur will move on to vitessa GitHub so what they had before vitess",
    "start": "36960",
    "end": "42840"
  },
  {
    "text": "why did they decide to move to the test and how does it look now that they have",
    "start": "42840",
    "end": "47879"
  },
  {
    "text": "fittest instead of the old solution then we'll move on to new and upcoming features and we'll",
    "start": "47879",
    "end": "55079"
  },
  {
    "text": "finish with some question answers and resources for you to learn more about your tests uh so before I get started I",
    "start": "55079",
    "end": "62520"
  },
  {
    "text": "just want to do like a quick survey who knows like who have heard about vitess before in the sense like did you play",
    "start": "62520",
    "end": "68760"
  },
  {
    "text": "with the test did you touch the test in the past just raise your hand okay cool that's a lot more than I",
    "start": "68760",
    "end": "74340"
  },
  {
    "text": "expected I'm happy all right what is v test so v test is a scalable",
    "start": "74340",
    "end": "79680"
  },
  {
    "text": "distributed Cloud native database system built around MySQL the main goal of your test is to be a",
    "start": "79680",
    "end": "86520"
  },
  {
    "text": "seamless replacement for your mystical and it was it is part sorry of the cncf",
    "start": "86520",
    "end": "92700"
  },
  {
    "text": "it is a graduated project I think it reached graduation around 2019 and it originally started as a skating",
    "start": "92700",
    "end": "99659"
  },
  {
    "text": "solution for my SQL at YouTube around 2010. so it was created at YouTube it",
    "start": "99659",
    "end": "105180"
  },
  {
    "text": "grew in YouTube and in 2018 YouTube donated the whole project to this to the",
    "start": "105180",
    "end": "112439"
  },
  {
    "text": "cncf is massively scalable this is uh thanks",
    "start": "112439",
    "end": "117659"
  },
  {
    "text": "to sharding and it is also highly available so we run MySQL in replicated",
    "start": "117659",
    "end": "123060"
  },
  {
    "text": "mode so we have primaries we have replicas which means that whenever you have a node that like a primary node",
    "start": "123060",
    "end": "128340"
  },
  {
    "text": "that fails we can fade over to the replica which is why it is highly available",
    "start": "128340",
    "end": "134239"
  },
  {
    "text": "vtest is compatible with mySQL 5.7 mysql80 it used to be also compatible",
    "start": "134239",
    "end": "139860"
  },
  {
    "text": "with Maria DB but since the two code bases uh diverged myodb in MySQL we prefer to focus only on my SQL",
    "start": "139860",
    "end": "148920"
  },
  {
    "text": "uh Vitesse is used by many large deployments small or large deployments uh over the world",
    "start": "148920",
    "end": "155099"
  },
  {
    "text": "um it's not all the names I know that Activision is also using B test uh just",
    "start": "155099",
    "end": "160500"
  },
  {
    "text": "a quick survey who is using vitess in production here okay cool yeah GitHub and Activision uh",
    "start": "160500",
    "end": "168720"
  },
  {
    "text": "cool all right among all of this we have some",
    "start": "168720",
    "end": "173760"
  },
  {
    "text": "key adopters uh slack who's a hundred percent on the test they wrote a very good blog post which is linked right at",
    "start": "173760",
    "end": "179940"
  },
  {
    "text": "the bottom of the slide uh explaining how they migrated why they migrated Etc GitHub which we'll talk about in a",
    "start": "179940",
    "end": "187019"
  },
  {
    "text": "moment gd.com this is a very important Chinese website like Commerce website",
    "start": "187019",
    "end": "192599"
  },
  {
    "text": "and they have more than 10 000 databases running in production and also Planet scale they have also more than 10 000",
    "start": "192599",
    "end": "199440"
  },
  {
    "text": "clusters of the test running in production",
    "start": "199440",
    "end": "203720"
  },
  {
    "text": "uh so obviously vitess is a open source project that's why it's in the",
    "start": "204599",
    "end": "209760"
  },
  {
    "text": "maintenance track so we have 15 maintainers contributing to the test on",
    "start": "209760",
    "end": "215220"
  },
  {
    "text": "the on very regularly and in 2022 we had more than 200 contributors from which",
    "start": "215220",
    "end": "221580"
  },
  {
    "text": "more than 100 were caught contributors all of those contributors came from 57",
    "start": "221580",
    "end": "226920"
  },
  {
    "text": "different companies and the Cod contributors came from 22 companies",
    "start": "226920",
    "end": "233159"
  },
  {
    "text": "all right before we go into the more technical part of the talk I just want to introduce two key words",
    "start": "233159",
    "end": "239760"
  },
  {
    "text": "um about vitess so the first one is key space a key space is the equivalent of a MySQL logical database so I don't know",
    "start": "239760",
    "end": "246420"
  },
  {
    "text": "you can have a key space user for example where you're going to have like a bunch of table around user user data",
    "start": "246420",
    "end": "251580"
  },
  {
    "text": "Etc and a key space can be composed of one or more shards so A Shard is basically",
    "start": "251580",
    "end": "257459"
  },
  {
    "text": "going to be a subset of all your data inside the key space and a short is composed of a primary and",
    "start": "257459",
    "end": "265320"
  },
  {
    "text": "one or more replicas this is a simple uh graph or diagram of",
    "start": "265320",
    "end": "272940"
  },
  {
    "text": "an architect of the architecture of a v test cluster on the right you can see short one two three n",
    "start": "272940",
    "end": "279360"
  },
  {
    "text": "and these charts are composed like I said before of primary and replicas one",
    "start": "279360",
    "end": "284759"
  },
  {
    "text": "primary for example is composed of the MySQL the instance where you're going to have all the data stored and which is",
    "start": "284759",
    "end": "290880"
  },
  {
    "text": "going to act just like a MySQL d and attached to it as a sidecar we have VT",
    "start": "290880",
    "end": "296040"
  },
  {
    "text": "tablet component and the VG tablet is responsible for managing the entire Mass equality instance",
    "start": "296040",
    "end": "302639"
  },
  {
    "text": "with the tablet also exposes a grpc API which will be used by vtgate which is",
    "start": "302639",
    "end": "308820"
  },
  {
    "text": "the small component in the middle to receive query and instructions",
    "start": "308820",
    "end": "315120"
  },
  {
    "text": "all right so vitigate vitigate this is the most uh user-facing component of the test this is where you're going to send",
    "start": "315120",
    "end": "322259"
  },
  {
    "text": "queries this is where you're going to send instructions uh it exposes an RPC",
    "start": "322259",
    "end": "328520"
  },
  {
    "text": "uh yeah RP grpc API and it uses the SQL protocol to talk with applications",
    "start": "328520",
    "end": "335759"
  },
  {
    "text": "whenever you send the query to your vtask cluster it will go directly to your vtgate and vitigate will parse the",
    "start": "335759",
    "end": "342960"
  },
  {
    "text": "query evaluate the query and then send the query to the correct Shard and correct tablet",
    "start": "342960",
    "end": "349080"
  },
  {
    "text": "in yellow we have the control plane of the cluster so we have this TLD which is the administration tool of the test we",
    "start": "349080",
    "end": "355380"
  },
  {
    "text": "also have vtorc which will allow you to repair any fader in your cluster and VT",
    "start": "355380",
    "end": "361620"
  },
  {
    "text": "admin which is a front-end UI that allows you to visualize and manage your cluster",
    "start": "361620",
    "end": "367139"
  },
  {
    "text": "finally in red this is the Topo server so those can be like etcd uh used to be",
    "start": "367139",
    "end": "373440"
  },
  {
    "text": "console but now we're deprecating this and it can also be zookeeper so this is",
    "start": "373440",
    "end": "378960"
  },
  {
    "text": "where we're going to store the metadata of the cluster and the configuration all right",
    "start": "378960",
    "end": "385380"
  },
  {
    "text": "so why do you want to choose vitessa like what's the main features of the test compared to uh vanilla MySQL",
    "start": "385380",
    "end": "392880"
  },
  {
    "text": "so we try to be as compatible as possible with my SQL because the goal is to be",
    "start": "392880",
    "end": "398340"
  },
  {
    "text": "like a seamless replacement of MySQL um we have the restarting feature which",
    "start": "398340",
    "end": "403979"
  },
  {
    "text": "allows you to partition your key space into different shards we also have materialization which is the same as the",
    "start": "403979",
    "end": "410880"
  },
  {
    "text": "SQL marginalization but in SQL you have to manually update your view to refresh",
    "start": "410880",
    "end": "416819"
  },
  {
    "text": "your data and vtest will just do that for you for you automatically",
    "start": "416819",
    "end": "421919"
  },
  {
    "text": "we also have close to management so we have a tool that allows you to manage your cluster we have online schema changes which",
    "start": "421919",
    "end": "429120"
  },
  {
    "text": "allows you to do non-blocking and non-blocking schema changes so you can",
    "start": "429120",
    "end": "435180"
  },
  {
    "text": "do like a big Alto table and you won't have any downtime or maybe like a few seconds or maybe like a minute",
    "start": "435180",
    "end": "441419"
  },
  {
    "text": "we have seamless backup recovery operation uh query consolidation which is in the VT tablet whenever you have",
    "start": "441419",
    "end": "449460"
  },
  {
    "text": "the same query multiple times and concurrently will just execute the same",
    "start": "449460",
    "end": "454560"
  },
  {
    "text": "the query only once get the result and return it to all the requests",
    "start": "454560",
    "end": "459660"
  },
  {
    "text": "uh and finally we have automatic fader detection and repair that's thanks to",
    "start": "459660",
    "end": "464759"
  },
  {
    "text": "Vitor I don't know if you've attended the talk uh of Activision yesterday but they talk about them trying to destroy",
    "start": "464759",
    "end": "471240"
  },
  {
    "text": "the cluster Etc and the cluster always being repaired that's thanks to this uh now I'm gonna hand it over to Arthur",
    "start": "471240",
    "end": "478740"
  },
  {
    "text": "to talk about v-tests at GitHub okay hi um so before I'm going to start to talk",
    "start": "478740",
    "end": "484860"
  },
  {
    "text": "about uh GitHub I think we should first look at how",
    "start": "484860",
    "end": "490580"
  },
  {
    "text": "MySQL is being run at GitHub right so at GitHub we have a fairly standard MySQL setup so we have around 80 clusters",
    "start": "490580",
    "end": "497900"
  },
  {
    "text": "running on across like 2000 1200 MySQL instances",
    "start": "497900",
    "end": "503699"
  },
  {
    "text": "those are all bare Metals so these are like bare metal hosts that we are running we have I think like 300",
    "start": "503699",
    "end": "510120"
  },
  {
    "text": "additional hosts that are running on Azure VMS um our MySQL classes are usually grouped",
    "start": "510120",
    "end": "516839"
  },
  {
    "text": "into like feature-based clusters like I don't know actions and checks are like stored in one cluster we store issues in",
    "start": "516839",
    "end": "522899"
  },
  {
    "text": "pull requests and one cluster or shared clusters where like data from different features is stored together and",
    "start": "522899",
    "end": "529080"
  },
  {
    "text": "sometimes like we do joins between tables with different features and stuff like that um",
    "start": "529080",
    "end": "535680"
  },
  {
    "text": "at Peak we have around like five million kilos per second going to the replicas",
    "start": "535680",
    "end": "541620"
  },
  {
    "text": "that we've run and we have around 500 000 cubes per second going to the primaries of those clusters",
    "start": "541620",
    "end": "547680"
  },
  {
    "text": "um so we basically have a very uh read heavy load on our database",
    "start": "547680",
    "end": "552959"
  },
  {
    "text": "customers but it's not true for all of them right some are more bright heavy some are more read heavy but overall it's quite read heavy in total we store",
    "start": "552959",
    "end": "560700"
  },
  {
    "text": "330 terabytes of data across the primaries and that data is then obviously replicated to all the replicas",
    "start": "560700",
    "end": "567060"
  },
  {
    "text": "that we have so um our scaling strategy for MySQL so",
    "start": "567060",
    "end": "574860"
  },
  {
    "text": "far uh consists like of different things that we could do um like when we created new features we",
    "start": "574860",
    "end": "580440"
  },
  {
    "text": "would set up like a completely separate cluster to make sure that like the new like new features don't run into issues",
    "start": "580440",
    "end": "587459"
  },
  {
    "text": "with like on existing clusters where other features might live and there's like no uh yeah like noise and neighbor",
    "start": "587459",
    "end": "593279"
  },
  {
    "text": "issues or anything like that um we also spend a lot of time breaking up existing clusters so I just mentioned",
    "start": "593279",
    "end": "599640"
  },
  {
    "text": "that like issues and pull requests live in their own cluster right but um like I think four years ago I",
    "start": "599640",
    "end": "605519"
  },
  {
    "text": "actually worked on a project where before that user data repository data issue data",
    "start": "605519",
    "end": "612480"
  },
  {
    "text": "pull request data and like a whole bunch of other data was all stored in one cluster and that customer just couldn't",
    "start": "612480",
    "end": "618300"
  },
  {
    "text": "maintain that load anymore so we basically went back and like did like brain surgery and took like",
    "start": "618300",
    "end": "624240"
  },
  {
    "text": "some tables out of the cluster into like a separate cluster to spread the loads um across like different uh database",
    "start": "624240",
    "end": "630600"
  },
  {
    "text": "clusters um another strategy that we employed quite often is like adding more replicas",
    "start": "630600",
    "end": "636600"
  },
  {
    "text": "so we try to send as much read load as we can to replica instances",
    "start": "636600",
    "end": "641899"
  },
  {
    "text": "and if you have a read heavy cluster this is like perfect you just add more replicas and the existing replicas that",
    "start": "641899",
    "end": "647100"
  },
  {
    "text": "you have and become more healthy because they need to serve less load right but often if you have problems with the",
    "start": "647100",
    "end": "653220"
  },
  {
    "text": "primaries the only way to escape like to scale that is by just switching out the machines right and often it's really",
    "start": "653220",
    "end": "660480"
  },
  {
    "text": "hard or like sometimes it's wasteful right like you have one machine size and then you reach",
    "start": "660480",
    "end": "665640"
  },
  {
    "text": "the limits of that machine and then you need to upgrade every machine in the cluster to go to like a bigger machine size and then suddenly you have like",
    "start": "665640",
    "end": "671579"
  },
  {
    "text": "twice the amount of ram as before but you only needed 10 more right so there's like some problems there",
    "start": "671579",
    "end": "677700"
  },
  {
    "text": "um but that was like one strategy that we employed um but then eventually",
    "start": "677700",
    "end": "683700"
  },
  {
    "text": "we also ran into problems with those scaling approaches right like as I mentioned before better Hardware is more",
    "start": "683700",
    "end": "689399"
  },
  {
    "text": "expensive right and if you have to go to like really really big Hardware it's getting really really expensive",
    "start": "689399",
    "end": "694860"
  },
  {
    "text": "um we also ran into like really unbearable schema migration times so",
    "start": "694860",
    "end": "700019"
  },
  {
    "text": "um we have a tool called ghost which is an online schema migration tool for MySQL so we can add indexes and columns",
    "start": "700019",
    "end": "706560"
  },
  {
    "text": "without any downtime right and this works basically like we create a ghost table and then we",
    "start": "706560",
    "end": "713339"
  },
  {
    "text": "write to both tables and we copy the existing data to this goes table so it's all like seamless and zero downtime but",
    "start": "713339",
    "end": "720600"
  },
  {
    "text": "it takes a long time um like we have one cluster we're just like adding a new column to the biggest",
    "start": "720600",
    "end": "726000"
  },
  {
    "text": "table takes like two months or something like that and that's like that's for a developing team that's not great",
    "start": "726000",
    "end": "732000"
  },
  {
    "text": "right they build their feature and then they come to you and they are like yeah we need to run this tiny migration we need this tiny in column here and you're",
    "start": "732000",
    "end": "737519"
  },
  {
    "text": "like yeah cool way to manage for that to finish right [Music] um we also ran into problems",
    "start": "737519",
    "end": "744779"
  },
  {
    "text": "um which I'm calling like buffer pool refreshing so my sequel has a cache that",
    "start": "744779",
    "end": "750000"
  },
  {
    "text": "sits um in front of the disk that you have right so every time like a Creator",
    "start": "750000",
    "end": "755640"
  },
  {
    "text": "comes in and um uh it requests some data right my",
    "start": "755640",
    "end": "760860"
  },
  {
    "text": "secret goals and first looks into the cache is the data there if yes and it takes it from from the cache which is fast because it's in memory",
    "start": "760860",
    "end": "767339"
  },
  {
    "text": "if it's not there it has to go to disk right and load it from disk so the query becomes slower we",
    "start": "767339",
    "end": "774240"
  },
  {
    "text": "like if if the working set for like the date the working set of the cluster uh becomes bigger like the frequently used",
    "start": "774240",
    "end": "780839"
  },
  {
    "text": "working set of the class becomes bigger than the memory you have you start running into like really weird effects",
    "start": "780839",
    "end": "786360"
  },
  {
    "text": "where basically like a Creator comes in you don't have the data you read it from from from disk into cache Next Period",
    "start": "786360",
    "end": "792360"
  },
  {
    "text": "comes in you don't have the data throw whatever you write out to read something new and you cannot really reuse the data",
    "start": "792360",
    "end": "798600"
  },
  {
    "text": "that you're caching and the cache becomes useless and you start like reading enormous amounts of data from",
    "start": "798600",
    "end": "803940"
  },
  {
    "text": "disk over and over again and then we also ran into issues with replication lag so all the changes that",
    "start": "803940",
    "end": "811680"
  },
  {
    "text": "happen on the primary need to be streamed out to the replicas and they need to reapply those changes if you",
    "start": "811680",
    "end": "817200"
  },
  {
    "text": "have like a huge amount of changes happening on the primary those changes also happen on the replicas and the",
    "start": "817200",
    "end": "822839"
  },
  {
    "text": "replicas become basically just applying those changes and then you cannot actually like serve the read load",
    "start": "822839",
    "end": "828600"
  },
  {
    "text": "anymore um and in some cases uh like if if the replication traffic becomes so big",
    "start": "828600",
    "end": "836519"
  },
  {
    "text": "um that the replicas cannot even keep up anymore applying those changes they fall behind and you start serving sale data",
    "start": "836519",
    "end": "844500"
  },
  {
    "text": "um also you cannot scale this by adding more replicas because those replicas",
    "start": "844500",
    "end": "849660"
  },
  {
    "text": "will also be busy with the same data right like it's the same data coming in through the replication screen",
    "start": "849660",
    "end": "856040"
  },
  {
    "text": "so we looked at different solutions um to solve these issues or help us to",
    "start": "856040",
    "end": "861779"
  },
  {
    "text": "work around these issues and eventually we picked the test um like the biggest reason for us is",
    "start": "861779",
    "end": "868680"
  },
  {
    "text": "that essentially if it has still my secret right like we have a huge MySQL setup we have a lot of automation around",
    "start": "868680",
    "end": "875519"
  },
  {
    "text": "MySQL we have a lot of tooling that we built for my SQL all our applications or",
    "start": "875519",
    "end": "880620"
  },
  {
    "text": "like most applications are built with mySQL Engineers know how to write a code that uses MySQL right for us it doesn't",
    "start": "880620",
    "end": "887100"
  },
  {
    "text": "really make a lot of sense to switch to something else where we don't know how to operate it we don't know what the",
    "start": "887100",
    "end": "892320"
  },
  {
    "text": "problems will be and Engineers need to basically learn from scratch how to build things using a different system so",
    "start": "892320",
    "end": "899220"
  },
  {
    "text": "having like a solution for these problems that still is MySQL is great for us",
    "start": "899220",
    "end": "905579"
  },
  {
    "text": "um then another point also is that like the sharding model that we test has really",
    "start": "905579",
    "end": "910980"
  },
  {
    "text": "fits our data model very well like if you think about the data on GitHub most of it is is like scoped by repository",
    "start": "910980",
    "end": "918000"
  },
  {
    "text": "right so for us sharding by repository ID is like the easiest way to scale the",
    "start": "918000",
    "end": "925440"
  },
  {
    "text": "data out right and charge it into like good chunks um and the third point is that the query",
    "start": "925440",
    "end": "931860"
  },
  {
    "text": "compatibility between the test and MySQL is acceptable I'm not going to say it's",
    "start": "931860",
    "end": "937380"
  },
  {
    "text": "perfect because we ran into a lot of issues but over time we work through all of them like with the help of",
    "start": "937380",
    "end": "942839"
  },
  {
    "text": "maintainers like Florence and also on our own and eventually like by providing",
    "start": "942839",
    "end": "948300"
  },
  {
    "text": "fixes for for the test created planner that's how I eventually became a maintainer of v-test as well",
    "start": "948300",
    "end": "954860"
  },
  {
    "text": "so let's take a look at the Timeline um in 2019 and I think actually in 2018",
    "start": "955079",
    "end": "960199"
  },
  {
    "text": "we ran the first experiments using the tests and we started moving things over in",
    "start": "960199",
    "end": "967740"
  },
  {
    "text": "2020 with all the notification data that we have like all the emails and stuff that you get that is actually being like",
    "start": "967740",
    "end": "974100"
  },
  {
    "text": "stored and cleared from AV test cluster and this was a very basic cluster like there's",
    "start": "974100",
    "end": "980880"
  },
  {
    "text": "only two tables in it but a lot of data um in early 2022",
    "start": "980880",
    "end": "987240"
  },
  {
    "text": "uh we shipped a change that moved all actions checks and statuses data over to",
    "start": "987240",
    "end": "993360"
  },
  {
    "text": "the test and in early 2023 up so like just a few months ago we finished the migration of issues and pull request",
    "start": "993360",
    "end": "999480"
  },
  {
    "text": "data to the test cluster so today we run 20 key spaces on vtest",
    "start": "999480",
    "end": "1008660"
  },
  {
    "text": "and you might remember like these 350 terabytes of data that I mentioned before running in MySQL that included",
    "start": "1008660",
    "end": "1015019"
  },
  {
    "text": "everything that is actually running in the test as well so we have like 150 terabytes of data on the primary CV test",
    "start": "1015019",
    "end": "1021620"
  },
  {
    "text": "that's like almost 50 of our data is behind the tests um and we see around like uh 750",
    "start": "1021620",
    "end": "1030260"
  },
  {
    "text": "000 cubes per second across like primaries and replicas in the test at Peak",
    "start": "1030260",
    "end": "1036938"
  },
  {
    "text": "um yeah so basically like I think like a big chunk of our largest features run on the test nowadays",
    "start": "1038900",
    "end": "1045438"
  },
  {
    "text": "so let's take a look at the issues and pull requests um cluster which is like the latest one",
    "start": "1045439",
    "end": "1052340"
  },
  {
    "text": "that we moved to the test uh it runs on six InCharge so like we don't have like a crazy massively Shard setup right now uh like",
    "start": "1052340",
    "end": "1060260"
  },
  {
    "text": "Activision hat we have like a fairly low number of charts usually um so we have 16 primaries",
    "start": "1060260",
    "end": "1067160"
  },
  {
    "text": "an 84 84 replicas in those 16 charts so it's like 64 machines",
    "start": "1067160",
    "end": "1072760"
  },
  {
    "text": "we store around 26 terabytes of data in this cluster and at Peak we have 30 000",
    "start": "1072760",
    "end": "1079760"
  },
  {
    "text": "cubes per second on rep on the primaries and 220 000 views per second on the replica so this is like an example of a",
    "start": "1079760",
    "end": "1086660"
  },
  {
    "text": "very read heavy cluster and now um I have like this huge table",
    "start": "1086660",
    "end": "1092600"
  },
  {
    "text": "here which shows like the effects of moving from my SQL to uh to the test so",
    "start": "1092600",
    "end": "1098960"
  },
  {
    "text": "the old cluster that we have had that served this data had one primary and 100",
    "start": "1098960",
    "end": "1104179"
  },
  {
    "text": "replicas um and you can see here as well like these are",
    "start": "1104179",
    "end": "1110120"
  },
  {
    "text": "pretty beefy machines right like each machine had like 780 68 gigabytes of memory",
    "start": "1110120",
    "end": "1117260"
  },
  {
    "text": "and they're like all running on nvme uh like like at least they they all use",
    "start": "1117260",
    "end": "1122360"
  },
  {
    "text": "ssds like to provide very fast data access um and we were able to move this to only",
    "start": "1122360",
    "end": "1129940"
  },
  {
    "text": "64 hosts right like so we'd reduced the number of hosts that we need to serve this traffic but also reducing the",
    "start": "1129940",
    "end": "1136520"
  },
  {
    "text": "memory like we are now using like cheaper machines to search the same data with less machines than before you can",
    "start": "1136520",
    "end": "1142460"
  },
  {
    "text": "see this here in the discrete rate um like instead of reading like this kind of like goes into this",
    "start": "1142460",
    "end": "1147980"
  },
  {
    "text": "buffer pool refreshing problem instead of reading 11 gigabytes per second across the whole cluster we're now only",
    "start": "1147980",
    "end": "1153919"
  },
  {
    "text": "reading 800 megabytes a second uh from the disk across all these machines",
    "start": "1153919",
    "end": "1159080"
  },
  {
    "text": "um and we also see like an improvement in Retreat on uh the primaries as well",
    "start": "1159080",
    "end": "1164299"
  },
  {
    "text": "just like not just on the replicas I forgot to include the right uh rate",
    "start": "1164299",
    "end": "1169400"
  },
  {
    "text": "but that also um like it didn't drop but now instead of like writing all the data",
    "start": "1169400",
    "end": "1175880"
  },
  {
    "text": "only on the primary on one primary right like we we were able to split the data",
    "start": "1175880",
    "end": "1180980"
  },
  {
    "text": "uh across all these 16 primers that we run now right like each primary is less busy writing data and that also leads to",
    "start": "1180980",
    "end": "1188000"
  },
  {
    "text": "less replication lag because all the replicas only need to read like 160. read and apply 1 16 of the changes that",
    "start": "1188000",
    "end": "1194960"
  },
  {
    "text": "happen across the whole cluster and I have like this online schema change duration Improvement down there",
    "start": "1194960",
    "end": "1201980"
  },
  {
    "text": "so the largest largest table that we have on this cluster is like uh um it's called issue events so basically",
    "start": "1201980",
    "end": "1209120"
  },
  {
    "text": "like you know if you go on an Asia page you see like when someone like adds a label or removes the label you see like",
    "start": "1209120",
    "end": "1214700"
  },
  {
    "text": "it's tiny events there um previously I checked the numbers like",
    "start": "1214700",
    "end": "1219740"
  },
  {
    "text": "the last migration we ran before we moved to the test took three weeks to execute",
    "start": "1219740",
    "end": "1225200"
  },
  {
    "text": "um and I think we even had cases depending on like how much traffic we see where it took longer than that and now it takes two days like it's a huge",
    "start": "1225200",
    "end": "1231919"
  },
  {
    "text": "Improvement for developers if they wanted to changes um on this cluster",
    "start": "1231919",
    "end": "1237140"
  },
  {
    "text": "okay um so our current architecture for the test is",
    "start": "1237140",
    "end": "1243020"
  },
  {
    "text": "basically built on top of our MySQL setup as I mentioned before um we don't have like we don't do what I",
    "start": "1243020",
    "end": "1249919"
  },
  {
    "text": "would call like a best practice we test it up where everything runs in kubernetes and like like automatically",
    "start": "1249919",
    "end": "1255380"
  },
  {
    "text": "scaled and we have tiny shards and all that stuff um we run BT tablets alongside MySQL on",
    "start": "1255380",
    "end": "1261260"
  },
  {
    "text": "bare metal hosts um the only thing that we run in kubernetes is the VT Gates VTC TLD and VT admin",
    "start": "1261260",
    "end": "1268220"
  },
  {
    "text": "part of the test so basically like you can have like a hybrid setup",
    "start": "1268220",
    "end": "1273380"
  },
  {
    "text": "um and we're also currently running on an older version of a test like it's not super old but um I think the latest one",
    "start": "1273380",
    "end": "1279559"
  },
  {
    "text": "is 16 v17 is getting released soon so we're a bit behind",
    "start": "1279559",
    "end": "1284600"
  },
  {
    "text": "in the future I actually would like us to see like or like I'm trying to move us into a model where we upgrade to",
    "start": "1284600",
    "end": "1290240"
  },
  {
    "text": "nearly test versions more often to get the latest fixes and performance improvements and we're also looking into",
    "start": "1290240",
    "end": "1297260"
  },
  {
    "text": "moving potentially like more clusters to the test but it really depends on uh on the load on the clusters right",
    "start": "1297260",
    "end": "1304400"
  },
  {
    "text": "like we're always or at least right now we're shooting for a hybrid solution where we move things to the test where it makes sense and leave things on my",
    "start": "1304400",
    "end": "1311240"
  },
  {
    "text": "SQL where it doesn't um because there is like for us without having like a full kubernetes setup",
    "start": "1311240",
    "end": "1316820"
  },
  {
    "text": "there is an overhead to managing the test and we don't want to pay that overhead if we don't really need to",
    "start": "1316820",
    "end": "1324400"
  },
  {
    "text": "okay so in conclusion um like we test has enabled GitHub to scale my SQL much further than we were",
    "start": "1324860",
    "end": "1331159"
  },
  {
    "text": "able to scale it before and I think if you like if those problems that you've",
    "start": "1331159",
    "end": "1336380"
  },
  {
    "text": "heard about sound familiar I think you definitely should check out the test it's a potential solution",
    "start": "1336380",
    "end": "1341620"
  },
  {
    "text": "if you are curious how we did the setup without downtime and stuff like that and",
    "start": "1341620",
    "end": "1346820"
  },
  {
    "text": "how we ran those migrations feel free to come to me after the talk and we can talk about that as well okay so I'm",
    "start": "1346820",
    "end": "1354200"
  },
  {
    "text": "giving it back to floro who's going to talk about new and upcoming features in BTS yes uh all right so let's start with",
    "start": "1354200",
    "end": "1362360"
  },
  {
    "text": "the new features so this is all the most important features that happen since",
    "start": "1362360",
    "end": "1368539"
  },
  {
    "text": "kubecon Detroit so that includes the ones in v15 and V16 like Arthur said the",
    "start": "1368539",
    "end": "1375500"
  },
  {
    "text": "latest version of it has is V16 it was released a couple months ago I think uh",
    "start": "1375500",
    "end": "1380600"
  },
  {
    "text": "so we had two new B components marked as GA in v15 which are vtorc and VT admin",
    "start": "1380600",
    "end": "1386500"
  },
  {
    "text": "vtorg is the component that will repair your cluster if it's failing and with",
    "start": "1386500",
    "end": "1392720"
  },
  {
    "text": "the admin is the like I mentioned before it's the new front-end UI to visualize and manage your cluster we also reworked",
    "start": "1392720",
    "end": "1399620"
  },
  {
    "text": "all of our CLI Flags uh so we changed the infrastructure between behind all of",
    "start": "1399620",
    "end": "1404780"
  },
  {
    "text": "the flags that we had there are now more thin they look more the same and now we",
    "start": "1404780",
    "end": "1410780"
  },
  {
    "text": "can reiterate on our CLI Flags build some new tools new documentation Etc",
    "start": "1410780",
    "end": "1417700"
  },
  {
    "text": "um we also have incremental backup and point in time recovery that was added in v15 we also reworked the entire",
    "start": "1417799",
    "end": "1425600"
  },
  {
    "text": "documentation so we have a big documentation project and we started in V16 by going through all of the pages",
    "start": "1425600",
    "end": "1431480"
  },
  {
    "text": "that we had and just making sure that they were accurate up to date and that they were",
    "start": "1431480",
    "end": "1437240"
  },
  {
    "text": "saying the right thing and the next step for the documentation is that we actually want to restructure the entire",
    "start": "1437240",
    "end": "1442880"
  },
  {
    "text": "website so we wanted to make sure that the content was right before we changed the entire structure and we also added",
    "start": "1442880",
    "end": "1450260"
  },
  {
    "text": "sharded views support so this is the SQL view it wasn't supported for the like a",
    "start": "1450260",
    "end": "1455539"
  },
  {
    "text": "Charlotte key space but now it is so that's pretty cool that was the new features and now I'm",
    "start": "1455539",
    "end": "1461900"
  },
  {
    "text": "going to talk about the upcoming features so this is for v17 which is going to be released in June and Beyond",
    "start": "1461900",
    "end": "1468860"
  },
  {
    "text": "foreign key support so we don't have support for foreign Keys we want to support them we have a big project for",
    "start": "1468860",
    "end": "1475400"
  },
  {
    "text": "this it's not going to be released in v17 but it's going to be released Beyond v17 uh we have schema tracking improvements",
    "start": "1475400",
    "end": "1483440"
  },
  {
    "text": "on the queue as well uh schema tracking is a feature that is very important to us because it allows us to track the",
    "start": "1483440",
    "end": "1489679"
  },
  {
    "text": "schema across all of the shards and make vitigate aware of your SQL schema which",
    "start": "1489679",
    "end": "1496580"
  },
  {
    "text": "allows for a lot more queries than if skimmer tracking was off so anyway we're gonna do some improvements for to this",
    "start": "1496580",
    "end": "1503539"
  },
  {
    "text": "we're also going to improve the MySQL compatibility we have a list of all the queries that we do not support and that",
    "start": "1503539",
    "end": "1511220"
  },
  {
    "text": "MySQL support and the goal is to continue consuming that list and support more queries so we're just going to work",
    "start": "1511220",
    "end": "1517940"
  },
  {
    "text": "on that and we're also going to enhance our fastjet UI uh we have a benchmarking",
    "start": "1517940",
    "end": "1524600"
  },
  {
    "text": "tool for vtest which allows us to run some nightly Benchmark and see if someone degraded the performance over",
    "start": "1524600",
    "end": "1531140"
  },
  {
    "text": "cut base and that's RFI set and we want to improve the UI and the tool itself",
    "start": "1531140",
    "end": "1538159"
  },
  {
    "text": "all right here are some resources for you if you want to learn more about vitess if you want to get started there",
    "start": "1538159",
    "end": "1544880"
  },
  {
    "text": "are some tutorials that you can run on Mini Cube locally or just outside of mini cubes on your local machine",
    "start": "1544880",
    "end": "1551380"
  },
  {
    "text": "uh you have or a slack if you want to ask any question if you just want to talk to us we're right here and uh thank",
    "start": "1551380",
    "end": "1559340"
  },
  {
    "text": "you so much [Applause]",
    "start": "1559340",
    "end": "1569120"
  },
  {
    "text": "cool yeah if you have any questions don't hesitate yeah",
    "start": "1569120",
    "end": "1577419"
  },
  {
    "text": "you mentioned earlier that you the compatibility with my skewer is not perfect do you have any example of such",
    "start": "1580580",
    "end": "1587360"
  },
  {
    "text": "a use case where you have to modify your query so that it's compatible with Vitas classical one let's say right right I",
    "start": "1587360",
    "end": "1594380"
  },
  {
    "text": "think yeah I think get locks is not supported like name blocks but you can go ahead",
    "start": "1594380",
    "end": "1602120"
  },
  {
    "text": "um sure yeah so uh one thing that that I worked on was um",
    "start": "1602120",
    "end": "1608120"
  },
  {
    "text": "uh sub query compatibility so if you have like if you have a query and you have a sub query and those two are",
    "start": "1608120",
    "end": "1616039"
  },
  {
    "text": "correlated to actually like only load data from one chart right which is like a good case like it's a case that might",
    "start": "1616039",
    "end": "1621679"
  },
  {
    "text": "be tests Theory should support very well right that wasn't supported so for example I",
    "start": "1621679",
    "end": "1627380"
  },
  {
    "text": "worked on that and on fixing that um we had like we actually had like a lot of sub",
    "start": "1627380",
    "end": "1633860"
  },
  {
    "text": "query things that we needed to fix to make it work when we initially ran like the way we migrated to vitesses we",
    "start": "1633860",
    "end": "1639320"
  },
  {
    "text": "basically took our CI set up switched out my SQL for for the tests and granted and uh Activision also ran into like",
    "start": "1639320",
    "end": "1646760"
  },
  {
    "text": "everything failed like it would time out and wouldn't finish and we sat down like got a list of things that were broken",
    "start": "1646760",
    "end": "1652700"
  },
  {
    "text": "tried to like group them together and then we went through one by one through and figured out whether like it's",
    "start": "1652700",
    "end": "1658279"
  },
  {
    "text": "something that we can fix on our side by just like like doing tiny changes to the queries including sharding keys or",
    "start": "1658279",
    "end": "1663799"
  },
  {
    "text": "something like that um or if that was like a thing that we test should support out of the box but",
    "start": "1663799",
    "end": "1670640"
  },
  {
    "text": "doesn't and we have to go and like fix it yeah I think mostly everything like all the queries are going to be",
    "start": "1670640",
    "end": "1676340"
  },
  {
    "text": "cross-charted like you have to send it to one chart another chart and then aggregate the result at the vitigate",
    "start": "1676340",
    "end": "1681380"
  },
  {
    "text": "level like that's the most complex kind of query so that's why we've added schema tracking to be able to know which",
    "start": "1681380",
    "end": "1687020"
  },
  {
    "text": "columns we have in which table and then Etc but we're working on it sir",
    "start": "1687020",
    "end": "1692360"
  },
  {
    "text": "yeah and wait we have a file that lists all the queries that we don't support if",
    "start": "1692360",
    "end": "1698059"
  },
  {
    "text": "you want I can send you the file",
    "start": "1698059",
    "end": "1701380"
  },
  {
    "text": "yeah so I want to ask about the online schema changes I know that there are different strategies to run them like",
    "start": "1706400",
    "end": "1712279"
  },
  {
    "text": "direct park on the percona thing course with this so first question is how does the bites weigh compared to Ghost and",
    "start": "1712279",
    "end": "1719539"
  },
  {
    "text": "then the second question would be like um if there are any performance",
    "start": "1719539",
    "end": "1725140"
  },
  {
    "text": "differences okay I've never run online ddl on my own so I",
    "start": "1725140",
    "end": "1730520"
  },
  {
    "text": "couldn't answer but maybe after did so uh I think the the test way nowadays is",
    "start": "1730520",
    "end": "1736880"
  },
  {
    "text": "actually like pretty similar to Ghost there are some changes but it's like done by the same offer like shlomi is",
    "start": "1736880",
    "end": "1742580"
  },
  {
    "text": "working he he built ghost and now he's working on DB tests uh online schema",
    "start": "1742580",
    "end": "1747740"
  },
  {
    "text": "change um so they're fairly similar I think the main difference is that",
    "start": "1747740",
    "end": "1753559"
  },
  {
    "text": "um I think the downtime is show like not the downtime but the the time that the tables get locked in ghost is shorter",
    "start": "1753559",
    "end": "1759980"
  },
  {
    "text": "than in DB test solution but overall it's it's a very similar approach like",
    "start": "1759980",
    "end": "1765860"
  },
  {
    "text": "the wearer face yes okay like in in the test I think the way it works is um if",
    "start": "1765860",
    "end": "1771020"
  },
  {
    "text": "we just get paused activity gate level until the cutover is done and then queues are let through again to the new",
    "start": "1771020",
    "end": "1777260"
  },
  {
    "text": "table and in ghost it's like on my SQL level thank you",
    "start": "1777260",
    "end": "1783398"
  },
  {
    "text": "hi I have two questions first do you recommend starting at let's assume you",
    "start": "1788360",
    "end": "1794419"
  },
  {
    "text": "start a new project do you recommend starting with the test so migrating in a later stage and the second question I",
    "start": "1794419",
    "end": "1801860"
  },
  {
    "text": "have is you mentioned you had the overhead right to managing with this can you elaborate a bit on that yes so the",
    "start": "1801860",
    "end": "1809179"
  },
  {
    "text": "first question is if you if you think you'll get to that size that I've shown then yes you probably should start out",
    "start": "1809179",
    "end": "1815480"
  },
  {
    "text": "with the test because it's easier if you like in our case we had a lot of queries that were like written with mySQL",
    "start": "1815480",
    "end": "1821539"
  },
  {
    "text": "support in mind like they worked fine on my SQL but then you need to think about like how do I start my data",
    "start": "1821539",
    "end": "1827600"
  },
  {
    "text": "um how will cross Shard queries work what will the performance be like if you start out with the tests you have to",
    "start": "1827600",
    "end": "1834740"
  },
  {
    "text": "think about that first and then design your system and then maybe some features you won't even build them in the first",
    "start": "1834740",
    "end": "1841279"
  },
  {
    "text": "place because they don't make sense in in your like architecture right for us it was the other way around we had to",
    "start": "1841279",
    "end": "1846799"
  },
  {
    "text": "figure out like how do we even support this feature in like the new world right um and then the second question was the",
    "start": "1846799",
    "end": "1853399"
  },
  {
    "text": "overhead um so let's say curious management overhead like from a",
    "start": "1853399",
    "end": "1860059"
  },
  {
    "text": "from a database uh operations perspective I mean if you're using kubernetes like that overhead almost",
    "start": "1860059",
    "end": "1865159"
  },
  {
    "text": "goes away but if you do everything manually then you need to like I don't know start start uh",
    "start": "1865159",
    "end": "1872360"
  },
  {
    "text": "like add additional tooling to manage TV test part of things right like to start DVD tablet you need to make sure that",
    "start": "1872360",
    "end": "1877940"
  },
  {
    "text": "everything like ad monitoring support for all these things right and then there's also overhead on a design",
    "start": "1877940",
    "end": "1884659"
  },
  {
    "text": "perspective right um so one example that I can give is if you go and use our rest API we give you",
    "start": "1884659",
    "end": "1891440"
  },
  {
    "text": "for example repository IDs right and then you can later look up that repository by issue IDs and you can look",
    "start": "1891440",
    "end": "1898460"
  },
  {
    "text": "up that issue by ID we don't Shard by issue ID so if you have a request that comes in like an SQL query where you",
    "start": "1898460",
    "end": "1905419"
  },
  {
    "text": "like try to find an issue just by its ID um by default that would be like a",
    "start": "1905419",
    "end": "1910820"
  },
  {
    "text": "scatter together type of query where the test is going to send that to all sharks if you have hundreds of shards you send",
    "start": "1910820",
    "end": "1916460"
  },
  {
    "text": "that queue to all hundreds of sharks um and we use things like look uh it's",
    "start": "1916460",
    "end": "1921500"
  },
  {
    "text": "called look up the index so basically the test keeps the mapping of the index side the issue ID back to which chart",
    "start": "1921500",
    "end": "1929059"
  },
  {
    "text": "the issue lives on right um basically you can think of it as a mapping between issue ID and repo ID",
    "start": "1929059",
    "end": "1935299"
  },
  {
    "text": "that's essentially like The Logical way of how to think about it and that needs",
    "start": "1935299",
    "end": "1940340"
  },
  {
    "text": "to be stored in my like in in B test as well so you have like need to store this mapping in between in my SQL as well and",
    "start": "1940340",
    "end": "1947960"
  },
  {
    "text": "it takes a disk space and if you then do a lookup you have to run this lookup",
    "start": "1947960",
    "end": "1953480"
  },
  {
    "text": "first like if you try to load an issue just by ID you need to First do this V index lookup and then like a second",
    "start": "1953480",
    "end": "1958760"
  },
  {
    "text": "query to actually fetch the data so there's like some performance overhead as well it's not huge like it's fine I",
    "start": "1958760",
    "end": "1965419"
  },
  {
    "text": "don't think any one of you noticed that like we changed everything out underneath but it's something that you",
    "start": "1965419",
    "end": "1970820"
  },
  {
    "text": "need to be aware of thank you",
    "start": "1970820",
    "end": "1977440"
  },
  {
    "text": "um I have a question about like um how should I use v test like as in UCS",
    "start": "1979880",
    "end": "1986179"
  },
  {
    "text": "purely like a oltp like a transactional database or are there use cases for like analytical",
    "start": "1986179",
    "end": "1992419"
  },
  {
    "text": "um you know use cases I mean right now it seems GitHub is more using it as a transactional database yeah yeah so we",
    "start": "1992419",
    "end": "1999260"
  },
  {
    "text": "have oltp we used to have support for Ola olap",
    "start": "1999260",
    "end": "2004539"
  },
  {
    "text": "I think we have plans on removing it I'm not entirely sure uh what's the plan about olap but we had support for olap",
    "start": "2004539",
    "end": "2011080"
  },
  {
    "text": "in the past that's for sure um I'm not entirely sure what the future of",
    "start": "2011080",
    "end": "2016179"
  },
  {
    "text": "the olap and the streaming mode of the test is honestly but yeah I can I can double check right",
    "start": "2016179",
    "end": "2022179"
  },
  {
    "text": "after but I'm not exactly sure yeah we're in the process of merging well like the old",
    "start": "2022179",
    "end": "2027840"
  },
  {
    "text": "sequential execution function streaming function for executing queries together",
    "start": "2027840",
    "end": "2033820"
  },
  {
    "text": "but what will all that become I'm not exactly sure for now you can but in the future I'm not entirely sure okay so I",
    "start": "2033820",
    "end": "2041080"
  },
  {
    "text": "guess it's more we should probably use it more for transactional purposes than that yeah yeah",
    "start": "2041080",
    "end": "2047940"
  },
  {
    "text": "in your architecture diagram you've thrown a load building so how does it handle like adding replicas in failing",
    "start": "2055540",
    "end": "2061000"
  },
  {
    "text": "Library class replicas are they automatically added to the load balancer or do you have like a unified like",
    "start": "2061000",
    "end": "2067358"
  },
  {
    "text": "access point uh",
    "start": "2067359",
    "end": "2070618"
  },
  {
    "text": "uh so the gray part here the load balancer is not really part of uh v test",
    "start": "2073480",
    "end": "2079358"
  },
  {
    "text": "like that's the thing that you have to add on your own basically okay uh you have your layer of vtk you might have one a hundred one thousand but you need",
    "start": "2079359",
    "end": "2086980"
  },
  {
    "text": "some kind of load balancer to distribute all your queries across all your vitigate so that's yeah I can also talk",
    "start": "2086980",
    "end": "2094599"
  },
  {
    "text": "about this and like how it works in our setup um so we have a TCP load balancer at the front where like my SQL connections come",
    "start": "2094599",
    "end": "2100359"
  },
  {
    "text": "in like I like a shirt or like a standardized DNS name right and then",
    "start": "2100359",
    "end": "2106300"
  },
  {
    "text": "um we like the load balancer just distributes across 3D Gates and then inside the BT gate layer",
    "start": "2106300",
    "end": "2112540"
  },
  {
    "text": "um every query that comes in is automatically distributed across like",
    "start": "2112540",
    "end": "2118420"
  },
  {
    "text": "it's let's say you have four replicas and you run four queries right uh each",
    "start": "2118420",
    "end": "2124420"
  },
  {
    "text": "query one after the other might actually land to a different replica so like it's automatically distributed activity layer",
    "start": "2124420",
    "end": "2129520"
  },
  {
    "text": "but you need to also uh distribute like uh requests or like connections coming in from your application to DVT Gates",
    "start": "2129520",
    "end": "2136060"
  },
  {
    "text": "somehow uh another for the um question how many VT Gates do you need like uh like do you",
    "start": "2136060",
    "end": "2143800"
  },
  {
    "text": "need one retigate for one primary or is it like uh no you can have honestly you can have as",
    "start": "2143800",
    "end": "2151359"
  },
  {
    "text": "many so you have one vtk at least 158 per cluster uh uh behind your vtk you",
    "start": "2151359",
    "end": "2158800"
  },
  {
    "text": "can have as many key spaces as you want but no you can have just as many video",
    "start": "2158800",
    "end": "2164020"
  },
  {
    "text": "Gates as you want I think it depends on like how like what the load is on your video if you want to scale up or down",
    "start": "2164020",
    "end": "2169060"
  },
  {
    "text": "but yeah you can have multiple applications on the same video Even though",
    "start": "2169060",
    "end": "2174579"
  },
  {
    "text": "see it might not be the best practice I think but you're cold yeah there's like connection pooling happens at DVT tablet",
    "start": "2174579",
    "end": "2181599"
  },
  {
    "text": "layer and like also like things like uh what did you call it uh consolidation yeah and there's also connection pooling",
    "start": "2181599",
    "end": "2187900"
  },
  {
    "text": "happening at DVT gate layer so if you have only one we ticket and you have like I don't know 100 000 connections",
    "start": "2187900",
    "end": "2194500"
  },
  {
    "text": "come in like we'll probably fall over and die right but if you you can scale different parts of your test",
    "start": "2194500",
    "end": "2200020"
  },
  {
    "text": "independently like if you have many connections but don't actually run many queries you just scale the BT gate layer and you're fine if you have",
    "start": "2200020",
    "end": "2207040"
  },
  {
    "text": "um many theories but uh like not so high number of connections",
    "start": "2207040",
    "end": "2212859"
  },
  {
    "text": "um then you scale the BT tablet later okay thank you",
    "start": "2212859",
    "end": "2218880"
  },
  {
    "text": "um yeah so I noticed the other",
    "start": "2220900",
    "end": "2226920"
  },
  {
    "text": "GitHub or controller plan of with us and kubernetes but the data plan on their",
    "start": "2226920",
    "end": "2234480"
  },
  {
    "text": "meta servers so my question is now can with us around all controller plan data",
    "start": "2234480",
    "end": "2242500"
  },
  {
    "text": "plan on containers and coupon that is",
    "start": "2242500",
    "end": "2248940"
  },
  {
    "text": "like do we do GitHub want to move everything together to kubernetes is that the question yes",
    "start": "2249160",
    "end": "2256060"
  },
  {
    "text": "yeah so we were actually talking about this the last two lunches we're talking about",
    "start": "2256060",
    "end": "2261160"
  },
  {
    "text": "yeah usual move all your VT tablet and mysqold instance is the cube and it isn't just like yeah like that's the",
    "start": "2261160",
    "end": "2266260"
  },
  {
    "text": "preferred way of how to run the test is like have everything in kubernetes right both the data and DVT gate layer",
    "start": "2266260",
    "end": "2273940"
  },
  {
    "text": "um I don't know like we don't have any plans it would be nice or interesting to",
    "start": "2273940",
    "end": "2278980"
  },
  {
    "text": "do right because it allows you to be more flexible in scaling um and it also would allow us to not",
    "start": "2278980",
    "end": "2285400"
  },
  {
    "text": "have to go and like provision huge boxes for some of the Clusters that are not as big",
    "start": "2285400",
    "end": "2290800"
  },
  {
    "text": "um but we don't have any plans today yeah okay hi thank you",
    "start": "2290800",
    "end": "2299099"
  },
  {
    "text": "cool thank you [Applause]",
    "start": "2304060",
    "end": "2310899"
  }
]