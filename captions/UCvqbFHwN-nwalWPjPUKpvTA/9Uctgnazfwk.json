[
  {
    "text": "called weave scope and this is about virtual about visualizing your micro-services applications I stopped",
    "start": "0",
    "end": "8700"
  },
  {
    "text": "working this about six months ago but during the time we we built this open-source project it's got a very pretty UI the UI team in Berlin did an",
    "start": "8700",
    "end": "15839"
  },
  {
    "text": "absolutely fantastic job but mostly this year we build a hosted version of this called weave cloud which you can go and",
    "start": "15839",
    "end": "22020"
  },
  {
    "text": "use if you like and you know running a service meant we had to monitor it so",
    "start": "22020",
    "end": "27029"
  },
  {
    "text": "you know some of recommendations of my friends I was told to check out prometheus and we instrumented scope and",
    "start": "27029",
    "end": "32910"
  },
  {
    "text": "the service with Prometheus and we absolutely loved it and we're like hey about about six months ago we write hey",
    "start": "32910",
    "end": "38579"
  },
  {
    "text": "you know we should we should include prometheus and a Prometheus service in weave cloud because it's so useful and",
    "start": "38579",
    "end": "43800"
  },
  {
    "text": "so we set down the path of how we're going to build a scalable version of prometheus that we can use to provide a",
    "start": "43800",
    "end": "50670"
  },
  {
    "text": "Prometheus as a service so that's what how it got here in the process we've moved everything on",
    "start": "50670",
    "end": "57149"
  },
  {
    "text": "to kubernetes so that's kind of you know a bit more synergy with this conference I guess and we we did a series of four",
    "start": "57149",
    "end": "64830"
  },
  {
    "text": "blog posts last month I think maybe the month before about our experience of using Prometheus and Kuban at ease",
    "start": "64830",
    "end": "70020"
  },
  {
    "text": "together so you know they're on our blog you can check them out they're kind of interesting I mean the best one is this deploying one this is kind of the thing",
    "start": "70020",
    "end": "76799"
  },
  {
    "text": "not often talked about how to actually like set up and deploy a production you",
    "start": "76799",
    "end": "82830"
  },
  {
    "text": "know grade Prometheus on your Cuba Nettie's cluster and what the different trade-offs you had you have in the",
    "start": "82830",
    "end": "87930"
  },
  {
    "text": "monitoring the applications everyone's written about that so there's not much point in reading that one to be honest anyway I've plugged the blog let's just",
    "start": "87930",
    "end": "95189"
  },
  {
    "text": "dive straight into the design of the system and like every good engineer we sat down and we collected some requirements and we had we had Fabian",
    "start": "95189",
    "end": "102150"
  },
  {
    "text": "and Bjorn over to the office Fabian biana the other Prometheus core developers or some of the other ones we",
    "start": "102150",
    "end": "108570"
  },
  {
    "text": "had them over to the Weaver Works office and we talked to them about this idea and oh really encouraging and they basically completely changed the design",
    "start": "108570",
    "end": "114060"
  },
  {
    "text": "in like one half an hour meeting which was fantastic we started off you know we love Prometheus we love the the query",
    "start": "114060",
    "end": "120570"
  },
  {
    "text": "language it's really concise and you know it's really powerful so we're like we definitely want to be completely",
    "start": "120570",
    "end": "126540"
  },
  {
    "text": "compatible with Prometheus and to the extent where you go look at the code base we just reuse wholesale massive",
    "start": "126540",
    "end": "132030"
  },
  {
    "text": "swaths of Prometheus code we just vendor in the Prometheus project into our into our binary and just just cool into it",
    "start": "132030",
    "end": "138190"
  },
  {
    "text": "it's fantastic we also mean we're a very small engineering team that we've works we've got about 20 of us and on the",
    "start": "138190",
    "end": "144700"
  },
  {
    "text": "weave cloud project and on this project there's like five so on this project it's actually just me and two other",
    "start": "144700",
    "end": "149860"
  },
  {
    "text": "people and we've cloud and the hosted version scope maybe two or three more people so this had to be super easy to",
    "start": "149860",
    "end": "155770"
  },
  {
    "text": "operate and we had to we had to build this thing so that you know it it was just it just had to be easy it had to",
    "start": "155770",
    "end": "162400"
  },
  {
    "text": "scale it had to not have any you know single points of failure it had to generally you know be easy to upgrade",
    "start": "162400",
    "end": "168370"
  },
  {
    "text": "because we do continuous deployment and we're redeploying this system you know eight ten times a day we you know we're",
    "start": "168370",
    "end": "175300"
  },
  {
    "text": "pretty ambitious we want thousands of users I think we've got like ten right now but we want thousands and when it",
    "start": "175300",
    "end": "182050"
  },
  {
    "text": "makes it cheap to run and because we want to sell it you know we're a company we believe in open source everything we",
    "start": "182050",
    "end": "187300"
  },
  {
    "text": "do is open source but we're going to run out of money sooner or later and you know by then we'll hopefully have a bit",
    "start": "187300",
    "end": "192910"
  },
  {
    "text": "of revenue to to pay for these things so we do want to sell this eventually as a service and you know take all the kind",
    "start": "192910",
    "end": "199900"
  },
  {
    "text": "of operational load off of the users yeah and we wanted to reuse as much as prometheus as possible Prometheus is one",
    "start": "199900",
    "end": "205690"
  },
  {
    "text": "of the best piece of software I've ever worked with it's one of the nicest code bases I've ever worked with and it's",
    "start": "205690",
    "end": "211300"
  },
  {
    "text": "just really high quality and so we wanted to you know we wanted to reuse that so yeah don't worry this is all open",
    "start": "211300",
    "end": "218320"
  },
  {
    "text": "source I know people get kind of a bit a bit worried about this last bit here but I like to be really really honest about",
    "start": "218320",
    "end": "224470"
  },
  {
    "text": "these things we are a company we are trying to make money so we do want to sell this but it is all open-source we're 100% committed to this we've done",
    "start": "224470",
    "end": "230860"
  },
  {
    "text": "all of the development of this in the open so I published her with Julis and I published a design doc under the old",
    "start": "230860",
    "end": "237459"
  },
  {
    "text": "name on the Prometheus developers mailing list oh yeah so you come well",
    "start": "237459",
    "end": "243280"
  },
  {
    "text": "it's not that interesting but just got some dates on it yeah we started the design dock in June",
    "start": "243280",
    "end": "249660"
  },
  {
    "text": "we put the first jobs up about a month later into our cluster and it kind of worked and then a month after that I",
    "start": "249660",
    "end": "256359"
  },
  {
    "text": "presented this at prom Con in Berlin where it was it was just about working by then and since then we spent a lot of",
    "start": "256359",
    "end": "262780"
  },
  {
    "text": "time making a lot more robust and today I'm talking about it cubic on so yeah if you want to check out the",
    "start": "262780",
    "end": "268720"
  },
  {
    "text": "design dockets it's kind of interesting doing the whole open design thing where you get you know feedback from people in",
    "start": "268720",
    "end": "274780"
  },
  {
    "text": "the community who basically know more than I do about this stuff was a really interesting experience to me and I'd massively encourage it the github repo",
    "start": "274780",
    "end": "282040"
  },
  {
    "text": "that we did the development in was open from day 0 you know this wasn't a we're going to go away right thousands of",
    "start": "282040",
    "end": "287620"
  },
  {
    "text": "lines of code and magically open source it and make a big PR thing out of it we we started with an empty repo and it was",
    "start": "287620",
    "end": "292960"
  },
  {
    "text": "always open source in fact I think it started as a branch as a fork of the Prometheus repo so it wasn't an empty",
    "start": "292960",
    "end": "300130"
  },
  {
    "text": "repo anyway so next what does the system look like well",
    "start": "300130",
    "end": "305160"
  },
  {
    "text": "we've broadly divided the problem up into three different services you know",
    "start": "305160",
    "end": "310419"
  },
  {
    "text": "Mike reservist is are all the rage so we thought hey why not do that and we've taken we've got this retrieval agent",
    "start": "310419",
    "end": "315910"
  },
  {
    "text": "that you'll run in your cluster this will scrape the jobs do the service discovery that prometheus is already",
    "start": "315910",
    "end": "321400"
  },
  {
    "text": "good at and we'll send all the samples in batches over to weave cloud so we've",
    "start": "321400",
    "end": "326530"
  },
  {
    "text": "got a in weave cloud we've got a front-end this already existed this is like the only bit of closed",
    "start": "326530",
    "end": "332080"
  },
  {
    "text": "source code in the whole thing right and it's it's mainly closed source because it's embarrassing but we'll fix that at some point it was",
    "start": "332080",
    "end": "339669"
  },
  {
    "text": "nginx now it's go you know it will probably be something else in a few months time but now we really get into",
    "start": "339669",
    "end": "345640"
  },
  {
    "text": "the into the Frankenstein bits and we've split the job up into a distributor and this is the thing that basically acts as",
    "start": "345640",
    "end": "352210"
  },
  {
    "text": "the Frankenstein front-end is not cool Frankenstein it's called cortex as the cortex front-end and it decides where to",
    "start": "352210",
    "end": "359020"
  },
  {
    "text": "route the Cuesta to basically and it deals with things like replication and then we've got the in jesters these",
    "start": "359020",
    "end": "364570"
  },
  {
    "text": "things will collect samples batch them up and when they've got a big enough batch of samples called a chunk they'll",
    "start": "364570",
    "end": "370540"
  },
  {
    "text": "get they'll get flushed to s3 where we store the chunks and inverted index and DynamoDB so we can later find the chunks",
    "start": "370540",
    "end": "377350"
  },
  {
    "text": "so let's walk through that one more time just to hammer this home your samples",
    "start": "377350",
    "end": "382450"
  },
  {
    "text": "start in your jobs exposed via the Prometheus slash metrics endpoint and the scraper the retriever will come",
    "start": "382450",
    "end": "388660"
  },
  {
    "text": "along and scrape them into a little batch not the same as a chunk but a little batch and then it will send them",
    "start": "388660",
    "end": "394840"
  },
  {
    "text": "over via the front end to one of the distributors now at this level we're just using the coop Roxie to do random",
    "start": "394840",
    "end": "401800"
  },
  {
    "text": "load-balancing so we don't really have control over where they're going to hit that's that's desirable because that",
    "start": "401800",
    "end": "407440"
  },
  {
    "text": "means you know these distributors are completely stateless so if one of the misbehaves or we want to scale them up it's it's super easy works really well",
    "start": "407440",
    "end": "413950"
  },
  {
    "text": "but and once they get to the distributor we must start thinking about some of the cortex specific logic so what we've done",
    "start": "413950",
    "end": "422350"
  },
  {
    "text": "is we've used concepts from Cassandra a distributed database so we have a hash ring I'll go to the whiteboard minute",
    "start": "422350",
    "end": "428170"
  },
  {
    "text": "draw this out but basically each sample belongs to a specific time series and each time series belongs to a specific",
    "start": "428170",
    "end": "434350"
  },
  {
    "text": "and gesture so we use this routing information that we maintain to break this this batch of samples up into you",
    "start": "434350",
    "end": "442360"
  },
  {
    "text": "know different samples that are sent to different injustice and then this process repeats itself and eventually I'm sorry I've got an animation in but I",
    "start": "442360",
    "end": "448960"
  },
  {
    "text": "think it looks good eventually you've got enough enough samples in an gesture to form a chunk and the chunks will then",
    "start": "448960",
    "end": "454930"
  },
  {
    "text": "get written to s3 and indexed in DynamoDB it's really straightforward now",
    "start": "454930",
    "end": "460360"
  },
  {
    "text": "a few things to note queries also handled by the distributor julius and i",
    "start": "460360",
    "end": "465760"
  },
  {
    "text": "went round on maybe we should have a query service but at the end of the day the code just lives in a distributor",
    "start": "465760",
    "end": "472290"
  },
  {
    "text": "the in jesters have to have an in-memory inverted index as well and so that you",
    "start": "472290",
    "end": "477700"
  },
  {
    "text": "can find the queries now gives you some examples in a minute of how that works what else should I mention here",
    "start": "477700",
    "end": "483990"
  },
  {
    "text": "yeah why why have we got this separation between retriever and distributor is because mainly you know your code is",
    "start": "483990",
    "end": "489550"
  },
  {
    "text": "going to hopefully live behind a firewall and it's going to be really hard for us in in our data center to",
    "start": "489550",
    "end": "494770"
  },
  {
    "text": "scrape your jobs so another thing to know the retriever is actually just an open source Prometheus and one of the",
    "start": "494770",
    "end": "500410"
  },
  {
    "text": "questions before the talk I had was has any of the changes we've made in the code we've developed gone upstream and yeah in the in retriever there's a",
    "start": "500410",
    "end": "508000"
  },
  {
    "text": "generic write API or spi I guess that you can implement the cortex is one",
    "start": "508000",
    "end": "514090"
  },
  {
    "text": "of the implementations of so basically you can tell Prometheus to send your samples somewhere else so this here is",
    "start": "514090",
    "end": "519400"
  },
  {
    "text": "just an open source upstream Prometheus works with 1.2 and 1.3 and why are we doing this interesting",
    "start": "519400",
    "end": "527220"
  },
  {
    "text": "batching model here this is because s3 doesn't really deal very well with with",
    "start": "527220",
    "end": "533350"
  },
  {
    "text": "16 byte object you couldn't write every every sample test three you just you you",
    "start": "533350",
    "end": "538540"
  },
  {
    "text": "know you just destroy it similarly dynamite does deal very well with small objects but it's incredibly expensive so",
    "start": "538540",
    "end": "545370"
  },
  {
    "text": "by batching up the samples into chunks you manage to basically minimize the",
    "start": "545370",
    "end": "551019"
  },
  {
    "text": "cost of the cloud services we use to give you an example I think we're",
    "start": "551019",
    "end": "556870"
  },
  {
    "text": "using the double delta encoding right now we get about 300 samples per chunk and with there's about 10 rights per",
    "start": "556870",
    "end": "563079"
  },
  {
    "text": "chunk into DynamoDB this would serve dynamo d I'll get into more detail later but DynamoDB stores an inverted index so",
    "start": "563079",
    "end": "569230"
  },
  {
    "text": "every label value pair has to be written into dynamo DB and so on average we found it's between 8 and 10 users who",
    "start": "569230",
    "end": "575709"
  },
  {
    "text": "are using our system have that many label value pairs on each metric so if",
    "start": "575709",
    "end": "580870"
  },
  {
    "text": "you multiply that out for every sample that comes in the front end we write about point three point zero three over",
    "start": "580870",
    "end": "588370"
  },
  {
    "text": "right two DynamoDB which makes this system cost-effective you know if we didn't if we didn't have this system",
    "start": "588370",
    "end": "593470"
  },
  {
    "text": "basically we would be charging users eventually one day you know thousands and thousands of dollars to run this and yeah I know let's go through each system",
    "start": "593470",
    "end": "602199"
  },
  {
    "text": "a bit more how do you set this up so as I said Retriever is a vanilla open-source Prometheus go and download",
    "start": "602199",
    "end": "607690"
  },
  {
    "text": "it from docker hub or prometheus website all the service discovery scraping that",
    "start": "607690",
    "end": "613959"
  },
  {
    "text": "you know and love about Prometheus if people don't know what I'm talking about then hopefully someone else will cover that later but I can go into in detail",
    "start": "613959",
    "end": "619959"
  },
  {
    "text": "if you like all you have to do is add this to your configuration previously",
    "start": "619959",
    "end": "625769"
  },
  {
    "text": "this will command line flanks but now it's now it's in the config and you can",
    "start": "625769",
    "end": "631060"
  },
  {
    "text": "put some some credentials in there and you put an API endpoint the whole idea here is that you can also you know if",
    "start": "631060",
    "end": "636430"
  },
  {
    "text": "you if you don't wanna use cortex if you don't use weave work so you want to host it yourself you put your own URL in there or if you want it to send it to a",
    "start": "636430",
    "end": "641920"
  },
  {
    "text": "Kafka or or something else then you build a little proxy for doing that and you put that in there you know this is",
    "start": "641920",
    "end": "647740"
  },
  {
    "text": "the generic completely non cortex specific bit of the project and there's",
    "start": "647740",
    "end": "652750"
  },
  {
    "text": "some PRS down here which which did all of this one of the interesting so there was",
    "start": "652750",
    "end": "658180"
  },
  {
    "text": "another project announced at prom con a few months ago basically doing a very similar thing by digitalocean called",
    "start": "658180",
    "end": "665279"
  },
  {
    "text": "Vulcan and one of the big you know people ask me what is the difference between cortex",
    "start": "665279",
    "end": "671779"
  },
  {
    "text": "and Vulcan one of the big differences is they've considered the scalability of the",
    "start": "671779",
    "end": "676970"
  },
  {
    "text": "retrievers so they have a method for sharding their retrievers out so they can run tens of retrievers you know",
    "start": "676970",
    "end": "682100"
  },
  {
    "text": "hitting millions of targets we didn't we just didn't think of that the Retriever is not really a single not really a",
    "start": "682100",
    "end": "689180"
  },
  {
    "text": "bottleneck in this system it is a single point of failure in the current set up but the idea would be you just run two",
    "start": "689180",
    "end": "695630"
  },
  {
    "text": "so that's not that not actually we do did you ping on the server side so that's not actually a big deal but yeah",
    "start": "695630",
    "end": "701600"
  },
  {
    "text": "that's one of the differences between Vulcan and Consular cortex so back to here we've covered the retrievers let's",
    "start": "701600",
    "end": "707570"
  },
  {
    "text": "move on to the distributors this is kind of the maybe the most",
    "start": "707570",
    "end": "713630"
  },
  {
    "text": "hardcore distributed system bits of this have you guys have you guys seen Cassandra's virtual node implementation",
    "start": "713630",
    "end": "719930"
  },
  {
    "text": "yeah okay one guys nodding his head great it's based on it's based on dynamo the",
    "start": "719930",
    "end": "726350"
  },
  {
    "text": "original dynamo paper from Amazon and it considers this token ring you consider this to be like zero to two to the 64",
    "start": "726350",
    "end": "733220"
  },
  {
    "text": "and then you pick each each in gesture that comes along will pick up a bunch of random points in this ring you know just",
    "start": "733220",
    "end": "739610"
  },
  {
    "text": "just sit there pick hold onto random numbers insert them into the ring and say I now own everything from this random number to the next random number",
    "start": "739610",
    "end": "746860"
  },
  {
    "text": "so the nice property of this system is it gives you a completely even layout of",
    "start": "746860",
    "end": "752930"
  },
  {
    "text": "distribution like the more random tokens you're in you inject the more even the distribution between and jesters become",
    "start": "752930",
    "end": "760180"
  },
  {
    "text": "the downside this system is just complicated you have to do and you know it's Julius and I found out in the past",
    "start": "760180",
    "end": "766040"
  },
  {
    "text": "few months as we've been implementing this particular feature if you want to do replication you basically start with a hash of the you know in our sense it's",
    "start": "766040",
    "end": "774380"
  },
  {
    "text": "a time series it's a fingerprint you start with the hash you'd look for the key closest to the hash and then you",
    "start": "774380",
    "end": "779600"
  },
  {
    "text": "count you know say we're doing replication of three you'd count three steps around the ring and that would be",
    "start": "779600",
    "end": "784670"
  },
  {
    "text": "your replica set but you've inserted multiple tokens for each",
    "start": "784670",
    "end": "791120"
  },
  {
    "text": "and gesture so you need to count three times around the ring but not you know not double count and not send you know",
    "start": "791120",
    "end": "797779"
  },
  {
    "text": "not treat the same in gesture as two replicas for the same sample so okay so now we're gonna deegeu pit and do the",
    "start": "797779",
    "end": "803179"
  },
  {
    "text": "next three distinct intestines but what happens if an and gesture is leaving you know because we've shut it down because",
    "start": "803179",
    "end": "808519"
  },
  {
    "text": "we're doing the role and upgrade okay so we now have a state associated with each and gesture we count round the ring and",
    "start": "808519",
    "end": "814160"
  },
  {
    "text": "if we encounter a leaving and gesture then what we do is we increase the number of replicas we want so we said we",
    "start": "814160",
    "end": "820999"
  },
  {
    "text": "run with three but with we hit a leaving one we ask for four and then we don't send the sample to the leaving one",
    "start": "820999",
    "end": "827119"
  },
  {
    "text": "otherwise it will just never get a chance to fully flush out to s3 and then similarly you have a mirror of",
    "start": "827119",
    "end": "833269"
  },
  {
    "text": "this if you're doing reeds if you're doing reeds you need to increase by one again and then do your read from an",
    "start": "833269",
    "end": "839240"
  },
  {
    "text": "extra one otherwise you might potentially miss data so just one one last point on the",
    "start": "839240",
    "end": "845929"
  },
  {
    "text": "distributor when you've got your finally got your replica set your three or four or maybe even five nodes that formula replica set",
    "start": "845929",
    "end": "852589"
  },
  {
    "text": "you need to do a write to them and it's really inefficient to write to every single one of them and of course we want to we want to tolerate failure so if we",
    "start": "852589",
    "end": "859459"
  },
  {
    "text": "try and write to every single one of them wait for them all and one of them fails kind of so what we actually do is we write this is probably what I'm",
    "start": "859459",
    "end": "865040"
  },
  {
    "text": "implementing so right now we just write to all of them if one of them fails we don't matter it doesn't really matter we just make sure we've written to at least",
    "start": "865040",
    "end": "870679"
  },
  {
    "text": "n over two plus one so at least a quorum of them and the whole idea of writing to a quorum of them is that you're",
    "start": "870679",
    "end": "876949"
  },
  {
    "text": "guaranteed that a subsequent read that might hit a different quorum will at least have a single overlapping in",
    "start": "876949",
    "end": "882559"
  },
  {
    "text": "gesture so that's where the plus one comes in see it's kind of cooled like this in a",
    "start": "882559",
    "end": "888649"
  },
  {
    "text": "previous life I started a company called a kunu which did the virtual node implementation in Cassandra and if you",
    "start": "888649",
    "end": "894829"
  },
  {
    "text": "go here there's a presentation funnily enough by me like six years ago discussing exactly the same topic in in",
    "start": "894829",
    "end": "900619"
  },
  {
    "text": "the end the idea of big data and no sequel so it's you know there's no new ideas in in computer science right",
    "start": "900619",
    "end": "907689"
  },
  {
    "text": "anyway so distributor also handles queries we in Cassandra and in other",
    "start": "907689",
    "end": "913519"
  },
  {
    "text": "dhts this this this ring will be gossiped around the cluster using the gossip algorithm and Ciardi tease we",
    "start": "913519",
    "end": "920839"
  },
  {
    "text": "didn't bother doing that we we just stuck in console and made a couple of educators slightly easier to reason",
    "start": "920839",
    "end": "926179"
  },
  {
    "text": "about one of the things i was talking about doing maybe over december when it's",
    "start": "926179",
    "end": "931579"
  },
  {
    "text": "little bit more quiet is is using weaves which is our gossip library to also make this gossiped and that should make it",
    "start": "931579",
    "end": "937399"
  },
  {
    "text": "much easier for people who want to run this themselves to get started let me see so dynamo star replication",
    "start": "937399",
    "end": "944570"
  },
  {
    "text": "we've talked about console yeah so that's the distributor in our system",
    "start": "944570",
    "end": "951350"
  },
  {
    "text": "were actually in running three of these they're not particularly resource intensive all they're doing is figuring out where to Ford the data to they do",
    "start": "951350",
    "end": "957230"
  },
  {
    "text": "run queries and so the more you have the faster your queries but you know we're we're in the early days there's not a",
    "start": "957230",
    "end": "962510"
  },
  {
    "text": "huge number of users on the system yet so three is enough the ingest is on the other hand we're running six now I think because these are the ones doing the",
    "start": "962510",
    "end": "969140"
  },
  {
    "text": "real heavy lifting and the chunking scheme is really kind of the key are sorry about the whiteboard I didn't",
    "start": "969140",
    "end": "974450"
  },
  {
    "text": "think I put anything in the left anyway so the in gesture started off life as a heavily modified version of Prometheus",
    "start": "974450",
    "end": "980959"
  },
  {
    "text": "and then it would we would push data to it and I'm not going to get into a push versus pull debate here but we were",
    "start": "980959",
    "end": "986120"
  },
  {
    "text": "pushing data to it it would then effectively use the memory series storage code from within Prometheus to",
    "start": "986120",
    "end": "991550"
  },
  {
    "text": "chunk this up it used exactly the same chunking schema as Prometheus this was developed by by Fabian oh no sorry by",
    "start": "991550",
    "end": "998720"
  },
  {
    "text": "Bjorn and it's just absolutely fantastic one of the most interesting pieces of",
    "start": "998720",
    "end": "1003760"
  },
  {
    "text": "code I've ever had to stare at for for hours on end and it's it's heavily inspired you want gives a fantastic talk",
    "start": "1003760",
    "end": "1010420"
  },
  {
    "text": "about how it's really inspired by Facebook's gorilla paper is Facebook isn't it yeah I think so so you know go",
    "start": "1010420",
    "end": "1018250"
  },
  {
    "text": "and read the gorilla paper go and read the source code it's really well commented and one of the one of the reasons we we use the same chunk format",
    "start": "1018250",
    "end": "1024938"
  },
  {
    "text": "as Prometheus is it's incredibly efficient the chunk for my own Prometheus so we take these samples we",
    "start": "1024939",
    "end": "1030790"
  },
  {
    "text": "batch them up into a one kilobyte chunk and the chunk format does all sorts of in-memory compression on them so with",
    "start": "1030790",
    "end": "1037089"
  },
  {
    "text": "the latest var bit encoding there's three different encodings for the chunk but with the latest one I think if I",
    "start": "1037089",
    "end": "1042790"
  },
  {
    "text": "remember correctly its 1.3 bytes per sample and the sample is 128 bits so is is 16 bytes so that's like you know 12",
    "start": "1042790",
    "end": "1050500"
  },
  {
    "text": "13 X compression in memory with no performance overhead basically it's absolutely fantastic and again this",
    "start": "1050500",
    "end": "1056380"
  },
  {
    "text": "makes the system cost-effective to run so yeah the in gesture keeps everything can memory for an hour we limit the we",
    "start": "1056380",
    "end": "1063010"
  },
  {
    "text": "limit the chunk sizes to one kilobyte or one hour whatever we hit first and this",
    "start": "1063010",
    "end": "1068200"
  },
  {
    "text": "is mainly in case we have an in gesture failure it limits the amount of replay and the amount of recovery we have to do",
    "start": "1068200",
    "end": "1074410"
  },
  {
    "text": "oh oh yeah the invert in the inverted index so this was actually inspired by Fabian",
    "start": "1074410",
    "end": "1081730"
  },
  {
    "text": "the the indexing scheme in the Prometheus upstream is that is very",
    "start": "1081730",
    "end": "1087070"
  },
  {
    "text": "different to the way we do it basically we store an in memory one our inverted index and inverted index is the same",
    "start": "1087070",
    "end": "1092830"
  },
  {
    "text": "thing that elastic search or Google will use right it's where you you basically",
    "start": "1092830",
    "end": "1097840"
  },
  {
    "text": "take the corpus of text and you insert for every word in that corpus you insert",
    "start": "1097840",
    "end": "1103360"
  },
  {
    "text": "an entry into the index and then if you want to do queries like saying please give me every document that has the word",
    "start": "1103360",
    "end": "1108940"
  },
  {
    "text": "X&Y in it then you just do a query for extra query for Y and a set intersection on that on the results so we do that we",
    "start": "1108940",
    "end": "1116080"
  },
  {
    "text": "store this in memory at this stage this allows us to have pretty fast queries actually for querying stuff in",
    "start": "1116080",
    "end": "1121900"
  },
  {
    "text": "the last hour it's it's milliseconds if the Wi-Fi works",
    "start": "1121900",
    "end": "1127350"
  },
  {
    "text": "will then talk about the index we use in dynamo d is is similar but very bit different and it's kind of important",
    "start": "1128460",
    "end": "1134350"
  },
  {
    "text": "difference so yeah if if one of these investors fail you're going to lose up to an hour's",
    "start": "1134350",
    "end": "1140530"
  },
  {
    "text": "worth of data we have the replication to work around that so right now really you'll only lose data if if three",
    "start": "1140530",
    "end": "1146980"
  },
  {
    "text": "replicas fail but that's not really good enough for us either so we're gonna add we haven't go",
    "start": "1146980",
    "end": "1152380"
  },
  {
    "text": "around to doing it we're going to add a writer head log that will log out to disk and if something fails we'll have a",
    "start": "1152380",
    "end": "1157870"
  },
  {
    "text": "manual step to recover that log so that you don't lose data you know one of the big differences between",
    "start": "1157870",
    "end": "1163860"
  },
  {
    "text": "cortex and Prometheus is the the concepts of durability and and how long",
    "start": "1163860",
    "end": "1169270"
  },
  {
    "text": "we can retain data for so we really want to make it as durable and and have the best retention as possible so back to",
    "start": "1169270",
    "end": "1175900"
  },
  {
    "text": "the diagram talks about the distributors the retriever and final step is how do we use dynamodb and s3",
    "start": "1175900",
    "end": "1183270"
  },
  {
    "text": "so this is maybe the trickiest thing to explain and I really haven't come up with a good way of explaining it s3 is",
    "start": "1183270",
    "end": "1189130"
  },
  {
    "text": "easy we just saw the one kilobyte chunks as objects in s3 that's easy there's just a bucket but",
    "start": "1189130",
    "end": "1195820"
  },
  {
    "text": "dynamodb dynamized v's a really weird database who here's used dynamo DB and pro against it okay other you do count",
    "start": "1195820",
    "end": "1203230"
  },
  {
    "text": "Julius because you wrote some of this right say it looks a lot like Cassandra so Cassandra",
    "start": "1203230",
    "end": "1209980"
  },
  {
    "text": "has this concepts of column families and rows right and the wrote the the values",
    "start": "1209980",
    "end": "1216190"
  },
  {
    "text": "inside the row don't have to be overlapping between different rows so it's effectively just a set of nested",
    "start": "1216190",
    "end": "1222940"
  },
  {
    "text": "dictionaries that are ordered okay dynamodb thought that was a little bit too complicated for people to get their",
    "start": "1222940",
    "end": "1229000"
  },
  {
    "text": "head around I think so we're going to make it basically a list of items",
    "start": "1229000",
    "end": "1234090"
  },
  {
    "text": "documents effectively and you can pick two keys in your document one will be",
    "start": "1234090",
    "end": "1239650"
  },
  {
    "text": "the hash key and one will be the range key and the hash key is used for distributing data between nodes in",
    "start": "1239650",
    "end": "1246220"
  },
  {
    "text": "DynamoDB you know I've never seen the code for an urban area I'm just inferring this is how it works from their documentation and the range key is",
    "start": "1246220",
    "end": "1253360"
  },
  {
    "text": "used for ordering items on an individual node within DynamoDB and what this means",
    "start": "1253360",
    "end": "1258730"
  },
  {
    "text": "for user is you can't do range queries over hash keys because that would have to that would have to consult every",
    "start": "1258730",
    "end": "1264490"
  },
  {
    "text": "single node in your DynamoDB cluster it would be terribly unavailable and really slow and you can't do gets on range on",
    "start": "1264490",
    "end": "1271390"
  },
  {
    "text": "range keys so you basically have to specify a specific hash key and then a",
    "start": "1271390",
    "end": "1276610"
  },
  {
    "text": "range of you know start and an end for a range key so this is great actually we we we form our hash keys they're not",
    "start": "1276610",
    "end": "1282550"
  },
  {
    "text": "actually strings concatenated by colons there's a library for doing really cool",
    "start": "1282550",
    "end": "1288840"
  },
  {
    "text": "multi-dimensional array encoding into into bytes and go if you look at code",
    "start": "1288840",
    "end": "1293860"
  },
  {
    "text": "it's in there I didn't write it but it guarantees lexical ordering of the byte array so this means we can do things we",
    "start": "1293860",
    "end": "1300760"
  },
  {
    "text": "would put the label name the label value in the chunk ID the chunk ID we just generate randomly into the range key and",
    "start": "1300760",
    "end": "1306190"
  },
  {
    "text": "so we can we can say if you want to get all chunks for a given label and a given value we just do a range key from label",
    "start": "1306190",
    "end": "1313990"
  },
  {
    "text": "name label value 0 to label name label value you know max in type thing and that gives us all chunks in that range",
    "start": "1313990",
    "end": "1320140"
  },
  {
    "text": "and then similarly we we use this big bucket small bucket scheme so currently",
    "start": "1320140",
    "end": "1325600"
  },
  {
    "text": "I think we use our as the as the key in hash key so we always know the our we want to get the get the chunks for and",
    "start": "1325600",
    "end": "1332800"
  },
  {
    "text": "we use the metric name there have really does heavily affect how the system scales but it's again a bit more.you",
    "start": "1332800",
    "end": "1339139"
  },
  {
    "text": "satiric and we partition the first thing in all of our data stores has always the user ID and this is really the only",
    "start": "1339139",
    "end": "1345379"
  },
  {
    "text": "mention of multi-tenancy and the whole talk is you stick user ID in the primary key so that was easy",
    "start": "1345379",
    "end": "1352719"
  },
  {
    "text": "I'll come back to explain this a bit better later because it's kind of it's tricky this the the model here would",
    "start": "1352719",
    "end": "1359539"
  },
  {
    "text": "work really really well on Cassandra we just didn't want to run our own Cassandra cluster I've had too much experience doing that in the past so",
    "start": "1359539",
    "end": "1366349"
  },
  {
    "text": "yeah okay so that's the finish that's all there's all the stages in the system",
    "start": "1366349",
    "end": "1372649"
  },
  {
    "text": "there's a praying cat because I'm going to do a demo and so far every time I've tried to show it it hasn't worked",
    "start": "1372649",
    "end": "1378080"
  },
  {
    "text": "because of the Wi-Fi I'm going to say at least right then",
    "start": "1378080",
    "end": "1385148"
  },
  {
    "text": "everyone can see my screen this is this is actually we use",
    "start": "1387549",
    "end": "1392980"
  },
  {
    "text": "Prometheus obviously to monitor cortex so I'll go through this dashboard in a minute but let's",
    "start": "1392980",
    "end": "1399450"
  },
  {
    "text": "go to cloud but we if it works I'll show you the Devon stance so we",
    "start": "1399450",
    "end": "1405009"
  },
  {
    "text": "don't break quad this is this is our product we're trying to sell",
    "start": "1405009",
    "end": "1410019"
  },
  {
    "text": "the Wi-Fi is awfully slow but this shows you the topology of your application which is kind of neat",
    "start": "1410019",
    "end": "1416580"
  },
  {
    "text": "so this is the front-end components of our application and we can see what cortex looks like on this in cortex",
    "start": "1416580",
    "end": "1423070"
  },
  {
    "text": "we've got the you've got the distributor that I talked about you've",
    "start": "1423070",
    "end": "1428649"
  },
  {
    "text": "got the in gesture you've got you've got the console sorry this is out focus I might make it a little bit bigger",
    "start": "1428649",
    "end": "1435960"
  },
  {
    "text": "yeah you've got the retrieval agents that I discussed and then down here you've got things like s3 and",
    "start": "1437009",
    "end": "1444960"
  },
  {
    "text": "dynamo DB Quay dynamodb the internet memcache all we store the chunks in",
    "start": "1445049",
    "end": "1451149"
  },
  {
    "text": "memcache for speed as well anyway I'm not here to show you that I'm here to show you this",
    "start": "1451149",
    "end": "1455998"
  },
  {
    "text": "so this is the interface this looks a lot like the open-source Prometheus query interface because it is",
    "start": "1459740",
    "end": "1466940"
  },
  {
    "text": "again we just vendor this in we've added this is actually something we've added in the past few days so I don't really",
    "start": "1466940",
    "end": "1473010"
  },
  {
    "text": "know much about how this a little bit works but it's just a little banner that helps you navigate the metrics inside Prometheus so let's look at",
    "start": "1473010",
    "end": "1481100"
  },
  {
    "text": "CPU usage though this one here we let it load",
    "start": "1481100",
    "end": "1487490"
  },
  {
    "text": "no two two and a half seconds to load that that's pretty poor but this is the standard Prometheus interface and and",
    "start": "1487490",
    "end": "1493680"
  },
  {
    "text": "you know you've got the grass as well so this just shows you you'll have to trust me that this isn't actually just hitting a rule Prometheus I guess what I could",
    "start": "1493680",
    "end": "1501150"
  },
  {
    "text": "show you is if I do but we've got works in a different",
    "start": "1501150",
    "end": "1508110"
  },
  {
    "text": "window and go to prod so this one if you look is looking at dev and this one will be looking at prod",
    "start": "1508110",
    "end": "1515520"
  },
  {
    "text": "and I put the same expression in its graph and executes",
    "start": "1515520",
    "end": "1522350"
  },
  {
    "text": "and doesn't work fantastic oh no they go so if you look",
    "start": "1522350",
    "end": "1527910"
  },
  {
    "text": "between these two things hopefully I've demonstrated that this is really multi-talented because it's given you",
    "start": "1527910",
    "end": "1532940"
  },
  {
    "text": "different data for the same query for different users haha",
    "start": "1532940",
    "end": "1538340"
  },
  {
    "text": "so yeah so that basically you know it looks just like it looks just like Prometheus and and that's by design",
    "start": "1538340",
    "end": "1544410"
  },
  {
    "text": "because we think prometheus is awesome this little widget up here is really cool David wrote this recently but it",
    "start": "1544410",
    "end": "1549720"
  },
  {
    "text": "basically splits the metric names on underscore and then builds a little tree out of them and so you can say oh I'm really interested in cortex metrics and",
    "start": "1549720",
    "end": "1557010"
  },
  {
    "text": "I'm really interested in the in gesture and I want to know stuff about chunks",
    "start": "1557010",
    "end": "1562530"
  },
  {
    "text": "and I want to know stuff about chunk age and it's got it in seconds I want it's a histogram and we don't help you with",
    "start": "1562530",
    "end": "1568050"
  },
  {
    "text": "that but let's just do a count and do a rate of this",
    "start": "1568050",
    "end": "1574070"
  },
  {
    "text": "I make this fullscreen again there you go",
    "start": "1574070",
    "end": "1581230"
  },
  {
    "text": "that clue is a bit quicker you know normal query query latency on this thing has been about you know in the order of",
    "start": "1581580",
    "end": "1587130"
  },
  {
    "text": "maybe 500 milliseconds we haven't done any optimization of the query path right now we've just been focused on the right",
    "start": "1587130",
    "end": "1592590"
  },
  {
    "text": "path but yeah that's the system and it works oh",
    "start": "1592590",
    "end": "1597740"
  },
  {
    "text": "I was going to show you like how to install it we've got this cool installer that installs the node exporter and the",
    "start": "1597740",
    "end": "1602910"
  },
  {
    "text": "and Prometheus and pointed at the cluster but there's on our website this is a dashboard that we used to monitor",
    "start": "1602910",
    "end": "1609810"
  },
  {
    "text": "it so I talked about how we've got chunks and the ingest has build these chunks so up here we've got a total",
    "start": "1609810",
    "end": "1615630"
  },
  {
    "text": "number of chunks in the system and you see basically because we flush every hour and because we released this version at the same time as everything",
    "start": "1615630",
    "end": "1622200"
  },
  {
    "text": "else every hour you basically see this huge lump as we start flushing and duplicating all these chunks and you see",
    "start": "1622200",
    "end": "1627780"
  },
  {
    "text": "this down here is the length of the queue for the flushing you see this this go huge and then we get lots of errors",
    "start": "1627780",
    "end": "1633660"
  },
  {
    "text": "from DynamoDB and and then we've got up here we've got the number of chunks per series so this should basically be always one because the minute a chunk",
    "start": "1633660",
    "end": "1640470"
  },
  {
    "text": "gets closed because it's either old or full we try and flush it but again every hour you see that's that spikes up and",
    "start": "1640470",
    "end": "1646260"
  },
  {
    "text": "slowly dribbles down and then you know Bob's along nicely we've got some distributions here that I recently",
    "start": "1646260",
    "end": "1651840"
  },
  {
    "text": "discovered a completely wrong but basically tell you how full the chunks are when we flush them so most chunks",
    "start": "1651840",
    "end": "1658020"
  },
  {
    "text": "are completely full when we flush them which is great that's nice and efficient how old they are I was originally trying",
    "start": "1658020",
    "end": "1663030"
  },
  {
    "text": "to design the system so there'd be one hour old but actually for various reasons because we're not using the most",
    "start": "1663030",
    "end": "1668280"
  },
  {
    "text": "efficient encoding and because we're pushing much more data into the system than we thought we were the chunks tend",
    "start": "1668280",
    "end": "1673860"
  },
  {
    "text": "to be about 25 minutes old and how many entries in them they tend to have 300 entries in",
    "start": "1673860",
    "end": "1679820"
  },
  {
    "text": "okay the presentation",
    "start": "1679820",
    "end": "1684740"
  },
  {
    "text": "that's the demo really evaluation did this work well this is I",
    "start": "1688760",
    "end": "1694220"
  },
  {
    "text": "built this proper graph earlier today this shows distributor latency so when you send a chunk into the system how",
    "start": "1694220",
    "end": "1701240"
  },
  {
    "text": "longs it take to process and store that chunk and the 99th percentile is a hundred and twenty milliseconds so I'm actually pretty happy with that",
    "start": "1701240",
    "end": "1707950"
  },
  {
    "text": "the this is the 50th which is 60 milliseconds and like the average it no",
    "start": "1707950",
    "end": "1714470"
  },
  {
    "text": "that must be the average this is the 50th down here the the interesting thing here is this is only ever doing pretty",
    "start": "1714470",
    "end": "1720230"
  },
  {
    "text": "much in memory operations so why does it take 120 milliseconds to do in memory operations right and if you look at the",
    "start": "1720230",
    "end": "1725929"
  },
  {
    "text": "individual in jester's the latency for a chunk append for a batch of samples append it is actually you know six or",
    "start": "1725929",
    "end": "1732770"
  },
  {
    "text": "seven milliseconds so most of the time I think is being spent a because we're writing to all three and jester's when",
    "start": "1732770",
    "end": "1739340"
  },
  {
    "text": "really we only need to kind of write to return and let the third one kind of kind of go along later but also like the",
    "start": "1739340",
    "end": "1745970"
  },
  {
    "text": "that calculation I was saying in the ring when you need to try and work out the replicas set I think is quite expensive and I think we could probably",
    "start": "1745970",
    "end": "1752080"
  },
  {
    "text": "improve that algorithmically so it wasn't so expensive queries so this oh yeah so to give you",
    "start": "1752080",
    "end": "1758780"
  },
  {
    "text": "some context this was on a system with about a million series in it",
    "start": "1758780",
    "end": "1763870"
  },
  {
    "text": "this was about a hundred thousand samples a second being added to the series so a single Prometheus could do",
    "start": "1763870",
    "end": "1769970"
  },
  {
    "text": "this just as well to be brutally honest but this was only running on six nodes or five nodes so it's not a particularly",
    "start": "1769970",
    "end": "1776090"
  },
  {
    "text": "large system and yeah about a hundred milliseconds hundred twenty milliseconds at the 99th percentile queries is a",
    "start": "1776090",
    "end": "1782630"
  },
  {
    "text": "different story right now my next job is to optimize the hell out of this but the",
    "start": "1782630",
    "end": "1788390"
  },
  {
    "text": "peak of this is at a hundred milliseconds so that's not too bad I guess and down here this is generally I",
    "start": "1788390",
    "end": "1794870"
  },
  {
    "text": "think when people just weren't using it and so hundred milliseconds for queriers is probably probably reasonable the big",
    "start": "1794870",
    "end": "1802760"
  },
  {
    "text": "problem actually is just the queries generate quite a lot of data they have to fetch quite a lot of data they have to process quite a lot of data and the response contains quite a lot of data so",
    "start": "1802760",
    "end": "1809600"
  },
  {
    "text": "when this goes through the horrible author thing I was the authenticating front-end I was referring to earlier",
    "start": "1809600",
    "end": "1814760"
  },
  {
    "text": "this actually kind of gets delayed by a couple hundred milliseconds as we copy massive blobs of JSON around and so I'm",
    "start": "1814760",
    "end": "1821360"
  },
  {
    "text": "hoping to optimize the hell out of that as well we're seeing that across all of the service in we cloud",
    "start": "1821360",
    "end": "1827890"
  },
  {
    "text": "why not just run my own Prometheus well we do right that's how we monitor cortex and you probably should as well",
    "start": "1828400",
    "end": "1836440"
  },
  {
    "text": "prometheus is as I said one of the best pieces of software out there Prometheus intentionally has a very different story",
    "start": "1836440",
    "end": "1842810"
  },
  {
    "text": "around high availability to cortex so the story in prometheus is run - and then you've got the most reliable system",
    "start": "1842810",
    "end": "1849020"
  },
  {
    "text": "you ever will possibly want and that's great that works really well and that's that's how Google operate that's how we operate and that's how most people",
    "start": "1849020",
    "end": "1854840"
  },
  {
    "text": "should operate there are some downsides to running your own Prometheus if you do have a failure you might get a few gaps",
    "start": "1854840",
    "end": "1860660"
  },
  {
    "text": "in the graphs although that should be too hard to fix one of the other things like you might want to you know share",
    "start": "1860660",
    "end": "1866240"
  },
  {
    "text": "the access to the Prometheus you know we'll probably do some - boarding that you might want to share in the future so",
    "start": "1866240",
    "end": "1871250"
  },
  {
    "text": "you need access control you need authentication you know we've cloud does all of that that's kind of what we hope you'll pay",
    "start": "1871250",
    "end": "1877220"
  },
  {
    "text": "for Prometheus you know is not trying to be everything to everyone so rightfully isn't doing that and the other nice",
    "start": "1877220",
    "end": "1884180"
  },
  {
    "text": "thing about this design that really contrasts it with prometheus is we have",
    "start": "1884180",
    "end": "1889490"
  },
  {
    "text": "virtually infinite retention of your data because we're not actually storing it s3 + DynamoDB are storing it and that",
    "start": "1889490",
    "end": "1895880"
  },
  {
    "text": "they are incredibly reliable systems you know s3 seems to serve most internet sites on the web now most web sites on",
    "start": "1895880",
    "end": "1902210"
  },
  {
    "text": "the internet now so this is kind of maybe the biggest difference between Prometheus and cortex is data attention",
    "start": "1902210",
    "end": "1909640"
  },
  {
    "text": "prometheus is designed to run on a single machine designed to be super reliable it is designed not to depend on",
    "start": "1909640",
    "end": "1914900"
  },
  {
    "text": "external services and therefore is limited to how much data it can store on that machine and typically you see",
    "start": "1914900",
    "end": "1920870"
  },
  {
    "text": "people we run with about a three month data retention I'm hoping once I start working on query",
    "start": "1920870",
    "end": "1928850"
  },
  {
    "text": "optimization I can start parallelizing the query execution you know and start having a pool of query workers and and",
    "start": "1928850",
    "end": "1934340"
  },
  {
    "text": "doing tricks like that and I'm hoping that our query performance will be able to exceed that of three theaters as well",
    "start": "1934340",
    "end": "1939680"
  },
  {
    "text": "but having said that for me fifth query performance is pretty good so that's a tough target and we want to have a very",
    "start": "1939680",
    "end": "1946640"
  },
  {
    "text": "different story about high availability so not just having really good durability and retention but should like",
    "start": "1946640",
    "end": "1952370"
  },
  {
    "text": "should you know one of the availability zone in our Amazon they tend to go down we we want to have a different story and",
    "start": "1952370",
    "end": "1958589"
  },
  {
    "text": "a better story around you know not having holes in your graphs during that process there is a lot left to do this is an",
    "start": "1958589",
    "end": "1964949"
  },
  {
    "text": "open source project it's there's only two or three of us working on it so when I presented this slide in two months ago",
    "start": "1964949",
    "end": "1970949"
  },
  {
    "text": "it none of it was crossed out and since then we've we've done a whole bunch of code cleanup we've done a whole bunch of up streaming the replication between in",
    "start": "1970949",
    "end": "1978509"
  },
  {
    "text": "jesters went in about a month ago the life cycle of in jesters joining and",
    "start": "1978509",
    "end": "1983639"
  },
  {
    "text": "leaving the cluster without the cluster going down and exploding that went in about a week ago",
    "start": "1983639",
    "end": "1988940"
  },
  {
    "text": "commit log and and set for query services stuff we're going to work on next and jonno the other chap working on",
    "start": "1988940",
    "end": "1994679"
  },
  {
    "text": "this is currently implementing recording rules and alerting and to kind of round",
    "start": "1994679",
    "end": "1999809"
  },
  {
    "text": "everything off so yeah oh yeah we probably should like",
    "start": "1999809",
    "end": "2005149"
  },
  {
    "text": "figure out a way to build for it and implement rate limiting and stuff like that because right now if you sent me a million samples a second you probably",
    "start": "2005149",
    "end": "2011089"
  },
  {
    "text": "take those down wait that's me that's Tom any questions",
    "start": "2011089",
    "end": "2016279"
  },
  {
    "text": "if you want to try it out I'll put the slides online so you can copy their code of that but this is the way of kind of",
    "start": "2016279",
    "end": "2021919"
  },
  {
    "text": "getting started go and sign up put your token in here and and put this command into your Kyubey Nettie's cluster or",
    "start": "2021919",
    "end": "2027169"
  },
  {
    "text": "check the code out and tell us if it works for you so questions",
    "start": "2027169",
    "end": "2033339"
  },
  {
    "text": "so actually in the past four days the load on the system has gone up by about 10x so I very much have been frantically",
    "start": "2048720",
    "end": "2057370"
  },
  {
    "text": "scrambling around behind our booth scaling the system up to deal with that so yeah we wouldn't like the whole point",
    "start": "2057370",
    "end": "2063460"
  },
  {
    "text": "of this system is each customer doesn't get their own Prometheus because we want to have you know we hope to have",
    "start": "2063460",
    "end": "2069250"
  },
  {
    "text": "thousands of customers and we don't want to manage thousands of Prometheus's so instead of you know sharding by customer",
    "start": "2069250",
    "end": "2076389"
  },
  {
    "text": "where sharding I didn't talk about this did I I'll completely missed it we shard by customer ID and metric name and we",
    "start": "2076390",
    "end": "2083260"
  },
  {
    "text": "hashed that into the ring so the idea is this should be very horizontally scalable at multi-tenant so you know if",
    "start": "2083260",
    "end": "2089710"
  },
  {
    "text": "a customer does come along and try and us as we should hopefully just scale the system up and and survive",
    "start": "2089710",
    "end": "2096120"
  },
  {
    "text": "I mean the performant yeah the challenge isn't going to be the performance of the chunking scheme or of anything like that",
    "start": "2101940",
    "end": "2108510"
  },
  {
    "text": "the problems is our current bottleneck is is actually how quickly we can flush stuff the s3 and and how how we use",
    "start": "2108510",
    "end": "2114930"
  },
  {
    "text": "DynamoDB because DynamoDB is a statically provisioned system so we provision it right now for I think a",
    "start": "2114930",
    "end": "2120450"
  },
  {
    "text": "couple of thousand QPS and if you start exceeding that they throttle you so we're currently exceeding and that's",
    "start": "2120450",
    "end": "2127230"
  },
  {
    "text": "why all the graphs were really colourful at bottom of the screen because we're currently exceeding that and we're being throttled and I need to go and scale it up and think about ways of being",
    "start": "2127230",
    "end": "2133290"
  },
  {
    "text": "slightly more efficient in our use of DynamoDB but but the I mean I'll also yeah we'll",
    "start": "2133290",
    "end": "2138359"
  },
  {
    "text": "just scale up and up yeah we've got funding from GV so that's all being funneled to Amazon",
    "start": "2138359",
    "end": "2144410"
  },
  {
    "text": "everything we got we're being recorded another question chapter the back",
    "start": "2144410",
    "end": "2150589"
  },
  {
    "text": "yes so he said that can",
    "start": "2153170",
    "end": "2158390"
  },
  {
    "text": "yes each batch that comes in gets we run a little algorithm which decides which",
    "start": "2164190",
    "end": "2169980"
  },
  {
    "text": "sample belongs to which in gesture and then we split it up into multiple different batches and send them on so so",
    "start": "2169980",
    "end": "2176309"
  },
  {
    "text": "there is a write amplification going on there oh yes so if one ingestion is down we",
    "start": "2176309",
    "end": "2182369"
  },
  {
    "text": "actually only need you know if we're writing 2/3 which is what we currently do we only need 2 to act the right so if one in gesture is",
    "start": "2182369",
    "end": "2188970"
  },
  {
    "text": "down that's fine we can just you know that's that doesn't really particularly matter if to ingest to the down then we've got a problem but but yeah we only",
    "start": "2188970",
    "end": "2195450"
  },
  {
    "text": "need 2 acknowledgments of the right from two different ingest as before",
    "start": "2195450",
    "end": "2200569"
  },
  {
    "text": "they're both flushed them we've the data will be replicated multiple times in in dynamodb and in s3",
    "start": "2203119",
    "end": "2211010"
  },
  {
    "text": "yes so one of the things we want to do because that's through itself is replicated behind the scenes we want to",
    "start": "2217470",
    "end": "2222750"
  },
  {
    "text": "have a background process which is going along coalescing these chunks and maybe forming slightly bigger chunks to give",
    "start": "2222750",
    "end": "2228390"
  },
  {
    "text": "you better performance for larger queries as well we haven't got round to that and it's not particularly important",
    "start": "2228390",
    "end": "2233640"
  },
  {
    "text": "right now",
    "start": "2233640",
    "end": "2235818"
  },
  {
    "text": "yes 11930 bility is a lot better than cortex as availability so it's not a",
    "start": "2239660",
    "end": "2247349"
  },
  {
    "text": "limiting factor right now I mean our idea will be eventually when we're properly selling this commercially we",
    "start": "2247349",
    "end": "2253079"
  },
  {
    "text": "will run it in two different locations and you know we'll have a proper hey CheY and so on but but right now you",
    "start": "2253079",
    "end": "2258660"
  },
  {
    "text": "know it's still pretty reliable it's you know if you consider a single Prometheus node is as reliable as the machine it's",
    "start": "2258660",
    "end": "2264270"
  },
  {
    "text": "on I think we'll we will eventually want to be working in a lot more get to better reliability than Prometheus but",
    "start": "2264270",
    "end": "2270660"
  },
  {
    "text": "at the expense of much more complexity so it's going to take a lot of work to get there",
    "start": "2270660",
    "end": "2276530"
  },
  {
    "text": "thank you any more questions I think you had some questions didn't you earlier oh",
    "start": "2276530",
    "end": "2281990"
  },
  {
    "text": "sorry gone jörgen",
    "start": "2281990",
    "end": "2285920"
  },
  {
    "text": "yeah yes we did the main thing was we just didn't want to run too many big",
    "start": "2299119",
    "end": "2304859"
  },
  {
    "text": "complicated distributed systems we believe you know we're offering this as a service because we we want to use",
    "start": "2304859",
    "end": "2310859"
  },
  {
    "text": "systems that are offered as a service we want somebody else's the the operation this isn't be someone else's problem and",
    "start": "2310859",
    "end": "2316410"
  },
  {
    "text": "we're happy to pay for it on the other hand like the interfaces internally are designed such that you could swap this",
    "start": "2316410",
    "end": "2322050"
  },
  {
    "text": "out for Cassandra I mean Cassandra is really the one that we've mainly kind of",
    "start": "2322050",
    "end": "2327119"
  },
  {
    "text": "talked about but there would be nothing to stop you using big table or HDFS or maybe even react or something like that",
    "start": "2327119",
    "end": "2334099"
  },
  {
    "text": "the only interesting kind of side effect here is the chunk size is quite small so",
    "start": "2334099",
    "end": "2340680"
  },
  {
    "text": "one kilobyte is easy is small even for something like s3 and in it kind of fits",
    "start": "2340680",
    "end": "2346200"
  },
  {
    "text": "it kind of slips between the the small value databases like Sandra Hoover which",
    "start": "2346200",
    "end": "2352050"
  },
  {
    "text": "are very good at storing values of a few bytes in size and the large value stores",
    "start": "2352050",
    "end": "2357329"
  },
  {
    "text": "like s3 because that's just an easy problem and there's kind of a gap around the one kilobyte Park of like what do",
    "start": "2357329",
    "end": "2363119"
  },
  {
    "text": "you do with one kilobytes they're a little bit too big to put in Cassandra you probably just put me into a little bit too big you know you'd have to tune",
    "start": "2363119",
    "end": "2369089"
  },
  {
    "text": "your compaction settings and they're a little bit small for something like cluster or HDFS or so on so so that's",
    "start": "2369089",
    "end": "2375839"
  },
  {
    "text": "kind of an interesting problem I think you know hopefully the coalescing system that I was referring to when we get round to writing that will allow us to",
    "start": "2375839",
    "end": "2381750"
  },
  {
    "text": "build much larger chunks and do some DJ ping and that will maybe make it more applicable for something like HDFS or",
    "start": "2381750",
    "end": "2387869"
  },
  {
    "text": "Gloucester or something",
    "start": "2387869",
    "end": "2390588"
  },
  {
    "text": "you know what 99% of us stuff hits our mone Cassius most people just query the last hour of",
    "start": "2394090",
    "end": "2401090"
  },
  {
    "text": "data so so we the actual read QPS from s3 is is",
    "start": "2401090",
    "end": "2406510"
  },
  {
    "text": "excruciating lilo we also do that because it makes it more",
    "start": "2406510",
    "end": "2411740"
  },
  {
    "text": "cost effective because we don't have to pay if we if we get a hit in memcache the right performance for s3 is is not",
    "start": "2411740",
    "end": "2418430"
  },
  {
    "text": "as nice as we like and it has some weird characteristics you you you can kind of tell what it's doing them behind the",
    "start": "2418430",
    "end": "2423860"
  },
  {
    "text": "scenes and it's clearly like forming ranges of keys and splitting them off and and giving them to other machines",
    "start": "2423860",
    "end": "2430040"
  },
  {
    "text": "because you see really weird step changes in the latency of s3 as you're running the system but once you you know",
    "start": "2430040",
    "end": "2436160"
  },
  {
    "text": "if you tweak your the name you give your keys so if you give your keys in s3 like nice random distributions then yeah you",
    "start": "2436160",
    "end": "2442310"
  },
  {
    "text": "actually behaves quite nicely I must be enough for five minutes right left",
    "start": "2442310",
    "end": "2448510"
  },
  {
    "text": "any more questions okay cool well please try it out and we",
    "start": "2448510",
    "end": "2455960"
  },
  {
    "text": "as we've works have offices in three places so you know we're hiring we've got office in San Francisco Berlin and I",
    "start": "2455960",
    "end": "2462380"
  },
  {
    "text": "work in London so yeah if you want a job working on cortex then come and work for us thanks very much",
    "start": "2462380",
    "end": "2468560"
  },
  {
    "text": "[Applause]",
    "start": "2468560",
    "end": "2473199"
  }
]