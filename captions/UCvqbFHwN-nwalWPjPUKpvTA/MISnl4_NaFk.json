[
  {
    "text": "hi guys uh thanks for being here so this is the last session of uh today's agenda",
    "start": "320",
    "end": "6480"
  },
  {
    "text": "and you guys are the true friend of the open source community so my name is Eric",
    "start": "6480",
    "end": "12360"
  },
  {
    "text": "I'm from pensl so if you have attended the keyote uh this morning you guys may",
    "start": "12360",
    "end": "18119"
  },
  {
    "text": "know uh my colleague fog uh she has introduced B ml uh as a product like",
    "start": "18119",
    "end": "25320"
  },
  {
    "text": "what we are offering today and for today's uh session that uh I will try to",
    "start": "25320",
    "end": "31240"
  },
  {
    "text": "uh bring take you guys back and uh a little bit history of where we came",
    "start": "31240",
    "end": "37680"
  },
  {
    "text": "from so a little bit of myself so I'm Eric I'm helping bent ml expansion in",
    "start": "37680",
    "end": "44760"
  },
  {
    "text": "the APAC region and also in charge of the global Partnerships in the meanwhile I'm a True Believer of Open Source so",
    "start": "44760",
    "end": "52199"
  },
  {
    "text": "I'm involved in the cop flow Community specific uh working with st uh on the",
    "start": "52199",
    "end": "57280"
  },
  {
    "text": "cas serve the serving uh work group so before uh been to ml I was with",
    "start": "57280",
    "end": "62840"
  },
  {
    "text": "cloudfare for six years uh I helped cloudfare establish uh their China",
    "start": "62840",
    "end": "68360"
  },
  {
    "text": "business and before that I attend K Mullen uh University so this is my",
    "start": "68360",
    "end": "73600"
  },
  {
    "text": "WeChat and my Linkin feel free to add me cool so um uh we are let's take back",
    "start": "73600",
    "end": "82400"
  },
  {
    "text": "to uh 20 meet uh 2010 like 2015 2016 and",
    "start": "82400",
    "end": "88720"
  },
  {
    "text": "uh uh our CEO uh and uh co-founder uh CH",
    "start": "88720",
    "end": "94000"
  },
  {
    "text": "yuang he's a early data breakes engineer he's uh the first uh part-time uh PM of",
    "start": "94000",
    "end": "100759"
  },
  {
    "text": "the ml flow uh project uh when he helped uh his customers for example like rare",
    "start": "100759",
    "end": "106960"
  },
  {
    "text": "games and Capital One uh back then uh he disovered there are a lot of um",
    "start": "106960",
    "end": "114119"
  },
  {
    "text": "conflicts uh if you want to take your machine learning models uh into uh",
    "start": "114119",
    "end": "120200"
  },
  {
    "text": "production for example there are different personas and different uh uh",
    "start": "120200",
    "end": "125240"
  },
  {
    "text": "roles involved into this uh process for example you have your data scientists uh",
    "start": "125240",
    "end": "132720"
  },
  {
    "text": "want to train the models but before that you need data Engineers to do all the",
    "start": "132720",
    "end": "138160"
  },
  {
    "text": "ETL stuff to clean the data and after that uh the data scientist needs to ask",
    "start": "138160",
    "end": "143680"
  },
  {
    "text": "the M engineer to uh take the models into uh a cable we service API and then",
    "start": "143680",
    "end": "152239"
  },
  {
    "text": "the ml Engineers need to talk to the devops engineer to make sure the service is reliable and scalable and in the",
    "start": "152239",
    "end": "158800"
  },
  {
    "text": "meantime the PM wants to access uh uh the datas of the machine learning",
    "start": "158800",
    "end": "165000"
  },
  {
    "text": "Services uh and in the meantime there are lot of moving parts of the service",
    "start": "165000",
    "end": "170200"
  },
  {
    "text": "for example uh some of the the bat um uh jobs needs to be integrated with uh the",
    "start": "170200",
    "end": "178440"
  },
  {
    "text": "batch Pipeline and also in terms of the cicd stuff the machine learning is a",
    "start": "178440",
    "end": "184120"
  },
  {
    "text": "little bit different it's not just like uh code and it also has data and models",
    "start": "184120",
    "end": "189680"
  },
  {
    "text": "it it's a little bit of chaos even for rare games and Capital One uh they have",
    "start": "189680",
    "end": "195640"
  },
  {
    "text": "uh 20 to 30 Engineers within their ml organizations is still hard they need",
    "start": "195640",
    "end": "201640"
  },
  {
    "text": "help from datab braas and uh experienced uh engineers and PM like our founder so",
    "start": "201640",
    "end": "209280"
  },
  {
    "text": "uh back to like uh 2016 and 2015 uh there",
    "start": "209280",
    "end": "214920"
  },
  {
    "text": "is a new uh paradigms or New Concept called ml Ops and MLS uh is trying to",
    "start": "214920",
    "end": "223080"
  },
  {
    "text": "address the issue that uh uh Chu experience with uh uh his uh users like",
    "start": "223080",
    "end": "230760"
  },
  {
    "text": "rare games and Capital One they want to help to make sure the machine learning assets are treated as uh other software",
    "start": "230760",
    "end": "238360"
  },
  {
    "text": "assets for example like code uh within a cicd environment so in order to do that",
    "start": "238360",
    "end": "245480"
  },
  {
    "text": "uh there are different kind of tools Concepts and also steps that got",
    "start": "245480",
    "end": "251599"
  },
  {
    "text": "invented uh during that period and the most importantly uh those tools and",
    "start": "251599",
    "end": "257600"
  },
  {
    "text": "Concepts want to achieve is trying to uh make the ass machine learning assess",
    "start": "257600",
    "end": "264400"
  },
  {
    "text": "versionable uh testable and also make sure that you can uh automate the",
    "start": "264400",
    "end": "271199"
  },
  {
    "text": "process and make it reproducible so uh after that uh Chou",
    "start": "271199",
    "end": "277600"
  },
  {
    "text": "our co-founder want to focus on uh model inference So within the model inference",
    "start": "277600",
    "end": "283000"
  },
  {
    "text": "uh there are not only the model is only small part of uh the process you also",
    "start": "283000",
    "end": "289400"
  },
  {
    "text": "need to figure out how to do your pre-processing of the data and also post",
    "start": "289400",
    "end": "295560"
  },
  {
    "text": "processing of your data so if you you are using uh streaming architecture you",
    "start": "295560",
    "end": "301199"
  },
  {
    "text": "also need to figure out how to do the uh featur Transformations and also if you",
    "start": "301199",
    "end": "306520"
  },
  {
    "text": "have a lot of different business logic you need to know how to incorporate your uh business logic into the model",
    "start": "306520",
    "end": "313400"
  },
  {
    "text": "inference process so uh when we start the project we interview different uh",
    "start": "313400",
    "end": "320360"
  },
  {
    "text": "users uh using different solutions and every companies has their own implementations and we divided into this",
    "start": "320360",
    "end": "328440"
  },
  {
    "text": "uh implementation in into two Dimensions the First Dimension is is of use and the",
    "start": "328440",
    "end": "333720"
  },
  {
    "text": "second dimension is flexibilities so in terms of is of use the most",
    "start": "333720",
    "end": "339039"
  },
  {
    "text": "straightforward Solutions is the offshelf solutions that offered by the",
    "start": "339039",
    "end": "344360"
  },
  {
    "text": "machine learning training framework for example like a tensor flow has transor flow serve and uh uh P torch has torch",
    "start": "344360",
    "end": "352479"
  },
  {
    "text": "serve and uh uh even for NVIDIA they have the Triton inference uh servers so",
    "start": "352479",
    "end": "359240"
  },
  {
    "text": "the the the advantage of this kind of offshelf solutions is uh uh it can",
    "start": "359240",
    "end": "366520"
  },
  {
    "text": "easily get used or get started by a solo data scientist and also because uh those",
    "start": "366520",
    "end": "373400"
  },
  {
    "text": "uh for example 10 Sur flow serving is specialized to serve the TF flow runtime",
    "start": "373400",
    "end": "379759"
  },
  {
    "text": "you can have uh a comparably better uh serving results but the questions come",
    "start": "379759",
    "end": "386680"
  },
  {
    "text": "to the the the disadvantage of this uh offsh Solutions specifically is uh when",
    "start": "386680",
    "end": "394400"
  },
  {
    "text": "your uh machine learning organizations grow different types of uh uh machine",
    "start": "394400",
    "end": "400880"
  },
  {
    "text": "learning user cases may require different types of uh training framework and uh different of uh use cases or",
    "start": "400880",
    "end": "409400"
  },
  {
    "text": "workflows might need uh multiple models to work together so let's uh summarize",
    "start": "409400",
    "end": "417560"
  },
  {
    "text": "the disadvantage then you will know if you're experien this pain is if you use",
    "start": "417560",
    "end": "423440"
  },
  {
    "text": "only the offshelf solutions you'll need to get stuck with their configurations",
    "start": "423440",
    "end": "429599"
  },
  {
    "text": "and also it's not uh flexible to customize and uh not very easy to work",
    "start": "429599",
    "end": "435080"
  },
  {
    "text": "with different like other uh training Frameworks and another Trend that we",
    "start": "435080",
    "end": "440440"
  },
  {
    "text": "have seen in the past is like uh uh technology forward company has a",
    "start": "440440",
    "end": "447520"
  },
  {
    "text": "software engineering team and and most likely they like to build their own uh in-house Solutions so uh the advantage",
    "start": "447520",
    "end": "455160"
  },
  {
    "text": "of the inhouse solutions is very flexible and uh it can adapt to",
    "start": "455160",
    "end": "460199"
  },
  {
    "text": "different Frameworks and most likely it's going to be adapt to that specific",
    "start": "460199",
    "end": "466319"
  },
  {
    "text": "companies uh infrastructure but it comes with uh disadvantages as well for",
    "start": "466319",
    "end": "471960"
  },
  {
    "text": "example not every company especially not every non-technology company has the",
    "start": "471960",
    "end": "477240"
  },
  {
    "text": "engineering resources or can ire the engineer that who knows how to build",
    "start": "477240",
    "end": "482919"
  },
  {
    "text": "this type of infrastructure and more importantly even you have that resources",
    "start": "482919",
    "end": "488720"
  },
  {
    "text": "you will cost you at least six months to nine months to implement but uh for a CEO or executive team that's lost",
    "start": "488720",
    "end": "496520"
  },
  {
    "text": "opportunity or lost money in terms of uh those AI uh applications and also uh Mo",
    "start": "496520",
    "end": "504360"
  },
  {
    "text": "for traditional ml most of the organization starts with uh data team for example data engineers and data",
    "start": "504360",
    "end": "511279"
  },
  {
    "text": "scientists uh is a a skill set mismatch because those uh data scientists mostly",
    "start": "511279",
    "end": "518360"
  },
  {
    "text": "came from uh uh Statics or mathematics background they will need to time need",
    "start": "518360",
    "end": "524640"
  },
  {
    "text": "time to learn those uh devops or engineering uh expertise to move forward",
    "start": "524640",
    "end": "530640"
  },
  {
    "text": "so after interviewed all those users and uh all those uh use cases so this is",
    "start": "530640",
    "end": "538160"
  },
  {
    "text": "what we have dream before and uh uh most importantly is um the solution that uh",
    "start": "538160",
    "end": "546200"
  },
  {
    "text": "we see is we're trying to be uh flexible and uh easy to use the first is we",
    "start": "546200",
    "end": "553279"
  },
  {
    "text": "decide uh to use python in in instead of uh declarative yamos we felt that's",
    "start": "553279",
    "end": "560120"
  },
  {
    "text": "natural language and natural progressions for data scientist to move forward to the next step uh from",
    "start": "560120",
    "end": "566120"
  },
  {
    "text": "training to uh inference and also have uh optimized different runtime and",
    "start": "566120",
    "end": "572880"
  },
  {
    "text": "Hardware to make sure that um the workload can uh scale and also can work",
    "start": "572880",
    "end": "579680"
  },
  {
    "text": "for different framework and different environment and most likely uh and and more importantly that for development uh",
    "start": "579680",
    "end": "587040"
  },
  {
    "text": "destinations we are agostic in that part so no matter you're on different Cloud",
    "start": "587040",
    "end": "592839"
  },
  {
    "text": "providers or your on uh on promise deployment we can support and also no",
    "start": "592839",
    "end": "600240"
  },
  {
    "text": "matter you are using uh B batch inference or you want to do it online",
    "start": "600240",
    "end": "605480"
  },
  {
    "text": "inference we uh need to uh support that as well so here comes to bentto ml so we",
    "start": "605480",
    "end": "611880"
  },
  {
    "text": "are open source uh AI application framework like I introduced before uh we",
    "start": "611880",
    "end": "618519"
  },
  {
    "text": "support different of data science Clips and also ml Frameworks and this works",
    "start": "618519",
    "end": "623640"
  },
  {
    "text": "best if you have a larger or comparably larger data science setting",
    "start": "623640",
    "end": "629560"
  },
  {
    "text": "and you have various of uh uh Mo learning use cases you need pytorch you",
    "start": "629560",
    "end": "635440"
  },
  {
    "text": "need uh SK learn you need Transformers even uh we all support and also we",
    "start": "635440",
    "end": "641480"
  },
  {
    "text": "support R API and grpc and you can uh abstract your business logic uh into",
    "start": "641480",
    "end": "648760"
  },
  {
    "text": "different um uh uh implementations and into our API servers and also uh for our",
    "start": "648760",
    "end": "656800"
  },
  {
    "text": "model Runner uh we have extra trct uh as a uh to support different uh uh runtime",
    "start": "656800",
    "end": "664639"
  },
  {
    "text": "as well for example onx and Invidia Triton and uh uh for online inference",
    "start": "664639",
    "end": "670880"
  },
  {
    "text": "streaming batch scoring we all support and most likely what I want to emphasize",
    "start": "670880",
    "end": "676440"
  },
  {
    "text": "is the the powerful Runner architecture that we uh design uh for example uh if",
    "start": "676440",
    "end": "682920"
  },
  {
    "text": "you have a machine learning workflow and you have you need different models uh",
    "start": "682920",
    "end": "688800"
  },
  {
    "text": "with in the same use cases you can abtract the pre-processing and post-processing logic into the API",
    "start": "688800",
    "end": "695560"
  },
  {
    "text": "server and uh uh make sure the uh uh machine learning uh heavy uh logic uh",
    "start": "695560",
    "end": "705399"
  },
  {
    "text": "reset into the runner and also we can have a distributed way to uh distribute",
    "start": "705399",
    "end": "711800"
  },
  {
    "text": "the API server to run on a CPU clusters and those uh compute intensive uh",
    "start": "711800",
    "end": "717279"
  },
  {
    "text": "workload into the the GPU clusters uh in in the uh current environment the GPU is",
    "start": "717279",
    "end": "724680"
  },
  {
    "text": "very uh hard to get so it works well for our users so uh right now uh bentto ml is uh",
    "start": "724680",
    "end": "733760"
  },
  {
    "text": "uh 3,000 community members strong strong and we're serving like billions predictions per day uh uh over a",
    "start": "733760",
    "end": "740199"
  },
  {
    "text": "thousand of organizations uh are using us in their production environment so",
    "start": "740199",
    "end": "746199"
  },
  {
    "text": "one of uh the uh user s is that we really like is uh shingen from Porsche",
    "start": "746199",
    "end": "752800"
  },
  {
    "text": "so before uh B ml he is the only mle within his team uh if he a manager ask",
    "start": "752800",
    "end": "760040"
  },
  {
    "text": "him to uh put uh machal mod models into production you take him uh eight weeks",
    "start": "760040",
    "end": "767399"
  },
  {
    "text": "but after he use Bal ml he just need the three days so if you have a small no",
    "start": "767399",
    "end": "775000"
  },
  {
    "text": "matter if you have a small uh data scien team and you have larger this diff uh",
    "start": "775000",
    "end": "780800"
  },
  {
    "text": "larger data science in Uhl is really your uh good friend so uh in terms of",
    "start": "780800",
    "end": "788360"
  },
  {
    "text": "Asia expansions and with the open source Community you really need a champion to",
    "start": "788360",
    "end": "795000"
  },
  {
    "text": "get to a different market for example uh Mr Kim is engineering from line he",
    "start": "795000",
    "end": "802000"
  },
  {
    "text": "discovered a b ml back in 2019 and um he start to Leverage uh",
    "start": "802000",
    "end": "809040"
  },
  {
    "text": "Bento ml to build uh the uh they call ml Universe within line and after one years",
    "start": "809040",
    "end": "817519"
  },
  {
    "text": "uh development uh Beno AML are supporting at least uh three different",
    "start": "817519",
    "end": "823279"
  },
  {
    "text": "use cases uh within uh the line uh app and the line organizations and moreover",
    "start": "823279",
    "end": "830440"
  },
  {
    "text": "after a year and uh uh a half later I think we discovered that uh Mr Lee he",
    "start": "830440",
    "end": "838000"
  },
  {
    "text": "doesn't work in the same group of line because line is very big organizations",
    "start": "838000",
    "end": "843320"
  },
  {
    "text": "he works with in the line financials he's also using line uh he's also using",
    "start": "843320",
    "end": "848959"
  },
  {
    "text": "bent IML within their uh machine learning team and uh the use cases is to",
    "start": "848959",
    "end": "855320"
  },
  {
    "text": "uh calculate calculate uh credit score and with our C South Korean Community",
    "start": "855320",
    "end": "861880"
  },
  {
    "text": "growing uh we we got into a lot of uh different uh internet companies uh with",
    "start": "861880",
    "end": "868839"
  },
  {
    "text": "in South Korea and then it comes to our uh friend neighor so neighor is uh the",
    "start": "868839",
    "end": "874839"
  },
  {
    "text": "largest search engine and most uh visit website uh in Korea they have uh uh 6",
    "start": "874839",
    "end": "881360"
  },
  {
    "text": "thousands employees and uh their HQ is in uh uh the South C so and um uh in",
    "start": "881360",
    "end": "889800"
  },
  {
    "text": "terms of market cap they are also the largest uh company as well so here comes",
    "start": "889800",
    "end": "895120"
  },
  {
    "text": "to uh our friend uh Mr King him he is uh",
    "start": "895120",
    "end": "900320"
  },
  {
    "text": "bam open source uh contributors and he has prepared uh short video to discuss",
    "start": "900320",
    "end": "908519"
  },
  {
    "text": "why neighor pick UPL and there's nothing more uh more more stronger than uh the",
    "start": "908519",
    "end": "915839"
  },
  {
    "text": "sement come from uh the user themselves so please uh take a",
    "start": "915839",
    "end": "922600"
  },
  {
    "text": "look hi everyone my name is Son and I'm",
    "start": "927600",
    "end": "934560"
  },
  {
    "text": "engineer working at the team which named AI serving Dev and neighbor I'll",
    "start": "934560",
    "end": "940639"
  },
  {
    "text": "introduce B usage in my team in neighbor each team select and",
    "start": "940639",
    "end": "948920"
  },
  {
    "text": "use many framework according to their situation and my team use VL uh in this",
    "start": "948920",
    "end": "955720"
  },
  {
    "text": "section I'm going to talk about how I use BML and why I use",
    "start": "955720",
    "end": "961600"
  },
  {
    "text": "BML first BML is a simple to deploy uh we just make a depend pile j. Y and",
    "start": "961600",
    "end": "971160"
  },
  {
    "text": "execute build command and B was built",
    "start": "971160",
    "end": "976399"
  },
  {
    "text": "and we just execute command B container life that's all uh we can retrieve",
    "start": "976399",
    "end": "984079"
  },
  {
    "text": "container image which can be deployed mod serving and as you can see this",
    "start": "984079",
    "end": "989839"
  },
  {
    "text": "picture is very simple uh I think it's a really powerful picture automatic contain",
    "start": "989839",
    "end": "996319"
  },
  {
    "text": "Li uh then why does not recommend pespi or plusk uh someone say pespi is good",
    "start": "996319",
    "end": "1005800"
  },
  {
    "text": "choice to model serving but I believe it's not a good choice typically ml",
    "start": "1005800",
    "end": "1011880"
  },
  {
    "text": "serving support uh picture that MTI threading for model",
    "start": "1011880",
    "end": "1017319"
  },
  {
    "text": "importance uh or like a deploy mode model like a touch script because of",
    "start": "1017319",
    "end": "1024400"
  },
  {
    "text": "this it is more efficient only to deploy the single process impos",
    "start": "1024400",
    "end": "1031880"
  },
  {
    "text": "worker but P API it is a web framework uh it does not consider about this Marti",
    "start": "1031880",
    "end": "1040640"
  },
  {
    "text": "thread so if you serve the model with pest API without any concern uh pest API",
    "start": "1040640",
    "end": "1048280"
  },
  {
    "text": "with deploy model multiprocessing structure with Unicon or",
    "start": "1048280",
    "end": "1054160"
  },
  {
    "text": "gicon but P BML is mod deserving framework uh BML use only one process",
    "start": "1054160",
    "end": "1062240"
  },
  {
    "text": "during development mode but when the production mode B will deploy inference",
    "start": "1062240",
    "end": "1068760"
  },
  {
    "text": "worker AKA lunner as a single process uh and Runner communicate with the multi uh",
    "start": "1068760",
    "end": "1076960"
  },
  {
    "text": "API server with Pok multiprocess of course you can config",
    "start": "1076960",
    "end": "1082280"
  },
  {
    "text": "this option uh you can spawn more importance worker process just modify",
    "start": "1082280",
    "end": "1087640"
  },
  {
    "text": "compete option uh especially need when deploy lunner with multi GPU uh you can",
    "start": "1087640",
    "end": "1094520"
  },
  {
    "text": "see the more detail in this document section resource scheduling",
    "start": "1094520",
    "end": "1101240"
  },
  {
    "text": "strategy when you're serving lightweight uh model uh the overhead from process to",
    "start": "1101240",
    "end": "1107400"
  },
  {
    "text": "process communication can much slower than model's imper calculation um in this case",
    "start": "1107400",
    "end": "1114720"
  },
  {
    "text": "multiprocessing architecture can be more past uh if you just set eded through",
    "start": "1114720",
    "end": "1120280"
  },
  {
    "text": "option to Runner uh you can you can easily uh switch the",
    "start": "1120280",
    "end": "1126760"
  },
  {
    "text": "deployment strateg I also use this strateg per some",
    "start": "1126760",
    "end": "1132290"
  },
  {
    "text": "[Music] model data distributed importance um",
    "start": "1132290",
    "end": "1137720"
  },
  {
    "text": "this is strage in verb Bond two or more same Runner and then distribute imper",
    "start": "1137720",
    "end": "1145120"
  },
  {
    "text": "request between them this strateg has more efficient in kubernetes with like",
    "start": "1145120",
    "end": "1151600"
  },
  {
    "text": "yti each Runner is deployment in independently uh this approach approach",
    "start": "1151600",
    "end": "1159520"
  },
  {
    "text": "can effic efficiently improve the performance of The Mod server especially",
    "start": "1159520",
    "end": "1166159"
  },
  {
    "text": "for the importance request with r b s uh the data distributed learner looks",
    "start": "1166159",
    "end": "1173159"
  },
  {
    "text": "like this picture uh when request with low size 200 uh each two runner",
    "start": "1173159",
    "end": "1182159"
  },
  {
    "text": "calculate 100 um and if you want more boost latency uh we just spawn more",
    "start": "1182159",
    "end": "1189240"
  },
  {
    "text": "Runner and distribute imprint batch size when you use this strates with BML there",
    "start": "1189240",
    "end": "1197000"
  },
  {
    "text": "is only few Cod change that's",
    "start": "1197000",
    "end": "1201600"
  },
  {
    "text": "it uh if you manag serving now uh you",
    "start": "1202240",
    "end": "1207600"
  },
  {
    "text": "know that uh about the almost import request is not kind it's uh not pretty",
    "start": "1207600",
    "end": "1213919"
  },
  {
    "text": "tensor uh there is a many request perment there is uh there is that means",
    "start": "1213919",
    "end": "1222039"
  },
  {
    "text": "we need to preprocessing before model imps uh in this case can make easy to",
    "start": "1222039",
    "end": "1229799"
  },
  {
    "text": "pre-processing logic with P python um let's compare the deployment",
    "start": "1229799",
    "end": "1237679"
  },
  {
    "text": "strategy with vml and other framework first level one there is a two way W",
    "start": "1237679",
    "end": "1246120"
  },
  {
    "text": "serving or invaded learner band serving it's a simple and same uh and next level",
    "start": "1246120",
    "end": "1253400"
  },
  {
    "text": "two uh you need to switch framework into Tron or tens PL serving or touch serving",
    "start": "1253400",
    "end": "1261120"
  },
  {
    "text": "but B also support this level and level three uh this Rebel",
    "start": "1261120",
    "end": "1267720"
  },
  {
    "text": "added pre-processing or pcture store Connection in this case you need to pass",
    "start": "1267720",
    "end": "1273320"
  },
  {
    "text": "the IPI or other server to with Pro pre-processing logic but BML still",
    "start": "1273320",
    "end": "1279640"
  },
  {
    "text": "supports this level level Po in this case you would to use mod",
    "start": "1279640",
    "end": "1287520"
  },
  {
    "text": "serving PL platform ker like a ker ker person two Transformer made by past API",
    "start": "1287520",
    "end": "1295679"
  },
  {
    "text": "and Pento ml can deploy with yaai um and",
    "start": "1295679",
    "end": "1301520"
  },
  {
    "text": "when you when you install Bento ml Tron extra option uh you can switch the",
    "start": "1301520",
    "end": "1307480"
  },
  {
    "text": "runner to Tron impros server uh PML and ton impress",
    "start": "1307480",
    "end": "1314919"
  },
  {
    "text": "server is not a replacement for each other uh they have coworker relationship",
    "start": "1314919",
    "end": "1322919"
  },
  {
    "text": "uh and our teams are current use level three that's all uh in this section I",
    "start": "1322919",
    "end": "1331480"
  },
  {
    "text": "talk about why we use bandl and how we use bandl thank you for watching this",
    "start": "1331480",
    "end": "1338880"
  },
  {
    "text": "section my name is Kim and it's been a pleasure sharing my",
    "start": "1338880",
    "end": "1345360"
  },
  {
    "text": "usage uh have a good conference",
    "start": "1345360",
    "end": "1349960"
  },
  {
    "text": "bye hi everyone my name is",
    "start": "1352000",
    "end": "1356400"
  },
  {
    "text": "Son and I'm okay so let's continue um uh here's",
    "start": "1358760",
    "end": "1366799"
  },
  {
    "text": "a little bit of recap like uh deal production scale model serving uh Solutions first it should be uh uh used",
    "start": "1366799",
    "end": "1375880"
  },
  {
    "text": "to customize business logic and also so infrastruct uh inference graph uh",
    "start": "1375880",
    "end": "1381240"
  },
  {
    "text": "preferably in Python which is uh uh the data scientist uh natural programming",
    "start": "1381240",
    "end": "1386440"
  },
  {
    "text": "language and also uh ideal solution should be optimized uh the runtime and",
    "start": "1386440",
    "end": "1392039"
  },
  {
    "text": "Hardwares to allow uh independent autoscaling so that's why we created the",
    "start": "1392039",
    "end": "1397600"
  },
  {
    "text": "runner architecture and the API server so last a must support different deploy",
    "start": "1397600",
    "end": "1403919"
  },
  {
    "text": "environment without writing uh single lines of the code",
    "start": "1403919",
    "end": "1409679"
  },
  {
    "text": "cool so we uh have talked a little bit on the traditional ML and now it's 2023",
    "start": "1409679",
    "end": "1416080"
  },
  {
    "text": "and what's coming next so uh I think uh we all know with the TR gbt uh release",
    "start": "1416080",
    "end": "1422919"
  },
  {
    "text": "last uh November and there are a lot of uh movement on the foundation models especially on the large land model side",
    "start": "1422919",
    "end": "1431279"
  },
  {
    "text": "uh the rice of Open Source uh uh large language models are really a phenomenon",
    "start": "1431279",
    "end": "1436919"
  },
  {
    "text": "so we have uh llama Falcon uh even llama 2 uh on the in the US or outside of",
    "start": "1436919",
    "end": "1444559"
  },
  {
    "text": "China and also within China we have chm we have bon and I think uh uh the the",
    "start": "1444559",
    "end": "1452760"
  },
  {
    "text": "Alibaba team release chanin uh open source models uh as well however uh same",
    "start": "1452760",
    "end": "1459400"
  },
  {
    "text": "with uh the traditional ml there are different challenges uh for the developers to productionize large enry",
    "start": "1459400",
    "end": "1466520"
  },
  {
    "text": "models the first is about the large language models quality is is good",
    "start": "1466520",
    "end": "1471679"
  },
  {
    "text": "enough to generate uh good response and uh we have seen uh different use cases",
    "start": "1471679",
    "end": "1478600"
  },
  {
    "text": "especially for fine twin vertical models it actually uh is comparable with uh GPT",
    "start": "1478600",
    "end": "1485640"
  },
  {
    "text": "uh 3.5 or even GPT 4 in some some cases and the second is operability uh is the",
    "start": "1485640",
    "end": "1493360"
  },
  {
    "text": "large r model can be uh integrated with the existing uh infrastructures and also your",
    "start": "1493360",
    "end": "1500720"
  },
  {
    "text": "especially Hardware uh GPU environment and the third is the throughput uh in",
    "start": "1500720",
    "end": "1506480"
  },
  {
    "text": "terms of concurrencies uh can you guys or or can the uh engineering team to",
    "start": "1506480",
    "end": "1513080"
  },
  {
    "text": "produce a good enough throughput to uh lower the cost and also get a better uh",
    "start": "1513080",
    "end": "1519600"
  },
  {
    "text": "response and the fourth is uh latency uh for API cost and is this good enough uh",
    "start": "1519600",
    "end": "1526600"
  },
  {
    "text": "to uh human users to for the first several responses and tokens is this",
    "start": "1526600",
    "end": "1532399"
  },
  {
    "text": "good enough uh for the real users and more importantly uh for uh large shry",
    "start": "1532399",
    "end": "1539240"
  },
  {
    "text": "models if I want to put into our Productions uh can I afford the cost so here is openlm openl is an open",
    "start": "1539240",
    "end": "1548520"
  },
  {
    "text": "source platform that we design to facilitate the deployment and operations of large language models so in order to",
    "start": "1548520",
    "end": "1555679"
  },
  {
    "text": "address those four pillars said I discussed before uh we support almost",
    "start": "1555679",
    "end": "1560760"
  },
  {
    "text": "all open source uh in terms of quality wise uh we support in terms of all the",
    "start": "1560760",
    "end": "1566399"
  },
  {
    "text": "open source large lry models and for op probabilities uh we support quantizations you can do uh even AB",
    "start": "1566399",
    "end": "1574679"
  },
  {
    "text": "chronization with open LM and also we support uh model parallelism as well for",
    "start": "1574679",
    "end": "1580399"
  },
  {
    "text": "throughput we have a GPU specific optimizations uh especially with Cuda and also uh we uh have support",
    "start": "1580399",
    "end": "1588200"
  },
  {
    "text": "continuous batching uh to increase throughput as well and in what we use is",
    "start": "1588200",
    "end": "1594159"
  },
  {
    "text": "a token streaming to uh address uh the license issue of the production uh",
    "start": "1594159",
    "end": "1600000"
  },
  {
    "text": "larger language model so uh with production lary models the ml Ops",
    "start": "1600000",
    "end": "1605600"
  },
  {
    "text": "challenges didn't R away so that's why within uh openlm openlm all the",
    "start": "1605600",
    "end": "1612600"
  },
  {
    "text": "advantages that you have seen that I have talked about with bent ml are uh",
    "start": "1612600",
    "end": "1618360"
  },
  {
    "text": "fully baked in for example uh it's customizable with python you can Version",
    "start": "1618360",
    "end": "1623640"
  },
  {
    "text": "Control the Bandos and the models uh is all oci compli uh compliant and also uh",
    "start": "1623640",
    "end": "1630120"
  },
  {
    "text": "it support realtime inference and batch INF uh inference you can uh switch uh",
    "start": "1630120",
    "end": "1636640"
  },
  {
    "text": "different uh uh deployment destinations without changing uh the code so lastly",
    "start": "1636640",
    "end": "1643440"
  },
  {
    "text": "uh we have a build uh a a platform as service called bental uh Cloud uh this",
    "start": "1643440",
    "end": "1649640"
  },
  {
    "text": "is uh a serverless uh uh product uh of",
    "start": "1649640",
    "end": "1654919"
  },
  {
    "text": "of uh the company and um it can be uh scaled to zero and it can scale up and",
    "start": "1654919",
    "end": "1661679"
  },
  {
    "text": "it support all the good things with uh Bento ml uh as well more importantly is",
    "start": "1661679",
    "end": "1668080"
  },
  {
    "text": "uh uh distribute the deployment versions and is currently in private beta please",
    "start": "1668080",
    "end": "1674799"
  },
  {
    "text": "feel free to sign up on whe list and and uh that will be all thank you guys",
    "start": "1674799",
    "end": "1680080"
  },
  {
    "text": "please find B ML and op op LM on GitHub and Stardust and at the bottom is uh my",
    "start": "1680080",
    "end": "1687559"
  },
  {
    "text": "WeChat I'm linking and uh right now uh we're open to",
    "start": "1687559",
    "end": "1693399"
  },
  {
    "text": "questions",
    "start": "1706440",
    "end": "1709440"
  },
  {
    "text": "traditional ml the person data scientist machine learning",
    "start": "1729840",
    "end": "1736200"
  },
  {
    "text": "engineer",
    "start": "1736399",
    "end": "1739399"
  },
  {
    "text": "Focus the engineer",
    "start": "1765600",
    "end": "1769320"
  },
  {
    "text": "dat scientist web",
    "start": "1776600",
    "end": "1782720"
  },
  {
    "text": "service",
    "start": "1796320",
    "end": "1799320"
  },
  {
    "text": "for",
    "start": "1802240",
    "end": "1805240"
  }
]