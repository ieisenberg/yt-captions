[
  {
    "start": "0",
    "end": "57000"
  },
  {
    "text": "hello everyone welcome to kubecon welcome to my office and welcome to my talk on cluster",
    "start": "560",
    "end": "6399"
  },
  {
    "text": "reconciliation and how to manage multiple clusters uh my name is valerie lancy i've been",
    "start": "6399",
    "end": "12639"
  },
  {
    "text": "involved in kubernetes for a handful of years now kubecon 2017 if",
    "start": "12639",
    "end": "17680"
  },
  {
    "text": "you recognize this was my first time attending and i have been tilting at the problem",
    "start": "17680",
    "end": "25119"
  },
  {
    "text": "of how to handle multiple clusters for most of that time starting from very early into my",
    "start": "25119",
    "end": "31519"
  },
  {
    "text": "experience with kubernetes aside from all the problems that come with trying to you know jam a stateful app that was always",
    "start": "31519",
    "end": "37760"
  },
  {
    "text": "meant to run one single vm forever into a cluster i quickly encountered a lot of problems",
    "start": "37760",
    "end": "43200"
  },
  {
    "text": "with how to run you know a redundant geo-distributed system in criminalized effectively and",
    "start": "43200",
    "end": "48640"
  },
  {
    "text": "there's been some evolution that i'll talk about but it's still something that's already bespoke and",
    "start": "48640",
    "end": "54000"
  },
  {
    "text": "there's a lot of way to go so first off i want to talk about why we",
    "start": "54000",
    "end": "61280"
  },
  {
    "start": "57000",
    "end": "57000"
  },
  {
    "text": "want to use multi-cluster at all there's roughly four reasons that i usually kind of see",
    "start": "61280",
    "end": "67200"
  },
  {
    "text": "identified first one is regional points of presence kubernetes clusters don't span multiple",
    "start": "67200",
    "end": "75520"
  },
  {
    "text": "regions or data centers very well there's an expectation that's very low latency between the components there's a",
    "start": "75520",
    "end": "81680"
  },
  {
    "text": "huge amount of internal traffic between components and they don't exactly partition well",
    "start": "81680",
    "end": "86880"
  },
  {
    "text": "so if you are running your app in multiple places each place should be a cluster",
    "start": "86880",
    "end": "92880"
  },
  {
    "text": "you will often want cluster level redundancy clusters can fail in",
    "start": "92960",
    "end": "99200"
  },
  {
    "text": "many many interesting ways you can deploy something cluster-wide that fails",
    "start": "99200",
    "end": "104960"
  },
  {
    "text": "you can take down the cluster control plane you can take down some key automation",
    "start": "104960",
    "end": "111600"
  },
  {
    "text": "and this especially goes if you are running your own control plane if you're not using managed service if you're running your own control plane",
    "start": "111600",
    "end": "118159"
  },
  {
    "text": "please i'm begging you run redundant clusters you do not want to be fixing that live",
    "start": "118159",
    "end": "126320"
  },
  {
    "text": "next one also a big plea for i guess cases where you have",
    "start": "126320",
    "end": "131680"
  },
  {
    "text": "sufficiently heterogeneous workloads is for cluster level security isolation",
    "start": "131680",
    "end": "137680"
  },
  {
    "text": "there are many many ways between past cves and just common misconfigurations and attack",
    "start": "137680",
    "end": "144080"
  },
  {
    "text": "surfaces that someone can take a fairly inconspicuous level of access to the",
    "start": "144080",
    "end": "149760"
  },
  {
    "text": "cluster and wind up managing to exfiltrate substantial amounts of data from",
    "start": "149760",
    "end": "156000"
  },
  {
    "text": "workloads or take over the cluster altogether in particular ian coldwater and tim",
    "start": "156000",
    "end": "162080"
  },
  {
    "text": "alclair have had some fantastic prior kubecon talks about some of the ways that",
    "start": "162080",
    "end": "168000"
  },
  {
    "text": "things can be misconfigured or exploited and the last one is um it's common",
    "start": "168000",
    "end": "174560"
  },
  {
    "text": "in large setups to have blue-green cluster upgrades potentially having clusters that you specifically",
    "start": "174560",
    "end": "182080"
  },
  {
    "text": "spin up and replace instead of trying to upgrade everything in place and again this kind of goes well with like having cluster redundancy versus",
    "start": "182080",
    "end": "189680"
  },
  {
    "text": "we'll do it live so what kind of things do we actually put in a cluster we have things that",
    "start": "189680",
    "end": "196239"
  },
  {
    "start": "193000",
    "end": "193000"
  },
  {
    "text": "would kind of consider to be the core cluster setup stuff like the admin are back the basic",
    "start": "196239",
    "end": "201440"
  },
  {
    "text": "monitoring stack maybe like data center or cloud provider integrations things that we probably want to put everywhere",
    "start": "201440",
    "end": "207599"
  },
  {
    "text": "there's a lot of stuff that maybe is more specific it might change more often or it's more specific to the workloads",
    "start": "207599",
    "end": "212959"
  },
  {
    "text": "we're running so like various storage backends might be specific ingress controllers that like",
    "start": "212959",
    "end": "218000"
  },
  {
    "text": "a specific application or team needs it might be you know operator or framework like something to run spark",
    "start": "218000",
    "end": "224239"
  },
  {
    "text": "and then most importantly we have the applications things that actually you know make our business money and are the",
    "start": "224239",
    "end": "229599"
  },
  {
    "text": "reason that we are actually here doing this we also need to deploy those",
    "start": "229599",
    "end": "235920"
  },
  {
    "start": "236000",
    "end": "236000"
  },
  {
    "text": "so i can sometimes be a detractor of how the cloud native world turns into",
    "start": "236480",
    "end": "242400"
  },
  {
    "text": "a lot of very fancy technology and checklists and you'll hear me call back to that a few",
    "start": "242400",
    "end": "247519"
  },
  {
    "text": "times but i want to explicitly call out what problems that we are considering in scope and what problems that we're",
    "start": "247519",
    "end": "253200"
  },
  {
    "text": "trying to solve in terms of building a cluster of",
    "start": "253200",
    "end": "258639"
  },
  {
    "text": "reconciliation system so we want to deploy common resources to multiple clusters",
    "start": "258639",
    "end": "264479"
  },
  {
    "text": "we want to loosely couple the clusters and those resource definitions we want",
    "start": "264479",
    "end": "270320"
  },
  {
    "text": "to clearly map the relationship between those resources and those clusters and we want to be able to",
    "start": "270320",
    "end": "276320"
  },
  {
    "text": "garbage collect deployed resources that should no longer be deployed there",
    "start": "276320",
    "end": "283840"
  },
  {
    "start": "284000",
    "end": "284000"
  },
  {
    "text": "so what does failure to build a decent multi-class system look like",
    "start": "284960",
    "end": "291680"
  },
  {
    "text": "well so what does failure to build",
    "start": "294840",
    "end": "300800"
  },
  {
    "text": "a working multi-cluster system look like",
    "start": "300800",
    "end": "304638"
  },
  {
    "text": "what does failure to build a good multi-cloud system look like you can easily wind up",
    "start": "307759",
    "end": "313600"
  },
  {
    "text": "with a dependency tangle between resources and versions anything that is consuming the kubernetes api",
    "start": "313600",
    "end": "319039"
  },
  {
    "text": "can become very tricky to version and there is a staggering effect for then components",
    "start": "319039",
    "end": "325039"
  },
  {
    "text": "that consume components you also need to be aware of any",
    "start": "325039",
    "end": "330240"
  },
  {
    "text": "dependencies that are required in your application because you typically will want to have a",
    "start": "330240",
    "end": "336000"
  },
  {
    "text": "certain amount of locality between actual services that you're running making sure that you are not like having",
    "start": "336000",
    "end": "341280"
  },
  {
    "text": "a golden path workflow that goes between different data centers or making sure you have data locality",
    "start": "341280",
    "end": "346880"
  },
  {
    "text": "for services that need to consume that data you could easily end up with orphaned resources on clusters so this would be",
    "start": "346880",
    "end": "353360"
  },
  {
    "text": "like a decommissioned service or an experiment or something that is running in a place that shouldn't",
    "start": "353360",
    "end": "359039"
  },
  {
    "text": "because we don't tend to be deploying it anymore but we're not cleaning it up you might wind up with wonky workload",
    "start": "359039",
    "end": "364880"
  },
  {
    "text": "and cluster mapping where you have a cluster that's been around forever and you know everything's on it you might spin up a",
    "start": "364880",
    "end": "369919"
  },
  {
    "text": "new cluster that's cheaper and or in a better location or something and there's just not much stuff on it people haven't you know",
    "start": "369919",
    "end": "375360"
  },
  {
    "text": "adopted it yet and all these things can make it very hard to bootstrap new clusters so if you have that really",
    "start": "375360",
    "end": "382479"
  },
  {
    "text": "nasty dependency chain and you have to kind of figure out how to fend angle everything into the cluster it can be difficult to just deploy",
    "start": "382479",
    "end": "388720"
  },
  {
    "text": "somewhere new if you're a random application",
    "start": "388720",
    "end": "394319"
  },
  {
    "text": "so in order to talk about specifically why some of these challenges are surprisingly hard to do well",
    "start": "394319",
    "end": "400479"
  },
  {
    "text": "i want to talk about what deploying something actually means and a lot of the logistics of the kubernetes",
    "start": "400479",
    "end": "405919"
  },
  {
    "text": "api itself so forgive me if this is overly basic but i don't want to assume",
    "start": "405919",
    "end": "412240"
  },
  {
    "start": "408000",
    "end": "408000"
  },
  {
    "text": "anyone's level of knowledge walking into this talk i want to briefly cover how the",
    "start": "412240",
    "end": "417520"
  },
  {
    "text": "kubernetes api itself works so kubernetes api has an object model",
    "start": "417520",
    "end": "422960"
  },
  {
    "text": "tiny bit analogous to like linux's everything is a file model so everything is represented with a",
    "start": "422960",
    "end": "429919"
  },
  {
    "text": "group aversion and a kind which identifies it as like a deployment is a particular object in",
    "start": "429919",
    "end": "437039"
  },
  {
    "text": "the apps v1 api so we see that represented in like the ammo representation",
    "start": "437039",
    "end": "442080"
  },
  {
    "text": "of a thing and every single object also has an identifier",
    "start": "442080",
    "end": "447680"
  },
  {
    "text": "so it's how we tell a resource like a deployment apart we know that one particular",
    "start": "447680",
    "end": "453840"
  },
  {
    "text": "deployment is different from another because it has a unique name namespace pair every resource has a name any resource",
    "start": "453840",
    "end": "460720"
  },
  {
    "text": "that exists in a namespace has a namespace so most things exist in a namespace like you know pods",
    "start": "460720",
    "end": "466800"
  },
  {
    "text": "deployments etc some things like nodes or cluster role bindings don't exist in a namespace so they have no namespace field",
    "start": "466800",
    "end": "474639"
  },
  {
    "text": "and kubernetes itself is a rest-ish http api",
    "start": "474639",
    "end": "479919"
  },
  {
    "text": "we often don't see that because we're busy dealing with like a typed client or group ctl or something but",
    "start": "479919",
    "end": "485280"
  },
  {
    "text": "um this all forms together to create like a fairly long api that we identify any individual",
    "start": "485280",
    "end": "491199"
  },
  {
    "text": "object with kubernetes supports a bunch of different",
    "start": "491199",
    "end": "496800"
  },
  {
    "start": "494000",
    "end": "494000"
  },
  {
    "text": "verbs i'll point out the ones that are relevant to making changes to things",
    "start": "496800",
    "end": "503039"
  },
  {
    "text": "we have create we have update they work like you'd expect with a rest cred api either create a new thing doesn't",
    "start": "503039",
    "end": "509599"
  },
  {
    "text": "already exist or replace an object wholesale with a new state update has a version check which is",
    "start": "509599",
    "end": "516000"
  },
  {
    "text": "quite helpful um there's a version in the metadata of any existing object so if you fetch",
    "start": "516000",
    "end": "523518"
  },
  {
    "text": "an object change it in memory and then go to reapply it",
    "start": "523519",
    "end": "529839"
  },
  {
    "text": "the kubernetes api will warn you if there's a version conflict in other words if you fetched and changed a different version",
    "start": "529839",
    "end": "536000"
  },
  {
    "text": "than it's in memory right now there's a patch operator which lets you have custom",
    "start": "536000",
    "end": "542399"
  },
  {
    "text": "semantics to say like i only want to update a specific field and i don't want to worry about or touch",
    "start": "542399",
    "end": "547839"
  },
  {
    "text": "anything else and then there is deleting which removes the object from the cluster",
    "start": "547839",
    "end": "554880"
  },
  {
    "text": "uh it's worth knowing there's something called finalizers on objects it's a list of",
    "start": "554880",
    "end": "562080"
  },
  {
    "text": "individual things that must be all removed before the object is able to be deleted",
    "start": "562080",
    "end": "567519"
  },
  {
    "text": "so if you have some kind of cleanup operation that's required before an object can be deleted um say like making sure that you know a",
    "start": "567519",
    "end": "574640"
  },
  {
    "text": "pod is actually distracted and removed from the system or making sure that like some load",
    "start": "574640",
    "end": "579839"
  },
  {
    "text": "balancer infrastructure is torn down from an ingress you have a finalizer on the object",
    "start": "579839",
    "end": "585680"
  },
  {
    "text": "which then whatever machine is responsible for that tear down removes the finalizer once it's done once all finalizers are gone the delete",
    "start": "585680",
    "end": "592320"
  },
  {
    "text": "happens so this is also why you usually shouldn't force delete because if you're forced deleting it's because",
    "start": "592320",
    "end": "597440"
  },
  {
    "text": "there's a finalizer trying to do some cleanup and it's not making progress if you delete the progress never happens",
    "start": "597440",
    "end": "605839"
  },
  {
    "start": "605000",
    "end": "605000"
  },
  {
    "text": "there's also the server side apply api this was ruled out very roughly a year ago i am",
    "start": "606560",
    "end": "613360"
  },
  {
    "text": "not as familiar with it but it provides the coup detail apply type behavior as an actual api",
    "start": "613360",
    "end": "621760"
  },
  {
    "text": "in kubernetes as opposed to forcing people to use good ctl apply the cli tool",
    "start": "621760",
    "end": "627920"
  },
  {
    "text": "so this adds a bunch of metadata giving ownership to specific fields in an object if you",
    "start": "627920",
    "end": "636560"
  },
  {
    "text": "change a field then you are considered an owner so like individual controllers or coupes detail itself",
    "start": "636560",
    "end": "642240"
  },
  {
    "text": "are examples of owners and when you want to specify a change only",
    "start": "642240",
    "end": "649760"
  },
  {
    "text": "the fields that you own are taken into account so if you own a field and you don't specify it in your new desired version",
    "start": "649760",
    "end": "656000"
  },
  {
    "text": "then it will be removed if there are fields that you do not own they're not in your",
    "start": "656000",
    "end": "661279"
  },
  {
    "text": "desired version then they're they're ignored the existing version is capped",
    "start": "661279",
    "end": "667680"
  },
  {
    "text": "understandably this can lead to api conflicts when you want to change something that you don't own there are",
    "start": "668320",
    "end": "674959"
  },
  {
    "text": "three different ways that you can handle this one is to reapply without the fields that are",
    "start": "674959",
    "end": "680000"
  },
  {
    "text": "conflicting in other words give up on trying to make that specific part of the change the next one is a no-op in terms of the",
    "start": "680000",
    "end": "688560"
  },
  {
    "text": "actual state is to change the field value that you're trying to apply",
    "start": "688560",
    "end": "694560"
  },
  {
    "text": "to the existing one so this doesn't change the state that data is in but you become a shared owner",
    "start": "694560",
    "end": "699920"
  },
  {
    "text": "of that field which then means in future apply attempts you would be able to apply without any",
    "start": "699920",
    "end": "706560"
  },
  {
    "text": "conflict and the last one is doing a force supply you become the sole owner of that field and you update it",
    "start": "706560",
    "end": "716880"
  },
  {
    "text": "so there are many gotchas when it comes to the mechanics of trying to",
    "start": "716880",
    "end": "722240"
  },
  {
    "text": "deploy or update selling in kubernetes i'm going to go through a couple specific cases and then kind of talk about how the",
    "start": "722240",
    "end": "730160"
  },
  {
    "text": "number of kind of intent issues can rise drastically when you don't know what",
    "start": "730160",
    "end": "735519"
  },
  {
    "text": "kinds of objects and intent you're dealing with",
    "start": "735519",
    "end": "740720"
  },
  {
    "start": "740000",
    "end": "740000"
  },
  {
    "text": "so for the sake of simplicity when i'm going to talk about a service in kubernetes i'm going to say it's a deployment it's a service object",
    "start": "741200",
    "end": "747360"
  },
  {
    "text": "and we we know it's a bad confusing name and an ingress maybe there's like config maps",
    "start": "747360",
    "end": "752880"
  },
  {
    "text": "or secrets or something but we're just talking about like a deployment its way to get traffic and",
    "start": "752880",
    "end": "758480"
  },
  {
    "text": "the accoutrement with that so i'm going to show a couple examples of",
    "start": "758480",
    "end": "765600"
  },
  {
    "text": "state that we're trying to apply and stay it's in the cluster the state that we're trying to apply is on the left the cluster state i'm going",
    "start": "765600",
    "end": "771680"
  },
  {
    "text": "to show on the right so here on the left we want to deploy you know like the archetypical nginx example",
    "start": "771680",
    "end": "778880"
  },
  {
    "text": "so say we've actually now we've deployed that and we want to patch in",
    "start": "780000",
    "end": "787200"
  },
  {
    "text": "a specific um image change for our deployment i think i have the",
    "start": "787200",
    "end": "792800"
  },
  {
    "text": "version slightly mixed up here but if we are only specifying a subset of the fields",
    "start": "792800",
    "end": "798399"
  },
  {
    "text": "in the version that we're trying to apply what we will likely accidentally do",
    "start": "798399",
    "end": "805120"
  },
  {
    "text": "say if we're just doing an update is we will wipe out a bunch of fields that",
    "start": "805120",
    "end": "811200"
  },
  {
    "text": "we wanted to keep so",
    "start": "811200",
    "end": "817120"
  },
  {
    "text": "when we update the individual nginx container struct if we only specify some fields in it",
    "start": "817680",
    "end": "823839"
  },
  {
    "text": "everything else in that just goes away so suddenly we have no",
    "start": "823839",
    "end": "829199"
  },
  {
    "text": "port on our nginx and it will understandably probably cause some things to fail",
    "start": "829199",
    "end": "836320"
  },
  {
    "text": "suppose we want to change the name instead of calling it nginx for whatever reason we want to call it web server so we change the name in our deploy",
    "start": "837519",
    "end": "843760"
  },
  {
    "text": "system we deploy and now we have two objects and this happens",
    "start": "843760",
    "end": "849120"
  },
  {
    "text": "because we've changed the primary key of our object there's no garbage collection with like any kind of",
    "start": "849120",
    "end": "855920"
  },
  {
    "text": "naive coop ctl apply approach or whatever so now we have two",
    "start": "855920",
    "end": "861040"
  },
  {
    "text": "maybe this is gonna just run some extra resources that we shouldn't be running it's also kind of attack surface implications and leaving an old thing",
    "start": "861040",
    "end": "867199"
  },
  {
    "text": "running forever um this could cause really really wacky behavior say if this is like a thing",
    "start": "867199",
    "end": "872880"
  },
  {
    "text": "that's pulling jobs out of a queue because then you have you know ancient software doing whatever",
    "start": "872880",
    "end": "878160"
  },
  {
    "text": "and taking away real work so there's lots of really weird ways that this can eat up resources or fail and this is a",
    "start": "878160",
    "end": "885279"
  },
  {
    "text": "common thing i see when i'm looking at kind of an under-managed cluster",
    "start": "885279",
    "end": "890160"
  },
  {
    "text": "another problem we can have is trying to figure out if we should remove a field or not so",
    "start": "891120",
    "end": "898399"
  },
  {
    "text": "we have our service definition on the last on the left we're trying to create a cluster ip service to our demo",
    "start": "898399",
    "end": "907199"
  },
  {
    "text": "and we don't specify a cluster ip because that's normally something that's created automatically grabbing like whatever",
    "start": "907199",
    "end": "913760"
  },
  {
    "text": "sets up the load balancer typically could proxy grabs which i don't think it's cute proxy but we grab",
    "start": "913760",
    "end": "920079"
  },
  {
    "text": "an ip out of our cluster epispace and",
    "start": "920079",
    "end": "924399"
  },
  {
    "text": "we put that in the cluster state however we now have a field that is",
    "start": "925120",
    "end": "931839"
  },
  {
    "text": "different it is empty in our desired state it is present in our current state",
    "start": "931839",
    "end": "937040"
  },
  {
    "text": "and a naive reconcile tool might want to replace that offhand i actually don't remember if the",
    "start": "937040",
    "end": "942720"
  },
  {
    "text": "behavior is to allow that or allow that field to be",
    "start": "942720",
    "end": "949120"
  },
  {
    "text": "removed and wind up generating a new ip or if it's an immutable field but either way",
    "start": "949120",
    "end": "955279"
  },
  {
    "text": "it's not a thing that we want our tool to try but it's a thing that a naive approach",
    "start": "955279",
    "end": "961120"
  },
  {
    "text": "would try and the kind of counterpoint to this if we just left fields is we could easily wind up with drift of",
    "start": "961120",
    "end": "969680"
  },
  {
    "text": "you know automation or people either innocently or maliciously inserting things into our production",
    "start": "969680",
    "end": "976160"
  },
  {
    "text": "copies and us not noticing or at least not ever",
    "start": "976160",
    "end": "981519"
  },
  {
    "text": "removing those fields if we also don't have you know a tombstone or garbage collection method for specific fields",
    "start": "981519",
    "end": "987360"
  },
  {
    "text": "if we take a field out of our definition and then apply we might have kind of a similar problem to you know the two engine x copies",
    "start": "987360",
    "end": "993759"
  },
  {
    "text": "thing where we don't know that we're supposed to clean up our own field so it's just remaining there forever",
    "start": "993759",
    "end": "998959"
  },
  {
    "text": "even though it's not in our definition",
    "start": "998959",
    "end": "1003680"
  },
  {
    "start": "1003000",
    "end": "1003000"
  },
  {
    "text": "so to kind of summarize and bring up another couple use cases um we'll often a lot of update methods",
    "start": "1004639",
    "end": "1011040"
  },
  {
    "text": "will leave fields untouched it's also easy to try to update fields that you don't want to update such as",
    "start": "1011040",
    "end": "1017120"
  },
  {
    "text": "fields that are managed by other automation things that are upset by the cluster",
    "start": "1017120",
    "end": "1022839"
  },
  {
    "text": "um there are a number of fields on resources that are defaulted on admission which means that when you",
    "start": "1022839",
    "end": "1029678"
  },
  {
    "text": "create a resource there are fields that go from being unset to being explicitly set to some",
    "start": "1029679",
    "end": "1035600"
  },
  {
    "text": "default so you might kind of wind up in a reconcile loop if you're just trying to always change",
    "start": "1035600",
    "end": "1041280"
  },
  {
    "text": "something when you see it's changed because there will always be that set of",
    "start": "1041280",
    "end": "1046558"
  },
  {
    "text": "missing fields in your intent that semantically translate onto what's in the cluster but",
    "start": "1046559",
    "end": "1051600"
  },
  {
    "text": "it's not like a literal deep copy match and there's also a couple kind of weird",
    "start": "1051600",
    "end": "1057440"
  },
  {
    "text": "gauchos with specific fields like some fields are immutable some of the fields in service",
    "start": "1057440",
    "end": "1062559"
  },
  {
    "text": "i believe are immutable and secrets is a common case you can't switch between uh data",
    "start": "1062559",
    "end": "1068320"
  },
  {
    "text": "and binary data formats in a secret this exposes a kind of unpleasant",
    "start": "1068320",
    "end": "1075039"
  },
  {
    "text": "problem called break before make when you have a singular identified",
    "start": "1075039",
    "end": "1080240"
  },
  {
    "text": "object that is immutable and you want to make a change to it you",
    "start": "1080240",
    "end": "1085919"
  },
  {
    "text": "basically have to delete it and recreate it and there are many many things that can go wrong with",
    "start": "1085919",
    "end": "1092160"
  },
  {
    "text": "that such as if the create fails in some way or the delete hangs you're kind of stuck with no resource if",
    "start": "1092160",
    "end": "1099200"
  },
  {
    "text": "there's any cascading effects that can be very bad if it's just a slow process compared to you know",
    "start": "1099200",
    "end": "1105520"
  },
  {
    "text": "whatever's running that can be very bad it's a problem that can come up a lot in",
    "start": "1105520",
    "end": "1111280"
  },
  {
    "text": "infrastructure automation and it usually comes from kind of poor api designs and then trying to do",
    "start": "1111280",
    "end": "1117200"
  },
  {
    "text": "generic actions on top of that",
    "start": "1117200",
    "end": "1122559"
  },
  {
    "text": "so let's talk about the journey to having a multi-cluster system",
    "start": "1122559",
    "end": "1127519"
  },
  {
    "start": "1128000",
    "end": "1128000"
  },
  {
    "text": "i'm going to try to break this down things that i've seen into three general models but there's a",
    "start": "1128000",
    "end": "1135600"
  },
  {
    "text": "general trend as the number of services and the number of clusters at a company increases to",
    "start": "1135600",
    "end": "1143120"
  },
  {
    "text": "going from tight coupling to looser coupling between those workloads and the infrastructure details and there's a",
    "start": "1143120",
    "end": "1149440"
  },
  {
    "text": "trend from being very manually managed to being much more automatically managed and focusing more on",
    "start": "1149440",
    "end": "1155280"
  },
  {
    "text": "the user's intent and as a reminder the goal of building",
    "start": "1155280",
    "end": "1161120"
  },
  {
    "text": "this kind of stuff is to reduce uncertainty and toil the goal is not to build a beautiful machine",
    "start": "1161120",
    "end": "1167679"
  },
  {
    "text": "that you know works magically and it's all cool the beautiful system is one that works well enough",
    "start": "1167679",
    "end": "1173360"
  },
  {
    "text": "so be sure about the problems that you need to solve and",
    "start": "1173360",
    "end": "1179919"
  },
  {
    "text": "how expensive ops work is to avoid syncing you know weeks or months of",
    "start": "1179919",
    "end": "1187520"
  },
  {
    "text": "development work to solve pretty marginal burdens",
    "start": "1187520",
    "end": "1194399"
  },
  {
    "start": "1195000",
    "end": "1195000"
  },
  {
    "text": "so our first model it's kind of the starting place is having hard-coded clusters we",
    "start": "1195120",
    "end": "1200240"
  },
  {
    "text": "explicitly have some kind of deploy config that says this workload goes to this cluster",
    "start": "1200240",
    "end": "1205760"
  },
  {
    "text": "maybe this is like a jenkins pipeline where each stage specifically like uses one cubesat context to apply to",
    "start": "1205760",
    "end": "1211440"
  },
  {
    "text": "something we wind up with a fair bit of duplication as we have an increasing number of",
    "start": "1211440",
    "end": "1217280"
  },
  {
    "text": "services because we have to specify in each one how they map out",
    "start": "1217280",
    "end": "1223520"
  },
  {
    "text": "we have a configuration lag as the cluster topology changes you know as we add a new cluster we have to opt new",
    "start": "1223520",
    "end": "1230000"
  },
  {
    "text": "services into it if we travel things around we have to",
    "start": "1230000",
    "end": "1235360"
  },
  {
    "text": "change what workloads point to where if we remove a cluster we probably have to update all those deploy pipelines or we",
    "start": "1235360",
    "end": "1240480"
  },
  {
    "text": "break them so it's it's super easy to start with but it definitely",
    "start": "1240480",
    "end": "1246240"
  },
  {
    "text": "doesn't scale with the organization well you start to experience more and more friction",
    "start": "1246240",
    "end": "1251280"
  },
  {
    "start": "1251000",
    "end": "1251000"
  },
  {
    "text": "so once you experience that friction kind of the logical way a lot of people alleviate that is with creating cluster groups",
    "start": "1251280",
    "end": "1260840"
  },
  {
    "text": "so instead of explicitly targeting",
    "start": "1260840",
    "end": "1267120"
  },
  {
    "text": "individual named clusters with a workload you target some group and a group is",
    "start": "1267120",
    "end": "1273280"
  },
  {
    "text": "just a mapping of labeling a cluster in some way",
    "start": "1273280",
    "end": "1279200"
  },
  {
    "text": "so like production or you know this app or staging or production us something of that sort or like",
    "start": "1279200",
    "end": "1286240"
  },
  {
    "text": "production gpu that describes the capabilities or attributes of the",
    "start": "1286240",
    "end": "1292080"
  },
  {
    "text": "cluster that we care about but a human has to create that group map it all out",
    "start": "1292080",
    "end": "1297200"
  },
  {
    "text": "a human has to decide what group to target and we don't necessarily get",
    "start": "1297200",
    "end": "1305840"
  },
  {
    "text": "we don't necessarily get any choice in this manner when it comes to anything anything",
    "start": "1306960",
    "end": "1314799"
  },
  {
    "text": "beyond that mapping so a common case is i want to run on some clusters",
    "start": "1314799",
    "end": "1319919"
  },
  {
    "text": "i don't care which ones like if i have just some fairly ephemeral batch workloads",
    "start": "1319919",
    "end": "1324960"
  },
  {
    "text": "the kind of common way to do this is you more or less designate batch clusters or you explicitly pick a random cluster",
    "start": "1324960",
    "end": "1330720"
  },
  {
    "text": "like it doesn't realistically need to be us west one a it's just",
    "start": "1330720",
    "end": "1335840"
  },
  {
    "text": "someone hard-coded that in because they don't want to apply it to the whole fleet so this can work well when you have a",
    "start": "1335840",
    "end": "1342159"
  },
  {
    "text": "moderate amount of stuff but the kind of number of arbitrary choices made",
    "start": "1342159",
    "end": "1347520"
  },
  {
    "text": "or just the sheer amount of mapping to describe different workloads with different requirements",
    "start": "1347520",
    "end": "1353600"
  },
  {
    "text": "starts to become unwieldy so the evolution from that which",
    "start": "1353600",
    "end": "1361679"
  },
  {
    "start": "1357000",
    "end": "1357000"
  },
  {
    "text": "from my understanding is not something that as many kubernetes users are doing is to",
    "start": "1361679",
    "end": "1368400"
  },
  {
    "text": "start to treat workloads in kubernetes kind of like we",
    "start": "1368400",
    "end": "1374960"
  },
  {
    "text": "treat pods so we specify in this model",
    "start": "1374960",
    "end": "1380960"
  },
  {
    "text": "the constraints on a workload like we want to put this on three geodistributed clusters in the us",
    "start": "1380960",
    "end": "1386240"
  },
  {
    "text": "we want to run this on any one cluster that runs x workload that we're dependent on we",
    "start": "1386240",
    "end": "1392799"
  },
  {
    "text": "specify abstract constraints that describe the requirements that we have",
    "start": "1392799",
    "end": "1398720"
  },
  {
    "text": "and we let something actually make that scheduling choice for us",
    "start": "1398720",
    "end": "1404080"
  },
  {
    "text": "as opposed to us making either an ad hoc or arbitrary choice about what that's",
    "start": "1404080",
    "end": "1410000"
  },
  {
    "text": "going to be so to summarize this we really start to capture the intent",
    "start": "1410000",
    "end": "1415039"
  },
  {
    "text": "of someone who's configuring this workload without directly coupling things together",
    "start": "1415039",
    "end": "1421440"
  },
  {
    "text": "so let's look more at a model of this",
    "start": "1421440",
    "end": "1425919"
  },
  {
    "text": "we want to be able to treat a workload a lot like we treat a workload within a",
    "start": "1426799",
    "end": "1432559"
  },
  {
    "text": "cluster now so let's look at how we want to schedule workloads",
    "start": "1432559",
    "end": "1437679"
  },
  {
    "text": "across clusters this is a very kind of vague architecture here because this is something that",
    "start": "1437679",
    "end": "1443840"
  },
  {
    "text": "most of it does not exist in any open source form that i know of today",
    "start": "1443840",
    "end": "1450000"
  },
  {
    "text": "we have a bunch of individual clusters we have some kind of cluster reconciliation tool",
    "start": "1450000",
    "end": "1456320"
  },
  {
    "text": "that is able to fetch the desired state for that cluster so everything that should be in that",
    "start": "1456320",
    "end": "1461520"
  },
  {
    "text": "cluster from the admin are back to all the services in it and what the definitions of all those",
    "start": "1461520",
    "end": "1466559"
  },
  {
    "text": "services it's able to fetch that from some api and it's able to push any changes from that",
    "start": "1466559",
    "end": "1473279"
  },
  {
    "text": "api such as you know image updates or configuration updates into the cluster as well as",
    "start": "1473279",
    "end": "1479600"
  },
  {
    "text": "reconcile and fix anything in the cost that deviates due to something changing it from the",
    "start": "1479600",
    "end": "1485360"
  },
  {
    "text": "intended state we have a top-level workload api that",
    "start": "1485360",
    "end": "1490400"
  },
  {
    "text": "represents you know like application foo globally that then is materialized into",
    "start": "1490400",
    "end": "1496159"
  },
  {
    "text": "those individual clusters and what does that decision is a cluster",
    "start": "1496159",
    "end": "1501919"
  },
  {
    "text": "scheduler so i submit something to the workload api and say hey i just need one cluster that runs this",
    "start": "1501919",
    "end": "1508080"
  },
  {
    "text": "database because i need to run a batch job and so the cluster scheduler goes okay you need dfinity with",
    "start": "1508080",
    "end": "1514080"
  },
  {
    "text": "x x is scheduled here i'm going to schedule you here and in order to have that knowledge",
    "start": "1514080",
    "end": "1521600"
  },
  {
    "text": "we also need a cluster registry something that is aware of all clusters that we have in the fleet",
    "start": "1521600",
    "end": "1526960"
  },
  {
    "text": "is aware of the various you know tags and metadata that make up these attributes is aware of their current status so",
    "start": "1526960",
    "end": "1533360"
  },
  {
    "text": "making sure the cost is online potentially being able to factor in things like you know cost and capacity in these decisions as",
    "start": "1533360",
    "end": "1540320"
  },
  {
    "text": "well as being able to easily manage credentials so that we can authenticate back and forth with arbitrarily",
    "start": "1540320",
    "end": "1546559"
  },
  {
    "text": "arbitrary cluster control plans uh if you want to learn a little bit",
    "start": "1546559",
    "end": "1551760"
  },
  {
    "text": "more about this model because i'm only describing it briefly here i wrote a blog post that",
    "start": "1551760",
    "end": "1556880"
  },
  {
    "text": "i think someone generously described as a white paper at one point but it's about a 10 minute read into kind of the",
    "start": "1556880",
    "end": "1563279"
  },
  {
    "text": "idea of this kind of model more about what the api could look like and why i think it makes sense",
    "start": "1563279",
    "end": "1569120"
  },
  {
    "text": "this is something that um someone in sigmulti cluster saw and because of that we've started",
    "start": "1569120",
    "end": "1574400"
  },
  {
    "text": "prototyping a little bit of this material so the cluster inventory api i mentioned",
    "start": "1574400",
    "end": "1580240"
  },
  {
    "start": "1575000",
    "end": "1575000"
  },
  {
    "text": "is the part where it's the concrete view of you know im cluster us east one these are the things i",
    "start": "1580240",
    "end": "1586240"
  },
  {
    "text": "should have in it it's pretty much you know a precise definition of a million lines of yaml or something",
    "start": "1586240",
    "end": "1593120"
  },
  {
    "text": "of the sort there's no heart level ambiguity it's just a dump of the state",
    "start": "1593120",
    "end": "1599840"
  },
  {
    "start": "1599000",
    "end": "1599000"
  },
  {
    "text": "there's the cluster reconciler which is responsible for doing the sinking so anything that is in that",
    "start": "1600000",
    "end": "1607760"
  },
  {
    "text": "cluster inventory that is missing create it if anything has a differing state",
    "start": "1607760",
    "end": "1614480"
  },
  {
    "text": "then make the update and anything was previously managed by the cluster inventory api and is no",
    "start": "1614480",
    "end": "1621840"
  },
  {
    "text": "longer in that api should get deleted so garbage collect anything that should no longer be applied that",
    "start": "1621840",
    "end": "1628320"
  },
  {
    "text": "can be surprisingly effective for doing cleanup um one of the ambiguities",
    "start": "1628320",
    "end": "1636159"
  },
  {
    "text": "currently in our prototype of this is how we should handle updates because as as i mentioned in like the api",
    "start": "1636159",
    "end": "1642399"
  },
  {
    "text": "logistics portion of this talk there are many ways where it's hard to capture the intention of",
    "start": "1642399",
    "end": "1650080"
  },
  {
    "text": "the user into what operations to do currently one thing i've considered is",
    "start": "1650080",
    "end": "1655520"
  },
  {
    "text": "trying to add some kind of basically metadata to indicate your intention with specific keys like if you see this key",
    "start": "1655520",
    "end": "1662080"
  },
  {
    "text": "in the cluster don't change it this is an area where building custom tooling for",
    "start": "1662080",
    "end": "1670240"
  },
  {
    "text": "the stack that you're running especially if you have more of a platform as a service versus kubernetes",
    "start": "1670240",
    "end": "1676159"
  },
  {
    "text": "as a service internally can be very valuable because in reality for any any",
    "start": "1676159",
    "end": "1682080"
  },
  {
    "text": "set of objects there's a finite number of gotchas around doing updates with the state",
    "start": "1682080",
    "end": "1688559"
  },
  {
    "text": "things like you know the cluster ip um if you're able to build tooling explicitly around those",
    "start": "1688559",
    "end": "1695360"
  },
  {
    "text": "you will probably have an easier time than trying to build a tool that solves every single use case it's",
    "start": "1695360",
    "end": "1701440"
  },
  {
    "text": "challenging to build an open source tool because you basically have to accommodate with",
    "start": "1701440",
    "end": "1707279"
  },
  {
    "text": "some reasonable wiggle room everyone out there doing everything so for example stuff like crds",
    "start": "1707279",
    "end": "1713600"
  },
  {
    "text": "you have to either say you're on your own if you had an edge case or you have to give the user a way to kind of",
    "start": "1713600",
    "end": "1720320"
  },
  {
    "text": "specify that intention if you know something that's supposed to be right once or whatever",
    "start": "1720320",
    "end": "1726480"
  },
  {
    "text": "and as i said we're we're prototyping some of the stuff now um it's fairly early stages of",
    "start": "1728399",
    "end": "1734480"
  },
  {
    "text": "prototyping we're doing specifically a minimal version of the inventory api just enough of something that we can",
    "start": "1734480",
    "end": "1741120"
  },
  {
    "text": "reasonably do a reconciliation process and figuring out the mechanics of how the reconcilers",
    "start": "1741120",
    "end": "1746159"
  },
  {
    "text": "should work and it's one of those things where it's it's easy to do something but it's hard to necessarily do what you",
    "start": "1746159",
    "end": "1752799"
  },
  {
    "text": "want and this is an area where the sig and i personally would really really appreciate some",
    "start": "1752799",
    "end": "1758320"
  },
  {
    "text": "feedback as the exact use cases and problems that you have",
    "start": "1758320",
    "end": "1763919"
  },
  {
    "start": "1764000",
    "end": "1764000"
  },
  {
    "text": "so there is also a cluster registry that part is not vaporware however it has a somewhat uncertain feature i",
    "start": "1764159",
    "end": "1771279"
  },
  {
    "text": "forget the exact fate that was decided upon because it's not super well maintained",
    "start": "1771279",
    "end": "1778080"
  },
  {
    "text": "and as far as we're aware it's not extremely well adopted but it's something that you're able to",
    "start": "1778080",
    "end": "1784159"
  },
  {
    "text": "kind of insert all your clusters so you can list them out get credentials for them and put status",
    "start": "1784159",
    "end": "1789440"
  },
  {
    "text": "for them in a central place and this is something that would require for a cluster scheduling system because",
    "start": "1789440",
    "end": "1795520"
  },
  {
    "text": "again you need like the inventory available of what clusters are at your disposal and the metadata about",
    "start": "1795520",
    "end": "1801679"
  },
  {
    "text": "them that you need to make a decision and this is the point where like we get",
    "start": "1801679",
    "end": "1806720"
  },
  {
    "text": "into full conjecture land and this is something that the original blog post about this idea",
    "start": "1806720",
    "end": "1811919"
  },
  {
    "text": "covers in more detail but for our workload api there's extreme",
    "start": "1811919",
    "end": "1816960"
  },
  {
    "start": "1814000",
    "end": "1814000"
  },
  {
    "text": "ambiguity as to how you define a workload exactly and how you deal with the",
    "start": "1816960",
    "end": "1824320"
  },
  {
    "text": "one-to-many mapping that happens when you materialize it to multiple clusters so how do you gracefully roll that",
    "start": "1824320",
    "end": "1830880"
  },
  {
    "text": "version out when i have service x how do i you know not deploy it everywhere because no no",
    "start": "1830880",
    "end": "1837279"
  },
  {
    "text": "change should ever go everywhere all at once so how do you roll it between clusters how manual is",
    "start": "1837279",
    "end": "1842720"
  },
  {
    "text": "that how automated is that how does it work how do you deal with templating if you want or need to have",
    "start": "1842720",
    "end": "1849200"
  },
  {
    "text": "specific fields in your configuration in your like gamble or whatever changed between the primary version and the",
    "start": "1849200",
    "end": "1855679"
  },
  {
    "text": "version for any given cluster what kind of scheduling options do you have and how sticky is that cluster scheduling like",
    "start": "1855679",
    "end": "1862640"
  },
  {
    "text": "what should happen if you delete a cluster and no longer satisfy scheduling constraints or something like",
    "start": "1862640",
    "end": "1869360"
  },
  {
    "start": "1866000",
    "end": "1866000"
  },
  {
    "text": "if you go from having three replicas or three cluster replicas to having two cluster replicas of a thing",
    "start": "1869360",
    "end": "1875279"
  },
  {
    "text": "do you spin it up somewhere else do you make like the state in the load balance",
    "start": "1875279",
    "end": "1880399"
  },
  {
    "text": "or follow that around you make a human intervene there's a very very long tale as to how",
    "start": "1880399",
    "end": "1885919"
  },
  {
    "text": "to do that and it winds up being very domain specific there's a couple people in the community",
    "start": "1885919",
    "end": "1892080"
  },
  {
    "text": "who have definitely inspired a lot of the way that i've thought about this in particular i want to call out paul mori from sigmund",
    "start": "1892080",
    "end": "1898480"
  },
  {
    "text": "cluster who strongly advised that we start prototyping from the bottom up being like the reconciler up rather than",
    "start": "1898480",
    "end": "1905600"
  },
  {
    "text": "starting from the top down with the scheduler and workload api because there are many many ways to do this and",
    "start": "1905600",
    "end": "1913840"
  },
  {
    "text": "there's many tools that try to do vaguely similar things that become very opinionated",
    "start": "1913840",
    "end": "1922080"
  },
  {
    "text": "so what we're trying to do on the open source side is figure out the building blocks",
    "start": "1922080",
    "end": "1928799"
  },
  {
    "text": "that we can give you so that you provide the very opinionated parts that are",
    "start": "1928799",
    "end": "1934960"
  },
  {
    "text": "specific to how you run apps and how your infrastructure works for less cost than it would take to",
    "start": "1934960",
    "end": "1941039"
  },
  {
    "text": "build the one size doesn't quite fit anybody component",
    "start": "1941039",
    "end": "1946000"
  },
  {
    "text": "so thank you very much for attending this is not specifically a talk about apple systems but i work at apple um",
    "start": "1947360",
    "end": "1955200"
  },
  {
    "text": "i really really like the people who i work with so if you want to work with me and people who i really look up to you",
    "start": "1955200",
    "end": "1960880"
  },
  {
    "text": "please reach out or stop by the apple booth to learn more we're looking for a lot of kubernetes people right now",
    "start": "1960880",
    "end": "1967360"
  },
  {
    "text": "um now i should be able to take questions you will be speaking to a future version of me who is hopefully",
    "start": "1967360",
    "end": "1973679"
  },
  {
    "text": "slightly wiser thank you for joining",
    "start": "1973679",
    "end": "1979519"
  }
]