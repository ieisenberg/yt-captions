[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "yeah thank you for the introduction the next 30 minutes we talk about databases on kubernetes using a custom operator on",
    "start": "1190",
    "end": "8670"
  },
  {
    "text": "day one two and a year later and actually we do have databases running in communities for a year now though before",
    "start": "8670",
    "end": "17130"
  },
  {
    "text": "we begin this is me my name is Johannes I'm working for a database company the",
    "start": "17130",
    "end": "22230"
  },
  {
    "text": "data this company is called neo4j it's a graph database we store things a bit differently than you would expect",
    "start": "22230",
    "end": "28380"
  },
  {
    "text": "from a relational database but actually I'm not a database guy and more like a container guy which is good because we",
    "start": "28380",
    "end": "35130"
  },
  {
    "text": "keep conrad and we do have a team at near forge a building near future cloud it's kind of a managed neo4j",
    "start": "35130",
    "end": "41879"
  },
  {
    "text": "as a service database as a service and we want to talk about this today about",
    "start": "41879",
    "end": "47219"
  },
  {
    "text": "our story building this offering but maybe before we begin give me a short",
    "start": "47219",
    "end": "52649"
  },
  {
    "start": "50000",
    "end": "77000"
  },
  {
    "text": "hands up do you building operators on your own do you have experience with that all right",
    "start": "52649",
    "end": "58350"
  },
  {
    "text": "so they're quite some hands and this is amazing see if I'm telling something that didn't work out for you at all",
    "start": "58350",
    "end": "65250"
  },
  {
    "text": "please start the conversation with me I would love to hear your story what worked for you and what worked not for",
    "start": "65250",
    "end": "71070"
  },
  {
    "text": "you because I can only tell you what worked for us and what didn't work for us and I would super interested in what",
    "start": "71070",
    "end": "76080"
  },
  {
    "text": "worked for you is one okay so new for what you maybe seen this logo before",
    "start": "76080",
    "end": "81930"
  },
  {
    "start": "77000",
    "end": "104000"
  },
  {
    "text": "let me shortly introduce Naver J because it helps the story so you may have been",
    "start": "81930",
    "end": "89130"
  },
  {
    "text": "heard about the Panama papers so this was about taxing the fraud system so",
    "start": "89130",
    "end": "94920"
  },
  {
    "text": "people try to hide the money from the text system and they usually do this by opening a bank account in the Bahamas",
    "start": "94920",
    "end": "101340"
  },
  {
    "text": "and this case on Panama and a few years ago there was a league of a law company",
    "start": "101340",
    "end": "107399"
  },
  {
    "start": "104000",
    "end": "191000"
  },
  {
    "text": "in Panama they leaked at 11 and a half million document a million documents like email skins receives and this was",
    "start": "107399",
    "end": "115200"
  },
  {
    "text": "two and a half terabyte worth of raw data and this was given to journalism's",
    "start": "115200",
    "end": "120899"
  },
  {
    "text": "and as every sane journalist would do they put the data into Excel so they",
    "start": "120899",
    "end": "127079"
  },
  {
    "text": "recognized that they didn't work out that well so because they tried to find patterns of people trying to rob",
    "start": "127079",
    "end": "133500"
  },
  {
    "text": "system and in that case they try to find the pattern of persons that has a bank",
    "start": "133500",
    "end": "139410"
  },
  {
    "text": "account and the bank account is in a bank in Panama so they didn't found annual any of those people trying to",
    "start": "139410",
    "end": "146760"
  },
  {
    "text": "throw the Texas and because people who want to hide loads of Texas are smarter than that so they search deeper and find another",
    "start": "146760",
    "end": "153780"
  },
  {
    "text": "pattern so there was the pattern of an address where the people lived and there was another one living at the same address and the other one has was",
    "start": "153780",
    "end": "161790"
  },
  {
    "text": "officer the company and the company has a registered bank account in Panama and",
    "start": "161790",
    "end": "166910"
  },
  {
    "text": "whoever tried to do join over 70 tables in relation database probably know that",
    "start": "166910",
    "end": "173040"
  },
  {
    "text": "this is can be potentially quite painful but this is the innate a model of nature",
    "start": "173040",
    "end": "179400"
  },
  {
    "text": "for days so we do have notes like a person's and notes could have classes it's optional but there can have",
    "start": "179400",
    "end": "187050"
  },
  {
    "text": "relations are really easy to traverse relationships but the great thing is",
    "start": "187050",
    "end": "192390"
  },
  {
    "start": "191000",
    "end": "217000"
  },
  {
    "text": "once you have found the pattern of those seven nodes you can query all of your data set or the eleven and a half",
    "start": "192390",
    "end": "198480"
  },
  {
    "text": "million documents and you get all the people who try to probe the text system so all the Formula One drivers who",
    "start": "198480",
    "end": "204030"
  },
  {
    "text": "brought the text system it didn't pay taxes in the home country or the football players or the Russian nobleman",
    "start": "204030",
    "end": "210840"
  },
  {
    "text": "so they were found through the journalism's and they use neo4j for that which is really cool and they're also",
    "start": "210840",
    "end": "218010"
  },
  {
    "start": "217000",
    "end": "230000"
  },
  {
    "text": "doctors trying to find the cure for cancer and they're putting our patient data in there for Jay and their",
    "start": "218010",
    "end": "223829"
  },
  {
    "text": "treatments and how this worked out and tried to find a pattern to help cure",
    "start": "223829",
    "end": "229440"
  },
  {
    "text": "cancer under NASA is putting all the engineering data like decades of engineering data in in a graph to build",
    "start": "229440",
    "end": "235950"
  },
  {
    "start": "230000",
    "end": "254000"
  },
  {
    "text": "up a knowledge graph to help to bring people to the mass so this is really really exciting but we also have more",
    "start": "235950",
    "end": "242070"
  },
  {
    "text": "enterprise use cases for example for protection is highly it's really good",
    "start": "242070",
    "end": "247500"
  },
  {
    "text": "doable by pattern matching or recommendations so these are more enterprise-e",
    "start": "247500",
    "end": "252590"
  },
  {
    "text": "use cases but what all of those users have in common that they put their",
    "start": "252590",
    "end": "257850"
  },
  {
    "start": "254000",
    "end": "288000"
  },
  {
    "text": "mission-critical data in a database but it's not their core core business to",
    "start": "257850",
    "end": "263580"
  },
  {
    "text": "operate a database to make it away with a twenty 4:7 they needed 24/7 but it's not their",
    "start": "263580",
    "end": "269560"
  },
  {
    "text": "core business and this is why people tend to like as a service product a data",
    "start": "269560",
    "end": "276580"
  },
  {
    "text": "basis as a services monitoring as a service metric system like all the products there in the exhibition hall so",
    "start": "276580",
    "end": "284050"
  },
  {
    "text": "we decided to build and manage database as a service offering and we started",
    "start": "284050",
    "end": "290350"
  },
  {
    "start": "288000",
    "end": "376000"
  },
  {
    "text": "over two years ago and we started with a tiny web application and in the web",
    "start": "290350",
    "end": "295870"
  },
  {
    "text": "application the user was had a button to start an FHA instance so we provisioned an ec2 instance started a single",
    "start": "295870",
    "end": "303190"
  },
  {
    "text": "instance neo4j database on it and then report at the end point back to the user this was a first iteration of the",
    "start": "303190",
    "end": "309310"
  },
  {
    "text": "product and where did invite-only beta we're roughly 50 customers using this and there was no too bad so they this",
    "start": "309310",
    "end": "316660"
  },
  {
    "text": "was okayish but it had some downsides to it the first down said is we forced the",
    "start": "316660",
    "end": "322600"
  },
  {
    "text": "customer to give us a downtown and maintenance window while the customer allowed us to take the database down to",
    "start": "322600",
    "end": "328570"
  },
  {
    "text": "do updates and fixes and whatever and I don't think this is the way to offer a",
    "start": "328570",
    "end": "334750"
  },
  {
    "text": "database a service offering in 218 or to 19 furthermore we relied on the",
    "start": "334750",
    "end": "340570"
  },
  {
    "text": "availability on ec2 which wasn't that bad either except we had some problems",
    "start": "340570",
    "end": "347169"
  },
  {
    "text": "in August last year we had really tough time but I don't want to talk about that today and we relied on for tolerance on",
    "start": "347169",
    "end": "355240"
  },
  {
    "text": "EBS so we use block storage to set store data and again sometimes we had hiccups",
    "start": "355240",
    "end": "360790"
  },
  {
    "text": "in EBS where it took up to 30 minutes to unknown the EBS from a failing instance",
    "start": "360790",
    "end": "366490"
  },
  {
    "text": "to remount it to a new one so this was also not really let's say that way our Christmas were not really happy about",
    "start": "366490",
    "end": "372310"
  },
  {
    "text": "that so we wanted to go away from single instance to clusters because clusters",
    "start": "372310",
    "end": "379000"
  },
  {
    "start": "376000",
    "end": "395000"
  },
  {
    "text": "promised to have zero downtime because we can do reroll in updates on the fly and don't expose down times to our",
    "start": "379000",
    "end": "385810"
  },
  {
    "text": "customers and again downtime was really bad and availability is probably not the",
    "start": "385810",
    "end": "392110"
  },
  {
    "text": "best thing in single instance and while we were operating all the",
    "start": "392110",
    "end": "398110"
  },
  {
    "start": "395000",
    "end": "464000"
  },
  {
    "text": "single instances in easy - we were building all our or management services",
    "start": "398110",
    "end": "403120"
  },
  {
    "text": "the UI services the backend or the workers we deployed that on kubernetes",
    "start": "403120",
    "end": "408460"
  },
  {
    "text": "who were operating this on kubernetes we opted in for manage given IDs on Google",
    "start": "408460",
    "end": "414190"
  },
  {
    "text": "so we use GK for that because at the time we started it was a really good and easy way for us to get started",
    "start": "414190",
    "end": "420880"
  },
  {
    "text": "so we gained experience over one year running stateless services in communities and at that time",
    "start": "420880",
    "end": "427800"
  },
  {
    "text": "unfortunately there was no graph database as a service offering so we also bought in relational database as a",
    "start": "427800",
    "end": "433780"
  },
  {
    "text": "service because at that point it was not our business to operate a relational database so we just bought it in at that",
    "start": "433780",
    "end": "441880"
  },
  {
    "text": "time and currently we are evaluating to change it to our own product which is super super funny anyways so we had",
    "start": "441880",
    "end": "448090"
  },
  {
    "text": "experience in communities running statistics services and then we thought we want to do clusters but we don't want",
    "start": "448090",
    "end": "454180"
  },
  {
    "text": "to do crust this on ec2 and doing all the scheduling on our own so we thought",
    "start": "454180",
    "end": "459280"
  },
  {
    "text": "we're gonna leverage our communities the scheduling our tools in kubernetes and then Kelsey Hightower gave presentation",
    "start": "459280",
    "end": "467289"
  },
  {
    "start": "464000",
    "end": "492000"
  },
  {
    "text": "most people get really excited about running a database inside of a cluster",
    "start": "467289",
    "end": "472330"
  },
  {
    "text": "manager like Nomad Kevin Eddy's this is going to make you lose your job guaranteed yeah I'm still happily",
    "start": "472330",
    "end": "483039"
  },
  {
    "text": "employed but this was too thick 16 so Kelsey is right so there's more in the",
    "start": "483039",
    "end": "488470"
  },
  {
    "text": "talk if you if you want check it out us from hash econ 4 to 16 but then this",
    "start": "488470",
    "end": "494289"
  },
  {
    "start": "492000",
    "end": "521000"
  },
  {
    "text": "just the backup in case the video didn't work but last year Kelsey said companies made huge improvements to run stateful",
    "start": "494289",
    "end": "500919"
  },
  {
    "text": "workouts including databases and managed cue our message queues but I still prefer not to do it fair enough and on",
    "start": "500919",
    "end": "508510"
  },
  {
    "text": "the same day in the further threat Kelsey said I think it's important to remember that kubernetes only solves",
    "start": "508510",
    "end": "513969"
  },
  {
    "text": "half of a problem the other parts must be solved by stay for services and operational experience and I think he's",
    "start": "513969",
    "end": "519789"
  },
  {
    "text": "very true and the great thing is we had loads of experience operational",
    "start": "519789",
    "end": "524829"
  },
  {
    "start": "521000",
    "end": "531000"
  },
  {
    "text": "experience in running databases and we were confident that we want to put it in communities and we wanted to",
    "start": "524829",
    "end": "532010"
  },
  {
    "start": "531000",
    "end": "652000"
  },
  {
    "text": "run clusters so let's have a short look about clusters in there for J Nia J is a",
    "start": "532010",
    "end": "537110"
  },
  {
    "text": "fully replicated database so all the nodes have all the data therefore J implements the Ralphs protocol it's",
    "start": "537110",
    "end": "543589"
  },
  {
    "text": "basically a distributed algorithm to do distributed consensus so commits in a distributed system and it's basically",
    "start": "543589",
    "end": "550640"
  },
  {
    "text": "like praxis just a bit weaker so we have a leader follower concept you can write",
    "start": "550640",
    "end": "555680"
  },
  {
    "text": "requests to the leader and the leader would then go out to the followers and",
    "start": "555680",
    "end": "560779"
  },
  {
    "text": "try to get the majority of commits back from them and when he I got the majority",
    "start": "560779",
    "end": "566000"
  },
  {
    "text": "of acknowledges of the transaction he would give back the transaction to the client and market has completed we are",
    "start": "566000",
    "end": "574070"
  },
  {
    "text": "running all over near 4-day instances in a dedicated step for said so we do have three stateful sets for cluster of three",
    "start": "574070",
    "end": "580190"
  },
  {
    "text": "notes we're using persistent volumes offered by GK to store the data",
    "start": "580190",
    "end": "587360"
  },
  {
    "text": "directories and we do backup we would be using backup workers and the end goal is",
    "start": "587360",
    "end": "593060"
  },
  {
    "text": "were not quite there yet in the fermentation but the end goal is that we have the backup workers and the backup",
    "start": "593060",
    "end": "599329"
  },
  {
    "text": "worker is doing backups constantly but he's not doing a full backup he's doing incremental backups so most databases",
    "start": "599329",
    "end": "606079"
  },
  {
    "text": "supports this and this means that the backup worker hold state that's why he has a PV attached to it and hello knows",
    "start": "606079",
    "end": "613070"
  },
  {
    "text": "about the last backup in the last backed up transaction ID so one he was requesting a backup he would only",
    "start": "613070",
    "end": "619449"
  },
  {
    "text": "synchronize the last transactions to it so when we have that that we're running",
    "start": "619449",
    "end": "625730"
  },
  {
    "text": "backups constantly all the time we maybe want to get rid of the PBS of the main",
    "start": "625730",
    "end": "631130"
  },
  {
    "text": "cluster to gain more performance because then we actually don't need it when we",
    "start": "631130",
    "end": "636260"
  },
  {
    "text": "have the backup working constantly doing the backups and the cluster is faltering by itself but we're not quite there yet",
    "start": "636260",
    "end": "642829"
  },
  {
    "text": "and we're not decided if we want to go that step or not so let's see and wait",
    "start": "642829",
    "end": "647990"
  },
  {
    "text": "for the future okay but this cluster technology so let's dive in communities and the",
    "start": "647990",
    "end": "653269"
  },
  {
    "start": "652000",
    "end": "715000"
  },
  {
    "text": "operator we started the operator building operator about one year ago a bit before",
    "start": "653269",
    "end": "659759"
  },
  {
    "text": "Kubek on Europe last year and we decided to use the go language to do it as a",
    "start": "659759",
    "end": "665639"
  },
  {
    "text": "neo4j is written in Java most of the other stuff is written in Python or bash and other languages so we didn't have go",
    "start": "665639",
    "end": "672029"
  },
  {
    "text": "experience at all what we wanted to go with a language that kubernetes is written in so we decided to use the",
    "start": "672029",
    "end": "678990"
  },
  {
    "text": "cover Nettie's a driver and the community's clients and we're pretty happy to the day that we chose a go for",
    "start": "678990",
    "end": "685649"
  },
  {
    "text": "that we didn't use any operator kit or SDK or whatever because they were in",
    "start": "685649",
    "end": "692220"
  },
  {
    "text": "just a whale but at the time and the actual code gluing our business logic on",
    "start": "692220",
    "end": "699990"
  },
  {
    "text": "top of the communities API the chameleon disk controller if you know it's just so thin that we didn't fell that we want to",
    "start": "699990",
    "end": "706889"
  },
  {
    "text": "do it in the last year so we sticked with a vanilla coaching generated coins",
    "start": "706889",
    "end": "712170"
  },
  {
    "text": "from the kubernetes code and in the data model of kubernetes and you probably all",
    "start": "712170",
    "end": "720000"
  },
  {
    "start": "715000",
    "end": "814000"
  },
  {
    "text": "know that you have like a loose set of resources so you have deployments replica set spots stateful services and",
    "start": "720000",
    "end": "726449"
  },
  {
    "text": "so on and the great thing about all those resources is that you have a controller attached on every abstraction",
    "start": "726449",
    "end": "733439"
  },
  {
    "text": "layer so you will have a stateful set controller who's doing a constant task reconciliation and reconciliation just a",
    "start": "733439",
    "end": "741149"
  },
  {
    "text": "fancy word of comparing the current state to the target state so he would ask for the CDs and then he would dis",
    "start": "741149",
    "end": "749160"
  },
  {
    "text": "calculate the target state if this is not matching what is currently running in kubernetes the stateful set",
    "start": "749160",
    "end": "755579"
  },
  {
    "text": "controller would decide to start a pot and he does that but just creating a new",
    "start": "755579",
    "end": "761220"
  },
  {
    "text": "pot resource so then the pot controller would kick in and see that there's a new port in the targets de that is not in",
    "start": "761220",
    "end": "767459"
  },
  {
    "text": "the current state so then the port controller will try to find a Koopa to stop the pod and so on and so on and the",
    "start": "767459",
    "end": "773519"
  },
  {
    "text": "great thing here is that we can extend the data model so we can just add a new",
    "start": "773519",
    "end": "779579"
  },
  {
    "text": "type and call it near for a database and we wrote this controller logic so that a",
    "start": "779579",
    "end": "786180"
  },
  {
    "text": "client could do cube cut' will apply whatever thing you want supply in forge a database and underneath for Jo a",
    "start": "786180",
    "end": "792600"
  },
  {
    "text": "controller will kick in and see oh there's something in my target state that it's not in my running current",
    "start": "792600",
    "end": "798060"
  },
  {
    "text": "state so I need to start a new stateful set for example and then the state for such controller which again and do its",
    "start": "798060",
    "end": "803550"
  },
  {
    "text": "thing and then a pod controller and so on answer and this really creates we re using loads of logic from kubernetes",
    "start": "803550",
    "end": "809910"
  },
  {
    "text": "itself which is super helpful for building this product but as we were",
    "start": "809910",
    "end": "816600"
  },
  {
    "start": "814000",
    "end": "826000"
  },
  {
    "text": "developing the operator the this reconciliation logic got bigger and",
    "start": "816600",
    "end": "821880"
  },
  {
    "text": "bigger and bigger and we thought that we want to change that so we split down the",
    "start": "821880",
    "end": "828900"
  },
  {
    "start": "826000",
    "end": "885000"
  },
  {
    "text": "whole reconciliation logic in things and let's call it mini reconcile for now so",
    "start": "828900",
    "end": "834450"
  },
  {
    "text": "we're splitting out logic for dedicated things in me mini reconcile us so we have a dedicated mini reconciler for",
    "start": "834450",
    "end": "841110"
  },
  {
    "text": "backups or mini reconciler for load so when the user requests the loader database or restore a database we have a",
    "start": "841110",
    "end": "847770"
  },
  {
    "text": "dedicated mini reconcile of config Maps offer certificates or the communication is encrypted so we haven't reconcile",
    "start": "847770",
    "end": "854850"
  },
  {
    "text": "that to create the certificates and put it in secrets or for DNS entries so that the end user connects the database from",
    "start": "854850",
    "end": "861870"
  },
  {
    "text": "the outside and for the instances and for eviction and pdb so we just added",
    "start": "861870",
    "end": "867990"
  },
  {
    "text": "last week a new reconciler for eviction and we'll talk about a bit later so we want to talk to the communities the GK",
    "start": "867990",
    "end": "875370"
  },
  {
    "text": "autoscaler I want to give him hints when it's safe to clean pots and and whatnot and we talk late a bit about it so we",
    "start": "875370",
    "end": "883200"
  },
  {
    "text": "separated this so our main reconciliation said the main reconciler",
    "start": "883200",
    "end": "889050"
  },
  {
    "start": "885000",
    "end": "983000"
  },
  {
    "text": "basically just builds up the current state and the target state and he does that by asking the mini reconcile s for",
    "start": "889050",
    "end": "895980"
  },
  {
    "text": "their view on the world so the main reconciler would ask the config mate reconcile for example what is your",
    "start": "895980",
    "end": "901620"
  },
  {
    "text": "current state and what is your target state so he would gather all those many information from the mini reconcile I",
    "start": "901620",
    "end": "908370"
  },
  {
    "text": "would aggregated and declare this the whole current state and the whole target",
    "start": "908370",
    "end": "913500"
  },
  {
    "text": "state and then he would loop over all the mini reconcile s so they are ordered",
    "start": "913500",
    "end": "919290"
  },
  {
    "text": "and when everything finished completely the reconciler a reconciliation for that",
    "start": "919290",
    "end": "924730"
  },
  {
    "text": "database would be marked as good ago no nothing needs to be changed or maybe",
    "start": "924730",
    "end": "930429"
  },
  {
    "text": "there's an error on the way so one of the reconciler marks an error he can't talk to communities or whatever so",
    "start": "930429",
    "end": "936939"
  },
  {
    "text": "reconciliation would stop at that point and the reconciler would fall to sleep and then maybe picks that database up in",
    "start": "936939",
    "end": "943209"
  },
  {
    "text": "the next reconciliation loop or maybe one of the mini reconciles requesters",
    "start": "943209",
    "end": "948610"
  },
  {
    "text": "changes so we separate the mini reconcile us are talking to cabana DS but they're reporting back to the main",
    "start": "948610",
    "end": "954819"
  },
  {
    "text": "reconciler that they've changed something so that the main reconsider would also step stopped at that point",
    "start": "954819",
    "end": "960429"
  },
  {
    "text": "when one of the mini reconciler requested a change and will pick it up in the next reconciliation who began to",
    "start": "960429",
    "end": "966639"
  },
  {
    "text": "give some time for the current action to take into place and we're pretty happy",
    "start": "966639",
    "end": "972369"
  },
  {
    "text": "about the current situation maybe it's not the most performant one so maybe as some one of you have some suggestions",
    "start": "972369",
    "end": "978549"
  },
  {
    "text": "how we maybe improve that but so far it was pretty reliable for us and let's",
    "start": "978549",
    "end": "984999"
  },
  {
    "start": "983000",
    "end": "1013000"
  },
  {
    "text": "dive into some examples so this box is a CID so someone creates a new co DS",
    "start": "984999",
    "end": "991239"
  },
  {
    "text": "saying I want to have an earful j database so the reconciler would pick it up and decide that he wants to start",
    "start": "991239",
    "end": "996579"
  },
  {
    "text": "three databases in the target state so he would start the first instance white until is healthy and then the second one",
    "start": "996579",
    "end": "1003689"
  },
  {
    "text": "and then the third one and then he would mark the entire our CID as healthy so",
    "start": "1003689",
    "end": "1009600"
  },
  {
    "text": "we're using conditions for that we come back to it later but let's do the next example sewer heal so we have three",
    "start": "1009600",
    "end": "1016350"
  },
  {
    "start": "1013000",
    "end": "1072000"
  },
  {
    "text": "running healthy instances so everything is fine but all of a sudden one of the",
    "start": "1016350",
    "end": "1021809"
  },
  {
    "text": "pots has a problem so it goes back to two running instances that are healthy",
    "start": "1021809",
    "end": "1027269"
  },
  {
    "text": "and we want to start a third one to go back to the desired state we have three running instances so then the operator",
    "start": "1027269",
    "end": "1034168"
  },
  {
    "text": "would just start a new port wait under it's healthy undone dijo of theory would",
    "start": "1034169",
    "end": "1039240"
  },
  {
    "text": "be marked as healthy and the great thing here about the this reconciliation pattern is that this is the exact same",
    "start": "1039240",
    "end": "1046620"
  },
  {
    "text": "logic for create at healing so we don't have dedicated failure handling here because we're",
    "start": "1046620",
    "end": "1052100"
  },
  {
    "text": "doing target and his target state calculation so this are the exact same steps here so this is really amazing for",
    "start": "1052100",
    "end": "1059990"
  },
  {
    "text": "us and if you think about how you would do this in the shell script for example this potentially could pretty messy but",
    "start": "1059990",
    "end": "1066560"
  },
  {
    "text": "in the undiscovered Eddie's way of engineering this is really clean and straightforward okay",
    "start": "1066560",
    "end": "1073090"
  },
  {
    "start": "1072000",
    "end": "1209000"
  },
  {
    "text": "let's look how we update a database how we do a zero downtime rolling update to a database so when we request a change",
    "start": "1073090",
    "end": "1081470"
  },
  {
    "text": "to the CID and this could have multiple reasons for example this in Union for JV",
    "start": "1081470",
    "end": "1086960"
  },
  {
    "text": "version or we have changed resources so the customer wants a bigger database you",
    "start": "1086960",
    "end": "1092240"
  },
  {
    "text": "wants more CPU or more memory or whatever or he wants to read that the password so neo4j itself has user",
    "start": "1092240",
    "end": "1098360"
  },
  {
    "text": "management but maybe the customer loss are the master password so we can reserve that for the customer so we want",
    "start": "1098360",
    "end": "1105950"
  },
  {
    "text": "to change it and that's a simple example the lowest note is the leader and the",
    "start": "1105950",
    "end": "1112160"
  },
  {
    "text": "idea here is to print leader elections so the leader the one who can talk to",
    "start": "1112160",
    "end": "1117440"
  },
  {
    "text": "for write requests can change but when you do an update you want to not do this you don't want to do more confusing in",
    "start": "1117440",
    "end": "1124400"
  },
  {
    "text": "the cluster so we want to preserve the leader as long as possible our strategy",
    "start": "1124400",
    "end": "1130370"
  },
  {
    "text": "here is to firstly start a new pot so that we go up to four as you notice the",
    "start": "1130370",
    "end": "1137120"
  },
  {
    "text": "majority changed now so in a class out of three you need the majority of two nodes to acknowledge and commit but when",
    "start": "1137120",
    "end": "1143900"
  },
  {
    "text": "you have four nodes in the cluster the majority all of a sudden is three so near itself would cope with that so that",
    "start": "1143900",
    "end": "1150380"
  },
  {
    "text": "you don't run into sprain scenarios or something so now you have a higher quorum sauce and if that node becomes",
    "start": "1150380",
    "end": "1157730"
  },
  {
    "text": "healthy the operator would start a new one so then we have five this core mister three-year and then he would",
    "start": "1157730",
    "end": "1164870"
  },
  {
    "text": "start to stop old notes so he would start a follower so we again we want to",
    "start": "1164870",
    "end": "1170720"
  },
  {
    "text": "keep the leader being a leader so he would start a follower and then we would go up to five again wait under there",
    "start": "1170720",
    "end": "1176660"
  },
  {
    "text": "notice how see and then we have here we have the target state so this is the Custer we",
    "start": "1176660",
    "end": "1182370"
  },
  {
    "text": "want to have three healthy new nodes in the cluster and then the operator would stop one of the followers and then he",
    "start": "1182370",
    "end": "1190290"
  },
  {
    "text": "would trigger a leader application that means he asked that that know to step down as a leader so that someone else",
    "start": "1190290",
    "end": "1196740"
  },
  {
    "text": "can pick up the leadership and after that happened we stopped that node and",
    "start": "1196740",
    "end": "1202160"
  },
  {
    "text": "then our update is completed and we can mark this year Diaz healthy again great",
    "start": "1202160",
    "end": "1210890"
  },
  {
    "start": "1209000",
    "end": "1377000"
  },
  {
    "text": "the next logical step is to update the operator and our first iteration was we",
    "start": "1210890",
    "end": "1218070"
  },
  {
    "text": "just deployed the new operator code the problem here was assuming you have I",
    "start": "1218070",
    "end": "1223370"
  },
  {
    "text": "don't know 50 100 running databases so the operator maybe you added a new",
    "start": "1223370",
    "end": "1228750"
  },
  {
    "text": "feature detected a change in the current state and the target state so he would loop over all hundred databases and",
    "start": "1228750",
    "end": "1234900"
  },
  {
    "text": "detect that there's something to be done so he would do one baby step start a new",
    "start": "1234900",
    "end": "1240060"
  },
  {
    "text": "pot for every of those hundred databases so all of a sudden you have hundred new stateful sets to be started probably",
    "start": "1240060",
    "end": "1246450"
  },
  {
    "text": "don't have enough capacity in you come and I discussed sir so this would kick in the GK autoscaler so you will get",
    "start": "1246450",
    "end": "1252630"
  },
  {
    "text": "loads of new couplets to complete your deployment and at the end you have loads",
    "start": "1252630",
    "end": "1258120"
  },
  {
    "text": "of key loads of new couplets and you have a low utilization and this will cost lots of money you probably don't",
    "start": "1258120",
    "end": "1265080"
  },
  {
    "text": "want to do this maybe you maybe you want we we didn't want it to spend that much",
    "start": "1265080",
    "end": "1270360"
  },
  {
    "text": "money so we introduced a number so the",
    "start": "1270360",
    "end": "1275790"
  },
  {
    "text": "resources are aware of the operator number that is currently responsible for them and the operator itself is aware of",
    "start": "1275790",
    "end": "1283380"
  },
  {
    "text": "its current number so the operator with the number 42 for example would only pick up databases that are assigned to",
    "start": "1283380",
    "end": "1290700"
  },
  {
    "text": "that operator and when we deploy new your praetor operator 42 the first thing",
    "start": "1290700",
    "end": "1296250"
  },
  {
    "text": "he does is nothing because you don't have any resources attached and now we",
    "start": "1296250",
    "end": "1303990"
  },
  {
    "text": "ordered the data and then we prioritize the databases in three levels low medium",
    "start": "1303990",
    "end": "1309090"
  },
  {
    "text": "and high so low priority databases would be canary databases or internal engineering",
    "start": "1309090",
    "end": "1316719"
  },
  {
    "text": "databases so no custom attached to them and then we would start with a low priority databases to roll them in and",
    "start": "1316719",
    "end": "1322719"
  },
  {
    "text": "see if our change was good and if the databases can complete the updates and so on and when we're confident and this",
    "start": "1322719",
    "end": "1328929"
  },
  {
    "text": "is good for low protégée databases we can go to medium and high quality database of high priority databases so",
    "start": "1328929",
    "end": "1336099"
  },
  {
    "text": "if we then change maybe 20% of our databases of the low priority to operate",
    "start": "1336099",
    "end": "1342729"
  },
  {
    "text": "a 42 he would pick it up he would see those resources in the reconciliation",
    "start": "1342729",
    "end": "1348069"
  },
  {
    "text": "loop and he would roll in like the first row of clusters he would do the rolling update we just described and then we",
    "start": "1348069",
    "end": "1354549"
  },
  {
    "text": "would do it for me the improper tea and maybe lastly for the high priority and that everything went good and no",
    "start": "1354549",
    "end": "1362619"
  },
  {
    "text": "databases are left for the old operator we can then safely shut down the old",
    "start": "1362619",
    "end": "1368529"
  },
  {
    "text": "operator version because it's not needed anymore and this is how we do like rolling deployments off the operator but",
    "start": "1368529",
    "end": "1377979"
  },
  {
    "start": "1377000",
    "end": "1506000"
  },
  {
    "text": "there's so much more to talk about so we're in 23 minutes so we have some time left there's so much more to talk about",
    "start": "1377979",
    "end": "1384669"
  },
  {
    "text": "so we're using heavily conditions to reflect if a Custer is fourth ruined or",
    "start": "1384669",
    "end": "1390789"
  },
  {
    "text": "available so in every conciliation you we ask the cluster for higher level",
    "start": "1390789",
    "end": "1396039"
  },
  {
    "text": "metrics so if it's the cluster available so do we have enough nodes inside the",
    "start": "1396039",
    "end": "1401169"
  },
  {
    "text": "raft group to perform right requests or if it's for pterence we have quorum size",
    "start": "1401169",
    "end": "1407919"
  },
  {
    "text": "plus one and we store it in conditions in the CD and after reports were using",
    "start": "1407919",
    "end": "1414249"
  },
  {
    "text": "the conditions in metrics so we're building dashboards out of it if the databases are photo ruined or if they",
    "start": "1414249",
    "end": "1421539"
  },
  {
    "text": "are available so that our SRE team can kick in when a database is only for",
    "start": "1421539",
    "end": "1426789"
  },
  {
    "text": "turrent is only available for let's say five minutes but not for turrent they can",
    "start": "1426789",
    "end": "1432189"
  },
  {
    "text": "actively go into and fix the problem to bring the database back to four pterence before it loses the available",
    "start": "1432189",
    "end": "1440939"
  },
  {
    "text": "another tea that he was to do this for eviction and put disruption budgets so",
    "start": "1441010",
    "end": "1446360"
  },
  {
    "text": "put disruption budget basically says for the autoscaler when you want in to roll in a new notes note pool through your",
    "start": "1446360",
    "end": "1452750"
  },
  {
    "text": "cluster that cluster out of three nodes is allowed that you can take one note",
    "start": "1452750",
    "end": "1458059"
  },
  {
    "text": "down at a time and to like to keep the cluster alive but this is only for the",
    "start": "1458059",
    "end": "1464480"
  },
  {
    "text": "happy case so what if one of the nodes is currently catching up so the cluster can't sacrifice any of the nodes",
    "start": "1464480",
    "end": "1471980"
  },
  {
    "text": "you can't yeah you could do it with the pot disruption budget but there's an annotation you can set to a pot to tell",
    "start": "1471980",
    "end": "1479390"
  },
  {
    "text": "the kubernetes autoscaler to not touch it when rolling in new a new node pool",
    "start": "1479390",
    "end": "1484789"
  },
  {
    "text": "and this is the safe to evict annotation so have a reconciler doing this every",
    "start": "1484789",
    "end": "1490010"
  },
  {
    "text": "reconciliation you checking if the database is for torrent and if the if the cluster is not for through and",
    "start": "1490010",
    "end": "1495200"
  },
  {
    "text": "setting the annotation to stop the autoscaler from killing node killing",
    "start": "1495200",
    "end": "1501289"
  },
  {
    "text": "pots and bringing the cluster in a not really good situation and we need to",
    "start": "1501289",
    "end": "1508370"
  },
  {
    "start": "1506000",
    "end": "1611000"
  },
  {
    "text": "talk about site casts so we try to stick to the official neo4j docker images as",
    "start": "1508370",
    "end": "1513530"
  },
  {
    "text": "close as possible and we're using the official name of which a docker images but there's loads of stuff inside guys",
    "start": "1513530",
    "end": "1519860"
  },
  {
    "text": "going on that we needed to add to make that happen so we have cycles for logging for metrics for initialization",
    "start": "1519860",
    "end": "1528400"
  },
  {
    "text": "for debug ability and all that kind of stuff because we wanted to be compatible",
    "start": "1528400",
    "end": "1534530"
  },
  {
    "text": "to the official release so when there's the new release we can get it right away without going through an insane build",
    "start": "1534530",
    "end": "1541580"
  },
  {
    "text": "pipeline on our own so we're using the official images for that and we added a",
    "start": "1541580",
    "end": "1547280"
  },
  {
    "text": "thing for debug ability so this is a really cool because when our operator",
    "start": "1547280",
    "end": "1552289"
  },
  {
    "text": "was in a state where it was not as good as it is today so when we needed to execute in the cluster quite often to",
    "start": "1552289",
    "end": "1558679"
  },
  {
    "text": "debug why something is not working properly we have the problem that we were in a race condition with the",
    "start": "1558679",
    "end": "1564740"
  },
  {
    "text": "operator trying to hear the custom so we have a lot of time that we executed in",
    "start": "1564740",
    "end": "1569750"
  },
  {
    "text": "the inner pod to Ted why it's not working and then the operator killed the instance underneath",
    "start": "1569750",
    "end": "1576440"
  },
  {
    "text": "our shells so we added a simple annotation and we called it we did we",
    "start": "1576440",
    "end": "1582110"
  },
  {
    "text": "didn't call it doing a touch mode but it is a dinner touch mode if the annotation is present the operator won't touch any",
    "start": "1582110",
    "end": "1589760"
  },
  {
    "text": "of the pots or any of the CDs so it will leave it alone so that we can go into",
    "start": "1589760",
    "end": "1596330"
  },
  {
    "text": "and repair what we think we need to do to bring it back to life and then we remove the annotation the operator kicks",
    "start": "1596330",
    "end": "1602780"
  },
  {
    "text": "in again for that cluster and then take takes over the the operation for the",
    "start": "1602780",
    "end": "1607880"
  },
  {
    "text": "database cool is everything happy then",
    "start": "1607880",
    "end": "1613720"
  },
  {
    "start": "1611000",
    "end": "1680000"
  },
  {
    "text": "kind of so we lost loads of pots over",
    "start": "1613720",
    "end": "1618799"
  },
  {
    "text": "the time loads of containers but we didn't didn't lost data because we we had the Peavey's in place and we had the",
    "start": "1618799",
    "end": "1624230"
  },
  {
    "text": "backups in place so this was really good so as I said we had some tough time when",
    "start": "1624230",
    "end": "1630860"
  },
  {
    "text": "we had problems like we're doing a rolling deployment or operator code is not as good as today and then the",
    "start": "1630860",
    "end": "1637040"
  },
  {
    "text": "autoscaler kicks in all of a sudden and kills some pots and sometimes he killed",
    "start": "1637040",
    "end": "1642049"
  },
  {
    "text": "the wrong one so the cluster was stuck so then we needed to go in and recover the cluster manually until the cluster",
    "start": "1642049",
    "end": "1648530"
  },
  {
    "text": "okay shut down those two ports at the Doudna touch annotation first shut down those two ports then mark that node as",
    "start": "1648530",
    "end": "1655910"
  },
  {
    "text": "you're the survivor remove the annotation so the operator kicks in and",
    "start": "1655910",
    "end": "1661130"
  },
  {
    "text": "then deuced the thing we have one healthy instance and we want to have two",
    "start": "1661130",
    "end": "1666230"
  },
  {
    "text": "more and then he's doing basically the create a government again so we had loads of this and every time we did that",
    "start": "1666230",
    "end": "1672520"
  },
  {
    "text": "we try to bring the manual steps we made into the operator code so we try to",
    "start": "1672520",
    "end": "1678200"
  },
  {
    "text": "learn from that okay so we have some time left before the question so would",
    "start": "1678200",
    "end": "1684860"
  },
  {
    "start": "1680000",
    "end": "1733000"
  },
  {
    "text": "shortly go over testing because we're you really feel strongly about testing our operator code and I last year in",
    "start": "1684860",
    "end": "1692480"
  },
  {
    "text": "Cuba and there was loads of discussions about testing so one to give you a short overview how we do it so we do testing",
    "start": "1692480",
    "end": "1698299"
  },
  {
    "text": "on different levels so the first thing is unit testing so we have loads of table tests so please be patient",
    "start": "1698299",
    "end": "1705179"
  },
  {
    "text": "I'm coming from a Java backgrounds we don't have table tests and super excited about table tests so what we do here is",
    "start": "1705179",
    "end": "1711869"
  },
  {
    "text": "we describe a fixture so the current state and then we describe our",
    "start": "1711869",
    "end": "1717359"
  },
  {
    "text": "expectation and then we have a tiny function like if the cluster is available so the calculation if I have",
    "start": "1717359",
    "end": "1723509"
  },
  {
    "text": "at least quorum size of available notes and this is looping although all of",
    "start": "1723509",
    "end": "1728879"
  },
  {
    "text": "those tests in the table test so we have hundreds of those and the next step up",
    "start": "1728879",
    "end": "1734669"
  },
  {
    "start": "1733000",
    "end": "1777000"
  },
  {
    "text": "would be into what we call integration tests so we mock the entire cabinet API",
    "start": "1734669",
    "end": "1740099"
  },
  {
    "text": "and in go language there is a library to also make those calls Bibles so you can",
    "start": "1740099",
    "end": "1746519"
  },
  {
    "text": "assert that something was called in that case we're testing a conflict map the",
    "start": "1746519",
    "end": "1752070"
  },
  {
    "text": "actual state is no conflict map is present the desired status that we want to have a conflict map so we're mocking",
    "start": "1752070",
    "end": "1757679"
  },
  {
    "text": "all the stuff then we ask the mini reconcile of config Maps please do the reconciliation and we expect that this",
    "start": "1757679",
    "end": "1766409"
  },
  {
    "text": "was true so the the reconciliation returned true and we assert that create",
    "start": "1766409",
    "end": "1772080"
  },
  {
    "text": "config map from the communities API was called ok so the next logical step up",
    "start": "1772080",
    "end": "1777570"
  },
  {
    "start": "1777000",
    "end": "1793000"
  },
  {
    "text": "would be what we call system integration tests where you have here Yama files and you do Q cuttle apply in Yama file and",
    "start": "1777570",
    "end": "1784889"
  },
  {
    "text": "then you expect a running cluster maybe five minutes later two minutes later whatever timespan later we don't have",
    "start": "1784889",
    "end": "1791969"
  },
  {
    "text": "that in that case what we have we have loads of automated and turn tests so we",
    "start": "1791969",
    "end": "1798509"
  },
  {
    "start": "1793000",
    "end": "1828000"
  },
  {
    "text": "heard under an test from our previous systems and those covered all the user stories we defined and those automated",
    "start": "1798509",
    "end": "1805950"
  },
  {
    "text": "and turn tests they would use the same api's as the UI would use so we're not",
    "start": "1805950",
    "end": "1811109"
  },
  {
    "text": "testing from the UI level but from the API level so we're using the the end",
    "start": "1811109",
    "end": "1816450"
  },
  {
    "text": "user facing API eyes to trigger a database create and then we're so that",
    "start": "1816450",
    "end": "1821639"
  },
  {
    "text": "somewhere in cabinet is someone later there would be a running database this is also really cool and then as I tease",
    "start": "1821639",
    "end": "1830279"
  },
  {
    "start": "1828000",
    "end": "1867000"
  },
  {
    "text": "that maybe before we are running Canaries so in every environment staging production and personal environments",
    "start": "1830279",
    "end": "1835799"
  },
  {
    "text": "we've run the canary databases so data with no customer touched it but other processes running constant load against",
    "start": "1835799",
    "end": "1842560"
  },
  {
    "text": "it and we run in chaos monkeys against those canary databases in staging in in",
    "start": "1842560",
    "end": "1849100"
  },
  {
    "text": "production and this gave us so much valuable insights about the cost of technology how to handle failure how",
    "start": "1849100",
    "end": "1856510"
  },
  {
    "text": "clients would react to failure so we learn loads of those using chaos monkeys",
    "start": "1856510",
    "end": "1864220"
  },
  {
    "text": "and also in production ok this is the time to thank you all very much if this",
    "start": "1864220",
    "end": "1871480"
  },
  {
    "start": "1867000",
    "end": "2411000"
  },
  {
    "text": "would be interesting to you please feel free to to ping me to talk to me and if",
    "start": "1871480",
    "end": "1877120"
  },
  {
    "text": "it's so interesting to you that you want to work with us you can follow that sneaky QR code there and most",
    "start": "1877120",
    "end": "1882730"
  },
  {
    "text": "importantly enjoy the conference thank you [Music]",
    "start": "1882730",
    "end": "1890099"
  },
  {
    "text": "so 3 minutes more of questions do someone want to ask a question",
    "start": "1894820",
    "end": "1901950"
  },
  {
    "text": "hey thanks for your talk you talked a little bit about leadership election yes",
    "start": "1915920",
    "end": "1921140"
  },
  {
    "text": "aware operation of you know things could you talk a little more about the data aspect of operating databases so for",
    "start": "1921140",
    "end": "1926840"
  },
  {
    "text": "instance when you kill nodes and spawn new notes you probably want to make sure they spawn in a location where the data is available to limit copying stuff",
    "start": "1926840",
    "end": "1933200"
  },
  {
    "text": "around how do you deal with things like corruptions at the data level you consider repairing things or do you just",
    "start": "1933200",
    "end": "1939920"
  },
  {
    "text": "kill the thing and try to get it from a backup or balance it from other nodes so could you talk a little more about the",
    "start": "1939920",
    "end": "1945380"
  },
  {
    "text": "data aspect okay I think so I I hope I got you a question correct so it was",
    "start": "1945380",
    "end": "1950630"
  },
  {
    "text": "quite noisy here so if I go to correct the question was how to deal with data corruption in case of failure so in the",
    "start": "1950630",
    "end": "1958070"
  },
  {
    "text": "in the raft protocol and make sure that you but you only can talk to the leader and if you don't have leadership you",
    "start": "1958070",
    "end": "1964550"
  },
  {
    "text": "can't perform right recurse so beside the transaction IDs of the actual",
    "start": "1964550",
    "end": "1971270"
  },
  {
    "text": "database we do have transaction IDs for the raft messages herself so when the",
    "start": "1971270",
    "end": "1976640"
  },
  {
    "text": "leader loses leadership so when he is not able to ping the other nodes if he",
    "start": "1976640",
    "end": "1983540"
  },
  {
    "text": "would detect that he's in a cluster and don't get the Kuril correlated health checks from the other nodes he would",
    "start": "1983540",
    "end": "1990320"
  },
  {
    "text": "step down the leadership so that he would recognize that he's not allowed to",
    "start": "1990320",
    "end": "1995360"
  },
  {
    "text": "write anymore what you could what could happen in theory when you on the leader",
    "start": "1995360",
    "end": "2000910"
  },
  {
    "text": "you get a transaction through and you get only one commit back from one of the",
    "start": "2000910",
    "end": "2006370"
  },
  {
    "text": "followers but not from the other one because maybe is already petitioned away or something and then you could both of",
    "start": "2006370",
    "end": "2014080"
  },
  {
    "text": "the nodes you that acknowledge the transaction this could potentially lead into problems when you just hit it in",
    "start": "2014080",
    "end": "2021460"
  },
  {
    "text": "that situation as as I described we're using PBS to that so currently it would",
    "start": "2021460",
    "end": "2029530"
  },
  {
    "text": "not be possible to kill the entire stay because as long as you don't kill the remote storage as well",
    "start": "2029530",
    "end": "2035830"
  },
  {
    "text": "we wouldn't lose the overall data set from those two nodes so we would then",
    "start": "2035830",
    "end": "2041350"
  },
  {
    "text": "try to bring back the nodes based on on those PVS but if the PB has also gone",
    "start": "2041350",
    "end": "2047470"
  },
  {
    "text": "like both TVs which we didn't had so far then",
    "start": "2047470",
    "end": "2053148"
  },
  {
    "text": "potentially we would lose the transaction because we don't have the data said because this was like a",
    "start": "2053149",
    "end": "2059608"
  },
  {
    "text": "failure of two nodes too quick after only though two nodes committed the",
    "start": "2059609",
    "end": "2064618"
  },
  {
    "text": "transaction and the third one didn't committed and the backup worker didn't got it so in that case you could",
    "start": "2064619",
    "end": "2071040"
  },
  {
    "text": "potentially lose data to be honest we",
    "start": "2071040",
    "end": "2076079"
  },
  {
    "text": "didn't lost the PV since we're using Kevin alleys so we really where we were",
    "start": "2076079",
    "end": "2083070"
  },
  {
    "text": "trusting Google to do good things with our pivot persistent disks and so far",
    "start": "2083070",
    "end": "2088980"
  },
  {
    "text": "one year of operating we didn't what so we weren't disappointed yet I did like",
    "start": "2088980",
    "end": "2094138"
  },
  {
    "text": "it doesn't it doesn't say that we will never lose data but so far with the",
    "start": "2094139",
    "end": "2099900"
  },
  {
    "text": "persistent volumes we were pretty good so far maybe if you're like if you want",
    "start": "2099900",
    "end": "2108030"
  },
  {
    "text": "to talk more but maybe you can talk it offline so I would love to her about you",
    "start": "2108030",
    "end": "2113040"
  },
  {
    "text": "like your concerns and what problems you probably experience to ask that question in that forum",
    "start": "2113040",
    "end": "2119069"
  },
  {
    "text": "okay thank you very much",
    "start": "2119069",
    "end": "2123440"
  },
  {
    "text": "[Applause] oh cool we have time for one more",
    "start": "2124210",
    "end": "2131960"
  },
  {
    "text": "question that's amazing thank you",
    "start": "2131960",
    "end": "2135670"
  },
  {
    "text": "hi there hi yeah you spoke about upgrading from operator 41 to 42 yes you",
    "start": "2139390",
    "end": "2145790"
  },
  {
    "text": "made a bit of a manual process have you gotten to a stage where you've automated that now yes so currently this is a",
    "start": "2145790",
    "end": "2153760"
  },
  {
    "text": "halfway automated process so every deploys goes through it release pipeline",
    "start": "2153760",
    "end": "2159200"
  },
  {
    "text": "so the operator would be deployed when someone says that he wants a particular version going from staging to production",
    "start": "2159200",
    "end": "2165940"
  },
  {
    "text": "well then currently the process I think it's partly manual where we say we",
    "start": "2165940",
    "end": "2172550"
  },
  {
    "text": "deploy lower priority databases first and when everything is happily we go to the next ones I think this is just like",
    "start": "2172550",
    "end": "2179240"
  },
  {
    "text": "this we net we call it phasing rollout of like of database updates is so new",
    "start": "2179240",
    "end": "2185119"
  },
  {
    "text": "that we want to experience currently first with it and one we're confident then our manual workflow is good enough",
    "start": "2185119",
    "end": "2192740"
  },
  {
    "text": "then we can put it in an automated way but maybe I'm wrong maybe I need to refer to our SOE people also in the",
    "start": "2192740",
    "end": "2199760"
  },
  {
    "text": "audience to to get back to tell you the truth but yeah the end goal is to automate everything Chiqui second one",
    "start": "2199760",
    "end": "2209540"
  },
  {
    "text": "yes how did you get to the metrics annotations that would be helpful to",
    "start": "2209540",
    "end": "2215750"
  },
  {
    "text": "your sres are we calculated so we do in the mini reconciliation for for tolerance and",
    "start": "2215750",
    "end": "2222920"
  },
  {
    "text": "availability we calculate if a cluster is 4,000 anyways because we're only",
    "start": "2222920",
    "end": "2229970"
  },
  {
    "text": "doing certain things when the database is fault-tolerant so we calculate it anyways and then we're doing like the",
    "start": "2229970",
    "end": "2237080"
  },
  {
    "text": "update on the cid in every reconciliation loop if we detect that there's a change so this is something we",
    "start": "2237080",
    "end": "2243890"
  },
  {
    "text": "we do in encode basically all right yes",
    "start": "2243890",
    "end": "2250280"
  },
  {
    "text": "hi so I'm also developing operators right now so what's the tipping point",
    "start": "2250280",
    "end": "2256880"
  },
  {
    "text": "for like when you decide if something should go to military coincide or like a",
    "start": "2256880",
    "end": "2262100"
  },
  {
    "text": "different operator business zone resource so if I got the question",
    "start": "2262100",
    "end": "2267590"
  },
  {
    "text": "correct it was how we decide if we want to pass in something to me reconcile",
    "start": "2267590",
    "end": "2272660"
  },
  {
    "text": "that Oh to another reconciler our to another operator like for example",
    "start": "2272660",
    "end": "2280130"
  },
  {
    "text": "for service you have one Operator and for backups you have another and yet one main operator that is your coinciding",
    "start": "2280130",
    "end": "2286820"
  },
  {
    "text": "resources only yeah so this is a list of operators so we do have all operators in",
    "start": "2286820",
    "end": "2292400"
  },
  {
    "text": "the list and we think we found a good way how to how to find the sequence of",
    "start": "2292400",
    "end": "2297620"
  },
  {
    "text": "operations we want to go through and then we always want to go through the operation through the reconciliation of",
    "start": "2297620",
    "end": "2304220"
  },
  {
    "text": "one mini reconciler after the others so we're only passing to the next one if",
    "start": "2304220",
    "end": "2310880"
  },
  {
    "text": "the previous step was was fine at that point so if if a mini reconciler",
    "start": "2310880",
    "end": "2315920"
  },
  {
    "text": "and position two maybe request the changes and we would not go to the next level we equate to the next entire",
    "start": "2315920",
    "end": "2322370"
  },
  {
    "text": "reconciliation loop to them check in step two that everything is okay and then go to the next step so we only",
    "start": "2322370",
    "end": "2329240"
  },
  {
    "text": "always going through the entire sequence of freaking silos and only we're going to the next step if everything was fine",
    "start": "2329240",
    "end": "2335900"
  },
  {
    "text": "in the current step and then go to the next one and then go to the next one and go to the next one okay one more",
    "start": "2335900",
    "end": "2344200"
  },
  {
    "text": "I'm terracing in data two and it's not clear to me if you wanna talk about",
    "start": "2344200",
    "end": "2350240"
  },
  {
    "text": "persistent volume you are talking about file system volume or if you using some",
    "start": "2350240",
    "end": "2356060"
  },
  {
    "text": "overlay you talk about the abs from Amazon I know if you're using it in",
    "start": "2356060",
    "end": "2362690"
  },
  {
    "text": "overlay what kind of latency some problems in performances it brings yeah",
    "start": "2362690",
    "end": "2369020"
  },
  {
    "text": "so if I got the question correct this is about we're using remote storage so",
    "start": "2369020",
    "end": "2374090"
  },
  {
    "text": "we're facing latency yeah so yeah this is very true",
    "start": "2374090",
    "end": "2380500"
  },
  {
    "text": "I don't know how Google is managing this but the latency is not that bad as you",
    "start": "2380550",
    "end": "2385800"
  },
  {
    "text": "would expect it so this blog post on on the on the Google computing website why",
    "start": "2385800",
    "end": "2391380"
  },
  {
    "text": "they say how much performance you get in order like related to the instance I",
    "start": "2391380",
    "end": "2397410"
  },
  {
    "text": "like this size you're buying so you get more i ops if you're buying big inside like pick this and so on there's a huge",
    "start": "2397410",
    "end": "2403980"
  },
  {
    "text": "blog post about that and we're trying to get the best performance for our of our customers but you're right always when",
    "start": "2403980",
    "end": "2411510"
  },
  {
    "text": "you try when you're talking to remote search you have you you will lose performance that's why we maybe want to",
    "start": "2411510",
    "end": "2418160"
  },
  {
    "text": "change that in future for near 4 J itself it's kind of easy in comparison",
    "start": "2418160",
    "end": "2426210"
  },
  {
    "text": "to other databases because you don't have random reads so often an ephah J you would have way more reads than you",
    "start": "2426210",
    "end": "2434550"
  },
  {
    "text": "have rights and most of the time the reads can surf by memory so you don't",
    "start": "2434550",
    "end": "2439830"
  },
  {
    "text": "have as you would have in Cassandra or something while you have random access to whatever plays in your disk you",
    "start": "2439830",
    "end": "2446640"
  },
  {
    "text": "typically don't have that in AO so for the read it's not that important because you're mostly surf it from memory so you",
    "start": "2446640",
    "end": "2452580"
  },
  {
    "text": "don't have the performance impact there and for the right yeah there they take longer that you if you would have like a",
    "start": "2452580",
    "end": "2459150"
  },
  {
    "text": "single instance running on an SSD and the same in the same box yeah that's true",
    "start": "2459150",
    "end": "2465619"
  },
  {
    "text": "okay thank you very much and thank you for all the questions",
    "start": "2468470",
    "end": "2474349"
  }
]