[
  {
    "text": "hi everyone um my name is uh Abdullah and I'm here to present um our work on",
    "start": "280",
    "end": "6200"
  },
  {
    "text": "the leader workers set API for distributed inference um this is a joint work with uh my colleague ruping and um",
    "start": "6200",
    "end": "15080"
  },
  {
    "text": "and other uh people in the community so before I get started let's just show",
    "start": "15080",
    "end": "20560"
  },
  {
    "text": "hands how many people are familiar with llm serving okay that's good and how many of",
    "start": "20560",
    "end": "26320"
  },
  {
    "text": "you are familiar with um distribu multi node multi-host",
    "start": "26320",
    "end": "33120"
  },
  {
    "text": "serving okay that's good and how many are familiar with disaggregated",
    "start": "33120",
    "end": "39280"
  },
  {
    "text": "serving only three okay and how many of you have a flight at",
    "start": "39480",
    "end": "46480"
  },
  {
    "text": "7:45 okay um so most likely I'm going to run",
    "start": "46960",
    "end": "52520"
  },
  {
    "text": "out of here right after this talk I'm sorry very sorry but I had to catch my flight um but I'm really happy to to",
    "start": "52520",
    "end": "60320"
  },
  {
    "text": "discuss anything related to distributed inference and leader worker Set uh through our like you know Community uh",
    "start": "60320",
    "end": "68720"
  },
  {
    "text": "channels all right so what is the problem we're trying to solve here well large language models are getting bigger",
    "start": "68720",
    "end": "75280"
  },
  {
    "text": "and bigger by definition they are large but they're getting larger and larger so we start like right now average model",
    "start": "75280",
    "end": "81759"
  },
  {
    "text": "size is 70b like if you ask people they either use 7B Lama for example and now",
    "start": "81759",
    "end": "87159"
  },
  {
    "text": "they are more frequently using 70b Lama models those models still fit in the GPU",
    "start": "87159",
    "end": "93280"
  },
  {
    "text": "memory um of one or two chaps like if you think about an a100 for example or",
    "start": "93280",
    "end": "100479"
  },
  {
    "text": "h100 GPU and a100 for example have like 40 GB of memory right so 7B um uh model",
    "start": "100479",
    "end": "109240"
  },
  {
    "text": "uh with um you know uh full Precision for example 16",
    "start": "109240",
    "end": "115119"
  },
  {
    "text": "bit it it can fit easily however now we're crossing that",
    "start": "115119",
    "end": "121479"
  },
  {
    "text": "into a new territory even the open moders like we all we already know that",
    "start": "121479",
    "end": "127240"
  },
  {
    "text": "for example GPT uh even Gemini they are probably one trillion plus right so that's like",
    "start": "127240",
    "end": "134519"
  },
  {
    "text": "that's not a secret but even the open models that we have out there uh are",
    "start": "134519",
    "end": "139720"
  },
  {
    "text": "getting larger and so the latest one is Lama 3 405b that meta released um a few",
    "start": "139720",
    "end": "146200"
  },
  {
    "text": "months back those models they barely fit the GPU memory of a single machine most",
    "start": "146200",
    "end": "154160"
  },
  {
    "text": "machines the largest one for example on any cloud provider you got eight a100",
    "start": "154160",
    "end": "160000"
  },
  {
    "text": "or8 h100 gpus so if you do a math like for 405b you need 800 GB full Precision uh s",
    "start": "160000",
    "end": "169640"
  },
  {
    "text": "so that that would not fit the GPU machine um a single GPU machine so instantly you need to have at least two",
    "start": "169640",
    "end": "176800"
  },
  {
    "text": "um even if you do like you know quantization and um make it like you know 400 GB size",
    "start": "176800",
    "end": "183920"
  },
  {
    "text": "it would fit a single machine but you wouldn't have enough space on the GPU memory to do a lot large enough batches",
    "start": "183920",
    "end": "192120"
  },
  {
    "text": "right um and so we expect that this 405p is just also",
    "start": "192120",
    "end": "197560"
  },
  {
    "text": "the beginning it's going to continue to grow granted that gpus will have more memory will have bigger machines but",
    "start": "197560",
    "end": "204519"
  },
  {
    "text": "models will also grow larger and larger now on the TP side we even have a more",
    "start": "204519",
    "end": "210519"
  },
  {
    "text": "like you know critical problem like with tpus we scale horizontally like a single",
    "start": "210519",
    "end": "216760"
  },
  {
    "text": "TPU device has like you know uh 32 or even 16 gab of of memory um and a single",
    "start": "216760",
    "end": "223080"
  },
  {
    "text": "node actually has only four chips and so at at Google we like you know quickly",
    "start": "223080",
    "end": "230360"
  },
  {
    "text": "require multi no multi-host serving so that's why we're trying to think about",
    "start": "230360",
    "end": "235400"
  },
  {
    "text": "how to solve this problem um and so",
    "start": "235400",
    "end": "241560"
  },
  {
    "text": "that's generally the the the the the problem the issue here is that we don't",
    "start": "241560",
    "end": "247239"
  },
  {
    "text": "have an automated way of deploying such large language models um on on kuber let",
    "start": "247239",
    "end": "253400"
  },
  {
    "text": "me explain why so a single model server is a single pod",
    "start": "253400",
    "end": "259280"
  },
  {
    "text": "right so it fit single machine even if you give it eight gpus on like it's going to work just fine like you could",
    "start": "259280",
    "end": "265000"
  },
  {
    "text": "you could like you know uh use a deployment you create a pod per per server and that server could you know uh",
    "start": "265000",
    "end": "272520"
  },
  {
    "text": "um occupy all eight gpus however once you go multi-gpu multi multi node you",
    "start": "272520",
    "end": "278160"
  },
  {
    "text": "don't have the concept of a pod that runs containers across nodes right and",
    "start": "278160",
    "end": "284520"
  },
  {
    "text": "so now we have a we need to have a New Concept okay how do I deploy a model",
    "start": "284520",
    "end": "289919"
  },
  {
    "text": "several multiple Parts you can do it right you can use a stateful set run for",
    "start": "289919",
    "end": "295240"
  },
  {
    "text": "example in the case of 405b on on uh an A3 machine for example on on Google",
    "start": "295240",
    "end": "301320"
  },
  {
    "text": "Cloud um you can create state of two pods but then okay how do I scale up",
    "start": "301320",
    "end": "306720"
  },
  {
    "text": "like I want another one the third one fourth one are you going to manually try to create new stateful set or script it",
    "start": "306720",
    "end": "313360"
  },
  {
    "text": "you need to hook it up to HPA right like how how is that going to work and so that's why we propos this new API called",
    "start": "313360",
    "end": "318520"
  },
  {
    "text": "leader workers set so the leader worker said API is as",
    "start": "318520",
    "end": "326280"
  },
  {
    "text": "just as I described um it's an API that basically tries to create a",
    "start": "326280",
    "end": "331880"
  },
  {
    "text": "deployment of super pods by a super pod I mean a pod a super like a think of it",
    "start": "331880",
    "end": "338000"
  },
  {
    "text": "like as a pod that runs processes across multiple nodes but it acts as a",
    "start": "338000",
    "end": "343479"
  },
  {
    "text": "unit and so the idea here is that I want to be",
    "start": "343479",
    "end": "348960"
  },
  {
    "text": "able to create groups of part as a unit scale them up and down behave exactly like a",
    "start": "348960",
    "end": "354080"
  },
  {
    "text": "deployment as much as possible to Plug and Play like a deployment like if you if you are doing single horse you use",
    "start": "354080",
    "end": "359360"
  },
  {
    "text": "deploy deployment if you want to use mult if you have a multi-host uh workload you just basically remove the deployment plug in leader worker set and",
    "start": "359360",
    "end": "366560"
  },
  {
    "text": "everything should work just as fine so how did we do this take a look uh at the right side so",
    "start": "366560",
    "end": "373080"
  },
  {
    "text": "what we did was basically as as like you know all custom apis are we use",
    "start": "373080",
    "end": "378319"
  },
  {
    "text": "crd the way that we did it is we create we we structur it as like",
    "start": "378319",
    "end": "384319"
  },
  {
    "text": "two level so we have built on top of stateful set API when not trying to",
    "start": "384319",
    "end": "390000"
  },
  {
    "text": "reinvent the wheel always trying to compose from components that work and prove to be working really well so that",
    "start": "390000",
    "end": "397160"
  },
  {
    "text": "stateful set that we created at the top we call it the leaders stateful set is the one that's going to",
    "start": "397160",
    "end": "403599"
  },
  {
    "text": "manage the the groups notice that in each group it's",
    "start": "403599",
    "end": "408720"
  },
  {
    "text": "not homogeneous we have what we call like a dual template group there's one",
    "start": "408720",
    "end": "415080"
  },
  {
    "text": "pod with one template and then a group of PODS we call them call them the workers that are that can be created",
    "start": "415080",
    "end": "422639"
  },
  {
    "text": "from a different template the leader pods are again created using the state like the leader",
    "start": "422639",
    "end": "428319"
  },
  {
    "text": "state will set at the top so when you create an lws a stateful set leader state is going to be created state will",
    "start": "428319",
    "end": "434639"
  },
  {
    "text": "set API and controller kicks in is going to create however many replicas you want",
    "start": "434639",
    "end": "440560"
  },
  {
    "text": "now the leader worker set controller what it's going to do is okay for each leader part that gets created I'm going",
    "start": "440560",
    "end": "447199"
  },
  {
    "text": "to create an a stateful set the worker state will said and assign the owner",
    "start": "447199",
    "end": "453280"
  },
  {
    "text": "assign that pod the leader pod to it as an owner and so the leader pod is going to each each pod in the",
    "start": "453280",
    "end": "460400"
  },
  {
    "text": "leader state for is going to own a stateful set that represents the workers why did we use a stateful set",
    "start": "460400",
    "end": "467000"
  },
  {
    "text": "again we're composing but also because we need stable indices we need stable",
    "start": "467000",
    "end": "472080"
  },
  {
    "text": "DNS host names as well that is extremely necessary to enable",
    "start": "472080",
    "end": "478240"
  },
  {
    "text": "distributed um uh distributed um inference because if you think about it",
    "start": "478240",
    "end": "483639"
  },
  {
    "text": "like each group is basically a distributed job of some sort so usually like probably most you are familiar here",
    "start": "483639",
    "end": "490960"
  },
  {
    "text": "you use if you want to use VM you're going to uh instantiate Ray underneath it and then you've got like you know the",
    "start": "490960",
    "end": "497199"
  },
  {
    "text": "rhead is going to run on the leader stateful set and then the workers are going to run on the workers uh um",
    "start": "497199",
    "end": "503319"
  },
  {
    "text": "stateful Set uh now as I mentioned I want to go quickly about like the set of features that we Implement that we have",
    "start": "503319",
    "end": "509240"
  },
  {
    "text": "here but I I will also later hand it over to ruping to go into detail how we",
    "start": "509240",
    "end": "514399"
  },
  {
    "text": "implemented them um so we we have also group startup policy again um the idea",
    "start": "514399",
    "end": "520880"
  },
  {
    "text": "here is that we're only going to create the workers state will set only when the leader is ready because a lot of",
    "start": "520880",
    "end": "526640"
  },
  {
    "text": "Frameworks require that the leader is ready basically it's running because they are going to connect to it back and",
    "start": "526640",
    "end": "532080"
  },
  {
    "text": "so that makes it easier um for for these applic for these Frameworks to uh start up quickly um we also support as I",
    "start": "532080",
    "end": "539640"
  },
  {
    "text": "mentioned horizontal scaling um again as each group as a unit gets scaled up and",
    "start": "539640",
    "end": "545279"
  },
  {
    "text": "down um we have group restart we basically we're restarting the super",
    "start": "545279",
    "end": "550959"
  },
  {
    "text": "part as a unit again we always want to think about that group as a unit we don't scale up and down the group itself",
    "start": "550959",
    "end": "557279"
  },
  {
    "text": "we scale up up and down the number of groups um we also have support for Rolling updates as we like we don't roll",
    "start": "557279",
    "end": "563880"
  },
  {
    "text": "do a rolling update within a group we we roll updates the group so it's a group as a unit is going to be updated and",
    "start": "563880",
    "end": "569839"
  },
  {
    "text": "then we're doing it one by one and grouping is going to talk in detail how we do that and implement it and finally",
    "start": "569839",
    "end": "575760"
  },
  {
    "text": "it's we have placement policy again usually with multi-host serving what you",
    "start": "575760",
    "end": "580839"
  },
  {
    "text": "need to do is you need to place the workers on within the same network topology with tpus is usually a TPU",
    "start": "580839",
    "end": "587760"
  },
  {
    "text": "slice where the nodes are connected with an ICI link just special high high high",
    "start": "587760",
    "end": "594279"
  },
  {
    "text": "high high uh communication uh interconnect with gpus you probably want to place them on the same rack and then",
    "start": "594279",
    "end": "599800"
  },
  {
    "text": "with the new GPU generation it's probably going to be the same NV NV link domain um so we have that as well uh",
    "start": "599800",
    "end": "606600"
  },
  {
    "text": "with that I'm going to hand it to Ring to talk in detail about um horizontal",
    "start": "606600",
    "end": "613279"
  },
  {
    "text": "scaling",
    "start": "614279",
    "end": "617279"
  },
  {
    "text": "I oh second Al Sor it",
    "start": "620720",
    "end": "626920"
  },
  {
    "text": "works yeah howo everyone uh I'm uh first join C so a little L if I yeah",
    "start": "626959",
    "end": "634160"
  },
  {
    "text": "so um today I will be cover mostly the informtion detail of the leader work Set",
    "start": "634160",
    "end": "640040"
  },
  {
    "text": "uh yeah for the Autos scaling part so leader work set actually exposes uh",
    "start": "640040",
    "end": "645120"
  },
  {
    "text": "scale sub resources to allow HPA to adjust the number of replicas and that",
    "start": "645120",
    "end": "651160"
  },
  {
    "text": "way uh the way that replicas uh scaled up and down is through managing the",
    "start": "651160",
    "end": "656360"
  },
  {
    "text": "number of leader ports in the lader St set uh below is a example how we",
    "start": "656360",
    "end": "661839"
  },
  {
    "text": "configure um the uh HP uh Le work set for uh using HPA to Auto scale so uh",
    "start": "661839",
    "end": "671160"
  },
  {
    "text": "this is a end to end flow chart and once we configure The Matrix um for hp2 scale",
    "start": "671160",
    "end": "678120"
  },
  {
    "text": "for example like the prefer U log size uh or of a motel server and then once",
    "start": "678120",
    "end": "685000"
  },
  {
    "text": "like we see this Matrix SP up then HP will trigger the sub resources of the",
    "start": "685000",
    "end": "690760"
  },
  {
    "text": "Elda base and set the little work as replicas to scale up and as we can see",
    "start": "690760",
    "end": "697760"
  },
  {
    "text": "previously only have one po group here once uh little work um replica number is",
    "start": "697760",
    "end": "703519"
  },
  {
    "text": "adjusted it will spin up a new leader port and the new worker step set will be",
    "start": "703519",
    "end": "709200"
  },
  {
    "text": "created as well uh which will share the same life cycle and the DU Port group will come up and running",
    "start": "709200",
    "end": "718040"
  },
  {
    "text": "uh next part is fure handling nowadays most distributed INF workloads needs to be reinitiated when",
    "start": "719360",
    "end": "726360"
  },
  {
    "text": "adding process or worker in the group fails so this is a common property of this distributed M Frameworks like a",
    "start": "726360",
    "end": "733519"
  },
  {
    "text": "mode controller jacks or distributed pie torch and how we make it works and",
    "start": "733519",
    "end": "738639"
  },
  {
    "text": "little work said is that when any port or container restart in the single serving replica and it will require the",
    "start": "738639",
    "end": "746480"
  },
  {
    "text": "entire replica to be recreated or restarted for example here as we can see in the",
    "start": "746480",
    "end": "753519"
  },
  {
    "text": "port group one we have a worker running here and when adding port or container",
    "start": "753519",
    "end": "758920"
  },
  {
    "text": "restarted in this worker or like in adding like the worker Port here or leader Port here so leader work set will",
    "start": "758920",
    "end": "765079"
  },
  {
    "text": "detected those and restart the entire Port group one which uh this port group will be",
    "start": "765079",
    "end": "771800"
  },
  {
    "text": "recreated and then the the initialization process will uh run again for this uh Distributing inference",
    "start": "771800",
    "end": "778519"
  },
  {
    "text": "workflow next one is topology aware scheduling so",
    "start": "778519",
    "end": "784399"
  },
  {
    "text": "in distributed inference we need to schedule the same group of ports on",
    "start": "784399",
    "end": "789480"
  },
  {
    "text": "nodes within the same network topology to minimize a cross node communication",
    "start": "789480",
    "end": "795040"
  },
  {
    "text": "overhead so one port group will be exclusively scheduled to one topology",
    "start": "795040",
    "end": "800320"
  },
  {
    "text": "domain for example a note group a load pool so one port group is a single",
    "start": "800320",
    "end": "806240"
  },
  {
    "text": "multi-host serving instance and a topolog domain here is a group of nodes",
    "start": "806240",
    "end": "812079"
  },
  {
    "text": "that shares the same node ENB which C categorize them to be the same group for",
    "start": "812079",
    "end": "817720"
  },
  {
    "text": "example a multihost TP slice can be a topology domain here and also a slice of",
    "start": "817720",
    "end": "823519"
  },
  {
    "text": "a GPU rank or any like a uh a group of",
    "start": "823519",
    "end": "828600"
  },
  {
    "text": "those that have a very high uh nwork",
    "start": "828600",
    "end": "834160"
  },
  {
    "text": "collected uh in the picture below as we can see we have two pole group here and",
    "start": "834160",
    "end": "839680"
  },
  {
    "text": "we have two node group as well uh which is a TPU multihost slice and this TPU host are collected by the ICI which is",
    "start": "839680",
    "end": "847320"
  },
  {
    "text": "very fast for data transferring and each leader and worker Port will be scheduled on one TP host or TP",
    "start": "847320",
    "end": "854399"
  },
  {
    "text": "node yeah and those are belongs to the a replica and those are belongs to The topolog",
    "start": "854399",
    "end": "861680"
  },
  {
    "text": "Domain this is how we implemented the to OFW scheduling so we use um port a fin",
    "start": "862519",
    "end": "869240"
  },
  {
    "text": "and Port anti Affinity uh with like a a unique K and unique value to ensure like",
    "start": "869240",
    "end": "875680"
  },
  {
    "text": "the uh port in the same group will only be scheduled on the same topology",
    "start": "875680",
    "end": "881759"
  },
  {
    "text": "domain yeah next one is ring",
    "start": "881759",
    "end": "887240"
  },
  {
    "text": "update uh support So leader work said support ring update uh strategy with Max",
    "start": "887240",
    "end": "893279"
  },
  {
    "text": "unvailable and Max search setting so max unvailable here means like let's say we we have five",
    "start": "893279",
    "end": "899639"
  },
  {
    "text": "uh replicas so we only ensures uh at most uh Max number a Max available",
    "start": "899639",
    "end": "907120"
  },
  {
    "text": "number of replicas is not available right now and with Max search supports that when we are running the rolling",
    "start": "907120",
    "end": "913639"
  },
  {
    "text": "update we will always have Max SE number of replicas uh like being brought up",
    "start": "913639",
    "end": "919920"
  },
  {
    "text": "before we proceeding with the running upgrade and each replica is a unit for",
    "start": "919920",
    "end": "926199"
  },
  {
    "text": "updating the triggering of the up upgrade is a template hash that be",
    "start": "926199",
    "end": "931639"
  },
  {
    "text": "generated through the leader work template and some API fails that we believe is important uh that we need to",
    "start": "931639",
    "end": "939120"
  },
  {
    "text": "have the all the replica to be upgraded so this hash will be uh stored as a",
    "start": "939120",
    "end": "945160"
  },
  {
    "text": "label on the leader St Port template and when this hash ble changed it will like",
    "start": "945160",
    "end": "952440"
  },
  {
    "text": "trigger the leader step to start the Drone update and upgrade process um site like",
    "start": "952440",
    "end": "960199"
  },
  {
    "text": "the leader work set actually relies on the leader stoc set to manage the process of the upgrade the upgrade will",
    "start": "960199",
    "end": "966560"
  },
  {
    "text": "stop when the upgrade replica already upgrade replica on Hy for example let's say we have five",
    "start": "966560",
    "end": "972560"
  },
  {
    "text": "replicas and we already upgraded a two replica but like the first replica is UN Healy right now at this time we will",
    "start": "972560",
    "end": "979240"
  },
  {
    "text": "stop the upgrade and and like stop there and like the process of the this is",
    "start": "979240",
    "end": "985680"
  },
  {
    "text": "managed by the leader St partition so so whenever we are upgrading right and uh",
    "start": "985680",
    "end": "992120"
  },
  {
    "text": "we reach let's say we reach a thir replica and we will use partition to",
    "start": "992120",
    "end": "998000"
  },
  {
    "text": "manage like a uh the progress of the uh upgrade only if we change the",
    "start": "998000",
    "end": "1003040"
  },
  {
    "text": "little partition number then the upgrade will",
    "start": "1003040",
    "end": "1007959"
  },
  {
    "text": "pered and for the uh upgrading of a single replica the mechanism here is like we will restart this uh upgrading",
    "start": "1008279",
    "end": "1016639"
  },
  {
    "text": "replica and so like recreate the leader port and the because of worker C is have",
    "start": "1016639",
    "end": "1022959"
  },
  {
    "text": "a same life cycle with the leader Port so when we deleted the leader Port the workers of state of will also be deleted",
    "start": "1022959",
    "end": "1031000"
  },
  {
    "text": "uh at the same time so uh when we are upgrading this single replica like we are always doing this through recreating",
    "start": "1031000",
    "end": "1037880"
  },
  {
    "text": "the entire Port group on the right side is a picture of how this upgrade process happen uh we",
    "start": "1037880",
    "end": "1045120"
  },
  {
    "text": "always upgraded the uh replicas from the largest index to the small smallest",
    "start": "1045120",
    "end": "1050440"
  },
  {
    "text": "index so for example here we upgrade Port group three and then Port group two",
    "start": "1050440",
    "end": "1055799"
  },
  {
    "text": "all the way to Port group z this is a larger picture of how up",
    "start": "1055799",
    "end": "1064480"
  },
  {
    "text": "grade works I will skip this uh for now yeah next we will uh show several",
    "start": "1064480",
    "end": "1073200"
  },
  {
    "text": "example how we run the multi node inference on GP and TPU this is a seta for uh",
    "start": "1073200",
    "end": "1079799"
  },
  {
    "text": "running multi Noe inference on GPU uh as as we can see from right side U we have",
    "start": "1079799",
    "end": "1086640"
  },
  {
    "text": "we have two A3 nodes with h100 GPU chips using compact placement and the the good",
    "start": "1086640",
    "end": "1095200"
  },
  {
    "text": "side of compact placement is that we were collocating this GPU nodes to minimize the cross node communication",
    "start": "1095200",
    "end": "1101559"
  },
  {
    "text": "latency especially this decision transfer and the m r time we use here is",
    "start": "1101559",
    "end": "1106880"
  },
  {
    "text": "like we use the V plus r and we are running n 345b model with",
    "start": "1106880",
    "end": "1112400"
  },
  {
    "text": "tens Paris equals to 8 which is as we can see here number of gpus per node",
    "start": "1112400",
    "end": "1118520"
  },
  {
    "text": "because we use remember two A3 nodes with eight h100 GPU chips so the tensor",
    "start": "1118520",
    "end": "1124520"
  },
  {
    "text": "power here we use eight and this will ensures the all together happen which",
    "start": "1124520",
    "end": "1131280"
  },
  {
    "text": "requires a lot much larger uh data transfer uh throughput will only happen",
    "start": "1131280",
    "end": "1136880"
  },
  {
    "text": "within one GPU node and we use the piper paron as two which means like we split",
    "start": "1136880",
    "end": "1142919"
  },
  {
    "text": "the model layers between two uh GP nodes and we in each model of order paath we",
    "start": "1142919",
    "end": "1148320"
  },
  {
    "text": "only ensure like the latency uh penalty to this cross node communication is very",
    "start": "1148320",
    "end": "1153919"
  },
  {
    "text": "minimum when we are doing the uh INF",
    "start": "1153919",
    "end": "1159120"
  },
  {
    "text": "request and ideally this GPU node should have RDMA enabled which is like a",
    "start": "1159679",
    "end": "1165480"
  },
  {
    "text": "accelerated uh direct memory access",
    "start": "1165480",
    "end": "1170720"
  },
  {
    "text": "next one is the example for TP multi host",
    "start": "1172280",
    "end": "1178000"
  },
  {
    "text": "inference right side is how we set it set it up so we will be using two TP",
    "start": "1178000",
    "end": "1184200"
  },
  {
    "text": "slid of V5 V16 which will have four TP host uh each TP host will",
    "start": "1184200",
    "end": "1190320"
  },
  {
    "text": "have four TP chips as well uh we will have five ports per replica running on",
    "start": "1190320",
    "end": "1196320"
  },
  {
    "text": "four nodes one one uh one node will be running one leader",
    "start": "1196320",
    "end": "1201960"
  },
  {
    "text": "port and as well as like one worker Port as on the worker Port we will running the passway worker and one leader Port",
    "start": "1201960",
    "end": "1208559"
  },
  {
    "text": "will be run in the model server which is Ja program and some like proxy R manager",
    "start": "1208559",
    "end": "1214320"
  },
  {
    "text": "which is like the M Stack uh so the model Ser uh model",
    "start": "1214320",
    "end": "1221080"
  },
  {
    "text": "server we will be using is is the jream TP inference server um password is a",
    "start": "1221080",
    "end": "1226360"
  },
  {
    "text": "distributed compute uh run time here and we will still be running the Lama 3 45b",
    "start": "1226360",
    "end": "1233159"
  },
  {
    "text": "model with only tensor prism because uh all tpus are conncted with ICI it's a",
    "start": "1233159",
    "end": "1239840"
  },
  {
    "text": "two-dimension topology so it's is very good at uh transferring data uh so using",
    "start": "1239840",
    "end": "1247520"
  },
  {
    "text": "tensor PR here is like already good enough and cross host communication only will happen through inter chip",
    "start": "1247520",
    "end": "1254080"
  },
  {
    "text": "interconnect",
    "start": "1254080",
    "end": "1257080"
  },
  {
    "text": "next we will discuss about another major uh Distributing invance uh work type",
    "start": "1259919",
    "end": "1265280"
  },
  {
    "text": "which is called this ACC serving so uh the process of a LM inference can be",
    "start": "1265280",
    "end": "1271039"
  },
  {
    "text": "splitted into two phases which is pref and decode prefill is much compute bound",
    "start": "1271039",
    "end": "1278960"
  },
  {
    "text": "and decode pH is much more memory bound and the two faces will interfere with",
    "start": "1278960",
    "end": "1284640"
  },
  {
    "text": "each other running on one server because they will compute for resources and um and like when we are running the",
    "start": "1284640",
    "end": "1291840"
  },
  {
    "text": "decode Loop whenever there's a compute L inference reest comes in the decod will",
    "start": "1291840",
    "end": "1297400"
  },
  {
    "text": "be interrupted and to do the prefitting of the request so all this kind of thing will cause a RoR efficiency and",
    "start": "1297400",
    "end": "1304320"
  },
  {
    "text": "unpredictable latency as well so to address this problem uh we uh could run",
    "start": "1304320",
    "end": "1310640"
  },
  {
    "text": "this a serving which will split the pref and decode stages uh to run on separate",
    "start": "1310640",
    "end": "1317120"
  },
  {
    "text": "servers this will brings the following benefits so first one is reduce the prefer decod interference so preferes",
    "start": "1317120",
    "end": "1324400"
  },
  {
    "text": "you usually work at bad size one and decode pH running with a much larger",
    "start": "1324400",
    "end": "1330640"
  },
  {
    "text": "batch size so you know the prefer stage when we're doing prefitting with the new",
    "start": "1330640",
    "end": "1336039"
  },
  {
    "text": "incoming request we will always interrupt in the decod loop and this will uh no matter make the hbm to be",
    "start": "1336039",
    "end": "1343480"
  },
  {
    "text": "occupied for a amount of time and as well as like a stop the de C Loop for",
    "start": "1343480",
    "end": "1349760"
  },
  {
    "text": "some time and decod loop will not use all the computer resources in the TP chips as well so which will kep the uh",
    "start": "1349760",
    "end": "1356000"
  },
  {
    "text": "computer reses to be ideal for some time so the ideal solution here is like to split this two faces and which to",
    "start": "1356000",
    "end": "1362720"
  },
  {
    "text": "optimize both memory usage and computer usage the ne uh next benefits is like it",
    "start": "1362720",
    "end": "1369520"
  },
  {
    "text": "can also reduce time to First token latency under a highr latency situation",
    "start": "1369520",
    "end": "1375360"
  },
  {
    "text": "uh and first token deny is very important because it is a c customer perceive latency and it is the first",
    "start": "1375360",
    "end": "1381000"
  },
  {
    "text": "token is a time like one customer first time seeing the return token right so um",
    "start": "1381000",
    "end": "1387720"
  },
  {
    "text": "optimizing that will have customer to make customer to have a better user experience as well and a thir part is",
    "start": "1387720",
    "end": "1394559"
  },
  {
    "text": "enable heterogeneous VM types for prefer and decode that matches better the",
    "start": "1394559",
    "end": "1399840"
  },
  {
    "text": "computer characteristic of each phas so remember prefer and decode have different um bound right uh so for",
    "start": "1399840",
    "end": "1406679"
  },
  {
    "text": "prefer we can use a power computer powerful machine for decoder we use a memory powerful machine and the good",
    "start": "1406679",
    "end": "1413559"
  },
  {
    "text": "side for that is like this machine can sometimes have different prices right enable heterogeneous V types could ask",
    "start": "1413559",
    "end": "1420720"
  },
  {
    "text": "could have like uh Cloud providers to use different V types to with even lower",
    "start": "1420720",
    "end": "1426000"
  },
  {
    "text": "cost to have this combination of uh infrastructure to do a inference requ to",
    "start": "1426000",
    "end": "1432039"
  },
  {
    "text": "work around INF workload and with like extra benefits as well uh the last one is uh better",
    "start": "1432039",
    "end": "1438559"
  },
  {
    "text": "overall re efficiency and lower cost because uh actually we have like some",
    "start": "1438559",
    "end": "1444400"
  },
  {
    "text": "benchmarking and also experiment uh running like we can see actually two point two times uh cost reduction uh",
    "start": "1444400",
    "end": "1451240"
  },
  {
    "text": "with head genius and dis serving uh with like pretty similar uh lat CSL um which",
    "start": "1451240",
    "end": "1458640"
  },
  {
    "text": "uh we run like a no matter uh h100 uh combining with TP V5 uh TP V5 we use",
    "start": "1458640",
    "end": "1465640"
  },
  {
    "text": "that for decode and um h100 we use that PR and we have see actually pretty good",
    "start": "1465640",
    "end": "1471039"
  },
  {
    "text": "overall cost reduction as well so um for running this serving here we use l work",
    "start": "1471039",
    "end": "1477960"
  },
  {
    "text": "set to help oate this kind of serving Paradigm um this is a picture of how",
    "start": "1477960",
    "end": "1485799"
  },
  {
    "text": "exactly we set up the thisx sering with the work Set uh this is a more detailed uh one",
    "start": "1485799",
    "end": "1494000"
  },
  {
    "text": "replica of like uh serving for code so here we have two prefill TP",
    "start": "1494000",
    "end": "1501120"
  },
  {
    "text": "slice and we have one decode TP slice and which which like when the um",
    "start": "1501120",
    "end": "1509360"
  },
  {
    "text": "inference request comes in to the this serator it will get prefilled once is",
    "start": "1509360",
    "end": "1515279"
  },
  {
    "text": "finished pref we will transfer the data uh k k Cas to the uh decode uh TP size",
    "start": "1515279",
    "end": "1524000"
  },
  {
    "text": "here and this all happens uh and the otion of those",
    "start": "1524000",
    "end": "1529039"
  },
  {
    "text": "assignment of thisp slices um for prefer decode is done by the model server or",
    "start": "1529039",
    "end": "1534799"
  },
  {
    "text": "dis oor and all the computation task dispatching and all the like Jack API C",
    "start": "1534799",
    "end": "1541399"
  },
  {
    "text": "is handled by the a proxy and also the passway worker here and this is a second replica of the",
    "start": "1541399",
    "end": "1548279"
  },
  {
    "text": "data serving uh so all the service will only communicate with leader ports model",
    "start": "1548279",
    "end": "1553360"
  },
  {
    "text": "server uh container here okay so next one we will go through",
    "start": "1553360",
    "end": "1560559"
  },
  {
    "text": "a quick demo of like uh how to use the work set to run mot host inference uh on",
    "start": "1560559",
    "end": "1568600"
  },
  {
    "text": "TPU right now let's do a quick demo this is a kuber is Manifest for",
    "start": "1569720",
    "end": "1575640"
  },
  {
    "text": "deploy the jream password serving instances in this file we have specified",
    "start": "1575640",
    "end": "1581559"
  },
  {
    "text": "the number of rcas as one and enable exclusive",
    "start": "1581559",
    "end": "1586640"
  },
  {
    "text": "placement let's ensure one GK not Port will be one replica we configure the restart policy",
    "start": "1586640",
    "end": "1594039"
  },
  {
    "text": "to recreate group on P restart it ensures gun restart for",
    "start": "1594039",
    "end": "1599760"
  },
  {
    "text": "individual port or container failure this deployment targets TPU V 5e",
    "start": "1599760",
    "end": "1606600"
  },
  {
    "text": "Port slice with four hosts in this single replica there will",
    "start": "1606600",
    "end": "1612919"
  },
  {
    "text": "be five ports being created one leader port and four worker port",
    "start": "1612919",
    "end": "1619840"
  },
  {
    "text": "each worker Port will consume four TP",
    "start": "1619840",
    "end": "1624080"
  },
  {
    "text": "chips let's deploy this manifest into our kubernetes cluster we can see all",
    "start": "1627799",
    "end": "1633600"
  },
  {
    "text": "five ports are created now the first one is a leader Port others are the worker",
    "start": "1633600",
    "end": "1639840"
  },
  {
    "text": "ports let's send a sample request to the jream server",
    "start": "1639840",
    "end": "1646320"
  },
  {
    "text": "this is a response we get with leader worker set we also",
    "start": "1656279",
    "end": "1662320"
  },
  {
    "text": "support Auto scanning failure handling and rolling update right now I will show the auto scaling",
    "start": "1662320",
    "end": "1668039"
  },
  {
    "text": "part this is using cor horizontal Port Auto scaler component in the HPA",
    "start": "1668039",
    "end": "1673799"
  },
  {
    "text": "manifest we have set up the minimum number of replicas as two Maxim number of replicas as five and the target",
    "start": "1673799",
    "end": "1681600"
  },
  {
    "text": "metric as just stram prefill backr size this is a custom metric we have",
    "start": "1681600",
    "end": "1687279"
  },
  {
    "text": "added to the ging orchestrator it shows the current node of the model server we set the average value as 10",
    "start": "1687279",
    "end": "1695240"
  },
  {
    "text": "which means for the past 50 seconds if the average number of requests in the gam prefill backr is 10 we should scale",
    "start": "1695240",
    "end": "1702559"
  },
  {
    "text": "up the number of replicas in the Target reference section we have configured to the leader worker",
    "start": "1702559",
    "end": "1709840"
  },
  {
    "text": "set object named ging passw now let's Deport this HPA",
    "start": "1709840",
    "end": "1717519"
  },
  {
    "text": "manifest as we can see the original number of serving replica is one after",
    "start": "1717519",
    "end": "1723000"
  },
  {
    "text": "zero seconds the scale up was triggered by the HPA component to increase the number of replicas to",
    "start": "1723000",
    "end": "1729559"
  },
  {
    "text": "two if much more requests are sent to the jream servers which can't be handled",
    "start": "1729559",
    "end": "1734880"
  },
  {
    "text": "by the existing replicas timely the scale up will be triggered again",
    "start": "1734880",
    "end": "1741000"
  },
  {
    "text": "automatically next let's rout to the feed recovery support when any hardware or workload",
    "start": "1743519",
    "end": "1750559"
  },
  {
    "text": "feeder happens to recover this serving replica all PS in the Sam group should",
    "start": "1750559",
    "end": "1756720"
  },
  {
    "text": "be restarted by the controller here I will delete one of the worker PS to simulate the fature",
    "start": "1756720",
    "end": "1763960"
  },
  {
    "text": "scenario in this replica",
    "start": "1763960",
    "end": "1767880"
  },
  {
    "text": "as we can see after leader workers set detected one port in this group has been",
    "start": "1773919",
    "end": "1779360"
  },
  {
    "text": "restarted it will automatically restart the entire group of",
    "start": "1779360",
    "end": "1784880"
  },
  {
    "text": "ports yeah I think that's a demo here I think pry much it thanks",
    "start": "1788200",
    "end": "1796919"
  },
  {
    "text": "I have few minutes for questions but then I have to",
    "start": "1799120",
    "end": "1803960"
  },
  {
    "text": "run go",
    "start": "1806320",
    "end": "1809398"
  },
  {
    "text": "ahead one for prefill and decod and right but how are these two supposed to",
    "start": "1813720",
    "end": "1821080"
  },
  {
    "text": "be connected so the TPU slies are actually",
    "start": "1821080",
    "end": "1827960"
  },
  {
    "text": "connected via dcn so you have two TPU slices one is",
    "start": "1827960",
    "end": "1833039"
  },
  {
    "text": "running decode the other one is running prefill if they are not on the same TPU pod right they are going to be on two",
    "start": "1833039",
    "end": "1840080"
  },
  {
    "text": "different pods the communication between them is going to be through DC and D Center networ so the KV cach transfer is",
    "start": "1840080",
    "end": "1845840"
  },
  {
    "text": "going to go through DCS yeah so the first one is doing a",
    "start": "1845840",
    "end": "1853679"
  },
  {
    "text": "prefill and then generates the KV cache it will transfer it to the other one which does the decode so it yeah think",
    "start": "1853679",
    "end": "1859880"
  },
  {
    "text": "of it as a pipeline parallelism basically yeah yes how does this intersect with",
    "start": "1859880",
    "end": "1867120"
  },
  {
    "text": "Dr it doesn't or it does to a degree Dr is single node focused this is multi",
    "start": "1867120",
    "end": "1873600"
  },
  {
    "text": "node did you see Kevin did a lightning talk today about the new BG",
    "start": "1873600",
    "end": "1881880"
  },
  {
    "text": "250s uh Nvidia yeah yeah that the Nvidia new",
    "start": "1882320",
    "end": "1887600"
  },
  {
    "text": "Nvidia Hardware you mean yeah yeah that's that's going to work on it really well show",
    "start": "1887600",
    "end": "1893000"
  },
  {
    "text": "that yeah how does",
    "start": "1893000",
    "end": "1897519"
  },
  {
    "text": "topolog um so um it's not at that level like the",
    "start": "1900000",
    "end": "1908039"
  },
  {
    "text": "the like the you think about it as exclusive placement like you want to collocate them on the",
    "start": "1908039",
    "end": "1914039"
  },
  {
    "text": "same topology zone is way too big right like set I understand for the four parts",
    "start": "1914039",
    "end": "1920159"
  },
  {
    "text": "right in a set but if I want three replicas oh right just like you're doing with deployment think about it as a",
    "start": "1920159",
    "end": "1926279"
  },
  {
    "text": "super pod those are going to act together as a unit right uh because they have the same template so when we inject",
    "start": "1926279",
    "end": "1932559"
  },
  {
    "text": "any node Affinity Etc they going to go together right with the exclusive placement the leader actually the way",
    "start": "1932559",
    "end": "1939639"
  },
  {
    "text": "that we Implement exclusive placement is actually interesting so we first schedule the leader pod and then we know",
    "start": "1939639",
    "end": "1948360"
  },
  {
    "text": "where it's being scheduled and then for the workers we're going to inject a n Affinity to the topology where the",
    "start": "1948360",
    "end": "1955240"
  },
  {
    "text": "leader landed right so we we ensure that the workers are following the",
    "start": "1955240",
    "end": "1961200"
  },
  {
    "text": "leader one question your pay model seems to be stick to All or Nothing so when one worker goes you restart the whole",
    "start": "1962960",
    "end": "1970000"
  },
  {
    "text": "group is this really an intentional Des decision or you think to make this configurable later it's configurable",
    "start": "1970000",
    "end": "1975120"
  },
  {
    "text": "it's already configurable but like 99.9% of the Frameworks they're not intelligent enough to handle like",
    "start": "1975120",
    "end": "1981559"
  },
  {
    "text": "individual worker failures they have to get everybody restarted to start from the same uh um State basically but the",
    "start": "1981559",
    "end": "1989480"
  },
  {
    "text": "crd basically allows yeah it does basically you could you could get this normal stateful set Behavior with one",
    "start": "1989480",
    "end": "1996440"
  },
  {
    "text": "caveat if the leader is is dead the work is going to be dead why because the",
    "start": "1996440",
    "end": "2002799"
  },
  {
    "text": "leader is the owner of the of the so so they are connected",
    "start": "2002799",
    "end": "2009278"
  },
  {
    "text": "so we're thinking about that as a like a scheduler feature you see like here we are claiming exclusive placement but",
    "start": "2016600",
    "end": "2021919"
  },
  {
    "text": "we're not actually doing the scheduling we are injecting scheduling constraints to inform the schedule what we want to",
    "start": "2021919",
    "end": "2026960"
  },
  {
    "text": "do um beyond that it's going to have to be like you know uh a gang scheduler um",
    "start": "2026960",
    "end": "2034519"
  },
  {
    "text": "we do have that in Q or we will have it in q um ideally we will have something in like you know in Cube schedu like you",
    "start": "2034519",
    "end": "2041639"
  },
  {
    "text": "know as a",
    "start": "2041639",
    "end": "2044440"
  },
  {
    "text": "plugin",
    "start": "2051520",
    "end": "2054520"
  },
  {
    "text": "correct so we're the group is uh immutable think of it like it's it's a unit when you when",
    "start": "2057000",
    "end": "2063960"
  },
  {
    "text": "you um created the leader worker set right you said I have four workers you",
    "start": "2063960",
    "end": "2069240"
  },
  {
    "text": "can't change that but those do not scale so each superp is fixed to four and then",
    "start": "2069240",
    "end": "2075800"
  },
  {
    "text": "I'm going to autoscale the number of these",
    "start": "2075800",
    "end": "2081520"
  },
  {
    "text": "superpods yeah right add",
    "start": "2086480",
    "end": "2092720"
  },
  {
    "text": "another yeah but it's not an inference server that like the group the whole group is a single inference server that",
    "start": "2092720",
    "end": "2098160"
  },
  {
    "text": "is running on a distributed set of nodes so this is fixed right because it depends on your",
    "start": "2098160",
    "end": "2104040"
  },
  {
    "text": "model size so you say 405b requires two um you know nodes that's it like you",
    "start": "2104040",
    "end": "2111480"
  },
  {
    "text": "you like that's what's going to happen now if you want want a horizontal scale you're going to create another group of two notes another group of two notes",
    "start": "2111480",
    "end": "2117320"
  },
  {
    "text": "right so that's how and that's supported that's how we do it because I got conf",
    "start": "2117320",
    "end": "2124520"
  },
  {
    "text": "no no no we're not increasing the number of workers we're increasing the number of groups GRS yeah",
    "start": "2128480",
    "end": "2135560"
  },
  {
    "text": "exactly exactly exactly it's those are like one unit",
    "start": "2137920",
    "end": "2144240"
  },
  {
    "text": "right yeah no",
    "start": "2144240",
    "end": "2150280"
  },
  {
    "text": "problem you have one one service Per Mar right because that that was my question",
    "start": "2150280",
    "end": "2159440"
  },
  {
    "text": "number of workers per model then basically if you have much bigger models",
    "start": "2161960",
    "end": "2167599"
  },
  {
    "text": "you'll have to like even if you're scheduling smaller models which don't need as many workers you'll be scheduling the maximum number of",
    "start": "2167599",
    "end": "2174240"
  },
  {
    "text": "workers yeah it's fixed so basically you you you want to know like what is your",
    "start": "2174240",
    "end": "2179640"
  },
  {
    "text": "model size and say okay this is is it same thing as when you decide how many gpus you need right",
    "start": "2179640",
    "end": "2188200"
  },
  {
    "text": "just want to go here any other",
    "start": "2190240",
    "end": "2196560"
  },
  {
    "text": "questions it's pretty much the same so you want to think of it again as a superp with the leader board being your",
    "start": "2201720",
    "end": "2207839"
  },
  {
    "text": "facade right so it's your representative of the service so when you configure",
    "start": "2207839",
    "end": "2214079"
  },
  {
    "text": "your service you're going to set up um a selector right so that's the is going to only select the leaders and the leader",
    "start": "2214079",
    "end": "2220319"
  },
  {
    "text": "is going to receive the request fanner the the like starts the distributed job basically like the",
    "start": "2220319",
    "end": "2227040"
  },
  {
    "text": "distributed inference uh processing it will collect the result and then will return",
    "start": "2227040",
    "end": "2232280"
  },
  {
    "text": "it even if a service mesh or something is involved the request will always flow through the leader yeah you want to",
    "start": "2232280",
    "end": "2238440"
  },
  {
    "text": "present this as a single unit all all the time like with your service mesh setup you want to forget about the workers and just",
    "start": "2238440",
    "end": "2246560"
  },
  {
    "text": "think about the leaders right so the workers are basically the workers for that uh for that leader",
    "start": "2246560",
    "end": "2254119"
  },
  {
    "text": "yeah so we have a PO template right for the um for the workers separate from the",
    "start": "2261599",
    "end": "2268079"
  },
  {
    "text": "leader so it depends on how you set it up so you can it's it's yeah you should be able",
    "start": "2268079",
    "end": "2273680"
  },
  {
    "text": "to yeah exactly it depends on the storage like you know itself characteristics if it can be shared",
    "start": "2273680",
    "end": "2279000"
  },
  {
    "text": "between the yeah and yeah but we don't actually we don't expose the stateful Set uh characterist um knobs with state",
    "start": "2279000",
    "end": "2287319"
  },
  {
    "text": "for I think you can also create you know create like you have a actually you can if you oh no you don't the problem is",
    "start": "2287319",
    "end": "2294240"
  },
  {
    "text": "that with the leader worker set template you don't have the stateful set template to create a PVC",
    "start": "2294240",
    "end": "2299760"
  },
  {
    "text": "template yeah we we can expose that we can we can try to yeah if it is needed but I I",
    "start": "2299760",
    "end": "2306640"
  },
  {
    "text": "don't think it is",
    "start": "2306640",
    "end": "2309440"
  },
  {
    "text": "so leader worker set is going to do the orchestration right it's going to set things up it's up to the disaggregated",
    "start": "2317280",
    "end": "2323480"
  },
  {
    "text": "serving implementation um so with like what grouping uh described the jet stream",
    "start": "2323480",
    "end": "2329040"
  },
  {
    "text": "which is like the new open source model server we have on tpus um it's going to",
    "start": "2329040",
    "end": "2334680"
  },
  {
    "text": "have like a an orchestrator that will know like it will assign okay this slice",
    "start": "2334680",
    "end": "2340240"
  },
  {
    "text": "is decode this slice is prefill and you have to configure it like with some parameters right",
    "start": "2340240",
    "end": "2347520"
  },
  {
    "text": "yeah that say be heterogeneous so a small",
    "start": "2347520",
    "end": "2354040"
  },
  {
    "text": "GP right but not like it's not going to be within a single so that that that's",
    "start": "2354040",
    "end": "2359280"
  },
  {
    "text": "that's a very good question so the way that we envision this working you have to set up two lws right and then because the the um",
    "start": "2359280",
    "end": "2367880"
  },
  {
    "text": "each replica has an index you can all and you can match them",
    "start": "2367880",
    "end": "2374400"
  },
  {
    "text": "exactly right yeah so because I can we want to",
    "start": "2376319",
    "end": "2383359"
  },
  {
    "text": "simplify the like the for the simple CA like the more traditional case if we go",
    "start": "2383359",
    "end": "2388599"
  },
  {
    "text": "into that level like heterogeneous workers the complexity is going to be 10x super complex exactly I work",
    "start": "2388599",
    "end": "2396520"
  },
  {
    "text": "sometimes with team anding ated support yeah but with this I think",
    "start": "2396520",
    "end": "2402920"
  },
  {
    "text": "with two that's that's a very good question and we thought about it from the beginning for hetrogeneous setup",
    "start": "2402920",
    "end": "2408200"
  },
  {
    "text": "like the way that again we we're we're following some sort of like a a model where we're composing right so we",
    "start": "2408200",
    "end": "2413640"
  },
  {
    "text": "compose this lws from stateful set and the idea is like we're going to compose this aggregated serving orchestration",
    "start": "2413640",
    "end": "2419000"
  },
  {
    "text": "from two lwss a followup to that might be too complicated an easier followup you also",
    "start": "2419000",
    "end": "2424640"
  },
  {
    "text": "mentioned rtma that in",
    "start": "2424640",
    "end": "2429319"
  },
  {
    "text": "we're just hyp like you know this is has nothing to do with lws itself it's depends depends on the",
    "start": "2429880",
    "end": "2436920"
  },
  {
    "text": "provider you have the full P template so whatever you you're able to do with a state for set you should be able to do",
    "start": "2438880",
    "end": "2444240"
  },
  {
    "text": "here yeah can someone order an Uber for [Laughter]",
    "start": "2444240",
    "end": "2452640"
  },
  {
    "text": "me good how does observability look for lws when you look at uh measuring",
    "start": "2452640",
    "end": "2458760"
  },
  {
    "text": "performance or it is again the same it depends on the framework um like Ray for example",
    "start": "2458760",
    "end": "2465800"
  },
  {
    "text": "you're going to have rayad running on the leader and Ray workers running on the on the workers right so if the rayad",
    "start": "2465800",
    "end": "2471920"
  },
  {
    "text": "is the one that is emitting the metrics you're going to have to set up like you know but when health is reported does it",
    "start": "2471920",
    "end": "2478599"
  },
  {
    "text": "report as oh Health right so you again it's it like it depends on your",
    "start": "2478599",
    "end": "2483760"
  },
  {
    "text": "framework uh in our case for example jet stream server itself is on the leader it's the one that's going to be",
    "start": "2483760",
    "end": "2489119"
  },
  {
    "text": "responsible for declaring that the superp is healthy or not right so you",
    "start": "2489119",
    "end": "2494960"
  },
  {
    "text": "you're going to do Health checking on the on the on the on the leader at least that's the pattern that we have in mind",
    "start": "2494960",
    "end": "2500400"
  },
  {
    "text": "and we've been using right thank you so much",
    "start": "2500400",
    "end": "2508160"
  }
]