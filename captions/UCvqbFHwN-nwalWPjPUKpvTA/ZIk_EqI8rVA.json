[
  {
    "text": "hi everyone uh here's the session six scheduling intro and updates i'm Kens from Tate working for",
    "start": "0",
    "end": "8240"
  },
  {
    "text": "service me stuff and and I am M from Google i am working in AI training team",
    "start": "8240",
    "end": "15839"
  },
  {
    "text": "all right so let's get started so s scheduling we are maintaining the",
    "start": "15839",
    "end": "22080"
  },
  {
    "text": "components that are related to the you know part p part placement and kubuler",
    "start": "22080",
    "end": "28560"
  },
  {
    "text": "is the main component that we are maintaining in the kubernest uh upstream",
    "start": "28560",
    "end": "34320"
  },
  {
    "text": "repository and others are the sub projects uh that helps users",
    "start": "34320",
    "end": "39680"
  },
  {
    "text": "additionally in this area in many you know different ways in this session",
    "start": "39680",
    "end": "44879"
  },
  {
    "text": "we'll introduce the you know brief introduction about the command scheduleuler itself and discuss the",
    "start": "44879",
    "end": "51760"
  },
  {
    "text": "latest uh enhancements around the scheduleuler and lastly we will show",
    "start": "51760",
    "end": "57039"
  },
  {
    "text": "some other uh updates in the major sub",
    "start": "57039",
    "end": "62800"
  },
  {
    "text": "projects so command scheduleuler is the component that uh decides the pot part",
    "start": "62840",
    "end": "69200"
  },
  {
    "text": "placement uh so it it implements the features like you know uh resource",
    "start": "69200",
    "end": "74840"
  },
  {
    "text": "requirement port affinity node affinity and you know part spread or some others",
    "start": "74840",
    "end": "81280"
  },
  {
    "text": "as well and each feature is actually implemented as a program in generally",
    "start": "81280",
    "end": "86799"
  },
  {
    "text": "and so for example we have a resource fit program that handles resource requirement and we have an uh interot",
    "start": "86799",
    "end": "95200"
  },
  {
    "text": "interf at affinity and yeah each feature is",
    "start": "95200",
    "end": "102000"
  },
  {
    "text": "implemented like that so each plugin can work at several",
    "start": "102000",
    "end": "107439"
  },
  {
    "text": "interfaces which is called um extension point so those uh futures and scores are",
    "start": "107439",
    "end": "114799"
  },
  {
    "text": "two major extension points that involves the scheduling decision uh at the future",
    "start": "114799",
    "end": "121200"
  },
  {
    "text": "extension point uh we like uh programs can reject node that shouldn't run the",
    "start": "121200",
    "end": "127360"
  },
  {
    "text": "port so for example uh like resource feed programing rejects nodes that",
    "start": "127360",
    "end": "133040"
  },
  {
    "text": "doesn't have enough resources for the port and you know node affinity pluging",
    "start": "133040",
    "end": "138400"
  },
  {
    "text": "rejects nodes that don't have enough you know labels and stuff and then after",
    "start": "138400",
    "end": "144239"
  },
  {
    "text": "that at uh the score extension point uh programs can score nodes based on their",
    "start": "144239",
    "end": "152640"
  },
  {
    "text": "preference preference and yeah for example they There is a plugin called image locality",
    "start": "152640",
    "end": "160160"
  },
  {
    "text": "plugin that gives high higher score to nodes that have uh container image at",
    "start": "160160",
    "end": "165599"
  },
  {
    "text": "the cash already so we will see how it works uh",
    "start": "165599",
    "end": "172000"
  },
  {
    "text": "the scheduleuler schedules parts basically one by one and each time evaluate all nodes so that it can",
    "start": "172000",
    "end": "178879"
  },
  {
    "text": "determine the best node for each part in this example let's say we have four",
    "start": "178879",
    "end": "184000"
  },
  {
    "text": "nodes and also we have two future plugins and two score",
    "start": "184000",
    "end": "189319"
  },
  {
    "text": "plugins first all nodes are evaluated by the future plugins uh in this example",
    "start": "189319",
    "end": "196319"
  },
  {
    "text": "looks like uh future A plugin rejects node for so for some you know some",
    "start": "196319",
    "end": "203920"
  },
  {
    "text": "reason and feature B uh plugin rejects node three for another reason so looks",
    "start": "203920",
    "end": "211040"
  },
  {
    "text": "like only node one and two are going for the scoring phase and then you know each",
    "start": "211040",
    "end": "216720"
  },
  {
    "text": "score plugin scores each node based on their perspective and let's say they",
    "start": "216720",
    "end": "222400"
  },
  {
    "text": "score them like this the total scores will be like this",
    "start": "222400",
    "end": "228400"
  },
  {
    "text": "so in this case node two uh got the highest you know total score so this",
    "start": "228400",
    "end": "234400"
  },
  {
    "text": "part will eventually go to the node to get the",
    "start": "234400",
    "end": "239680"
  },
  {
    "text": "result i only introduced about two major you know extension points feature in",
    "start": "239879",
    "end": "245840"
  },
  {
    "text": "school but actually there are more extension points so this is the actual",
    "start": "245840",
    "end": "252159"
  },
  {
    "text": "you know like full picture of it and this entire architecture is called scheduling",
    "start": "252159",
    "end": "259160"
  },
  {
    "text": "framework this green area is called scheduling cycle it is uh responsible",
    "start": "259160",
    "end": "264720"
  },
  {
    "text": "for you know making a decision of pot placement like I described and yeah",
    "start": "264720",
    "end": "271360"
  },
  {
    "text": "future and score will be the major ones here and if the scheduleuler decided to",
    "start": "271360",
    "end": "277919"
  },
  {
    "text": "decides to schedule parts for some node uh then this bot will be uh proceeding",
    "start": "277919",
    "end": "284639"
  },
  {
    "text": "to the next step which is called binding cycle this yellow area shows the binding cycle",
    "start": "284639",
    "end": "292240"
  },
  {
    "text": "actually yeah so this uh binding cycle is responsible for applying the decision",
    "start": "292240",
    "end": "302440"
  },
  {
    "text": "on command API sub actually so like specifically it updates the parts like",
    "start": "302440",
    "end": "309120"
  },
  {
    "text": "there's a field called node name uh in the part so",
    "start": "309120",
    "end": "315360"
  },
  {
    "text": "uh at the binding cycle the scheduleuler updates the the node name field with the",
    "start": "315360",
    "end": "322160"
  },
  {
    "text": "scheduling result and outside the scheduleuler uh pro uh cublet will",
    "start": "322160",
    "end": "327199"
  },
  {
    "text": "notice the port update with the node name and starts the port accordingly so one note is that uh the",
    "start": "327199",
    "end": "336000"
  },
  {
    "text": "scheduleuler runs the scheduling cycle one by one but runs this binding cycle",
    "start": "336000",
    "end": "341039"
  },
  {
    "text": "as synchronously so this is for better you know this is for better efficiency",
    "start": "341039",
    "end": "346479"
  },
  {
    "text": "uh as I mentioned the binding cycle makes an API call so you know that is a",
    "start": "346479",
    "end": "352479"
  },
  {
    "text": "kind of expensive uh operation right so this architecture like uh scheduling",
    "start": "352479",
    "end": "358639"
  },
  {
    "text": "cycle and binding cycle allows allows us to decouple making a decision and",
    "start": "358639",
    "end": "365600"
  },
  {
    "text": "actually applying it via API calls",
    "start": "365600",
    "end": "370680"
  },
  {
    "text": "uh also we have a scheduling queue uh that holds all pending parts and decides",
    "start": "371680",
    "end": "377600"
  },
  {
    "text": "which part to schedule next so it takes a like a crucial role for you know",
    "start": "377600",
    "end": "383840"
  },
  {
    "text": "determining which part to retire first and which is not like based on priority or based on some you know cross update",
    "start": "383840",
    "end": "391840"
  },
  {
    "text": "and etc uh we'll see the detail in the later",
    "start": "391840",
    "end": "398880"
  },
  {
    "text": "section all right so we saw the overview of commander scheduleuler and from here",
    "start": "399080",
    "end": "404560"
  },
  {
    "text": "we will discuss about the recent update in the command scheduleuler so we will take a look at",
    "start": "404560",
    "end": "412000"
  },
  {
    "text": "each one by one we basically have these four major updates recently the first one is quing hints so",
    "start": "412000",
    "end": "421440"
  },
  {
    "text": "in command the scheduleuler the performance really matters like uh it is",
    "start": "421440",
    "end": "427039"
  },
  {
    "text": "you know really important to uh like because the scheduling cycle schedules",
    "start": "427039",
    "end": "432880"
  },
  {
    "text": "ports one by one right as I mentioned so when your cluster gets bigger and bigger",
    "start": "432880",
    "end": "439680"
  },
  {
    "text": "the amount of created parts could go beyond the scheduling at some point and",
    "start": "439680",
    "end": "446720"
  },
  {
    "text": "it could cause the those kind of uh those ports uh being pending for a long",
    "start": "446720",
    "end": "452080"
  },
  {
    "text": "time so that is the you know worst case scenario that we want to prevent and we",
    "start": "452080",
    "end": "458560"
  },
  {
    "text": "have a you know certain goal for our scheduling throughput and try to keep",
    "start": "458560",
    "end": "464720"
  },
  {
    "text": "the throughput with the goal in every scenario we actually have a like a scheduling performance test in the up",
    "start": "464720",
    "end": "472000"
  },
  {
    "text": "upstream scheduleuler so that we can keep the goal with every",
    "start": "472000",
    "end": "477599"
  },
  {
    "text": "change and yes so therefore uh in the recent VV cycles uh we've been basically",
    "start": "477639",
    "end": "484639"
  },
  {
    "text": "you know focusing uh on the performance improvement more than adding new features",
    "start": "484639",
    "end": "491599"
  },
  {
    "text": "so the recent enhancements that we'll introduce today are mostly about the",
    "start": "491599",
    "end": "497400"
  },
  {
    "text": "performance i mean the OA is only the exception I guess uh like yes so queing",
    "start": "497400",
    "end": "504000"
  },
  {
    "text": "hint is one of that uh when your part is unscheduable",
    "start": "504000",
    "end": "509599"
  },
  {
    "text": "uh like what could make your part scheduleable again uh so thinking about",
    "start": "509599",
    "end": "515599"
  },
  {
    "text": "it like what makes port schedulable is some change in the crust some resource",
    "start": "515599",
    "end": "522240"
  },
  {
    "text": "change in the cluster so we go search any change in the cluster like uh the cluster events",
    "start": "522240",
    "end": "530160"
  },
  {
    "text": "in the scheduleuler um for example like uh node is updated",
    "start": "530160",
    "end": "536880"
  },
  {
    "text": "node is created pot is updated piv is you know deleted etc all",
    "start": "536880",
    "end": "543560"
  },
  {
    "text": "those uh changes are called we call them crust events so let's see how uh you",
    "start": "543560",
    "end": "552720"
  },
  {
    "text": "know does Q handles those events and make the decision of retrying",
    "start": "552720",
    "end": "558120"
  },
  {
    "text": "parts so this is the previous state uh so if the part is rejected at the",
    "start": "558120",
    "end": "563839"
  },
  {
    "text": "scheduling cycle it comes back to the queue with the note about which plugins",
    "start": "563839",
    "end": "569959"
  },
  {
    "text": "rejects rejected this part so in this example looks like the part is rejected",
    "start": "569959",
    "end": "575920"
  },
  {
    "text": "by the resource feed program the world is very small and the resource fit",
    "start": "575920",
    "end": "582720"
  },
  {
    "text": "plugin is the plug-in that checks each node's you know resource requirement uh resources resource capacity and",
    "start": "582720",
    "end": "589600"
  },
  {
    "text": "calculates whether the part can go to there or not so in this example probably",
    "start": "589600",
    "end": "596320"
  },
  {
    "text": "like there's no part that that has enough resource to accommodate this uh",
    "start": "596320",
    "end": "601760"
  },
  {
    "text": "scheduling part so each plugin basically registers",
    "start": "601760",
    "end": "607440"
  },
  {
    "text": "which kind of uh cross the events that could uh resolve uh their scheduling",
    "start": "607440",
    "end": "613360"
  },
  {
    "text": "failure so in this case resource fe uh scheduling failure could be resolved by",
    "start": "613360",
    "end": "620320"
  },
  {
    "text": "new node is created or pot is up uh sorry node is updated to have more",
    "start": "620320",
    "end": "626640"
  },
  {
    "text": "allocatable capacity or some you know existing parts are uh",
    "start": "626640",
    "end": "634279"
  },
  {
    "text": "removed so this resource feed program registers those events",
    "start": "634279",
    "end": "640760"
  },
  {
    "text": "and what we are doing is like uh basically we keep checking the events",
    "start": "640760",
    "end": "647279"
  },
  {
    "text": "and when the queue observes node event node addition event uh which the plugin",
    "start": "647279",
    "end": "654480"
  },
  {
    "text": "registers it requires the it triggers the retire of the scheduling of this",
    "start": "654480",
    "end": "659839"
  },
  {
    "text": "part so this is how previously the scheduling you know ritual works",
    "start": "659839",
    "end": "667959"
  },
  {
    "text": "but thinking more about it uh like basically like the idea",
    "start": "668079",
    "end": "675240"
  },
  {
    "text": "is not all node addition event could make this part scheduleable actually",
    "start": "675240",
    "end": "682079"
  },
  {
    "text": "because for example what if the new node is very small and cannot run the pending",
    "start": "682079",
    "end": "690120"
  },
  {
    "text": "part uh then here is the motivation of curing hint so queuing hint is the",
    "start": "690120",
    "end": "696959"
  },
  {
    "text": "feature that allows progress to future out across the events so that you know",
    "start": "696959",
    "end": "702000"
  },
  {
    "text": "the queue can determine when to reach Y more",
    "start": "702000",
    "end": "707560"
  },
  {
    "text": "wisely so in this example the program has the Q hint uh to check uh whether a",
    "start": "707560",
    "end": "714399"
  },
  {
    "text": "new node is big enough for this part's request and if yes then retry this part",
    "start": "714399",
    "end": "721360"
  },
  {
    "text": "and if no then ignore this event so this Q hint helps in reducing unnecessary",
    "start": "721360",
    "end": "728320"
  },
  {
    "text": "scheduling rituals which allows us you know eventually increase the scheduling",
    "start": "728320",
    "end": "735399"
  },
  {
    "text": "book all right so next one is async pre preeemption so if your parts uh get",
    "start": "735399",
    "end": "742920"
  },
  {
    "text": "unscheduable uh they may go through the process called preeemption",
    "start": "742920",
    "end": "749959"
  },
  {
    "text": "so it's like uh um this is the feature in command",
    "start": "749959",
    "end": "757120"
  },
  {
    "text": "schedule uh that allows higher priority parts to delete some lower priority",
    "start": "757120",
    "end": "762720"
  },
  {
    "text": "parts so that it can make the price for this higher priority part instead of you",
    "start": "762720",
    "end": "767920"
  },
  {
    "text": "know delet uh those uh lower priority parts so in this case uh okay this guy",
    "start": "767920",
    "end": "777279"
  },
  {
    "text": "this high priority part wants to go to node one by deleting some parts there",
    "start": "777279",
    "end": "783959"
  },
  {
    "text": "and the scheduleuler deletes those parts so that this part could likely go to",
    "start": "783959",
    "end": "789200"
  },
  {
    "text": "there in the next scheduling cycle but we have a problem here uh like when the",
    "start": "789200",
    "end": "795360"
  },
  {
    "text": "preeemption happens uh scheduling cycle takes time to complete because",
    "start": "795360",
    "end": "802720"
  },
  {
    "text": "uh the preeemption process has to make API calls to delete those",
    "start": "802720",
    "end": "809519"
  },
  {
    "text": "parts so given the preeemption happens within the scheduling cycle it negatively affects our scheduling",
    "start": "810600",
    "end": "817440"
  },
  {
    "text": "throughputs so basically you know the scheduleuler",
    "start": "817440",
    "end": "823760"
  },
  {
    "text": "uh waits for all the the API calls to delete those boards to finish and then",
    "start": "823760",
    "end": "830560"
  },
  {
    "text": "starts the next scheduling cycle so that's you know",
    "start": "830560",
    "end": "836720"
  },
  {
    "text": "expensive so this ex uh this enhancement literally just tries to run those API",
    "start": "837079",
    "end": "843279"
  },
  {
    "text": "calls asynchronously to decouple them from the scheduling cycle it's like a the idea similar to you know binding",
    "start": "843279",
    "end": "851160"
  },
  {
    "text": "cycles so when the scheduleuler decides to uh delete some parts on node one it",
    "start": "851160",
    "end": "857120"
  },
  {
    "text": "it just reserves the praise for this part on uh node one and it just starts",
    "start": "857120",
    "end": "864959"
  },
  {
    "text": "the next scheduling cycle without waiting for you know preeemption API calls to be done so given we we we we",
    "start": "864959",
    "end": "872560"
  },
  {
    "text": "made a reservation before starting next scheduling cycle uh next scheduling",
    "start": "872560",
    "end": "877680"
  },
  {
    "text": "cycles take uh this uh ongoing preeemption process into consideration",
    "start": "877680",
    "end": "883199"
  },
  {
    "text": "when calculating the scheduling uh scheduling",
    "start": "883199",
    "end": "888240"
  },
  {
    "text": "result yes so the next feature that we want to mention today is pop bots from",
    "start": "889880",
    "end": "894959"
  },
  {
    "text": "back of Q1 active is empty uh why we made this feature when the",
    "start": "894959",
    "end": "900480"
  },
  {
    "text": "active queue is when the active queue is not empty and we want to schedule a port",
    "start": "900480",
    "end": "905600"
  },
  {
    "text": "we just take the first part from the active queue and schedule it but when the active Q is empty the cube schedule",
    "start": "905600",
    "end": "911920"
  },
  {
    "text": "idles which means that it does nothing for some period of time even if the pots",
    "start": "911920",
    "end": "917839"
  },
  {
    "text": "could be in another queue like bag of Q waiting for the bag of penalty to finish",
    "start": "917839",
    "end": "922880"
  },
  {
    "text": "so we decided that we could pop the pots from the bag of Q if the activity is",
    "start": "922880",
    "end": "928240"
  },
  {
    "text": "empty so we could utilize the scheduleuler resources appropriately so let's see how it look",
    "start": "928240",
    "end": "934880"
  },
  {
    "text": "like previously we have three cues in the scheduleuler unscheduable ports which is the factor a map bag of Q and",
    "start": "934880",
    "end": "942000"
  },
  {
    "text": "active Q and we want to schedule one port that was rejected by scheduling",
    "start": "942000",
    "end": "947920"
  },
  {
    "text": "cycle for example uh some node hasn't enough capacity so pot has to wait for a",
    "start": "947920",
    "end": "954959"
  },
  {
    "text": "new node to appear and when there is a event node add to the cluster the Q hint",
    "start": "954959",
    "end": "960880"
  },
  {
    "text": "could decide to retry the pot and then pot is moved to the back of Q waiting",
    "start": "960880",
    "end": "966160"
  },
  {
    "text": "for its back pen penalty to finish and this this backoff is scale with the",
    "start": "966160",
    "end": "971680"
  },
  {
    "text": "number of attempts so it could be a few seconds for example and each second the",
    "start": "971680",
    "end": "977040"
  },
  {
    "text": "periodic flash happens that pops pops all the pots from the peg of Q to the active queue but before moving the port",
    "start": "977040",
    "end": "984959"
  },
  {
    "text": "to the active queue uh the pre andq plugins are called for that port and if",
    "start": "984959",
    "end": "990560"
  },
  {
    "text": "they fail the pot is moved to the ankos level pots again but if they succeed the",
    "start": "990560",
    "end": "996399"
  },
  {
    "text": "pot is in the active queue now waiting to be popped by the scheduling cycle when it's",
    "start": "996399",
    "end": "1003000"
  },
  {
    "text": "first so how looks the proposal for this feature first of all as I said we want",
    "start": "1003000",
    "end": "1008399"
  },
  {
    "text": "to pop the port from the bag of Q and active Q is empty then PNQ plugins had",
    "start": "1008399",
    "end": "1014160"
  },
  {
    "text": "to be moved before adding the port to the bag of Q because we want the pop",
    "start": "1014160",
    "end": "1019440"
  },
  {
    "text": "operation to be as performant as possible the third thing is that we had to change the bag of Q's auditing uh",
    "start": "1019440",
    "end": "1027678"
  },
  {
    "text": "function to order the pots but by active cues a ordering so order them by",
    "start": "1027679",
    "end": "1033600"
  },
  {
    "text": "priority because if we pop the pot earlier we want to pop the highest priority pots if possible to reduce the",
    "start": "1033600",
    "end": "1040640"
  },
  {
    "text": "number of preemptions and the last one pops that are in the bag of queue",
    "start": "1040640",
    "end": "1046319"
  },
  {
    "text": "because of some errors for example API errors during the scheduling we want to",
    "start": "1046319",
    "end": "1051840"
  },
  {
    "text": "keep those spots still in the bag of queue because it's some kind of dead",
    "start": "1051840",
    "end": "1057039"
  },
  {
    "text": "limiting for the API server to not be exhausted so how the scheduling queue",
    "start": "1057039",
    "end": "1065200"
  },
  {
    "text": "looks like with the new feature first of all we can see that P andQ plugins were moved left and that for Q and a new",
    "start": "1065200",
    "end": "1073200"
  },
  {
    "text": "green line was added to the schema so again we have a port in unscable ports",
    "start": "1073200",
    "end": "1078640"
  },
  {
    "text": "then now the PNQ plugins are executed and if they pass the pot is moved to the",
    "start": "1078640",
    "end": "1084080"
  },
  {
    "text": "back of Q but now if active Q is empty we can pop this pot at any time from the",
    "start": "1084080",
    "end": "1091360"
  },
  {
    "text": "back of Q to the scheduling cycle but if the active Q is empty it works the same",
    "start": "1091360",
    "end": "1097200"
  },
  {
    "text": "as before so pop waits to be periodically moved to the active queue",
    "start": "1097200",
    "end": "1102480"
  },
  {
    "text": "so now we have a pot in the scheduling cycle the other thing that we want to mention",
    "start": "1102480",
    "end": "1108160"
  },
  {
    "text": "today is the array so how can we express our resource needs in the port we can",
    "start": "1108160",
    "end": "1114960"
  },
  {
    "text": "specify CPU or memory as well as the storage or some hardware like number of",
    "start": "1114960",
    "end": "1121400"
  },
  {
    "text": "GPUs but it's not enough because sometimes we want to uh get a fraction",
    "start": "1121400",
    "end": "1127520"
  },
  {
    "text": "of some resources so we could request for a storage or use some CRDs",
    "start": "1127520",
    "end": "1133320"
  },
  {
    "text": "always but if we know what we need but not like exactly some we want some kind",
    "start": "1133320",
    "end": "1139840"
  },
  {
    "text": "of device and some number of memory we could use the array where we can express",
    "start": "1139840",
    "end": "1145760"
  },
  {
    "text": "what we need in resource climb like in this example and nodes have some",
    "start": "1145760",
    "end": "1152000"
  },
  {
    "text": "resource slices populated that sketch that could match those claims to the",
    "start": "1152000",
    "end": "1158679"
  },
  {
    "text": "slices so now what were what were the the features sponsored by six scheduling",
    "start": "1158679",
    "end": "1165679"
  },
  {
    "text": "in 1.33 first of all partitionable devices which mean that port can apply for the",
    "start": "1165679",
    "end": "1172720"
  },
  {
    "text": "partition of a larger device likely a multihost device that spans multiple nodes the prioritize alternatives even",
    "start": "1172720",
    "end": "1180720"
  },
  {
    "text": "device requests which means that if we can't get a specific device we can take",
    "start": "1180720",
    "end": "1188080"
  },
  {
    "text": "another one but it's less preferred so we can make some preferred order list for the devices and device times and",
    "start": "1188080",
    "end": "1195600"
  },
  {
    "text": "tolerations that are similar to the note times if we apply a time on a device we",
    "start": "1195600",
    "end": "1202000"
  },
  {
    "text": "can aict all the parts that use this device yes and what are the future",
    "start": "1202000",
    "end": "1207360"
  },
  {
    "text": "challenges of the DA that are related to the cube scheduleuler support for",
    "start": "1207360",
    "end": "1213679"
  },
  {
    "text": "compassible disagregated infrastructure so we could like to attach some devices",
    "start": "1213679",
    "end": "1220600"
  },
  {
    "text": "dynamically as well as some cross node dependencies where pot bypots scheduling",
    "start": "1220600",
    "end": "1228320"
  },
  {
    "text": "of the cubeuler might be not enough like for the multi-host feature in the",
    "start": "1228320",
    "end": "1235039"
  },
  {
    "text": "partitionable devices and there were there were other cube concessions unfortunately before",
    "start": "1235039",
    "end": "1241440"
  },
  {
    "text": "ours that told more about the new DA feature",
    "start": "1241440",
    "end": "1246960"
  },
  {
    "text": "so you can always watch those session afterwards and what were the other major",
    "start": "1246960",
    "end": "1254480"
  },
  {
    "text": "updates in 133 related to cube scheduleuler it was graduation of match label keys input affinity and",
    "start": "1254480",
    "end": "1261039"
  },
  {
    "text": "anti-affffinity to GI as well as node inclusion policy input topology spread",
    "start": "1261039",
    "end": "1266960"
  },
  {
    "text": "to GI and we improved scheduling throughput of ports that use interot affinity and port topology spread",
    "start": "1266960",
    "end": "1275000"
  },
  {
    "text": "filtering as well as some preeemption scenarios by around 20% in large clusters so that's a quite good uh",
    "start": "1275000",
    "end": "1283440"
  },
  {
    "text": "improvement but you know it's not always uh it's not like working in all the use",
    "start": "1283440",
    "end": "1291039"
  },
  {
    "text": "cases so you could check how it works in your cluster so now let's move on",
    "start": "1291039",
    "end": "1297600"
  },
  {
    "text": "quickly to the sub projects updates so first sub sub project that we want to",
    "start": "1297600",
    "end": "1303200"
  },
  {
    "text": "mention is Q so what is Q q is a component that manages quotas and how",
    "start": "1303200",
    "end": "1310159"
  },
  {
    "text": "jobs consume them so it decides when a job should start should wait should be admitted to start and when it should be",
    "start": "1310159",
    "end": "1318200"
  },
  {
    "text": "preempted so what's new in the queue there is a building integration for",
    "start": "1318200",
    "end": "1323919"
  },
  {
    "text": "upper upper and leader worker set as well as multiq supports now both cube",
    "start": "1323919",
    "end": "1329440"
  },
  {
    "text": "flow jobs and ray cluster fair shedding was added which allows to split the",
    "start": "1329440",
    "end": "1335840"
  },
  {
    "text": "borrowability sources in some fmaner topology scheduling was added as well as",
    "start": "1335840",
    "end": "1342960"
  },
  {
    "text": "ranging for it so if you know again more about the",
    "start": "1342960",
    "end": "1348480"
  },
  {
    "text": "queue there were other cube concessions and we want to encourage you to watch",
    "start": "1348480",
    "end": "1353520"
  },
  {
    "text": "them if you didn't join them now now we can move on to the the",
    "start": "1353520",
    "end": "1360440"
  },
  {
    "text": "scheduleuler so the scheduleuler the cubeuler only schedules spots so it",
    "start": "1360440",
    "end": "1367919"
  },
  {
    "text": "looks only on the pending pots in the cluster and it doesn't care much about",
    "start": "1367919",
    "end": "1373120"
  },
  {
    "text": "those pots after they are put on the nodes so for example some uh constraints",
    "start": "1373120",
    "end": "1380000"
  },
  {
    "text": "that that pot head for example some pot topology spreading that means that we",
    "start": "1380000",
    "end": "1385520"
  },
  {
    "text": "want to spread pot equally in the cluster or some anti-affinity could change over time in the cluster meaning",
    "start": "1385520",
    "end": "1392880"
  },
  {
    "text": "that those rules are not are no longer met by such pot and you can use the",
    "start": "1392880",
    "end": "1400240"
  },
  {
    "text": "scheduleuler to evict such ports that doesn't meet the policies that we",
    "start": "1400240",
    "end": "1406159"
  },
  {
    "text": "require to enforce so what's new there the cube schedule the the scheduleuler",
    "start": "1406159",
    "end": "1411919"
  },
  {
    "text": "this matrix now so we can use promuse and kubernetes matrix to guide the the",
    "start": "1411919",
    "end": "1420080"
  },
  {
    "text": "scheduleuler if we should have the pot for example that is consuming too many resources on a node and we want to",
    "start": "1420080",
    "end": "1427679"
  },
  {
    "text": "utilize less and the last sub project that we want to mention today is cube schedule",
    "start": "1427679",
    "end": "1434440"
  },
  {
    "text": "simulator so for example if you want to uh test some custom plugins or tweak",
    "start": "1434440",
    "end": "1441120"
  },
  {
    "text": "configuration on of cubeuler and you don't want to run in it in your real",
    "start": "1441120",
    "end": "1447200"
  },
  {
    "text": "cluster you can always use cube scheduleuler simulator it uses quark to",
    "start": "1447200",
    "end": "1453520"
  },
  {
    "text": "create a fake cluster and it has a nice UI that allows you to show each",
    "start": "1453520",
    "end": "1459039"
  },
  {
    "text": "scheduling step especially for example what uh plugin rejected what note and",
    "start": "1459039",
    "end": "1466880"
  },
  {
    "text": "what were the scores of each plugin for your pots yeah and what's new there the",
    "start": "1466880",
    "end": "1475039"
  },
  {
    "text": "this the cube scul simulator can be connected to your real cluster it means",
    "start": "1475039",
    "end": "1480720"
  },
  {
    "text": "that you don't need to manually create all the resources of your cluster in the simulator but you can connect your cube",
    "start": "1480720",
    "end": "1489120"
  },
  {
    "text": "config to the scheduleuler simulator and it will download all the node spots at",
    "start": "1489120",
    "end": "1494640"
  },
  {
    "text": "Cira and you have you can try the tweaked scheduleuler on a real cluster",
    "start": "1494640",
    "end": "1502440"
  },
  {
    "text": "likely so how to get involved in the six scheduling first of all you can write in",
    "start": "1502440",
    "end": "1510159"
  },
  {
    "text": "the six schedulings channel on Slack on Kubernetes Slack as well as we have two",
    "start": "1510159",
    "end": "1516799"
  },
  {
    "text": "periodical community meetings first is the Asia and Europe which was recently",
    "start": "1516799",
    "end": "1522400"
  },
  {
    "text": "created is in the friendly time for those continents on Tuesdays as well as",
    "start": "1522400",
    "end": "1528240"
  },
  {
    "text": "the well-known America and Europe meeting on Thursdays of course you can all always",
    "start": "1528240",
    "end": "1535919"
  },
  {
    "text": "create some issues in the Kubernetes repository as well as pull request and we are happy to review your",
    "start": "1535919",
    "end": "1543480"
  },
  {
    "text": "needs and you can scan a QR code if you want to take the sessions and we are",
    "start": "1543480",
    "end": "1549919"
  },
  {
    "text": "waiting for your questions now if you have any the microphone is here thank you",
    "start": "1549919",
    "end": "1559158"
  },
  {
    "text": "um I went to the Q session yesterday and it from what I understood they're building a lot of theuling logic into Q",
    "start": "1575520",
    "end": "1581440"
  },
  {
    "text": "so it's kind of pre-cheduled by Q and then it goes to the Kubernetes scheduleuler to actually schedule on uh",
    "start": "1581440",
    "end": "1587840"
  },
  {
    "text": "onto nodes uh it seems like with there's a bit of like overlap there and if is",
    "start": "1587840",
    "end": "1593200"
  },
  {
    "text": "there a way of handling that better so that more Q logic is integrated within",
    "start": "1593200",
    "end": "1598400"
  },
  {
    "text": "uh the scheduleuler or the scheduleuler provides some kind ofuling service or something like that so there's not this",
    "start": "1598400",
    "end": "1604400"
  },
  {
    "text": "like kind of double up yes that's a correct issue but",
    "start": "1604400",
    "end": "1611480"
  },
  {
    "text": "yeah as far as I know the Q has the topology of scheduling and it plans to",
    "start": "1611480",
    "end": "1617679"
  },
  {
    "text": "utilize the scheduling framework work there of course it's not great because we have like separate schedulers into",
    "start": "1617679",
    "end": "1623919"
  },
  {
    "text": "places but now it's the the best we can do because you know the scheduleuler itself the scheduleuler schedule spot by",
    "start": "1623919",
    "end": "1631360"
  },
  {
    "text": "pot and changing this mechanism is a long story but yeah in the future",
    "start": "1631360",
    "end": "1639480"
  },
  {
    "text": "probably this could change so okay cool thank you thank you",
    "start": "1639480",
    "end": "1647679"
  },
  {
    "text": "hello I have question can we like live monitoring this uh active queue and",
    "start": "1647679",
    "end": "1653919"
  },
  {
    "text": "backend queue like it's could be more complicated in huge cluster how it's",
    "start": "1653919",
    "end": "1660320"
  },
  {
    "text": "what's going on in this queue and or there is only like we can monitor the",
    "start": "1660320",
    "end": "1666000"
  },
  {
    "text": "events for them it's like the monitoring the queue i think there are metrics exposed that",
    "start": "1666000",
    "end": "1673279"
  },
  {
    "text": "tell you how many ports are each in each of the cues so you could check them as well as you can check some metrics for",
    "start": "1673279",
    "end": "1679919"
  },
  {
    "text": "the events so you can see how those spots move between the kills",
    "start": "1679919",
    "end": "1685840"
  },
  {
    "text": "includ autoscaling and I got a question in my session that I hope you might be able to help answer uh which is um in a",
    "start": "1694600",
    "end": "1701840"
  },
  {
    "text": "simplified case imagine that uh you have like a logs collection agent and a workload that are on the same node uh",
    "start": "1701840",
    "end": "1709039"
  },
  {
    "text": "and the log collection agent ramps up with the workload uh so with uh",
    "start": "1709039",
    "end": "1715399"
  },
  {
    "text": "autoscaling like the two of them exceed the capacity of the node but like just",
    "start": "1715399",
    "end": "1721120"
  },
  {
    "text": "the agent can scale down when the workload's not there and then creates space for the workload again if that",
    "start": "1721120",
    "end": "1727840"
  },
  {
    "text": "makes sense like you end up in a cycle where the autoscaling forces you to exceed the",
    "start": "1727840",
    "end": "1734799"
  },
  {
    "text": "capacity of the node and then you would expect it to scale back down once something's evicted uh do you have any",
    "start": "1734799",
    "end": "1740960"
  },
  {
    "text": "ideas about like how the scheduler might be able to handle a situation like that",
    "start": "1740960",
    "end": "1748600"
  },
  {
    "text": "i don't know to be honest I didn't either so that's fair yeah can",
    "start": "1748799",
    "end": "1754880"
  },
  {
    "text": "we discuss after this session yeah yeah yeah if you could like join the schedule",
    "start": "1754880",
    "end": "1761679"
  },
  {
    "text": "you can just discuss it with us thank you",
    "start": "1761679",
    "end": "1766799"
  },
  {
    "text": "i do have a a separate question you talk about Q you didn't talk about B groups",
    "start": "1766799",
    "end": "1772080"
  },
  {
    "text": "like I feel like I'd be curious to hear like what is what are the talks like about supporting better supporting cost",
    "start": "1772080",
    "end": "1778320"
  },
  {
    "text": "scheduling maybe introducing like some native construct to group BS",
    "start": "1778320",
    "end": "1785159"
  },
  {
    "text": "like you want to yeah you can speak like probably in the future we will",
    "start": "1787080",
    "end": "1794399"
  },
  {
    "text": "extend the support for such cases inuler but now you know we can use skew to to",
    "start": "1794399",
    "end": "1800960"
  },
  {
    "text": "do that up as a native solution of Kubernetes awesome thank you thank you",
    "start": "1800960",
    "end": "1809080"
  },
  {
    "text": "okay so if there are no more questions we can end today so thank you for joining everyone",
    "start": "1813679",
    "end": "1820220"
  },
  {
    "text": "[Applause]",
    "start": "1820220",
    "end": "1824789"
  }
]