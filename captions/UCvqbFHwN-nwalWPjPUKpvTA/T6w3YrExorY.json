[
  {
    "text": "hi everybody uh thanks for coming to this uh pre-lo or after lunch post lunch",
    "start": "179",
    "end": "5400"
  },
  {
    "text": "uh talk please don't fall asleep I hope it's exciting hopefully you can stay awake uh to introduce myself I'm Chris I",
    "start": "5400",
    "end": "12900"
  },
  {
    "text": "work for apple as a software engineer and I'm joined with hi I'm Eric I'm also a software engineer over at Apple cool",
    "start": "12900",
    "end": "19560"
  },
  {
    "text": "uh as you can see we're gonna be talking about cotta containers and uh virtual cluster we're not really going to touch",
    "start": "19560",
    "end": "26039"
  },
  {
    "text": "on cluster API but that's basis the basis for this is all being provisioned through cluster API",
    "start": "26039",
    "end": "31920"
  },
  {
    "text": "so to get us started we are in the multi-tenancy track right now so as I'm",
    "start": "31920",
    "end": "38040"
  },
  {
    "text": "gonna just make an assumption that all of you are interested in running multi-tenant kubernetes uh if not",
    "start": "38040",
    "end": "43680"
  },
  {
    "text": "hopefully it's still interesting to you but as everybody here knows multi-tenancy and kubernetes is it's",
    "start": "43680",
    "end": "49200"
  },
  {
    "text": "hard it's not easy there's a lot of steps that you need to do and realistically out of the box it just doesn't work uh there's a lot of pieces",
    "start": "49200",
    "end": "56160"
  },
  {
    "text": "that you want to use as a user of kubernetes that just aren't accessible and to make this even harder hard",
    "start": "56160",
    "end": "62579"
  },
  {
    "text": "multi-tenancy actual isolation of workloads is very difficult on top of that and this is all based on the the",
    "start": "62579",
    "end": "69600"
  },
  {
    "text": "amount of attack vectors that kubernetes has and the difference between the",
    "start": "69600",
    "end": "75119"
  },
  {
    "text": "access levels that a data plane has versus a control plane and the pieces that are attached to a control plane that you have access to when you get",
    "start": "75119",
    "end": "81720"
  },
  {
    "text": "something like cluster admin this talk is going to be split into two sections we're gonna first start talk about",
    "start": "81720",
    "end": "87439"
  },
  {
    "text": "control plane multi-tenancy and then we're going to talk about data plane multi-tenancy and then we're going to",
    "start": "87439",
    "end": "92880"
  },
  {
    "text": "talk about uh some some improvements that we've done or features that we've added to cada recently to make that",
    "start": "92880",
    "end": "99900"
  },
  {
    "text": "multi-tenant story even more advanced in kubernetes itself so to get us started",
    "start": "99900",
    "end": "106320"
  },
  {
    "text": "we're going to first talk about multi-tenant control planes and there's a bunch of tools in this space these days to do this and we're only going to",
    "start": "106320",
    "end": "112439"
  },
  {
    "text": "focus on one uh it's one that I'm a maintainer for it's called virtual cluster there's another project called",
    "start": "112439",
    "end": "117600"
  },
  {
    "text": "the cluster that you could use some of these same exact Technologies and tools to to implement with that",
    "start": "117600",
    "end": "123420"
  },
  {
    "text": "um lots of other tools in this space as well so before we get started on that we're",
    "start": "123420",
    "end": "129060"
  },
  {
    "text": "going to first talk about some issues that you have running multi-tenant control planes itself one of the biggest",
    "start": "129060",
    "end": "134340"
  },
  {
    "text": "things that you run into or I hope a lot of you have run into is Clumsy and thrashy clients this is something that",
    "start": "134340",
    "end": "140819"
  },
  {
    "text": "like if you have a kubernetes API server you expose it to a bunch of different tenants they're going to do things that",
    "start": "140819",
    "end": "146340"
  },
  {
    "text": "you don't want and things like not setting resource version zero and getting basically direct NCD hits on say",
    "start": "146340",
    "end": "153900"
  },
  {
    "text": "for instance a pod list maybe in a 2000 pod namespace where it's just having to do the Json serialization it's hitting",
    "start": "153900",
    "end": "160200"
  },
  {
    "text": "at CD it's causing a lot of churn and in essence causing problems for other tenants through those through those",
    "start": "160200",
    "end": "166200"
  },
  {
    "text": "requests and so in this example we're basically just saying three tenants doing this pod list and basically",
    "start": "166200",
    "end": "171660"
  },
  {
    "text": "hammering SCD causing problems for other tenants and in essence taking down that single that single control plane from",
    "start": "171660",
    "end": "177180"
  },
  {
    "text": "that standpoint on top of that users these days as kubernetes has",
    "start": "177180",
    "end": "182879"
  },
  {
    "text": "become this base platform that we can build platforms on top of they want access to all of it and out of the box",
    "start": "182879",
    "end": "188340"
  },
  {
    "text": "you can't do this you can't add uh namespace access to just any random tenant without a lot of controls around",
    "start": "188340",
    "end": "194819"
  },
  {
    "text": "it without figuring out how to implement like custom apis to make all of that possible things like hnc that you could",
    "start": "194819",
    "end": "201060"
  },
  {
    "text": "throw in here to do this but out of the box it's just not very it's not very possible and when you layer those those",
    "start": "201060",
    "end": "206580"
  },
  {
    "text": "Technologies on top you end up in a space that isn't exactly kubernetes you",
    "start": "206580",
    "end": "212040"
  },
  {
    "text": "end up then in a place where you can't do Cloud native tools things like normal CI CD tools that want access to the",
    "start": "212040",
    "end": "218159"
  },
  {
    "text": "entire cluster or multiple namespaces or the ability to create a whole namespace to deploy like a red green deployment or",
    "start": "218159",
    "end": "225599"
  },
  {
    "text": "a blue green deployment and so you end up in this place where it's just hard to implement these",
    "start": "225599",
    "end": "231000"
  },
  {
    "text": "strategies I hope that's things that you all have actually experienced as well now",
    "start": "231000",
    "end": "237299"
  },
  {
    "text": "in talking about isolating these again this is only one strategy to do it there's many here behind this but this",
    "start": "237299",
    "end": "243120"
  },
  {
    "text": "is one strategy that that we have been working on for a bit uh which is running virtual cluster and I'm going to talk",
    "start": "243120",
    "end": "248760"
  },
  {
    "text": "high level I'm not going to go into the in-depth of this because this is in essence rehashing a talk from uh fanguo",
    "start": "248760",
    "end": "255239"
  },
  {
    "text": "from Alibaba or now at Microsoft who who did a presentation about this in 2020 in",
    "start": "255239",
    "end": "261720"
  },
  {
    "text": "kubecon um but in essence what we're talking about here is taking kubernetes and throwing it on that pink box on the side",
    "start": "261720",
    "end": "267660"
  },
  {
    "text": "and calling it a super cluster and then taking kubernetes API servers controller",
    "start": "267660",
    "end": "272759"
  },
  {
    "text": "managers and etcd instances that are dedicated to tenants and running those as pods inside that cluster or even",
    "start": "272759",
    "end": "279660"
  },
  {
    "text": "outside of that cluster but just those pieces so you'll notice that there's not a scheduler there",
    "start": "279660",
    "end": "284940"
  },
  {
    "text": "but then you deploy two more main components that VC Sinker up at the top and VC manager VC managers behind the",
    "start": "284940",
    "end": "292199"
  },
  {
    "text": "scenes what's actually orchestrating creating those control planes along with cluster API and the VC Sinker is sitting",
    "start": "292199",
    "end": "298620"
  },
  {
    "text": "there as a multi-tenant uh scheduler that listens to all tenant control",
    "start": "298620",
    "end": "304259"
  },
  {
    "text": "planes and takes pod scheduleable resources and syncs them down to the super cluster this is going to be a",
    "start": "304259",
    "end": "310740"
  },
  {
    "text": "massive piece of work but it's in essence the same amount of work that this the normal Cube scheduler has to do",
    "start": "310740",
    "end": "315900"
  },
  {
    "text": "but just augmented by different control planes now and then on top of that at the node level because we're talking",
    "start": "315900",
    "end": "321840"
  },
  {
    "text": "about data plane isolation in a bit there's a handful of pieces that are important that green box or I guess technically all the green boxes are",
    "start": "321840",
    "end": "328440"
  },
  {
    "text": "virtual cluster and there's a piece in there called VN agent or virtual node agent and this is an on on node agent",
    "start": "328440",
    "end": "335880"
  },
  {
    "text": "that acts as a proxy that takes in requests from each one of those tenant API servers there's a lot behind this I",
    "start": "335880",
    "end": "341820"
  },
  {
    "text": "don't expect you to understand all of these pieces but if you're interested phase talk goes really deep into it and",
    "start": "341820",
    "end": "347100"
  },
  {
    "text": "it's it's a fantastic Deep dive behind the scenes the rest of it the blue boxes are all kubernetes events that we all",
    "start": "347100",
    "end": "352560"
  },
  {
    "text": "know and use Cris cubelets so on and so forth and we got workloads that come from those tenant control planes at the",
    "start": "352560",
    "end": "358080"
  },
  {
    "text": "end of the day a user of this system would talk to one of those dedicated control planes tenant",
    "start": "358080",
    "end": "363720"
  },
  {
    "text": "a tenant b or tenant C and they would end up as pod scheduleable resources in that lower level cluster",
    "start": "363720",
    "end": "371220"
  },
  {
    "text": "now what what this actually looks like behind the scenes is something like this so imagine you have a massive cloud of infrastructure",
    "start": "371220",
    "end": "377820"
  },
  {
    "text": "and you want to leverage that and you want to start bin packing workloads into it and you now have something like",
    "start": "377820",
    "end": "383639"
  },
  {
    "text": "tenant a b and c and you can even have things like a malicious tenant and a clumsy tenant all talking to their own",
    "start": "383639",
    "end": "389340"
  },
  {
    "text": "API servers and those workloads are then getting scheduled down in the super cluster but all of that's isolated so",
    "start": "389340",
    "end": "395280"
  },
  {
    "text": "any of those interactions and we're only talking about the data the control plane here all of those interactions are isolated so if somebody goes and says is",
    "start": "395280",
    "end": "402539"
  },
  {
    "text": "the clumsy client and does massive pod list they don't affect anybody but themselves because it's just hitting",
    "start": "402539",
    "end": "408900"
  },
  {
    "text": "their SED instance they're not then causing problems for everybody else or that malicious tenant that's trying to",
    "start": "408900",
    "end": "414180"
  },
  {
    "text": "do something bad we actually have pieces in the Sinker that'll allow us to stop pods from being scheduled if we don't",
    "start": "414180",
    "end": "420000"
  },
  {
    "text": "want them to for whatever reason pod security policies for example that they can't change again being a cluster level",
    "start": "420000",
    "end": "426600"
  },
  {
    "text": "resource yes pod security policies are going away but there's other resources that are like it I forget what it's called Uh so anyways this then takes you",
    "start": "426600",
    "end": "434160"
  },
  {
    "text": "infrastructure and allows you to start bin packing it in a much denser way so you can say if you look at this and say",
    "start": "434160",
    "end": "440400"
  },
  {
    "text": "that that first node or that first rack now has some workloads from the malicious tenant the the clumsy tenant",
    "start": "440400",
    "end": "446880"
  },
  {
    "text": "and that tenant a and this can be done with anything from run C to Kata to G",
    "start": "446880",
    "end": "452099"
  },
  {
    "text": "visor any of those Stacks under the hood but we can get some benefits out of actually working on the data plane and making it more isolated",
    "start": "452099",
    "end": "458819"
  },
  {
    "text": "so to go back to this issues that we have with multi-tenant control planes when we inject something",
    "start": "458819",
    "end": "464639"
  },
  {
    "text": "like virtual cluster or V cluster since it has a very similar architecture you're going to get rid of those clumsy or you can solve the clumsy clients",
    "start": "464639",
    "end": "471000"
  },
  {
    "text": "because they're not going to affect anybody worst case scenario they're hurting themselves which is really not",
    "start": "471000",
    "end": "476099"
  },
  {
    "text": "not as bad as hurting the rest of the system that's really what we're trying to protect against you then get access to Cluster level",
    "start": "476099",
    "end": "482520"
  },
  {
    "text": "resources because you have cluster admin to that tenant control plane and it doesn't all those resources don't end up",
    "start": "482520",
    "end": "488520"
  },
  {
    "text": "in the supercluster and then you can also use all those Cloud native tools like you normally",
    "start": "488520",
    "end": "494220"
  },
  {
    "text": "would any of the operators that you want to deploy things like cross plane you could deploy into this because you have",
    "start": "494220",
    "end": "499259"
  },
  {
    "text": "cluster admin capabilities and you can deploy cluster scoped resources",
    "start": "499259",
    "end": "504300"
  },
  {
    "text": "police solves half the problem workload isolation sweet so uh",
    "start": "504300",
    "end": "509759"
  },
  {
    "text": "pretending I'm an infrastructure provider at this point in the context of this conversation",
    "start": "509759",
    "end": "515219"
  },
  {
    "text": "basically we're doing a remote code execution as a service and it's not that I'm doing that I'm doing it for multiple",
    "start": "515219",
    "end": "521339"
  },
  {
    "text": "uh end users um if we're to look at the most basic level people want to be able to run",
    "start": "521339",
    "end": "527279"
  },
  {
    "text": "their workload on a machine and because we do multi-tenancy you can both run on the same exact machine and they're just",
    "start": "527279",
    "end": "534600"
  },
  {
    "text": "the process running on the host running on a host Linux kernel on your one node one of many nodes where they're all",
    "start": "534600",
    "end": "540540"
  },
  {
    "text": "running we're going to use containers because we want to have a view that they're the only thing running on the machine and that it's constrained",
    "start": "540540",
    "end": "547920"
  },
  {
    "text": "appropriately there's no denial of service between workloads everything else so these are great features of the",
    "start": "547920",
    "end": "553740"
  },
  {
    "text": "Linux kernel using namespaces using c groups judiciously picking what capabilities are appropriate for the",
    "start": "553740",
    "end": "559440"
  },
  {
    "text": "workload filtering out different sys calls things like that and it works pretty well the concern",
    "start": "559440",
    "end": "566700"
  },
  {
    "text": "from a security perspective is that this is a single interface um so all of these capabilities are",
    "start": "566700",
    "end": "572459"
  },
  {
    "text": "based out of the host kernel so if you do have a zero day in any kind of privilege escalation you are now rude on",
    "start": "572459",
    "end": "578100"
  },
  {
    "text": "the host me as a provider that's concerning um but you as maybe one of many tenants",
    "start": "578100",
    "end": "583620"
  },
  {
    "text": "on that node should be pretty concerned about this as well so neutral and trusting tenants yeah that's",
    "start": "583620",
    "end": "590160"
  },
  {
    "text": "that's bad for them in that case so we have different options here the first",
    "start": "590160",
    "end": "595320"
  },
  {
    "text": "option is essentially YOLO like the security profile that we have like what",
    "start": "595320",
    "end": "600360"
  },
  {
    "text": "is the cost of an escape it's pretty low cost if we don't have very sensitive information if the tenants kind of do",
    "start": "600360",
    "end": "606839"
  },
  {
    "text": "trust each other a little bit it doesn't matter don't pay the cost for extra isolation we're talking about hard",
    "start": "606839",
    "end": "612000"
  },
  {
    "text": "multi-tenancy so that that's out um the other option you have is don't run",
    "start": "612000",
    "end": "617940"
  },
  {
    "text": "multiple workloads from different tenants on the same node so give everybody a different node pool this has",
    "start": "617940",
    "end": "623220"
  },
  {
    "text": "a challenge one you can have fragmentation or resources where two tenants one of them uses 100 of the capacity another needs is five percent",
    "start": "623220",
    "end": "630899"
  },
  {
    "text": "that's that's uh unfortunate too if you do have an escape infrastructure provider that's concerning um I don't",
    "start": "630899",
    "end": "637320"
  },
  {
    "text": "have two layers of isolation anymore also if you have an escape and yeah you're you're stuck on your one",
    "start": "637320",
    "end": "643260"
  },
  {
    "text": "kubernetes worker node but a worker node has a lot of capabilities as far as",
    "start": "643260",
    "end": "649440"
  },
  {
    "text": "authorization to kubernetes apis as well as whatever the network has access to so for this we're going to say that's",
    "start": "649440",
    "end": "655560"
  },
  {
    "text": "that's not quite enough and we're going to look at providing stronger workload isolation so let's have two layers of",
    "start": "655560",
    "end": "661200"
  },
  {
    "text": "isolation and kind of in kubernetes and container area we call this like",
    "start": "661200",
    "end": "667140"
  },
  {
    "text": "generally sandboxed runtimes being that we have two layers of isolation a good example is G visor for this another one",
    "start": "667140",
    "end": "673620"
  },
  {
    "text": "since we're talking about Kata you can guess it is we're going to be talking about cotta containers so in the example going from a",
    "start": "673620",
    "end": "680279"
  },
  {
    "text": "traditional container on on that side I guess it's your left side versus the right hand side with kind of containers",
    "start": "680279",
    "end": "686579"
  },
  {
    "text": "we launch a minimal virtual machine so one layer hardware virtualization and",
    "start": "686579",
    "end": "692100"
  },
  {
    "text": "then within that on a guest Linux kernel they then we're creating regular containers using names to AC",
    "start": "692100",
    "end": "698640"
  },
  {
    "text": "groups all the things second layer of isolation the guest kernel a touch more detail what that ends up looking like",
    "start": "698640",
    "end": "705180"
  },
  {
    "text": "cubelet toxic container D or cryo talks to a task shim uh below that so like a",
    "start": "705180",
    "end": "711240"
  },
  {
    "text": "Kata shim or it could be traditionally like a run c one or it could be anything really in the case of Kata what we do is",
    "start": "711240",
    "end": "717060"
  },
  {
    "text": "we work with a virtual machine monitor so maybe like a qmu cloud hypervisor to launch that minimally configured virtual",
    "start": "717060",
    "end": "724019"
  },
  {
    "text": "machine little Linux kernel boots up user space and knit process will be this Kata agent",
    "start": "724019",
    "end": "729420"
  },
  {
    "text": "who we actually manage the life cycle of the container itself inside the guest we'll talk about these little components",
    "start": "729420",
    "end": "736260"
  },
  {
    "text": "a little bit and that's why I'm kind of introducing it on the bottom there's way too much information you shouldn't care about other than to know that networking",
    "start": "736260",
    "end": "743100"
  },
  {
    "text": "just works usually you drop a v eth into a network name space we then all traffic",
    "start": "743100",
    "end": "748740"
  },
  {
    "text": "is piped directly into the gas such that the container workload has access to it without any configuration necessary",
    "start": "748740",
    "end": "755760"
  },
  {
    "text": "and then we can get into how we can leverage these things to come together",
    "start": "755760",
    "end": "760800"
  },
  {
    "text": "cool uh thank you for that uh so virtual cluster introduces one thing which I",
    "start": "760800",
    "end": "766079"
  },
  {
    "text": "left completely out until we got to this out of the box kubernetes does one thing that is kind of odd uh in essence",
    "start": "766079",
    "end": "772860"
  },
  {
    "text": "Services because they're a virtual component are actually allocated the IPS from them are allocated from a cider",
    "start": "772860",
    "end": "778860"
  },
  {
    "text": "range that's hard-coded into the API server Flags so you set that and it basically gets allocated now if we go",
    "start": "778860",
    "end": "784560"
  },
  {
    "text": "back if you think back to that that diagram before you in essence have three tenants talking to another tenant what tenants",
    "start": "784560",
    "end": "791820"
  },
  {
    "text": "are you supposed to basically make those those service IPS routable from that's",
    "start": "791820",
    "end": "796860"
  },
  {
    "text": "the space where things start to break down and so uh we got together basically tried to figure out how we can make this all possible using Kata under the hood",
    "start": "796860",
    "end": "803639"
  },
  {
    "text": "and that's what we're going to dive into so again this that architecture imagine those three tenants those service those",
    "start": "803639",
    "end": "810000"
  },
  {
    "text": "service cluster IP ranges that are in those API server Flags those are allocating the routable IP addresses for",
    "start": "810000",
    "end": "816600"
  },
  {
    "text": "them and they could be overlapping 192.168 32 right so we're going from",
    "start": "816600",
    "end": "821940"
  },
  {
    "text": "there and we're punching that into the Super cluster which now has a completely different range or potentially",
    "start": "821940",
    "end": "827940"
  },
  {
    "text": "overlapping and you can cause a lot of problems so what we started to do and I'll walk through a quick little uh like",
    "start": "827940",
    "end": "834240"
  },
  {
    "text": "breakdown of this is we're going to talk through Andrea up in the corner coming through and creating a pod against the",
    "start": "834240",
    "end": "841380"
  },
  {
    "text": "tenant API server uh Andrea is a a decent client not going to cause too many problems for us but it's going to",
    "start": "841380",
    "end": "847260"
  },
  {
    "text": "go and create a pod and to walk through how all this stuff functions and what we've added to make this possible is at the same time when",
    "start": "847260",
    "end": "853740"
  },
  {
    "text": "that pod gets created in the API server we actually have an instance of cube proxy running alongside that this is in",
    "start": "853740",
    "end": "860100"
  },
  {
    "text": "a cotta container that's considered a privilege caught a container for the control plane and it has a one single",
    "start": "860100",
    "end": "866279"
  },
  {
    "text": "side car along with it so we're not making any changes to any of that core code base and that's called the exporter and the exporter sits there and it says",
    "start": "866279",
    "end": "873060"
  },
  {
    "text": "hey I got an update on my IP tables I need to do something now that's jumping ahead a little bit",
    "start": "873060",
    "end": "878459"
  },
  {
    "text": "because at this point virtual cluster like the architecture that I was showing before is actually syncing that pod",
    "start": "878459",
    "end": "883560"
  },
  {
    "text": "object down into the API server and it's doing its normal kubernetes events hitting the cubelet or the cube that's",
    "start": "883560",
    "end": "889320"
  },
  {
    "text": "getting a notification saying oh I got to schedule this workload it hits container D or cryo it schedules the Pod",
    "start": "889320",
    "end": "895560"
  },
  {
    "text": "and it creates that entire space in this world now we're showing akata workload running there rather than something like",
    "start": "895560",
    "end": "900839"
  },
  {
    "text": "run C so going back to cube proxy over here we I'm going to use the fun things there",
    "start": "900839",
    "end": "906480"
  },
  {
    "text": "Cube proxy over here we're actually going to get an update on that and we're going to push iptables rules",
    "start": "906480",
    "end": "913320"
  },
  {
    "text": "back into the API server now some of this is a little bit uh is not the most scalable architecture as it currently",
    "start": "913320",
    "end": "919139"
  },
  {
    "text": "stands and we'll talk about that in the uh in a couple slides later but right now what we're doing is basically taking",
    "start": "919139",
    "end": "924420"
  },
  {
    "text": "those iptables syncing them up to the tenant control plane saying hey this this tenant has this set of Ip tables",
    "start": "924420",
    "end": "930240"
  },
  {
    "text": "and I want you to apply that to every single workload that it has now I talked about this VN agent before being an on",
    "start": "930240",
    "end": "935820"
  },
  {
    "text": "node agent that basically takes in those those IP tables and it says Hey katashem",
    "start": "935820",
    "end": "941459"
  },
  {
    "text": "set these because this is that same tenant so now we have per tenant iptable rules that are getting pushed to every",
    "start": "941459",
    "end": "947699"
  },
  {
    "text": "single workload that it goes alongside so if you repeat this entire process you're pushing a new set of rules into",
    "start": "947699",
    "end": "953220"
  },
  {
    "text": "those uh into those Kata containers so that it has that new space and this creates a routable service cluster IP",
    "start": "953220",
    "end": "959579"
  },
  {
    "text": "range I forgot one last slide which is it pushing through the kadashim to the Cod agent and pushing the rules in yeah",
    "start": "959579",
    "end": "965519"
  },
  {
    "text": "and at that point yeah if you're in one of these tenants we'll show you a little bit deeper dive of that so imagine now",
    "start": "965519",
    "end": "971639"
  },
  {
    "text": "we have two nodes back in that example and we have two different Kata uh workloads running one's at ten zero zero",
    "start": "971639",
    "end": "978600"
  },
  {
    "text": "one and ten uh one zero one and those are on some random node IPS that we don't really care about in this one so",
    "start": "978600",
    "end": "984959"
  },
  {
    "text": "you're coming from that one where we're doing that curl to the Food Service this is one of those things that gets pushed",
    "start": "984959",
    "end": "990180"
  },
  {
    "text": "into the iPad tables rules this is actually now taking 192.168 uh 220 and saying hey I got to",
    "start": "990180",
    "end": "997680"
  },
  {
    "text": "figure out where to Route this now in the caught a guest we can actually do iptables denatting to that ten one zero",
    "start": "997680",
    "end": "1004339"
  },
  {
    "text": "one and then it just hits host networking goes across and lands in that actual workload uh yeah so quick demo of how this all",
    "start": "1004339",
    "end": "1013100"
  },
  {
    "text": "functions and then I'll come back to that slide just to give you a little bit of a refresher",
    "start": "1013100",
    "end": "1018800"
  },
  {
    "text": "and this is all recorded because I am not going to do this live so this is going to go through and do",
    "start": "1018800",
    "end": "1025100"
  },
  {
    "text": "exactly what we're talking about I'll show you the deployments here just to show what a cluster is like to create so if we actually look at this this is what",
    "start": "1025100",
    "end": "1030678"
  },
  {
    "text": "a virtual cluster spec looks like am I doing time-wise okay so that's what a virtual cluster spec looks like that you go create this cluster you have a",
    "start": "1030679",
    "end": "1036798"
  },
  {
    "text": "cluster domain in there that gets pushed into things like core DNS that you'll see deployed and some other flags that",
    "start": "1036799",
    "end": "1041959"
  },
  {
    "text": "don't really matter for this demo I think that started cool",
    "start": "1041959",
    "end": "1047480"
  },
  {
    "text": "so this is now going to go and it's going to go VC it's going to use a cube CTL virtual cluster plug-in that allows",
    "start": "1047480",
    "end": "1054740"
  },
  {
    "text": "it to actually write out a cubeconfig for this tenant control plane specifically so it's going to bring up an NCD instance remember I was telling",
    "start": "1054740",
    "end": "1060440"
  },
  {
    "text": "you before it only brings up a couple components so it brings up the the NCD instance the API server and controller",
    "start": "1060440",
    "end": "1065960"
  },
  {
    "text": "manager the cluster is now created and now we have access to it and just to show this is what a cube config looks",
    "start": "1065960",
    "end": "1072260"
  },
  {
    "text": "like for one of these tenant control planes looks something like this um we're adapting all the certificate",
    "start": "1072260",
    "end": "1078140"
  },
  {
    "text": "data and all of that and maybe I can speed this up",
    "start": "1078140",
    "end": "1083660"
  },
  {
    "text": "from here I'm still nervous I know it's recorded but yeah I guess I should probably speed this up",
    "start": "1083660",
    "end": "1090200"
  },
  {
    "text": "but anyways we're going to now show you what that virtual cluster looks like so if you Cube cuddle get all Dash a so all",
    "start": "1090200",
    "end": "1095240"
  },
  {
    "text": "namespaces this is all you see I'm gonna do one more time where I'm actually hitting the super cluster just to give you the difference so we're talking to a",
    "start": "1095240",
    "end": "1102260"
  },
  {
    "text": "tenant control plane running in the cluster that I'm now hitting which has all of these resources and there's a",
    "start": "1102260",
    "end": "1107780"
  },
  {
    "text": "bunch of pieces here that are that are important that I'll show later but in essence we now have",
    "start": "1107780",
    "end": "1113720"
  },
  {
    "text": "uh a couple new name spaces so that default Dash with a hash VC sample one",
    "start": "1113720",
    "end": "1118880"
  },
  {
    "text": "that's the cluster name and the prefixed namespaces and this is how we can basically create namespaces on behalf of",
    "start": "1118880",
    "end": "1124039"
  },
  {
    "text": "users in a lower level super cluster and isolate them so we prefix all those namespaces if you have long name spaces",
    "start": "1124039",
    "end": "1130340"
  },
  {
    "text": "this is always a question that gets asked if you have long namespaces it truncates it for you but still makes it uh",
    "start": "1130340",
    "end": "1136520"
  },
  {
    "text": "unique so we're going to clear that up so we get it back up to the top and now we're going to go and we're going to",
    "start": "1136520",
    "end": "1141740"
  },
  {
    "text": "deploy something into that cluster first thing we're going to do is deploy core DNS because who needs a cluster if you don't have DNS in there and what's",
    "start": "1141740",
    "end": "1148280"
  },
  {
    "text": "important about this is those namespace prefixes that I just showed those are actual parts of the fqdn when you're in",
    "start": "1148280",
    "end": "1154880"
  },
  {
    "text": "a cluster and so we need this because we configure this coordiness instance as the name server on every single pod that",
    "start": "1154880",
    "end": "1161480"
  },
  {
    "text": "comes from your tenant control plane so everybody has their own isolated DNS servers you can set that to resolve to",
    "start": "1161480",
    "end": "1167059"
  },
  {
    "text": "whatever behind the scenes you want again I'm going to show you the difference in clusters so what's cool",
    "start": "1167059",
    "end": "1172580"
  },
  {
    "text": "here is I'm going to highlight that there's replica sets and deployments but nothing from that tenant and this is why",
    "start": "1172580",
    "end": "1177620"
  },
  {
    "text": "we deploy a controller manager because we only care about pod scheduleable resources we can take that and say once",
    "start": "1177620",
    "end": "1183740"
  },
  {
    "text": "the controller manager and the Tenant scheduled it or created a pod for it we'll take that and push it into the cluster now the super cluster becomes a",
    "start": "1183740",
    "end": "1190400"
  },
  {
    "text": "pod scheduling domain and allows us to scale that even wider and I missed talking about it but it",
    "start": "1190400",
    "end": "1196340"
  },
  {
    "text": "showed the pot up at the top with its Dash default namespace so next thing we're going to do is we're",
    "start": "1196340",
    "end": "1201799"
  },
  {
    "text": "going to deploy a Kata workload and this actually isn't calling out Kata in here from a runtime class standpoint",
    "start": "1201799",
    "end": "1208160"
  },
  {
    "text": "because what we've done is to make it extra secure you don't even have to set a runtime class we inject that",
    "start": "1208160",
    "end": "1213500"
  },
  {
    "text": "automatically you deploy a pod like a normal kubernetes deployment and it ends up in Kata behind the scenes",
    "start": "1213500",
    "end": "1219679"
  },
  {
    "text": "and what I'm deploying here is two different applications PHP Apache because we all love PHP right right cool",
    "start": "1219679",
    "end": "1226340"
  },
  {
    "text": "and a networking pod so we can actually do some tests and one service so we can actually do some uh routing tests in the",
    "start": "1226340",
    "end": "1233780"
  },
  {
    "text": "actual cluster right now",
    "start": "1233780",
    "end": "1239840"
  },
  {
    "text": "we're going to Speed it I wanna I think I could probably speed this up a little bit",
    "start": "1239840",
    "end": "1245419"
  },
  {
    "text": "so now we're gonna go get pods and services in that tenant control plane show that those things were created and show the service cluster IPS so you'll",
    "start": "1245419",
    "end": "1252380"
  },
  {
    "text": "notice here that we have 10 32.01 for the kubernetes service 10 32 0106 that's",
    "start": "1252380",
    "end": "1259160"
  },
  {
    "text": "allocated from the tenant control plane on the services and then those IPS for the pods are that 192.168 range that's",
    "start": "1259160",
    "end": "1266000"
  },
  {
    "text": "exposed by the super cluster so in this world right now we have routable pod ranges and we expect things like cnis to",
    "start": "1266000",
    "end": "1273559"
  },
  {
    "text": "do zero trust out of the box at that same point now you'll see in the tenant control plane same eyepiece for the pods",
    "start": "1273559",
    "end": "1279080"
  },
  {
    "text": "but different IPS for the actual super cluster services so these are here but they're non-routable from the the tenant",
    "start": "1279080",
    "end": "1285919"
  },
  {
    "text": "standpoint now if we go and say exec into this pod and we hit iptable save just to show",
    "start": "1285919",
    "end": "1292400"
  },
  {
    "text": "what's happening and I'm going to use default kubernetes just to show what's actually happening so this wrote a rule",
    "start": "1292400",
    "end": "1297440"
  },
  {
    "text": "into that Kata container that is that 103201 range not the Super clusters",
    "start": "1297440",
    "end": "1303260"
  },
  {
    "text": "range with the comments about that Cube proxy normally injects and I'll do a couple more tests in here",
    "start": "1303260",
    "end": "1309799"
  },
  {
    "text": "just to show show how things function",
    "start": "1309799",
    "end": "1313539"
  },
  {
    "text": "so here we're going to do uh using that using one of the actual",
    "start": "1315320",
    "end": "1321140"
  },
  {
    "text": "PHP applications we're going to grab the Pod IP and we're going to try and route to it which is going to go and hit the",
    "start": "1321140",
    "end": "1326539"
  },
  {
    "text": "same route the same networking stack under the hood and so we'll do exec and we'll use that networking pod again",
    "start": "1326539",
    "end": "1332419"
  },
  {
    "text": "and we're going to oh we're going to grep for that one to see how that actually translates in there and you'll see that this is a",
    "start": "1332419",
    "end": "1339799"
  },
  {
    "text": "destination mapping to that 192 range I'm not showing that Cube's sep actual",
    "start": "1339799",
    "end": "1345200"
  },
  {
    "text": "definition but in essence that's the it's the same as that 1032 range that's exposed at the tenant control plane",
    "start": "1345200",
    "end": "1351980"
  },
  {
    "text": "continuing this through we're now going to curl that and we're going to curl the kubernetes service and I'm just going to hit the health endpoint because I'm",
    "start": "1351980",
    "end": "1358100"
  },
  {
    "text": "unauthorized anyways and so this is hitting the DNS server inside the cluster because we configure the name servers automatically for you on every",
    "start": "1358100",
    "end": "1364580"
  },
  {
    "text": "single pod and that hits your tenant control plane not the Super cluster no it doesn't ever hit any of those",
    "start": "1364580",
    "end": "1369799"
  },
  {
    "text": "services that are exposed at the lower level and then we're going to try one more thing we're going to grab the service",
    "start": "1369799",
    "end": "1374840"
  },
  {
    "text": "cluster IP of that kubernetes service do the exact same request slash to that Health endpoint again this is not",
    "start": "1374840",
    "end": "1381740"
  },
  {
    "text": "routable in the super cluster if I went to one of those nodes and tried to curl this it wouldn't work and I'm getting that Health uh okay",
    "start": "1381740",
    "end": "1388400"
  },
  {
    "text": "endpoint I'm gonna do a couple more tests in here because it gets fun we're gonna do networking we're going to",
    "start": "1388400",
    "end": "1393799"
  },
  {
    "text": "curl the PHP application on default and that's going to return back okay Bang Yeah and we're going to do one more",
    "start": "1393799",
    "end": "1401960"
  },
  {
    "text": "time where we go and do the exact same thing where we grab the service cluster IP and then we'll do one more test after that just to show all these things",
    "start": "1401960",
    "end": "1407539"
  },
  {
    "text": "working we'll grab the the actual IP of the pod",
    "start": "1407539",
    "end": "1412120"
  },
  {
    "text": "cool so now we have that 1032 range we'll curl that",
    "start": "1413419",
    "end": "1419559"
  },
  {
    "text": "and okay bang sweet okay so now we're",
    "start": "1422179",
    "end": "1427640"
  },
  {
    "text": "basically are showing that this is now a new routable range for all these pods",
    "start": "1427640",
    "end": "1433100"
  },
  {
    "text": "um we're gonna do one more test which is again that IP address that we had above so we showed this IP address before but this",
    "start": "1433100",
    "end": "1439760"
  },
  {
    "text": "is that test networking pod one more time getting an okay Bang Yeah sweet so now we're actually able to hit",
    "start": "1439760",
    "end": "1446000"
  },
  {
    "text": "the pods directly if we want to and then we're going to do one other thing which is I think an interesting layering of the architectures here so",
    "start": "1446000",
    "end": "1453080"
  },
  {
    "text": "now I can take this load balancer service so if you noticed before when I was talking about that virtual cluster this is on on AWS we have the cloud",
    "start": "1453080",
    "end": "1460760"
  },
  {
    "text": "controller manager deployed for AWS configured but because we do dual Services when we actually deploy this we",
    "start": "1460760",
    "end": "1466700"
  },
  {
    "text": "can leverage the platform to actually expose things at a platform level so if you wanted to do a platform level",
    "start": "1466700",
    "end": "1471740"
  },
  {
    "text": "Ingress for example you can do that kind of function or in this one a type load balancer that's going to go and actually",
    "start": "1471740",
    "end": "1477919"
  },
  {
    "text": "create an EOB and attach it to the same search to the same pods that are in the cluster and be routable as a a platform",
    "start": "1477919",
    "end": "1485360"
  },
  {
    "text": "level load balancer rather than having each tenant need to own those type of tools so you can layer this and build",
    "start": "1485360",
    "end": "1490820"
  },
  {
    "text": "build pieces that are in essence invisible to the user but act kind of like magic now I sped",
    "start": "1490820",
    "end": "1497720"
  },
  {
    "text": "this up like crazy because if you've used AWS you know that these elbs aren't immediately routable and I'm going to",
    "start": "1497720",
    "end": "1503720"
  },
  {
    "text": "pause it because it goes back and that is the piece so now we're basically curling that elb endpoint and",
    "start": "1503720",
    "end": "1510679"
  },
  {
    "text": "being able to get that okay Bang so going back to the slides I think I'm",
    "start": "1510679",
    "end": "1515900"
  },
  {
    "text": "getting close on time going back to the slides real quick",
    "start": "1515900",
    "end": "1520539"
  },
  {
    "text": "this is showing that entire same structure so behind the scenes and when you deploy any of those pods or",
    "start": "1521720",
    "end": "1527840"
  },
  {
    "text": "any of those services this entire flow is is happening uh behind the scenes for you pushing those iptables rules into",
    "start": "1527840",
    "end": "1533840"
  },
  {
    "text": "the Kata uh containers and making them all routable now I'm talking about the feature and this is where I was saying that this",
    "start": "1533840",
    "end": "1540140"
  },
  {
    "text": "isn't the most scalable architecture if you remember 116 days with things like iptables the",
    "start": "1540140",
    "end": "1545480"
  },
  {
    "text": "amount of uh the amount of network traffic that goes through each time one of these things is updating it's a lot",
    "start": "1545480",
    "end": "1551360"
  },
  {
    "text": "it's an absolute lot luckily the community actually started this we learned about this after we started this project or after we pretty much finished",
    "start": "1551360",
    "end": "1557659"
  },
  {
    "text": "this project there's a new piece of work coming that is still a cap but it's called kaping and kaping is at",
    "start": "1557659",
    "end": "1565059"
  },
  {
    "text": "sigs.ks.io kpng and this is the new node group proxy or an NG proxy and this",
    "start": "1565059",
    "end": "1571159"
  },
  {
    "text": "allows you to actually set up a network control plane which every single workload or every single node at that",
    "start": "1571159",
    "end": "1576260"
  },
  {
    "text": "point would then have a network agent rather than using something like the VN agent or or any of the cnis it then gets",
    "start": "1576260",
    "end": "1583700"
  },
  {
    "text": "all or sorry Cube proxy uh what it's going to do is it's going to actually have a grpc connection back to that",
    "start": "1583700",
    "end": "1590539"
  },
  {
    "text": "Network control plane so our long-term plans are to integrate with this instead because we can actually do really",
    "start": "1590539",
    "end": "1596000"
  },
  {
    "text": "interesting things like pushing single rules through because that's a piece that they in they've started to implement in in the kaping world rather",
    "start": "1596000",
    "end": "1603200"
  },
  {
    "text": "than pushing the entire IP rules set there's also a bunch of syncs that they're calling it uh behind the scenes",
    "start": "1603200",
    "end": "1609260"
  },
  {
    "text": "on the Node on the Node agent side where you can do push it straight into ebpf and all of that stuff it's a really",
    "start": "1609260",
    "end": "1614419"
  },
  {
    "text": "really cool project if you haven't heard about it um again still in a kept phase but could be very cool for for kubernetes to",
    "start": "1614419",
    "end": "1620779"
  },
  {
    "text": "remove some of those big panes that we have with Cube proxy and then the last thing now I talked",
    "start": "1620779",
    "end": "1627919"
  },
  {
    "text": "about this being hard multi-tenancy we're getting closer we're not entirely there those power those pods are still",
    "start": "1627919",
    "end": "1633500"
  },
  {
    "text": "routable on the cluster and you expect something to have like a zero trust Network out of the box that is the one piece of this that we're still that we",
    "start": "1633500",
    "end": "1639919"
  },
  {
    "text": "want to still get to and that's going to come in another kubecon talk so and this is the end so yeah please provide",
    "start": "1639919",
    "end": "1647059"
  },
  {
    "text": "feedback if you have any questions we're happy to unless we don't have time I don't know I don't know",
    "start": "1647059",
    "end": "1652600"
  },
  {
    "text": "how one question",
    "start": "1652640",
    "end": "1655419"
  },
  {
    "text": "hey this was really cool um so you showed the the routing with the IP tables and everything I just curious",
    "start": "1659000",
    "end": "1664760"
  },
  {
    "text": "um service measures are obviously huge is this something that would work with meshes out of the box and",
    "start": "1664760",
    "end": "1670279"
  },
  {
    "text": "um if so what about like if your tenants want to use meshes inside their virtual clusters yep so they could actually",
    "start": "1670279",
    "end": "1676400"
  },
  {
    "text": "technically run uh as long as they have the Privileges to be able to access that guest OS they could technically run a",
    "start": "1676400",
    "end": "1681620"
  },
  {
    "text": "service mesh in there and do some of those same functions the pieces where service meshes don't allow us to do the",
    "start": "1681620",
    "end": "1687679"
  },
  {
    "text": "same type of function is um the way that the informers work for",
    "start": "1687679",
    "end": "1693320"
  },
  {
    "text": "those types of interactions and are set up to talk to a kubernetes API server out of the box they're going to talk to",
    "start": "1693320",
    "end": "1699740"
  },
  {
    "text": "the super cluster and so you have to break that down so there's a little bit of work to make that possible but with",
    "start": "1699740",
    "end": "1704960"
  },
  {
    "text": "this architecture you could just layer a service measure on top of it and allow that to function so you could have like Linker D and istio in the same",
    "start": "1704960",
    "end": "1711140"
  },
  {
    "text": "cluster at that point",
    "start": "1711140",
    "end": "1713799"
  },
  {
    "text": "so the second part of the talk is pretty much about um the techniques that you use to",
    "start": "1716720",
    "end": "1722360"
  },
  {
    "text": "isolate the the workloads in that big data plane right but that's only a problem that you need to solve because",
    "start": "1722360",
    "end": "1728299"
  },
  {
    "text": "you were with the v-cluster approach that has the supercluster with the massive data playing so you could have",
    "start": "1728299",
    "end": "1736220"
  },
  {
    "text": "done something different like you mentioned a few options at the beginning maybe have some hosted control planes manage cluster and then have completely",
    "start": "1736220",
    "end": "1743419"
  },
  {
    "text": "separate and dedicated data planes for each of them like whatever so can you talk a little bit about how like why you",
    "start": "1743419",
    "end": "1750679"
  },
  {
    "text": "choose the big cluster approach and how how it's uh a good fit for your scenario against different options yeah absolutely uh so",
    "start": "1750679",
    "end": "1758419"
  },
  {
    "text": "why virtual cluster is a a good use case to actually leverage um control planes can be expensive uh",
    "start": "1758419",
    "end": "1764059"
  },
  {
    "text": "especially if you live in a in a on-premise world where you're deploying into bare metal host yeah it's a very",
    "start": "1764059",
    "end": "1770120"
  },
  {
    "text": "expensive thing and so being able to layer this architecture allows you to build massive clusters that you support at that level and then smaller more",
    "start": "1770120",
    "end": "1778220"
  },
  {
    "text": "single purpose control planes that you can easily isolate that's pretty much",
    "start": "1778220",
    "end": "1783620"
  },
  {
    "text": "the big piece behind this um there's other there's other tools like the where this originally came from",
    "start": "1783620",
    "end": "1789679"
  },
  {
    "text": "from the folks at Alibaba it allows them to do this sort of isolation in their massive multi-tenant environments",
    "start": "1789679",
    "end": "1798159"
  },
  {
    "text": "cool hey um two questions um starting with um the V cluster",
    "start": "1798380",
    "end": "1804620"
  },
  {
    "text": "um so once you set up V cluster you know we think all your problems are solved you can give it to your customers and like hey you have basically your own",
    "start": "1804620",
    "end": "1810260"
  },
  {
    "text": "kubernetes cluster do whatever you want two minutes later they're gonna go and try to deploy for me if you stack or the Nvidia dcgm exporter or something like",
    "start": "1810260",
    "end": "1816980"
  },
  {
    "text": "that that requires a Daemon set with host access and they're going to be really annoyed and said hey you didn't give me a true true kubernetes cluster I",
    "start": "1816980",
    "end": "1824480"
  },
  {
    "text": "wonder if uh both with and without cattle containers you have been doing any work around that like if you say",
    "start": "1824480",
    "end": "1830659"
  },
  {
    "text": "that you run with cattle containers then you really would need to schedule that Pub inside the same container as the",
    "start": "1830659",
    "end": "1836240"
  },
  {
    "text": "workload is running to be able to extract metrics for that workload the same goes with like log collection fluent bit and that type of stuff",
    "start": "1836240",
    "end": "1844059"
  },
  {
    "text": "yeah I'm sort of struggling can you can you help me what yeah help me a little bit with what",
    "start": "1846399",
    "end": "1852559"
  },
  {
    "text": "you're what you're trying to get to because I'm trying to get to it you know if users want to run their own say",
    "start": "1852559",
    "end": "1857779"
  },
  {
    "text": "Prometheus instance they're going to want to run their own node exporter and you can't obviously can't let them run",
    "start": "1857779",
    "end": "1864020"
  },
  {
    "text": "that on um our metal house because that's where everything runs and then do you do you provide any facility for them",
    "start": "1864020",
    "end": "1871100"
  },
  {
    "text": "to run that inside the cattle container the same container that their other workloads are running in not as of now",
    "start": "1871100",
    "end": "1877039"
  },
  {
    "text": "yeah uh that's something you could potentially do um it's just something we haven't we haven't looked at we are doing things uh",
    "start": "1877039",
    "end": "1882980"
  },
  {
    "text": "that we haven't pushed up yet but we are actively talking with the rest of the the folks on Virtual cluster around how",
    "start": "1882980",
    "end": "1888380"
  },
  {
    "text": "to expose metrics better but not node level metrics um so like not not NPD or",
    "start": "1888380",
    "end": "1893779"
  },
  {
    "text": "like yeah node problems what do you do with kind of logging like how do people get get logging if they can't deploy",
    "start": "1893779",
    "end": "1899000"
  },
  {
    "text": "their own like fluent bit demons and that type of stuff yeah uh so out of the box which you uh you get access to logs",
    "start": "1899000",
    "end": "1905480"
  },
  {
    "text": "through normal like through that VN agent it's basically a proxy that allows you to get to the to the actual logs on",
    "start": "1905480",
    "end": "1910640"
  },
  {
    "text": "that on an individual container it injects the prefixes which is what keeps it so that you can't talk to another tenant's workloads for running things",
    "start": "1910640",
    "end": "1918020"
  },
  {
    "text": "like a Daemon set that has a logging agent out of the box into it",
    "start": "1918020",
    "end": "1923240"
  },
  {
    "text": "that's something that we don't have yet yeah okay yeah I mean you can run your own logging agent then yeah collect that",
    "start": "1923240",
    "end": "1928279"
  },
  {
    "text": "on their behalf and do some multi-tenancy there I guess which is the same thing that we do the next question is around performance and cattle",
    "start": "1928279",
    "end": "1933860"
  },
  {
    "text": "containers yeah no I haven't looked at cattle containers in a while but both both in terms of kind of GPU",
    "start": "1933860",
    "end": "1940039"
  },
  {
    "text": "pass-through and storage it looked like it would have a lot of overhead like the",
    "start": "1940039",
    "end": "1945080"
  },
  {
    "text": "cattle container way of doing storage password with vertile FS seemed very unoptimized um at least a while back",
    "start": "1945080",
    "end": "1951140"
  },
  {
    "text": "there's actually a wonderful talk on Friday afternoon like when everyone else is gone almost like a good talk right",
    "start": "1951140",
    "end": "1958279"
  },
  {
    "text": "about then about how to mitigate the cost of shared file system performance okay I guess I need to change my flight",
    "start": "1958279",
    "end": "1967059"
  },
  {
    "text": "but I mean basically to be I mean best case scenario provider has some kind of direct assigned storage",
    "start": "1967720",
    "end": "1974600"
  },
  {
    "text": "um but yeah so you would mount it inside a cattle container with like NFS or something like that yeah or um to be",
    "start": "1974600",
    "end": "1979940"
  },
  {
    "text": "able to do direct assignment of the black device itself so a lot of times you end up creating a black device attaching it to a host mounting it",
    "start": "1979940",
    "end": "1986539"
  },
  {
    "text": "passing that mount into and it's just a little bit extra if you can actually just only do the mounts inside the guest and you're dealing with it at a black",
    "start": "1986539",
    "end": "1993019"
  },
  {
    "text": "yeah it's better it's still much better but it still is yeah what type of",
    "start": "1993019",
    "end": "1998659"
  },
  {
    "text": "applications do you guys use this for like the user-friendly high performance applications like neural net training I",
    "start": "1998659",
    "end": "2004299"
  },
  {
    "text": "mean it's up to the end end User it's it's a variety yeah um including you know if you look at different database",
    "start": "2004299",
    "end": "2010240"
  },
  {
    "text": "applications things like this so you don't you you feel you consider the",
    "start": "2010240",
    "end": "2015580"
  },
  {
    "text": "performance acceptable for like all types of use cases um",
    "start": "2015580",
    "end": "2020980"
  },
  {
    "text": "in our specific instances I think that uh yeah I I haven't had any issues in our",
    "start": "2020980",
    "end": "2027640"
  },
  {
    "text": "particular but really it varies very much depending on your end user and particularly what they're looking at",
    "start": "2027640",
    "end": "2033519"
  },
  {
    "text": "cool thank you yeah great presentation",
    "start": "2033519",
    "end": "2037380"
  },
  {
    "text": "one minute left so if any quick questions any other last minute thoughts",
    "start": "2039220",
    "end": "2045460"
  },
  {
    "text": "or comments comments yeah you can also file feedback right here",
    "start": "2045460",
    "end": "2051520"
  },
  {
    "text": "awesome oh we got one more oh",
    "start": "2051520",
    "end": "2056338"
  },
  {
    "text": "so I I see that you're using kind of containers for the workloads uh but have you thought about using kind of",
    "start": "2056919",
    "end": "2062378"
  },
  {
    "text": "containers to isolate the control plans why why wouldn't you or why would you uh In This World Behind the Scenes these",
    "start": "2062379",
    "end": "2068440"
  },
  {
    "text": "are actually being deployed into cotton containers we're just not talking about that okay that piece um under the hood because those are",
    "start": "2068440",
    "end": "2073599"
  },
  {
    "text": "being deployed into one or many of those super clusters that are configured as",
    "start": "2073599",
    "end": "2078820"
  },
  {
    "text": "like the default runtime classes being set to microvm it's just automatically deploying uh we're just not calling it",
    "start": "2078820",
    "end": "2084280"
  },
  {
    "text": "out in this slide so okay thanks yeah as well there's some cotta stickers",
    "start": "2084280",
    "end": "2090520"
  },
  {
    "text": "up here if you all want to uh after I don't know if there's enough for everybody but there's some",
    "start": "2090520",
    "end": "2097440"
  },
  {
    "text": "cool all right thanks everybody [Applause]",
    "start": "2097480",
    "end": "2103150"
  }
]