[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "hi i'm ian coldwater and i'm brad giesemann and we attack kubernetes we're here to",
    "start": "80",
    "end": "6399"
  },
  {
    "text": "talk about the evolution of the kubernetes project and its attack surface and to look into the future of what attacks on kubernetes can look like",
    "start": "6399",
    "end": "13440"
  },
  {
    "text": "and to make things fun we'd like to show you a few kubernetes turned six years old this",
    "start": "13440",
    "end": "18720"
  },
  {
    "text": "year and both of us have been working with kubernetes for a few years now both of us have watched the project then",
    "start": "18720",
    "end": "24320"
  },
  {
    "text": "its rapid rate of adoption grows and change over time and we as attackers have evolved along",
    "start": "24320",
    "end": "29519"
  },
  {
    "text": "with it when brad and i first started learning kubernetes it was easier to be able to",
    "start": "29519",
    "end": "35280"
  },
  {
    "start": "31000",
    "end": "31000"
  },
  {
    "text": "understand the whole stack this diagram here was the first kubernetes architecture diagram it's from 2014 before the project went",
    "start": "35280",
    "end": "42239"
  },
  {
    "text": "public even back then it wasn't entirely accurate the person who made this diagram now describes the authorization",
    "start": "42239",
    "end": "48480"
  },
  {
    "text": "on it as aspirational because at the time there wasn't any authorization on kubernetes at all",
    "start": "48480",
    "end": "55120"
  },
  {
    "text": "kubernetes has grown a lot since then and its security posture has improved a lot it's still not secure",
    "start": "55120",
    "end": "60480"
  },
  {
    "text": "by default but it's better than it was we learned kubernetes back when things were simpler and possible to learn",
    "start": "60480",
    "end": "66880"
  },
  {
    "text": "enough of everything but newcomers now have a much steeper hill to climb",
    "start": "66880",
    "end": "72880"
  },
  {
    "text": "yeah take a look at this image it's not just kubernetes anymore that's growing and evolving",
    "start": "72880",
    "end": "77920"
  },
  {
    "text": "the entire ecosystem around it is growing and evolving confused it's not just you a few years",
    "start": "77920",
    "end": "84560"
  },
  {
    "text": "ago kubernetes was the platform now it's a platform for building platforms",
    "start": "84560",
    "end": "90240"
  },
  {
    "text": "this cursed image here is the cloud native computing foundation landscape since you're a cloud native con you've",
    "start": "90240",
    "end": "95360"
  },
  {
    "text": "probably seen it this image is so overwhelming that even this screenshot from cncf's webpage asks",
    "start": "95360",
    "end": "101840"
  },
  {
    "text": "on the top if you're overwhelmed it's a lot for everyone to keep track of even for people who often work with and",
    "start": "101840",
    "end": "107040"
  },
  {
    "text": "pay attention to this stuff additional features plugins and configuration on top of",
    "start": "107040",
    "end": "112479"
  },
  {
    "text": "kubernetes add whole new layers of complexity all of these have to be kept up with and secured independently and together",
    "start": "112479",
    "end": "119600"
  },
  {
    "text": "and they don't always play nice with each other so as complexity grows defenders and operators have a much",
    "start": "119600",
    "end": "125439"
  },
  {
    "text": "harder time understanding the aspects of each component and their interactions in the environment with more moving parts",
    "start": "125439",
    "end": "132160"
  },
  {
    "text": "there's more to configure and with more complexity per part they're more likely to be misconfigured",
    "start": "132160",
    "end": "139360"
  },
  {
    "text": "historically new versions of kubernetes have been released about every 90 days and the support for security fixes is",
    "start": "139360",
    "end": "145280"
  },
  {
    "text": "only applicable to the most recent three releases that means given a given version is only",
    "start": "145280",
    "end": "150720"
  },
  {
    "text": "supported for approximately nine months due to that rapid cadence managed kubernetes providers",
    "start": "150720",
    "end": "156879"
  },
  {
    "text": "typically only support versions that are one or more minor releases behind the latest and organizations managing their own",
    "start": "156879",
    "end": "163440"
  },
  {
    "text": "kubernetes clusters often do this too as it always takes some time to test and validate new releases before they make",
    "start": "163440",
    "end": "169840"
  },
  {
    "text": "it to production for example the latest generally available kubernetes release is 1.19",
    "start": "169840",
    "end": "175440"
  },
  {
    "text": "but many are running 1.15 or 1.16 in production simply because those are the latest versions available to them",
    "start": "175440",
    "end": "181840"
  },
  {
    "text": "but that can actually work to our benefit as defenders we can use this time to look at new features now",
    "start": "181840",
    "end": "188319"
  },
  {
    "text": "before they're actually used in a few months but as attackers that just means there's plenty of old",
    "start": "188319",
    "end": "193519"
  },
  {
    "text": "clusters to break so everything is growing and evolving",
    "start": "193519",
    "end": "198640"
  },
  {
    "text": "really rapidly and changing really rapidly so as kubernetes evolves attackers have to evolve with it as the",
    "start": "198640",
    "end": "206080"
  },
  {
    "text": "kubernetes project has implemented more security over time attackers have had to learn more about kubernetes to be effective than they",
    "start": "206080",
    "end": "211840"
  },
  {
    "text": "used to be it used to be really easy to attack kubernetes you could just do things like execute",
    "start": "211840",
    "end": "217200"
  },
  {
    "text": "commands as root with an unauthenticated curl call not actually kidding now you need a deeper understanding of",
    "start": "217200",
    "end": "223200"
  },
  {
    "text": "the moving parts on the high level and often the linux primitives that underlie the low levels so all of us have to level up attackers",
    "start": "223200",
    "end": "230480"
  },
  {
    "text": "defenders everybody and in that spirit let's look ahead a bit we'd like to help you level up",
    "start": "230480",
    "end": "237439"
  },
  {
    "text": "what does the landscape look like coming down the pike what might attackers be able to do moving forward we'd like to",
    "start": "237439",
    "end": "244959"
  },
  {
    "text": "demonstrate some novel attacks to show you some examples of what attacks can look like in the future what could a sophisticated",
    "start": "244959",
    "end": "252000"
  },
  {
    "text": "attacker who knows kubernetes well be capable of we're pretty excited about these attacks we know you will be too oh yeah before",
    "start": "252000",
    "end": "259759"
  },
  {
    "start": "259000",
    "end": "259000"
  },
  {
    "text": "we get into the demos let's get into an attacker mindset what might an attacker bent on foul play",
    "start": "259759",
    "end": "265759"
  },
  {
    "text": "generally want to do in kubernetes well here's what we might want to do first we want to become cluster admin",
    "start": "265759",
    "end": "272960"
  },
  {
    "text": "this could happen by either compromising the cluster and becoming cluster admin or maybe you are a malicious administrator or insider",
    "start": "272960",
    "end": "279520"
  },
  {
    "text": "threat every attack that we're going to be demonstrating in this talk assumes that you've done this already then you probably want to",
    "start": "279520",
    "end": "285759"
  },
  {
    "text": "get all the secrets you don't want to get blocked for sure and establish persistence without",
    "start": "285759",
    "end": "291199"
  },
  {
    "text": "getting caught it's important to remember that different attackers have different goals and motivations",
    "start": "291199",
    "end": "297040"
  },
  {
    "text": "cluster admin isn't always the end goal not every attacker comes in from the outside and sometimes the goal can be outside of",
    "start": "297040",
    "end": "303280"
  },
  {
    "text": "the cluster itself so on that let's say we are",
    "start": "303280",
    "end": "308560"
  },
  {
    "start": "305000",
    "end": "305000"
  },
  {
    "text": "maybe a malicious administrator or an attacker who has access to a cluster as cluster admin",
    "start": "308560",
    "end": "314000"
  },
  {
    "text": "we have access to all of the data and secrets in this cluster already and that's pretty cool but we only have",
    "start": "314000",
    "end": "320560"
  },
  {
    "text": "those for right now and they might change over time so it would be nice to see it in real time and get the newest credentials if they",
    "start": "320560",
    "end": "326960"
  },
  {
    "text": "get rolled or something like that as they get updated there is a component of the api server that we can misuse for this purpose",
    "start": "326960",
    "end": "333520"
  },
  {
    "text": "it's a form of admission control called a validating web a validating web hook is a native",
    "start": "333520",
    "end": "339360"
  },
  {
    "start": "338000",
    "end": "338000"
  },
  {
    "text": "configuration option that lets a cluster administrator check with an external web service for whether or not to allow a",
    "start": "339360",
    "end": "344400"
  },
  {
    "text": "resource to get created if you've heard of projects like open policy agent gatekeeper or k-rail",
    "start": "344400",
    "end": "350479"
  },
  {
    "text": "all of these projects use validating web hooks as a mechanism for enforcing security policy here's how it works when a user submits",
    "start": "350479",
    "end": "357840"
  },
  {
    "text": "a request to do something like create a new pod after going through authentication and authorization it gets to the validating",
    "start": "357840",
    "end": "363600"
  },
  {
    "text": "admission controllers if a validating web hook is configured a full copy of the pod request is sent",
    "start": "363600",
    "end": "369199"
  },
  {
    "text": "the admission controller then parses the request and uses custom logic to give a yes or no response",
    "start": "369199",
    "end": "374960"
  },
  {
    "text": "if the logic allows it by sending a yes response the pod can be saved in etsy",
    "start": "374960",
    "end": "380080"
  },
  {
    "text": "if not then not you can have multiple validating web hooks if you want to and you can pick which resources you",
    "start": "380080",
    "end": "386319"
  },
  {
    "text": "want to trigger on and the best part is that request doesn't have to get sent",
    "start": "386319",
    "end": "392800"
  },
  {
    "text": "or run inside the cluster it can go to any arbitrary url on a system somewhere else",
    "start": "392800",
    "end": "400720"
  },
  {
    "text": "as an attacker or a malicious admin you might want a web hook to be called only when secrets are created or updated",
    "start": "400800",
    "end": "407600"
  },
  {
    "text": "you could install a web hook like this filter only on secrets and then send it to an attacking system",
    "start": "407600",
    "end": "412800"
  },
  {
    "text": "that just logs what it receives so revisiting this data flow but with our malicious web hook in line",
    "start": "412800",
    "end": "419840"
  },
  {
    "text": "sometime passes and a user might create a new secret via the api server and we'll assume it gets accepted by the",
    "start": "419840",
    "end": "426080"
  },
  {
    "text": "first validating web hook in steps two and three in step four the entire secret gets sent",
    "start": "426080",
    "end": "431280"
  },
  {
    "text": "to the attacker controlled application via web request we can configure our malicious application to simply log the",
    "start": "431280",
    "end": "437520"
  },
  {
    "text": "secret and always send an approval back to the response it gets saved into etcd then as normal",
    "start": "437520",
    "end": "444160"
  },
  {
    "text": "but being good evil attackers we don't want them to notice if our custom web hook service is ever down",
    "start": "444160",
    "end": "449280"
  },
  {
    "text": "so we can tell the api server to wait up to one second for our web service to respond yes",
    "start": "449280",
    "end": "454319"
  },
  {
    "text": "and then just fail open as if we send a yes anyway if we don't send a response so maybe we add a tiny bit of delay but",
    "start": "454319",
    "end": "460880"
  },
  {
    "text": "not enough for most folks to even notice let's demonstrate",
    "start": "460880",
    "end": "466319"
  },
  {
    "text": "so we have a demo cluster and we have cluster admin access so we can run kube control get nodes and",
    "start": "467280",
    "end": "475039"
  },
  {
    "text": "see that we have a single control plane worker sorry a single control plane node and a single kind worker we're using kind here if",
    "start": "475039",
    "end": "483759"
  },
  {
    "text": "that wasn't obvious we can run a coupe control get pods in all name spaces and get secrets to give you an idea that this is basically an",
    "start": "483759",
    "end": "490479"
  },
  {
    "text": "empty cluster okay so validating web hooks require tls",
    "start": "490479",
    "end": "497680"
  },
  {
    "text": "so we can generate a self-signed certificate we have a script to do just that and",
    "start": "497680",
    "end": "504400"
  },
  {
    "text": "what that's done is created a self-sertin and what that's done is created a certificate signing request and had it",
    "start": "504400",
    "end": "510960"
  },
  {
    "text": "automatically approved and that was stored in the secret named validator",
    "start": "510960",
    "end": "516959"
  },
  {
    "text": "so now that that was just generated and stored in the secret we can fetch it out and save it locally to a file called",
    "start": "519599",
    "end": "525680"
  },
  {
    "text": "cert.pem",
    "start": "525680",
    "end": "528480"
  },
  {
    "text": "so next we're going to install the malicious validating web hook application named validator which is going to",
    "start": "532240",
    "end": "537440"
  },
  {
    "text": "capture all the secrets sent to it by the api server",
    "start": "537440",
    "end": "542320"
  },
  {
    "text": "so we've deployed a config map the deployment itself and a service called validator that points to it",
    "start": "543360",
    "end": "550480"
  },
  {
    "text": "so let's look at the validating web hook configuration that we're about to apply",
    "start": "550640",
    "end": "556160"
  },
  {
    "text": "and as you can see it's a validating web hook configuration resource and we're pointing it at the validator",
    "start": "558240",
    "end": "563760"
  },
  {
    "text": "service in the default namespace and the failure policy is to ignore which is the fail open setting",
    "start": "563760",
    "end": "569600"
  },
  {
    "text": "we only wait one second in case our application is ever down and we continue on anyway",
    "start": "569600",
    "end": "575200"
  },
  {
    "text": "we've configured the rules to trigger only on create update or delete of secrets resources",
    "start": "575200",
    "end": "584160"
  },
  {
    "text": "so let's install the validating webhook configuration into the api server",
    "start": "584959",
    "end": "591040"
  },
  {
    "text": "okay so and now when a regular user creates a new secret such as this one",
    "start": "591040",
    "end": "598720"
  },
  {
    "text": "the damage is already done we can go look at the validating webhook logs",
    "start": "601600",
    "end": "608720"
  },
  {
    "text": "with capturing the secret in the clear from our malicious webhook application",
    "start": "608720",
    "end": "614959"
  },
  {
    "text": "and as you can see we have honk and that's the base 64 encoded",
    "start": "614959",
    "end": "620800"
  },
  {
    "text": "version of that secret awesome awesome so now we have a permanent",
    "start": "620800",
    "end": "627120"
  },
  {
    "text": "method for ensuring we are always aware of the latest credentials and secrets stored in this cluster in real time that's pretty sweet and",
    "start": "627120",
    "end": "634399"
  },
  {
    "text": "that was pretty cool but it's not very stealthy the api server logs everything we do and an observant",
    "start": "634399",
    "end": "641920"
  },
  {
    "text": "admin could maybe notice it the api server also has a lot of authentication authorization",
    "start": "641920",
    "end": "647440"
  },
  {
    "text": "options that an administrator could use to stop us",
    "start": "647440",
    "end": "652800"
  },
  {
    "text": "that's kind of a bummer we could maybe get around it by doing various things that are kind of convoluted",
    "start": "652800",
    "end": "658640"
  },
  {
    "text": "but i feel like maybe we could make it easier for ourselves maybe if we bypass the api server",
    "start": "658640",
    "end": "666399"
  },
  {
    "text": "we don't have to conform to its security policy and maybe we won't be logged",
    "start": "666399",
    "end": "671440"
  },
  {
    "text": "in some clusters you can schedule pods on the same node as where the api server runs",
    "start": "671440",
    "end": "678079"
  },
  {
    "start": "678000",
    "end": "678000"
  },
  {
    "text": "so what if we scheduled a full copy of the real api server on the same control plane node",
    "start": "678079",
    "end": "684079"
  },
  {
    "text": "and used the same keys and network paths to talk to etcd that the real control plane has",
    "start": "684079",
    "end": "689680"
  },
  {
    "text": "but instead of logging activities or blocking anything we just have it set to allow full access as an",
    "start": "689680",
    "end": "695279"
  },
  {
    "text": "attacker that's pretty handy right keeps the logs of the api server clean the real one",
    "start": "695279",
    "end": "700320"
  },
  {
    "text": "and maintains a persistent access channel to the underlying state of the cluster where all the loot is kept with our own",
    "start": "700320",
    "end": "707680"
  },
  {
    "text": "access to xcd we can control everything in the cluster and depending on what kind of metadata",
    "start": "707680",
    "end": "713040"
  },
  {
    "text": "is in there we could potentially control access to cloud accounts outside of the cluster as well",
    "start": "713040",
    "end": "718720"
  },
  {
    "text": "let's see what that would look like yeah",
    "start": "718720",
    "end": "722800"
  },
  {
    "text": "so we've already made an exact copy of the current api server configuration and we've made a few key changes so",
    "start": "724560",
    "end": "731760"
  },
  {
    "text": "let's take a look we'll call your attention to us setting",
    "start": "731760",
    "end": "736959"
  },
  {
    "text": "anonymous auth equals true and authorization mode to always allow this effectively disables",
    "start": "736959",
    "end": "743279"
  },
  {
    "text": "authentication and authorization in the api server and we set the insecure port meaning the",
    "start": "743279",
    "end": "748560"
  },
  {
    "text": "plain http service listing on port 443",
    "start": "748560",
    "end": "753760"
  },
  {
    "text": "the last thing i want to call attention to is because we're running on the same control plane node we have access to the",
    "start": "753760",
    "end": "759200"
  },
  {
    "text": "certificates that are used to communicate with ftb we're using tcp 443 because",
    "start": "759200",
    "end": "765200"
  },
  {
    "text": "it's a common traffic port that is pretty unlikely to be blocked by firewall rules also traffic on 443 is not very likely to be",
    "start": "765200",
    "end": "771839"
  },
  {
    "text": "noticed indeed so let's go ahead and install that",
    "start": "771839",
    "end": "777360"
  },
  {
    "text": "all right so we've installed that on the control plane node right now so let's test our use of this what we",
    "start": "777519",
    "end": "783920"
  },
  {
    "text": "can do is we can run we'll call it an attack pod inside the cluster and this is a tool that was built by a",
    "start": "783920",
    "end": "790720"
  },
  {
    "text": "friend of ours rory mccune great storage",
    "start": "790720",
    "end": "795279"
  },
  {
    "text": "so from this newly created attack pod we can simply just use curl to hit our api server not the real",
    "start": "799040",
    "end": "805920"
  },
  {
    "text": "one directly and dump all the secrets as you can see we're hitting it on the api v1 secrets endpoint which is saying all",
    "start": "805920",
    "end": "812800"
  },
  {
    "text": "secrets all name spaces and there we go everything coming from",
    "start": "812800",
    "end": "817839"
  },
  {
    "text": "at cd directly nice so now we can use our own api",
    "start": "817839",
    "end": "825360"
  },
  {
    "text": "server to interact with sed directly whenever we want without fear of being blocked or logged",
    "start": "825360",
    "end": "831040"
  },
  {
    "text": "this diagram explains how what we just did worked and that's pretty cool",
    "start": "831040",
    "end": "836240"
  },
  {
    "text": "but it's not quite a persistent strategy let's broaden our horizons a bit i think",
    "start": "836240",
    "end": "842079"
  },
  {
    "text": "we could go bigger what if we deployed a c2",
    "start": "842079",
    "end": "847519"
  },
  {
    "text": "a c2 is a command and control infrastructure red teamers or attackers might use it",
    "start": "847519",
    "end": "853839"
  },
  {
    "text": "to exfiltrate data to an external place or maybe have a persistent channel that can",
    "start": "853839",
    "end": "860000"
  },
  {
    "text": "move data back and forth you can also command and control the",
    "start": "860000",
    "end": "866480"
  },
  {
    "text": "server that you're attacking from your c2 infrastructure and red teamers have you know given",
    "start": "866480",
    "end": "872880"
  },
  {
    "text": "talks about how you can set up your command and control infrastructure on kubernetes",
    "start": "872880",
    "end": "878000"
  },
  {
    "text": "basically teaching each other how to deploy things that's pretty cool but that's not what i mean here i feel",
    "start": "878000",
    "end": "883839"
  },
  {
    "text": "like what if we built a c2",
    "start": "883839",
    "end": "889839"
  },
  {
    "text": "not spinning it up on kubernetes what if we built a c2 out of kubernetes itself",
    "start": "889920",
    "end": "898079"
  },
  {
    "text": "we could call it see tubernettis what if the moment we got cluster admin",
    "start": "898079",
    "end": "904720"
  },
  {
    "text": "we could deploy a persistence mechanism really quickly with the least amount of logs tracking",
    "start": "904720",
    "end": "910160"
  },
  {
    "text": "that activity and the greatest chance of going undetected for the longest period of time",
    "start": "910160",
    "end": "915519"
  },
  {
    "text": "we could maybe do this by installing an agent on all cluster nodes that joined them to an external cluster that we",
    "start": "915519",
    "end": "921040"
  },
  {
    "text": "control they would still be a part of their own cluster and otherwise behave normally so",
    "start": "921040",
    "end": "926079"
  },
  {
    "text": "probably an admin wouldn't notice but they'd also just happen to be a part of our cluster too",
    "start": "926079",
    "end": "932000"
  },
  {
    "text": "that would mean that as attackers we would be using kubernetes as our command and control infrastructure to control multiple compromised clusters",
    "start": "932000",
    "end": "938959"
  },
  {
    "text": "in a single place while staying lurking and undetected in the shadows how could we do this i have an idea",
    "start": "938959",
    "end": "947120"
  },
  {
    "text": "we can build it using kubernetes itself we could build it using k3s",
    "start": "947120",
    "end": "954800"
  },
  {
    "text": "what is k3s and why did we pick k3s to deploy our c2 with",
    "start": "954800",
    "end": "960160"
  },
  {
    "text": "kcs is smaller and simpler than a regular kubernetes distribution it's designed to be lightweight for",
    "start": "960160",
    "end": "965680"
  },
  {
    "text": "resource constrained environments such as internet of things and k3s's requirements reflect that k3s",
    "start": "965680",
    "end": "972240"
  },
  {
    "text": "has different network requirements than regular kubernetes which are advantages for the purposes of rc2",
    "start": "972240",
    "end": "978079"
  },
  {
    "text": "most importantly it only requires a single tls connection outbound from knowns to the control plane",
    "start": "978079",
    "end": "983199"
  },
  {
    "text": "which is very likely to be available and also likely to blend in with valid traffic so it might not be noticed k3s has",
    "start": "983199",
    "end": "990480"
  },
  {
    "start": "989000",
    "end": "989000"
  },
  {
    "text": "nearly all of the same moving parts as a full version of kubernetes on the right here in this slide is the simplified visual of the k3s control",
    "start": "990480",
    "end": "996959"
  },
  {
    "text": "plane components in green and the k3s nodes in red that we'll overlay into the next slide",
    "start": "996959",
    "end": "1002959"
  },
  {
    "text": "on the left of this slide is a kubernetes architecture diagram in a normal cluster so how could we use",
    "start": "1002959",
    "end": "1009759"
  },
  {
    "text": "k3s to build a c2 i think we could install the control plane component on a vm in another cloud",
    "start": "1009759",
    "end": "1016480"
  },
  {
    "text": "say listening on tcp 443 again who blocks that anyway but with cluster admin access to the",
    "start": "1016480",
    "end": "1022160"
  },
  {
    "text": "target kubernetes cluster we can deploy a workload that immediately escapes the container to the underlying host",
    "start": "1022160",
    "end": "1028160"
  },
  {
    "text": "on every worker node from there we can install the k3s node in the background with a configuration",
    "start": "1028160",
    "end": "1035360"
  },
  {
    "text": "to auto join our k3s cluster on start and then the container just exits a few seconds after",
    "start": "1035360",
    "end": "1041760"
  },
  {
    "text": "establishing that persistence the end result would look kind of like this a couple really cool things that",
    "start": "1041760",
    "end": "1047760"
  },
  {
    "text": "happened with this approach the k3s agents have full access to the real hosts file system",
    "start": "1047760",
    "end": "1053360"
  },
  {
    "text": "so we can grab anything we want directly from all the nodes the best part is the original cluster",
    "start": "1053360",
    "end": "1058960"
  },
  {
    "text": "continues to be fully operational we didn't break anything that the admins might even notice the next time they run",
    "start": "1058960",
    "end": "1064880"
  },
  {
    "text": "code control get pods on their cluster they won't see any k3s pods and they won't see any logs of us hitting",
    "start": "1064880",
    "end": "1071039"
  },
  {
    "text": "their api server pretty cool so let's see that in action shall we",
    "start": "1071039",
    "end": "1077520"
  },
  {
    "text": "so now on our attacker own k3s control play node running on another cloud virtual machine",
    "start": "1079440",
    "end": "1086080"
  },
  {
    "text": "we've already set up the k3s control plane and as you can see we have",
    "start": "1086080",
    "end": "1092160"
  },
  {
    "text": "just the control plane node running there's no worker nodes attached",
    "start": "1092160",
    "end": "1097200"
  },
  {
    "text": "we also have recently compromised a gke cluster and it has three nodes we have cluster admin access to this",
    "start": "1099039",
    "end": "1106840"
  },
  {
    "text": "cluster let's look closely at our specially",
    "start": "1106840",
    "end": "1112640"
  },
  {
    "text": "crafted c2 daemon set so we're going to install in the cube",
    "start": "1112640",
    "end": "1118320"
  },
  {
    "text": "system namespace we're going to run in the host network namespace and it's",
    "start": "1118320",
    "end": "1123679"
  },
  {
    "text": "just a busy box image and there's a lot going on here but i'll summarize we're basically",
    "start": "1123679",
    "end": "1129440"
  },
  {
    "text": "installing a separate docker network if we need to and then we're running directly via",
    "start": "1129440",
    "end": "1134880"
  },
  {
    "text": "docker run in the background the k3s agent and we're supplying the privileged flag",
    "start": "1134880",
    "end": "1140799"
  },
  {
    "text": "and the auto join credentials so that it knows how to phone home to our k3s control plane",
    "start": "1140799",
    "end": "1146160"
  },
  {
    "text": "automatically and we're mounting the root file system on the underlying node in slash root fs and we're just throwing",
    "start": "1146160",
    "end": "1153360"
  },
  {
    "text": "in some tolerations here to make sure we get scheduled on any node that is in a cluster",
    "start": "1153360",
    "end": "1159600"
  },
  {
    "text": "so let's go ahead and deploy that c2 to our gke cluster",
    "start": "1160240",
    "end": "1165840"
  },
  {
    "text": "and so when we could control get pods in the cube system namespace we can see our honk c2 daemon set is",
    "start": "1168480",
    "end": "1175120"
  },
  {
    "text": "currently running okay so back on k3s on our control plane",
    "start": "1175120",
    "end": "1182799"
  },
  {
    "text": "uh we can run kube control get nodes again but this time the gke nodes are also",
    "start": "1182799",
    "end": "1190400"
  },
  {
    "text": "part of our c2 control plane",
    "start": "1190400",
    "end": "1194640"
  },
  {
    "text": "so not playing favorites we've also compromised eks",
    "start": "1196880",
    "end": "1202000"
  },
  {
    "text": "let's get nodes on that cluster",
    "start": "1202000",
    "end": "1207840"
  },
  {
    "text": "and there they are just like the gke cluster three nodes let's deploy rc2 to eks",
    "start": "1208880",
    "end": "1216400"
  },
  {
    "text": "in a similar process",
    "start": "1217120",
    "end": "1221840"
  },
  {
    "text": "finally last but not least we don't want to leave anyone out we've compromised an aks cluster",
    "start": "1222960",
    "end": "1229360"
  },
  {
    "text": "and it too has three nodes so let's deploy rc2 to aks in the exact",
    "start": "1229760",
    "end": "1236159"
  },
  {
    "text": "same way okay",
    "start": "1236159",
    "end": "1240960"
  },
  {
    "text": "so we don't want to leave any tracks right so now we can take the opportunity to clean up our daemon sets",
    "start": "1242640",
    "end": "1248320"
  },
  {
    "text": "we can do that with the kube control delete so now that's removed from the gke cluster",
    "start": "1248320",
    "end": "1256400"
  },
  {
    "text": "if we get pods in all name spaces inside the gke cluster",
    "start": "1258960",
    "end": "1264080"
  },
  {
    "text": "we can see that it's now gone so let's do that same process to eks",
    "start": "1264080",
    "end": "1271919"
  },
  {
    "text": "cluster we'll delete that daemon set",
    "start": "1271919",
    "end": "1277840"
  },
  {
    "text": "and we'll do the same to aks fully cover our tracks",
    "start": "1278480",
    "end": "1283679"
  },
  {
    "text": "alright so all of them are now deleted",
    "start": "1283919",
    "end": "1287679"
  },
  {
    "text": "so back on k3s we can now run code control get nodes",
    "start": "1289280",
    "end": "1295840"
  },
  {
    "text": "and we can see all three nodes from all three clusters from all three clouds",
    "start": "1296240",
    "end": "1301760"
  },
  {
    "text": "how's that for a multi-cloud strategy i know right",
    "start": "1301760",
    "end": "1306240"
  },
  {
    "text": "so and you were talking about stealing all the secrets right well now that we have the ability to see",
    "start": "1308720",
    "end": "1314400"
  },
  {
    "text": "the root file system on all the nodes why don't we make a daemon set to run inside k3s",
    "start": "1314400",
    "end": "1321440"
  },
  {
    "text": "that steals all the secrets from the nodes sounds awesome let's do it yeah so a simple busy box image",
    "start": "1321440",
    "end": "1328799"
  },
  {
    "text": "doing a find to get all the mounted secrets attached to the kubelet",
    "start": "1328799",
    "end": "1334159"
  },
  {
    "text": "and it's mounting the host file path and we can deploy that again we're",
    "start": "1334159",
    "end": "1339919"
  },
  {
    "text": "deploying this to the k3s control plane so that it will run on all the clusters",
    "start": "1339919",
    "end": "1347840"
  },
  {
    "text": "and now we can review the loot oh there it is so all the certificates",
    "start": "1351280",
    "end": "1359120"
  },
  {
    "text": "all the keys all the service count tokens not bad not bad",
    "start": "1359120",
    "end": "1364880"
  },
  {
    "text": "you also talked ian about expanding to the rest of the cloud infrastructure right so why don't we touch the instance",
    "start": "1368159",
    "end": "1374320"
  },
  {
    "text": "metadata credentials that sounds great then we can compromise the cloud accounts not just the clusters",
    "start": "1374320",
    "end": "1381039"
  },
  {
    "text": "so we can w get on the magic ips for both aws and gcp and for azure",
    "start": "1381039",
    "end": "1388159"
  },
  {
    "text": "we can go go ahead and grab that from the file system of the worker nodes again a very similar data set",
    "start": "1388159",
    "end": "1395919"
  },
  {
    "text": "so we can deploy that data set again to the k3s cluster so that it runs on all the nodes across all the clusters",
    "start": "1396480",
    "end": "1403919"
  },
  {
    "text": "across all the clouds",
    "start": "1403919",
    "end": "1406799"
  },
  {
    "text": "oh and there it is so we've got gcp access tokens we've got aws",
    "start": "1412080",
    "end": "1417919"
  },
  {
    "text": "credentials and we've got azure principles and client secrets",
    "start": "1417919",
    "end": "1424159"
  },
  {
    "text": "all your clouds are belong to us we now have our cluster of clusters",
    "start": "1424159",
    "end": "1432880"
  },
  {
    "start": "1427000",
    "end": "1427000"
  },
  {
    "text": "that was pretty sweet yeah as far as we know no one has ever done anything like this",
    "start": "1432880",
    "end": "1438559"
  },
  {
    "start": "1433000",
    "end": "1433000"
  },
  {
    "text": "before we think it's a good example of what cloud native attacks can look like in the future from sophisticated attackers who",
    "start": "1438559",
    "end": "1445279"
  },
  {
    "text": "understand kubernetes at a deep level know how to use it and have the mindset of someone who can figure out how to",
    "start": "1445279",
    "end": "1450480"
  },
  {
    "text": "misuse it this feature is exciting and if you're a defender maybe a little bit scary",
    "start": "1450480",
    "end": "1456960"
  },
  {
    "text": "and all of us have to prepare for it because as sure as kubernetes releases come about every 90 days",
    "start": "1456960",
    "end": "1462880"
  },
  {
    "text": "it's going to keep coming so while we're looking into the future let's take a look at what's coming and",
    "start": "1462880",
    "end": "1469279"
  },
  {
    "start": "1465000",
    "end": "1465000"
  },
  {
    "text": "what's already here here are some upcoming and new kubernetes features to keep an eye on",
    "start": "1469279",
    "end": "1474400"
  },
  {
    "text": "but we're pretty excited about the possibilities of as attackers and maybe that you might want to look",
    "start": "1474400",
    "end": "1479440"
  },
  {
    "text": "out for in your environments if you're a defender these can do interesting things individually and together",
    "start": "1479440",
    "end": "1486559"
  },
  {
    "text": "the first one we wanted to point out is a brand new one as of kubernetes 1.19 coupe ctrl run",
    "start": "1486559",
    "end": "1493520"
  },
  {
    "text": "now has a privileged flag it's like docker's privileged flag",
    "start": "1493520",
    "end": "1499039"
  },
  {
    "text": "as penetration testers who work in container environments we know that setting your docker runs to",
    "start": "1499039",
    "end": "1505919"
  },
  {
    "text": "privileged means that basically you're giving us a very short and easy day",
    "start": "1505919",
    "end": "1511520"
  },
  {
    "text": "in previous versions kubernetes components were largely configurable via the files on the node's file system",
    "start": "1511520",
    "end": "1517200"
  },
  {
    "text": "or via cli switches this provided a natural separation between configuration",
    "start": "1517200",
    "end": "1522320"
  },
  {
    "text": "in kubernetes versus configuration of kubernetes components but it can introduce friction when",
    "start": "1522320",
    "end": "1528159"
  },
  {
    "text": "automating widespread configuration changes so to help with this a couple of new feature gates are available that can",
    "start": "1528159",
    "end": "1534159"
  },
  {
    "text": "enable dynamic reconfigurations of kubernetes components via the kubernetes api so for example",
    "start": "1534159",
    "end": "1541440"
  },
  {
    "text": "the dynamic audit logsync configuration can't modify which api",
    "start": "1541440",
    "end": "1546960"
  },
  {
    "text": "actions to audit log but it does control where those audit logs should be sent so we can use this",
    "start": "1546960",
    "end": "1552320"
  },
  {
    "text": "dynamic configuration setting to send all kubernetes audit logs to an application that we control",
    "start": "1552320",
    "end": "1557840"
  },
  {
    "text": "instead we can then filter those logs and forward them on to the original destination to hide",
    "start": "1557840",
    "end": "1563440"
  },
  {
    "text": "just our malicious log entries and send the rest on and thus covering our tracks that's",
    "start": "1563440",
    "end": "1569679"
  },
  {
    "text": "pretty sweet but wait there's two things on this slide what can we do with dynamic kubla configuration",
    "start": "1569679",
    "end": "1575679"
  },
  {
    "text": "we can use our privileged access to kubernetes to reconfigure kubernetes itself",
    "start": "1575679",
    "end": "1580720"
  },
  {
    "text": "and change its own security posture wow kubernetes security posture really has",
    "start": "1580720",
    "end": "1586240"
  },
  {
    "text": "improved a lot remember those attacks when we first started i do it was a simpler time one of the",
    "start": "1586240",
    "end": "1592240"
  },
  {
    "text": "ones i really liked was kublet exploit greets dakires kublat exploit was one of",
    "start": "1592240",
    "end": "1597520"
  },
  {
    "text": "the first kubernetes hacks it was involved in infamous security peaches it could be easily used and was",
    "start": "1597520",
    "end": "1604320"
  },
  {
    "text": "exploited in the wild since the defaults have improved you can't really use kublet exploit anymore",
    "start": "1604320",
    "end": "1610080"
  },
  {
    "text": "and personally i kind of miss its ease of use well actually the kublet api exploit",
    "start": "1610080",
    "end": "1618000"
  },
  {
    "text": "isn't really an exploit it's just a configuration on the kubelet's api that let you run arbitrary commands",
    "start": "1618000",
    "end": "1623600"
  },
  {
    "text": "inside any pods container on that node without authenticating okay sure but historically it really was",
    "start": "1623600",
    "end": "1630159"
  },
  {
    "text": "exploited a lot true true kubla's default configuration has become more secure over time",
    "start": "1630159",
    "end": "1636159"
  },
  {
    "text": "disallowing old attacks that used to work like this but now that reconfiguring the kubelet is so easy via",
    "start": "1636159",
    "end": "1641840"
  },
  {
    "text": "the api let's do that and bring the kubelet exploit back",
    "start": "1641840",
    "end": "1647360"
  },
  {
    "text": "all right let's see that let's see that in action so back on our demo cluster we can run",
    "start": "1647360",
    "end": "1654640"
  },
  {
    "text": "another attack pod we're going to use the same image as before",
    "start": "1654640",
    "end": "1661520"
  },
  {
    "text": "and what we want to do is show you what it looks like to try the kubrick exploit and see it successfully blocked by the",
    "start": "1664080",
    "end": "1670159"
  },
  {
    "text": "coolest configuration we can do this by executing into that attack pod and just using curl to hit",
    "start": "1670159",
    "end": "1677520"
  },
  {
    "text": "the kubelet's api on port 102.50 against the running pod's endpoint",
    "start": "1677520",
    "end": "1682720"
  },
  {
    "text": "as you can see it's unauthorized so before we want to change the kublet's",
    "start": "1682720",
    "end": "1688880"
  },
  {
    "text": "configuration we probably want to get its current version and just tweak it a little bit so let's do that let's pull the current",
    "start": "1688880",
    "end": "1695120"
  },
  {
    "text": "kubelet's configuration out of the cluster and save it as a local file so we can edit that we'll",
    "start": "1695120",
    "end": "1701120"
  },
  {
    "text": "call it kubelet config z kind worker and now let's modify that",
    "start": "1701120",
    "end": "1711200"
  },
  {
    "text": "so what we want to do is go to the web hook and disable that and turn on",
    "start": "1711200",
    "end": "1720080"
  },
  {
    "text": "anonymous authentication and then change the authorization mode from web",
    "start": "1720080",
    "end": "1725520"
  },
  {
    "text": "hook to always allow",
    "start": "1725520",
    "end": "1730960"
  },
  {
    "text": "okay we made that modification next we want to create a config map",
    "start": "1732240",
    "end": "1737760"
  },
  {
    "text": "that's going to hold this kubelet's configuration we'll call it my node config and save it in the kube system name",
    "start": "1737760",
    "end": "1744159"
  },
  {
    "text": "space before we deploy that i just want to show you that we indeed have the web hook off",
    "start": "1744159",
    "end": "1749679"
  },
  {
    "text": "anonymous auth is true and the authorization mode is always allow",
    "start": "1749679",
    "end": "1755919"
  },
  {
    "text": "so let's apply the updated kubelet configuration we do this by patching the node object",
    "start": "1758559",
    "end": "1766240"
  },
  {
    "text": "we say change it to use the config map name my node config in the cube system namespace okay so",
    "start": "1766240",
    "end": "1773039"
  },
  {
    "text": "that's set now let's look at the new node configuration that the kubelet has taken",
    "start": "1773039",
    "end": "1778840"
  },
  {
    "text": "on as you can see we've got the config source is the config map that we just described",
    "start": "1778840",
    "end": "1786240"
  },
  {
    "text": "so back on our same attack pod we can now use curl to hit the local nodes kublet api directly again",
    "start": "1791679",
    "end": "1797840"
  },
  {
    "text": "using that same mechanism",
    "start": "1797840",
    "end": "1801200"
  },
  {
    "text": "and this time when we hit the running pods api we're going to pipe it into jq to make it a little bit easier to see",
    "start": "1803440",
    "end": "1809520"
  },
  {
    "text": "and there we go we can see all the running containers pods and what name spaces they're in",
    "start": "1809520",
    "end": "1815039"
  },
  {
    "text": "on the cluster and what that lets us do",
    "start": "1815039",
    "end": "1823840"
  },
  {
    "text": "with those bits of information it lets us run a curl command",
    "start": "1824559",
    "end": "1831279"
  },
  {
    "text": "to list the root file system as an example command you can run anything against the kubelet api and pointing",
    "start": "1831279",
    "end": "1838000"
  },
  {
    "text": "that to the coupe proxy pod in the kube system namespace coproxy is",
    "start": "1838000",
    "end": "1843760"
  },
  {
    "text": "a useful one to pick because it always runs privileged and it's available to every node",
    "start": "1843760",
    "end": "1848880"
  },
  {
    "text": "so doing that we're pretty close to root access and as attackers that's pretty cool",
    "start": "1848880",
    "end": "1855679"
  },
  {
    "start": "1856000",
    "end": "1856000"
  },
  {
    "text": "so now with comfort circle what's old is new again the increasing complexity and security",
    "start": "1856399",
    "end": "1862559"
  },
  {
    "text": "of kubernetes and its ecosystem leads to demands from developers and administrators for more simplicity",
    "start": "1862559",
    "end": "1868720"
  },
  {
    "text": "and more ease of use this can lead to opening up new security holes or perhaps reopening old ones",
    "start": "1868720",
    "end": "1877039"
  },
  {
    "text": "as attackers we're really excited about the possibilities that we see in in the future and we",
    "start": "1877039",
    "end": "1883120"
  },
  {
    "text": "would like to encourage you people who are maybe developers or operators or defenders",
    "start": "1883120",
    "end": "1889760"
  },
  {
    "text": "to take a look at these upcoming features and the systems you build through our eyes",
    "start": "1889760",
    "end": "1896159"
  },
  {
    "text": "you know how you would use it but how might an attacker misuse it being able to look at your systems and",
    "start": "1896159",
    "end": "1902799"
  },
  {
    "text": "what you're building with the perspective of an attacker can help you understand and building what you're building with",
    "start": "1902799",
    "end": "1909200"
  },
  {
    "text": "this perspective in mind can help you protect against it and make your systems more secure overall",
    "start": "1909200",
    "end": "1915440"
  },
  {
    "text": "because as attackers we promise we're going to keep finding ways to misuse things",
    "start": "1915440",
    "end": "1920640"
  },
  {
    "text": "we're pretty excited about this we also really like challenges so we'd like to encourage you to secure",
    "start": "1920640",
    "end": "1926960"
  },
  {
    "text": "your systems against folks like us so that we have some challenges",
    "start": "1926960",
    "end": "1932720"
  },
  {
    "text": "here are some resources and further reading if you want to learn more about hacking and hardening kubernetes for yourself",
    "start": "1932880",
    "end": "1937919"
  },
  {
    "start": "1933000",
    "end": "1933000"
  },
  {
    "text": "and together you and i would like to say honk the planet thank you all very much for taking the",
    "start": "1937919",
    "end": "1943919"
  },
  {
    "text": "time with us we really appreciate it have a great coupon",
    "start": "1943919",
    "end": "1948880"
  }
]