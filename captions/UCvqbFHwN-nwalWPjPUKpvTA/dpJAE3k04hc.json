[
  {
    "text": "all right I am Dave's Allah to ski I am a back-end engineer on Spotify the infrastructure team in Stockholm",
    "start": "0",
    "end": "5970"
  },
  {
    "text": "I'm James Wan also an infrastructure engineer on the New York side of things right now I primarily work on the",
    "start": "5970",
    "end": "12809"
  },
  {
    "text": "kubernetes migration for Spotify and we're here to tell you guys about Spotify his introductory transition over",
    "start": "12809",
    "end": "19350"
  },
  {
    "text": "the past few years maybe all right so a",
    "start": "19350",
    "end": "28830"
  },
  {
    "text": "few years ago we were running all of our infrastructure all of our services all of our data jobs everything in our own",
    "start": "28830",
    "end": "34320"
  },
  {
    "text": "data centers around Europe in the US but a couple years ago we started looking",
    "start": "34320",
    "end": "41250"
  },
  {
    "text": "into moving out of those data centers and moving to being cloud hosted and then we're gonna talk about the last",
    "start": "41250",
    "end": "46379"
  },
  {
    "text": "step which is where we are now where we are almost entirely cloud hosted when really figuring out what the next steps",
    "start": "46379",
    "end": "52050"
  },
  {
    "text": "are how to be more cloud native how to take services that really weren't born in the cloud at all and make them operate and feel like they work so let's",
    "start": "52050",
    "end": "61680"
  },
  {
    "text": "start at the beginning let's start with what we had on our data centers and what our services looked like when we were running in data centers we had a whole",
    "start": "61680",
    "end": "69299"
  },
  {
    "text": "bunch of custom things but we also picked up a fair amount of interesting new technology we were a fairly early adopter of docker containers and we were",
    "start": "69299",
    "end": "75960"
  },
  {
    "text": "running a lot of our services our Java services in docker containers in like 2013 2014 we built our own container",
    "start": "75960",
    "end": "83820"
  },
  {
    "text": "expression engine because at that point that really weren't great and engines available elsewhere and we eventually",
    "start": "83820",
    "end": "89790"
  },
  {
    "text": "open-source to staying this link goes to our github page we open sourced it right on the time cover IDs as open source or",
    "start": "89790",
    "end": "95460"
  },
  {
    "text": "announced but obviously crazy things at the time so we couldn't use it we have",
    "start": "95460",
    "end": "101820"
  },
  {
    "text": "our own custom monitoring platform we have a time series database we call heroic which is open source it's based",
    "start": "101820",
    "end": "108270"
  },
  {
    "text": "on an open source time series database called open TS DB but has some differences that we needed and then we",
    "start": "108270",
    "end": "115110"
  },
  {
    "text": "have an internal we have an open source that dashboarding system we call iliyan that does our dash boarding our alerts and gets us paged and the internals of",
    "start": "115110",
    "end": "124710"
  },
  {
    "text": "all of this stuff is based on things like Cassandra we use elastic search for metadata and it's all kind of globally",
    "start": "124710",
    "end": "130140"
  },
  {
    "text": "federated through our data centers so you can get single dashboards for all of your services on top of that we have our own",
    "start": "130140",
    "end": "136890"
  },
  {
    "text": "proprietary message protocol this thing we call Hermes it's based on zero MQ it's mostly protobuf / 0 mq and the",
    "start": "136890",
    "end": "146190"
  },
  {
    "text": "biggest drivers for having our own protocol were that we needed message multiplexing over single TCP connections",
    "start": "146190",
    "end": "151770"
  },
  {
    "text": "and back when we were starting to do this around like 2006-2007 there really",
    "start": "151770",
    "end": "157470"
  },
  {
    "text": "wasn't anything available like that there was no G RPC there really wasn't an HTTP - and most of our services most",
    "start": "157470",
    "end": "163380"
  },
  {
    "text": "of the streaming things we needed to do we really couldn't do over things like HTTP 1 we have our own custom Java",
    "start": "163380",
    "end": "171959"
  },
  {
    "text": "framework this thing we call Apollo which is open-source but has modules some of which are open source on which",
    "start": "171959",
    "end": "177150"
  },
  {
    "text": "or not and you can think of this thing as something kind of like spring or Spring but-- but obviously on an",
    "start": "177150",
    "end": "182400"
  },
  {
    "text": "internal thing and not nearly as kind of full-featured and it has built-in clients for things like Hermes protocol",
    "start": "182400",
    "end": "189030"
  },
  {
    "text": "and for heroic metrics and all of our other kind of custom pieces it's really a way to bootstrap a service give all",
    "start": "189030",
    "end": "194940"
  },
  {
    "text": "the kind of initial boilerplate code that all of our services need most of",
    "start": "194940",
    "end": "201120"
  },
  {
    "text": "our storage was either in Postgres with my sequel for kind of very early Spotify",
    "start": "201120",
    "end": "206220"
  },
  {
    "text": "it was almost entirely Postgres then as we grew enough we started switching over to Cassandra and then really more",
    "start": "206220",
    "end": "212610"
  },
  {
    "text": "recently Cassandra has been our kind of one database for everything if we can and that means that we have an",
    "start": "212610",
    "end": "218370"
  },
  {
    "text": "incredible number of Cassandra clusters that cover almost every use case we have huge ones we have like three node ones",
    "start": "218370",
    "end": "224970"
  },
  {
    "text": "with a few megabytes of data we have ones that get incredible amounts of read load or write load once they get request",
    "start": "224970",
    "end": "232290"
  },
  {
    "text": "add a kind of any possible use case we probably have a Cassandra database somewhere in our deployment that youth",
    "start": "232290",
    "end": "239220"
  },
  {
    "text": "has that rough use and of course that means that we have a lot of experience",
    "start": "239220",
    "end": "244739"
  },
  {
    "text": "throughout the entire company on kind of operate needs Cassandra clusters and we have a team in our infrastructure team that is kind of really Cassandra",
    "start": "244739",
    "end": "252720"
  },
  {
    "text": "specialists for all of these different use cases the other really big thing we",
    "start": "252720",
    "end": "258000"
  },
  {
    "text": "used a lot of is Hadoop we use so much a dip in fact that we had the largest single tenant to do cluster in Europe",
    "start": "258000",
    "end": "264020"
  },
  {
    "text": "this thing ran I think it was just over 20,000 jobs a",
    "start": "264020",
    "end": "269590"
  },
  {
    "text": "day when we were kind of at his peak it had around 3,000 nodes it ran kind of",
    "start": "269590",
    "end": "278020"
  },
  {
    "text": "hundreds of different types of events it got hundreds of thousands of events per second in some specific streams and many",
    "start": "278020",
    "end": "285129"
  },
  {
    "text": "many more than that when you combine all the streams across all the clients and many of the data pipelines that ran of",
    "start": "285129",
    "end": "290800"
  },
  {
    "text": "those 20,000 jobs were kind of complex chained pipelines so it would be some long Hadoop job that then fed into three",
    "start": "290800",
    "end": "296590"
  },
  {
    "text": "or four different jobs and each one of those might offender to three or four different jobs and so on so we had to like just drawing out the full kind of",
    "start": "296590",
    "end": "303780"
  },
  {
    "text": "set of all of our pipelines from start to finish itself would take more than",
    "start": "303780",
    "end": "308919"
  },
  {
    "text": "most of this bori probably and we ended up having to build some of our own tools some which we open sourced like this thing called Luigi that does the",
    "start": "308919",
    "end": "315669"
  },
  {
    "text": "orchestration and ordering of all these jobs just because there were so many and they depend on each other in so many different ways and then kind of tying",
    "start": "315669",
    "end": "323710"
  },
  {
    "text": "all of this together in a similar way to the way that Apollo was our framework for allowing services to kind of",
    "start": "323710",
    "end": "329770"
  },
  {
    "text": "bootstrap and get going with all of the back end things tied together we have these this concept of a golden path which is something like Netflix is paved",
    "start": "329770",
    "end": "336250"
  },
  {
    "text": "road if that's what you've heard of more it's kind of a documented way to tell a",
    "start": "336250",
    "end": "341259"
  },
  {
    "text": "developer within our team if you want to create a service follow these steps and you'll have a blessed kind of official",
    "start": "341259",
    "end": "347139"
  },
  {
    "text": "Spotify way of creating a service and we start having a few of these we had one for kind of how you could a data pipeline how you create it back in",
    "start": "347139",
    "end": "352690"
  },
  {
    "text": "service and so on how you create a mobile feature etc and the a lot of",
    "start": "352690",
    "end": "359379"
  },
  {
    "text": "these things come out of our tech platform or so we have now we have close",
    "start": "359379",
    "end": "365139"
  },
  {
    "text": "to 4,000 employees and we have a 500 person or so type platform org",
    "start": "365139",
    "end": "370690"
  },
  {
    "text": "of which 2 to 300 is just infrastructure and operations and data infrastructure so people that run all of these things",
    "start": "370690",
    "end": "377500"
  },
  {
    "text": "and just ensure that all of our back-end services are using kind of reasonable new technology we're doing things like moving over to cloud one plan go it now",
    "start": "377500",
    "end": "383650"
  },
  {
    "text": "and then like the data in front of team is things like maintaining all the tools that we had for the Hadoop cluster and transitioning all of that to cloud so we",
    "start": "383650",
    "end": "394270"
  },
  {
    "text": "had all of these things in our data centers we had all these proprietary tools and eventually we realized that",
    "start": "394270",
    "end": "400899"
  },
  {
    "text": "kind of way too much work to maintain all this stuff there's so many things that just are not our core business we are at the",
    "start": "400899",
    "end": "407080"
  },
  {
    "text": "end of the day a music streaming service and even though we're a fairly large company we're not big enough to kind of",
    "start": "407080",
    "end": "412989"
  },
  {
    "text": "have enough people to really focus on running data centers and running all these things so mostly for the sake of focus but also for all the other",
    "start": "412989",
    "end": "420189"
  },
  {
    "text": "benefits everyone talks about the elasticity the managed services all these other great features we knew we",
    "start": "420189",
    "end": "425889"
  },
  {
    "text": "wanted to move to cloud but at that point we really had no idea kind of where to start how to do it what to do",
    "start": "425889",
    "end": "432129"
  },
  {
    "text": "we just knew that we wanted to do it so we decided that the right thing to do",
    "start": "432129",
    "end": "437289"
  },
  {
    "text": "would be to figure out kind of stages to figure out milestones to do it like any other software project and our first",
    "start": "437289",
    "end": "443829"
  },
  {
    "text": "really big goal was just move out of our data centers like set the goal as stop paying rent kind of stop having data",
    "start": "443829",
    "end": "450189"
  },
  {
    "text": "centers and then we'll figure out the rest later and that's exactly what we",
    "start": "450189",
    "end": "455349"
  },
  {
    "text": "did so our platform teams infrastructure and operations teams the datum structure",
    "start": "455349",
    "end": "460779"
  },
  {
    "text": "all these teams then kind of got to work on figuring out what that even meant for everyone else because everyone else in the company is",
    "start": "460779",
    "end": "466029"
  },
  {
    "text": "building features of building their services they don't want to have to think about what is under what is running their services ideally that's",
    "start": "466029",
    "end": "471639"
  },
  {
    "text": "our problem so we went through and figured out the network connectivity between our data centers and our cloud",
    "start": "471639",
    "end": "477249"
  },
  {
    "text": "providers data centers we figured out how to kind of extract away machine provisioning so that the tools that all",
    "start": "477249",
    "end": "483189"
  },
  {
    "text": "of our developers used to use the provision machines now could provision VMs in a cloud data center things like",
    "start": "483189",
    "end": "489939"
  },
  {
    "text": "deployment same thing kind of they could just say I'm deploying the service and it ends up in the cloud instead of in",
    "start": "489939",
    "end": "495550"
  },
  {
    "text": "one of our data centers but it feels the same to them so we're not teaching them all new things service discovery of",
    "start": "495550",
    "end": "501759"
  },
  {
    "text": "course also had to work across cloud all of these things kind of came out of the platform we could before we did anything",
    "start": "501759",
    "end": "506939"
  },
  {
    "text": "after that we really had to go on kind of a tour across the company to get every service moved and get all of that",
    "start": "506939",
    "end": "513698"
  },
  {
    "text": "working so the platform team at that point kind of went around the company working with services the next big thing",
    "start": "513699",
    "end": "520959"
  },
  {
    "text": "that giant Hadoop cluster and this really was the harder part because with",
    "start": "520959",
    "end": "527170"
  },
  {
    "text": "services it was a lot easier to just say move you're like you have bare-metal in furniture here you have a VM there kind",
    "start": "527170",
    "end": "534339"
  },
  {
    "text": "of looks and feels it's the Linux it's still running for container still using the same container orchestration system we can fake it with",
    "start": "534339",
    "end": "540460"
  },
  {
    "text": "data jobs it was a lot harder because we really didn't want to just spin up 3000",
    "start": "540460",
    "end": "545920"
  },
  {
    "text": "vm's that looked like our Hadoop cluster and create a the world's largest single tenant Hadoop cluster in someone else's",
    "start": "545920",
    "end": "551050"
  },
  {
    "text": "data center but because we have this these really complicated data pipelines the initial thought is kind of we have",
    "start": "551050",
    "end": "557290"
  },
  {
    "text": "to start either at the beginning of all of these pipelines or at the very end and then move the other way and that was",
    "start": "557290",
    "end": "563860"
  },
  {
    "text": "really hard to because the jobs of the beginning or the jobs at the end may be owned by one of many different teams they look and feel very differently it",
    "start": "563860",
    "end": "569950"
  },
  {
    "text": "would have been very difficult to kind of coordinate everyone on either end of this kind of complicated data pipeline",
    "start": "569950",
    "end": "575590"
  },
  {
    "text": "to really start and flow across and then like one team has to move their first",
    "start": "575590",
    "end": "581650"
  },
  {
    "text": "job and then they might own the fourth job so we come back to them two weeks later it just wasn't going to work so we",
    "start": "581650",
    "end": "587650"
  },
  {
    "text": "kind of came back with in the data team thought about what else we need and created a bunch of tools to make this",
    "start": "587650",
    "end": "593020"
  },
  {
    "text": "easier obviously we needed better network connections between the data centers we needed to figure out kind of",
    "start": "593020",
    "end": "600550"
  },
  {
    "text": "how we were going to run jobs we figured out all of the kind of core pieces and then one of the people on our team had",
    "start": "600550",
    "end": "606010"
  },
  {
    "text": "this idea to make it so that we could just move any given job in the middle and the way that we were going to do",
    "start": "606010",
    "end": "612580"
  },
  {
    "text": "that was we created this tool called spider Oh which and we did open source it and it effectively lets you create",
    "start": "612580",
    "end": "619750"
  },
  {
    "text": "kind of ephemeral Hadoop clusters in any cloud provider that lets you have Hadoop",
    "start": "619750",
    "end": "625120"
  },
  {
    "text": "clusters and tear them down but on top of that it lets you pick with a command line flag if you want to schedule to the on-prem one or in the cloud so at that",
    "start": "625120",
    "end": "632860"
  },
  {
    "text": "point we did have to have the feature teams change some things but all they really have to change was add a flag that said like we've tested this and we",
    "start": "632860",
    "end": "639190"
  },
  {
    "text": "think it works in clouds so now like spider kick-off job - - cloud as opposed to just leaving it by default the other",
    "start": "639190",
    "end": "646900"
  },
  {
    "text": "thing we needed of course now if we had random jobs in this giant pipeline going in one place and random jobs and the other was a way to have them reach their",
    "start": "646900",
    "end": "653320"
  },
  {
    "text": "data so we could have created a modified version of Hadoop distributed copy that",
    "start": "653320",
    "end": "659320"
  },
  {
    "text": "just copied from our HDFS into blob storage in the cloud and back so by",
    "start": "659320",
    "end": "666550"
  },
  {
    "text": "combining these two tools we really let any pipeline any in the middle now move and then if the",
    "start": "666550",
    "end": "672400"
  },
  {
    "text": "pipeline consuming it was in the other place then it would just run disk copy at the beginning to copy the data over",
    "start": "672400",
    "end": "678010"
  },
  {
    "text": "to itself and the same thing afterwards if you move if your job moves and the job after you didn't then then you would",
    "start": "678010",
    "end": "683860"
  },
  {
    "text": "just have to copy their back so the job after you can run what can run normally and then when that job moves you no",
    "start": "683860",
    "end": "689170"
  },
  {
    "text": "longer do the disk copy you just run next to it and this was really the biggest thing to let us start moving our",
    "start": "689170",
    "end": "694750"
  },
  {
    "text": "data jobs at this point we now could just go to individual teams and say hey we want to work with you but let's move all of your jobs and it didn't matter",
    "start": "694750",
    "end": "700990"
  },
  {
    "text": "where they were let's work with you guys we'll move all of your jobs the biggest problem with this of course was we're constantly running distributed copies we",
    "start": "700990",
    "end": "707190"
  },
  {
    "text": "maxed out the pipe we had between our data center and our cloud providers constantly and kept growing it all the",
    "start": "707190",
    "end": "712840"
  },
  {
    "text": "time through this but once it was actually done it was amazing and we had moved twenty thousand jobs a day and",
    "start": "712840",
    "end": "719470"
  },
  {
    "text": "this enormous new cluster so despite all the pain it was absolutely worth it so",
    "start": "719470",
    "end": "725020"
  },
  {
    "text": "now we're effectively moved out there's kind of there's always a long tail of",
    "start": "725020",
    "end": "730570"
  },
  {
    "text": "things but the intense majority of the things we've done have moved out this",
    "start": "730570",
    "end": "736240"
  },
  {
    "text": "was overall probably what a three and a half year effort from deciding me to do it to being where we are now but the",
    "start": "736240",
    "end": "742840"
  },
  {
    "text": "biggest push was 2017 the at the beginning of the year we really had a lot of the infrastructure for moving some of the network connections the",
    "start": "742840",
    "end": "749320"
  },
  {
    "text": "ideas for how we were gonna do it some of these open source tools and the internal tool that just talked about but very little was actually moved and over",
    "start": "749320",
    "end": "755650"
  },
  {
    "text": "the course of 2017 we basically had both the infrastructure team and the data",
    "start": "755650",
    "end": "761680"
  },
  {
    "text": "team in our platform move across the company and get everyone over and through this we built a lot of amazing",
    "start": "761680",
    "end": "767620"
  },
  {
    "text": "relationships both with varying communities that we worked with on some of these tools and of course our cloud",
    "start": "767620",
    "end": "772810"
  },
  {
    "text": "provider and some of the other providers and people we talked to just to give us ideas on how to do this so we're done",
    "start": "772810",
    "end": "778870"
  },
  {
    "text": "right of course at this point we're not we're now running a replica of our",
    "start": "778870",
    "end": "785320"
  },
  {
    "text": "on-prem data centers in someone else's data centers so sure we don't support the hardware but every one of those",
    "start": "785320",
    "end": "790390"
  },
  {
    "text": "proprietary things I just told you about we still have and we're not getting great relation with hardware we're not getting great use of all the managed",
    "start": "790390",
    "end": "796660"
  },
  {
    "text": "services and we're just not still not completely focusing on streaming music we still have a whole lot of stuff to do",
    "start": "796660",
    "end": "802899"
  },
  {
    "text": "so now we need to figure out what it means to be kind of to be cloud native to feel like we're born in the cloud but",
    "start": "802899",
    "end": "808689"
  },
  {
    "text": "we didn't really even know what that meant we've been running in data centers forever like sure there's auto-scaling and there's open source communities",
    "start": "808689",
    "end": "814839"
  },
  {
    "text": "there's managed services but what do we do so we figured we'd start somewhere relatively obvious and just start",
    "start": "814839",
    "end": "820629"
  },
  {
    "text": "experimenting with things like kubernetes and G RPC and figuring out how to do the kind of the simple the right sizing the Auto scale and all this",
    "start": "820629",
    "end": "826660"
  },
  {
    "text": "stuff I'll let James tell you about the committee side of that okay so I'm gonna",
    "start": "826660",
    "end": "832600"
  },
  {
    "text": "talk more very explicitly about how we're migrating to communities like what we've been doing what we're doing and",
    "start": "832600",
    "end": "839350"
  },
  {
    "text": "what we're going to do so our goal here is to run all of our stateless back-end services on kubernetes and the approach",
    "start": "839350",
    "end": "846309"
  },
  {
    "text": "we took was very like iterative one with procedural stages in each stage is very succinct goal so for our very first",
    "start": "846309",
    "end": "853480"
  },
  {
    "text": "stage was kind of an experimental one and we sought to just run one service on a single cluster and have that service",
    "start": "853480",
    "end": "859990"
  },
  {
    "text": "received production traffic for just one hour and so despite that being a very",
    "start": "859990",
    "end": "865329"
  },
  {
    "text": "like a small goal I drove out a lot of things for us for example we actually",
    "start": "865329",
    "end": "870730"
  },
  {
    "text": "needed to create cluster at this point it's manual process since it's a single cluster we needed to set up DNS",
    "start": "870730",
    "end": "877929"
  },
  {
    "text": "networking route ability for example we needed to set up the correct upstream name resolvers IP msq agents so that the",
    "start": "877929",
    "end": "885699"
  },
  {
    "text": "infrastructure stuff for clusters and the service could resolve the correct host names for our internal Spotify",
    "start": "885699",
    "end": "892540"
  },
  {
    "text": "stuff we needed to set up integration where there is a staying service discovery which is an in-house system",
    "start": "892540",
    "end": "898569"
  },
  {
    "text": "called nameless so that other back-end hosts and services would know to talk to this service running on kubernetes we",
    "start": "898569",
    "end": "905350"
  },
  {
    "text": "needed to set up integration with our existing metrics metric system and logging so that the we knew that the",
    "start": "905350",
    "end": "913509"
  },
  {
    "text": "service was operating correctly it was receiving traffic and was responding to requests properly so that all worked as",
    "start": "913509",
    "end": "921610"
  },
  {
    "text": "expected and then for our next experimental phase we sought to run three services on a shared cluster so",
    "start": "921610",
    "end": "927610"
  },
  {
    "text": "now we introduced the element of multi-tenancy I'll be on like a very small scale and so with multi-tenancy",
    "start": "927610",
    "end": "934389"
  },
  {
    "text": "now we need to think about namespaces permissioning and resource quotas for",
    "start": "934389",
    "end": "939410"
  },
  {
    "text": "those namespaces to prevent those three different teams with their different services from stepping on each other's toes both from an our back and",
    "start": "939410",
    "end": "946610"
  },
  {
    "text": "permissions perspective but also from a resource utilization perspective and we",
    "start": "946610",
    "end": "952550"
  },
  {
    "text": "also start to create more fleshed-out developer documentation because up to",
    "start": "952550",
    "end": "957980"
  },
  {
    "text": "this point we were dealing with a single team it was easy enough to jump in there slack channel or schedule a Google",
    "start": "957980",
    "end": "963199"
  },
  {
    "text": "hangout meeting and talk to them directly but now we don't want to have to repeat the same exact message two or",
    "start": "963199",
    "end": "969079"
  },
  {
    "text": "three different teams and later multiple teams so now we start to write developer documentation so after that we've ended",
    "start": "969079",
    "end": "977389"
  },
  {
    "text": "the experimental of phases because at this point we've gained enough",
    "start": "977389",
    "end": "982639"
  },
  {
    "text": "confidence to say that we can probably run most of our services eventually on communities it's just figuring out kind",
    "start": "982639",
    "end": "989660"
  },
  {
    "text": "of how of things and all the unknowns there and so now we enter into the now",
    "start": "989660",
    "end": "997279"
  },
  {
    "text": "we sending out a survey via an email to",
    "start": "997279",
    "end": "1013540"
  },
  {
    "text": "all of our D that asked interested teams to volunteer their services to run on",
    "start": "1013540",
    "end": "1018639"
  },
  {
    "text": "kubernetes and there were two intentions here the first intention was to elicit",
    "start": "1018639",
    "end": "1024699"
  },
  {
    "text": "volunteers but the second was also to let all of our D and our internal stakeholders know of our intentions for",
    "start": "1024699",
    "end": "1031298"
  },
  {
    "text": "communities and where our runtime platform was headed and so most of the",
    "start": "1031299",
    "end": "1036938"
  },
  {
    "text": "services volunteered for this variety of like minor tweaks like co-located",
    "start": "1036939",
    "end": "1045100"
  },
  {
    "text": "memcache is requiring GCSB service credentials to operate minor things like that but for the most part their golden",
    "start": "1045100",
    "end": "1051760"
  },
  {
    "text": "path services so we're pretty familiar with them and we know how to deal with them and so for this phase it drove out",
    "start": "1051760",
    "end": "1059950"
  },
  {
    "text": "the need for test clusters because previously we were running services for say an hour or a few days but now we're",
    "start": "1059950",
    "end": "1066970"
  },
  {
    "text": "running them indefinitely so we want to isolate the clusters that we experiment with",
    "start": "1066970",
    "end": "1072940"
  },
  {
    "text": "resource manifests on away from the production clusters so we have no with",
    "start": "1072940",
    "end": "1079450"
  },
  {
    "text": "the test clusters we now have multiple clusters that we want to have mostly the",
    "start": "1079450",
    "end": "1084490"
  },
  {
    "text": "same configuration on so we script out the creation of those clusters we start to automate that process and then the we",
    "start": "1084490",
    "end": "1093150"
  },
  {
    "text": "integrate with the existing a secret system so we have an in-house system called a CLO and a lot of services rely",
    "start": "1093150",
    "end": "1101350"
  },
  {
    "text": "on this system so we basically write integration that automatically puts their Co secrets as secret resources in",
    "start": "1101350",
    "end": "1108670"
  },
  {
    "text": "their namespaces so they don't have to worry about that kind of stuff note that for this it's kind of a stopgap solution",
    "start": "1108670",
    "end": "1115990"
  },
  {
    "text": "in the future we want to move to something more like vault or something more robust but for this it's not really",
    "start": "1115990",
    "end": "1124330"
  },
  {
    "text": "necessary we're only dealing with a small amount of services and the next",
    "start": "1124330",
    "end": "1129490"
  },
  {
    "text": "thing was initial deployment so for this we wrote a pretty small wrapper around cube CTL that and manually invoke in",
    "start": "1129490",
    "end": "1141340"
  },
  {
    "text": "their pipelines just like a one or two line thing that would deploy to the clusters that the production clusters",
    "start": "1141340",
    "end": "1146560"
  },
  {
    "text": "that we have set up for them and right now this is like good enough of a solution because we're dealing with say",
    "start": "1146560",
    "end": "1152350"
  },
  {
    "text": "less than 20 teams less than 20 Jenkins instances and in this phase we got a lot",
    "start": "1152350",
    "end": "1158590"
  },
  {
    "text": "of learning from learnings from running these uh this variety of services I'm not going to I'm not going to go into a",
    "start": "1158590",
    "end": "1164560"
  },
  {
    "text": "fold the technical details but for example we found weird MTU networking",
    "start": "1164560",
    "end": "1169780"
  },
  {
    "text": "issues we found some unexpected behavior with readiness probes we found this",
    "start": "1169780",
    "end": "1175390"
  },
  {
    "text": "control arm n max issues with the like packet buffer size on nodes that kind of",
    "start": "1175390",
    "end": "1180490"
  },
  {
    "text": "bit us I've seen some people nodding actually so looks like we're not alone there we also ran into the limitation",
    "start": "1180490",
    "end": "1188560"
  },
  {
    "text": "with cube DNS where it didn't support reverse DNS lookups for upstream name resolvers but props to the cube DNS team",
    "start": "1188560",
    "end": "1195760"
  },
  {
    "text": "because just two weeks after we created that issue they app they added that feature and then",
    "start": "1195760",
    "end": "1201100"
  },
  {
    "text": "close that the issue so we were really impressed with the really quick turnaround they're an important part of",
    "start": "1201100",
    "end": "1207640"
  },
  {
    "text": "this kind of alpha phase was that we really wanted to run to high traffic",
    "start": "1207640",
    "end": "1212710"
  },
  {
    "text": "services on communities and have them the two we chose were fan-out",
    "start": "1212710",
    "end": "1219940"
  },
  {
    "text": "and mediator poxy fan-out you can think of as a an internal pub/sub system meditator proxy think of as the gateway",
    "start": "1219940",
    "end": "1226960"
  },
  {
    "text": "to all of the metadata as part of Phi so all that mission-critical song and artist info in terms of load fan-out",
    "start": "1226960",
    "end": "1234460"
  },
  {
    "text": "gets about 1.5 million requests per second and that works out to about 15k requests per second for instance and",
    "start": "1234460",
    "end": "1241360"
  },
  {
    "text": "permitted at epoxy it receives about 500k requests per second and 500",
    "start": "1241360",
    "end": "1246400"
  },
  {
    "text": "requests per second for instance services and so what this drove out for",
    "start": "1246400",
    "end": "1252100"
  },
  {
    "text": "us was we started to use an experiment with auto scaling horizontal auto scaling at this point it really forced",
    "start": "1252100",
    "end": "1258159"
  },
  {
    "text": "us to understand the need of Grady of our network setup because of the low latency requirement of these services",
    "start": "1258159",
    "end": "1264159"
  },
  {
    "text": "and most importantly is served as a great reference for other teams who were migrating we had direct feedback from",
    "start": "1264159",
    "end": "1270460"
  },
  {
    "text": "some teams during migration that they were initially worried about their services how they were going to run on",
    "start": "1270460",
    "end": "1276490"
  },
  {
    "text": "kubernetes they thought they had but we would point them to the example of these",
    "start": "1276490",
    "end": "1281740"
  },
  {
    "text": "two services that were able to run on the communities clusters that we set up and services were for the most part like",
    "start": "1281740",
    "end": "1291580"
  },
  {
    "text": "less complex and had lower performance requirements than these services thank",
    "start": "1291580",
    "end": "1297880"
  },
  {
    "text": "you so the next phase after the alpha phase that we recently wrapped up is the",
    "start": "1297880",
    "end": "1305110"
  },
  {
    "text": "beta phase what we consider the beta phase and in this we have self service migration and so in here teams that are",
    "start": "1305110",
    "end": "1312280"
  },
  {
    "text": "interested can follow infrastructure provided Docs guidelines tooling to self service",
    "start": "1312280",
    "end": "1318299"
  },
  {
    "text": "migrate either partially or fully their capacity from the old Helios runtime to",
    "start": "1318299",
    "end": "1325510"
  },
  {
    "text": "this new kubernetes runtime that we're providing and so the two elements that we introduced here are",
    "start": "1325510",
    "end": "1331580"
  },
  {
    "text": "as operators now we have stuff running in our clusters that we're not familiar with so for the previous stages when we",
    "start": "1331580",
    "end": "1338900"
  },
  {
    "text": "work with teams we would do most of the legwork involved in getting working resource manifests creating PRS and",
    "start": "1338900",
    "end": "1345770"
  },
  {
    "text": "really hand-holding them along the way but now teams could theoretically migrate their services to kubernetes",
    "start": "1345770",
    "end": "1352340"
  },
  {
    "text": "without ever actually directly conversing with us as the infrastructure team which is actually you know the",
    "start": "1352340",
    "end": "1358130"
  },
  {
    "text": "intention there and the other part is that we would support full migration so whereas before with partial migration",
    "start": "1358130",
    "end": "1364880"
  },
  {
    "text": "they're both running on Helios and criminales in the case of an incident",
    "start": "1364880",
    "end": "1369980"
  },
  {
    "text": "it's what we recommend for them is to just destroy their kubernetes capacity",
    "start": "1369980",
    "end": "1375320"
  },
  {
    "text": "and fall back on their already long standing helios capacity that they know how to deal with that they have a lot of",
    "start": "1375320",
    "end": "1380990"
  },
  {
    "text": "experience with but with full migration this no longer becomes as easy and we need to introduce a lot of reliability",
    "start": "1380990",
    "end": "1387680"
  },
  {
    "text": "on the part of our clusters so we start to look at reliability we start to set up on-call for clusters and the",
    "start": "1387680",
    "end": "1394160"
  },
  {
    "text": "associated alerts in playbooks we do like dirt testing for disaster recovery",
    "start": "1394160",
    "end": "1399980"
  },
  {
    "text": "and backups on the service side we start to think about how do we let developers",
    "start": "1399980",
    "end": "1406490"
  },
  {
    "text": "create their own namespaces but have those names to be set up with the proper are back permissions and saying resource",
    "start": "1406490",
    "end": "1413990"
  },
  {
    "text": "quotas so that they don't have to really worry about these kinds of things on the deployment side we start to look past",
    "start": "1413990",
    "end": "1422170"
  },
  {
    "text": "manual command vacations in Jenkins pipelines and look at a more robust",
    "start": "1422170",
    "end": "1427490"
  },
  {
    "text": "deployment orchestration layer we chose spinnaker for this and assigned note",
    "start": "1427490",
    "end": "1433670"
  },
  {
    "text": "that we're kind of on the leading edge of spinnaker and that we're using the v2 the new v2 communities provider so if",
    "start": "1433670",
    "end": "1440780"
  },
  {
    "text": "you're using this and you have any thoughts or want to compare notes were highly interested in in that we needed",
    "start": "1440780",
    "end": "1447320"
  },
  {
    "text": "to make a decision on how to store manifests in terms of version control what we decided on for the time being is",
    "start": "1447320",
    "end": "1453740"
  },
  {
    "text": "to store all infrastructure related stuff in a mono repo and have developers",
    "start": "1453740",
    "end": "1459220"
  },
  {
    "text": "resource manifests and co-located with their application repos but we designed",
    "start": "1459220",
    "end": "1464480"
  },
  {
    "text": "everything in a way that we're not fixed to this and that we can migrate to other paradigms if needed",
    "start": "1464480",
    "end": "1469490"
  },
  {
    "text": "later on and so the phase after that is",
    "start": "1469490",
    "end": "1474710"
  },
  {
    "text": "the golden path phase so this would be GA general availability and this would",
    "start": "1474710",
    "end": "1479870"
  },
  {
    "text": "be where any new service at Spotify would be deployed to only kubernetes and",
    "start": "1479870",
    "end": "1484970"
  },
  {
    "text": "have be able to leverage all the native kubernetes benefits like auto scaling I",
    "start": "1484970",
    "end": "1491270"
  },
  {
    "text": "via the vertical pod autoscaler so that would take away some of the capacity",
    "start": "1491270",
    "end": "1496820"
  },
  {
    "text": "planning concerns from developers horizontal node auto scaling would take away some of our operator capacity",
    "start": "1496820",
    "end": "1503299"
  },
  {
    "text": "planning concerns the one-click migration is kind of the Golden Goose here and that we would like to provide",
    "start": "1503299",
    "end": "1509900"
  },
  {
    "text": "either a button in our like UI console for everything called System Z or a simple script or a CLI tool that would",
    "start": "1509900",
    "end": "1517309"
  },
  {
    "text": "be able to convert developers running services on Helios to kubernetes for",
    "start": "1517309",
    "end": "1523549"
  },
  {
    "text": "them so they don't have to follow some like you know long series of Doc's or guidelines ideally the everything else",
    "start": "1523549",
    "end": "1531799"
  },
  {
    "text": "falls into this category so following like the 80/20 paradigm the this would entail us investigating the 20% of kind",
    "start": "1531799",
    "end": "1539750"
  },
  {
    "text": "of tail end services that have additional requirements for example IT GC services like IT general control",
    "start": "1539750",
    "end": "1546590"
  },
  {
    "text": "services at about if I have additional security and auditing requirements for example all SSH activity currently needs",
    "start": "1546590",
    "end": "1553700"
  },
  {
    "text": "to be tracked monitored and audited so increment Eddy's we need to figure out what does this mean do we track like",
    "start": "1553700",
    "end": "1559700"
  },
  {
    "text": "cube CTO exit do we just not allow them to do that these are things that would need to go into GA the we would also do",
    "start": "1559700",
    "end": "1568490"
  },
  {
    "text": "migration tracking as part of this phase so we would actively track via metrics how many services are still running on",
    "start": "1568490",
    "end": "1575780"
  },
  {
    "text": "Helios how many are migrated to kubernetes the rate of change from one to the other and hope to boost that",
    "start": "1575780",
    "end": "1581900"
  },
  {
    "text": "through things like the migration road team so this is a strategy we use during our GCP migration with a lot of success",
    "start": "1581900",
    "end": "1588890"
  },
  {
    "text": "and this was a temporary team that was spun up that visited office all the",
    "start": "1588890",
    "end": "1595340"
  },
  {
    "text": "offices that had developers in them and these this temporary team held lunch and learns like GCP trainings and sat down",
    "start": "1595340",
    "end": "1602150"
  },
  {
    "text": "with developers to actually help them migrate their services to GCP and then at the when we were all migrated to DCP",
    "start": "1602150",
    "end": "1609320"
  },
  {
    "text": "this team spun down so we hope to do something similar for that for kubernetes and so the takeaways and like",
    "start": "1609320",
    "end": "1619460"
  },
  {
    "text": "lessons we had from the migration process we've done so far is that scoping the scoping this into stages",
    "start": "1619460",
    "end": "1626840"
  },
  {
    "text": "really helped us avoid like a very monolithic like migration movement when",
    "start": "1626840",
    "end": "1632030"
  },
  {
    "text": "we were initially thinking about things at the very beginning of migration especially with our long-standing legacy",
    "start": "1632030",
    "end": "1637820"
  },
  {
    "text": "stack we were really thinking oh my god there's so much stuff that we need to integrate and adjust fix or like there's",
    "start": "1637820",
    "end": "1646640"
  },
  {
    "text": "just so much stuff especially because we've been around this legacy instructure has been around since like 2005 2006 and whatnot it's a lot to deal",
    "start": "1646640",
    "end": "1653840"
  },
  {
    "text": "with but designing each stage around the type of service there's like scope of",
    "start": "1653840",
    "end": "1659059"
  },
  {
    "text": "the service that you want to migrate really helped us in terms of discovering all these like hidden unknowns that we",
    "start": "1659059",
    "end": "1665419"
  },
  {
    "text": "knew were out there at a very controllable pace and so the when we",
    "start": "1665419",
    "end": "1671480"
  },
  {
    "text": "discovered these unknowns what we would do is basically consider does this need to be part of this migration phase for",
    "start": "1671480",
    "end": "1677840"
  },
  {
    "text": "example like secret secrets integration at the experimental stage when we were",
    "start": "1677840",
    "end": "1683510"
  },
  {
    "text": "running three services like do we need to solve that no we don't let's put it on a backlog and remember to address",
    "start": "1683510",
    "end": "1690770"
  },
  {
    "text": "that in a later stage the one that's relevant and so the caveat to that is to",
    "start": "1690770",
    "end": "1696580"
  },
  {
    "text": "when in doubt leverage time box investigations to reduce risk what we",
    "start": "1696580",
    "end": "1702890"
  },
  {
    "text": "mean by that is that when we were throwing these things on the backlog very often some team members would have",
    "start": "1702890",
    "end": "1708440"
  },
  {
    "text": "fears that when we later need to implement this thing we're going to have",
    "start": "1708440",
    "end": "1713570"
  },
  {
    "text": "to rewrite a lot of the existing stuff that we just did for this stage this current staging that we kind of did and",
    "start": "1713570",
    "end": "1719600"
  },
  {
    "text": "maybe a hacky or quick way so to alleviate those fears we would have a",
    "start": "1719600",
    "end": "1725149"
  },
  {
    "text": "team member basically take a time box investigation and she alleviating that fear so for example in",
    "start": "1725149",
    "end": "1732240"
  },
  {
    "text": "our spinnaker integration work for the beta phase we were seeking to do a simple deployment pipeline and Gi phase",
    "start": "1732240",
    "end": "1739260"
  },
  {
    "text": "we're looking to support Canaries Bluegreen deployments more complex stuff and so we were worried that the",
    "start": "1739260",
    "end": "1746630"
  },
  {
    "text": "integrations we were doing with spinnaker for beta we would have to rewrite for the GA phase so a team",
    "start": "1746630",
    "end": "1752550"
  },
  {
    "text": "member took a day really dug into spinnaker pipeline design they manually created a canary and blue green",
    "start": "1752550",
    "end": "1759450"
  },
  {
    "text": "appointment pipeline and they vetted that okay what we're currently doing with spinnaker well that was to easily",
    "start": "1759450",
    "end": "1765510"
  },
  {
    "text": "support complex strategies later so we greatly emitted a mitigated that risk there and so that's our equipment",
    "start": "1765510",
    "end": "1773910"
  },
  {
    "text": "IDs migration so far that's the runtime side Dave is going to talk about the protocol side of things so there's still",
    "start": "1773910",
    "end": "1782370"
  },
  {
    "text": "a few more of those things I talked about earlier that are are custom things that we'd like to get rid of at the moment at the top of the list is hermie",
    "start": "1782370",
    "end": "1787890"
  },
  {
    "text": "that's our protocol having a provider per protocol is really not a lot of fun",
    "start": "1787890",
    "end": "1792950"
  },
  {
    "text": "none of the great l7 features we can get from cloud features like cloud load balancers work we can use a service mesh",
    "start": "1792950",
    "end": "1800550"
  },
  {
    "text": "but we don't get any of the cool l7 any of the protocol inspection features of that we obviously don't get any free",
    "start": "1800550",
    "end": "1806940"
  },
  {
    "text": "features from someone else doing work on the project it's harder to onboard new people it's harder to onboard new",
    "start": "1806940",
    "end": "1812280"
  },
  {
    "text": "languages it's harder to get new P to get kind of debugging deep into the",
    "start": "1812280",
    "end": "1818180"
  },
  {
    "text": "protocol if you see some messages need to figure out what's going on you have to kind of dig through the code as well just because there really aren't great",
    "start": "1818180",
    "end": "1824160"
  },
  {
    "text": "tools aside from the ones we built when we had some issues so we needed something so we really want to switch protocols but on the other hand it's",
    "start": "1824160",
    "end": "1830820"
  },
  {
    "text": "really hard when you have say a thousand production micro services that are all communicating instant protocol to just",
    "start": "1830820",
    "end": "1836010"
  },
  {
    "text": "flip a switch and make them all speak a different protocol so the similar to the",
    "start": "1836010",
    "end": "1841730"
  },
  {
    "text": "to the kubernetes migration we're trying to figure out how to stage this and some teams have just said we're gonna use G",
    "start": "1841730",
    "end": "1848040"
  },
  {
    "text": "RPC because it's just better and even if the interactive team isn't providing it we don't care we want G RPC we're",
    "start": "1848040",
    "end": "1854640"
  },
  {
    "text": "working with those teams to kind of understand how they did it to understand what they're doing to understand how they're interacting with the existing Hermes services and to create a model",
    "start": "1854640",
    "end": "1861180"
  },
  {
    "text": "for the rest of the company so we can move across there obviously if a whole bunch of our existing services are",
    "start": "1861180",
    "end": "1867070"
  },
  {
    "text": "still speaking Hermes any new service will steena at Hermes client but ideally we don't need Hermes servers anymore because they'll all just serve in G RPC",
    "start": "1867070",
    "end": "1873880"
  },
  {
    "text": "and connect to the older services or the not yet migrated services over Hermes",
    "start": "1873880",
    "end": "1879010"
  },
  {
    "text": "but still leaves a whole bunch of big questions for us things like kind of do",
    "start": "1879010",
    "end": "1884560"
  },
  {
    "text": "we migrate all the old services do we migrate some of the old services do we kind of not migrate anything just give",
    "start": "1884560",
    "end": "1890590"
  },
  {
    "text": "tools to create new ones and see what if the feature teams care about their old ones we're picking so far we're kind of",
    "start": "1890590",
    "end": "1897160"
  },
  {
    "text": "leaning towards a middle path of trying to migrate the most critical services but kind of give a help in supporting",
    "start": "1897160",
    "end": "1904120"
  },
  {
    "text": "the others but not necessarily be personally involved because they're over 200 teams we have and only so many of us",
    "start": "1904120",
    "end": "1910390"
  },
  {
    "text": "at infrastructure but kind of I'm happy to hear anyone else's stories of similar migrations to see what they've done and",
    "start": "1910390",
    "end": "1916060"
  },
  {
    "text": "what's worked the other really big thing we're trying to figure out with my guruji RPC is schema management like",
    "start": "1916060",
    "end": "1921490"
  },
  {
    "text": "protobuf schema management with our existing protocol we've found that some",
    "start": "1921490",
    "end": "1926740"
  },
  {
    "text": "of our more critical services have over a hundred different clients spread across all of our internal code bases we",
    "start": "1926740",
    "end": "1932140"
  },
  {
    "text": "don't really want to have over a hundred different versions of the same protobuf spread across all of these repos so",
    "start": "1932140",
    "end": "1938080"
  },
  {
    "text": "we're trying to figure out how to do that because we don't have a mono repo and but we also don't really want people copy and pasting each other's proto",
    "start": "1938080",
    "end": "1943660"
  },
  {
    "text": "files into their repos right now we're leaning towards building a schema management tool and open sourcing it and",
    "start": "1943660",
    "end": "1949930"
  },
  {
    "text": "kind of seeing if the rest of the community picks it up we're happy to again kind of hear what other people in",
    "start": "1949930",
    "end": "1955090"
  },
  {
    "text": "a similar situation do here and potentially work with them to build something good and the other thing thing",
    "start": "1955090",
    "end": "1962290"
  },
  {
    "text": "everybody's talking about at this conference is this do we have clearly want to use this do we see a lot of",
    "start": "1962290",
    "end": "1968710"
  },
  {
    "text": "benefits from things that it does we have done a few pocs but they've been very simple we did a quick POC just to",
    "start": "1968710",
    "end": "1974530"
  },
  {
    "text": "see like will it work at all if we deploy a bunch of histiocytes cards with our services does everything break and",
    "start": "1974530",
    "end": "1980380"
  },
  {
    "text": "that obviously didn't happen it worked fine so we filed a few bugs with this do a few bugs with the Envoy but overall it",
    "start": "1980380",
    "end": "1986050"
  },
  {
    "text": "worked then we moved on to testing services service authentication and we still have thousands of VM as we will",
    "start": "1986050",
    "end": "1992230"
  },
  {
    "text": "probably for a very long time we'll have services in multiple different clusters so we were figuring out service the service off with services",
    "start": "1992230",
    "end": "1998380"
  },
  {
    "text": "across clusters and across VMs and found that we can make it work but there are a lot of pain points so where it's gonna",
    "start": "1998380",
    "end": "2004650"
  },
  {
    "text": "still figuring out how to smooth those over or try to get it so that it by use do one or by the time that we have time",
    "start": "2004650",
    "end": "2010830"
  },
  {
    "text": "to really deploy us do we can get them fixed and the next thing we really want to investigate with this do is tracing",
    "start": "2010830",
    "end": "2016169"
  },
  {
    "text": "kind of how to do a good distributed tracing with it and then we're trying to figure out kind of what our next thing",
    "start": "2016169",
    "end": "2022470"
  },
  {
    "text": "we're working with the end user community we're looking at other projects whether it's prometheus or some",
    "start": "2022470",
    "end": "2027840"
  },
  {
    "text": "other things that we haven't listed here looking at non CN CF things and then",
    "start": "2027840",
    "end": "2032880"
  },
  {
    "text": "there other big things like we have no plan right now for it see how to replace our framework for Apollo but we don't",
    "start": "2032880",
    "end": "2038640"
  },
  {
    "text": "really want to have our own framework once we're and if kubernetes and gr pc and sto when running with all these",
    "start": "2038640",
    "end": "2044150"
  },
  {
    "text": "community tools so we're still figuring that out and again kind of we'd love to",
    "start": "2044150",
    "end": "2049169"
  },
  {
    "text": "work with people in similar places rather than invent this on our own and",
    "start": "2049169",
    "end": "2055138"
  },
  {
    "text": "that's it and we're happy to take a few questions if we still have time [Applause]",
    "start": "2055139",
    "end": "2067449"
  }
]