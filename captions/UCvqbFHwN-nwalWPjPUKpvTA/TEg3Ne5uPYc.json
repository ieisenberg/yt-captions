[
  {
    "text": "uh we'll start the session uh briefly uh thanks so much for showing up I'm really",
    "start": "4799",
    "end": "10380"
  },
  {
    "text": "happy to see a packed room um but today we'll be talking about uh",
    "start": "10380",
    "end": "16440"
  },
  {
    "text": "something a bit specific and eventful day that we had a",
    "start": "16440",
    "end": "22320"
  },
  {
    "text": "kind of a year ago at CERN now so my name is Ricardo",
    "start": "22320",
    "end": "27359"
  },
  {
    "text": "I'm Spiros yeah we'll be talking about today where we deleted a significant amount of our production workloads and",
    "start": "27359",
    "end": "33719"
  },
  {
    "text": "everything that comes with it and how we we live through the day and hopefully this will be a bit entertaining as well",
    "start": "33719",
    "end": "40579"
  },
  {
    "text": "so Welcome to our therapy session I think this is a good",
    "start": "40579",
    "end": "46559"
  },
  {
    "text": "way to describe it this is our way to to deal with it after a year as well",
    "start": "46559",
    "end": "52200"
  },
  {
    "text": "so again I'm a computer engineer at CERN I work a lot of in kubernetes containers a bit of machine learning as well I'm",
    "start": "52200",
    "end": "58199"
  },
  {
    "text": "also I have a couple of roles in the cncf I'm in the TOC and also I call it",
    "start": "58199",
    "end": "64739"
  },
  {
    "text": "the cncf researches group and I work at the Cloudy matern working",
    "start": "64739",
    "end": "70799"
  },
  {
    "text": "a lot of with kubernetes openstack and networking so a few numbers about the",
    "start": "70799",
    "end": "78119"
  },
  {
    "text": "infrastructure that we try to eliminate and we have currently after the incident",
    "start": "78119",
    "end": "84960"
  },
  {
    "text": "we will describe like more than 400 clusters almost 3 000 nodes and quite a few cores",
    "start": "84960",
    "end": "91619"
  },
  {
    "text": "and dramas you can see like 15 000 36 terabytes of rock Ram in the first pie chart you see the",
    "start": "91619",
    "end": "98520"
  },
  {
    "text": "grouping of clusters like the group you see that we have a few let's say power users that have quite a few clusters and",
    "start": "98520",
    "end": "106020"
  },
  {
    "text": "on the second pie chart you see the number of nodes that they have like you can see that the second user which is",
    "start": "106020",
    "end": "112619"
  },
  {
    "text": "the color is matching has quite a few more nodes with less clusters so they",
    "start": "112619",
    "end": "117899"
  },
  {
    "text": "have bigger clusters so about the production kubernetes",
    "start": "117899",
    "end": "124740"
  },
  {
    "text": "service we have at CERN it's a central API service that users can provision clusters",
    "start": "124740",
    "end": "131280"
  },
  {
    "text": "it's like one click provisioning for their clusters and then they can scale them up down at different kind of nodes",
    "start": "131280",
    "end": "138300"
  },
  {
    "text": "and like the purpose of this service is that to allow users to create clusters with different flavors",
    "start": "138300",
    "end": "145080"
  },
  {
    "text": "different kubernetes versions to select the nature control plane or not and they can select which kind which",
    "start": "145080",
    "end": "152220"
  },
  {
    "text": "add-ons they want with cni with CSI drivers enabled monitoring if they want",
    "start": "152220",
    "end": "157560"
  },
  {
    "text": "what kind of fingers controller and in our ID department and also but",
    "start": "157560",
    "end": "163739"
  },
  {
    "text": "also other departments in the organization we have multiple teams that consume the service and they run",
    "start": "163739",
    "end": "169260"
  },
  {
    "text": "multiple ID Services each team and they have usually some production clusters at",
    "start": "169260",
    "end": "175080"
  },
  {
    "text": "least one QA and a few testing clusters and there are also like personal environments for users to experiment",
    "start": "175080",
    "end": "180720"
  },
  {
    "text": "with new technologies and learn new things and one of the reasons that users or",
    "start": "180720",
    "end": "188819"
  },
  {
    "text": "teams have multiple clusters is that they treat their classes are as cattle they try to implement best server best",
    "start": "188819",
    "end": "195239"
  },
  {
    "text": "practices and not have like single points of failure usually they create one cluster and then they start adding",
    "start": "195239",
    "end": "201659"
  },
  {
    "text": "more clusters and usually those clusters are behind a single or like multiple",
    "start": "201659",
    "end": "207300"
  },
  {
    "text": "load balancers so that they can distribute the load they can do red to green deployments and",
    "start": "207300",
    "end": "214319"
  },
  {
    "text": "they can roll out new features new clusters like new versions as you can see here for example you can have like",
    "start": "214319",
    "end": "220159"
  },
  {
    "text": "121 122 they can add later 123 Etc",
    "start": "220159",
    "end": "225920"
  },
  {
    "text": "all right so uh as per said we we have uh quite a few clusters we have this",
    "start": "226739",
    "end": "231959"
  },
  {
    "text": "model where we offer cluster as a service a bit like a public cloud provider would do we offer this",
    "start": "231959",
    "end": "237360"
  },
  {
    "text": "internally I'll go quickly through some of the use cases we have for this uh the",
    "start": "237360",
    "end": "242580"
  },
  {
    "text": "important thing is that what we are talking here is mostly about Services there are some specialized deployment as",
    "start": "242580",
    "end": "247680"
  },
  {
    "text": "well of kubernetes that are single use case and can be kind of large scale as well I'll mention one as an example so",
    "start": "247680",
    "end": "254459"
  },
  {
    "text": "the first example of something that runs in this service where we try to delete the full thing",
    "start": "254459",
    "end": "260820"
  },
  {
    "text": "um it's a it's Atlas is one of the big experiments at CERN for the Large Hadron",
    "start": "260820",
    "end": "266220"
  },
  {
    "text": "Collider it's a big physics detector that is on the ground",
    "start": "266220",
    "end": "272699"
  },
  {
    "text": "and they have pretty big numbers they generate a lot of data and just to have an idea of the scale of the thing we can",
    "start": "272699",
    "end": "278699"
  },
  {
    "text": "see they actually move something like 50 petabytes of data a week they generate a lot but they also move it around between",
    "start": "278699",
    "end": "284759"
  },
  {
    "text": "CERN and other centers and also we concern in different places so the control plane for that and all the",
    "start": "284759",
    "end": "291240"
  },
  {
    "text": "bookkeeping that needs to be done to do all of this is run and manage through the kubernetes service",
    "start": "291240",
    "end": "298139"
  },
  {
    "text": "another one is uh physicists publish a lot of Articles and they need a nice way",
    "start": "298139",
    "end": "305460"
  },
  {
    "text": "to to kind of find this data so this is also something we run in this service there's a team that runs this Inspire",
    "start": "305460",
    "end": "313440"
  },
  {
    "text": "um Inspire app and Hep data projects and this is a nice portal where they do some sort of also machine learning over the",
    "start": "313440",
    "end": "320160"
  },
  {
    "text": "papers and then they do they allow people to discover nice content to to go through for their research topics",
    "start": "320160",
    "end": "327780"
  },
  {
    "text": "and then even the campus services so if any like I've been I've seen a lot of",
    "start": "327780",
    "end": "332940"
  },
  {
    "text": "people that are either still at CERN or were at CERN before during conference so if if you see these logos it probably",
    "start": "332940",
    "end": "338460"
  },
  {
    "text": "doesn't mean much to you but if you've been at CERN before working all of this is very familiar these are all the",
    "start": "338460",
    "end": "343620"
  },
  {
    "text": "internal Services we use for all sorts of things they're kind of critical and we also run them in the service you can",
    "start": "343620",
    "end": "351120"
  },
  {
    "text": "see it's uh it's kind of even if it's just for the Campus Management because it's a large organization it can be",
    "start": "351120",
    "end": "357840"
  },
  {
    "text": "quite big so they have like a total of 400 notes just for this in the different services",
    "start": "357840",
    "end": "364340"
  },
  {
    "text": "all right and then as I mentioned this is kind of the managed kubernetes that",
    "start": "365220",
    "end": "370979"
  },
  {
    "text": "we offer on demand uh there's also use cases where they are much larger scale and more critical and things like",
    "start": "370979",
    "end": "378180"
  },
  {
    "text": "running the the actual triggers or the event filters for the experiment so I mentioned Atlas before you can see a",
    "start": "378180",
    "end": "384660"
  },
  {
    "text": "picture of the of the detector there generates one petabyte a second needs to reduce that to something like 10",
    "start": "384660",
    "end": "390539"
  },
  {
    "text": "gigabytes a second traditionally they've been doing this using a large CPU cluster they are evolving that to use",
    "start": "390539",
    "end": "396840"
  },
  {
    "text": "gpus and other resources but actually the big transition they will do in the",
    "start": "396840",
    "end": "401880"
  },
  {
    "text": "next couple of years is that we did an evaluation where we what they want to do is instead of having their traditional",
    "start": "401880",
    "end": "408360"
  },
  {
    "text": "way of managing applications there they want to run a large kubernetes cluster and the reasons for that is the",
    "start": "408360",
    "end": "413699"
  },
  {
    "text": "flexibility and changing the workloads and and managing them so just to have an idea of what they are trying to do",
    "start": "413699",
    "end": "420500"
  },
  {
    "text": "they run something like 30 000 applications at any moment when there's beam running when the Beam stops they",
    "start": "420500",
    "end": "427139"
  },
  {
    "text": "want to switch that to simulation workloads and they have to do this in like one minute or two and when bin",
    "start": "427139",
    "end": "433199"
  },
  {
    "text": "comes back up they have to relaunch the 30 000 applications again in within one minute so we we did a lot of work also",
    "start": "433199",
    "end": "439620"
  },
  {
    "text": "with six scalability in kubernetes to make this scale we had a test cluster that had two and a half thousand nodes",
    "start": "439620",
    "end": "445440"
  },
  {
    "text": "and where we cook these numbers that you see here and we verify that this can work so the production cluster will",
    "start": "445440",
    "end": "451740"
  },
  {
    "text": "actually be a single cluster with 5000 nodes that will be serving the critical part of the atlas experiment computing",
    "start": "451740",
    "end": "460039"
  },
  {
    "text": "so let's start going to the event that we will describe today",
    "start": "461460",
    "end": "467099"
  },
  {
    "text": "this is like a spoiler alert in the end things went fine but even that they see",
    "start": "467099",
    "end": "474479"
  },
  {
    "text": "it now it still gets me a bit stressed but it kind of worked and also our",
    "start": "474479",
    "end": "479520"
  },
  {
    "text": "colleagues were quite nice with us and they were very understanding so the incident was that",
    "start": "479520",
    "end": "486500"
  },
  {
    "text": "by accident we had a maintenance tool that started to delete uh",
    "start": "486500",
    "end": "492539"
  },
  {
    "text": "full production that we had and thankfully he tried to do that one by",
    "start": "492539",
    "end": "498300"
  },
  {
    "text": "one so not everything in one go it actually deleted almost 120 clusters",
    "start": "498300",
    "end": "505620"
  },
  {
    "text": "and thankfully this resulted okay initially we didn't know that it was",
    "start": "505620",
    "end": "510660"
  },
  {
    "text": "only degradation and loss of capacity there was no downtime for like like no",
    "start": "510660",
    "end": "516180"
  },
  {
    "text": "Production Services had down time only a few like testing testing distances",
    "start": "516180",
    "end": "521339"
  },
  {
    "text": "next one sorry so the root cause is that this main transcript that I described so we had",
    "start": "521339",
    "end": "527339"
  },
  {
    "text": "implemented like a tool that we run regularly to clean up like orphan",
    "start": "527339",
    "end": "533100"
  },
  {
    "text": "clusters and orphan clusters are at kubernetes clusters that have their VMS",
    "start": "533100",
    "end": "538440"
  },
  {
    "text": "deleted and a lot balancers and other related computer storage resources",
    "start": "538440",
    "end": "544080"
  },
  {
    "text": "but there are still entries in the database of the service and this makes it a bit cumbersome for us to do",
    "start": "544080",
    "end": "549839"
  },
  {
    "text": "operations do monitoring and take metric the service so regularly we just clean up",
    "start": "549839",
    "end": "556080"
  },
  {
    "text": "similar to what the control Loop would do but it's like an external tool",
    "start": "556080",
    "end": "561260"
  },
  {
    "text": "the resolution to fix the incident was to just to recreate whatever the tool",
    "start": "561260",
    "end": "567420"
  },
  {
    "text": "are deleted but this was quite challenging as we will see in the next slides",
    "start": "567420",
    "end": "573300"
  },
  {
    "text": "yeah so I'll start with this so we'll go through the timeline of the day so the day started as most days start at CERN",
    "start": "573300",
    "end": "579600"
  },
  {
    "text": "which is over coffee and a lot of discussion this was early in the morning over coffee we usually talk about Dark",
    "start": "579600",
    "end": "586440"
  },
  {
    "text": "Energy dark matter and the expanding Universe actually not really this",
    "start": "586440",
    "end": "592380"
  },
  {
    "text": "particular day we were actually discussing the change we were about to apply even if we didn't feel it was like",
    "start": "592380",
    "end": "598500"
  },
  {
    "text": "a big change we still review what we're about to do during the day so spirits will give a bit more details so there",
    "start": "598500",
    "end": "606660"
  },
  {
    "text": "were discussions like okay everything is fine let's do it it's a small change it's nothing big I would have tested",
    "start": "606660",
    "end": "612839"
  },
  {
    "text": "this and all all the return codes and tests are correct we spent two three weeks",
    "start": "612839",
    "end": "619620"
  },
  {
    "text": "validating this so everything will go fine that was before lunch next",
    "start": "619620",
    "end": "626820"
  },
  {
    "text": "so at 2 30 we said okay we'll go for lunch everything is fine and then we clicked it and we started",
    "start": "626820",
    "end": "634980"
  },
  {
    "text": "and well we were waiting a bit to for the tool to kick in and then we will",
    "start": "634980",
    "end": "640680"
  },
  {
    "text": "start monitoring the logs and a few minutes after like one or two minutes after the the start of the script like",
    "start": "640680",
    "end": "646920"
  },
  {
    "text": "one of my colleagues from the identity service came in and said I think we're hacked like all the identities of the",
    "start": "646920",
    "end": "652260"
  },
  {
    "text": "Clusters are getting deleted and don't worry it's fine and we're just doing this to clean up the orphan cluster so",
    "start": "652260",
    "end": "658980"
  },
  {
    "text": "it's expected and then the other colleague that the registry the container registry runs in",
    "start": "658980",
    "end": "666000"
  },
  {
    "text": "kubernetes cluster that we with our own food and he said I don't see the registry",
    "start": "666000",
    "end": "671100"
  },
  {
    "text": "clusters something must be wrong so I was a bit worried so I clicked the abort",
    "start": "671100",
    "end": "676260"
  },
  {
    "text": "button to stop all of this and then I try to list the set clusters",
    "start": "676260",
    "end": "682079"
  },
  {
    "text": "because I thought they came maybe my colleague was looking at the wrong project or was doing something wrong in",
    "start": "682079",
    "end": "687959"
  },
  {
    "text": "his environment but it turns out that it wasn't and the next minutes were felt a bit like the",
    "start": "687959",
    "end": "694800"
  },
  {
    "text": "next slide like this one it lasted a few minutes in this mode and",
    "start": "694800",
    "end": "702300"
  },
  {
    "text": "then I I passed by Ricardo's office and he will yeah so the next few minutes after this",
    "start": "702300",
    "end": "709019"
  },
  {
    "text": "uh this is a very important point in any organization and CERN takes a lot of care about the safety of everyone so",
    "start": "709019",
    "end": "715980"
  },
  {
    "text": "this is actually a screenshot from the learning catalog at Cerny anyone can apply for this training it's first aid",
    "start": "715980",
    "end": "723120"
  },
  {
    "text": "and life-saving it says things like how to react in code of in case of external",
    "start": "723120",
    "end": "729240"
  },
  {
    "text": "Hemorrhage wounds loss of consciousness which is pretty much what was about to",
    "start": "729240",
    "end": "734700"
  },
  {
    "text": "happen here so it's also very important that the target audience is all Personnel working at CERN which covers",
    "start": "734700",
    "end": "741839"
  },
  {
    "text": "also I.T people so this is what what we had to do for for for the next couple of",
    "start": "741839",
    "end": "747240"
  },
  {
    "text": "seconds at least this is an actual picture of the defibrillators and the and fire extinguishers you can say you",
    "start": "747240",
    "end": "753959"
  },
  {
    "text": "can find that CERN so this lasted a couple minutes then we kind of calmed down and started looking into the issue",
    "start": "753959",
    "end": "759540"
  },
  {
    "text": "so around like what is important here is that we started this operation at like",
    "start": "759540",
    "end": "765480"
  },
  {
    "text": "235 239 we detected it we got the first report from a user about something is",
    "start": "765480",
    "end": "772320"
  },
  {
    "text": "wrong like really wrong around 41 so we got this uh we were also getting alarms",
    "start": "772320",
    "end": "777360"
  },
  {
    "text": "and things but uh those we kind of were ignoring trying to understand exactly",
    "start": "777360",
    "end": "783180"
  },
  {
    "text": "what were was happening because there was way too much coming so some user reported that the UI for the registry is",
    "start": "783180",
    "end": "789600"
  },
  {
    "text": "failing for me this is okay something is big so we spent the next 45 minutes after",
    "start": "789600",
    "end": "797459"
  },
  {
    "text": "this trying to do an assessment I we understood okay something went really wrong how do we how do we deal with this",
    "start": "797459",
    "end": "803399"
  },
  {
    "text": "and this was really important to to kind of structure this so the first thing first thing to see is like what's still",
    "start": "803399",
    "end": "809519"
  },
  {
    "text": "up and what's not so we immediately identified that one of the services that was down and actually the only service",
    "start": "809519",
    "end": "816000"
  },
  {
    "text": "that was down was the registry and the reason for that is that we actually got",
    "start": "816000",
    "end": "822480"
  },
  {
    "text": "unlucky let's say like this and we deleted all the registry clusters behind it which ended up bringing down the",
    "start": "822480",
    "end": "828899"
  },
  {
    "text": "service now in normally and we've tested adding new clusters this wouldn't be a big issue",
    "start": "828899",
    "end": "836120"
  },
  {
    "text": "and we could get the cluster back and and the service back in a couple of minutes in this case it wasn't quite the",
    "start": "836120",
    "end": "842940"
  },
  {
    "text": "case and we'll go into a bit more detail of why this was the case uh in in this event at the same time we were lucky",
    "start": "842940",
    "end": "848940"
  },
  {
    "text": "because we are under registered clusters right that it was the only service so one one thing that became very clear",
    "start": "848940",
    "end": "855540"
  },
  {
    "text": "also after this experience is that the registry is really like completely critical it's important to launch new",
    "start": "855540",
    "end": "862380"
  },
  {
    "text": "new applications of course like for stuff that is running and that we didn't delete uh they kept running without a",
    "start": "862380",
    "end": "869040"
  },
  {
    "text": "registry but if you delete the cluster of your users and you ask them to recreate it's kind of critical to have a",
    "start": "869040",
    "end": "875279"
  },
  {
    "text": "registry running so this was the first priority and what we did is we branched out the team so there were people",
    "start": "875279",
    "end": "881519"
  },
  {
    "text": "focusing on okay go get the registry back and do whatever is needed and the rest of us we started doing the rest",
    "start": "881519",
    "end": "887940"
  },
  {
    "text": "what was needed on the other side which is basically this is way too big to to",
    "start": "887940",
    "end": "893940"
  },
  {
    "text": "do the normal procedure so let's start contacting the different teams at CERN to fully understand what's the impact in",
    "start": "893940",
    "end": "900360"
  },
  {
    "text": "their own service and this was really essential so the covet era actually",
    "start": "900360",
    "end": "905940"
  },
  {
    "text": "helped a lot because we have this direct communication even using Zoom with many of the services and we basically",
    "start": "905940",
    "end": "912360"
  },
  {
    "text": "established this communication you have quick calls with them what's up how can we help is it down and what we started",
    "start": "912360",
    "end": "918899"
  },
  {
    "text": "realizing is that no one was down I think there was one case of a service",
    "start": "918899",
    "end": "924060"
  },
  {
    "text": "that was done but most service were saying okay we are still up we're degraded but we're still up so this kind",
    "start": "924060",
    "end": "930480"
  },
  {
    "text": "of helped us kind of relax a bit and the reason that uh that these Services were still up is",
    "start": "930480",
    "end": "938040"
  },
  {
    "text": "again back to this cluster as a scattle and the the dissemination we've been doing internally uh for for Best",
    "start": "938040",
    "end": "945300"
  },
  {
    "text": "Practices on githubs and Automation and I I show here like the our best User",
    "start": "945300",
    "end": "950339"
  },
  {
    "text": "it's a team that manages multiple Services you can see here actually I see",
    "start": "950339",
    "end": "955680"
  },
  {
    "text": "the person there so it's even better and this is uh one one of the like the most",
    "start": "955680",
    "end": "962220"
  },
  {
    "text": "well-behaved user and you can see what the impact of this event was for his own service or the team Services you have",
    "start": "962220",
    "end": "970079"
  },
  {
    "text": "like for each of the services they're maintaining they have something like eight clusters in total we deleted six",
    "start": "970079",
    "end": "975899"
  },
  {
    "text": "and still the service was still running so for the other ones in some cases none",
    "start": "975899",
    "end": "981600"
  },
  {
    "text": "was impacted but in no case we actually had a loss of complete capacity and this",
    "start": "981600",
    "end": "988500"
  },
  {
    "text": "is really a shout out to everyone in the certain teams also for for doing the right thing this took quite a long time",
    "start": "988500",
    "end": "995100"
  },
  {
    "text": "to contact everyone so really it's it was really important to Branch out and do and split the tasks you can see on",
    "start": "995100",
    "end": "1002420"
  },
  {
    "text": "the right plot you can see a similar view in number of nodes of course the services were not behaving as expected",
    "start": "1002420",
    "end": "1008920"
  },
  {
    "text": "but but it helped quite a bit so I guess Ricardo mentioned like the",
    "start": "1008920",
    "end": "1015320"
  },
  {
    "text": "first part was to bring the registry up and and we didn't realize that we had",
    "start": "1015320",
    "end": "1020540"
  },
  {
    "text": "created the circular dependency in the kubernetes service by introducing the harbor registry",
    "start": "1020540",
    "end": "1026360"
  },
  {
    "text": "before the hardware registry were using the gitlab registry and then we set up hardware and we were",
    "start": "1026360",
    "end": "1034280"
  },
  {
    "text": "advertising the users to consume it set up like rules for making immutable tags",
    "start": "1034280",
    "end": "1040938"
  },
  {
    "text": "and using vulnerability scans Etc and then we thought we should use the same",
    "start": "1040939",
    "end": "1045980"
  },
  {
    "text": "best practices and we started using them but by doing this we created the circular dependency I mentioned so it",
    "start": "1045980",
    "end": "1053179"
  },
  {
    "text": "was quite critical for our colleagues for our team members that were at the",
    "start": "1053179",
    "end": "1058220"
  },
  {
    "text": "task to to bring it up and they managed to do it quite quite fast we were also",
    "start": "1058220",
    "end": "1064240"
  },
  {
    "text": "kind of lucky because the load balancer was not was untouched so the DNS name",
    "start": "1064240",
    "end": "1069559"
  },
  {
    "text": "certificates all of that would be easy to bring back so this lasted like 40",
    "start": "1069559",
    "end": "1074780"
  },
  {
    "text": "minutes I think or something and then a user would be able to start",
    "start": "1074780",
    "end": "1080059"
  },
  {
    "text": "recreating their clusters and next slide yeah so usually this would take like maybe five to ten minutes to bring it",
    "start": "1080059",
    "end": "1086539"
  },
  {
    "text": "back because of the circular dependencies and some workarounds we had to apply to something like 45 minutes",
    "start": "1086539",
    "end": "1094059"
  },
  {
    "text": "and so for the majority of services as soon as we notified users that okay everything is fine now for the registry",
    "start": "1094220",
    "end": "1100280"
  },
  {
    "text": "you can move on like adding capacity to your services back and most of them took",
    "start": "1100280",
    "end": "1105679"
  },
  {
    "text": "them like 15 minutes maybe 30 some and then at the end of the long tail there",
    "start": "1105679",
    "end": "1110780"
  },
  {
    "text": "were some cases that needed more more time because it had special firewall rules that are not managed centrally I",
    "start": "1110780",
    "end": "1116960"
  },
  {
    "text": "like DNS name propagation that were kind of special and manual manual steps which shows that everything that involved",
    "start": "1116960",
    "end": "1123440"
  },
  {
    "text": "manual steps was a bit slower because the services were was built like that",
    "start": "1123440",
    "end": "1128740"
  },
  {
    "text": "in a very long time so in the incident they had to recreate everything very fast",
    "start": "1128740",
    "end": "1135799"
  },
  {
    "text": "and then we did a survey to understand what happened and how it was fixed and",
    "start": "1135799",
    "end": "1142400"
  },
  {
    "text": "how everything came back and really GitHub helped a lot here and",
    "start": "1142400",
    "end": "1147559"
  },
  {
    "text": "as we can see with the survey even for cluster provisioning and there were",
    "start": "1147559",
    "end": "1152660"
  },
  {
    "text": "quite some best practices used by by our users they were using terraform",
    "start": "1152660",
    "end": "1158120"
  },
  {
    "text": "and others wanted to use terraform but because of the setup of their cluster and the lack of feature support and the",
    "start": "1158120",
    "end": "1164240"
  },
  {
    "text": "provider and they were not using it but they wanted to cross plane also is the ultimate goal but it's not available yet",
    "start": "1164240",
    "end": "1170900"
  },
  {
    "text": "for our environment other users had very detailed documentation for their own team how to",
    "start": "1170900",
    "end": "1177500"
  },
  {
    "text": "create everything or how everything is built so they managed using the existing documentation to recreate everything",
    "start": "1177500",
    "end": "1183799"
  },
  {
    "text": "very fast even though they don't have automation but also for the application which was",
    "start": "1183799",
    "end": "1190660"
  },
  {
    "text": "quite a success about implementing best practices like almost everyone was doing",
    "start": "1190660",
    "end": "1196520"
  },
  {
    "text": "githubs with Argos your Flex V1 this was one year ago now a lot of people usually",
    "start": "1196520",
    "end": "1202340"
  },
  {
    "text": "are using also Flex V2 and other users had Helm charts but everything was packaged and configured in the values so",
    "start": "1202340",
    "end": "1210500"
  },
  {
    "text": "it was very easy to produce the deployment and that's why they were able to bring",
    "start": "1210500",
    "end": "1216620"
  },
  {
    "text": "up the capacity in such a short notice yeah so another thing to highlight here is that you see how diverse some of the",
    "start": "1216620",
    "end": "1223640"
  },
  {
    "text": "tools we use are and this is because we have very diverse teams we have best practices and for the like the most",
    "start": "1223640",
    "end": "1230120"
  },
  {
    "text": "critical Services we we kind of follow the same policy in the terms of tools",
    "start": "1230120",
    "end": "1235760"
  },
  {
    "text": "being used but at the same time different people have different requirements and the teams have different types of knowledge so we don't",
    "start": "1235760",
    "end": "1241880"
  },
  {
    "text": "currently enforce the kind of tool they should use so the the rest of the day so now we are",
    "start": "1241880",
    "end": "1248600"
  },
  {
    "text": "end of the afternoon uh we we spent kind of looking at the tail of the issues uh",
    "start": "1248600",
    "end": "1253760"
  },
  {
    "text": "for most Services people realize this pretty quickly and they reacted quickly we did realize that some things that",
    "start": "1253760",
    "end": "1260900"
  },
  {
    "text": "were impacted uh didn't get an immediate reply which probably means they are less critical but we wanted to make sure that",
    "start": "1260900",
    "end": "1267679"
  },
  {
    "text": "this was followed as well so this was kind of the tale of the the work here",
    "start": "1267679",
    "end": "1272860"
  },
  {
    "text": "so as a summary of all of this there are some highlights to to do there are",
    "start": "1274400",
    "end": "1280520"
  },
  {
    "text": "things that went quite well the main one is that we had no data loss this would be the main problem uh why this happened",
    "start": "1280520",
    "end": "1287240"
  },
  {
    "text": "we will we'll talk later a bit but it's it's also like not we we don't have",
    "start": "1287240",
    "end": "1293360"
  },
  {
    "text": "well-defined policies to to guarantee this unfortunately yet the deployments are really well managed this is really",
    "start": "1293360",
    "end": "1299720"
  },
  {
    "text": "something we invested quite a bit of time in dissemination of uh get Up's",
    "start": "1299720",
    "end": "1305679"
  },
  {
    "text": "processes not necessarily enforcing a tool but making sure that people are aware that this should be the case that",
    "start": "1305679",
    "end": "1311179"
  },
  {
    "text": "they should be able to deploy their applications very easily there's something that is really good in one way",
    "start": "1311179",
    "end": "1318500"
  },
  {
    "text": "which is cluster creation has been optimized over the years and it's really fast so it takes only a couple of",
    "start": "1318500",
    "end": "1323960"
  },
  {
    "text": "minutes to get a new cluster which means even in an event like this which is pretty big we can get the capacity back",
    "start": "1323960",
    "end": "1329179"
  },
  {
    "text": "pretty quickly multi-cluster and cluster has catalogs and workload splitting it does work like",
    "start": "1329179",
    "end": "1335539"
  },
  {
    "text": "uh these are the cases where we we kind of even if we go really hard on it",
    "start": "1335539",
    "end": "1340640"
  },
  {
    "text": "reducing the blast radios for the full service is really a big thing we ended up with degradation and almost no",
    "start": "1340640",
    "end": "1347120"
  },
  {
    "text": "downtime direct communication it was also very important",
    "start": "1347120",
    "end": "1353200"
  },
  {
    "text": "um on the what went wrong side the secular dependency was the biggest issue and that was the most stress to us after",
    "start": "1353360",
    "end": "1360020"
  },
  {
    "text": "the initial assessment but also what went wrong is that cluster deletion is also optimized and it's very",
    "start": "1360020",
    "end": "1366140"
  },
  {
    "text": "fast and successful uh so I don't know if we should do",
    "start": "1366140",
    "end": "1371539"
  },
  {
    "text": "something about it to make it less less efficient and then other things that went wrong is what I mentioned that some",
    "start": "1371539",
    "end": "1378080"
  },
  {
    "text": "some changes were manual like DNS updates firewall rules Etc",
    "start": "1378080",
    "end": "1383720"
  },
  {
    "text": "and also what we are slightly lacking behind apart from the use case with terraform is cluster bootstrapping is a",
    "start": "1383720",
    "end": "1389360"
  },
  {
    "text": "manual process so if you have something that consolidates all the time also that",
    "start": "1389360",
    "end": "1395240"
  },
  {
    "text": "would have been addressed by itself like they should have another space yeah so this is this is one of the main",
    "start": "1395240",
    "end": "1401240"
  },
  {
    "text": "Investments we are doing and we'll talk a bit more about in the demo uh",
    "start": "1401240",
    "end": "1407600"
  },
  {
    "text": "finally like the where we got lucky the final for the final part of this post-mortem is is",
    "start": "1407600",
    "end": "1413780"
  },
  {
    "text": "um we kind of got overconfident about what was clearly seen as a small change we knew that the impact could be big but",
    "start": "1413780",
    "end": "1420980"
  },
  {
    "text": "it looked trivial enough to and well tested enough to to to kind of Let It Go",
    "start": "1420980",
    "end": "1426140"
  },
  {
    "text": "and this is something that is constantly in our minds in the in the last few months so we are a lot more careful and",
    "start": "1426140",
    "end": "1432559"
  },
  {
    "text": "we Define some criteria on how to follow these changes the other thing we get lucky is that we",
    "start": "1432559",
    "end": "1438799"
  },
  {
    "text": "identify this circular dependency what happened is that we wanted to consolidate the service we ended up",
    "start": "1438799",
    "end": "1445100"
  },
  {
    "text": "running the registry in the service itself just kind of grow grown by itself",
    "start": "1445100",
    "end": "1451520"
  },
  {
    "text": "this idea without verifying necessarily so what we did after is really we did a",
    "start": "1451520",
    "end": "1457120"
  },
  {
    "text": "detailed analysis of where are the other circular dependencies that we missed over time so this is an effort that is",
    "start": "1457120",
    "end": "1463640"
  },
  {
    "text": "quite important as well the final one is that we have no visibility on data persistency or backups this is nice in",
    "start": "1463640",
    "end": "1471820"
  },
  {
    "text": "kubernetes you have this possibility to declare snapshots and even generate",
    "start": "1471820",
    "end": "1476900"
  },
  {
    "text": "backups this is not something we have in place right now for some limitations of",
    "start": "1476900",
    "end": "1482000"
  },
  {
    "text": "the integration we have with the storage systems and it's where we are also investing a lot because this will allow",
    "start": "1482000",
    "end": "1487760"
  },
  {
    "text": "us to look at the Clusters and see where the data is and what's backed up what's not and what kind of Trace to prevent",
    "start": "1487760",
    "end": "1495980"
  },
  {
    "text": "data loss in the future in this case we didn't have any and another part that we invested",
    "start": "1495980",
    "end": "1501679"
  },
  {
    "text": "sometime apart from I'm making the cluster provisioning like more automated",
    "start": "1501679",
    "end": "1506840"
  },
  {
    "text": "is to change a bitter policy and never like ever delete anything on behalf of",
    "start": "1506840",
    "end": "1512240"
  },
  {
    "text": "the user if there even if they request about it we should always delegate to them and tell them how to do it so we",
    "start": "1512240",
    "end": "1518539"
  },
  {
    "text": "never like we block ourselves from even being able to do it",
    "start": "1518539",
    "end": "1524179"
  },
  {
    "text": "right so this this will be a very quick demo I must confess that initially uh the idea was to do a real live",
    "start": "1524179",
    "end": "1532100"
  },
  {
    "text": "production deletion of a real CERN service and discussing with colleagues",
    "start": "1532100",
    "end": "1537919"
  },
  {
    "text": "involved and not involved in this incident they said if this is a therapy session I don't think that's a good idea",
    "start": "1537919",
    "end": "1543140"
  },
  {
    "text": "because it might just ruin it so we'll do the second best thing I'll try to demonstrate uh a bit what we try to",
    "start": "1543140",
    "end": "1551659"
  },
  {
    "text": "advertise as automation of the cluster management this is really the part that we're missing all the services are well",
    "start": "1551659",
    "end": "1558440"
  },
  {
    "text": "well done and we understand well how to to to manage them the cluster part is",
    "start": "1558440",
    "end": "1563539"
  },
  {
    "text": "still a challenge ideally we'll get to integrating this with cross plane as well very soon",
    "start": "1563539",
    "end": "1569539"
  },
  {
    "text": "so just a very simple uh description of how this works so in this case we are",
    "start": "1569539",
    "end": "1575539"
  },
  {
    "text": "relying on Argo CD we have this bootstrap which means we can pop up a cluster deploy this in the cluster and",
    "start": "1575539",
    "end": "1582799"
  },
  {
    "text": "we'll have a nargo CD deployment which also has a secret Management Service internally and manages itself Argo CD so",
    "start": "1582799",
    "end": "1590960"
  },
  {
    "text": "that's good we also deploy Argo workflows in this case and this is because there's a lot of manual tasks",
    "start": "1590960",
    "end": "1596659"
  },
  {
    "text": "where you might want to go and write a crd and a controller and do proper",
    "start": "1596659",
    "end": "1602539"
  },
  {
    "text": "reconciliation but that can be quite a lot of work and not all users will will",
    "start": "1602539",
    "end": "1608000"
  },
  {
    "text": "be willing to to develop and maintain those so the second best thing is just to offer them a way to integrate it with",
    "start": "1608000",
    "end": "1614900"
  },
  {
    "text": "the workflows uh then we have the the Clusters description so this is the dream uh it's",
    "start": "1614900",
    "end": "1622039"
  },
  {
    "text": "to have one yaml file where all the Clusters are defined or a couple of yaml files and you can see here like I have a",
    "start": "1622039",
    "end": "1628220"
  },
  {
    "text": "couple of clusters and for each cluster we have the configuration which is basically the version that we want to",
    "start": "1628220",
    "end": "1633620"
  },
  {
    "text": "run and then some flavors for the nodes and the Masters as well as node groups definitions a lot of these clusters are",
    "start": "1633620",
    "end": "1640100"
  },
  {
    "text": "heterogeneous so we want to create like nodes in different availability zones or different flavors GPU CPUs things like",
    "start": "1640100",
    "end": "1646039"
  },
  {
    "text": "this and then something that is really important is this part here you can Define Argo CD labels which are",
    "start": "1646039",
    "end": "1653120"
  },
  {
    "text": "basically the type of workload that this cluster should be running and this will allow us to do a kind of a matchmaking",
    "start": "1653120",
    "end": "1658940"
  },
  {
    "text": "with the applications without having to do a hard coding of the the mappings and you can see other clusters here if we",
    "start": "1658940",
    "end": "1666200"
  },
  {
    "text": "look at the the existing Argo configuration we can see the",
    "start": "1666200",
    "end": "1672020"
  },
  {
    "text": "Clusters here I have 124 125 and I have an extra one that has nothing deployed right now so and you can see here all",
    "start": "1672020",
    "end": "1679940"
  },
  {
    "text": "the details about how they are deployed again we don't have cross plane integration yet it will come soon so the",
    "start": "1679940",
    "end": "1686539"
  },
  {
    "text": "second best thing is to to use a workflow for that and then we have the services so if we",
    "start": "1686539",
    "end": "1692419"
  },
  {
    "text": "go here quickly for a service so this is a very simple",
    "start": "1692419",
    "end": "1698779"
  },
  {
    "text": "example of what the service could be like this is a serving service for a machine learning application uh the the",
    "start": "1698779",
    "end": "1706159"
  },
  {
    "text": "really nice thing here in addition to all the things I'm sure all of you that used Argo know about is that we are",
    "start": "1706159",
    "end": "1712820"
  },
  {
    "text": "using this label selector so that we can match the workloads to the Clusters that are saying please give me this type of",
    "start": "1712820",
    "end": "1718520"
  },
  {
    "text": "workloads and this is really a separation of of that that is very useful in this case",
    "start": "1718520",
    "end": "1725419"
  },
  {
    "text": "and I think that's it for the description so what I will do here again if I look at the applications and I",
    "start": "1725419",
    "end": "1733640"
  },
  {
    "text": "select and I select here quickly",
    "start": "1733640",
    "end": "1740299"
  },
  {
    "text": "specific project we see we have two instances of this ml service application it's in two different clusters this is",
    "start": "1740299",
    "end": "1746720"
  },
  {
    "text": "what we want to provide so what we will do we'll do two things the first one is",
    "start": "1746720",
    "end": "1751880"
  },
  {
    "text": "We'll add the same label so this ml true which means deployment applications in",
    "start": "1751880",
    "end": "1758360"
  },
  {
    "text": "an additional cluster and at the same time I will also deploy a new cluster",
    "start": "1758360",
    "end": "1765220"
  },
  {
    "text": "so let's call it 10. I'll just keep the same configuration to speed up things and I will say that I",
    "start": "1765260",
    "end": "1771620"
  },
  {
    "text": "also need ml in this cluster so now I'll just quickly add this",
    "start": "1771620",
    "end": "1780220"
  },
  {
    "text": "push it and just for all right it's there and if I go back to",
    "start": "1788539",
    "end": "1796399"
  },
  {
    "text": "to my clusters I do a refresh quickly and you can see that it's starting to to",
    "start": "1796399",
    "end": "1803419"
  },
  {
    "text": "to launch these new workflows and there's two one for the previous cluster",
    "start": "1803419",
    "end": "1808640"
  },
  {
    "text": "because it needs to update the labels and there's a second one which is deploying the new cluster which is the",
    "start": "1808640",
    "end": "1814100"
  },
  {
    "text": "010 cluster I just showed so in the new one we see the first pods here starting",
    "start": "1814100",
    "end": "1819640"
  },
  {
    "text": "really quickly at the logs we can see that the cluster is already in progress so this takes a couple of minutes and",
    "start": "1819640",
    "end": "1824899"
  },
  {
    "text": "because we want to leave time for questions what I will show you quickly is the changes that are being applied in",
    "start": "1824899",
    "end": "1830720"
  },
  {
    "text": "the other cluster where it's basically saying it's applying a bunch of resizes",
    "start": "1830720",
    "end": "1835760"
  },
  {
    "text": "so I'll come come back to it when it registers it as well so the way the workflow does is it tries to create the",
    "start": "1835760",
    "end": "1842419"
  },
  {
    "text": "cluster if there's a cluster just ignores it and then it tries to go through all the node groups and this is what it's doing like for each node group",
    "start": "1842419",
    "end": "1849679"
  },
  {
    "text": "so in this case this was node group yeah it's trying to resize the cluster",
    "start": "1849679",
    "end": "1856100"
  },
  {
    "text": "and then applying all the changes again ideally we should have this done with",
    "start": "1856100",
    "end": "1862100"
  },
  {
    "text": "the proper reconciliation so that is applied faster workflows are the second best thing and honestly there are a lot",
    "start": "1862100",
    "end": "1869240"
  },
  {
    "text": "of processes internally where it's much easier to offer workflows and offer this flexibility to users while we wait I'll",
    "start": "1869240",
    "end": "1876260"
  },
  {
    "text": "just highlight from the Argo documentation this when you start doing application sets which are the key here",
    "start": "1876260",
    "end": "1882460"
  },
  {
    "text": "there's these different generators one of them is this cluster generator and this is where the magic happens which is",
    "start": "1882460",
    "end": "1889399"
  },
  {
    "text": "you can Define your application set and then have the labels saying where where the application should be mapped",
    "start": "1889399",
    "end": "1896419"
  },
  {
    "text": "so let me see if this will be fast enough to to actually appear",
    "start": "1896419",
    "end": "1903460"
  },
  {
    "text": "yep so this is the one it's about to change the labels it did the update ml",
    "start": "1907940",
    "end": "1913520"
  },
  {
    "text": "true so if we go back to Applications",
    "start": "1913520",
    "end": "1917679"
  },
  {
    "text": "welcome back I need to turn off the CERN so sorry about that if we come back we see",
    "start": "1920539",
    "end": "1926899"
  },
  {
    "text": "actually an additional ml application there so if I do ml instead of two now we have the application running in the",
    "start": "1926899",
    "end": "1933200"
  },
  {
    "text": "news in a new cluster fully up and running and in just a couple of minutes so this is really what we are trying to",
    "start": "1933200",
    "end": "1938600"
  },
  {
    "text": "advertise to the users and convince them that this is useful for their own sake as well",
    "start": "1938600",
    "end": "1944480"
  },
  {
    "text": "so yeah it worked in the end all right so I think that's pretty much",
    "start": "1944480",
    "end": "1950720"
  },
  {
    "text": "it what we have and we have a few minutes for questions so happy to to answer IA so just ending here with",
    "start": "1950720",
    "end": "1958100"
  },
  {
    "text": "another quote from from our nice users that say in the end it was a chaos chaos",
    "start": "1958100",
    "end": "1963380"
  },
  {
    "text": "monkey test and we kind of passed it so it's really nice so yeah any questions",
    "start": "1963380",
    "end": "1969960"
  },
  {
    "text": "[Applause]",
    "start": "1969960",
    "end": "1981469"
  },
  {
    "text": "yep yeah I have a question regarding um I imagine that you had persistent volumes",
    "start": "1984500",
    "end": "1990919"
  },
  {
    "text": "in the Clusters and I know that for example using ceph you have a reference with the image ID but if you lose the",
    "start": "1990919",
    "end": "1998539"
  },
  {
    "text": "cluster then you how did you recover that image ID to to recreate the the",
    "start": "1998539",
    "end": "2004659"
  },
  {
    "text": "persistent volume connected to the right self image RPD for example right so for",
    "start": "2004659",
    "end": "2011019"
  },
  {
    "text": "for Seth there's two steps one is the provisioning of the volumes the second one is the attaching of the volumes so",
    "start": "2011019",
    "end": "2019779"
  },
  {
    "text": "in most cases people use provisioning but then they reuse the provision volumes meaning that you can actually",
    "start": "2019779",
    "end": "2026880"
  },
  {
    "text": "Mount your application in multiple clusters if you're doing RBD this is",
    "start": "2026880",
    "end": "2031960"
  },
  {
    "text": "much more complicated so that's why actually the usage of CFS at CERN for",
    "start": "2031960",
    "end": "2037179"
  },
  {
    "text": "kubernetes is much larger than the usage of RBD itself because you can have this",
    "start": "2037179",
    "end": "2042880"
  },
  {
    "text": "multi-attach much easier Okay so basically you didn't have",
    "start": "2042880",
    "end": "2048000"
  },
  {
    "text": "this RPD images right there were cases but you can also have multi-attach in",
    "start": "2048000",
    "end": "2054220"
  },
  {
    "text": "RBD but it's done differently it's like kind of a active passive mode instead of having multi-attach okay thank you",
    "start": "2054220",
    "end": "2061839"
  },
  {
    "text": "a quick question how did you end up solving your circular dependency on",
    "start": "2061839",
    "end": "2067000"
  },
  {
    "text": "Harbor so what what's the solution that you chose that's a very good question uh",
    "start": "2067000",
    "end": "2072158"
  },
  {
    "text": "should I answer so uh yeah the solution it was like kind of obvious like you can",
    "start": "2072159",
    "end": "2079179"
  },
  {
    "text": "have a second backup of the images outside so in another registry so this is an in the end the solution",
    "start": "2079179",
    "end": "2085960"
  },
  {
    "text": "yeah and the circle the service itself allows us to specify the registry where",
    "start": "2085960",
    "end": "2091240"
  },
  {
    "text": "the core images should come from so we can flip that when you launch a cluster you can choose which back-end register",
    "start": "2091240",
    "end": "2097660"
  },
  {
    "text": "should be used for the core images so right now we have the core the main",
    "start": "2097660",
    "end": "2103300"
  },
  {
    "text": "registry we have a replica on premises and we have a replica of the core images in a external registry outside CERN as",
    "start": "2103300",
    "end": "2111640"
  },
  {
    "text": "well for for a disaster recovery thanks make sense yeah it's a good question",
    "start": "2111640",
    "end": "2118800"
  }
]