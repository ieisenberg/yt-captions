[
  {
    "text": "okay so we would be talking about uh in to end llm Ms with Q flu so who all are",
    "start": "480",
    "end": "7240"
  },
  {
    "text": "in the ml space who has been trying um ml models or who are planning to use ml",
    "start": "7240",
    "end": "14960"
  },
  {
    "text": "in the next year next one year okay many are there okay good so in the next Cube con we'll see more talks on the ml side",
    "start": "14960",
    "end": "21279"
  },
  {
    "text": "of things okay um talking about myself I'm",
    "start": "21279",
    "end": "26599"
  },
  {
    "text": "a technical director at nanic lead um AI efforts specifically in the system side",
    "start": "26599",
    "end": "32480"
  },
  {
    "text": "I'm part of the C flow steering committee uh I'm lead couple of efforts there primarily in the training and the",
    "start": "32480",
    "end": "37520"
  },
  {
    "text": "automl I'm co-chairing some efforts there as well uh Gish is part of the ker",
    "start": "37520",
    "end": "43480"
  },
  {
    "text": "which is a serving side of things in Q flow he's a maintainer there is is leading some of the efforts in the",
    "start": "43480",
    "end": "49199"
  },
  {
    "text": "observability Ajay and Krishna couldn't attend this time unfortunately aay is leading um some efforts in uh qlu",
    "start": "49199",
    "end": "56160"
  },
  {
    "text": "releases and Krishna from the platform side of things",
    "start": "56160",
    "end": "61600"
  },
  {
    "text": "so just to give a high level overview of what C flow plans to do and what it is",
    "start": "62879",
    "end": "68479"
  },
  {
    "text": "doing now right the idea is to make machine learning on kubernetes specifically easy portable and scalable",
    "start": "68479",
    "end": "75799"
  },
  {
    "text": "so it targets two personas specifically from um uh when you think about machine",
    "start": "75799",
    "end": "81920"
  },
  {
    "text": "learning um there's a ml engineer Persona there is a infra Persona right",
    "start": "81920",
    "end": "87320"
  },
  {
    "text": "so ml Engineers need a simple pythonic interface right so that's something that",
    "start": "87320",
    "end": "92880"
  },
  {
    "text": "qflow wants to give to the ml side of the from the ml engineer perspective now from the infra side you need monitoring",
    "start": "92880",
    "end": "100479"
  },
  {
    "text": "and other things to see how well you're using infra to the best utilization right that's what we have to do from the",
    "start": "100479",
    "end": "106840"
  },
  {
    "text": "easy easiness so portable means um say I start my experiment on my laptop today",
    "start": "106840",
    "end": "114159"
  },
  {
    "text": "right I have to at some point I see that I have to do it on my um maybe like in",
    "start": "114159",
    "end": "119320"
  },
  {
    "text": "the in a private cloud or a public cloud or in a hybrid nature can I take the",
    "start": "119320",
    "end": "124479"
  },
  {
    "text": "same system that I'm having now and deploy it there right that's the portability that we are looking at the",
    "start": "124479",
    "end": "130879"
  },
  {
    "text": "scalability in the sense I start really small right like I have a single node with me maybe I train a model with just",
    "start": "130879",
    "end": "137239"
  },
  {
    "text": "one GPU today how do I make it to a larger scale with very minimal changes",
    "start": "137239",
    "end": "143760"
  },
  {
    "text": "and that's one other goal that qf flow is looking at um on a very high level um like U if",
    "start": "143760",
    "end": "153239"
  },
  {
    "text": "you look at the ml model life cycle um a ml engineer comes in and start",
    "start": "153239",
    "end": "159080"
  },
  {
    "text": "experimenting with a model like primarily it will be a jupyter notebook interface right um that he's familiar",
    "start": "159080",
    "end": "165560"
  },
  {
    "text": "with so he does some experiments develops a very basic model prototype um",
    "start": "165560",
    "end": "171519"
  },
  {
    "text": "maybe like during his um prototyping he would have actually used some hyper parameters so hyper parameters refers to",
    "start": "171519",
    "end": "178200"
  },
  {
    "text": "parameters that are fixed during training right so there are different ways of model optimization um hyper",
    "start": "178200",
    "end": "184440"
  },
  {
    "text": "parameter optimization is just one of that right say if you're using neural Nets um you can use Nas for example",
    "start": "184440",
    "end": "190040"
  },
  {
    "text": "neural architecture search so there are different model optimization techniques so that would be the second phase where",
    "start": "190040",
    "end": "195920"
  },
  {
    "text": "you do some kind of model tuning on the Prototype that that you have created once you have a basic prototype",
    "start": "195920",
    "end": "203599"
  },
  {
    "text": "you start to train at a very large scale right so you had a single system to try",
    "start": "203599",
    "end": "209480"
  },
  {
    "text": "on initially post that you take that to a much larger distributed systems maybe",
    "start": "209480",
    "end": "215040"
  },
  {
    "text": "like tens or hundreds of system um systems with gpus or uh tpus whatever uh",
    "start": "215040",
    "end": "221080"
  },
  {
    "text": "accelerator that you have and train it to a larger extent so you have a train model from there you start your",
    "start": "221080",
    "end": "228400"
  },
  {
    "text": "deployment process right um essentially you're creating a server for your users",
    "start": "228400",
    "end": "234159"
  },
  {
    "text": "to start sending requests and get predictions out of that so this is a bigger picture of what",
    "start": "234159",
    "end": "241799"
  },
  {
    "text": "Q flow is so on the top you see there are different ml Frameworks uh say be it",
    "start": "241799",
    "end": "247560"
  },
  {
    "text": "P torch uh tensor flow XG boost as IIT learn or any framework that uh be it",
    "start": "247560",
    "end": "253519"
  },
  {
    "text": "that you are using now um Q flow is independent of that right and the bottom part it's running natively on kubernetes",
    "start": "253519",
    "end": "260880"
  },
  {
    "text": "so all the different components that you're seeing of Cu flow are custom controllers built on um kubernetes",
    "start": "260880",
    "end": "266840"
  },
  {
    "text": "natively um so this actually means you can run on any hardware be it Nvidia GPU",
    "start": "266840",
    "end": "272960"
  },
  {
    "text": "AMD GPU uh inel AMX doesn't really matter right so there are different",
    "start": "272960",
    "end": "278800"
  },
  {
    "text": "components in Q flow one uh the qlow notebooks which is web based IDE spark",
    "start": "278800",
    "end": "284280"
  },
  {
    "text": "operator for running spark jobs for uh data preparation or pre-processing training operator for large scale model",
    "start": "284280",
    "end": "291160"
  },
  {
    "text": "training model registry for storing model metadata C for model tuning ques",
    "start": "291160",
    "end": "296880"
  },
  {
    "text": "for uh model serving or inference uh qflow pipelines for workflow",
    "start": "296880",
    "end": "302000"
  },
  {
    "text": "orchestration and the central dashboard uh to have kind of like a web interface to all the components that you have so",
    "start": "302000",
    "end": "308320"
  },
  {
    "text": "you can imagine this to be a a suite of uh components coming together to build",
    "start": "308320",
    "end": "313960"
  },
  {
    "text": "as a ml platform so if you need you need you can have one or more uh components",
    "start": "313960",
    "end": "320639"
  },
  {
    "text": "suiting your use case it doesn't need to be the entire platform as such so if someone is interested in just serving",
    "start": "320639",
    "end": "325960"
  },
  {
    "text": "part you can install ker if someone is interested in just um dist the training part you can install the training",
    "start": "325960",
    "end": "332800"
  },
  {
    "text": "operator right so from a c perspective you can get a PL platform which is well",
    "start": "332800",
    "end": "338680"
  },
  {
    "text": "connected which actually means you can do some kind of training keep your model",
    "start": "338680",
    "end": "344759"
  },
  {
    "text": "in model registry pick that model into your K of you have connections with",
    "start": "344759",
    "end": "350319"
  },
  {
    "text": "jupyter notebook so you have ecosystem that you can actually get for",
    "start": "350319",
    "end": "355360"
  },
  {
    "text": "free um very high level overview notebooks it's the entire components",
    "start": "355560",
    "end": "360880"
  },
  {
    "text": "that you have which you have seen the last slide they are multi-tenant right so in organization you can have one user",
    "start": "360880",
    "end": "366680"
  },
  {
    "text": "creating one notebook it is completely isolated from the notebooks that you're create creating from the other it's all",
    "start": "366680",
    "end": "371800"
  },
  {
    "text": "Nam spaced um resource in qf flow um give it any CPU memory or the hardware",
    "start": "371800",
    "end": "378720"
  },
  {
    "text": "accelerator that you have if you need long um a longer duration for your data you can add persistent volume it",
    "start": "378720",
    "end": "384520"
  },
  {
    "text": "supports uh Art Studio Visual Studio jupyter notebook uh standard jupyter notebook or you can bring your own",
    "start": "384520",
    "end": "391199"
  },
  {
    "text": "custom image as well um so coming to training operator",
    "start": "391199",
    "end": "397280"
  },
  {
    "text": "um uh you get distributor training out of box right so depending on the framework um you can use different",
    "start": "397280",
    "end": "405000"
  },
  {
    "text": "training strategies for example tensorflow supports uh the the centralized parameter server uh",
    "start": "405000",
    "end": "411440"
  },
  {
    "text": "distributed mirror strategy pytorch has data parallelism um model parallelism",
    "start": "411440",
    "end": "416960"
  },
  {
    "text": "for with a tensor and a pip and parallelism elastic search in the elastic training in the sense you can",
    "start": "416960",
    "end": "423919"
  },
  {
    "text": "have faults in your system node can go down but again training can still continue right in a realistic scenario",
    "start": "423919",
    "end": "430479"
  },
  {
    "text": "uh MPI XD boost beat any framework you can support that um so from your spec",
    "start": "430479",
    "end": "438199"
  },
  {
    "text": "perspective what you have to do would be just say how many workers do I need and",
    "start": "438199",
    "end": "443879"
  },
  {
    "text": "how many gpus I need to provide to that right rest all things are taken care automatically by the operator without",
    "start": "443879",
    "end": "450599"
  },
  {
    "text": "understanding anything about how it is implemented on um there are Advanced",
    "start": "450599",
    "end": "455879"
  },
  {
    "text": "scheduling support as well using Vulcan and Q you would have heard these things uh in other talks as",
    "start": "455879",
    "end": "461759"
  },
  {
    "text": "well um uh the other component is a model tuner CP which supports different",
    "start": "461759",
    "end": "468280"
  },
  {
    "text": "optimization techniques um hyper paramet optimization neural AR neural architecture search early stopping in",
    "start": "468280",
    "end": "474840"
  },
  {
    "text": "the sense um if you see certain experiments are not good enough I don't need to try for really long I'll stop",
    "start": "474840",
    "end": "481639"
  },
  {
    "text": "early so that like I can save resources I can track all the different experiments to see which experiment um",
    "start": "481639",
    "end": "488759"
  },
  {
    "text": "provides me the best metric right so uh from a model tuner perspective I can say",
    "start": "488759",
    "end": "494280"
  },
  {
    "text": "that I need to maximize accuracy for example for for for my model given",
    "start": "494280",
    "end": "500280"
  },
  {
    "text": "parameters that you specify in your config right say suppose I have certain parameters like learning rate momentum U",
    "start": "500280",
    "end": "507960"
  },
  {
    "text": "Etc that you would be using in your trade tring right I don't know what's the right parameters to be used for training so I can say these are the",
    "start": "507960",
    "end": "514039"
  },
  {
    "text": "parameters these are the parameter space that I'm going to give for them find me the best uh uh parameter value that I",
    "start": "514039",
    "end": "521880"
  },
  {
    "text": "should use for my full skill training that's what CP does on the right you'll see the lots of visualization around",
    "start": "521880",
    "end": "529560"
  },
  {
    "text": "what are the best metrics possible for training and it supports all uh different algorithms which are required",
    "start": "529560",
    "end": "535320"
  },
  {
    "text": "for the hyperparameter optimization so once you have a train model you can can use Cas serve for your model serving",
    "start": "535320",
    "end": "541959"
  },
  {
    "text": "right you would have heard this um component in multiple talks so it supports your traditional predictive",
    "start": "541959",
    "end": "548120"
  },
  {
    "text": "models and the newer generative model support and it has all um New Generation",
    "start": "548120",
    "end": "555279"
  },
  {
    "text": "uh techniques for making inference faster um it has Auto scaling where you can scale down to zero and then on and",
    "start": "555279",
    "end": "562560"
  },
  {
    "text": "on traffic demand it can scale up and down right it supports Canary roll outs",
    "start": "562560",
    "end": "567640"
  },
  {
    "text": "for versioning with versioning um it has pluggable inference run times",
    "start": "567640",
    "end": "572800"
  },
  {
    "text": "where you can use be it py tor or hugging pH uh VM um uh it's all crd",
    "start": "572800",
    "end": "580079"
  },
  {
    "text": "based uh it can do GPU autoscaling in the sense when you have more traffic coming in um it can automatically Scale",
    "start": "580079",
    "end": "586760"
  },
  {
    "text": "based on your Demand right that you confer for in the config file um it has",
    "start": "586760",
    "end": "592279"
  },
  {
    "text": "optimized Hardware inference in the sense it can run on different Hardware",
    "start": "592279",
    "end": "597880"
  },
  {
    "text": "based on your system that you are using right like um for example there are",
    "start": "597880",
    "end": "602959"
  },
  {
    "text": "optimized inference run times for AMD and C um Intel and Nvidia um by default",
    "start": "602959",
    "end": "611160"
  },
  {
    "text": "you don't need to do anything by default you'll get that performance automatically um it's ml framework",
    "start": "611160",
    "end": "616640"
  },
  {
    "text": "agnostic you can bring in any Frameworks and you can keep your model um be beat",
    "start": "616640",
    "end": "623640"
  },
  {
    "text": "in S3 or NFS or if you have any custom storage you can add that as a um",
    "start": "623640",
    "end": "628959"
  },
  {
    "text": "standard itself and once you have kind of like",
    "start": "628959",
    "end": "635120"
  },
  {
    "text": "different building blocks you can use ql pipelines to connect right end of it this is a workflow",
    "start": "635120",
    "end": "640200"
  },
  {
    "text": "orchestrator so you can imagine this to be a dag created for you where you can connect between multiple nodes right so",
    "start": "640200",
    "end": "648680"
  },
  {
    "text": "on the right you would see that I create say PVC and then fine tuning we'll talk more during the demo so this is natively",
    "start": "648680",
    "end": "655760"
  },
  {
    "text": "written in python in the sense you can use Python function it will containerize for you and run on",
    "start": "655760",
    "end": "662839"
  },
  {
    "text": "kubernetes right so it is a it gives you a pythonic interface for data scientist um and it it's a it's a you can imagine",
    "start": "662839",
    "end": "670320"
  },
  {
    "text": "this to be a template where um multiple runs can be generated using different parameter values and um and it",
    "start": "670320",
    "end": "677880"
  },
  {
    "text": "automatically provides you all visualization which is required for your",
    "start": "677880",
    "end": "683360"
  },
  {
    "text": "run so coming to llm specifically so we have all the blocks and we we have just",
    "start": "683360",
    "end": "689600"
  },
  {
    "text": "talked about the basic components right so when you think about the customer user Journey for uh the llm side of",
    "start": "689600",
    "end": "695399"
  },
  {
    "text": "things we have all the components now how do we tune that for the specific large language models right so can",
    "start": "695399",
    "end": "702600"
  },
  {
    "text": "imagine this to be someone starting to experiment the large language model using a q flow notebook um again uh do",
    "start": "702600",
    "end": "710880"
  },
  {
    "text": "some tuning using CB post that um if it is kind of like a use case where you",
    "start": "710880",
    "end": "717000"
  },
  {
    "text": "actually want to do a full uh training from uh randomly initialized weight you can have a pre-training or a fine tuning",
    "start": "717000",
    "end": "724519"
  },
  {
    "text": "in the sense that you have you take a base model like llama and you f tune on the data set that you have right that's",
    "start": "724519",
    "end": "730800"
  },
  {
    "text": "your choice you can do that using training operator once you have a train model you can use serving component",
    "start": "730800",
    "end": "737240"
  },
  {
    "text": "which is called kerve and the whole thing can be orchestrated using qflow pipelines and on the top you see the",
    "start": "737240",
    "end": "742760"
  },
  {
    "text": "model register which is Alpha component right now where um the different versions of models can be pushed to",
    "start": "742760",
    "end": "748639"
  },
  {
    "text": "model register right in a way that you can always track models um what H what have been created",
    "start": "748639",
    "end": "755240"
  },
  {
    "text": "and the right models can be used for serving right so the the great the the",
    "start": "755240",
    "end": "760519"
  },
  {
    "text": "the great Advantage would be like in the case of you can say that like what version of model should I inference um",
    "start": "760519",
    "end": "767639"
  },
  {
    "text": "based on the performance that I see and it can take care of that as well and one um the key addition that we",
    "start": "767639",
    "end": "776959"
  },
  {
    "text": "have added in the last two releases um was regarding the apis right so as I",
    "start": "776959",
    "end": "782880"
  },
  {
    "text": "was saying ml engineer needs some kind of a pythonic interface for doing",
    "start": "782880",
    "end": "788160"
  },
  {
    "text": "experiments so what we tried to do was give a very high level API nothing",
    "start": "788160",
    "end": "794639"
  },
  {
    "text": "connected with kubernetes at all um it should be one API where they can just",
    "start": "794639",
    "end": "800320"
  },
  {
    "text": "talk from your python function that they have to tune right so in C we have a tune function in training operator we",
    "start": "800320",
    "end": "806600"
  },
  {
    "text": "have a train function so if you see the train function I provide some base model",
    "start": "806600",
    "end": "811760"
  },
  {
    "text": "I give location to a data set that can be in uh S3 or um your local or NFS um",
    "start": "811760",
    "end": "819800"
  },
  {
    "text": "provide how many gpus you have and and output directory where your train model",
    "start": "819800",
    "end": "824920"
  },
  {
    "text": "should lie right and that is automatically picked by the llm",
    "start": "824920",
    "end": "830360"
  },
  {
    "text": "serving using its python client right you can just say the storage URI to",
    "start": "830360",
    "end": "835639"
  },
  {
    "text": "point to the same output output directory that it has been created so either you can keep it in the PVC or you",
    "start": "835639",
    "end": "841880"
  },
  {
    "text": "can use the model registry that we have talked about earlier so this is something that has been added and and we",
    "start": "841880",
    "end": "848519"
  },
  {
    "text": "will talk during the demo time yeah hello everyone Thanks for attending",
    "start": "848519",
    "end": "856279"
  },
  {
    "text": "uh it's the last talk and you guys have stayed back so thanks for that so uh we'll start with the demo I'll first",
    "start": "856279",
    "end": "863399"
  },
  {
    "text": "give you an idea what we are planning to uh show in the demo so the idea is to show how Q flow simplifies ml workload",
    "start": "863399",
    "end": "871000"
  },
  {
    "text": "management for uh data scientists and we'll be focusing on the data scientist",
    "start": "871000",
    "end": "876199"
  },
  {
    "text": "Persona so we'll be using a notebook uh to run our ml workload uh so first of",
    "start": "876199",
    "end": "881759"
  },
  {
    "text": "all we'll install uh qf flow and once Q flow is installed we we'll connect to",
    "start": "881759",
    "end": "887120"
  },
  {
    "text": "the qflow cluster and after that we'll uh run the following workload in in a",
    "start": "887120",
    "end": "893560"
  },
  {
    "text": "notebook uh so in the workload we'll be basically downloading a Lama 3.1 8B",
    "start": "893560",
    "end": "898639"
  },
  {
    "text": "model it's a standard model uh and then we'll serve it right for",
    "start": "898639",
    "end": "905279"
  },
  {
    "text": "inference so we'll get the demo",
    "start": "905320",
    "end": "911600"
  },
  {
    "text": "started",
    "start": "915000",
    "end": "918000"
  },
  {
    "text": "okay okay so we are at the qflow Manifest repo and we'll uh basically Now",
    "start": "920399",
    "end": "927120"
  },
  {
    "text": "find a command to install all the qlow components so there's this single command that you can use and it'll take",
    "start": "927120",
    "end": "934319"
  },
  {
    "text": "care of all the qflow components so this command uh will it's as you can see it's downloading it's creating all the name",
    "start": "934319",
    "end": "940480"
  },
  {
    "text": "spaces installing all the crds and creating uh yeah and creating uh all the",
    "start": "940480",
    "end": "948199"
  },
  {
    "text": "Q flow uh sorry kubernetes components and uh once the command completes we can",
    "start": "948199",
    "end": "954560"
  },
  {
    "text": "check what has been installed in the qlow name space okay so you can see pods for uh cup",
    "start": "954560",
    "end": "962319"
  },
  {
    "text": "controller then there's kerve controller and there is ml pipeline pods and then",
    "start": "962319",
    "end": "967800"
  },
  {
    "text": "there's a pod for training operator as well so we have completely installed the qf flow stack uh which Janu just talked",
    "start": "967800",
    "end": "974199"
  },
  {
    "text": "about now um yeah so we'll connect to the qflow cluster now uh we'll be using port",
    "start": "974199",
    "end": "981720"
  },
  {
    "text": "forwarding for the demo and we'll port forward sto Ingress Gateway service and yeah",
    "start": "981720",
    "end": "990920"
  },
  {
    "text": "so now that we have Port forwarded we can access the qflow UI from our uh",
    "start": "993600",
    "end": "999519"
  },
  {
    "text": "local so yeah I'll take a pause here and I'll talk about uh the qflow UI so in",
    "start": "999519",
    "end": "1005440"
  },
  {
    "text": "the left pane you can see notebooks tensor boards volumes cab experiments",
    "start": "1005440",
    "end": "1010800"
  },
  {
    "text": "caser R Points and pipelines so uh this is basically the dashboard uh for",
    "start": "1010800",
    "end": "1017480"
  },
  {
    "text": "managing all the Q components that you deploy uh so in the interest of the demo",
    "start": "1017480",
    "end": "1022800"
  },
  {
    "text": "I'll uh head straight to the",
    "start": "1022800",
    "end": "1026319"
  },
  {
    "text": "notebooks yeah so we'll connect to this",
    "start": "1027839",
    "end": "1031880"
  },
  {
    "text": "notebook uh so the idea is to deploy uh a qf flow pipeline that will uh download",
    "start": "1033799",
    "end": "1040839"
  },
  {
    "text": "and serve the uh llama model so we'll install the uh qf flow packages uh on",
    "start": "1040839",
    "end": "1047199"
  },
  {
    "text": "our notebook first",
    "start": "1047199",
    "end": "1050840"
  },
  {
    "text": "okay so once the packages are installed uh you can see uh also can you see I",
    "start": "1052480",
    "end": "1058039"
  },
  {
    "text": "mean it's I can I can narrate for you uh you can zoom in thanks so there are two components uh",
    "start": "1058039",
    "end": "1067720"
  },
  {
    "text": "this is the first and this is the second so these These are basically Q flow pipeline components these are the",
    "start": "1067720",
    "end": "1073000"
  },
  {
    "text": "building blocks for qflow pipelines so these are essentially uh like the this",
    "start": "1073000",
    "end": "1078200"
  },
  {
    "text": "piece of code will be deployed on a container and uh we have we can configure the base image for the",
    "start": "1078200",
    "end": "1084159"
  },
  {
    "text": "container uh uh for example here we have used a q flow like Ive used a base image",
    "start": "1084159",
    "end": "1090760"
  },
  {
    "text": "and uh we can also specify what packages we want to install right so um yeah for",
    "start": "1090760",
    "end": "1097520"
  },
  {
    "text": "example in the second component we are installing case uh sorry yeah okay in the second component we are",
    "start": "1097520",
    "end": "1105559"
  },
  {
    "text": "uh installing kerve and uh kubernetes client",
    "start": "1105559",
    "end": "1110760"
  },
  {
    "text": "okay so uh the first component basically downloads a model from hugging face and",
    "start": "1113440",
    "end": "1118559"
  },
  {
    "text": "it stores it on a uh on a PVC so this is we are downloading Lama",
    "start": "1118559",
    "end": "1126080"
  },
  {
    "text": "3.1 ADB and storing it at this path we're also downloading Auto token sorry tokenizer from hugging face and saving",
    "start": "1126080",
    "end": "1132480"
  },
  {
    "text": "at the same path so uh this is a caser requirement so uh",
    "start": "1132480",
    "end": "1140679"
  },
  {
    "text": "yeah the second component uh is meant for serving the model that we have downloaded and um after like installing",
    "start": "1140679",
    "end": "1149559"
  },
  {
    "text": "all the packages that we need uh we'll create an inference server and we'll serve the model that we downloaded uh",
    "start": "1149559",
    "end": "1156120"
  },
  {
    "text": "earlier so yeah so we provided the same",
    "start": "1156120",
    "end": "1161240"
  },
  {
    "text": "location okay so now let's take a look at uh how the serving pipeline",
    "start": "1161240",
    "end": "1167280"
  },
  {
    "text": "initialization looks like so we'll declare all the components that we have uh so we have a download component we",
    "start": "1167280",
    "end": "1173640"
  },
  {
    "text": "have a serving component and there's a component for uh creating PVC so also we'll establish relationships among all",
    "start": "1173640",
    "end": "1181280"
  },
  {
    "text": "these components for example we want uh the downloading task uh to take place after",
    "start": "1181280",
    "end": "1189240"
  },
  {
    "text": "provisioning the storage like after creating PVC um",
    "start": "1189240",
    "end": "1194280"
  },
  {
    "text": "yeah so yeah we are mounting the PVC as well to",
    "start": "1194280",
    "end": "1200919"
  },
  {
    "text": "the downloading task so let's run the pipeline now so we'll go back to the qflow UI",
    "start": "1200919",
    "end": "1208640"
  },
  {
    "text": "right and you can see a dag a graph created here uh for all the components and the the PVC is already created we'll",
    "start": "1208640",
    "end": "1216000"
  },
  {
    "text": "check the logs for uh download",
    "start": "1216000",
    "end": "1219600"
  },
  {
    "text": "model yeah so the model is getting downloaded so once the model is downloaded uh we'll like wait for the",
    "start": "1221240",
    "end": "1229720"
  },
  {
    "text": "serving uh to get deployed so okay so the inference server",
    "start": "1229720",
    "end": "1235120"
  },
  {
    "text": "is also ready so now we can uh like we are ready to go for",
    "start": "1235120",
    "end": "1241080"
  },
  {
    "text": "inference uh so I'll I have Port forwarded um the predictor pod and now",
    "start": "1241080",
    "end": "1248320"
  },
  {
    "text": "we are planning to uh like send an inference request to the model that we have just uh deployed and we'll ask it a",
    "start": "1248320",
    "end": "1255039"
  },
  {
    "text": "generic question uh we'll ask it what is kubernetes okay",
    "start": "1255039",
    "end": "1260600"
  },
  {
    "text": "so it replied saying that kubernetes is an open source container orchestration system uh for automating the deployment",
    "start": "1263880",
    "end": "1270799"
  },
  {
    "text": "etc etc so looks like it has the context for what kubernetes is right so now",
    "start": "1270799",
    "end": "1276720"
  },
  {
    "text": "let's try asking it uh something more specific maybe we'll ask about cubec",
    "start": "1276720",
    "end": "1283240"
  },
  {
    "text": "con so we are asking it about the last Cube con that was",
    "start": "1285240",
    "end": "1290799"
  },
  {
    "text": "held okay so uh it says that the information is not up to date and it",
    "start": "1292120",
    "end": "1297840"
  },
  {
    "text": "only has information till 2022 uh okay now let's try asking is",
    "start": "1297840",
    "end": "1303919"
  },
  {
    "text": "asking it about cubec con Delhi uh",
    "start": "1303919",
    "end": "1311919"
  },
  {
    "text": "yeah okay so it says it right away says that it does not have information on cucon plus Cloud native con to be held",
    "start": "1316760",
    "end": "1323440"
  },
  {
    "text": "in Delhi and then it'll give a generic answer uh to our question so this is",
    "start": "1323440",
    "end": "1329480"
  },
  {
    "text": "where fine tuning comes into picture so we are now going to feed the context of",
    "start": "1329480",
    "end": "1334880"
  },
  {
    "text": "cubec con plus Cloud native con Delhi and I'll hand over the demo to uh",
    "start": "1334880",
    "end": "1341039"
  },
  {
    "text": "Gish uh welcome everyone so you could see that the standard models which are",
    "start": "1341720",
    "end": "1349640"
  },
  {
    "text": "deployed here they have some limitations the first thing is they don't have a lot",
    "start": "1349640",
    "end": "1355279"
  },
  {
    "text": "of domain knowledge and second thing is how like it starts even if it does not",
    "start": "1355279",
    "end": "1362559"
  },
  {
    "text": "know about it it starts blabbering some things about it so that's when if I want some domain knowledge to be put into the",
    "start": "1362559",
    "end": "1369520"
  },
  {
    "text": "model I need to do a process called fine-tuning so in easy words it's like",
    "start": "1369520",
    "end": "1375159"
  },
  {
    "text": "getting a already existing train model tell it that please learn more steps",
    "start": "1375159",
    "end": "1381159"
  },
  {
    "text": "which is very domain specific to my uh use case and now after uh it's been",
    "start": "1381159",
    "end": "1388520"
  },
  {
    "text": "fine- tuned like learn learned all the steps let's like figure out how much it",
    "start": "1388520",
    "end": "1393880"
  },
  {
    "text": "can uh like proide results for so in the previous step we learned how to install",
    "start": "1393880",
    "end": "1401080"
  },
  {
    "text": "and serve in this step we will learn on what we should do to fine-tune the model",
    "start": "1401080",
    "end": "1407679"
  },
  {
    "text": "so the first step is we take a pre-train model and then fine tune it with an",
    "start": "1407679",
    "end": "1413360"
  },
  {
    "text": "instruction data set so for this particular use case which is the cucon daily use case we scraped the data from",
    "start": "1413360",
    "end": "1420400"
  },
  {
    "text": "the website we added some information that who is the speaker where is it",
    "start": "1420400",
    "end": "1425640"
  },
  {
    "text": "happening when is it happening and what are the Keynotes and other stuff so it's",
    "start": "1425640",
    "end": "1431080"
  },
  {
    "text": "a very small data set uh so let's see and I'm planning to tr fine tune it for",
    "start": "1431080",
    "end": "1436279"
  },
  {
    "text": "like uh 2 3 minutes 1 minute it mostly and with only three EO and let's see how",
    "start": "1436279",
    "end": "1442200"
  },
  {
    "text": "much data can the model pull in about the domain so finally once the model has",
    "start": "1442200",
    "end": "1448960"
  },
  {
    "text": "been fine-tuned I will go back to the serving platform with this new model and",
    "start": "1448960",
    "end": "1454760"
  },
  {
    "text": "with the same case of API I'll deploy it and then inference it to get the",
    "start": "1454760",
    "end": "1460200"
  },
  {
    "text": "results so for this particular use case I'll start with a new uh notebook the",
    "start": "1460200",
    "end": "1467159"
  },
  {
    "text": "same API stand room but for the finetune I'll be using the cube flow train API so",
    "start": "1467159",
    "end": "1473200"
  },
  {
    "text": "the train API takes in uh the base model the data set and also the training",
    "start": "1473200",
    "end": "1480399"
  },
  {
    "text": "parameters has uh parameters for the API so here we're using Lama 3 as a base",
    "start": "1480399",
    "end": "1486919"
  },
  {
    "text": "model then we're using the data set which we picked from the website we're",
    "start": "1486919",
    "end": "1492080"
  },
  {
    "text": "using PFT with Laura config has the training approach and we are using some",
    "start": "1492080",
    "end": "1497720"
  },
  {
    "text": "training argument which we find we figured out from tuning for this particular use case we'll run it for",
    "start": "1497720",
    "end": "1504039"
  },
  {
    "text": "three Epoch one worker node with one GPU so once this whole process is done",
    "start": "1504039",
    "end": "1511520"
  },
  {
    "text": "which is the training operator job we have to get the model back to the original state so the standard models",
    "start": "1511520",
    "end": "1520159"
  },
  {
    "text": "are in some state but the once you do a fine tuning it moves to a adapter state so it's easy if you already have a",
    "start": "1520159",
    "end": "1527760"
  },
  {
    "text": "serving PIP plan set so get back to the original like the similar state has the standard models so this particular step",
    "start": "1527760",
    "end": "1535880"
  },
  {
    "text": "which is store model step just does that Basics uh movement it's not very complex",
    "start": "1535880",
    "end": "1541840"
  },
  {
    "text": "here once I'm able to save it I'll move it to the serve model step here again",
    "start": "1541840",
    "end": "1548520"
  },
  {
    "text": "the same serving pipeline steps which from K serve API I create an inference",
    "start": "1548520",
    "end": "1553720"
  },
  {
    "text": "service with the same storage URI location and we are ready to go",
    "start": "1553720",
    "end": "1560360"
  },
  {
    "text": "there now with these three steps again the same uh ml pipeline we accumulate",
    "start": "1560360",
    "end": "1566840"
  },
  {
    "text": "all the tasks together get a new dag up for this case and then get it",
    "start": "1566840",
    "end": "1573640"
  },
  {
    "text": "running so once we hit the run all St",
    "start": "1573640",
    "end": "1579799"
  },
  {
    "text": "cells yeah so we'll have four uh notes here in this rag uh dag so",
    "start": "1583039",
    "end": "1591000"
  },
  {
    "text": "the first one is the create PVC the second one is the finetune model then store model and ser model for the create",
    "start": "1591000",
    "end": "1597799"
  },
  {
    "text": "PVC step there are alternatives like uh model registry which is also a CU flow",
    "start": "1597799",
    "end": "1603799"
  },
  {
    "text": "component you can use that in place of uh cre PVC and also the CP can be one of",
    "start": "1603799",
    "end": "1609799"
  },
  {
    "text": "the steps here which can be before the finetune step if you want to find out what is the correct trading parameters",
    "start": "1609799",
    "end": "1616440"
  },
  {
    "text": "you want to use so in this dag it's we just use a normal uh PVC and then the fine tune step will",
    "start": "1616440",
    "end": "1624080"
  },
  {
    "text": "trigger a py job with one single node in it with one",
    "start": "1624080",
    "end": "1629640"
  },
  {
    "text": "GPU attached so we'll check the logs for this Pyon job so here that cucon Master",
    "start": "1629640",
    "end": "1638279"
  },
  {
    "text": "is the job which has been running and we can see the logs on what are the things running inside it so all the container",
    "start": "1638279",
    "end": "1645200"
  },
  {
    "text": "logs so the first step is that it goes",
    "start": "1645200",
    "end": "1651480"
  },
  {
    "text": "through loading the model then loading the tokenizer gets the data set then",
    "start": "1651480",
    "end": "1657480"
  },
  {
    "text": "evaluates what's your training parameters to be added and then actually starts training so in our particular",
    "start": "1657480",
    "end": "1665320"
  },
  {
    "text": "example we selected that I want to run this for three Epoch and that's where",
    "start": "1665320",
    "end": "1672279"
  },
  {
    "text": "the every step it lcks out what's the information what's the epoch information",
    "start": "1672279",
    "end": "1677519"
  },
  {
    "text": "one thing to note here is the loss so generally speaking the loss if you're",
    "start": "1677519",
    "end": "1683000"
  },
  {
    "text": "training something the loss should be in a downward trend has you train more and",
    "start": "1683000",
    "end": "1688720"
  },
  {
    "text": "it can be one of the parameters for you to exit if you have very minimal or acceptable loss you can use that as an",
    "start": "1688720",
    "end": "1694440"
  },
  {
    "text": "exit parameter but for this particular case we are just training it for one minute so we just selected three Bo so",
    "start": "1694440",
    "end": "1701559"
  },
  {
    "text": "once the third Epoch is completed the training job exits and we'll check the",
    "start": "1701559",
    "end": "1707080"
  },
  {
    "text": "law pod state status so the part status for this cucon",
    "start": "1707080",
    "end": "1713559"
  },
  {
    "text": "master is in completed state so that's when it moves to store model so in store",
    "start": "1713559",
    "end": "1718840"
  },
  {
    "text": "model step we just change the state back to the standardized State and then we send it to the K serve serving model so",
    "start": "1718840",
    "end": "1726919"
  },
  {
    "text": "K serve serving model takes uh using the same K serve serve API we'll deploy an",
    "start": "1726919",
    "end": "1734000"
  },
  {
    "text": "inference service and it's that the predictor pod which had just started 2 minutes ago so",
    "start": "1734000",
    "end": "1740960"
  },
  {
    "text": "this is the pod which has actually been deployed just now for this use case and",
    "start": "1740960",
    "end": "1747080"
  },
  {
    "text": "then we'll check what's the inference service status so it's also in a fully",
    "start": "1747080",
    "end": "1752399"
  },
  {
    "text": "ready state so now using this predictor pod we'll inference the same old",
    "start": "1752399",
    "end": "1758080"
  },
  {
    "text": "question which we spoke about which is can cubec con India what is cubec Con",
    "start": "1758080",
    "end": "1765519"
  },
  {
    "text": "India about so before that we'll port for this proor",
    "start": "1765519",
    "end": "1770720"
  },
  {
    "text": "part and we'll hit that question so we'll see in 1 minute of training on one",
    "start": "1770720",
    "end": "1776760"
  },
  {
    "text": "node how much details Could It",
    "start": "1776760",
    "end": "1781320"
  },
  {
    "text": "capture okay so it does start out well saying that it's going to held on first",
    "start": "1784240",
    "end": "1790480"
  },
  {
    "text": "11th and 12th of December in Delhi with cncf Foundation Technologies Etc so it",
    "start": "1790480",
    "end": "1797480"
  },
  {
    "text": "can it's able to get some extra context with just a single minute of training",
    "start": "1797480",
    "end": "1803080"
  },
  {
    "text": "and just one GPU node so with this like with this training process we",
    "start": "1803080",
    "end": "1811679"
  },
  {
    "text": "just uh like with just a very small data set and the train operator train API I",
    "start": "1811679",
    "end": "1819039"
  },
  {
    "text": "was able to achieve a domain specific model which can be used for very compact use cases and can be extended",
    "start": "1819039",
    "end": "1826679"
  },
  {
    "text": "more uh yeah that's all from the training pipeline side we have a summary",
    "start": "1826679",
    "end": "1834039"
  },
  {
    "text": "slide yeah so just to summarize right so what did we uh what did we see today so",
    "start": "1834039",
    "end": "1839960"
  },
  {
    "text": "we started with a base llm did some experiments with qlow notebooks fine",
    "start": "1839960",
    "end": "1845120"
  },
  {
    "text": "tuned using training operator did very initial inference",
    "start": "1845120",
    "end": "1850159"
  },
  {
    "text": "using KV orchestrated the entire workflow through qflow pipelines um had",
    "start": "1850159",
    "end": "1855720"
  },
  {
    "text": "a web interface via qf flow Central dashboard we can add more and more components based on your use case you",
    "start": "1855720",
    "end": "1861880"
  },
  {
    "text": "can do some kind of pre-processing if your data set is not ready using qlow spark operator we can do tuning using CP",
    "start": "1861880",
    "end": "1869159"
  },
  {
    "text": "use um qlow model registry for versioning models and then advanc R out",
    "start": "1869159",
    "end": "1874440"
  },
  {
    "text": "strategies using ker and every qflow component exposes Prometheus metrics for",
    "start": "1874440",
    "end": "1880279"
  },
  {
    "text": "monitoring right this can be consumed via third party Integrations grafana or whatever it is it which will give more",
    "start": "1880279",
    "end": "1886679"
  },
  {
    "text": "clarity on how how in Fr is doing that's",
    "start": "1886679",
    "end": "1892518"
  },
  {
    "text": "[Applause] all",
    "start": "1894690",
    "end": "1901559"
  },
  {
    "text": "sure uh so is it possible for you to compare",
    "start": "1905000",
    "end": "1910080"
  },
  {
    "text": "flight Cube flow and ml flow because there are other Alternatives also",
    "start": "1910080",
    "end": "1917600"
  },
  {
    "text": "so um so the the comparison is actually difficult it is not that straightforward",
    "start": "1921639",
    "end": "1927399"
  },
  {
    "text": "but still um From qflow perspective I would say the biggest Advantage would be",
    "start": "1927399",
    "end": "1932600"
  },
  {
    "text": "actually on the interfaces right how easy it would be for a ml engineer to",
    "start": "1932600",
    "end": "1938279"
  },
  {
    "text": "deploy models and from infra guide to actually do that monitoring so the",
    "start": "1938279",
    "end": "1943720"
  },
  {
    "text": "biggest Advantage would be how different components are connected and give a",
    "start": "1943720",
    "end": "1949279"
  },
  {
    "text": "unique platform all across so for example the training operator can push models within the code uh to have a",
    "start": "1949279",
    "end": "1956880"
  },
  {
    "text": "version model onto the model registry which can be automatically picked by the ker for example right and the entire",
    "start": "1956880",
    "end": "1965039"
  },
  {
    "text": "thing can be easily orchestrated using qlow pipelines because qlow pipelines by default it has components for the",
    "start": "1965039",
    "end": "1972120"
  },
  {
    "text": "training operator and kelf so the advantage would be that getting that single unified PL platform for different",
    "start": "1972120",
    "end": "1980399"
  },
  {
    "text": "use cases that you have in your ml Journey which is not there so in for a",
    "start": "1980399",
    "end": "1986200"
  },
  {
    "text": "beginner ml flow is really great like we um I would suggest to actually start",
    "start": "1986200",
    "end": "1991279"
  },
  {
    "text": "with ML flow U because that actually gives you a pip install and then start on it right but here the advantage would",
    "start": "1991279",
    "end": "1997840"
  },
  {
    "text": "be actually on a much bigger scale when you actually want to do much more distributed manner because this is actually used in production various",
    "start": "1997840",
    "end": "2003720"
  },
  {
    "text": "companies so I would say start small but again at the point when you actually really need scale this would be amazing",
    "start": "2003720",
    "end": "2012039"
  },
  {
    "text": "uh actually I have one more question sure uh so I see there's a still once you deploy the model now you need to",
    "start": "2012039",
    "end": "2019159"
  },
  {
    "text": "monitor it monitor the performance of the model right so are you doing those parts also so to that point right all",
    "start": "2019159",
    "end": "2026200"
  },
  {
    "text": "the components in CU flow produce Prometheus metrics be it serving be",
    "start": "2026200",
    "end": "2031399"
  },
  {
    "text": "training operator so I can see how many training jobs are being happening like how are they running are they completed",
    "start": "2031399",
    "end": "2037279"
  },
  {
    "text": "the case of side of things you can see um different runtime metrics as well for example in llms you can see you can",
    "start": "2037279",
    "end": "2043639"
  },
  {
    "text": "consume all the um TT um um time to",
    "start": "2043639",
    "end": "2048919"
  },
  {
    "text": "First token uh in token latency all these to all these metrics are automatically exposed for you to consume",
    "start": "2048919",
    "end": "2055079"
  },
  {
    "text": "so these are Prometheus metrics we don't have any specific monitoring tool as such it is up to the user to consume it",
    "start": "2055079",
    "end": "2061839"
  },
  {
    "text": "via the tools that you have like standard promethus metrics okay thank you hello yeah my name is Rahul I have",
    "start": "2061839",
    "end": "2070839"
  },
  {
    "text": "two questions quite basic uh fine tuning basically meant supplying more data to",
    "start": "2070839",
    "end": "2076839"
  },
  {
    "text": "the model that is all right that is all the second question was during fine",
    "start": "2076839",
    "end": "2083480"
  },
  {
    "text": "tuning he mentioned that llama some other model was used as a base but like",
    "start": "2083480",
    "end": "2090760"
  },
  {
    "text": "if original like uh demo that she showed if there this base model was used llama",
    "start": "2090760",
    "end": "2097320"
  },
  {
    "text": "will the performance would have been different or no no no so the what fine",
    "start": "2097320",
    "end": "2102520"
  },
  {
    "text": "tuning does is say suppose you have a 7 billion or 8 billion model with you right so it is",
    "start": "2102520",
    "end": "2108560"
  },
  {
    "text": "essentially uh a set of model weights basically you can imagine this to be um",
    "start": "2108560",
    "end": "2114599"
  },
  {
    "text": "a set of Weights associated with different um links in the neural network yes so what funing does is it Alters the",
    "start": "2114599",
    "end": "2121400"
  },
  {
    "text": "weight the architecture is still the same right so the complexity or the time",
    "start": "2121400",
    "end": "2126480"
  },
  {
    "text": "taken to uh do a single request doesn't doesn't change at all it's exactly the",
    "start": "2126480",
    "end": "2133160"
  },
  {
    "text": "same architecture but again modified weights to actually have a better it gives you a better results basically yes",
    "start": "2133160",
    "end": "2138760"
  },
  {
    "text": "yes thank you hello uh my name is m so my doubt is",
    "start": "2138760",
    "end": "2144280"
  },
  {
    "text": "b u mostly on the output part of the ker API so how it uh I saw like uh you are",
    "start": "2144280",
    "end": "2151079"
  },
  {
    "text": "uh mentioning in the uh serve uh pipeline where K uh K serve inference uh",
    "start": "2151079",
    "end": "2158839"
  },
  {
    "text": "model name and you mentioning it as hugging pH so it assumes that okay the output schema would be uh model that",
    "start": "2158839",
    "end": "2165960"
  },
  {
    "text": "that's a good question so what K of does is so there is something called open inference protocol okay which has been",
    "start": "2165960",
    "end": "2172520"
  },
  {
    "text": "created outside qu of and that was created by the K of community itself okay for um for uh the specific",
    "start": "2172520",
    "end": "2180800"
  },
  {
    "text": "requirement to have a standard across multiple run times run times so the one that you seen is a chat completions",
    "start": "2180800",
    "end": "2186359"
  },
  {
    "text": "protocol which is an open compliant protocol so you can actually use the same protocol by just changing",
    "start": "2186359",
    "end": "2192240"
  },
  {
    "text": "the host you can actually point to openai by changing the host to your service you can actually use your on",
    "start": "2192240",
    "end": "2197960"
  },
  {
    "text": "Prem or your specific deployment so that's the beauty of it right because the standard protocol that we have",
    "start": "2197960",
    "end": "2203560"
  },
  {
    "text": "created and on the chat completions We have basically used the open part directly but again for the other um",
    "start": "2203560",
    "end": "2211160"
  },
  {
    "text": "traditional models there's a standard which we have created the YOLO the the detections and all the there's a",
    "start": "2211160",
    "end": "2216440"
  },
  {
    "text": "standard protocol which we have okay okay yeah thank",
    "start": "2216440",
    "end": "2221280"
  },
  {
    "text": "you uh does CU flow support Auto scaling uh meaning will it scale up yes as the",
    "start": "2222000",
    "end": "2230000"
  },
  {
    "text": "demand goes up correct correct so for KV um it actually works on um like there",
    "start": "2230000",
    "end": "2235839"
  },
  {
    "text": "are multiple autoscaling strategies uh the most common common or widely used is",
    "start": "2235839",
    "end": "2241880"
  },
  {
    "text": "the uh demand based Auto scaling so I can say that my service will actually",
    "start": "2241880",
    "end": "2246960"
  },
  {
    "text": "take a single single replica can actually take 10 requests at a time right so 10 requests per second more",
    "start": "2246960",
    "end": "2252520"
  },
  {
    "text": "than that you scale up you scale up to a point where you can configure Max replicas right it can go till that",
    "start": "2252520",
    "end": "2258839"
  },
  {
    "text": "particular point or it can be a custom metric where um it might be on the GPU",
    "start": "2258839",
    "end": "2264160"
  },
  {
    "text": "utilization right when my GPU utilization goes Beyond 50% automatically scale up when I'm not",
    "start": "2264160",
    "end": "2270440"
  },
  {
    "text": "seeing traffic scale down right and you can say that minimum can be zero for example then all the replicas will be",
    "start": "2270440",
    "end": "2276400"
  },
  {
    "text": "deleted which actually means you free up the entire gpus right so it can it it can be a",
    "start": "2276400",
    "end": "2281839"
  },
  {
    "text": "demand based and that's the most commonly used uh strategy in Cas and uh can the single Cube flow installation",
    "start": "2281839",
    "end": "2289440"
  },
  {
    "text": "support multi-tenant uh training as well as inflence it is there by default yeah",
    "start": "2289440",
    "end": "2295359"
  },
  {
    "text": "it is controlled using um component called profiles so each user is mapped onto a single name space so that is",
    "start": "2295359",
    "end": "2301920"
  },
  {
    "text": "there by default when you install it thank you",
    "start": "2301920",
    "end": "2307599"
  },
  {
    "text": "I question like uh does uh uh does Cube flow supports integration with the",
    "start": "2308000",
    "end": "2313040"
  },
  {
    "text": "Triton inference or it has by default there is by default yeah there's a runtime for triton you don't need to do",
    "start": "2313040",
    "end": "2319280"
  },
  {
    "text": "anything you just say that put your model in some location reference that and say runtime is straight and it works",
    "start": "2319280",
    "end": "2325240"
  },
  {
    "text": "okay what are locations are supported like it S3 local and yeah all the all the major ones are there if you have a",
    "start": "2325240",
    "end": "2331440"
  },
  {
    "text": "custom storage protocol you can actually add that so that's a separate CD there okay okay and unloading loading again",
    "start": "2331440",
    "end": "2337480"
  },
  {
    "text": "everything is already it's all taken care automatically for you so if you see the inference service that he has created right it's basically a storage",
    "start": "2337480",
    "end": "2343640"
  },
  {
    "text": "location and what type of modalities oh okay that's and and say what resources I",
    "start": "2343640",
    "end": "2349680"
  },
  {
    "text": "need like I need X CPU y memory and I need 2 GPS rest all things are",
    "start": "2349680",
    "end": "2355119"
  },
  {
    "text": "automatically mapped okay and the inference grouping",
    "start": "2355119",
    "end": "2360280"
  },
  {
    "text": "is also supported right in the sense uh the GPU mapping for the yeah yeah yeah yeah yeah yeah yeah yeah so that's all",
    "start": "2360280",
    "end": "2366960"
  },
  {
    "text": "taken automatically you when so that's in the demo it is like one GPU single",
    "start": "2366960",
    "end": "2372880"
  },
  {
    "text": "node one GPU right like say for example if you're saying I need two replica you can imagine this to be there are two",
    "start": "2372880",
    "end": "2378720"
  },
  {
    "text": "different parts loading exact same model and load balancing between each other so",
    "start": "2378720",
    "end": "2385079"
  },
  {
    "text": "you automatic you can imagine this to be a throughput linearly scales with replica right so you need today you you",
    "start": "2385079",
    "end": "2392000"
  },
  {
    "text": "you are seeing some X through tomorrow you need 2 X Change Your replicas to two right you automatically get that for",
    "start": "2392000",
    "end": "2398319"
  },
  {
    "text": "free because you will yeah what essentially does is it scales up by buy one more and creates a second Port right",
    "start": "2398319",
    "end": "2406480"
  },
  {
    "text": "so when you get a request it automatically load balance to the second one as well",
    "start": "2406480",
    "end": "2413680"
  }
]