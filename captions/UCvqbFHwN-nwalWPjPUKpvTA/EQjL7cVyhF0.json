[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "all right so cool hi I'm selling Ross I",
    "start": "30",
    "end": "9379"
  },
  {
    "text": "controller tooling as well as auto scaling and metrics and I'm one of the",
    "start": "10490",
    "end": "15960"
  },
  {
    "text": "sig leads for sake auto scaling hi and I'm Michael I'm a developer advocate at redhead all right so what is auto",
    "start": "15960",
    "end": "25830"
  },
  {
    "start": "23000",
    "end": "23000"
  },
  {
    "text": "scaling so basically auto scaling is taking calculations of usage right",
    "start": "25830",
    "end": "35250"
  },
  {
    "text": "metrics like CPU memory or even things",
    "start": "35250",
    "end": "40440"
  },
  {
    "text": "like requests per second and using that to ensure that your application can",
    "start": "40440",
    "end": "48000"
  },
  {
    "text": "fulfill its service objectives and agreements right so making sure that you",
    "start": "48000",
    "end": "55350"
  },
  {
    "text": "are up all the time making sure that you can serve requests and under a certain",
    "start": "55350",
    "end": "62129"
  },
  {
    "text": "amount of time things like that so in kubernetes we have basically two kind of",
    "start": "62129",
    "end": "69770"
  },
  {
    "start": "65000",
    "end": "65000"
  },
  {
    "text": "categories of auto scaling so we have cluster auto scaling so that's creating",
    "start": "69770",
    "end": "75150"
  },
  {
    "text": "more nodes when you run out of space to schedule your pods on to and then we have application level auto scaling so",
    "start": "75150",
    "end": "82200"
  },
  {
    "text": "that's making sure your applications themselves have enough room to operate",
    "start": "82200",
    "end": "87299"
  },
  {
    "text": "in and that there's enough instances of",
    "start": "87299",
    "end": "93030"
  },
  {
    "text": "your application or your applications big enough we quickly do a show of hands",
    "start": "93030",
    "end": "98250"
  },
  {
    "text": "at that point in time who is using a cluster autoscaler all right who's using",
    "start": "98250",
    "end": "103549"
  },
  {
    "text": "HBA okay VP anyone already awesome few",
    "start": "103549",
    "end": "111110"
  },
  {
    "text": "few of you right so so for those of you who may not be familiar we call the",
    "start": "111110",
    "end": "117450"
  },
  {
    "text": "horizontal pod autoscaler so that's horizontal application auto scaling making more replicas of your application",
    "start": "117450",
    "end": "123479"
  },
  {
    "text": "we call that the HPA and vertical auto scaling that's increasing the resource",
    "start": "123479",
    "end": "129000"
  },
  {
    "text": "limits of your application we call that the VP a",
    "start": "129000",
    "end": "133879"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "do you want to did you want to alright so so the cluster autoscaler in in",
    "start": "136030",
    "end": "143930"
  },
  {
    "text": "kubernetes is a tool for increasing",
    "start": "143930",
    "end": "150250"
  },
  {
    "text": "automatically increasing and decreasing the number of nodes available and it",
    "start": "150250",
    "end": "155270"
  },
  {
    "text": "does this by talking to the cloud providers and actually running a scheduling simulation so it constantly",
    "start": "155270",
    "end": "162350"
  },
  {
    "text": "is looking at the state of the cluster figuring out whether or not",
    "start": "162350",
    "end": "167570"
  },
  {
    "text": "pods would become unschedulable and if so trying to increase the notes and",
    "start": "167570",
    "end": "173239"
  },
  {
    "text": "simultaneously if there's too many nodes and they're kind of all have lower",
    "start": "173239",
    "end": "181880"
  },
  {
    "text": "utilization then perhaps you might like it will attempt to figure out which",
    "start": "181880",
    "end": "187730"
  },
  {
    "text": "nodes can be removed without violating your pod disruption budget so without necessarily killing off a node that has",
    "start": "187730",
    "end": "196370"
  },
  {
    "text": "some critical workloads running on it even though it might be underutilized because that would not be good so moving",
    "start": "196370",
    "end": "206989"
  },
  {
    "start": "205000",
    "end": "205000"
  },
  {
    "text": "beyond cluster into the individual app level there's also a horizontal auto scaling and so the idea behind",
    "start": "206989",
    "end": "213799"
  },
  {
    "text": "horizontal auto scaling is that we declare a target value for some metrics",
    "start": "213799",
    "end": "220820"
  },
  {
    "text": "so for CPU we might declare that we constantly want our application to kind of target around",
    "start": "220820",
    "end": "226790"
  },
  {
    "text": "I know 500 mil of cores half of a core or we might say that we generally want",
    "start": "226790",
    "end": "233510"
  },
  {
    "text": "each instance of our application to be serving say 50 requests per second and",
    "start": "233510",
    "end": "241180"
  },
  {
    "text": "then at a regular interval the horizontal pot autoscaler we'll look at",
    "start": "241180",
    "end": "246950"
  },
  {
    "text": "are the actual resource usage of our application so maybe our actual requests",
    "start": "246950",
    "end": "253700"
  },
  {
    "text": "per second that a single instance is serving is a hundred or the actual CPU",
    "start": "253700",
    "end": "260120"
  },
  {
    "text": "is one core and we say that's too high we're getting into the danger zone if maybe we're going to start getting performance issues and so what it will",
    "start": "260120",
    "end": "268270"
  },
  {
    "text": "do is it will compare those that will form a ratio and we say a hundred 100 is the actual usage divided by 50 is what",
    "start": "268270",
    "end": "275650"
  },
  {
    "text": "we have desired so we scale up by a factor of two and simultaneously if we were instead each instance was only",
    "start": "275650",
    "end": "282160"
  },
  {
    "text": "serving 25 we say maybe we don't need as many instances because we know from our",
    "start": "282160",
    "end": "287430"
  },
  {
    "text": "previous knowledge that we can actually serve we can actually serve 50",
    "start": "287430",
    "end": "293110"
  },
  {
    "text": "comfortably so 25 over 50 is 1/2 so we divide the number of replicas by 2 and",
    "start": "293110",
    "end": "301750"
  },
  {
    "text": "this works really really well when you have an application that can be scaled",
    "start": "301750",
    "end": "307450"
  },
  {
    "text": "horizontally it's great for things are somewhat stateless things like web",
    "start": "307450",
    "end": "313360"
  },
  {
    "text": "applications not so great for things like databases frequently unless you",
    "start": "313360",
    "end": "318400"
  },
  {
    "text": "have an awesome horizontally scalable database there are a few of those and so",
    "start": "318400",
    "end": "324460"
  },
  {
    "start": "323000",
    "end": "323000"
  },
  {
    "text": "for something like a database oftentimes vertical auto scaling can be better so",
    "start": "324460",
    "end": "330430"
  },
  {
    "text": "the idea behind vertical auto scaling is that we look at your resource usage and",
    "start": "330430",
    "end": "336540"
  },
  {
    "text": "we figure out if you're bumping up against the limits that you've set and",
    "start": "336540",
    "end": "341710"
  },
  {
    "text": "we try to increase them so right now that process actually involves",
    "start": "341710",
    "end": "348610"
  },
  {
    "text": "restarting a pot but in the future we hope to just be able to increase in",
    "start": "348610",
    "end": "353979"
  },
  {
    "text": "place and so this works really great if you if increasing the number of replicas",
    "start": "353979",
    "end": "360430"
  },
  {
    "text": "won't necessarily solve your problem",
    "start": "360430",
    "end": "364320"
  },
  {
    "start": "365000",
    "end": "365000"
  },
  {
    "text": "alright so in the past the way auto-scaling worked is is we only really",
    "start": "366080",
    "end": "373290"
  },
  {
    "text": "in the past had horizontal auto Skilling's way back in the annals of kubernetes history and so what happened",
    "start": "373290",
    "end": "380040"
  },
  {
    "text": "was we had this tool called heap stir and heap screw would query all of the notes and expose an API and then the",
    "start": "380040",
    "end": "388860"
  },
  {
    "text": "horizontal pot autoscaler would go to heap stir and say give me all of the metrics for the containers that I care",
    "start": "388860",
    "end": "394440"
  },
  {
    "text": "about for each of the pods and we could auto scale we only auto scaled on CPU",
    "start": "394440",
    "end": "400500"
  },
  {
    "text": "and then a memory and then memory went away because it was hard to auto scale on and we could only auto scale on CPU",
    "start": "400500",
    "end": "406680"
  },
  {
    "text": "and that was the way it was for a long time and eventually people said I would",
    "start": "406680",
    "end": "416010"
  },
  {
    "text": "like to scale on custom metrics and we had a few false starts on custom metrics and we eventually settled on something",
    "start": "416010",
    "end": "422160"
  },
  {
    "text": "called the custom metrics API so the idea behind the custom metrics API is that people have very strong opinions on",
    "start": "422160",
    "end": "431580"
  },
  {
    "start": "424000",
    "end": "424000"
  },
  {
    "text": "how they should monitor their applications some people love Prometheus other people",
    "start": "431580",
    "end": "436590"
  },
  {
    "text": "love stackdriver some people love influx what-have-you",
    "start": "436590",
    "end": "442140"
  },
  {
    "text": "so we defined an API that didn't have an implementation kind of kind of the same",
    "start": "442140",
    "end": "447690"
  },
  {
    "text": "way ingress doesn't have a canonical implementation the custom metrics API does not have a canonical implementation",
    "start": "447690",
    "end": "454160"
  },
  {
    "text": "instead we have these things called adapters and adapters sit as a layer",
    "start": "454160",
    "end": "463020"
  },
  {
    "text": "between your metrics storage and the",
    "start": "463020",
    "end": "468630"
  },
  {
    "text": "Horace horizontal pot autoscaler so the horizontal pot autoscaler can go to the custom metrics API and say give me",
    "start": "468630",
    "end": "474360"
  },
  {
    "text": "custom metric X give me requests per second on this pot or give me a queue",
    "start": "474360",
    "end": "481380"
  },
  {
    "text": "length on this object and then the adapter will translate that into a query",
    "start": "481380",
    "end": "488490"
  },
  {
    "text": "to Prometheus a call to stackdriver what have you and this allows us to let",
    "start": "488490",
    "end": "494310"
  },
  {
    "text": "people bring their own monitoring and just start using what they have with the Horde other skill all right",
    "start": "494310",
    "end": "504500"
  },
  {
    "text": "probably should have mentioned in the beginning that today we're gonna give the introduction overview and tomorrow",
    "start": "504500",
    "end": "510979"
  },
  {
    "text": "yeah we will have a guest from reg Alex who will talk a little bit more about",
    "start": "510979",
    "end": "516599"
  },
  {
    "text": "predictive analytics around that topic they have an offering there and we also do Q&A today I don't know how much time",
    "start": "516599",
    "end": "523229"
  },
  {
    "text": "we will have for Q&A but yeah let's see all right vertical pot auto-scaling s",
    "start": "523229",
    "end": "529890"
  },
  {
    "start": "528000",
    "end": "528000"
  },
  {
    "text": "already mentioned there are certain use cases where the HPA stuff doesn't really",
    "start": "529890",
    "end": "536190"
  },
  {
    "text": "apply or doesn't really play that nicely and the starting point really is if you",
    "start": "536190",
    "end": "541200"
  },
  {
    "text": "look at the CMO file here essentially the and the resource specification where",
    "start": "541200",
    "end": "548880"
  },
  {
    "text": "you essentially define requests and limits and that is something that is",
    "start": "548880",
    "end": "554339"
  },
  {
    "text": "super important and very often not done because it's kind of like how should I know how much memory my application is",
    "start": "554339",
    "end": "561600"
  },
  {
    "text": "gonna use so a bit of background and terminology here before we dive deeper",
    "start": "561600",
    "end": "567750"
  },
  {
    "text": "into that the process of scheduling is essentially the part of the control plane the scheduler there that year on",
    "start": "567750",
    "end": "573900"
  },
  {
    "text": "the one hand have notes the work notes that offer resources like CPU and RAM and then you have parts that consume",
    "start": "573900",
    "end": "580380"
  },
  {
    "text": "resources right and all the scheduler really does is trying to match that right saying can I find a node or nodes",
    "start": "580380",
    "end": "587010"
  },
  {
    "text": "where the requests that these resources these parts come along with can match",
    "start": "587010",
    "end": "593400"
  },
  {
    "text": "and I can schedule it there the rest isn't taking care of my the cubelet on the respective node there are different",
    "start": "593400",
    "end": "601589"
  },
  {
    "text": "kinds of resources if you think about memory that is something that is incompressible you cannot just say okay",
    "start": "601589",
    "end": "607800"
  },
  {
    "text": "I'm gonna randomly take some pages away and hope that the whole process still running whereas CPU is compressible right you",
    "start": "607800",
    "end": "614520"
  },
  {
    "text": "can have assigned fewer CPU cycles to a process was still perfectly run just",
    "start": "614520",
    "end": "621930"
  },
  {
    "text": "slower so you have to look at your at your the type of research that you're",
    "start": "621930",
    "end": "627390"
  },
  {
    "text": "dealing with here and most importantly the thinking here the underlying thinking here is the so-called quality of service so for",
    "start": "627390",
    "end": "634790"
  },
  {
    "text": "example if you take resource limits and resource requests and you set them to equal values you will have a Q s of",
    "start": "634790",
    "end": "641630"
  },
  {
    "text": "guaranteed right this will will be there you have you can have the limits setting",
    "start": "641630",
    "end": "648350"
  },
  {
    "text": "larger than having a higher value than the request CPU and memory and and vast",
    "start": "648350",
    "end": "655130"
  },
  {
    "text": "0 then it's so-called burst of all Q s and if you don't set them which a lot of",
    "start": "655130",
    "end": "660589"
  },
  {
    "text": "people are fortunate still do then it's essentially on the best effort basis right anything can happen we don't give",
    "start": "660589",
    "end": "666649"
  },
  {
    "text": "any or Kalidas doesn't give any guarantees there so you really want to have a predictable system which implies",
    "start": "666649",
    "end": "672709"
  },
  {
    "text": "you should and must be setting the resource limits and request state now",
    "start": "672709",
    "end": "682550"
  },
  {
    "start": "682000",
    "end": "682000"
  },
  {
    "text": "the question is where does that come from right and I just pointed out to blog post from end users who essentially",
    "start": "682550",
    "end": "690199"
  },
  {
    "text": "complained about that very rightfully so if you look at the timeline it's a bit",
    "start": "690199",
    "end": "695360"
  },
  {
    "text": "you know to two years or whatever but it's still very much true that unfortunately they didn't use to be any",
    "start": "695360",
    "end": "703459"
  },
  {
    "text": "anything there that would help guide users to actually come up with these resource limits and resource requests",
    "start": "703459",
    "end": "710389"
  },
  {
    "text": "there so the goal really is in a sense that we can automatically without any",
    "start": "710389",
    "end": "716389"
  },
  {
    "start": "711000",
    "end": "711000"
  },
  {
    "text": "human intervention figure out what the optimal requests and limits they are",
    "start": "716389",
    "end": "722329"
  },
  {
    "text": "because manually it's it's just not really manageable and with that we have a quality of service or guaranteed",
    "start": "722329",
    "end": "730009"
  },
  {
    "text": "quality of service otherwise you end up in a best-effort case the other hand we want to improve the utilization",
    "start": "730009",
    "end": "735410"
  },
  {
    "text": "obviously the more we can pin back on a certain note the higher utilization gets",
    "start": "735410",
    "end": "742220"
  },
  {
    "text": "of course there's always a certain you want to leave a certain Headroom you don't want to go up to a hundred percent",
    "start": "742220",
    "end": "747649"
  },
  {
    "text": "but let's say 80 90 percent is great and you also want to have the impact on the",
    "start": "747649",
    "end": "754880"
  },
  {
    "text": "other function it is like whom handling or yeah enabling it still I think not",
    "start": "754880",
    "end": "761760"
  },
  {
    "text": "really optimizing scheduler but is there already PRF I have no idea but it should",
    "start": "761760",
    "end": "766829"
  },
  {
    "text": "should enable that as well hopefully at some point in time as already mentioned",
    "start": "766829",
    "end": "772589"
  },
  {
    "start": "770000",
    "end": "770000"
  },
  {
    "text": "by solid earlier on one typical use case where it is any kind of stateful apps",
    "start": "772589",
    "end": "778350"
  },
  {
    "text": "think of database like my cycle or or Postgres or anything that kind of",
    "start": "778350",
    "end": "784560"
  },
  {
    "text": "packages that like WordPress that you know under the hood also uses a database I will point out at this point in time",
    "start": "784560",
    "end": "791339"
  },
  {
    "text": "and you already said that given that we have currently no way to in place",
    "start": "791339",
    "end": "797540"
  },
  {
    "text": "replace the part essentially means a new part with the new effective resource",
    "start": "797540",
    "end": "803190"
  },
  {
    "text": "limits will be created this first statement is not entirely true it is only true if you can actually",
    "start": "803190",
    "end": "810630"
  },
  {
    "text": "manage the state in a way that you either make sure that it is scheduled on the same node or that the data goes",
    "start": "810630",
    "end": "818160"
  },
  {
    "text": "where the new part potentially will be scheduled there are proposals or this",
    "start": "818160",
    "end": "823589"
  },
  {
    "text": "one proposal I'm aware of that's together with or in sick node I believe where it essentially boils down to can",
    "start": "823589",
    "end": "830610"
  },
  {
    "text": "the underlying a container runtime change to C group settings on a running",
    "start": "830610",
    "end": "835889"
  },
  {
    "text": "container as long as that is not the case then obviously we need to fall back into what we currently have creating a",
    "start": "835889",
    "end": "841410"
  },
  {
    "text": "new part with the new limits but there is there is a proposal in place and we what do hope to eventually have in place",
    "start": "841410",
    "end": "847589"
  },
  {
    "text": "resizing yeah and definitely if someone from sick node is round and dares to",
    "start": "847589",
    "end": "853019"
  },
  {
    "text": "raise their hand okay so this is really something that at least for me I mean we",
    "start": "853019",
    "end": "859260"
  },
  {
    "text": "when we initially and was mainly Google in implemented an oauth2 implementation",
    "start": "859260",
    "end": "865649"
  },
  {
    "text": "there it was clearly called out this is a current limitation no limitation but",
    "start": "865649",
    "end": "870899"
  },
  {
    "text": "it's better than having nothing even the recommendations already give you some some guidance there to me personally at",
    "start": "870899",
    "end": "876990"
  },
  {
    "text": "the current point in time with the current implementation maybe the second bullet point is even more interesting",
    "start": "876990",
    "end": "882839"
  },
  {
    "text": "and that is if you have a you know legacy app that's called a dead so you",
    "start": "882839",
    "end": "888029"
  },
  {
    "text": "have some monolith that you just want to lift and shift into communities so you might not have the source code for it",
    "start": "888029",
    "end": "895139"
  },
  {
    "text": "might not be able to you know reaaargh attacked rate or whatever reasons you just take an existing binary or whatever",
    "start": "895139",
    "end": "902160"
  },
  {
    "text": "and you put it into a container and that is something where in this case you simply cannot use a horizontal part",
    "start": "902160",
    "end": "909149"
  },
  {
    "text": "autoscaler because there is nothing to you know you it doesn't make sense to have multiple instances or replicas",
    "start": "909149",
    "end": "914999"
  },
  {
    "text": "running in that but you definitely have but with growing traffic you have more",
    "start": "914999",
    "end": "920220"
  },
  {
    "text": "resource needs and with that you would need to allocate more memory or End or",
    "start": "920220",
    "end": "925319"
  },
  {
    "text": "CPU and for that I think at the current point in time this is certainly something if you have that case lift and",
    "start": "925319",
    "end": "932040"
  },
  {
    "text": "shift of an existing one of the where you can't react attacked her you probably want to have a look at the vp8",
    "start": "932040",
    "end": "938189"
  },
  {
    "text": "vertical part autoscaler a quick look at how this actually works internally the",
    "start": "938189",
    "end": "946079"
  },
  {
    "start": "940000",
    "end": "940000"
  },
  {
    "text": "API server the control plane most importantly has the API server in there everything goes through the API server",
    "start": "946079",
    "end": "952379"
  },
  {
    "text": "everyone talks through the API server and that are on a principled level e the",
    "start": "952379",
    "end": "957480"
  },
  {
    "text": "faces they are right you get a HTTP request with adjacent payload in there there is some authentication",
    "start": "957480",
    "end": "962519"
  },
  {
    "text": "authorization verifying who that entity is and if that entity is allowed to carry out the operation like you know",
    "start": "962519",
    "end": "969419"
  },
  {
    "text": "creating a part or creating a service and then this step the yellow or little",
    "start": "969419",
    "end": "975299"
  },
  {
    "text": "step this is the place where you can actually we have a chance in that flow and that request flow to change the",
    "start": "975299",
    "end": "982290"
  },
  {
    "text": "actual representation of that respective resource or object in this case before",
    "start": "982290",
    "end": "987480"
  },
  {
    "text": "it actually eventually gets get stored in that CD so here you can for example say oh the user didn't set any request",
    "start": "987480",
    "end": "995429"
  },
  {
    "text": "limits I know what to do and I at this point in time mutate I change the",
    "start": "995429",
    "end": "1000639"
  },
  {
    "text": "definition of their specification of that resource for example the part in this case at this point in time and then",
    "start": "1000639",
    "end": "1007009"
  },
  {
    "text": "I go further on validated and so on and stored eventually in EDD in the API server so this is the basic trick here",
    "start": "1007009",
    "end": "1013970"
  },
  {
    "text": "that we are leveraging so overall how is that implemented obviously we in order",
    "start": "1013970",
    "end": "1021350"
  },
  {
    "start": "1016000",
    "end": "1016000"
  },
  {
    "text": "to come up with a recommendation we for for the resource limits and requests we need to",
    "start": "1021350",
    "end": "1026790"
  },
  {
    "text": "serve what is that application actually using and that is yeah done and and come",
    "start": "1026790",
    "end": "1034380"
  },
  {
    "text": "comes up built up this historic profile that's the recommender part and then optionally you don't have to you can",
    "start": "1034380",
    "end": "1042089"
  },
  {
    "text": "apply that you can say okay now in force they've now set the actual limits and with that you would at the current point",
    "start": "1042090",
    "end": "1048360"
  },
  {
    "text": "in time create a new part and that is on a very high level D the architecture",
    "start": "1048360",
    "end": "1053550"
  },
  {
    "start": "1050000",
    "end": "1050000"
  },
  {
    "text": "there as you can see there are a lot of moving components that is no doubt I",
    "start": "1053550",
    "end": "1060510"
  },
  {
    "text": "guess part of the the motivation for the design was to keep it flexible but also",
    "start": "1060510",
    "end": "1065940"
  },
  {
    "text": "scalable and if you want to talk more about that architecture at this point in",
    "start": "1065940",
    "end": "1071910"
  },
  {
    "text": "time or yeah I mean I can talk a little bit more about it so the basic idea like",
    "start": "1071910",
    "end": "1078060"
  },
  {
    "text": "Michael mentioned is that we have a recommendation engine and which is kind",
    "start": "1078060",
    "end": "1083580"
  },
  {
    "text": "of the heart and so the recommendation engine periodic will will fetch and",
    "start": "1083580",
    "end": "1091860"
  },
  {
    "text": "periodically update a recommendation based on current metrics and so it will look at it will look at the metrics we",
    "start": "1091860",
    "end": "1098250"
  },
  {
    "text": "have available and when it starts up if historical storage is available through",
    "start": "1098250",
    "end": "1104520"
  },
  {
    "text": "something like Prometheus it will attempt to pull a bunch of historical metrics and via Prometheus and then as",
    "start": "1104520",
    "end": "1111060"
  },
  {
    "text": "it runs it will also periodically attempt to pull the resource metrics API",
    "start": "1111060",
    "end": "1116460"
  },
  {
    "text": "so generally provided by metric server potentially provided by Prometheus as well and update its recommendations look",
    "start": "1116460",
    "end": "1125730"
  },
  {
    "text": "at what was used previously and figure out what it should probably be using now based on you know how much it's using",
    "start": "1125730",
    "end": "1132300"
  },
  {
    "text": "how close it is to its limits and then there's there's two steps so the",
    "start": "1132300",
    "end": "1137760"
  },
  {
    "text": "recommendation is always actually applied when a pod comes in so a pod comes in for creation it goes through",
    "start": "1137760",
    "end": "1146610"
  },
  {
    "text": "that API server workflow it hits the mutating admission web hook which then",
    "start": "1146610",
    "end": "1151950"
  },
  {
    "text": "calls out looks at the recommendation for that pod if there is one and applies that and so that that workflow can be",
    "start": "1151950",
    "end": "1159150"
  },
  {
    "text": "used by itself so you can just when I start up my pods I'm not gonna manually set a request in a limit I'm",
    "start": "1159150",
    "end": "1166320"
  },
  {
    "text": "going to let the vertical pot autoscaler recommend request or a limit for me but",
    "start": "1166320",
    "end": "1173700"
  },
  {
    "text": "you can also say look for things that are currently under stress right and so",
    "start": "1173700",
    "end": "1180750"
  },
  {
    "text": "the vertical pot autoscaler controller will periodically look for pods that are",
    "start": "1180750",
    "end": "1187440"
  },
  {
    "text": "using more than they should be whose recommendation is off from what they are actually set as and it will evict them",
    "start": "1187440",
    "end": "1195240"
  },
  {
    "text": "and then whatever is controlling that pot whatever owns that pod say a replica sat as part of a deployment well restart",
    "start": "1195240",
    "end": "1203820"
  },
  {
    "text": "a new pod create a new pod and it will go through that flow again and we'll hit the recommendation and then you'll get a new pod with the right resources cool",
    "start": "1203820",
    "end": "1212310"
  },
  {
    "text": "thank you already mentioned that we are still in a rod or early phase I don't",
    "start": "1212310",
    "end": "1219180"
  },
  {
    "start": "1214000",
    "end": "1214000"
  },
  {
    "text": "actually know the actual current state status with 1.13 but I think it's still not alpha right we didn't I think it's",
    "start": "1219180",
    "end": "1225990"
  },
  {
    "text": "alpha is it also okay sorry almost beta I like your optimism",
    "start": "1225990",
    "end": "1233910"
  },
  {
    "text": "so it's let's agree it's itself okay cool and I think that the number one asked",
    "start": "1233910",
    "end": "1240930"
  },
  {
    "text": "here is especially for people who are ready either looking into using it or already using it give us feedback write",
    "start": "1240930",
    "end": "1247410"
  },
  {
    "text": "any kind of data points that we can get to see in what direction because",
    "start": "1247410",
    "end": "1253050"
  },
  {
    "text": "obviously the the current efforts are on stabilizing that and once the API is stable well you can't really expect",
    "start": "1253050",
    "end": "1260730"
  },
  {
    "text": "changes anymore right so now it's the time I would argue to you know be loud",
    "start": "1260730",
    "end": "1266580"
  },
  {
    "text": "about it and then share your opinions write a blog post come come to us and tell us especially if you have a use",
    "start": "1266580",
    "end": "1273900"
  },
  {
    "text": "case that's just very clearly not met by the current implementation of the vertical pot autoscaler if that use",
    "start": "1273900",
    "end": "1282510"
  },
  {
    "text": "cases I need in place updates we are aware of that one yeah but like if you have something else that's very clearly",
    "start": "1282510",
    "end": "1288660"
  },
  {
    "text": "like you're like oh it looked like the vertical up pot autoscaler might work and then I tried to apply it",
    "start": "1288660",
    "end": "1294240"
  },
  {
    "text": "and there was just some complete impedance mismatch with my use case or",
    "start": "1294240",
    "end": "1299450"
  },
  {
    "text": "everything caught on fire and my cluster exploded like let us know because we're",
    "start": "1299450",
    "end": "1304980"
  },
  {
    "text": "really interested in those so that we can update our design change how we think about the problem before we go to",
    "start": "1304980",
    "end": "1311250"
  },
  {
    "text": "beta NGA right I guess the majority of you know formal informal feedback so far",
    "start": "1311250",
    "end": "1317730"
  },
  {
    "text": "is essentially the experience that Google internally has with autopilot where I don't know 60% of whatever the",
    "start": "1317730",
    "end": "1323760"
  },
  {
    "text": "users actually use that but that is you know you can't really translate that one-to-one here so we really depend on",
    "start": "1323760",
    "end": "1329910"
  },
  {
    "text": "you know feedback from the community from from you guys out there to know is that the right decision or should we",
    "start": "1329910",
    "end": "1336780"
  },
  {
    "text": "change something there and now I think we still have time for a little demo",
    "start": "1336780",
    "end": "1342390"
  },
  {
    "text": "that Saul you're accordance I think in an earlier version let's quickly look at",
    "start": "1342390",
    "end": "1348720"
  },
  {
    "text": "that okay do you want to voice it or",
    "start": "1348720",
    "end": "1357320"
  },
  {
    "text": "okay so what we see here with me what we",
    "start": "1357650",
    "end": "1363990"
  },
  {
    "text": "see here on the left hand side it's definitely other side is a committees",
    "start": "1363990",
    "end": "1369780"
  },
  {
    "text": "cluster and other side the the command-line interface and essentially",
    "start": "1369780",
    "end": "1375990"
  },
  {
    "text": "we have something prepared there that we want to use or apply the TVPA on so",
    "start": "1375990",
    "end": "1381870"
  },
  {
    "text": "let's have a look at the the actual resource the statement there the",
    "start": "1381870",
    "end": "1387120"
  },
  {
    "text": "manifest there and if you look at you can't really see it nicely there but the",
    "start": "1387120",
    "end": "1393080"
  },
  {
    "text": "the goal here would be the hundred milli CPU and 50 megabytes memory and so this",
    "start": "1393080",
    "end": "1400920"
  },
  {
    "text": "is the the target deployment and WPA in there and the manifest and now we essentially apply or create that in the",
    "start": "1400920",
    "end": "1409440"
  },
  {
    "text": "first step and as we would expect the this example hamster app comes up and at",
    "start": "1409440",
    "end": "1415860"
  },
  {
    "text": "some point in time we should see what is going on there in terms of what what it",
    "start": "1415860",
    "end": "1421830"
  },
  {
    "text": "actually uses and after some more time okay there you",
    "start": "1421830",
    "end": "1428330"
  },
  {
    "text": "see the actual hundred Willy course and fifty megabytes right and now having a",
    "start": "1428330",
    "end": "1437030"
  },
  {
    "text": "look at the status of the VP a so it is in in the update mode mode Auto which we",
    "start": "1437030",
    "end": "1443780"
  },
  {
    "text": "didn't really talk about a lot but might have still some time there that's essentially the policy that you can",
    "start": "1443780",
    "end": "1450350"
  },
  {
    "text": "choose how to go about things and a little further in",
    "start": "1450350",
    "end": "1458470"
  },
  {
    "text": "takes a bit okay and now you see the actual change there right so you have",
    "start": "1463720",
    "end": "1469600"
  },
  {
    "text": "for the recommendation the lower bound the target and the upper pound pound that is the recommendation we got out of",
    "start": "1469600",
    "end": "1475900"
  },
  {
    "text": "the BPA there that you can choose to apply or you know you might might just",
    "start": "1475900",
    "end": "1482650"
  },
  {
    "text": "take that into account and and do something manner with it but at least you have a based on the data a clear",
    "start": "1482650",
    "end": "1488530"
  },
  {
    "text": "indication on what your application actually consumes I don't know really",
    "start": "1488530",
    "end": "1496150"
  },
  {
    "text": "think there's much more there right we eventually see the EPA actually apply",
    "start": "1496150",
    "end": "1502510"
  },
  {
    "text": "the recommendation okay yeah yeah and so you can you can see there that the EPA",
    "start": "1502510",
    "end": "1509020"
  },
  {
    "text": "has applied the CPU and the memory recommendations to the pod the potted",
    "start": "1509020",
    "end": "1514150"
  },
  {
    "text": "was restarted sometime in the middle of that skip and we now see things have",
    "start": "1514150",
    "end": "1521530"
  },
  {
    "text": "changed great okay I think with that we",
    "start": "1521530",
    "end": "1527080"
  },
  {
    "text": "are almost at the end I think it's just a few pointers therein in case you you",
    "start": "1527080",
    "end": "1533740"
  },
  {
    "text": "do have feedback and and want to get engaged so we do still meet more or less",
    "start": "1533740",
    "end": "1538840"
  },
  {
    "text": "every Monday on on on Zoo and there is a",
    "start": "1538840",
    "end": "1544300"
  },
  {
    "text": "rather active now SiC auto-scaling slack channel on the communities select",
    "start": "1544300",
    "end": "1550179"
  },
  {
    "text": "community and yeah yeah we try to always respond to things on socata scaling if",
    "start": "1550179",
    "end": "1559510"
  },
  {
    "text": "you find you message some someone and they have an or ask a question no one's",
    "start": "1559510",
    "end": "1564730"
  },
  {
    "text": "responded feel free to add mention us but we try to respond and we have pretty",
    "start": "1564730",
    "end": "1570940"
  },
  {
    "text": "decent time zone coverage between some of the folks in Poland and I'm out here",
    "start": "1570940",
    "end": "1579760"
  },
  {
    "text": "now so definitely if like a pack would definitely think could need a bit Mia",
    "start": "1579760",
    "end": "1586720"
  },
  {
    "text": "coverage in this little part of the world right so it's true yeah somewhere",
    "start": "1586720",
    "end": "1592150"
  },
  {
    "text": "from Asia Australia somewhere in that area and to help out or even get more engaged and",
    "start": "1592150",
    "end": "1598609"
  },
  {
    "text": "contribute definitely hang out and slack trying to set meetings and some of us do",
    "start": "1598609",
    "end": "1604609"
  },
  {
    "text": "just keep weird hours and respond to slack messages I mean all the night though yeah okay I think that's pretty",
    "start": "1604609",
    "end": "1611899"
  },
  {
    "text": "much it what we had for today so I don't know how much time we have five minutes",
    "start": "1611899",
    "end": "1618589"
  },
  {
    "text": "does anyone know is it five minutes I think so I actually have two jet sorry",
    "start": "1618589",
    "end": "1626109"
  },
  {
    "text": "eight minutes I take the eight minutes so then we can actually take questions already today if there are any right",
    "start": "1626109",
    "end": "1632690"
  },
  {
    "text": "away and as I said tomorrow it and the deep dive fizzing in the afternoon 2:30 whatever it there is some all right",
    "start": "1632690",
    "end": "1638809"
  },
  {
    "text": "let's go just go front back so we have a microphone here how the recommender",
    "start": "1638809",
    "end": "1655580"
  },
  {
    "text": "works how does it a life is historical data all capacitor is this simple",
    "start": "1655580",
    "end": "1661129"
  },
  {
    "text": "statistical analysis is so much learning",
    "start": "1661129",
    "end": "1665440"
  },
  {
    "text": "I believe it's just looking at the",
    "start": "1668799",
    "end": "1677359"
  },
  {
    "text": "percentage of usage or any of the BPA",
    "start": "1677359",
    "end": "1683059"
  },
  {
    "text": "folks in the audience right now",
    "start": "1683059",
    "end": "1686200"
  },
  {
    "text": "so basically at the moment it's towards eight days of usage temples in a",
    "start": "1703440",
    "end": "1709810"
  },
  {
    "text": "histogram and then it takes a very high percentile of that pencil target recommendation some statistical know",
    "start": "1709810",
    "end": "1717430"
  },
  {
    "text": "machine learning at the moment and we're seeing how it turns out awesome thanks",
    "start": "1717430",
    "end": "1722730"
  },
  {
    "start": "1722000",
    "end": "1722000"
  },
  {
    "text": "alright so I think yeah I was wondering",
    "start": "1722730",
    "end": "1730900"
  },
  {
    "text": "if you guys and already you can see the somehow combination of the HPA and vp8",
    "start": "1730900",
    "end": "1737020"
  },
  {
    "text": "because they've did that use case that's a common question it works in some cases",
    "start": "1737020",
    "end": "1747430"
  },
  {
    "text": "especially if you're using different metrics if you're scaling on the same",
    "start": "1747430",
    "end": "1753280"
  },
  {
    "text": "metrics then you run into problems because the HPA is gonna try to optimize",
    "start": "1753280",
    "end": "1758980"
  },
  {
    "text": "on one access and then the vpa is also trying to use those signals to optimize",
    "start": "1758980",
    "end": "1764800"
  },
  {
    "text": "on a different axis but i think there have been a couple of users that have had limited success in in scaling on",
    "start": "1764800",
    "end": "1772060"
  },
  {
    "text": "difference so they might scale on HPA and requests per second and vpa scaling on memory or something like that yeah",
    "start": "1772060",
    "end": "1780700"
  },
  {
    "text": "for us by the way I'm Emma from the JD drama we're running one of the largest the kubernetes cluster in production we",
    "start": "1780700",
    "end": "1786010"
  },
  {
    "text": "have a half media continents are all skinny very important so so why use",
    "start": "1786010",
    "end": "1791080"
  },
  {
    "text": "cases for us is more yeah when I could have EPA and it's simple right easy for stateful application but sometimes it's",
    "start": "1791080",
    "end": "1797560"
  },
  {
    "text": "limit here constraint by your local resource right you cannot and scre it unlimited also so we could do in the",
    "start": "1797560",
    "end": "1803440"
  },
  {
    "text": "wave here sometimes and but up to a certain limit then we have to do and yeah HPA I just think it may be",
    "start": "1803440",
    "end": "1810400"
  },
  {
    "text": "architecture framework see yeah well you knew fly that yeah I I",
    "start": "1810400",
    "end": "1816570"
  },
  {
    "text": "think that's that's definitely an interesting use case it's I I don't think an architecture that we support",
    "start": "1816570",
    "end": "1822600"
  },
  {
    "text": "right now but it's definitely something to add to the list of things to look at",
    "start": "1822600",
    "end": "1827940"
  },
  {
    "text": "in the future and I definitely would encourage you to write up stuff if you can share half either you know",
    "start": "1827940",
    "end": "1833760"
  },
  {
    "text": "definitely post or whatever I can block them or Google yeah that would be",
    "start": "1833760",
    "end": "1839100"
  },
  {
    "text": "awesome if you guys have a design that you think would be awesome like you could even submit that submit that as a",
    "start": "1839100",
    "end": "1844799"
  },
  {
    "text": "cap a kubernetes enhancement depends on you know what you have and how much time",
    "start": "1844799",
    "end": "1852600"
  },
  {
    "text": "and resources you can allocate and what you can share yeah right who else had",
    "start": "1852600",
    "end": "1861570"
  },
  {
    "text": "questions alright so back there hi",
    "start": "1861570",
    "end": "1867480"
  },
  {
    "text": "can I ask a question about HP a sure is there plans or future plans to add the",
    "start": "1867480",
    "end": "1872909"
  },
  {
    "text": "feature to scale to 0 and back up from 0 oh boy yeah so that that's a fairly",
    "start": "1872909",
    "end": "1880320"
  },
  {
    "text": "common question there's not a plan right now scaling to 0 it turns out is very",
    "start": "1880320",
    "end": "1891210"
  },
  {
    "text": "tricky because it's really hard to scale back from 0 and accidentally scaling",
    "start": "1891210",
    "end": "1897480"
  },
  {
    "text": "your app into a place where it just gets stuck and then sitting there for a while is not conducive to not getting paged at",
    "start": "1897480",
    "end": "1906659"
  },
  {
    "text": "3 in the morning which is one of the reasons we've avoided it that's far the other reason is when we started out with",
    "start": "1906659",
    "end": "1912299"
  },
  {
    "text": "CPU it's hard to scale on CPU to zero because CPU never really goes to full-on",
    "start": "1912299",
    "end": "1918870"
  },
  {
    "text": "zero so so it is something that people",
    "start": "1918870",
    "end": "1926010"
  },
  {
    "text": "have asked about before and if again kind of if you have a good design that",
    "start": "1926010",
    "end": "1931710"
  },
  {
    "text": "you think would work well I'd love to hear it so if you're just being shy or I",
    "start": "1931710",
    "end": "1938640"
  },
  {
    "text": "mean doesn't Canadian do that somehow no yeah I think that might be an option",
    "start": "1938640",
    "end": "1943650"
  },
  {
    "text": "right yes yes make sure one two three just to make sure you didn't know so one of our",
    "start": "1943650",
    "end": "1954110"
  },
  {
    "text": "biggest blockers from to moving to kubernetes is scaling down our pots to",
    "start": "1954110",
    "end": "1961910"
  },
  {
    "text": "have pots are performing critical operations and cannot ever be sat down",
    "start": "1961910",
    "end": "1968720"
  },
  {
    "text": "arbitrarily I noticed a similar problem with the game server guy what they had",
    "start": "1968720",
    "end": "1976460"
  },
  {
    "text": "to was interesting they had to do their own I think replication controllers and their own see artists to solve a simple",
    "start": "1976460",
    "end": "1982220"
  },
  {
    "text": "problem I want you to scale down but this pot in this pot do not touch please",
    "start": "1982220",
    "end": "1987800"
  },
  {
    "text": "so we have exactly that a use case is that something you've thought about or",
    "start": "1987800",
    "end": "1994450"
  },
  {
    "text": "or or you have options for me to on how to solve so if you if you don't want",
    "start": "1994450",
    "end": "2003280"
  },
  {
    "text": "particular pods to be auto scales I would put them in a different like",
    "start": "2003280",
    "end": "2010240"
  },
  {
    "text": "managed under different objects I'm maybe I'm misunderstanding your use case",
    "start": "2010240",
    "end": "2016950"
  },
  {
    "text": "yes",
    "start": "2027960",
    "end": "2030960"
  },
  {
    "text": "but those so there's there's a couple",
    "start": "2042280",
    "end": "2054340"
  },
  {
    "text": "ways to deal with this depending on how long your critical operation is you can",
    "start": "2054340",
    "end": "2061388"
  },
  {
    "text": "always set liveness and and readiness and then also set make sure to set",
    "start": "2061389",
    "end": "2067169"
  },
  {
    "text": "termination handlers but that doesn't always help in your use case we also we",
    "start": "2067169",
    "end": "2083110"
  },
  {
    "text": "do have some we have some priority features we could probably improve those",
    "start": "2083110",
    "end": "2089200"
  },
  {
    "text": "to deal with that use case a little bit better come talk to me at the deep dive",
    "start": "2089200",
    "end": "2094510"
  },
  {
    "text": "if you if you are able to make it or send me a message because that is that",
    "start": "2094510",
    "end": "2099880"
  },
  {
    "text": "is something that we could deal with a lot better well but you won't always",
    "start": "2099880",
    "end": "2109120"
  },
  {
    "text": "know like right like if you're to the to go back to the original question right you have some number of workers that are",
    "start": "2109120",
    "end": "2115630"
  },
  {
    "text": "part of the same deployment and they're pulling items off of the queue and you don't want to kill the ones that are",
    "start": "2115630",
    "end": "2120940"
  },
  {
    "text": "currently processing things so you can't just dynamically move them to a different you don't you don't",
    "start": "2120940",
    "end": "2138430"
  },
  {
    "text": "necessarily want like it might be two it",
    "start": "2138430",
    "end": "2144220"
  },
  {
    "text": "might it might not necessarily be specifically two it's just whichever ones are currently actively working on the workload I mean audit you create an",
    "start": "2144220",
    "end": "2155020"
  },
  {
    "text": "individual autoscaler object targeting a particular scaleable so a deployment or whatever so you can always just not",
    "start": "2155020",
    "end": "2162340"
  },
  {
    "text": "create an auto scaler right but in the case where you don't necessarily opry or",
    "start": "2162340",
    "end": "2168460"
  },
  {
    "text": "you know what your pods are that you don't want to touch it's a little bit harder so so right now",
    "start": "2168460",
    "end": "2190110"
  },
  {
    "text": "we don't have a great signal in kubernetes to indicate that is part of",
    "start": "2190110",
    "end": "2196020"
  },
  {
    "text": "the problem there is not a plan that I",
    "start": "2196020",
    "end": "2201830"
  },
  {
    "text": "know of off the top of my head but that's certainly something worth investigating",
    "start": "2201830",
    "end": "2207890"
  },
  {
    "text": "all right actually I just wanted to point out I'm the auto-scaling",
    "start": "2207890",
    "end": "2213450"
  },
  {
    "text": "working group lead for cañedo and so if you want to talk about scale zero running HBA and VP a at the same time or",
    "start": "2213450",
    "end": "2220860"
  },
  {
    "text": "we actually we deal with the work in flight problem as well so we have like",
    "start": "2220860",
    "end": "2226680"
  },
  {
    "text": "hooks so that existing HTTP requests will finish before we kill upon yes yeah",
    "start": "2226680",
    "end": "2234420"
  },
  {
    "text": "you should oh I don't know if I'm gonna be at the deep dive I have a talk at 2:35 on auto-scaling punkin native",
    "start": "2234420",
    "end": "2241290"
  },
  {
    "text": "tomorrow 2:35 yeah it's called zero to infinity",
    "start": "2241290",
    "end": "2247850"
  },
  {
    "text": "yeah yeah sure I don't know sounds great all right that's it see you tomorrow",
    "start": "2252550",
    "end": "2259850"
  },
  {
    "text": "2:30 right thank you Hey",
    "start": "2259850",
    "end": "2264430"
  }
]