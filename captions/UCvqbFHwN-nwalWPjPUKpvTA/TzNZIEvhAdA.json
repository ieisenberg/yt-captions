[
  {
    "start": "0",
    "end": "463000"
  },
  {
    "text": "okay give it up please for Rob's Killington thanks Brian yeah so",
    "start": "0",
    "end": "10230"
  },
  {
    "text": "definitely was an interesting decision that we made I try to convince myself",
    "start": "10230",
    "end": "18390"
  },
  {
    "text": "every night that it was correct so anyway I just wanted to yeah say thanks",
    "start": "18390",
    "end": "24180"
  },
  {
    "text": "for coming today I'm going to talk to you about deep linking metrics and traces with open telemetry open metrics",
    "start": "24180",
    "end": "31529"
  },
  {
    "text": "Prometheus and m3 so a little bit about myself I'm the CTO at a startup called",
    "start": "31529",
    "end": "39660"
  },
  {
    "text": "chronosphere previously it was the m3 and n 30 B technical lead at uber and",
    "start": "39660",
    "end": "45170"
  },
  {
    "text": "these are open source metrics platforms that are compatible with Prometheus is a",
    "start": "45170",
    "end": "50280"
  },
  {
    "text": "remote storage back-end and I'm gonna open metric a contributor which is a CNC",
    "start": "50280",
    "end": "56280"
  },
  {
    "text": "F project and I've been on that committee for two years now so I wanted",
    "start": "56280",
    "end": "62489"
  },
  {
    "text": "to talk to you about the state of monitoring a little bit just to cover the context of logs metrics traces the",
    "start": "62489",
    "end": "70710"
  },
  {
    "text": "different signals that a lot of us rely on today for instrumentation and observability of our systems then I also",
    "start": "70710",
    "end": "76920"
  },
  {
    "text": "wanted to talk to you today about combining logs metrics and traces and what that looks like today and then also",
    "start": "76920",
    "end": "83130"
  },
  {
    "text": "of course wanted to talk to you about the topic that I submitted here and this",
    "start": "83130",
    "end": "88799"
  },
  {
    "text": "is a the issue of deep linking metrics and traces and what that looks like so I",
    "start": "88799",
    "end": "97350"
  },
  {
    "text": "wanted to go through yeah just cover a little bit of context on some of these core signals that we use today for observability and and monitoring so logs",
    "start": "97350",
    "end": "106860"
  },
  {
    "text": "essentially are used have been used since the dawn of time they've been made",
    "start": "106860",
    "end": "112860"
  },
  {
    "text": "more useful of course with the advent of fast log search whether that's using indexing or or not they're usually not",
    "start": "112860",
    "end": "120119"
  },
  {
    "text": "structured much some have had more success here at structuring their logs",
    "start": "120119",
    "end": "125460"
  },
  {
    "text": "than others and that essentially comes down to at your organization you know what kind of logging frameworks you're",
    "start": "125460",
    "end": "131340"
  },
  {
    "text": "using what kind of metadata you're attached what kind of restrictions you have on your log messages but at the end",
    "start": "131340",
    "end": "137340"
  },
  {
    "text": "of the day still a lot of us tend to have a significant amount of uncertain",
    "start": "137340",
    "end": "143250"
  },
  {
    "text": "unstructured components to most log messages so log output from a process",
    "start": "143250",
    "end": "149010"
  },
  {
    "text": "can all of course be straight into one place and collected somewhere and then of course you can do your slicing and dicing on it but fundamentally there's",
    "start": "149010",
    "end": "156060"
  },
  {
    "text": "usually some unstructured nough stew parts of your logs so now I want to talk",
    "start": "156060",
    "end": "162629"
  },
  {
    "text": "to you a little bit about metrics so you know metrics have also been around forever as well they're usually a little",
    "start": "162629",
    "end": "168330"
  },
  {
    "text": "bit more structured than logs they basically because of this big become",
    "start": "168330",
    "end": "175170"
  },
  {
    "text": "faster to query and aggregate for specific queries that you that you know",
    "start": "175170",
    "end": "180480"
  },
  {
    "text": "ahead of time and with lots of log data you know if you're socially frequently frequently searching that and",
    "start": "180480",
    "end": "186209"
  },
  {
    "text": "aggregating log mess roar log messages and visualizing them they tend to be a",
    "start": "186209",
    "end": "192750"
  },
  {
    "text": "little bit slower at that because you know the very data structures that that",
    "start": "192750",
    "end": "197790"
  },
  {
    "text": "they are are slower to aggregate by the collection time and also slower to query",
    "start": "197790",
    "end": "203940"
  },
  {
    "text": "and aggregate at query time so you know of course the downside of this though is",
    "start": "203940",
    "end": "209879"
  },
  {
    "text": "that they don't have per event detail and you usually can use them for high-level analysis however you it stops",
    "start": "209879",
    "end": "218519"
  },
  {
    "text": "short being able to show you the things like the request and response details so you",
    "start": "218519",
    "end": "225390"
  },
  {
    "text": "know looking a little bit about the popularity of metrics Google Trends is never a good way to measure anything but",
    "start": "225390",
    "end": "232170"
  },
  {
    "text": "you can see here that at least four people trying to educate themselves on you know popular metrics formats",
    "start": "232170",
    "end": "238620"
  },
  {
    "text": "especially over the past decade the the last two three or four years have has",
    "start": "238620",
    "end": "244200"
  },
  {
    "text": "seen an acceleration or people interested in this topic and you know I wanted to share some numbers here that",
    "start": "244200",
    "end": "250890"
  },
  {
    "text": "were presented at prompt con just two weeks ago actually about the install",
    "start": "250890",
    "end": "257880"
  },
  {
    "text": "base of Prometheus so the number of Prometheus instances",
    "start": "257880",
    "end": "264060"
  },
  {
    "text": "being used in Griffin ER has grown four to 5x year of a year for the past four",
    "start": "264060",
    "end": "269850"
  },
  {
    "text": "years last year in 2018 54,000 people had a Prometheus datasource installed in",
    "start": "269850",
    "end": "275790"
  },
  {
    "text": "their governor this year two hundred forty two thousand which is is incredibly huge huge amount of growth so",
    "start": "275790",
    "end": "284940"
  },
  {
    "text": "you know talking a little bit about tracing what that is this is just one actually pretty poor and not very",
    "start": "284940",
    "end": "292080"
  },
  {
    "text": "interesting trace but it this is an example of what tracing his it's",
    "start": "292080",
    "end": "297480"
  },
  {
    "text": "essentially the way of looking at event data broken down by the operations that your code paths are taking so whether",
    "start": "297480",
    "end": "304950"
  },
  {
    "text": "that's in a you know a model if architecture or a micro service architecture it essentially has parallel",
    "start": "304950",
    "end": "310920"
  },
  {
    "text": "operation data stitched together and relate into each other visually with",
    "start": "310920",
    "end": "316770"
  },
  {
    "text": "relevant timing information so you can see on the wall clock you know how long things are taking",
    "start": "316770",
    "end": "322250"
  },
  {
    "text": "executing certain requests and response flows and then of course yeah this is",
    "start": "322250",
    "end": "327990"
  },
  {
    "text": "even a worse measure perhaps of popularity than the Google trend search but there's nothing like Griffin or",
    "start": "327990",
    "end": "335730"
  },
  {
    "text": "stats for for any kind of tracing projects out there so you know here we're just looking at the github star",
    "start": "335730",
    "end": "341760"
  },
  {
    "text": "numbers over time for the past few years of j√§ger which is one of the most popular solutions out there for for open",
    "start": "341760",
    "end": "349950"
  },
  {
    "text": "open source tracing open Zipkin is another also very popular and then I",
    "start": "349950",
    "end": "355110"
  },
  {
    "text": "wanted to give you a you know one perspective on on these three signals you know you could think of logs is",
    "start": "355110",
    "end": "361770"
  },
  {
    "text": "essentially when they are all string to one place as a ton of different unique events essentially and when they're",
    "start": "361770",
    "end": "368970"
  },
  {
    "text": "stream to one place it's they're all bundled together and yes you can aggregate and slice and dice in them using a log back-end but this is how at",
    "start": "368970",
    "end": "376620"
  },
  {
    "text": "least they're structured internally and strained to get to your back-end so a",
    "start": "376620",
    "end": "381860"
  },
  {
    "text": "perspective on traces these are submitted and basically assembled as",
    "start": "381860",
    "end": "388550"
  },
  {
    "text": "linked events so events that might have happened from different applications in",
    "start": "388550",
    "end": "393870"
  },
  {
    "text": "your in a microservice architecture or different components in monolith architecture all stitched",
    "start": "393870",
    "end": "399339"
  },
  {
    "text": "together and and how they're related together is intrinsically captured as part of a trace and then of course",
    "start": "399339",
    "end": "405729"
  },
  {
    "text": "metrics you know is basically a report or a categorized view of the events coming out of applications and so it's",
    "start": "405729",
    "end": "412659"
  },
  {
    "text": "it's really like with taking a spreadsheet and then creating a graph from the raw events that you capture in",
    "start": "412659",
    "end": "418839"
  },
  {
    "text": "a spreadsheet that's that's really what metrics has been you know typically used for and that's that's why obviously it's",
    "start": "418839",
    "end": "427689"
  },
  {
    "text": "it's made in a format that can be easily aggregated at collection and query time and it's much better for volumetric",
    "start": "427689",
    "end": "434379"
  },
  {
    "text": "analysis because you take essentially you know samples at at periods in times rather than just collecting every single",
    "start": "434379",
    "end": "441219"
  },
  {
    "text": "thing the drawback of course though is that you don't have the event data anymore so when you need to root cause",
    "start": "441219",
    "end": "446529"
  },
  {
    "text": "an issue you need to drill down further you need to see maybe request response details between two micro-services in",
    "start": "446529",
    "end": "453639"
  },
  {
    "text": "your architecture doesn't have that but again there many reasons why it's it's",
    "start": "453639",
    "end": "460539"
  },
  {
    "text": "very they're very useful so I wanted to talk to you a little bit today now about",
    "start": "460539",
    "end": "465729"
  },
  {
    "start": "463000",
    "end": "747000"
  },
  {
    "text": "what does combining logs metrics and traces today look like so increasingly",
    "start": "465729",
    "end": "472599"
  },
  {
    "text": "more observability platforms are providing at least two signals perhaps",
    "start": "472599",
    "end": "477819"
  },
  {
    "text": "all three four four observability for monitoring and for instrumenting your",
    "start": "477819",
    "end": "483669"
  },
  {
    "text": "systems so here we see Prometheus as well as Lukie you know this is basically",
    "start": "483669",
    "end": "490360"
  },
  {
    "text": "all two projects that you can use in the graph on a UI to basically view metrics",
    "start": "490360",
    "end": "496899"
  },
  {
    "text": "and traces sorry metrics and logs rather and then if we talk a little bit about what the integration looks like with",
    "start": "496899",
    "end": "503979"
  },
  {
    "text": "metrics and traces today there usually is that there's a few products out there",
    "start": "503979",
    "end": "509439"
  },
  {
    "text": "that do this but they're all basically using some form of matching of metric",
    "start": "509439",
    "end": "514659"
  },
  {
    "text": "metadata basically like labels if we're talking about Prometheus and a time",
    "start": "514659",
    "end": "520268"
  },
  {
    "text": "window so for instance you might have like an arrant era graph here with a count of era values over time and then",
    "start": "520269",
    "end": "528010"
  },
  {
    "text": "some of the metadata on these on these metrics basically can help",
    "start": "528010",
    "end": "534370"
  },
  {
    "text": "describe where those which container those metrics came from and then essentially those are used to try and",
    "start": "534370",
    "end": "541780"
  },
  {
    "text": "work out which trace may be representative of the metric that you're",
    "start": "541780",
    "end": "547240"
  },
  {
    "text": "looking at and you know if you do if you do a if you have a lot of very good",
    "start": "547240",
    "end": "552850"
  },
  {
    "text": "structure internally to how you collect metrics and traces and you do a stellar",
    "start": "552850",
    "end": "557890"
  },
  {
    "text": "job at basically keeping these systems configured very similarly in the the",
    "start": "557890",
    "end": "563080"
  },
  {
    "text": "tags and labels that they use then you can then you can basically narrow down the search space that you're looking for",
    "start": "563080",
    "end": "568960"
  },
  {
    "text": "and usually you'll find a representative trace some of the time depending on your",
    "start": "568960",
    "end": "574300"
  },
  {
    "text": "sampling strategy of course and we'll talk more about that in a minute so you know this this is a this works",
    "start": "574300",
    "end": "582130"
  },
  {
    "text": "for again if you're basically very meticulous and how you can figure everything if you sample it a hundred",
    "start": "582130",
    "end": "587740"
  },
  {
    "text": "percent and basically have made sure",
    "start": "587740",
    "end": "594340"
  },
  {
    "text": "that this is going to work well unfortunately you know that's not the",
    "start": "594340",
    "end": "599830"
  },
  {
    "text": "real world and a lot of the time most of the useful graphs that you're looking at in a metric dashboard as well has some",
    "start": "599830",
    "end": "607210"
  },
  {
    "text": "kind of aggregate function applied which actually will drop the metadata so the labels for Prometheus metrics and the",
    "start": "607210",
    "end": "615280"
  },
  {
    "text": "context then is basically lost so if you have a sum of error counts across all of",
    "start": "615280",
    "end": "620560"
  },
  {
    "text": "your containers any viewing that in a dashboard there's there's no way that you can really extract those labels out",
    "start": "620560",
    "end": "628330"
  },
  {
    "text": "they've been aggregated together to give you an aggregate view in the metrics dashboard so it's it's a lot harder to",
    "start": "628330",
    "end": "633460"
  },
  {
    "text": "go from a metric graph straight to a trace and again of course it's only magical when you store every trace",
    "start": "633460",
    "end": "639700"
  },
  {
    "text": "otherwise you might just be jumping to the the tracing UI and there's just nothing there because the trace for that",
    "start": "639700",
    "end": "645550"
  },
  {
    "text": "data point wasn't sampled and you know",
    "start": "645550",
    "end": "650890"
  },
  {
    "text": "one thing that's interesting of course here is that actually saman from slack",
    "start": "650890",
    "end": "656740"
  },
  {
    "text": "on Monday gave a talk and the observability practitioners summit you know where you mentioned of course that",
    "start": "656740",
    "end": "661780"
  },
  {
    "text": "and you know it's it's like they basically only do one two cent of sampling of traces in production",
    "start": "661780",
    "end": "668470"
  },
  {
    "text": "so you know depending on your organization and depending on on how how",
    "start": "668470",
    "end": "675760"
  },
  {
    "text": "volumetric things are for you with your back ends it just might not be possible to store you know a hundred percent of",
    "start": "675760",
    "end": "682540"
  },
  {
    "text": "traces and then you know we would really like the experience to be something of",
    "start": "682540",
    "end": "688660"
  },
  {
    "text": "looking at an exact data point on a graph and being able to go from a",
    "start": "688660",
    "end": "693820"
  },
  {
    "text": "request that comprised that data point exactly so if you're looking at a 500 internal server error that happened on a",
    "start": "693820",
    "end": "700330"
  },
  {
    "text": "specific HTTP route that has very low frequency traffic then you you you want",
    "start": "700330",
    "end": "707740"
  },
  {
    "text": "to be able to look at that exact issue and you want to have high confidence is going to be traced there to be able to",
    "start": "707740",
    "end": "713080"
  },
  {
    "text": "go directly from a graph to a trace so that brings us to deep linking metrics",
    "start": "713080",
    "end": "718750"
  },
  {
    "text": "and traces so essentially you know what deep linking metrics and traces are is",
    "start": "718750",
    "end": "724120"
  },
  {
    "text": "it's a concept where we basically embed a tiny bit of metadata in the metric",
    "start": "724120",
    "end": "729340"
  },
  {
    "text": "itself that we use as a foreign key into a trace that is captured as part of the",
    "start": "729340",
    "end": "736930"
  },
  {
    "text": "request that the backend system is currently processing when it increments",
    "start": "736930",
    "end": "742780"
  },
  {
    "text": "a metric counter or records a latency measurement in your metrics and so I'm",
    "start": "742780",
    "end": "748930"
  },
  {
    "start": "747000",
    "end": "925000"
  },
  {
    "text": "going to run through a demo of that right now so we first we're going to show a Jager",
    "start": "748930",
    "end": "756100"
  },
  {
    "text": "example application sorry the Jager example application and open source this",
    "start": "756100",
    "end": "761410"
  },
  {
    "text": "has been modified to basically use the the new open metrics x position format which is a Prometheus extended metric",
    "start": "761410",
    "end": "769210"
  },
  {
    "text": "disposition format with support for exemplars and then we're going to show you how some of the changes we're trying",
    "start": "769210",
    "end": "774790"
  },
  {
    "text": "to upstream to grow finer to to leverage the deep links that we've made between metrics and traces the first this is our",
    "start": "774790",
    "end": "782560"
  },
  {
    "text": "Jager demo application it's basically generating a whole bunch of synthetic",
    "start": "782560",
    "end": "789820"
  },
  {
    "text": "transactions which you're simulating requests and and basically generating",
    "start": "789820",
    "end": "794860"
  },
  {
    "text": "metrics from that as well as traces and then if we jump over base to our our metrics here in Griffin er",
    "start": "794860",
    "end": "802320"
  },
  {
    "text": "which are coming back from from m3 actually so we're using Prometheus to",
    "start": "802320",
    "end": "808150"
  },
  {
    "text": "scrape these metrics and then Prometheus is remember writing these metrics to m3",
    "start": "808150",
    "end": "813270"
  },
  {
    "text": "with the exemplars attached all the way from the emit open metrics exposition format to the back end we can see here",
    "start": "813270",
    "end": "820660"
  },
  {
    "text": "that trace IDs show up next to data points in our graphs now and so if we",
    "start": "820660",
    "end": "827440"
  },
  {
    "text": "just like silo out the 500 internal server errors that that are occurring in",
    "start": "827440",
    "end": "832630"
  },
  {
    "text": "some of these simulated requests we can see that we can just go directly from one of these errors in the graph to a",
    "start": "832630",
    "end": "839680"
  },
  {
    "text": "representative trace and then you know if we want to try to drill down into this issue say we've been paged we've",
    "start": "839680",
    "end": "846040"
  },
  {
    "text": "just looked at our graph we've clicked on a on a trace we can now basically see",
    "start": "846040",
    "end": "851200"
  },
  {
    "text": "that this this span here so this part of the operation in as part of the slash",
    "start": "851200",
    "end": "858430"
  },
  {
    "text": "dispatch HTTP request is failing so basically it comes into you know slash",
    "start": "858430",
    "end": "864040"
  },
  {
    "text": "dispatch which then makes a self request to the same application actually",
    "start": "864040",
    "end": "869620"
  },
  {
    "text": "internally called slash customer and then basically front find nearest driver",
    "start": "869620",
    "end": "875590"
  },
  {
    "text": "is is another operation that's being called as part of this request and essentially this is the the operation",
    "start": "875590",
    "end": "883360"
  },
  {
    "text": "that's causing the error here and we can see here that this is due to a redis timeout after trying to basically look",
    "start": "883360",
    "end": "890950"
  },
  {
    "text": "up some some details for for a user account so that's kind of basically the",
    "start": "890950",
    "end": "901570"
  },
  {
    "text": "crux of what we're doing here we're just enabling the ability to go from a single jump from a metric dashboard and a",
    "start": "901570",
    "end": "907480"
  },
  {
    "text": "single data point exactly directly to a directly to a trace and so you know I",
    "start": "907480",
    "end": "915490"
  },
  {
    "text": "wanted to explain a little bit more about this and then of course kind of work through step through piece by piece",
    "start": "915490",
    "end": "922770"
  },
  {
    "text": "what how we receiving this so this is essentially the picture of all the demo",
    "start": "922770",
    "end": "930490"
  },
  {
    "start": "925000",
    "end": "1192000"
  },
  {
    "text": "and then like what we're what we're doing so we have an application we're using open telemetry",
    "start": "930490",
    "end": "936160"
  },
  {
    "text": "which is a instrumentation SDK essentially open telemetry makes it a",
    "start": "936160",
    "end": "942940"
  },
  {
    "text": "little bit easier to collect metrics traces logs and export them to a variety",
    "start": "942940",
    "end": "948010"
  },
  {
    "text": "of different supported backends so you",
    "start": "948010",
    "end": "954100"
  },
  {
    "text": "know whether that's a vendor whether that's Prometheus and whether that's Jaeger for traces it doesn't really",
    "start": "954100",
    "end": "960580"
  },
  {
    "text": "matter do you write the code once and it exports to these different formats and",
    "start": "960580",
    "end": "966340"
  },
  {
    "text": "and backends and so I wanted to dive in",
    "start": "966340",
    "end": "971590"
  },
  {
    "text": "a little bit more into open telemetry so you can see basically here this is an",
    "start": "971590",
    "end": "977110"
  },
  {
    "text": "example of us using a gauge to track as",
    "start": "977110",
    "end": "982720"
  },
  {
    "text": "a with a metric the number of jobs queued inside a a server so maybe you're",
    "start": "982720",
    "end": "988120"
  },
  {
    "text": "sending out some emails or some other long running tasks and essentially here you're also capturing a trip spans to",
    "start": "988120",
    "end": "997540"
  },
  {
    "text": "basically try to model the operations that are going on internally so here we're before we incur a job we create a",
    "start": "997540",
    "end": "1003840"
  },
  {
    "text": "span would to capture the job in queue operation we're doing and then you know",
    "start": "1003840",
    "end": "1010170"
  },
  {
    "text": "highlighting here the context that we're passing to the metric gauge when we set",
    "start": "1010170",
    "end": "1015210"
  },
  {
    "text": "the the current value of the jobs currently in queued that context because",
    "start": "1015210",
    "end": "1020520"
  },
  {
    "text": "that context is associated with that job in queue operation and that job in queue operation is part of a trace which has a",
    "start": "1020520",
    "end": "1027060"
  },
  {
    "text": "trace ID internally open telemetry is able to provide the metrics exposition",
    "start": "1027060",
    "end": "1033540"
  },
  {
    "text": "format which you know such as open metrics which is Prometheus based the",
    "start": "1033540",
    "end": "1039300"
  },
  {
    "text": "trace ID that this request was part of so looking a little bit more at open",
    "start": "1039300",
    "end": "1046260"
  },
  {
    "text": "metrics so this is the project of this is basically the Prometheus exposition",
    "start": "1046260",
    "end": "1051480"
  },
  {
    "text": "format with with extensions for exemplars and a whole bunch of other things but that's a whole whole",
    "start": "1051480",
    "end": "1057090"
  },
  {
    "text": "different talk and I could talk a lot of about open metrics but essentially it is",
    "start": "1057090",
    "end": "1062430"
  },
  {
    "text": "now here in this demo basically propagating the trace ID correlated to the metrics",
    "start": "1062430",
    "end": "1068220"
  },
  {
    "text": "that we captured and this is a little bit about what open metrics looks like on the wire if you're familiar with the",
    "start": "1068220",
    "end": "1074970"
  },
  {
    "text": "Prometheus exposition format it might look very familiar and basically exemplars here are",
    "start": "1074970",
    "end": "1080539"
  },
  {
    "text": "captured as a trailing comment next to a",
    "start": "1080539",
    "end": "1085890"
  },
  {
    "text": "metric value so essentially here we have some counters for HTTP requests total",
    "start": "1085890",
    "end": "1093500"
  },
  {
    "text": "broken down by different status codes and we also have some histogram histogram values that were also tracking",
    "start": "1093500",
    "end": "1101360"
  },
  {
    "text": "and each one of these has a corresponding trace ID so basically we two of these is at least one trace",
    "start": "1101360",
    "end": "1108059"
  },
  {
    "text": "associated with that that metric and that label set so and you know putting",
    "start": "1108059",
    "end": "1116250"
  },
  {
    "text": "it all together we're asking you know how does this look like from end to end",
    "start": "1116250",
    "end": "1122220"
  },
  {
    "text": "essentially we are collecting the the metrics and traces together because we",
    "start": "1122220",
    "end": "1127950"
  },
  {
    "text": "do that we are able to associate the trace ID to the metrics that gets basically transmitted to Prometheus",
    "start": "1127950",
    "end": "1134730"
  },
  {
    "text": "using the open metrics exposition format and then Prometheus is essentially",
    "start": "1134730",
    "end": "1141409"
  },
  {
    "text": "taking that exemplars during added memory and then sending it can then of course send it on to a a back end and",
    "start": "1141409",
    "end": "1149100"
  },
  {
    "text": "then in this example we're gonna use you know we are using a 3 DB here and 3 DB",
    "start": "1149100",
    "end": "1154830"
  },
  {
    "text": "is essentially an open source project as I said which came out of Heber it was a",
    "start": "1154830",
    "end": "1160860"
  },
  {
    "text": "it was a basically the core back end for the metrics platform at ruber I'm and is",
    "start": "1160860",
    "end": "1168990"
  },
  {
    "text": "you know Prometheus from my storage compatible back-end and it it kind of",
    "start": "1168990",
    "end": "1176070"
  },
  {
    "text": "howls more than 10 billion metrics said uber and ran across thousands of",
    "start": "1176070",
    "end": "1181110"
  },
  {
    "text": "machines comprising 2 to 3% of the computer footprint at re-roof so we use",
    "start": "1181110",
    "end": "1186360"
  },
  {
    "text": "it we used it at over for a global view of metrics data across regions so diving",
    "start": "1186360",
    "end": "1194309"
  },
  {
    "start": "1192000",
    "end": "1302000"
  },
  {
    "text": "it a little bit more to how Prometheus and 3 work together here essentially Prometheus scrapes the exemplar",
    "start": "1194309",
    "end": "1199909"
  },
  {
    "text": "and as I mentioned it will keep it locally in memory so you can kind of have that same experience that you saw",
    "start": "1199909",
    "end": "1205279"
  },
  {
    "text": "in the dashboard just using Prometheus loom and then in this demo we're writing it to m3 which actually stores it in",
    "start": "1205279",
    "end": "1213019"
  },
  {
    "text": "durable storage next before you write next year metric for whatever retention",
    "start": "1213019",
    "end": "1218059"
  },
  {
    "text": "period you've configured in through your and 3db to store that exists or metrics for so here you can see that a single",
    "start": "1218059",
    "end": "1225529"
  },
  {
    "text": "storage really a single stored value bit packs in the m3 DB column format and",
    "start": "1225529",
    "end": "1233389"
  },
  {
    "text": "basically time stamp has the sorry the first part of this component is the",
    "start": "1233389",
    "end": "1238579"
  },
  {
    "text": "timestamp Delta Delta bits the second part is the float64 value deltas or bits and then third part is exactly the",
    "start": "1238579",
    "end": "1245419"
  },
  {
    "text": "exemplar bits now trace ID has relatively high entropy because it's usually based on a UUID and uu IDs I'm",
    "start": "1245419",
    "end": "1253039"
  },
  {
    "text": "providing you with high entropy so there's no collisions however there are certain things you can do to try and",
    "start": "1253039",
    "end": "1258859"
  },
  {
    "text": "compress this a little bit better than then you might might think but that's a",
    "start": "1258859",
    "end": "1266239"
  },
  {
    "text": "topic for another day so when querying the data m3 query make sure to keep at least one representative exemplar data",
    "start": "1266239",
    "end": "1271879"
  },
  {
    "text": "point as part of the result even after applying a sum or a histogram quantile",
    "start": "1271879",
    "end": "1277369"
  },
  {
    "text": "function or any kind of other aggregation function and it does this by",
    "start": "1277369",
    "end": "1282859"
  },
  {
    "text": "basically every time you fold one of your data points together when you're",
    "start": "1282859",
    "end": "1289069"
  },
  {
    "text": "doing an aggregate function by basically just making sure that it keeps at least one of the Associated trace IDs to the",
    "start": "1289069",
    "end": "1295249"
  },
  {
    "text": "data points for that given time stamp in step size that you're you're currently",
    "start": "1295249",
    "end": "1300649"
  },
  {
    "text": "calculating so there's still one big caveat here though with this approach",
    "start": "1300649",
    "end": "1305929"
  },
  {
    "text": "we're still relying on traces existing you're now tracing back end that we picked to correlate to our metric data",
    "start": "1305929",
    "end": "1311659"
  },
  {
    "text": "point so you know when you're talking about applications or services maybe not",
    "start": "1311659",
    "end": "1319069"
  },
  {
    "text": "in a single container but across many containers that process thousands or if not hundreds of thousands of requests in",
    "start": "1319069",
    "end": "1325489"
  },
  {
    "text": "a given time window you know especially if you're basically collecting data points at a 60 second resolution but you",
    "start": "1325489",
    "end": "1331519"
  },
  {
    "text": "can also obviously step that down to lower as well this is relatively efficient because you have on the metrics back",
    "start": "1331519",
    "end": "1338610"
  },
  {
    "text": "inside because you have a single data point stored with a single trace ID correlated to that data point however on",
    "start": "1338610",
    "end": "1347220"
  },
  {
    "text": "the tracing backend side you need to store all basically thousands of traces",
    "start": "1347220",
    "end": "1352679"
  },
  {
    "text": "here to guarantee that the trace that he correlated with the metric data point is going to be available when you click",
    "start": "1352679",
    "end": "1358830"
  },
  {
    "text": "that trace ID in the graph and and you want to jump from the metric data point to the trace in the tracing system",
    "start": "1358830",
    "end": "1365580"
  },
  {
    "text": "because the tracing system doesn't have any idea of which trace ID you ended up picking wouldn't it be nice if the trace",
    "start": "1365580",
    "end": "1373320"
  },
  {
    "text": "ID we picked as an exemplar for any given metric data point could be guaranteed to be stored in our tracing",
    "start": "1373320",
    "end": "1379799"
  },
  {
    "text": "back end so it turns out that using metrics aggregation is actually a really",
    "start": "1379799",
    "end": "1386850"
  },
  {
    "start": "1382000",
    "end": "1462000"
  },
  {
    "text": "it has a lot of upsides basically for keeping traces that are interesting to",
    "start": "1386850",
    "end": "1394139"
  },
  {
    "text": "the human firstly it aggregates them across time so basically it's going to make sure that you consistently get a",
    "start": "1394139",
    "end": "1400559"
  },
  {
    "text": "certain amount of traces at a useful time interval that's meaningful to you metric tags are also great capturing",
    "start": "1400559",
    "end": "1408149"
  },
  {
    "text": "unique combinations of events so you know you already break up your metrics",
    "start": "1408149",
    "end": "1413190"
  },
  {
    "text": "by era success era status code the latency bucket that a data point is",
    "start": "1413190",
    "end": "1419100"
  },
  {
    "text": "falling into sorry a request is falling into for a representative data point this also ensures basically unique",
    "start": "1419100",
    "end": "1426480"
  },
  {
    "text": "combinations of types of traces because you know a lot of the sampling today head base sampling and adaptive sampling",
    "start": "1426480",
    "end": "1432269"
  },
  {
    "text": "and tracing is all based on as soon as you start a trace the incoming data of",
    "start": "1432269",
    "end": "1437460"
  },
  {
    "text": "the request so basically you can try to categorize and take one of each type but",
    "start": "1437460",
    "end": "1443460"
  },
  {
    "text": "that type can only be calculated using information at the beginning of a trace so you know what I should peer out or",
    "start": "1443460",
    "end": "1449309"
  },
  {
    "text": "what services are but it can't use things like how long that request took or how if that request like served an",
    "start": "1449309",
    "end": "1456269"
  },
  {
    "text": "error and the reason it can't is because it has to decide whether it's going to sample it at the beginning of the trace so putting it all together again",
    "start": "1456269",
    "end": "1466570"
  },
  {
    "text": "basically using metric aggregation can allow you to select the interesting",
    "start": "1466570",
    "end": "1472570"
  },
  {
    "text": "traces across tags and dimensions that are meaningful to you in your labels for",
    "start": "1472570",
    "end": "1480280"
  },
  {
    "text": "your metrics so basically in this model we wait until metrics are aggregated and",
    "start": "1480280",
    "end": "1486130"
  },
  {
    "text": "then we basically call out to a cache that is holding all old traces that were",
    "start": "1486130",
    "end": "1494220"
  },
  {
    "text": "sampling and for this model to work you really need a hundred percent sampling at the application layer and then",
    "start": "1494220",
    "end": "1500770"
  },
  {
    "text": "holding those 100 percent of traces in in a cache you can spread that across",
    "start": "1500770",
    "end": "1505870"
  },
  {
    "text": "like a typical well-known cash like Redis or memcache D or something and essentially once metrics have been",
    "start": "1505870",
    "end": "1513520"
  },
  {
    "text": "weighted and collected you basically choose exactly the trace IDs that you",
    "start": "1513520",
    "end": "1519450"
  },
  {
    "text": "chose to correlate to your data points out of that cache and make sure that",
    "start": "1519450",
    "end": "1525250"
  },
  {
    "text": "they're stored in your tracing back-end and it's worth mentioning here of course that you know you can of course have",
    "start": "1525250",
    "end": "1531910"
  },
  {
    "text": "that in addition to your default one to two percent sampling strategy using head based sampling or adaptive sampling and",
    "start": "1531910",
    "end": "1540490"
  },
  {
    "start": "1539000",
    "end": "1644000"
  },
  {
    "text": "so you know I wanted to kind of give obviously a update of the status of you",
    "start": "1540490",
    "end": "1547750"
  },
  {
    "text": "know this project this is the current end-to-end demo that I ran today however",
    "start": "1547750",
    "end": "1554740"
  },
  {
    "text": "that demo realize on a lot of upstream changes some of which should be merged some of which haven't some of which also",
    "start": "1554740",
    "end": "1561970"
  },
  {
    "text": "require discussion with the community and developers of Prometheus and of open",
    "start": "1561970",
    "end": "1569320"
  },
  {
    "text": "telemetry so basically the demo is also currently using Jaeger to export the",
    "start": "1569320",
    "end": "1576460"
  },
  {
    "text": "traces and the open metrics exposition format itself due to basically our RF is",
    "start": "1576460",
    "end": "1583630"
  },
  {
    "text": "trying to use the the current head development check out of open telemetry",
    "start": "1583630",
    "end": "1589750"
  },
  {
    "text": "and and not quite being able to make that work with with our current demo and",
    "start": "1589750",
    "end": "1595690"
  },
  {
    "text": "I also wanted to mention that the Capri Thea's client library requires significant and there needs to be an in-depth",
    "start": "1595690",
    "end": "1602600"
  },
  {
    "text": "discussion with the maintainer x' which they're obviously we're kind of in talks",
    "start": "1602600",
    "end": "1607730"
  },
  {
    "text": "about that on how to make this a clean kind of change so a shout out like as I",
    "start": "1607730",
    "end": "1615620"
  },
  {
    "text": "mentioned to to everyone who's been like basically oh the Prometheus contributors",
    "start": "1615620",
    "end": "1620690"
  },
  {
    "text": "a few people that co-founder helping discuss review and and make some",
    "start": "1620690",
    "end": "1625700"
  },
  {
    "text": "progress with us on on this and if you",
    "start": "1625700",
    "end": "1630740"
  },
  {
    "text": "want to download this slides at the end of the talk which I'll I didn't wasn't able to do right before the talk so I",
    "start": "1630740",
    "end": "1635810"
  },
  {
    "text": "apologize about that but basically here here are those links you can download slides and then kind of like Splunk",
    "start": "1635810",
    "end": "1642260"
  },
  {
    "text": "around the different projects and with that I just want to say thank you for being here thanks for coming and you",
    "start": "1642260",
    "end": "1649670"
  },
  {
    "start": "1644000",
    "end": "1684000"
  },
  {
    "text": "come say hi at our booth where yes again we work from I work for chronosphere and where we have a booth in the sponsor",
    "start": "1649670",
    "end": "1656750"
  },
  {
    "text": "booth but yeah thanks so much for for coming seeing the talk if you have a question",
    "start": "1656750",
    "end": "1666020"
  },
  {
    "text": "please raise your hand I'll bring the mic also while I'm while I'm finding",
    "start": "1666020",
    "end": "1673790"
  },
  {
    "text": "questions please take a moment to open up the sked app and rate the session we",
    "start": "1673790",
    "end": "1679730"
  },
  {
    "text": "really need the feedback to make a better scoop con next time my my",
    "start": "1679730",
    "end": "1686570"
  },
  {
    "start": "1684000",
    "end": "1772000"
  },
  {
    "text": "question is that this this traces send the matrix do usually do it for",
    "start": "1686570",
    "end": "1692000"
  },
  {
    "text": "applications for like for traces for inside information or you it's possible",
    "start": "1692000",
    "end": "1699380"
  },
  {
    "text": "to embed it I mean do you know cases that the traces could be embedded for",
    "start": "1699380",
    "end": "1704570"
  },
  {
    "text": "applications that shipped to customers that's a good question I've definitely",
    "start": "1704570",
    "end": "1710540"
  },
  {
    "text": "directed organizations where your instrumentation coming from a mobile",
    "start": "1710540",
    "end": "1716870"
  },
  {
    "text": "like phone application is transmitted to the back end I'm not sure if that's",
    "start": "1716870",
    "end": "1722750"
  },
  {
    "text": "exactly kind of what you're mentioning but basically yeah there's obviously the",
    "start": "1722750",
    "end": "1729620"
  },
  {
    "text": "ability to link client-side traces with server-side trace it does require like a pretty robust",
    "start": "1729620",
    "end": "1736730"
  },
  {
    "text": "tracing pipeline where you know your mobile client knows how to send up",
    "start": "1736730",
    "end": "1742280"
  },
  {
    "text": "relevant metadata that you captured internally so that it can be linked to",
    "start": "1742280",
    "end": "1747620"
  },
  {
    "text": "the tracing data and the server back inside and a lot of a lot of companies every everyone I've seen they're all",
    "start": "1747620",
    "end": "1754430"
  },
  {
    "text": "doing that a little bit differently so unless they're using like some vendor solution that's out of the box but even",
    "start": "1754430",
    "end": "1760550"
  },
  {
    "text": "then I don't know of any vendor solution that's doing tracing that works both in the mobile application and the backend",
    "start": "1760550",
    "end": "1765920"
  },
  {
    "text": "and joins the traces together I just I just haven't seen that it out there generally available for questions so",
    "start": "1765920",
    "end": "1775670"
  },
  {
    "start": "1772000",
    "end": "1871000"
  },
  {
    "text": "you're just taking example or in that particular period threat so I guess the",
    "start": "1775670",
    "end": "1781070"
  },
  {
    "text": "question does it explode like the cardinality like within Prometheus yeah",
    "start": "1781070",
    "end": "1786200"
  },
  {
    "text": "great question so that's what's really nice about this implementation is that you're not actually adding a label to",
    "start": "1786200",
    "end": "1793730"
  },
  {
    "text": "the metric itself which generates you know would generate a new time series in the backend this is actually taking the",
    "start": "1793730",
    "end": "1800450"
  },
  {
    "text": "existing time series that already has an existing set of labels and right next to",
    "start": "1800450",
    "end": "1805880"
  },
  {
    "text": "the float64 value that you capture and the time stamp that you capture for that data point you're packing a few extra",
    "start": "1805880",
    "end": "1812060"
  },
  {
    "text": "well a lot of extra bits actually if you're recording a UUID to record the",
    "start": "1812060",
    "end": "1818300"
  },
  {
    "text": "data points so it doesn't change the cardinality of your metrics whatsoever but it does make the bytes per sample go",
    "start": "1818300",
    "end": "1824660"
  },
  {
    "text": "up by a significant amount depending on what you're using for trace IDs now that",
    "start": "1824660",
    "end": "1830450"
  },
  {
    "text": "would be scary if every single metric had a trace ID correlated to it but if",
    "start": "1830450",
    "end": "1835670"
  },
  {
    "text": "you really there's definitely ways to basically configure this so that only certain metrics may be your RPC metrics",
    "start": "1835670",
    "end": "1842710"
  },
  {
    "text": "maybe you're like front-end you know HT PG RPC metrics actually get",
    "start": "1842710",
    "end": "1848360"
  },
  {
    "text": "this annotations on the trace IDs sorry off trace IDs to them which means like you know basically only a few",
    "start": "1848360",
    "end": "1854780"
  },
  {
    "text": "percent of your overall metrics ever get an exemple are associated with it and that helps control the overall extra",
    "start": "1854780",
    "end": "1861440"
  },
  {
    "text": "space required for music exemplars that we found but thankfully it doesn't touch the",
    "start": "1861440",
    "end": "1866710"
  },
  {
    "text": "Nellie at all as well no matter no matter how you can figure it hey can you",
    "start": "1866710",
    "end": "1873039"
  },
  {
    "start": "1871000",
    "end": "1958000"
  },
  {
    "text": "talk more about how you're able to find the unique combinations of metrics like where's this logic handled or stored a",
    "start": "1873039",
    "end": "1879539"
  },
  {
    "text": "combination of metrics yes so that's a great question as well you know if you",
    "start": "1879539",
    "end": "1884590"
  },
  {
    "text": "basically typically use Prometheus out of the box you are kind of guaranteed",
    "start": "1884590",
    "end": "1891850"
  },
  {
    "text": "that if as you have like a container label so you know basically it's gonna",
    "start": "1891850",
    "end": "1897070"
  },
  {
    "text": "subdivide all your metrics by the process that actually gathered them because of the the label on the container basically within m3 is a",
    "start": "1897070",
    "end": "1906490"
  },
  {
    "text": "little bit different in that it has an aggregation to here that sits in front of that if you're using just default m3",
    "start": "1906490",
    "end": "1914320"
  },
  {
    "text": "that actually is an embedded process that happens in a coordinator and it's",
    "start": "1914320",
    "end": "1919929"
  },
  {
    "text": "called the entry coordinator that sits as a side car next year Prometheus instance but if you're not running a",
    "start": "1919929",
    "end": "1926049"
  },
  {
    "text": "side car model then you have a central replicated in free aggregator server the",
    "start": "1926049",
    "end": "1931390"
  },
  {
    "text": "metrics will flow through that and you basically configure rules which look",
    "start": "1931390",
    "end": "1936460"
  },
  {
    "text": "very you know which basically are recording rules in Prometheus land to to",
    "start": "1936460",
    "end": "1943539"
  },
  {
    "text": "like kind of mesh metrics together to reduce the cardinality for across the",
    "start": "1943539",
    "end": "1949000"
  },
  {
    "text": "queries so it's kind of that process of that's what determines of like which labels end up in your metrics eventually",
    "start": "1949000",
    "end": "1954909"
  },
  {
    "text": "thank you here not the question about",
    "start": "1954909",
    "end": "1960250"
  },
  {
    "start": "1958000",
    "end": "2031000"
  },
  {
    "text": "the database is it is this certificate good good for storing apprentice events and then match them to okay not for",
    "start": "1960250",
    "end": "1967570"
  },
  {
    "text": "trade not was trace lot but with metrics maybe with race locks as well because events they usually start for one hour",
    "start": "1967570",
    "end": "1973390"
  },
  {
    "text": "and it's they need to be storage as well just so just to clarify you're you're",
    "start": "1973390",
    "end": "1979450"
  },
  {
    "text": "asking can things like infer DB be used for storing events yeah yeah yeah that's",
    "start": "1979450",
    "end": "1987279"
  },
  {
    "text": "a good question and so I mean I would say that it's definitely was not",
    "start": "1987279",
    "end": "1992470"
  },
  {
    "text": "designed at first to do that and that I think that or vanilla column stores I",
    "start": "1992470",
    "end": "2000540"
  },
  {
    "text": "probably better at doing it in 3d B does have support for storing data like data",
    "start": "2000540",
    "end": "2007140"
  },
  {
    "text": "that's encoded in protobuf so you can store like native protobuf messages over",
    "start": "2007140",
    "end": "2013890"
  },
  {
    "text": "time for a time series but I wouldn't really advise it and this Tara there's not much if any documentation on how to",
    "start": "2013890",
    "end": "2020280"
  },
  {
    "text": "do that with him Freddy B so yeah I personally would just use it typical like some other columns store for that",
    "start": "2020280",
    "end": "2025740"
  },
  {
    "text": "like I don't click house or you know there's plenty of stuff out there that",
    "start": "2025740",
    "end": "2030800"
  },
  {
    "text": "oh and I'm not sure I understood how what the state is in Prometheus about",
    "start": "2030800",
    "end": "2037410"
  },
  {
    "start": "2031000",
    "end": "2112000"
  },
  {
    "text": "exemplar addition to the matrix yeah yeah great question so you we merged one",
    "start": "2037410",
    "end": "2045120"
  },
  {
    "text": "PR now that the rest are all is still open end or in discussion the yeah that",
    "start": "2045120",
    "end": "2052139"
  },
  {
    "text": "we merged was adding the support for pausing the exemplars in the open",
    "start": "2052140",
    "end": "2058379"
  },
  {
    "text": "metrics exposition format however Prometheus right now even if using a client library that supports that",
    "start": "2058380",
    "end": "2064580"
  },
  {
    "text": "exposition format of which right now is only the Python client library Prometheus will will do nothing with",
    "start": "2064580",
    "end": "2071520"
  },
  {
    "text": "that it basically throws it away there is like as in that demo there's you know",
    "start": "2071520",
    "end": "2077040"
  },
  {
    "text": "a link to a like basically the modified Prometheus that we're using that we're",
    "start": "2077040",
    "end": "2082350"
  },
  {
    "text": "trying to upstream that basically puts that in a memory storage and then allows that memory storage to be accessed as",
    "start": "2082350",
    "end": "2087990"
  },
  {
    "text": "you're writing remote values to an a remote storage but that's a very",
    "start": "2087990",
    "end": "2094290"
  },
  {
    "text": "controversial PR that I think will go back and forth for a little while and I honestly Callum at Edgar fauna is kind",
    "start": "2094290",
    "end": "2102600"
  },
  {
    "text": "of actually carving out that piece of work to work on himself and so we're gonna probably let him take that from",
    "start": "2102600",
    "end": "2108960"
  },
  {
    "text": "there but we'll just see what happens I think that's gonna be the last question",
    "start": "2108960",
    "end": "2114270"
  },
  {
    "text": "unless anyone's really desperate so amazing idea to connect traces and",
    "start": "2114270",
    "end": "2121050"
  },
  {
    "text": "metrics as you were talking I was thinking whether you all have thought about the other way around so instead of",
    "start": "2121050",
    "end": "2126630"
  },
  {
    "text": "having a foreign key within metrics and connecting that to traces let's say",
    "start": "2126630",
    "end": "2131730"
  },
  {
    "text": "having a notion of a foreign key within the trace as baggage and then using that to connect to metric so I'm imagining all",
    "start": "2131730",
    "end": "2139319"
  },
  {
    "text": "the labels associated with the metrics as tags into the traces and then tail",
    "start": "2139319",
    "end": "2145799"
  },
  {
    "text": "sampling based on unique combination of those and keeping every unique trees",
    "start": "2145799",
    "end": "2151279"
  },
  {
    "text": "it's a very interesting idea yeah I definitely will help you honest no I",
    "start": "2151279",
    "end": "2156569"
  },
  {
    "text": "haven't thought about that I think what's interesting is when you introduce",
    "start": "2156569",
    "end": "2162329"
  },
  {
    "text": "aggregations to this a lot of this stuff becomes possible because you're basically holding a whole lot of state",
    "start": "2162329",
    "end": "2168690"
  },
  {
    "text": "in memory as data is flowing through the system problem with that that's a district that's a relatively stateful",
    "start": "2168690",
    "end": "2174599"
  },
  {
    "text": "system that as you scale up you know is obviously another get another distributed system that you're running alongside your database so it's it's",
    "start": "2174599",
    "end": "2183420"
  },
  {
    "text": "definitely not a small feat to do you could do that because you're using the metrics aggregation to choose the trace",
    "start": "2183420",
    "end": "2189930"
  },
  {
    "text": "ID so you have the relevant information of it but yeah I haven't thought about how you would store that or how that",
    "start": "2189930",
    "end": "2195390"
  },
  {
    "text": "that efficiency model would look like that's a very interesting idea yeah okay",
    "start": "2195390",
    "end": "2202079"
  },
  {
    "text": "remember the feedback on the sched app and thank you very much Rob thank [Applause]",
    "start": "2202079",
    "end": "2209089"
  }
]